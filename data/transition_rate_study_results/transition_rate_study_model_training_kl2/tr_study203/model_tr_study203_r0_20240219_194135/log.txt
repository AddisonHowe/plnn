Args:
Namespace(name='model_tr_study203', outdir='out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0', training_data='data/transition_rate_studies/tr_study203/tr_study203_training/r0', validation_data='data/transition_rate_studies/tr_study203/tr_study203_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3482517076

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 12.352762100051708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.352762100051708 | validation: 13.139263229005135]
	TIME [epoch: 52.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.704729390813783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.704729390813783 | validation: 8.56824126306755]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.396411121827619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.396411121827619 | validation: 6.004700020039966]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.658295609900568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.658295609900568 | validation: 6.236804950592745]
	TIME [epoch: 8.47 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.352605427357789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.352605427357789 | validation: 4.991811343936792]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.604542009779442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.604542009779442 | validation: 6.89423689469907]
	TIME [epoch: 8.48 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.81394959085629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.81394959085629 | validation: 8.604859659085724]
	TIME [epoch: 8.48 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.337544662549961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.337544662549961 | validation: 5.165220624438575]
	TIME [epoch: 8.47 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.037022539749737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.037022539749737 | validation: 4.9384172988047865]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.989081213185648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.989081213185648 | validation: 5.491259275794487]
	TIME [epoch: 8.47 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.250261577150742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.250261577150742 | validation: 5.686609541619839]
	TIME [epoch: 8.47 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.440943651982041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.440943651982041 | validation: 5.262249214757836]
	TIME [epoch: 8.47 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.250664552901107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.250664552901107 | validation: 6.574179116194319]
	TIME [epoch: 8.48 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.171754802338898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.171754802338898 | validation: 6.007563991751384]
	TIME [epoch: 8.48 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.127272033620693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.127272033620693 | validation: 4.558908529128966]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.326477619212769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.326477619212769 | validation: 5.17831129375257]
	TIME [epoch: 8.47 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.458087336584016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.458087336584016 | validation: 4.462397704430423]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.2574451254332555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2574451254332555 | validation: 4.499770020760906]
	TIME [epoch: 8.47 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.825485809236742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.825485809236742 | validation: 3.661083061277427]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.560088289457146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.560088289457146 | validation: 4.033091650722909]
	TIME [epoch: 8.44 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.5313248825674695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5313248825674695 | validation: 3.679252024454451]
	TIME [epoch: 8.44 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.411552687313227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.411552687313227 | validation: 2.966166939362066]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1103236404094865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1103236404094865 | validation: 3.328491165185355]
	TIME [epoch: 8.46 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.054519342456912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.054519342456912 | validation: 3.479203839178811]
	TIME [epoch: 8.46 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.198713436556161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.198713436556161 | validation: 3.224655356870823]
	TIME [epoch: 8.46 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.938163213693111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.938163213693111 | validation: 2.8373134869898298]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.0606068574621785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0606068574621785 | validation: 3.014298679617917]
	TIME [epoch: 8.47 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.337532419875293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.337532419875293 | validation: 2.5302916896722705]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.993682687902288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.993682687902288 | validation: 5.013373843573502]
	TIME [epoch: 8.46 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.306320750084721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.306320750084721 | validation: 2.8539134514364006]
	TIME [epoch: 8.48 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6970895926959337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6970895926959337 | validation: 3.1311542983373877]
	TIME [epoch: 8.47 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7551111389219614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7551111389219614 | validation: 2.3496320597837173]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.726077127325135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.726077127325135 | validation: 2.4978256339655642]
	TIME [epoch: 8.46 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6327502023209726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6327502023209726 | validation: 2.6742618526836073]
	TIME [epoch: 8.45 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.067105715339697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.067105715339697 | validation: 2.2596527486083726]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.594588116395667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.594588116395667 | validation: 2.519239452088477]
	TIME [epoch: 8.46 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4744207584019633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4744207584019633 | validation: 3.007256733509352]
	TIME [epoch: 8.46 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5416284100612145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5416284100612145 | validation: 2.464868645381958]
	TIME [epoch: 8.46 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.430166131336223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.430166131336223 | validation: 2.5816507142776723]
	TIME [epoch: 8.48 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.500737668030471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.500737668030471 | validation: 2.3509856828715776]
	TIME [epoch: 8.47 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3740939215068764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3740939215068764 | validation: 2.332487462112865]
	TIME [epoch: 8.45 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4267640164689914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4267640164689914 | validation: 2.3891220122671415]
	TIME [epoch: 8.47 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.344760524503416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.344760524503416 | validation: 2.6809139446379993]
	TIME [epoch: 8.45 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4224335972032223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4224335972032223 | validation: 2.686109295759306]
	TIME [epoch: 8.49 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3989780621428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3989780621428 | validation: 2.6856616546133516]
	TIME [epoch: 8.46 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3780376752689607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3780376752689607 | validation: 2.468204274825423]
	TIME [epoch: 8.46 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3200126379380066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3200126379380066 | validation: 2.5894919414235202]
	TIME [epoch: 8.45 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2676714844942567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2676714844942567 | validation: 3.0017172503593623]
	TIME [epoch: 8.47 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.239607474802713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.239607474802713 | validation: 3.3867887420004683]
	TIME [epoch: 8.46 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5315146320394732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5315146320394732 | validation: 2.5750781477505154]
	TIME [epoch: 8.45 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.225036435948738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.225036435948738 | validation: 2.628952426169866]
	TIME [epoch: 8.45 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.369504498023274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.369504498023274 | validation: 1.9853459469636383]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.299372709440708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.299372709440708 | validation: 2.103931858279887]
	TIME [epoch: 8.47 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.689027900084046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.689027900084046 | validation: 2.271559656300588]
	TIME [epoch: 8.45 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5795138326945155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5795138326945155 | validation: 2.275156661830846]
	TIME [epoch: 8.46 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2726631982686682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2726631982686682 | validation: 2.577391540535428]
	TIME [epoch: 8.46 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3598398615631155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3598398615631155 | validation: 2.1684746289397765]
	TIME [epoch: 8.47 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2204775602842277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2204775602842277 | validation: 2.3484462148570624]
	TIME [epoch: 8.45 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.213665544719395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.213665544719395 | validation: 2.5861704959487146]
	TIME [epoch: 8.45 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7055962035331156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7055962035331156 | validation: 1.919593048009666]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.330440441743993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.330440441743993 | validation: 4.260550894905801]
	TIME [epoch: 8.48 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6136989497655123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6136989497655123 | validation: 4.795540264939389]
	TIME [epoch: 8.46 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5234880152891925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5234880152891925 | validation: 2.193183691609242]
	TIME [epoch: 8.45 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2968455388527547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2968455388527547 | validation: 1.8840379264687743]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.187373599448596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.187373599448596 | validation: 1.909488012459823]
	TIME [epoch: 8.45 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2337756765480266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2337756765480266 | validation: 2.270074486987269]
	TIME [epoch: 8.47 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.158060505018895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.158060505018895 | validation: 2.130208456690679]
	TIME [epoch: 8.45 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1298486417574467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1298486417574467 | validation: 2.2842984541629257]
	TIME [epoch: 8.44 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9808724188105327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9808724188105327 | validation: 1.7383167202256997]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.104082859693985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.104082859693985 | validation: 2.381854319388779]
	TIME [epoch: 8.47 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.231870144427589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.231870144427589 | validation: 2.475013975686778]
	TIME [epoch: 8.46 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1668393901353125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1668393901353125 | validation: 2.168608267047276]
	TIME [epoch: 8.44 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4624836585514203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4624836585514203 | validation: 1.9207374711879042]
	TIME [epoch: 8.44 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1720336941123577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1720336941123577 | validation: 2.1125221301638333]
	TIME [epoch: 8.44 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.390372770861211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.390372770861211 | validation: 1.9600791898832823]
	TIME [epoch: 8.47 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0221552596695416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0221552596695416 | validation: 2.1893241184966863]
	TIME [epoch: 8.46 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4832284063183963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4832284063183963 | validation: 1.89186951464628]
	TIME [epoch: 8.44 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.054933398527314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.054933398527314 | validation: 2.039405325183614]
	TIME [epoch: 8.45 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.050941606784796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.050941606784796 | validation: 2.0107235157168026]
	TIME [epoch: 8.46 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0968142301147954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0968142301147954 | validation: 2.7743375351396375]
	TIME [epoch: 8.44 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6040133366379146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6040133366379146 | validation: 1.9855140391755857]
	TIME [epoch: 8.43 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9503507937942666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9503507937942666 | validation: 1.9104157648096651]
	TIME [epoch: 8.43 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.975408928563559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.975408928563559 | validation: 2.098208877346433]
	TIME [epoch: 8.45 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0434813921660027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0434813921660027 | validation: 1.699740471458201]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.922710574933268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.922710574933268 | validation: 2.116512235269769]
	TIME [epoch: 8.43 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0514215582232938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0514215582232938 | validation: 2.296423679354835]
	TIME [epoch: 8.43 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.960584015201979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.960584015201979 | validation: 1.6820709236345632]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.865825354290286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.865825354290286 | validation: 1.9835135851142165]
	TIME [epoch: 8.45 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9359819291635283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9359819291635283 | validation: 1.941000692016459]
	TIME [epoch: 8.45 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.799531079440379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.799531079440379 | validation: 3.3195738115615514]
	TIME [epoch: 8.44 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.678740623263825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.678740623263825 | validation: 3.315566675047329]
	TIME [epoch: 8.45 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4236966329362133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4236966329362133 | validation: 2.1095363162266496]
	TIME [epoch: 8.48 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9073957197437883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9073957197437883 | validation: 1.8257332254523368]
	TIME [epoch: 8.44 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.954921901518929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.954921901518929 | validation: 2.061002959616283]
	TIME [epoch: 8.44 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.895792934216023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.895792934216023 | validation: 1.9981468297768181]
	TIME [epoch: 8.44 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.921740937700602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.921740937700602 | validation: 1.9951335295555226]
	TIME [epoch: 8.47 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8638838590505196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8638838590505196 | validation: 1.6905437807465216]
	TIME [epoch: 8.44 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8094467832356558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8094467832356558 | validation: 2.2761668956896157]
	TIME [epoch: 8.42 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8764768282996362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8764768282996362 | validation: 2.5232568563149447]
	TIME [epoch: 8.44 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0053815480617248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0053815480617248 | validation: 1.554350048588069]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.925073550500434		[learning rate: 0.0099673]
	Learning Rate: 0.00996733
	LOSS [training: 2.925073550500434 | validation: 2.462514700179177]
	TIME [epoch: 8.45 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8761679768343984		[learning rate: 0.0099312]
	Learning Rate: 0.00993116
	LOSS [training: 2.8761679768343984 | validation: 2.136422919889088]
	TIME [epoch: 8.43 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9241171201131904		[learning rate: 0.0098951]
	Learning Rate: 0.00989512
	LOSS [training: 2.9241171201131904 | validation: 1.6206250005005514]
	TIME [epoch: 8.44 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9750522438390585		[learning rate: 0.0098592]
	Learning Rate: 0.00985921
	LOSS [training: 2.9750522438390585 | validation: 1.7309536539973103]
	TIME [epoch: 8.44 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.007934067182833		[learning rate: 0.0098234]
	Learning Rate: 0.00982343
	LOSS [training: 3.007934067182833 | validation: 1.7021189545685558]
	TIME [epoch: 8.47 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.833298508169775		[learning rate: 0.0097878]
	Learning Rate: 0.00978778
	LOSS [training: 2.833298508169775 | validation: 1.7322929112097862]
	TIME [epoch: 8.44 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7830604642889805		[learning rate: 0.0097523]
	Learning Rate: 0.00975226
	LOSS [training: 2.7830604642889805 | validation: 1.7685581906475232]
	TIME [epoch: 8.43 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.809788747776767		[learning rate: 0.0097169]
	Learning Rate: 0.00971687
	LOSS [training: 2.809788747776767 | validation: 2.0125709947385286]
	TIME [epoch: 8.42 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8207386216175254		[learning rate: 0.0096816]
	Learning Rate: 0.0096816
	LOSS [training: 2.8207386216175254 | validation: 1.7018091658617802]
	TIME [epoch: 8.44 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.792652710562349		[learning rate: 0.0096465]
	Learning Rate: 0.00964647
	LOSS [training: 2.792652710562349 | validation: 1.859135273766167]
	TIME [epoch: 8.43 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7078403532182063		[learning rate: 0.0096115]
	Learning Rate: 0.00961146
	LOSS [training: 2.7078403532182063 | validation: 2.558399041547556]
	TIME [epoch: 8.42 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.867474542363433		[learning rate: 0.0095766]
	Learning Rate: 0.00957658
	LOSS [training: 2.867474542363433 | validation: 1.7562612533550637]
	TIME [epoch: 8.44 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7826413211629526		[learning rate: 0.0095418]
	Learning Rate: 0.00954183
	LOSS [training: 2.7826413211629526 | validation: 1.5002722482076951]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7440668498664085		[learning rate: 0.0095072]
	Learning Rate: 0.0095072
	LOSS [training: 2.7440668498664085 | validation: 1.9226445159422996]
	TIME [epoch: 8.47 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7292447111485227		[learning rate: 0.0094727]
	Learning Rate: 0.0094727
	LOSS [training: 2.7292447111485227 | validation: 1.5002755265870917]
	TIME [epoch: 8.43 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6284324650797		[learning rate: 0.0094383]
	Learning Rate: 0.00943832
	LOSS [training: 2.6284324650797 | validation: 1.6616297989142372]
	TIME [epoch: 8.44 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5481649884315907		[learning rate: 0.0094041]
	Learning Rate: 0.00940407
	LOSS [training: 2.5481649884315907 | validation: 1.9732466335979093]
	TIME [epoch: 8.44 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4667377643194843		[learning rate: 0.0093699]
	Learning Rate: 0.00936994
	LOSS [training: 2.4667377643194843 | validation: 1.127777373985294]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.265778334169185		[learning rate: 0.0093359]
	Learning Rate: 0.00933594
	LOSS [training: 2.265778334169185 | validation: 1.2125585377573467]
	TIME [epoch: 8.45 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2407426260299816		[learning rate: 0.0093021]
	Learning Rate: 0.00930206
	LOSS [training: 2.2407426260299816 | validation: 0.8459759262782063]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7663606961709233		[learning rate: 0.0092683]
	Learning Rate: 0.0092683
	LOSS [training: 2.7663606961709233 | validation: 0.9140820610651231]
	TIME [epoch: 8.44 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6562070138582627		[learning rate: 0.0092347]
	Learning Rate: 0.00923466
	LOSS [training: 2.6562070138582627 | validation: 1.0624044637438559]
	TIME [epoch: 8.44 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.364890112889794		[learning rate: 0.0092011]
	Learning Rate: 0.00920115
	LOSS [training: 2.364890112889794 | validation: 0.860862318262529]
	TIME [epoch: 8.45 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.261974895881065		[learning rate: 0.0091678]
	Learning Rate: 0.00916776
	LOSS [training: 2.261974895881065 | validation: 0.7367352050945974]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1023331860211054		[learning rate: 0.0091345]
	Learning Rate: 0.00913449
	LOSS [training: 2.1023331860211054 | validation: 1.575546430863525]
	TIME [epoch: 8.45 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5358545073442573		[learning rate: 0.0091013]
	Learning Rate: 0.00910134
	LOSS [training: 2.5358545073442573 | validation: 1.1896698439092706]
	TIME [epoch: 8.44 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2972171534214976		[learning rate: 0.0090683]
	Learning Rate: 0.00906831
	LOSS [training: 2.2972171534214976 | validation: 0.9910149041849388]
	TIME [epoch: 8.46 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.156630697897116		[learning rate: 0.0090354]
	Learning Rate: 0.0090354
	LOSS [training: 2.156630697897116 | validation: 0.8311449134631451]
	TIME [epoch: 8.44 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.188319099480225		[learning rate: 0.0090026]
	Learning Rate: 0.00900261
	LOSS [training: 2.188319099480225 | validation: 1.8126800319041214]
	TIME [epoch: 8.44 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.337240819165541		[learning rate: 0.0089699]
	Learning Rate: 0.00896994
	LOSS [training: 2.337240819165541 | validation: 0.9436208649576783]
	TIME [epoch: 8.43 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.421443730754688		[learning rate: 0.0089374]
	Learning Rate: 0.00893739
	LOSS [training: 2.421443730754688 | validation: 1.1212681369641386]
	TIME [epoch: 8.45 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2487528776634904		[learning rate: 0.008905]
	Learning Rate: 0.00890495
	LOSS [training: 2.2487528776634904 | validation: 1.1017222420981212]
	TIME [epoch: 8.45 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2099560241032146		[learning rate: 0.0088726]
	Learning Rate: 0.00887263
	LOSS [training: 2.2099560241032146 | validation: 1.11279599831228]
	TIME [epoch: 8.45 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2128584784320777		[learning rate: 0.0088404]
	Learning Rate: 0.00884044
	LOSS [training: 2.2128584784320777 | validation: 1.1391624129225701]
	TIME [epoch: 8.42 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.40750896843314		[learning rate: 0.0088084]
	Learning Rate: 0.00880835
	LOSS [training: 2.40750896843314 | validation: 0.9908065493900697]
	TIME [epoch: 8.44 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.167512841931064		[learning rate: 0.0087764]
	Learning Rate: 0.00877639
	LOSS [training: 2.167512841931064 | validation: 1.0911377791563528]
	TIME [epoch: 8.46 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.077839017500966		[learning rate: 0.0087445]
	Learning Rate: 0.00874454
	LOSS [training: 2.077839017500966 | validation: 0.8630732579380871]
	TIME [epoch: 8.45 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9883940271612854		[learning rate: 0.0087128]
	Learning Rate: 0.0087128
	LOSS [training: 1.9883940271612854 | validation: 0.8862842795011643]
	TIME [epoch: 8.43 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.360485233070229		[learning rate: 0.0086812]
	Learning Rate: 0.00868118
	LOSS [training: 2.360485233070229 | validation: 1.8473486925457145]
	TIME [epoch: 8.44 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.214471090280618		[learning rate: 0.0086497]
	Learning Rate: 0.00864968
	LOSS [training: 2.214471090280618 | validation: 0.626085007063675]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0590391043613137		[learning rate: 0.0086183]
	Learning Rate: 0.00861829
	LOSS [training: 2.0590391043613137 | validation: 1.2775279290847812]
	TIME [epoch: 8.44 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.102585643848374		[learning rate: 0.008587]
	Learning Rate: 0.00858701
	LOSS [training: 2.102585643848374 | validation: 0.8068506902230079]
	TIME [epoch: 8.44 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9765834355580387		[learning rate: 0.0085559]
	Learning Rate: 0.00855585
	LOSS [training: 1.9765834355580387 | validation: 1.3487189148888337]
	TIME [epoch: 8.43 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1601791869341236		[learning rate: 0.0085248]
	Learning Rate: 0.0085248
	LOSS [training: 2.1601791869341236 | validation: 1.118432323407525]
	TIME [epoch: 8.46 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.090559827220172		[learning rate: 0.0084939]
	Learning Rate: 0.00849386
	LOSS [training: 2.090559827220172 | validation: 1.5223744183005616]
	TIME [epoch: 8.44 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2149864458503417		[learning rate: 0.008463]
	Learning Rate: 0.00846304
	LOSS [training: 2.2149864458503417 | validation: 0.81100328011836]
	TIME [epoch: 8.43 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1208876828370484		[learning rate: 0.0084323]
	Learning Rate: 0.00843233
	LOSS [training: 2.1208876828370484 | validation: 1.0081210077068996]
	TIME [epoch: 8.44 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1588763242174673		[learning rate: 0.0084017]
	Learning Rate: 0.00840172
	LOSS [training: 2.1588763242174673 | validation: 1.2593871268799781]
	TIME [epoch: 8.46 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.339174002781155		[learning rate: 0.0083712]
	Learning Rate: 0.00837123
	LOSS [training: 2.339174002781155 | validation: 1.4416326687896417]
	TIME [epoch: 8.44 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2637948317115173		[learning rate: 0.0083409]
	Learning Rate: 0.00834085
	LOSS [training: 2.2637948317115173 | validation: 1.1116636623675409]
	TIME [epoch: 8.43 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9404635249472044		[learning rate: 0.0083106]
	Learning Rate: 0.00831059
	LOSS [training: 1.9404635249472044 | validation: 1.4624314920385506]
	TIME [epoch: 8.43 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2370661553767777		[learning rate: 0.0082804]
	Learning Rate: 0.00828043
	LOSS [training: 2.2370661553767777 | validation: 0.962017412342723]
	TIME [epoch: 8.45 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0147774045393336		[learning rate: 0.0082504]
	Learning Rate: 0.00825037
	LOSS [training: 2.0147774045393336 | validation: 0.88993356565946]
	TIME [epoch: 8.45 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9857863643360545		[learning rate: 0.0082204]
	Learning Rate: 0.00822043
	LOSS [training: 1.9857863643360545 | validation: 0.8177988870190724]
	TIME [epoch: 8.44 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0617477017516896		[learning rate: 0.0081906]
	Learning Rate: 0.0081906
	LOSS [training: 2.0617477017516896 | validation: 1.171527201518804]
	TIME [epoch: 8.44 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0123426155509847		[learning rate: 0.0081609]
	Learning Rate: 0.00816088
	LOSS [training: 2.0123426155509847 | validation: 1.0562111956029083]
	TIME [epoch: 8.44 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0897367952113437		[learning rate: 0.0081313]
	Learning Rate: 0.00813126
	LOSS [training: 2.0897367952113437 | validation: 0.866390602908909]
	TIME [epoch: 8.46 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0716996216759123		[learning rate: 0.0081018]
	Learning Rate: 0.00810175
	LOSS [training: 2.0716996216759123 | validation: 0.8146743895986712]
	TIME [epoch: 8.44 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9660891132200724		[learning rate: 0.0080724]
	Learning Rate: 0.00807235
	LOSS [training: 1.9660891132200724 | validation: 1.0349547068048999]
	TIME [epoch: 8.44 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.19938306208896		[learning rate: 0.0080431]
	Learning Rate: 0.00804305
	LOSS [training: 2.19938306208896 | validation: 0.8272513171600431]
	TIME [epoch: 8.44 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4193077200690487		[learning rate: 0.0080139]
	Learning Rate: 0.00801387
	LOSS [training: 2.4193077200690487 | validation: 0.7264310239601756]
	TIME [epoch: 8.46 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0316108161560065		[learning rate: 0.0079848]
	Learning Rate: 0.00798478
	LOSS [training: 2.0316108161560065 | validation: 0.7179060125353236]
	TIME [epoch: 8.44 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0800368434101157		[learning rate: 0.0079558]
	Learning Rate: 0.00795581
	LOSS [training: 2.0800368434101157 | validation: 0.8819185080324581]
	TIME [epoch: 8.44 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9039850787794799		[learning rate: 0.0079269]
	Learning Rate: 0.00792693
	LOSS [training: 1.9039850787794799 | validation: 0.7331392729270925]
	TIME [epoch: 8.45 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.460081018717184		[learning rate: 0.0078982]
	Learning Rate: 0.00789817
	LOSS [training: 2.460081018717184 | validation: 0.6776697071491893]
	TIME [epoch: 8.46 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0052271627154292		[learning rate: 0.0078695]
	Learning Rate: 0.0078695
	LOSS [training: 2.0052271627154292 | validation: 0.6920960225164773]
	TIME [epoch: 8.44 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.231064566898411		[learning rate: 0.0078409]
	Learning Rate: 0.00784094
	LOSS [training: 2.231064566898411 | validation: 0.7439096903561235]
	TIME [epoch: 8.44 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9641366116933696		[learning rate: 0.0078125]
	Learning Rate: 0.00781249
	LOSS [training: 1.9641366116933696 | validation: 0.7992750624223062]
	TIME [epoch: 8.44 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9524520634669833		[learning rate: 0.0077841]
	Learning Rate: 0.00778414
	LOSS [training: 1.9524520634669833 | validation: 1.2530400352287165]
	TIME [epoch: 8.46 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.061909139058432		[learning rate: 0.0077559]
	Learning Rate: 0.00775589
	LOSS [training: 2.061909139058432 | validation: 0.6228134906755998]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.951725586663925		[learning rate: 0.0077277]
	Learning Rate: 0.00772774
	LOSS [training: 1.951725586663925 | validation: 0.8044457031085178]
	TIME [epoch: 8.44 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3006267366271613		[learning rate: 0.0076997]
	Learning Rate: 0.0076997
	LOSS [training: 2.3006267366271613 | validation: 1.2641799130439577]
	TIME [epoch: 8.44 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9910599364465782		[learning rate: 0.0076718]
	Learning Rate: 0.00767176
	LOSS [training: 1.9910599364465782 | validation: 0.6781836462308407]
	TIME [epoch: 8.47 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9256143288848069		[learning rate: 0.0076439]
	Learning Rate: 0.00764391
	LOSS [training: 1.9256143288848069 | validation: 1.6479771735190822]
	TIME [epoch: 8.45 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3746128285585413		[learning rate: 0.0076162]
	Learning Rate: 0.00761617
	LOSS [training: 2.3746128285585413 | validation: 1.42987436934731]
	TIME [epoch: 8.44 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.990911451991457		[learning rate: 0.0075885]
	Learning Rate: 0.00758853
	LOSS [training: 1.990911451991457 | validation: 0.6150078403500971]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9430872732477393		[learning rate: 0.007561]
	Learning Rate: 0.00756099
	LOSS [training: 1.9430872732477393 | validation: 1.5081080128451196]
	TIME [epoch: 8.45 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8862259701463604		[learning rate: 0.0075336]
	Learning Rate: 0.00753356
	LOSS [training: 1.8862259701463604 | validation: 0.7453511030836744]
	TIME [epoch: 8.45 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7154567466560837		[learning rate: 0.0075062]
	Learning Rate: 0.00750622
	LOSS [training: 1.7154567466560837 | validation: 0.669081765954818]
	TIME [epoch: 8.45 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4502235221556465		[learning rate: 0.007479]
	Learning Rate: 0.00747897
	LOSS [training: 1.4502235221556465 | validation: 1.163302325393308]
	TIME [epoch: 8.44 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2730191472310006		[learning rate: 0.0074518]
	Learning Rate: 0.00745183
	LOSS [training: 1.2730191472310006 | validation: 0.6990908272515008]
	TIME [epoch: 8.44 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1242942569940733		[learning rate: 0.0074248]
	Learning Rate: 0.00742479
	LOSS [training: 1.1242942569940733 | validation: 0.7254642936579198]
	TIME [epoch: 8.47 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4188235485816791		[learning rate: 0.0073978]
	Learning Rate: 0.00739784
	LOSS [training: 1.4188235485816791 | validation: 1.8664995283595402]
	TIME [epoch: 8.45 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5224961716816616		[learning rate: 0.007371]
	Learning Rate: 0.007371
	LOSS [training: 1.5224961716816616 | validation: 1.3631901583427428]
	TIME [epoch: 8.45 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4344055076941034		[learning rate: 0.0073442]
	Learning Rate: 0.00734425
	LOSS [training: 1.4344055076941034 | validation: 1.3790136046934025]
	TIME [epoch: 8.45 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3680181212099112		[learning rate: 0.0073176]
	Learning Rate: 0.0073176
	LOSS [training: 1.3680181212099112 | validation: 0.8318627059774473]
	TIME [epoch: 8.46 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.413415284418834		[learning rate: 0.007291]
	Learning Rate: 0.00729104
	LOSS [training: 1.413415284418834 | validation: 0.7466341528812945]
	TIME [epoch: 8.45 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2874424648107325		[learning rate: 0.0072646]
	Learning Rate: 0.00726458
	LOSS [training: 1.2874424648107325 | validation: 1.0486419242223204]
	TIME [epoch: 8.45 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4577938819049496		[learning rate: 0.0072382]
	Learning Rate: 0.00723822
	LOSS [training: 1.4577938819049496 | validation: 0.8203397992522488]
	TIME [epoch: 8.44 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.069325393763933		[learning rate: 0.0072119]
	Learning Rate: 0.00721195
	LOSS [training: 1.069325393763933 | validation: 0.7550841859047341]
	TIME [epoch: 8.46 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4031558957529922		[learning rate: 0.0071858]
	Learning Rate: 0.00718578
	LOSS [training: 1.4031558957529922 | validation: 0.7703367636640648]
	TIME [epoch: 8.45 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.970511263258053		[learning rate: 0.0071597]
	Learning Rate: 0.0071597
	LOSS [training: 0.970511263258053 | validation: 0.8254654590710406]
	TIME [epoch: 8.45 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1849410917706569		[learning rate: 0.0071337]
	Learning Rate: 0.00713371
	LOSS [training: 1.1849410917706569 | validation: 0.8343380820989196]
	TIME [epoch: 8.45 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1192693819640471		[learning rate: 0.0071078]
	Learning Rate: 0.00710783
	LOSS [training: 1.1192693819640471 | validation: 0.7851118490818074]
	TIME [epoch: 8.47 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.105624882181101		[learning rate: 0.007082]
	Learning Rate: 0.00708203
	LOSS [training: 1.105624882181101 | validation: 0.5118642480551578]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.960382618520805		[learning rate: 0.0070563]
	Learning Rate: 0.00705633
	LOSS [training: 0.960382618520805 | validation: 0.47290845855885]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0905225230304496		[learning rate: 0.0070307]
	Learning Rate: 0.00703072
	LOSS [training: 1.0905225230304496 | validation: 1.1690660310149508]
	TIME [epoch: 8.46 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0370752978801265		[learning rate: 0.0070052]
	Learning Rate: 0.00700521
	LOSS [training: 1.0370752978801265 | validation: 0.6010014611260014]
	TIME [epoch: 8.47 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.993246095844609		[learning rate: 0.0069798]
	Learning Rate: 0.00697979
	LOSS [training: 0.993246095844609 | validation: 0.49800919027130475]
	TIME [epoch: 8.46 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2641934610255294		[learning rate: 0.0069545]
	Learning Rate: 0.00695446
	LOSS [training: 1.2641934610255294 | validation: 0.6881700024930895]
	TIME [epoch: 8.45 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2099575541566157		[learning rate: 0.0069292]
	Learning Rate: 0.00692922
	LOSS [training: 1.2099575541566157 | validation: 0.7988366618953311]
	TIME [epoch: 8.45 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.141599650774333		[learning rate: 0.0069041]
	Learning Rate: 0.00690407
	LOSS [training: 1.141599650774333 | validation: 0.6277658616524258]
	TIME [epoch: 8.45 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1529012583517326		[learning rate: 0.006879]
	Learning Rate: 0.00687902
	LOSS [training: 1.1529012583517326 | validation: 0.7450005900664183]
	TIME [epoch: 8.47 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.055344585770815		[learning rate: 0.0068541]
	Learning Rate: 0.00685405
	LOSS [training: 1.055344585770815 | validation: 0.7981347626049566]
	TIME [epoch: 8.46 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8583655719601648		[learning rate: 0.0068292]
	Learning Rate: 0.00682918
	LOSS [training: 0.8583655719601648 | validation: 0.7557114662135296]
	TIME [epoch: 8.45 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8853633808068089		[learning rate: 0.0068044]
	Learning Rate: 0.00680439
	LOSS [training: 0.8853633808068089 | validation: 0.6822004271603413]
	TIME [epoch: 8.45 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9236067529648764		[learning rate: 0.0067797]
	Learning Rate: 0.0067797
	LOSS [training: 0.9236067529648764 | validation: 0.6369798299221703]
	TIME [epoch: 8.48 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.053563067788069		[learning rate: 0.0067551]
	Learning Rate: 0.0067551
	LOSS [training: 1.053563067788069 | validation: 0.679464001835661]
	TIME [epoch: 8.46 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8516184909858527		[learning rate: 0.0067306]
	Learning Rate: 0.00673058
	LOSS [training: 0.8516184909858527 | validation: 0.7546546225905573]
	TIME [epoch: 8.44 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9081272211753804		[learning rate: 0.0067062]
	Learning Rate: 0.00670616
	LOSS [training: 0.9081272211753804 | validation: 0.6645464628867883]
	TIME [epoch: 8.45 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9308380997752657		[learning rate: 0.0066818]
	Learning Rate: 0.00668182
	LOSS [training: 0.9308380997752657 | validation: 0.5403428853746227]
	TIME [epoch: 8.47 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8821320317578337		[learning rate: 0.0066576]
	Learning Rate: 0.00665757
	LOSS [training: 0.8821320317578337 | validation: 0.5325082543562014]
	TIME [epoch: 8.45 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8827292289924615		[learning rate: 0.0066334]
	Learning Rate: 0.00663341
	LOSS [training: 0.8827292289924615 | validation: 0.6907243302924585]
	TIME [epoch: 8.44 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8929553639306509		[learning rate: 0.0066093]
	Learning Rate: 0.00660934
	LOSS [training: 0.8929553639306509 | validation: 0.6002054981263434]
	TIME [epoch: 8.44 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.012476278183547		[learning rate: 0.0065854]
	Learning Rate: 0.00658535
	LOSS [training: 1.012476278183547 | validation: 0.5026081680802801]
	TIME [epoch: 8.47 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9810327264133031		[learning rate: 0.0065615]
	Learning Rate: 0.00656145
	LOSS [training: 0.9810327264133031 | validation: 2.330671928117633]
	TIME [epoch: 8.45 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.46968962092555		[learning rate: 0.0065376]
	Learning Rate: 0.00653764
	LOSS [training: 1.46968962092555 | validation: 1.162959112115705]
	TIME [epoch: 8.44 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.214956821608785		[learning rate: 0.0065139]
	Learning Rate: 0.00651392
	LOSS [training: 1.214956821608785 | validation: 1.3223331661686386]
	TIME [epoch: 8.45 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.195715934690906		[learning rate: 0.0064903]
	Learning Rate: 0.00649028
	LOSS [training: 1.195715934690906 | validation: 1.1435501254767688]
	TIME [epoch: 8.47 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1409813492313037		[learning rate: 0.0064667]
	Learning Rate: 0.00646672
	LOSS [training: 1.1409813492313037 | validation: 1.351174259127266]
	TIME [epoch: 8.46 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0462955274973158		[learning rate: 0.0064433]
	Learning Rate: 0.00644325
	LOSS [training: 1.0462955274973158 | validation: 0.6290515971920414]
	TIME [epoch: 8.45 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0203783244139537		[learning rate: 0.0064199]
	Learning Rate: 0.00641987
	LOSS [training: 1.0203783244139537 | validation: 0.810473430131087]
	TIME [epoch: 8.45 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8302293499065818		[learning rate: 0.0063966]
	Learning Rate: 0.00639657
	LOSS [training: 0.8302293499065818 | validation: 0.5193445414598107]
	TIME [epoch: 8.45 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0218453254643425		[learning rate: 0.0063734]
	Learning Rate: 0.00637336
	LOSS [training: 1.0218453254643425 | validation: 0.599445471928629]
	TIME [epoch: 8.47 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8322142362355269		[learning rate: 0.0063502]
	Learning Rate: 0.00635023
	LOSS [training: 0.8322142362355269 | validation: 0.6836147257058665]
	TIME [epoch: 8.45 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8078480419441691		[learning rate: 0.0063272]
	Learning Rate: 0.00632718
	LOSS [training: 0.8078480419441691 | validation: 0.8679574543710495]
	TIME [epoch: 8.46 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.025851562620726		[learning rate: 0.0063042]
	Learning Rate: 0.00630422
	LOSS [training: 1.025851562620726 | validation: 0.8577780169296925]
	TIME [epoch: 8.45 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2221916899536307		[learning rate: 0.0062813]
	Learning Rate: 0.00628134
	LOSS [training: 1.2221916899536307 | validation: 0.6974612014603463]
	TIME [epoch: 8.46 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2593509729892953		[learning rate: 0.0062585]
	Learning Rate: 0.00625855
	LOSS [training: 1.2593509729892953 | validation: 0.8619480381409999]
	TIME [epoch: 8.45 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1562300783287571		[learning rate: 0.0062358]
	Learning Rate: 0.00623584
	LOSS [training: 1.1562300783287571 | validation: 0.7875265141796095]
	TIME [epoch: 8.44 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.944468328013126		[learning rate: 0.0062132]
	Learning Rate: 0.00621321
	LOSS [training: 0.944468328013126 | validation: 0.7488619870658828]
	TIME [epoch: 8.44 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8582239008278572		[learning rate: 0.0061907]
	Learning Rate: 0.00619066
	LOSS [training: 0.8582239008278572 | validation: 0.7251034502947176]
	TIME [epoch: 8.47 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.020433444440806		[learning rate: 0.0061682]
	Learning Rate: 0.00616819
	LOSS [training: 1.020433444440806 | validation: 0.5953436514457795]
	TIME [epoch: 8.45 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.038751303913216		[learning rate: 0.0061458]
	Learning Rate: 0.00614581
	LOSS [training: 1.038751303913216 | validation: 0.5666349641309089]
	TIME [epoch: 8.44 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9532424016709923		[learning rate: 0.0061235]
	Learning Rate: 0.0061235
	LOSS [training: 0.9532424016709923 | validation: 0.6275143366964506]
	TIME [epoch: 8.44 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7819883463154105		[learning rate: 0.0061013]
	Learning Rate: 0.00610128
	LOSS [training: 0.7819883463154105 | validation: 0.4981800361017885]
	TIME [epoch: 8.46 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0388809376265749		[learning rate: 0.0060791]
	Learning Rate: 0.00607914
	LOSS [training: 1.0388809376265749 | validation: 0.5603692177497961]
	TIME [epoch: 8.45 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0029145705449758		[learning rate: 0.0060571]
	Learning Rate: 0.00605708
	LOSS [training: 1.0029145705449758 | validation: 0.6285124226370804]
	TIME [epoch: 8.44 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9679166150938439		[learning rate: 0.0060351]
	Learning Rate: 0.0060351
	LOSS [training: 0.9679166150938439 | validation: 0.639817203750156]
	TIME [epoch: 8.44 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0054032056280444		[learning rate: 0.0060132]
	Learning Rate: 0.00601319
	LOSS [training: 1.0054032056280444 | validation: 0.5778655755488853]
	TIME [epoch: 8.46 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9834243365144172		[learning rate: 0.0059914]
	Learning Rate: 0.00599137
	LOSS [training: 0.9834243365144172 | validation: 1.1892145784084764]
	TIME [epoch: 8.45 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1096057772665315		[learning rate: 0.0059696]
	Learning Rate: 0.00596963
	LOSS [training: 1.1096057772665315 | validation: 1.0410211386274808]
	TIME [epoch: 8.44 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8797509328622501		[learning rate: 0.005948]
	Learning Rate: 0.00594797
	LOSS [training: 0.8797509328622501 | validation: 0.6678083525328082]
	TIME [epoch: 8.45 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9193558382494844		[learning rate: 0.0059264]
	Learning Rate: 0.00592638
	LOSS [training: 0.9193558382494844 | validation: 0.5598715709639837]
	TIME [epoch: 8.46 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.025653449872384		[learning rate: 0.0059049]
	Learning Rate: 0.00590487
	LOSS [training: 1.025653449872384 | validation: 0.6111866790497729]
	TIME [epoch: 8.44 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0351572657687433		[learning rate: 0.0058834]
	Learning Rate: 0.00588344
	LOSS [training: 1.0351572657687433 | validation: 0.6087793515955432]
	TIME [epoch: 8.44 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8523515692859348		[learning rate: 0.0058621]
	Learning Rate: 0.00586209
	LOSS [training: 0.8523515692859348 | validation: 0.9331828509413848]
	TIME [epoch: 8.44 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.884906105596342		[learning rate: 0.0058408]
	Learning Rate: 0.00584082
	LOSS [training: 0.884906105596342 | validation: 0.7981411205159963]
	TIME [epoch: 8.46 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7823447817312299		[learning rate: 0.0058196]
	Learning Rate: 0.00581962
	LOSS [training: 0.7823447817312299 | validation: 0.5315119007028231]
	TIME [epoch: 8.45 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8994320364223629		[learning rate: 0.0057985]
	Learning Rate: 0.0057985
	LOSS [training: 0.8994320364223629 | validation: 0.5110551834858267]
	TIME [epoch: 8.44 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0062492919715542		[learning rate: 0.0057775]
	Learning Rate: 0.00577746
	LOSS [training: 1.0062492919715542 | validation: 0.5930781338782742]
	TIME [epoch: 8.44 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9260892024637798		[learning rate: 0.0057565]
	Learning Rate: 0.00575649
	LOSS [training: 0.9260892024637798 | validation: 0.572553586353597]
	TIME [epoch: 8.45 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.071013591858985		[learning rate: 0.0057356]
	Learning Rate: 0.0057356
	LOSS [training: 1.071013591858985 | validation: 1.1645583387034564]
	TIME [epoch: 8.45 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9805237246582281		[learning rate: 0.0057148]
	Learning Rate: 0.00571479
	LOSS [training: 0.9805237246582281 | validation: 0.9331266074644613]
	TIME [epoch: 8.44 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8602576061031322		[learning rate: 0.005694]
	Learning Rate: 0.00569405
	LOSS [training: 0.8602576061031322 | validation: 0.4469511251409524]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8359140881708859		[learning rate: 0.0056734]
	Learning Rate: 0.00567338
	LOSS [training: 0.8359140881708859 | validation: 0.7411782751925999]
	TIME [epoch: 8.45 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1058176614692585		[learning rate: 0.0056528]
	Learning Rate: 0.00565279
	LOSS [training: 1.1058176614692585 | validation: 0.6431099467571109]
	TIME [epoch: 8.46 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.896830159511044		[learning rate: 0.0056323]
	Learning Rate: 0.00563228
	LOSS [training: 0.896830159511044 | validation: 0.48811213264823905]
	TIME [epoch: 8.44 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6878490127504933		[learning rate: 0.0056118]
	Learning Rate: 0.00561184
	LOSS [training: 0.6878490127504933 | validation: 0.5032815925684315]
	TIME [epoch: 8.43 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8643985019355954		[learning rate: 0.0055915]
	Learning Rate: 0.00559147
	LOSS [training: 0.8643985019355954 | validation: 1.3070352704353099]
	TIME [epoch: 8.44 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8519588676809399		[learning rate: 0.0055712]
	Learning Rate: 0.00557118
	LOSS [training: 0.8519588676809399 | validation: 0.9240506722468531]
	TIME [epoch: 8.47 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8298726852374614		[learning rate: 0.005551]
	Learning Rate: 0.00555096
	LOSS [training: 0.8298726852374614 | validation: 1.145241550823727]
	TIME [epoch: 8.46 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8209320753287483		[learning rate: 0.0055308]
	Learning Rate: 0.00553082
	LOSS [training: 0.8209320753287483 | validation: 0.4132344053822814]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7431457582122523		[learning rate: 0.0055107]
	Learning Rate: 0.00551075
	LOSS [training: 0.7431457582122523 | validation: 0.5057199115610536]
	TIME [epoch: 8.45 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7065016284155564		[learning rate: 0.0054907]
	Learning Rate: 0.00549075
	LOSS [training: 0.7065016284155564 | validation: 1.1226671510920756]
	TIME [epoch: 8.47 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.946272233106437		[learning rate: 0.0054708]
	Learning Rate: 0.00547082
	LOSS [training: 0.946272233106437 | validation: 0.9626077482740878]
	TIME [epoch: 8.44 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7335316455191851		[learning rate: 0.005451]
	Learning Rate: 0.00545097
	LOSS [training: 0.7335316455191851 | validation: 0.6559469467857914]
	TIME [epoch: 8.45 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9370663580142553		[learning rate: 0.0054312]
	Learning Rate: 0.00543119
	LOSS [training: 0.9370663580142553 | validation: 0.6867308126167284]
	TIME [epoch: 8.46 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8637552435933958		[learning rate: 0.0054115]
	Learning Rate: 0.00541148
	LOSS [training: 0.8637552435933958 | validation: 0.9914867050447076]
	TIME [epoch: 8.44 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8105293960135416		[learning rate: 0.0053918]
	Learning Rate: 0.00539184
	LOSS [training: 0.8105293960135416 | validation: 0.6608810065717828]
	TIME [epoch: 8.48 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7091298202913732		[learning rate: 0.0053723]
	Learning Rate: 0.00537227
	LOSS [training: 0.7091298202913732 | validation: 0.3676849986734867]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6066044212698047		[learning rate: 0.0053528]
	Learning Rate: 0.00535277
	LOSS [training: 0.6066044212698047 | validation: 0.626834574750942]
	TIME [epoch: 8.45 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8189074017620985		[learning rate: 0.0053333]
	Learning Rate: 0.00533335
	LOSS [training: 0.8189074017620985 | validation: 0.6247103593060934]
	TIME [epoch: 8.46 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0235088881191667		[learning rate: 0.005314]
	Learning Rate: 0.00531399
	LOSS [training: 1.0235088881191667 | validation: 0.5451184908554713]
	TIME [epoch: 8.47 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8119945833950674		[learning rate: 0.0052947]
	Learning Rate: 0.00529471
	LOSS [training: 0.8119945833950674 | validation: 0.7874855296475776]
	TIME [epoch: 8.45 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7561182271231226		[learning rate: 0.0052755]
	Learning Rate: 0.00527549
	LOSS [training: 0.7561182271231226 | validation: 0.5869609004377214]
	TIME [epoch: 8.45 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6489680305085697		[learning rate: 0.0052563]
	Learning Rate: 0.00525635
	LOSS [training: 0.6489680305085697 | validation: 0.49612623505306575]
	TIME [epoch: 8.43 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7203296861300672		[learning rate: 0.0052373]
	Learning Rate: 0.00523727
	LOSS [training: 0.7203296861300672 | validation: 0.36596326462997053]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.653693691208305		[learning rate: 0.0052183]
	Learning Rate: 0.00521827
	LOSS [training: 0.653693691208305 | validation: 0.4519896192658224]
	TIME [epoch: 8.47 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8279811414297156		[learning rate: 0.0051993]
	Learning Rate: 0.00519933
	LOSS [training: 0.8279811414297156 | validation: 0.4873867939310612]
	TIME [epoch: 8.44 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6847532738717261		[learning rate: 0.0051805]
	Learning Rate: 0.00518046
	LOSS [training: 0.6847532738717261 | validation: 0.6244441594539768]
	TIME [epoch: 8.43 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6871785475440673		[learning rate: 0.0051617]
	Learning Rate: 0.00516166
	LOSS [training: 0.6871785475440673 | validation: 0.41237803493238984]
	TIME [epoch: 8.45 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8828282315532305		[learning rate: 0.0051429]
	Learning Rate: 0.00514293
	LOSS [training: 0.8828282315532305 | validation: 0.9252463033208624]
	TIME [epoch: 8.46 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6535326508101954		[learning rate: 0.0051243]
	Learning Rate: 0.00512426
	LOSS [training: 0.6535326508101954 | validation: 0.5098245258891819]
	TIME [epoch: 8.44 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8413607676237795		[learning rate: 0.0051057]
	Learning Rate: 0.00510567
	LOSS [training: 0.8413607676237795 | validation: 0.6507924103604985]
	TIME [epoch: 8.43 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7482032405908676		[learning rate: 0.0050871]
	Learning Rate: 0.00508714
	LOSS [training: 0.7482032405908676 | validation: 0.4440280477218549]
	TIME [epoch: 8.44 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0672487598859455		[learning rate: 0.0050687]
	Learning Rate: 0.00506868
	LOSS [training: 1.0672487598859455 | validation: 0.6495973326573689]
	TIME [epoch: 8.43 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.630187342851823		[learning rate: 0.0050503]
	Learning Rate: 0.00505028
	LOSS [training: 0.630187342851823 | validation: 0.3756009524181917]
	TIME [epoch: 8.46 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6714348597918809		[learning rate: 0.005032]
	Learning Rate: 0.00503196
	LOSS [training: 0.6714348597918809 | validation: 0.5035245848664516]
	TIME [epoch: 8.45 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9053448970597581		[learning rate: 0.0050137]
	Learning Rate: 0.00501369
	LOSS [training: 0.9053448970597581 | validation: 0.6044353446170183]
	TIME [epoch: 8.43 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7605374634751801		[learning rate: 0.0049955]
	Learning Rate: 0.0049955
	LOSS [training: 0.7605374634751801 | validation: 0.4092470414132574]
	TIME [epoch: 8.44 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.026079350930137		[learning rate: 0.0049774]
	Learning Rate: 0.00497737
	LOSS [training: 1.026079350930137 | validation: 0.7190010868032782]
	TIME [epoch: 8.47 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7677446320121907		[learning rate: 0.0049593]
	Learning Rate: 0.00495931
	LOSS [training: 0.7677446320121907 | validation: 0.433470687917946]
	TIME [epoch: 8.44 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8153005207098234		[learning rate: 0.0049413]
	Learning Rate: 0.00494131
	LOSS [training: 0.8153005207098234 | validation: 0.3996529197588422]
	TIME [epoch: 8.44 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7677716997104354		[learning rate: 0.0049234]
	Learning Rate: 0.00492338
	LOSS [training: 0.7677716997104354 | validation: 0.5303717462554952]
	TIME [epoch: 8.43 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8069300845217816		[learning rate: 0.0049055]
	Learning Rate: 0.00490551
	LOSS [training: 0.8069300845217816 | validation: 0.7260248965141567]
	TIME [epoch: 8.45 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.688032282000041		[learning rate: 0.0048877]
	Learning Rate: 0.00488771
	LOSS [training: 0.688032282000041 | validation: 0.4757347751876011]
	TIME [epoch: 8.45 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6135753487076692		[learning rate: 0.00487]
	Learning Rate: 0.00486997
	LOSS [training: 0.6135753487076692 | validation: 0.6950832055102028]
	TIME [epoch: 8.44 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6710121022723586		[learning rate: 0.0048523]
	Learning Rate: 0.0048523
	LOSS [training: 0.6710121022723586 | validation: 1.152800039593661]
	TIME [epoch: 8.44 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9156916355114131		[learning rate: 0.0048347]
	Learning Rate: 0.00483469
	LOSS [training: 0.9156916355114131 | validation: 0.6178423355532432]
	TIME [epoch: 8.46 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7840890398488317		[learning rate: 0.0048171]
	Learning Rate: 0.00481714
	LOSS [training: 0.7840890398488317 | validation: 0.8292571665697175]
	TIME [epoch: 8.44 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8085891684535685		[learning rate: 0.0047997]
	Learning Rate: 0.00479966
	LOSS [training: 0.8085891684535685 | validation: 1.1905494880424048]
	TIME [epoch: 8.43 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8087420725747089		[learning rate: 0.0047822]
	Learning Rate: 0.00478224
	LOSS [training: 0.8087420725747089 | validation: 0.8029424886987642]
	TIME [epoch: 8.45 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6543213020433811		[learning rate: 0.0047649]
	Learning Rate: 0.00476489
	LOSS [training: 0.6543213020433811 | validation: 0.34745962488642657]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7031972317841262		[learning rate: 0.0047476]
	Learning Rate: 0.00474759
	LOSS [training: 0.7031972317841262 | validation: 0.5936497471145538]
	TIME [epoch: 8.44 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6178330273431136		[learning rate: 0.0047304]
	Learning Rate: 0.00473037
	LOSS [training: 0.6178330273431136 | validation: 0.7732999554313791]
	TIME [epoch: 8.43 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7375297265363642		[learning rate: 0.0047132]
	Learning Rate: 0.0047132
	LOSS [training: 0.7375297265363642 | validation: 0.3979471356060453]
	TIME [epoch: 8.43 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5354100109444435		[learning rate: 0.0046961]
	Learning Rate: 0.00469609
	LOSS [training: 0.5354100109444435 | validation: 0.43101952468464716]
	TIME [epoch: 8.44 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6340121145117024		[learning rate: 0.0046791]
	Learning Rate: 0.00467905
	LOSS [training: 0.6340121145117024 | validation: 0.48585521052461667]
	TIME [epoch: 8.44 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6901323574507077		[learning rate: 0.0046621]
	Learning Rate: 0.00466207
	LOSS [training: 0.6901323574507077 | validation: 0.4805806605166403]
	TIME [epoch: 8.43 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8397220207454994		[learning rate: 0.0046452]
	Learning Rate: 0.00464515
	LOSS [training: 0.8397220207454994 | validation: 0.5565586878072053]
	TIME [epoch: 8.43 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8142704376785895		[learning rate: 0.0046283]
	Learning Rate: 0.0046283
	LOSS [training: 0.8142704376785895 | validation: 0.386246459197197]
	TIME [epoch: 8.44 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5463148083013978		[learning rate: 0.0046115]
	Learning Rate: 0.0046115
	LOSS [training: 0.5463148083013978 | validation: 0.47152423728876836]
	TIME [epoch: 8.45 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7060384499611132		[learning rate: 0.0045948]
	Learning Rate: 0.00459476
	LOSS [training: 0.7060384499611132 | validation: 0.42364063344221925]
	TIME [epoch: 8.44 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8444187010762411		[learning rate: 0.0045781]
	Learning Rate: 0.00457809
	LOSS [training: 0.8444187010762411 | validation: 0.3975881058865864]
	TIME [epoch: 8.43 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6399301665122887		[learning rate: 0.0045615]
	Learning Rate: 0.00456148
	LOSS [training: 0.6399301665122887 | validation: 0.6007366295074512]
	TIME [epoch: 8.43 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7567145400376003		[learning rate: 0.0045449]
	Learning Rate: 0.00454492
	LOSS [training: 0.7567145400376003 | validation: 0.934310592742545]
	TIME [epoch: 8.46 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7254038394645403		[learning rate: 0.0045284]
	Learning Rate: 0.00452843
	LOSS [training: 0.7254038394645403 | validation: 0.4825952153922958]
	TIME [epoch: 8.44 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5272589129360982		[learning rate: 0.004512]
	Learning Rate: 0.00451199
	LOSS [training: 0.5272589129360982 | validation: 0.7421276024910599]
	TIME [epoch: 8.43 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.591161823213118		[learning rate: 0.0044956]
	Learning Rate: 0.00449562
	LOSS [training: 0.591161823213118 | validation: 0.3558218803942428]
	TIME [epoch: 8.43 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.825158174226565		[learning rate: 0.0044793]
	Learning Rate: 0.0044793
	LOSS [training: 0.825158174226565 | validation: 0.5170699316463898]
	TIME [epoch: 8.45 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8076362793171004		[learning rate: 0.004463]
	Learning Rate: 0.00446305
	LOSS [training: 0.8076362793171004 | validation: 0.5289311606207596]
	TIME [epoch: 8.43 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8533602350838265		[learning rate: 0.0044469]
	Learning Rate: 0.00444685
	LOSS [training: 0.8533602350838265 | validation: 0.6315887940114413]
	TIME [epoch: 8.43 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8022609383310559		[learning rate: 0.0044307]
	Learning Rate: 0.00443071
	LOSS [training: 0.8022609383310559 | validation: 0.40635591303068214]
	TIME [epoch: 8.43 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8317755352036305		[learning rate: 0.0044146]
	Learning Rate: 0.00441463
	LOSS [training: 0.8317755352036305 | validation: 0.5692180075919123]
	TIME [epoch: 8.44 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6309151040332561		[learning rate: 0.0043986]
	Learning Rate: 0.00439861
	LOSS [training: 0.6309151040332561 | validation: 0.9529789809002017]
	TIME [epoch: 8.44 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7041172048342685		[learning rate: 0.0043827]
	Learning Rate: 0.00438265
	LOSS [training: 0.7041172048342685 | validation: 0.3785968583208599]
	TIME [epoch: 8.43 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6410217916446219		[learning rate: 0.0043667]
	Learning Rate: 0.00436675
	LOSS [training: 0.6410217916446219 | validation: 0.41247205717974955]
	TIME [epoch: 8.43 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9209235488418388		[learning rate: 0.0043509]
	Learning Rate: 0.0043509
	LOSS [training: 0.9209235488418388 | validation: 0.4155636163882822]
	TIME [epoch: 8.45 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.687746938394562		[learning rate: 0.0043351]
	Learning Rate: 0.00433511
	LOSS [training: 0.687746938394562 | validation: 0.5328530771081534]
	TIME [epoch: 8.46 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8592934853745202		[learning rate: 0.0043194]
	Learning Rate: 0.00431938
	LOSS [training: 0.8592934853745202 | validation: 1.1500460648443278]
	TIME [epoch: 8.43 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7455058032019386		[learning rate: 0.0043037]
	Learning Rate: 0.0043037
	LOSS [training: 0.7455058032019386 | validation: 0.7471428227079322]
	TIME [epoch: 8.43 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7088225086017987		[learning rate: 0.0042881]
	Learning Rate: 0.00428808
	LOSS [training: 0.7088225086017987 | validation: 0.7444792000735014]
	TIME [epoch: 8.44 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7943494159617116		[learning rate: 0.0042725]
	Learning Rate: 0.00427252
	LOSS [training: 0.7943494159617116 | validation: 0.44049804968993034]
	TIME [epoch: 8.47 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.609861618034185		[learning rate: 0.004257]
	Learning Rate: 0.00425702
	LOSS [training: 0.609861618034185 | validation: 0.5953608555595097]
	TIME [epoch: 8.44 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5707222905570446		[learning rate: 0.0042416]
	Learning Rate: 0.00424157
	LOSS [training: 0.5707222905570446 | validation: 0.5975657410971452]
	TIME [epoch: 8.44 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8502967048951046		[learning rate: 0.0042262]
	Learning Rate: 0.00422617
	LOSS [training: 0.8502967048951046 | validation: 1.0047832700821309]
	TIME [epoch: 8.43 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7360946899785049		[learning rate: 0.0042108]
	Learning Rate: 0.00421084
	LOSS [training: 0.7360946899785049 | validation: 0.5033418913902357]
	TIME [epoch: 8.46 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8732899748326132		[learning rate: 0.0041956]
	Learning Rate: 0.00419556
	LOSS [training: 0.8732899748326132 | validation: 0.585091924635077]
	TIME [epoch: 8.44 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7669218298668963		[learning rate: 0.0041803]
	Learning Rate: 0.00418033
	LOSS [training: 0.7669218298668963 | validation: 0.9793495454901873]
	TIME [epoch: 8.43 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6088756425415828		[learning rate: 0.0041652]
	Learning Rate: 0.00416516
	LOSS [training: 0.6088756425415828 | validation: 0.5442930931989967]
	TIME [epoch: 8.43 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8452065141799985		[learning rate: 0.00415]
	Learning Rate: 0.00415004
	LOSS [training: 0.8452065141799985 | validation: 0.39464682656836597]
	TIME [epoch: 8.47 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9538224044554143		[learning rate: 0.004135]
	Learning Rate: 0.00413498
	LOSS [training: 0.9538224044554143 | validation: 0.4362583294724173]
	TIME [epoch: 8.45 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8486150193704043		[learning rate: 0.00412]
	Learning Rate: 0.00411998
	LOSS [training: 0.8486150193704043 | validation: 0.6134207653691868]
	TIME [epoch: 8.44 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9515410099121766		[learning rate: 0.004105]
	Learning Rate: 0.00410502
	LOSS [training: 0.9515410099121766 | validation: 0.5831372718559586]
	TIME [epoch: 8.43 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7601186758501584		[learning rate: 0.0040901]
	Learning Rate: 0.00409013
	LOSS [training: 0.7601186758501584 | validation: 0.8490911199317411]
	TIME [epoch: 8.46 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9476947728577605		[learning rate: 0.0040753]
	Learning Rate: 0.00407528
	LOSS [training: 0.9476947728577605 | validation: 0.8965275703510691]
	TIME [epoch: 8.45 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7461911682256861		[learning rate: 0.0040605]
	Learning Rate: 0.00406049
	LOSS [training: 0.7461911682256861 | validation: 0.4656047435289218]
	TIME [epoch: 8.43 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.544873448303794		[learning rate: 0.0040458]
	Learning Rate: 0.00404576
	LOSS [training: 0.544873448303794 | validation: 0.48631298628602304]
	TIME [epoch: 8.43 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.602643576189851		[learning rate: 0.0040311]
	Learning Rate: 0.00403108
	LOSS [training: 0.602643576189851 | validation: 1.3995969141040128]
	TIME [epoch: 8.45 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.840596096421297		[learning rate: 0.0040164]
	Learning Rate: 0.00401645
	LOSS [training: 0.840596096421297 | validation: 1.1847576188207654]
	TIME [epoch: 8.43 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.717791445369816		[learning rate: 0.0040019]
	Learning Rate: 0.00400187
	LOSS [training: 0.717791445369816 | validation: 0.45660874785880645]
	TIME [epoch: 8.43 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7248275916224759		[learning rate: 0.0039873]
	Learning Rate: 0.00398735
	LOSS [training: 0.7248275916224759 | validation: 0.5119665947613272]
	TIME [epoch: 8.43 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5483692783961626		[learning rate: 0.0039729]
	Learning Rate: 0.00397288
	LOSS [training: 0.5483692783961626 | validation: 0.3667172196705205]
	TIME [epoch: 8.45 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5457611823071657		[learning rate: 0.0039585]
	Learning Rate: 0.00395846
	LOSS [training: 0.5457611823071657 | validation: 1.4307431428276118]
	TIME [epoch: 8.44 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8038886941018708		[learning rate: 0.0039441]
	Learning Rate: 0.00394409
	LOSS [training: 0.8038886941018708 | validation: 0.5236354442774056]
	TIME [epoch: 8.44 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7191568285320522		[learning rate: 0.0039298]
	Learning Rate: 0.00392978
	LOSS [training: 0.7191568285320522 | validation: 1.1584903961817172]
	TIME [epoch: 8.43 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6893985549752211		[learning rate: 0.0039155]
	Learning Rate: 0.00391552
	LOSS [training: 0.6893985549752211 | validation: 0.502465046017313]
	TIME [epoch: 8.45 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6577730039402907		[learning rate: 0.0039013]
	Learning Rate: 0.00390131
	LOSS [training: 0.6577730039402907 | validation: 0.41434102864375155]
	TIME [epoch: 8.44 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5883596751246316		[learning rate: 0.0038872]
	Learning Rate: 0.00388715
	LOSS [training: 0.5883596751246316 | validation: 1.0139857144303541]
	TIME [epoch: 8.44 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8616068222729301		[learning rate: 0.003873]
	Learning Rate: 0.00387305
	LOSS [training: 0.8616068222729301 | validation: 0.39035279073760537]
	TIME [epoch: 8.44 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.625088857526438		[learning rate: 0.003859]
	Learning Rate: 0.00385899
	LOSS [training: 0.625088857526438 | validation: 0.8964122939174697]
	TIME [epoch: 8.44 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6882553564697915		[learning rate: 0.003845]
	Learning Rate: 0.00384499
	LOSS [training: 0.6882553564697915 | validation: 0.6424475926373315]
	TIME [epoch: 8.46 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6049629221855546		[learning rate: 0.003831]
	Learning Rate: 0.00383103
	LOSS [training: 0.6049629221855546 | validation: 0.5798328473753742]
	TIME [epoch: 8.44 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6354432360261429		[learning rate: 0.0038171]
	Learning Rate: 0.00381713
	LOSS [training: 0.6354432360261429 | validation: 0.40390182960900617]
	TIME [epoch: 8.43 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.664535656393007		[learning rate: 0.0038033]
	Learning Rate: 0.00380328
	LOSS [training: 0.664535656393007 | validation: 0.6516557709783701]
	TIME [epoch: 8.44 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.60284173996452		[learning rate: 0.0037895]
	Learning Rate: 0.00378947
	LOSS [training: 0.60284173996452 | validation: 0.3693660916295079]
	TIME [epoch: 8.46 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.635449534311751		[learning rate: 0.0037757]
	Learning Rate: 0.00377572
	LOSS [training: 0.635449534311751 | validation: 0.5386264783098925]
	TIME [epoch: 8.44 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6392904111372016		[learning rate: 0.003762]
	Learning Rate: 0.00376202
	LOSS [training: 0.6392904111372016 | validation: 0.46544784733049416]
	TIME [epoch: 8.44 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8194145930994644		[learning rate: 0.0037484]
	Learning Rate: 0.00374837
	LOSS [training: 0.8194145930994644 | validation: 0.6189549449795273]
	TIME [epoch: 8.43 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5888781254345494		[learning rate: 0.0037348]
	Learning Rate: 0.00373476
	LOSS [training: 0.5888781254345494 | validation: 0.5501132459605036]
	TIME [epoch: 8.46 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7433705337507892		[learning rate: 0.0037212]
	Learning Rate: 0.00372121
	LOSS [training: 0.7433705337507892 | validation: 0.41719115569710663]
	TIME [epoch: 8.44 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6544468909443076		[learning rate: 0.0037077]
	Learning Rate: 0.00370771
	LOSS [training: 0.6544468909443076 | validation: 0.6909622374664515]
	TIME [epoch: 8.43 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7200278142183415		[learning rate: 0.0036943]
	Learning Rate: 0.00369425
	LOSS [training: 0.7200278142183415 | validation: 0.5073099605345015]
	TIME [epoch: 8.44 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5279848365284231		[learning rate: 0.0036808]
	Learning Rate: 0.00368084
	LOSS [training: 0.5279848365284231 | validation: 0.44132311602685903]
	TIME [epoch: 8.46 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5671509028908133		[learning rate: 0.0036675]
	Learning Rate: 0.00366749
	LOSS [training: 0.5671509028908133 | validation: 0.3856259892317355]
	TIME [epoch: 8.45 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7601620267187936		[learning rate: 0.0036542]
	Learning Rate: 0.00365418
	LOSS [training: 0.7601620267187936 | validation: 0.5087324281521937]
	TIME [epoch: 8.44 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6631450778178407		[learning rate: 0.0036409]
	Learning Rate: 0.00364092
	LOSS [training: 0.6631450778178407 | validation: 0.885580662801722]
	TIME [epoch: 8.44 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6471506991443068		[learning rate: 0.0036277]
	Learning Rate: 0.0036277
	LOSS [training: 0.6471506991443068 | validation: 0.6899372242612987]
	TIME [epoch: 8.46 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.638205251513046		[learning rate: 0.0036145]
	Learning Rate: 0.00361454
	LOSS [training: 0.638205251513046 | validation: 0.8167389941295089]
	TIME [epoch: 8.44 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7126927594783716		[learning rate: 0.0036014]
	Learning Rate: 0.00360142
	LOSS [training: 0.7126927594783716 | validation: 0.5655345341246781]
	TIME [epoch: 8.43 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6068980830444116		[learning rate: 0.0035883]
	Learning Rate: 0.00358835
	LOSS [training: 0.6068980830444116 | validation: 0.40716904476909876]
	TIME [epoch: 8.44 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5790013130773253		[learning rate: 0.0035753]
	Learning Rate: 0.00357533
	LOSS [training: 0.5790013130773253 | validation: 0.33904647658440445]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6323757808684622		[learning rate: 0.0035624]
	Learning Rate: 0.00356235
	LOSS [training: 0.6323757808684622 | validation: 0.3921657580422684]
	TIME [epoch: 8.44 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6140483622957211		[learning rate: 0.0035494]
	Learning Rate: 0.00354942
	LOSS [training: 0.6140483622957211 | validation: 0.35399157465563913]
	TIME [epoch: 8.45 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.582882760345173		[learning rate: 0.0035365]
	Learning Rate: 0.00353654
	LOSS [training: 0.582882760345173 | validation: 0.5857577424561249]
	TIME [epoch: 8.44 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.621498466099139		[learning rate: 0.0035237]
	Learning Rate: 0.00352371
	LOSS [training: 0.621498466099139 | validation: 0.5919067204712216]
	TIME [epoch: 8.45 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5571183185752911		[learning rate: 0.0035109]
	Learning Rate: 0.00351092
	LOSS [training: 0.5571183185752911 | validation: 0.34241292580498994]
	TIME [epoch: 8.47 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5624410927479924		[learning rate: 0.0034982]
	Learning Rate: 0.00349818
	LOSS [training: 0.5624410927479924 | validation: 0.4408211909766189]
	TIME [epoch: 8.43 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48583771277964843		[learning rate: 0.0034855]
	Learning Rate: 0.00348548
	LOSS [training: 0.48583771277964843 | validation: 0.42210959639136336]
	TIME [epoch: 8.45 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4806347664443056		[learning rate: 0.0034728]
	Learning Rate: 0.00347284
	LOSS [training: 0.4806347664443056 | validation: 0.3451796267727737]
	TIME [epoch: 8.45 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4948370708963911		[learning rate: 0.0034602]
	Learning Rate: 0.00346023
	LOSS [training: 0.4948370708963911 | validation: 0.2740663519984773]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6854358360284		[learning rate: 0.0034477]
	Learning Rate: 0.00344767
	LOSS [training: 0.6854358360284 | validation: 0.45120510651943946]
	TIME [epoch: 8.45 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6027601529263615		[learning rate: 0.0034352]
	Learning Rate: 0.00343516
	LOSS [training: 0.6027601529263615 | validation: 0.40033235096938935]
	TIME [epoch: 8.44 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6610175109986517		[learning rate: 0.0034227]
	Learning Rate: 0.0034227
	LOSS [training: 0.6610175109986517 | validation: 0.5868249094759382]
	TIME [epoch: 8.44 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5973413573026082		[learning rate: 0.0034103]
	Learning Rate: 0.00341028
	LOSS [training: 0.5973413573026082 | validation: 0.44213032670144076]
	TIME [epoch: 8.45 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5686708500715645		[learning rate: 0.0033979]
	Learning Rate: 0.0033979
	LOSS [training: 0.5686708500715645 | validation: 0.2974176423001839]
	TIME [epoch: 8.46 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6498611113249779		[learning rate: 0.0033856]
	Learning Rate: 0.00338557
	LOSS [training: 0.6498611113249779 | validation: 0.5908370855958305]
	TIME [epoch: 8.45 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5757690989509483		[learning rate: 0.0033733]
	Learning Rate: 0.00337328
	LOSS [training: 0.5757690989509483 | validation: 0.3569938634538251]
	TIME [epoch: 8.45 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5504302810145674		[learning rate: 0.003361]
	Learning Rate: 0.00336104
	LOSS [training: 0.5504302810145674 | validation: 0.8085063909309999]
	TIME [epoch: 8.44 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6165725606065641		[learning rate: 0.0033488]
	Learning Rate: 0.00334884
	LOSS [training: 0.6165725606065641 | validation: 0.9797376918098519]
	TIME [epoch: 8.46 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6791388035562083		[learning rate: 0.0033367]
	Learning Rate: 0.00333669
	LOSS [training: 0.6791388035562083 | validation: 0.6242401423216826]
	TIME [epoch: 8.44 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.50751330838895		[learning rate: 0.0033246]
	Learning Rate: 0.00332458
	LOSS [training: 0.50751330838895 | validation: 0.45229898028733806]
	TIME [epoch: 8.44 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5679905725053608		[learning rate: 0.0033125]
	Learning Rate: 0.00331252
	LOSS [training: 0.5679905725053608 | validation: 0.3186396627412913]
	TIME [epoch: 8.44 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5779622561586399		[learning rate: 0.0033005]
	Learning Rate: 0.00330049
	LOSS [training: 0.5779622561586399 | validation: 0.7484709780084211]
	TIME [epoch: 8.46 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6812616072662778		[learning rate: 0.0032885]
	Learning Rate: 0.00328852
	LOSS [training: 0.6812616072662778 | validation: 0.4674707300360268]
	TIME [epoch: 8.45 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5895971316629423		[learning rate: 0.0032766]
	Learning Rate: 0.00327658
	LOSS [training: 0.5895971316629423 | validation: 0.3880796737986565]
	TIME [epoch: 8.44 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5058127537005043		[learning rate: 0.0032647]
	Learning Rate: 0.00326469
	LOSS [training: 0.5058127537005043 | validation: 0.36404105887948524]
	TIME [epoch: 8.43 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5370348328094247		[learning rate: 0.0032528]
	Learning Rate: 0.00325284
	LOSS [training: 0.5370348328094247 | validation: 0.37699390782631004]
	TIME [epoch: 8.45 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6098142717082683		[learning rate: 0.003241]
	Learning Rate: 0.00324104
	LOSS [training: 0.6098142717082683 | validation: 0.5415584340267924]
	TIME [epoch: 8.46 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5864019499525066		[learning rate: 0.0032293]
	Learning Rate: 0.00322928
	LOSS [training: 0.5864019499525066 | validation: 0.4532315826115454]
	TIME [epoch: 8.44 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5499214445373533		[learning rate: 0.0032176]
	Learning Rate: 0.00321756
	LOSS [training: 0.5499214445373533 | validation: 0.706117185453216]
	TIME [epoch: 8.43 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6073066857315128		[learning rate: 0.0032059]
	Learning Rate: 0.00320588
	LOSS [training: 0.6073066857315128 | validation: 0.4185339169050598]
	TIME [epoch: 8.45 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5527319147615535		[learning rate: 0.0031942]
	Learning Rate: 0.00319425
	LOSS [training: 0.5527319147615535 | validation: 0.4078217060710154]
	TIME [epoch: 8.46 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4692291027153866		[learning rate: 0.0031827]
	Learning Rate: 0.00318265
	LOSS [training: 0.4692291027153866 | validation: 0.27524611475560434]
	TIME [epoch: 8.45 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47907172277653676		[learning rate: 0.0031711]
	Learning Rate: 0.0031711
	LOSS [training: 0.47907172277653676 | validation: 0.6337480374530423]
	TIME [epoch: 8.44 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5086458176813201		[learning rate: 0.0031596]
	Learning Rate: 0.0031596
	LOSS [training: 0.5086458176813201 | validation: 0.30083416465629575]
	TIME [epoch: 8.44 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5703685650378082		[learning rate: 0.0031481]
	Learning Rate: 0.00314813
	LOSS [training: 0.5703685650378082 | validation: 0.4408513802300081]
	TIME [epoch: 8.47 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4986185104946014		[learning rate: 0.0031367]
	Learning Rate: 0.00313671
	LOSS [training: 0.4986185104946014 | validation: 0.3816364608882304]
	TIME [epoch: 8.43 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5145062680729843		[learning rate: 0.0031253]
	Learning Rate: 0.00312532
	LOSS [training: 0.5145062680729843 | validation: 0.349069822175727]
	TIME [epoch: 8.44 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4854943217681374		[learning rate: 0.003114]
	Learning Rate: 0.00311398
	LOSS [training: 0.4854943217681374 | validation: 0.40772090770558933]
	TIME [epoch: 8.44 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5441812995767789		[learning rate: 0.0031027]
	Learning Rate: 0.00310268
	LOSS [training: 0.5441812995767789 | validation: 0.8039293578415052]
	TIME [epoch: 8.46 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6005957905013066		[learning rate: 0.0030914]
	Learning Rate: 0.00309142
	LOSS [training: 0.6005957905013066 | validation: 0.9043868599475715]
	TIME [epoch: 8.45 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5626684600843811		[learning rate: 0.0030802]
	Learning Rate: 0.0030802
	LOSS [training: 0.5626684600843811 | validation: 0.476083487496614]
	TIME [epoch: 8.43 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6162658792040625		[learning rate: 0.003069]
	Learning Rate: 0.00306902
	LOSS [training: 0.6162658792040625 | validation: 0.6311461273139245]
	TIME [epoch: 8.44 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49036238865804976		[learning rate: 0.0030579]
	Learning Rate: 0.00305788
	LOSS [training: 0.49036238865804976 | validation: 0.39793928437575543]
	TIME [epoch: 8.48 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5923521595989775		[learning rate: 0.0030468]
	Learning Rate: 0.00304679
	LOSS [training: 0.5923521595989775 | validation: 0.9277874925873764]
	TIME [epoch: 8.46 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6289984194710515		[learning rate: 0.0030357]
	Learning Rate: 0.00303573
	LOSS [training: 0.6289984194710515 | validation: 0.452714728852625]
	TIME [epoch: 8.44 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44401613771907034		[learning rate: 0.0030247]
	Learning Rate: 0.00302471
	LOSS [training: 0.44401613771907034 | validation: 0.45562339749257463]
	TIME [epoch: 8.45 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7531559101840674		[learning rate: 0.0030137]
	Learning Rate: 0.00301374
	LOSS [training: 0.7531559101840674 | validation: 0.8460371610505804]
	TIME [epoch: 8.48 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5498445475703962		[learning rate: 0.0030028]
	Learning Rate: 0.0030028
	LOSS [training: 0.5498445475703962 | validation: 0.3402076595401699]
	TIME [epoch: 8.45 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5760135634243587		[learning rate: 0.0029919]
	Learning Rate: 0.0029919
	LOSS [training: 0.5760135634243587 | validation: 0.886688789166341]
	TIME [epoch: 8.45 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5897180249905766		[learning rate: 0.002981]
	Learning Rate: 0.00298104
	LOSS [training: 0.5897180249905766 | validation: 0.5088134819619741]
	TIME [epoch: 8.45 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4372686870418572		[learning rate: 0.0029702]
	Learning Rate: 0.00297023
	LOSS [training: 0.4372686870418572 | validation: 0.37858080269770866]
	TIME [epoch: 8.46 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6253845272442835		[learning rate: 0.0029594]
	Learning Rate: 0.00295945
	LOSS [training: 0.6253845272442835 | validation: 0.32129638196932475]
	TIME [epoch: 8.45 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5743604030639403		[learning rate: 0.0029487]
	Learning Rate: 0.00294871
	LOSS [training: 0.5743604030639403 | validation: 0.3146309688289935]
	TIME [epoch: 8.43 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5866332762713486		[learning rate: 0.002938]
	Learning Rate: 0.00293801
	LOSS [training: 0.5866332762713486 | validation: 0.37447947027889655]
	TIME [epoch: 8.46 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5686174793149911		[learning rate: 0.0029273]
	Learning Rate: 0.00292734
	LOSS [training: 0.5686174793149911 | validation: 0.4595862136399697]
	TIME [epoch: 8.47 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5286350129842199		[learning rate: 0.0029167]
	Learning Rate: 0.00291672
	LOSS [training: 0.5286350129842199 | validation: 0.5105527223742578]
	TIME [epoch: 8.45 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48547553521022346		[learning rate: 0.0029061]
	Learning Rate: 0.00290614
	LOSS [training: 0.48547553521022346 | validation: 0.6807058009749667]
	TIME [epoch: 8.44 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5115124436563091		[learning rate: 0.0028956]
	Learning Rate: 0.00289559
	LOSS [training: 0.5115124436563091 | validation: 0.40197977924988193]
	TIME [epoch: 8.44 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4062363857090645		[learning rate: 0.0028851]
	Learning Rate: 0.00288508
	LOSS [training: 0.4062363857090645 | validation: 0.2766962147701415]
	TIME [epoch: 8.45 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48319865528493544		[learning rate: 0.0028746]
	Learning Rate: 0.00287461
	LOSS [training: 0.48319865528493544 | validation: 0.3867308080768345]
	TIME [epoch: 8.46 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5099517791855969		[learning rate: 0.0028642]
	Learning Rate: 0.00286418
	LOSS [training: 0.5099517791855969 | validation: 0.4219224588617566]
	TIME [epoch: 8.43 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5657392663794807		[learning rate: 0.0028538]
	Learning Rate: 0.00285378
	LOSS [training: 0.5657392663794807 | validation: 0.6761571865895251]
	TIME [epoch: 8.45 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5613263842744647		[learning rate: 0.0028434]
	Learning Rate: 0.00284343
	LOSS [training: 0.5613263842744647 | validation: 0.3234504154606448]
	TIME [epoch: 8.43 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4368845377476817		[learning rate: 0.0028331]
	Learning Rate: 0.00283311
	LOSS [training: 0.4368845377476817 | validation: 0.40790138123017916]
	TIME [epoch: 8.46 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5446116242928147		[learning rate: 0.0028228]
	Learning Rate: 0.00282283
	LOSS [training: 0.5446116242928147 | validation: 0.44012480463106696]
	TIME [epoch: 8.44 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.504079333662103		[learning rate: 0.0028126]
	Learning Rate: 0.00281258
	LOSS [training: 0.504079333662103 | validation: 0.41638939087540094]
	TIME [epoch: 8.43 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5013433615604592		[learning rate: 0.0028024]
	Learning Rate: 0.00280238
	LOSS [training: 0.5013433615604592 | validation: 0.31638600770706893]
	TIME [epoch: 8.44 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7065302466453309		[learning rate: 0.0027922]
	Learning Rate: 0.00279221
	LOSS [training: 0.7065302466453309 | validation: 0.7123362500080437]
	TIME [epoch: 8.47 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5515883025950339		[learning rate: 0.0027821]
	Learning Rate: 0.00278207
	LOSS [training: 0.5515883025950339 | validation: 0.3933586096249838]
	TIME [epoch: 8.42 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5320298682079494		[learning rate: 0.002772]
	Learning Rate: 0.00277198
	LOSS [training: 0.5320298682079494 | validation: 0.3114895475526556]
	TIME [epoch: 8.44 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4512783113560503		[learning rate: 0.0027619]
	Learning Rate: 0.00276192
	LOSS [training: 0.4512783113560503 | validation: 0.3065125188290949]
	TIME [epoch: 8.44 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5143524025511726		[learning rate: 0.0027519]
	Learning Rate: 0.00275189
	LOSS [training: 0.5143524025511726 | validation: 0.4084288331267084]
	TIME [epoch: 8.46 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4975322928793834		[learning rate: 0.0027419]
	Learning Rate: 0.00274191
	LOSS [training: 0.4975322928793834 | validation: 0.5800090062262829]
	TIME [epoch: 8.45 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4497351900793879		[learning rate: 0.002732]
	Learning Rate: 0.00273196
	LOSS [training: 0.4497351900793879 | validation: 0.33815099389817455]
	TIME [epoch: 8.44 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43003743756726687		[learning rate: 0.002722]
	Learning Rate: 0.00272204
	LOSS [training: 0.43003743756726687 | validation: 0.3336415289382752]
	TIME [epoch: 8.44 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5398394947323096		[learning rate: 0.0027122]
	Learning Rate: 0.00271216
	LOSS [training: 0.5398394947323096 | validation: 0.2684500766886342]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_459.pth
	Model improved!!!
EPOCH 460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.565081766971642		[learning rate: 0.0027023]
	Learning Rate: 0.00270232
	LOSS [training: 0.565081766971642 | validation: 0.26806899665510897]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41968904048714356		[learning rate: 0.0026925]
	Learning Rate: 0.00269251
	LOSS [training: 0.41968904048714356 | validation: 0.2812151756770177]
	TIME [epoch: 8.44 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5527277091230938		[learning rate: 0.0026827]
	Learning Rate: 0.00268274
	LOSS [training: 0.5527277091230938 | validation: 0.3243876875000944]
	TIME [epoch: 8.43 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4417357488518675		[learning rate: 0.002673]
	Learning Rate: 0.00267301
	LOSS [training: 0.4417357488518675 | validation: 0.31159792563738214]
	TIME [epoch: 8.43 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5199142892612553		[learning rate: 0.0026633]
	Learning Rate: 0.00266331
	LOSS [training: 0.5199142892612553 | validation: 0.5712083482041688]
	TIME [epoch: 8.47 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5191010026666314		[learning rate: 0.0026536]
	Learning Rate: 0.00265364
	LOSS [training: 0.5191010026666314 | validation: 0.7397251225330489]
	TIME [epoch: 8.45 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5369793698287407		[learning rate: 0.002644]
	Learning Rate: 0.00264401
	LOSS [training: 0.5369793698287407 | validation: 0.4248067394185493]
	TIME [epoch: 8.44 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6381115130127231		[learning rate: 0.0026344]
	Learning Rate: 0.00263441
	LOSS [training: 0.6381115130127231 | validation: 0.32394049672399927]
	TIME [epoch: 8.45 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43573205840253115		[learning rate: 0.0026249]
	Learning Rate: 0.00262485
	LOSS [training: 0.43573205840253115 | validation: 0.7675011685738669]
	TIME [epoch: 8.46 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6197747501169859		[learning rate: 0.0026153]
	Learning Rate: 0.00261533
	LOSS [training: 0.6197747501169859 | validation: 0.5917646816825555]
	TIME [epoch: 8.46 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5195700083738208		[learning rate: 0.0026058]
	Learning Rate: 0.00260584
	LOSS [training: 0.5195700083738208 | validation: 0.29094527085274174]
	TIME [epoch: 8.44 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.607916164696496		[learning rate: 0.0025964]
	Learning Rate: 0.00259638
	LOSS [training: 0.607916164696496 | validation: 0.3418651548748225]
	TIME [epoch: 8.45 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5258413481325647		[learning rate: 0.002587]
	Learning Rate: 0.00258696
	LOSS [training: 0.5258413481325647 | validation: 0.36825994667746687]
	TIME [epoch: 8.47 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44183882422296444		[learning rate: 0.0025776]
	Learning Rate: 0.00257757
	LOSS [training: 0.44183882422296444 | validation: 0.3201284857334582]
	TIME [epoch: 8.45 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5885968399264965		[learning rate: 0.0025682]
	Learning Rate: 0.00256822
	LOSS [training: 0.5885968399264965 | validation: 0.30704576127692795]
	TIME [epoch: 8.44 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5023644927724953		[learning rate: 0.0025589]
	Learning Rate: 0.0025589
	LOSS [training: 0.5023644927724953 | validation: 0.504802792899948]
	TIME [epoch: 8.43 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5408242591456957		[learning rate: 0.0025496]
	Learning Rate: 0.00254961
	LOSS [training: 0.5408242591456957 | validation: 0.39588767722092744]
	TIME [epoch: 8.46 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5482064853303933		[learning rate: 0.0025404]
	Learning Rate: 0.00254036
	LOSS [training: 0.5482064853303933 | validation: 0.3323073815591786]
	TIME [epoch: 8.46 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4893673704679566		[learning rate: 0.0025311]
	Learning Rate: 0.00253114
	LOSS [training: 0.4893673704679566 | validation: 0.3259131879229761]
	TIME [epoch: 8.45 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4449335929320588		[learning rate: 0.002522]
	Learning Rate: 0.00252195
	LOSS [training: 0.4449335929320588 | validation: 0.37344440035891135]
	TIME [epoch: 8.43 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48031824084434777		[learning rate: 0.0025128]
	Learning Rate: 0.0025128
	LOSS [training: 0.48031824084434777 | validation: 0.2822427164517652]
	TIME [epoch: 8.44 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4362367434137201		[learning rate: 0.0025037]
	Learning Rate: 0.00250368
	LOSS [training: 0.4362367434137201 | validation: 0.2847221468213484]
	TIME [epoch: 8.46 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4715398296593788		[learning rate: 0.0024946]
	Learning Rate: 0.00249459
	LOSS [training: 0.4715398296593788 | validation: 0.6045681228149165]
	TIME [epoch: 8.45 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5933746614950046		[learning rate: 0.0024855]
	Learning Rate: 0.00248554
	LOSS [training: 0.5933746614950046 | validation: 0.42633675995340903]
	TIME [epoch: 8.44 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4967063793079748		[learning rate: 0.0024765]
	Learning Rate: 0.00247652
	LOSS [training: 0.4967063793079748 | validation: 0.35207001407567445]
	TIME [epoch: 8.45 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45975055668308296		[learning rate: 0.0024675]
	Learning Rate: 0.00246753
	LOSS [training: 0.45975055668308296 | validation: 0.3342489023056586]
	TIME [epoch: 8.45 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4998689842902227		[learning rate: 0.0024586]
	Learning Rate: 0.00245858
	LOSS [training: 0.4998689842902227 | validation: 0.28057966186649747]
	TIME [epoch: 8.45 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4895646648114944		[learning rate: 0.0024497]
	Learning Rate: 0.00244966
	LOSS [training: 0.4895646648114944 | validation: 0.5100533381518951]
	TIME [epoch: 8.43 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5615235718286493		[learning rate: 0.0024408]
	Learning Rate: 0.00244077
	LOSS [training: 0.5615235718286493 | validation: 0.49123851883267877]
	TIME [epoch: 8.43 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5441330764405116		[learning rate: 0.0024319]
	Learning Rate: 0.00243191
	LOSS [training: 0.5441330764405116 | validation: 0.34350682178334013]
	TIME [epoch: 8.47 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38586642898857965		[learning rate: 0.0024231]
	Learning Rate: 0.00242308
	LOSS [training: 0.38586642898857965 | validation: 0.31218951785218985]
	TIME [epoch: 8.43 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4308949986395595		[learning rate: 0.0024143]
	Learning Rate: 0.00241429
	LOSS [training: 0.4308949986395595 | validation: 0.42754086632392985]
	TIME [epoch: 8.44 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4727856309242721		[learning rate: 0.0024055]
	Learning Rate: 0.00240553
	LOSS [training: 0.4727856309242721 | validation: 0.7473018441114812]
	TIME [epoch: 8.43 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4842122084682523		[learning rate: 0.0023968]
	Learning Rate: 0.0023968
	LOSS [training: 0.4842122084682523 | validation: 0.4233053979271908]
	TIME [epoch: 8.46 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4942336951413262		[learning rate: 0.0023881]
	Learning Rate: 0.0023881
	LOSS [training: 0.4942336951413262 | validation: 0.47265376342707593]
	TIME [epoch: 8.43 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47801386919518085		[learning rate: 0.0023794]
	Learning Rate: 0.00237943
	LOSS [training: 0.47801386919518085 | validation: 0.236117415746823]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4207693670354192		[learning rate: 0.0023708]
	Learning Rate: 0.0023708
	LOSS [training: 0.4207693670354192 | validation: 0.3387051022896802]
	TIME [epoch: 8.44 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6412888347169566		[learning rate: 0.0023622]
	Learning Rate: 0.0023622
	LOSS [training: 0.6412888347169566 | validation: 0.39806668770751996]
	TIME [epoch: 8.45 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44670530588984486		[learning rate: 0.0023536]
	Learning Rate: 0.00235362
	LOSS [training: 0.44670530588984486 | validation: 0.2803776567812119]
	TIME [epoch: 8.44 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5055363627602791		[learning rate: 0.0023451]
	Learning Rate: 0.00234508
	LOSS [training: 0.5055363627602791 | validation: 0.32587441303649917]
	TIME [epoch: 8.44 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47942003649838105		[learning rate: 0.0023366]
	Learning Rate: 0.00233657
	LOSS [training: 0.47942003649838105 | validation: 0.3812760020721808]
	TIME [epoch: 8.43 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5098789203140164		[learning rate: 0.0023281]
	Learning Rate: 0.00232809
	LOSS [training: 0.5098789203140164 | validation: 0.5176799297623189]
	TIME [epoch: 8.43 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49883258262926916		[learning rate: 0.0023196]
	Learning Rate: 0.00231964
	LOSS [training: 0.49883258262926916 | validation: 0.42091535841859073]
	TIME [epoch: 8.46 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4421110665920426		[learning rate: 0.0023112]
	Learning Rate: 0.00231122
	LOSS [training: 0.4421110665920426 | validation: 0.3251592249965416]
	TIME [epoch: 8.43 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7646437792398355		[learning rate: 0.0023028]
	Learning Rate: 0.00230284
	LOSS [training: 0.7646437792398355 | validation: 0.2942790772261181]
	TIME [epoch: 8.43 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45221427223813554		[learning rate: 0.0022945]
	Learning Rate: 0.00229448
	LOSS [training: 0.45221427223813554 | validation: 0.31816746022689546]
	TIME [epoch: 8.43 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37261923667665575		[learning rate: 0.0022862]
	Learning Rate: 0.00228615
	LOSS [training: 0.37261923667665575 | validation: 0.3357911725769587]
	TIME [epoch: 8.45 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4594100501700421		[learning rate: 0.0022779]
	Learning Rate: 0.00227786
	LOSS [training: 0.4594100501700421 | validation: 0.35023012525257885]
	TIME [epoch: 8.44 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4386528169833678		[learning rate: 0.0022696]
	Learning Rate: 0.00226959
	LOSS [training: 0.4386528169833678 | validation: 0.32129260387861597]
	TIME [epoch: 8.44 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44624226388675864		[learning rate: 0.0022614]
	Learning Rate: 0.00226135
	LOSS [training: 0.44624226388675864 | validation: 0.266962640298349]
	TIME [epoch: 8.44 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41945242916854875		[learning rate: 0.0022531]
	Learning Rate: 0.00225315
	LOSS [training: 0.41945242916854875 | validation: 0.6995949073221852]
	TIME [epoch: 8.46 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43858268450301113		[learning rate: 0.002245]
	Learning Rate: 0.00224497
	LOSS [training: 0.43858268450301113 | validation: 0.5022270431050726]
	TIME [epoch: 8.44 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4190908539033976		[learning rate: 0.0022368]
	Learning Rate: 0.00223682
	LOSS [training: 0.4190908539033976 | validation: 0.274611697287807]
	TIME [epoch: 8.43 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40261311545501766		[learning rate: 0.0022287]
	Learning Rate: 0.00222871
	LOSS [training: 0.40261311545501766 | validation: 0.23796067174981156]
	TIME [epoch: 8.44 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4117820897058226		[learning rate: 0.0022206]
	Learning Rate: 0.00222062
	LOSS [training: 0.4117820897058226 | validation: 0.28146219542638906]
	TIME [epoch: 8.47 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40259954834280604		[learning rate: 0.0022126]
	Learning Rate: 0.00221256
	LOSS [training: 0.40259954834280604 | validation: 0.24612252421859668]
	TIME [epoch: 8.45 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36348817123317767		[learning rate: 0.0022045]
	Learning Rate: 0.00220453
	LOSS [training: 0.36348817123317767 | validation: 0.4032664202887867]
	TIME [epoch: 8.44 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35858063361529935		[learning rate: 0.0021965]
	Learning Rate: 0.00219653
	LOSS [training: 0.35858063361529935 | validation: 0.25375784032436416]
	TIME [epoch: 8.45 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46098415732737796		[learning rate: 0.0021886]
	Learning Rate: 0.00218856
	LOSS [training: 0.46098415732737796 | validation: 0.33455861510914287]
	TIME [epoch: 8.46 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4315403847236281		[learning rate: 0.0021806]
	Learning Rate: 0.00218061
	LOSS [training: 0.4315403847236281 | validation: 0.2989995629552536]
	TIME [epoch: 8.44 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3949676315101388		[learning rate: 0.0021727]
	Learning Rate: 0.0021727
	LOSS [training: 0.3949676315101388 | validation: 0.2666052759210301]
	TIME [epoch: 8.43 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4528133774323848		[learning rate: 0.0021648]
	Learning Rate: 0.00216482
	LOSS [training: 0.4528133774323848 | validation: 0.26600188165322813]
	TIME [epoch: 8.43 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39571086475662937		[learning rate: 0.002157]
	Learning Rate: 0.00215696
	LOSS [training: 0.39571086475662937 | validation: 0.466281037575651]
	TIME [epoch: 8.46 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42392892471034693		[learning rate: 0.0021491]
	Learning Rate: 0.00214913
	LOSS [training: 0.42392892471034693 | validation: 0.31699651186758054]
	TIME [epoch: 8.44 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41247982671437156		[learning rate: 0.0021413]
	Learning Rate: 0.00214133
	LOSS [training: 0.41247982671437156 | validation: 0.2968059703317859]
	TIME [epoch: 8.43 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34213234560465905		[learning rate: 0.0021336]
	Learning Rate: 0.00213356
	LOSS [training: 0.34213234560465905 | validation: 0.33679386741719997]
	TIME [epoch: 8.45 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38107778705224754		[learning rate: 0.0021258]
	Learning Rate: 0.00212582
	LOSS [training: 0.38107778705224754 | validation: 0.2963822158481981]
	TIME [epoch: 8.45 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4076196750244635		[learning rate: 0.0021181]
	Learning Rate: 0.0021181
	LOSS [training: 0.4076196750244635 | validation: 0.28461370899275357]
	TIME [epoch: 8.46 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4065717066147013		[learning rate: 0.0021104]
	Learning Rate: 0.00211042
	LOSS [training: 0.4065717066147013 | validation: 0.30270671467426696]
	TIME [epoch: 8.44 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40926846317999094		[learning rate: 0.0021028]
	Learning Rate: 0.00210276
	LOSS [training: 0.40926846317999094 | validation: 0.35496205588073526]
	TIME [epoch: 8.44 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3831509590032553		[learning rate: 0.0020951]
	Learning Rate: 0.00209513
	LOSS [training: 0.3831509590032553 | validation: 0.5839563499375966]
	TIME [epoch: 8.45 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45504727173730136		[learning rate: 0.0020875]
	Learning Rate: 0.00208752
	LOSS [training: 0.45504727173730136 | validation: 0.251361127470429]
	TIME [epoch: 8.47 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5334496478112507		[learning rate: 0.0020799]
	Learning Rate: 0.00207995
	LOSS [training: 0.5334496478112507 | validation: 0.2662490691069265]
	TIME [epoch: 8.43 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39291229105466996		[learning rate: 0.0020724]
	Learning Rate: 0.0020724
	LOSS [training: 0.39291229105466996 | validation: 0.24973992914012982]
	TIME [epoch: 8.43 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4253380128263206		[learning rate: 0.0020649]
	Learning Rate: 0.00206488
	LOSS [training: 0.4253380128263206 | validation: 0.36098856829601994]
	TIME [epoch: 8.43 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43131631055122827		[learning rate: 0.0020574]
	Learning Rate: 0.00205739
	LOSS [training: 0.43131631055122827 | validation: 0.34632162193361676]
	TIME [epoch: 8.46 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48228033866342723		[learning rate: 0.0020499]
	Learning Rate: 0.00204992
	LOSS [training: 0.48228033866342723 | validation: 0.48694541242255207]
	TIME [epoch: 8.43 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4438884748194993		[learning rate: 0.0020425]
	Learning Rate: 0.00204248
	LOSS [training: 0.4438884748194993 | validation: 0.2352545564185007]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38843628768251254		[learning rate: 0.0020351]
	Learning Rate: 0.00203507
	LOSS [training: 0.38843628768251254 | validation: 0.23780345774918504]
	TIME [epoch: 8.43 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37486179561556005		[learning rate: 0.0020277]
	Learning Rate: 0.00202768
	LOSS [training: 0.37486179561556005 | validation: 0.2491607647491284]
	TIME [epoch: 8.45 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44662472555034743		[learning rate: 0.0020203]
	Learning Rate: 0.00202032
	LOSS [training: 0.44662472555034743 | validation: 0.2315450673254435]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41281601462422113		[learning rate: 0.002013]
	Learning Rate: 0.00201299
	LOSS [training: 0.41281601462422113 | validation: 0.26959442290389735]
	TIME [epoch: 8.44 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42157456693390555		[learning rate: 0.0020057]
	Learning Rate: 0.00200569
	LOSS [training: 0.42157456693390555 | validation: 0.5025127625006773]
	TIME [epoch: 8.43 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4533916913708613		[learning rate: 0.0019984]
	Learning Rate: 0.00199841
	LOSS [training: 0.4533916913708613 | validation: 0.40611829848461084]
	TIME [epoch: 8.44 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3656785743001242		[learning rate: 0.0019912]
	Learning Rate: 0.00199116
	LOSS [training: 0.3656785743001242 | validation: 0.3628524275158393]
	TIME [epoch: 8.43 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3878713371436907		[learning rate: 0.0019839]
	Learning Rate: 0.00198393
	LOSS [training: 0.3878713371436907 | validation: 0.31913406106487513]
	TIME [epoch: 8.43 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3458934821032674		[learning rate: 0.0019767]
	Learning Rate: 0.00197673
	LOSS [training: 0.3458934821032674 | validation: 0.2441122801669167]
	TIME [epoch: 8.42 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3605486190995821		[learning rate: 0.0019696]
	Learning Rate: 0.00196956
	LOSS [training: 0.3605486190995821 | validation: 0.39139250425780747]
	TIME [epoch: 8.42 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37227093116973914		[learning rate: 0.0019624]
	Learning Rate: 0.00196241
	LOSS [training: 0.37227093116973914 | validation: 0.26247798865013594]
	TIME [epoch: 8.44 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4071821178942985		[learning rate: 0.0019553]
	Learning Rate: 0.00195529
	LOSS [training: 0.4071821178942985 | validation: 0.4483733296767656]
	TIME [epoch: 8.43 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3771547948963102		[learning rate: 0.0019482]
	Learning Rate: 0.00194819
	LOSS [training: 0.3771547948963102 | validation: 0.4954622267823626]
	TIME [epoch: 8.42 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43395068593191793		[learning rate: 0.0019411]
	Learning Rate: 0.00194112
	LOSS [training: 0.43395068593191793 | validation: 0.2962743907845452]
	TIME [epoch: 8.43 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40415465353010943		[learning rate: 0.0019341]
	Learning Rate: 0.00193408
	LOSS [training: 0.40415465353010943 | validation: 0.36899829190946554]
	TIME [epoch: 8.45 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38833567215763704		[learning rate: 0.0019271]
	Learning Rate: 0.00192706
	LOSS [training: 0.38833567215763704 | validation: 0.516611130151974]
	TIME [epoch: 8.44 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3824465262067766		[learning rate: 0.0019201]
	Learning Rate: 0.00192006
	LOSS [training: 0.3824465262067766 | validation: 0.3270735086816839]
	TIME [epoch: 8.43 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36101700550297083		[learning rate: 0.0019131]
	Learning Rate: 0.0019131
	LOSS [training: 0.36101700550297083 | validation: 0.4414631871159681]
	TIME [epoch: 8.43 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3624084405537379		[learning rate: 0.0019062]
	Learning Rate: 0.00190615
	LOSS [training: 0.3624084405537379 | validation: 0.4587723648159518]
	TIME [epoch: 8.45 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3562986303938525		[learning rate: 0.0018992]
	Learning Rate: 0.00189924
	LOSS [training: 0.3562986303938525 | validation: 0.30499520126966007]
	TIME [epoch: 8.44 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41106723355456565		[learning rate: 0.0018923]
	Learning Rate: 0.00189234
	LOSS [training: 0.41106723355456565 | validation: 0.34854521291578533]
	TIME [epoch: 8.44 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49299574807262153		[learning rate: 0.0018855]
	Learning Rate: 0.00188548
	LOSS [training: 0.49299574807262153 | validation: 0.2898034108565546]
	TIME [epoch: 8.43 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3919231094545358		[learning rate: 0.0018786]
	Learning Rate: 0.00187863
	LOSS [training: 0.3919231094545358 | validation: 0.258781689282488]
	TIME [epoch: 8.45 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38662095610471636		[learning rate: 0.0018718]
	Learning Rate: 0.00187182
	LOSS [training: 0.38662095610471636 | validation: 0.49760537400803767]
	TIME [epoch: 8.44 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44849851248728523		[learning rate: 0.001865]
	Learning Rate: 0.00186502
	LOSS [training: 0.44849851248728523 | validation: 0.8234899530599171]
	TIME [epoch: 8.41 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40973448788753875		[learning rate: 0.0018583]
	Learning Rate: 0.00185825
	LOSS [training: 0.40973448788753875 | validation: 0.3119344580289893]
	TIME [epoch: 8.43 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39515701803496384		[learning rate: 0.0018515]
	Learning Rate: 0.00185151
	LOSS [training: 0.39515701803496384 | validation: 0.40527163837845603]
	TIME [epoch: 8.45 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3908228735944932		[learning rate: 0.0018448]
	Learning Rate: 0.00184479
	LOSS [training: 0.3908228735944932 | validation: 0.29917680267344093]
	TIME [epoch: 8.43 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4499460583735451		[learning rate: 0.0018381]
	Learning Rate: 0.0018381
	LOSS [training: 0.4499460583735451 | validation: 0.4617735799732982]
	TIME [epoch: 8.43 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3695997596479067		[learning rate: 0.0018314]
	Learning Rate: 0.00183143
	LOSS [training: 0.3695997596479067 | validation: 0.26716066319624354]
	TIME [epoch: 8.43 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3297941965791666		[learning rate: 0.0018248]
	Learning Rate: 0.00182478
	LOSS [training: 0.3297941965791666 | validation: 0.3767193974207881]
	TIME [epoch: 8.43 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3865932437488431		[learning rate: 0.0018182]
	Learning Rate: 0.00181816
	LOSS [training: 0.3865932437488431 | validation: 0.24590722130178977]
	TIME [epoch: 8.44 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39148784677277226		[learning rate: 0.0018116]
	Learning Rate: 0.00181156
	LOSS [training: 0.39148784677277226 | validation: 0.4026832495039133]
	TIME [epoch: 8.43 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36432988240897707		[learning rate: 0.001805]
	Learning Rate: 0.00180499
	LOSS [training: 0.36432988240897707 | validation: 0.6769587148554541]
	TIME [epoch: 8.43 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4564182581443249		[learning rate: 0.0017984]
	Learning Rate: 0.00179844
	LOSS [training: 0.4564182581443249 | validation: 0.30986179457778995]
	TIME [epoch: 8.44 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3622707258940566		[learning rate: 0.0017919]
	Learning Rate: 0.00179191
	LOSS [training: 0.3622707258940566 | validation: 0.48383520968510374]
	TIME [epoch: 8.47 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.434461286476278		[learning rate: 0.0017854]
	Learning Rate: 0.00178541
	LOSS [training: 0.434461286476278 | validation: 0.4270117381464433]
	TIME [epoch: 8.43 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3740280491694408		[learning rate: 0.0017789]
	Learning Rate: 0.00177893
	LOSS [training: 0.3740280491694408 | validation: 0.3118141026533895]
	TIME [epoch: 8.43 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33491323967368786		[learning rate: 0.0017725]
	Learning Rate: 0.00177247
	LOSS [training: 0.33491323967368786 | validation: 0.25589672771471106]
	TIME [epoch: 8.43 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38111740970867214		[learning rate: 0.001766]
	Learning Rate: 0.00176604
	LOSS [training: 0.38111740970867214 | validation: 0.23886313350003702]
	TIME [epoch: 8.45 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32913580139930143		[learning rate: 0.0017596]
	Learning Rate: 0.00175963
	LOSS [training: 0.32913580139930143 | validation: 0.244744938803654]
	TIME [epoch: 8.43 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33191178197525617		[learning rate: 0.0017532]
	Learning Rate: 0.00175324
	LOSS [training: 0.33191178197525617 | validation: 0.2919468774865238]
	TIME [epoch: 8.43 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4220585451917647		[learning rate: 0.0017469]
	Learning Rate: 0.00174688
	LOSS [training: 0.4220585451917647 | validation: 0.30080809061343294]
	TIME [epoch: 8.42 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31077612466009874		[learning rate: 0.0017405]
	Learning Rate: 0.00174054
	LOSS [training: 0.31077612466009874 | validation: 0.2195605736997312]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_581.pth
	Model improved!!!
EPOCH 582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3947868597798551		[learning rate: 0.0017342]
	Learning Rate: 0.00173422
	LOSS [training: 0.3947868597798551 | validation: 0.29246152130658076]
	TIME [epoch: 8.44 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38065212108619756		[learning rate: 0.0017279]
	Learning Rate: 0.00172793
	LOSS [training: 0.38065212108619756 | validation: 0.2513537378021843]
	TIME [epoch: 8.42 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3495289892497213		[learning rate: 0.0017217]
	Learning Rate: 0.00172166
	LOSS [training: 0.3495289892497213 | validation: 0.2233152430437756]
	TIME [epoch: 8.44 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3436351851983417		[learning rate: 0.0017154]
	Learning Rate: 0.00171541
	LOSS [training: 0.3436351851983417 | validation: 0.24381050776134494]
	TIME [epoch: 8.45 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3699335807223879		[learning rate: 0.0017092]
	Learning Rate: 0.00170919
	LOSS [training: 0.3699335807223879 | validation: 0.5440417220104581]
	TIME [epoch: 8.43 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4070184486604365		[learning rate: 0.001703]
	Learning Rate: 0.00170298
	LOSS [training: 0.4070184486604365 | validation: 0.22300480141355794]
	TIME [epoch: 8.43 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35782151702603066		[learning rate: 0.0016968]
	Learning Rate: 0.0016968
	LOSS [training: 0.35782151702603066 | validation: 0.24581320027484255]
	TIME [epoch: 8.43 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34019585428073285		[learning rate: 0.0016906]
	Learning Rate: 0.00169065
	LOSS [training: 0.34019585428073285 | validation: 0.3380970016973427]
	TIME [epoch: 8.43 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3873903482165759		[learning rate: 0.0016845]
	Learning Rate: 0.00168451
	LOSS [training: 0.3873903482165759 | validation: 0.3421748504474207]
	TIME [epoch: 8.45 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36652797918914537		[learning rate: 0.0016784]
	Learning Rate: 0.0016784
	LOSS [training: 0.36652797918914537 | validation: 0.21971099901863467]
	TIME [epoch: 8.43 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3770229678370662		[learning rate: 0.0016723]
	Learning Rate: 0.00167231
	LOSS [training: 0.3770229678370662 | validation: 0.2598419554661966]
	TIME [epoch: 8.42 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34529934400051726		[learning rate: 0.0016662]
	Learning Rate: 0.00166624
	LOSS [training: 0.34529934400051726 | validation: 0.35171308370657284]
	TIME [epoch: 8.42 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32222758179558614		[learning rate: 0.0016602]
	Learning Rate: 0.00166019
	LOSS [training: 0.32222758179558614 | validation: 0.21093358490972533]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_594.pth
	Model improved!!!
EPOCH 595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3625175921766356		[learning rate: 0.0016542]
	Learning Rate: 0.00165417
	LOSS [training: 0.3625175921766356 | validation: 0.20008788231238395]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31557613853566985		[learning rate: 0.0016482]
	Learning Rate: 0.00164816
	LOSS [training: 0.31557613853566985 | validation: 0.2511590104414136]
	TIME [epoch: 8.42 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3464876286355962		[learning rate: 0.0016422]
	Learning Rate: 0.00164218
	LOSS [training: 0.3464876286355962 | validation: 0.20108318502899855]
	TIME [epoch: 8.43 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4233817992685095		[learning rate: 0.0016362]
	Learning Rate: 0.00163622
	LOSS [training: 0.4233817992685095 | validation: 0.25859624291542394]
	TIME [epoch: 8.45 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33759962273373645		[learning rate: 0.0016303]
	Learning Rate: 0.00163028
	LOSS [training: 0.33759962273373645 | validation: 0.2865442693136886]
	TIME [epoch: 8.42 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.334091177354482		[learning rate: 0.0016244]
	Learning Rate: 0.00162437
	LOSS [training: 0.334091177354482 | validation: 0.3680662019875267]
	TIME [epoch: 8.43 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41480924107100964		[learning rate: 0.0016185]
	Learning Rate: 0.00161847
	LOSS [training: 0.41480924107100964 | validation: 0.3256560911701479]
	TIME [epoch: 8.41 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.341060925198605		[learning rate: 0.0016126]
	Learning Rate: 0.0016126
	LOSS [training: 0.341060925198605 | validation: 0.25820717068664906]
	TIME [epoch: 8.45 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34633833775288114		[learning rate: 0.0016067]
	Learning Rate: 0.00160675
	LOSS [training: 0.34633833775288114 | validation: 0.2660488482542318]
	TIME [epoch: 8.41 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.334208827309595		[learning rate: 0.0016009]
	Learning Rate: 0.00160092
	LOSS [training: 0.334208827309595 | validation: 0.26222084607265284]
	TIME [epoch: 8.42 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3452547353217926		[learning rate: 0.0015951]
	Learning Rate: 0.00159511
	LOSS [training: 0.3452547353217926 | validation: 0.21299944380489738]
	TIME [epoch: 8.41 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3350005486568941		[learning rate: 0.0015893]
	Learning Rate: 0.00158932
	LOSS [training: 0.3350005486568941 | validation: 0.3755601660779376]
	TIME [epoch: 8.42 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3484059258072653		[learning rate: 0.0015835]
	Learning Rate: 0.00158355
	LOSS [training: 0.3484059258072653 | validation: 0.29154598417901123]
	TIME [epoch: 8.45 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32514533826800907		[learning rate: 0.0015778]
	Learning Rate: 0.0015778
	LOSS [training: 0.32514533826800907 | validation: 0.2267822458162814]
	TIME [epoch: 8.41 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.345525198910858		[learning rate: 0.0015721]
	Learning Rate: 0.00157208
	LOSS [training: 0.345525198910858 | validation: 0.2020204676410691]
	TIME [epoch: 8.42 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33164634681238087		[learning rate: 0.0015664]
	Learning Rate: 0.00156637
	LOSS [training: 0.33164634681238087 | validation: 0.267561447019097]
	TIME [epoch: 8.42 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3297265815807404		[learning rate: 0.0015607]
	Learning Rate: 0.00156069
	LOSS [training: 0.3297265815807404 | validation: 0.37366681300513116]
	TIME [epoch: 8.45 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34688875384692575		[learning rate: 0.001555]
	Learning Rate: 0.00155502
	LOSS [training: 0.34688875384692575 | validation: 0.23456243251584996]
	TIME [epoch: 8.41 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34141833207937616		[learning rate: 0.0015494]
	Learning Rate: 0.00154938
	LOSS [training: 0.34141833207937616 | validation: 0.23117126766694876]
	TIME [epoch: 8.42 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29107060351304814		[learning rate: 0.0015438]
	Learning Rate: 0.00154376
	LOSS [training: 0.29107060351304814 | validation: 0.3796304266708731]
	TIME [epoch: 8.43 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3136409013831961		[learning rate: 0.0015382]
	Learning Rate: 0.00153815
	LOSS [training: 0.3136409013831961 | validation: 0.2546598405917334]
	TIME [epoch: 8.43 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3592977146825637		[learning rate: 0.0015326]
	Learning Rate: 0.00153257
	LOSS [training: 0.3592977146825637 | validation: 0.2585990961559146]
	TIME [epoch: 8.43 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3403590228957002		[learning rate: 0.001527]
	Learning Rate: 0.00152701
	LOSS [training: 0.3403590228957002 | validation: 0.3410491459386069]
	TIME [epoch: 8.42 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3728718168654405		[learning rate: 0.0015215]
	Learning Rate: 0.00152147
	LOSS [training: 0.3728718168654405 | validation: 0.22719520549069278]
	TIME [epoch: 8.43 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28843885476885195		[learning rate: 0.0015159]
	Learning Rate: 0.00151595
	LOSS [training: 0.28843885476885195 | validation: 0.2091860013247583]
	TIME [epoch: 8.43 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2864032856286386		[learning rate: 0.0015104]
	Learning Rate: 0.00151045
	LOSS [training: 0.2864032856286386 | validation: 0.3772185474463505]
	TIME [epoch: 8.43 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30558769825968435		[learning rate: 0.001505]
	Learning Rate: 0.00150496
	LOSS [training: 0.30558769825968435 | validation: 0.268906012837273]
	TIME [epoch: 8.42 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30946458962227974		[learning rate: 0.0014995]
	Learning Rate: 0.0014995
	LOSS [training: 0.30946458962227974 | validation: 0.23714409717303161]
	TIME [epoch: 8.44 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30487578639147433		[learning rate: 0.0014941]
	Learning Rate: 0.00149406
	LOSS [training: 0.30487578639147433 | validation: 0.23374120753734484]
	TIME [epoch: 8.43 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3576651194164469		[learning rate: 0.0014886]
	Learning Rate: 0.00148864
	LOSS [training: 0.3576651194164469 | validation: 0.20390794031213103]
	TIME [epoch: 8.42 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3085072807441138		[learning rate: 0.0014832]
	Learning Rate: 0.00148324
	LOSS [training: 0.3085072807441138 | validation: 0.2199033533533799]
	TIME [epoch: 8.42 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3534622247895207		[learning rate: 0.0014779]
	Learning Rate: 0.00147785
	LOSS [training: 0.3534622247895207 | validation: 0.2161028679130233]
	TIME [epoch: 8.44 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3025326116564671		[learning rate: 0.0014725]
	Learning Rate: 0.00147249
	LOSS [training: 0.3025326116564671 | validation: 0.23452809748874182]
	TIME [epoch: 8.42 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2792952853888334		[learning rate: 0.0014671]
	Learning Rate: 0.00146715
	LOSS [training: 0.2792952853888334 | validation: 0.20109842301172665]
	TIME [epoch: 8.41 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36579970014229085		[learning rate: 0.0014618]
	Learning Rate: 0.00146182
	LOSS [training: 0.36579970014229085 | validation: 0.26366811769709203]
	TIME [epoch: 8.42 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3131396071240081		[learning rate: 0.0014565]
	Learning Rate: 0.00145652
	LOSS [training: 0.3131396071240081 | validation: 0.2072497956044959]
	TIME [epoch: 8.45 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32497992445920565		[learning rate: 0.0014512]
	Learning Rate: 0.00145123
	LOSS [training: 0.32497992445920565 | validation: 0.2157853882870338]
	TIME [epoch: 8.43 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3024222166677692		[learning rate: 0.001446]
	Learning Rate: 0.00144597
	LOSS [training: 0.3024222166677692 | validation: 0.2856191381965806]
	TIME [epoch: 8.43 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43328384956879135		[learning rate: 0.0014407]
	Learning Rate: 0.00144072
	LOSS [training: 0.43328384956879135 | validation: 0.19451282498283198]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_633.pth
	Model improved!!!
EPOCH 634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32791221122573794		[learning rate: 0.0014355]
	Learning Rate: 0.00143549
	LOSS [training: 0.32791221122573794 | validation: 0.2523978994843156]
	TIME [epoch: 8.46 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30968593606059913		[learning rate: 0.0014303]
	Learning Rate: 0.00143028
	LOSS [training: 0.30968593606059913 | validation: 0.2692335831616153]
	TIME [epoch: 8.42 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29402864644807325		[learning rate: 0.0014251]
	Learning Rate: 0.00142509
	LOSS [training: 0.29402864644807325 | validation: 0.4000524886188077]
	TIME [epoch: 8.43 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32607654991427193		[learning rate: 0.0014199]
	Learning Rate: 0.00141992
	LOSS [training: 0.32607654991427193 | validation: 0.21478147705302097]
	TIME [epoch: 8.42 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.306754121333625		[learning rate: 0.0014148]
	Learning Rate: 0.00141476
	LOSS [training: 0.306754121333625 | validation: 0.218320656350036]
	TIME [epoch: 8.45 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27716278934588345		[learning rate: 0.0014096]
	Learning Rate: 0.00140963
	LOSS [training: 0.27716278934588345 | validation: 0.25226839986185723]
	TIME [epoch: 8.43 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29374260212523684		[learning rate: 0.0014045]
	Learning Rate: 0.00140451
	LOSS [training: 0.29374260212523684 | validation: 0.18490682951985515]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_640.pth
	Model improved!!!
EPOCH 641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37196608747962123		[learning rate: 0.0013994]
	Learning Rate: 0.00139942
	LOSS [training: 0.37196608747962123 | validation: 0.21450504688485164]
	TIME [epoch: 8.42 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31376636370086486		[learning rate: 0.0013943]
	Learning Rate: 0.00139434
	LOSS [training: 0.31376636370086486 | validation: 0.32030407720711196]
	TIME [epoch: 8.45 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3594753441267676		[learning rate: 0.0013893]
	Learning Rate: 0.00138928
	LOSS [training: 0.3594753441267676 | validation: 0.3795409392597787]
	TIME [epoch: 8.43 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2995569212898202		[learning rate: 0.0013842]
	Learning Rate: 0.00138424
	LOSS [training: 0.2995569212898202 | validation: 0.2908575398933802]
	TIME [epoch: 8.42 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33605983111748494		[learning rate: 0.0013792]
	Learning Rate: 0.00137921
	LOSS [training: 0.33605983111748494 | validation: 0.260924381461646]
	TIME [epoch: 8.42 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32752784628527126		[learning rate: 0.0013742]
	Learning Rate: 0.00137421
	LOSS [training: 0.32752784628527126 | validation: 0.2595952446357693]
	TIME [epoch: 8.42 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3491092076084734		[learning rate: 0.0013692]
	Learning Rate: 0.00136922
	LOSS [training: 0.3491092076084734 | validation: 0.3816065510365273]
	TIME [epoch: 8.45 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4078541911583836		[learning rate: 0.0013643]
	Learning Rate: 0.00136425
	LOSS [training: 0.4078541911583836 | validation: 0.2810900046626068]
	TIME [epoch: 8.42 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3153567943199346		[learning rate: 0.0013593]
	Learning Rate: 0.0013593
	LOSS [training: 0.3153567943199346 | validation: 0.19757437253615606]
	TIME [epoch: 8.42 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2849196328872078		[learning rate: 0.0013544]
	Learning Rate: 0.00135437
	LOSS [training: 0.2849196328872078 | validation: 0.22495868701150257]
	TIME [epoch: 8.44 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32011231885208485		[learning rate: 0.0013495]
	Learning Rate: 0.00134945
	LOSS [training: 0.32011231885208485 | validation: 0.18504534880638207]
	TIME [epoch: 8.44 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.310927576704715		[learning rate: 0.0013446]
	Learning Rate: 0.00134456
	LOSS [training: 0.310927576704715 | validation: 0.234259677690431]
	TIME [epoch: 8.42 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.303191375884947		[learning rate: 0.0013397]
	Learning Rate: 0.00133968
	LOSS [training: 0.303191375884947 | validation: 0.1902737142991351]
	TIME [epoch: 8.42 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2884948512638145		[learning rate: 0.0013348]
	Learning Rate: 0.00133481
	LOSS [training: 0.2884948512638145 | validation: 0.21452460992229147]
	TIME [epoch: 8.43 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2929221735537188		[learning rate: 0.00133]
	Learning Rate: 0.00132997
	LOSS [training: 0.2929221735537188 | validation: 0.3340371459932681]
	TIME [epoch: 8.43 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30977350450056085		[learning rate: 0.0013251]
	Learning Rate: 0.00132514
	LOSS [training: 0.30977350450056085 | validation: 0.24893966928360145]
	TIME [epoch: 8.42 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35168012240584734		[learning rate: 0.0013203]
	Learning Rate: 0.00132034
	LOSS [training: 0.35168012240584734 | validation: 0.18449974964369684]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_657.pth
	Model improved!!!
EPOCH 658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2892148706591996		[learning rate: 0.0013155]
	Learning Rate: 0.00131554
	LOSS [training: 0.2892148706591996 | validation: 0.27193864180413274]
	TIME [epoch: 8.44 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31334950892032604		[learning rate: 0.0013108]
	Learning Rate: 0.00131077
	LOSS [training: 0.31334950892032604 | validation: 0.26108506299265605]
	TIME [epoch: 8.43 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2889572276622118		[learning rate: 0.001306]
	Learning Rate: 0.00130601
	LOSS [training: 0.2889572276622118 | validation: 0.23115397424443604]
	TIME [epoch: 8.42 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2907161930131431		[learning rate: 0.0013013]
	Learning Rate: 0.00130127
	LOSS [training: 0.2907161930131431 | validation: 0.1858519678801594]
	TIME [epoch: 8.42 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3000529135277974		[learning rate: 0.0012966]
	Learning Rate: 0.00129655
	LOSS [training: 0.3000529135277974 | validation: 0.30509075734713187]
	TIME [epoch: 8.43 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28980963667079485		[learning rate: 0.0012918]
	Learning Rate: 0.00129185
	LOSS [training: 0.28980963667079485 | validation: 0.19186338215790205]
	TIME [epoch: 8.45 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3593918846081888		[learning rate: 0.0012872]
	Learning Rate: 0.00128716
	LOSS [training: 0.3593918846081888 | validation: 0.19915522000104677]
	TIME [epoch: 8.43 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29384712107746		[learning rate: 0.0012825]
	Learning Rate: 0.00128249
	LOSS [training: 0.29384712107746 | validation: 0.23200805375528766]
	TIME [epoch: 8.42 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31425070371965474		[learning rate: 0.0012778]
	Learning Rate: 0.00127783
	LOSS [training: 0.31425070371965474 | validation: 0.21306023295591314]
	TIME [epoch: 8.43 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31302142172598424		[learning rate: 0.0012732]
	Learning Rate: 0.00127319
	LOSS [training: 0.31302142172598424 | validation: 0.2153746758679203]
	TIME [epoch: 8.43 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3355737573807855		[learning rate: 0.0012686]
	Learning Rate: 0.00126857
	LOSS [training: 0.3355737573807855 | validation: 0.25808510052194]
	TIME [epoch: 8.42 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3218776491633073		[learning rate: 0.001264]
	Learning Rate: 0.00126397
	LOSS [training: 0.3218776491633073 | validation: 0.22912145005336831]
	TIME [epoch: 8.42 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29696916113373967		[learning rate: 0.0012594]
	Learning Rate: 0.00125938
	LOSS [training: 0.29696916113373967 | validation: 0.23228833020234602]
	TIME [epoch: 8.44 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28948623168343635		[learning rate: 0.0012548]
	Learning Rate: 0.00125481
	LOSS [training: 0.28948623168343635 | validation: 0.2577893390918519]
	TIME [epoch: 8.43 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2829932656565681		[learning rate: 0.0012503]
	Learning Rate: 0.00125026
	LOSS [training: 0.2829932656565681 | validation: 0.4451163649241884]
	TIME [epoch: 8.43 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32593026157925664		[learning rate: 0.0012457]
	Learning Rate: 0.00124572
	LOSS [training: 0.32593026157925664 | validation: 0.24102699396382443]
	TIME [epoch: 8.43 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28071209901050537		[learning rate: 0.0012412]
	Learning Rate: 0.0012412
	LOSS [training: 0.28071209901050537 | validation: 0.2877068594770756]
	TIME [epoch: 8.44 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29933113689436885		[learning rate: 0.0012367]
	Learning Rate: 0.0012367
	LOSS [training: 0.29933113689436885 | validation: 0.22617343855623623]
	TIME [epoch: 8.43 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3293158330548552		[learning rate: 0.0012322]
	Learning Rate: 0.00123221
	LOSS [training: 0.3293158330548552 | validation: 0.3499329983563224]
	TIME [epoch: 8.43 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29227850924044685		[learning rate: 0.0012277]
	Learning Rate: 0.00122774
	LOSS [training: 0.29227850924044685 | validation: 0.19929469102849937]
	TIME [epoch: 8.42 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33169994580833934		[learning rate: 0.0012233]
	Learning Rate: 0.00122328
	LOSS [training: 0.33169994580833934 | validation: 0.22998569151604392]
	TIME [epoch: 8.45 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30386097350539587		[learning rate: 0.0012188]
	Learning Rate: 0.00121884
	LOSS [training: 0.30386097350539587 | validation: 0.18797509186649064]
	TIME [epoch: 8.43 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.283647522868577		[learning rate: 0.0012144]
	Learning Rate: 0.00121442
	LOSS [training: 0.283647522868577 | validation: 0.21706781741381342]
	TIME [epoch: 8.44 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2803385373874681		[learning rate: 0.00121]
	Learning Rate: 0.00121001
	LOSS [training: 0.2803385373874681 | validation: 0.20750391968318616]
	TIME [epoch: 8.42 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3246190055260099		[learning rate: 0.0012056]
	Learning Rate: 0.00120562
	LOSS [training: 0.3246190055260099 | validation: 0.27541275516137426]
	TIME [epoch: 8.45 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2820235256505773		[learning rate: 0.0012012]
	Learning Rate: 0.00120125
	LOSS [training: 0.2820235256505773 | validation: 0.22398463885668501]
	TIME [epoch: 8.44 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2841068820123675		[learning rate: 0.0011969]
	Learning Rate: 0.00119689
	LOSS [training: 0.2841068820123675 | validation: 0.18055637115926432]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_684.pth
	Model improved!!!
EPOCH 685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2885896464817478		[learning rate: 0.0011925]
	Learning Rate: 0.00119254
	LOSS [training: 0.2885896464817478 | validation: 0.20794865351293879]
	TIME [epoch: 8.43 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29469108297944013		[learning rate: 0.0011882]
	Learning Rate: 0.00118821
	LOSS [training: 0.29469108297944013 | validation: 0.22621538007542255]
	TIME [epoch: 8.46 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3081022277920826		[learning rate: 0.0011839]
	Learning Rate: 0.0011839
	LOSS [training: 0.3081022277920826 | validation: 0.28164073157588576]
	TIME [epoch: 8.43 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3068206902826988		[learning rate: 0.0011796]
	Learning Rate: 0.00117961
	LOSS [training: 0.3068206902826988 | validation: 0.2083653454795875]
	TIME [epoch: 8.43 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31936044145418835		[learning rate: 0.0011753]
	Learning Rate: 0.00117532
	LOSS [training: 0.31936044145418835 | validation: 0.26435136270239146]
	TIME [epoch: 8.44 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25849180418006845		[learning rate: 0.0011711]
	Learning Rate: 0.00117106
	LOSS [training: 0.25849180418006845 | validation: 0.3487680615844253]
	TIME [epoch: 8.46 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3395576602718365		[learning rate: 0.0011668]
	Learning Rate: 0.00116681
	LOSS [training: 0.3395576602718365 | validation: 0.19585591828034965]
	TIME [epoch: 8.44 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3992019326779763		[learning rate: 0.0011626]
	Learning Rate: 0.00116258
	LOSS [training: 0.3992019326779763 | validation: 0.22014423736452166]
	TIME [epoch: 8.43 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29494978154762835		[learning rate: 0.0011584]
	Learning Rate: 0.00115836
	LOSS [training: 0.29494978154762835 | validation: 0.3034601374901888]
	TIME [epoch: 8.43 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32562232897752125		[learning rate: 0.0011542]
	Learning Rate: 0.00115415
	LOSS [training: 0.32562232897752125 | validation: 0.17733361429878614]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3206950376236909		[learning rate: 0.00115]
	Learning Rate: 0.00114996
	LOSS [training: 0.3206950376236909 | validation: 0.25753632600209575]
	TIME [epoch: 8.43 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2843483111730562		[learning rate: 0.0011458]
	Learning Rate: 0.00114579
	LOSS [training: 0.2843483111730562 | validation: 0.23106234226274242]
	TIME [epoch: 8.43 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30517575821311105		[learning rate: 0.0011416]
	Learning Rate: 0.00114163
	LOSS [training: 0.30517575821311105 | validation: 0.35720844303261823]
	TIME [epoch: 8.43 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29478354028479414		[learning rate: 0.0011375]
	Learning Rate: 0.00113749
	LOSS [training: 0.29478354028479414 | validation: 0.28692111180560204]
	TIME [epoch: 8.43 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28862294146663753		[learning rate: 0.0011334]
	Learning Rate: 0.00113336
	LOSS [training: 0.28862294146663753 | validation: 0.20503876356668072]
	TIME [epoch: 8.44 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27487718044381776		[learning rate: 0.0011292]
	Learning Rate: 0.00112925
	LOSS [training: 0.27487718044381776 | validation: 0.16851240660109656]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_700.pth
	Model improved!!!
EPOCH 701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3167798372693397		[learning rate: 0.0011252]
	Learning Rate: 0.00112515
	LOSS [training: 0.3167798372693397 | validation: 0.20743659042810986]
	TIME [epoch: 8.42 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28661926526824016		[learning rate: 0.0011211]
	Learning Rate: 0.00112107
	LOSS [training: 0.28661926526824016 | validation: 0.2067839153137526]
	TIME [epoch: 8.43 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25084828119450614		[learning rate: 0.001117]
	Learning Rate: 0.001117
	LOSS [training: 0.25084828119450614 | validation: 0.23369319165985236]
	TIME [epoch: 8.46 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29635360459330035		[learning rate: 0.0011129]
	Learning Rate: 0.00111295
	LOSS [training: 0.29635360459330035 | validation: 0.17723589257682798]
	TIME [epoch: 8.44 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28921794044432414		[learning rate: 0.0011089]
	Learning Rate: 0.00110891
	LOSS [training: 0.28921794044432414 | validation: 0.2329515107791168]
	TIME [epoch: 8.43 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2985590593701595		[learning rate: 0.0011049]
	Learning Rate: 0.00110488
	LOSS [training: 0.2985590593701595 | validation: 0.18846572592018274]
	TIME [epoch: 8.43 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26487609939324186		[learning rate: 0.0011009]
	Learning Rate: 0.00110087
	LOSS [training: 0.26487609939324186 | validation: 0.26083096026239166]
	TIME [epoch: 8.44 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2757092674020956		[learning rate: 0.0010969]
	Learning Rate: 0.00109688
	LOSS [training: 0.2757092674020956 | validation: 0.23023730465383524]
	TIME [epoch: 8.43 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29527499277674446		[learning rate: 0.0010929]
	Learning Rate: 0.0010929
	LOSS [training: 0.29527499277674446 | validation: 0.2611804894287749]
	TIME [epoch: 8.43 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3498228907094261		[learning rate: 0.0010889]
	Learning Rate: 0.00108893
	LOSS [training: 0.3498228907094261 | validation: 0.4058398994766702]
	TIME [epoch: 8.43 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3459591815924351		[learning rate: 0.001085]
	Learning Rate: 0.00108498
	LOSS [training: 0.3459591815924351 | validation: 0.2627126872257976]
	TIME [epoch: 8.45 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2799647907986631		[learning rate: 0.001081]
	Learning Rate: 0.00108104
	LOSS [training: 0.2799647907986631 | validation: 0.19042654244985574]
	TIME [epoch: 8.43 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3442553771196894		[learning rate: 0.0010771]
	Learning Rate: 0.00107712
	LOSS [training: 0.3442553771196894 | validation: 0.2848073788321328]
	TIME [epoch: 8.42 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2792156803172383		[learning rate: 0.0010732]
	Learning Rate: 0.00107321
	LOSS [training: 0.2792156803172383 | validation: 0.2672835841506765]
	TIME [epoch: 8.43 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2761494077233684		[learning rate: 0.0010693]
	Learning Rate: 0.00106931
	LOSS [training: 0.2761494077233684 | validation: 0.1999534781838393]
	TIME [epoch: 8.45 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29295475277048527		[learning rate: 0.0010654]
	Learning Rate: 0.00106543
	LOSS [training: 0.29295475277048527 | validation: 0.21811332446130094]
	TIME [epoch: 8.44 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2902563972573614		[learning rate: 0.0010616]
	Learning Rate: 0.00106157
	LOSS [training: 0.2902563972573614 | validation: 0.2399802518778325]
	TIME [epoch: 8.43 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2887882537801385		[learning rate: 0.0010577]
	Learning Rate: 0.00105771
	LOSS [training: 0.2887882537801385 | validation: 0.2000204334633826]
	TIME [epoch: 8.43 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31757924098463386		[learning rate: 0.0010539]
	Learning Rate: 0.00105388
	LOSS [training: 0.31757924098463386 | validation: 0.23841030976036082]
	TIME [epoch: 8.46 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26512718707220534		[learning rate: 0.0010501]
	Learning Rate: 0.00105005
	LOSS [training: 0.26512718707220534 | validation: 0.23108198315439288]
	TIME [epoch: 8.44 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26419791009077814		[learning rate: 0.0010462]
	Learning Rate: 0.00104624
	LOSS [training: 0.26419791009077814 | validation: 0.20331474550505155]
	TIME [epoch: 8.42 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30514477249229943		[learning rate: 0.0010424]
	Learning Rate: 0.00104244
	LOSS [training: 0.30514477249229943 | validation: 0.21282095492263742]
	TIME [epoch: 8.43 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28404184477844135		[learning rate: 0.0010387]
	Learning Rate: 0.00103866
	LOSS [training: 0.28404184477844135 | validation: 0.24169731236202263]
	TIME [epoch: 8.45 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2812393547396149		[learning rate: 0.0010349]
	Learning Rate: 0.00103489
	LOSS [training: 0.2812393547396149 | validation: 0.3974209444434932]
	TIME [epoch: 8.45 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30513834090204683		[learning rate: 0.0010311]
	Learning Rate: 0.00103114
	LOSS [training: 0.30513834090204683 | validation: 0.2179615386308941]
	TIME [epoch: 8.43 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26712514676999194		[learning rate: 0.0010274]
	Learning Rate: 0.00102739
	LOSS [training: 0.26712514676999194 | validation: 0.20545181188106293]
	TIME [epoch: 8.43 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2735297415178752		[learning rate: 0.0010237]
	Learning Rate: 0.00102367
	LOSS [training: 0.2735297415178752 | validation: 0.21137319110694436]
	TIME [epoch: 8.45 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3347955296192526		[learning rate: 0.00102]
	Learning Rate: 0.00101995
	LOSS [training: 0.3347955296192526 | validation: 0.1680386232988742]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_728.pth
	Model improved!!!
EPOCH 729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29355077277134123		[learning rate: 0.0010162]
	Learning Rate: 0.00101625
	LOSS [training: 0.29355077277134123 | validation: 0.29848073502002226]
	TIME [epoch: 8.44 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3405719846961743		[learning rate: 0.0010126]
	Learning Rate: 0.00101256
	LOSS [training: 0.3405719846961743 | validation: 0.2038003491864897]
	TIME [epoch: 8.44 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.283363053591028		[learning rate: 0.0010089]
	Learning Rate: 0.00100889
	LOSS [training: 0.283363053591028 | validation: 0.1812311363554991]
	TIME [epoch: 8.46 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3094277104307086		[learning rate: 0.0010052]
	Learning Rate: 0.00100522
	LOSS [training: 0.3094277104307086 | validation: 0.2794573065505743]
	TIME [epoch: 8.44 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2787784964295025		[learning rate: 0.0010016]
	Learning Rate: 0.00100158
	LOSS [training: 0.2787784964295025 | validation: 0.26377527458920336]
	TIME [epoch: 8.42 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25161681115377293		[learning rate: 0.00099794]
	Learning Rate: 0.000997942
	LOSS [training: 0.25161681115377293 | validation: 0.20413732323602277]
	TIME [epoch: 8.43 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2405184291602763		[learning rate: 0.00099432]
	Learning Rate: 0.00099432
	LOSS [training: 0.2405184291602763 | validation: 0.31153103745063876]
	TIME [epoch: 8.45 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26780688553548015		[learning rate: 0.00099071]
	Learning Rate: 0.000990712
	LOSS [training: 0.26780688553548015 | validation: 0.2555364942031999]
	TIME [epoch: 8.44 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24470925948707353		[learning rate: 0.00098712]
	Learning Rate: 0.000987116
	LOSS [training: 0.24470925948707353 | validation: 0.20383151563633306]
	TIME [epoch: 8.42 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26286421453193554		[learning rate: 0.00098353]
	Learning Rate: 0.000983534
	LOSS [training: 0.26286421453193554 | validation: 0.21866373925889523]
	TIME [epoch: 8.43 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29082270696436335		[learning rate: 0.00097996]
	Learning Rate: 0.000979965
	LOSS [training: 0.29082270696436335 | validation: 0.20948014933658288]
	TIME [epoch: 8.45 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24079735210063974		[learning rate: 0.00097641]
	Learning Rate: 0.000976409
	LOSS [training: 0.24079735210063974 | validation: 0.21263389871102711]
	TIME [epoch: 8.42 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26804080921090995		[learning rate: 0.00097287]
	Learning Rate: 0.000972865
	LOSS [training: 0.26804080921090995 | validation: 0.256089284361611]
	TIME [epoch: 8.42 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2437408978030927		[learning rate: 0.00096933]
	Learning Rate: 0.000969335
	LOSS [training: 0.2437408978030927 | validation: 0.21221892451162444]
	TIME [epoch: 8.42 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26262883762998795		[learning rate: 0.00096582]
	Learning Rate: 0.000965817
	LOSS [training: 0.26262883762998795 | validation: 0.18057065307599907]
	TIME [epoch: 8.44 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23951017474391767		[learning rate: 0.00096231]
	Learning Rate: 0.000962312
	LOSS [training: 0.23951017474391767 | validation: 0.193632802676941]
	TIME [epoch: 8.43 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25626375122977263		[learning rate: 0.00095882]
	Learning Rate: 0.00095882
	LOSS [training: 0.25626375122977263 | validation: 0.21750503662971749]
	TIME [epoch: 8.42 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2582200745045681		[learning rate: 0.00095534]
	Learning Rate: 0.00095534
	LOSS [training: 0.2582200745045681 | validation: 0.29724576026612126]
	TIME [epoch: 8.42 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26604663103924714		[learning rate: 0.00095187]
	Learning Rate: 0.000951873
	LOSS [training: 0.26604663103924714 | validation: 0.16581007171808404]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24747067855898314		[learning rate: 0.00094842]
	Learning Rate: 0.000948419
	LOSS [training: 0.24747067855898314 | validation: 0.3059610333448083]
	TIME [epoch: 8.41 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2677663933470361		[learning rate: 0.00094498]
	Learning Rate: 0.000944977
	LOSS [training: 0.2677663933470361 | validation: 0.19342542845823116]
	TIME [epoch: 8.42 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24555499175760934		[learning rate: 0.00094155]
	Learning Rate: 0.000941547
	LOSS [training: 0.24555499175760934 | validation: 0.20514336147097267]
	TIME [epoch: 8.42 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30149019303113306		[learning rate: 0.00093813]
	Learning Rate: 0.00093813
	LOSS [training: 0.30149019303113306 | validation: 0.217400699833683]
	TIME [epoch: 8.44 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2621590876757206		[learning rate: 0.00093473]
	Learning Rate: 0.000934726
	LOSS [training: 0.2621590876757206 | validation: 0.1619675383514797]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2618161803442059		[learning rate: 0.00093133]
	Learning Rate: 0.000931334
	LOSS [training: 0.2618161803442059 | validation: 0.19503854848753588]
	TIME [epoch: 8.42 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2952702892636767		[learning rate: 0.00092795]
	Learning Rate: 0.000927954
	LOSS [training: 0.2952702892636767 | validation: 0.23278547871329974]
	TIME [epoch: 8.42 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25645179120016054		[learning rate: 0.00092459]
	Learning Rate: 0.000924586
	LOSS [training: 0.25645179120016054 | validation: 0.2391420454538166]
	TIME [epoch: 8.43 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2452254142826283		[learning rate: 0.00092123]
	Learning Rate: 0.000921231
	LOSS [training: 0.2452254142826283 | validation: 0.21198154043513337]
	TIME [epoch: 8.43 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27793336429959237		[learning rate: 0.00091789]
	Learning Rate: 0.000917888
	LOSS [training: 0.27793336429959237 | validation: 0.18929004325325569]
	TIME [epoch: 8.41 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2267389174144998		[learning rate: 0.00091456]
	Learning Rate: 0.000914556
	LOSS [training: 0.2267389174144998 | validation: 0.1872497529923703]
	TIME [epoch: 8.41 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25648249750406793		[learning rate: 0.00091124]
	Learning Rate: 0.000911237
	LOSS [training: 0.25648249750406793 | validation: 0.17253802100252066]
	TIME [epoch: 8.42 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2320923919016767		[learning rate: 0.00090793]
	Learning Rate: 0.000907931
	LOSS [training: 0.2320923919016767 | validation: 0.2761717948107114]
	TIME [epoch: 8.45 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29603469756042167		[learning rate: 0.00090464]
	Learning Rate: 0.000904636
	LOSS [training: 0.29603469756042167 | validation: 0.2959677389051885]
	TIME [epoch: 8.43 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23995851881566627		[learning rate: 0.00090135]
	Learning Rate: 0.000901353
	LOSS [training: 0.23995851881566627 | validation: 0.1707999021014921]
	TIME [epoch: 8.42 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2684969728883796		[learning rate: 0.00089808]
	Learning Rate: 0.000898082
	LOSS [training: 0.2684969728883796 | validation: 0.378031537553219]
	TIME [epoch: 8.42 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2998070858238926		[learning rate: 0.00089482]
	Learning Rate: 0.000894822
	LOSS [training: 0.2998070858238926 | validation: 0.2851803036093038]
	TIME [epoch: 8.44 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27898715460610035		[learning rate: 0.00089157]
	Learning Rate: 0.000891575
	LOSS [training: 0.27898715460610035 | validation: 0.2490525087322633]
	TIME [epoch: 8.42 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2439843397712973		[learning rate: 0.00088834]
	Learning Rate: 0.000888339
	LOSS [training: 0.2439843397712973 | validation: 0.29655333138350853]
	TIME [epoch: 8.41 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24088337985716896		[learning rate: 0.00088512]
	Learning Rate: 0.000885116
	LOSS [training: 0.24088337985716896 | validation: 0.16279632997934934]
	TIME [epoch: 8.42 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2809190007980709		[learning rate: 0.0008819]
	Learning Rate: 0.000881903
	LOSS [training: 0.2809190007980709 | validation: 0.2848402101564724]
	TIME [epoch: 8.45 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2568199992810259		[learning rate: 0.0008787]
	Learning Rate: 0.000878703
	LOSS [training: 0.2568199992810259 | validation: 0.2004094081267703]
	TIME [epoch: 8.42 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25488960088046886		[learning rate: 0.00087551]
	Learning Rate: 0.000875514
	LOSS [training: 0.25488960088046886 | validation: 0.1788437489631629]
	TIME [epoch: 8.42 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25677965655980617		[learning rate: 0.00087234]
	Learning Rate: 0.000872337
	LOSS [training: 0.25677965655980617 | validation: 0.18530595272733158]
	TIME [epoch: 8.44 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25387233688220545		[learning rate: 0.00086917]
	Learning Rate: 0.000869171
	LOSS [training: 0.25387233688220545 | validation: 0.2357814730883631]
	TIME [epoch: 8.43 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23701404236446622		[learning rate: 0.00086602]
	Learning Rate: 0.000866017
	LOSS [training: 0.23701404236446622 | validation: 0.17852486917602134]
	TIME [epoch: 8.42 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25458462277155475		[learning rate: 0.00086287]
	Learning Rate: 0.000862874
	LOSS [training: 0.25458462277155475 | validation: 0.17733143607372245]
	TIME [epoch: 8.42 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26217132934764054		[learning rate: 0.00085974]
	Learning Rate: 0.000859743
	LOSS [training: 0.26217132934764054 | validation: 0.1916469875640266]
	TIME [epoch: 8.43 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2589081823938587		[learning rate: 0.00085662]
	Learning Rate: 0.000856623
	LOSS [training: 0.2589081823938587 | validation: 0.20689152167850733]
	TIME [epoch: 8.43 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2989297515947544		[learning rate: 0.00085351]
	Learning Rate: 0.000853514
	LOSS [training: 0.2989297515947544 | validation: 0.1775819091027795]
	TIME [epoch: 8.41 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2604353475131292		[learning rate: 0.00085042]
	Learning Rate: 0.000850416
	LOSS [training: 0.2604353475131292 | validation: 0.17553777901305345]
	TIME [epoch: 8.42 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24227806476863462		[learning rate: 0.00084733]
	Learning Rate: 0.00084733
	LOSS [training: 0.24227806476863462 | validation: 0.18591320344674456]
	TIME [epoch: 8.44 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26688439421720445		[learning rate: 0.00084426]
	Learning Rate: 0.000844255
	LOSS [training: 0.26688439421720445 | validation: 0.18305922027798482]
	TIME [epoch: 8.42 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2840502106192927		[learning rate: 0.00084119]
	Learning Rate: 0.000841191
	LOSS [training: 0.2840502106192927 | validation: 0.17563073975423954]
	TIME [epoch: 8.42 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2715500526465878		[learning rate: 0.00083814]
	Learning Rate: 0.000838139
	LOSS [training: 0.2715500526465878 | validation: 0.20645249530378762]
	TIME [epoch: 8.42 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23164808813876042		[learning rate: 0.0008351]
	Learning Rate: 0.000835097
	LOSS [training: 0.23164808813876042 | validation: 0.1870060445242716]
	TIME [epoch: 8.44 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25034087301808955		[learning rate: 0.00083207]
	Learning Rate: 0.000832066
	LOSS [training: 0.25034087301808955 | validation: 0.23723663662295524]
	TIME [epoch: 8.43 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24117492520437822		[learning rate: 0.00082905]
	Learning Rate: 0.000829046
	LOSS [training: 0.24117492520437822 | validation: 0.18746632771842137]
	TIME [epoch: 8.42 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23821887721012797		[learning rate: 0.00082604]
	Learning Rate: 0.000826038
	LOSS [training: 0.23821887721012797 | validation: 0.18506459170913178]
	TIME [epoch: 8.43 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2648334374360193		[learning rate: 0.00082304]
	Learning Rate: 0.00082304
	LOSS [training: 0.2648334374360193 | validation: 0.28894547373146684]
	TIME [epoch: 8.44 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2593393832274016		[learning rate: 0.00082005]
	Learning Rate: 0.000820053
	LOSS [training: 0.2593393832274016 | validation: 0.3079440088401837]
	TIME [epoch: 8.42 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24534412062912417		[learning rate: 0.00081708]
	Learning Rate: 0.000817077
	LOSS [training: 0.24534412062912417 | validation: 0.18561544520151446]
	TIME [epoch: 8.42 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2766687992489721		[learning rate: 0.00081411]
	Learning Rate: 0.000814112
	LOSS [training: 0.2766687992489721 | validation: 0.22127819761914488]
	TIME [epoch: 8.42 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24091636606946829		[learning rate: 0.00081116]
	Learning Rate: 0.000811158
	LOSS [training: 0.24091636606946829 | validation: 0.17110331018671646]
	TIME [epoch: 8.44 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23014819938858916		[learning rate: 0.00080821]
	Learning Rate: 0.000808214
	LOSS [training: 0.23014819938858916 | validation: 0.20865509803040774]
	TIME [epoch: 8.42 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24359648319447938		[learning rate: 0.00080528]
	Learning Rate: 0.000805281
	LOSS [training: 0.24359648319447938 | validation: 0.18414316503596848]
	TIME [epoch: 8.42 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2663540728820808		[learning rate: 0.00080236]
	Learning Rate: 0.000802358
	LOSS [training: 0.2663540728820808 | validation: 0.21423545308167513]
	TIME [epoch: 8.42 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24789004970136355		[learning rate: 0.00079945]
	Learning Rate: 0.000799447
	LOSS [training: 0.24789004970136355 | validation: 0.2205819668631871]
	TIME [epoch: 8.45 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26015240752122315		[learning rate: 0.00079655]
	Learning Rate: 0.000796545
	LOSS [training: 0.26015240752122315 | validation: 0.16474044083175388]
	TIME [epoch: 8.43 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26085800160524875		[learning rate: 0.00079365]
	Learning Rate: 0.000793655
	LOSS [training: 0.26085800160524875 | validation: 0.22854311530796295]
	TIME [epoch: 8.42 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2671834137138631		[learning rate: 0.00079077]
	Learning Rate: 0.000790775
	LOSS [training: 0.2671834137138631 | validation: 0.23543820667915494]
	TIME [epoch: 8.43 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26564545547034124		[learning rate: 0.0007879]
	Learning Rate: 0.000787905
	LOSS [training: 0.26564545547034124 | validation: 0.17243929955487355]
	TIME [epoch: 8.44 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2854269989816055		[learning rate: 0.00078505]
	Learning Rate: 0.000785045
	LOSS [training: 0.2854269989816055 | validation: 0.23591356028408814]
	TIME [epoch: 8.43 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2796614014960004		[learning rate: 0.0007822]
	Learning Rate: 0.000782196
	LOSS [training: 0.2796614014960004 | validation: 0.20285696306173417]
	TIME [epoch: 8.42 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27549051321333945		[learning rate: 0.00077936]
	Learning Rate: 0.000779358
	LOSS [training: 0.27549051321333945 | validation: 0.17961188866280614]
	TIME [epoch: 8.42 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23158561657076868		[learning rate: 0.00077653]
	Learning Rate: 0.000776529
	LOSS [training: 0.23158561657076868 | validation: 0.17345330511332102]
	TIME [epoch: 8.44 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26358377372369235		[learning rate: 0.00077371]
	Learning Rate: 0.000773711
	LOSS [training: 0.26358377372369235 | validation: 0.34548018158502125]
	TIME [epoch: 8.42 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23570923968236696		[learning rate: 0.0007709]
	Learning Rate: 0.000770903
	LOSS [training: 0.23570923968236696 | validation: 0.16891410036397903]
	TIME [epoch: 8.43 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26674712100459025		[learning rate: 0.00076811]
	Learning Rate: 0.000768106
	LOSS [training: 0.26674712100459025 | validation: 0.1776769895122192]
	TIME [epoch: 8.42 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.275913053411308		[learning rate: 0.00076532]
	Learning Rate: 0.000765318
	LOSS [training: 0.275913053411308 | validation: 0.30423874792837147]
	TIME [epoch: 8.45 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24895146452010483		[learning rate: 0.00076254]
	Learning Rate: 0.000762541
	LOSS [training: 0.24895146452010483 | validation: 0.17685601244578275]
	TIME [epoch: 8.43 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2631056704313483		[learning rate: 0.00075977]
	Learning Rate: 0.000759774
	LOSS [training: 0.2631056704313483 | validation: 0.21116456274148782]
	TIME [epoch: 8.44 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2655234465579527		[learning rate: 0.00075702]
	Learning Rate: 0.000757016
	LOSS [training: 0.2655234465579527 | validation: 0.18412142081595023]
	TIME [epoch: 8.42 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25125772387721246		[learning rate: 0.00075427]
	Learning Rate: 0.000754269
	LOSS [training: 0.25125772387721246 | validation: 0.16653463883532427]
	TIME [epoch: 8.45 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2675614434150679		[learning rate: 0.00075153]
	Learning Rate: 0.000751532
	LOSS [training: 0.2675614434150679 | validation: 0.1595459049333515]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_812.pth
	Model improved!!!
EPOCH 813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2602935571179937		[learning rate: 0.0007488]
	Learning Rate: 0.000748804
	LOSS [training: 0.2602935571179937 | validation: 0.1720141857855072]
	TIME [epoch: 8.44 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22313988966108256		[learning rate: 0.00074609]
	Learning Rate: 0.000746087
	LOSS [training: 0.22313988966108256 | validation: 0.21023380584250112]
	TIME [epoch: 8.43 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2779848409747955		[learning rate: 0.00074338]
	Learning Rate: 0.000743379
	LOSS [training: 0.2779848409747955 | validation: 0.1881261935796794]
	TIME [epoch: 8.46 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2574262634526071		[learning rate: 0.00074068]
	Learning Rate: 0.000740682
	LOSS [training: 0.2574262634526071 | validation: 0.18565267767142934]
	TIME [epoch: 8.42 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23183717148597535		[learning rate: 0.00073799]
	Learning Rate: 0.000737994
	LOSS [training: 0.23183717148597535 | validation: 0.19051239748437723]
	TIME [epoch: 8.42 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21812005174541996		[learning rate: 0.00073532]
	Learning Rate: 0.000735315
	LOSS [training: 0.21812005174541996 | validation: 0.16602996682797638]
	TIME [epoch: 8.43 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23343212928661283		[learning rate: 0.00073265]
	Learning Rate: 0.000732647
	LOSS [training: 0.23343212928661283 | validation: 0.2197532297287848]
	TIME [epoch: 8.45 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.222753977296798		[learning rate: 0.00072999]
	Learning Rate: 0.000729988
	LOSS [training: 0.222753977296798 | validation: 0.17249284113779154]
	TIME [epoch: 8.42 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2302520398294678		[learning rate: 0.00072734]
	Learning Rate: 0.000727339
	LOSS [training: 0.2302520398294678 | validation: 0.19071809927749606]
	TIME [epoch: 8.43 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2463382305229101		[learning rate: 0.0007247]
	Learning Rate: 0.000724699
	LOSS [training: 0.2463382305229101 | validation: 0.21759670292410907]
	TIME [epoch: 8.42 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25283722240967776		[learning rate: 0.00072207]
	Learning Rate: 0.000722069
	LOSS [training: 0.25283722240967776 | validation: 0.17752681936994896]
	TIME [epoch: 8.45 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2368421412392069		[learning rate: 0.00071945]
	Learning Rate: 0.000719449
	LOSS [training: 0.2368421412392069 | validation: 0.22325787072578732]
	TIME [epoch: 8.42 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26896649795165983		[learning rate: 0.00071684]
	Learning Rate: 0.000716838
	LOSS [training: 0.26896649795165983 | validation: 0.1957239574776036]
	TIME [epoch: 8.42 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2499751015302289		[learning rate: 0.00071424]
	Learning Rate: 0.000714237
	LOSS [training: 0.2499751015302289 | validation: 0.20678935395282222]
	TIME [epoch: 8.42 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22329267343725978		[learning rate: 0.00071164]
	Learning Rate: 0.000711645
	LOSS [training: 0.22329267343725978 | validation: 0.19071298679455012]
	TIME [epoch: 8.45 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2403852504666431		[learning rate: 0.00070906]
	Learning Rate: 0.000709062
	LOSS [training: 0.2403852504666431 | validation: 0.16960286042527617]
	TIME [epoch: 8.42 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22136493606659627		[learning rate: 0.00070649]
	Learning Rate: 0.000706489
	LOSS [training: 0.22136493606659627 | validation: 0.1784832579184461]
	TIME [epoch: 8.42 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23055748577168028		[learning rate: 0.00070392]
	Learning Rate: 0.000703925
	LOSS [training: 0.23055748577168028 | validation: 0.17591247630991988]
	TIME [epoch: 8.43 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25538142964884986		[learning rate: 0.00070137]
	Learning Rate: 0.00070137
	LOSS [training: 0.25538142964884986 | validation: 0.21497161895490152]
	TIME [epoch: 8.45 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23965785834486392		[learning rate: 0.00069883]
	Learning Rate: 0.000698825
	LOSS [training: 0.23965785834486392 | validation: 0.16853343298019158]
	TIME [epoch: 8.43 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25613140962757897		[learning rate: 0.00069629]
	Learning Rate: 0.000696289
	LOSS [training: 0.25613140962757897 | validation: 0.18824574552300513]
	TIME [epoch: 8.42 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2296328666377882		[learning rate: 0.00069376]
	Learning Rate: 0.000693762
	LOSS [training: 0.2296328666377882 | validation: 0.1429871832620063]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2119416102678863		[learning rate: 0.00069124]
	Learning Rate: 0.000691244
	LOSS [training: 0.2119416102678863 | validation: 0.15731108580471767]
	TIME [epoch: 8.44 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23540777936845242		[learning rate: 0.00068874]
	Learning Rate: 0.000688736
	LOSS [training: 0.23540777936845242 | validation: 0.1953178536959043]
	TIME [epoch: 8.43 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26087920475538046		[learning rate: 0.00068624]
	Learning Rate: 0.000686236
	LOSS [training: 0.26087920475538046 | validation: 0.18222368863022959]
	TIME [epoch: 8.43 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2105020166705885		[learning rate: 0.00068375]
	Learning Rate: 0.000683746
	LOSS [training: 0.2105020166705885 | validation: 0.15896235470742312]
	TIME [epoch: 8.43 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22037106803737597		[learning rate: 0.00068126]
	Learning Rate: 0.000681265
	LOSS [training: 0.22037106803737597 | validation: 0.18188634399061665]
	TIME [epoch: 8.45 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2303038764507876		[learning rate: 0.00067879]
	Learning Rate: 0.000678792
	LOSS [training: 0.2303038764507876 | validation: 0.17441752061886812]
	TIME [epoch: 8.43 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23684600257277405		[learning rate: 0.00067633]
	Learning Rate: 0.000676329
	LOSS [training: 0.23684600257277405 | validation: 0.17444436613595435]
	TIME [epoch: 8.43 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2586470645494994		[learning rate: 0.00067387]
	Learning Rate: 0.000673874
	LOSS [training: 0.2586470645494994 | validation: 0.24031935566460472]
	TIME [epoch: 8.44 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23090540921038344		[learning rate: 0.00067143]
	Learning Rate: 0.000671429
	LOSS [training: 0.23090540921038344 | validation: 0.2639410198925597]
	TIME [epoch: 8.45 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23633459320429978		[learning rate: 0.00066899]
	Learning Rate: 0.000668992
	LOSS [training: 0.23633459320429978 | validation: 0.19987567845813636]
	TIME [epoch: 8.44 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2334666274190927		[learning rate: 0.00066656]
	Learning Rate: 0.000666564
	LOSS [training: 0.2334666274190927 | validation: 0.2114107546379422]
	TIME [epoch: 8.43 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23652204712445596		[learning rate: 0.00066415]
	Learning Rate: 0.000664145
	LOSS [training: 0.23652204712445596 | validation: 0.17874230219135906]
	TIME [epoch: 8.43 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21858792624855633		[learning rate: 0.00066174]
	Learning Rate: 0.000661735
	LOSS [training: 0.21858792624855633 | validation: 0.1757553042325759]
	TIME [epoch: 8.46 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23895012760723708		[learning rate: 0.00065933]
	Learning Rate: 0.000659334
	LOSS [training: 0.23895012760723708 | validation: 0.1574205565728248]
	TIME [epoch: 8.44 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2214574923561637		[learning rate: 0.00065694]
	Learning Rate: 0.000656941
	LOSS [training: 0.2214574923561637 | validation: 0.2203602490970023]
	TIME [epoch: 8.44 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21259625648958344		[learning rate: 0.00065456]
	Learning Rate: 0.000654557
	LOSS [training: 0.21259625648958344 | validation: 0.2751349742319774]
	TIME [epoch: 8.43 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23479994466422957		[learning rate: 0.00065218]
	Learning Rate: 0.000652182
	LOSS [training: 0.23479994466422957 | validation: 0.1883842630788243]
	TIME [epoch: 8.45 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23662708260971885		[learning rate: 0.00064981]
	Learning Rate: 0.000649815
	LOSS [training: 0.23662708260971885 | validation: 0.14505763714921696]
	TIME [epoch: 8.43 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2538715718074342		[learning rate: 0.00064746]
	Learning Rate: 0.000647456
	LOSS [training: 0.2538715718074342 | validation: 0.1630903877226922]
	TIME [epoch: 8.44 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23198508946779114		[learning rate: 0.00064511]
	Learning Rate: 0.000645107
	LOSS [training: 0.23198508946779114 | validation: 0.182303418897318]
	TIME [epoch: 8.43 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22532698135260332		[learning rate: 0.00064277]
	Learning Rate: 0.000642766
	LOSS [training: 0.22532698135260332 | validation: 0.1751473107493993]
	TIME [epoch: 8.46 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20908534211957827		[learning rate: 0.00064043]
	Learning Rate: 0.000640433
	LOSS [training: 0.20908534211957827 | validation: 0.1557575137703716]
	TIME [epoch: 8.44 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21890508579001106		[learning rate: 0.00063811]
	Learning Rate: 0.000638109
	LOSS [training: 0.21890508579001106 | validation: 0.15427887075051855]
	TIME [epoch: 8.42 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21397055563532388		[learning rate: 0.00063579]
	Learning Rate: 0.000635793
	LOSS [training: 0.21397055563532388 | validation: 0.18465932685531491]
	TIME [epoch: 8.43 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21313042139693777		[learning rate: 0.00063349]
	Learning Rate: 0.000633486
	LOSS [training: 0.21313042139693777 | validation: 0.1576307389843932]
	TIME [epoch: 8.44 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21156406964941757		[learning rate: 0.00063119]
	Learning Rate: 0.000631187
	LOSS [training: 0.21156406964941757 | validation: 0.17333528504921308]
	TIME [epoch: 8.43 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2703027455271552		[learning rate: 0.0006289]
	Learning Rate: 0.000628896
	LOSS [training: 0.2703027455271552 | validation: 0.15079445522761695]
	TIME [epoch: 8.43 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20777241445354694		[learning rate: 0.00062661]
	Learning Rate: 0.000626614
	LOSS [training: 0.20777241445354694 | validation: 0.17927253050194664]
	TIME [epoch: 8.43 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2189206297178826		[learning rate: 0.00062434]
	Learning Rate: 0.00062434
	LOSS [training: 0.2189206297178826 | validation: 0.15185428535179957]
	TIME [epoch: 8.45 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21071860665523096		[learning rate: 0.00062207]
	Learning Rate: 0.000622074
	LOSS [training: 0.21071860665523096 | validation: 0.1755063484950935]
	TIME [epoch: 8.43 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22737315943795872		[learning rate: 0.00061982]
	Learning Rate: 0.000619816
	LOSS [training: 0.22737315943795872 | validation: 0.20690144675209177]
	TIME [epoch: 8.43 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23698208390052744		[learning rate: 0.00061757]
	Learning Rate: 0.000617567
	LOSS [training: 0.23698208390052744 | validation: 0.15689781074606501]
	TIME [epoch: 8.43 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2244010268522399		[learning rate: 0.00061533]
	Learning Rate: 0.000615326
	LOSS [training: 0.2244010268522399 | validation: 0.14849084650213895]
	TIME [epoch: 8.45 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21448268288929725		[learning rate: 0.00061309]
	Learning Rate: 0.000613093
	LOSS [training: 0.21448268288929725 | validation: 0.32864127174369506]
	TIME [epoch: 8.42 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23568635899972484		[learning rate: 0.00061087]
	Learning Rate: 0.000610868
	LOSS [training: 0.23568635899972484 | validation: 0.15070039322166418]
	TIME [epoch: 8.43 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20012809106875967		[learning rate: 0.00060865]
	Learning Rate: 0.000608651
	LOSS [training: 0.20012809106875967 | validation: 0.17279529713093433]
	TIME [epoch: 8.43 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24356637974215717		[learning rate: 0.00060644]
	Learning Rate: 0.000606442
	LOSS [training: 0.24356637974215717 | validation: 0.1695876608763505]
	TIME [epoch: 8.46 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2170616198087662		[learning rate: 0.00060424]
	Learning Rate: 0.000604242
	LOSS [training: 0.2170616198087662 | validation: 0.18138488592819368]
	TIME [epoch: 8.43 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21781900791957737		[learning rate: 0.00060205]
	Learning Rate: 0.000602049
	LOSS [training: 0.21781900791957737 | validation: 0.1400963906047512]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_873.pth
	Model improved!!!
EPOCH 874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21685935778101323		[learning rate: 0.00059986]
	Learning Rate: 0.000599864
	LOSS [training: 0.21685935778101323 | validation: 0.17199668297834597]
	TIME [epoch: 8.44 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22666869467098355		[learning rate: 0.00059769]
	Learning Rate: 0.000597687
	LOSS [training: 0.22666869467098355 | validation: 0.16683749371376486]
	TIME [epoch: 8.45 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2006813880272827		[learning rate: 0.00059552]
	Learning Rate: 0.000595518
	LOSS [training: 0.2006813880272827 | validation: 0.22101688600319205]
	TIME [epoch: 8.44 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2377547364337481		[learning rate: 0.00059336]
	Learning Rate: 0.000593357
	LOSS [training: 0.2377547364337481 | validation: 0.2496102593508997]
	TIME [epoch: 8.43 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24298374460518374		[learning rate: 0.0005912]
	Learning Rate: 0.000591203
	LOSS [training: 0.24298374460518374 | validation: 0.15353267956418493]
	TIME [epoch: 8.43 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22508796656482066		[learning rate: 0.00058906]
	Learning Rate: 0.000589058
	LOSS [training: 0.22508796656482066 | validation: 0.20228678444860626]
	TIME [epoch: 8.45 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22627894705735274		[learning rate: 0.00058692]
	Learning Rate: 0.00058692
	LOSS [training: 0.22627894705735274 | validation: 0.1505203610074146]
	TIME [epoch: 8.44 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20000266778403542		[learning rate: 0.00058479]
	Learning Rate: 0.00058479
	LOSS [training: 0.20000266778403542 | validation: 0.18609501987960195]
	TIME [epoch: 8.43 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20671372154243234		[learning rate: 0.00058267]
	Learning Rate: 0.000582668
	LOSS [training: 0.20671372154243234 | validation: 0.1615539258937392]
	TIME [epoch: 8.42 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22871410753776705		[learning rate: 0.00058055]
	Learning Rate: 0.000580553
	LOSS [training: 0.22871410753776705 | validation: 0.17046269486130766]
	TIME [epoch: 8.44 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21441895820923512		[learning rate: 0.00057845]
	Learning Rate: 0.000578446
	LOSS [training: 0.21441895820923512 | validation: 0.18327556680830054]
	TIME [epoch: 8.43 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20966359155008182		[learning rate: 0.00057635]
	Learning Rate: 0.000576347
	LOSS [training: 0.20966359155008182 | validation: 0.31562591834137943]
	TIME [epoch: 8.43 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24640852390370646		[learning rate: 0.00057426]
	Learning Rate: 0.000574256
	LOSS [training: 0.24640852390370646 | validation: 0.20681412817585915]
	TIME [epoch: 8.43 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21442752469863374		[learning rate: 0.00057217]
	Learning Rate: 0.000572172
	LOSS [training: 0.21442752469863374 | validation: 0.16691547475218566]
	TIME [epoch: 8.45 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2188086466017193		[learning rate: 0.0005701]
	Learning Rate: 0.000570095
	LOSS [training: 0.2188086466017193 | validation: 0.16594540460896984]
	TIME [epoch: 8.43 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23092275021161757		[learning rate: 0.00056803]
	Learning Rate: 0.000568026
	LOSS [training: 0.23092275021161757 | validation: 0.14390203140259777]
	TIME [epoch: 8.42 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22046016527742235		[learning rate: 0.00056596]
	Learning Rate: 0.000565965
	LOSS [training: 0.22046016527742235 | validation: 0.16944675126614173]
	TIME [epoch: 8.44 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2233494968873339		[learning rate: 0.00056391]
	Learning Rate: 0.000563911
	LOSS [training: 0.2233494968873339 | validation: 0.15528639735519092]
	TIME [epoch: 8.45 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20629031661105152		[learning rate: 0.00056186]
	Learning Rate: 0.000561864
	LOSS [training: 0.20629031661105152 | validation: 0.18774943481907574]
	TIME [epoch: 8.43 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21098213027149684		[learning rate: 0.00055983]
	Learning Rate: 0.000559825
	LOSS [training: 0.21098213027149684 | validation: 0.14506447339093487]
	TIME [epoch: 8.43 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22061942708910917		[learning rate: 0.00055779]
	Learning Rate: 0.000557794
	LOSS [training: 0.22061942708910917 | validation: 0.18430966705618895]
	TIME [epoch: 8.43 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2239830467541779		[learning rate: 0.00055577]
	Learning Rate: 0.00055577
	LOSS [training: 0.2239830467541779 | validation: 0.18209824372512054]
	TIME [epoch: 8.46 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2103857621573065		[learning rate: 0.00055375]
	Learning Rate: 0.000553753
	LOSS [training: 0.2103857621573065 | validation: 0.14840180807646103]
	TIME [epoch: 8.42 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2363393266403661		[learning rate: 0.00055174]
	Learning Rate: 0.000551743
	LOSS [training: 0.2363393266403661 | validation: 0.17471054830626265]
	TIME [epoch: 8.43 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2075087735463011		[learning rate: 0.00054974]
	Learning Rate: 0.000549741
	LOSS [training: 0.2075087735463011 | validation: 0.20341155358842908]
	TIME [epoch: 8.44 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20361465601661935		[learning rate: 0.00054775]
	Learning Rate: 0.000547746
	LOSS [training: 0.20361465601661935 | validation: 0.1668201354460109]
	TIME [epoch: 8.44 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20763206880232915		[learning rate: 0.00054576]
	Learning Rate: 0.000545758
	LOSS [training: 0.20763206880232915 | validation: 0.1666910702003169]
	TIME [epoch: 8.43 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22200584273546548		[learning rate: 0.00054378]
	Learning Rate: 0.000543777
	LOSS [training: 0.22200584273546548 | validation: 0.1574234136881376]
	TIME [epoch: 8.43 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2090182595045243		[learning rate: 0.0005418]
	Learning Rate: 0.000541804
	LOSS [training: 0.2090182595045243 | validation: 0.17729553929821618]
	TIME [epoch: 8.46 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21339063765307342		[learning rate: 0.00053984]
	Learning Rate: 0.000539838
	LOSS [training: 0.21339063765307342 | validation: 0.2029606626943271]
	TIME [epoch: 8.45 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25022205288290744		[learning rate: 0.00053788]
	Learning Rate: 0.000537879
	LOSS [training: 0.25022205288290744 | validation: 0.19605726065520357]
	TIME [epoch: 8.45 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20946830210730286		[learning rate: 0.00053593]
	Learning Rate: 0.000535926
	LOSS [training: 0.20946830210730286 | validation: 0.15361919573118538]
	TIME [epoch: 8.44 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2083864161135145		[learning rate: 0.00053398]
	Learning Rate: 0.000533982
	LOSS [training: 0.2083864161135145 | validation: 0.214866454000546]
	TIME [epoch: 8.43 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2149454800229893		[learning rate: 0.00053204]
	Learning Rate: 0.000532044
	LOSS [training: 0.2149454800229893 | validation: 0.16488246754264552]
	TIME [epoch: 8.46 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23095345079846147		[learning rate: 0.00053011]
	Learning Rate: 0.000530113
	LOSS [training: 0.23095345079846147 | validation: 0.22126526116767636]
	TIME [epoch: 8.43 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.212442930768999		[learning rate: 0.00052819]
	Learning Rate: 0.000528189
	LOSS [training: 0.212442930768999 | validation: 0.16285944941556185]
	TIME [epoch: 8.44 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2265598696035495		[learning rate: 0.00052627]
	Learning Rate: 0.000526272
	LOSS [training: 0.2265598696035495 | validation: 0.14433952052403068]
	TIME [epoch: 8.44 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20949108766550525		[learning rate: 0.00052436]
	Learning Rate: 0.000524362
	LOSS [training: 0.20949108766550525 | validation: 0.16692350424999325]
	TIME [epoch: 8.47 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20010328202263672		[learning rate: 0.00052246]
	Learning Rate: 0.00052246
	LOSS [training: 0.20010328202263672 | validation: 0.15998221009376973]
	TIME [epoch: 8.44 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1927258027939204		[learning rate: 0.00052056]
	Learning Rate: 0.000520563
	LOSS [training: 0.1927258027939204 | validation: 0.1931369551537399]
	TIME [epoch: 8.44 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2097563832750557		[learning rate: 0.00051867]
	Learning Rate: 0.000518674
	LOSS [training: 0.2097563832750557 | validation: 0.21657394469648922]
	TIME [epoch: 8.44 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21539876296222382		[learning rate: 0.00051679]
	Learning Rate: 0.000516792
	LOSS [training: 0.21539876296222382 | validation: 0.17165472860099346]
	TIME [epoch: 8.46 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2152215841443153		[learning rate: 0.00051492]
	Learning Rate: 0.000514917
	LOSS [training: 0.2152215841443153 | validation: 0.1709973751712909]
	TIME [epoch: 8.44 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22170471698541844		[learning rate: 0.00051305]
	Learning Rate: 0.000513048
	LOSS [training: 0.22170471698541844 | validation: 0.13898126263370614]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_917.pth
	Model improved!!!
EPOCH 918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20635152704800927		[learning rate: 0.00051119]
	Learning Rate: 0.000511186
	LOSS [training: 0.20635152704800927 | validation: 0.13766296058633476]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_918.pth
	Model improved!!!
EPOCH 919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21828624686084375		[learning rate: 0.00050933]
	Learning Rate: 0.000509331
	LOSS [training: 0.21828624686084375 | validation: 0.16218006544093766]
	TIME [epoch: 8.44 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2127183522942256		[learning rate: 0.00050748]
	Learning Rate: 0.000507483
	LOSS [training: 0.2127183522942256 | validation: 0.21059123805274588]
	TIME [epoch: 8.44 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21688355677620827		[learning rate: 0.00050564]
	Learning Rate: 0.000505641
	LOSS [training: 0.21688355677620827 | validation: 0.1714757551829521]
	TIME [epoch: 8.43 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20334581995121578		[learning rate: 0.00050381]
	Learning Rate: 0.000503806
	LOSS [training: 0.20334581995121578 | validation: 0.1432805612971581]
	TIME [epoch: 8.44 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20974997155146258		[learning rate: 0.00050198]
	Learning Rate: 0.000501977
	LOSS [training: 0.20974997155146258 | validation: 0.1283866073192697]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_923.pth
	Model improved!!!
EPOCH 924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20721449030970146		[learning rate: 0.00050016]
	Learning Rate: 0.000500156
	LOSS [training: 0.20721449030970146 | validation: 0.15032164342444979]
	TIME [epoch: 8.43 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2048908487536909		[learning rate: 0.00049834]
	Learning Rate: 0.000498341
	LOSS [training: 0.2048908487536909 | validation: 0.14961073435046154]
	TIME [epoch: 8.43 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22089247983308874		[learning rate: 0.00049653]
	Learning Rate: 0.000496532
	LOSS [training: 0.22089247983308874 | validation: 0.14195928238873295]
	TIME [epoch: 8.43 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28186461140068186		[learning rate: 0.00049473]
	Learning Rate: 0.00049473
	LOSS [training: 0.28186461140068186 | validation: 0.14544009537429306]
	TIME [epoch: 8.45 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21351263971814557		[learning rate: 0.00049293]
	Learning Rate: 0.000492935
	LOSS [training: 0.21351263971814557 | validation: 0.18105513125456418]
	TIME [epoch: 8.43 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21990820068236908		[learning rate: 0.00049115]
	Learning Rate: 0.000491146
	LOSS [training: 0.21990820068236908 | validation: 0.15306142513754206]
	TIME [epoch: 8.43 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20859222353355505		[learning rate: 0.00048936]
	Learning Rate: 0.000489364
	LOSS [training: 0.20859222353355505 | validation: 0.1746878326197166]
	TIME [epoch: 8.43 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21163681154713934		[learning rate: 0.00048759]
	Learning Rate: 0.000487588
	LOSS [training: 0.21163681154713934 | validation: 0.1503352060018017]
	TIME [epoch: 8.46 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20771737202904234		[learning rate: 0.00048582]
	Learning Rate: 0.000485818
	LOSS [training: 0.20771737202904234 | validation: 0.15816945013026268]
	TIME [epoch: 8.44 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19556774976327418		[learning rate: 0.00048406]
	Learning Rate: 0.000484055
	LOSS [training: 0.19556774976327418 | validation: 0.2175106075415208]
	TIME [epoch: 8.42 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21313276876033008		[learning rate: 0.0004823]
	Learning Rate: 0.000482298
	LOSS [training: 0.21313276876033008 | validation: 0.17133245330722663]
	TIME [epoch: 8.43 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20229479763963623		[learning rate: 0.00048055]
	Learning Rate: 0.000480548
	LOSS [training: 0.20229479763963623 | validation: 0.15680033711700492]
	TIME [epoch: 8.45 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2255228876923019		[learning rate: 0.0004788]
	Learning Rate: 0.000478804
	LOSS [training: 0.2255228876923019 | validation: 0.16366403775933022]
	TIME [epoch: 8.44 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19957980861350358		[learning rate: 0.00047707]
	Learning Rate: 0.000477067
	LOSS [training: 0.19957980861350358 | validation: 0.14293933911882217]
	TIME [epoch: 8.43 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2247558694760705		[learning rate: 0.00047534]
	Learning Rate: 0.000475335
	LOSS [training: 0.2247558694760705 | validation: 0.13288461938775178]
	TIME [epoch: 8.43 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20235599858705391		[learning rate: 0.00047361]
	Learning Rate: 0.00047361
	LOSS [training: 0.20235599858705391 | validation: 0.15582996669033256]
	TIME [epoch: 8.45 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21476624097240532		[learning rate: 0.00047189]
	Learning Rate: 0.000471891
	LOSS [training: 0.21476624097240532 | validation: 0.13691363199750603]
	TIME [epoch: 8.43 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2316000430503368		[learning rate: 0.00047018]
	Learning Rate: 0.000470179
	LOSS [training: 0.2316000430503368 | validation: 0.2007204544029071]
	TIME [epoch: 8.43 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2212970753404277		[learning rate: 0.00046847]
	Learning Rate: 0.000468473
	LOSS [training: 0.2212970753404277 | validation: 0.1485492947788375]
	TIME [epoch: 8.43 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1936408629126086		[learning rate: 0.00046677]
	Learning Rate: 0.000466773
	LOSS [training: 0.1936408629126086 | validation: 0.14172380767683343]
	TIME [epoch: 8.46 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20970193092876369		[learning rate: 0.00046508]
	Learning Rate: 0.000465079
	LOSS [training: 0.20970193092876369 | validation: 0.1970525428283214]
	TIME [epoch: 8.43 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2183331126098053		[learning rate: 0.00046339]
	Learning Rate: 0.000463391
	LOSS [training: 0.2183331126098053 | validation: 0.14864822527840332]
	TIME [epoch: 8.43 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20522718122757153		[learning rate: 0.00046171]
	Learning Rate: 0.000461709
	LOSS [training: 0.20522718122757153 | validation: 0.2415718254233832]
	TIME [epoch: 8.43 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21200307262760032		[learning rate: 0.00046003]
	Learning Rate: 0.000460034
	LOSS [training: 0.21200307262760032 | validation: 0.16333896677210724]
	TIME [epoch: 8.45 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20630701716008093		[learning rate: 0.00045836]
	Learning Rate: 0.000458364
	LOSS [training: 0.20630701716008093 | validation: 0.179298603001187]
	TIME [epoch: 8.43 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20334500912304204		[learning rate: 0.0004567]
	Learning Rate: 0.000456701
	LOSS [training: 0.20334500912304204 | validation: 0.19618999989053626]
	TIME [epoch: 8.44 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2183209673007953		[learning rate: 0.00045504]
	Learning Rate: 0.000455043
	LOSS [training: 0.2183209673007953 | validation: 0.1388068655893277]
	TIME [epoch: 8.42 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24827646053539146		[learning rate: 0.00045339]
	Learning Rate: 0.000453392
	LOSS [training: 0.24827646053539146 | validation: 0.15349376942269874]
	TIME [epoch: 8.45 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21437545854526982		[learning rate: 0.00045175]
	Learning Rate: 0.000451746
	LOSS [training: 0.21437545854526982 | validation: 0.1506792417248616]
	TIME [epoch: 8.42 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19678667525593657		[learning rate: 0.00045011]
	Learning Rate: 0.000450107
	LOSS [training: 0.19678667525593657 | validation: 0.16554163510015563]
	TIME [epoch: 8.43 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21978096026466526		[learning rate: 0.00044847]
	Learning Rate: 0.000448474
	LOSS [training: 0.21978096026466526 | validation: 0.14528589299617695]
	TIME [epoch: 8.43 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2058529090167068		[learning rate: 0.00044685]
	Learning Rate: 0.000446846
	LOSS [training: 0.2058529090167068 | validation: 0.15507481695937525]
	TIME [epoch: 8.45 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19246443604511654		[learning rate: 0.00044522]
	Learning Rate: 0.000445224
	LOSS [training: 0.19246443604511654 | validation: 0.14747421096562746]
	TIME [epoch: 8.43 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21304848483549005		[learning rate: 0.00044361]
	Learning Rate: 0.000443609
	LOSS [training: 0.21304848483549005 | validation: 0.19284913178963303]
	TIME [epoch: 8.42 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1970653830382094		[learning rate: 0.000442]
	Learning Rate: 0.000441999
	LOSS [training: 0.1970653830382094 | validation: 0.14448061305766716]
	TIME [epoch: 8.42 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20220643126080992		[learning rate: 0.00044039]
	Learning Rate: 0.000440395
	LOSS [training: 0.20220643126080992 | validation: 0.14176074679450026]
	TIME [epoch: 8.45 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21558418053670864		[learning rate: 0.0004388]
	Learning Rate: 0.000438797
	LOSS [training: 0.21558418053670864 | validation: 0.2395597082413412]
	TIME [epoch: 8.43 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20930980961161288		[learning rate: 0.0004372]
	Learning Rate: 0.000437204
	LOSS [training: 0.20930980961161288 | validation: 0.16685413596706905]
	TIME [epoch: 8.43 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19574421108288992		[learning rate: 0.00043562]
	Learning Rate: 0.000435617
	LOSS [training: 0.19574421108288992 | validation: 0.15222050554996028]
	TIME [epoch: 8.43 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21196138725790567		[learning rate: 0.00043404]
	Learning Rate: 0.000434037
	LOSS [training: 0.21196138725790567 | validation: 0.14344339942388018]
	TIME [epoch: 8.45 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21184297500422486		[learning rate: 0.00043246]
	Learning Rate: 0.000432461
	LOSS [training: 0.21184297500422486 | validation: 0.17530394206263983]
	TIME [epoch: 8.42 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20113990737952547		[learning rate: 0.00043089]
	Learning Rate: 0.000430892
	LOSS [training: 0.20113990737952547 | validation: 0.1557081269130248]
	TIME [epoch: 8.43 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20587354543451813		[learning rate: 0.00042933]
	Learning Rate: 0.000429328
	LOSS [training: 0.20587354543451813 | validation: 0.15649649359751222]
	TIME [epoch: 8.44 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21818304909530012		[learning rate: 0.00042777]
	Learning Rate: 0.00042777
	LOSS [training: 0.21818304909530012 | validation: 0.23552352628617662]
	TIME [epoch: 8.45 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22804069257815515		[learning rate: 0.00042622]
	Learning Rate: 0.000426218
	LOSS [training: 0.22804069257815515 | validation: 0.13952451408613126]
	TIME [epoch: 8.42 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2294772459841959		[learning rate: 0.00042467]
	Learning Rate: 0.000424671
	LOSS [training: 0.2294772459841959 | validation: 0.14769590454621492]
	TIME [epoch: 8.43 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.190571629091267		[learning rate: 0.00042313]
	Learning Rate: 0.00042313
	LOSS [training: 0.190571629091267 | validation: 0.21174530012662718]
	TIME [epoch: 8.44 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2779616863525424		[learning rate: 0.00042159]
	Learning Rate: 0.000421594
	LOSS [training: 0.2779616863525424 | validation: 0.151101242977475]
	TIME [epoch: 8.44 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20986558994189003		[learning rate: 0.00042006]
	Learning Rate: 0.000420064
	LOSS [training: 0.20986558994189003 | validation: 0.14801566782285086]
	TIME [epoch: 8.43 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22973404269047223		[learning rate: 0.00041854]
	Learning Rate: 0.00041854
	LOSS [training: 0.22973404269047223 | validation: 0.1463466142207066]
	TIME [epoch: 8.43 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19189871888155366		[learning rate: 0.00041702]
	Learning Rate: 0.000417021
	LOSS [training: 0.19189871888155366 | validation: 0.15210620696425228]
	TIME [epoch: 8.45 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21768200322469658		[learning rate: 0.00041551]
	Learning Rate: 0.000415508
	LOSS [training: 0.21768200322469658 | validation: 0.1357049088673839]
	TIME [epoch: 8.43 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19447067501304144		[learning rate: 0.000414]
	Learning Rate: 0.000414
	LOSS [training: 0.19447067501304144 | validation: 0.16964816401993849]
	TIME [epoch: 8.43 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20445459318089004		[learning rate: 0.0004125]
	Learning Rate: 0.000412497
	LOSS [training: 0.20445459318089004 | validation: 0.19116770346345965]
	TIME [epoch: 8.43 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2191142641799388		[learning rate: 0.000411]
	Learning Rate: 0.000411
	LOSS [training: 0.2191142641799388 | validation: 0.14005579016164776]
	TIME [epoch: 8.45 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19718788620050381		[learning rate: 0.00040951]
	Learning Rate: 0.000409509
	LOSS [training: 0.19718788620050381 | validation: 0.15253159441078526]
	TIME [epoch: 8.44 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18972110672961173		[learning rate: 0.00040802]
	Learning Rate: 0.000408023
	LOSS [training: 0.18972110672961173 | validation: 0.13783709705514124]
	TIME [epoch: 8.44 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19746943649959203		[learning rate: 0.00040654]
	Learning Rate: 0.000406542
	LOSS [training: 0.19746943649959203 | validation: 0.17025130486751153]
	TIME [epoch: 8.43 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19693288324836386		[learning rate: 0.00040507]
	Learning Rate: 0.000405067
	LOSS [training: 0.19693288324836386 | validation: 0.14056360136144]
	TIME [epoch: 8.45 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1935632804429281		[learning rate: 0.0004036]
	Learning Rate: 0.000403597
	LOSS [training: 0.1935632804429281 | validation: 0.15146072321353865]
	TIME [epoch: 8.43 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20405324151402676		[learning rate: 0.00040213]
	Learning Rate: 0.000402132
	LOSS [training: 0.20405324151402676 | validation: 0.13364098185706397]
	TIME [epoch: 8.43 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20122140618456869		[learning rate: 0.00040067]
	Learning Rate: 0.000400672
	LOSS [training: 0.20122140618456869 | validation: 0.15564208789789435]
	TIME [epoch: 8.43 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1899949167451663		[learning rate: 0.00039922]
	Learning Rate: 0.000399218
	LOSS [training: 0.1899949167451663 | validation: 0.17925592222815473]
	TIME [epoch: 8.45 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20318010091217337		[learning rate: 0.00039777]
	Learning Rate: 0.00039777
	LOSS [training: 0.20318010091217337 | validation: 0.14051726193860872]
	TIME [epoch: 8.43 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20012533299984012		[learning rate: 0.00039633]
	Learning Rate: 0.000396326
	LOSS [training: 0.20012533299984012 | validation: 0.17438145962349005]
	TIME [epoch: 8.43 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19393377465789996		[learning rate: 0.00039489]
	Learning Rate: 0.000394888
	LOSS [training: 0.19393377465789996 | validation: 0.14390427852770377]
	TIME [epoch: 8.43 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20969274007896405		[learning rate: 0.00039345]
	Learning Rate: 0.000393455
	LOSS [training: 0.20969274007896405 | validation: 0.13979577104916538]
	TIME [epoch: 8.44 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19153835475739128		[learning rate: 0.00039203]
	Learning Rate: 0.000392027
	LOSS [training: 0.19153835475739128 | validation: 0.13987522870741242]
	TIME [epoch: 8.42 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18686116039800688		[learning rate: 0.0003906]
	Learning Rate: 0.000390604
	LOSS [training: 0.18686116039800688 | validation: 0.17230526118712008]
	TIME [epoch: 8.42 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22276337201977742		[learning rate: 0.00038919]
	Learning Rate: 0.000389187
	LOSS [training: 0.22276337201977742 | validation: 0.14617091353035955]
	TIME [epoch: 8.42 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1911469292475113		[learning rate: 0.00038777]
	Learning Rate: 0.000387774
	LOSS [training: 0.1911469292475113 | validation: 0.13555875498094605]
	TIME [epoch: 8.45 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19339225860313797		[learning rate: 0.00038637]
	Learning Rate: 0.000386367
	LOSS [training: 0.19339225860313797 | validation: 0.13376086866382778]
	TIME [epoch: 8.43 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1950612211942933		[learning rate: 0.00038496]
	Learning Rate: 0.000384965
	LOSS [training: 0.1950612211942933 | validation: 0.1269717980709647]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_996.pth
	Model improved!!!
EPOCH 997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1939393932136711		[learning rate: 0.00038357]
	Learning Rate: 0.000383568
	LOSS [training: 0.1939393932136711 | validation: 0.13497555932307623]
	TIME [epoch: 8.44 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21453640220302925		[learning rate: 0.00038218]
	Learning Rate: 0.000382176
	LOSS [training: 0.21453640220302925 | validation: 0.1677766021856793]
	TIME [epoch: 8.45 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20281030474849207		[learning rate: 0.00038079]
	Learning Rate: 0.000380789
	LOSS [training: 0.20281030474849207 | validation: 0.14992594474829257]
	TIME [epoch: 8.42 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20275664248449038		[learning rate: 0.00037941]
	Learning Rate: 0.000379407
	LOSS [training: 0.20275664248449038 | validation: 0.1544017345249142]
	TIME [epoch: 8.44 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19654899430655462		[learning rate: 0.00037803]
	Learning Rate: 0.00037803
	LOSS [training: 0.19654899430655462 | validation: 0.1916362781702839]
	TIME [epoch: 8.42 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18782561251649194		[learning rate: 0.00037666]
	Learning Rate: 0.000376658
	LOSS [training: 0.18782561251649194 | validation: 0.1621643219571687]
	TIME [epoch: 8.45 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21471356953709622		[learning rate: 0.00037529]
	Learning Rate: 0.000375291
	LOSS [training: 0.21471356953709622 | validation: 0.15087883880663275]
	TIME [epoch: 8.43 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20752452434384389		[learning rate: 0.00037393]
	Learning Rate: 0.000373929
	LOSS [training: 0.20752452434384389 | validation: 0.14522306748278407]
	TIME [epoch: 8.43 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20845399874888618		[learning rate: 0.00037257]
	Learning Rate: 0.000372572
	LOSS [training: 0.20845399874888618 | validation: 0.15307124521200488]
	TIME [epoch: 8.43 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18797090337335326		[learning rate: 0.00037122]
	Learning Rate: 0.00037122
	LOSS [training: 0.18797090337335326 | validation: 0.13857049915286854]
	TIME [epoch: 8.45 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19618926928245378		[learning rate: 0.00036987]
	Learning Rate: 0.000369873
	LOSS [training: 0.19618926928245378 | validation: 0.17294997726192435]
	TIME [epoch: 8.44 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20388104667840562		[learning rate: 0.00036853]
	Learning Rate: 0.000368531
	LOSS [training: 0.20388104667840562 | validation: 0.126065991978877]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_1008.pth
	Model improved!!!
EPOCH 1009/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18643036550916486		[learning rate: 0.00036719]
	Learning Rate: 0.000367193
	LOSS [training: 0.18643036550916486 | validation: 0.1954456868287225]
	TIME [epoch: 8.43 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20836715330411115		[learning rate: 0.00036586]
	Learning Rate: 0.000365861
	LOSS [training: 0.20836715330411115 | validation: 0.12690682113421464]
	TIME [epoch: 8.45 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21330167980800302		[learning rate: 0.00036453]
	Learning Rate: 0.000364533
	LOSS [training: 0.21330167980800302 | validation: 0.13512425929960348]
	TIME [epoch: 8.43 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2048697609147831		[learning rate: 0.00036321]
	Learning Rate: 0.00036321
	LOSS [training: 0.2048697609147831 | validation: 0.13447691416621252]
	TIME [epoch: 8.42 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1896390851644694		[learning rate: 0.00036189]
	Learning Rate: 0.000361892
	LOSS [training: 0.1896390851644694 | validation: 0.14929281256769197]
	TIME [epoch: 8.42 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2103208293675646		[learning rate: 0.00036058]
	Learning Rate: 0.000360579
	LOSS [training: 0.2103208293675646 | validation: 0.16406378124779328]
	TIME [epoch: 8.44 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.187638033736436		[learning rate: 0.00035927]
	Learning Rate: 0.00035927
	LOSS [training: 0.187638033736436 | validation: 0.1372326244907881]
	TIME [epoch: 8.42 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17554144948222827		[learning rate: 0.00035797]
	Learning Rate: 0.000357966
	LOSS [training: 0.17554144948222827 | validation: 0.13669802528575373]
	TIME [epoch: 8.43 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18350900448296265		[learning rate: 0.00035667]
	Learning Rate: 0.000356667
	LOSS [training: 0.18350900448296265 | validation: 0.1537440283474964]
	TIME [epoch: 8.42 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19885104296097617		[learning rate: 0.00035537]
	Learning Rate: 0.000355373
	LOSS [training: 0.19885104296097617 | validation: 0.14487879530033082]
	TIME [epoch: 8.45 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17882314902393068		[learning rate: 0.00035408]
	Learning Rate: 0.000354083
	LOSS [training: 0.17882314902393068 | validation: 0.12189087152593626]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_1019.pth
	Model improved!!!
EPOCH 1020/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19671317053271745		[learning rate: 0.0003528]
	Learning Rate: 0.000352798
	LOSS [training: 0.19671317053271745 | validation: 0.12641754954486606]
	TIME [epoch: 8.42 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19287493428058472		[learning rate: 0.00035152]
	Learning Rate: 0.000351518
	LOSS [training: 0.19287493428058472 | validation: 0.18674579076349102]
	TIME [epoch: 8.42 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2022774468265586		[learning rate: 0.00035024]
	Learning Rate: 0.000350242
	LOSS [training: 0.2022774468265586 | validation: 0.12755703624519593]
	TIME [epoch: 8.45 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19883863581068922		[learning rate: 0.00034897]
	Learning Rate: 0.000348971
	LOSS [training: 0.19883863581068922 | validation: 0.17563181699230354]
	TIME [epoch: 8.42 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1991424320176061		[learning rate: 0.0003477]
	Learning Rate: 0.000347705
	LOSS [training: 0.1991424320176061 | validation: 0.14916586042293034]
	TIME [epoch: 8.43 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19028783323187004		[learning rate: 0.00034644]
	Learning Rate: 0.000346443
	LOSS [training: 0.19028783323187004 | validation: 0.14050253495472614]
	TIME [epoch: 8.41 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20420796408426387		[learning rate: 0.00034519]
	Learning Rate: 0.000345186
	LOSS [training: 0.20420796408426387 | validation: 0.17271646980053632]
	TIME [epoch: 8.44 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2069095180798577		[learning rate: 0.00034393]
	Learning Rate: 0.000343933
	LOSS [training: 0.2069095180798577 | validation: 0.18630906931009217]
	TIME [epoch: 8.43 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19503277084060494		[learning rate: 0.00034268]
	Learning Rate: 0.000342685
	LOSS [training: 0.19503277084060494 | validation: 0.17040303626547393]
	TIME [epoch: 8.42 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18773333758039942		[learning rate: 0.00034144]
	Learning Rate: 0.000341441
	LOSS [training: 0.18773333758039942 | validation: 0.14190001852405754]
	TIME [epoch: 8.43 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18436849146127413		[learning rate: 0.0003402]
	Learning Rate: 0.000340202
	LOSS [training: 0.18436849146127413 | validation: 0.1941794018912817]
	TIME [epoch: 8.42 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20594726852601108		[learning rate: 0.00033897]
	Learning Rate: 0.000338967
	LOSS [training: 0.20594726852601108 | validation: 0.13396782429893708]
	TIME [epoch: 8.44 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1872218032986153		[learning rate: 0.00033774]
	Learning Rate: 0.000337737
	LOSS [training: 0.1872218032986153 | validation: 0.12496931013447973]
	TIME [epoch: 8.43 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1975494076961726		[learning rate: 0.00033651]
	Learning Rate: 0.000336512
	LOSS [training: 0.1975494076961726 | validation: 0.21561579532960057]
	TIME [epoch: 8.43 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18571026251672654		[learning rate: 0.00033529]
	Learning Rate: 0.00033529
	LOSS [training: 0.18571026251672654 | validation: 0.12986169289821742]
	TIME [epoch: 8.44 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18467824558591778		[learning rate: 0.00033407]
	Learning Rate: 0.000334074
	LOSS [training: 0.18467824558591778 | validation: 0.14031951432141854]
	TIME [epoch: 8.44 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19141673657459307		[learning rate: 0.00033286]
	Learning Rate: 0.000332861
	LOSS [training: 0.19141673657459307 | validation: 0.15900085499487432]
	TIME [epoch: 8.42 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19957878094967055		[learning rate: 0.00033165]
	Learning Rate: 0.000331653
	LOSS [training: 0.19957878094967055 | validation: 0.1391106446795951]
	TIME [epoch: 8.43 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19347116923764648		[learning rate: 0.00033045]
	Learning Rate: 0.00033045
	LOSS [training: 0.19347116923764648 | validation: 0.16467387432783942]
	TIME [epoch: 8.44 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2086549721336612		[learning rate: 0.00032925]
	Learning Rate: 0.00032925
	LOSS [training: 0.2086549721336612 | validation: 0.13532761047921912]
	TIME [epoch: 8.44 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18547545215193972		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 0.18547545215193972 | validation: 0.16657184317272572]
	TIME [epoch: 8.42 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20479718592324164		[learning rate: 0.00032686]
	Learning Rate: 0.000326865
	LOSS [training: 0.20479718592324164 | validation: 0.19404072526617208]
	TIME [epoch: 8.43 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20169992639944886		[learning rate: 0.00032568]
	Learning Rate: 0.000325679
	LOSS [training: 0.20169992639944886 | validation: 0.13045781056324024]
	TIME [epoch: 8.45 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19784684137294964		[learning rate: 0.0003245]
	Learning Rate: 0.000324497
	LOSS [training: 0.19784684137294964 | validation: 0.19952086210904552]
	TIME [epoch: 8.43 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2080552562686954		[learning rate: 0.00032332]
	Learning Rate: 0.000323319
	LOSS [training: 0.2080552562686954 | validation: 0.13694350768276942]
	TIME [epoch: 8.43 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18517764097320577		[learning rate: 0.00032215]
	Learning Rate: 0.000322146
	LOSS [training: 0.18517764097320577 | validation: 0.13887395704770686]
	TIME [epoch: 8.43 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18983252189006222		[learning rate: 0.00032098]
	Learning Rate: 0.000320977
	LOSS [training: 0.18983252189006222 | validation: 0.13343574781484074]
	TIME [epoch: 8.44 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18568043368589654		[learning rate: 0.00031981]
	Learning Rate: 0.000319812
	LOSS [training: 0.18568043368589654 | validation: 0.12519819896785803]
	TIME [epoch: 8.43 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21572482177443858		[learning rate: 0.00031865]
	Learning Rate: 0.000318651
	LOSS [training: 0.21572482177443858 | validation: 0.16156147122701886]
	TIME [epoch: 8.43 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20329962867478785		[learning rate: 0.0003175]
	Learning Rate: 0.000317495
	LOSS [training: 0.20329962867478785 | validation: 0.15235508673624065]
	TIME [epoch: 8.43 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19103416373583648		[learning rate: 0.00031634]
	Learning Rate: 0.000316343
	LOSS [training: 0.19103416373583648 | validation: 0.16481922452408374]
	TIME [epoch: 8.44 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1860533958662441		[learning rate: 0.00031519]
	Learning Rate: 0.000315195
	LOSS [training: 0.1860533958662441 | validation: 0.14459501412158832]
	TIME [epoch: 8.43 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2001119881269727		[learning rate: 0.00031405]
	Learning Rate: 0.000314051
	LOSS [training: 0.2001119881269727 | validation: 0.15132674930975956]
	TIME [epoch: 8.42 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19251795761549248		[learning rate: 0.00031291]
	Learning Rate: 0.000312911
	LOSS [training: 0.19251795761549248 | validation: 0.12562440554771107]
	TIME [epoch: 8.43 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17712948667928421		[learning rate: 0.00031178]
	Learning Rate: 0.000311776
	LOSS [training: 0.17712948667928421 | validation: 0.1380463904751459]
	TIME [epoch: 8.44 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17345779754852542		[learning rate: 0.00031064]
	Learning Rate: 0.000310644
	LOSS [training: 0.17345779754852542 | validation: 0.13596114856149003]
	TIME [epoch: 8.44 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19039711782327456		[learning rate: 0.00030952]
	Learning Rate: 0.000309517
	LOSS [training: 0.19039711782327456 | validation: 0.13224690049076449]
	TIME [epoch: 8.42 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20730694938205724		[learning rate: 0.00030839]
	Learning Rate: 0.000308394
	LOSS [training: 0.20730694938205724 | validation: 0.1641059203040705]
	TIME [epoch: 8.43 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1900419933014807		[learning rate: 0.00030727]
	Learning Rate: 0.000307274
	LOSS [training: 0.1900419933014807 | validation: 0.13154250142349552]
	TIME [epoch: 8.45 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1994158990394173		[learning rate: 0.00030616]
	Learning Rate: 0.000306159
	LOSS [training: 0.1994158990394173 | validation: 0.15870983213351175]
	TIME [epoch: 8.43 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18997377395002768		[learning rate: 0.00030505]
	Learning Rate: 0.000305048
	LOSS [training: 0.18997377395002768 | validation: 0.13157500061005653]
	TIME [epoch: 8.44 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19357432637217215		[learning rate: 0.00030394]
	Learning Rate: 0.000303941
	LOSS [training: 0.19357432637217215 | validation: 0.1525281241292359]
	TIME [epoch: 8.42 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1998500153466834		[learning rate: 0.00030284]
	Learning Rate: 0.000302838
	LOSS [training: 0.1998500153466834 | validation: 0.14826650842171193]
	TIME [epoch: 8.46 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18677167635870795		[learning rate: 0.00030174]
	Learning Rate: 0.000301739
	LOSS [training: 0.18677167635870795 | validation: 0.17246602609042055]
	TIME [epoch: 8.43 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18937877452856228		[learning rate: 0.00030064]
	Learning Rate: 0.000300644
	LOSS [training: 0.18937877452856228 | validation: 0.12755847864066971]
	TIME [epoch: 8.43 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19930926078338368		[learning rate: 0.00029955]
	Learning Rate: 0.000299553
	LOSS [training: 0.19930926078338368 | validation: 0.1500715664248889]
	TIME [epoch: 8.42 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17871886750091423		[learning rate: 0.00029847]
	Learning Rate: 0.000298466
	LOSS [training: 0.17871886750091423 | validation: 0.14830986648190608]
	TIME [epoch: 8.46 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2049333024889765		[learning rate: 0.00029738]
	Learning Rate: 0.000297383
	LOSS [training: 0.2049333024889765 | validation: 0.15735402324100572]
	TIME [epoch: 8.43 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18501207650364954		[learning rate: 0.0002963]
	Learning Rate: 0.000296304
	LOSS [training: 0.18501207650364954 | validation: 0.13214488695066437]
	TIME [epoch: 8.43 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18917445967826535		[learning rate: 0.00029523]
	Learning Rate: 0.000295228
	LOSS [training: 0.18917445967826535 | validation: 0.14018144411620742]
	TIME [epoch: 8.43 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18058015567597585		[learning rate: 0.00029416]
	Learning Rate: 0.000294157
	LOSS [training: 0.18058015567597585 | validation: 0.14954175451666316]
	TIME [epoch: 8.46 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19353797725294863		[learning rate: 0.00029309]
	Learning Rate: 0.000293089
	LOSS [training: 0.19353797725294863 | validation: 0.1604865435664253]
	TIME [epoch: 8.43 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18105457901288527		[learning rate: 0.00029203]
	Learning Rate: 0.000292026
	LOSS [training: 0.18105457901288527 | validation: 0.16483118560985657]
	TIME [epoch: 8.43 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18206784419065059		[learning rate: 0.00029097]
	Learning Rate: 0.000290966
	LOSS [training: 0.18206784419065059 | validation: 0.15029400113110233]
	TIME [epoch: 8.43 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18631056938439694		[learning rate: 0.00028991]
	Learning Rate: 0.00028991
	LOSS [training: 0.18631056938439694 | validation: 0.145715803425495]
	TIME [epoch: 8.44 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18248774632375037		[learning rate: 0.00028886]
	Learning Rate: 0.000288858
	LOSS [training: 0.18248774632375037 | validation: 0.14659871371887206]
	TIME [epoch: 8.44 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1955723459879515		[learning rate: 0.00028781]
	Learning Rate: 0.00028781
	LOSS [training: 0.1955723459879515 | validation: 0.12236121268458892]
	TIME [epoch: 8.43 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18169596179014905		[learning rate: 0.00028677]
	Learning Rate: 0.000286765
	LOSS [training: 0.18169596179014905 | validation: 0.15619741350785854]
	TIME [epoch: 8.42 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20639783759177494		[learning rate: 0.00028572]
	Learning Rate: 0.000285724
	LOSS [training: 0.20639783759177494 | validation: 0.14258485678136396]
	TIME [epoch: 8.44 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19375163131077566		[learning rate: 0.00028469]
	Learning Rate: 0.000284688
	LOSS [training: 0.19375163131077566 | validation: 0.13549290028298555]
	TIME [epoch: 8.42 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18625437912672643		[learning rate: 0.00028365]
	Learning Rate: 0.000283654
	LOSS [training: 0.18625437912672643 | validation: 0.14484223859273285]
	TIME [epoch: 8.43 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2052677443586975		[learning rate: 0.00028263]
	Learning Rate: 0.000282625
	LOSS [training: 0.2052677443586975 | validation: 0.13159098942668446]
	TIME [epoch: 8.43 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19374609654958788		[learning rate: 0.0002816]
	Learning Rate: 0.000281599
	LOSS [training: 0.19374609654958788 | validation: 0.13680175992579188]
	TIME [epoch: 8.45 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17842535595948528		[learning rate: 0.00028058]
	Learning Rate: 0.000280577
	LOSS [training: 0.17842535595948528 | validation: 0.1314155780360136]
	TIME [epoch: 8.43 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1870349974365676		[learning rate: 0.00027956]
	Learning Rate: 0.000279559
	LOSS [training: 0.1870349974365676 | validation: 0.15267986404600103]
	TIME [epoch: 8.42 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19565597941163604		[learning rate: 0.00027854]
	Learning Rate: 0.000278545
	LOSS [training: 0.19565597941163604 | validation: 0.14460261500918148]
	TIME [epoch: 8.44 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1840020658749997		[learning rate: 0.00027753]
	Learning Rate: 0.000277534
	LOSS [training: 0.1840020658749997 | validation: 0.1526897575870128]
	TIME [epoch: 8.44 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2022429388997098		[learning rate: 0.00027653]
	Learning Rate: 0.000276527
	LOSS [training: 0.2022429388997098 | validation: 0.18631550596899255]
	TIME [epoch: 8.42 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19056791053961658		[learning rate: 0.00027552]
	Learning Rate: 0.000275523
	LOSS [training: 0.19056791053961658 | validation: 0.12964266382987405]
	TIME [epoch: 8.42 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1750697823082092		[learning rate: 0.00027452]
	Learning Rate: 0.000274523
	LOSS [training: 0.1750697823082092 | validation: 0.15748335196089258]
	TIME [epoch: 8.44 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20577004488437703		[learning rate: 0.00027353]
	Learning Rate: 0.000273527
	LOSS [training: 0.20577004488437703 | validation: 0.14796070913903375]
	TIME [epoch: 8.43 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18511592022909368		[learning rate: 0.00027253]
	Learning Rate: 0.000272534
	LOSS [training: 0.18511592022909368 | validation: 0.1378859413242257]
	TIME [epoch: 8.43 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18907957595180908		[learning rate: 0.00027155]
	Learning Rate: 0.000271545
	LOSS [training: 0.18907957595180908 | validation: 0.14526373390660727]
	TIME [epoch: 8.42 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18091053664451376		[learning rate: 0.00027056]
	Learning Rate: 0.00027056
	LOSS [training: 0.18091053664451376 | validation: 0.14844678915347723]
	TIME [epoch: 8.45 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1836496218453198		[learning rate: 0.00026958]
	Learning Rate: 0.000269578
	LOSS [training: 0.1836496218453198 | validation: 0.1324871432263037]
	TIME [epoch: 8.43 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1926674115072324		[learning rate: 0.0002686]
	Learning Rate: 0.0002686
	LOSS [training: 0.1926674115072324 | validation: 0.13455230876702912]
	TIME [epoch: 8.43 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19195263745696833		[learning rate: 0.00026762]
	Learning Rate: 0.000267625
	LOSS [training: 0.19195263745696833 | validation: 0.1611569460510457]
	TIME [epoch: 8.43 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19827502521077783		[learning rate: 0.00026665]
	Learning Rate: 0.000266654
	LOSS [training: 0.19827502521077783 | validation: 0.13703038065365472]
	TIME [epoch: 8.46 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19439654193316278		[learning rate: 0.00026569]
	Learning Rate: 0.000265686
	LOSS [training: 0.19439654193316278 | validation: 0.15445608886647946]
	TIME [epoch: 8.43 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19281039843699962		[learning rate: 0.00026472]
	Learning Rate: 0.000264722
	LOSS [training: 0.19281039843699962 | validation: 0.14801219756751596]
	TIME [epoch: 8.43 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1838043446905694		[learning rate: 0.00026376]
	Learning Rate: 0.000263761
	LOSS [training: 0.1838043446905694 | validation: 0.16027822496455085]
	TIME [epoch: 8.42 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20614673833219452		[learning rate: 0.0002628]
	Learning Rate: 0.000262804
	LOSS [training: 0.20614673833219452 | validation: 0.13092287366259564]
	TIME [epoch: 8.46 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17832974078945218		[learning rate: 0.00026185]
	Learning Rate: 0.00026185
	LOSS [training: 0.17832974078945218 | validation: 0.14345200603189234]
	TIME [epoch: 8.43 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19053966969598704		[learning rate: 0.0002609]
	Learning Rate: 0.0002609
	LOSS [training: 0.19053966969598704 | validation: 0.1751392000955926]
	TIME [epoch: 8.43 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18959330441974995		[learning rate: 0.00025995]
	Learning Rate: 0.000259953
	LOSS [training: 0.18959330441974995 | validation: 0.13619779974564053]
	TIME [epoch: 8.43 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17473405195328962		[learning rate: 0.00025901]
	Learning Rate: 0.00025901
	LOSS [training: 0.17473405195328962 | validation: 0.13654474239519498]
	TIME [epoch: 8.46 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1721382081239599		[learning rate: 0.00025807]
	Learning Rate: 0.00025807
	LOSS [training: 0.1721382081239599 | validation: 0.12552182665932232]
	TIME [epoch: 8.44 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20108861222660757		[learning rate: 0.00025713]
	Learning Rate: 0.000257133
	LOSS [training: 0.20108861222660757 | validation: 0.14184919730888615]
	TIME [epoch: 8.43 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17745305112144544		[learning rate: 0.0002562]
	Learning Rate: 0.0002562
	LOSS [training: 0.17745305112144544 | validation: 0.12470873530757934]
	TIME [epoch: 8.44 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17452697995397862		[learning rate: 0.00025527]
	Learning Rate: 0.00025527
	LOSS [training: 0.17452697995397862 | validation: 0.1390399952031151]
	TIME [epoch: 8.45 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18845479304159934		[learning rate: 0.00025434]
	Learning Rate: 0.000254344
	LOSS [training: 0.18845479304159934 | validation: 0.12858518967486438]
	TIME [epoch: 8.43 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17153477794989078		[learning rate: 0.00025342]
	Learning Rate: 0.000253421
	LOSS [training: 0.17153477794989078 | validation: 0.13792466900946912]
	TIME [epoch: 8.42 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1944804173242011		[learning rate: 0.0002525]
	Learning Rate: 0.000252501
	LOSS [training: 0.1944804173242011 | validation: 0.17873345692678122]
	TIME [epoch: 8.43 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18714163767404998		[learning rate: 0.00025158]
	Learning Rate: 0.000251585
	LOSS [training: 0.18714163767404998 | validation: 0.17388126751777264]
	TIME [epoch: 8.45 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1980400129559926		[learning rate: 0.00025067]
	Learning Rate: 0.000250672
	LOSS [training: 0.1980400129559926 | validation: 0.1402437345998112]
	TIME [epoch: 8.43 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18474171272670903		[learning rate: 0.00024976]
	Learning Rate: 0.000249762
	LOSS [training: 0.18474171272670903 | validation: 0.1413719077162049]
	TIME [epoch: 8.43 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1843843422206231		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 0.1843843422206231 | validation: 0.14537669293757138]
	TIME [epoch: 8.43 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19484722266202376		[learning rate: 0.00024795]
	Learning Rate: 0.000247952
	LOSS [training: 0.19484722266202376 | validation: 0.12765426355638754]
	TIME [epoch: 8.44 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19198142844233043		[learning rate: 0.00024705]
	Learning Rate: 0.000247053
	LOSS [training: 0.19198142844233043 | validation: 0.14118023569146362]
	TIME [epoch: 8.42 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19884657144468382		[learning rate: 0.00024616]
	Learning Rate: 0.000246156
	LOSS [training: 0.19884657144468382 | validation: 0.13562893228128098]
	TIME [epoch: 8.43 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18002328810212473		[learning rate: 0.00024526]
	Learning Rate: 0.000245263
	LOSS [training: 0.18002328810212473 | validation: 0.12734294110669092]
	TIME [epoch: 8.42 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18220275624443094		[learning rate: 0.00024437]
	Learning Rate: 0.000244373
	LOSS [training: 0.18220275624443094 | validation: 0.14929330293680382]
	TIME [epoch: 8.44 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17516369169231222		[learning rate: 0.00024349]
	Learning Rate: 0.000243486
	LOSS [training: 0.17516369169231222 | validation: 0.1410010911274165]
	TIME [epoch: 8.42 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17962849375266657		[learning rate: 0.0002426]
	Learning Rate: 0.000242602
	LOSS [training: 0.17962849375266657 | validation: 0.20990456145151126]
	TIME [epoch: 8.44 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1870972397011367		[learning rate: 0.00024172]
	Learning Rate: 0.000241722
	LOSS [training: 0.1870972397011367 | validation: 0.12333021201862615]
	TIME [epoch: 8.42 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17663957153017365		[learning rate: 0.00024084]
	Learning Rate: 0.000240845
	LOSS [training: 0.17663957153017365 | validation: 0.15191716206791486]
	TIME [epoch: 8.45 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17694898643095464		[learning rate: 0.00023997]
	Learning Rate: 0.000239971
	LOSS [training: 0.17694898643095464 | validation: 0.1342752454173967]
	TIME [epoch: 8.42 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18889571477232045		[learning rate: 0.0002391]
	Learning Rate: 0.0002391
	LOSS [training: 0.18889571477232045 | validation: 0.13753481247885632]
	TIME [epoch: 8.42 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18483340892570813		[learning rate: 0.00023823]
	Learning Rate: 0.000238232
	LOSS [training: 0.18483340892570813 | validation: 0.1306718070473183]
	TIME [epoch: 8.43 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16822518109283507		[learning rate: 0.00023737]
	Learning Rate: 0.000237367
	LOSS [training: 0.16822518109283507 | validation: 0.14538339567954336]
	TIME [epoch: 8.45 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1824358702530854		[learning rate: 0.00023651]
	Learning Rate: 0.000236506
	LOSS [training: 0.1824358702530854 | validation: 0.16103575651363788]
	TIME [epoch: 8.42 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19349329573634727		[learning rate: 0.00023565]
	Learning Rate: 0.000235648
	LOSS [training: 0.19349329573634727 | validation: 0.14027476476658934]
	TIME [epoch: 8.43 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17951522859194083		[learning rate: 0.00023479]
	Learning Rate: 0.000234793
	LOSS [training: 0.17951522859194083 | validation: 0.14062275117346967]
	TIME [epoch: 8.43 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20692395413550296		[learning rate: 0.00023394]
	Learning Rate: 0.00023394
	LOSS [training: 0.20692395413550296 | validation: 0.126658394156988]
	TIME [epoch: 8.44 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1839520759916306		[learning rate: 0.00023309]
	Learning Rate: 0.000233091
	LOSS [training: 0.1839520759916306 | validation: 0.14632445584676887]
	TIME [epoch: 8.43 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1804793337619775		[learning rate: 0.00023225]
	Learning Rate: 0.000232246
	LOSS [training: 0.1804793337619775 | validation: 0.15498899025771823]
	TIME [epoch: 8.43 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17289518157358236		[learning rate: 0.0002314]
	Learning Rate: 0.000231403
	LOSS [training: 0.17289518157358236 | validation: 0.13561033865293776]
	TIME [epoch: 8.43 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17086196066458398		[learning rate: 0.00023056]
	Learning Rate: 0.000230563
	LOSS [training: 0.17086196066458398 | validation: 0.1433347644001833]
	TIME [epoch: 8.44 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1773040258996469		[learning rate: 0.00022973]
	Learning Rate: 0.000229726
	LOSS [training: 0.1773040258996469 | validation: 0.1337880227846047]
	TIME [epoch: 8.43 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19260237000506578		[learning rate: 0.00022889]
	Learning Rate: 0.000228893
	LOSS [training: 0.19260237000506578 | validation: 0.1393020517379041]
	TIME [epoch: 8.43 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18548096340180145		[learning rate: 0.00022806]
	Learning Rate: 0.000228062
	LOSS [training: 0.18548096340180145 | validation: 0.12380023477953318]
	TIME [epoch: 8.44 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1813460506685692		[learning rate: 0.00022723]
	Learning Rate: 0.000227234
	LOSS [training: 0.1813460506685692 | validation: 0.17640287605486016]
	TIME [epoch: 8.43 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18132343611760632		[learning rate: 0.00022641]
	Learning Rate: 0.00022641
	LOSS [training: 0.18132343611760632 | validation: 0.13543435893542802]
	TIME [epoch: 8.43 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1873682132986986		[learning rate: 0.00022559]
	Learning Rate: 0.000225588
	LOSS [training: 0.1873682132986986 | validation: 0.14148747857898733]
	TIME [epoch: 8.43 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18419905340074455		[learning rate: 0.00022477]
	Learning Rate: 0.000224769
	LOSS [training: 0.18419905340074455 | validation: 0.1416117315201259]
	TIME [epoch: 8.45 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1921618800222992		[learning rate: 0.00022395]
	Learning Rate: 0.000223954
	LOSS [training: 0.1921618800222992 | validation: 0.13591695650946148]
	TIME [epoch: 8.43 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1783502186043024		[learning rate: 0.00022314]
	Learning Rate: 0.000223141
	LOSS [training: 0.1783502186043024 | validation: 0.13606279380554212]
	TIME [epoch: 8.42 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1756832618205642		[learning rate: 0.00022233]
	Learning Rate: 0.000222331
	LOSS [training: 0.1756832618205642 | validation: 0.13180702330037694]
	TIME [epoch: 8.43 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17718416211903515		[learning rate: 0.00022152]
	Learning Rate: 0.000221524
	LOSS [training: 0.17718416211903515 | validation: 0.13349831417404745]
	TIME [epoch: 8.45 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1850079689235894		[learning rate: 0.00022072]
	Learning Rate: 0.00022072
	LOSS [training: 0.1850079689235894 | validation: 0.13959980932945804]
	TIME [epoch: 8.43 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18216464905178267		[learning rate: 0.00021992]
	Learning Rate: 0.000219919
	LOSS [training: 0.18216464905178267 | validation: 0.1486411495102273]
	TIME [epoch: 8.43 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1802830918336881		[learning rate: 0.00021912]
	Learning Rate: 0.000219121
	LOSS [training: 0.1802830918336881 | validation: 0.1315261784821914]
	TIME [epoch: 8.43 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17432122718159646		[learning rate: 0.00021833]
	Learning Rate: 0.000218326
	LOSS [training: 0.17432122718159646 | validation: 0.13684139388884375]
	TIME [epoch: 8.45 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18477289340120348		[learning rate: 0.00021753]
	Learning Rate: 0.000217534
	LOSS [training: 0.18477289340120348 | validation: 0.12215567387271256]
	TIME [epoch: 8.43 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17701408374784325		[learning rate: 0.00021674]
	Learning Rate: 0.000216744
	LOSS [training: 0.17701408374784325 | validation: 0.14559156729012068]
	TIME [epoch: 8.43 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18177153378334351		[learning rate: 0.00021596]
	Learning Rate: 0.000215958
	LOSS [training: 0.18177153378334351 | validation: 0.13008002672403912]
	TIME [epoch: 8.43 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1861227493093484		[learning rate: 0.00021517]
	Learning Rate: 0.000215174
	LOSS [training: 0.1861227493093484 | validation: 0.13271381160219797]
	TIME [epoch: 8.45 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18352816159836033		[learning rate: 0.00021439]
	Learning Rate: 0.000214393
	LOSS [training: 0.18352816159836033 | validation: 0.1304446503005999]
	TIME [epoch: 8.43 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1719873703992672		[learning rate: 0.00021361]
	Learning Rate: 0.000213615
	LOSS [training: 0.1719873703992672 | validation: 0.1186286894097815]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_1158.pth
	Model improved!!!
EPOCH 1159/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18501654521017913		[learning rate: 0.00021284]
	Learning Rate: 0.00021284
	LOSS [training: 0.18501654521017913 | validation: 0.14549145084925408]
	TIME [epoch: 8.42 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18796088693233193		[learning rate: 0.00021207]
	Learning Rate: 0.000212067
	LOSS [training: 0.18796088693233193 | validation: 0.16178376836372513]
	TIME [epoch: 8.44 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18857401587849287		[learning rate: 0.0002113]
	Learning Rate: 0.000211298
	LOSS [training: 0.18857401587849287 | validation: 0.14889727033092903]
	TIME [epoch: 8.43 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18094947900412522		[learning rate: 0.00021053]
	Learning Rate: 0.000210531
	LOSS [training: 0.18094947900412522 | validation: 0.13141192959623965]
	TIME [epoch: 8.42 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1881556076011969		[learning rate: 0.00020977]
	Learning Rate: 0.000209767
	LOSS [training: 0.1881556076011969 | validation: 0.14273470999922647]
	TIME [epoch: 8.43 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1865047481512932		[learning rate: 0.00020901]
	Learning Rate: 0.000209006
	LOSS [training: 0.1865047481512932 | validation: 0.13176950998410097]
	TIME [epoch: 8.44 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17124990786368088		[learning rate: 0.00020825]
	Learning Rate: 0.000208247
	LOSS [training: 0.17124990786368088 | validation: 0.15741164900365245]
	TIME [epoch: 8.43 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17983968240710446		[learning rate: 0.00020749]
	Learning Rate: 0.000207491
	LOSS [training: 0.17983968240710446 | validation: 0.11994007876956356]
	TIME [epoch: 8.42 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16912055721282046		[learning rate: 0.00020674]
	Learning Rate: 0.000206738
	LOSS [training: 0.16912055721282046 | validation: 0.13175845156427116]
	TIME [epoch: 8.43 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18625427838407405		[learning rate: 0.00020599]
	Learning Rate: 0.000205988
	LOSS [training: 0.18625427838407405 | validation: 0.13797309593223092]
	TIME [epoch: 8.44 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17844092712025444		[learning rate: 0.00020524]
	Learning Rate: 0.000205241
	LOSS [training: 0.17844092712025444 | validation: 0.12806979302899785]
	TIME [epoch: 8.42 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1731263711927551		[learning rate: 0.0002045]
	Learning Rate: 0.000204496
	LOSS [training: 0.1731263711927551 | validation: 0.13039740368839448]
	TIME [epoch: 8.43 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17336739361632364		[learning rate: 0.00020375]
	Learning Rate: 0.000203754
	LOSS [training: 0.17336739361632364 | validation: 0.13256472217748747]
	TIME [epoch: 8.42 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18041734469055437		[learning rate: 0.00020301]
	Learning Rate: 0.000203014
	LOSS [training: 0.18041734469055437 | validation: 0.17786512218853578]
	TIME [epoch: 8.44 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18720524854047574		[learning rate: 0.00020228]
	Learning Rate: 0.000202277
	LOSS [training: 0.18720524854047574 | validation: 0.14226388410982546]
	TIME [epoch: 8.42 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18888521027365512		[learning rate: 0.00020154]
	Learning Rate: 0.000201543
	LOSS [training: 0.18888521027365512 | validation: 0.1709103022325527]
	TIME [epoch: 8.42 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1852611289796038		[learning rate: 0.00020081]
	Learning Rate: 0.000200812
	LOSS [training: 0.1852611289796038 | validation: 0.13114859775802606]
	TIME [epoch: 8.41 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17601090811757472		[learning rate: 0.00020008]
	Learning Rate: 0.000200083
	LOSS [training: 0.17601090811757472 | validation: 0.1378265461274317]
	TIME [epoch: 8.43 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1853603795157546		[learning rate: 0.00019936]
	Learning Rate: 0.000199357
	LOSS [training: 0.1853603795157546 | validation: 0.12966564905348968]
	TIME [epoch: 8.42 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1695024662074403		[learning rate: 0.00019863]
	Learning Rate: 0.000198634
	LOSS [training: 0.1695024662074403 | validation: 0.14851616568204085]
	TIME [epoch: 8.41 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18231953283990782		[learning rate: 0.00019791]
	Learning Rate: 0.000197913
	LOSS [training: 0.18231953283990782 | validation: 0.12341838938156008]
	TIME [epoch: 8.41 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16881849499155727		[learning rate: 0.00019719]
	Learning Rate: 0.000197194
	LOSS [training: 0.16881849499155727 | validation: 0.1344090995936943]
	TIME [epoch: 8.43 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17026390846997613		[learning rate: 0.00019648]
	Learning Rate: 0.000196479
	LOSS [training: 0.17026390846997613 | validation: 0.1248986178348222]
	TIME [epoch: 8.42 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1748655954456469		[learning rate: 0.00019577]
	Learning Rate: 0.000195766
	LOSS [training: 0.1748655954456469 | validation: 0.13176445994397964]
	TIME [epoch: 8.41 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17419887190023683		[learning rate: 0.00019506]
	Learning Rate: 0.000195055
	LOSS [training: 0.17419887190023683 | validation: 0.12925691660158967]
	TIME [epoch: 8.42 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18318125626488305		[learning rate: 0.00019435]
	Learning Rate: 0.000194347
	LOSS [training: 0.18318125626488305 | validation: 0.13502928219878058]
	TIME [epoch: 8.44 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1986694028916671		[learning rate: 0.00019364]
	Learning Rate: 0.000193642
	LOSS [training: 0.1986694028916671 | validation: 0.1746535475959258]
	TIME [epoch: 8.42 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2007003811308743		[learning rate: 0.00019294]
	Learning Rate: 0.000192939
	LOSS [training: 0.2007003811308743 | validation: 0.1274498363455444]
	TIME [epoch: 8.42 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17198771091219606		[learning rate: 0.00019224]
	Learning Rate: 0.000192239
	LOSS [training: 0.17198771091219606 | validation: 0.1350515846462747]
	TIME [epoch: 8.43 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17608769249117923		[learning rate: 0.00019154]
	Learning Rate: 0.000191542
	LOSS [training: 0.17608769249117923 | validation: 0.12363213534449542]
	TIME [epoch: 8.44 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1878444638349246		[learning rate: 0.00019085]
	Learning Rate: 0.000190846
	LOSS [training: 0.1878444638349246 | validation: 0.14523276124220663]
	TIME [epoch: 8.42 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1755260679440399		[learning rate: 0.00019015]
	Learning Rate: 0.000190154
	LOSS [training: 0.1755260679440399 | validation: 0.13565691424766946]
	TIME [epoch: 8.43 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17802459299579687		[learning rate: 0.00018946]
	Learning Rate: 0.000189464
	LOSS [training: 0.17802459299579687 | validation: 0.12964646002578617]
	TIME [epoch: 8.41 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17211837317367354		[learning rate: 0.00018878]
	Learning Rate: 0.000188776
	LOSS [training: 0.17211837317367354 | validation: 0.15067786782760798]
	TIME [epoch: 8.44 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16590883148511446		[learning rate: 0.00018809]
	Learning Rate: 0.000188091
	LOSS [training: 0.16590883148511446 | validation: 0.12745906525519818]
	TIME [epoch: 8.41 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17289920180755763		[learning rate: 0.00018741]
	Learning Rate: 0.000187409
	LOSS [training: 0.17289920180755763 | validation: 0.14567018414633254]
	TIME [epoch: 8.42 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1720101170387186		[learning rate: 0.00018673]
	Learning Rate: 0.000186729
	LOSS [training: 0.1720101170387186 | validation: 0.12624624325625078]
	TIME [epoch: 8.43 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17701628244685877		[learning rate: 0.00018605]
	Learning Rate: 0.000186051
	LOSS [training: 0.17701628244685877 | validation: 0.13824972814491057]
	TIME [epoch: 8.44 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1770889651273827		[learning rate: 0.00018538]
	Learning Rate: 0.000185376
	LOSS [training: 0.1770889651273827 | validation: 0.1304291853855848]
	TIME [epoch: 8.42 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17808358644996064		[learning rate: 0.0001847]
	Learning Rate: 0.000184703
	LOSS [training: 0.17808358644996064 | validation: 0.1111572653022298]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_1198.pth
	Model improved!!!
EPOCH 1199/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17475915133703385		[learning rate: 0.00018403]
	Learning Rate: 0.000184033
	LOSS [training: 0.17475915133703385 | validation: 0.15445531728201975]
	TIME [epoch: 8.43 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17881099589665908		[learning rate: 0.00018336]
	Learning Rate: 0.000183365
	LOSS [training: 0.17881099589665908 | validation: 0.1326148360524263]
	TIME [epoch: 8.46 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1805450412469121		[learning rate: 0.0001827]
	Learning Rate: 0.000182699
	LOSS [training: 0.1805450412469121 | validation: 0.13368292543771673]
	TIME [epoch: 8.42 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19407309666323816		[learning rate: 0.00018204]
	Learning Rate: 0.000182036
	LOSS [training: 0.19407309666323816 | validation: 0.12038988540233708]
	TIME [epoch: 8.42 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1722854753973542		[learning rate: 0.00018138]
	Learning Rate: 0.000181376
	LOSS [training: 0.1722854753973542 | validation: 0.13188144418578052]
	TIME [epoch: 8.43 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16797843124873882		[learning rate: 0.00018072]
	Learning Rate: 0.000180717
	LOSS [training: 0.16797843124873882 | validation: 0.12887694665600372]
	TIME [epoch: 8.45 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17917530076673457		[learning rate: 0.00018006]
	Learning Rate: 0.000180062
	LOSS [training: 0.17917530076673457 | validation: 0.12293593675281148]
	TIME [epoch: 8.43 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1805524733995126		[learning rate: 0.00017941]
	Learning Rate: 0.000179408
	LOSS [training: 0.1805524733995126 | validation: 0.12823514492499724]
	TIME [epoch: 8.43 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17894358376159342		[learning rate: 0.00017876]
	Learning Rate: 0.000178757
	LOSS [training: 0.17894358376159342 | validation: 0.15885546421172803]
	TIME [epoch: 8.42 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1819087585875136		[learning rate: 0.00017811]
	Learning Rate: 0.000178108
	LOSS [training: 0.1819087585875136 | validation: 0.1178964266060456]
	TIME [epoch: 8.44 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19719962005860808		[learning rate: 0.00017746]
	Learning Rate: 0.000177462
	LOSS [training: 0.19719962005860808 | validation: 0.13118057995575333]
	TIME [epoch: 8.42 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1706390779591986		[learning rate: 0.00017682]
	Learning Rate: 0.000176818
	LOSS [training: 0.1706390779591986 | validation: 0.12541039657175151]
	TIME [epoch: 8.44 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1714566680942939		[learning rate: 0.00017618]
	Learning Rate: 0.000176176
	LOSS [training: 0.1714566680942939 | validation: 0.1295454522169907]
	TIME [epoch: 8.43 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17361192528244448		[learning rate: 0.00017554]
	Learning Rate: 0.000175537
	LOSS [training: 0.17361192528244448 | validation: 0.11702246283974524]
	TIME [epoch: 8.46 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18270573775366533		[learning rate: 0.0001749]
	Learning Rate: 0.0001749
	LOSS [training: 0.18270573775366533 | validation: 0.13012510833893726]
	TIME [epoch: 8.43 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19603571114126567		[learning rate: 0.00017427]
	Learning Rate: 0.000174265
	LOSS [training: 0.19603571114126567 | validation: 0.14017615023018803]
	TIME [epoch: 8.43 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17644510475878442		[learning rate: 0.00017363]
	Learning Rate: 0.000173633
	LOSS [training: 0.17644510475878442 | validation: 0.12974296268964816]
	TIME [epoch: 8.43 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18282640997806138		[learning rate: 0.000173]
	Learning Rate: 0.000173003
	LOSS [training: 0.18282640997806138 | validation: 0.1300197196211928]
	TIME [epoch: 8.45 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17553864584861534		[learning rate: 0.00017237]
	Learning Rate: 0.000172375
	LOSS [training: 0.17553864584861534 | validation: 0.11951300665941758]
	TIME [epoch: 8.42 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19667082213891499		[learning rate: 0.00017175]
	Learning Rate: 0.000171749
	LOSS [training: 0.19667082213891499 | validation: 0.13999129809389232]
	TIME [epoch: 8.43 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18604661522886445		[learning rate: 0.00017113]
	Learning Rate: 0.000171126
	LOSS [training: 0.18604661522886445 | validation: 0.1286307757120672]
	TIME [epoch: 8.44 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17813167202900215		[learning rate: 0.0001705]
	Learning Rate: 0.000170505
	LOSS [training: 0.17813167202900215 | validation: 0.13177814098517743]
	TIME [epoch: 8.45 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17862058546194798		[learning rate: 0.00016989]
	Learning Rate: 0.000169886
	LOSS [training: 0.17862058546194798 | validation: 0.13684229495495115]
	TIME [epoch: 8.42 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16823700549588436		[learning rate: 0.00016927]
	Learning Rate: 0.00016927
	LOSS [training: 0.16823700549588436 | validation: 0.12863274487648788]
	TIME [epoch: 8.42 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18282143689747477		[learning rate: 0.00016866]
	Learning Rate: 0.000168655
	LOSS [training: 0.18282143689747477 | validation: 0.13404605291300187]
	TIME [epoch: 8.44 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1765357723047466		[learning rate: 0.00016804]
	Learning Rate: 0.000168043
	LOSS [training: 0.1765357723047466 | validation: 0.13612914130229564]
	TIME [epoch: 8.45 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17645707785058376		[learning rate: 0.00016743]
	Learning Rate: 0.000167433
	LOSS [training: 0.17645707785058376 | validation: 0.14063707151684277]
	TIME [epoch: 8.43 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17546333865581243		[learning rate: 0.00016683]
	Learning Rate: 0.000166826
	LOSS [training: 0.17546333865581243 | validation: 0.15684443942336662]
	TIME [epoch: 8.43 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21055324906978154		[learning rate: 0.00016622]
	Learning Rate: 0.00016622
	LOSS [training: 0.21055324906978154 | validation: 0.14128273005451736]
	TIME [epoch: 8.44 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1762507393732626		[learning rate: 0.00016562]
	Learning Rate: 0.000165617
	LOSS [training: 0.1762507393732626 | validation: 0.13201775403503963]
	TIME [epoch: 8.43 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17939627880916909		[learning rate: 0.00016502]
	Learning Rate: 0.000165016
	LOSS [training: 0.17939627880916909 | validation: 0.16273874711610692]
	TIME [epoch: 8.43 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16964328932838063		[learning rate: 0.00016442]
	Learning Rate: 0.000164417
	LOSS [training: 0.16964328932838063 | validation: 0.1321248827544139]
	TIME [epoch: 8.43 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16946060999363988		[learning rate: 0.00016382]
	Learning Rate: 0.000163821
	LOSS [training: 0.16946060999363988 | validation: 0.1349923340068771]
	TIME [epoch: 8.45 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18295024647330843		[learning rate: 0.00016323]
	Learning Rate: 0.000163226
	LOSS [training: 0.18295024647330843 | validation: 0.1447113099650098]
	TIME [epoch: 8.44 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17980340332533523		[learning rate: 0.00016263]
	Learning Rate: 0.000162634
	LOSS [training: 0.17980340332533523 | validation: 0.1406433356631549]
	TIME [epoch: 8.43 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1819884262890472		[learning rate: 0.00016204]
	Learning Rate: 0.000162043
	LOSS [training: 0.1819884262890472 | validation: 0.1375324804851304]
	TIME [epoch: 8.42 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17964358728401658		[learning rate: 0.00016146]
	Learning Rate: 0.000161455
	LOSS [training: 0.17964358728401658 | validation: 0.13612778177170326]
	TIME [epoch: 8.46 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17350368204941433		[learning rate: 0.00016087]
	Learning Rate: 0.000160869
	LOSS [training: 0.17350368204941433 | validation: 0.13053122119263916]
	TIME [epoch: 8.43 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1763487737908691		[learning rate: 0.00016029]
	Learning Rate: 0.000160286
	LOSS [training: 0.1763487737908691 | validation: 0.11752649311629484]
	TIME [epoch: 8.42 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17928028587341593		[learning rate: 0.0001597]
	Learning Rate: 0.000159704
	LOSS [training: 0.17928028587341593 | validation: 0.13222712232183348]
	TIME [epoch: 8.43 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17617452346670717		[learning rate: 0.00015912]
	Learning Rate: 0.000159124
	LOSS [training: 0.17617452346670717 | validation: 0.15150944378799422]
	TIME [epoch: 8.45 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18098539865242758		[learning rate: 0.00015855]
	Learning Rate: 0.000158547
	LOSS [training: 0.18098539865242758 | validation: 0.12597310513777785]
	TIME [epoch: 8.43 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17501291994159046		[learning rate: 0.00015797]
	Learning Rate: 0.000157972
	LOSS [training: 0.17501291994159046 | validation: 0.12512047682337937]
	TIME [epoch: 8.42 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.173003221924241		[learning rate: 0.0001574]
	Learning Rate: 0.000157398
	LOSS [training: 0.173003221924241 | validation: 0.12318170884636603]
	TIME [epoch: 8.44 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17246481167493424		[learning rate: 0.00015683]
	Learning Rate: 0.000156827
	LOSS [training: 0.17246481167493424 | validation: 0.12116341923836435]
	TIME [epoch: 8.45 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1654929058066747		[learning rate: 0.00015626]
	Learning Rate: 0.000156258
	LOSS [training: 0.1654929058066747 | validation: 0.14234194857133486]
	TIME [epoch: 8.43 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1729276782124754		[learning rate: 0.00015569]
	Learning Rate: 0.000155691
	LOSS [training: 0.1729276782124754 | validation: 0.13457689826000113]
	TIME [epoch: 8.43 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16799071820657407		[learning rate: 0.00015513]
	Learning Rate: 0.000155126
	LOSS [training: 0.16799071820657407 | validation: 0.1351365695437293]
	TIME [epoch: 8.43 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17683995221727813		[learning rate: 0.00015456]
	Learning Rate: 0.000154563
	LOSS [training: 0.17683995221727813 | validation: 0.13413838668739747]
	TIME [epoch: 8.45 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1756576009505639		[learning rate: 0.000154]
	Learning Rate: 0.000154002
	LOSS [training: 0.1756576009505639 | validation: 0.13660949276587767]
	TIME [epoch: 8.43 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17551057125345598		[learning rate: 0.00015344]
	Learning Rate: 0.000153443
	LOSS [training: 0.17551057125345598 | validation: 0.1265586668371256]
	TIME [epoch: 8.43 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1653879567812877		[learning rate: 0.00015289]
	Learning Rate: 0.000152886
	LOSS [training: 0.1653879567812877 | validation: 0.13521257498778444]
	TIME [epoch: 8.43 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17953169235930785		[learning rate: 0.00015233]
	Learning Rate: 0.000152331
	LOSS [training: 0.17953169235930785 | validation: 0.13918217934915753]
	TIME [epoch: 8.46 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18611771248560854		[learning rate: 0.00015178]
	Learning Rate: 0.000151779
	LOSS [training: 0.18611771248560854 | validation: 0.1335619935615648]
	TIME [epoch: 8.44 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17548518069453023		[learning rate: 0.00015123]
	Learning Rate: 0.000151228
	LOSS [training: 0.17548518069453023 | validation: 0.1495454841079739]
	TIME [epoch: 8.43 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17909197406548366		[learning rate: 0.00015068]
	Learning Rate: 0.000150679
	LOSS [training: 0.17909197406548366 | validation: 0.13386606290046554]
	TIME [epoch: 8.44 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16765447172323694		[learning rate: 0.00015013]
	Learning Rate: 0.000150132
	LOSS [training: 0.16765447172323694 | validation: 0.1344901183198964]
	TIME [epoch: 8.45 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1738382144360727		[learning rate: 0.00014959]
	Learning Rate: 0.000149587
	LOSS [training: 0.1738382144360727 | validation: 0.16717274967336987]
	TIME [epoch: 8.43 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18665886574002372		[learning rate: 0.00014904]
	Learning Rate: 0.000149044
	LOSS [training: 0.18665886574002372 | validation: 0.13130773141135346]
	TIME [epoch: 8.43 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17052588530532223		[learning rate: 0.0001485]
	Learning Rate: 0.000148504
	LOSS [training: 0.17052588530532223 | validation: 0.13218204948087575]
	TIME [epoch: 8.43 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18319795274223138		[learning rate: 0.00014796]
	Learning Rate: 0.000147965
	LOSS [training: 0.18319795274223138 | validation: 0.12989102803709546]
	TIME [epoch: 8.44 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17340296814295503		[learning rate: 0.00014743]
	Learning Rate: 0.000147428
	LOSS [training: 0.17340296814295503 | validation: 0.14344843953408487]
	TIME [epoch: 8.43 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.173283695906959		[learning rate: 0.00014689]
	Learning Rate: 0.000146893
	LOSS [training: 0.173283695906959 | validation: 0.12788659637803682]
	TIME [epoch: 8.43 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17315309348340002		[learning rate: 0.00014636]
	Learning Rate: 0.00014636
	LOSS [training: 0.17315309348340002 | validation: 0.13924548321972507]
	TIME [epoch: 8.43 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19103364209364493		[learning rate: 0.00014583]
	Learning Rate: 0.000145828
	LOSS [training: 0.19103364209364493 | validation: 0.13412843290621712]
	TIME [epoch: 8.45 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18414036654000202		[learning rate: 0.0001453]
	Learning Rate: 0.000145299
	LOSS [training: 0.18414036654000202 | validation: 0.12932211276752584]
	TIME [epoch: 8.43 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1722036418983522		[learning rate: 0.00014477]
	Learning Rate: 0.000144772
	LOSS [training: 0.1722036418983522 | validation: 0.12679786003287644]
	TIME [epoch: 8.42 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17658540311781196		[learning rate: 0.00014425]
	Learning Rate: 0.000144247
	LOSS [training: 0.17658540311781196 | validation: 0.14120154114389236]
	TIME [epoch: 8.43 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17119368253396625		[learning rate: 0.00014372]
	Learning Rate: 0.000143723
	LOSS [training: 0.17119368253396625 | validation: 0.13944849691099584]
	TIME [epoch: 8.45 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1685885974018726		[learning rate: 0.0001432]
	Learning Rate: 0.000143201
	LOSS [training: 0.1685885974018726 | validation: 0.16522561810562714]
	TIME [epoch: 8.44 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1759509814046178		[learning rate: 0.00014268]
	Learning Rate: 0.000142682
	LOSS [training: 0.1759509814046178 | validation: 0.12638439560802128]
	TIME [epoch: 8.44 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17136386924364144		[learning rate: 0.00014216]
	Learning Rate: 0.000142164
	LOSS [training: 0.17136386924364144 | validation: 0.14117531645860512]
	TIME [epoch: 8.43 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1742384239608615		[learning rate: 0.00014165]
	Learning Rate: 0.000141648
	LOSS [training: 0.1742384239608615 | validation: 0.1399025628599698]
	TIME [epoch: 8.45 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17299632310324248		[learning rate: 0.00014113]
	Learning Rate: 0.000141134
	LOSS [training: 0.17299632310324248 | validation: 0.13899610066209853]
	TIME [epoch: 8.43 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1697810682285966		[learning rate: 0.00014062]
	Learning Rate: 0.000140622
	LOSS [training: 0.1697810682285966 | validation: 0.12754055340104709]
	TIME [epoch: 8.43 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16514605486914885		[learning rate: 0.00014011]
	Learning Rate: 0.000140112
	LOSS [training: 0.16514605486914885 | validation: 0.1539328027630714]
	TIME [epoch: 8.43 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17773318106170882		[learning rate: 0.0001396]
	Learning Rate: 0.000139603
	LOSS [training: 0.17773318106170882 | validation: 0.131761786694821]
	TIME [epoch: 8.45 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16908819516914633		[learning rate: 0.0001391]
	Learning Rate: 0.000139096
	LOSS [training: 0.16908819516914633 | validation: 0.12274736104581549]
	TIME [epoch: 8.42 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16584652738049216		[learning rate: 0.00013859]
	Learning Rate: 0.000138592
	LOSS [training: 0.16584652738049216 | validation: 0.13748841461708156]
	TIME [epoch: 8.43 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17839121145121645		[learning rate: 0.00013809]
	Learning Rate: 0.000138089
	LOSS [training: 0.17839121145121645 | validation: 0.12287082764330227]
	TIME [epoch: 8.43 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17701765841397826		[learning rate: 0.00013759]
	Learning Rate: 0.000137587
	LOSS [training: 0.17701765841397826 | validation: 0.12942751281692877]
	TIME [epoch: 8.44 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16056042310532292		[learning rate: 0.00013709]
	Learning Rate: 0.000137088
	LOSS [training: 0.16056042310532292 | validation: 0.13483069540031764]
	TIME [epoch: 8.43 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17825003762509933		[learning rate: 0.00013659]
	Learning Rate: 0.000136591
	LOSS [training: 0.17825003762509933 | validation: 0.1321813695881103]
	TIME [epoch: 8.43 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18416695760687685		[learning rate: 0.00013609]
	Learning Rate: 0.000136095
	LOSS [training: 0.18416695760687685 | validation: 0.13598002326879385]
	TIME [epoch: 8.43 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17842942997118322		[learning rate: 0.0001356]
	Learning Rate: 0.000135601
	LOSS [training: 0.17842942997118322 | validation: 0.15359810657672301]
	TIME [epoch: 8.44 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16899552713352772		[learning rate: 0.00013511]
	Learning Rate: 0.000135109
	LOSS [training: 0.16899552713352772 | validation: 0.13577776234394798]
	TIME [epoch: 8.42 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1748714548438885		[learning rate: 0.00013462]
	Learning Rate: 0.000134619
	LOSS [training: 0.1748714548438885 | validation: 0.12514641101748297]
	TIME [epoch: 8.43 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17674895700148463		[learning rate: 0.00013413]
	Learning Rate: 0.00013413
	LOSS [training: 0.17674895700148463 | validation: 0.1485902861848562]
	TIME [epoch: 8.43 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18322993176519203		[learning rate: 0.00013364]
	Learning Rate: 0.000133643
	LOSS [training: 0.18322993176519203 | validation: 0.14237838191416546]
	TIME [epoch: 8.46 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16617311530411358		[learning rate: 0.00013316]
	Learning Rate: 0.000133158
	LOSS [training: 0.16617311530411358 | validation: 0.1416751245796565]
	TIME [epoch: 8.42 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1801656913519322		[learning rate: 0.00013268]
	Learning Rate: 0.000132675
	LOSS [training: 0.1801656913519322 | validation: 0.12914355456846535]
	TIME [epoch: 8.42 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16644409980662359		[learning rate: 0.00013219]
	Learning Rate: 0.000132194
	LOSS [training: 0.16644409980662359 | validation: 0.12025546958779638]
	TIME [epoch: 8.43 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17424456660134		[learning rate: 0.00013171]
	Learning Rate: 0.000131714
	LOSS [training: 0.17424456660134 | validation: 0.14065618250158307]
	TIME [epoch: 8.45 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1694117974005846		[learning rate: 0.00013124]
	Learning Rate: 0.000131236
	LOSS [training: 0.1694117974005846 | validation: 0.12788915614097998]
	TIME [epoch: 8.43 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17460961794425417		[learning rate: 0.00013076]
	Learning Rate: 0.00013076
	LOSS [training: 0.17460961794425417 | validation: 0.1257817059935465]
	TIME [epoch: 8.43 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1644680130052242		[learning rate: 0.00013029]
	Learning Rate: 0.000130285
	LOSS [training: 0.1644680130052242 | validation: 0.15039386659319182]
	TIME [epoch: 8.44 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17608156583614626		[learning rate: 0.00012981]
	Learning Rate: 0.000129812
	LOSS [training: 0.17608156583614626 | validation: 0.13268231266589406]
	TIME [epoch: 8.45 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16469187772026692		[learning rate: 0.00012934]
	Learning Rate: 0.000129341
	LOSS [training: 0.16469187772026692 | validation: 0.11894402257615858]
	TIME [epoch: 8.43 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17582562554662926		[learning rate: 0.00012887]
	Learning Rate: 0.000128872
	LOSS [training: 0.17582562554662926 | validation: 0.14247992136506646]
	TIME [epoch: 8.43 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16335164639346317		[learning rate: 0.0001284]
	Learning Rate: 0.000128404
	LOSS [training: 0.16335164639346317 | validation: 0.12480977957987882]
	TIME [epoch: 8.44 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1728110186667326		[learning rate: 0.00012794]
	Learning Rate: 0.000127938
	LOSS [training: 0.1728110186667326 | validation: 0.1329348824633692]
	TIME [epoch: 8.44 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1720845450057215		[learning rate: 0.00012747]
	Learning Rate: 0.000127474
	LOSS [training: 0.1720845450057215 | validation: 0.12564498663482573]
	TIME [epoch: 8.43 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16214529657386978		[learning rate: 0.00012701]
	Learning Rate: 0.000127011
	LOSS [training: 0.16214529657386978 | validation: 0.12349712263489146]
	TIME [epoch: 8.42 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1691253282924683		[learning rate: 0.00012655]
	Learning Rate: 0.00012655
	LOSS [training: 0.1691253282924683 | validation: 0.12438004896978808]
	TIME [epoch: 8.45 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17773249553000928		[learning rate: 0.00012609]
	Learning Rate: 0.000126091
	LOSS [training: 0.17773249553000928 | validation: 0.14207954683122168]
	TIME [epoch: 8.43 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1740044056552479		[learning rate: 0.00012563]
	Learning Rate: 0.000125633
	LOSS [training: 0.1740044056552479 | validation: 0.13858553694420214]
	TIME [epoch: 8.43 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16981558574614766		[learning rate: 0.00012518]
	Learning Rate: 0.000125178
	LOSS [training: 0.16981558574614766 | validation: 0.13336340659479287]
	TIME [epoch: 8.43 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1698808025002318		[learning rate: 0.00012472]
	Learning Rate: 0.000124723
	LOSS [training: 0.1698808025002318 | validation: 0.13056212683802798]
	TIME [epoch: 8.44 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16333886162602534		[learning rate: 0.00012427]
	Learning Rate: 0.000124271
	LOSS [training: 0.16333886162602534 | validation: 0.12431657580277024]
	TIME [epoch: 8.44 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17173771075816097		[learning rate: 0.00012382]
	Learning Rate: 0.00012382
	LOSS [training: 0.17173771075816097 | validation: 0.1433560628639206]
	TIME [epoch: 8.42 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1734489361247017		[learning rate: 0.00012337]
	Learning Rate: 0.00012337
	LOSS [training: 0.1734489361247017 | validation: 0.12542495034805226]
	TIME [epoch: 8.43 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15803751314279163		[learning rate: 0.00012292]
	Learning Rate: 0.000122923
	LOSS [training: 0.15803751314279163 | validation: 0.12386149302467414]
	TIME [epoch: 8.45 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16943708497271764		[learning rate: 0.00012248]
	Learning Rate: 0.000122476
	LOSS [training: 0.16943708497271764 | validation: 0.12334144786114512]
	TIME [epoch: 8.44 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17446530205450822		[learning rate: 0.00012203]
	Learning Rate: 0.000122032
	LOSS [training: 0.17446530205450822 | validation: 0.126651558708455]
	TIME [epoch: 8.44 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16974640920407605		[learning rate: 0.00012159]
	Learning Rate: 0.000121589
	LOSS [training: 0.16974640920407605 | validation: 0.11701960320378241]
	TIME [epoch: 8.43 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17030410233197932		[learning rate: 0.00012115]
	Learning Rate: 0.000121148
	LOSS [training: 0.17030410233197932 | validation: 0.11437668324859196]
	TIME [epoch: 8.45 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1579875217757242		[learning rate: 0.00012071]
	Learning Rate: 0.000120708
	LOSS [training: 0.1579875217757242 | validation: 0.13148443885190347]
	TIME [epoch: 8.44 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1668105685699712		[learning rate: 0.00012027]
	Learning Rate: 0.00012027
	LOSS [training: 0.1668105685699712 | validation: 0.1204394705492467]
	TIME [epoch: 8.44 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16703568847969721		[learning rate: 0.00011983]
	Learning Rate: 0.000119834
	LOSS [training: 0.16703568847969721 | validation: 0.1088621337504838]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_1317.pth
	Model improved!!!
EPOCH 1318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17076989483166022		[learning rate: 0.0001194]
	Learning Rate: 0.000119399
	LOSS [training: 0.17076989483166022 | validation: 0.1511334707387633]
	TIME [epoch: 8.45 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17055097466508592		[learning rate: 0.00011897]
	Learning Rate: 0.000118966
	LOSS [training: 0.17055097466508592 | validation: 0.12713424810941806]
	TIME [epoch: 8.44 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17658072535458028		[learning rate: 0.00011853]
	Learning Rate: 0.000118534
	LOSS [training: 0.17658072535458028 | validation: 0.1260449425001247]
	TIME [epoch: 8.42 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15660957316252333		[learning rate: 0.0001181]
	Learning Rate: 0.000118104
	LOSS [training: 0.15660957316252333 | validation: 0.14006043323391845]
	TIME [epoch: 8.44 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18495855838517633		[learning rate: 0.00011767]
	Learning Rate: 0.000117675
	LOSS [training: 0.18495855838517633 | validation: 0.12166012401626691]
	TIME [epoch: 8.44 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16879643691140406		[learning rate: 0.00011725]
	Learning Rate: 0.000117248
	LOSS [training: 0.16879643691140406 | validation: 0.1317119032351847]
	TIME [epoch: 8.43 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17260205536523182		[learning rate: 0.00011682]
	Learning Rate: 0.000116822
	LOSS [training: 0.17260205536523182 | validation: 0.11241092356062579]
	TIME [epoch: 8.43 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16788900910615695		[learning rate: 0.0001164]
	Learning Rate: 0.000116398
	LOSS [training: 0.16788900910615695 | validation: 0.12436774702447187]
	TIME [epoch: 8.43 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16772092451696508		[learning rate: 0.00011598]
	Learning Rate: 0.000115976
	LOSS [training: 0.16772092451696508 | validation: 0.11592199542455942]
	TIME [epoch: 8.45 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17452484585488667		[learning rate: 0.00011556]
	Learning Rate: 0.000115555
	LOSS [training: 0.17452484585488667 | validation: 0.128727347208001]
	TIME [epoch: 8.45 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17032480211249673		[learning rate: 0.00011514]
	Learning Rate: 0.000115136
	LOSS [training: 0.17032480211249673 | validation: 0.12201479155682735]
	TIME [epoch: 8.43 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15834342707771137		[learning rate: 0.00011472]
	Learning Rate: 0.000114718
	LOSS [training: 0.15834342707771137 | validation: 0.12424882879075831]
	TIME [epoch: 8.43 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15937142871887494		[learning rate: 0.0001143]
	Learning Rate: 0.000114302
	LOSS [training: 0.15937142871887494 | validation: 0.12283090666820484]
	TIME [epoch: 8.44 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16793270526974666		[learning rate: 0.00011389]
	Learning Rate: 0.000113887
	LOSS [training: 0.16793270526974666 | validation: 0.13926321190171165]
	TIME [epoch: 8.44 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17206174589887224		[learning rate: 0.00011347]
	Learning Rate: 0.000113474
	LOSS [training: 0.17206174589887224 | validation: 0.13041245878471838]
	TIME [epoch: 8.43 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.174337254128774		[learning rate: 0.00011306]
	Learning Rate: 0.000113062
	LOSS [training: 0.174337254128774 | validation: 0.12217002429771413]
	TIME [epoch: 8.44 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16339844584914248		[learning rate: 0.00011265]
	Learning Rate: 0.000112651
	LOSS [training: 0.16339844584914248 | validation: 0.135817970354772]
	TIME [epoch: 8.45 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16629292766039347		[learning rate: 0.00011224]
	Learning Rate: 0.000112243
	LOSS [training: 0.16629292766039347 | validation: 0.13142200614531296]
	TIME [epoch: 8.44 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16386965264285322		[learning rate: 0.00011184]
	Learning Rate: 0.000111835
	LOSS [training: 0.16386965264285322 | validation: 0.1301484774086691]
	TIME [epoch: 8.42 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17454285207051226		[learning rate: 0.00011143]
	Learning Rate: 0.000111429
	LOSS [training: 0.17454285207051226 | validation: 0.14430890710423433]
	TIME [epoch: 8.44 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17369897754503844		[learning rate: 0.00011103]
	Learning Rate: 0.000111025
	LOSS [training: 0.17369897754503844 | validation: 0.12079517350951313]
	TIME [epoch: 8.44 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17358715065947838		[learning rate: 0.00011062]
	Learning Rate: 0.000110622
	LOSS [training: 0.17358715065947838 | validation: 0.12513859060344085]
	TIME [epoch: 8.44 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16047716091568978		[learning rate: 0.00011022]
	Learning Rate: 0.000110221
	LOSS [training: 0.16047716091568978 | validation: 0.13190753886805928]
	TIME [epoch: 8.42 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1648103353558105		[learning rate: 0.00010982]
	Learning Rate: 0.000109821
	LOSS [training: 0.1648103353558105 | validation: 0.12938282002789142]
	TIME [epoch: 8.43 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1665365418764984		[learning rate: 0.00010942]
	Learning Rate: 0.000109422
	LOSS [training: 0.1665365418764984 | validation: 0.12394828764686869]
	TIME [epoch: 8.44 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17012478674281617		[learning rate: 0.00010903]
	Learning Rate: 0.000109025
	LOSS [training: 0.17012478674281617 | validation: 0.12682756314183802]
	TIME [epoch: 8.44 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16485896185967341		[learning rate: 0.00010863]
	Learning Rate: 0.000108629
	LOSS [training: 0.16485896185967341 | validation: 0.1280667911792214]
	TIME [epoch: 8.42 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16644590297591103		[learning rate: 0.00010824]
	Learning Rate: 0.000108235
	LOSS [training: 0.16644590297591103 | validation: 0.13787626266419584]
	TIME [epoch: 8.43 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16516177026004425		[learning rate: 0.00010784]
	Learning Rate: 0.000107842
	LOSS [training: 0.16516177026004425 | validation: 0.1178053990521327]
	TIME [epoch: 8.45 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15997667186998518		[learning rate: 0.00010745]
	Learning Rate: 0.000107451
	LOSS [training: 0.15997667186998518 | validation: 0.12561244097441088]
	TIME [epoch: 8.43 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17259310540056205		[learning rate: 0.00010706]
	Learning Rate: 0.000107061
	LOSS [training: 0.17259310540056205 | validation: 0.1272341497522781]
	TIME [epoch: 8.42 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.162388731891596		[learning rate: 0.00010667]
	Learning Rate: 0.000106673
	LOSS [training: 0.162388731891596 | validation: 0.12685527233662103]
	TIME [epoch: 8.43 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16445141261076085		[learning rate: 0.00010629]
	Learning Rate: 0.000106285
	LOSS [training: 0.16445141261076085 | validation: 0.13194001943625427]
	TIME [epoch: 8.46 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16339268424036252		[learning rate: 0.0001059]
	Learning Rate: 0.0001059
	LOSS [training: 0.16339268424036252 | validation: 0.13610043828897275]
	TIME [epoch: 8.43 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17551063232601632		[learning rate: 0.00010552]
	Learning Rate: 0.000105515
	LOSS [training: 0.17551063232601632 | validation: 0.14252695886399622]
	TIME [epoch: 8.43 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16208192419858516		[learning rate: 0.00010513]
	Learning Rate: 0.000105132
	LOSS [training: 0.16208192419858516 | validation: 0.13945052608790387]
	TIME [epoch: 8.42 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1765147686256354		[learning rate: 0.00010475]
	Learning Rate: 0.000104751
	LOSS [training: 0.1765147686256354 | validation: 0.12476635457282106]
	TIME [epoch: 8.45 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1687390681994171		[learning rate: 0.00010437]
	Learning Rate: 0.000104371
	LOSS [training: 0.1687390681994171 | validation: 0.12987182187948393]
	TIME [epoch: 8.43 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17102632674797608		[learning rate: 0.00010399]
	Learning Rate: 0.000103992
	LOSS [training: 0.17102632674797608 | validation: 0.13971272727972303]
	TIME [epoch: 8.43 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16752490822821764		[learning rate: 0.00010361]
	Learning Rate: 0.000103615
	LOSS [training: 0.16752490822821764 | validation: 0.13309344022523004]
	TIME [epoch: 8.41 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17026961067284235		[learning rate: 0.00010324]
	Learning Rate: 0.000103239
	LOSS [training: 0.17026961067284235 | validation: 0.1289867153347122]
	TIME [epoch: 8.45 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18042040527395004		[learning rate: 0.00010286]
	Learning Rate: 0.000102864
	LOSS [training: 0.18042040527395004 | validation: 0.13535614436036775]
	TIME [epoch: 8.42 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1634116161090558		[learning rate: 0.00010249]
	Learning Rate: 0.000102491
	LOSS [training: 0.1634116161090558 | validation: 0.13416442650486426]
	TIME [epoch: 8.43 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16691108758634185		[learning rate: 0.00010212]
	Learning Rate: 0.000102119
	LOSS [training: 0.16691108758634185 | validation: 0.1306837110520673]
	TIME [epoch: 8.42 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1699722628600837		[learning rate: 0.00010175]
	Learning Rate: 0.000101748
	LOSS [training: 0.1699722628600837 | validation: 0.1285606373074597]
	TIME [epoch: 8.45 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16217140297909602		[learning rate: 0.00010138]
	Learning Rate: 0.000101379
	LOSS [training: 0.16217140297909602 | validation: 0.13920726736098482]
	TIME [epoch: 8.43 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17219536286480353		[learning rate: 0.00010101]
	Learning Rate: 0.000101011
	LOSS [training: 0.17219536286480353 | validation: 0.11483918695798177]
	TIME [epoch: 8.42 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17340158591697807		[learning rate: 0.00010064]
	Learning Rate: 0.000100644
	LOSS [training: 0.17340158591697807 | validation: 0.13705583794501563]
	TIME [epoch: 8.43 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17633988844127052		[learning rate: 0.00010028]
	Learning Rate: 0.000100279
	LOSS [training: 0.17633988844127052 | validation: 0.1280140812894596]
	TIME [epoch: 8.45 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17668637535315485		[learning rate: 9.9915e-05]
	Learning Rate: 9.99152e-05
	LOSS [training: 0.17668637535315485 | validation: 0.14111710406061173]
	TIME [epoch: 8.43 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1751453988928308		[learning rate: 9.9553e-05]
	Learning Rate: 9.95526e-05
	LOSS [training: 0.1751453988928308 | validation: 0.14042781419693376]
	TIME [epoch: 8.42 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17103019441649595		[learning rate: 9.9191e-05]
	Learning Rate: 9.91913e-05
	LOSS [training: 0.17103019441649595 | validation: 0.1281083745494251]
	TIME [epoch: 8.43 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16535775223516203		[learning rate: 9.8831e-05]
	Learning Rate: 9.88314e-05
	LOSS [training: 0.16535775223516203 | validation: 0.12945509417890194]
	TIME [epoch: 8.44 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1688570579741067		[learning rate: 9.8473e-05]
	Learning Rate: 9.84727e-05
	LOSS [training: 0.1688570579741067 | validation: 0.12748388620654086]
	TIME [epoch: 8.43 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1665101691957531		[learning rate: 9.8115e-05]
	Learning Rate: 9.81153e-05
	LOSS [training: 0.1665101691957531 | validation: 0.1299104858610948]
	TIME [epoch: 8.43 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16581699024583857		[learning rate: 9.7759e-05]
	Learning Rate: 9.77592e-05
	LOSS [training: 0.16581699024583857 | validation: 0.14357076837374128]
	TIME [epoch: 8.43 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1761604139275094		[learning rate: 9.7404e-05]
	Learning Rate: 9.74045e-05
	LOSS [training: 0.1761604139275094 | validation: 0.13010579320202062]
	TIME [epoch: 8.45 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17067496918798533		[learning rate: 9.7051e-05]
	Learning Rate: 9.7051e-05
	LOSS [training: 0.17067496918798533 | validation: 0.1306782486519048]
	TIME [epoch: 8.42 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16279269958053755		[learning rate: 9.6699e-05]
	Learning Rate: 9.66988e-05
	LOSS [training: 0.16279269958053755 | validation: 0.1436452680213507]
	TIME [epoch: 8.43 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17173676098863208		[learning rate: 9.6348e-05]
	Learning Rate: 9.63479e-05
	LOSS [training: 0.17173676098863208 | validation: 0.13122054062320465]
	TIME [epoch: 8.42 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16473892970252144		[learning rate: 9.5998e-05]
	Learning Rate: 9.59982e-05
	LOSS [training: 0.16473892970252144 | validation: 0.125232804328758]
	TIME [epoch: 8.45 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1645069080725136		[learning rate: 9.565e-05]
	Learning Rate: 9.56499e-05
	LOSS [training: 0.1645069080725136 | validation: 0.13188114497816292]
	TIME [epoch: 8.43 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17080250678394984		[learning rate: 9.5303e-05]
	Learning Rate: 9.53027e-05
	LOSS [training: 0.17080250678394984 | validation: 0.13423160030945036]
	TIME [epoch: 8.43 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16386452926385792		[learning rate: 9.4957e-05]
	Learning Rate: 9.49569e-05
	LOSS [training: 0.16386452926385792 | validation: 0.14288424135271532]
	TIME [epoch: 8.43 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1674518311986907		[learning rate: 9.4612e-05]
	Learning Rate: 9.46122e-05
	LOSS [training: 0.1674518311986907 | validation: 0.13272354585990762]
	TIME [epoch: 8.44 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1593767663012382		[learning rate: 9.4269e-05]
	Learning Rate: 9.42689e-05
	LOSS [training: 0.1593767663012382 | validation: 0.12123449536751156]
	TIME [epoch: 8.43 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17193184768093833		[learning rate: 9.3927e-05]
	Learning Rate: 9.39268e-05
	LOSS [training: 0.17193184768093833 | validation: 0.1444287966117623]
	TIME [epoch: 8.42 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16491915294363663		[learning rate: 9.3586e-05]
	Learning Rate: 9.35859e-05
	LOSS [training: 0.16491915294363663 | validation: 0.13778174619219533]
	TIME [epoch: 8.45 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17049926617155825		[learning rate: 9.3246e-05]
	Learning Rate: 9.32463e-05
	LOSS [training: 0.17049926617155825 | validation: 0.12495362889928262]
	TIME [epoch: 8.44 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17590943520832597		[learning rate: 9.2908e-05]
	Learning Rate: 9.29079e-05
	LOSS [training: 0.17590943520832597 | validation: 0.1388453537506007]
	TIME [epoch: 8.42 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16354890226602098		[learning rate: 9.2571e-05]
	Learning Rate: 9.25707e-05
	LOSS [training: 0.16354890226602098 | validation: 0.13541736266601376]
	TIME [epoch: 8.43 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1641160502387041		[learning rate: 9.2235e-05]
	Learning Rate: 9.22348e-05
	LOSS [training: 0.1641160502387041 | validation: 0.13156664323660822]
	TIME [epoch: 8.45 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16930486807684683		[learning rate: 9.19e-05]
	Learning Rate: 9.19001e-05
	LOSS [training: 0.16930486807684683 | validation: 0.13902465716407325]
	TIME [epoch: 8.45 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15906266909994937		[learning rate: 9.1567e-05]
	Learning Rate: 9.15665e-05
	LOSS [training: 0.15906266909994937 | validation: 0.13066113154587727]
	TIME [epoch: 8.44 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18051539904863045		[learning rate: 9.1234e-05]
	Learning Rate: 9.12343e-05
	LOSS [training: 0.18051539904863045 | validation: 0.14176175440672678]
	TIME [epoch: 8.44 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1636951342300697		[learning rate: 9.0903e-05]
	Learning Rate: 9.09031e-05
	LOSS [training: 0.1636951342300697 | validation: 0.12714401652514445]
	TIME [epoch: 8.46 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16785617215640145		[learning rate: 9.0573e-05]
	Learning Rate: 9.05733e-05
	LOSS [training: 0.16785617215640145 | validation: 0.13115742619334525]
	TIME [epoch: 8.44 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16472215077821928		[learning rate: 9.0245e-05]
	Learning Rate: 9.02446e-05
	LOSS [training: 0.16472215077821928 | validation: 0.12289894806297834]
	TIME [epoch: 8.44 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16549933927849872		[learning rate: 8.9917e-05]
	Learning Rate: 8.99171e-05
	LOSS [training: 0.16549933927849872 | validation: 0.14057825992645845]
	TIME [epoch: 8.43 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17210765609669237		[learning rate: 8.9591e-05]
	Learning Rate: 8.95908e-05
	LOSS [training: 0.17210765609669237 | validation: 0.1268633945061033]
	TIME [epoch: 8.45 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16769768538552263		[learning rate: 8.9266e-05]
	Learning Rate: 8.92656e-05
	LOSS [training: 0.16769768538552263 | validation: 0.12232404330879876]
	TIME [epoch: 8.42 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16322753565499218		[learning rate: 8.8942e-05]
	Learning Rate: 8.89417e-05
	LOSS [training: 0.16322753565499218 | validation: 0.14428199460719676]
	TIME [epoch: 8.43 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17647960729228465		[learning rate: 8.8619e-05]
	Learning Rate: 8.86189e-05
	LOSS [training: 0.17647960729228465 | validation: 0.12302340856274815]
	TIME [epoch: 8.43 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16436898730165034		[learning rate: 8.8297e-05]
	Learning Rate: 8.82973e-05
	LOSS [training: 0.16436898730165034 | validation: 0.12062574523060007]
	TIME [epoch: 8.44 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17580892833998404		[learning rate: 8.7977e-05]
	Learning Rate: 8.79769e-05
	LOSS [training: 0.17580892833998404 | validation: 0.1367766379467052]
	TIME [epoch: 8.44 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17141996516713892		[learning rate: 8.7658e-05]
	Learning Rate: 8.76576e-05
	LOSS [training: 0.17141996516713892 | validation: 0.12310783947938714]
	TIME [epoch: 8.42 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16112636951148931		[learning rate: 8.7339e-05]
	Learning Rate: 8.73395e-05
	LOSS [training: 0.16112636951148931 | validation: 0.13745023228078115]
	TIME [epoch: 8.42 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16047822725800767		[learning rate: 8.7023e-05]
	Learning Rate: 8.70225e-05
	LOSS [training: 0.16047822725800767 | validation: 0.11619740539672896]
	TIME [epoch: 8.44 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16200333928945043		[learning rate: 8.6707e-05]
	Learning Rate: 8.67067e-05
	LOSS [training: 0.16200333928945043 | validation: 0.13864332503633503]
	TIME [epoch: 8.43 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16654376377393126		[learning rate: 8.6392e-05]
	Learning Rate: 8.63921e-05
	LOSS [training: 0.16654376377393126 | validation: 0.12164295016989424]
	TIME [epoch: 8.42 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15924883383621563		[learning rate: 8.6079e-05]
	Learning Rate: 8.60785e-05
	LOSS [training: 0.15924883383621563 | validation: 0.12835315836526567]
	TIME [epoch: 8.43 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16336875087026007		[learning rate: 8.5766e-05]
	Learning Rate: 8.57661e-05
	LOSS [training: 0.16336875087026007 | validation: 0.12320223984872572]
	TIME [epoch: 8.44 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17853630211985752		[learning rate: 8.5455e-05]
	Learning Rate: 8.54549e-05
	LOSS [training: 0.17853630211985752 | validation: 0.1424642306003833]
	TIME [epoch: 8.43 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1762138405712962		[learning rate: 8.5145e-05]
	Learning Rate: 8.51448e-05
	LOSS [training: 0.1762138405712962 | validation: 0.14457265653946452]
	TIME [epoch: 8.42 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17054602237950922		[learning rate: 8.4836e-05]
	Learning Rate: 8.48358e-05
	LOSS [training: 0.17054602237950922 | validation: 0.13117622350687122]
	TIME [epoch: 8.42 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1704725555473106		[learning rate: 8.4528e-05]
	Learning Rate: 8.45279e-05
	LOSS [training: 0.1704725555473106 | validation: 0.1303320335854209]
	TIME [epoch: 8.45 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16504582496066483		[learning rate: 8.4221e-05]
	Learning Rate: 8.42211e-05
	LOSS [training: 0.16504582496066483 | validation: 0.1297012007245392]
	TIME [epoch: 8.43 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16458059242960477		[learning rate: 8.3916e-05]
	Learning Rate: 8.39155e-05
	LOSS [training: 0.16458059242960477 | validation: 0.12357558418847972]
	TIME [epoch: 8.43 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16293663029842395		[learning rate: 8.3611e-05]
	Learning Rate: 8.3611e-05
	LOSS [training: 0.16293663029842395 | validation: 0.12582856277039933]
	TIME [epoch: 8.43 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18349671341721047		[learning rate: 8.3308e-05]
	Learning Rate: 8.33075e-05
	LOSS [training: 0.18349671341721047 | validation: 0.13948116914243208]
	TIME [epoch: 8.45 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17238623042065424		[learning rate: 8.3005e-05]
	Learning Rate: 8.30052e-05
	LOSS [training: 0.17238623042065424 | validation: 0.1270899812471478]
	TIME [epoch: 8.43 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16773235251602361		[learning rate: 8.2704e-05]
	Learning Rate: 8.2704e-05
	LOSS [training: 0.16773235251602361 | validation: 0.13621339053461984]
	TIME [epoch: 8.43 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1650807380875023		[learning rate: 8.2404e-05]
	Learning Rate: 8.24038e-05
	LOSS [training: 0.1650807380875023 | validation: 0.12607976298343013]
	TIME [epoch: 8.43 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16605352090486197		[learning rate: 8.2105e-05]
	Learning Rate: 8.21048e-05
	LOSS [training: 0.16605352090486197 | validation: 0.1308102905765957]
	TIME [epoch: 8.45 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16890981696444993		[learning rate: 8.1807e-05]
	Learning Rate: 8.18068e-05
	LOSS [training: 0.16890981696444993 | validation: 0.12231263300477921]
	TIME [epoch: 8.42 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17137683927772301		[learning rate: 8.151e-05]
	Learning Rate: 8.15099e-05
	LOSS [training: 0.17137683927772301 | validation: 0.13219552203044282]
	TIME [epoch: 8.43 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16641288100472784		[learning rate: 8.1214e-05]
	Learning Rate: 8.12142e-05
	LOSS [training: 0.16641288100472784 | validation: 0.13135000835407673]
	TIME [epoch: 8.43 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15853238649477142		[learning rate: 8.0919e-05]
	Learning Rate: 8.09194e-05
	LOSS [training: 0.15853238649477142 | validation: 0.12017734863077348]
	TIME [epoch: 8.46 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16535764169840025		[learning rate: 8.0626e-05]
	Learning Rate: 8.06257e-05
	LOSS [training: 0.16535764169840025 | validation: 0.1319420342891418]
	TIME [epoch: 8.42 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1683862749653283		[learning rate: 8.0333e-05]
	Learning Rate: 8.03331e-05
	LOSS [training: 0.1683862749653283 | validation: 0.12581388360323298]
	TIME [epoch: 8.43 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15984584082011924		[learning rate: 8.0042e-05]
	Learning Rate: 8.00416e-05
	LOSS [training: 0.15984584082011924 | validation: 0.1312308914124211]
	TIME [epoch: 8.44 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16719751793707424		[learning rate: 7.9751e-05]
	Learning Rate: 7.97511e-05
	LOSS [training: 0.16719751793707424 | validation: 0.14246487645032746]
	TIME [epoch: 8.44 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16731313593640337		[learning rate: 7.9462e-05]
	Learning Rate: 7.94617e-05
	LOSS [training: 0.16731313593640337 | validation: 0.1452588787133751]
	TIME [epoch: 8.42 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16081253266148313		[learning rate: 7.9173e-05]
	Learning Rate: 7.91733e-05
	LOSS [training: 0.16081253266148313 | validation: 0.12987645044287693]
	TIME [epoch: 8.42 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16573802587993466		[learning rate: 7.8886e-05]
	Learning Rate: 7.8886e-05
	LOSS [training: 0.16573802587993466 | validation: 0.1210715422585147]
	TIME [epoch: 8.44 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17061942792819224		[learning rate: 7.86e-05]
	Learning Rate: 7.85997e-05
	LOSS [training: 0.17061942792819224 | validation: 0.12012423370734068]
	TIME [epoch: 8.44 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17051011889767662		[learning rate: 7.8315e-05]
	Learning Rate: 7.83145e-05
	LOSS [training: 0.17051011889767662 | validation: 0.15244755157265405]
	TIME [epoch: 8.43 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17053878993897148		[learning rate: 7.803e-05]
	Learning Rate: 7.80303e-05
	LOSS [training: 0.17053878993897148 | validation: 0.14160918032649022]
	TIME [epoch: 8.42 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16501044281540006		[learning rate: 7.7747e-05]
	Learning Rate: 7.77471e-05
	LOSS [training: 0.16501044281540006 | validation: 0.12204071762947208]
	TIME [epoch: 8.43 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15961821551829852		[learning rate: 7.7465e-05]
	Learning Rate: 7.7465e-05
	LOSS [training: 0.15961821551829852 | validation: 0.1113315260755375]
	TIME [epoch: 8.42 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16671422659255453		[learning rate: 7.7184e-05]
	Learning Rate: 7.71838e-05
	LOSS [training: 0.16671422659255453 | validation: 0.1332131965786239]
	TIME [epoch: 8.43 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16473787040862972		[learning rate: 7.6904e-05]
	Learning Rate: 7.69037e-05
	LOSS [training: 0.16473787040862972 | validation: 0.12884190852819472]
	TIME [epoch: 8.42 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1631053575413184		[learning rate: 7.6625e-05]
	Learning Rate: 7.66246e-05
	LOSS [training: 0.1631053575413184 | validation: 0.11616461253575924]
	TIME [epoch: 8.44 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16554413053568884		[learning rate: 7.6347e-05]
	Learning Rate: 7.63466e-05
	LOSS [training: 0.16554413053568884 | validation: 0.11646556761324875]
	TIME [epoch: 8.43 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15697138269505245		[learning rate: 7.607e-05]
	Learning Rate: 7.60695e-05
	LOSS [training: 0.15697138269505245 | validation: 0.11486101331526136]
	TIME [epoch: 8.42 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16506733831933576		[learning rate: 7.5793e-05]
	Learning Rate: 7.57935e-05
	LOSS [training: 0.16506733831933576 | validation: 0.11973168293867875]
	TIME [epoch: 8.43 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16609986061860676		[learning rate: 7.5518e-05]
	Learning Rate: 7.55184e-05
	LOSS [training: 0.16609986061860676 | validation: 0.12034200183662172]
	TIME [epoch: 8.45 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16656892649902352		[learning rate: 7.5244e-05]
	Learning Rate: 7.52443e-05
	LOSS [training: 0.16656892649902352 | validation: 0.114084804576602]
	TIME [epoch: 8.43 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1642176168665127		[learning rate: 7.4971e-05]
	Learning Rate: 7.49712e-05
	LOSS [training: 0.1642176168665127 | validation: 0.12630714887472672]
	TIME [epoch: 8.42 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16501163585719641		[learning rate: 7.4699e-05]
	Learning Rate: 7.46992e-05
	LOSS [training: 0.16501163585719641 | validation: 0.1277495564700957]
	TIME [epoch: 8.43 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16692862774028688		[learning rate: 7.4428e-05]
	Learning Rate: 7.44281e-05
	LOSS [training: 0.16692862774028688 | validation: 0.12770388232214946]
	TIME [epoch: 8.45 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1647782650073964		[learning rate: 7.4158e-05]
	Learning Rate: 7.4158e-05
	LOSS [training: 0.1647782650073964 | validation: 0.12877247259494817]
	TIME [epoch: 8.43 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16936052297910179		[learning rate: 7.3889e-05]
	Learning Rate: 7.38889e-05
	LOSS [training: 0.16936052297910179 | validation: 0.13267216420914352]
	TIME [epoch: 8.42 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16008585306500883		[learning rate: 7.3621e-05]
	Learning Rate: 7.36207e-05
	LOSS [training: 0.16008585306500883 | validation: 0.13051577590424548]
	TIME [epoch: 8.42 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1610613083760155		[learning rate: 7.3354e-05]
	Learning Rate: 7.33536e-05
	LOSS [training: 0.1610613083760155 | validation: 0.14085044242682665]
	TIME [epoch: 8.45 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1665639395321579		[learning rate: 7.3087e-05]
	Learning Rate: 7.30873e-05
	LOSS [training: 0.1665639395321579 | validation: 0.11350601504478268]
	TIME [epoch: 8.42 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16563724690359058		[learning rate: 7.2822e-05]
	Learning Rate: 7.28221e-05
	LOSS [training: 0.16563724690359058 | validation: 0.12549073524213078]
	TIME [epoch: 8.43 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16559892856293015		[learning rate: 7.2558e-05]
	Learning Rate: 7.25578e-05
	LOSS [training: 0.16559892856293015 | validation: 0.12757809944212756]
	TIME [epoch: 8.42 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15870611413912544		[learning rate: 7.2295e-05]
	Learning Rate: 7.22945e-05
	LOSS [training: 0.15870611413912544 | validation: 0.11998887556669127]
	TIME [epoch: 8.45 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.169086221805768		[learning rate: 7.2032e-05]
	Learning Rate: 7.20321e-05
	LOSS [training: 0.169086221805768 | validation: 0.13485461928719272]
	TIME [epoch: 8.43 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16311630000085525		[learning rate: 7.1771e-05]
	Learning Rate: 7.17707e-05
	LOSS [training: 0.16311630000085525 | validation: 0.13518318692071488]
	TIME [epoch: 8.42 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15713039743338536		[learning rate: 7.151e-05]
	Learning Rate: 7.15103e-05
	LOSS [training: 0.15713039743338536 | validation: 0.1259586269566068]
	TIME [epoch: 8.43 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15806912478804236		[learning rate: 7.1251e-05]
	Learning Rate: 7.12508e-05
	LOSS [training: 0.15806912478804236 | validation: 0.13694055647548822]
	TIME [epoch: 8.43 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1852723864268323		[learning rate: 7.0992e-05]
	Learning Rate: 7.09922e-05
	LOSS [training: 0.1852723864268323 | validation: 0.1331098157005145]
	TIME [epoch: 8.43 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16236966014367746		[learning rate: 7.0735e-05]
	Learning Rate: 7.07345e-05
	LOSS [training: 0.16236966014367746 | validation: 0.1232825307280493]
	TIME [epoch: 8.43 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1694451357860164		[learning rate: 7.0478e-05]
	Learning Rate: 7.04778e-05
	LOSS [training: 0.1694451357860164 | validation: 0.13835104803975845]
	TIME [epoch: 8.42 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1691854257618064		[learning rate: 7.0222e-05]
	Learning Rate: 7.02221e-05
	LOSS [training: 0.1691854257618064 | validation: 0.1481117180094768]
	TIME [epoch: 8.44 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17444008909952663		[learning rate: 6.9967e-05]
	Learning Rate: 6.99672e-05
	LOSS [training: 0.17444008909952663 | validation: 0.14331144093021977]
	TIME [epoch: 8.45 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17093271510047806		[learning rate: 6.9713e-05]
	Learning Rate: 6.97133e-05
	LOSS [training: 0.17093271510047806 | validation: 0.12486528468775276]
	TIME [epoch: 8.43 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16056369974859377		[learning rate: 6.946e-05]
	Learning Rate: 6.94603e-05
	LOSS [training: 0.16056369974859377 | validation: 0.1133524190564561]
	TIME [epoch: 8.43 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17546488814961356		[learning rate: 6.9208e-05]
	Learning Rate: 6.92083e-05
	LOSS [training: 0.17546488814961356 | validation: 0.1169439354630393]
	TIME [epoch: 8.43 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1601380979520345		[learning rate: 6.8957e-05]
	Learning Rate: 6.89571e-05
	LOSS [training: 0.1601380979520345 | validation: 0.12583221322307497]
	TIME [epoch: 8.42 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16303023297577884		[learning rate: 6.8707e-05]
	Learning Rate: 6.87068e-05
	LOSS [training: 0.16303023297577884 | validation: 0.11659747739187551]
	TIME [epoch: 8.41 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1623225428571391		[learning rate: 6.8457e-05]
	Learning Rate: 6.84575e-05
	LOSS [training: 0.1623225428571391 | validation: 0.12727608561296166]
	TIME [epoch: 8.43 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16640693077924534		[learning rate: 6.8209e-05]
	Learning Rate: 6.82091e-05
	LOSS [training: 0.16640693077924534 | validation: 0.12656824584955456]
	TIME [epoch: 8.45 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16695391992305864		[learning rate: 6.7962e-05]
	Learning Rate: 6.79615e-05
	LOSS [training: 0.16695391992305864 | validation: 0.12662416801763876]
	TIME [epoch: 8.43 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16982961928001564		[learning rate: 6.7715e-05]
	Learning Rate: 6.77149e-05
	LOSS [training: 0.16982961928001564 | validation: 0.13531075019087072]
	TIME [epoch: 8.42 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17324269384737565		[learning rate: 6.7469e-05]
	Learning Rate: 6.74692e-05
	LOSS [training: 0.17324269384737565 | validation: 0.13415059505173482]
	TIME [epoch: 8.43 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16608429732337407		[learning rate: 6.7224e-05]
	Learning Rate: 6.72243e-05
	LOSS [training: 0.16608429732337407 | validation: 0.11874541663555116]
	TIME [epoch: 8.44 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1638818070451493		[learning rate: 6.698e-05]
	Learning Rate: 6.69804e-05
	LOSS [training: 0.1638818070451493 | validation: 0.1313102399012201]
	TIME [epoch: 8.43 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16568246400210337		[learning rate: 6.6737e-05]
	Learning Rate: 6.67373e-05
	LOSS [training: 0.16568246400210337 | validation: 0.12931811010749916]
	TIME [epoch: 8.42 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1701286833745505		[learning rate: 6.6495e-05]
	Learning Rate: 6.64951e-05
	LOSS [training: 0.1701286833745505 | validation: 0.12397637077375896]
	TIME [epoch: 8.42 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16827338297604283		[learning rate: 6.6254e-05]
	Learning Rate: 6.62538e-05
	LOSS [training: 0.16827338297604283 | validation: 0.1317964076669191]
	TIME [epoch: 8.44 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16117234046083267		[learning rate: 6.6013e-05]
	Learning Rate: 6.60133e-05
	LOSS [training: 0.16117234046083267 | validation: 0.13568188714318472]
	TIME [epoch: 8.42 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16986007830613464		[learning rate: 6.5774e-05]
	Learning Rate: 6.57738e-05
	LOSS [training: 0.16986007830613464 | validation: 0.11171156624742358]
	TIME [epoch: 8.43 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16285056086490957		[learning rate: 6.5535e-05]
	Learning Rate: 6.55351e-05
	LOSS [training: 0.16285056086490957 | validation: 0.1196847326405657]
	TIME [epoch: 8.42 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16310173245231493		[learning rate: 6.5297e-05]
	Learning Rate: 6.52972e-05
	LOSS [training: 0.16310173245231493 | validation: 0.12825593007882308]
	TIME [epoch: 8.44 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15920440089277646		[learning rate: 6.506e-05]
	Learning Rate: 6.50603e-05
	LOSS [training: 0.15920440089277646 | validation: 0.1375857970374313]
	TIME [epoch: 8.42 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15630871064077204		[learning rate: 6.4824e-05]
	Learning Rate: 6.48242e-05
	LOSS [training: 0.15630871064077204 | validation: 0.1343995187541872]
	TIME [epoch: 8.43 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1556687461679042		[learning rate: 6.4589e-05]
	Learning Rate: 6.45889e-05
	LOSS [training: 0.1556687461679042 | validation: 0.1333230919043184]
	TIME [epoch: 8.44 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17083791669037315		[learning rate: 6.4354e-05]
	Learning Rate: 6.43545e-05
	LOSS [training: 0.17083791669037315 | validation: 0.13548741940812353]
	TIME [epoch: 8.45 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16085357975479017		[learning rate: 6.4121e-05]
	Learning Rate: 6.4121e-05
	LOSS [training: 0.16085357975479017 | validation: 0.12508526272452683]
	TIME [epoch: 8.42 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1596616570148795		[learning rate: 6.3888e-05]
	Learning Rate: 6.38883e-05
	LOSS [training: 0.1596616570148795 | validation: 0.12597243349087894]
	TIME [epoch: 8.42 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15508622872347771		[learning rate: 6.3656e-05]
	Learning Rate: 6.36564e-05
	LOSS [training: 0.15508622872347771 | validation: 0.12852998617739433]
	TIME [epoch: 8.44 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1647816822321058		[learning rate: 6.3425e-05]
	Learning Rate: 6.34254e-05
	LOSS [training: 0.1647816822321058 | validation: 0.11581669178333169]
	TIME [epoch: 8.43 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16055462094270267		[learning rate: 6.3195e-05]
	Learning Rate: 6.31952e-05
	LOSS [training: 0.16055462094270267 | validation: 0.12849077544089918]
	TIME [epoch: 8.43 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17024243399556166		[learning rate: 6.2966e-05]
	Learning Rate: 6.29659e-05
	LOSS [training: 0.17024243399556166 | validation: 0.13578915444161224]
	TIME [epoch: 8.42 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1616504025954147		[learning rate: 6.2737e-05]
	Learning Rate: 6.27374e-05
	LOSS [training: 0.1616504025954147 | validation: 0.12811669799129355]
	TIME [epoch: 8.45 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16826587625528805		[learning rate: 6.251e-05]
	Learning Rate: 6.25097e-05
	LOSS [training: 0.16826587625528805 | validation: 0.125065965741063]
	TIME [epoch: 8.43 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16382769007649423		[learning rate: 6.2283e-05]
	Learning Rate: 6.22828e-05
	LOSS [training: 0.16382769007649423 | validation: 0.12462346406808023]
	TIME [epoch: 8.42 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16577523309073566		[learning rate: 6.2057e-05]
	Learning Rate: 6.20568e-05
	LOSS [training: 0.16577523309073566 | validation: 0.12555497740911256]
	TIME [epoch: 8.42 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17205858048946837		[learning rate: 6.1832e-05]
	Learning Rate: 6.18316e-05
	LOSS [training: 0.17205858048946837 | validation: 0.12565830025685099]
	TIME [epoch: 8.45 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1653841147152134		[learning rate: 6.1607e-05]
	Learning Rate: 6.16072e-05
	LOSS [training: 0.1653841147152134 | validation: 0.12880851891057607]
	TIME [epoch: 8.44 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16035842095228614		[learning rate: 6.1384e-05]
	Learning Rate: 6.13836e-05
	LOSS [training: 0.16035842095228614 | validation: 0.12325283288961494]
	TIME [epoch: 8.42 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15896048116540337		[learning rate: 6.1161e-05]
	Learning Rate: 6.11609e-05
	LOSS [training: 0.15896048116540337 | validation: 0.12109420660519579]
	TIME [epoch: 8.42 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16477471417969636		[learning rate: 6.0939e-05]
	Learning Rate: 6.09389e-05
	LOSS [training: 0.16477471417969636 | validation: 0.12025817563737086]
	TIME [epoch: 8.45 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15733373907862422		[learning rate: 6.0718e-05]
	Learning Rate: 6.07178e-05
	LOSS [training: 0.15733373907862422 | validation: 0.1244546799634991]
	TIME [epoch: 8.43 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15922727024317682		[learning rate: 6.0497e-05]
	Learning Rate: 6.04974e-05
	LOSS [training: 0.15922727024317682 | validation: 0.12644236224663621]
	TIME [epoch: 8.43 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16958147315793548		[learning rate: 6.0278e-05]
	Learning Rate: 6.02779e-05
	LOSS [training: 0.16958147315793548 | validation: 0.12444147989683488]
	TIME [epoch: 8.43 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16543451666371944		[learning rate: 6.0059e-05]
	Learning Rate: 6.00591e-05
	LOSS [training: 0.16543451666371944 | validation: 0.13790120010011825]
	TIME [epoch: 8.45 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15945789470706756		[learning rate: 5.9841e-05]
	Learning Rate: 5.98412e-05
	LOSS [training: 0.15945789470706756 | validation: 0.1210460047108392]
	TIME [epoch: 8.43 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16378848974954985		[learning rate: 5.9624e-05]
	Learning Rate: 5.9624e-05
	LOSS [training: 0.16378848974954985 | validation: 0.13052925967641726]
	TIME [epoch: 8.43 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15822816265306894		[learning rate: 5.9408e-05]
	Learning Rate: 5.94076e-05
	LOSS [training: 0.15822816265306894 | validation: 0.13425380389794292]
	TIME [epoch: 8.42 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16638084455029065		[learning rate: 5.9192e-05]
	Learning Rate: 5.9192e-05
	LOSS [training: 0.16638084455029065 | validation: 0.10860783774370714]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_1511.pth
	Model improved!!!
EPOCH 1512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16361499216714911		[learning rate: 5.8977e-05]
	Learning Rate: 5.89772e-05
	LOSS [training: 0.16361499216714911 | validation: 0.1178925978646426]
	TIME [epoch: 8.44 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17428648838599647		[learning rate: 5.8763e-05]
	Learning Rate: 5.87632e-05
	LOSS [training: 0.17428648838599647 | validation: 0.13541611818075114]
	TIME [epoch: 8.43 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16126739013015218		[learning rate: 5.855e-05]
	Learning Rate: 5.85499e-05
	LOSS [training: 0.16126739013015218 | validation: 0.11380985485980907]
	TIME [epoch: 8.42 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15566719525335024		[learning rate: 5.8337e-05]
	Learning Rate: 5.83374e-05
	LOSS [training: 0.15566719525335024 | validation: 0.12373378095534324]
	TIME [epoch: 8.46 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15716981614009495		[learning rate: 5.8126e-05]
	Learning Rate: 5.81257e-05
	LOSS [training: 0.15716981614009495 | validation: 0.12278238417402296]
	TIME [epoch: 8.42 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16962748457129317		[learning rate: 5.7915e-05]
	Learning Rate: 5.79148e-05
	LOSS [training: 0.16962748457129317 | validation: 0.11957028752836793]
	TIME [epoch: 8.43 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1658803103213649		[learning rate: 5.7705e-05]
	Learning Rate: 5.77046e-05
	LOSS [training: 0.1658803103213649 | validation: 0.12013873626847169]
	TIME [epoch: 8.43 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16613921835715006		[learning rate: 5.7495e-05]
	Learning Rate: 5.74952e-05
	LOSS [training: 0.16613921835715006 | validation: 0.13233377445249112]
	TIME [epoch: 8.44 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17371763590031014		[learning rate: 5.7287e-05]
	Learning Rate: 5.72866e-05
	LOSS [training: 0.17371763590031014 | validation: 0.12326810947795665]
	TIME [epoch: 8.44 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16336166982886519		[learning rate: 5.7079e-05]
	Learning Rate: 5.70787e-05
	LOSS [training: 0.16336166982886519 | validation: 0.1171114185225639]
	TIME [epoch: 8.43 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1720768655438112		[learning rate: 5.6872e-05]
	Learning Rate: 5.68715e-05
	LOSS [training: 0.1720768655438112 | validation: 0.12155584679307191]
	TIME [epoch: 8.43 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1608914885003636		[learning rate: 5.6665e-05]
	Learning Rate: 5.66651e-05
	LOSS [training: 0.1608914885003636 | validation: 0.11793350922538276]
	TIME [epoch: 8.45 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16958714742622233		[learning rate: 5.6459e-05]
	Learning Rate: 5.64595e-05
	LOSS [training: 0.16958714742622233 | validation: 0.12330257924495358]
	TIME [epoch: 8.43 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1589100091635633		[learning rate: 5.6255e-05]
	Learning Rate: 5.62546e-05
	LOSS [training: 0.1589100091635633 | validation: 0.13209456059133629]
	TIME [epoch: 8.44 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1570082459747551		[learning rate: 5.605e-05]
	Learning Rate: 5.60504e-05
	LOSS [training: 0.1570082459747551 | validation: 0.11437982801339197]
	TIME [epoch: 8.43 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1677020104977302		[learning rate: 5.5847e-05]
	Learning Rate: 5.5847e-05
	LOSS [training: 0.1677020104977302 | validation: 0.130137953466687]
	TIME [epoch: 8.46 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16275636469967497		[learning rate: 5.5644e-05]
	Learning Rate: 5.56444e-05
	LOSS [training: 0.16275636469967497 | validation: 0.11804484964250368]
	TIME [epoch: 8.44 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1614564852475217		[learning rate: 5.5442e-05]
	Learning Rate: 5.54424e-05
	LOSS [training: 0.1614564852475217 | validation: 0.12491700656627049]
	TIME [epoch: 8.43 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16339688825209797		[learning rate: 5.5241e-05]
	Learning Rate: 5.52412e-05
	LOSS [training: 0.16339688825209797 | validation: 0.12514582395207696]
	TIME [epoch: 8.42 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16094198256983674		[learning rate: 5.5041e-05]
	Learning Rate: 5.50407e-05
	LOSS [training: 0.16094198256983674 | validation: 0.12814678397386642]
	TIME [epoch: 8.44 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1632651916795234		[learning rate: 5.4841e-05]
	Learning Rate: 5.4841e-05
	LOSS [training: 0.1632651916795234 | validation: 0.11318377028869822]
	TIME [epoch: 8.43 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16091309918212834		[learning rate: 5.4642e-05]
	Learning Rate: 5.4642e-05
	LOSS [training: 0.16091309918212834 | validation: 0.1280068685114072]
	TIME [epoch: 8.43 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16646780293245872		[learning rate: 5.4444e-05]
	Learning Rate: 5.44437e-05
	LOSS [training: 0.16646780293245872 | validation: 0.11841513031077087]
	TIME [epoch: 8.43 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1614343738373958		[learning rate: 5.4246e-05]
	Learning Rate: 5.42461e-05
	LOSS [training: 0.1614343738373958 | validation: 0.12362202872780584]
	TIME [epoch: 8.45 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16148121639595966		[learning rate: 5.4049e-05]
	Learning Rate: 5.40492e-05
	LOSS [training: 0.16148121639595966 | validation: 0.13350918974542228]
	TIME [epoch: 8.43 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1757826553144704		[learning rate: 5.3853e-05]
	Learning Rate: 5.38531e-05
	LOSS [training: 0.1757826553144704 | validation: 0.14528397527580453]
	TIME [epoch: 8.43 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16505311286633167		[learning rate: 5.3658e-05]
	Learning Rate: 5.36577e-05
	LOSS [training: 0.16505311286633167 | validation: 0.12022877426853354]
	TIME [epoch: 8.43 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16022107263867463		[learning rate: 5.3463e-05]
	Learning Rate: 5.34629e-05
	LOSS [training: 0.16022107263867463 | validation: 0.13584816980459044]
	TIME [epoch: 8.45 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1559564658816543		[learning rate: 5.3269e-05]
	Learning Rate: 5.32689e-05
	LOSS [training: 0.1559564658816543 | validation: 0.1373821020591756]
	TIME [epoch: 8.44 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1609107870828758		[learning rate: 5.3076e-05]
	Learning Rate: 5.30756e-05
	LOSS [training: 0.1609107870828758 | validation: 0.11949196907587728]
	TIME [epoch: 8.42 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16501757859256022		[learning rate: 5.2883e-05]
	Learning Rate: 5.2883e-05
	LOSS [training: 0.16501757859256022 | validation: 0.13144353590629698]
	TIME [epoch: 8.43 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16435524630354312		[learning rate: 5.2691e-05]
	Learning Rate: 5.26911e-05
	LOSS [training: 0.16435524630354312 | validation: 0.1255297189863889]
	TIME [epoch: 8.44 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1659920437266178		[learning rate: 5.25e-05]
	Learning Rate: 5.24998e-05
	LOSS [training: 0.1659920437266178 | validation: 0.13493775330593524]
	TIME [epoch: 8.43 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15991280936216462		[learning rate: 5.2309e-05]
	Learning Rate: 5.23093e-05
	LOSS [training: 0.15991280936216462 | validation: 0.12599178792752072]
	TIME [epoch: 8.42 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1613254627051929		[learning rate: 5.2119e-05]
	Learning Rate: 5.21195e-05
	LOSS [training: 0.1613254627051929 | validation: 0.12994935461918766]
	TIME [epoch: 8.43 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1682358673204603		[learning rate: 5.193e-05]
	Learning Rate: 5.19303e-05
	LOSS [training: 0.1682358673204603 | validation: 0.1278697970280362]
	TIME [epoch: 8.45 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1596249578072268		[learning rate: 5.1742e-05]
	Learning Rate: 5.17419e-05
	LOSS [training: 0.1596249578072268 | validation: 0.1209859511686501]
	TIME [epoch: 8.43 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1598390263805185		[learning rate: 5.1554e-05]
	Learning Rate: 5.15541e-05
	LOSS [training: 0.1598390263805185 | validation: 0.1286286559274551]
	TIME [epoch: 8.43 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16438386433812485		[learning rate: 5.1367e-05]
	Learning Rate: 5.1367e-05
	LOSS [training: 0.16438386433812485 | validation: 0.1230739801516249]
	TIME [epoch: 8.44 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17055546745389827		[learning rate: 5.1181e-05]
	Learning Rate: 5.11806e-05
	LOSS [training: 0.17055546745389827 | validation: 0.11660501333196914]
	TIME [epoch: 8.45 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15734702590098096		[learning rate: 5.0995e-05]
	Learning Rate: 5.09949e-05
	LOSS [training: 0.15734702590098096 | validation: 0.13243710593318309]
	TIME [epoch: 8.44 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16220037201672238		[learning rate: 5.081e-05]
	Learning Rate: 5.08098e-05
	LOSS [training: 0.16220037201672238 | validation: 0.12185599754255544]
	TIME [epoch: 8.42 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16310812987109546		[learning rate: 5.0625e-05]
	Learning Rate: 5.06254e-05
	LOSS [training: 0.16310812987109546 | validation: 0.11686127645803002]
	TIME [epoch: 8.43 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16349552879310789		[learning rate: 5.0442e-05]
	Learning Rate: 5.04417e-05
	LOSS [training: 0.16349552879310789 | validation: 0.12321875928111978]
	TIME [epoch: 8.45 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1609910664044103		[learning rate: 5.0259e-05]
	Learning Rate: 5.02586e-05
	LOSS [training: 0.1609910664044103 | validation: 0.12488060357448032]
	TIME [epoch: 8.43 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1675998902529589		[learning rate: 5.0076e-05]
	Learning Rate: 5.00762e-05
	LOSS [training: 0.1675998902529589 | validation: 0.1180086502311241]
	TIME [epoch: 8.44 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15427555114750127		[learning rate: 4.9894e-05]
	Learning Rate: 4.98945e-05
	LOSS [training: 0.15427555114750127 | validation: 0.12452944108005454]
	TIME [epoch: 8.42 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1656163317165997		[learning rate: 4.9713e-05]
	Learning Rate: 4.97134e-05
	LOSS [training: 0.1656163317165997 | validation: 0.1254452502267318]
	TIME [epoch: 8.45 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16723648262102858		[learning rate: 4.9533e-05]
	Learning Rate: 4.9533e-05
	LOSS [training: 0.16723648262102858 | validation: 0.12434020031837194]
	TIME [epoch: 8.42 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16531640696122832		[learning rate: 4.9353e-05]
	Learning Rate: 4.93533e-05
	LOSS [training: 0.16531640696122832 | validation: 0.1263572356004191]
	TIME [epoch: 8.42 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16170971805860063		[learning rate: 4.9174e-05]
	Learning Rate: 4.91742e-05
	LOSS [training: 0.16170971805860063 | validation: 0.12892247222923126]
	TIME [epoch: 8.42 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15574672131295547		[learning rate: 4.8996e-05]
	Learning Rate: 4.89957e-05
	LOSS [training: 0.15574672131295547 | validation: 0.1216185893089077]
	TIME [epoch: 8.44 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1639113298516379		[learning rate: 4.8818e-05]
	Learning Rate: 4.88179e-05
	LOSS [training: 0.1639113298516379 | validation: 0.11347358318376563]
	TIME [epoch: 8.43 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1571036942334391		[learning rate: 4.8641e-05]
	Learning Rate: 4.86407e-05
	LOSS [training: 0.1571036942334391 | validation: 0.12551355907251452]
	TIME [epoch: 8.42 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1665170409670623		[learning rate: 4.8464e-05]
	Learning Rate: 4.84642e-05
	LOSS [training: 0.1665170409670623 | validation: 0.12398360885363652]
	TIME [epoch: 8.42 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15862718725470698		[learning rate: 4.8288e-05]
	Learning Rate: 4.82883e-05
	LOSS [training: 0.15862718725470698 | validation: 0.11873351142229249]
	TIME [epoch: 8.44 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15842403043266928		[learning rate: 4.8113e-05]
	Learning Rate: 4.81131e-05
	LOSS [training: 0.15842403043266928 | validation: 0.13642948362203403]
	TIME [epoch: 8.43 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16941685950072305		[learning rate: 4.7938e-05]
	Learning Rate: 4.79385e-05
	LOSS [training: 0.16941685950072305 | validation: 0.12159522321342167]
	TIME [epoch: 8.42 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16091593183308311		[learning rate: 4.7765e-05]
	Learning Rate: 4.77645e-05
	LOSS [training: 0.16091593183308311 | validation: 0.11865535025110124]
	TIME [epoch: 8.43 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15748260033434208		[learning rate: 4.7591e-05]
	Learning Rate: 4.75912e-05
	LOSS [training: 0.15748260033434208 | validation: 0.14209033550925323]
	TIME [epoch: 8.44 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17189566233679138		[learning rate: 4.7418e-05]
	Learning Rate: 4.74185e-05
	LOSS [training: 0.17189566233679138 | validation: 0.12037708501845462]
	TIME [epoch: 8.43 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1641062569471746		[learning rate: 4.7246e-05]
	Learning Rate: 4.72464e-05
	LOSS [training: 0.1641062569471746 | validation: 0.12337143213276558]
	TIME [epoch: 8.42 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16141731145106825		[learning rate: 4.7075e-05]
	Learning Rate: 4.70749e-05
	LOSS [training: 0.16141731145106825 | validation: 0.11719101085744371]
	TIME [epoch: 8.42 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15706229830725385		[learning rate: 4.6904e-05]
	Learning Rate: 4.69041e-05
	LOSS [training: 0.15706229830725385 | validation: 0.13558371070990916]
	TIME [epoch: 8.45 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16271782839275978		[learning rate: 4.6734e-05]
	Learning Rate: 4.67339e-05
	LOSS [training: 0.16271782839275978 | validation: 0.12323745715540912]
	TIME [epoch: 8.42 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1639637444557856		[learning rate: 4.6564e-05]
	Learning Rate: 4.65643e-05
	LOSS [training: 0.1639637444557856 | validation: 0.13573911806852856]
	TIME [epoch: 8.44 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16417570145142257		[learning rate: 4.6395e-05]
	Learning Rate: 4.63953e-05
	LOSS [training: 0.16417570145142257 | validation: 0.12080328971816048]
	TIME [epoch: 8.42 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15575132082872303		[learning rate: 4.6227e-05]
	Learning Rate: 4.62269e-05
	LOSS [training: 0.15575132082872303 | validation: 0.1371668260451203]
	TIME [epoch: 8.45 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15977434088526882		[learning rate: 4.6059e-05]
	Learning Rate: 4.60591e-05
	LOSS [training: 0.15977434088526882 | validation: 0.1274305699841169]
	TIME [epoch: 8.43 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.162678641235317		[learning rate: 4.5892e-05]
	Learning Rate: 4.5892e-05
	LOSS [training: 0.162678641235317 | validation: 0.12711700777715967]
	TIME [epoch: 8.42 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16291742666180803		[learning rate: 4.5725e-05]
	Learning Rate: 4.57254e-05
	LOSS [training: 0.16291742666180803 | validation: 0.12597536947128465]
	TIME [epoch: 8.42 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1577080012981669		[learning rate: 4.556e-05]
	Learning Rate: 4.55595e-05
	LOSS [training: 0.1577080012981669 | validation: 0.12460812264895188]
	TIME [epoch: 8.45 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.157622879312902		[learning rate: 4.5394e-05]
	Learning Rate: 4.53942e-05
	LOSS [training: 0.157622879312902 | validation: 0.1293371717774027]
	TIME [epoch: 8.42 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16221856611491767		[learning rate: 4.5229e-05]
	Learning Rate: 4.52294e-05
	LOSS [training: 0.16221856611491767 | validation: 0.12201580059233902]
	TIME [epoch: 8.43 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1648702997673089		[learning rate: 4.5065e-05]
	Learning Rate: 4.50653e-05
	LOSS [training: 0.1648702997673089 | validation: 0.1290964686842858]
	TIME [epoch: 8.43 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1734294025144225		[learning rate: 4.4902e-05]
	Learning Rate: 4.49017e-05
	LOSS [training: 0.1734294025144225 | validation: 0.12507964816287542]
	TIME [epoch: 8.44 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1687840253002369		[learning rate: 4.4739e-05]
	Learning Rate: 4.47388e-05
	LOSS [training: 0.1687840253002369 | validation: 0.11841030219991322]
	TIME [epoch: 8.42 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1660762709306166		[learning rate: 4.4576e-05]
	Learning Rate: 4.45764e-05
	LOSS [training: 0.1660762709306166 | validation: 0.12259499843874522]
	TIME [epoch: 8.43 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16181594979053202		[learning rate: 4.4415e-05]
	Learning Rate: 4.44147e-05
	LOSS [training: 0.16181594979053202 | validation: 0.1294649741815944]
	TIME [epoch: 8.43 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1584112611308977		[learning rate: 4.4253e-05]
	Learning Rate: 4.42535e-05
	LOSS [training: 0.1584112611308977 | validation: 0.13084084997061318]
	TIME [epoch: 8.43 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1593443055775732		[learning rate: 4.4093e-05]
	Learning Rate: 4.40929e-05
	LOSS [training: 0.1593443055775732 | validation: 0.12577274312270947]
	TIME [epoch: 8.42 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15560302639058454		[learning rate: 4.3933e-05]
	Learning Rate: 4.39329e-05
	LOSS [training: 0.15560302639058454 | validation: 0.11439811842152206]
	TIME [epoch: 8.42 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16352626144064225		[learning rate: 4.3773e-05]
	Learning Rate: 4.37734e-05
	LOSS [training: 0.16352626144064225 | validation: 0.11813773498807298]
	TIME [epoch: 8.44 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15835921221404825		[learning rate: 4.3615e-05]
	Learning Rate: 4.36146e-05
	LOSS [training: 0.15835921221404825 | validation: 0.12572901458521482]
	TIME [epoch: 8.43 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1592066139733685		[learning rate: 4.3456e-05]
	Learning Rate: 4.34563e-05
	LOSS [training: 0.1592066139733685 | validation: 0.11421922557244507]
	TIME [epoch: 8.42 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.158451177136721		[learning rate: 4.3299e-05]
	Learning Rate: 4.32986e-05
	LOSS [training: 0.158451177136721 | validation: 0.121959780774681]
	TIME [epoch: 8.42 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15984973182907541		[learning rate: 4.3141e-05]
	Learning Rate: 4.31415e-05
	LOSS [training: 0.15984973182907541 | validation: 0.11194271014370977]
	TIME [epoch: 8.43 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15886356131405616		[learning rate: 4.2985e-05]
	Learning Rate: 4.29849e-05
	LOSS [training: 0.15886356131405616 | validation: 0.11867947849157723]
	TIME [epoch: 8.42 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15564270706493963		[learning rate: 4.2829e-05]
	Learning Rate: 4.28289e-05
	LOSS [training: 0.15564270706493963 | validation: 0.12159395349837265]
	TIME [epoch: 8.41 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15919741468678278		[learning rate: 4.2673e-05]
	Learning Rate: 4.26735e-05
	LOSS [training: 0.15919741468678278 | validation: 0.13254661899964332]
	TIME [epoch: 8.43 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15373393381004624		[learning rate: 4.2519e-05]
	Learning Rate: 4.25186e-05
	LOSS [training: 0.15373393381004624 | validation: 0.10942409358111607]
	TIME [epoch: 8.43 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16032098063166267		[learning rate: 4.2364e-05]
	Learning Rate: 4.23643e-05
	LOSS [training: 0.16032098063166267 | validation: 0.11798569282023999]
	TIME [epoch: 8.43 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16178889350598682		[learning rate: 4.2211e-05]
	Learning Rate: 4.22106e-05
	LOSS [training: 0.16178889350598682 | validation: 0.11711819386090132]
	TIME [epoch: 8.43 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15869913248514117		[learning rate: 4.2057e-05]
	Learning Rate: 4.20574e-05
	LOSS [training: 0.15869913248514117 | validation: 0.11563075713817099]
	TIME [epoch: 8.42 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1553248259246971		[learning rate: 4.1905e-05]
	Learning Rate: 4.19047e-05
	LOSS [training: 0.1553248259246971 | validation: 0.11545677479550956]
	TIME [epoch: 8.43 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15827441425575559		[learning rate: 4.1753e-05]
	Learning Rate: 4.17527e-05
	LOSS [training: 0.15827441425575559 | validation: 0.13929900741436357]
	TIME [epoch: 8.43 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1607868521243489		[learning rate: 4.1601e-05]
	Learning Rate: 4.16012e-05
	LOSS [training: 0.1607868521243489 | validation: 0.12990862195152603]
	TIME [epoch: 8.41 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15585796324295648		[learning rate: 4.145e-05]
	Learning Rate: 4.14502e-05
	LOSS [training: 0.15585796324295648 | validation: 0.12110669068681179]
	TIME [epoch: 8.42 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16349113002421176		[learning rate: 4.13e-05]
	Learning Rate: 4.12998e-05
	LOSS [training: 0.16349113002421176 | validation: 0.11802521737375345]
	TIME [epoch: 8.44 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1534301082056149		[learning rate: 4.115e-05]
	Learning Rate: 4.11499e-05
	LOSS [training: 0.1534301082056149 | validation: 0.12922847758321515]
	TIME [epoch: 8.43 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16002756829236375		[learning rate: 4.1001e-05]
	Learning Rate: 4.10005e-05
	LOSS [training: 0.16002756829236375 | validation: 0.12479387033234798]
	TIME [epoch: 8.41 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16321691903236188		[learning rate: 4.0852e-05]
	Learning Rate: 4.08517e-05
	LOSS [training: 0.16321691903236188 | validation: 0.12219342490488093]
	TIME [epoch: 8.42 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15459176806483832		[learning rate: 4.0703e-05]
	Learning Rate: 4.07035e-05
	LOSS [training: 0.15459176806483832 | validation: 0.1305060834261853]
	TIME [epoch: 8.44 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1627173780744098		[learning rate: 4.0556e-05]
	Learning Rate: 4.05558e-05
	LOSS [training: 0.1627173780744098 | validation: 0.11052359097017295]
	TIME [epoch: 8.43 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16731893953719865		[learning rate: 4.0409e-05]
	Learning Rate: 4.04086e-05
	LOSS [training: 0.16731893953719865 | validation: 0.10902843818713329]
	TIME [epoch: 8.42 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16537357543949355		[learning rate: 4.0262e-05]
	Learning Rate: 4.0262e-05
	LOSS [training: 0.16537357543949355 | validation: 0.12437250916609521]
	TIME [epoch: 8.43 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1594214824292612		[learning rate: 4.0116e-05]
	Learning Rate: 4.01158e-05
	LOSS [training: 0.1594214824292612 | validation: 0.1325161020482955]
	TIME [epoch: 8.44 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16124560965367912		[learning rate: 3.997e-05]
	Learning Rate: 3.99703e-05
	LOSS [training: 0.16124560965367912 | validation: 0.13377936941493168]
	TIME [epoch: 8.44 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16587428052506184		[learning rate: 3.9825e-05]
	Learning Rate: 3.98252e-05
	LOSS [training: 0.16587428052506184 | validation: 0.13686795912310135]
	TIME [epoch: 8.43 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16252010513646792		[learning rate: 3.9681e-05]
	Learning Rate: 3.96807e-05
	LOSS [training: 0.16252010513646792 | validation: 0.1235503350831642]
	TIME [epoch: 8.42 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16166739957445828		[learning rate: 3.9537e-05]
	Learning Rate: 3.95367e-05
	LOSS [training: 0.16166739957445828 | validation: 0.12963698273703803]
	TIME [epoch: 8.44 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1563871201924677		[learning rate: 3.9393e-05]
	Learning Rate: 3.93932e-05
	LOSS [training: 0.1563871201924677 | validation: 0.11776141928915648]
	TIME [epoch: 8.43 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16481419721742405		[learning rate: 3.925e-05]
	Learning Rate: 3.92502e-05
	LOSS [training: 0.16481419721742405 | validation: 0.12761775743350967]
	TIME [epoch: 8.43 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15766294133278022		[learning rate: 3.9108e-05]
	Learning Rate: 3.91078e-05
	LOSS [training: 0.15766294133278022 | validation: 0.13300204506248198]
	TIME [epoch: 8.42 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16246140518733598		[learning rate: 3.8966e-05]
	Learning Rate: 3.89659e-05
	LOSS [training: 0.16246140518733598 | validation: 0.13068707243543132]
	TIME [epoch: 8.44 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16302177134262857		[learning rate: 3.8824e-05]
	Learning Rate: 3.88245e-05
	LOSS [training: 0.16302177134262857 | validation: 0.1234714764586993]
	TIME [epoch: 8.41 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15685353053881781		[learning rate: 3.8684e-05]
	Learning Rate: 3.86836e-05
	LOSS [training: 0.15685353053881781 | validation: 0.11541525371180082]
	TIME [epoch: 8.42 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16641329070023897		[learning rate: 3.8543e-05]
	Learning Rate: 3.85432e-05
	LOSS [training: 0.16641329070023897 | validation: 0.12492510885816702]
	TIME [epoch: 8.43 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15731525875794458		[learning rate: 3.8403e-05]
	Learning Rate: 3.84033e-05
	LOSS [training: 0.15731525875794458 | validation: 0.11959094840742505]
	TIME [epoch: 8.47 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16605596270218		[learning rate: 3.8264e-05]
	Learning Rate: 3.82639e-05
	LOSS [training: 0.16605596270218 | validation: 0.1214205124348039]
	TIME [epoch: 8.44 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15247897189628584		[learning rate: 3.8125e-05]
	Learning Rate: 3.81251e-05
	LOSS [training: 0.15247897189628584 | validation: 0.1263382740106866]
	TIME [epoch: 8.44 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15895116606883586		[learning rate: 3.7987e-05]
	Learning Rate: 3.79867e-05
	LOSS [training: 0.15895116606883586 | validation: 0.12608097660297485]
	TIME [epoch: 8.45 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15968437588741122		[learning rate: 3.7849e-05]
	Learning Rate: 3.78489e-05
	LOSS [training: 0.15968437588741122 | validation: 0.12760734136937812]
	TIME [epoch: 8.47 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15940424820227925		[learning rate: 3.7711e-05]
	Learning Rate: 3.77115e-05
	LOSS [training: 0.15940424820227925 | validation: 0.12788814842993573]
	TIME [epoch: 8.44 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16058813015011958		[learning rate: 3.7575e-05]
	Learning Rate: 3.75746e-05
	LOSS [training: 0.16058813015011958 | validation: 0.12799737723166146]
	TIME [epoch: 8.45 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16257696706349783		[learning rate: 3.7438e-05]
	Learning Rate: 3.74383e-05
	LOSS [training: 0.16257696706349783 | validation: 0.12620487604697356]
	TIME [epoch: 8.45 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16125815414308303		[learning rate: 3.7302e-05]
	Learning Rate: 3.73024e-05
	LOSS [training: 0.16125815414308303 | validation: 0.12890366348990812]
	TIME [epoch: 8.45 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1575337912413348		[learning rate: 3.7167e-05]
	Learning Rate: 3.7167e-05
	LOSS [training: 0.1575337912413348 | validation: 0.14125235129394506]
	TIME [epoch: 8.45 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16009984161475202		[learning rate: 3.7032e-05]
	Learning Rate: 3.70322e-05
	LOSS [training: 0.16009984161475202 | validation: 0.12403449209805059]
	TIME [epoch: 8.45 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1600661157325441		[learning rate: 3.6898e-05]
	Learning Rate: 3.68978e-05
	LOSS [training: 0.1600661157325441 | validation: 0.1268295051651548]
	TIME [epoch: 8.46 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15684989541896685		[learning rate: 3.6764e-05]
	Learning Rate: 3.67639e-05
	LOSS [training: 0.15684989541896685 | validation: 0.10669929968899283]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_1642.pth
	Model improved!!!
EPOCH 1643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1549319002352188		[learning rate: 3.663e-05]
	Learning Rate: 3.66304e-05
	LOSS [training: 0.1549319002352188 | validation: 0.12816678566699916]
	TIME [epoch: 8.43 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1603124774392426		[learning rate: 3.6498e-05]
	Learning Rate: 3.64975e-05
	LOSS [training: 0.1603124774392426 | validation: 0.11891953631177783]
	TIME [epoch: 8.44 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1536557946383979		[learning rate: 3.6365e-05]
	Learning Rate: 3.63651e-05
	LOSS [training: 0.1536557946383979 | validation: 0.12776235040240158]
	TIME [epoch: 8.47 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17111953276717107		[learning rate: 3.6233e-05]
	Learning Rate: 3.62331e-05
	LOSS [training: 0.17111953276717107 | validation: 0.1342293455967542]
	TIME [epoch: 8.45 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1582297703318853		[learning rate: 3.6102e-05]
	Learning Rate: 3.61016e-05
	LOSS [training: 0.1582297703318853 | validation: 0.12886836903501814]
	TIME [epoch: 8.43 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16201494906828368		[learning rate: 3.5971e-05]
	Learning Rate: 3.59706e-05
	LOSS [training: 0.16201494906828368 | validation: 0.10626255379423158]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_1648.pth
	Model improved!!!
EPOCH 1649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1609999352584379		[learning rate: 3.584e-05]
	Learning Rate: 3.584e-05
	LOSS [training: 0.1609999352584379 | validation: 0.11905485361733559]
	TIME [epoch: 8.48 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1558505040175596		[learning rate: 3.571e-05]
	Learning Rate: 3.571e-05
	LOSS [training: 0.1558505040175596 | validation: 0.12485350028487649]
	TIME [epoch: 8.47 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16401228542869184		[learning rate: 3.558e-05]
	Learning Rate: 3.55804e-05
	LOSS [training: 0.16401228542869184 | validation: 0.1458121302051568]
	TIME [epoch: 8.5 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16436586807081002		[learning rate: 3.5451e-05]
	Learning Rate: 3.54513e-05
	LOSS [training: 0.16436586807081002 | validation: 0.13073108523611604]
	TIME [epoch: 8.45 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15543784564746582		[learning rate: 3.5323e-05]
	Learning Rate: 3.53226e-05
	LOSS [training: 0.15543784564746582 | validation: 0.1263051143615562]
	TIME [epoch: 8.45 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15858156026284118		[learning rate: 3.5194e-05]
	Learning Rate: 3.51944e-05
	LOSS [training: 0.15858156026284118 | validation: 0.12444534070196586]
	TIME [epoch: 8.48 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15539818687764134		[learning rate: 3.5067e-05]
	Learning Rate: 3.50667e-05
	LOSS [training: 0.15539818687764134 | validation: 0.12181338992648338]
	TIME [epoch: 8.45 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15559556042938755		[learning rate: 3.4939e-05]
	Learning Rate: 3.49394e-05
	LOSS [training: 0.15559556042938755 | validation: 0.11668110284065937]
	TIME [epoch: 8.44 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15714473425924658		[learning rate: 3.4813e-05]
	Learning Rate: 3.48126e-05
	LOSS [training: 0.15714473425924658 | validation: 0.11878105917082901]
	TIME [epoch: 8.45 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15758821044807625		[learning rate: 3.4686e-05]
	Learning Rate: 3.46863e-05
	LOSS [training: 0.15758821044807625 | validation: 0.11517152442910103]
	TIME [epoch: 8.5 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1553804905825075		[learning rate: 3.456e-05]
	Learning Rate: 3.45604e-05
	LOSS [training: 0.1553804905825075 | validation: 0.12735414584113425]
	TIME [epoch: 8.47 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16247768788128203		[learning rate: 3.4435e-05]
	Learning Rate: 3.4435e-05
	LOSS [training: 0.16247768788128203 | validation: 0.12851260521930455]
	TIME [epoch: 8.45 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15788517524272896		[learning rate: 3.431e-05]
	Learning Rate: 3.431e-05
	LOSS [training: 0.15788517524272896 | validation: 0.12464196082796848]
	TIME [epoch: 8.46 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15324987437897053		[learning rate: 3.4186e-05]
	Learning Rate: 3.41855e-05
	LOSS [training: 0.15324987437897053 | validation: 0.12331943476993923]
	TIME [epoch: 8.48 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16771331404310913		[learning rate: 3.4061e-05]
	Learning Rate: 3.40615e-05
	LOSS [training: 0.16771331404310913 | validation: 0.1178525619038008]
	TIME [epoch: 8.45 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15870618717363974		[learning rate: 3.3938e-05]
	Learning Rate: 3.39378e-05
	LOSS [training: 0.15870618717363974 | validation: 0.11657647101189489]
	TIME [epoch: 8.46 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15940531557859558		[learning rate: 3.3815e-05]
	Learning Rate: 3.38147e-05
	LOSS [training: 0.15940531557859558 | validation: 0.13612250160331285]
	TIME [epoch: 8.47 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16492908767839903		[learning rate: 3.3692e-05]
	Learning Rate: 3.3692e-05
	LOSS [training: 0.16492908767839903 | validation: 0.12794362375262336]
	TIME [epoch: 8.47 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16330531404061466		[learning rate: 3.357e-05]
	Learning Rate: 3.35697e-05
	LOSS [training: 0.16330531404061466 | validation: 0.12501753784976727]
	TIME [epoch: 8.46 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16135858347457574		[learning rate: 3.3448e-05]
	Learning Rate: 3.34479e-05
	LOSS [training: 0.16135858347457574 | validation: 0.11635281664073957]
	TIME [epoch: 8.46 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16265174909751084		[learning rate: 3.3326e-05]
	Learning Rate: 3.33265e-05
	LOSS [training: 0.16265174909751084 | validation: 0.12581049223618]
	TIME [epoch: 8.51 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15626377249807727		[learning rate: 3.3206e-05]
	Learning Rate: 3.32055e-05
	LOSS [training: 0.15626377249807727 | validation: 0.10594462706895816]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_1670.pth
	Model improved!!!
EPOCH 1671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15702717846159867		[learning rate: 3.3085e-05]
	Learning Rate: 3.3085e-05
	LOSS [training: 0.15702717846159867 | validation: 0.1280063120616955]
	TIME [epoch: 8.48 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16083735419167267		[learning rate: 3.2965e-05]
	Learning Rate: 3.2965e-05
	LOSS [training: 0.16083735419167267 | validation: 0.12181402388468004]
	TIME [epoch: 8.46 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15755043395066687		[learning rate: 3.2845e-05]
	Learning Rate: 3.28453e-05
	LOSS [training: 0.15755043395066687 | validation: 0.11905014932978202]
	TIME [epoch: 8.49 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1559740333103658		[learning rate: 3.2726e-05]
	Learning Rate: 3.27261e-05
	LOSS [training: 0.1559740333103658 | validation: 0.12168729155764649]
	TIME [epoch: 8.46 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16187792186911293		[learning rate: 3.2607e-05]
	Learning Rate: 3.26074e-05
	LOSS [training: 0.16187792186911293 | validation: 0.1333991909150229]
	TIME [epoch: 8.47 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15693719728098193		[learning rate: 3.2489e-05]
	Learning Rate: 3.2489e-05
	LOSS [training: 0.15693719728098193 | validation: 0.12247078918586941]
	TIME [epoch: 8.47 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1556479806181429		[learning rate: 3.2371e-05]
	Learning Rate: 3.23711e-05
	LOSS [training: 0.1556479806181429 | validation: 0.12677265620847336]
	TIME [epoch: 8.48 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16338005888474316		[learning rate: 3.2254e-05]
	Learning Rate: 3.22537e-05
	LOSS [training: 0.16338005888474316 | validation: 0.1279736086334367]
	TIME [epoch: 8.48 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16342558613470876		[learning rate: 3.2137e-05]
	Learning Rate: 3.21366e-05
	LOSS [training: 0.16342558613470876 | validation: 0.11321131527426803]
	TIME [epoch: 8.46 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15991639104045657		[learning rate: 3.202e-05]
	Learning Rate: 3.202e-05
	LOSS [training: 0.15991639104045657 | validation: 0.11984099457457428]
	TIME [epoch: 8.46 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16174582542651447		[learning rate: 3.1904e-05]
	Learning Rate: 3.19038e-05
	LOSS [training: 0.16174582542651447 | validation: 0.12195889206234273]
	TIME [epoch: 8.49 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1727817948055404		[learning rate: 3.1788e-05]
	Learning Rate: 3.1788e-05
	LOSS [training: 0.1727817948055404 | validation: 0.12609719637953798]
	TIME [epoch: 8.46 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16254465362470835		[learning rate: 3.1673e-05]
	Learning Rate: 3.16726e-05
	LOSS [training: 0.16254465362470835 | validation: 0.10297223462777322]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_1683.pth
	Model improved!!!
EPOCH 1684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16018239844518806		[learning rate: 3.1558e-05]
	Learning Rate: 3.15577e-05
	LOSS [training: 0.16018239844518806 | validation: 0.10646055323600376]
	TIME [epoch: 8.46 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15500641981599983		[learning rate: 3.1443e-05]
	Learning Rate: 3.14432e-05
	LOSS [training: 0.15500641981599983 | validation: 0.1266937585884328]
	TIME [epoch: 8.48 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15619103444113197		[learning rate: 3.1329e-05]
	Learning Rate: 3.13291e-05
	LOSS [training: 0.15619103444113197 | validation: 0.11980868884209087]
	TIME [epoch: 8.46 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16302742049872537		[learning rate: 3.1215e-05]
	Learning Rate: 3.12154e-05
	LOSS [training: 0.16302742049872537 | validation: 0.10648659719594566]
	TIME [epoch: 8.46 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15408833262118904		[learning rate: 3.1102e-05]
	Learning Rate: 3.11021e-05
	LOSS [training: 0.15408833262118904 | validation: 0.1134912169712929]
	TIME [epoch: 8.46 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16147445765040894		[learning rate: 3.0989e-05]
	Learning Rate: 3.09892e-05
	LOSS [training: 0.16147445765040894 | validation: 0.11626542969961506]
	TIME [epoch: 8.47 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16170503495106564		[learning rate: 3.0877e-05]
	Learning Rate: 3.08768e-05
	LOSS [training: 0.16170503495106564 | validation: 0.11873309508802339]
	TIME [epoch: 8.46 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16399828417307294		[learning rate: 3.0765e-05]
	Learning Rate: 3.07647e-05
	LOSS [training: 0.16399828417307294 | validation: 0.13874581913753004]
	TIME [epoch: 8.45 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16214025321209302		[learning rate: 3.0653e-05]
	Learning Rate: 3.0653e-05
	LOSS [training: 0.16214025321209302 | validation: 0.13172591860975313]
	TIME [epoch: 8.45 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15292348741379683		[learning rate: 3.0542e-05]
	Learning Rate: 3.05418e-05
	LOSS [training: 0.15292348741379683 | validation: 0.12917287524541232]
	TIME [epoch: 8.48 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15695160201279812		[learning rate: 3.0431e-05]
	Learning Rate: 3.0431e-05
	LOSS [training: 0.15695160201279812 | validation: 0.12303010494438965]
	TIME [epoch: 8.46 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1607845312324427		[learning rate: 3.0321e-05]
	Learning Rate: 3.03205e-05
	LOSS [training: 0.1607845312324427 | validation: 0.11795484810797185]
	TIME [epoch: 8.46 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15407347172598632		[learning rate: 3.0211e-05]
	Learning Rate: 3.02105e-05
	LOSS [training: 0.15407347172598632 | validation: 0.12800504789102793]
	TIME [epoch: 8.46 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1557558660881611		[learning rate: 3.0101e-05]
	Learning Rate: 3.01009e-05
	LOSS [training: 0.1557558660881611 | validation: 0.13114384251240424]
	TIME [epoch: 8.49 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16465422579977465		[learning rate: 2.9992e-05]
	Learning Rate: 2.99916e-05
	LOSS [training: 0.16465422579977465 | validation: 0.11881486956491118]
	TIME [epoch: 8.47 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15709600808154134		[learning rate: 2.9883e-05]
	Learning Rate: 2.98828e-05
	LOSS [training: 0.15709600808154134 | validation: 0.11930358586581186]
	TIME [epoch: 8.46 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15966051536558268		[learning rate: 2.9774e-05]
	Learning Rate: 2.97743e-05
	LOSS [training: 0.15966051536558268 | validation: 0.12244135075252519]
	TIME [epoch: 8.46 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16536110301337295		[learning rate: 2.9666e-05]
	Learning Rate: 2.96663e-05
	LOSS [training: 0.16536110301337295 | validation: 0.11993276355300758]
	TIME [epoch: 8.5 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15832785735609636		[learning rate: 2.9559e-05]
	Learning Rate: 2.95586e-05
	LOSS [training: 0.15832785735609636 | validation: 0.12815685532059518]
	TIME [epoch: 8.47 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16173206494190012		[learning rate: 2.9451e-05]
	Learning Rate: 2.94514e-05
	LOSS [training: 0.16173206494190012 | validation: 0.12520913701222924]
	TIME [epoch: 8.47 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1587529594495441		[learning rate: 2.9344e-05]
	Learning Rate: 2.93445e-05
	LOSS [training: 0.1587529594495441 | validation: 0.11446025366486962]
	TIME [epoch: 8.46 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16174129034656945		[learning rate: 2.9238e-05]
	Learning Rate: 2.9238e-05
	LOSS [training: 0.16174129034656945 | validation: 0.12184541139784572]
	TIME [epoch: 8.48 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1631778661433732		[learning rate: 2.9132e-05]
	Learning Rate: 2.91319e-05
	LOSS [training: 0.1631778661433732 | validation: 0.10895103412015136]
	TIME [epoch: 8.46 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16383216026182543		[learning rate: 2.9026e-05]
	Learning Rate: 2.90262e-05
	LOSS [training: 0.16383216026182543 | validation: 0.12637456880021075]
	TIME [epoch: 8.45 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16008554736873268		[learning rate: 2.8921e-05]
	Learning Rate: 2.89208e-05
	LOSS [training: 0.16008554736873268 | validation: 0.11419597852667507]
	TIME [epoch: 8.46 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1559590562120034		[learning rate: 2.8816e-05]
	Learning Rate: 2.88159e-05
	LOSS [training: 0.1559590562120034 | validation: 0.11924430922077024]
	TIME [epoch: 8.48 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1566310769683577		[learning rate: 2.8711e-05]
	Learning Rate: 2.87113e-05
	LOSS [training: 0.1566310769683577 | validation: 0.12624552059999627]
	TIME [epoch: 8.46 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16300193309061134		[learning rate: 2.8607e-05]
	Learning Rate: 2.86071e-05
	LOSS [training: 0.16300193309061134 | validation: 0.11993295900952769]
	TIME [epoch: 8.45 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16141906368402617		[learning rate: 2.8503e-05]
	Learning Rate: 2.85033e-05
	LOSS [training: 0.16141906368402617 | validation: 0.13125980317133973]
	TIME [epoch: 8.47 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16105578479873986		[learning rate: 2.84e-05]
	Learning Rate: 2.83998e-05
	LOSS [training: 0.16105578479873986 | validation: 0.11840095537395172]
	TIME [epoch: 8.48 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1619920147831659		[learning rate: 2.8297e-05]
	Learning Rate: 2.82968e-05
	LOSS [training: 0.1619920147831659 | validation: 0.11124312805245612]
	TIME [epoch: 8.46 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16174442689410046		[learning rate: 2.8194e-05]
	Learning Rate: 2.81941e-05
	LOSS [training: 0.16174442689410046 | validation: 0.10731933801155978]
	TIME [epoch: 8.46 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15495283984844344		[learning rate: 2.8092e-05]
	Learning Rate: 2.80918e-05
	LOSS [training: 0.15495283984844344 | validation: 0.12163062055296636]
	TIME [epoch: 8.47 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15657655823729905		[learning rate: 2.799e-05]
	Learning Rate: 2.79898e-05
	LOSS [training: 0.15657655823729905 | validation: 0.12688508599491324]
	TIME [epoch: 8.5 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15920158420146233		[learning rate: 2.7888e-05]
	Learning Rate: 2.78882e-05
	LOSS [training: 0.15920158420146233 | validation: 0.12356319166614439]
	TIME [epoch: 8.47 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15992862691599233		[learning rate: 2.7787e-05]
	Learning Rate: 2.7787e-05
	LOSS [training: 0.15992862691599233 | validation: 0.11835278801807206]
	TIME [epoch: 8.47 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1608623775120011		[learning rate: 2.7686e-05]
	Learning Rate: 2.76862e-05
	LOSS [training: 0.1608623775120011 | validation: 0.11643951835226883]
	TIME [epoch: 8.46 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15480775065778635		[learning rate: 2.7586e-05]
	Learning Rate: 2.75857e-05
	LOSS [training: 0.15480775065778635 | validation: 0.12186343474764899]
	TIME [epoch: 8.49 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16307985063149175		[learning rate: 2.7486e-05]
	Learning Rate: 2.74856e-05
	LOSS [training: 0.16307985063149175 | validation: 0.11918847231344115]
	TIME [epoch: 8.47 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15557080299627132		[learning rate: 2.7386e-05]
	Learning Rate: 2.73859e-05
	LOSS [training: 0.15557080299627132 | validation: 0.11117263906448427]
	TIME [epoch: 8.46 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15974467616798194		[learning rate: 2.7286e-05]
	Learning Rate: 2.72865e-05
	LOSS [training: 0.15974467616798194 | validation: 0.123918081786599]
	TIME [epoch: 8.46 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16093451235911485		[learning rate: 2.7187e-05]
	Learning Rate: 2.71875e-05
	LOSS [training: 0.16093451235911485 | validation: 0.12407745485418886]
	TIME [epoch: 8.48 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15680305109223136		[learning rate: 2.7089e-05]
	Learning Rate: 2.70888e-05
	LOSS [training: 0.15680305109223136 | validation: 0.12326375532892886]
	TIME [epoch: 8.46 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15802763863743158		[learning rate: 2.699e-05]
	Learning Rate: 2.69905e-05
	LOSS [training: 0.15802763863743158 | validation: 0.11195071591606884]
	TIME [epoch: 8.46 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1634212208809723		[learning rate: 2.6893e-05]
	Learning Rate: 2.68925e-05
	LOSS [training: 0.1634212208809723 | validation: 0.12081758791857215]
	TIME [epoch: 8.46 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1630044153710039		[learning rate: 2.6795e-05]
	Learning Rate: 2.67949e-05
	LOSS [training: 0.1630044153710039 | validation: 0.10208292896802748]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r0_20240219_194135/states/model_tr_study203_1729.pth
	Model improved!!!
EPOCH 1730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15975423769097025		[learning rate: 2.6698e-05]
	Learning Rate: 2.66977e-05
	LOSS [training: 0.15975423769097025 | validation: 0.12208986168080213]
	TIME [epoch: 8.46 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1575673471557077		[learning rate: 2.6601e-05]
	Learning Rate: 2.66008e-05
	LOSS [training: 0.1575673471557077 | validation: 0.11716540646308309]
	TIME [epoch: 8.46 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15444168838111144		[learning rate: 2.6504e-05]
	Learning Rate: 2.65043e-05
	LOSS [training: 0.15444168838111144 | validation: 0.12309594685744435]
	TIME [epoch: 8.48 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15928427521618016		[learning rate: 2.6408e-05]
	Learning Rate: 2.64081e-05
	LOSS [training: 0.15928427521618016 | validation: 0.12256306719700642]
	TIME [epoch: 8.47 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15208211779874198		[learning rate: 2.6312e-05]
	Learning Rate: 2.63123e-05
	LOSS [training: 0.15208211779874198 | validation: 0.12876748845114763]
	TIME [epoch: 8.44 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1534688863727993		[learning rate: 2.6217e-05]
	Learning Rate: 2.62168e-05
	LOSS [training: 0.1534688863727993 | validation: 0.10946611927494782]
	TIME [epoch: 8.44 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1592839647469004		[learning rate: 2.6122e-05]
	Learning Rate: 2.61216e-05
	LOSS [training: 0.1592839647469004 | validation: 0.12068239303695133]
	TIME [epoch: 8.44 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16021684312678783		[learning rate: 2.6027e-05]
	Learning Rate: 2.60268e-05
	LOSS [training: 0.16021684312678783 | validation: 0.1307893055430934]
	TIME [epoch: 8.46 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15399995111970047		[learning rate: 2.5932e-05]
	Learning Rate: 2.59324e-05
	LOSS [training: 0.15399995111970047 | validation: 0.12788077613328794]
	TIME [epoch: 8.44 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16157617298777024		[learning rate: 2.5838e-05]
	Learning Rate: 2.58383e-05
	LOSS [training: 0.16157617298777024 | validation: 0.12757604080366117]
	TIME [epoch: 8.45 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15614865336785924		[learning rate: 2.5744e-05]
	Learning Rate: 2.57445e-05
	LOSS [training: 0.15614865336785924 | validation: 0.12459166509635843]
	TIME [epoch: 8.44 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1673355972089321		[learning rate: 2.5651e-05]
	Learning Rate: 2.56511e-05
	LOSS [training: 0.1673355972089321 | validation: 0.1229440157460745]
	TIME [epoch: 8.45 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15425093588415392		[learning rate: 2.5558e-05]
	Learning Rate: 2.5558e-05
	LOSS [training: 0.15425093588415392 | validation: 0.1220900710909485]
	TIME [epoch: 8.43 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15890067885042924		[learning rate: 2.5465e-05]
	Learning Rate: 2.54652e-05
	LOSS [training: 0.15890067885042924 | validation: 0.1228625409624488]
	TIME [epoch: 8.44 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15617197208235262		[learning rate: 2.5373e-05]
	Learning Rate: 2.53728e-05
	LOSS [training: 0.15617197208235262 | validation: 0.12985683171109513]
	TIME [epoch: 8.45 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16092036421850864		[learning rate: 2.5281e-05]
	Learning Rate: 2.52807e-05
	LOSS [training: 0.16092036421850864 | validation: 0.12433519814862823]
	TIME [epoch: 8.44 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15812814926933172		[learning rate: 2.5189e-05]
	Learning Rate: 2.5189e-05
	LOSS [training: 0.15812814926933172 | validation: 0.12172131681227341]
	TIME [epoch: 8.45 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15750625003300936		[learning rate: 2.5098e-05]
	Learning Rate: 2.50976e-05
	LOSS [training: 0.15750625003300936 | validation: 0.10724272651040156]
	TIME [epoch: 8.43 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16347644109611198		[learning rate: 2.5006e-05]
	Learning Rate: 2.50065e-05
	LOSS [training: 0.16347644109611198 | validation: 0.11498324255231865]
	TIME [epoch: 8.46 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15669525063078765		[learning rate: 2.4916e-05]
	Learning Rate: 2.49157e-05
	LOSS [training: 0.15669525063078765 | validation: 0.1277008514634909]
	TIME [epoch: 8.44 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16453745849762266		[learning rate: 2.4825e-05]
	Learning Rate: 2.48253e-05
	LOSS [training: 0.16453745849762266 | validation: 0.11827150498820388]
	TIME [epoch: 8.44 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1560621804424215		[learning rate: 2.4735e-05]
	Learning Rate: 2.47352e-05
	LOSS [training: 0.1560621804424215 | validation: 0.11893239644087464]
	TIME [epoch: 8.44 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16201877082437155		[learning rate: 2.4645e-05]
	Learning Rate: 2.46455e-05
	LOSS [training: 0.16201877082437155 | validation: 0.12379285011384089]
	TIME [epoch: 8.46 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1648382388275353		[learning rate: 2.4556e-05]
	Learning Rate: 2.4556e-05
	LOSS [training: 0.1648382388275353 | validation: 0.11822111059173387]
	TIME [epoch: 8.44 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15833695234625159		[learning rate: 2.4467e-05]
	Learning Rate: 2.44669e-05
	LOSS [training: 0.15833695234625159 | validation: 0.11744202539732074]
	TIME [epoch: 8.44 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1598330329821716		[learning rate: 2.4378e-05]
	Learning Rate: 2.43781e-05
	LOSS [training: 0.1598330329821716 | validation: 0.10745972753835961]
	TIME [epoch: 8.44 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1629606541764237		[learning rate: 2.429e-05]
	Learning Rate: 2.42896e-05
	LOSS [training: 0.1629606541764237 | validation: 0.11820816414029472]
	TIME [epoch: 8.45 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15466270993374034		[learning rate: 2.4201e-05]
	Learning Rate: 2.42015e-05
	LOSS [training: 0.15466270993374034 | validation: 0.12698752763126142]
	TIME [epoch: 8.44 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1666450775491922		[learning rate: 2.4114e-05]
	Learning Rate: 2.41137e-05
	LOSS [training: 0.1666450775491922 | validation: 0.11963458454998388]
	TIME [epoch: 8.44 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15780968509305826		[learning rate: 2.4026e-05]
	Learning Rate: 2.40262e-05
	LOSS [training: 0.15780968509305826 | validation: 0.11777333545315258]
	TIME [epoch: 8.44 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16738643077730161		[learning rate: 2.3939e-05]
	Learning Rate: 2.3939e-05
	LOSS [training: 0.16738643077730161 | validation: 0.12283407662594151]
	TIME [epoch: 8.46 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16065556095812167		[learning rate: 2.3852e-05]
	Learning Rate: 2.38521e-05
	LOSS [training: 0.16065556095812167 | validation: 0.12828947346985384]
	TIME [epoch: 8.44 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16096785879513859		[learning rate: 2.3766e-05]
	Learning Rate: 2.37655e-05
	LOSS [training: 0.16096785879513859 | validation: 0.11772516829464771]
	TIME [epoch: 8.44 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15723137908339552		[learning rate: 2.3679e-05]
	Learning Rate: 2.36793e-05
	LOSS [training: 0.15723137908339552 | validation: 0.11851062523222972]
	TIME [epoch: 8.44 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15858125352241254		[learning rate: 2.3593e-05]
	Learning Rate: 2.35933e-05
	LOSS [training: 0.15858125352241254 | validation: 0.12723562605933353]
	TIME [epoch: 8.47 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1590392080872139		[learning rate: 2.3508e-05]
	Learning Rate: 2.35077e-05
	LOSS [training: 0.1590392080872139 | validation: 0.1264392288306901]
	TIME [epoch: 8.46 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1635892014414487		[learning rate: 2.3422e-05]
	Learning Rate: 2.34224e-05
	LOSS [training: 0.1635892014414487 | validation: 0.11121582163322506]
	TIME [epoch: 8.46 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15988349285963363		[learning rate: 2.3337e-05]
	Learning Rate: 2.33374e-05
	LOSS [training: 0.15988349285963363 | validation: 0.12034852724601708]
	TIME [epoch: 8.46 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15642915629271203		[learning rate: 2.3253e-05]
	Learning Rate: 2.32527e-05
	LOSS [training: 0.15642915629271203 | validation: 0.11540038742144335]
	TIME [epoch: 8.47 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1579967699348466		[learning rate: 2.3168e-05]
	Learning Rate: 2.31683e-05
	LOSS [training: 0.1579967699348466 | validation: 0.120757520041195]
	TIME [epoch: 8.47 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1617055250681269		[learning rate: 2.3084e-05]
	Learning Rate: 2.30843e-05
	LOSS [training: 0.1617055250681269 | validation: 0.12106084896302052]
	TIME [epoch: 8.46 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1574156385612095		[learning rate: 2.3e-05]
	Learning Rate: 2.30005e-05
	LOSS [training: 0.1574156385612095 | validation: 0.13323030065285535]
	TIME [epoch: 8.46 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15697564874832823		[learning rate: 2.2917e-05]
	Learning Rate: 2.2917e-05
	LOSS [training: 0.15697564874832823 | validation: 0.12577882556878306]
	TIME [epoch: 8.49 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1607581260005167		[learning rate: 2.2834e-05]
	Learning Rate: 2.28338e-05
	LOSS [training: 0.1607581260005167 | validation: 0.12236528546483491]
	TIME [epoch: 8.47 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.152164183859979		[learning rate: 2.2751e-05]
	Learning Rate: 2.2751e-05
	LOSS [training: 0.152164183859979 | validation: 0.12241779174055173]
	TIME [epoch: 8.46 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1572431054191617		[learning rate: 2.2668e-05]
	Learning Rate: 2.26684e-05
	LOSS [training: 0.1572431054191617 | validation: 0.11463606896637431]
	TIME [epoch: 8.45 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15838189297225103		[learning rate: 2.2586e-05]
	Learning Rate: 2.25862e-05
	LOSS [training: 0.15838189297225103 | validation: 0.12280104601907278]
	TIME [epoch: 8.48 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15412265777798467		[learning rate: 2.2504e-05]
	Learning Rate: 2.25042e-05
	LOSS [training: 0.15412265777798467 | validation: 0.12675218231470742]
	TIME [epoch: 8.46 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1616412284942807		[learning rate: 2.2423e-05]
	Learning Rate: 2.24225e-05
	LOSS [training: 0.1616412284942807 | validation: 0.12192048813845774]
	TIME [epoch: 8.45 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15213046348565226		[learning rate: 2.2341e-05]
	Learning Rate: 2.23411e-05
	LOSS [training: 0.15213046348565226 | validation: 0.12694570575599523]
	TIME [epoch: 8.46 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15661802234781005		[learning rate: 2.226e-05]
	Learning Rate: 2.22601e-05
	LOSS [training: 0.15661802234781005 | validation: 0.12454027471180854]
	TIME [epoch: 8.48 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15911083106812324		[learning rate: 2.2179e-05]
	Learning Rate: 2.21793e-05
	LOSS [training: 0.15911083106812324 | validation: 0.12232151714077183]
	TIME [epoch: 8.46 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1575093231621735		[learning rate: 2.2099e-05]
	Learning Rate: 2.20988e-05
	LOSS [training: 0.1575093231621735 | validation: 0.11974315904396568]
	TIME [epoch: 8.45 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15316180264908416		[learning rate: 2.2019e-05]
	Learning Rate: 2.20186e-05
	LOSS [training: 0.15316180264908416 | validation: 0.12376111562018685]
	TIME [epoch: 8.46 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15845657915306546		[learning rate: 2.1939e-05]
	Learning Rate: 2.19387e-05
	LOSS [training: 0.15845657915306546 | validation: 0.118268966183989]
	TIME [epoch: 8.49 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15560499247414267		[learning rate: 2.1859e-05]
	Learning Rate: 2.18591e-05
	LOSS [training: 0.15560499247414267 | validation: 0.11894250890628841]
	TIME [epoch: 8.46 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15765738098582296		[learning rate: 2.178e-05]
	Learning Rate: 2.17797e-05
	LOSS [training: 0.15765738098582296 | validation: 0.1250268066345801]
	TIME [epoch: 8.45 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15984334943635226		[learning rate: 2.1701e-05]
	Learning Rate: 2.17007e-05
	LOSS [training: 0.15984334943635226 | validation: 0.11723300629998208]
	TIME [epoch: 8.47 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1607983862881316		[learning rate: 2.1622e-05]
	Learning Rate: 2.16219e-05
	LOSS [training: 0.1607983862881316 | validation: 0.11653506353342216]
	TIME [epoch: 8.47 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1616101525628859		[learning rate: 2.1543e-05]
	Learning Rate: 2.15435e-05
	LOSS [training: 0.1616101525628859 | validation: 0.11275742565944133]
	TIME [epoch: 8.46 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15638657926065486		[learning rate: 2.1465e-05]
	Learning Rate: 2.14653e-05
	LOSS [training: 0.15638657926065486 | validation: 0.12442592062591588]
	TIME [epoch: 8.45 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16076880844552666		[learning rate: 2.1387e-05]
	Learning Rate: 2.13874e-05
	LOSS [training: 0.16076880844552666 | validation: 0.12131249555775926]
	TIME [epoch: 8.46 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1505625097458806		[learning rate: 2.131e-05]
	Learning Rate: 2.13098e-05
	LOSS [training: 0.1505625097458806 | validation: 0.11513687399014269]
	TIME [epoch: 8.46 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1604691832026067		[learning rate: 2.1232e-05]
	Learning Rate: 2.12325e-05
	LOSS [training: 0.1604691832026067 | validation: 0.1203916622048723]
	TIME [epoch: 8.45 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15777538064001756		[learning rate: 2.1155e-05]
	Learning Rate: 2.11554e-05
	LOSS [training: 0.15777538064001756 | validation: 0.11109404727641578]
	TIME [epoch: 8.45 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15755946998616238		[learning rate: 2.1079e-05]
	Learning Rate: 2.10786e-05
	LOSS [training: 0.15755946998616238 | validation: 0.11711487049899635]
	TIME [epoch: 8.46 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1591065040008884		[learning rate: 2.1002e-05]
	Learning Rate: 2.10021e-05
	LOSS [training: 0.1591065040008884 | validation: 0.11890700954229905]
	TIME [epoch: 8.47 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16036462163002055		[learning rate: 2.0926e-05]
	Learning Rate: 2.09259e-05
	LOSS [training: 0.16036462163002055 | validation: 0.11743964224433112]
	TIME [epoch: 8.46 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1677647357164033		[learning rate: 2.085e-05]
	Learning Rate: 2.085e-05
	LOSS [training: 0.1677647357164033 | validation: 0.11471215173770055]
	TIME [epoch: 8.45 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15981512768079603		[learning rate: 2.0774e-05]
	Learning Rate: 2.07743e-05
	LOSS [training: 0.15981512768079603 | validation: 0.12210211656402406]
	TIME [epoch: 8.45 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15826010275552724		[learning rate: 2.0699e-05]
	Learning Rate: 2.06989e-05
	LOSS [training: 0.15826010275552724 | validation: 0.11134936165516279]
	TIME [epoch: 8.47 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15418671072380424		[learning rate: 2.0624e-05]
	Learning Rate: 2.06238e-05
	LOSS [training: 0.15418671072380424 | validation: 0.12379588793460794]
	TIME [epoch: 8.46 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15398297511522846		[learning rate: 2.0549e-05]
	Learning Rate: 2.05489e-05
	LOSS [training: 0.15398297511522846 | validation: 0.11783821954378117]
	TIME [epoch: 8.45 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1571983908547472		[learning rate: 2.0474e-05]
	Learning Rate: 2.04744e-05
	LOSS [training: 0.1571983908547472 | validation: 0.11812482093313845]
	TIME [epoch: 8.48 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16240635588825766		[learning rate: 2.04e-05]
	Learning Rate: 2.04001e-05
	LOSS [training: 0.16240635588825766 | validation: 0.11866224034391268]
	TIME [epoch: 8.47 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.155682341725917		[learning rate: 2.0326e-05]
	Learning Rate: 2.0326e-05
	LOSS [training: 0.155682341725917 | validation: 0.13454831506358322]
	TIME [epoch: 8.46 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16043380019630976		[learning rate: 2.0252e-05]
	Learning Rate: 2.02523e-05
	LOSS [training: 0.16043380019630976 | validation: 0.12252696395623132]
	TIME [epoch: 8.45 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15692815020056594		[learning rate: 2.0179e-05]
	Learning Rate: 2.01788e-05
	LOSS [training: 0.15692815020056594 | validation: 0.12793110899243912]
	TIME [epoch: 8.48 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15549625609764034		[learning rate: 2.0106e-05]
	Learning Rate: 2.01055e-05
	LOSS [training: 0.15549625609764034 | validation: 0.13226216835107574]
	TIME [epoch: 8.47 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15900484147056654		[learning rate: 2.0033e-05]
	Learning Rate: 2.00326e-05
	LOSS [training: 0.15900484147056654 | validation: 0.12162136318362066]
	TIME [epoch: 8.46 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16226997194905482		[learning rate: 1.996e-05]
	Learning Rate: 1.99599e-05
	LOSS [training: 0.16226997194905482 | validation: 0.12235076880283464]
	TIME [epoch: 8.45 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15545875866418413		[learning rate: 1.9887e-05]
	Learning Rate: 1.98874e-05
	LOSS [training: 0.15545875866418413 | validation: 0.1245136325479041]
	TIME [epoch: 8.48 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15737055155042526		[learning rate: 1.9815e-05]
	Learning Rate: 1.98153e-05
	LOSS [training: 0.15737055155042526 | validation: 0.11046643522846564]
	TIME [epoch: 8.46 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15948432601586798		[learning rate: 1.9743e-05]
	Learning Rate: 1.97434e-05
	LOSS [training: 0.15948432601586798 | validation: 0.1146996358954302]
	TIME [epoch: 8.46 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15329994790426843		[learning rate: 1.9672e-05]
	Learning Rate: 1.96717e-05
	LOSS [training: 0.15329994790426843 | validation: 0.12268339050689835]
	TIME [epoch: 8.46 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16027331199880607		[learning rate: 1.96e-05]
	Learning Rate: 1.96003e-05
	LOSS [training: 0.16027331199880607 | validation: 0.13245463336949742]
	TIME [epoch: 8.48 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16088922460309105		[learning rate: 1.9529e-05]
	Learning Rate: 1.95292e-05
	LOSS [training: 0.16088922460309105 | validation: 0.12318838675605445]
	TIME [epoch: 8.46 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1576772039581376		[learning rate: 1.9458e-05]
	Learning Rate: 1.94583e-05
	LOSS [training: 0.1576772039581376 | validation: 0.11733693379262786]
	TIME [epoch: 8.46 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15640154600953088		[learning rate: 1.9388e-05]
	Learning Rate: 1.93877e-05
	LOSS [training: 0.15640154600953088 | validation: 0.11769136568345162]
	TIME [epoch: 8.46 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1625852561273961		[learning rate: 1.9317e-05]
	Learning Rate: 1.93173e-05
	LOSS [training: 0.1625852561273961 | validation: 0.10732849981611872]
	TIME [epoch: 8.48 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15496613673950987		[learning rate: 1.9247e-05]
	Learning Rate: 1.92472e-05
	LOSS [training: 0.15496613673950987 | validation: 0.13110712356505355]
	TIME [epoch: 8.47 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15344087839797754		[learning rate: 1.9177e-05]
	Learning Rate: 1.91774e-05
	LOSS [training: 0.15344087839797754 | validation: 0.12237870349824696]
	TIME [epoch: 8.47 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1631719507699032		[learning rate: 1.9108e-05]
	Learning Rate: 1.91078e-05
	LOSS [training: 0.1631719507699032 | validation: 0.1214830694299326]
	TIME [epoch: 8.45 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15424624782477056		[learning rate: 1.9038e-05]
	Learning Rate: 1.90384e-05
	LOSS [training: 0.15424624782477056 | validation: 0.11689163737973143]
	TIME [epoch: 8.48 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15570973420282785		[learning rate: 1.8969e-05]
	Learning Rate: 1.89694e-05
	LOSS [training: 0.15570973420282785 | validation: 0.12582304963889668]
	TIME [epoch: 8.45 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15944314231518547		[learning rate: 1.8901e-05]
	Learning Rate: 1.89005e-05
	LOSS [training: 0.15944314231518547 | validation: 0.11922106256848944]
	TIME [epoch: 8.46 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15837472573221412		[learning rate: 1.8832e-05]
	Learning Rate: 1.88319e-05
	LOSS [training: 0.15837472573221412 | validation: 0.13002726574794915]
	TIME [epoch: 8.45 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15124127362846035		[learning rate: 1.8764e-05]
	Learning Rate: 1.87636e-05
	LOSS [training: 0.15124127362846035 | validation: 0.11854417730200133]
	TIME [epoch: 8.48 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15936137835247247		[learning rate: 1.8695e-05]
	Learning Rate: 1.86955e-05
	LOSS [training: 0.15936137835247247 | validation: 0.11650020792767783]
	TIME [epoch: 8.46 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1629739179390572		[learning rate: 1.8628e-05]
	Learning Rate: 1.86276e-05
	LOSS [training: 0.1629739179390572 | validation: 0.11404851839393575]
	TIME [epoch: 8.46 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16193150459993366		[learning rate: 1.856e-05]
	Learning Rate: 1.856e-05
	LOSS [training: 0.16193150459993366 | validation: 0.11617087095237176]
	TIME [epoch: 8.45 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15691202662145876		[learning rate: 1.8493e-05]
	Learning Rate: 1.84927e-05
	LOSS [training: 0.15691202662145876 | validation: 0.11391782669899349]
	TIME [epoch: 8.48 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16425112608510253		[learning rate: 1.8426e-05]
	Learning Rate: 1.84256e-05
	LOSS [training: 0.16425112608510253 | validation: 0.11793530416070994]
	TIME [epoch: 8.46 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15312805999559156		[learning rate: 1.8359e-05]
	Learning Rate: 1.83587e-05
	LOSS [training: 0.15312805999559156 | validation: 0.1208910970014617]
	TIME [epoch: 8.45 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15602328964020806		[learning rate: 1.8292e-05]
	Learning Rate: 1.82921e-05
	LOSS [training: 0.15602328964020806 | validation: 0.11732115482550758]
	TIME [epoch: 8.46 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15731893198081578		[learning rate: 1.8226e-05]
	Learning Rate: 1.82257e-05
	LOSS [training: 0.15731893198081578 | validation: 0.12084319115372005]
	TIME [epoch: 8.47 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15879969283536383		[learning rate: 1.816e-05]
	Learning Rate: 1.81596e-05
	LOSS [training: 0.15879969283536383 | validation: 0.13110058556009724]
	TIME [epoch: 8.46 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16891360900415797		[learning rate: 1.8094e-05]
	Learning Rate: 1.80937e-05
	LOSS [training: 0.16891360900415797 | validation: 0.12492539873671822]
	TIME [epoch: 8.45 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15644752952807411		[learning rate: 1.8028e-05]
	Learning Rate: 1.8028e-05
	LOSS [training: 0.15644752952807411 | validation: 0.1218068709284689]
	TIME [epoch: 8.47 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15868715416814128		[learning rate: 1.7963e-05]
	Learning Rate: 1.79626e-05
	LOSS [training: 0.15868715416814128 | validation: 0.13102123094021637]
	TIME [epoch: 8.49 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16765576484340133		[learning rate: 1.7897e-05]
	Learning Rate: 1.78974e-05
	LOSS [training: 0.16765576484340133 | validation: 0.11997687164477336]
	TIME [epoch: 8.46 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15952252461402905		[learning rate: 1.7832e-05]
	Learning Rate: 1.78324e-05
	LOSS [training: 0.15952252461402905 | validation: 0.12802811691048457]
	TIME [epoch: 8.45 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16024509953685193		[learning rate: 1.7768e-05]
	Learning Rate: 1.77677e-05
	LOSS [training: 0.16024509953685193 | validation: 0.11510004833658052]
	TIME [epoch: 8.45 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1544216380792025		[learning rate: 1.7703e-05]
	Learning Rate: 1.77032e-05
	LOSS [training: 0.1544216380792025 | validation: 0.11304726384228482]
	TIME [epoch: 8.48 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1525367037576021		[learning rate: 1.7639e-05]
	Learning Rate: 1.7639e-05
	LOSS [training: 0.1525367037576021 | validation: 0.12147979719647363]
	TIME [epoch: 8.45 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15559571121758048		[learning rate: 1.7575e-05]
	Learning Rate: 1.7575e-05
	LOSS [training: 0.15559571121758048 | validation: 0.12486470811446729]
	TIME [epoch: 8.45 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15664981847743745		[learning rate: 1.7511e-05]
	Learning Rate: 1.75112e-05
	LOSS [training: 0.15664981847743745 | validation: 0.11588479188424279]
	TIME [epoch: 8.46 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15832372404201903		[learning rate: 1.7448e-05]
	Learning Rate: 1.74476e-05
	LOSS [training: 0.15832372404201903 | validation: 0.12222372751341812]
	TIME [epoch: 8.45 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16329594717701798		[learning rate: 1.7384e-05]
	Learning Rate: 1.73843e-05
	LOSS [training: 0.16329594717701798 | validation: 0.12136663107989021]
	TIME [epoch: 8.45 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16246780385147475		[learning rate: 1.7321e-05]
	Learning Rate: 1.73212e-05
	LOSS [training: 0.16246780385147475 | validation: 0.1213732289926959]
	TIME [epoch: 8.46 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15786793343675595		[learning rate: 1.7258e-05]
	Learning Rate: 1.72584e-05
	LOSS [training: 0.15786793343675595 | validation: 0.11015368410800115]
	TIME [epoch: 8.47 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15798930518220272		[learning rate: 1.7196e-05]
	Learning Rate: 1.71957e-05
	LOSS [training: 0.15798930518220272 | validation: 0.12969188381180852]
	TIME [epoch: 8.45 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15646020949455827		[learning rate: 1.7133e-05]
	Learning Rate: 1.71333e-05
	LOSS [training: 0.15646020949455827 | validation: 0.11571216455071333]
	TIME [epoch: 8.45 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15519389705985265		[learning rate: 1.7071e-05]
	Learning Rate: 1.70712e-05
	LOSS [training: 0.15519389705985265 | validation: 0.11183681958352895]
	TIME [epoch: 8.44 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15366593176671345		[learning rate: 1.7009e-05]
	Learning Rate: 1.70092e-05
	LOSS [training: 0.15366593176671345 | validation: 0.11881002105612642]
	TIME [epoch: 8.47 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16014752999617538		[learning rate: 1.6947e-05]
	Learning Rate: 1.69475e-05
	LOSS [training: 0.16014752999617538 | validation: 0.12616065426263107]
	TIME [epoch: 8.44 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16234457927445003		[learning rate: 1.6886e-05]
	Learning Rate: 1.6886e-05
	LOSS [training: 0.16234457927445003 | validation: 0.11455151495087028]
	TIME [epoch: 8.44 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15574990960480434		[learning rate: 1.6825e-05]
	Learning Rate: 1.68247e-05
	LOSS [training: 0.15574990960480434 | validation: 0.1201388869151759]
	TIME [epoch: 8.45 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15814724689845647		[learning rate: 1.6764e-05]
	Learning Rate: 1.67636e-05
	LOSS [training: 0.15814724689845647 | validation: 0.12224939802618978]
	TIME [epoch: 8.46 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15810945536990859		[learning rate: 1.6703e-05]
	Learning Rate: 1.67028e-05
	LOSS [training: 0.15810945536990859 | validation: 0.1313226891609361]
	TIME [epoch: 8.44 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15585876177003805		[learning rate: 1.6642e-05]
	Learning Rate: 1.66422e-05
	LOSS [training: 0.15585876177003805 | validation: 0.12413295147502038]
	TIME [epoch: 8.44 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16767960740595403		[learning rate: 1.6582e-05]
	Learning Rate: 1.65818e-05
	LOSS [training: 0.16767960740595403 | validation: 0.12437009228499324]
	TIME [epoch: 8.45 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16104564866828247		[learning rate: 1.6522e-05]
	Learning Rate: 1.65216e-05
	LOSS [training: 0.16104564866828247 | validation: 0.12629645352771618]
	TIME [epoch: 8.46 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15356698318350404		[learning rate: 1.6462e-05]
	Learning Rate: 1.64617e-05
	LOSS [training: 0.15356698318350404 | validation: 0.12547235757885583]
	TIME [epoch: 8.45 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16668656170906843		[learning rate: 1.6402e-05]
	Learning Rate: 1.64019e-05
	LOSS [training: 0.16668656170906843 | validation: 0.11557010809004914]
	TIME [epoch: 8.44 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16726067006419693		[learning rate: 1.6342e-05]
	Learning Rate: 1.63424e-05
	LOSS [training: 0.16726067006419693 | validation: 0.12944422460545013]
	TIME [epoch: 8.43 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15268193198101926		[learning rate: 1.6283e-05]
	Learning Rate: 1.62831e-05
	LOSS [training: 0.15268193198101926 | validation: 0.13875331961215634]
	TIME [epoch: 8.46 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16257799002759504		[learning rate: 1.6224e-05]
	Learning Rate: 1.6224e-05
	LOSS [training: 0.16257799002759504 | validation: 0.12958624006493819]
	TIME [epoch: 8.44 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15920289368114363		[learning rate: 1.6165e-05]
	Learning Rate: 1.61651e-05
	LOSS [training: 0.15920289368114363 | validation: 0.11962206042850795]
	TIME [epoch: 8.44 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.156733253594312		[learning rate: 1.6106e-05]
	Learning Rate: 1.61065e-05
	LOSS [training: 0.156733253594312 | validation: 0.12484823984948237]
	TIME [epoch: 8.44 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15750738124541747		[learning rate: 1.6048e-05]
	Learning Rate: 1.6048e-05
	LOSS [training: 0.15750738124541747 | validation: 0.12442995174445662]
	TIME [epoch: 8.47 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16320620261758712		[learning rate: 1.599e-05]
	Learning Rate: 1.59898e-05
	LOSS [training: 0.16320620261758712 | validation: 0.12170896537593828]
	TIME [epoch: 8.45 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1549006536025293		[learning rate: 1.5932e-05]
	Learning Rate: 1.59317e-05
	LOSS [training: 0.1549006536025293 | validation: 0.12019503465313741]
	TIME [epoch: 8.44 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1572143962529698		[learning rate: 1.5874e-05]
	Learning Rate: 1.58739e-05
	LOSS [training: 0.1572143962529698 | validation: 0.10756928355577017]
	TIME [epoch: 8.45 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1572142373857262		[learning rate: 1.5816e-05]
	Learning Rate: 1.58163e-05
	LOSS [training: 0.1572142373857262 | validation: 0.12229054654853998]
	TIME [epoch: 8.46 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15729888310846138		[learning rate: 1.5759e-05]
	Learning Rate: 1.57589e-05
	LOSS [training: 0.15729888310846138 | validation: 0.12083027059509444]
	TIME [epoch: 8.45 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15432197045325197		[learning rate: 1.5702e-05]
	Learning Rate: 1.57017e-05
	LOSS [training: 0.15432197045325197 | validation: 0.11595120114581396]
	TIME [epoch: 8.46 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15277710635806876		[learning rate: 1.5645e-05]
	Learning Rate: 1.56447e-05
	LOSS [training: 0.15277710635806876 | validation: 0.10796173780151838]
	TIME [epoch: 8.44 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15370517367409356		[learning rate: 1.5588e-05]
	Learning Rate: 1.5588e-05
	LOSS [training: 0.15370517367409356 | validation: 0.11630799312151888]
	TIME [epoch: 8.47 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16038953968369943		[learning rate: 1.5531e-05]
	Learning Rate: 1.55314e-05
	LOSS [training: 0.16038953968369943 | validation: 0.12327146103933999]
	TIME [epoch: 8.46 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1548457123960339		[learning rate: 1.5475e-05]
	Learning Rate: 1.5475e-05
	LOSS [training: 0.1548457123960339 | validation: 0.11911701445038866]
	TIME [epoch: 8.45 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15886706304642123		[learning rate: 1.5419e-05]
	Learning Rate: 1.54189e-05
	LOSS [training: 0.15886706304642123 | validation: 0.11745049222555995]
	TIME [epoch: 8.45 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15776867907560146		[learning rate: 1.5363e-05]
	Learning Rate: 1.53629e-05
	LOSS [training: 0.15776867907560146 | validation: 0.123077451655839]
	TIME [epoch: 8.49 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1508565292406034		[learning rate: 1.5307e-05]
	Learning Rate: 1.53072e-05
	LOSS [training: 0.1508565292406034 | validation: 0.12636787747467887]
	TIME [epoch: 8.47 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15704425964294585		[learning rate: 1.5252e-05]
	Learning Rate: 1.52516e-05
	LOSS [training: 0.15704425964294585 | validation: 0.11819587540370963]
	TIME [epoch: 8.45 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16010579961981794		[learning rate: 1.5196e-05]
	Learning Rate: 1.51963e-05
	LOSS [training: 0.16010579961981794 | validation: 0.11950121505275135]
	TIME [epoch: 8.45 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15993461920462132		[learning rate: 1.5141e-05]
	Learning Rate: 1.51411e-05
	LOSS [training: 0.15993461920462132 | validation: 0.10838245026262908]
	TIME [epoch: 8.47 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15458866012970557		[learning rate: 1.5086e-05]
	Learning Rate: 1.50862e-05
	LOSS [training: 0.15458866012970557 | validation: 0.11668509664246299]
	TIME [epoch: 8.45 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15532115804688623		[learning rate: 1.5031e-05]
	Learning Rate: 1.50314e-05
	LOSS [training: 0.15532115804688623 | validation: 0.12324123926394957]
	TIME [epoch: 8.46 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16190625001740072		[learning rate: 1.4977e-05]
	Learning Rate: 1.49769e-05
	LOSS [training: 0.16190625001740072 | validation: 0.11475827516780189]
	TIME [epoch: 8.47 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.167044531210014		[learning rate: 1.4923e-05]
	Learning Rate: 1.49225e-05
	LOSS [training: 0.167044531210014 | validation: 0.11824316209446883]
	TIME [epoch: 8.47 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15665367386355764		[learning rate: 1.4868e-05]
	Learning Rate: 1.48684e-05
	LOSS [training: 0.15665367386355764 | validation: 0.12129555125113595]
	TIME [epoch: 8.46 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15844843844238388		[learning rate: 1.4814e-05]
	Learning Rate: 1.48144e-05
	LOSS [training: 0.15844843844238388 | validation: 0.1248961532212305]
	TIME [epoch: 8.45 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1583703567256154		[learning rate: 1.4761e-05]
	Learning Rate: 1.47606e-05
	LOSS [training: 0.1583703567256154 | validation: 0.1210402567720961]
	TIME [epoch: 8.48 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15425248221404814		[learning rate: 1.4707e-05]
	Learning Rate: 1.47071e-05
	LOSS [training: 0.15425248221404814 | validation: 0.1256954582485569]
	TIME [epoch: 8.46 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15503614887260375		[learning rate: 1.4654e-05]
	Learning Rate: 1.46537e-05
	LOSS [training: 0.15503614887260375 | validation: 0.1133014573558273]
	TIME [epoch: 8.46 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15871599412908272		[learning rate: 1.4601e-05]
	Learning Rate: 1.46005e-05
	LOSS [training: 0.15871599412908272 | validation: 0.11313350686086336]
	TIME [epoch: 8.45 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16000859805015513		[learning rate: 1.4548e-05]
	Learning Rate: 1.45475e-05
	LOSS [training: 0.16000859805015513 | validation: 0.11021991810304432]
	TIME [epoch: 8.48 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15513161763767047		[learning rate: 1.4495e-05]
	Learning Rate: 1.44947e-05
	LOSS [training: 0.15513161763767047 | validation: 0.11831199115900944]
	TIME [epoch: 8.48 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15830039540395532		[learning rate: 1.4442e-05]
	Learning Rate: 1.44421e-05
	LOSS [training: 0.15830039540395532 | validation: 0.12417345454997201]
	TIME [epoch: 8.47 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1607222757379221		[learning rate: 1.439e-05]
	Learning Rate: 1.43897e-05
	LOSS [training: 0.1607222757379221 | validation: 0.12122354498202816]
	TIME [epoch: 8.46 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15352263480858835		[learning rate: 1.4338e-05]
	Learning Rate: 1.43375e-05
	LOSS [training: 0.15352263480858835 | validation: 0.11656746520793197]
	TIME [epoch: 8.47 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15177843406230518		[learning rate: 1.4285e-05]
	Learning Rate: 1.42855e-05
	LOSS [training: 0.15177843406230518 | validation: 0.12505025314081936]
	TIME [epoch: 8.45 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15957868656711927		[learning rate: 1.4234e-05]
	Learning Rate: 1.42336e-05
	LOSS [training: 0.15957868656711927 | validation: 0.13498494377928622]
	TIME [epoch: 8.45 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15544042882313913		[learning rate: 1.4182e-05]
	Learning Rate: 1.4182e-05
	LOSS [training: 0.15544042882313913 | validation: 0.12156434760751814]
	TIME [epoch: 8.45 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15654413317061674		[learning rate: 1.4131e-05]
	Learning Rate: 1.41305e-05
	LOSS [training: 0.15654413317061674 | validation: 0.11854314048791514]
	TIME [epoch: 8.48 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16265538336769209		[learning rate: 1.4079e-05]
	Learning Rate: 1.40792e-05
	LOSS [training: 0.16265538336769209 | validation: 0.11981524312672798]
	TIME [epoch: 8.46 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1561642700183084		[learning rate: 1.4028e-05]
	Learning Rate: 1.40281e-05
	LOSS [training: 0.1561642700183084 | validation: 0.12640127716486205]
	TIME [epoch: 8.47 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16291813390580775		[learning rate: 1.3977e-05]
	Learning Rate: 1.39772e-05
	LOSS [training: 0.16291813390580775 | validation: 0.11754679012524463]
	TIME [epoch: 8.46 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15504078943020266		[learning rate: 1.3927e-05]
	Learning Rate: 1.39265e-05
	LOSS [training: 0.15504078943020266 | validation: 0.12378028357861748]
	TIME [epoch: 8.48 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16101837325116652		[learning rate: 1.3876e-05]
	Learning Rate: 1.3876e-05
	LOSS [training: 0.16101837325116652 | validation: 0.12450018484118963]
	TIME [epoch: 8.47 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16454726037140582		[learning rate: 1.3826e-05]
	Learning Rate: 1.38256e-05
	LOSS [training: 0.16454726037140582 | validation: 0.12067107508375916]
	TIME [epoch: 8.46 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16285843200333033		[learning rate: 1.3775e-05]
	Learning Rate: 1.37754e-05
	LOSS [training: 0.16285843200333033 | validation: 0.14102139472507008]
	TIME [epoch: 8.48 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1666991628436621		[learning rate: 1.3725e-05]
	Learning Rate: 1.37254e-05
	LOSS [training: 0.1666991628436621 | validation: 0.1298425744103427]
	TIME [epoch: 8.47 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15243919945515866		[learning rate: 1.3676e-05]
	Learning Rate: 1.36756e-05
	LOSS [training: 0.15243919945515866 | validation: 0.12316263482043271]
	TIME [epoch: 8.47 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15933117992876736		[learning rate: 1.3626e-05]
	Learning Rate: 1.3626e-05
	LOSS [training: 0.15933117992876736 | validation: 0.12463686276841765]
	TIME [epoch: 8.46 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15932574070113506		[learning rate: 1.3577e-05]
	Learning Rate: 1.35766e-05
	LOSS [training: 0.15932574070113506 | validation: 0.11040960140236086]
	TIME [epoch: 8.46 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1555243150488989		[learning rate: 1.3527e-05]
	Learning Rate: 1.35273e-05
	LOSS [training: 0.1555243150488989 | validation: 0.11275611827189844]
	TIME [epoch: 8.48 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15961611378033103		[learning rate: 1.3478e-05]
	Learning Rate: 1.34782e-05
	LOSS [training: 0.15961611378033103 | validation: 0.11998256418303246]
	TIME [epoch: 8.45 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15480355521172307		[learning rate: 1.3429e-05]
	Learning Rate: 1.34293e-05
	LOSS [training: 0.15480355521172307 | validation: 0.13063324497430578]
	TIME [epoch: 8.44 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14988661769652972		[learning rate: 1.3381e-05]
	Learning Rate: 1.33805e-05
	LOSS [training: 0.14988661769652972 | validation: 0.12748014978513503]
	TIME [epoch: 8.44 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1574508907777767		[learning rate: 1.3332e-05]
	Learning Rate: 1.3332e-05
	LOSS [training: 0.1574508907777767 | validation: 0.1279946435922543]
	TIME [epoch: 8.47 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14942050522740832		[learning rate: 1.3284e-05]
	Learning Rate: 1.32836e-05
	LOSS [training: 0.14942050522740832 | validation: 0.11915238123715988]
	TIME [epoch: 8.44 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16555433637878694		[learning rate: 1.3235e-05]
	Learning Rate: 1.32354e-05
	LOSS [training: 0.16555433637878694 | validation: 0.10976611502001962]
	TIME [epoch: 8.44 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16258425282596622		[learning rate: 1.3187e-05]
	Learning Rate: 1.31874e-05
	LOSS [training: 0.16258425282596622 | validation: 0.12295895310342717]
	TIME [epoch: 8.45 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16417896059415965		[learning rate: 1.314e-05]
	Learning Rate: 1.31395e-05
	LOSS [training: 0.16417896059415965 | validation: 0.10887299246553525]
	TIME [epoch: 8.46 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15975890068302917		[learning rate: 1.3092e-05]
	Learning Rate: 1.30918e-05
	LOSS [training: 0.15975890068302917 | validation: 0.11576090138936834]
	TIME [epoch: 8.44 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16027627450771526		[learning rate: 1.3044e-05]
	Learning Rate: 1.30443e-05
	LOSS [training: 0.16027627450771526 | validation: 0.12490031580102778]
	TIME [epoch: 8.45 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15623543474636908		[learning rate: 1.2997e-05]
	Learning Rate: 1.2997e-05
	LOSS [training: 0.15623543474636908 | validation: 0.12068364637436362]
	TIME [epoch: 8.44 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16376979085281837		[learning rate: 1.295e-05]
	Learning Rate: 1.29498e-05
	LOSS [training: 0.16376979085281837 | validation: 0.12789748580636]
	TIME [epoch: 8.47 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1621627767357863		[learning rate: 1.2903e-05]
	Learning Rate: 1.29028e-05
	LOSS [training: 0.1621627767357863 | validation: 0.11773106323452151]
	TIME [epoch: 8.44 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1567135238705233		[learning rate: 1.2856e-05]
	Learning Rate: 1.2856e-05
	LOSS [training: 0.1567135238705233 | validation: 0.11466964880111644]
	TIME [epoch: 8.45 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16366052527281524		[learning rate: 1.2809e-05]
	Learning Rate: 1.28093e-05
	LOSS [training: 0.16366052527281524 | validation: 0.1186629308492454]
	TIME [epoch: 8.46 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15696669480866127		[learning rate: 1.2763e-05]
	Learning Rate: 1.27628e-05
	LOSS [training: 0.15696669480866127 | validation: 0.1243562292947745]
	TIME [epoch: 8.47 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15519419505382798		[learning rate: 1.2717e-05]
	Learning Rate: 1.27165e-05
	LOSS [training: 0.15519419505382798 | validation: 0.11136421563636009]
	TIME [epoch: 8.46 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15444196057282875		[learning rate: 1.267e-05]
	Learning Rate: 1.26704e-05
	LOSS [training: 0.15444196057282875 | validation: 0.12930280747752995]
	TIME [epoch: 8.44 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15778698932641766		[learning rate: 1.2624e-05]
	Learning Rate: 1.26244e-05
	LOSS [training: 0.15778698932641766 | validation: 0.11565216963638492]
	TIME [epoch: 8.45 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15639600112605834		[learning rate: 1.2579e-05]
	Learning Rate: 1.25786e-05
	LOSS [training: 0.15639600112605834 | validation: 0.1160860544127098]
	TIME [epoch: 8.45 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.153412184875509		[learning rate: 1.2533e-05]
	Learning Rate: 1.25329e-05
	LOSS [training: 0.153412184875509 | validation: 0.11977033062436632]
	TIME [epoch: 8.45 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15398277812677588		[learning rate: 1.2487e-05]
	Learning Rate: 1.24874e-05
	LOSS [training: 0.15398277812677588 | validation: 0.11991791558018988]
	TIME [epoch: 8.46 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15827045649770083		[learning rate: 1.2442e-05]
	Learning Rate: 1.24421e-05
	LOSS [training: 0.15827045649770083 | validation: 0.12009453417061437]
	TIME [epoch: 8.48 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15852346369092124		[learning rate: 1.2397e-05]
	Learning Rate: 1.2397e-05
	LOSS [training: 0.15852346369092124 | validation: 0.12435786898691004]
	TIME [epoch: 8.46 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16115552994124385		[learning rate: 1.2352e-05]
	Learning Rate: 1.2352e-05
	LOSS [training: 0.16115552994124385 | validation: 0.11854823867900813]
	TIME [epoch: 8.47 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1679299087326199		[learning rate: 1.2307e-05]
	Learning Rate: 1.23072e-05
	LOSS [training: 0.1679299087326199 | validation: 0.1211534518478207]
	TIME [epoch: 8.45 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15809666867516498		[learning rate: 1.2263e-05]
	Learning Rate: 1.22625e-05
	LOSS [training: 0.15809666867516498 | validation: 0.12002588740147505]
	TIME [epoch: 8.49 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1567775822111678		[learning rate: 1.2218e-05]
	Learning Rate: 1.2218e-05
	LOSS [training: 0.1567775822111678 | validation: 0.11984475162697879]
	TIME [epoch: 8.47 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1575394944400404		[learning rate: 1.2174e-05]
	Learning Rate: 1.21737e-05
	LOSS [training: 0.1575394944400404 | validation: 0.11646793882418471]
	TIME [epoch: 8.45 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15598630907477415		[learning rate: 1.2129e-05]
	Learning Rate: 1.21295e-05
	LOSS [training: 0.15598630907477415 | validation: 0.12079569308072083]
	TIME [epoch: 8.46 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16613007572530564		[learning rate: 1.2085e-05]
	Learning Rate: 1.20855e-05
	LOSS [training: 0.16613007572530564 | validation: 0.11123517175891737]
	TIME [epoch: 8.48 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1597197599762111		[learning rate: 1.2042e-05]
	Learning Rate: 1.20416e-05
	LOSS [training: 0.1597197599762111 | validation: 0.12020280324324742]
	TIME [epoch: 8.47 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15259434030631816		[learning rate: 1.1998e-05]
	Learning Rate: 1.19979e-05
	LOSS [training: 0.15259434030631816 | validation: 0.10765420220879506]
	TIME [epoch: 8.44 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16173916290588633		[learning rate: 1.1954e-05]
	Learning Rate: 1.19544e-05
	LOSS [training: 0.16173916290588633 | validation: 0.11575752181844685]
	TIME [epoch: 8.46 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15495108735509674		[learning rate: 1.1911e-05]
	Learning Rate: 1.1911e-05
	LOSS [training: 0.15495108735509674 | validation: 0.13195884571108726]
	TIME [epoch: 8.48 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15921821492995242		[learning rate: 1.1868e-05]
	Learning Rate: 1.18678e-05
	LOSS [training: 0.15921821492995242 | validation: 0.13520419652074706]
	TIME [epoch: 8.46 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1561360580424428		[learning rate: 1.1825e-05]
	Learning Rate: 1.18247e-05
	LOSS [training: 0.1561360580424428 | validation: 0.12271514417259569]
	TIME [epoch: 8.46 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15523303722161608		[learning rate: 1.1782e-05]
	Learning Rate: 1.17818e-05
	LOSS [training: 0.15523303722161608 | validation: 0.13354995680559875]
	TIME [epoch: 8.45 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15584157525712372		[learning rate: 1.1739e-05]
	Learning Rate: 1.1739e-05
	LOSS [training: 0.15584157525712372 | validation: 0.12139147518784321]
	TIME [epoch: 8.47 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1606153866485548		[learning rate: 1.1696e-05]
	Learning Rate: 1.16964e-05
	LOSS [training: 0.1606153866485548 | validation: 0.11377601190390106]
	TIME [epoch: 8.46 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1581993454934097		[learning rate: 1.1654e-05]
	Learning Rate: 1.1654e-05
	LOSS [training: 0.1581993454934097 | validation: 0.12455623776181084]
	TIME [epoch: 8.44 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1533957901355148		[learning rate: 1.1612e-05]
	Learning Rate: 1.16117e-05
	LOSS [training: 0.1533957901355148 | validation: 0.1210719621232299]
	TIME [epoch: 8.45 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1569503477562288		[learning rate: 1.157e-05]
	Learning Rate: 1.15695e-05
	LOSS [training: 0.1569503477562288 | validation: 0.1133489405828585]
	TIME [epoch: 8.46 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15713630754066982		[learning rate: 1.1528e-05]
	Learning Rate: 1.15275e-05
	LOSS [training: 0.15713630754066982 | validation: 0.1216389738572135]
	TIME [epoch: 8.45 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16075627989556512		[learning rate: 1.1486e-05]
	Learning Rate: 1.14857e-05
	LOSS [training: 0.16075627989556512 | validation: 0.11870174498655242]
	TIME [epoch: 8.44 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1576979328784545		[learning rate: 1.1444e-05]
	Learning Rate: 1.1444e-05
	LOSS [training: 0.1576979328784545 | validation: 0.10929667685128192]
	TIME [epoch: 8.44 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15741464875879257		[learning rate: 1.1402e-05]
	Learning Rate: 1.14025e-05
	LOSS [training: 0.15741464875879257 | validation: 0.11471852771843095]
	TIME [epoch: 8.47 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1599722010167845		[learning rate: 1.1361e-05]
	Learning Rate: 1.13611e-05
	LOSS [training: 0.1599722010167845 | validation: 0.10815707770014962]
	TIME [epoch: 8.44 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15783172779416224		[learning rate: 1.132e-05]
	Learning Rate: 1.13199e-05
	LOSS [training: 0.15783172779416224 | validation: 0.12329599995084384]
	TIME [epoch: 8.45 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1539837535147472		[learning rate: 1.1279e-05]
	Learning Rate: 1.12788e-05
	LOSS [training: 0.1539837535147472 | validation: 0.1341862090959203]
	TIME [epoch: 8.44 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15439816237781337		[learning rate: 1.1238e-05]
	Learning Rate: 1.12379e-05
	LOSS [training: 0.15439816237781337 | validation: 0.11913065798645639]
	TIME [epoch: 8.46 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16140387006243007		[learning rate: 1.1197e-05]
	Learning Rate: 1.11971e-05
	LOSS [training: 0.16140387006243007 | validation: 0.12374745212256197]
	TIME [epoch: 8.45 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16402081188915263		[learning rate: 1.1156e-05]
	Learning Rate: 1.11565e-05
	LOSS [training: 0.16402081188915263 | validation: 0.11164315992272356]
	TIME [epoch: 8.44 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15648245005548428		[learning rate: 1.1116e-05]
	Learning Rate: 1.1116e-05
	LOSS [training: 0.15648245005548428 | validation: 0.11907915792929004]
	TIME [epoch: 8.46 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15458484005497733		[learning rate: 1.1076e-05]
	Learning Rate: 1.10756e-05
	LOSS [training: 0.15458484005497733 | validation: 0.11698183511696655]
	TIME [epoch: 8.47 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16037616704793456		[learning rate: 1.1035e-05]
	Learning Rate: 1.10354e-05
	LOSS [training: 0.16037616704793456 | validation: 0.12139201203441677]
	TIME [epoch: 8.46 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15188881771521295		[learning rate: 1.0995e-05]
	Learning Rate: 1.09954e-05
	LOSS [training: 0.15188881771521295 | validation: 0.12337261588522136]
	TIME [epoch: 8.45 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15502392899683376		[learning rate: 1.0955e-05]
	Learning Rate: 1.09555e-05
	LOSS [training: 0.15502392899683376 | validation: 0.11239599395689423]
	TIME [epoch: 8.47 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15706067977028276		[learning rate: 1.0916e-05]
	Learning Rate: 1.09157e-05
	LOSS [training: 0.15706067977028276 | validation: 0.11135760055973087]
	TIME [epoch: 8.46 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15337811182802324		[learning rate: 1.0876e-05]
	Learning Rate: 1.08761e-05
	LOSS [training: 0.15337811182802324 | validation: 0.12999336582876436]
	TIME [epoch: 8.45 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15485294416258172		[learning rate: 1.0837e-05]
	Learning Rate: 1.08366e-05
	LOSS [training: 0.15485294416258172 | validation: 0.12315102026808059]
	TIME [epoch: 8.44 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1582593861738901		[learning rate: 1.0797e-05]
	Learning Rate: 1.07973e-05
	LOSS [training: 0.1582593861738901 | validation: 0.11765630942100735]
	TIME [epoch: 8.46 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15708049145059017		[learning rate: 1.0758e-05]
	Learning Rate: 1.07581e-05
	LOSS [training: 0.15708049145059017 | validation: 0.1212966866863336]
	TIME [epoch: 8.44 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15381740831482552		[learning rate: 1.0719e-05]
	Learning Rate: 1.07191e-05
	LOSS [training: 0.15381740831482552 | validation: 0.1149640573479222]
	TIME [epoch: 8.44 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15846223522803632		[learning rate: 1.068e-05]
	Learning Rate: 1.06802e-05
	LOSS [training: 0.15846223522803632 | validation: 0.12222753576344575]
	TIME [epoch: 8.46 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1489735507176791		[learning rate: 1.0641e-05]
	Learning Rate: 1.06414e-05
	LOSS [training: 0.1489735507176791 | validation: 0.13644548939958764]
	TIME [epoch: 8.47 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1629425607606672		[learning rate: 1.0603e-05]
	Learning Rate: 1.06028e-05
	LOSS [training: 0.1629425607606672 | validation: 0.1177425107676904]
	TIME [epoch: 8.45 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15631105898747805		[learning rate: 1.0564e-05]
	Learning Rate: 1.05643e-05
	LOSS [training: 0.15631105898747805 | validation: 0.1139817129326311]
	TIME [epoch: 8.46 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15542643706419773		[learning rate: 1.0526e-05]
	Learning Rate: 1.0526e-05
	LOSS [training: 0.15542643706419773 | validation: 0.11427524319398702]
	TIME [epoch: 8.44 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1564028132097987		[learning rate: 1.0488e-05]
	Learning Rate: 1.04878e-05
	LOSS [training: 0.1564028132097987 | validation: 0.11633578132924609]
	TIME [epoch: 8.48 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15511012923732562		[learning rate: 1.045e-05]
	Learning Rate: 1.04497e-05
	LOSS [training: 0.15511012923732562 | validation: 0.11249053953982993]
	TIME [epoch: 8.47 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15236327748566716		[learning rate: 1.0412e-05]
	Learning Rate: 1.04118e-05
	LOSS [training: 0.15236327748566716 | validation: 0.11971429117521304]
	TIME [epoch: 8.47 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15764451142034647		[learning rate: 1.0374e-05]
	Learning Rate: 1.0374e-05
	LOSS [training: 0.15764451142034647 | validation: 0.11308407190743452]
	TIME [epoch: 8.46 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15529862117938473		[learning rate: 1.0336e-05]
	Learning Rate: 1.03364e-05
	LOSS [training: 0.15529862117938473 | validation: 0.11539202089484435]
	TIME [epoch: 8.48 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16411985274991903		[learning rate: 1.0299e-05]
	Learning Rate: 1.02989e-05
	LOSS [training: 0.16411985274991903 | validation: 0.11919582851892649]
	TIME [epoch: 8.46 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15717872971146837		[learning rate: 1.0261e-05]
	Learning Rate: 1.02615e-05
	LOSS [training: 0.15717872971146837 | validation: 0.11133543255500356]
	TIME [epoch: 8.46 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15607960339886448		[learning rate: 1.0224e-05]
	Learning Rate: 1.02243e-05
	LOSS [training: 0.15607960339886448 | validation: 0.12295639604182454]
	TIME [epoch: 8.46 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15189104718980212		[learning rate: 1.0187e-05]
	Learning Rate: 1.01872e-05
	LOSS [training: 0.15189104718980212 | validation: 0.12012721728357742]
	TIME [epoch: 8.48 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15333823801266253		[learning rate: 1.015e-05]
	Learning Rate: 1.01502e-05
	LOSS [training: 0.15333823801266253 | validation: 0.12069490391334725]
	TIME [epoch: 8.46 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15270299129803494		[learning rate: 1.0113e-05]
	Learning Rate: 1.01133e-05
	LOSS [training: 0.15270299129803494 | validation: 0.12314654000173907]
	TIME [epoch: 8.47 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16067072637895946		[learning rate: 1.0077e-05]
	Learning Rate: 1.00766e-05
	LOSS [training: 0.16067072637895946 | validation: 0.11501282866998651]
	TIME [epoch: 8.48 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15873566177413242		[learning rate: 1.004e-05]
	Learning Rate: 1.00401e-05
	LOSS [training: 0.15873566177413242 | validation: 0.11972584306465171]
	TIME [epoch: 8.49 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1537852352118528		[learning rate: 1.0004e-05]
	Learning Rate: 1.00036e-05
	LOSS [training: 0.1537852352118528 | validation: 0.11123525890740302]
	TIME [epoch: 8.45 sec]
Finished training in 17015.261 seconds.
