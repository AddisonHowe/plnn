Args:
Namespace(name='model_tr_study203', outdir='out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5', training_data='data/transition_rate_studies/tr_study203/tr_study203_training/r5', validation_data='data/transition_rate_studies/tr_study203/tr_study203_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 128674610

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 12.880354613978195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.880354613978195 | validation: 12.213602287152323]
	TIME [epoch: 52.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.961513860784208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.961513860784208 | validation: 12.17869639819531]
	TIME [epoch: 8.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.188101254792773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.188101254792773 | validation: 9.729911895371114]
	TIME [epoch: 8.64 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.2262134530697155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.2262134530697155 | validation: 6.89922242117912]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.355782245575173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.355782245575173 | validation: 4.324232758786934]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.899745489648072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.899745489648072 | validation: 4.179528429463834]
	TIME [epoch: 8.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.818132371638627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.818132371638627 | validation: 4.803410857491016]
	TIME [epoch: 8.64 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.840040858748035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.840040858748035 | validation: 4.141597606125549]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.233305129836717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.233305129836717 | validation: 3.9969813555367035]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.542811611377141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.542811611377141 | validation: 4.185674107512116]
	TIME [epoch: 8.66 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.455365807626371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.455365807626371 | validation: 4.210386121094812]
	TIME [epoch: 8.64 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.442919812803441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.442919812803441 | validation: 4.149292727940524]
	TIME [epoch: 8.63 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.621597248852058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.621597248852058 | validation: 3.7278329971214195]
	TIME [epoch: 8.64 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.170561540330504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.170561540330504 | validation: 3.471236379164969]
	TIME [epoch: 8.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.324106988190324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.324106988190324 | validation: 3.3465002712770175]
	TIME [epoch: 8.64 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.269377737548563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.269377737548563 | validation: 3.364304177910066]
	TIME [epoch: 8.63 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.416513247314495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.416513247314495 | validation: 3.480242889086637]
	TIME [epoch: 8.65 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.921623682314929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.921623682314929 | validation: 3.64308243616677]
	TIME [epoch: 8.64 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.864735855875183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.864735855875183 | validation: 3.3923084866836657]
	TIME [epoch: 8.63 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.232352499318113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.232352499318113 | validation: 3.1288576493704907]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.93945703987554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.93945703987554 | validation: 4.490948626240838]
	TIME [epoch: 8.66 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.93951795338896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.93951795338896 | validation: 3.380161964845578]
	TIME [epoch: 8.63 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.778378717428273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.778378717428273 | validation: 3.09618476441407]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.597982246676949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.597982246676949 | validation: 2.9134307309358727]
	TIME [epoch: 8.65 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.755032053802782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.755032053802782 | validation: 2.90003222747422]
	TIME [epoch: 8.64 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.524656651987205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.524656651987205 | validation: 3.2108597729816966]
	TIME [epoch: 8.63 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.506065497773212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.506065497773212 | validation: 2.886307525379352]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.544672819576239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.544672819576239 | validation: 3.9015241290070835]
	TIME [epoch: 8.66 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.978848684204549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.978848684204549 | validation: 3.737346959679125]
	TIME [epoch: 8.64 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.562984121011442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.562984121011442 | validation: 3.156551048052969]
	TIME [epoch: 8.64 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.4874212339419035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4874212339419035 | validation: 2.570820715878954]
	TIME [epoch: 8.64 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.603414787993959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.603414787993959 | validation: 3.688286344397489]
	TIME [epoch: 8.66 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.478337393632836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.478337393632836 | validation: 3.1587676036018486]
	TIME [epoch: 8.64 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.431229761536797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.431229761536797 | validation: 5.023748405681001]
	TIME [epoch: 8.64 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.9789764468468025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9789764468468025 | validation: 5.050535680928208]
	TIME [epoch: 8.64 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.032936459828368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.032936459828368 | validation: 2.891028287873458]
	TIME [epoch: 8.66 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.488301800635227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.488301800635227 | validation: 2.7305123924323498]
	TIME [epoch: 8.64 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.393121393382237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.393121393382237 | validation: 2.604255885598522]
	TIME [epoch: 8.65 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.578021915299671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.578021915299671 | validation: 3.1974542955104415]
	TIME [epoch: 8.64 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.331715020325268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.331715020325268 | validation: 3.5446423056719567]
	TIME [epoch: 8.67 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.491341438757782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.491341438757782 | validation: 2.9839588775410197]
	TIME [epoch: 8.64 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.498172375074467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.498172375074467 | validation: 2.7353922218245437]
	TIME [epoch: 8.64 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.390313922720542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.390313922720542 | validation: 2.412773173188852]
	TIME [epoch: 8.64 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.789888920554668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.789888920554668 | validation: 3.5016846326679625]
	TIME [epoch: 8.66 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.4301412799380495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4301412799380495 | validation: 2.345521837631937]
	TIME [epoch: 8.64 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.360179013945105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.360179013945105 | validation: 2.749813498290383]
	TIME [epoch: 8.64 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.374246647330213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.374246647330213 | validation: 2.472864608866174]
	TIME [epoch: 8.64 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.435724647426999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.435724647426999 | validation: 2.3136523746653355]
	TIME [epoch: 8.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.234800728067541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.234800728067541 | validation: 3.335442418282149]
	TIME [epoch: 8.64 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.373383595375918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.373383595375918 | validation: 2.8364808941994135]
	TIME [epoch: 8.63 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.398155245417313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.398155245417313 | validation: 2.7221691941004886]
	TIME [epoch: 8.63 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.5352183600261755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5352183600261755 | validation: 2.8909580208004373]
	TIME [epoch: 8.65 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.3837220603330405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3837220603330405 | validation: 2.651490499207881]
	TIME [epoch: 8.63 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.3156820729880625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3156820729880625 | validation: 2.768529418195877]
	TIME [epoch: 8.63 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.271761564190929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.271761564190929 | validation: 2.789871327780881]
	TIME [epoch: 8.64 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.298033093459176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.298033093459176 | validation: 3.471627038384124]
	TIME [epoch: 8.65 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.5759728977372145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5759728977372145 | validation: 2.5321874757312393]
	TIME [epoch: 8.63 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.370119652170166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.370119652170166 | validation: 3.134693512633513]
	TIME [epoch: 8.63 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.256069414665655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.256069414665655 | validation: 3.1331000215649287]
	TIME [epoch: 8.63 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.324735700191239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.324735700191239 | validation: 2.2608713028905605]
	TIME [epoch: 8.65 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.199862402131819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.199862402131819 | validation: 2.586566638661592]
	TIME [epoch: 8.63 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.261800658578806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.261800658578806 | validation: 2.5974755619410947]
	TIME [epoch: 8.63 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.618979722107818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.618979722107818 | validation: 2.195844677811713]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.50027330208029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.50027330208029 | validation: 2.8219689063791065]
	TIME [epoch: 8.64 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.172245178754816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.172245178754816 | validation: 2.9580058609504967]
	TIME [epoch: 8.63 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.2311492112357705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2311492112357705 | validation: 2.8136779901114384]
	TIME [epoch: 8.62 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.2128319382808534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2128319382808534 | validation: 2.252634666763463]
	TIME [epoch: 8.63 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.2013812994450195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2013812994450195 | validation: 3.0146542988876606]
	TIME [epoch: 8.64 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.9312976393788786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9312976393788786 | validation: 2.6989036663746533]
	TIME [epoch: 8.63 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.536116437931836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.536116437931836 | validation: 2.6647447593057048]
	TIME [epoch: 8.62 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.234192846505755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.234192846505755 | validation: 3.6872382315605448]
	TIME [epoch: 8.65 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.26675772317394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.26675772317394 | validation: 2.663736203787962]
	TIME [epoch: 8.64 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1305494673858005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1305494673858005 | validation: 2.3245607272629782]
	TIME [epoch: 8.63 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1143751782631295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1143751782631295 | validation: 2.9092551005707294]
	TIME [epoch: 8.62 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.449264856787474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.449264856787474 | validation: 3.386504712731621]
	TIME [epoch: 8.66 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.240616176692547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.240616176692547 | validation: 2.2700426071090303]
	TIME [epoch: 8.63 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.0549356074053575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0549356074053575 | validation: 2.9166911933053283]
	TIME [epoch: 8.62 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.736390989577463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.736390989577463 | validation: 4.890204904417786]
	TIME [epoch: 8.63 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.992085587494115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.992085587494115 | validation: 2.855753490255163]
	TIME [epoch: 8.64 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.3432676599074895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3432676599074895 | validation: 2.3013476478888286]
	TIME [epoch: 8.63 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.402224992710742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.402224992710742 | validation: 2.492963593310047]
	TIME [epoch: 8.63 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.14947308760097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.14947308760097 | validation: 2.8142344393491707]
	TIME [epoch: 8.65 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.112511674806135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.112511674806135 | validation: 2.9553378224272118]
	TIME [epoch: 8.63 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.078432829584438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.078432829584438 | validation: 2.272359193876081]
	TIME [epoch: 8.63 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.5856106745807494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5856106745807494 | validation: 3.8285558584518555]
	TIME [epoch: 8.63 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.525160582997538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.525160582997538 | validation: 2.3994008539515574]
	TIME [epoch: 8.66 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.110939917993763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.110939917993763 | validation: 2.2584040034161306]
	TIME [epoch: 8.63 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.3999060788380975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3999060788380975 | validation: 3.567510459996619]
	TIME [epoch: 8.63 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.393467266019849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.393467266019849 | validation: 3.504383136889424]
	TIME [epoch: 8.63 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.330132698310513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.330132698310513 | validation: 2.2149746930729215]
	TIME [epoch: 8.65 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.2198675689643546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2198675689643546 | validation: 2.3274519127108437]
	TIME [epoch: 8.63 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.078983087102012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.078983087102012 | validation: 2.0821632434657595]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.983312725763117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.983312725763117 | validation: 3.3061421018858295]
	TIME [epoch: 8.65 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.6086193211499955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6086193211499955 | validation: 4.272298429356466]
	TIME [epoch: 8.63 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.602507014039843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.602507014039843 | validation: 3.240643999210518]
	TIME [epoch: 8.62 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.348694588804891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.348694588804891 | validation: 2.771329378567765]
	TIME [epoch: 8.62 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.373815854290305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.373815854290305 | validation: 2.921576258742466]
	TIME [epoch: 8.64 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.074998529710213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.074998529710213 | validation: 2.454624190754556]
	TIME [epoch: 8.62 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.17410511945668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.17410511945668 | validation: 2.0667822145190193]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.404716394678031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.404716394678031 | validation: 2.3357542598982466]
	TIME [epoch: 8.62 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.01992708323541		[learning rate: 0.0099673]
	Learning Rate: 0.00996733
	LOSS [training: 4.01992708323541 | validation: 3.371425381684793]
	TIME [epoch: 8.65 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.068363396962749		[learning rate: 0.0099312]
	Learning Rate: 0.00993116
	LOSS [training: 4.068363396962749 | validation: 2.5177520380143115]
	TIME [epoch: 8.63 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.839887546327352		[learning rate: 0.0098951]
	Learning Rate: 0.00989512
	LOSS [training: 3.839887546327352 | validation: 2.387927933216589]
	TIME [epoch: 8.62 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.955869713961546		[learning rate: 0.0098592]
	Learning Rate: 0.00985921
	LOSS [training: 3.955869713961546 | validation: 1.952051659865426]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9334674502614297		[learning rate: 0.0098234]
	Learning Rate: 0.00982343
	LOSS [training: 3.9334674502614297 | validation: 2.1252356773890004]
	TIME [epoch: 8.64 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.005026812423723		[learning rate: 0.0097878]
	Learning Rate: 0.00978778
	LOSS [training: 4.005026812423723 | validation: 1.9180310373778409]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.22141133441		[learning rate: 0.0097523]
	Learning Rate: 0.00975226
	LOSS [training: 4.22141133441 | validation: 2.1055396068618886]
	TIME [epoch: 8.64 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.980170360050143		[learning rate: 0.0097169]
	Learning Rate: 0.00971687
	LOSS [training: 3.980170360050143 | validation: 3.3165110703203053]
	TIME [epoch: 8.64 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.180456132200435		[learning rate: 0.0096816]
	Learning Rate: 0.0096816
	LOSS [training: 4.180456132200435 | validation: 2.332830834519638]
	TIME [epoch: 8.67 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.368786686724876		[learning rate: 0.0096465]
	Learning Rate: 0.00964647
	LOSS [training: 4.368786686724876 | validation: 2.1139281877140608]
	TIME [epoch: 8.64 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.902107306129352		[learning rate: 0.0096115]
	Learning Rate: 0.00961146
	LOSS [training: 3.902107306129352 | validation: 2.052104654095473]
	TIME [epoch: 8.64 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8008118434615286		[learning rate: 0.0095766]
	Learning Rate: 0.00957658
	LOSS [training: 3.8008118434615286 | validation: 2.0273973623779344]
	TIME [epoch: 8.64 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8136477196718053		[learning rate: 0.0095418]
	Learning Rate: 0.00954183
	LOSS [training: 3.8136477196718053 | validation: 2.0376664160242797]
	TIME [epoch: 8.66 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8707065003002668		[learning rate: 0.0095072]
	Learning Rate: 0.0095072
	LOSS [training: 3.8707065003002668 | validation: 1.9593967720136538]
	TIME [epoch: 8.64 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.028366168729793		[learning rate: 0.0094727]
	Learning Rate: 0.0094727
	LOSS [training: 4.028366168729793 | validation: 2.6157881734274167]
	TIME [epoch: 8.64 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.112392717154625		[learning rate: 0.0094383]
	Learning Rate: 0.00943832
	LOSS [training: 4.112392717154625 | validation: 1.9552154613122827]
	TIME [epoch: 8.65 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8814124367851286		[learning rate: 0.0094041]
	Learning Rate: 0.00940407
	LOSS [training: 3.8814124367851286 | validation: 1.9384347944229021]
	TIME [epoch: 8.66 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.071977300714278		[learning rate: 0.0093699]
	Learning Rate: 0.00936994
	LOSS [training: 4.071977300714278 | validation: 3.2912705500772366]
	TIME [epoch: 8.64 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.2018392879321285		[learning rate: 0.0093359]
	Learning Rate: 0.00933594
	LOSS [training: 4.2018392879321285 | validation: 3.088886416066272]
	TIME [epoch: 8.64 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9353658426250617		[learning rate: 0.0093021]
	Learning Rate: 0.00930206
	LOSS [training: 3.9353658426250617 | validation: 2.905057794880735]
	TIME [epoch: 8.66 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.017972598088446		[learning rate: 0.0092683]
	Learning Rate: 0.0092683
	LOSS [training: 4.017972598088446 | validation: 1.798511187054233]
	TIME [epoch: 8.65 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.211903887628778		[learning rate: 0.0092347]
	Learning Rate: 0.00923466
	LOSS [training: 4.211903887628778 | validation: 1.8751551416851402]
	TIME [epoch: 8.64 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7704344821559674		[learning rate: 0.0092011]
	Learning Rate: 0.00920115
	LOSS [training: 3.7704344821559674 | validation: 1.9688093919844993]
	TIME [epoch: 8.63 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8608182457669047		[learning rate: 0.0091678]
	Learning Rate: 0.00916776
	LOSS [training: 3.8608182457669047 | validation: 1.970323924868524]
	TIME [epoch: 8.65 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.477913226494218		[learning rate: 0.0091345]
	Learning Rate: 0.00913449
	LOSS [training: 4.477913226494218 | validation: 2.0817646130068224]
	TIME [epoch: 8.63 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.915010287509587		[learning rate: 0.0091013]
	Learning Rate: 0.00910134
	LOSS [training: 3.915010287509587 | validation: 2.329504910949911]
	TIME [epoch: 8.63 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.137279814253747		[learning rate: 0.0090683]
	Learning Rate: 0.00906831
	LOSS [training: 4.137279814253747 | validation: 3.1858156616715254]
	TIME [epoch: 8.63 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.271606591026844		[learning rate: 0.0090354]
	Learning Rate: 0.0090354
	LOSS [training: 4.271606591026844 | validation: 2.3765766801429327]
	TIME [epoch: 8.66 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7883392265627434		[learning rate: 0.0090026]
	Learning Rate: 0.00900261
	LOSS [training: 3.7883392265627434 | validation: 2.5460276791018606]
	TIME [epoch: 8.63 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.337114654449416		[learning rate: 0.0089699]
	Learning Rate: 0.00896994
	LOSS [training: 4.337114654449416 | validation: 2.727497893265699]
	TIME [epoch: 8.63 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.160840808035315		[learning rate: 0.0089374]
	Learning Rate: 0.00893739
	LOSS [training: 4.160840808035315 | validation: 2.312989556723399]
	TIME [epoch: 8.64 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.024634403720847		[learning rate: 0.008905]
	Learning Rate: 0.00890495
	LOSS [training: 4.024634403720847 | validation: 2.418957885414242]
	TIME [epoch: 8.65 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8926772126781204		[learning rate: 0.0088726]
	Learning Rate: 0.00887263
	LOSS [training: 3.8926772126781204 | validation: 2.4914428989674455]
	TIME [epoch: 8.63 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7802976310191134		[learning rate: 0.0088404]
	Learning Rate: 0.00884044
	LOSS [training: 3.7802976310191134 | validation: 1.9739415354614642]
	TIME [epoch: 8.63 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7216313595442854		[learning rate: 0.0088084]
	Learning Rate: 0.00880835
	LOSS [training: 3.7216313595442854 | validation: 2.3196985764541287]
	TIME [epoch: 8.65 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.148215204832868		[learning rate: 0.0087764]
	Learning Rate: 0.00877639
	LOSS [training: 4.148215204832868 | validation: 2.9478485958603553]
	TIME [epoch: 8.64 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.147884441529624		[learning rate: 0.0087445]
	Learning Rate: 0.00874454
	LOSS [training: 4.147884441529624 | validation: 2.959751001543884]
	TIME [epoch: 8.62 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8690655574349924		[learning rate: 0.0087128]
	Learning Rate: 0.0087128
	LOSS [training: 3.8690655574349924 | validation: 1.997073107944537]
	TIME [epoch: 8.63 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7973271713146604		[learning rate: 0.0086812]
	Learning Rate: 0.00868118
	LOSS [training: 3.7973271713146604 | validation: 2.198972879410036]
	TIME [epoch: 8.65 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.775088570183692		[learning rate: 0.0086497]
	Learning Rate: 0.00864968
	LOSS [training: 3.775088570183692 | validation: 2.276501663024294]
	TIME [epoch: 8.63 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7254922184584744		[learning rate: 0.0086183]
	Learning Rate: 0.00861829
	LOSS [training: 3.7254922184584744 | validation: 2.951668370583391]
	TIME [epoch: 8.63 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8237660135338323		[learning rate: 0.008587]
	Learning Rate: 0.00858701
	LOSS [training: 3.8237660135338323 | validation: 1.7704857102589737]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.67508462984438		[learning rate: 0.0085559]
	Learning Rate: 0.00855585
	LOSS [training: 3.67508462984438 | validation: 1.8431267720020477]
	TIME [epoch: 8.66 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8549218726037453		[learning rate: 0.0085248]
	Learning Rate: 0.0085248
	LOSS [training: 3.8549218726037453 | validation: 2.0497323940872354]
	TIME [epoch: 8.64 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1211503269811764		[learning rate: 0.0084939]
	Learning Rate: 0.00849386
	LOSS [training: 4.1211503269811764 | validation: 2.6099383776086995]
	TIME [epoch: 8.64 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9927040450756963		[learning rate: 0.008463]
	Learning Rate: 0.00846304
	LOSS [training: 3.9927040450756963 | validation: 3.3601236914273036]
	TIME [epoch: 8.64 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.883215446392814		[learning rate: 0.0084323]
	Learning Rate: 0.00843233
	LOSS [training: 3.883215446392814 | validation: 2.505652072630039]
	TIME [epoch: 8.65 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.763273126748961		[learning rate: 0.0084017]
	Learning Rate: 0.00840172
	LOSS [training: 3.763273126748961 | validation: 1.9168555411395678]
	TIME [epoch: 8.63 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7259568381239645		[learning rate: 0.0083712]
	Learning Rate: 0.00837123
	LOSS [training: 3.7259568381239645 | validation: 1.901189807434713]
	TIME [epoch: 8.63 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7030501807598384		[learning rate: 0.0083409]
	Learning Rate: 0.00834085
	LOSS [training: 3.7030501807598384 | validation: 2.5074232576344544]
	TIME [epoch: 8.63 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9770645933259927		[learning rate: 0.0083106]
	Learning Rate: 0.00831059
	LOSS [training: 3.9770645933259927 | validation: 2.524386770234668]
	TIME [epoch: 8.65 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9368490006101426		[learning rate: 0.0082804]
	Learning Rate: 0.00828043
	LOSS [training: 3.9368490006101426 | validation: 1.8170946661228338]
	TIME [epoch: 8.63 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.081836622040131		[learning rate: 0.0082504]
	Learning Rate: 0.00825037
	LOSS [training: 4.081836622040131 | validation: 2.3081210773553824]
	TIME [epoch: 8.63 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7412632569807465		[learning rate: 0.0082204]
	Learning Rate: 0.00822043
	LOSS [training: 3.7412632569807465 | validation: 2.092689291525741]
	TIME [epoch: 8.64 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.665016498511423		[learning rate: 0.0081906]
	Learning Rate: 0.0081906
	LOSS [training: 3.665016498511423 | validation: 1.7362986783095498]
	TIME [epoch: 8.65 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.0629145168369565		[learning rate: 0.0081609]
	Learning Rate: 0.00816088
	LOSS [training: 4.0629145168369565 | validation: 2.0712165553413913]
	TIME [epoch: 8.63 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6906255402970323		[learning rate: 0.0081313]
	Learning Rate: 0.00813126
	LOSS [training: 3.6906255402970323 | validation: 1.7312007991269285]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7296956560231207		[learning rate: 0.0081018]
	Learning Rate: 0.00810175
	LOSS [training: 3.7296956560231207 | validation: 2.2570582733390423]
	TIME [epoch: 8.66 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8819481720017857		[learning rate: 0.0080724]
	Learning Rate: 0.00807235
	LOSS [training: 3.8819481720017857 | validation: 1.7388212913014278]
	TIME [epoch: 8.63 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.691923380868257		[learning rate: 0.0080431]
	Learning Rate: 0.00804305
	LOSS [training: 3.691923380868257 | validation: 1.9664766377376297]
	TIME [epoch: 8.63 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6655142380918284		[learning rate: 0.0080139]
	Learning Rate: 0.00801387
	LOSS [training: 3.6655142380918284 | validation: 2.91008582150314]
	TIME [epoch: 8.63 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7853828404791843		[learning rate: 0.0079848]
	Learning Rate: 0.00798478
	LOSS [training: 3.7853828404791843 | validation: 2.0443143666035426]
	TIME [epoch: 8.65 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8128571266721503		[learning rate: 0.0079558]
	Learning Rate: 0.00795581
	LOSS [training: 3.8128571266721503 | validation: 1.9140211240456921]
	TIME [epoch: 8.63 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8646943847431117		[learning rate: 0.0079269]
	Learning Rate: 0.00792693
	LOSS [training: 3.8646943847431117 | validation: 2.601749548607947]
	TIME [epoch: 8.63 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7051957555057173		[learning rate: 0.0078982]
	Learning Rate: 0.00789817
	LOSS [training: 3.7051957555057173 | validation: 2.305939293250906]
	TIME [epoch: 8.63 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.624212107337191		[learning rate: 0.0078695]
	Learning Rate: 0.0078695
	LOSS [training: 3.624212107337191 | validation: 2.02255832034728]
	TIME [epoch: 8.65 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6681842051164324		[learning rate: 0.0078409]
	Learning Rate: 0.00784094
	LOSS [training: 3.6681842051164324 | validation: 1.8557852325797126]
	TIME [epoch: 8.63 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6542844821599063		[learning rate: 0.0078125]
	Learning Rate: 0.00781249
	LOSS [training: 3.6542844821599063 | validation: 1.6902924060825937]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6140574430993544		[learning rate: 0.0077841]
	Learning Rate: 0.00778414
	LOSS [training: 3.6140574430993544 | validation: 1.8130824406475585]
	TIME [epoch: 8.63 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.590748669627922		[learning rate: 0.0077559]
	Learning Rate: 0.00775589
	LOSS [training: 3.590748669627922 | validation: 2.950878854296154]
	TIME [epoch: 8.65 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.962388549782884		[learning rate: 0.0077277]
	Learning Rate: 0.00772774
	LOSS [training: 3.962388549782884 | validation: 2.0951517793470256]
	TIME [epoch: 8.63 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8516585967521317		[learning rate: 0.0076997]
	Learning Rate: 0.0076997
	LOSS [training: 3.8516585967521317 | validation: 2.1409878594591816]
	TIME [epoch: 8.62 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6650540327674874		[learning rate: 0.0076718]
	Learning Rate: 0.00767176
	LOSS [training: 3.6650540327674874 | validation: 2.94378549696861]
	TIME [epoch: 8.63 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.777678526855052		[learning rate: 0.0076439]
	Learning Rate: 0.00764391
	LOSS [training: 3.777678526855052 | validation: 1.714523710113474]
	TIME [epoch: 8.66 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.81604600189704		[learning rate: 0.0076162]
	Learning Rate: 0.00761617
	LOSS [training: 3.81604600189704 | validation: 1.7537501994365678]
	TIME [epoch: 8.64 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5921169040353917		[learning rate: 0.0075885]
	Learning Rate: 0.00758853
	LOSS [training: 3.5921169040353917 | validation: 1.890720194649032]
	TIME [epoch: 8.63 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9811783825463776		[learning rate: 0.007561]
	Learning Rate: 0.00756099
	LOSS [training: 3.9811783825463776 | validation: 1.6884116825752262]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.651144853817273		[learning rate: 0.0075336]
	Learning Rate: 0.00753356
	LOSS [training: 3.651144853817273 | validation: 1.731082796647547]
	TIME [epoch: 8.65 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.620749204482377		[learning rate: 0.0075062]
	Learning Rate: 0.00750622
	LOSS [training: 3.620749204482377 | validation: 2.564577151363557]
	TIME [epoch: 8.63 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6629154917969173		[learning rate: 0.007479]
	Learning Rate: 0.00747897
	LOSS [training: 3.6629154917969173 | validation: 1.8087315985607275]
	TIME [epoch: 8.63 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6275936248547573		[learning rate: 0.0074518]
	Learning Rate: 0.00745183
	LOSS [training: 3.6275936248547573 | validation: 1.7054864243697776]
	TIME [epoch: 8.62 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5361392349403653		[learning rate: 0.0074248]
	Learning Rate: 0.00742479
	LOSS [training: 3.5361392349403653 | validation: 2.392278927479416]
	TIME [epoch: 8.65 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.108278739994082		[learning rate: 0.0073978]
	Learning Rate: 0.00739784
	LOSS [training: 4.108278739994082 | validation: 3.110940346969905]
	TIME [epoch: 8.63 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.872096189856914		[learning rate: 0.007371]
	Learning Rate: 0.007371
	LOSS [training: 3.872096189856914 | validation: 1.7386190967515782]
	TIME [epoch: 8.62 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9016832129422654		[learning rate: 0.0073442]
	Learning Rate: 0.00734425
	LOSS [training: 3.9016832129422654 | validation: 1.890672984418656]
	TIME [epoch: 8.63 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5897787116989397		[learning rate: 0.0073176]
	Learning Rate: 0.0073176
	LOSS [training: 3.5897787116989397 | validation: 1.6390645913410888]
	TIME [epoch: 8.65 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7088988688684537		[learning rate: 0.007291]
	Learning Rate: 0.00729104
	LOSS [training: 3.7088988688684537 | validation: 2.012360633817214]
	TIME [epoch: 8.64 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5631411737575966		[learning rate: 0.0072646]
	Learning Rate: 0.00726458
	LOSS [training: 3.5631411737575966 | validation: 1.9412658058067265]
	TIME [epoch: 8.64 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6464831315446906		[learning rate: 0.0072382]
	Learning Rate: 0.00723822
	LOSS [training: 3.6464831315446906 | validation: 1.6930828327155498]
	TIME [epoch: 8.64 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.544349596084318		[learning rate: 0.0072119]
	Learning Rate: 0.00721195
	LOSS [training: 3.544349596084318 | validation: 1.8138011882455933]
	TIME [epoch: 8.64 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.642575063140728		[learning rate: 0.0071858]
	Learning Rate: 0.00718578
	LOSS [training: 3.642575063140728 | validation: 1.6342472738505325]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6327525037012682		[learning rate: 0.0071597]
	Learning Rate: 0.0071597
	LOSS [training: 3.6327525037012682 | validation: 2.025268148211475]
	TIME [epoch: 8.63 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5653589155394783		[learning rate: 0.0071337]
	Learning Rate: 0.00713371
	LOSS [training: 3.5653589155394783 | validation: 2.0579613733989506]
	TIME [epoch: 8.65 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7429556344007286		[learning rate: 0.0071078]
	Learning Rate: 0.00710783
	LOSS [training: 3.7429556344007286 | validation: 1.6911395153483473]
	TIME [epoch: 8.64 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.634983664824226		[learning rate: 0.007082]
	Learning Rate: 0.00708203
	LOSS [training: 3.634983664824226 | validation: 2.0922821337721818]
	TIME [epoch: 8.64 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5506506155018096		[learning rate: 0.0070563]
	Learning Rate: 0.00705633
	LOSS [training: 3.5506506155018096 | validation: 1.7342413392427551]
	TIME [epoch: 8.62 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.590989030186247		[learning rate: 0.0070307]
	Learning Rate: 0.00703072
	LOSS [training: 3.590989030186247 | validation: 2.362909572924354]
	TIME [epoch: 8.64 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7522180624082453		[learning rate: 0.0070052]
	Learning Rate: 0.00700521
	LOSS [training: 3.7522180624082453 | validation: 1.8011129253105016]
	TIME [epoch: 8.65 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5453866087354045		[learning rate: 0.0069798]
	Learning Rate: 0.00697979
	LOSS [training: 3.5453866087354045 | validation: 1.8179820330866239]
	TIME [epoch: 8.62 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7531047184873096		[learning rate: 0.0069545]
	Learning Rate: 0.00695446
	LOSS [training: 3.7531047184873096 | validation: 1.7378165416664149]
	TIME [epoch: 8.63 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8381349094037027		[learning rate: 0.0069292]
	Learning Rate: 0.00692922
	LOSS [training: 3.8381349094037027 | validation: 2.027571533943793]
	TIME [epoch: 8.65 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6390688731200656		[learning rate: 0.0069041]
	Learning Rate: 0.00690407
	LOSS [training: 3.6390688731200656 | validation: 1.7035870885544198]
	TIME [epoch: 8.64 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.725097350494972		[learning rate: 0.006879]
	Learning Rate: 0.00687902
	LOSS [training: 3.725097350494972 | validation: 1.7949076203723942]
	TIME [epoch: 8.64 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.711853476988428		[learning rate: 0.0068541]
	Learning Rate: 0.00685405
	LOSS [training: 3.711853476988428 | validation: 2.852741156534802]
	TIME [epoch: 8.63 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6968616700194312		[learning rate: 0.0068292]
	Learning Rate: 0.00682918
	LOSS [training: 3.6968616700194312 | validation: 2.1988103226499205]
	TIME [epoch: 8.66 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6031234698130645		[learning rate: 0.0068044]
	Learning Rate: 0.00680439
	LOSS [training: 3.6031234698130645 | validation: 1.8480873979103887]
	TIME [epoch: 8.63 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7378193095595647		[learning rate: 0.0067797]
	Learning Rate: 0.0067797
	LOSS [training: 3.7378193095595647 | validation: 2.1878528271444764]
	TIME [epoch: 8.63 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.549534368917164		[learning rate: 0.0067551]
	Learning Rate: 0.0067551
	LOSS [training: 3.549534368917164 | validation: 1.81629616679455]
	TIME [epoch: 8.63 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.573676559084113		[learning rate: 0.0067306]
	Learning Rate: 0.00673058
	LOSS [training: 3.573676559084113 | validation: 1.6703040843056645]
	TIME [epoch: 8.65 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5627410548350347		[learning rate: 0.0067062]
	Learning Rate: 0.00670616
	LOSS [training: 3.5627410548350347 | validation: 1.6335810953572103]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.584542754505114		[learning rate: 0.0066818]
	Learning Rate: 0.00668182
	LOSS [training: 3.584542754505114 | validation: 1.6389783409406138]
	TIME [epoch: 8.64 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8527653587631208		[learning rate: 0.0066576]
	Learning Rate: 0.00665757
	LOSS [training: 3.8527653587631208 | validation: 1.9220904301789412]
	TIME [epoch: 8.65 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.540549146485553		[learning rate: 0.0066334]
	Learning Rate: 0.00663341
	LOSS [training: 3.540549146485553 | validation: 1.7554982936087982]
	TIME [epoch: 8.64 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.505093232937758		[learning rate: 0.0066093]
	Learning Rate: 0.00660934
	LOSS [training: 3.505093232937758 | validation: 2.2215229166469634]
	TIME [epoch: 8.64 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.536818442732566		[learning rate: 0.0065854]
	Learning Rate: 0.00658535
	LOSS [training: 3.536818442732566 | validation: 1.7891744798956257]
	TIME [epoch: 8.63 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4663845838066742		[learning rate: 0.0065615]
	Learning Rate: 0.00656145
	LOSS [training: 3.4663845838066742 | validation: 1.8927867492198434]
	TIME [epoch: 8.66 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.758752480677895		[learning rate: 0.0065376]
	Learning Rate: 0.00653764
	LOSS [training: 3.758752480677895 | validation: 2.9826125488401347]
	TIME [epoch: 8.65 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6642860062839544		[learning rate: 0.0065139]
	Learning Rate: 0.00651392
	LOSS [training: 3.6642860062839544 | validation: 1.7114389956851976]
	TIME [epoch: 8.64 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6056334494218505		[learning rate: 0.0064903]
	Learning Rate: 0.00649028
	LOSS [training: 3.6056334494218505 | validation: 2.7130446797837133]
	TIME [epoch: 8.64 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.626674698794831		[learning rate: 0.0064667]
	Learning Rate: 0.00646672
	LOSS [training: 3.626674698794831 | validation: 1.7283795268541393]
	TIME [epoch: 8.65 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5172398905501354		[learning rate: 0.0064433]
	Learning Rate: 0.00644325
	LOSS [training: 3.5172398905501354 | validation: 1.8826703597001142]
	TIME [epoch: 8.64 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6229373941705902		[learning rate: 0.0064199]
	Learning Rate: 0.00641987
	LOSS [training: 3.6229373941705902 | validation: 1.8705922543682698]
	TIME [epoch: 8.63 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5000035542500667		[learning rate: 0.0063966]
	Learning Rate: 0.00639657
	LOSS [training: 3.5000035542500667 | validation: 1.9503418442926703]
	TIME [epoch: 8.64 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4660189576198897		[learning rate: 0.0063734]
	Learning Rate: 0.00637336
	LOSS [training: 3.4660189576198897 | validation: 1.7657640436365343]
	TIME [epoch: 8.65 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6285477380461173		[learning rate: 0.0063502]
	Learning Rate: 0.00635023
	LOSS [training: 3.6285477380461173 | validation: 1.904050779957831]
	TIME [epoch: 8.63 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.505272069073821		[learning rate: 0.0063272]
	Learning Rate: 0.00632718
	LOSS [training: 3.505272069073821 | validation: 2.261898971173485]
	TIME [epoch: 8.63 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.490277082848472		[learning rate: 0.0063042]
	Learning Rate: 0.00630422
	LOSS [training: 3.490277082848472 | validation: 1.7258237322512227]
	TIME [epoch: 8.63 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.463342247763394		[learning rate: 0.0062813]
	Learning Rate: 0.00628134
	LOSS [training: 3.463342247763394 | validation: 1.5979260112383673]
	TIME [epoch: 8.65 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5210859503759124		[learning rate: 0.0062585]
	Learning Rate: 0.00625855
	LOSS [training: 3.5210859503759124 | validation: 1.7541387485941764]
	TIME [epoch: 8.64 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5036155607515242		[learning rate: 0.0062358]
	Learning Rate: 0.00623584
	LOSS [training: 3.5036155607515242 | validation: 1.5960193387715726]
	TIME [epoch: 8.64 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.453278964067869		[learning rate: 0.0062132]
	Learning Rate: 0.00621321
	LOSS [training: 3.453278964067869 | validation: 1.950582432972009]
	TIME [epoch: 8.63 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5966384855206357		[learning rate: 0.0061907]
	Learning Rate: 0.00619066
	LOSS [training: 3.5966384855206357 | validation: 2.0813918970850827]
	TIME [epoch: 8.65 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6071339103567		[learning rate: 0.0061682]
	Learning Rate: 0.00616819
	LOSS [training: 3.6071339103567 | validation: 2.508514817225222]
	TIME [epoch: 8.64 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8460664609218704		[learning rate: 0.0061458]
	Learning Rate: 0.00614581
	LOSS [training: 3.8460664609218704 | validation: 1.6851764999892762]
	TIME [epoch: 8.63 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.523147907406881		[learning rate: 0.0061235]
	Learning Rate: 0.0061235
	LOSS [training: 3.523147907406881 | validation: 1.7433310184204325]
	TIME [epoch: 8.63 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5897944895887983		[learning rate: 0.0061013]
	Learning Rate: 0.00610128
	LOSS [training: 3.5897944895887983 | validation: 1.7703468097240092]
	TIME [epoch: 8.65 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.578143054039792		[learning rate: 0.0060791]
	Learning Rate: 0.00607914
	LOSS [training: 3.578143054039792 | validation: 2.0525098549301393]
	TIME [epoch: 8.64 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.558560435340671		[learning rate: 0.0060571]
	Learning Rate: 0.00605708
	LOSS [training: 3.558560435340671 | validation: 1.7745968369911234]
	TIME [epoch: 8.63 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5414121312821485		[learning rate: 0.0060351]
	Learning Rate: 0.0060351
	LOSS [training: 3.5414121312821485 | validation: 1.985528965586267]
	TIME [epoch: 8.63 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6203036887230695		[learning rate: 0.0060132]
	Learning Rate: 0.00601319
	LOSS [training: 3.6203036887230695 | validation: 1.7527749230526262]
	TIME [epoch: 8.66 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4362540860479798		[learning rate: 0.0059914]
	Learning Rate: 0.00599137
	LOSS [training: 3.4362540860479798 | validation: 2.057329706361352]
	TIME [epoch: 8.64 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5968329871698317		[learning rate: 0.0059696]
	Learning Rate: 0.00596963
	LOSS [training: 3.5968329871698317 | validation: 2.2778827039413843]
	TIME [epoch: 8.64 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4500393234403797		[learning rate: 0.005948]
	Learning Rate: 0.00594797
	LOSS [training: 3.4500393234403797 | validation: 2.295310304631494]
	TIME [epoch: 8.64 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4996546710980154		[learning rate: 0.0059264]
	Learning Rate: 0.00592638
	LOSS [training: 3.4996546710980154 | validation: 1.6682408642107416]
	TIME [epoch: 8.66 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.467835699530197		[learning rate: 0.0059049]
	Learning Rate: 0.00590487
	LOSS [training: 3.467835699530197 | validation: 1.5953873444566948]
	TIME [epoch: 8.64 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5675731323670723		[learning rate: 0.0058834]
	Learning Rate: 0.00588344
	LOSS [training: 3.5675731323670723 | validation: 1.6224181467985717]
	TIME [epoch: 8.64 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5415059053099336		[learning rate: 0.0058621]
	Learning Rate: 0.00586209
	LOSS [training: 3.5415059053099336 | validation: 1.7568132521053796]
	TIME [epoch: 8.63 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5749934821516036		[learning rate: 0.0058408]
	Learning Rate: 0.00584082
	LOSS [training: 3.5749934821516036 | validation: 1.6243836172047859]
	TIME [epoch: 8.66 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.769655147354082		[learning rate: 0.0058196]
	Learning Rate: 0.00581962
	LOSS [training: 3.769655147354082 | validation: 1.6567845851652012]
	TIME [epoch: 8.64 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.460827403544922		[learning rate: 0.0057985]
	Learning Rate: 0.0057985
	LOSS [training: 3.460827403544922 | validation: 2.130257739590366]
	TIME [epoch: 8.64 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5120839739568632		[learning rate: 0.0057775]
	Learning Rate: 0.00577746
	LOSS [training: 3.5120839739568632 | validation: 1.5778601592793116]
	TIME [epoch: 8.64 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5098368418151806		[learning rate: 0.0057565]
	Learning Rate: 0.00575649
	LOSS [training: 3.5098368418151806 | validation: 2.121117985850501]
	TIME [epoch: 8.66 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.470152498175357		[learning rate: 0.0057356]
	Learning Rate: 0.0057356
	LOSS [training: 3.470152498175357 | validation: 1.9554172702270458]
	TIME [epoch: 8.64 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5918910518766887		[learning rate: 0.0057148]
	Learning Rate: 0.00571479
	LOSS [training: 3.5918910518766887 | validation: 1.5859411846516314]
	TIME [epoch: 8.63 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.496702179127019		[learning rate: 0.005694]
	Learning Rate: 0.00569405
	LOSS [training: 3.496702179127019 | validation: 2.5826768220951792]
	TIME [epoch: 8.63 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.508381211568947		[learning rate: 0.0056734]
	Learning Rate: 0.00567338
	LOSS [training: 3.508381211568947 | validation: 2.0340496209108814]
	TIME [epoch: 8.65 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5671608253656215		[learning rate: 0.0056528]
	Learning Rate: 0.00565279
	LOSS [training: 3.5671608253656215 | validation: 1.5790066401212492]
	TIME [epoch: 8.63 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.511802967443599		[learning rate: 0.0056323]
	Learning Rate: 0.00563228
	LOSS [training: 3.511802967443599 | validation: 2.058220910664369]
	TIME [epoch: 8.63 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.569756598554494		[learning rate: 0.0056118]
	Learning Rate: 0.00561184
	LOSS [training: 3.569756598554494 | validation: 1.6713846108865034]
	TIME [epoch: 8.62 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4049868064309536		[learning rate: 0.0055915]
	Learning Rate: 0.00559147
	LOSS [training: 3.4049868064309536 | validation: 1.8757445426654513]
	TIME [epoch: 8.65 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.409652597132361		[learning rate: 0.0055712]
	Learning Rate: 0.00557118
	LOSS [training: 3.409652597132361 | validation: 1.7380371166298518]
	TIME [epoch: 8.63 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5569297923128573		[learning rate: 0.005551]
	Learning Rate: 0.00555096
	LOSS [training: 3.5569297923128573 | validation: 2.2360268088291675]
	TIME [epoch: 8.64 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7375204920713285		[learning rate: 0.0055308]
	Learning Rate: 0.00553082
	LOSS [training: 3.7375204920713285 | validation: 1.6469404325532202]
	TIME [epoch: 8.62 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4483999689237534		[learning rate: 0.0055107]
	Learning Rate: 0.00551075
	LOSS [training: 3.4483999689237534 | validation: 1.858153542116678]
	TIME [epoch: 8.63 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.497368267446718		[learning rate: 0.0054907]
	Learning Rate: 0.00549075
	LOSS [training: 3.497368267446718 | validation: 2.0336780602942177]
	TIME [epoch: 8.64 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4580851246248665		[learning rate: 0.0054708]
	Learning Rate: 0.00547082
	LOSS [training: 3.4580851246248665 | validation: 2.0051091598690047]
	TIME [epoch: 8.62 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.679367181362025		[learning rate: 0.005451]
	Learning Rate: 0.00545097
	LOSS [training: 3.679367181362025 | validation: 2.220753140285917]
	TIME [epoch: 8.62 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.529280036242812		[learning rate: 0.0054312]
	Learning Rate: 0.00543119
	LOSS [training: 3.529280036242812 | validation: 1.723615883740787]
	TIME [epoch: 8.64 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.401814060339808		[learning rate: 0.0054115]
	Learning Rate: 0.00541148
	LOSS [training: 3.401814060339808 | validation: 1.6110394801336099]
	TIME [epoch: 8.64 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4107501366664104		[learning rate: 0.0053918]
	Learning Rate: 0.00539184
	LOSS [training: 3.4107501366664104 | validation: 1.7524031931153898]
	TIME [epoch: 8.63 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.722406038874483		[learning rate: 0.0053723]
	Learning Rate: 0.00537227
	LOSS [training: 3.722406038874483 | validation: 1.8774068817961236]
	TIME [epoch: 8.63 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4802810315704824		[learning rate: 0.0053528]
	Learning Rate: 0.00535277
	LOSS [training: 3.4802810315704824 | validation: 2.0296801094867187]
	TIME [epoch: 8.63 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4116019001458198		[learning rate: 0.0053333]
	Learning Rate: 0.00533335
	LOSS [training: 3.4116019001458198 | validation: 1.5994761465140934]
	TIME [epoch: 8.64 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3866055989870554		[learning rate: 0.005314]
	Learning Rate: 0.00531399
	LOSS [training: 3.3866055989870554 | validation: 1.8582472774273098]
	TIME [epoch: 8.63 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.494805007815786		[learning rate: 0.0052947]
	Learning Rate: 0.00529471
	LOSS [training: 3.494805007815786 | validation: 1.6700875262734465]
	TIME [epoch: 8.63 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3776665305393316		[learning rate: 0.0052755]
	Learning Rate: 0.00527549
	LOSS [training: 3.3776665305393316 | validation: 1.81998369469341]
	TIME [epoch: 8.63 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3617037749249357		[learning rate: 0.0052563]
	Learning Rate: 0.00525635
	LOSS [training: 3.3617037749249357 | validation: 1.7523908089312448]
	TIME [epoch: 8.64 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.504523654171738		[learning rate: 0.0052373]
	Learning Rate: 0.00523727
	LOSS [training: 3.504523654171738 | validation: 1.5051751597913696]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5912348817298083		[learning rate: 0.0052183]
	Learning Rate: 0.00521827
	LOSS [training: 3.5912348817298083 | validation: 2.169245218309824]
	TIME [epoch: 8.63 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5138484225963253		[learning rate: 0.0051993]
	Learning Rate: 0.00519933
	LOSS [training: 3.5138484225963253 | validation: 2.2848180231409208]
	TIME [epoch: 8.64 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.565247647881131		[learning rate: 0.0051805]
	Learning Rate: 0.00518046
	LOSS [training: 3.565247647881131 | validation: 2.096295343194046]
	TIME [epoch: 8.64 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4814591106115436		[learning rate: 0.0051617]
	Learning Rate: 0.00516166
	LOSS [training: 3.4814591106115436 | validation: 1.7620026472415282]
	TIME [epoch: 8.63 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4151643063542907		[learning rate: 0.0051429]
	Learning Rate: 0.00514293
	LOSS [training: 3.4151643063542907 | validation: 1.6770799048722127]
	TIME [epoch: 8.62 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5706104112703407		[learning rate: 0.0051243]
	Learning Rate: 0.00512426
	LOSS [training: 3.5706104112703407 | validation: 1.6315383361705462]
	TIME [epoch: 8.64 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5052105986602093		[learning rate: 0.0051057]
	Learning Rate: 0.00510567
	LOSS [training: 3.5052105986602093 | validation: 1.7317383865837552]
	TIME [epoch: 8.65 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4621257101083898		[learning rate: 0.0050871]
	Learning Rate: 0.00508714
	LOSS [training: 3.4621257101083898 | validation: 1.8669529043138817]
	TIME [epoch: 8.63 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.385160169197774		[learning rate: 0.0050687]
	Learning Rate: 0.00506868
	LOSS [training: 3.385160169197774 | validation: 1.7488957522678301]
	TIME [epoch: 8.62 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5960104708660596		[learning rate: 0.0050503]
	Learning Rate: 0.00505028
	LOSS [training: 3.5960104708660596 | validation: 1.9232587794455434]
	TIME [epoch: 8.64 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4211759535737074		[learning rate: 0.005032]
	Learning Rate: 0.00503196
	LOSS [training: 3.4211759535737074 | validation: 1.688941033345472]
	TIME [epoch: 8.64 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.398260118177211		[learning rate: 0.0050137]
	Learning Rate: 0.00501369
	LOSS [training: 3.398260118177211 | validation: 1.6002164858801582]
	TIME [epoch: 8.63 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3753898383668206		[learning rate: 0.0049955]
	Learning Rate: 0.0049955
	LOSS [training: 3.3753898383668206 | validation: 2.3850680664702493]
	TIME [epoch: 8.63 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4395185574456937		[learning rate: 0.0049774]
	Learning Rate: 0.00497737
	LOSS [training: 3.4395185574456937 | validation: 1.846789402486138]
	TIME [epoch: 8.63 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5081927401799278		[learning rate: 0.0049593]
	Learning Rate: 0.00495931
	LOSS [training: 3.5081927401799278 | validation: 1.6487486324491298]
	TIME [epoch: 8.65 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3730186895583776		[learning rate: 0.0049413]
	Learning Rate: 0.00494131
	LOSS [training: 3.3730186895583776 | validation: 1.6726629018866932]
	TIME [epoch: 8.63 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.388812072687206		[learning rate: 0.0049234]
	Learning Rate: 0.00492338
	LOSS [training: 3.388812072687206 | validation: 1.6673670728657388]
	TIME [epoch: 8.62 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5014693377076673		[learning rate: 0.0049055]
	Learning Rate: 0.00490551
	LOSS [training: 3.5014693377076673 | validation: 1.642233966136721]
	TIME [epoch: 8.63 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3716451962758702		[learning rate: 0.0048877]
	Learning Rate: 0.00488771
	LOSS [training: 3.3716451962758702 | validation: 1.5367109939314245]
	TIME [epoch: 8.65 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3700616569680775		[learning rate: 0.00487]
	Learning Rate: 0.00486997
	LOSS [training: 3.3700616569680775 | validation: 1.5487424932940808]
	TIME [epoch: 8.63 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.299429107254473		[learning rate: 0.0048523]
	Learning Rate: 0.0048523
	LOSS [training: 3.299429107254473 | validation: 1.7630281593100061]
	TIME [epoch: 8.63 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4198799006793066		[learning rate: 0.0048347]
	Learning Rate: 0.00483469
	LOSS [training: 3.4198799006793066 | validation: 1.7071005329757556]
	TIME [epoch: 8.63 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3893959944493863		[learning rate: 0.0048171]
	Learning Rate: 0.00481714
	LOSS [training: 3.3893959944493863 | validation: 1.6519144096674525]
	TIME [epoch: 8.65 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4706716004792852		[learning rate: 0.0047997]
	Learning Rate: 0.00479966
	LOSS [training: 3.4706716004792852 | validation: 1.7106542457299725]
	TIME [epoch: 8.63 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3870977663429445		[learning rate: 0.0047822]
	Learning Rate: 0.00478224
	LOSS [training: 3.3870977663429445 | validation: 1.6162389118554237]
	TIME [epoch: 8.63 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4895413547543557		[learning rate: 0.0047649]
	Learning Rate: 0.00476489
	LOSS [training: 3.4895413547543557 | validation: 1.593051320361299]
	TIME [epoch: 8.62 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.41698579400596		[learning rate: 0.0047476]
	Learning Rate: 0.00474759
	LOSS [training: 3.41698579400596 | validation: 1.5837299044759527]
	TIME [epoch: 8.66 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.490178527847145		[learning rate: 0.0047304]
	Learning Rate: 0.00473037
	LOSS [training: 3.490178527847145 | validation: 1.9829985811326223]
	TIME [epoch: 8.63 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.561782946932605		[learning rate: 0.0047132]
	Learning Rate: 0.0047132
	LOSS [training: 3.561782946932605 | validation: 1.6434067826919312]
	TIME [epoch: 8.63 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4567909849745933		[learning rate: 0.0046961]
	Learning Rate: 0.00469609
	LOSS [training: 3.4567909849745933 | validation: 1.632242741081336]
	TIME [epoch: 8.63 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.381772916375567		[learning rate: 0.0046791]
	Learning Rate: 0.00467905
	LOSS [training: 3.381772916375567 | validation: 1.5929306113348876]
	TIME [epoch: 8.65 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.344117011878235		[learning rate: 0.0046621]
	Learning Rate: 0.00466207
	LOSS [training: 3.344117011878235 | validation: 2.0615930161358706]
	TIME [epoch: 8.63 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3964322055691007		[learning rate: 0.0046452]
	Learning Rate: 0.00464515
	LOSS [training: 3.3964322055691007 | validation: 1.5073361876447449]
	TIME [epoch: 8.63 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4309687660679002		[learning rate: 0.0046283]
	Learning Rate: 0.0046283
	LOSS [training: 3.4309687660679002 | validation: 1.6158085902099684]
	TIME [epoch: 8.63 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4260767092412956		[learning rate: 0.0046115]
	Learning Rate: 0.0046115
	LOSS [training: 3.4260767092412956 | validation: 1.769744908267684]
	TIME [epoch: 8.66 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.318865520494204		[learning rate: 0.0045948]
	Learning Rate: 0.00459476
	LOSS [training: 3.318865520494204 | validation: 2.267157233786657]
	TIME [epoch: 8.63 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4395088435928605		[learning rate: 0.0045781]
	Learning Rate: 0.00457809
	LOSS [training: 3.4395088435928605 | validation: 1.9961734514630693]
	TIME [epoch: 8.63 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.377364847282491		[learning rate: 0.0045615]
	Learning Rate: 0.00456148
	LOSS [training: 3.377364847282491 | validation: 1.8650385886558685]
	TIME [epoch: 8.63 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4013166924648273		[learning rate: 0.0045449]
	Learning Rate: 0.00454492
	LOSS [training: 3.4013166924648273 | validation: 1.7521511073014957]
	TIME [epoch: 8.65 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3505141580780764		[learning rate: 0.0045284]
	Learning Rate: 0.00452843
	LOSS [training: 3.3505141580780764 | validation: 1.5521799800706648]
	TIME [epoch: 8.63 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.371858550412496		[learning rate: 0.004512]
	Learning Rate: 0.00451199
	LOSS [training: 3.371858550412496 | validation: 1.5639817552348223]
	TIME [epoch: 8.63 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.356895303627527		[learning rate: 0.0044956]
	Learning Rate: 0.00449562
	LOSS [training: 3.356895303627527 | validation: 1.9414609734077937]
	TIME [epoch: 8.62 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.388230335827435		[learning rate: 0.0044793]
	Learning Rate: 0.0044793
	LOSS [training: 3.388230335827435 | validation: 1.4940877387332399]
	TIME [epoch: 8.65 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4378486426720896		[learning rate: 0.004463]
	Learning Rate: 0.00446305
	LOSS [training: 3.4378486426720896 | validation: 1.5508043274422227]
	TIME [epoch: 8.62 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4352951082748797		[learning rate: 0.0044469]
	Learning Rate: 0.00444685
	LOSS [training: 3.4352951082748797 | validation: 1.817106132674607]
	TIME [epoch: 8.62 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3482441281323942		[learning rate: 0.0044307]
	Learning Rate: 0.00443071
	LOSS [training: 3.3482441281323942 | validation: 1.562060899619352]
	TIME [epoch: 8.62 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.298272580437371		[learning rate: 0.0044146]
	Learning Rate: 0.00441463
	LOSS [training: 3.298272580437371 | validation: 1.8264595337598664]
	TIME [epoch: 8.64 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4437279240228333		[learning rate: 0.0043986]
	Learning Rate: 0.00439861
	LOSS [training: 3.4437279240228333 | validation: 1.6384014987824476]
	TIME [epoch: 8.62 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.384178152708227		[learning rate: 0.0043827]
	Learning Rate: 0.00438265
	LOSS [training: 3.384178152708227 | validation: 1.5457963588127155]
	TIME [epoch: 8.62 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3532224619065842		[learning rate: 0.0043667]
	Learning Rate: 0.00436675
	LOSS [training: 3.3532224619065842 | validation: 1.6215173462498966]
	TIME [epoch: 8.62 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4469495913290666		[learning rate: 0.0043509]
	Learning Rate: 0.0043509
	LOSS [training: 3.4469495913290666 | validation: 1.5140649509437856]
	TIME [epoch: 8.66 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.348215893044087		[learning rate: 0.0043351]
	Learning Rate: 0.00433511
	LOSS [training: 3.348215893044087 | validation: 1.5524099538963871]
	TIME [epoch: 8.63 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3844303831491525		[learning rate: 0.0043194]
	Learning Rate: 0.00431938
	LOSS [training: 3.3844303831491525 | validation: 1.690980154463768]
	TIME [epoch: 8.62 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.407275195451257		[learning rate: 0.0043037]
	Learning Rate: 0.0043037
	LOSS [training: 3.407275195451257 | validation: 1.603143518440468]
	TIME [epoch: 8.63 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.407917758858099		[learning rate: 0.0042881]
	Learning Rate: 0.00428808
	LOSS [training: 3.407917758858099 | validation: 1.7402007485025626]
	TIME [epoch: 8.64 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.336447495941558		[learning rate: 0.0042725]
	Learning Rate: 0.00427252
	LOSS [training: 3.336447495941558 | validation: 1.59619046703616]
	TIME [epoch: 8.62 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.379057446507866		[learning rate: 0.004257]
	Learning Rate: 0.00425702
	LOSS [training: 3.379057446507866 | validation: 1.6763206310407353]
	TIME [epoch: 8.63 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2856645639622264		[learning rate: 0.0042416]
	Learning Rate: 0.00424157
	LOSS [training: 3.2856645639622264 | validation: 1.9014770370631107]
	TIME [epoch: 8.63 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2984067842016693		[learning rate: 0.0042262]
	Learning Rate: 0.00422617
	LOSS [training: 3.2984067842016693 | validation: 1.5586963048207643]
	TIME [epoch: 8.64 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.422278085456662		[learning rate: 0.0042108]
	Learning Rate: 0.00421084
	LOSS [training: 3.422278085456662 | validation: 1.5401488403007155]
	TIME [epoch: 8.62 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3539249769129924		[learning rate: 0.0041956]
	Learning Rate: 0.00419556
	LOSS [training: 3.3539249769129924 | validation: 1.7025429381377082]
	TIME [epoch: 8.62 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.384094877724537		[learning rate: 0.0041803]
	Learning Rate: 0.00418033
	LOSS [training: 3.384094877724537 | validation: 1.806000374013781]
	TIME [epoch: 8.62 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3640887763418434		[learning rate: 0.0041652]
	Learning Rate: 0.00416516
	LOSS [training: 3.3640887763418434 | validation: 1.8597403963106567]
	TIME [epoch: 8.64 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3788671073490293		[learning rate: 0.00415]
	Learning Rate: 0.00415004
	LOSS [training: 3.3788671073490293 | validation: 1.9455838394634026]
	TIME [epoch: 8.62 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4178716128859747		[learning rate: 0.004135]
	Learning Rate: 0.00413498
	LOSS [training: 3.4178716128859747 | validation: 1.6804063933084645]
	TIME [epoch: 8.62 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3241482852431945		[learning rate: 0.00412]
	Learning Rate: 0.00411998
	LOSS [training: 3.3241482852431945 | validation: 1.600393401415597]
	TIME [epoch: 8.62 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4224313061582023		[learning rate: 0.004105]
	Learning Rate: 0.00410502
	LOSS [training: 3.4224313061582023 | validation: 1.9169895539599615]
	TIME [epoch: 8.64 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.395353543950308		[learning rate: 0.0040901]
	Learning Rate: 0.00409013
	LOSS [training: 3.395353543950308 | validation: 1.5235896969251246]
	TIME [epoch: 8.62 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3198474788520125		[learning rate: 0.0040753]
	Learning Rate: 0.00407528
	LOSS [training: 3.3198474788520125 | validation: 1.520427065297648]
	TIME [epoch: 8.61 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.296718149208458		[learning rate: 0.0040605]
	Learning Rate: 0.00406049
	LOSS [training: 3.296718149208458 | validation: 1.6264783653482058]
	TIME [epoch: 8.62 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4381641424768055		[learning rate: 0.0040458]
	Learning Rate: 0.00404576
	LOSS [training: 3.4381641424768055 | validation: 1.5211291157707274]
	TIME [epoch: 8.64 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.43143971129589		[learning rate: 0.0040311]
	Learning Rate: 0.00403108
	LOSS [training: 3.43143971129589 | validation: 1.543918495163925]
	TIME [epoch: 8.62 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.28602809471176		[learning rate: 0.0040164]
	Learning Rate: 0.00401645
	LOSS [training: 3.28602809471176 | validation: 1.8394941284145407]
	TIME [epoch: 8.61 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.371004476004978		[learning rate: 0.0040019]
	Learning Rate: 0.00400187
	LOSS [training: 3.371004476004978 | validation: 1.901403855034389]
	TIME [epoch: 8.62 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3408622136914503		[learning rate: 0.0039873]
	Learning Rate: 0.00398735
	LOSS [training: 3.3408622136914503 | validation: 1.5261820780465039]
	TIME [epoch: 8.64 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3125119083236		[learning rate: 0.0039729]
	Learning Rate: 0.00397288
	LOSS [training: 3.3125119083236 | validation: 1.5845507598273638]
	TIME [epoch: 8.62 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3026188692492946		[learning rate: 0.0039585]
	Learning Rate: 0.00395846
	LOSS [training: 3.3026188692492946 | validation: 1.4740474204672693]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.347745880860127		[learning rate: 0.0039441]
	Learning Rate: 0.00394409
	LOSS [training: 3.347745880860127 | validation: 1.600901745323867]
	TIME [epoch: 8.62 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4339884639865246		[learning rate: 0.0039298]
	Learning Rate: 0.00392978
	LOSS [training: 3.4339884639865246 | validation: 1.6079685999354019]
	TIME [epoch: 8.64 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.313783754727467		[learning rate: 0.0039155]
	Learning Rate: 0.00391552
	LOSS [training: 3.313783754727467 | validation: 1.5596879990624894]
	TIME [epoch: 8.62 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.331704010301416		[learning rate: 0.0039013]
	Learning Rate: 0.00390131
	LOSS [training: 3.331704010301416 | validation: 1.6473640224645203]
	TIME [epoch: 8.62 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3028844808829723		[learning rate: 0.0038872]
	Learning Rate: 0.00388715
	LOSS [training: 3.3028844808829723 | validation: 1.5176354538501713]
	TIME [epoch: 8.62 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3804226158174018		[learning rate: 0.003873]
	Learning Rate: 0.00387305
	LOSS [training: 3.3804226158174018 | validation: 1.5615242461204584]
	TIME [epoch: 8.64 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2780544139578254		[learning rate: 0.003859]
	Learning Rate: 0.00385899
	LOSS [training: 3.2780544139578254 | validation: 1.469814215867313]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2627160847288517		[learning rate: 0.003845]
	Learning Rate: 0.00384499
	LOSS [training: 3.2627160847288517 | validation: 1.6196126748349446]
	TIME [epoch: 8.61 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3073183925290017		[learning rate: 0.003831]
	Learning Rate: 0.00383103
	LOSS [training: 3.3073183925290017 | validation: 1.9871795279056013]
	TIME [epoch: 8.62 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.372248610654217		[learning rate: 0.0038171]
	Learning Rate: 0.00381713
	LOSS [training: 3.372248610654217 | validation: 1.8590615428500796]
	TIME [epoch: 8.64 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3671493368380787		[learning rate: 0.0038033]
	Learning Rate: 0.00380328
	LOSS [training: 3.3671493368380787 | validation: 1.485857992666709]
	TIME [epoch: 8.62 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.36979933508713		[learning rate: 0.0037895]
	Learning Rate: 0.00378947
	LOSS [training: 3.36979933508713 | validation: 1.828647451516755]
	TIME [epoch: 8.62 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4014207475831233		[learning rate: 0.0037757]
	Learning Rate: 0.00377572
	LOSS [training: 3.4014207475831233 | validation: 1.6242128701043965]
	TIME [epoch: 8.62 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2726292482249093		[learning rate: 0.003762]
	Learning Rate: 0.00376202
	LOSS [training: 3.2726292482249093 | validation: 1.6375945776246355]
	TIME [epoch: 8.63 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3682233519631586		[learning rate: 0.0037484]
	Learning Rate: 0.00374837
	LOSS [training: 3.3682233519631586 | validation: 1.5784444193510754]
	TIME [epoch: 8.62 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3859401659451436		[learning rate: 0.0037348]
	Learning Rate: 0.00373476
	LOSS [training: 3.3859401659451436 | validation: 1.963144757212089]
	TIME [epoch: 8.61 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3417961272550394		[learning rate: 0.0037212]
	Learning Rate: 0.00372121
	LOSS [training: 3.3417961272550394 | validation: 1.7322275720480975]
	TIME [epoch: 8.62 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.301798308073856		[learning rate: 0.0037077]
	Learning Rate: 0.00370771
	LOSS [training: 3.301798308073856 | validation: 1.6488884655988612]
	TIME [epoch: 8.62 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3691833579967323		[learning rate: 0.0036943]
	Learning Rate: 0.00369425
	LOSS [training: 3.3691833579967323 | validation: 1.488621224566736]
	TIME [epoch: 8.63 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.344949070299523		[learning rate: 0.0036808]
	Learning Rate: 0.00368084
	LOSS [training: 3.344949070299523 | validation: 2.1413292372443298]
	TIME [epoch: 8.62 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3022049603028747		[learning rate: 0.0036675]
	Learning Rate: 0.00366749
	LOSS [training: 3.3022049603028747 | validation: 1.6406345728305185]
	TIME [epoch: 8.61 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.287476958009568		[learning rate: 0.0036542]
	Learning Rate: 0.00365418
	LOSS [training: 3.287476958009568 | validation: 1.7455039526891003]
	TIME [epoch: 8.62 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.324686930354904		[learning rate: 0.0036409]
	Learning Rate: 0.00364092
	LOSS [training: 3.324686930354904 | validation: 1.8590765975330874]
	TIME [epoch: 8.63 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3248497984793786		[learning rate: 0.0036277]
	Learning Rate: 0.0036277
	LOSS [training: 3.3248497984793786 | validation: 1.9293351359334667]
	TIME [epoch: 8.61 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3691545930827305		[learning rate: 0.0036145]
	Learning Rate: 0.00361454
	LOSS [training: 3.3691545930827305 | validation: 1.576486745228157]
	TIME [epoch: 8.62 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.450203196940543		[learning rate: 0.0036014]
	Learning Rate: 0.00360142
	LOSS [training: 3.450203196940543 | validation: 1.498630138540571]
	TIME [epoch: 8.62 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.273690323962593		[learning rate: 0.0035883]
	Learning Rate: 0.00358835
	LOSS [training: 3.273690323962593 | validation: 1.687082470435224]
	TIME [epoch: 8.64 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3429807422869473		[learning rate: 0.0035753]
	Learning Rate: 0.00357533
	LOSS [training: 3.3429807422869473 | validation: 1.5383698090282494]
	TIME [epoch: 8.62 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2665041336991623		[learning rate: 0.0035624]
	Learning Rate: 0.00356235
	LOSS [training: 3.2665041336991623 | validation: 1.6947771771716356]
	TIME [epoch: 8.62 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.262116148216171		[learning rate: 0.0035494]
	Learning Rate: 0.00354942
	LOSS [training: 3.262116148216171 | validation: 1.7337445042560036]
	TIME [epoch: 8.63 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.30059551232902		[learning rate: 0.0035365]
	Learning Rate: 0.00353654
	LOSS [training: 3.30059551232902 | validation: 1.5717494041807436]
	TIME [epoch: 8.63 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.274140211729038		[learning rate: 0.0035237]
	Learning Rate: 0.00352371
	LOSS [training: 3.274140211729038 | validation: 2.1149082571282656]
	TIME [epoch: 8.62 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2942171119944303		[learning rate: 0.0035109]
	Learning Rate: 0.00351092
	LOSS [training: 3.2942171119944303 | validation: 1.7365491475232862]
	TIME [epoch: 8.62 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3522575694628984		[learning rate: 0.0034982]
	Learning Rate: 0.00349818
	LOSS [training: 3.3522575694628984 | validation: 1.5674062567770628]
	TIME [epoch: 8.62 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3419131124591175		[learning rate: 0.0034855]
	Learning Rate: 0.00348548
	LOSS [training: 3.3419131124591175 | validation: 1.502410011397524]
	TIME [epoch: 8.63 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2582856956722757		[learning rate: 0.0034728]
	Learning Rate: 0.00347284
	LOSS [training: 3.2582856956722757 | validation: 1.4576388227317536]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3171305209020354		[learning rate: 0.0034602]
	Learning Rate: 0.00346023
	LOSS [training: 3.3171305209020354 | validation: 1.5415470778503695]
	TIME [epoch: 8.61 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.292178512798184		[learning rate: 0.0034477]
	Learning Rate: 0.00344767
	LOSS [training: 3.292178512798184 | validation: 1.535918773062615]
	TIME [epoch: 8.63 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2908625851335005		[learning rate: 0.0034352]
	Learning Rate: 0.00343516
	LOSS [training: 3.2908625851335005 | validation: 1.4840795481202265]
	TIME [epoch: 8.63 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.35279924676748		[learning rate: 0.0034227]
	Learning Rate: 0.0034227
	LOSS [training: 3.35279924676748 | validation: 1.7428286809716107]
	TIME [epoch: 8.62 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2640022530968102		[learning rate: 0.0034103]
	Learning Rate: 0.00341028
	LOSS [training: 3.2640022530968102 | validation: 1.5239656998658782]
	TIME [epoch: 8.62 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.271420826299898		[learning rate: 0.0033979]
	Learning Rate: 0.0033979
	LOSS [training: 3.271420826299898 | validation: 1.5006718454890555]
	TIME [epoch: 8.62 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2652899510099034		[learning rate: 0.0033856]
	Learning Rate: 0.00338557
	LOSS [training: 3.2652899510099034 | validation: 1.931671458445245]
	TIME [epoch: 8.64 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3630363371513865		[learning rate: 0.0033733]
	Learning Rate: 0.00337328
	LOSS [training: 3.3630363371513865 | validation: 1.5194263981884046]
	TIME [epoch: 8.62 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3030770558305753		[learning rate: 0.003361]
	Learning Rate: 0.00336104
	LOSS [training: 3.3030770558305753 | validation: 1.9966688159069172]
	TIME [epoch: 8.62 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3222178377974507		[learning rate: 0.0033488]
	Learning Rate: 0.00334884
	LOSS [training: 3.3222178377974507 | validation: 1.502698478449057]
	TIME [epoch: 8.62 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2824519040882754		[learning rate: 0.0033367]
	Learning Rate: 0.00333669
	LOSS [training: 3.2824519040882754 | validation: 2.172614881585974]
	TIME [epoch: 8.64 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.365277608882715		[learning rate: 0.0033246]
	Learning Rate: 0.00332458
	LOSS [training: 3.365277608882715 | validation: 1.7625583423485325]
	TIME [epoch: 8.62 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.30618622480901		[learning rate: 0.0033125]
	Learning Rate: 0.00331252
	LOSS [training: 3.30618622480901 | validation: 1.532360227939088]
	TIME [epoch: 8.61 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2796037926488104		[learning rate: 0.0033005]
	Learning Rate: 0.00330049
	LOSS [training: 3.2796037926488104 | validation: 1.7105189376378311]
	TIME [epoch: 8.61 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.289647545664078		[learning rate: 0.0032885]
	Learning Rate: 0.00328852
	LOSS [training: 3.289647545664078 | validation: 1.8686200027408326]
	TIME [epoch: 8.64 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.307904283439851		[learning rate: 0.0032766]
	Learning Rate: 0.00327658
	LOSS [training: 3.307904283439851 | validation: 1.5525972961282721]
	TIME [epoch: 8.62 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.362584237926736		[learning rate: 0.0032647]
	Learning Rate: 0.00326469
	LOSS [training: 3.362584237926736 | validation: 1.5649960179818174]
	TIME [epoch: 8.62 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.460869558602453		[learning rate: 0.0032528]
	Learning Rate: 0.00325284
	LOSS [training: 3.460869558602453 | validation: 1.686981564350452]
	TIME [epoch: 8.62 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2792592639952103		[learning rate: 0.003241]
	Learning Rate: 0.00324104
	LOSS [training: 3.2792592639952103 | validation: 1.6997438237056053]
	TIME [epoch: 8.64 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2753041030032093		[learning rate: 0.0032293]
	Learning Rate: 0.00322928
	LOSS [training: 3.2753041030032093 | validation: 1.4930227045942346]
	TIME [epoch: 8.62 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.29009833784452		[learning rate: 0.0032176]
	Learning Rate: 0.00321756
	LOSS [training: 3.29009833784452 | validation: 1.5437031909905414]
	TIME [epoch: 8.63 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3694157200342167		[learning rate: 0.0032059]
	Learning Rate: 0.00320588
	LOSS [training: 3.3694157200342167 | validation: 1.5936293306302456]
	TIME [epoch: 8.61 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2243658778907913		[learning rate: 0.0031942]
	Learning Rate: 0.00319425
	LOSS [training: 3.2243658778907913 | validation: 1.6139254534251068]
	TIME [epoch: 8.65 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2837280711440515		[learning rate: 0.0031827]
	Learning Rate: 0.00318265
	LOSS [training: 3.2837280711440515 | validation: 2.2539555525362918]
	TIME [epoch: 8.62 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3385671782215858		[learning rate: 0.0031711]
	Learning Rate: 0.0031711
	LOSS [training: 3.3385671782215858 | validation: 1.7513669207715208]
	TIME [epoch: 8.61 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.279616769790761		[learning rate: 0.0031596]
	Learning Rate: 0.0031596
	LOSS [training: 3.279616769790761 | validation: 1.5176474093196068]
	TIME [epoch: 8.61 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2791073995531965		[learning rate: 0.0031481]
	Learning Rate: 0.00314813
	LOSS [training: 3.2791073995531965 | validation: 1.6210208740663483]
	TIME [epoch: 8.64 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.283616804349048		[learning rate: 0.0031367]
	Learning Rate: 0.00313671
	LOSS [training: 3.283616804349048 | validation: 1.4555378582617005]
	TIME [epoch: 8.61 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_419.pth
	Model improved!!!
EPOCH 420/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.265284713789039		[learning rate: 0.0031253]
	Learning Rate: 0.00312532
	LOSS [training: 3.265284713789039 | validation: 1.6383256558258732]
	TIME [epoch: 8.63 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3226827073500473		[learning rate: 0.003114]
	Learning Rate: 0.00311398
	LOSS [training: 3.3226827073500473 | validation: 1.7965041283354655]
	TIME [epoch: 8.63 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.286150012204174		[learning rate: 0.0031027]
	Learning Rate: 0.00310268
	LOSS [training: 3.286150012204174 | validation: 1.6998913035740875]
	TIME [epoch: 8.66 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2938927569089964		[learning rate: 0.0030914]
	Learning Rate: 0.00309142
	LOSS [training: 3.2938927569089964 | validation: 1.5927503261447273]
	TIME [epoch: 8.63 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2755494168272685		[learning rate: 0.0030802]
	Learning Rate: 0.0030802
	LOSS [training: 3.2755494168272685 | validation: 1.4513937020082213]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2744370701230325		[learning rate: 0.003069]
	Learning Rate: 0.00306902
	LOSS [training: 3.2744370701230325 | validation: 1.6770565026535373]
	TIME [epoch: 8.63 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2519234723557844		[learning rate: 0.0030579]
	Learning Rate: 0.00305788
	LOSS [training: 3.2519234723557844 | validation: 1.4919526536075627]
	TIME [epoch: 8.65 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2684402559519676		[learning rate: 0.0030468]
	Learning Rate: 0.00304679
	LOSS [training: 3.2684402559519676 | validation: 1.522866186439609]
	TIME [epoch: 8.62 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2800737770782233		[learning rate: 0.0030357]
	Learning Rate: 0.00303573
	LOSS [training: 3.2800737770782233 | validation: 1.5265822635822512]
	TIME [epoch: 8.62 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2515498909669063		[learning rate: 0.0030247]
	Learning Rate: 0.00302471
	LOSS [training: 3.2515498909669063 | validation: 1.6461005656431444]
	TIME [epoch: 8.62 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2859043614391665		[learning rate: 0.0030137]
	Learning Rate: 0.00301374
	LOSS [training: 3.2859043614391665 | validation: 1.5929368747529928]
	TIME [epoch: 8.65 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.260271505348148		[learning rate: 0.0030028]
	Learning Rate: 0.0030028
	LOSS [training: 3.260271505348148 | validation: 1.4923885494958136]
	TIME [epoch: 8.62 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2644587312977853		[learning rate: 0.0029919]
	Learning Rate: 0.0029919
	LOSS [training: 3.2644587312977853 | validation: 1.5792986917451837]
	TIME [epoch: 8.62 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.292702937007074		[learning rate: 0.002981]
	Learning Rate: 0.00298104
	LOSS [training: 3.292702937007074 | validation: 1.6120352185598765]
	TIME [epoch: 8.63 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2455486090813794		[learning rate: 0.0029702]
	Learning Rate: 0.00297023
	LOSS [training: 3.2455486090813794 | validation: 1.5097648207077365]
	TIME [epoch: 8.65 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2833172733951344		[learning rate: 0.0029594]
	Learning Rate: 0.00295945
	LOSS [training: 3.2833172733951344 | validation: 1.648717122890953]
	TIME [epoch: 8.62 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.272214504908688		[learning rate: 0.0029487]
	Learning Rate: 0.00294871
	LOSS [training: 3.272214504908688 | validation: 1.568855429394401]
	TIME [epoch: 8.62 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2391951837137305		[learning rate: 0.002938]
	Learning Rate: 0.00293801
	LOSS [training: 3.2391951837137305 | validation: 1.4771171789606365]
	TIME [epoch: 8.62 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2939222572043207		[learning rate: 0.0029273]
	Learning Rate: 0.00292734
	LOSS [training: 3.2939222572043207 | validation: 2.0838228383066832]
	TIME [epoch: 8.65 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2996001708420524		[learning rate: 0.0029167]
	Learning Rate: 0.00291672
	LOSS [training: 3.2996001708420524 | validation: 1.518659241498209]
	TIME [epoch: 8.63 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2412258544589974		[learning rate: 0.0029061]
	Learning Rate: 0.00290614
	LOSS [training: 3.2412258544589974 | validation: 1.6063862383091092]
	TIME [epoch: 8.62 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2612980849662927		[learning rate: 0.0028956]
	Learning Rate: 0.00289559
	LOSS [training: 3.2612980849662927 | validation: 1.5981622809578284]
	TIME [epoch: 8.62 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3097313577519882		[learning rate: 0.0028851]
	Learning Rate: 0.00288508
	LOSS [training: 3.3097313577519882 | validation: 1.4586285466838957]
	TIME [epoch: 8.64 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.259099701206133		[learning rate: 0.0028746]
	Learning Rate: 0.00287461
	LOSS [training: 3.259099701206133 | validation: 1.5094976241010825]
	TIME [epoch: 8.63 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2308170169402617		[learning rate: 0.0028642]
	Learning Rate: 0.00286418
	LOSS [training: 3.2308170169402617 | validation: 1.5252031247357265]
	TIME [epoch: 8.62 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2441027043132444		[learning rate: 0.0028538]
	Learning Rate: 0.00285378
	LOSS [training: 3.2441027043132444 | validation: 1.606333332607804]
	TIME [epoch: 8.62 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2559286491631467		[learning rate: 0.0028434]
	Learning Rate: 0.00284343
	LOSS [training: 3.2559286491631467 | validation: 1.5474291032505778]
	TIME [epoch: 8.64 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2419381037766897		[learning rate: 0.0028331]
	Learning Rate: 0.00283311
	LOSS [training: 3.2419381037766897 | validation: 1.5184664637873553]
	TIME [epoch: 8.63 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2443689438979995		[learning rate: 0.0028228]
	Learning Rate: 0.00282283
	LOSS [training: 3.2443689438979995 | validation: 1.5669453677211114]
	TIME [epoch: 8.62 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2938927732937713		[learning rate: 0.0028126]
	Learning Rate: 0.00281258
	LOSS [training: 3.2938927732937713 | validation: 1.649532512148623]
	TIME [epoch: 8.62 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2192669390155073		[learning rate: 0.0028024]
	Learning Rate: 0.00280238
	LOSS [training: 3.2192669390155073 | validation: 1.5706757306437034]
	TIME [epoch: 8.64 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.273163571884486		[learning rate: 0.0027922]
	Learning Rate: 0.00279221
	LOSS [training: 3.273163571884486 | validation: 1.6181241964586168]
	TIME [epoch: 8.63 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.236903439170077		[learning rate: 0.0027821]
	Learning Rate: 0.00278207
	LOSS [training: 3.236903439170077 | validation: 1.5692531931582443]
	TIME [epoch: 8.62 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2475267085213533		[learning rate: 0.002772]
	Learning Rate: 0.00277198
	LOSS [training: 3.2475267085213533 | validation: 1.5025425671507617]
	TIME [epoch: 8.62 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2402257803166727		[learning rate: 0.0027619]
	Learning Rate: 0.00276192
	LOSS [training: 3.2402257803166727 | validation: 2.0411479665109606]
	TIME [epoch: 8.64 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.280251821995357		[learning rate: 0.0027519]
	Learning Rate: 0.00275189
	LOSS [training: 3.280251821995357 | validation: 1.6138802012744013]
	TIME [epoch: 8.62 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.225718982991303		[learning rate: 0.0027419]
	Learning Rate: 0.00274191
	LOSS [training: 3.225718982991303 | validation: 1.7055612785355225]
	TIME [epoch: 8.63 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.255313969841184		[learning rate: 0.002732]
	Learning Rate: 0.00273196
	LOSS [training: 3.255313969841184 | validation: 1.4970605064442566]
	TIME [epoch: 8.62 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.216258227622956		[learning rate: 0.002722]
	Learning Rate: 0.00272204
	LOSS [training: 3.216258227622956 | validation: 1.6472326220579778]
	TIME [epoch: 8.65 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.253042125222797		[learning rate: 0.0027122]
	Learning Rate: 0.00271216
	LOSS [training: 3.253042125222797 | validation: 1.582636513864756]
	TIME [epoch: 8.62 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2759620295130327		[learning rate: 0.0027023]
	Learning Rate: 0.00270232
	LOSS [training: 3.2759620295130327 | validation: 1.4411106135741778]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2315070884014405		[learning rate: 0.0026925]
	Learning Rate: 0.00269251
	LOSS [training: 3.2315070884014405 | validation: 1.4782782863242245]
	TIME [epoch: 8.63 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2052150508680106		[learning rate: 0.0026827]
	Learning Rate: 0.00268274
	LOSS [training: 3.2052150508680106 | validation: 1.522117037833182]
	TIME [epoch: 8.65 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2665482246948208		[learning rate: 0.002673]
	Learning Rate: 0.00267301
	LOSS [training: 3.2665482246948208 | validation: 1.486327164597766]
	TIME [epoch: 8.63 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2083096851142114		[learning rate: 0.0026633]
	Learning Rate: 0.00266331
	LOSS [training: 3.2083096851142114 | validation: 1.4465281210715648]
	TIME [epoch: 8.62 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.227848823359124		[learning rate: 0.0026536]
	Learning Rate: 0.00265364
	LOSS [training: 3.227848823359124 | validation: 1.491593213736007]
	TIME [epoch: 8.62 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.264977977743257		[learning rate: 0.002644]
	Learning Rate: 0.00264401
	LOSS [training: 3.264977977743257 | validation: 1.561880231828447]
	TIME [epoch: 8.64 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.262690958540136		[learning rate: 0.0026344]
	Learning Rate: 0.00263441
	LOSS [training: 3.262690958540136 | validation: 1.5327980907588192]
	TIME [epoch: 8.63 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3938550561248446		[learning rate: 0.0026249]
	Learning Rate: 0.00262485
	LOSS [training: 3.3938550561248446 | validation: 1.4755969196821566]
	TIME [epoch: 8.62 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.227977167566624		[learning rate: 0.0026153]
	Learning Rate: 0.00261533
	LOSS [training: 3.227977167566624 | validation: 1.768774696127065]
	TIME [epoch: 8.62 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2311312817672624		[learning rate: 0.0026058]
	Learning Rate: 0.00260584
	LOSS [training: 3.2311312817672624 | validation: 1.498936532884253]
	TIME [epoch: 8.64 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.217208493889417		[learning rate: 0.0025964]
	Learning Rate: 0.00259638
	LOSS [training: 3.217208493889417 | validation: 1.594546437166573]
	TIME [epoch: 8.63 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.298915351862557		[learning rate: 0.002587]
	Learning Rate: 0.00258696
	LOSS [training: 3.298915351862557 | validation: 1.5753155855114558]
	TIME [epoch: 8.62 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.213403706004944		[learning rate: 0.0025776]
	Learning Rate: 0.00257757
	LOSS [training: 3.213403706004944 | validation: 1.4835507699509594]
	TIME [epoch: 8.62 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1918432661736977		[learning rate: 0.0025682]
	Learning Rate: 0.00256822
	LOSS [training: 3.1918432661736977 | validation: 1.4972783678193626]
	TIME [epoch: 8.64 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.235188729597187		[learning rate: 0.0025589]
	Learning Rate: 0.0025589
	LOSS [training: 3.235188729597187 | validation: 1.478855691332833]
	TIME [epoch: 8.63 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2014794174616164		[learning rate: 0.0025496]
	Learning Rate: 0.00254961
	LOSS [training: 3.2014794174616164 | validation: 1.5013920269540169]
	TIME [epoch: 8.62 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.225687917010501		[learning rate: 0.0025404]
	Learning Rate: 0.00254036
	LOSS [training: 3.225687917010501 | validation: 1.5009563875593304]
	TIME [epoch: 8.62 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2484410562657153		[learning rate: 0.0025311]
	Learning Rate: 0.00253114
	LOSS [training: 3.2484410562657153 | validation: 1.5585409555260228]
	TIME [epoch: 8.63 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2423297166129097		[learning rate: 0.002522]
	Learning Rate: 0.00252195
	LOSS [training: 3.2423297166129097 | validation: 1.4939130864740702]
	TIME [epoch: 8.64 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2306682011260137		[learning rate: 0.0025128]
	Learning Rate: 0.0025128
	LOSS [training: 3.2306682011260137 | validation: 1.610244814538621]
	TIME [epoch: 8.62 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.26474639775885		[learning rate: 0.0025037]
	Learning Rate: 0.00250368
	LOSS [training: 3.26474639775885 | validation: 1.4513026189203242]
	TIME [epoch: 8.63 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2195099685415314		[learning rate: 0.0024946]
	Learning Rate: 0.00249459
	LOSS [training: 3.2195099685415314 | validation: 1.4717004074028073]
	TIME [epoch: 8.63 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2416067335379766		[learning rate: 0.0024855]
	Learning Rate: 0.00248554
	LOSS [training: 3.2416067335379766 | validation: 1.5099003333498158]
	TIME [epoch: 8.64 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.224910376976596		[learning rate: 0.0024765]
	Learning Rate: 0.00247652
	LOSS [training: 3.224910376976596 | validation: 1.4518851800092223]
	TIME [epoch: 8.62 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2476333995051867		[learning rate: 0.0024675]
	Learning Rate: 0.00246753
	LOSS [training: 3.2476333995051867 | validation: 1.546573621299608]
	TIME [epoch: 8.62 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2171282928625535		[learning rate: 0.0024586]
	Learning Rate: 0.00245858
	LOSS [training: 3.2171282928625535 | validation: 1.6069878941759477]
	TIME [epoch: 8.63 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.201863374745745		[learning rate: 0.0024497]
	Learning Rate: 0.00244966
	LOSS [training: 3.201863374745745 | validation: 1.4400303746540406]
	TIME [epoch: 8.64 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.27521165538296		[learning rate: 0.0024408]
	Learning Rate: 0.00244077
	LOSS [training: 3.27521165538296 | validation: 1.5360096611581955]
	TIME [epoch: 8.62 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2327915537974348		[learning rate: 0.0024319]
	Learning Rate: 0.00243191
	LOSS [training: 3.2327915537974348 | validation: 1.4767732785912966]
	TIME [epoch: 8.62 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2410546325534684		[learning rate: 0.0024231]
	Learning Rate: 0.00242308
	LOSS [training: 3.2410546325534684 | validation: 1.4417637914459374]
	TIME [epoch: 8.62 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.237064961787084		[learning rate: 0.0024143]
	Learning Rate: 0.00241429
	LOSS [training: 3.237064961787084 | validation: 1.491008525738295]
	TIME [epoch: 8.63 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.252884906655251		[learning rate: 0.0024055]
	Learning Rate: 0.00240553
	LOSS [training: 3.252884906655251 | validation: 2.0622816930834778]
	TIME [epoch: 8.62 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.294291369441006		[learning rate: 0.0023968]
	Learning Rate: 0.0023968
	LOSS [training: 3.294291369441006 | validation: 1.8658232848838552]
	TIME [epoch: 8.62 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.234754061383795		[learning rate: 0.0023881]
	Learning Rate: 0.0023881
	LOSS [training: 3.234754061383795 | validation: 1.434148808663496]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2134631336845842		[learning rate: 0.0023794]
	Learning Rate: 0.00237943
	LOSS [training: 3.2134631336845842 | validation: 1.4959193748827209]
	TIME [epoch: 8.66 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.298935434757727		[learning rate: 0.0023708]
	Learning Rate: 0.0023708
	LOSS [training: 3.298935434757727 | validation: 1.5357753533435683]
	TIME [epoch: 8.62 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.195203084671198		[learning rate: 0.0023622]
	Learning Rate: 0.0023622
	LOSS [training: 3.195203084671198 | validation: 1.798301647013174]
	TIME [epoch: 8.62 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1907365487692894		[learning rate: 0.0023536]
	Learning Rate: 0.00235362
	LOSS [training: 3.1907365487692894 | validation: 1.5292747392267412]
	TIME [epoch: 8.63 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.208991582978077		[learning rate: 0.0023451]
	Learning Rate: 0.00234508
	LOSS [training: 3.208991582978077 | validation: 1.506377262064671]
	TIME [epoch: 8.63 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.21034134541123		[learning rate: 0.0023366]
	Learning Rate: 0.00233657
	LOSS [training: 3.21034134541123 | validation: 1.452009751237849]
	TIME [epoch: 8.62 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2030721703596554		[learning rate: 0.0023281]
	Learning Rate: 0.00232809
	LOSS [training: 3.2030721703596554 | validation: 1.502004442579163]
	TIME [epoch: 8.62 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2105379881150014		[learning rate: 0.0023196]
	Learning Rate: 0.00231964
	LOSS [training: 3.2105379881150014 | validation: 1.8299147592987124]
	TIME [epoch: 8.63 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2474523688350736		[learning rate: 0.0023112]
	Learning Rate: 0.00231122
	LOSS [training: 3.2474523688350736 | validation: 1.4556821452941422]
	TIME [epoch: 8.64 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.217482575949524		[learning rate: 0.0023028]
	Learning Rate: 0.00230284
	LOSS [training: 3.217482575949524 | validation: 1.6387770345539712]
	TIME [epoch: 8.62 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1980288632701233		[learning rate: 0.0022945]
	Learning Rate: 0.00229448
	LOSS [training: 3.1980288632701233 | validation: 1.5583973041566428]
	TIME [epoch: 8.62 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1902178299038253		[learning rate: 0.0022862]
	Learning Rate: 0.00228615
	LOSS [training: 3.1902178299038253 | validation: 1.647661671670649]
	TIME [epoch: 8.63 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.211708221690634		[learning rate: 0.0022779]
	Learning Rate: 0.00227786
	LOSS [training: 3.211708221690634 | validation: 1.5821879029216022]
	TIME [epoch: 8.63 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.232759516795391		[learning rate: 0.0022696]
	Learning Rate: 0.00226959
	LOSS [training: 3.232759516795391 | validation: 1.4332587531913585]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_508.pth
	Model improved!!!
EPOCH 509/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2280037534636343		[learning rate: 0.0022614]
	Learning Rate: 0.00226135
	LOSS [training: 3.2280037534636343 | validation: 1.6861268109155787]
	TIME [epoch: 8.62 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.186938094456487		[learning rate: 0.0022531]
	Learning Rate: 0.00225315
	LOSS [training: 3.186938094456487 | validation: 1.4589455450071585]
	TIME [epoch: 8.62 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2176106146638404		[learning rate: 0.002245]
	Learning Rate: 0.00224497
	LOSS [training: 3.2176106146638404 | validation: 1.5052817653786685]
	TIME [epoch: 8.63 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.174340602851623		[learning rate: 0.0022368]
	Learning Rate: 0.00223682
	LOSS [training: 3.174340602851623 | validation: 1.442275862075345]
	TIME [epoch: 8.62 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1940415898424783		[learning rate: 0.0022287]
	Learning Rate: 0.00222871
	LOSS [training: 3.1940415898424783 | validation: 1.4538306732195023]
	TIME [epoch: 8.62 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.173826656919272		[learning rate: 0.0022206]
	Learning Rate: 0.00222062
	LOSS [training: 3.173826656919272 | validation: 1.4352417774183903]
	TIME [epoch: 8.64 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2253832636982347		[learning rate: 0.0022126]
	Learning Rate: 0.00221256
	LOSS [training: 3.2253832636982347 | validation: 1.5331922600034047]
	TIME [epoch: 8.64 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2060767069784597		[learning rate: 0.0022045]
	Learning Rate: 0.00220453
	LOSS [training: 3.2060767069784597 | validation: 1.4311037402877778]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_516.pth
	Model improved!!!
EPOCH 517/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2085677589946493		[learning rate: 0.0021965]
	Learning Rate: 0.00219653
	LOSS [training: 3.2085677589946493 | validation: 1.5915621243628226]
	TIME [epoch: 8.62 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.268280381621588		[learning rate: 0.0021886]
	Learning Rate: 0.00218856
	LOSS [training: 3.268280381621588 | validation: 1.6932240989756502]
	TIME [epoch: 8.63 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.256276812833966		[learning rate: 0.0021806]
	Learning Rate: 0.00218061
	LOSS [training: 3.256276812833966 | validation: 1.4453000766446682]
	TIME [epoch: 8.64 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2178590992710054		[learning rate: 0.0021727]
	Learning Rate: 0.0021727
	LOSS [training: 3.2178590992710054 | validation: 1.5622761681771888]
	TIME [epoch: 8.62 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1937530561597156		[learning rate: 0.0021648]
	Learning Rate: 0.00216482
	LOSS [training: 3.1937530561597156 | validation: 1.5684463800713389]
	TIME [epoch: 8.62 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.205888005249291		[learning rate: 0.002157]
	Learning Rate: 0.00215696
	LOSS [training: 3.205888005249291 | validation: 1.4226529712329985]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_522.pth
	Model improved!!!
EPOCH 523/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.184326962220741		[learning rate: 0.0021491]
	Learning Rate: 0.00214913
	LOSS [training: 3.184326962220741 | validation: 1.4936985553583237]
	TIME [epoch: 8.64 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2097372493696037		[learning rate: 0.0021413]
	Learning Rate: 0.00214133
	LOSS [training: 3.2097372493696037 | validation: 1.5228894925637366]
	TIME [epoch: 8.62 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2540895678875295		[learning rate: 0.0021336]
	Learning Rate: 0.00213356
	LOSS [training: 3.2540895678875295 | validation: 1.4305077573534974]
	TIME [epoch: 8.62 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3063200747230206		[learning rate: 0.0021258]
	Learning Rate: 0.00212582
	LOSS [training: 3.3063200747230206 | validation: 1.6539588256921625]
	TIME [epoch: 8.62 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.172225784939303		[learning rate: 0.0021181]
	Learning Rate: 0.0021181
	LOSS [training: 3.172225784939303 | validation: 1.4386887707494886]
	TIME [epoch: 8.64 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1913262707767656		[learning rate: 0.0021104]
	Learning Rate: 0.00211042
	LOSS [training: 3.1913262707767656 | validation: 1.4287990316507042]
	TIME [epoch: 8.62 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.17245183033295		[learning rate: 0.0021028]
	Learning Rate: 0.00210276
	LOSS [training: 3.17245183033295 | validation: 1.5093608850785807]
	TIME [epoch: 8.62 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.163168747361292		[learning rate: 0.0020951]
	Learning Rate: 0.00209513
	LOSS [training: 3.163168747361292 | validation: 1.7202289422128116]
	TIME [epoch: 8.62 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2857018426554965		[learning rate: 0.0020875]
	Learning Rate: 0.00208752
	LOSS [training: 3.2857018426554965 | validation: 1.5683195047821068]
	TIME [epoch: 8.64 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1777389580972417		[learning rate: 0.0020799]
	Learning Rate: 0.00207995
	LOSS [training: 3.1777389580972417 | validation: 1.552840315150589]
	TIME [epoch: 8.62 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2179488834288876		[learning rate: 0.0020724]
	Learning Rate: 0.0020724
	LOSS [training: 3.2179488834288876 | validation: 1.457095773587021]
	TIME [epoch: 8.62 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1850321376263224		[learning rate: 0.0020649]
	Learning Rate: 0.00206488
	LOSS [training: 3.1850321376263224 | validation: 1.4801063859333445]
	TIME [epoch: 8.62 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.178478187073156		[learning rate: 0.0020574]
	Learning Rate: 0.00205739
	LOSS [training: 3.178478187073156 | validation: 1.538591421784275]
	TIME [epoch: 8.65 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.162202189838786		[learning rate: 0.0020499]
	Learning Rate: 0.00204992
	LOSS [training: 3.162202189838786 | validation: 1.5327472551818762]
	TIME [epoch: 8.62 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.204131946133296		[learning rate: 0.0020425]
	Learning Rate: 0.00204248
	LOSS [training: 3.204131946133296 | validation: 1.585001005609301]
	TIME [epoch: 8.62 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.180100753554912		[learning rate: 0.0020351]
	Learning Rate: 0.00203507
	LOSS [training: 3.180100753554912 | validation: 1.4262494697593986]
	TIME [epoch: 8.63 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1527669753133414		[learning rate: 0.0020277]
	Learning Rate: 0.00202768
	LOSS [training: 3.1527669753133414 | validation: 1.6245029106051772]
	TIME [epoch: 8.65 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.195833409207833		[learning rate: 0.0020203]
	Learning Rate: 0.00202032
	LOSS [training: 3.195833409207833 | validation: 1.447318203768387]
	TIME [epoch: 8.62 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1905250083900984		[learning rate: 0.002013]
	Learning Rate: 0.00201299
	LOSS [training: 3.1905250083900984 | validation: 1.5122770812962714]
	TIME [epoch: 8.62 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1790659913792525		[learning rate: 0.0020057]
	Learning Rate: 0.00200569
	LOSS [training: 3.1790659913792525 | validation: 1.4466622898544088]
	TIME [epoch: 8.62 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1642262879394107		[learning rate: 0.0019984]
	Learning Rate: 0.00199841
	LOSS [training: 3.1642262879394107 | validation: 1.4317974030730722]
	TIME [epoch: 8.64 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.18656807097629		[learning rate: 0.0019912]
	Learning Rate: 0.00199116
	LOSS [training: 3.18656807097629 | validation: 1.4304970626465094]
	TIME [epoch: 8.62 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.178915631188331		[learning rate: 0.0019839]
	Learning Rate: 0.00198393
	LOSS [training: 3.178915631188331 | validation: 1.65753365533451]
	TIME [epoch: 8.62 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.237881972390835		[learning rate: 0.0019767]
	Learning Rate: 0.00197673
	LOSS [training: 3.237881972390835 | validation: 1.6119403437839697]
	TIME [epoch: 8.62 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.179320245454653		[learning rate: 0.0019696]
	Learning Rate: 0.00196956
	LOSS [training: 3.179320245454653 | validation: 1.430812671215772]
	TIME [epoch: 8.64 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.163465658129574		[learning rate: 0.0019624]
	Learning Rate: 0.00196241
	LOSS [training: 3.163465658129574 | validation: 1.4622681488157812]
	TIME [epoch: 8.62 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.221503201270086		[learning rate: 0.0019553]
	Learning Rate: 0.00195529
	LOSS [training: 3.221503201270086 | validation: 1.4751035230528928]
	TIME [epoch: 8.62 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2053034741398028		[learning rate: 0.0019482]
	Learning Rate: 0.00194819
	LOSS [training: 3.2053034741398028 | validation: 1.5056288770342197]
	TIME [epoch: 8.62 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.158676996073061		[learning rate: 0.0019411]
	Learning Rate: 0.00194112
	LOSS [training: 3.158676996073061 | validation: 1.426535516141785]
	TIME [epoch: 8.65 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.194253034343058		[learning rate: 0.0019341]
	Learning Rate: 0.00193408
	LOSS [training: 3.194253034343058 | validation: 1.6490439892279944]
	TIME [epoch: 8.62 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.215864710651882		[learning rate: 0.0019271]
	Learning Rate: 0.00192706
	LOSS [training: 3.215864710651882 | validation: 1.4214348234286236]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1775345514061324		[learning rate: 0.0019201]
	Learning Rate: 0.00192006
	LOSS [training: 3.1775345514061324 | validation: 1.4678880856113712]
	TIME [epoch: 8.62 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1887826309021525		[learning rate: 0.0019131]
	Learning Rate: 0.0019131
	LOSS [training: 3.1887826309021525 | validation: 1.5766405968864499]
	TIME [epoch: 8.64 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.161437434021681		[learning rate: 0.0019062]
	Learning Rate: 0.00190615
	LOSS [training: 3.161437434021681 | validation: 1.4344416937124922]
	TIME [epoch: 8.62 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1691783419480104		[learning rate: 0.0018992]
	Learning Rate: 0.00189924
	LOSS [training: 3.1691783419480104 | validation: 1.5576460535971033]
	TIME [epoch: 8.61 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1936909382953833		[learning rate: 0.0018923]
	Learning Rate: 0.00189234
	LOSS [training: 3.1936909382953833 | validation: 1.4405656271879295]
	TIME [epoch: 8.62 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.172473028283304		[learning rate: 0.0018855]
	Learning Rate: 0.00188548
	LOSS [training: 3.172473028283304 | validation: 1.4125387208340405]
	TIME [epoch: 8.64 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_559.pth
	Model improved!!!
EPOCH 560/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.183290470444973		[learning rate: 0.0018786]
	Learning Rate: 0.00187863
	LOSS [training: 3.183290470444973 | validation: 1.646237640101547]
	TIME [epoch: 8.63 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2212309139120436		[learning rate: 0.0018718]
	Learning Rate: 0.00187182
	LOSS [training: 3.2212309139120436 | validation: 1.489344935408059]
	TIME [epoch: 8.62 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.177799347392516		[learning rate: 0.001865]
	Learning Rate: 0.00186502
	LOSS [training: 3.177799347392516 | validation: 1.5149525246488422]
	TIME [epoch: 8.62 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1976231815716707		[learning rate: 0.0018583]
	Learning Rate: 0.00185825
	LOSS [training: 3.1976231815716707 | validation: 1.431036077234792]
	TIME [epoch: 8.64 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.191312632745101		[learning rate: 0.0018515]
	Learning Rate: 0.00185151
	LOSS [training: 3.191312632745101 | validation: 1.4209601106275478]
	TIME [epoch: 8.62 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1714868210843052		[learning rate: 0.0018448]
	Learning Rate: 0.00184479
	LOSS [training: 3.1714868210843052 | validation: 1.528930685995768]
	TIME [epoch: 8.62 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1593880547664197		[learning rate: 0.0018381]
	Learning Rate: 0.0018381
	LOSS [training: 3.1593880547664197 | validation: 1.4240719201801655]
	TIME [epoch: 8.62 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1727140219211734		[learning rate: 0.0018314]
	Learning Rate: 0.00183143
	LOSS [training: 3.1727140219211734 | validation: 1.4820229771094384]
	TIME [epoch: 8.65 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1636545086227117		[learning rate: 0.0018248]
	Learning Rate: 0.00182478
	LOSS [training: 3.1636545086227117 | validation: 1.5203147183035461]
	TIME [epoch: 8.62 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.149050060239565		[learning rate: 0.0018182]
	Learning Rate: 0.00181816
	LOSS [training: 3.149050060239565 | validation: 1.5297071983763941]
	TIME [epoch: 8.61 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.211046753192626		[learning rate: 0.0018116]
	Learning Rate: 0.00181156
	LOSS [training: 3.211046753192626 | validation: 1.4720823075163192]
	TIME [epoch: 8.62 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.16536286573165		[learning rate: 0.001805]
	Learning Rate: 0.00180499
	LOSS [training: 3.16536286573165 | validation: 1.5083986225582033]
	TIME [epoch: 8.64 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.145254303451378		[learning rate: 0.0017984]
	Learning Rate: 0.00179844
	LOSS [training: 3.145254303451378 | validation: 1.4763560691620277]
	TIME [epoch: 8.63 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1631368265856366		[learning rate: 0.0017919]
	Learning Rate: 0.00179191
	LOSS [training: 3.1631368265856366 | validation: 1.460049169411836]
	TIME [epoch: 8.62 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1812641218774713		[learning rate: 0.0017854]
	Learning Rate: 0.00178541
	LOSS [training: 3.1812641218774713 | validation: 1.5018156044027284]
	TIME [epoch: 8.62 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.154651455543074		[learning rate: 0.0017789]
	Learning Rate: 0.00177893
	LOSS [training: 3.154651455543074 | validation: 1.4698843665387638]
	TIME [epoch: 8.64 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1734303032842237		[learning rate: 0.0017725]
	Learning Rate: 0.00177247
	LOSS [training: 3.1734303032842237 | validation: 1.46249921911121]
	TIME [epoch: 8.62 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1475173363895843		[learning rate: 0.001766]
	Learning Rate: 0.00176604
	LOSS [training: 3.1475173363895843 | validation: 1.5041195062590778]
	TIME [epoch: 8.62 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.168009366399888		[learning rate: 0.0017596]
	Learning Rate: 0.00175963
	LOSS [training: 3.168009366399888 | validation: 1.4239027845338499]
	TIME [epoch: 8.62 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1802354048744035		[learning rate: 0.0017532]
	Learning Rate: 0.00175324
	LOSS [training: 3.1802354048744035 | validation: 1.4175903863823798]
	TIME [epoch: 8.64 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2170741811553656		[learning rate: 0.0017469]
	Learning Rate: 0.00174688
	LOSS [training: 3.2170741811553656 | validation: 1.4146561245628182]
	TIME [epoch: 8.63 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.158640774307748		[learning rate: 0.0017405]
	Learning Rate: 0.00174054
	LOSS [training: 3.158640774307748 | validation: 1.4623908896902038]
	TIME [epoch: 8.62 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1440675909342266		[learning rate: 0.0017342]
	Learning Rate: 0.00173422
	LOSS [training: 3.1440675909342266 | validation: 1.579255856685074]
	TIME [epoch: 8.62 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.199113221869972		[learning rate: 0.0017279]
	Learning Rate: 0.00172793
	LOSS [training: 3.199113221869972 | validation: 1.4380712694099842]
	TIME [epoch: 8.64 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.179317495768184		[learning rate: 0.0017217]
	Learning Rate: 0.00172166
	LOSS [training: 3.179317495768184 | validation: 1.455228885219491]
	TIME [epoch: 8.62 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1690166804781272		[learning rate: 0.0017154]
	Learning Rate: 0.00171541
	LOSS [training: 3.1690166804781272 | validation: 1.394194003574312]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.154693394788702		[learning rate: 0.0017092]
	Learning Rate: 0.00170919
	LOSS [training: 3.154693394788702 | validation: 1.6245298845595335]
	TIME [epoch: 8.62 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.181049345318496		[learning rate: 0.001703]
	Learning Rate: 0.00170298
	LOSS [training: 3.181049345318496 | validation: 1.515547737641768]
	TIME [epoch: 8.64 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1465271138010666		[learning rate: 0.0016968]
	Learning Rate: 0.0016968
	LOSS [training: 3.1465271138010666 | validation: 1.4738761121500246]
	TIME [epoch: 8.63 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1855462721262553		[learning rate: 0.0016906]
	Learning Rate: 0.00169065
	LOSS [training: 3.1855462721262553 | validation: 1.662775537340204]
	TIME [epoch: 8.63 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.178102083814788		[learning rate: 0.0016845]
	Learning Rate: 0.00168451
	LOSS [training: 3.178102083814788 | validation: 1.413951947489491]
	TIME [epoch: 8.63 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1452621164475967		[learning rate: 0.0016784]
	Learning Rate: 0.0016784
	LOSS [training: 3.1452621164475967 | validation: 1.6340996165364197]
	TIME [epoch: 8.64 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1603964348312004		[learning rate: 0.0016723]
	Learning Rate: 0.00167231
	LOSS [training: 3.1603964348312004 | validation: 1.4657137293841236]
	TIME [epoch: 8.62 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.136866333309492		[learning rate: 0.0016662]
	Learning Rate: 0.00166624
	LOSS [training: 3.136866333309492 | validation: 1.4147595053754503]
	TIME [epoch: 8.63 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1431312820973685		[learning rate: 0.0016602]
	Learning Rate: 0.00166019
	LOSS [training: 3.1431312820973685 | validation: 1.4962996998785039]
	TIME [epoch: 8.86 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.157136927546874		[learning rate: 0.0016542]
	Learning Rate: 0.00165417
	LOSS [training: 3.157136927546874 | validation: 1.4866062218685117]
	TIME [epoch: 8.65 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1668591444007115		[learning rate: 0.0016482]
	Learning Rate: 0.00164816
	LOSS [training: 3.1668591444007115 | validation: 1.4127288757286676]
	TIME [epoch: 8.63 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1718065725815334		[learning rate: 0.0016422]
	Learning Rate: 0.00164218
	LOSS [training: 3.1718065725815334 | validation: 1.4233539457496516]
	TIME [epoch: 8.63 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1654845063647454		[learning rate: 0.0016362]
	Learning Rate: 0.00163622
	LOSS [training: 3.1654845063647454 | validation: 1.4830396349947557]
	TIME [epoch: 8.63 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.171626055242012		[learning rate: 0.0016303]
	Learning Rate: 0.00163028
	LOSS [training: 3.171626055242012 | validation: 1.5062040913030552]
	TIME [epoch: 8.65 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.142987402849588		[learning rate: 0.0016244]
	Learning Rate: 0.00162437
	LOSS [training: 3.142987402849588 | validation: 1.4227479557931326]
	TIME [epoch: 8.62 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.181396689196596		[learning rate: 0.0016185]
	Learning Rate: 0.00161847
	LOSS [training: 3.181396689196596 | validation: 1.463937220988265]
	TIME [epoch: 8.62 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.135620351460221		[learning rate: 0.0016126]
	Learning Rate: 0.0016126
	LOSS [training: 3.135620351460221 | validation: 1.6747465840628197]
	TIME [epoch: 8.62 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1638318386392843		[learning rate: 0.0016067]
	Learning Rate: 0.00160675
	LOSS [training: 3.1638318386392843 | validation: 1.4111741250606917]
	TIME [epoch: 8.65 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1484243750479948		[learning rate: 0.0016009]
	Learning Rate: 0.00160092
	LOSS [training: 3.1484243750479948 | validation: 1.4408183680977389]
	TIME [epoch: 8.62 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1358805570134187		[learning rate: 0.0015951]
	Learning Rate: 0.00159511
	LOSS [training: 3.1358805570134187 | validation: 1.4176689351623273]
	TIME [epoch: 8.63 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1926019749200725		[learning rate: 0.0015893]
	Learning Rate: 0.00158932
	LOSS [training: 3.1926019749200725 | validation: 1.4012619596247886]
	TIME [epoch: 8.63 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.167119093864732		[learning rate: 0.0015835]
	Learning Rate: 0.00158355
	LOSS [training: 3.167119093864732 | validation: 1.4644165054200995]
	TIME [epoch: 8.64 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1652324320498506		[learning rate: 0.0015778]
	Learning Rate: 0.0015778
	LOSS [training: 3.1652324320498506 | validation: 1.703665577842687]
	TIME [epoch: 8.62 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1781543074785894		[learning rate: 0.0015721]
	Learning Rate: 0.00157208
	LOSS [training: 3.1781543074785894 | validation: 1.4161176211587454]
	TIME [epoch: 8.62 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.130087571792406		[learning rate: 0.0015664]
	Learning Rate: 0.00156637
	LOSS [training: 3.130087571792406 | validation: 1.4767676584547975]
	TIME [epoch: 8.63 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1783594932960084		[learning rate: 0.0015607]
	Learning Rate: 0.00156069
	LOSS [training: 3.1783594932960084 | validation: 1.4251830857844292]
	TIME [epoch: 8.64 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.128414021418085		[learning rate: 0.001555]
	Learning Rate: 0.00155502
	LOSS [training: 3.128414021418085 | validation: 1.3998982463641427]
	TIME [epoch: 8.62 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1167801427636395		[learning rate: 0.0015494]
	Learning Rate: 0.00154938
	LOSS [training: 3.1167801427636395 | validation: 1.5847288569592426]
	TIME [epoch: 8.62 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.137958029836386		[learning rate: 0.0015438]
	Learning Rate: 0.00154376
	LOSS [training: 3.137958029836386 | validation: 1.400554440084929]
	TIME [epoch: 8.64 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1518678547989967		[learning rate: 0.0015382]
	Learning Rate: 0.00153815
	LOSS [training: 3.1518678547989967 | validation: 1.4419802155146773]
	TIME [epoch: 8.63 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1526691876329656		[learning rate: 0.0015326]
	Learning Rate: 0.00153257
	LOSS [training: 3.1526691876329656 | validation: 1.5574626640143832]
	TIME [epoch: 8.62 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.164245325128881		[learning rate: 0.001527]
	Learning Rate: 0.00152701
	LOSS [training: 3.164245325128881 | validation: 1.433428222136559]
	TIME [epoch: 8.63 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1529956329038518		[learning rate: 0.0015215]
	Learning Rate: 0.00152147
	LOSS [training: 3.1529956329038518 | validation: 1.642964871542533]
	TIME [epoch: 8.65 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1644765680283293		[learning rate: 0.0015159]
	Learning Rate: 0.00151595
	LOSS [training: 3.1644765680283293 | validation: 1.4418523316259384]
	TIME [epoch: 8.63 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1487910106088757		[learning rate: 0.0015104]
	Learning Rate: 0.00151045
	LOSS [training: 3.1487910106088757 | validation: 1.3989919217627773]
	TIME [epoch: 8.62 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1626831672861924		[learning rate: 0.001505]
	Learning Rate: 0.00150496
	LOSS [training: 3.1626831672861924 | validation: 1.4645093613474176]
	TIME [epoch: 8.62 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.133026439745		[learning rate: 0.0014995]
	Learning Rate: 0.0014995
	LOSS [training: 3.133026439745 | validation: 1.4398284836226316]
	TIME [epoch: 8.64 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1372225648541203		[learning rate: 0.0014941]
	Learning Rate: 0.00149406
	LOSS [training: 3.1372225648541203 | validation: 1.4483748292187821]
	TIME [epoch: 8.62 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1308351068376545		[learning rate: 0.0014886]
	Learning Rate: 0.00148864
	LOSS [training: 3.1308351068376545 | validation: 1.4038938730241526]
	TIME [epoch: 8.62 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.136305603987794		[learning rate: 0.0014832]
	Learning Rate: 0.00148324
	LOSS [training: 3.136305603987794 | validation: 1.444523304931786]
	TIME [epoch: 8.63 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.13323196554413		[learning rate: 0.0014779]
	Learning Rate: 0.00147785
	LOSS [training: 3.13323196554413 | validation: 1.4192544761917147]
	TIME [epoch: 8.65 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.129232458226424		[learning rate: 0.0014725]
	Learning Rate: 0.00147249
	LOSS [training: 3.129232458226424 | validation: 1.5364478996168343]
	TIME [epoch: 8.62 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.154559799831911		[learning rate: 0.0014671]
	Learning Rate: 0.00146715
	LOSS [training: 3.154559799831911 | validation: 1.4669205395941636]
	TIME [epoch: 8.63 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.121036110239335		[learning rate: 0.0014618]
	Learning Rate: 0.00146182
	LOSS [training: 3.121036110239335 | validation: 1.4030312550788464]
	TIME [epoch: 8.62 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.143789258292263		[learning rate: 0.0014565]
	Learning Rate: 0.00145652
	LOSS [training: 3.143789258292263 | validation: 1.4360245213847036]
	TIME [epoch: 8.65 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.129969418939277		[learning rate: 0.0014512]
	Learning Rate: 0.00145123
	LOSS [training: 3.129969418939277 | validation: 1.5730700343002437]
	TIME [epoch: 8.62 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1553174640180357		[learning rate: 0.001446]
	Learning Rate: 0.00144597
	LOSS [training: 3.1553174640180357 | validation: 1.4363798524292208]
	TIME [epoch: 8.62 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1672867957953326		[learning rate: 0.0014407]
	Learning Rate: 0.00144072
	LOSS [training: 3.1672867957953326 | validation: 1.4247091350959293]
	TIME [epoch: 8.62 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1165473174352365		[learning rate: 0.0014355]
	Learning Rate: 0.00143549
	LOSS [training: 3.1165473174352365 | validation: 1.4055023538066709]
	TIME [epoch: 8.64 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.142250784305527		[learning rate: 0.0014303]
	Learning Rate: 0.00143028
	LOSS [training: 3.142250784305527 | validation: 1.430458464369525]
	TIME [epoch: 8.62 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1198679647695795		[learning rate: 0.0014251]
	Learning Rate: 0.00142509
	LOSS [training: 3.1198679647695795 | validation: 1.460005847444832]
	TIME [epoch: 8.62 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1448637892709432		[learning rate: 0.0014199]
	Learning Rate: 0.00141992
	LOSS [training: 3.1448637892709432 | validation: 1.4100285552203675]
	TIME [epoch: 8.63 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1196477787015566		[learning rate: 0.0014148]
	Learning Rate: 0.00141476
	LOSS [training: 3.1196477787015566 | validation: 1.410805394507076]
	TIME [epoch: 8.63 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1460369832461663		[learning rate: 0.0014096]
	Learning Rate: 0.00140963
	LOSS [training: 3.1460369832461663 | validation: 1.437527511718071]
	TIME [epoch: 8.62 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1324642385087005		[learning rate: 0.0014045]
	Learning Rate: 0.00140451
	LOSS [training: 3.1324642385087005 | validation: 1.4123155990375769]
	TIME [epoch: 8.62 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1424484477115793		[learning rate: 0.0013994]
	Learning Rate: 0.00139942
	LOSS [training: 3.1424484477115793 | validation: 1.43869351102248]
	TIME [epoch: 8.64 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.129828562314631		[learning rate: 0.0013943]
	Learning Rate: 0.00139434
	LOSS [training: 3.129828562314631 | validation: 1.4933402321128977]
	TIME [epoch: 8.62 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1368165567344555		[learning rate: 0.0013893]
	Learning Rate: 0.00138928
	LOSS [training: 3.1368165567344555 | validation: 1.4173206046301168]
	TIME [epoch: 8.62 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1300682821402974		[learning rate: 0.0013842]
	Learning Rate: 0.00138424
	LOSS [training: 3.1300682821402974 | validation: 1.427106485908832]
	TIME [epoch: 8.62 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1430714861846583		[learning rate: 0.0013792]
	Learning Rate: 0.00137921
	LOSS [training: 3.1430714861846583 | validation: 1.4177787494299294]
	TIME [epoch: 8.64 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1357603530819693		[learning rate: 0.0013742]
	Learning Rate: 0.00137421
	LOSS [training: 3.1357603530819693 | validation: 1.5750141459995255]
	TIME [epoch: 8.62 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.197635957704557		[learning rate: 0.0013692]
	Learning Rate: 0.00136922
	LOSS [training: 3.197635957704557 | validation: 1.6186469935357055]
	TIME [epoch: 8.62 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1294523232719067		[learning rate: 0.0013643]
	Learning Rate: 0.00136425
	LOSS [training: 3.1294523232719067 | validation: 1.45590291983088]
	TIME [epoch: 8.62 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1505819300448		[learning rate: 0.0013593]
	Learning Rate: 0.0013593
	LOSS [training: 3.1505819300448 | validation: 1.4212546684789304]
	TIME [epoch: 8.64 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.137659262548821		[learning rate: 0.0013544]
	Learning Rate: 0.00135437
	LOSS [training: 3.137659262548821 | validation: 1.415840618152426]
	TIME [epoch: 8.62 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.133226496375904		[learning rate: 0.0013495]
	Learning Rate: 0.00134945
	LOSS [training: 3.133226496375904 | validation: 1.4324408837020646]
	TIME [epoch: 8.62 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.113467567476271		[learning rate: 0.0013446]
	Learning Rate: 0.00134456
	LOSS [training: 3.113467567476271 | validation: 1.4048315791458343]
	TIME [epoch: 8.62 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.10804906812522		[learning rate: 0.0013397]
	Learning Rate: 0.00133968
	LOSS [training: 3.10804906812522 | validation: 1.4460800172450872]
	TIME [epoch: 8.65 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.109812965528115		[learning rate: 0.0013348]
	Learning Rate: 0.00133481
	LOSS [training: 3.109812965528115 | validation: 1.5641855966810534]
	TIME [epoch: 8.62 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1612512855970722		[learning rate: 0.00133]
	Learning Rate: 0.00132997
	LOSS [training: 3.1612512855970722 | validation: 1.425501907576971]
	TIME [epoch: 8.62 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.157299073347683		[learning rate: 0.0013251]
	Learning Rate: 0.00132514
	LOSS [training: 3.157299073347683 | validation: 1.4502139045315499]
	TIME [epoch: 8.62 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.124295502199841		[learning rate: 0.0013203]
	Learning Rate: 0.00132034
	LOSS [training: 3.124295502199841 | validation: 1.4113737189824687]
	TIME [epoch: 8.64 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.131881104748666		[learning rate: 0.0013155]
	Learning Rate: 0.00131554
	LOSS [training: 3.131881104748666 | validation: 1.4530605600361808]
	TIME [epoch: 8.62 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1214545282713173		[learning rate: 0.0013108]
	Learning Rate: 0.00131077
	LOSS [training: 3.1214545282713173 | validation: 1.5394471254416289]
	TIME [epoch: 8.62 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.141515010877841		[learning rate: 0.001306]
	Learning Rate: 0.00130601
	LOSS [training: 3.141515010877841 | validation: 1.5221112071931409]
	TIME [epoch: 8.63 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.10915036178455		[learning rate: 0.0013013]
	Learning Rate: 0.00130127
	LOSS [training: 3.10915036178455 | validation: 1.5013137808659964]
	TIME [epoch: 8.64 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.161102987296417		[learning rate: 0.0012966]
	Learning Rate: 0.00129655
	LOSS [training: 3.161102987296417 | validation: 1.4220724844598418]
	TIME [epoch: 8.62 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.120893105188805		[learning rate: 0.0012918]
	Learning Rate: 0.00129185
	LOSS [training: 3.120893105188805 | validation: 1.4444586531629908]
	TIME [epoch: 8.62 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1389010117518223		[learning rate: 0.0012872]
	Learning Rate: 0.00128716
	LOSS [training: 3.1389010117518223 | validation: 1.4360632833514144]
	TIME [epoch: 8.64 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1156269756023383		[learning rate: 0.0012825]
	Learning Rate: 0.00128249
	LOSS [training: 3.1156269756023383 | validation: 1.5045920570773068]
	TIME [epoch: 8.62 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1478710272518233		[learning rate: 0.0012778]
	Learning Rate: 0.00127783
	LOSS [training: 3.1478710272518233 | validation: 1.4376528778173445]
	TIME [epoch: 8.62 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.097705826090686		[learning rate: 0.0012732]
	Learning Rate: 0.00127319
	LOSS [training: 3.097705826090686 | validation: 1.4732613107684465]
	TIME [epoch: 8.62 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.143730912938274		[learning rate: 0.0012686]
	Learning Rate: 0.00126857
	LOSS [training: 3.143730912938274 | validation: 1.4012624229360777]
	TIME [epoch: 8.64 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1155914682499435		[learning rate: 0.001264]
	Learning Rate: 0.00126397
	LOSS [training: 3.1155914682499435 | validation: 1.4013386457839254]
	TIME [epoch: 8.62 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1356475712130827		[learning rate: 0.0012594]
	Learning Rate: 0.00125938
	LOSS [training: 3.1356475712130827 | validation: 1.4087968805486244]
	TIME [epoch: 8.62 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.157729967173818		[learning rate: 0.0012548]
	Learning Rate: 0.00125481
	LOSS [training: 3.157729967173818 | validation: 1.3974691700674606]
	TIME [epoch: 8.62 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.118443110404221		[learning rate: 0.0012503]
	Learning Rate: 0.00125026
	LOSS [training: 3.118443110404221 | validation: 1.433141713127586]
	TIME [epoch: 8.64 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1477025783024297		[learning rate: 0.0012457]
	Learning Rate: 0.00124572
	LOSS [training: 3.1477025783024297 | validation: 1.4142523532602278]
	TIME [epoch: 8.63 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1154655044857256		[learning rate: 0.0012412]
	Learning Rate: 0.0012412
	LOSS [training: 3.1154655044857256 | validation: 1.4409532041632478]
	TIME [epoch: 8.62 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.122392959974816		[learning rate: 0.0012367]
	Learning Rate: 0.0012367
	LOSS [training: 3.122392959974816 | validation: 1.5179398528948156]
	TIME [epoch: 8.62 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1393950430860698		[learning rate: 0.0012322]
	Learning Rate: 0.00123221
	LOSS [training: 3.1393950430860698 | validation: 1.4185678941031032]
	TIME [epoch: 8.65 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1318317811906127		[learning rate: 0.0012277]
	Learning Rate: 0.00122774
	LOSS [training: 3.1318317811906127 | validation: 1.4022119518767076]
	TIME [epoch: 8.63 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.127246282678395		[learning rate: 0.0012233]
	Learning Rate: 0.00122328
	LOSS [training: 3.127246282678395 | validation: 1.39652389249987]
	TIME [epoch: 8.62 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1293846035548305		[learning rate: 0.0012188]
	Learning Rate: 0.00121884
	LOSS [training: 3.1293846035548305 | validation: 1.491469655501255]
	TIME [epoch: 8.62 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.127992414494057		[learning rate: 0.0012144]
	Learning Rate: 0.00121442
	LOSS [training: 3.127992414494057 | validation: 1.4135196836227042]
	TIME [epoch: 8.64 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1743350731112607		[learning rate: 0.00121]
	Learning Rate: 0.00121001
	LOSS [training: 3.1743350731112607 | validation: 1.410264480008138]
	TIME [epoch: 8.62 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.120178176496398		[learning rate: 0.0012056]
	Learning Rate: 0.00120562
	LOSS [training: 3.120178176496398 | validation: 1.3983572814830194]
	TIME [epoch: 8.62 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1112044714528646		[learning rate: 0.0012012]
	Learning Rate: 0.00120125
	LOSS [training: 3.1112044714528646 | validation: 1.4003396952006537]
	TIME [epoch: 8.63 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1328694677276223		[learning rate: 0.0011969]
	Learning Rate: 0.00119689
	LOSS [training: 3.1328694677276223 | validation: 1.498819070350843]
	TIME [epoch: 8.63 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1440496367950095		[learning rate: 0.0011925]
	Learning Rate: 0.00119254
	LOSS [training: 3.1440496367950095 | validation: 1.4161193334939761]
	TIME [epoch: 8.62 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1096835815807102		[learning rate: 0.0011882]
	Learning Rate: 0.00118821
	LOSS [training: 3.1096835815807102 | validation: 1.431005933913712]
	TIME [epoch: 8.62 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1128380645196483		[learning rate: 0.0011839]
	Learning Rate: 0.0011839
	LOSS [training: 3.1128380645196483 | validation: 1.605152427437416]
	TIME [epoch: 8.63 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1327467678413927		[learning rate: 0.0011796]
	Learning Rate: 0.00117961
	LOSS [training: 3.1327467678413927 | validation: 1.4216274756977059]
	TIME [epoch: 8.63 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.100527575596286		[learning rate: 0.0011753]
	Learning Rate: 0.00117532
	LOSS [training: 3.100527575596286 | validation: 1.4815788060868524]
	TIME [epoch: 8.62 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1486283446020993		[learning rate: 0.0011711]
	Learning Rate: 0.00117106
	LOSS [training: 3.1486283446020993 | validation: 1.4446352788103287]
	TIME [epoch: 8.62 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.139356345708		[learning rate: 0.0011668]
	Learning Rate: 0.00116681
	LOSS [training: 3.139356345708 | validation: 1.4335061872622612]
	TIME [epoch: 8.64 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1367547581359037		[learning rate: 0.0011626]
	Learning Rate: 0.00116258
	LOSS [training: 3.1367547581359037 | validation: 1.4409833370383458]
	TIME [epoch: 8.62 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.151321028633314		[learning rate: 0.0011584]
	Learning Rate: 0.00115836
	LOSS [training: 3.151321028633314 | validation: 1.4387254410860155]
	TIME [epoch: 8.62 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.181470565235215		[learning rate: 0.0011542]
	Learning Rate: 0.00115415
	LOSS [training: 3.181470565235215 | validation: 1.4089450390077942]
	TIME [epoch: 8.62 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1151656108297785		[learning rate: 0.00115]
	Learning Rate: 0.00114996
	LOSS [training: 3.1151656108297785 | validation: 1.4096014182218812]
	TIME [epoch: 8.64 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1055945678680024		[learning rate: 0.0011458]
	Learning Rate: 0.00114579
	LOSS [training: 3.1055945678680024 | validation: 1.3906291692525738]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_696.pth
	Model improved!!!
EPOCH 697/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.098513488527156		[learning rate: 0.0011416]
	Learning Rate: 0.00114163
	LOSS [training: 3.098513488527156 | validation: 1.3941134899382956]
	TIME [epoch: 8.61 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.145116227570508		[learning rate: 0.0011375]
	Learning Rate: 0.00113749
	LOSS [training: 3.145116227570508 | validation: 1.4952867698810124]
	TIME [epoch: 8.62 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1177237391691315		[learning rate: 0.0011334]
	Learning Rate: 0.00113336
	LOSS [training: 3.1177237391691315 | validation: 1.427499215464771]
	TIME [epoch: 8.64 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.097938994975364		[learning rate: 0.0011292]
	Learning Rate: 0.00112925
	LOSS [training: 3.097938994975364 | validation: 1.4239454543480718]
	TIME [epoch: 8.61 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.121128045343947		[learning rate: 0.0011252]
	Learning Rate: 0.00112515
	LOSS [training: 3.121128045343947 | validation: 1.4086987589763722]
	TIME [epoch: 8.61 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.151748810628199		[learning rate: 0.0011211]
	Learning Rate: 0.00112107
	LOSS [training: 3.151748810628199 | validation: 1.383555295649039]
	TIME [epoch: 8.61 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_702.pth
	Model improved!!!
EPOCH 703/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1168515567004076		[learning rate: 0.001117]
	Learning Rate: 0.001117
	LOSS [training: 3.1168515567004076 | validation: 1.4294854736947697]
	TIME [epoch: 8.63 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.105078799285618		[learning rate: 0.0011129]
	Learning Rate: 0.00111295
	LOSS [training: 3.105078799285618 | validation: 1.4227178250461994]
	TIME [epoch: 8.61 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1082297312243625		[learning rate: 0.0011089]
	Learning Rate: 0.00110891
	LOSS [training: 3.1082297312243625 | validation: 1.410447025342878]
	TIME [epoch: 8.62 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0957480278212635		[learning rate: 0.0011049]
	Learning Rate: 0.00110488
	LOSS [training: 3.0957480278212635 | validation: 1.3945613312494076]
	TIME [epoch: 8.62 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.098079932505636		[learning rate: 0.0011009]
	Learning Rate: 0.00110087
	LOSS [training: 3.098079932505636 | validation: 1.4081626976843962]
	TIME [epoch: 8.63 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.114797450477943		[learning rate: 0.0010969]
	Learning Rate: 0.00109688
	LOSS [training: 3.114797450477943 | validation: 1.563576251789563]
	TIME [epoch: 8.61 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1114340047576343		[learning rate: 0.0010929]
	Learning Rate: 0.0010929
	LOSS [training: 3.1114340047576343 | validation: 1.5133013247731966]
	TIME [epoch: 8.61 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.128709878197861		[learning rate: 0.0010889]
	Learning Rate: 0.00108893
	LOSS [training: 3.128709878197861 | validation: 1.4043343612094152]
	TIME [epoch: 8.62 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1184228732436976		[learning rate: 0.001085]
	Learning Rate: 0.00108498
	LOSS [training: 3.1184228732436976 | validation: 1.486765282594497]
	TIME [epoch: 8.62 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.106634690655595		[learning rate: 0.001081]
	Learning Rate: 0.00108104
	LOSS [training: 3.106634690655595 | validation: 1.4132747768400142]
	TIME [epoch: 8.61 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1493284324864943		[learning rate: 0.0010771]
	Learning Rate: 0.00107712
	LOSS [training: 3.1493284324864943 | validation: 1.4457757663923105]
	TIME [epoch: 8.62 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.111987701555847		[learning rate: 0.0010732]
	Learning Rate: 0.00107321
	LOSS [training: 3.111987701555847 | validation: 1.4844540706420508]
	TIME [epoch: 8.63 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.123327499996904		[learning rate: 0.0010693]
	Learning Rate: 0.00106931
	LOSS [training: 3.123327499996904 | validation: 1.4693171357266313]
	TIME [epoch: 8.62 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1145831632720737		[learning rate: 0.0010654]
	Learning Rate: 0.00106543
	LOSS [training: 3.1145831632720737 | validation: 1.4179450548090822]
	TIME [epoch: 8.61 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.124483580688238		[learning rate: 0.0010616]
	Learning Rate: 0.00106157
	LOSS [training: 3.124483580688238 | validation: 1.4478536644178368]
	TIME [epoch: 8.61 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1249505823186103		[learning rate: 0.0010577]
	Learning Rate: 0.00105771
	LOSS [training: 3.1249505823186103 | validation: 1.38726362681491]
	TIME [epoch: 8.63 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0970796326828385		[learning rate: 0.0010539]
	Learning Rate: 0.00105388
	LOSS [training: 3.0970796326828385 | validation: 1.4537048260503813]
	TIME [epoch: 8.62 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0992029205082416		[learning rate: 0.0010501]
	Learning Rate: 0.00105005
	LOSS [training: 3.0992029205082416 | validation: 1.4309588499192012]
	TIME [epoch: 8.61 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0953039843401404		[learning rate: 0.0010462]
	Learning Rate: 0.00104624
	LOSS [training: 3.0953039843401404 | validation: 1.5667027604418202]
	TIME [epoch: 8.62 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1191299340939835		[learning rate: 0.0010424]
	Learning Rate: 0.00104244
	LOSS [training: 3.1191299340939835 | validation: 1.448116727434937]
	TIME [epoch: 8.63 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1068705455273435		[learning rate: 0.0010387]
	Learning Rate: 0.00103866
	LOSS [training: 3.1068705455273435 | validation: 1.4329607424512922]
	TIME [epoch: 8.62 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1179035159687856		[learning rate: 0.0010349]
	Learning Rate: 0.00103489
	LOSS [training: 3.1179035159687856 | validation: 1.3946799866792325]
	TIME [epoch: 8.61 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0977406189054797		[learning rate: 0.0010311]
	Learning Rate: 0.00103114
	LOSS [training: 3.0977406189054797 | validation: 1.5110214765468624]
	TIME [epoch: 8.62 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.126547708567694		[learning rate: 0.0010274]
	Learning Rate: 0.00102739
	LOSS [training: 3.126547708567694 | validation: 1.3924384421336666]
	TIME [epoch: 8.64 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0937300813797095		[learning rate: 0.0010237]
	Learning Rate: 0.00102367
	LOSS [training: 3.0937300813797095 | validation: 1.5080662047970952]
	TIME [epoch: 8.62 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1150905037900865		[learning rate: 0.00102]
	Learning Rate: 0.00101995
	LOSS [training: 3.1150905037900865 | validation: 1.612975402728547]
	TIME [epoch: 8.61 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1182034848192637		[learning rate: 0.0010162]
	Learning Rate: 0.00101625
	LOSS [training: 3.1182034848192637 | validation: 1.4096006028715324]
	TIME [epoch: 8.62 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.115313634377036		[learning rate: 0.0010126]
	Learning Rate: 0.00101256
	LOSS [training: 3.115313634377036 | validation: 1.4102776328006326]
	TIME [epoch: 8.64 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1023041232674555		[learning rate: 0.0010089]
	Learning Rate: 0.00100889
	LOSS [training: 3.1023041232674555 | validation: 1.4418649341095706]
	TIME [epoch: 8.62 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1406451019574293		[learning rate: 0.0010052]
	Learning Rate: 0.00100522
	LOSS [training: 3.1406451019574293 | validation: 1.42018878148256]
	TIME [epoch: 8.61 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0965204080554463		[learning rate: 0.0010016]
	Learning Rate: 0.00100158
	LOSS [training: 3.0965204080554463 | validation: 1.3985570173884452]
	TIME [epoch: 8.62 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0909297925084416		[learning rate: 0.00099794]
	Learning Rate: 0.000997942
	LOSS [training: 3.0909297925084416 | validation: 1.3972426336702113]
	TIME [epoch: 8.63 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.092178262704171		[learning rate: 0.00099432]
	Learning Rate: 0.00099432
	LOSS [training: 3.092178262704171 | validation: 1.4129309022876178]
	TIME [epoch: 8.62 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1106859998698413		[learning rate: 0.00099071]
	Learning Rate: 0.000990712
	LOSS [training: 3.1106859998698413 | validation: 1.420050619313646]
	TIME [epoch: 8.62 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.133077204481895		[learning rate: 0.00098712]
	Learning Rate: 0.000987116
	LOSS [training: 3.133077204481895 | validation: 1.4264094094246909]
	TIME [epoch: 8.63 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.101606099901569		[learning rate: 0.00098353]
	Learning Rate: 0.000983534
	LOSS [training: 3.101606099901569 | validation: 1.3764845492242876]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_738.pth
	Model improved!!!
EPOCH 739/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1112902765133135		[learning rate: 0.00097996]
	Learning Rate: 0.000979965
	LOSS [training: 3.1112902765133135 | validation: 1.4091813764999896]
	TIME [epoch: 8.61 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.110748798366763		[learning rate: 0.00097641]
	Learning Rate: 0.000976409
	LOSS [training: 3.110748798366763 | validation: 1.3951921585049876]
	TIME [epoch: 8.62 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0936949305436228		[learning rate: 0.00097287]
	Learning Rate: 0.000972865
	LOSS [training: 3.0936949305436228 | validation: 1.3858805995935468]
	TIME [epoch: 8.63 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.145546997104251		[learning rate: 0.00096933]
	Learning Rate: 0.000969335
	LOSS [training: 3.145546997104251 | validation: 1.4135106479784538]
	TIME [epoch: 8.62 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0997973572783395		[learning rate: 0.00096582]
	Learning Rate: 0.000965817
	LOSS [training: 3.0997973572783395 | validation: 1.4088532327020908]
	TIME [epoch: 8.62 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.093576288171331		[learning rate: 0.00096231]
	Learning Rate: 0.000962312
	LOSS [training: 3.093576288171331 | validation: 1.4068833411461665]
	TIME [epoch: 8.62 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.115642524747254		[learning rate: 0.00095882]
	Learning Rate: 0.00095882
	LOSS [training: 3.115642524747254 | validation: 1.531527220166748]
	TIME [epoch: 8.64 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.132474872768545		[learning rate: 0.00095534]
	Learning Rate: 0.00095534
	LOSS [training: 3.132474872768545 | validation: 1.3987103061120965]
	TIME [epoch: 8.62 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.089314136749785		[learning rate: 0.00095187]
	Learning Rate: 0.000951873
	LOSS [training: 3.089314136749785 | validation: 1.4112343335467719]
	TIME [epoch: 8.61 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.086534149564578		[learning rate: 0.00094842]
	Learning Rate: 0.000948419
	LOSS [training: 3.086534149564578 | validation: 1.3961946776341247]
	TIME [epoch: 8.61 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1130790051847197		[learning rate: 0.00094498]
	Learning Rate: 0.000944977
	LOSS [training: 3.1130790051847197 | validation: 1.458030489553484]
	TIME [epoch: 8.64 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0916290966524897		[learning rate: 0.00094155]
	Learning Rate: 0.000941547
	LOSS [training: 3.0916290966524897 | validation: 1.4904708335127435]
	TIME [epoch: 8.62 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1057839603722495		[learning rate: 0.00093813]
	Learning Rate: 0.00093813
	LOSS [training: 3.1057839603722495 | validation: 1.4080182512784076]
	TIME [epoch: 8.62 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.098349713117264		[learning rate: 0.00093473]
	Learning Rate: 0.000934726
	LOSS [training: 3.098349713117264 | validation: 1.3989535526035477]
	TIME [epoch: 8.62 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1176434632081564		[learning rate: 0.00093133]
	Learning Rate: 0.000931334
	LOSS [training: 3.1176434632081564 | validation: 1.5151946799097318]
	TIME [epoch: 8.63 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.110318233432692		[learning rate: 0.00092795]
	Learning Rate: 0.000927954
	LOSS [training: 3.110318233432692 | validation: 1.411848308669315]
	TIME [epoch: 8.61 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1282250643664065		[learning rate: 0.00092459]
	Learning Rate: 0.000924586
	LOSS [training: 3.1282250643664065 | validation: 1.3942425669746763]
	TIME [epoch: 8.61 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.086333295416201		[learning rate: 0.00092123]
	Learning Rate: 0.000921231
	LOSS [training: 3.086333295416201 | validation: 1.3852147238077082]
	TIME [epoch: 8.62 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1126581638706585		[learning rate: 0.00091789]
	Learning Rate: 0.000917888
	LOSS [training: 3.1126581638706585 | validation: 1.4327382323920337]
	TIME [epoch: 8.63 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1146578969310723		[learning rate: 0.00091456]
	Learning Rate: 0.000914556
	LOSS [training: 3.1146578969310723 | validation: 1.4136485092256592]
	TIME [epoch: 8.62 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.115254393922425		[learning rate: 0.00091124]
	Learning Rate: 0.000911237
	LOSS [training: 3.115254393922425 | validation: 1.3882258411223136]
	TIME [epoch: 8.62 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.091186453406169		[learning rate: 0.00090793]
	Learning Rate: 0.000907931
	LOSS [training: 3.091186453406169 | validation: 1.4250095624984649]
	TIME [epoch: 8.63 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1049738752648275		[learning rate: 0.00090464]
	Learning Rate: 0.000904636
	LOSS [training: 3.1049738752648275 | validation: 1.3966465149445026]
	TIME [epoch: 8.62 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1042499763808267		[learning rate: 0.00090135]
	Learning Rate: 0.000901353
	LOSS [training: 3.1042499763808267 | validation: 1.4148864241966024]
	TIME [epoch: 8.61 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.102854418074093		[learning rate: 0.00089808]
	Learning Rate: 0.000898082
	LOSS [training: 3.102854418074093 | validation: 1.402626710644669]
	TIME [epoch: 8.61 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1052626318681638		[learning rate: 0.00089482]
	Learning Rate: 0.000894822
	LOSS [training: 3.1052626318681638 | validation: 1.4515264885708148]
	TIME [epoch: 8.64 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0951054602912853		[learning rate: 0.00089157]
	Learning Rate: 0.000891575
	LOSS [training: 3.0951054602912853 | validation: 1.4250524625376306]
	TIME [epoch: 8.62 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0939605636913297		[learning rate: 0.00088834]
	Learning Rate: 0.000888339
	LOSS [training: 3.0939605636913297 | validation: 1.3844191004433983]
	TIME [epoch: 8.61 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.097315202155443		[learning rate: 0.00088512]
	Learning Rate: 0.000885116
	LOSS [training: 3.097315202155443 | validation: 1.5186088534145226]
	TIME [epoch: 8.61 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1133550792241365		[learning rate: 0.0008819]
	Learning Rate: 0.000881903
	LOSS [training: 3.1133550792241365 | validation: 1.5007309644451257]
	TIME [epoch: 8.63 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1144087917508667		[learning rate: 0.0008787]
	Learning Rate: 0.000878703
	LOSS [training: 3.1144087917508667 | validation: 1.5246305772887296]
	TIME [epoch: 8.62 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.109196633050025		[learning rate: 0.00087551]
	Learning Rate: 0.000875514
	LOSS [training: 3.109196633050025 | validation: 1.3823398172682027]
	TIME [epoch: 8.61 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1415900759365774		[learning rate: 0.00087234]
	Learning Rate: 0.000872337
	LOSS [training: 3.1415900759365774 | validation: 1.4032841484941692]
	TIME [epoch: 8.62 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1019333360767534		[learning rate: 0.00086917]
	Learning Rate: 0.000869171
	LOSS [training: 3.1019333360767534 | validation: 1.3887746561501673]
	TIME [epoch: 8.64 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.093554770916805		[learning rate: 0.00086602]
	Learning Rate: 0.000866017
	LOSS [training: 3.093554770916805 | validation: 1.4288967639562526]
	TIME [epoch: 8.61 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.107705446911712		[learning rate: 0.00086287]
	Learning Rate: 0.000862874
	LOSS [training: 3.107705446911712 | validation: 1.449854665288084]
	TIME [epoch: 8.62 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1366931835239558		[learning rate: 0.00085974]
	Learning Rate: 0.000859743
	LOSS [training: 3.1366931835239558 | validation: 1.3988034746935505]
	TIME [epoch: 8.61 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0864358434465005		[learning rate: 0.00085662]
	Learning Rate: 0.000856623
	LOSS [training: 3.0864358434465005 | validation: 1.449503933090664]
	TIME [epoch: 8.63 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1165692094835404		[learning rate: 0.00085351]
	Learning Rate: 0.000853514
	LOSS [training: 3.1165692094835404 | validation: 1.5211450831173023]
	TIME [epoch: 8.61 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1235695102156615		[learning rate: 0.00085042]
	Learning Rate: 0.000850416
	LOSS [training: 3.1235695102156615 | validation: 1.4021350807370132]
	TIME [epoch: 8.61 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1096936316829358		[learning rate: 0.00084733]
	Learning Rate: 0.00084733
	LOSS [training: 3.1096936316829358 | validation: 1.5217957509442703]
	TIME [epoch: 8.62 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0910964249318833		[learning rate: 0.00084426]
	Learning Rate: 0.000844255
	LOSS [training: 3.0910964249318833 | validation: 1.4503827715151636]
	TIME [epoch: 8.63 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.098232942496796		[learning rate: 0.00084119]
	Learning Rate: 0.000841191
	LOSS [training: 3.098232942496796 | validation: 1.3943180821159487]
	TIME [epoch: 8.61 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1011024326239327		[learning rate: 0.00083814]
	Learning Rate: 0.000838139
	LOSS [training: 3.1011024326239327 | validation: 1.3967892742758035]
	TIME [epoch: 8.61 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.093062158013878		[learning rate: 0.0008351]
	Learning Rate: 0.000835097
	LOSS [training: 3.093062158013878 | validation: 1.4473624220154655]
	TIME [epoch: 8.62 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.094362869059032		[learning rate: 0.00083207]
	Learning Rate: 0.000832066
	LOSS [training: 3.094362869059032 | validation: 1.5011959697998694]
	TIME [epoch: 8.63 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.104268982994985		[learning rate: 0.00082905]
	Learning Rate: 0.000829046
	LOSS [training: 3.104268982994985 | validation: 1.39263462262282]
	TIME [epoch: 8.61 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.085353666700211		[learning rate: 0.00082604]
	Learning Rate: 0.000826038
	LOSS [training: 3.085353666700211 | validation: 1.4214735307630502]
	TIME [epoch: 8.61 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.105559928918234		[learning rate: 0.00082304]
	Learning Rate: 0.00082304
	LOSS [training: 3.105559928918234 | validation: 1.4250229311674607]
	TIME [epoch: 8.63 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.108791786683523		[learning rate: 0.00082005]
	Learning Rate: 0.000820053
	LOSS [training: 3.108791786683523 | validation: 1.3909699505267206]
	TIME [epoch: 8.62 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0866927131590693		[learning rate: 0.00081708]
	Learning Rate: 0.000817077
	LOSS [training: 3.0866927131590693 | validation: 1.4406974636727645]
	TIME [epoch: 8.61 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1015651802310815		[learning rate: 0.00081411]
	Learning Rate: 0.000814112
	LOSS [training: 3.1015651802310815 | validation: 1.4042631171072046]
	TIME [epoch: 8.61 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0871664577417715		[learning rate: 0.00081116]
	Learning Rate: 0.000811158
	LOSS [training: 3.0871664577417715 | validation: 1.458267977761996]
	TIME [epoch: 8.63 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.103364210997074		[learning rate: 0.00080821]
	Learning Rate: 0.000808214
	LOSS [training: 3.103364210997074 | validation: 1.4267164352305552]
	TIME [epoch: 8.61 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.088336476187338		[learning rate: 0.00080528]
	Learning Rate: 0.000805281
	LOSS [training: 3.088336476187338 | validation: 1.4398488698367884]
	TIME [epoch: 8.61 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1019099522119427		[learning rate: 0.00080236]
	Learning Rate: 0.000802358
	LOSS [training: 3.1019099522119427 | validation: 1.4566244533648025]
	TIME [epoch: 8.62 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1032461451786983		[learning rate: 0.00079945]
	Learning Rate: 0.000799447
	LOSS [training: 3.1032461451786983 | validation: 1.3942686596228104]
	TIME [epoch: 8.64 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.084461228153914		[learning rate: 0.00079655]
	Learning Rate: 0.000796545
	LOSS [training: 3.084461228153914 | validation: 1.3873616681259986]
	TIME [epoch: 8.61 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.098457155329693		[learning rate: 0.00079365]
	Learning Rate: 0.000793655
	LOSS [training: 3.098457155329693 | validation: 1.3889601120170005]
	TIME [epoch: 8.61 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.127994226221453		[learning rate: 0.00079077]
	Learning Rate: 0.000790775
	LOSS [training: 3.127994226221453 | validation: 1.3966324488285222]
	TIME [epoch: 8.61 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0994905154557277		[learning rate: 0.0007879]
	Learning Rate: 0.000787905
	LOSS [training: 3.0994905154557277 | validation: 1.3875410703282323]
	TIME [epoch: 8.64 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0883918034325775		[learning rate: 0.00078505]
	Learning Rate: 0.000785045
	LOSS [training: 3.0883918034325775 | validation: 1.3904862169464995]
	TIME [epoch: 8.62 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1074356506285405		[learning rate: 0.0007822]
	Learning Rate: 0.000782196
	LOSS [training: 3.1074356506285405 | validation: 1.3904303848924098]
	TIME [epoch: 8.61 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.081614374584309		[learning rate: 0.00077936]
	Learning Rate: 0.000779358
	LOSS [training: 3.081614374584309 | validation: 1.3916008930248789]
	TIME [epoch: 8.61 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0949172485918472		[learning rate: 0.00077653]
	Learning Rate: 0.000776529
	LOSS [training: 3.0949172485918472 | validation: 1.4602702011842237]
	TIME [epoch: 8.63 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.098814436820226		[learning rate: 0.00077371]
	Learning Rate: 0.000773711
	LOSS [training: 3.098814436820226 | validation: 1.3982575594267668]
	TIME [epoch: 8.62 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.089868346759452		[learning rate: 0.0007709]
	Learning Rate: 0.000770903
	LOSS [training: 3.089868346759452 | validation: 1.3789145331893753]
	TIME [epoch: 8.61 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0898901278290496		[learning rate: 0.00076811]
	Learning Rate: 0.000768106
	LOSS [training: 3.0898901278290496 | validation: 1.4678135881696734]
	TIME [epoch: 8.63 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0845338738503485		[learning rate: 0.00076532]
	Learning Rate: 0.000765318
	LOSS [training: 3.0845338738503485 | validation: 1.4042579052355961]
	TIME [epoch: 8.63 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.092570300555377		[learning rate: 0.00076254]
	Learning Rate: 0.000762541
	LOSS [training: 3.092570300555377 | validation: 1.4564228380965147]
	TIME [epoch: 8.61 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.10484758785546		[learning rate: 0.00075977]
	Learning Rate: 0.000759774
	LOSS [training: 3.10484758785546 | validation: 1.4306196321322395]
	TIME [epoch: 8.62 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.107880197814003		[learning rate: 0.00075702]
	Learning Rate: 0.000757016
	LOSS [training: 3.107880197814003 | validation: 1.4127415033844053]
	TIME [epoch: 8.63 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.087236468674576		[learning rate: 0.00075427]
	Learning Rate: 0.000754269
	LOSS [training: 3.087236468674576 | validation: 1.3826897600530248]
	TIME [epoch: 8.62 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0829554776930914		[learning rate: 0.00075153]
	Learning Rate: 0.000751532
	LOSS [training: 3.0829554776930914 | validation: 1.3828542801066765]
	TIME [epoch: 8.61 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0868998256820523		[learning rate: 0.0007488]
	Learning Rate: 0.000748804
	LOSS [training: 3.0868998256820523 | validation: 1.4117017257048436]
	TIME [epoch: 8.61 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0862655107403514		[learning rate: 0.00074609]
	Learning Rate: 0.000746087
	LOSS [training: 3.0862655107403514 | validation: 1.4063775412465487]
	TIME [epoch: 8.63 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0850260612672806		[learning rate: 0.00074338]
	Learning Rate: 0.000743379
	LOSS [training: 3.0850260612672806 | validation: 1.40520122234012]
	TIME [epoch: 8.61 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.081680898712661		[learning rate: 0.00074068]
	Learning Rate: 0.000740682
	LOSS [training: 3.081680898712661 | validation: 1.408532905660893]
	TIME [epoch: 8.61 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.100431075109923		[learning rate: 0.00073799]
	Learning Rate: 0.000737994
	LOSS [training: 3.100431075109923 | validation: 1.432604080760436]
	TIME [epoch: 8.61 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.095570255935482		[learning rate: 0.00073532]
	Learning Rate: 0.000735315
	LOSS [training: 3.095570255935482 | validation: 1.4192779506944988]
	TIME [epoch: 8.63 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0915969351858887		[learning rate: 0.00073265]
	Learning Rate: 0.000732647
	LOSS [training: 3.0915969351858887 | validation: 1.3839229344023147]
	TIME [epoch: 8.61 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.090558773456613		[learning rate: 0.00072999]
	Learning Rate: 0.000729988
	LOSS [training: 3.090558773456613 | validation: 1.394673423450174]
	TIME [epoch: 8.62 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0799394356271974		[learning rate: 0.00072734]
	Learning Rate: 0.000727339
	LOSS [training: 3.0799394356271974 | validation: 1.3987093807397732]
	TIME [epoch: 8.61 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0847985185901643		[learning rate: 0.0007247]
	Learning Rate: 0.000724699
	LOSS [training: 3.0847985185901643 | validation: 1.3977097766497375]
	TIME [epoch: 8.64 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.109513580904838		[learning rate: 0.00072207]
	Learning Rate: 0.000722069
	LOSS [training: 3.109513580904838 | validation: 1.381388828717613]
	TIME [epoch: 8.61 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.082317052233773		[learning rate: 0.00071945]
	Learning Rate: 0.000719449
	LOSS [training: 3.082317052233773 | validation: 1.4099463973827693]
	TIME [epoch: 8.61 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.08724734190361		[learning rate: 0.00071684]
	Learning Rate: 0.000716838
	LOSS [training: 3.08724734190361 | validation: 1.3749015578359807]
	TIME [epoch: 8.61 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_825.pth
	Model improved!!!
EPOCH 826/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.085664060353977		[learning rate: 0.00071424]
	Learning Rate: 0.000714237
	LOSS [training: 3.085664060353977 | validation: 1.4629065443499263]
	TIME [epoch: 8.64 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0827709980533164		[learning rate: 0.00071164]
	Learning Rate: 0.000711645
	LOSS [training: 3.0827709980533164 | validation: 1.454435412874459]
	TIME [epoch: 8.61 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.083466096305444		[learning rate: 0.00070906]
	Learning Rate: 0.000709062
	LOSS [training: 3.083466096305444 | validation: 1.4594923772930635]
	TIME [epoch: 8.61 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0928312586158055		[learning rate: 0.00070649]
	Learning Rate: 0.000706489
	LOSS [training: 3.0928312586158055 | validation: 1.4037010377388501]
	TIME [epoch: 8.62 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.084245333301223		[learning rate: 0.00070392]
	Learning Rate: 0.000703925
	LOSS [training: 3.084245333301223 | validation: 1.4159409959751068]
	TIME [epoch: 8.63 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0842640088507998		[learning rate: 0.00070137]
	Learning Rate: 0.00070137
	LOSS [training: 3.0842640088507998 | validation: 1.4429965834103382]
	TIME [epoch: 8.61 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.104986697093421		[learning rate: 0.00069883]
	Learning Rate: 0.000698825
	LOSS [training: 3.104986697093421 | validation: 1.468768350197193]
	TIME [epoch: 8.61 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0897165679358536		[learning rate: 0.00069629]
	Learning Rate: 0.000696289
	LOSS [training: 3.0897165679358536 | validation: 1.3826415619187857]
	TIME [epoch: 8.63 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0874296239945744		[learning rate: 0.00069376]
	Learning Rate: 0.000693762
	LOSS [training: 3.0874296239945744 | validation: 1.460853257720482]
	TIME [epoch: 8.62 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.082439438156336		[learning rate: 0.00069124]
	Learning Rate: 0.000691244
	LOSS [training: 3.082439438156336 | validation: 1.3895316801776307]
	TIME [epoch: 8.61 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0971902684651083		[learning rate: 0.00068874]
	Learning Rate: 0.000688736
	LOSS [training: 3.0971902684651083 | validation: 1.4402265884667693]
	TIME [epoch: 8.61 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0861489013106667		[learning rate: 0.00068624]
	Learning Rate: 0.000686236
	LOSS [training: 3.0861489013106667 | validation: 1.3723420142854834]
	TIME [epoch: 8.63 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_837.pth
	Model improved!!!
EPOCH 838/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.084642352434112		[learning rate: 0.00068375]
	Learning Rate: 0.000683746
	LOSS [training: 3.084642352434112 | validation: 1.390536550392944]
	TIME [epoch: 8.62 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0822813827022975		[learning rate: 0.00068126]
	Learning Rate: 0.000681265
	LOSS [training: 3.0822813827022975 | validation: 1.466355870869568]
	TIME [epoch: 8.62 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0916619404619836		[learning rate: 0.00067879]
	Learning Rate: 0.000678792
	LOSS [training: 3.0916619404619836 | validation: 1.387503115634884]
	TIME [epoch: 8.62 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.086845034082438		[learning rate: 0.00067633]
	Learning Rate: 0.000676329
	LOSS [training: 3.086845034082438 | validation: 1.3813838809706]
	TIME [epoch: 8.64 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.081543565626145		[learning rate: 0.00067387]
	Learning Rate: 0.000673874
	LOSS [training: 3.081543565626145 | validation: 1.5045818786014697]
	TIME [epoch: 8.62 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.097215215576786		[learning rate: 0.00067143]
	Learning Rate: 0.000671429
	LOSS [training: 3.097215215576786 | validation: 1.4881130315004]
	TIME [epoch: 8.61 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.098554116285753		[learning rate: 0.00066899]
	Learning Rate: 0.000668992
	LOSS [training: 3.098554116285753 | validation: 1.5427592450645493]
	TIME [epoch: 8.61 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0858919457915386		[learning rate: 0.00066656]
	Learning Rate: 0.000666564
	LOSS [training: 3.0858919457915386 | validation: 1.3790471465238896]
	TIME [epoch: 8.63 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0894347476710116		[learning rate: 0.00066415]
	Learning Rate: 0.000664145
	LOSS [training: 3.0894347476710116 | validation: 1.4100731060444445]
	TIME [epoch: 8.61 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0821514387433373		[learning rate: 0.00066174]
	Learning Rate: 0.000661735
	LOSS [training: 3.0821514387433373 | validation: 1.3985405095695276]
	TIME [epoch: 8.61 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.080463970177793		[learning rate: 0.00065933]
	Learning Rate: 0.000659334
	LOSS [training: 3.080463970177793 | validation: 1.3829273984909207]
	TIME [epoch: 8.62 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.089367319125864		[learning rate: 0.00065694]
	Learning Rate: 0.000656941
	LOSS [training: 3.089367319125864 | validation: 1.493800039804142]
	TIME [epoch: 8.63 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1059750640343062		[learning rate: 0.00065456]
	Learning Rate: 0.000654557
	LOSS [training: 3.1059750640343062 | validation: 1.3816660712425648]
	TIME [epoch: 8.62 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.070478575466331		[learning rate: 0.00065218]
	Learning Rate: 0.000652182
	LOSS [training: 3.070478575466331 | validation: 1.399447233310376]
	TIME [epoch: 8.62 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.088932244478967		[learning rate: 0.00064981]
	Learning Rate: 0.000649815
	LOSS [training: 3.088932244478967 | validation: 1.382590630292536]
	TIME [epoch: 8.62 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0837741397834337		[learning rate: 0.00064746]
	Learning Rate: 0.000647456
	LOSS [training: 3.0837741397834337 | validation: 1.3878028020244164]
	TIME [epoch: 8.62 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.076539839681989		[learning rate: 0.00064511]
	Learning Rate: 0.000645107
	LOSS [training: 3.076539839681989 | validation: 1.3777017018436677]
	TIME [epoch: 8.61 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.073812550550131		[learning rate: 0.00064277]
	Learning Rate: 0.000642766
	LOSS [training: 3.073812550550131 | validation: 1.3988290511721169]
	TIME [epoch: 8.61 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0794763232623152		[learning rate: 0.00064043]
	Learning Rate: 0.000640433
	LOSS [training: 3.0794763232623152 | validation: 1.4271724816652986]
	TIME [epoch: 8.62 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0775275920481615		[learning rate: 0.00063811]
	Learning Rate: 0.000638109
	LOSS [training: 3.0775275920481615 | validation: 1.5039770874222258]
	TIME [epoch: 8.63 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1277030202761105		[learning rate: 0.00063579]
	Learning Rate: 0.000635793
	LOSS [training: 3.1277030202761105 | validation: 1.3812509224375629]
	TIME [epoch: 8.61 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0790515603022066		[learning rate: 0.00063349]
	Learning Rate: 0.000633486
	LOSS [training: 3.0790515603022066 | validation: 1.3992853917839034]
	TIME [epoch: 8.61 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0855292547289226		[learning rate: 0.00063119]
	Learning Rate: 0.000631187
	LOSS [training: 3.0855292547289226 | validation: 1.402228385993434]
	TIME [epoch: 8.63 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.070614090432318		[learning rate: 0.0006289]
	Learning Rate: 0.000628896
	LOSS [training: 3.070614090432318 | validation: 1.4290383814837972]
	TIME [epoch: 8.61 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1302275172749443		[learning rate: 0.00062661]
	Learning Rate: 0.000626614
	LOSS [training: 3.1302275172749443 | validation: 1.3788267962859337]
	TIME [epoch: 8.61 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.079744605937359		[learning rate: 0.00062434]
	Learning Rate: 0.00062434
	LOSS [training: 3.079744605937359 | validation: 1.5917855307914468]
	TIME [epoch: 8.61 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0944840712533717		[learning rate: 0.00062207]
	Learning Rate: 0.000622074
	LOSS [training: 3.0944840712533717 | validation: 1.3928634643106288]
	TIME [epoch: 8.63 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0783875384568753		[learning rate: 0.00061982]
	Learning Rate: 0.000619816
	LOSS [training: 3.0783875384568753 | validation: 1.4436479453979056]
	TIME [epoch: 8.61 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0974335229146996		[learning rate: 0.00061757]
	Learning Rate: 0.000617567
	LOSS [training: 3.0974335229146996 | validation: 1.3902739882193733]
	TIME [epoch: 8.61 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0843854792525236		[learning rate: 0.00061533]
	Learning Rate: 0.000615326
	LOSS [training: 3.0843854792525236 | validation: 1.393536850953321]
	TIME [epoch: 8.61 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0788865157688665		[learning rate: 0.00061309]
	Learning Rate: 0.000613093
	LOSS [training: 3.0788865157688665 | validation: 1.4027245858967765]
	TIME [epoch: 8.64 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.069141423703414		[learning rate: 0.00061087]
	Learning Rate: 0.000610868
	LOSS [training: 3.069141423703414 | validation: 1.4667776103854506]
	TIME [epoch: 8.62 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0792893919998057		[learning rate: 0.00060865]
	Learning Rate: 0.000608651
	LOSS [training: 3.0792893919998057 | validation: 1.384127541523738]
	TIME [epoch: 8.62 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.075317952217064		[learning rate: 0.00060644]
	Learning Rate: 0.000606442
	LOSS [training: 3.075317952217064 | validation: 1.3974020706814523]
	TIME [epoch: 8.62 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.083618390101821		[learning rate: 0.00060424]
	Learning Rate: 0.000604242
	LOSS [training: 3.083618390101821 | validation: 1.4209660459956288]
	TIME [epoch: 8.64 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.075271086525233		[learning rate: 0.00060205]
	Learning Rate: 0.000602049
	LOSS [training: 3.075271086525233 | validation: 1.4732835943148685]
	TIME [epoch: 8.61 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.092239358656058		[learning rate: 0.00059986]
	Learning Rate: 0.000599864
	LOSS [training: 3.092239358656058 | validation: 1.3918229198232617]
	TIME [epoch: 8.62 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.07768814300413		[learning rate: 0.00059769]
	Learning Rate: 0.000597687
	LOSS [training: 3.07768814300413 | validation: 1.4027917426214733]
	TIME [epoch: 8.63 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0810280946141706		[learning rate: 0.00059552]
	Learning Rate: 0.000595518
	LOSS [training: 3.0810280946141706 | validation: 1.376638381225791]
	TIME [epoch: 8.63 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0727930659719185		[learning rate: 0.00059336]
	Learning Rate: 0.000593357
	LOSS [training: 3.0727930659719185 | validation: 1.4056704227755252]
	TIME [epoch: 8.61 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0924094349337126		[learning rate: 0.0005912]
	Learning Rate: 0.000591203
	LOSS [training: 3.0924094349337126 | validation: 1.37883350500809]
	TIME [epoch: 8.61 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.099943953061932		[learning rate: 0.00058906]
	Learning Rate: 0.000589058
	LOSS [training: 3.099943953061932 | validation: 1.3839973163371306]
	TIME [epoch: 8.62 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0781552508628964		[learning rate: 0.00058692]
	Learning Rate: 0.00058692
	LOSS [training: 3.0781552508628964 | validation: 1.3973164691730822]
	TIME [epoch: 8.63 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.079930916440076		[learning rate: 0.00058479]
	Learning Rate: 0.00058479
	LOSS [training: 3.079930916440076 | validation: 1.3854598206893731]
	TIME [epoch: 8.61 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.08244717841781		[learning rate: 0.00058267]
	Learning Rate: 0.000582668
	LOSS [training: 3.08244717841781 | validation: 1.3845322686157615]
	TIME [epoch: 8.62 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0761672566297578		[learning rate: 0.00058055]
	Learning Rate: 0.000580553
	LOSS [training: 3.0761672566297578 | validation: 1.4336729035948546]
	TIME [epoch: 8.62 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.098170020995311		[learning rate: 0.00057845]
	Learning Rate: 0.000578446
	LOSS [training: 3.098170020995311 | validation: 1.412460754939206]
	TIME [epoch: 8.61 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0678944009727216		[learning rate: 0.00057635]
	Learning Rate: 0.000576347
	LOSS [training: 3.0678944009727216 | validation: 1.4656410809848845]
	TIME [epoch: 8.61 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0701343658945683		[learning rate: 0.00057426]
	Learning Rate: 0.000574256
	LOSS [training: 3.0701343658945683 | validation: 1.3718812155363835]
	TIME [epoch: 8.61 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_886.pth
	Model improved!!!
EPOCH 887/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0810648859247745		[learning rate: 0.00057217]
	Learning Rate: 0.000572172
	LOSS [training: 3.0810648859247745 | validation: 1.4021973652518311]
	TIME [epoch: 8.63 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.075524090785633		[learning rate: 0.0005701]
	Learning Rate: 0.000570095
	LOSS [training: 3.075524090785633 | validation: 1.3901751587206457]
	TIME [epoch: 8.62 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.071455254649339		[learning rate: 0.00056803]
	Learning Rate: 0.000568026
	LOSS [training: 3.071455254649339 | validation: 1.378851939154801]
	TIME [epoch: 8.62 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0714183505744512		[learning rate: 0.00056596]
	Learning Rate: 0.000565965
	LOSS [training: 3.0714183505744512 | validation: 1.3836932058300881]
	TIME [epoch: 8.61 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.07699555094758		[learning rate: 0.00056391]
	Learning Rate: 0.000563911
	LOSS [training: 3.07699555094758 | validation: 1.3812978515334662]
	TIME [epoch: 8.63 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0784108543368203		[learning rate: 0.00056186]
	Learning Rate: 0.000561864
	LOSS [training: 3.0784108543368203 | validation: 1.466237462923226]
	TIME [epoch: 8.61 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0918877130792852		[learning rate: 0.00055983]
	Learning Rate: 0.000559825
	LOSS [training: 3.0918877130792852 | validation: 1.3815887449605717]
	TIME [epoch: 8.61 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.06260858995114		[learning rate: 0.00055779]
	Learning Rate: 0.000557794
	LOSS [training: 3.06260858995114 | validation: 1.4026806037865074]
	TIME [epoch: 8.61 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.08335405356759		[learning rate: 0.00055577]
	Learning Rate: 0.00055577
	LOSS [training: 3.08335405356759 | validation: 1.4168219206540584]
	TIME [epoch: 8.63 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0814736452759517		[learning rate: 0.00055375]
	Learning Rate: 0.000553753
	LOSS [training: 3.0814736452759517 | validation: 1.392929867729822]
	TIME [epoch: 8.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0834581645802355		[learning rate: 0.00055174]
	Learning Rate: 0.000551743
	LOSS [training: 3.0834581645802355 | validation: 1.3809862623692781]
	TIME [epoch: 8.61 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0787764224557224		[learning rate: 0.00054974]
	Learning Rate: 0.000549741
	LOSS [training: 3.0787764224557224 | validation: 1.3692608843120078]
	TIME [epoch: 8.61 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_898.pth
	Model improved!!!
EPOCH 899/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0693287816892854		[learning rate: 0.00054775]
	Learning Rate: 0.000547746
	LOSS [training: 3.0693287816892854 | validation: 1.3905991583357213]
	TIME [epoch: 8.62 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0934670619522953		[learning rate: 0.00054576]
	Learning Rate: 0.000545758
	LOSS [training: 3.0934670619522953 | validation: 1.3982012627523885]
	TIME [epoch: 8.61 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0621975605630247		[learning rate: 0.00054378]
	Learning Rate: 0.000543777
	LOSS [training: 3.0621975605630247 | validation: 1.4221851685283426]
	TIME [epoch: 8.61 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.109133930883342		[learning rate: 0.0005418]
	Learning Rate: 0.000541804
	LOSS [training: 3.109133930883342 | validation: 1.392056993861289]
	TIME [epoch: 8.62 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.067749790913633		[learning rate: 0.00053984]
	Learning Rate: 0.000539838
	LOSS [training: 3.067749790913633 | validation: 1.3806521149988675]
	TIME [epoch: 8.62 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0723970564543985		[learning rate: 0.00053788]
	Learning Rate: 0.000537879
	LOSS [training: 3.0723970564543985 | validation: 1.3824479667006604]
	TIME [epoch: 8.61 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0728968564600376		[learning rate: 0.00053593]
	Learning Rate: 0.000535926
	LOSS [training: 3.0728968564600376 | validation: 1.389240236817235]
	TIME [epoch: 8.61 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0744737162853606		[learning rate: 0.00053398]
	Learning Rate: 0.000533982
	LOSS [training: 3.0744737162853606 | validation: 1.3883535587879858]
	TIME [epoch: 8.63 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.067848333529298		[learning rate: 0.00053204]
	Learning Rate: 0.000532044
	LOSS [training: 3.067848333529298 | validation: 1.4191840991971867]
	TIME [epoch: 8.61 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0645409135251995		[learning rate: 0.00053011]
	Learning Rate: 0.000530113
	LOSS [training: 3.0645409135251995 | validation: 1.4427901231491702]
	TIME [epoch: 8.61 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.077086958243892		[learning rate: 0.00052819]
	Learning Rate: 0.000528189
	LOSS [training: 3.077086958243892 | validation: 1.3824412406308624]
	TIME [epoch: 8.61 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0765498169442886		[learning rate: 0.00052627]
	Learning Rate: 0.000526272
	LOSS [training: 3.0765498169442886 | validation: 1.404011216106217]
	TIME [epoch: 8.62 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.069214341685904		[learning rate: 0.00052436]
	Learning Rate: 0.000524362
	LOSS [training: 3.069214341685904 | validation: 1.383973401140286]
	TIME [epoch: 8.61 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.08741601249533		[learning rate: 0.00052246]
	Learning Rate: 0.00052246
	LOSS [training: 3.08741601249533 | validation: 1.4206985621472998]
	TIME [epoch: 8.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.082533174189277		[learning rate: 0.00052056]
	Learning Rate: 0.000520563
	LOSS [training: 3.082533174189277 | validation: 1.4037927538246404]
	TIME [epoch: 8.61 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0710086324423416		[learning rate: 0.00051867]
	Learning Rate: 0.000518674
	LOSS [training: 3.0710086324423416 | validation: 1.4028215914151643]
	TIME [epoch: 8.62 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0790064621041235		[learning rate: 0.00051679]
	Learning Rate: 0.000516792
	LOSS [training: 3.0790064621041235 | validation: 1.394200635962765]
	TIME [epoch: 8.61 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.067129132361798		[learning rate: 0.00051492]
	Learning Rate: 0.000514917
	LOSS [training: 3.067129132361798 | validation: 1.394955442093475]
	TIME [epoch: 8.61 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0760966036394652		[learning rate: 0.00051305]
	Learning Rate: 0.000513048
	LOSS [training: 3.0760966036394652 | validation: 1.4239803044508725]
	TIME [epoch: 8.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.064633532944344		[learning rate: 0.00051119]
	Learning Rate: 0.000511186
	LOSS [training: 3.064633532944344 | validation: 1.3931527922493923]
	TIME [epoch: 8.62 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0695740915908694		[learning rate: 0.00050933]
	Learning Rate: 0.000509331
	LOSS [training: 3.0695740915908694 | validation: 1.3858905306751517]
	TIME [epoch: 8.61 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.077097469229442		[learning rate: 0.00050748]
	Learning Rate: 0.000507483
	LOSS [training: 3.077097469229442 | validation: 1.4067609653173876]
	TIME [epoch: 8.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0833280288421494		[learning rate: 0.00050564]
	Learning Rate: 0.000505641
	LOSS [training: 3.0833280288421494 | validation: 1.4313624250267454]
	TIME [epoch: 8.61 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0723167374438214		[learning rate: 0.00050381]
	Learning Rate: 0.000503806
	LOSS [training: 3.0723167374438214 | validation: 1.4165718258829494]
	TIME [epoch: 8.62 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0775237275455867		[learning rate: 0.00050198]
	Learning Rate: 0.000501977
	LOSS [training: 3.0775237275455867 | validation: 1.408835913881787]
	TIME [epoch: 8.61 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0733552331275833		[learning rate: 0.00050016]
	Learning Rate: 0.000500156
	LOSS [training: 3.0733552331275833 | validation: 1.397634426941321]
	TIME [epoch: 8.61 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.075947207664572		[learning rate: 0.00049834]
	Learning Rate: 0.000498341
	LOSS [training: 3.075947207664572 | validation: 1.387084252142193]
	TIME [epoch: 8.61 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0790455477360843		[learning rate: 0.00049653]
	Learning Rate: 0.000496532
	LOSS [training: 3.0790455477360843 | validation: 1.4483883580838324]
	TIME [epoch: 8.62 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.071604427821627		[learning rate: 0.00049473]
	Learning Rate: 0.00049473
	LOSS [training: 3.071604427821627 | validation: 1.4356177431089425]
	TIME [epoch: 8.61 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0718577982240003		[learning rate: 0.00049293]
	Learning Rate: 0.000492935
	LOSS [training: 3.0718577982240003 | validation: 1.3770684756988572]
	TIME [epoch: 8.61 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0717656055402047		[learning rate: 0.00049115]
	Learning Rate: 0.000491146
	LOSS [training: 3.0717656055402047 | validation: 1.3901034793104659]
	TIME [epoch: 8.62 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.07761145224402		[learning rate: 0.00048936]
	Learning Rate: 0.000489364
	LOSS [training: 3.07761145224402 | validation: 1.4131214162835228]
	TIME [epoch: 8.61 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.083217998367378		[learning rate: 0.00048759]
	Learning Rate: 0.000487588
	LOSS [training: 3.083217998367378 | validation: 1.4177507802998537]
	TIME [epoch: 8.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.064453625667728		[learning rate: 0.00048582]
	Learning Rate: 0.000485818
	LOSS [training: 3.064453625667728 | validation: 1.3886408725205412]
	TIME [epoch: 8.6 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.062932991730716		[learning rate: 0.00048406]
	Learning Rate: 0.000484055
	LOSS [training: 3.062932991730716 | validation: 1.379590171465044]
	TIME [epoch: 8.62 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.070186644594277		[learning rate: 0.0004823]
	Learning Rate: 0.000482298
	LOSS [training: 3.070186644594277 | validation: 1.3903409083107698]
	TIME [epoch: 8.61 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0687879834721317		[learning rate: 0.00048055]
	Learning Rate: 0.000480548
	LOSS [training: 3.0687879834721317 | validation: 1.3876705676483625]
	TIME [epoch: 8.61 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0709948011991037		[learning rate: 0.0004788]
	Learning Rate: 0.000478804
	LOSS [training: 3.0709948011991037 | validation: 1.4085498985128564]
	TIME [epoch: 8.61 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0654102159875656		[learning rate: 0.00047707]
	Learning Rate: 0.000477067
	LOSS [training: 3.0654102159875656 | validation: 1.3935101653100128]
	TIME [epoch: 8.62 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.07551213857218		[learning rate: 0.00047534]
	Learning Rate: 0.000475335
	LOSS [training: 3.07551213857218 | validation: 1.3722211055296376]
	TIME [epoch: 8.61 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.105968438977821		[learning rate: 0.00047361]
	Learning Rate: 0.00047361
	LOSS [training: 3.105968438977821 | validation: 1.4071086269638449]
	TIME [epoch: 8.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0650963114902745		[learning rate: 0.00047189]
	Learning Rate: 0.000471891
	LOSS [training: 3.0650963114902745 | validation: 1.4418183706483436]
	TIME [epoch: 8.6 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.070494623535434		[learning rate: 0.00047018]
	Learning Rate: 0.000470179
	LOSS [training: 3.070494623535434 | validation: 1.407773422503247]
	TIME [epoch: 8.62 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0991328498762654		[learning rate: 0.00046847]
	Learning Rate: 0.000468473
	LOSS [training: 3.0991328498762654 | validation: 1.4327180608310406]
	TIME [epoch: 8.59 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.08809673648103		[learning rate: 0.00046677]
	Learning Rate: 0.000466773
	LOSS [training: 3.08809673648103 | validation: 1.3904994704052136]
	TIME [epoch: 8.6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0717457974340467		[learning rate: 0.00046508]
	Learning Rate: 0.000465079
	LOSS [training: 3.0717457974340467 | validation: 1.4559454899646094]
	TIME [epoch: 8.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0680032203301613		[learning rate: 0.00046339]
	Learning Rate: 0.000463391
	LOSS [training: 3.0680032203301613 | validation: 1.3684089873621017]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_945.pth
	Model improved!!!
EPOCH 946/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.060045338946219		[learning rate: 0.00046171]
	Learning Rate: 0.000461709
	LOSS [training: 3.060045338946219 | validation: 1.4669811269382746]
	TIME [epoch: 8.6 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0629290801139573		[learning rate: 0.00046003]
	Learning Rate: 0.000460034
	LOSS [training: 3.0629290801139573 | validation: 1.3823293611049845]
	TIME [epoch: 8.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0692365423737256		[learning rate: 0.00045836]
	Learning Rate: 0.000458364
	LOSS [training: 3.0692365423737256 | validation: 1.3965670297259059]
	TIME [epoch: 8.62 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0673873987518894		[learning rate: 0.0004567]
	Learning Rate: 0.000456701
	LOSS [training: 3.0673873987518894 | validation: 1.3811592641616248]
	TIME [epoch: 8.62 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.070326368938193		[learning rate: 0.00045504]
	Learning Rate: 0.000455043
	LOSS [training: 3.070326368938193 | validation: 1.3663365040051032]
	TIME [epoch: 8.61 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_950.pth
	Model improved!!!
EPOCH 951/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0627586618461207		[learning rate: 0.00045339]
	Learning Rate: 0.000453392
	LOSS [training: 3.0627586618461207 | validation: 1.378417556318051]
	TIME [epoch: 8.61 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.071141708311129		[learning rate: 0.00045175]
	Learning Rate: 0.000451746
	LOSS [training: 3.071141708311129 | validation: 1.4140734283002243]
	TIME [epoch: 8.61 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0803442503745555		[learning rate: 0.00045011]
	Learning Rate: 0.000450107
	LOSS [training: 3.0803442503745555 | validation: 1.4269026765478416]
	TIME [epoch: 8.6 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0769305586865636		[learning rate: 0.00044847]
	Learning Rate: 0.000448474
	LOSS [training: 3.0769305586865636 | validation: 1.380495115881027]
	TIME [epoch: 8.6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0739241180110026		[learning rate: 0.00044685]
	Learning Rate: 0.000446846
	LOSS [training: 3.0739241180110026 | validation: 1.472282808241518]
	TIME [epoch: 8.6 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0692100129100286		[learning rate: 0.00044522]
	Learning Rate: 0.000445224
	LOSS [training: 3.0692100129100286 | validation: 1.3748006908351522]
	TIME [epoch: 8.62 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.071078142341226		[learning rate: 0.00044361]
	Learning Rate: 0.000443609
	LOSS [training: 3.071078142341226 | validation: 1.378962069397152]
	TIME [epoch: 8.6 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.062503698720449		[learning rate: 0.000442]
	Learning Rate: 0.000441999
	LOSS [training: 3.062503698720449 | validation: 1.3732315070415453]
	TIME [epoch: 8.6 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.072664600726437		[learning rate: 0.00044039]
	Learning Rate: 0.000440395
	LOSS [training: 3.072664600726437 | validation: 1.4284610758408538]
	TIME [epoch: 8.6 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0632356410467105		[learning rate: 0.0004388]
	Learning Rate: 0.000438797
	LOSS [training: 3.0632356410467105 | validation: 1.3763287850097596]
	TIME [epoch: 8.62 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.073723642881513		[learning rate: 0.0004372]
	Learning Rate: 0.000437204
	LOSS [training: 3.073723642881513 | validation: 1.3893502009889431]
	TIME [epoch: 8.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0706044489166007		[learning rate: 0.00043562]
	Learning Rate: 0.000435617
	LOSS [training: 3.0706044489166007 | validation: 1.3840304535721808]
	TIME [epoch: 8.6 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0693217569920135		[learning rate: 0.00043404]
	Learning Rate: 0.000434037
	LOSS [training: 3.0693217569920135 | validation: 1.3795621366615323]
	TIME [epoch: 8.6 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0671056537782873		[learning rate: 0.00043246]
	Learning Rate: 0.000432461
	LOSS [training: 3.0671056537782873 | validation: 1.4128485828694228]
	TIME [epoch: 8.63 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0760431795338112		[learning rate: 0.00043089]
	Learning Rate: 0.000430892
	LOSS [training: 3.0760431795338112 | validation: 1.388511316723945]
	TIME [epoch: 8.6 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0630299541438535		[learning rate: 0.00042933]
	Learning Rate: 0.000429328
	LOSS [training: 3.0630299541438535 | validation: 1.3957056180753071]
	TIME [epoch: 8.6 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.064295919658419		[learning rate: 0.00042777]
	Learning Rate: 0.00042777
	LOSS [training: 3.064295919658419 | validation: 1.421416823728712]
	TIME [epoch: 8.6 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.070048356738753		[learning rate: 0.00042622]
	Learning Rate: 0.000426218
	LOSS [training: 3.070048356738753 | validation: 1.3667696281596218]
	TIME [epoch: 8.62 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0645366830848717		[learning rate: 0.00042467]
	Learning Rate: 0.000424671
	LOSS [training: 3.0645366830848717 | validation: 1.4002967671987783]
	TIME [epoch: 8.6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.072655463135038		[learning rate: 0.00042313]
	Learning Rate: 0.00042313
	LOSS [training: 3.072655463135038 | validation: 1.38536620228625]
	TIME [epoch: 8.6 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.061360524206886		[learning rate: 0.00042159]
	Learning Rate: 0.000421594
	LOSS [training: 3.061360524206886 | validation: 1.3844787680016126]
	TIME [epoch: 8.61 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0627059449660123		[learning rate: 0.00042006]
	Learning Rate: 0.000420064
	LOSS [training: 3.0627059449660123 | validation: 1.3911237477641052]
	TIME [epoch: 8.62 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.057615606818844		[learning rate: 0.00041854]
	Learning Rate: 0.00041854
	LOSS [training: 3.057615606818844 | validation: 1.3752170020399739]
	TIME [epoch: 8.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0579634581717636		[learning rate: 0.00041702]
	Learning Rate: 0.000417021
	LOSS [training: 3.0579634581717636 | validation: 1.3838933560297406]
	TIME [epoch: 8.6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0906770997272237		[learning rate: 0.00041551]
	Learning Rate: 0.000415508
	LOSS [training: 3.0906770997272237 | validation: 1.4175286828387175]
	TIME [epoch: 8.61 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0564287508903254		[learning rate: 0.000414]
	Learning Rate: 0.000414
	LOSS [training: 3.0564287508903254 | validation: 1.3824987379635656]
	TIME [epoch: 8.61 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.076300122683341		[learning rate: 0.0004125]
	Learning Rate: 0.000412497
	LOSS [training: 3.076300122683341 | validation: 1.3928100026826085]
	TIME [epoch: 8.6 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0625299282529		[learning rate: 0.000411]
	Learning Rate: 0.000411
	LOSS [training: 3.0625299282529 | validation: 1.4111631320866023]
	TIME [epoch: 8.6 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.069455628858846		[learning rate: 0.00040951]
	Learning Rate: 0.000409509
	LOSS [training: 3.069455628858846 | validation: 1.371784608562004]
	TIME [epoch: 8.61 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0566356391333747		[learning rate: 0.00040802]
	Learning Rate: 0.000408023
	LOSS [training: 3.0566356391333747 | validation: 1.3874970962478779]
	TIME [epoch: 8.6 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0564909565669103		[learning rate: 0.00040654]
	Learning Rate: 0.000406542
	LOSS [training: 3.0564909565669103 | validation: 1.369622816358529]
	TIME [epoch: 8.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.079768769772969		[learning rate: 0.00040507]
	Learning Rate: 0.000405067
	LOSS [training: 3.079768769772969 | validation: 1.37065198519448]
	TIME [epoch: 8.61 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0630282405770153		[learning rate: 0.0004036]
	Learning Rate: 0.000403597
	LOSS [training: 3.0630282405770153 | validation: 1.444125477605088]
	TIME [epoch: 8.63 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0714949872916124		[learning rate: 0.00040213]
	Learning Rate: 0.000402132
	LOSS [training: 3.0714949872916124 | validation: 1.4018885931549914]
	TIME [epoch: 8.62 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.066478370119759		[learning rate: 0.00040067]
	Learning Rate: 0.000400672
	LOSS [training: 3.066478370119759 | validation: 1.4023920920698394]
	TIME [epoch: 8.61 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.066232540895066		[learning rate: 0.00039922]
	Learning Rate: 0.000399218
	LOSS [training: 3.066232540895066 | validation: 1.4247460082252787]
	TIME [epoch: 8.6 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0816512245238017		[learning rate: 0.00039777]
	Learning Rate: 0.00039777
	LOSS [training: 3.0816512245238017 | validation: 1.384784573195875]
	TIME [epoch: 8.62 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0592505942227826		[learning rate: 0.00039633]
	Learning Rate: 0.000396326
	LOSS [training: 3.0592505942227826 | validation: 1.371063143955701]
	TIME [epoch: 8.6 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0636675129469078		[learning rate: 0.00039489]
	Learning Rate: 0.000394888
	LOSS [training: 3.0636675129469078 | validation: 1.3871652702710782]
	TIME [epoch: 8.6 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0614339107739217		[learning rate: 0.00039345]
	Learning Rate: 0.000393455
	LOSS [training: 3.0614339107739217 | validation: 1.3934305312305069]
	TIME [epoch: 8.61 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0645404890280847		[learning rate: 0.00039203]
	Learning Rate: 0.000392027
	LOSS [training: 3.0645404890280847 | validation: 1.375602656089045]
	TIME [epoch: 8.62 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0650640067715584		[learning rate: 0.0003906]
	Learning Rate: 0.000390604
	LOSS [training: 3.0650640067715584 | validation: 1.3964852850375338]
	TIME [epoch: 8.61 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.064974831622284		[learning rate: 0.00038919]
	Learning Rate: 0.000389187
	LOSS [training: 3.064974831622284 | validation: 1.4081733688179856]
	TIME [epoch: 8.6 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.063881680440721		[learning rate: 0.00038777]
	Learning Rate: 0.000387774
	LOSS [training: 3.063881680440721 | validation: 1.363721862428746]
	TIME [epoch: 8.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_994.pth
	Model improved!!!
EPOCH 995/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.060746441014942		[learning rate: 0.00038637]
	Learning Rate: 0.000386367
	LOSS [training: 3.060746441014942 | validation: 1.3652995359522433]
	TIME [epoch: 8.64 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0619808636735315		[learning rate: 0.00038496]
	Learning Rate: 0.000384965
	LOSS [training: 3.0619808636735315 | validation: 1.3623894421445215]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_996.pth
	Model improved!!!
EPOCH 997/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.063094948788872		[learning rate: 0.00038357]
	Learning Rate: 0.000383568
	LOSS [training: 3.063094948788872 | validation: 1.3761567947901088]
	TIME [epoch: 8.62 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0628413457321133		[learning rate: 0.00038218]
	Learning Rate: 0.000382176
	LOSS [training: 3.0628413457321133 | validation: 1.371079468438183]
	TIME [epoch: 8.62 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.069846636162432		[learning rate: 0.00038079]
	Learning Rate: 0.000380789
	LOSS [training: 3.069846636162432 | validation: 1.3848688261470257]
	TIME [epoch: 8.63 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0713972105789837		[learning rate: 0.00037941]
	Learning Rate: 0.000379407
	LOSS [training: 3.0713972105789837 | validation: 1.3770224321552895]
	TIME [epoch: 8.62 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0619504615201563		[learning rate: 0.00037803]
	Learning Rate: 0.00037803
	LOSS [training: 3.0619504615201563 | validation: 1.3921753589034165]
	TIME [epoch: 8.62 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0679894242643666		[learning rate: 0.00037666]
	Learning Rate: 0.000376658
	LOSS [training: 3.0679894242643666 | validation: 1.3601371471953125]
	TIME [epoch: 8.64 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_1002.pth
	Model improved!!!
EPOCH 1003/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0594414615074146		[learning rate: 0.00037529]
	Learning Rate: 0.000375291
	LOSS [training: 3.0594414615074146 | validation: 1.3724718516761196]
	TIME [epoch: 8.62 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0565529483478366		[learning rate: 0.00037393]
	Learning Rate: 0.000373929
	LOSS [training: 3.0565529483478366 | validation: 1.4021031052119697]
	TIME [epoch: 8.61 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0612401995580782		[learning rate: 0.00037257]
	Learning Rate: 0.000372572
	LOSS [training: 3.0612401995580782 | validation: 1.3978278185483977]
	TIME [epoch: 8.62 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0781920956746096		[learning rate: 0.00037122]
	Learning Rate: 0.00037122
	LOSS [training: 3.0781920956746096 | validation: 1.3808047201755131]
	TIME [epoch: 8.64 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.066242510844125		[learning rate: 0.00036987]
	Learning Rate: 0.000369873
	LOSS [training: 3.066242510844125 | validation: 1.3865719158311285]
	TIME [epoch: 8.62 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.074967038551367		[learning rate: 0.00036853]
	Learning Rate: 0.000368531
	LOSS [training: 3.074967038551367 | validation: 1.3645473983676037]
	TIME [epoch: 8.62 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0593887405486813		[learning rate: 0.00036719]
	Learning Rate: 0.000367193
	LOSS [training: 3.0593887405486813 | validation: 1.3878491975148626]
	TIME [epoch: 8.62 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0617893139381733		[learning rate: 0.00036586]
	Learning Rate: 0.000365861
	LOSS [training: 3.0617893139381733 | validation: 1.4284961037705388]
	TIME [epoch: 8.63 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0669867792072694		[learning rate: 0.00036453]
	Learning Rate: 0.000364533
	LOSS [training: 3.0669867792072694 | validation: 1.369481271724118]
	TIME [epoch: 8.61 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.064066607775212		[learning rate: 0.00036321]
	Learning Rate: 0.00036321
	LOSS [training: 3.064066607775212 | validation: 1.4241834649992655]
	TIME [epoch: 8.62 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.070592759065206		[learning rate: 0.00036189]
	Learning Rate: 0.000361892
	LOSS [training: 3.070592759065206 | validation: 1.3744457625374833]
	TIME [epoch: 8.61 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0614285434992157		[learning rate: 0.00036058]
	Learning Rate: 0.000360579
	LOSS [training: 3.0614285434992157 | validation: 1.3784396563650494]
	TIME [epoch: 8.63 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0659825774023686		[learning rate: 0.00035927]
	Learning Rate: 0.00035927
	LOSS [training: 3.0659825774023686 | validation: 1.38014216488794]
	TIME [epoch: 8.61 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.064183999877261		[learning rate: 0.00035797]
	Learning Rate: 0.000357966
	LOSS [training: 3.064183999877261 | validation: 1.4129745528722082]
	TIME [epoch: 8.62 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.063566412865765		[learning rate: 0.00035667]
	Learning Rate: 0.000356667
	LOSS [training: 3.063566412865765 | validation: 1.3635297990800264]
	TIME [epoch: 8.62 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0618034017826252		[learning rate: 0.00035537]
	Learning Rate: 0.000355373
	LOSS [training: 3.0618034017826252 | validation: 1.4114940610769702]
	TIME [epoch: 8.63 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.063250444199861		[learning rate: 0.00035408]
	Learning Rate: 0.000354083
	LOSS [training: 3.063250444199861 | validation: 1.3759191019687007]
	TIME [epoch: 8.61 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.053081106079751		[learning rate: 0.0003528]
	Learning Rate: 0.000352798
	LOSS [training: 3.053081106079751 | validation: 1.374844069586725]
	TIME [epoch: 8.62 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0490263503135666		[learning rate: 0.00035152]
	Learning Rate: 0.000351518
	LOSS [training: 3.0490263503135666 | validation: 1.3768504664933006]
	TIME [epoch: 8.62 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0676970219486086		[learning rate: 0.00035024]
	Learning Rate: 0.000350242
	LOSS [training: 3.0676970219486086 | validation: 1.3837288716339888]
	TIME [epoch: 8.63 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0647148816622067		[learning rate: 0.00034897]
	Learning Rate: 0.000348971
	LOSS [training: 3.0647148816622067 | validation: 1.3726225668187675]
	TIME [epoch: 8.61 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.059710435180988		[learning rate: 0.0003477]
	Learning Rate: 0.000347705
	LOSS [training: 3.059710435180988 | validation: 1.3825294449571146]
	TIME [epoch: 8.62 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.063767875814822		[learning rate: 0.00034644]
	Learning Rate: 0.000346443
	LOSS [training: 3.063767875814822 | validation: 1.378347389044301]
	TIME [epoch: 8.63 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0615265750114005		[learning rate: 0.00034519]
	Learning Rate: 0.000345186
	LOSS [training: 3.0615265750114005 | validation: 1.3987047035806606]
	TIME [epoch: 8.61 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.072177266886127		[learning rate: 0.00034393]
	Learning Rate: 0.000343933
	LOSS [training: 3.072177266886127 | validation: 1.3886556800968346]
	TIME [epoch: 8.61 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.073011392531116		[learning rate: 0.00034268]
	Learning Rate: 0.000342685
	LOSS [training: 3.073011392531116 | validation: 1.428209381128956]
	TIME [epoch: 8.61 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.073695261964383		[learning rate: 0.00034144]
	Learning Rate: 0.000341441
	LOSS [training: 3.073695261964383 | validation: 1.4004476380503992]
	TIME [epoch: 8.63 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0602149977054296		[learning rate: 0.0003402]
	Learning Rate: 0.000340202
	LOSS [training: 3.0602149977054296 | validation: 1.4001395184806231]
	TIME [epoch: 8.62 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0706291232368192		[learning rate: 0.00033897]
	Learning Rate: 0.000338967
	LOSS [training: 3.0706291232368192 | validation: 1.4193459009879899]
	TIME [epoch: 8.61 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.064703845813164		[learning rate: 0.00033774]
	Learning Rate: 0.000337737
	LOSS [training: 3.064703845813164 | validation: 1.377747436933113]
	TIME [epoch: 8.61 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.053839329163199		[learning rate: 0.00033651]
	Learning Rate: 0.000336512
	LOSS [training: 3.053839329163199 | validation: 1.399490078171588]
	TIME [epoch: 8.64 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.065200169748218		[learning rate: 0.00033529]
	Learning Rate: 0.00033529
	LOSS [training: 3.065200169748218 | validation: 1.3748272054191057]
	TIME [epoch: 8.61 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0677262043351132		[learning rate: 0.00033407]
	Learning Rate: 0.000334074
	LOSS [training: 3.0677262043351132 | validation: 1.4292901381834775]
	TIME [epoch: 8.61 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0751425566013606		[learning rate: 0.00033286]
	Learning Rate: 0.000332861
	LOSS [training: 3.0751425566013606 | validation: 1.3704966660372042]
	TIME [epoch: 8.61 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.071509474584369		[learning rate: 0.00033165]
	Learning Rate: 0.000331653
	LOSS [training: 3.071509474584369 | validation: 1.458371692521402]
	TIME [epoch: 8.64 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.077855387108942		[learning rate: 0.00033045]
	Learning Rate: 0.00033045
	LOSS [training: 3.077855387108942 | validation: 1.379865367387088]
	TIME [epoch: 8.61 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0544310936632204		[learning rate: 0.00032925]
	Learning Rate: 0.00032925
	LOSS [training: 3.0544310936632204 | validation: 1.3815310969442838]
	TIME [epoch: 8.61 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0665199782202532		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 3.0665199782202532 | validation: 1.38306287511106]
	TIME [epoch: 8.62 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0572844480770485		[learning rate: 0.00032686]
	Learning Rate: 0.000326865
	LOSS [training: 3.0572844480770485 | validation: 1.387310164137951]
	TIME [epoch: 8.63 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.058262280584608		[learning rate: 0.00032568]
	Learning Rate: 0.000325679
	LOSS [training: 3.058262280584608 | validation: 1.3883615309417057]
	TIME [epoch: 8.61 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.056227802722		[learning rate: 0.0003245]
	Learning Rate: 0.000324497
	LOSS [training: 3.056227802722 | validation: 1.3808596642672606]
	TIME [epoch: 8.6 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0678518560742654		[learning rate: 0.00032332]
	Learning Rate: 0.000323319
	LOSS [training: 3.0678518560742654 | validation: 1.3835696816035024]
	TIME [epoch: 8.62 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.061099133275217		[learning rate: 0.00032215]
	Learning Rate: 0.000322146
	LOSS [training: 3.061099133275217 | validation: 1.3922531971808707]
	TIME [epoch: 8.62 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0556964131617		[learning rate: 0.00032098]
	Learning Rate: 0.000320977
	LOSS [training: 3.0556964131617 | validation: 1.3772361261920134]
	TIME [epoch: 8.61 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.064773873769669		[learning rate: 0.00031981]
	Learning Rate: 0.000319812
	LOSS [training: 3.064773873769669 | validation: 1.3915619627953895]
	TIME [epoch: 8.6 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0529356942950225		[learning rate: 0.00031865]
	Learning Rate: 0.000318651
	LOSS [training: 3.0529356942950225 | validation: 1.3821958120954045]
	TIME [epoch: 8.62 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0583020605326494		[learning rate: 0.0003175]
	Learning Rate: 0.000317495
	LOSS [training: 3.0583020605326494 | validation: 1.3739304779607082]
	TIME [epoch: 8.61 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0617202515645774		[learning rate: 0.00031634]
	Learning Rate: 0.000316343
	LOSS [training: 3.0617202515645774 | validation: 1.4004600129836213]
	TIME [epoch: 8.6 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0740475204936457		[learning rate: 0.00031519]
	Learning Rate: 0.000315195
	LOSS [training: 3.0740475204936457 | validation: 1.4212125560011113]
	TIME [epoch: 8.6 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.067699023547235		[learning rate: 0.00031405]
	Learning Rate: 0.000314051
	LOSS [training: 3.067699023547235 | validation: 1.3859928795959502]
	TIME [epoch: 8.62 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.052820564241894		[learning rate: 0.00031291]
	Learning Rate: 0.000312911
	LOSS [training: 3.052820564241894 | validation: 1.3763151607703166]
	TIME [epoch: 8.61 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.057753937931397		[learning rate: 0.00031178]
	Learning Rate: 0.000311776
	LOSS [training: 3.057753937931397 | validation: 1.3757337461971961]
	TIME [epoch: 8.61 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0537452786466637		[learning rate: 0.00031064]
	Learning Rate: 0.000310644
	LOSS [training: 3.0537452786466637 | validation: 1.429776890574419]
	TIME [epoch: 8.61 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.065397061587608		[learning rate: 0.00030952]
	Learning Rate: 0.000309517
	LOSS [training: 3.065397061587608 | validation: 1.3827454958210408]
	TIME [epoch: 8.62 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.059571577940391		[learning rate: 0.00030839]
	Learning Rate: 0.000308394
	LOSS [training: 3.059571577940391 | validation: 1.3889761119326]
	TIME [epoch: 8.61 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.057453627161967		[learning rate: 0.00030727]
	Learning Rate: 0.000307274
	LOSS [training: 3.057453627161967 | validation: 1.4051261712364504]
	TIME [epoch: 8.61 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0758114556295637		[learning rate: 0.00030616]
	Learning Rate: 0.000306159
	LOSS [training: 3.0758114556295637 | validation: 1.3766693548440716]
	TIME [epoch: 8.6 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.064813947183234		[learning rate: 0.00030505]
	Learning Rate: 0.000305048
	LOSS [training: 3.064813947183234 | validation: 1.4045721978703276]
	TIME [epoch: 8.63 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0548864595834737		[learning rate: 0.00030394]
	Learning Rate: 0.000303941
	LOSS [training: 3.0548864595834737 | validation: 1.373678073639788]
	TIME [epoch: 8.61 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0659491555285934		[learning rate: 0.00030284]
	Learning Rate: 0.000302838
	LOSS [training: 3.0659491555285934 | validation: 1.3715452625694382]
	TIME [epoch: 8.61 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0561201817259223		[learning rate: 0.00030174]
	Learning Rate: 0.000301739
	LOSS [training: 3.0561201817259223 | validation: 1.3578709267454125]
	TIME [epoch: 8.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_1063.pth
	Model improved!!!
EPOCH 1064/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0518285955446496		[learning rate: 0.00030064]
	Learning Rate: 0.000300644
	LOSS [training: 3.0518285955446496 | validation: 1.3696603054881162]
	TIME [epoch: 8.62 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0716557434951737		[learning rate: 0.00029955]
	Learning Rate: 0.000299553
	LOSS [training: 3.0716557434951737 | validation: 1.4330522099133782]
	TIME [epoch: 8.6 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0535557194234917		[learning rate: 0.00029847]
	Learning Rate: 0.000298466
	LOSS [training: 3.0535557194234917 | validation: 1.3751391409610159]
	TIME [epoch: 8.6 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.055159026088787		[learning rate: 0.00029738]
	Learning Rate: 0.000297383
	LOSS [training: 3.055159026088787 | validation: 1.3919867993205515]
	TIME [epoch: 8.61 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.057818559437165		[learning rate: 0.0002963]
	Learning Rate: 0.000296304
	LOSS [training: 3.057818559437165 | validation: 1.4048519692714236]
	TIME [epoch: 8.61 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.050079014697533		[learning rate: 0.00029523]
	Learning Rate: 0.000295228
	LOSS [training: 3.050079014697533 | validation: 1.3635716758451712]
	TIME [epoch: 8.6 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0581764548860937		[learning rate: 0.00029416]
	Learning Rate: 0.000294157
	LOSS [training: 3.0581764548860937 | validation: 1.3725464358591273]
	TIME [epoch: 8.62 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0521375281758516		[learning rate: 0.00029309]
	Learning Rate: 0.000293089
	LOSS [training: 3.0521375281758516 | validation: 1.400997279979172]
	TIME [epoch: 8.61 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0633481090929107		[learning rate: 0.00029203]
	Learning Rate: 0.000292026
	LOSS [training: 3.0633481090929107 | validation: 1.3726559700370837]
	TIME [epoch: 8.61 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0543047091049735		[learning rate: 0.00029097]
	Learning Rate: 0.000290966
	LOSS [training: 3.0543047091049735 | validation: 1.370502978018113]
	TIME [epoch: 8.61 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0576424415210175		[learning rate: 0.00028991]
	Learning Rate: 0.00028991
	LOSS [training: 3.0576424415210175 | validation: 1.3686604422164401]
	TIME [epoch: 8.61 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0532779710306		[learning rate: 0.00028886]
	Learning Rate: 0.000288858
	LOSS [training: 3.0532779710306 | validation: 1.396496724252117]
	TIME [epoch: 8.62 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0617814503510123		[learning rate: 0.00028781]
	Learning Rate: 0.00028781
	LOSS [training: 3.0617814503510123 | validation: 1.3840367268737652]
	TIME [epoch: 8.6 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.053813899680401		[learning rate: 0.00028677]
	Learning Rate: 0.000286765
	LOSS [training: 3.053813899680401 | validation: 1.3783403250528612]
	TIME [epoch: 8.6 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0639989232382847		[learning rate: 0.00028572]
	Learning Rate: 0.000285724
	LOSS [training: 3.0639989232382847 | validation: 1.3750730709209855]
	TIME [epoch: 8.61 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0608727489217094		[learning rate: 0.00028469]
	Learning Rate: 0.000284688
	LOSS [training: 3.0608727489217094 | validation: 1.392760886743858]
	TIME [epoch: 8.64 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0627517436839806		[learning rate: 0.00028365]
	Learning Rate: 0.000283654
	LOSS [training: 3.0627517436839806 | validation: 1.3824202089380002]
	TIME [epoch: 8.61 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.057567532539138		[learning rate: 0.00028263]
	Learning Rate: 0.000282625
	LOSS [training: 3.057567532539138 | validation: 1.364619265385208]
	TIME [epoch: 8.61 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0682225269526078		[learning rate: 0.0002816]
	Learning Rate: 0.000281599
	LOSS [training: 3.0682225269526078 | validation: 1.384565537913678]
	TIME [epoch: 8.61 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.049504986982803		[learning rate: 0.00028058]
	Learning Rate: 0.000280577
	LOSS [training: 3.049504986982803 | validation: 1.3819669515012734]
	TIME [epoch: 8.63 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.048934024211307		[learning rate: 0.00027956]
	Learning Rate: 0.000279559
	LOSS [training: 3.048934024211307 | validation: 1.367864592207177]
	TIME [epoch: 8.61 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0572223518331723		[learning rate: 0.00027854]
	Learning Rate: 0.000278545
	LOSS [training: 3.0572223518331723 | validation: 1.377401557889029]
	TIME [epoch: 8.6 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.056540504221215		[learning rate: 0.00027753]
	Learning Rate: 0.000277534
	LOSS [training: 3.056540504221215 | validation: 1.3686458662644978]
	TIME [epoch: 8.59 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0531476063895333		[learning rate: 0.00027653]
	Learning Rate: 0.000276527
	LOSS [training: 3.0531476063895333 | validation: 1.3680086271208352]
	TIME [epoch: 8.62 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.052572453825065		[learning rate: 0.00027552]
	Learning Rate: 0.000275523
	LOSS [training: 3.052572453825065 | validation: 1.376516905232403]
	TIME [epoch: 8.59 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0509966380995235		[learning rate: 0.00027452]
	Learning Rate: 0.000274523
	LOSS [training: 3.0509966380995235 | validation: 1.3704984989459414]
	TIME [epoch: 8.61 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0504630797063714		[learning rate: 0.00027353]
	Learning Rate: 0.000273527
	LOSS [training: 3.0504630797063714 | validation: 1.3699845593734818]
	TIME [epoch: 8.62 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0548516487483353		[learning rate: 0.00027253]
	Learning Rate: 0.000272534
	LOSS [training: 3.0548516487483353 | validation: 1.3835413253784474]
	TIME [epoch: 8.62 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.057643014625551		[learning rate: 0.00027155]
	Learning Rate: 0.000271545
	LOSS [training: 3.057643014625551 | validation: 1.4681055194137311]
	TIME [epoch: 8.61 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0760041936848177		[learning rate: 0.00027056]
	Learning Rate: 0.00027056
	LOSS [training: 3.0760041936848177 | validation: 1.3676287692057845]
	TIME [epoch: 8.61 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0564893552523706		[learning rate: 0.00026958]
	Learning Rate: 0.000269578
	LOSS [training: 3.0564893552523706 | validation: 1.4227147003643024]
	TIME [epoch: 8.61 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0698703450356444		[learning rate: 0.0002686]
	Learning Rate: 0.0002686
	LOSS [training: 3.0698703450356444 | validation: 1.4242718446560818]
	TIME [epoch: 8.62 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0570709632166078		[learning rate: 0.00026762]
	Learning Rate: 0.000267625
	LOSS [training: 3.0570709632166078 | validation: 1.379365272743337]
	TIME [epoch: 8.61 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0493920015918974		[learning rate: 0.00026665]
	Learning Rate: 0.000266654
	LOSS [training: 3.0493920015918974 | validation: 1.4170526802708043]
	TIME [epoch: 8.61 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0548721395703002		[learning rate: 0.00026569]
	Learning Rate: 0.000265686
	LOSS [training: 3.0548721395703002 | validation: 1.4134011011839225]
	TIME [epoch: 8.62 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.061155422753631		[learning rate: 0.00026472]
	Learning Rate: 0.000264722
	LOSS [training: 3.061155422753631 | validation: 1.3879502206473138]
	TIME [epoch: 8.61 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.055274402710123		[learning rate: 0.00026376]
	Learning Rate: 0.000263761
	LOSS [training: 3.055274402710123 | validation: 1.3723785959402988]
	TIME [epoch: 8.6 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0483459378762596		[learning rate: 0.0002628]
	Learning Rate: 0.000262804
	LOSS [training: 3.0483459378762596 | validation: 1.4052816582532281]
	TIME [epoch: 8.61 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.063191491622333		[learning rate: 0.00026185]
	Learning Rate: 0.00026185
	LOSS [training: 3.063191491622333 | validation: 1.3686020054230044]
	TIME [epoch: 8.63 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.063334929919702		[learning rate: 0.0002609]
	Learning Rate: 0.0002609
	LOSS [training: 3.063334929919702 | validation: 1.3823068440384603]
	TIME [epoch: 8.61 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0519279420294056		[learning rate: 0.00025995]
	Learning Rate: 0.000259953
	LOSS [training: 3.0519279420294056 | validation: 1.384675282586997]
	TIME [epoch: 8.6 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0699885437539933		[learning rate: 0.00025901]
	Learning Rate: 0.00025901
	LOSS [training: 3.0699885437539933 | validation: 1.4246757587116217]
	TIME [epoch: 8.61 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.063560510538954		[learning rate: 0.00025807]
	Learning Rate: 0.00025807
	LOSS [training: 3.063560510538954 | validation: 1.3592119549434796]
	TIME [epoch: 8.63 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0535198689942824		[learning rate: 0.00025713]
	Learning Rate: 0.000257133
	LOSS [training: 3.0535198689942824 | validation: 1.3724661765539046]
	TIME [epoch: 8.61 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0528448893993114		[learning rate: 0.0002562]
	Learning Rate: 0.0002562
	LOSS [training: 3.0528448893993114 | validation: 1.372215724098626]
	TIME [epoch: 8.6 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0587488860775327		[learning rate: 0.00025527]
	Learning Rate: 0.00025527
	LOSS [training: 3.0587488860775327 | validation: 1.3745167391619237]
	TIME [epoch: 8.6 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.049946275307728		[learning rate: 0.00025434]
	Learning Rate: 0.000254344
	LOSS [training: 3.049946275307728 | validation: 1.388063814833717]
	TIME [epoch: 8.63 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0540598414774367		[learning rate: 0.00025342]
	Learning Rate: 0.000253421
	LOSS [training: 3.0540598414774367 | validation: 1.3798544529954133]
	TIME [epoch: 8.6 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.06874091842755		[learning rate: 0.0002525]
	Learning Rate: 0.000252501
	LOSS [training: 3.06874091842755 | validation: 1.3735741115437725]
	TIME [epoch: 8.61 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0521546375983104		[learning rate: 0.00025158]
	Learning Rate: 0.000251585
	LOSS [training: 3.0521546375983104 | validation: 1.3838600066937063]
	TIME [epoch: 8.61 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0498718709585946		[learning rate: 0.00025067]
	Learning Rate: 0.000250672
	LOSS [training: 3.0498718709585946 | validation: 1.3929028664181988]
	TIME [epoch: 8.62 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0651198406408096		[learning rate: 0.00024976]
	Learning Rate: 0.000249762
	LOSS [training: 3.0651198406408096 | validation: 1.3737242080926884]
	TIME [epoch: 8.6 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04942461410839		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 3.04942461410839 | validation: 1.374911165896246]
	TIME [epoch: 8.6 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0501416207098653		[learning rate: 0.00024795]
	Learning Rate: 0.000247952
	LOSS [training: 3.0501416207098653 | validation: 1.400438475573873]
	TIME [epoch: 8.61 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.068700498940403		[learning rate: 0.00024705]
	Learning Rate: 0.000247053
	LOSS [training: 3.068700498940403 | validation: 1.4411581176406765]
	TIME [epoch: 8.62 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0541382127918344		[learning rate: 0.00024616]
	Learning Rate: 0.000246156
	LOSS [training: 3.0541382127918344 | validation: 1.3872053116984255]
	TIME [epoch: 8.61 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.051350757360711		[learning rate: 0.00024526]
	Learning Rate: 0.000245263
	LOSS [training: 3.051350757360711 | validation: 1.367380725145309]
	TIME [epoch: 8.61 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.053400316155791		[learning rate: 0.00024437]
	Learning Rate: 0.000244373
	LOSS [training: 3.053400316155791 | validation: 1.3747167607101944]
	TIME [epoch: 8.62 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0509225769258146		[learning rate: 0.00024349]
	Learning Rate: 0.000243486
	LOSS [training: 3.0509225769258146 | validation: 1.3843751911919608]
	TIME [epoch: 8.61 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.058791184738702		[learning rate: 0.0002426]
	Learning Rate: 0.000242602
	LOSS [training: 3.058791184738702 | validation: 1.4023332486477122]
	TIME [epoch: 8.6 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.049576033490226		[learning rate: 0.00024172]
	Learning Rate: 0.000241722
	LOSS [training: 3.049576033490226 | validation: 1.3664981568654617]
	TIME [epoch: 8.6 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.057656607492004		[learning rate: 0.00024084]
	Learning Rate: 0.000240845
	LOSS [training: 3.057656607492004 | validation: 1.3797890875340566]
	TIME [epoch: 8.62 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0500524604790304		[learning rate: 0.00023997]
	Learning Rate: 0.000239971
	LOSS [training: 3.0500524604790304 | validation: 1.3690828753394442]
	TIME [epoch: 8.6 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0508990183427467		[learning rate: 0.0002391]
	Learning Rate: 0.0002391
	LOSS [training: 3.0508990183427467 | validation: 1.4135466438789865]
	TIME [epoch: 8.6 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0501450817074507		[learning rate: 0.00023823]
	Learning Rate: 0.000238232
	LOSS [training: 3.0501450817074507 | validation: 1.3687177264174968]
	TIME [epoch: 8.6 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0536172328087234		[learning rate: 0.00023737]
	Learning Rate: 0.000237367
	LOSS [training: 3.0536172328087234 | validation: 1.386056473592968]
	TIME [epoch: 8.63 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0539110051546725		[learning rate: 0.00023651]
	Learning Rate: 0.000236506
	LOSS [training: 3.0539110051546725 | validation: 1.37553895356969]
	TIME [epoch: 8.61 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0511718534663492		[learning rate: 0.00023565]
	Learning Rate: 0.000235648
	LOSS [training: 3.0511718534663492 | validation: 1.3893337418677174]
	TIME [epoch: 8.6 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0520692821032		[learning rate: 0.00023479]
	Learning Rate: 0.000234793
	LOSS [training: 3.0520692821032 | validation: 1.3755572210320413]
	TIME [epoch: 8.61 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0443501603631944		[learning rate: 0.00023394]
	Learning Rate: 0.00023394
	LOSS [training: 3.0443501603631944 | validation: 1.3830288209426387]
	TIME [epoch: 8.62 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0524349108545		[learning rate: 0.00023309]
	Learning Rate: 0.000233091
	LOSS [training: 3.0524349108545 | validation: 1.3926752330540295]
	TIME [epoch: 8.6 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0528494357885174		[learning rate: 0.00023225]
	Learning Rate: 0.000232246
	LOSS [training: 3.0528494357885174 | validation: 1.3741612537212085]
	TIME [epoch: 8.61 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0488104827146216		[learning rate: 0.0002314]
	Learning Rate: 0.000231403
	LOSS [training: 3.0488104827146216 | validation: 1.3865791090379525]
	TIME [epoch: 8.6 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0566147387441385		[learning rate: 0.00023056]
	Learning Rate: 0.000230563
	LOSS [training: 3.0566147387441385 | validation: 1.369877168563836]
	TIME [epoch: 8.63 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0543811197219224		[learning rate: 0.00022973]
	Learning Rate: 0.000229726
	LOSS [training: 3.0543811197219224 | validation: 1.3863484007489968]
	TIME [epoch: 8.61 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0559877061410643		[learning rate: 0.00022889]
	Learning Rate: 0.000228893
	LOSS [training: 3.0559877061410643 | validation: 1.369099862102642]
	TIME [epoch: 8.6 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.053664770358555		[learning rate: 0.00022806]
	Learning Rate: 0.000228062
	LOSS [training: 3.053664770358555 | validation: 1.4074644017113107]
	TIME [epoch: 8.62 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.055233725447041		[learning rate: 0.00022723]
	Learning Rate: 0.000227234
	LOSS [training: 3.055233725447041 | validation: 1.3605087993330658]
	TIME [epoch: 8.61 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0487831396420395		[learning rate: 0.00022641]
	Learning Rate: 0.00022641
	LOSS [training: 3.0487831396420395 | validation: 1.375023158350799]
	TIME [epoch: 8.61 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0499689207644054		[learning rate: 0.00022559]
	Learning Rate: 0.000225588
	LOSS [training: 3.0499689207644054 | validation: 1.37070141351206]
	TIME [epoch: 8.6 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0546052970238398		[learning rate: 0.00022477]
	Learning Rate: 0.000224769
	LOSS [training: 3.0546052970238398 | validation: 1.366009277732128]
	TIME [epoch: 8.62 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0453136758711423		[learning rate: 0.00022395]
	Learning Rate: 0.000223954
	LOSS [training: 3.0453136758711423 | validation: 1.3636406591255135]
	TIME [epoch: 8.62 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.048656447850562		[learning rate: 0.00022314]
	Learning Rate: 0.000223141
	LOSS [training: 3.048656447850562 | validation: 1.3805705910208717]
	TIME [epoch: 8.62 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.057835611309888		[learning rate: 0.00022233]
	Learning Rate: 0.000222331
	LOSS [training: 3.057835611309888 | validation: 1.3743328303922775]
	TIME [epoch: 8.61 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.049651107043656		[learning rate: 0.00022152]
	Learning Rate: 0.000221524
	LOSS [training: 3.049651107043656 | validation: 1.3800265679851982]
	TIME [epoch: 8.62 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.05237990042358		[learning rate: 0.00022072]
	Learning Rate: 0.00022072
	LOSS [training: 3.05237990042358 | validation: 1.3800790647411438]
	TIME [epoch: 8.62 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.054437107239483		[learning rate: 0.00021992]
	Learning Rate: 0.000219919
	LOSS [training: 3.054437107239483 | validation: 1.3781948253378853]
	TIME [epoch: 8.61 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.05448539872464		[learning rate: 0.00021912]
	Learning Rate: 0.000219121
	LOSS [training: 3.05448539872464 | validation: 1.3811670965195437]
	TIME [epoch: 8.61 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.065374738150129		[learning rate: 0.00021833]
	Learning Rate: 0.000218326
	LOSS [training: 3.065374738150129 | validation: 1.4270121412494154]
	TIME [epoch: 8.63 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0724580285495535		[learning rate: 0.00021753]
	Learning Rate: 0.000217534
	LOSS [training: 3.0724580285495535 | validation: 1.3927793543558418]
	TIME [epoch: 8.61 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0516782615117557		[learning rate: 0.00021674]
	Learning Rate: 0.000216744
	LOSS [training: 3.0516782615117557 | validation: 1.362556666236612]
	TIME [epoch: 8.61 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0554668756837544		[learning rate: 0.00021596]
	Learning Rate: 0.000215958
	LOSS [training: 3.0554668756837544 | validation: 1.3897010534752774]
	TIME [epoch: 8.6 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.057834489446254		[learning rate: 0.00021517]
	Learning Rate: 0.000215174
	LOSS [training: 3.057834489446254 | validation: 1.3747616384255106]
	TIME [epoch: 8.63 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0535451077498506		[learning rate: 0.00021439]
	Learning Rate: 0.000214393
	LOSS [training: 3.0535451077498506 | validation: 1.3691378954947822]
	TIME [epoch: 8.61 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.045665763966862		[learning rate: 0.00021361]
	Learning Rate: 0.000213615
	LOSS [training: 3.045665763966862 | validation: 1.3716048811232344]
	TIME [epoch: 8.6 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0603845569971484		[learning rate: 0.00021284]
	Learning Rate: 0.00021284
	LOSS [training: 3.0603845569971484 | validation: 1.3801714805059289]
	TIME [epoch: 8.61 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0536775871288566		[learning rate: 0.00021207]
	Learning Rate: 0.000212067
	LOSS [training: 3.0536775871288566 | validation: 1.3726801753155824]
	TIME [epoch: 8.63 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0543296389142687		[learning rate: 0.0002113]
	Learning Rate: 0.000211298
	LOSS [training: 3.0543296389142687 | validation: 1.377384249910276]
	TIME [epoch: 8.61 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0525304707606002		[learning rate: 0.00021053]
	Learning Rate: 0.000210531
	LOSS [training: 3.0525304707606002 | validation: 1.388893421438897]
	TIME [epoch: 8.61 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0513011013883533		[learning rate: 0.00020977]
	Learning Rate: 0.000209767
	LOSS [training: 3.0513011013883533 | validation: 1.37440829214013]
	TIME [epoch: 8.61 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.055050855786363		[learning rate: 0.00020901]
	Learning Rate: 0.000209006
	LOSS [training: 3.055050855786363 | validation: 1.371558670779883]
	TIME [epoch: 8.62 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0480634562650257		[learning rate: 0.00020825]
	Learning Rate: 0.000208247
	LOSS [training: 3.0480634562650257 | validation: 1.3817899492589716]
	TIME [epoch: 8.61 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.066242675788166		[learning rate: 0.00020749]
	Learning Rate: 0.000207491
	LOSS [training: 3.066242675788166 | validation: 1.3703519096272772]
	TIME [epoch: 8.61 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0449339740682992		[learning rate: 0.00020674]
	Learning Rate: 0.000206738
	LOSS [training: 3.0449339740682992 | validation: 1.367508198907851]
	TIME [epoch: 8.61 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0521408359695785		[learning rate: 0.00020599]
	Learning Rate: 0.000205988
	LOSS [training: 3.0521408359695785 | validation: 1.377662180444426]
	TIME [epoch: 8.62 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.056976794606132		[learning rate: 0.00020524]
	Learning Rate: 0.000205241
	LOSS [training: 3.056976794606132 | validation: 1.3711748097527592]
	TIME [epoch: 8.6 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0512859258922704		[learning rate: 0.0002045]
	Learning Rate: 0.000204496
	LOSS [training: 3.0512859258922704 | validation: 1.3858535174642068]
	TIME [epoch: 8.61 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0527572063784953		[learning rate: 0.00020375]
	Learning Rate: 0.000203754
	LOSS [training: 3.0527572063784953 | validation: 1.3798859697295085]
	TIME [epoch: 8.62 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0570351342861732		[learning rate: 0.00020301]
	Learning Rate: 0.000203014
	LOSS [training: 3.0570351342861732 | validation: 1.4084846847904542]
	TIME [epoch: 8.62 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0528837795821278		[learning rate: 0.00020228]
	Learning Rate: 0.000202277
	LOSS [training: 3.0528837795821278 | validation: 1.394400777265649]
	TIME [epoch: 8.6 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0436003623499692		[learning rate: 0.00020154]
	Learning Rate: 0.000201543
	LOSS [training: 3.0436003623499692 | validation: 1.3669114480064395]
	TIME [epoch: 8.62 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.048314045383848		[learning rate: 0.00020081]
	Learning Rate: 0.000200812
	LOSS [training: 3.048314045383848 | validation: 1.3742429023222935]
	TIME [epoch: 8.63 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043996521510297		[learning rate: 0.00020008]
	Learning Rate: 0.000200083
	LOSS [training: 3.043996521510297 | validation: 1.3796798959456924]
	TIME [epoch: 8.61 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.051447010256092		[learning rate: 0.00019936]
	Learning Rate: 0.000199357
	LOSS [training: 3.051447010256092 | validation: 1.371833169366127]
	TIME [epoch: 8.61 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0507934288483716		[learning rate: 0.00019863]
	Learning Rate: 0.000198634
	LOSS [training: 3.0507934288483716 | validation: 1.3975098172596054]
	TIME [epoch: 8.61 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0588324545364776		[learning rate: 0.00019791]
	Learning Rate: 0.000197913
	LOSS [training: 3.0588324545364776 | validation: 1.382193604367972]
	TIME [epoch: 8.63 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.052641110208373		[learning rate: 0.00019719]
	Learning Rate: 0.000197194
	LOSS [training: 3.052641110208373 | validation: 1.3842527410263714]
	TIME [epoch: 8.62 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.046580751742032		[learning rate: 0.00019648]
	Learning Rate: 0.000196479
	LOSS [training: 3.046580751742032 | validation: 1.3748066414867508]
	TIME [epoch: 8.61 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.051808132089648		[learning rate: 0.00019577]
	Learning Rate: 0.000195766
	LOSS [training: 3.051808132089648 | validation: 1.3713378067611273]
	TIME [epoch: 8.61 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.049107202501746		[learning rate: 0.00019506]
	Learning Rate: 0.000195055
	LOSS [training: 3.049107202501746 | validation: 1.3658701287066373]
	TIME [epoch: 8.63 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.047590998098597		[learning rate: 0.00019435]
	Learning Rate: 0.000194347
	LOSS [training: 3.047590998098597 | validation: 1.3765919180425636]
	TIME [epoch: 8.61 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.046476246246087		[learning rate: 0.00019364]
	Learning Rate: 0.000193642
	LOSS [training: 3.046476246246087 | validation: 1.3835558145459947]
	TIME [epoch: 8.61 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.051374292992453		[learning rate: 0.00019294]
	Learning Rate: 0.000192939
	LOSS [training: 3.051374292992453 | validation: 1.383587608494711]
	TIME [epoch: 8.61 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.063564094457226		[learning rate: 0.00019224]
	Learning Rate: 0.000192239
	LOSS [training: 3.063564094457226 | validation: 1.3791376923903145]
	TIME [epoch: 8.63 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.052506076600259		[learning rate: 0.00019154]
	Learning Rate: 0.000191542
	LOSS [training: 3.052506076600259 | validation: 1.3705649314243022]
	TIME [epoch: 8.61 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0467874489973514		[learning rate: 0.00019085]
	Learning Rate: 0.000190846
	LOSS [training: 3.0467874489973514 | validation: 1.3779752433529286]
	TIME [epoch: 8.63 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0484872694801455		[learning rate: 0.00019015]
	Learning Rate: 0.000190154
	LOSS [training: 3.0484872694801455 | validation: 1.3736815827965947]
	TIME [epoch: 8.61 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.045670670041006		[learning rate: 0.00018946]
	Learning Rate: 0.000189464
	LOSS [training: 3.045670670041006 | validation: 1.3630259310847295]
	TIME [epoch: 8.62 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.049932889358495		[learning rate: 0.00018878]
	Learning Rate: 0.000188776
	LOSS [training: 3.049932889358495 | validation: 1.3866245734733358]
	TIME [epoch: 8.61 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.052280105708435		[learning rate: 0.00018809]
	Learning Rate: 0.000188091
	LOSS [training: 3.052280105708435 | validation: 1.381611192493677]
	TIME [epoch: 8.61 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.047443916299328		[learning rate: 0.00018741]
	Learning Rate: 0.000187409
	LOSS [training: 3.047443916299328 | validation: 1.3746868571973796]
	TIME [epoch: 8.62 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.050195982237667		[learning rate: 0.00018673]
	Learning Rate: 0.000186729
	LOSS [training: 3.050195982237667 | validation: 1.3661995730455114]
	TIME [epoch: 8.62 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.048882498250733		[learning rate: 0.00018605]
	Learning Rate: 0.000186051
	LOSS [training: 3.048882498250733 | validation: 1.3754733887442037]
	TIME [epoch: 8.61 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0486821199014864		[learning rate: 0.00018538]
	Learning Rate: 0.000185376
	LOSS [training: 3.0486821199014864 | validation: 1.3685719979290554]
	TIME [epoch: 8.61 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0472748094103848		[learning rate: 0.0001847]
	Learning Rate: 0.000184703
	LOSS [training: 3.0472748094103848 | validation: 1.3685519081222348]
	TIME [epoch: 8.63 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0575287838141247		[learning rate: 0.00018403]
	Learning Rate: 0.000184033
	LOSS [training: 3.0575287838141247 | validation: 1.3679260176465717]
	TIME [epoch: 8.61 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0436988916921384		[learning rate: 0.00018336]
	Learning Rate: 0.000183365
	LOSS [training: 3.0436988916921384 | validation: 1.3808703293816706]
	TIME [epoch: 8.61 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0450903713250885		[learning rate: 0.0001827]
	Learning Rate: 0.000182699
	LOSS [training: 3.0450903713250885 | validation: 1.3732658618217428]
	TIME [epoch: 8.6 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.052476763891989		[learning rate: 0.00018204]
	Learning Rate: 0.000182036
	LOSS [training: 3.052476763891989 | validation: 1.38347457520244]
	TIME [epoch: 8.62 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0516353707241626		[learning rate: 0.00018138]
	Learning Rate: 0.000181376
	LOSS [training: 3.0516353707241626 | validation: 1.3880981145160676]
	TIME [epoch: 8.61 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.047228290542536		[learning rate: 0.00018072]
	Learning Rate: 0.000180717
	LOSS [training: 3.047228290542536 | validation: 1.3866860465761677]
	TIME [epoch: 8.6 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0464383082262447		[learning rate: 0.00018006]
	Learning Rate: 0.000180062
	LOSS [training: 3.0464383082262447 | validation: 1.3600221632965264]
	TIME [epoch: 8.6 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.044132116957729		[learning rate: 0.00017941]
	Learning Rate: 0.000179408
	LOSS [training: 3.044132116957729 | validation: 1.360541592409139]
	TIME [epoch: 8.63 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.046951169616448		[learning rate: 0.00017876]
	Learning Rate: 0.000178757
	LOSS [training: 3.046951169616448 | validation: 1.374641280121049]
	TIME [epoch: 8.6 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0468579684847663		[learning rate: 0.00017811]
	Learning Rate: 0.000178108
	LOSS [training: 3.0468579684847663 | validation: 1.386700601694836]
	TIME [epoch: 8.61 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0497230562291966		[learning rate: 0.00017746]
	Learning Rate: 0.000177462
	LOSS [training: 3.0497230562291966 | validation: 1.4094904962785675]
	TIME [epoch: 8.61 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0509357655593314		[learning rate: 0.00017682]
	Learning Rate: 0.000176818
	LOSS [training: 3.0509357655593314 | validation: 1.3846254282240917]
	TIME [epoch: 8.62 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0484750335747717		[learning rate: 0.00017618]
	Learning Rate: 0.000176176
	LOSS [training: 3.0484750335747717 | validation: 1.3596571850289751]
	TIME [epoch: 8.6 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.053571334305939		[learning rate: 0.00017554]
	Learning Rate: 0.000175537
	LOSS [training: 3.053571334305939 | validation: 1.3789523632775735]
	TIME [epoch: 8.61 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0436358497832434		[learning rate: 0.0001749]
	Learning Rate: 0.0001749
	LOSS [training: 3.0436358497832434 | validation: 1.3714466748965428]
	TIME [epoch: 8.6 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.048576434507742		[learning rate: 0.00017427]
	Learning Rate: 0.000174265
	LOSS [training: 3.048576434507742 | validation: 1.362518662107394]
	TIME [epoch: 8.61 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0438503670113777		[learning rate: 0.00017363]
	Learning Rate: 0.000173633
	LOSS [training: 3.0438503670113777 | validation: 1.4142260528747987]
	TIME [epoch: 8.61 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0506626935513403		[learning rate: 0.000173]
	Learning Rate: 0.000173003
	LOSS [training: 3.0506626935513403 | validation: 1.3840178864122692]
	TIME [epoch: 8.61 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0481046214415715		[learning rate: 0.00017237]
	Learning Rate: 0.000172375
	LOSS [training: 3.0481046214415715 | validation: 1.3692077109867367]
	TIME [epoch: 8.61 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043629052459169		[learning rate: 0.00017175]
	Learning Rate: 0.000171749
	LOSS [training: 3.043629052459169 | validation: 1.3695956321833302]
	TIME [epoch: 8.61 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.050151541962186		[learning rate: 0.00017113]
	Learning Rate: 0.000171126
	LOSS [training: 3.050151541962186 | validation: 1.377744113394263]
	TIME [epoch: 8.61 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.046619867040755		[learning rate: 0.0001705]
	Learning Rate: 0.000170505
	LOSS [training: 3.046619867040755 | validation: 1.4058986164927338]
	TIME [epoch: 8.61 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.051755163856178		[learning rate: 0.00016989]
	Learning Rate: 0.000169886
	LOSS [training: 3.051755163856178 | validation: 1.3711668413423654]
	TIME [epoch: 8.62 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0548168618195124		[learning rate: 0.00016927]
	Learning Rate: 0.00016927
	LOSS [training: 3.0548168618195124 | validation: 1.4095761117271934]
	TIME [epoch: 8.61 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.050868867632563		[learning rate: 0.00016866]
	Learning Rate: 0.000168655
	LOSS [training: 3.050868867632563 | validation: 1.3730760344332475]
	TIME [epoch: 8.61 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.055977255303797		[learning rate: 0.00016804]
	Learning Rate: 0.000168043
	LOSS [training: 3.055977255303797 | validation: 1.3717340334493686]
	TIME [epoch: 8.61 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04471218896511		[learning rate: 0.00016743]
	Learning Rate: 0.000167433
	LOSS [training: 3.04471218896511 | validation: 1.3534212960709406]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_1225.pth
	Model improved!!!
EPOCH 1226/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.049184245764214		[learning rate: 0.00016683]
	Learning Rate: 0.000166826
	LOSS [training: 3.049184245764214 | validation: 1.3808933588944008]
	TIME [epoch: 8.61 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.048217294476448		[learning rate: 0.00016622]
	Learning Rate: 0.00016622
	LOSS [training: 3.048217294476448 | validation: 1.3596533256478902]
	TIME [epoch: 8.62 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.052251681778174		[learning rate: 0.00016562]
	Learning Rate: 0.000165617
	LOSS [training: 3.052251681778174 | validation: 1.412239915935161]
	TIME [epoch: 8.61 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.052560177567268		[learning rate: 0.00016502]
	Learning Rate: 0.000165016
	LOSS [training: 3.052560177567268 | validation: 1.3791996401793305]
	TIME [epoch: 8.64 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04952656928212		[learning rate: 0.00016442]
	Learning Rate: 0.000164417
	LOSS [training: 3.04952656928212 | validation: 1.4075409368277938]
	TIME [epoch: 8.62 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0445490141573393		[learning rate: 0.00016382]
	Learning Rate: 0.000163821
	LOSS [training: 3.0445490141573393 | validation: 1.3820214179401196]
	TIME [epoch: 8.61 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0490306279822668		[learning rate: 0.00016323]
	Learning Rate: 0.000163226
	LOSS [training: 3.0490306279822668 | validation: 1.3651866070689123]
	TIME [epoch: 8.61 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0513079843853044		[learning rate: 0.00016263]
	Learning Rate: 0.000162634
	LOSS [training: 3.0513079843853044 | validation: 1.3809696560328009]
	TIME [epoch: 8.63 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.048111819310261		[learning rate: 0.00016204]
	Learning Rate: 0.000162043
	LOSS [training: 3.048111819310261 | validation: 1.3745109362023975]
	TIME [epoch: 8.62 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043466116456943		[learning rate: 0.00016146]
	Learning Rate: 0.000161455
	LOSS [training: 3.043466116456943 | validation: 1.375759040971094]
	TIME [epoch: 8.61 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039293538671484		[learning rate: 0.00016087]
	Learning Rate: 0.000160869
	LOSS [training: 3.039293538671484 | validation: 1.3749780331980315]
	TIME [epoch: 8.61 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.054502229625963		[learning rate: 0.00016029]
	Learning Rate: 0.000160286
	LOSS [training: 3.054502229625963 | validation: 1.3642366162698636]
	TIME [epoch: 8.62 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0465495961239024		[learning rate: 0.0001597]
	Learning Rate: 0.000159704
	LOSS [training: 3.0465495961239024 | validation: 1.372598055780791]
	TIME [epoch: 8.61 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0516974626774336		[learning rate: 0.00015912]
	Learning Rate: 0.000159124
	LOSS [training: 3.0516974626774336 | validation: 1.3732603561305168]
	TIME [epoch: 8.61 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0409070328985868		[learning rate: 0.00015855]
	Learning Rate: 0.000158547
	LOSS [training: 3.0409070328985868 | validation: 1.3845707093356132]
	TIME [epoch: 8.62 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0466972154243215		[learning rate: 0.00015797]
	Learning Rate: 0.000157972
	LOSS [training: 3.0466972154243215 | validation: 1.3684893306855006]
	TIME [epoch: 8.63 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0405104726968295		[learning rate: 0.0001574]
	Learning Rate: 0.000157398
	LOSS [training: 3.0405104726968295 | validation: 1.3795685814774608]
	TIME [epoch: 8.61 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0428015901544017		[learning rate: 0.00015683]
	Learning Rate: 0.000156827
	LOSS [training: 3.0428015901544017 | validation: 1.3746261908503354]
	TIME [epoch: 8.61 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043502831336886		[learning rate: 0.00015626]
	Learning Rate: 0.000156258
	LOSS [training: 3.043502831336886 | validation: 1.3697903259902042]
	TIME [epoch: 8.63 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0508434550870125		[learning rate: 0.00015569]
	Learning Rate: 0.000155691
	LOSS [training: 3.0508434550870125 | validation: 1.387261296012366]
	TIME [epoch: 8.62 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0446163459671216		[learning rate: 0.00015513]
	Learning Rate: 0.000155126
	LOSS [training: 3.0446163459671216 | validation: 1.3687921183354155]
	TIME [epoch: 8.61 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0403116182801395		[learning rate: 0.00015456]
	Learning Rate: 0.000154563
	LOSS [training: 3.0403116182801395 | validation: 1.3682144415343287]
	TIME [epoch: 8.61 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0454983214843634		[learning rate: 0.000154]
	Learning Rate: 0.000154002
	LOSS [training: 3.0454983214843634 | validation: 1.389994142450858]
	TIME [epoch: 8.63 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.052254083773359		[learning rate: 0.00015344]
	Learning Rate: 0.000153443
	LOSS [training: 3.052254083773359 | validation: 1.3829361927733284]
	TIME [epoch: 8.61 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0502141800773512		[learning rate: 0.00015289]
	Learning Rate: 0.000152886
	LOSS [training: 3.0502141800773512 | validation: 1.3725192051895572]
	TIME [epoch: 8.61 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04424941086924		[learning rate: 0.00015233]
	Learning Rate: 0.000152331
	LOSS [training: 3.04424941086924 | validation: 1.372264652948401]
	TIME [epoch: 8.62 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0482482802515536		[learning rate: 0.00015178]
	Learning Rate: 0.000151779
	LOSS [training: 3.0482482802515536 | validation: 1.410675122384129]
	TIME [epoch: 8.63 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04791710195918		[learning rate: 0.00015123]
	Learning Rate: 0.000151228
	LOSS [training: 3.04791710195918 | validation: 1.3693249385813622]
	TIME [epoch: 8.61 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.05244948024177		[learning rate: 0.00015068]
	Learning Rate: 0.000150679
	LOSS [training: 3.05244948024177 | validation: 1.4034710933085481]
	TIME [epoch: 8.6 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.045027952944902		[learning rate: 0.00015013]
	Learning Rate: 0.000150132
	LOSS [training: 3.045027952944902 | validation: 1.362821865668951]
	TIME [epoch: 8.61 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.046071226103886		[learning rate: 0.00014959]
	Learning Rate: 0.000149587
	LOSS [training: 3.046071226103886 | validation: 1.3917645729064885]
	TIME [epoch: 8.63 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0516271372757964		[learning rate: 0.00014904]
	Learning Rate: 0.000149044
	LOSS [training: 3.0516271372757964 | validation: 1.3707286423324516]
	TIME [epoch: 8.61 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.045188646338899		[learning rate: 0.0001485]
	Learning Rate: 0.000148504
	LOSS [training: 3.045188646338899 | validation: 1.360348855985426]
	TIME [epoch: 8.6 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043745007890077		[learning rate: 0.00014796]
	Learning Rate: 0.000147965
	LOSS [training: 3.043745007890077 | validation: 1.3877928676060338]
	TIME [epoch: 8.61 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.048709255971713		[learning rate: 0.00014743]
	Learning Rate: 0.000147428
	LOSS [training: 3.048709255971713 | validation: 1.360546155806246]
	TIME [epoch: 8.62 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0429906411154257		[learning rate: 0.00014689]
	Learning Rate: 0.000146893
	LOSS [training: 3.0429906411154257 | validation: 1.3804448057264582]
	TIME [epoch: 8.61 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.049493063539743		[learning rate: 0.00014636]
	Learning Rate: 0.00014636
	LOSS [training: 3.049493063539743 | validation: 1.3615904691157643]
	TIME [epoch: 8.61 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0517169983733554		[learning rate: 0.00014583]
	Learning Rate: 0.000145828
	LOSS [training: 3.0517169983733554 | validation: 1.371721076957488]
	TIME [epoch: 8.62 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.053769554768472		[learning rate: 0.0001453]
	Learning Rate: 0.000145299
	LOSS [training: 3.053769554768472 | validation: 1.3572089639597984]
	TIME [epoch: 8.62 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.042303926881437		[learning rate: 0.00014477]
	Learning Rate: 0.000144772
	LOSS [training: 3.042303926881437 | validation: 1.3667629291328662]
	TIME [epoch: 8.63 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.047875430844182		[learning rate: 0.00014425]
	Learning Rate: 0.000144247
	LOSS [training: 3.047875430844182 | validation: 1.3690342453444058]
	TIME [epoch: 8.61 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0465925378302634		[learning rate: 0.00014372]
	Learning Rate: 0.000143723
	LOSS [training: 3.0465925378302634 | validation: 1.3630595297077233]
	TIME [epoch: 8.62 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0491166993017003		[learning rate: 0.0001432]
	Learning Rate: 0.000143201
	LOSS [training: 3.0491166993017003 | validation: 1.4057590694799185]
	TIME [epoch: 8.62 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0485898460818452		[learning rate: 0.00014268]
	Learning Rate: 0.000142682
	LOSS [training: 3.0485898460818452 | validation: 1.3672062177715174]
	TIME [epoch: 8.6 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0466741590262485		[learning rate: 0.00014216]
	Learning Rate: 0.000142164
	LOSS [training: 3.0466741590262485 | validation: 1.3645639127222997]
	TIME [epoch: 8.6 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0467120625089033		[learning rate: 0.00014165]
	Learning Rate: 0.000141648
	LOSS [training: 3.0467120625089033 | validation: 1.3767289514828096]
	TIME [epoch: 8.63 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.049321340911093		[learning rate: 0.00014113]
	Learning Rate: 0.000141134
	LOSS [training: 3.049321340911093 | validation: 1.3751183901499597]
	TIME [epoch: 8.61 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0487973310699927		[learning rate: 0.00014062]
	Learning Rate: 0.000140622
	LOSS [training: 3.0487973310699927 | validation: 1.362456253327289]
	TIME [epoch: 8.6 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0464608904969577		[learning rate: 0.00014011]
	Learning Rate: 0.000140112
	LOSS [training: 3.0464608904969577 | validation: 1.3707494666092501]
	TIME [epoch: 8.61 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.050081673168108		[learning rate: 0.0001396]
	Learning Rate: 0.000139603
	LOSS [training: 3.050081673168108 | validation: 1.3700635952811167]
	TIME [epoch: 8.62 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.047732236023408		[learning rate: 0.0001391]
	Learning Rate: 0.000139096
	LOSS [training: 3.047732236023408 | validation: 1.3858965204427585]
	TIME [epoch: 8.61 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.058191336043622		[learning rate: 0.00013859]
	Learning Rate: 0.000138592
	LOSS [training: 3.058191336043622 | validation: 1.3684646993493237]
	TIME [epoch: 8.62 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0390404237250728		[learning rate: 0.00013809]
	Learning Rate: 0.000138089
	LOSS [training: 3.0390404237250728 | validation: 1.3767092009472413]
	TIME [epoch: 8.61 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.044254050727582		[learning rate: 0.00013759]
	Learning Rate: 0.000137587
	LOSS [training: 3.044254050727582 | validation: 1.3697813523022564]
	TIME [epoch: 8.62 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0431976270258074		[learning rate: 0.00013709]
	Learning Rate: 0.000137088
	LOSS [training: 3.0431976270258074 | validation: 1.3772303357181161]
	TIME [epoch: 8.61 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041683878980235		[learning rate: 0.00013659]
	Learning Rate: 0.000136591
	LOSS [training: 3.041683878980235 | validation: 1.3623125601309884]
	TIME [epoch: 8.61 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0532172684060868		[learning rate: 0.00013609]
	Learning Rate: 0.000136095
	LOSS [training: 3.0532172684060868 | validation: 1.3761792836416307]
	TIME [epoch: 8.6 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043101001172441		[learning rate: 0.0001356]
	Learning Rate: 0.000135601
	LOSS [training: 3.043101001172441 | validation: 1.3624519862688478]
	TIME [epoch: 8.63 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0554447805272056		[learning rate: 0.00013511]
	Learning Rate: 0.000135109
	LOSS [training: 3.0554447805272056 | validation: 1.4198819227386732]
	TIME [epoch: 8.6 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.055083854424943		[learning rate: 0.00013462]
	Learning Rate: 0.000134619
	LOSS [training: 3.055083854424943 | validation: 1.3942756747444511]
	TIME [epoch: 8.61 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.044529168646304		[learning rate: 0.00013413]
	Learning Rate: 0.00013413
	LOSS [training: 3.044529168646304 | validation: 1.380612554157997]
	TIME [epoch: 8.61 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0459391966060645		[learning rate: 0.00013364]
	Learning Rate: 0.000133643
	LOSS [training: 3.0459391966060645 | validation: 1.3674418200905296]
	TIME [epoch: 8.62 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0387678049369216		[learning rate: 0.00013316]
	Learning Rate: 0.000133158
	LOSS [training: 3.0387678049369216 | validation: 1.3661991910214009]
	TIME [epoch: 8.6 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.054577319895979		[learning rate: 0.00013268]
	Learning Rate: 0.000132675
	LOSS [training: 3.054577319895979 | validation: 1.4055701742435065]
	TIME [epoch: 8.6 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.051367380531052		[learning rate: 0.00013219]
	Learning Rate: 0.000132194
	LOSS [training: 3.051367380531052 | validation: 1.3865059804406563]
	TIME [epoch: 8.62 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.048092410646976		[learning rate: 0.00013171]
	Learning Rate: 0.000131714
	LOSS [training: 3.048092410646976 | validation: 1.3898712516187683]
	TIME [epoch: 8.61 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0462630254862866		[learning rate: 0.00013124]
	Learning Rate: 0.000131236
	LOSS [training: 3.0462630254862866 | validation: 1.3734429188675072]
	TIME [epoch: 8.61 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0453542888393352		[learning rate: 0.00013076]
	Learning Rate: 0.00013076
	LOSS [training: 3.0453542888393352 | validation: 1.3880539285816746]
	TIME [epoch: 8.6 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.050886748858888		[learning rate: 0.00013029]
	Learning Rate: 0.000130285
	LOSS [training: 3.050886748858888 | validation: 1.3807836832144083]
	TIME [epoch: 8.62 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.047617602005166		[learning rate: 0.00012981]
	Learning Rate: 0.000129812
	LOSS [training: 3.047617602005166 | validation: 1.3952060969882982]
	TIME [epoch: 8.61 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0443187093814688		[learning rate: 0.00012934]
	Learning Rate: 0.000129341
	LOSS [training: 3.0443187093814688 | validation: 1.3795856391372332]
	TIME [epoch: 8.6 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0529393460611685		[learning rate: 0.00012887]
	Learning Rate: 0.000128872
	LOSS [training: 3.0529393460611685 | validation: 1.396521128738721]
	TIME [epoch: 8.6 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.059352506059994		[learning rate: 0.0001284]
	Learning Rate: 0.000128404
	LOSS [training: 3.059352506059994 | validation: 1.3763153997548712]
	TIME [epoch: 8.63 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0435833664415854		[learning rate: 0.00012794]
	Learning Rate: 0.000127938
	LOSS [training: 3.0435833664415854 | validation: 1.3806101723898294]
	TIME [epoch: 8.61 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0438626503636774		[learning rate: 0.00012747]
	Learning Rate: 0.000127474
	LOSS [training: 3.0438626503636774 | validation: 1.3707265682925938]
	TIME [epoch: 8.6 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0459006635343635		[learning rate: 0.00012701]
	Learning Rate: 0.000127011
	LOSS [training: 3.0459006635343635 | validation: 1.3791540033835439]
	TIME [epoch: 8.61 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.042000264925023		[learning rate: 0.00012655]
	Learning Rate: 0.00012655
	LOSS [training: 3.042000264925023 | validation: 1.3795391187551345]
	TIME [epoch: 8.62 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0551339337116197		[learning rate: 0.00012609]
	Learning Rate: 0.000126091
	LOSS [training: 3.0551339337116197 | validation: 1.3708170536160342]
	TIME [epoch: 8.61 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.05165170875878		[learning rate: 0.00012563]
	Learning Rate: 0.000125633
	LOSS [training: 3.05165170875878 | validation: 1.3694295681630815]
	TIME [epoch: 8.61 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0464341044561007		[learning rate: 0.00012518]
	Learning Rate: 0.000125178
	LOSS [training: 3.0464341044561007 | validation: 1.3737212594398378]
	TIME [epoch: 8.61 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.042975094793139		[learning rate: 0.00012472]
	Learning Rate: 0.000124723
	LOSS [training: 3.042975094793139 | validation: 1.3751185453509276]
	TIME [epoch: 8.64 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0442470441331535		[learning rate: 0.00012427]
	Learning Rate: 0.000124271
	LOSS [training: 3.0442470441331535 | validation: 1.362357233774956]
	TIME [epoch: 8.61 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0474153062179203		[learning rate: 0.00012382]
	Learning Rate: 0.00012382
	LOSS [training: 3.0474153062179203 | validation: 1.3679490207015599]
	TIME [epoch: 8.6 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041365632066377		[learning rate: 0.00012337]
	Learning Rate: 0.00012337
	LOSS [training: 3.041365632066377 | validation: 1.3823081508049562]
	TIME [epoch: 8.61 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0424116041526537		[learning rate: 0.00012292]
	Learning Rate: 0.000122923
	LOSS [training: 3.0424116041526537 | validation: 1.3775399409492788]
	TIME [epoch: 8.63 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0436809844130592		[learning rate: 0.00012248]
	Learning Rate: 0.000122476
	LOSS [training: 3.0436809844130592 | validation: 1.3747484054949124]
	TIME [epoch: 8.61 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043487165757441		[learning rate: 0.00012203]
	Learning Rate: 0.000122032
	LOSS [training: 3.043487165757441 | validation: 1.3845913460850243]
	TIME [epoch: 8.61 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.047191761115342		[learning rate: 0.00012159]
	Learning Rate: 0.000121589
	LOSS [training: 3.047191761115342 | validation: 1.3778388321414246]
	TIME [epoch: 8.62 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043178931745733		[learning rate: 0.00012115]
	Learning Rate: 0.000121148
	LOSS [training: 3.043178931745733 | validation: 1.3602328495009468]
	TIME [epoch: 8.63 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.048625225081795		[learning rate: 0.00012071]
	Learning Rate: 0.000120708
	LOSS [training: 3.048625225081795 | validation: 1.3666718426460898]
	TIME [epoch: 8.61 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.047805177707072		[learning rate: 0.00012027]
	Learning Rate: 0.00012027
	LOSS [training: 3.047805177707072 | validation: 1.361089000586546]
	TIME [epoch: 8.6 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.040178365112338		[learning rate: 0.00011983]
	Learning Rate: 0.000119834
	LOSS [training: 3.040178365112338 | validation: 1.371156733862878]
	TIME [epoch: 8.62 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0446168968710023		[learning rate: 0.0001194]
	Learning Rate: 0.000119399
	LOSS [training: 3.0446168968710023 | validation: 1.367050022544122]
	TIME [epoch: 8.62 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0443327112684053		[learning rate: 0.00011897]
	Learning Rate: 0.000118966
	LOSS [training: 3.0443327112684053 | validation: 1.3764712655298952]
	TIME [epoch: 8.61 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0464827829774386		[learning rate: 0.00011853]
	Learning Rate: 0.000118534
	LOSS [training: 3.0464827829774386 | validation: 1.3645785434906132]
	TIME [epoch: 8.61 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0442738031129606		[learning rate: 0.0001181]
	Learning Rate: 0.000118104
	LOSS [training: 3.0442738031129606 | validation: 1.3750956208524234]
	TIME [epoch: 8.63 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043551796604072		[learning rate: 0.00011767]
	Learning Rate: 0.000117675
	LOSS [training: 3.043551796604072 | validation: 1.3703519973284828]
	TIME [epoch: 8.61 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0423483294545814		[learning rate: 0.00011725]
	Learning Rate: 0.000117248
	LOSS [training: 3.0423483294545814 | validation: 1.3788710237898574]
	TIME [epoch: 8.6 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043183458886719		[learning rate: 0.00011682]
	Learning Rate: 0.000116822
	LOSS [training: 3.043183458886719 | validation: 1.3614648338899957]
	TIME [epoch: 8.61 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0433695917362558		[learning rate: 0.0001164]
	Learning Rate: 0.000116398
	LOSS [training: 3.0433695917362558 | validation: 1.368845096987167]
	TIME [epoch: 8.63 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.046958943107847		[learning rate: 0.00011598]
	Learning Rate: 0.000115976
	LOSS [training: 3.046958943107847 | validation: 1.3779264301484166]
	TIME [epoch: 8.61 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.045425283089112		[learning rate: 0.00011556]
	Learning Rate: 0.000115555
	LOSS [training: 3.045425283089112 | validation: 1.3684736994659112]
	TIME [epoch: 8.61 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0620005739121323		[learning rate: 0.00011514]
	Learning Rate: 0.000115136
	LOSS [training: 3.0620005739121323 | validation: 1.3634311892628803]
	TIME [epoch: 8.62 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043988386827554		[learning rate: 0.00011472]
	Learning Rate: 0.000114718
	LOSS [training: 3.043988386827554 | validation: 1.36976431292635]
	TIME [epoch: 8.63 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.044936948474319		[learning rate: 0.0001143]
	Learning Rate: 0.000114302
	LOSS [training: 3.044936948474319 | validation: 1.365901676680176]
	TIME [epoch: 8.61 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0395762972571854		[learning rate: 0.00011389]
	Learning Rate: 0.000113887
	LOSS [training: 3.0395762972571854 | validation: 1.3900990185523996]
	TIME [epoch: 8.62 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.045246068691709		[learning rate: 0.00011347]
	Learning Rate: 0.000113474
	LOSS [training: 3.045246068691709 | validation: 1.369516810435206]
	TIME [epoch: 8.61 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039424234327611		[learning rate: 0.00011306]
	Learning Rate: 0.000113062
	LOSS [training: 3.039424234327611 | validation: 1.3672558504716221]
	TIME [epoch: 8.64 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0393997870079987		[learning rate: 0.00011265]
	Learning Rate: 0.000112651
	LOSS [training: 3.0393997870079987 | validation: 1.359430252319035]
	TIME [epoch: 8.61 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0441946393824355		[learning rate: 0.00011224]
	Learning Rate: 0.000112243
	LOSS [training: 3.0441946393824355 | validation: 1.378834083513533]
	TIME [epoch: 8.61 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0393175437644633		[learning rate: 0.00011184]
	Learning Rate: 0.000111835
	LOSS [training: 3.0393175437644633 | validation: 1.367770880830233]
	TIME [epoch: 8.62 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036053712831612		[learning rate: 0.00011143]
	Learning Rate: 0.000111429
	LOSS [training: 3.036053712831612 | validation: 1.3634026363383376]
	TIME [epoch: 8.62 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.040124562209521		[learning rate: 0.00011103]
	Learning Rate: 0.000111025
	LOSS [training: 3.040124562209521 | validation: 1.3581509339434015]
	TIME [epoch: 8.61 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04060712751574		[learning rate: 0.00011062]
	Learning Rate: 0.000110622
	LOSS [training: 3.04060712751574 | validation: 1.3685889329475098]
	TIME [epoch: 8.62 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0453616838708237		[learning rate: 0.00011022]
	Learning Rate: 0.000110221
	LOSS [training: 3.0453616838708237 | validation: 1.3716652125604016]
	TIME [epoch: 8.61 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041148079407543		[learning rate: 0.00010982]
	Learning Rate: 0.000109821
	LOSS [training: 3.041148079407543 | validation: 1.372156371895852]
	TIME [epoch: 8.61 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041253325433373		[learning rate: 0.00010942]
	Learning Rate: 0.000109422
	LOSS [training: 3.041253325433373 | validation: 1.3657144173531477]
	TIME [epoch: 8.61 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0385356623943167		[learning rate: 0.00010903]
	Learning Rate: 0.000109025
	LOSS [training: 3.0385356623943167 | validation: 1.368522663331074]
	TIME [epoch: 8.61 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.045565473113064		[learning rate: 0.00010863]
	Learning Rate: 0.000108629
	LOSS [training: 3.045565473113064 | validation: 1.399580297321692]
	TIME [epoch: 8.62 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0446795165089613		[learning rate: 0.00010824]
	Learning Rate: 0.000108235
	LOSS [training: 3.0446795165089613 | validation: 1.3672460872100876]
	TIME [epoch: 8.61 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0445808816009605		[learning rate: 0.00010784]
	Learning Rate: 0.000107842
	LOSS [training: 3.0445808816009605 | validation: 1.3708828066491257]
	TIME [epoch: 8.61 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.045470126292676		[learning rate: 0.00010745]
	Learning Rate: 0.000107451
	LOSS [training: 3.045470126292676 | validation: 1.377038221510444]
	TIME [epoch: 8.61 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0428505189743786		[learning rate: 0.00010706]
	Learning Rate: 0.000107061
	LOSS [training: 3.0428505189743786 | validation: 1.3669643440590837]
	TIME [epoch: 8.62 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038223574737805		[learning rate: 0.00010667]
	Learning Rate: 0.000106673
	LOSS [training: 3.038223574737805 | validation: 1.3869845242972305]
	TIME [epoch: 8.62 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.042750333754366		[learning rate: 0.00010629]
	Learning Rate: 0.000106285
	LOSS [training: 3.042750333754366 | validation: 1.3733490216468969]
	TIME [epoch: 8.61 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041160505786528		[learning rate: 0.0001059]
	Learning Rate: 0.0001059
	LOSS [training: 3.041160505786528 | validation: 1.3836961972048538]
	TIME [epoch: 8.61 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0448873007343833		[learning rate: 0.00010552]
	Learning Rate: 0.000105515
	LOSS [training: 3.0448873007343833 | validation: 1.371360567374894]
	TIME [epoch: 8.64 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041206050366721		[learning rate: 0.00010513]
	Learning Rate: 0.000105132
	LOSS [training: 3.041206050366721 | validation: 1.367163911183829]
	TIME [epoch: 8.61 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039184330899631		[learning rate: 0.00010475]
	Learning Rate: 0.000104751
	LOSS [training: 3.039184330899631 | validation: 1.3857698229839792]
	TIME [epoch: 8.61 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03977284152959		[learning rate: 0.00010437]
	Learning Rate: 0.000104371
	LOSS [training: 3.03977284152959 | validation: 1.3655134190118092]
	TIME [epoch: 8.61 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0491083977677547		[learning rate: 0.00010399]
	Learning Rate: 0.000103992
	LOSS [training: 3.0491083977677547 | validation: 1.3705754402500927]
	TIME [epoch: 8.64 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043239561424406		[learning rate: 0.00010361]
	Learning Rate: 0.000103615
	LOSS [training: 3.043239561424406 | validation: 1.3621057136313732]
	TIME [epoch: 8.61 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0415156050112477		[learning rate: 0.00010324]
	Learning Rate: 0.000103239
	LOSS [training: 3.0415156050112477 | validation: 1.3948738648066388]
	TIME [epoch: 8.61 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.040295436958781		[learning rate: 0.00010286]
	Learning Rate: 0.000102864
	LOSS [training: 3.040295436958781 | validation: 1.3606013928484946]
	TIME [epoch: 8.61 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0358612910967824		[learning rate: 0.00010249]
	Learning Rate: 0.000102491
	LOSS [training: 3.0358612910967824 | validation: 1.3927408276991144]
	TIME [epoch: 8.63 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.049129203158317		[learning rate: 0.00010212]
	Learning Rate: 0.000102119
	LOSS [training: 3.049129203158317 | validation: 1.3841452426733494]
	TIME [epoch: 8.61 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0443993080887664		[learning rate: 0.00010175]
	Learning Rate: 0.000101748
	LOSS [training: 3.0443993080887664 | validation: 1.381143030336322]
	TIME [epoch: 8.62 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039073560754782		[learning rate: 0.00010138]
	Learning Rate: 0.000101379
	LOSS [training: 3.039073560754782 | validation: 1.390845454750585]
	TIME [epoch: 8.62 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.040617591971432		[learning rate: 0.00010101]
	Learning Rate: 0.000101011
	LOSS [training: 3.040617591971432 | validation: 1.372615579052751]
	TIME [epoch: 8.63 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0408378641394034		[learning rate: 0.00010064]
	Learning Rate: 0.000100644
	LOSS [training: 3.0408378641394034 | validation: 1.3618265625459718]
	TIME [epoch: 8.61 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0402410927359567		[learning rate: 0.00010028]
	Learning Rate: 0.000100279
	LOSS [training: 3.0402410927359567 | validation: 1.3684967999096886]
	TIME [epoch: 8.61 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0371341545563375		[learning rate: 9.9915e-05]
	Learning Rate: 9.99152e-05
	LOSS [training: 3.0371341545563375 | validation: 1.3725545231826228]
	TIME [epoch: 8.63 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0421620899364696		[learning rate: 9.9553e-05]
	Learning Rate: 9.95526e-05
	LOSS [training: 3.0421620899364696 | validation: 1.3686268407160103]
	TIME [epoch: 8.61 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0421317804442753		[learning rate: 9.9191e-05]
	Learning Rate: 9.91913e-05
	LOSS [training: 3.0421317804442753 | validation: 1.3817615998198636]
	TIME [epoch: 8.62 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0421216290035216		[learning rate: 9.8831e-05]
	Learning Rate: 9.88314e-05
	LOSS [training: 3.0421216290035216 | validation: 1.3740513368073037]
	TIME [epoch: 8.61 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0350517364627168		[learning rate: 9.8473e-05]
	Learning Rate: 9.84727e-05
	LOSS [training: 3.0350517364627168 | validation: 1.37294369960907]
	TIME [epoch: 8.64 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0396425963706504		[learning rate: 9.8115e-05]
	Learning Rate: 9.81153e-05
	LOSS [training: 3.0396425963706504 | validation: 1.3677849315535893]
	TIME [epoch: 8.62 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.040069259344324		[learning rate: 9.7759e-05]
	Learning Rate: 9.77592e-05
	LOSS [training: 3.040069259344324 | validation: 1.3687599362349223]
	TIME [epoch: 8.61 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041349463757674		[learning rate: 9.7404e-05]
	Learning Rate: 9.74045e-05
	LOSS [training: 3.041349463757674 | validation: 1.3555247475635126]
	TIME [epoch: 8.62 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.047165066016123		[learning rate: 9.7051e-05]
	Learning Rate: 9.7051e-05
	LOSS [training: 3.047165066016123 | validation: 1.3806790896352905]
	TIME [epoch: 8.63 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0421459613245228		[learning rate: 9.6699e-05]
	Learning Rate: 9.66988e-05
	LOSS [training: 3.0421459613245228 | validation: 1.3627939645741705]
	TIME [epoch: 8.59 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041975121649453		[learning rate: 9.6348e-05]
	Learning Rate: 9.63479e-05
	LOSS [training: 3.041975121649453 | validation: 1.3656116210360452]
	TIME [epoch: 8.61 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0418401477997965		[learning rate: 9.5998e-05]
	Learning Rate: 9.59982e-05
	LOSS [training: 3.0418401477997965 | validation: 1.3643568350934463]
	TIME [epoch: 8.61 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0395431651105573		[learning rate: 9.565e-05]
	Learning Rate: 9.56499e-05
	LOSS [training: 3.0395431651105573 | validation: 1.376251564273187]
	TIME [epoch: 8.64 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0398436379649207		[learning rate: 9.5303e-05]
	Learning Rate: 9.53027e-05
	LOSS [training: 3.0398436379649207 | validation: 1.3675184201784523]
	TIME [epoch: 8.6 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.042427214912893		[learning rate: 9.4957e-05]
	Learning Rate: 9.49569e-05
	LOSS [training: 3.042427214912893 | validation: 1.3892342476275668]
	TIME [epoch: 8.61 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0458943266910454		[learning rate: 9.4612e-05]
	Learning Rate: 9.46122e-05
	LOSS [training: 3.0458943266910454 | validation: 1.3627400835098007]
	TIME [epoch: 8.62 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0372141533526715		[learning rate: 9.4269e-05]
	Learning Rate: 9.42689e-05
	LOSS [training: 3.0372141533526715 | validation: 1.3654145103253745]
	TIME [epoch: 8.62 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0456922939170887		[learning rate: 9.3927e-05]
	Learning Rate: 9.39268e-05
	LOSS [training: 3.0456922939170887 | validation: 1.3698031120475878]
	TIME [epoch: 8.6 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0436600163567356		[learning rate: 9.3586e-05]
	Learning Rate: 9.35859e-05
	LOSS [training: 3.0436600163567356 | validation: 1.3566960524935556]
	TIME [epoch: 8.61 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0398960685912146		[learning rate: 9.3246e-05]
	Learning Rate: 9.32463e-05
	LOSS [training: 3.0398960685912146 | validation: 1.3831667520338107]
	TIME [epoch: 8.61 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041443305870585		[learning rate: 9.2908e-05]
	Learning Rate: 9.29079e-05
	LOSS [training: 3.041443305870585 | validation: 1.3643721559861866]
	TIME [epoch: 8.62 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0409275127057014		[learning rate: 9.2571e-05]
	Learning Rate: 9.25707e-05
	LOSS [training: 3.0409275127057014 | validation: 1.3657238392498872]
	TIME [epoch: 8.6 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0441367780620943		[learning rate: 9.2235e-05]
	Learning Rate: 9.22348e-05
	LOSS [training: 3.0441367780620943 | validation: 1.4015950595277695]
	TIME [epoch: 8.6 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.046201059151359		[learning rate: 9.19e-05]
	Learning Rate: 9.19001e-05
	LOSS [training: 3.046201059151359 | validation: 1.3775482289797043]
	TIME [epoch: 8.6 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043771765322476		[learning rate: 9.1567e-05]
	Learning Rate: 9.15665e-05
	LOSS [training: 3.043771765322476 | validation: 1.3675094419974965]
	TIME [epoch: 8.63 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0485861171045254		[learning rate: 9.1234e-05]
	Learning Rate: 9.12343e-05
	LOSS [training: 3.0485861171045254 | validation: 1.3641824089837087]
	TIME [epoch: 8.62 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.049184158475336		[learning rate: 9.0903e-05]
	Learning Rate: 9.09031e-05
	LOSS [training: 3.049184158475336 | validation: 1.364319632228164]
	TIME [epoch: 8.61 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0431794620686494		[learning rate: 9.0573e-05]
	Learning Rate: 9.05733e-05
	LOSS [training: 3.0431794620686494 | validation: 1.3503786341312458]
	TIME [epoch: 8.62 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r5_20240219_200345/states/model_tr_study203_1394.pth
	Model improved!!!
EPOCH 1395/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036872778691557		[learning rate: 9.0245e-05]
	Learning Rate: 9.02446e-05
	LOSS [training: 3.036872778691557 | validation: 1.3832342335032297]
	TIME [epoch: 8.64 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0428797769641376		[learning rate: 8.9917e-05]
	Learning Rate: 8.99171e-05
	LOSS [training: 3.0428797769641376 | validation: 1.3749582008953753]
	TIME [epoch: 8.62 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0446017617483454		[learning rate: 8.9591e-05]
	Learning Rate: 8.95908e-05
	LOSS [training: 3.0446017617483454 | validation: 1.3837332164780625]
	TIME [epoch: 8.62 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04011073038519		[learning rate: 8.9266e-05]
	Learning Rate: 8.92656e-05
	LOSS [training: 3.04011073038519 | validation: 1.3682911835869418]
	TIME [epoch: 8.64 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039799220832484		[learning rate: 8.8942e-05]
	Learning Rate: 8.89417e-05
	LOSS [training: 3.039799220832484 | validation: 1.372709530026974]
	TIME [epoch: 8.63 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0416075670653457		[learning rate: 8.8619e-05]
	Learning Rate: 8.86189e-05
	LOSS [training: 3.0416075670653457 | validation: 1.3797612707436309]
	TIME [epoch: 8.63 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0411468536449386		[learning rate: 8.8297e-05]
	Learning Rate: 8.82973e-05
	LOSS [training: 3.0411468536449386 | validation: 1.3778085155903772]
	TIME [epoch: 8.62 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0436458451986175		[learning rate: 8.7977e-05]
	Learning Rate: 8.79769e-05
	LOSS [training: 3.0436458451986175 | validation: 1.3693342609020585]
	TIME [epoch: 8.64 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041123775673744		[learning rate: 8.7658e-05]
	Learning Rate: 8.76576e-05
	LOSS [training: 3.041123775673744 | validation: 1.3635912298812813]
	TIME [epoch: 8.63 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.045360280002861		[learning rate: 8.7339e-05]
	Learning Rate: 8.73395e-05
	LOSS [training: 3.045360280002861 | validation: 1.35168669248764]
	TIME [epoch: 8.62 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03450766798863		[learning rate: 8.7023e-05]
	Learning Rate: 8.70225e-05
	LOSS [training: 3.03450766798863 | validation: 1.3558266612735619]
	TIME [epoch: 8.63 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0402965902019714		[learning rate: 8.6707e-05]
	Learning Rate: 8.67067e-05
	LOSS [training: 3.0402965902019714 | validation: 1.361466010667949]
	TIME [epoch: 8.64 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.042920932579254		[learning rate: 8.6392e-05]
	Learning Rate: 8.63921e-05
	LOSS [training: 3.042920932579254 | validation: 1.381348952064337]
	TIME [epoch: 8.62 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0450691427593686		[learning rate: 8.6079e-05]
	Learning Rate: 8.60785e-05
	LOSS [training: 3.0450691427593686 | validation: 1.3679240820285892]
	TIME [epoch: 8.61 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0432990988296362		[learning rate: 8.5766e-05]
	Learning Rate: 8.57661e-05
	LOSS [training: 3.0432990988296362 | validation: 1.3948908534488345]
	TIME [epoch: 8.63 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04457337316261		[learning rate: 8.5455e-05]
	Learning Rate: 8.54549e-05
	LOSS [training: 3.04457337316261 | validation: 1.374092283043119]
	TIME [epoch: 8.64 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038921731117515		[learning rate: 8.5145e-05]
	Learning Rate: 8.51448e-05
	LOSS [training: 3.038921731117515 | validation: 1.3628688087596952]
	TIME [epoch: 8.62 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0411510551034184		[learning rate: 8.4836e-05]
	Learning Rate: 8.48358e-05
	LOSS [training: 3.0411510551034184 | validation: 1.3736987906394942]
	TIME [epoch: 8.63 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0456857174604757		[learning rate: 8.4528e-05]
	Learning Rate: 8.45279e-05
	LOSS [training: 3.0456857174604757 | validation: 1.3631873550576734]
	TIME [epoch: 8.64 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0426594061262704		[learning rate: 8.4221e-05]
	Learning Rate: 8.42211e-05
	LOSS [training: 3.0426594061262704 | validation: 1.37698818506659]
	TIME [epoch: 8.62 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037737564924395		[learning rate: 8.3916e-05]
	Learning Rate: 8.39155e-05
	LOSS [training: 3.037737564924395 | validation: 1.374915041784878]
	TIME [epoch: 8.62 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0356375052287086		[learning rate: 8.3611e-05]
	Learning Rate: 8.3611e-05
	LOSS [training: 3.0356375052287086 | validation: 1.3754190746194626]
	TIME [epoch: 8.63 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03640597565063		[learning rate: 8.3308e-05]
	Learning Rate: 8.33075e-05
	LOSS [training: 3.03640597565063 | validation: 1.3715815218008995]
	TIME [epoch: 8.64 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.047581453225958		[learning rate: 8.3005e-05]
	Learning Rate: 8.30052e-05
	LOSS [training: 3.047581453225958 | validation: 1.3700136927738225]
	TIME [epoch: 8.63 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039921389746669		[learning rate: 8.2704e-05]
	Learning Rate: 8.2704e-05
	LOSS [training: 3.039921389746669 | validation: 1.3709506615028064]
	TIME [epoch: 8.62 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0385876043474775		[learning rate: 8.2404e-05]
	Learning Rate: 8.24038e-05
	LOSS [training: 3.0385876043474775 | validation: 1.3638315426882324]
	TIME [epoch: 8.63 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0422919995365065		[learning rate: 8.2105e-05]
	Learning Rate: 8.21048e-05
	LOSS [training: 3.0422919995365065 | validation: 1.3630588314537082]
	TIME [epoch: 8.65 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0404133140131457		[learning rate: 8.1807e-05]
	Learning Rate: 8.18068e-05
	LOSS [training: 3.0404133140131457 | validation: 1.3694181662975091]
	TIME [epoch: 8.63 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041128833356418		[learning rate: 8.151e-05]
	Learning Rate: 8.15099e-05
	LOSS [training: 3.041128833356418 | validation: 1.3644102315719961]
	TIME [epoch: 8.63 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038346225601063		[learning rate: 8.1214e-05]
	Learning Rate: 8.12142e-05
	LOSS [training: 3.038346225601063 | validation: 1.386320283922239]
	TIME [epoch: 8.61 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0422501688229544		[learning rate: 8.0919e-05]
	Learning Rate: 8.09194e-05
	LOSS [training: 3.0422501688229544 | validation: 1.376176633437826]
	TIME [epoch: 8.64 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0410454240966027		[learning rate: 8.0626e-05]
	Learning Rate: 8.06257e-05
	LOSS [training: 3.0410454240966027 | validation: 1.371825754922125]
	TIME [epoch: 8.63 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0433241068415433		[learning rate: 8.0333e-05]
	Learning Rate: 8.03331e-05
	LOSS [training: 3.0433241068415433 | validation: 1.3697719720944905]
	TIME [epoch: 8.63 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.042540693442323		[learning rate: 8.0042e-05]
	Learning Rate: 8.00416e-05
	LOSS [training: 3.042540693442323 | validation: 1.3637159792918738]
	TIME [epoch: 8.63 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0418143412572842		[learning rate: 7.9751e-05]
	Learning Rate: 7.97511e-05
	LOSS [training: 3.0418143412572842 | validation: 1.3845794248696595]
	TIME [epoch: 8.65 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0417315882497618		[learning rate: 7.9462e-05]
	Learning Rate: 7.94617e-05
	LOSS [training: 3.0417315882497618 | validation: 1.3812470312054534]
	TIME [epoch: 8.63 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0432074459761758		[learning rate: 7.9173e-05]
	Learning Rate: 7.91733e-05
	LOSS [training: 3.0432074459761758 | validation: 1.3664888446553243]
	TIME [epoch: 8.63 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.044400425906651		[learning rate: 7.8886e-05]
	Learning Rate: 7.8886e-05
	LOSS [training: 3.044400425906651 | validation: 1.372267685582139]
	TIME [epoch: 8.64 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041440095354124		[learning rate: 7.86e-05]
	Learning Rate: 7.85997e-05
	LOSS [training: 3.041440095354124 | validation: 1.3631968340732297]
	TIME [epoch: 8.64 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0397279198076497		[learning rate: 7.8315e-05]
	Learning Rate: 7.83145e-05
	LOSS [training: 3.0397279198076497 | validation: 1.3616280814419846]
	TIME [epoch: 8.62 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0380211947820674		[learning rate: 7.803e-05]
	Learning Rate: 7.80303e-05
	LOSS [training: 3.0380211947820674 | validation: 1.3620101632766846]
	TIME [epoch: 8.62 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0424225838895085		[learning rate: 7.7747e-05]
	Learning Rate: 7.77471e-05
	LOSS [training: 3.0424225838895085 | validation: 1.3767270950202162]
	TIME [epoch: 8.64 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.042240187459949		[learning rate: 7.7465e-05]
	Learning Rate: 7.7465e-05
	LOSS [training: 3.042240187459949 | validation: 1.3648446635085296]
	TIME [epoch: 8.64 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041786051790987		[learning rate: 7.7184e-05]
	Learning Rate: 7.71838e-05
	LOSS [training: 3.041786051790987 | validation: 1.3771543313904195]
	TIME [epoch: 8.63 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0384622131330294		[learning rate: 7.6904e-05]
	Learning Rate: 7.69037e-05
	LOSS [training: 3.0384622131330294 | validation: 1.3583483815368302]
	TIME [epoch: 8.63 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0422775125971597		[learning rate: 7.6625e-05]
	Learning Rate: 7.66246e-05
	LOSS [training: 3.0422775125971597 | validation: 1.3627316329905217]
	TIME [epoch: 8.65 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0420004111307497		[learning rate: 7.6347e-05]
	Learning Rate: 7.63466e-05
	LOSS [training: 3.0420004111307497 | validation: 1.3675805195227222]
	TIME [epoch: 8.63 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0427436466386744		[learning rate: 7.607e-05]
	Learning Rate: 7.60695e-05
	LOSS [training: 3.0427436466386744 | validation: 1.3523914177217946]
	TIME [epoch: 8.63 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0382778759252433		[learning rate: 7.5793e-05]
	Learning Rate: 7.57935e-05
	LOSS [training: 3.0382778759252433 | validation: 1.3817907051699634]
	TIME [epoch: 8.63 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0428550406538526		[learning rate: 7.5518e-05]
	Learning Rate: 7.55184e-05
	LOSS [training: 3.0428550406538526 | validation: 1.3761882714339866]
	TIME [epoch: 8.65 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043692165005565		[learning rate: 7.5244e-05]
	Learning Rate: 7.52443e-05
	LOSS [training: 3.043692165005565 | validation: 1.3634583538184728]
	TIME [epoch: 8.64 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0459638391167396		[learning rate: 7.4971e-05]
	Learning Rate: 7.49712e-05
	LOSS [training: 3.0459638391167396 | validation: 1.3650209865703724]
	TIME [epoch: 8.63 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.045367879204053		[learning rate: 7.4699e-05]
	Learning Rate: 7.46992e-05
	LOSS [training: 3.045367879204053 | validation: 1.3652237990611753]
	TIME [epoch: 8.63 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0416826155776717		[learning rate: 7.4428e-05]
	Learning Rate: 7.44281e-05
	LOSS [training: 3.0416826155776717 | validation: 1.3621460133293775]
	TIME [epoch: 8.65 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0335650344827334		[learning rate: 7.4158e-05]
	Learning Rate: 7.4158e-05
	LOSS [training: 3.0335650344827334 | validation: 1.3652391819425906]
	TIME [epoch: 8.64 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0398191663029115		[learning rate: 7.3889e-05]
	Learning Rate: 7.38889e-05
	LOSS [training: 3.0398191663029115 | validation: 1.3820560895905278]
	TIME [epoch: 8.63 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.046768621866032		[learning rate: 7.3621e-05]
	Learning Rate: 7.36207e-05
	LOSS [training: 3.046768621866032 | validation: 1.3721666382475834]
	TIME [epoch: 8.63 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0409960518580275		[learning rate: 7.3354e-05]
	Learning Rate: 7.33536e-05
	LOSS [training: 3.0409960518580275 | validation: 1.3727922264093158]
	TIME [epoch: 8.66 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0434927107357606		[learning rate: 7.3087e-05]
	Learning Rate: 7.30873e-05
	LOSS [training: 3.0434927107357606 | validation: 1.3630801365378207]
	TIME [epoch: 8.62 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041389735887597		[learning rate: 7.2822e-05]
	Learning Rate: 7.28221e-05
	LOSS [training: 3.041389735887597 | validation: 1.3645659501413825]
	TIME [epoch: 8.63 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0437952606770464		[learning rate: 7.2558e-05]
	Learning Rate: 7.25578e-05
	LOSS [training: 3.0437952606770464 | validation: 1.3636478950694861]
	TIME [epoch: 8.63 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.044164676314785		[learning rate: 7.2295e-05]
	Learning Rate: 7.22945e-05
	LOSS [training: 3.044164676314785 | validation: 1.369477236007461]
	TIME [epoch: 8.64 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0364970810795384		[learning rate: 7.2032e-05]
	Learning Rate: 7.20321e-05
	LOSS [training: 3.0364970810795384 | validation: 1.3736709648704175]
	TIME [epoch: 8.64 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03719587867523		[learning rate: 7.1771e-05]
	Learning Rate: 7.17707e-05
	LOSS [training: 3.03719587867523 | validation: 1.3791187865673546]
	TIME [epoch: 8.63 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.044901247475657		[learning rate: 7.151e-05]
	Learning Rate: 7.15103e-05
	LOSS [training: 3.044901247475657 | validation: 1.375090048609747]
	TIME [epoch: 8.65 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038743844821627		[learning rate: 7.1251e-05]
	Learning Rate: 7.12508e-05
	LOSS [training: 3.038743844821627 | validation: 1.374083990399478]
	TIME [epoch: 8.67 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039206565810388		[learning rate: 7.0992e-05]
	Learning Rate: 7.09922e-05
	LOSS [training: 3.039206565810388 | validation: 1.3723230605984362]
	TIME [epoch: 8.63 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0423864300831434		[learning rate: 7.0735e-05]
	Learning Rate: 7.07345e-05
	LOSS [training: 3.0423864300831434 | validation: 1.377914883642597]
	TIME [epoch: 8.63 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0429241658002817		[learning rate: 7.0478e-05]
	Learning Rate: 7.04778e-05
	LOSS [training: 3.0429241658002817 | validation: 1.3657739060940868]
	TIME [epoch: 8.66 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037731522571931		[learning rate: 7.0222e-05]
	Learning Rate: 7.02221e-05
	LOSS [training: 3.037731522571931 | validation: 1.3724597571981763]
	TIME [epoch: 8.65 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0402812221734328		[learning rate: 6.9967e-05]
	Learning Rate: 6.99672e-05
	LOSS [training: 3.0402812221734328 | validation: 1.364549456050466]
	TIME [epoch: 8.61 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037051131113088		[learning rate: 6.9713e-05]
	Learning Rate: 6.97133e-05
	LOSS [training: 3.037051131113088 | validation: 1.3709436013486787]
	TIME [epoch: 8.59 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041965598568594		[learning rate: 6.946e-05]
	Learning Rate: 6.94603e-05
	LOSS [training: 3.041965598568594 | validation: 1.361617367296559]
	TIME [epoch: 8.61 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0443101481513417		[learning rate: 6.9208e-05]
	Learning Rate: 6.92083e-05
	LOSS [training: 3.0443101481513417 | validation: 1.3686807881604335]
	TIME [epoch: 8.6 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0370637414174184		[learning rate: 6.8957e-05]
	Learning Rate: 6.89571e-05
	LOSS [training: 3.0370637414174184 | validation: 1.3595407240169766]
	TIME [epoch: 8.61 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0369912356682947		[learning rate: 6.8707e-05]
	Learning Rate: 6.87068e-05
	LOSS [training: 3.0369912356682947 | validation: 1.3635838014066919]
	TIME [epoch: 8.61 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0421309320616396		[learning rate: 6.8457e-05]
	Learning Rate: 6.84575e-05
	LOSS [training: 3.0421309320616396 | validation: 1.3602384504539717]
	TIME [epoch: 8.63 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0377637332579615		[learning rate: 6.8209e-05]
	Learning Rate: 6.82091e-05
	LOSS [training: 3.0377637332579615 | validation: 1.3767581642206976]
	TIME [epoch: 8.6 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0374417589436513		[learning rate: 6.7962e-05]
	Learning Rate: 6.79615e-05
	LOSS [training: 3.0374417589436513 | validation: 1.3745495269017483]
	TIME [epoch: 8.59 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0399528010927637		[learning rate: 6.7715e-05]
	Learning Rate: 6.77149e-05
	LOSS [training: 3.0399528010927637 | validation: 1.3693022561570765]
	TIME [epoch: 8.58 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.042540201097033		[learning rate: 6.7469e-05]
	Learning Rate: 6.74692e-05
	LOSS [training: 3.042540201097033 | validation: 1.3701608434424153]
	TIME [epoch: 8.61 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038019405028323		[learning rate: 6.7224e-05]
	Learning Rate: 6.72243e-05
	LOSS [training: 3.038019405028323 | validation: 1.3787376663754745]
	TIME [epoch: 8.59 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.044327605024592		[learning rate: 6.698e-05]
	Learning Rate: 6.69804e-05
	LOSS [training: 3.044327605024592 | validation: 1.4121956159782671]
	TIME [epoch: 8.59 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.046517810819158		[learning rate: 6.6737e-05]
	Learning Rate: 6.67373e-05
	LOSS [training: 3.046517810819158 | validation: 1.3748478825775425]
	TIME [epoch: 8.6 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0379708893283612		[learning rate: 6.6495e-05]
	Learning Rate: 6.64951e-05
	LOSS [training: 3.0379708893283612 | validation: 1.3791621846408781]
	TIME [epoch: 8.61 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0404755210872145		[learning rate: 6.6254e-05]
	Learning Rate: 6.62538e-05
	LOSS [training: 3.0404755210872145 | validation: 1.3757078595449705]
	TIME [epoch: 8.59 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039608288238786		[learning rate: 6.6013e-05]
	Learning Rate: 6.60133e-05
	LOSS [training: 3.039608288238786 | validation: 1.377844664588072]
	TIME [epoch: 8.61 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0457477309328076		[learning rate: 6.5774e-05]
	Learning Rate: 6.57738e-05
	LOSS [training: 3.0457477309328076 | validation: 1.3640276065591068]
	TIME [epoch: 8.65 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0402920033527496		[learning rate: 6.5535e-05]
	Learning Rate: 6.55351e-05
	LOSS [training: 3.0402920033527496 | validation: 1.3728520621329316]
	TIME [epoch: 8.63 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038836023775384		[learning rate: 6.5297e-05]
	Learning Rate: 6.52972e-05
	LOSS [training: 3.038836023775384 | validation: 1.3721962032835133]
	TIME [epoch: 8.62 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0394435900112082		[learning rate: 6.506e-05]
	Learning Rate: 6.50603e-05
	LOSS [training: 3.0394435900112082 | validation: 1.366965642061433]
	TIME [epoch: 8.62 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039356021386838		[learning rate: 6.4824e-05]
	Learning Rate: 6.48242e-05
	LOSS [training: 3.039356021386838 | validation: 1.3712220005480726]
	TIME [epoch: 8.63 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0367898978235948		[learning rate: 6.4589e-05]
	Learning Rate: 6.45889e-05
	LOSS [training: 3.0367898978235948 | validation: 1.3548197927189531]
	TIME [epoch: 8.63 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0410015657720066		[learning rate: 6.4354e-05]
	Learning Rate: 6.43545e-05
	LOSS [training: 3.0410015657720066 | validation: 1.3697412582762727]
	TIME [epoch: 8.61 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.042407645766242		[learning rate: 6.4121e-05]
	Learning Rate: 6.4121e-05
	LOSS [training: 3.042407645766242 | validation: 1.3662138491870581]
	TIME [epoch: 8.62 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037664162986174		[learning rate: 6.3888e-05]
	Learning Rate: 6.38883e-05
	LOSS [training: 3.037664162986174 | validation: 1.3701762645461808]
	TIME [epoch: 8.62 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.040226319566693		[learning rate: 6.3656e-05]
	Learning Rate: 6.36564e-05
	LOSS [training: 3.040226319566693 | validation: 1.3640951357930544]
	TIME [epoch: 8.62 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0508617187880107		[learning rate: 6.3425e-05]
	Learning Rate: 6.34254e-05
	LOSS [training: 3.0508617187880107 | validation: 1.3695156968301923]
	TIME [epoch: 8.61 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039408634439358		[learning rate: 6.3195e-05]
	Learning Rate: 6.31952e-05
	LOSS [training: 3.039408634439358 | validation: 1.3736706129093739]
	TIME [epoch: 8.59 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04349043443386		[learning rate: 6.2966e-05]
	Learning Rate: 6.29659e-05
	LOSS [training: 3.04349043443386 | validation: 1.360382894178947]
	TIME [epoch: 8.63 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.047031777541771		[learning rate: 6.2737e-05]
	Learning Rate: 6.27374e-05
	LOSS [training: 3.047031777541771 | validation: 1.3545200247464637]
	TIME [epoch: 8.6 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035450882178629		[learning rate: 6.251e-05]
	Learning Rate: 6.25097e-05
	LOSS [training: 3.035450882178629 | validation: 1.368484828390336]
	TIME [epoch: 8.6 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0366113029356763		[learning rate: 6.2283e-05]
	Learning Rate: 6.22828e-05
	LOSS [training: 3.0366113029356763 | validation: 1.3727579457366104]
	TIME [epoch: 8.6 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0433070164298845		[learning rate: 6.2057e-05]
	Learning Rate: 6.20568e-05
	LOSS [training: 3.0433070164298845 | validation: 1.3689451438581137]
	TIME [epoch: 8.62 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0408090296079306		[learning rate: 6.1832e-05]
	Learning Rate: 6.18316e-05
	LOSS [training: 3.0408090296079306 | validation: 1.368559944504801]
	TIME [epoch: 8.6 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039407525407681		[learning rate: 6.1607e-05]
	Learning Rate: 6.16072e-05
	LOSS [training: 3.039407525407681 | validation: 1.3711920894973129]
	TIME [epoch: 8.6 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036133283855095		[learning rate: 6.1384e-05]
	Learning Rate: 6.13836e-05
	LOSS [training: 3.036133283855095 | validation: 1.3639828956059046]
	TIME [epoch: 8.6 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0351427249571143		[learning rate: 6.1161e-05]
	Learning Rate: 6.11609e-05
	LOSS [training: 3.0351427249571143 | validation: 1.3574561924734865]
	TIME [epoch: 8.61 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.042744215174053		[learning rate: 6.0939e-05]
	Learning Rate: 6.09389e-05
	LOSS [training: 3.042744215174053 | validation: 1.3747726068139434]
	TIME [epoch: 8.59 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043759877596394		[learning rate: 6.0718e-05]
	Learning Rate: 6.07178e-05
	LOSS [training: 3.043759877596394 | validation: 1.3693228479479393]
	TIME [epoch: 8.59 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0461686555187244		[learning rate: 6.0497e-05]
	Learning Rate: 6.04974e-05
	LOSS [training: 3.0461686555187244 | validation: 1.3640713917625065]
	TIME [epoch: 8.6 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0410974518333482		[learning rate: 6.0278e-05]
	Learning Rate: 6.02779e-05
	LOSS [training: 3.0410974518333482 | validation: 1.3627589056130844]
	TIME [epoch: 8.61 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039017348123067		[learning rate: 6.0059e-05]
	Learning Rate: 6.00591e-05
	LOSS [training: 3.039017348123067 | validation: 1.3676032514326681]
	TIME [epoch: 8.6 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039971425873145		[learning rate: 5.9841e-05]
	Learning Rate: 5.98412e-05
	LOSS [training: 3.039971425873145 | validation: 1.3659278938464834]
	TIME [epoch: 8.6 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043192552636274		[learning rate: 5.9624e-05]
	Learning Rate: 5.9624e-05
	LOSS [training: 3.043192552636274 | validation: 1.3670194205367814]
	TIME [epoch: 8.61 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0404146226940822		[learning rate: 5.9408e-05]
	Learning Rate: 5.94076e-05
	LOSS [training: 3.0404146226940822 | validation: 1.3662381442005325]
	TIME [epoch: 8.6 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0398879326922126		[learning rate: 5.9192e-05]
	Learning Rate: 5.9192e-05
	LOSS [training: 3.0398879326922126 | validation: 1.3682232535232537]
	TIME [epoch: 8.6 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0398645890861813		[learning rate: 5.8977e-05]
	Learning Rate: 5.89772e-05
	LOSS [training: 3.0398645890861813 | validation: 1.3872726649441856]
	TIME [epoch: 8.59 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04038068719732		[learning rate: 5.8763e-05]
	Learning Rate: 5.87632e-05
	LOSS [training: 3.04038068719732 | validation: 1.356645286904742]
	TIME [epoch: 8.62 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0414441074787946		[learning rate: 5.855e-05]
	Learning Rate: 5.85499e-05
	LOSS [training: 3.0414441074787946 | validation: 1.3628810554297175]
	TIME [epoch: 8.59 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0417510201531632		[learning rate: 5.8337e-05]
	Learning Rate: 5.83374e-05
	LOSS [training: 3.0417510201531632 | validation: 1.3708575836317194]
	TIME [epoch: 8.6 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04003183827115		[learning rate: 5.8126e-05]
	Learning Rate: 5.81257e-05
	LOSS [training: 3.04003183827115 | validation: 1.366675525828807]
	TIME [epoch: 8.6 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041266724676651		[learning rate: 5.7915e-05]
	Learning Rate: 5.79148e-05
	LOSS [training: 3.041266724676651 | validation: 1.3834041721491726]
	TIME [epoch: 8.62 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0430073663283315		[learning rate: 5.7705e-05]
	Learning Rate: 5.77046e-05
	LOSS [training: 3.0430073663283315 | validation: 1.378819889955599]
	TIME [epoch: 8.6 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.044730255060852		[learning rate: 5.7495e-05]
	Learning Rate: 5.74952e-05
	LOSS [training: 3.044730255060852 | validation: 1.3855269278592994]
	TIME [epoch: 8.59 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03942402177416		[learning rate: 5.7287e-05]
	Learning Rate: 5.72866e-05
	LOSS [training: 3.03942402177416 | validation: 1.3706750781241461]
	TIME [epoch: 8.6 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035658491113934		[learning rate: 5.7079e-05]
	Learning Rate: 5.70787e-05
	LOSS [training: 3.035658491113934 | validation: 1.3671429653642995]
	TIME [epoch: 8.62 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037755043731951		[learning rate: 5.6872e-05]
	Learning Rate: 5.68715e-05
	LOSS [training: 3.037755043731951 | validation: 1.3693942161263064]
	TIME [epoch: 8.6 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0381772080496967		[learning rate: 5.6665e-05]
	Learning Rate: 5.66651e-05
	LOSS [training: 3.0381772080496967 | validation: 1.3643746448911414]
	TIME [epoch: 8.6 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0383695565807276		[learning rate: 5.6459e-05]
	Learning Rate: 5.64595e-05
	LOSS [training: 3.0383695565807276 | validation: 1.3667083137072207]
	TIME [epoch: 8.59 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0369961150626414		[learning rate: 5.6255e-05]
	Learning Rate: 5.62546e-05
	LOSS [training: 3.0369961150626414 | validation: 1.377891311642908]
	TIME [epoch: 8.64 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036328022705726		[learning rate: 5.605e-05]
	Learning Rate: 5.60504e-05
	LOSS [training: 3.036328022705726 | validation: 1.3709749460046254]
	TIME [epoch: 8.6 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0387521575528518		[learning rate: 5.5847e-05]
	Learning Rate: 5.5847e-05
	LOSS [training: 3.0387521575528518 | validation: 1.372098214782607]
	TIME [epoch: 8.6 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034792922218679		[learning rate: 5.5644e-05]
	Learning Rate: 5.56444e-05
	LOSS [training: 3.034792922218679 | validation: 1.3794777517047818]
	TIME [epoch: 8.61 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0425788518961		[learning rate: 5.5442e-05]
	Learning Rate: 5.54424e-05
	LOSS [training: 3.0425788518961 | validation: 1.3780600021162046]
	TIME [epoch: 8.62 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04082032274239		[learning rate: 5.5241e-05]
	Learning Rate: 5.52412e-05
	LOSS [training: 3.04082032274239 | validation: 1.3756906040705934]
	TIME [epoch: 8.59 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038007938360106		[learning rate: 5.5041e-05]
	Learning Rate: 5.50407e-05
	LOSS [training: 3.038007938360106 | validation: 1.3702352121888952]
	TIME [epoch: 8.61 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0410584614317626		[learning rate: 5.4841e-05]
	Learning Rate: 5.4841e-05
	LOSS [training: 3.0410584614317626 | validation: 1.3688501699756221]
	TIME [epoch: 8.61 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036761742703822		[learning rate: 5.4642e-05]
	Learning Rate: 5.4642e-05
	LOSS [training: 3.036761742703822 | validation: 1.3891426760094747]
	TIME [epoch: 8.61 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0450778564095407		[learning rate: 5.4444e-05]
	Learning Rate: 5.44437e-05
	LOSS [training: 3.0450778564095407 | validation: 1.370955958421195]
	TIME [epoch: 8.61 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0439332272186923		[learning rate: 5.4246e-05]
	Learning Rate: 5.42461e-05
	LOSS [training: 3.0439332272186923 | validation: 1.3643361946170125]
	TIME [epoch: 8.6 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041505505004006		[learning rate: 5.4049e-05]
	Learning Rate: 5.40492e-05
	LOSS [training: 3.041505505004006 | validation: 1.3784477603867722]
	TIME [epoch: 8.62 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.040996488189028		[learning rate: 5.3853e-05]
	Learning Rate: 5.38531e-05
	LOSS [training: 3.040996488189028 | validation: 1.3787680493870442]
	TIME [epoch: 8.61 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0390139943744092		[learning rate: 5.3658e-05]
	Learning Rate: 5.36577e-05
	LOSS [training: 3.0390139943744092 | validation: 1.3744881962858253]
	TIME [epoch: 8.61 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0384703646670217		[learning rate: 5.3463e-05]
	Learning Rate: 5.34629e-05
	LOSS [training: 3.0384703646670217 | validation: 1.3684549217448523]
	TIME [epoch: 8.6 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0342730416356183		[learning rate: 5.3269e-05]
	Learning Rate: 5.32689e-05
	LOSS [training: 3.0342730416356183 | validation: 1.3634836549079115]
	TIME [epoch: 8.62 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037358001458912		[learning rate: 5.3076e-05]
	Learning Rate: 5.30756e-05
	LOSS [training: 3.037358001458912 | validation: 1.3657264073341429]
	TIME [epoch: 8.61 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0380806653927728		[learning rate: 5.2883e-05]
	Learning Rate: 5.2883e-05
	LOSS [training: 3.0380806653927728 | validation: 1.360636616226908]
	TIME [epoch: 8.61 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0387232435202294		[learning rate: 5.2691e-05]
	Learning Rate: 5.26911e-05
	LOSS [training: 3.0387232435202294 | validation: 1.3625066351875927]
	TIME [epoch: 8.61 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035571300381929		[learning rate: 5.25e-05]
	Learning Rate: 5.24998e-05
	LOSS [training: 3.035571300381929 | validation: 1.3758410258911622]
	TIME [epoch: 8.63 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036967058373849		[learning rate: 5.2309e-05]
	Learning Rate: 5.23093e-05
	LOSS [training: 3.036967058373849 | validation: 1.358114112296055]
	TIME [epoch: 8.61 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0430220037205298		[learning rate: 5.2119e-05]
	Learning Rate: 5.21195e-05
	LOSS [training: 3.0430220037205298 | validation: 1.3738197131268777]
	TIME [epoch: 8.61 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037581823244706		[learning rate: 5.193e-05]
	Learning Rate: 5.19303e-05
	LOSS [training: 3.037581823244706 | validation: 1.367248153740125]
	TIME [epoch: 8.62 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0357442219467434		[learning rate: 5.1742e-05]
	Learning Rate: 5.17419e-05
	LOSS [training: 3.0357442219467434 | validation: 1.3830726931880664]
	TIME [epoch: 8.62 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.042596615367901		[learning rate: 5.1554e-05]
	Learning Rate: 5.15541e-05
	LOSS [training: 3.042596615367901 | validation: 1.3675881094657814]
	TIME [epoch: 8.6 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037640231230849		[learning rate: 5.1367e-05]
	Learning Rate: 5.1367e-05
	LOSS [training: 3.037640231230849 | validation: 1.3696629388445782]
	TIME [epoch: 8.6 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0389885557768266		[learning rate: 5.1181e-05]
	Learning Rate: 5.11806e-05
	LOSS [training: 3.0389885557768266 | validation: 1.3643490118824024]
	TIME [epoch: 8.6 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0380998847238674		[learning rate: 5.0995e-05]
	Learning Rate: 5.09949e-05
	LOSS [training: 3.0380998847238674 | validation: 1.3662567922572961]
	TIME [epoch: 8.62 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0396569581379715		[learning rate: 5.081e-05]
	Learning Rate: 5.08098e-05
	LOSS [training: 3.0396569581379715 | validation: 1.3597189957266842]
	TIME [epoch: 8.61 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0375473265237103		[learning rate: 5.0625e-05]
	Learning Rate: 5.06254e-05
	LOSS [training: 3.0375473265237103 | validation: 1.3636565402160565]
	TIME [epoch: 8.6 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0407953204981872		[learning rate: 5.0442e-05]
	Learning Rate: 5.04417e-05
	LOSS [training: 3.0407953204981872 | validation: 1.3746788485020658]
	TIME [epoch: 8.61 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038666804917189		[learning rate: 5.0259e-05]
	Learning Rate: 5.02586e-05
	LOSS [training: 3.038666804917189 | validation: 1.3645322809233886]
	TIME [epoch: 8.61 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0391988678979978		[learning rate: 5.0076e-05]
	Learning Rate: 5.00762e-05
	LOSS [training: 3.0391988678979978 | validation: 1.364153435411902]
	TIME [epoch: 8.6 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0397431785139473		[learning rate: 4.9894e-05]
	Learning Rate: 4.98945e-05
	LOSS [training: 3.0397431785139473 | validation: 1.3603455376790612]
	TIME [epoch: 8.6 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0402388576759964		[learning rate: 4.9713e-05]
	Learning Rate: 4.97134e-05
	LOSS [training: 3.0402388576759964 | validation: 1.3775922131998999]
	TIME [epoch: 8.62 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039892250674078		[learning rate: 4.9533e-05]
	Learning Rate: 4.9533e-05
	LOSS [training: 3.039892250674078 | validation: 1.3721517796645786]
	TIME [epoch: 8.61 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0389783623018403		[learning rate: 4.9353e-05]
	Learning Rate: 4.93533e-05
	LOSS [training: 3.0389783623018403 | validation: 1.3713346587773463]
	TIME [epoch: 8.6 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039779973695416		[learning rate: 4.9174e-05]
	Learning Rate: 4.91742e-05
	LOSS [training: 3.039779973695416 | validation: 1.3646705440083309]
	TIME [epoch: 8.61 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034715915829323		[learning rate: 4.8996e-05]
	Learning Rate: 4.89957e-05
	LOSS [training: 3.034715915829323 | validation: 1.3624898856206162]
	TIME [epoch: 8.62 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0351648452122353		[learning rate: 4.8818e-05]
	Learning Rate: 4.88179e-05
	LOSS [training: 3.0351648452122353 | validation: 1.3618142244125888]
	TIME [epoch: 8.61 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0444563954000365		[learning rate: 4.8641e-05]
	Learning Rate: 4.86407e-05
	LOSS [training: 3.0444563954000365 | validation: 1.3613386824370366]
	TIME [epoch: 8.61 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0355152005760084		[learning rate: 4.8464e-05]
	Learning Rate: 4.84642e-05
	LOSS [training: 3.0355152005760084 | validation: 1.372905952105773]
	TIME [epoch: 8.61 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0407357708162213		[learning rate: 4.8288e-05]
	Learning Rate: 4.82883e-05
	LOSS [training: 3.0407357708162213 | validation: 1.3773711061548155]
	TIME [epoch: 8.62 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04090781522082		[learning rate: 4.8113e-05]
	Learning Rate: 4.81131e-05
	LOSS [training: 3.04090781522082 | validation: 1.3745219105279656]
	TIME [epoch: 8.61 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038071291717153		[learning rate: 4.7938e-05]
	Learning Rate: 4.79385e-05
	LOSS [training: 3.038071291717153 | validation: 1.3652749958384316]
	TIME [epoch: 8.6 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0371424164008283		[learning rate: 4.7765e-05]
	Learning Rate: 4.77645e-05
	LOSS [training: 3.0371424164008283 | validation: 1.3626097920070697]
	TIME [epoch: 8.6 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0372196802496516		[learning rate: 4.7591e-05]
	Learning Rate: 4.75912e-05
	LOSS [training: 3.0372196802496516 | validation: 1.3702473338235746]
	TIME [epoch: 8.63 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0400742464150126		[learning rate: 4.7418e-05]
	Learning Rate: 4.74185e-05
	LOSS [training: 3.0400742464150126 | validation: 1.3743806395434]
	TIME [epoch: 8.6 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0410964197442674		[learning rate: 4.7246e-05]
	Learning Rate: 4.72464e-05
	LOSS [training: 3.0410964197442674 | validation: 1.3595488111281315]
	TIME [epoch: 8.6 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0402871719981612		[learning rate: 4.7075e-05]
	Learning Rate: 4.70749e-05
	LOSS [training: 3.0402871719981612 | validation: 1.3571444236161625]
	TIME [epoch: 8.6 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0385023222931187		[learning rate: 4.6904e-05]
	Learning Rate: 4.69041e-05
	LOSS [training: 3.0385023222931187 | validation: 1.363778398136031]
	TIME [epoch: 8.62 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0394534575730145		[learning rate: 4.6734e-05]
	Learning Rate: 4.67339e-05
	LOSS [training: 3.0394534575730145 | validation: 1.3827080629893405]
	TIME [epoch: 8.6 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0367546790513873		[learning rate: 4.6564e-05]
	Learning Rate: 4.65643e-05
	LOSS [training: 3.0367546790513873 | validation: 1.36827160014837]
	TIME [epoch: 8.6 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0366423657287904		[learning rate: 4.6395e-05]
	Learning Rate: 4.63953e-05
	LOSS [training: 3.0366423657287904 | validation: 1.3670606189153025]
	TIME [epoch: 8.62 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0424995464446756		[learning rate: 4.6227e-05]
	Learning Rate: 4.62269e-05
	LOSS [training: 3.0424995464446756 | validation: 1.3684194500881475]
	TIME [epoch: 8.62 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0408694956110613		[learning rate: 4.6059e-05]
	Learning Rate: 4.60591e-05
	LOSS [training: 3.0408694956110613 | validation: 1.374889386082697]
	TIME [epoch: 8.61 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0415054620657225		[learning rate: 4.5892e-05]
	Learning Rate: 4.5892e-05
	LOSS [training: 3.0415054620657225 | validation: 1.3687177034580151]
	TIME [epoch: 8.6 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0373988316871006		[learning rate: 4.5725e-05]
	Learning Rate: 4.57254e-05
	LOSS [training: 3.0373988316871006 | validation: 1.369144202197048]
	TIME [epoch: 8.63 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0373419420444994		[learning rate: 4.556e-05]
	Learning Rate: 4.55595e-05
	LOSS [training: 3.0373419420444994 | validation: 1.3602926510305386]
	TIME [epoch: 8.62 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0379036405350286		[learning rate: 4.5394e-05]
	Learning Rate: 4.53942e-05
	LOSS [training: 3.0379036405350286 | validation: 1.3727616755955327]
	TIME [epoch: 8.61 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0394320183078323		[learning rate: 4.5229e-05]
	Learning Rate: 4.52294e-05
	LOSS [training: 3.0394320183078323 | validation: 1.3591977879742965]
	TIME [epoch: 8.6 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039772067180592		[learning rate: 4.5065e-05]
	Learning Rate: 4.50653e-05
	LOSS [training: 3.039772067180592 | validation: 1.371884081649781]
	TIME [epoch: 8.62 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0345889017840006		[learning rate: 4.4902e-05]
	Learning Rate: 4.49017e-05
	LOSS [training: 3.0345889017840006 | validation: 1.378116779267836]
	TIME [epoch: 8.61 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037889480149671		[learning rate: 4.4739e-05]
	Learning Rate: 4.47388e-05
	LOSS [training: 3.037889480149671 | validation: 1.3605435987867052]
	TIME [epoch: 8.61 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0362611620442688		[learning rate: 4.4576e-05]
	Learning Rate: 4.45764e-05
	LOSS [training: 3.0362611620442688 | validation: 1.369865006949092]
	TIME [epoch: 8.6 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038105001993414		[learning rate: 4.4415e-05]
	Learning Rate: 4.44147e-05
	LOSS [training: 3.038105001993414 | validation: 1.375509958628611]
	TIME [epoch: 8.62 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0361866866884566		[learning rate: 4.4253e-05]
	Learning Rate: 4.42535e-05
	LOSS [training: 3.0361866866884566 | validation: 1.3595777095490447]
	TIME [epoch: 8.6 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0317399785239694		[learning rate: 4.4093e-05]
	Learning Rate: 4.40929e-05
	LOSS [training: 3.0317399785239694 | validation: 1.3646318589077207]
	TIME [epoch: 8.6 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0351342725684516		[learning rate: 4.3933e-05]
	Learning Rate: 4.39329e-05
	LOSS [training: 3.0351342725684516 | validation: 1.3581263607459462]
	TIME [epoch: 8.6 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036964196709249		[learning rate: 4.3773e-05]
	Learning Rate: 4.37734e-05
	LOSS [training: 3.036964196709249 | validation: 1.3709675827069097]
	TIME [epoch: 8.62 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0365635075922275		[learning rate: 4.3615e-05]
	Learning Rate: 4.36146e-05
	LOSS [training: 3.0365635075922275 | validation: 1.3705678904084306]
	TIME [epoch: 8.6 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037478365571908		[learning rate: 4.3456e-05]
	Learning Rate: 4.34563e-05
	LOSS [training: 3.037478365571908 | validation: 1.3645404191099648]
	TIME [epoch: 8.6 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0379874113969807		[learning rate: 4.3299e-05]
	Learning Rate: 4.32986e-05
	LOSS [training: 3.0379874113969807 | validation: 1.3684042586947331]
	TIME [epoch: 8.61 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0348853393749473		[learning rate: 4.3141e-05]
	Learning Rate: 4.31415e-05
	LOSS [training: 3.0348853393749473 | validation: 1.3611241543472365]
	TIME [epoch: 8.62 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0391454661236885		[learning rate: 4.2985e-05]
	Learning Rate: 4.29849e-05
	LOSS [training: 3.0391454661236885 | validation: 1.3674096881040674]
	TIME [epoch: 8.6 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035435009304131		[learning rate: 4.2829e-05]
	Learning Rate: 4.28289e-05
	LOSS [training: 3.035435009304131 | validation: 1.367967464914763]
	TIME [epoch: 8.6 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0408696550883074		[learning rate: 4.2673e-05]
	Learning Rate: 4.26735e-05
	LOSS [training: 3.0408696550883074 | validation: 1.3690023645721252]
	TIME [epoch: 8.62 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036247818504537		[learning rate: 4.2519e-05]
	Learning Rate: 4.25186e-05
	LOSS [training: 3.036247818504537 | validation: 1.3620816202452868]
	TIME [epoch: 8.62 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0409150080975307		[learning rate: 4.2364e-05]
	Learning Rate: 4.23643e-05
	LOSS [training: 3.0409150080975307 | validation: 1.3712648220276182]
	TIME [epoch: 8.6 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0389172932531903		[learning rate: 4.2211e-05]
	Learning Rate: 4.22106e-05
	LOSS [training: 3.0389172932531903 | validation: 1.368407817616015]
	TIME [epoch: 8.6 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036906720171045		[learning rate: 4.2057e-05]
	Learning Rate: 4.20574e-05
	LOSS [training: 3.036906720171045 | validation: 1.366165033669331]
	TIME [epoch: 8.61 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0396817263534954		[learning rate: 4.1905e-05]
	Learning Rate: 4.19047e-05
	LOSS [training: 3.0396817263534954 | validation: 1.3559400933888868]
	TIME [epoch: 8.61 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0415437229315114		[learning rate: 4.1753e-05]
	Learning Rate: 4.17527e-05
	LOSS [training: 3.0415437229315114 | validation: 1.3660500019683768]
	TIME [epoch: 8.6 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0377033606083588		[learning rate: 4.1601e-05]
	Learning Rate: 4.16012e-05
	LOSS [training: 3.0377033606083588 | validation: 1.3655144866975368]
	TIME [epoch: 8.6 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0366088396539843		[learning rate: 4.145e-05]
	Learning Rate: 4.14502e-05
	LOSS [training: 3.0366088396539843 | validation: 1.3757476690567931]
	TIME [epoch: 8.62 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0400228992972296		[learning rate: 4.13e-05]
	Learning Rate: 4.12998e-05
	LOSS [training: 3.0400228992972296 | validation: 1.3719734290232162]
	TIME [epoch: 8.62 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036328883833412		[learning rate: 4.115e-05]
	Learning Rate: 4.11499e-05
	LOSS [training: 3.036328883833412 | validation: 1.3656437193000774]
	TIME [epoch: 8.6 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0383052918771623		[learning rate: 4.1001e-05]
	Learning Rate: 4.10005e-05
	LOSS [training: 3.0383052918771623 | validation: 1.3667900375819784]
	TIME [epoch: 8.6 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.041623050523932		[learning rate: 4.0852e-05]
	Learning Rate: 4.08517e-05
	LOSS [training: 3.041623050523932 | validation: 1.3729886291070343]
	TIME [epoch: 8.63 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.033675447043516		[learning rate: 4.0703e-05]
	Learning Rate: 4.07035e-05
	LOSS [training: 3.033675447043516 | validation: 1.3717232749022357]
	TIME [epoch: 8.6 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0371233159463213		[learning rate: 4.0556e-05]
	Learning Rate: 4.05558e-05
	LOSS [training: 3.0371233159463213 | validation: 1.361159206979139]
	TIME [epoch: 8.6 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038936281240299		[learning rate: 4.0409e-05]
	Learning Rate: 4.04086e-05
	LOSS [training: 3.038936281240299 | validation: 1.3769504463130593]
	TIME [epoch: 8.6 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043808452600435		[learning rate: 4.0262e-05]
	Learning Rate: 4.0262e-05
	LOSS [training: 3.043808452600435 | validation: 1.3768258045814386]
	TIME [epoch: 8.62 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037627016819159		[learning rate: 4.0116e-05]
	Learning Rate: 4.01158e-05
	LOSS [training: 3.037627016819159 | validation: 1.3627452265607856]
	TIME [epoch: 8.61 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039225125961398		[learning rate: 3.997e-05]
	Learning Rate: 3.99703e-05
	LOSS [training: 3.039225125961398 | validation: 1.3636603563416778]
	TIME [epoch: 8.61 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0387933293596894		[learning rate: 3.9825e-05]
	Learning Rate: 3.98252e-05
	LOSS [training: 3.0387933293596894 | validation: 1.3591419539008307]
	TIME [epoch: 8.61 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0440798805382943		[learning rate: 3.9681e-05]
	Learning Rate: 3.96807e-05
	LOSS [training: 3.0440798805382943 | validation: 1.3645868854509071]
	TIME [epoch: 8.62 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0365467511731166		[learning rate: 3.9537e-05]
	Learning Rate: 3.95367e-05
	LOSS [training: 3.0365467511731166 | validation: 1.353434269635861]
	TIME [epoch: 8.6 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0398967870640563		[learning rate: 3.9393e-05]
	Learning Rate: 3.93932e-05
	LOSS [training: 3.0398967870640563 | validation: 1.3546040566670732]
	TIME [epoch: 8.6 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035022407981641		[learning rate: 3.925e-05]
	Learning Rate: 3.92502e-05
	LOSS [training: 3.035022407981641 | validation: 1.365059713966338]
	TIME [epoch: 8.6 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0361964431679964		[learning rate: 3.9108e-05]
	Learning Rate: 3.91078e-05
	LOSS [training: 3.0361964431679964 | validation: 1.3728729673393993]
	TIME [epoch: 8.62 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0396210454366726		[learning rate: 3.8966e-05]
	Learning Rate: 3.89659e-05
	LOSS [training: 3.0396210454366726 | validation: 1.3767447306831027]
	TIME [epoch: 8.59 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0392392452722232		[learning rate: 3.8824e-05]
	Learning Rate: 3.88245e-05
	LOSS [training: 3.0392392452722232 | validation: 1.3688057707826107]
	TIME [epoch: 8.61 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0378148003433845		[learning rate: 3.8684e-05]
	Learning Rate: 3.86836e-05
	LOSS [training: 3.0378148003433845 | validation: 1.3781546868800512]
	TIME [epoch: 8.62 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0393324814590676		[learning rate: 3.8543e-05]
	Learning Rate: 3.85432e-05
	LOSS [training: 3.0393324814590676 | validation: 1.372860473024693]
	TIME [epoch: 8.63 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036167866988447		[learning rate: 3.8403e-05]
	Learning Rate: 3.84033e-05
	LOSS [training: 3.036167866988447 | validation: 1.3706849082355763]
	TIME [epoch: 8.61 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0388710090041364		[learning rate: 3.8264e-05]
	Learning Rate: 3.82639e-05
	LOSS [training: 3.0388710090041364 | validation: 1.3607633283039713]
	TIME [epoch: 8.6 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0417753766880034		[learning rate: 3.8125e-05]
	Learning Rate: 3.81251e-05
	LOSS [training: 3.0417753766880034 | validation: 1.362211644127518]
	TIME [epoch: 8.62 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0398703089924104		[learning rate: 3.7987e-05]
	Learning Rate: 3.79867e-05
	LOSS [training: 3.0398703089924104 | validation: 1.3764443421720503]
	TIME [epoch: 8.6 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0372984235927962		[learning rate: 3.7849e-05]
	Learning Rate: 3.78489e-05
	LOSS [training: 3.0372984235927962 | validation: 1.3745734344950904]
	TIME [epoch: 8.6 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0327292069048535		[learning rate: 3.7711e-05]
	Learning Rate: 3.77115e-05
	LOSS [training: 3.0327292069048535 | validation: 1.358516789189363]
	TIME [epoch: 8.6 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0385255357748573		[learning rate: 3.7575e-05]
	Learning Rate: 3.75746e-05
	LOSS [training: 3.0385255357748573 | validation: 1.3713289234733526]
	TIME [epoch: 8.62 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0391444553427642		[learning rate: 3.7438e-05]
	Learning Rate: 3.74383e-05
	LOSS [training: 3.0391444553427642 | validation: 1.3576674537865623]
	TIME [epoch: 8.6 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039951240162613		[learning rate: 3.7302e-05]
	Learning Rate: 3.73024e-05
	LOSS [training: 3.039951240162613 | validation: 1.369019799405834]
	TIME [epoch: 8.61 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037852391968765		[learning rate: 3.7167e-05]
	Learning Rate: 3.7167e-05
	LOSS [training: 3.037852391968765 | validation: 1.3511180291880653]
	TIME [epoch: 8.6 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0396826642948573		[learning rate: 3.7032e-05]
	Learning Rate: 3.70322e-05
	LOSS [training: 3.0396826642948573 | validation: 1.359269243956831]
	TIME [epoch: 8.62 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0396740752908373		[learning rate: 3.6898e-05]
	Learning Rate: 3.68978e-05
	LOSS [training: 3.0396740752908373 | validation: 1.373884160395772]
	TIME [epoch: 8.6 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0409490736290063		[learning rate: 3.6764e-05]
	Learning Rate: 3.67639e-05
	LOSS [training: 3.0409490736290063 | validation: 1.3739718801682483]
	TIME [epoch: 8.61 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0378269653399363		[learning rate: 3.663e-05]
	Learning Rate: 3.66304e-05
	LOSS [training: 3.0378269653399363 | validation: 1.3619986923218108]
	TIME [epoch: 8.61 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0411162331262025		[learning rate: 3.6498e-05]
	Learning Rate: 3.64975e-05
	LOSS [training: 3.0411162331262025 | validation: 1.3652122155914177]
	TIME [epoch: 8.63 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.033906332217776		[learning rate: 3.6365e-05]
	Learning Rate: 3.63651e-05
	LOSS [training: 3.033906332217776 | validation: 1.3616023190953697]
	TIME [epoch: 8.6 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0376108970557922		[learning rate: 3.6233e-05]
	Learning Rate: 3.62331e-05
	LOSS [training: 3.0376108970557922 | validation: 1.3676305934615773]
	TIME [epoch: 8.61 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0350458123316844		[learning rate: 3.6102e-05]
	Learning Rate: 3.61016e-05
	LOSS [training: 3.0350458123316844 | validation: 1.367512053975069]
	TIME [epoch: 8.6 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.040695361956041		[learning rate: 3.5971e-05]
	Learning Rate: 3.59706e-05
	LOSS [training: 3.040695361956041 | validation: 1.3640897652675088]
	TIME [epoch: 8.62 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039198982654698		[learning rate: 3.584e-05]
	Learning Rate: 3.584e-05
	LOSS [training: 3.039198982654698 | validation: 1.3641999690193525]
	TIME [epoch: 8.6 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0352425012822115		[learning rate: 3.571e-05]
	Learning Rate: 3.571e-05
	LOSS [training: 3.0352425012822115 | validation: 1.3727417155868593]
	TIME [epoch: 8.6 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0339891185841306		[learning rate: 3.558e-05]
	Learning Rate: 3.55804e-05
	LOSS [training: 3.0339891185841306 | validation: 1.3659438447832315]
	TIME [epoch: 8.62 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0404530351685897		[learning rate: 3.5451e-05]
	Learning Rate: 3.54513e-05
	LOSS [training: 3.0404530351685897 | validation: 1.3603021150817654]
	TIME [epoch: 8.61 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037820144617567		[learning rate: 3.5323e-05]
	Learning Rate: 3.53226e-05
	LOSS [training: 3.037820144617567 | validation: 1.3565006321380904]
	TIME [epoch: 8.6 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0429519450948823		[learning rate: 3.5194e-05]
	Learning Rate: 3.51944e-05
	LOSS [training: 3.0429519450948823 | validation: 1.361738879189484]
	TIME [epoch: 8.6 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0358683439232186		[learning rate: 3.5067e-05]
	Learning Rate: 3.50667e-05
	LOSS [training: 3.0358683439232186 | validation: 1.3552610803976695]
	TIME [epoch: 8.62 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0350981142940165		[learning rate: 3.4939e-05]
	Learning Rate: 3.49394e-05
	LOSS [training: 3.0350981142940165 | validation: 1.3722708324903186]
	TIME [epoch: 8.61 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0393969164692343		[learning rate: 3.4813e-05]
	Learning Rate: 3.48126e-05
	LOSS [training: 3.0393969164692343 | validation: 1.3754532695507589]
	TIME [epoch: 8.6 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0412079894268023		[learning rate: 3.4686e-05]
	Learning Rate: 3.46863e-05
	LOSS [training: 3.0412079894268023 | validation: 1.374748537955669]
	TIME [epoch: 8.6 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037048947138219		[learning rate: 3.456e-05]
	Learning Rate: 3.45604e-05
	LOSS [training: 3.037048947138219 | validation: 1.3707510962969505]
	TIME [epoch: 8.62 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035257357591386		[learning rate: 3.4435e-05]
	Learning Rate: 3.4435e-05
	LOSS [training: 3.035257357591386 | validation: 1.368361820137082]
	TIME [epoch: 8.6 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037998851132825		[learning rate: 3.431e-05]
	Learning Rate: 3.431e-05
	LOSS [training: 3.037998851132825 | validation: 1.3613001353293086]
	TIME [epoch: 8.6 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037584572942923		[learning rate: 3.4186e-05]
	Learning Rate: 3.41855e-05
	LOSS [training: 3.037584572942923 | validation: 1.363874519275865]
	TIME [epoch: 8.6 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0358571244174706		[learning rate: 3.4061e-05]
	Learning Rate: 3.40615e-05
	LOSS [training: 3.0358571244174706 | validation: 1.3658788118840643]
	TIME [epoch: 8.62 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0343673404500673		[learning rate: 3.3938e-05]
	Learning Rate: 3.39378e-05
	LOSS [training: 3.0343673404500673 | validation: 1.3596740086724726]
	TIME [epoch: 8.6 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0364318481258388		[learning rate: 3.3815e-05]
	Learning Rate: 3.38147e-05
	LOSS [training: 3.0364318481258388 | validation: 1.3658551894397897]
	TIME [epoch: 8.6 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0382810343867233		[learning rate: 3.3692e-05]
	Learning Rate: 3.3692e-05
	LOSS [training: 3.0382810343867233 | validation: 1.3666282398902596]
	TIME [epoch: 8.61 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037942453193285		[learning rate: 3.357e-05]
	Learning Rate: 3.35697e-05
	LOSS [training: 3.037942453193285 | validation: 1.3607271800175593]
	TIME [epoch: 8.63 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.033375472085681		[learning rate: 3.3448e-05]
	Learning Rate: 3.34479e-05
	LOSS [training: 3.033375472085681 | validation: 1.3674648848128046]
	TIME [epoch: 8.6 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0411357173536575		[learning rate: 3.3326e-05]
	Learning Rate: 3.33265e-05
	LOSS [training: 3.0411357173536575 | validation: 1.3649535417752459]
	TIME [epoch: 8.59 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035485829084175		[learning rate: 3.3206e-05]
	Learning Rate: 3.32055e-05
	LOSS [training: 3.035485829084175 | validation: 1.3718521839405509]
	TIME [epoch: 8.6 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0372798482813446		[learning rate: 3.3085e-05]
	Learning Rate: 3.3085e-05
	LOSS [training: 3.0372798482813446 | validation: 1.3602136509480494]
	TIME [epoch: 8.62 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038208621901319		[learning rate: 3.2965e-05]
	Learning Rate: 3.2965e-05
	LOSS [training: 3.038208621901319 | validation: 1.3617883684062524]
	TIME [epoch: 8.61 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039631934550526		[learning rate: 3.2845e-05]
	Learning Rate: 3.28453e-05
	LOSS [training: 3.039631934550526 | validation: 1.3743095323798147]
	TIME [epoch: 8.6 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0377970075844707		[learning rate: 3.2726e-05]
	Learning Rate: 3.27261e-05
	LOSS [training: 3.0377970075844707 | validation: 1.3679933493831433]
	TIME [epoch: 8.61 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.032425459074733		[learning rate: 3.2607e-05]
	Learning Rate: 3.26074e-05
	LOSS [training: 3.032425459074733 | validation: 1.363352632993371]
	TIME [epoch: 8.62 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034132352472508		[learning rate: 3.2489e-05]
	Learning Rate: 3.2489e-05
	LOSS [training: 3.034132352472508 | validation: 1.3607630539719586]
	TIME [epoch: 8.6 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03672540959968		[learning rate: 3.2371e-05]
	Learning Rate: 3.23711e-05
	LOSS [training: 3.03672540959968 | validation: 1.3698056736179236]
	TIME [epoch: 8.6 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037882032862664		[learning rate: 3.2254e-05]
	Learning Rate: 3.22537e-05
	LOSS [training: 3.037882032862664 | validation: 1.3660638126220177]
	TIME [epoch: 8.61 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038586187081624		[learning rate: 3.2137e-05]
	Learning Rate: 3.21366e-05
	LOSS [training: 3.038586187081624 | validation: 1.3695865547228097]
	TIME [epoch: 8.62 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0365803783013883		[learning rate: 3.202e-05]
	Learning Rate: 3.202e-05
	LOSS [training: 3.0365803783013883 | validation: 1.3566208009122172]
	TIME [epoch: 8.61 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036613792938787		[learning rate: 3.1904e-05]
	Learning Rate: 3.19038e-05
	LOSS [training: 3.036613792938787 | validation: 1.3631417466556945]
	TIME [epoch: 8.6 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034908098878632		[learning rate: 3.1788e-05]
	Learning Rate: 3.1788e-05
	LOSS [training: 3.034908098878632 | validation: 1.3698321019691848]
	TIME [epoch: 8.63 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0368431810299565		[learning rate: 3.1673e-05]
	Learning Rate: 3.16726e-05
	LOSS [training: 3.0368431810299565 | validation: 1.3678313195539369]
	TIME [epoch: 8.61 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039054850312632		[learning rate: 3.1558e-05]
	Learning Rate: 3.15577e-05
	LOSS [training: 3.039054850312632 | validation: 1.3782856145047422]
	TIME [epoch: 8.61 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0354759421196102		[learning rate: 3.1443e-05]
	Learning Rate: 3.14432e-05
	LOSS [training: 3.0354759421196102 | validation: 1.3714398433268973]
	TIME [epoch: 8.61 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039203533282989		[learning rate: 3.1329e-05]
	Learning Rate: 3.13291e-05
	LOSS [training: 3.039203533282989 | validation: 1.3792508968521375]
	TIME [epoch: 8.62 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0381455380956526		[learning rate: 3.1215e-05]
	Learning Rate: 3.12154e-05
	LOSS [training: 3.0381455380956526 | validation: 1.3625534976632676]
	TIME [epoch: 8.61 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039704789168043		[learning rate: 3.1102e-05]
	Learning Rate: 3.11021e-05
	LOSS [training: 3.039704789168043 | validation: 1.3648320867413428]
	TIME [epoch: 8.6 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.04024739680417		[learning rate: 3.0989e-05]
	Learning Rate: 3.09892e-05
	LOSS [training: 3.04024739680417 | validation: 1.3692586311892683]
	TIME [epoch: 8.6 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039414697945781		[learning rate: 3.0877e-05]
	Learning Rate: 3.08768e-05
	LOSS [training: 3.039414697945781 | validation: 1.3757908881629362]
	TIME [epoch: 8.62 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0380476072276497		[learning rate: 3.0765e-05]
	Learning Rate: 3.07647e-05
	LOSS [training: 3.0380476072276497 | validation: 1.3668996602451762]
	TIME [epoch: 8.6 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0394399694429675		[learning rate: 3.0653e-05]
	Learning Rate: 3.0653e-05
	LOSS [training: 3.0394399694429675 | validation: 1.359425381052455]
	TIME [epoch: 8.6 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037497558077723		[learning rate: 3.0542e-05]
	Learning Rate: 3.05418e-05
	LOSS [training: 3.037497558077723 | validation: 1.36290001773184]
	TIME [epoch: 8.6 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0377049668270235		[learning rate: 3.0431e-05]
	Learning Rate: 3.0431e-05
	LOSS [training: 3.0377049668270235 | validation: 1.3668872752363612]
	TIME [epoch: 8.63 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0378255129859015		[learning rate: 3.0321e-05]
	Learning Rate: 3.03205e-05
	LOSS [training: 3.0378255129859015 | validation: 1.3663870977091586]
	TIME [epoch: 8.6 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036477902796127		[learning rate: 3.0211e-05]
	Learning Rate: 3.02105e-05
	LOSS [training: 3.036477902796127 | validation: 1.3640428763515253]
	TIME [epoch: 8.6 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038478526354844		[learning rate: 3.0101e-05]
	Learning Rate: 3.01009e-05
	LOSS [training: 3.038478526354844 | validation: 1.3674440168227748]
	TIME [epoch: 8.6 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0366436286196623		[learning rate: 2.9992e-05]
	Learning Rate: 2.99916e-05
	LOSS [training: 3.0366436286196623 | validation: 1.3815642218687676]
	TIME [epoch: 8.62 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038204963193622		[learning rate: 2.9883e-05]
	Learning Rate: 2.98828e-05
	LOSS [training: 3.038204963193622 | validation: 1.3705643467012882]
	TIME [epoch: 8.6 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039491309238391		[learning rate: 2.9774e-05]
	Learning Rate: 2.97743e-05
	LOSS [training: 3.039491309238391 | validation: 1.378851360253365]
	TIME [epoch: 8.6 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0368177286232214		[learning rate: 2.9666e-05]
	Learning Rate: 2.96663e-05
	LOSS [training: 3.0368177286232214 | validation: 1.3768109683908478]
	TIME [epoch: 8.62 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0376574406155945		[learning rate: 2.9559e-05]
	Learning Rate: 2.95586e-05
	LOSS [training: 3.0376574406155945 | validation: 1.3692135382164863]
	TIME [epoch: 8.61 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0359113079999958		[learning rate: 2.9451e-05]
	Learning Rate: 2.94514e-05
	LOSS [training: 3.0359113079999958 | validation: 1.3560909361914473]
	TIME [epoch: 8.61 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0385600175456395		[learning rate: 2.9344e-05]
	Learning Rate: 2.93445e-05
	LOSS [training: 3.0385600175456395 | validation: 1.3668269475157053]
	TIME [epoch: 8.6 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0360874112527236		[learning rate: 2.9238e-05]
	Learning Rate: 2.9238e-05
	LOSS [training: 3.0360874112527236 | validation: 1.370057047248216]
	TIME [epoch: 8.61 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0339054808953065		[learning rate: 2.9132e-05]
	Learning Rate: 2.91319e-05
	LOSS [training: 3.0339054808953065 | validation: 1.362929522256277]
	TIME [epoch: 8.61 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0390880531094164		[learning rate: 2.9026e-05]
	Learning Rate: 2.90262e-05
	LOSS [training: 3.0390880531094164 | validation: 1.375057230259333]
	TIME [epoch: 8.6 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038554902795508		[learning rate: 2.8921e-05]
	Learning Rate: 2.89208e-05
	LOSS [training: 3.038554902795508 | validation: 1.3620015393002125]
	TIME [epoch: 8.61 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.032340385142557		[learning rate: 2.8816e-05]
	Learning Rate: 2.88159e-05
	LOSS [training: 3.032340385142557 | validation: 1.3772588783158073]
	TIME [epoch: 8.63 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03501497679849		[learning rate: 2.8711e-05]
	Learning Rate: 2.87113e-05
	LOSS [training: 3.03501497679849 | validation: 1.37302555413537]
	TIME [epoch: 8.61 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.032982013108132		[learning rate: 2.8607e-05]
	Learning Rate: 2.86071e-05
	LOSS [training: 3.032982013108132 | validation: 1.3648689063372215]
	TIME [epoch: 8.6 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036396185669628		[learning rate: 2.8503e-05]
	Learning Rate: 2.85033e-05
	LOSS [training: 3.036396185669628 | validation: 1.3625101360354699]
	TIME [epoch: 8.6 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038399289041459		[learning rate: 2.84e-05]
	Learning Rate: 2.83998e-05
	LOSS [training: 3.038399289041459 | validation: 1.3743429676506513]
	TIME [epoch: 8.62 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0354354086415882		[learning rate: 2.8297e-05]
	Learning Rate: 2.82968e-05
	LOSS [training: 3.0354354086415882 | validation: 1.3753849318600473]
	TIME [epoch: 8.61 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.032311914971375		[learning rate: 2.8194e-05]
	Learning Rate: 2.81941e-05
	LOSS [training: 3.032311914971375 | validation: 1.369410585544117]
	TIME [epoch: 8.61 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03777847426524		[learning rate: 2.8092e-05]
	Learning Rate: 2.80918e-05
	LOSS [training: 3.03777847426524 | validation: 1.376623943339517]
	TIME [epoch: 8.6 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0402435819036553		[learning rate: 2.799e-05]
	Learning Rate: 2.79898e-05
	LOSS [training: 3.0402435819036553 | validation: 1.3759595523038328]
	TIME [epoch: 8.63 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.047793032243082		[learning rate: 2.7888e-05]
	Learning Rate: 2.78882e-05
	LOSS [training: 3.047793032243082 | validation: 1.3737798608120149]
	TIME [epoch: 8.6 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0404286386569406		[learning rate: 2.7787e-05]
	Learning Rate: 2.7787e-05
	LOSS [training: 3.0404286386569406 | validation: 1.3832466577285403]
	TIME [epoch: 8.6 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038964037404754		[learning rate: 2.7686e-05]
	Learning Rate: 2.76862e-05
	LOSS [training: 3.038964037404754 | validation: 1.3785591334504124]
	TIME [epoch: 8.61 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036858526559688		[learning rate: 2.7586e-05]
	Learning Rate: 2.75857e-05
	LOSS [training: 3.036858526559688 | validation: 1.3707377582473719]
	TIME [epoch: 8.63 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0356869624424823		[learning rate: 2.7486e-05]
	Learning Rate: 2.74856e-05
	LOSS [training: 3.0356869624424823 | validation: 1.3866038063219992]
	TIME [epoch: 8.61 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037298778909244		[learning rate: 2.7386e-05]
	Learning Rate: 2.73859e-05
	LOSS [training: 3.037298778909244 | validation: 1.378911717836313]
	TIME [epoch: 8.6 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.040239164445796		[learning rate: 2.7286e-05]
	Learning Rate: 2.72865e-05
	LOSS [training: 3.040239164445796 | validation: 1.3766653678201568]
	TIME [epoch: 8.61 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037689879355996		[learning rate: 2.7187e-05]
	Learning Rate: 2.71875e-05
	LOSS [training: 3.037689879355996 | validation: 1.3708154893877798]
	TIME [epoch: 8.62 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0410224486267654		[learning rate: 2.7089e-05]
	Learning Rate: 2.70888e-05
	LOSS [training: 3.0410224486267654 | validation: 1.3782545067704222]
	TIME [epoch: 8.6 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0362630137755895		[learning rate: 2.699e-05]
	Learning Rate: 2.69905e-05
	LOSS [training: 3.0362630137755895 | validation: 1.3648378104946695]
	TIME [epoch: 8.61 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0354021380034504		[learning rate: 2.6893e-05]
	Learning Rate: 2.68925e-05
	LOSS [training: 3.0354021380034504 | validation: 1.3629186924283396]
	TIME [epoch: 8.61 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0377223025393962		[learning rate: 2.6795e-05]
	Learning Rate: 2.67949e-05
	LOSS [training: 3.0377223025393962 | validation: 1.369072536900485]
	TIME [epoch: 8.62 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034386289108363		[learning rate: 2.6698e-05]
	Learning Rate: 2.66977e-05
	LOSS [training: 3.034386289108363 | validation: 1.3760190297574986]
	TIME [epoch: 8.61 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0373607280828017		[learning rate: 2.6601e-05]
	Learning Rate: 2.66008e-05
	LOSS [training: 3.0373607280828017 | validation: 1.3623376690491034]
	TIME [epoch: 8.6 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0376441010797497		[learning rate: 2.6504e-05]
	Learning Rate: 2.65043e-05
	LOSS [training: 3.0376441010797497 | validation: 1.3689412700814703]
	TIME [epoch: 8.62 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0418673190232672		[learning rate: 2.6408e-05]
	Learning Rate: 2.64081e-05
	LOSS [training: 3.0418673190232672 | validation: 1.3796384608892773]
	TIME [epoch: 8.6 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0425239704118794		[learning rate: 2.6312e-05]
	Learning Rate: 2.63123e-05
	LOSS [training: 3.0425239704118794 | validation: 1.365830671970962]
	TIME [epoch: 8.6 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0395256505726125		[learning rate: 2.6217e-05]
	Learning Rate: 2.62168e-05
	LOSS [training: 3.0395256505726125 | validation: 1.3741537129580796]
	TIME [epoch: 8.6 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038717144657529		[learning rate: 2.6122e-05]
	Learning Rate: 2.61216e-05
	LOSS [training: 3.038717144657529 | validation: 1.3811688705016876]
	TIME [epoch: 8.62 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0377281466582042		[learning rate: 2.6027e-05]
	Learning Rate: 2.60268e-05
	LOSS [training: 3.0377281466582042 | validation: 1.3643735296243773]
	TIME [epoch: 8.61 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0346300941781146		[learning rate: 2.5932e-05]
	Learning Rate: 2.59324e-05
	LOSS [training: 3.0346300941781146 | validation: 1.3757839786614003]
	TIME [epoch: 8.6 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0360041238090347		[learning rate: 2.5838e-05]
	Learning Rate: 2.58383e-05
	LOSS [training: 3.0360041238090347 | validation: 1.3618315184854881]
	TIME [epoch: 8.6 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039193954294977		[learning rate: 2.5744e-05]
	Learning Rate: 2.57445e-05
	LOSS [training: 3.039193954294977 | validation: 1.3671790570470186]
	TIME [epoch: 8.62 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0365224905436543		[learning rate: 2.5651e-05]
	Learning Rate: 2.56511e-05
	LOSS [training: 3.0365224905436543 | validation: 1.3674797795405804]
	TIME [epoch: 8.61 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0341231195733847		[learning rate: 2.5558e-05]
	Learning Rate: 2.5558e-05
	LOSS [training: 3.0341231195733847 | validation: 1.3586879296798104]
	TIME [epoch: 8.6 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0364576616992176		[learning rate: 2.5465e-05]
	Learning Rate: 2.54652e-05
	LOSS [training: 3.0364576616992176 | validation: 1.3704159149276531]
	TIME [epoch: 8.6 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0360441035153274		[learning rate: 2.5373e-05]
	Learning Rate: 2.53728e-05
	LOSS [training: 3.0360441035153274 | validation: 1.3577002690291855]
	TIME [epoch: 8.62 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0354172930094983		[learning rate: 2.5281e-05]
	Learning Rate: 2.52807e-05
	LOSS [training: 3.0354172930094983 | validation: 1.3556813904183644]
	TIME [epoch: 8.6 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037740118161067		[learning rate: 2.5189e-05]
	Learning Rate: 2.5189e-05
	LOSS [training: 3.037740118161067 | validation: 1.3536974223906464]
	TIME [epoch: 8.61 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.033569273475654		[learning rate: 2.5098e-05]
	Learning Rate: 2.50976e-05
	LOSS [training: 3.033569273475654 | validation: 1.3690477991284187]
	TIME [epoch: 8.6 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037175313147776		[learning rate: 2.5006e-05]
	Learning Rate: 2.50065e-05
	LOSS [training: 3.037175313147776 | validation: 1.3714069172267207]
	TIME [epoch: 8.62 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036166919152209		[learning rate: 2.4916e-05]
	Learning Rate: 2.49157e-05
	LOSS [training: 3.036166919152209 | validation: 1.3616871705329572]
	TIME [epoch: 8.6 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036622348721718		[learning rate: 2.4825e-05]
	Learning Rate: 2.48253e-05
	LOSS [training: 3.036622348721718 | validation: 1.368619115332034]
	TIME [epoch: 8.61 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0387071273320103		[learning rate: 2.4735e-05]
	Learning Rate: 2.47352e-05
	LOSS [training: 3.0387071273320103 | validation: 1.3692010507055752]
	TIME [epoch: 8.62 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.033569896831799		[learning rate: 2.4645e-05]
	Learning Rate: 2.46455e-05
	LOSS [training: 3.033569896831799 | validation: 1.3718062414885477]
	TIME [epoch: 8.61 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.033853250240777		[learning rate: 2.4556e-05]
	Learning Rate: 2.4556e-05
	LOSS [training: 3.033853250240777 | validation: 1.374769975103796]
	TIME [epoch: 8.6 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036557277278961		[learning rate: 2.4467e-05]
	Learning Rate: 2.44669e-05
	LOSS [training: 3.036557277278961 | validation: 1.3610651858433815]
	TIME [epoch: 8.6 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0390838540612743		[learning rate: 2.4378e-05]
	Learning Rate: 2.43781e-05
	LOSS [training: 3.0390838540612743 | validation: 1.371086449092962]
	TIME [epoch: 8.61 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037666971560493		[learning rate: 2.429e-05]
	Learning Rate: 2.42896e-05
	LOSS [training: 3.037666971560493 | validation: 1.3786009944608302]
	TIME [epoch: 8.59 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039647631217842		[learning rate: 2.4201e-05]
	Learning Rate: 2.42015e-05
	LOSS [training: 3.039647631217842 | validation: 1.3730101035937063]
	TIME [epoch: 8.6 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0373594879923185		[learning rate: 2.4114e-05]
	Learning Rate: 2.41137e-05
	LOSS [training: 3.0373594879923185 | validation: 1.359004044965442]
	TIME [epoch: 8.6 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.033988356616867		[learning rate: 2.4026e-05]
	Learning Rate: 2.40262e-05
	LOSS [training: 3.033988356616867 | validation: 1.3656787973836344]
	TIME [epoch: 8.62 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034122611614789		[learning rate: 2.3939e-05]
	Learning Rate: 2.3939e-05
	LOSS [training: 3.034122611614789 | validation: 1.365409442996595]
	TIME [epoch: 8.6 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0352992642125276		[learning rate: 2.3852e-05]
	Learning Rate: 2.38521e-05
	LOSS [training: 3.0352992642125276 | validation: 1.3655492146735346]
	TIME [epoch: 8.59 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0346335451439264		[learning rate: 2.3766e-05]
	Learning Rate: 2.37655e-05
	LOSS [training: 3.0346335451439264 | validation: 1.3655273922061582]
	TIME [epoch: 8.59 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0372671990329776		[learning rate: 2.3679e-05]
	Learning Rate: 2.36793e-05
	LOSS [training: 3.0372671990329776 | validation: 1.381635665458655]
	TIME [epoch: 8.62 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0372905135995554		[learning rate: 2.3593e-05]
	Learning Rate: 2.35933e-05
	LOSS [training: 3.0372905135995554 | validation: 1.3706968272130249]
	TIME [epoch: 8.6 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03471950948332		[learning rate: 2.3508e-05]
	Learning Rate: 2.35077e-05
	LOSS [training: 3.03471950948332 | validation: 1.3587153907634797]
	TIME [epoch: 8.6 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0407347719985007		[learning rate: 2.3422e-05]
	Learning Rate: 2.34224e-05
	LOSS [training: 3.0407347719985007 | validation: 1.3689224997952623]
	TIME [epoch: 8.6 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035647036379905		[learning rate: 2.3337e-05]
	Learning Rate: 2.33374e-05
	LOSS [training: 3.035647036379905 | validation: 1.3617556666996977]
	TIME [epoch: 8.62 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0368714132068524		[learning rate: 2.3253e-05]
	Learning Rate: 2.32527e-05
	LOSS [training: 3.0368714132068524 | validation: 1.366757888051453]
	TIME [epoch: 8.6 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038118922245622		[learning rate: 2.3168e-05]
	Learning Rate: 2.31683e-05
	LOSS [training: 3.038118922245622 | validation: 1.3672755418672318]
	TIME [epoch: 8.61 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03876246191407		[learning rate: 2.3084e-05]
	Learning Rate: 2.30843e-05
	LOSS [training: 3.03876246191407 | validation: 1.3644192232613128]
	TIME [epoch: 8.61 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0384281323754534		[learning rate: 2.3e-05]
	Learning Rate: 2.30005e-05
	LOSS [training: 3.0384281323754534 | validation: 1.366364804791758]
	TIME [epoch: 8.61 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.033878419642907		[learning rate: 2.2917e-05]
	Learning Rate: 2.2917e-05
	LOSS [training: 3.033878419642907 | validation: 1.3714569767571378]
	TIME [epoch: 8.59 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0378808029926594		[learning rate: 2.2834e-05]
	Learning Rate: 2.28338e-05
	LOSS [training: 3.0378808029926594 | validation: 1.363983251825786]
	TIME [epoch: 8.6 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0379062720768895		[learning rate: 2.2751e-05]
	Learning Rate: 2.2751e-05
	LOSS [training: 3.0379062720768895 | validation: 1.368025725671705]
	TIME [epoch: 8.61 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0381552253832345		[learning rate: 2.2668e-05]
	Learning Rate: 2.26684e-05
	LOSS [training: 3.0381552253832345 | validation: 1.3636277564729635]
	TIME [epoch: 8.61 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0338017545233367		[learning rate: 2.2586e-05]
	Learning Rate: 2.25862e-05
	LOSS [training: 3.0338017545233367 | validation: 1.3740548653336462]
	TIME [epoch: 8.6 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0351721128202818		[learning rate: 2.2504e-05]
	Learning Rate: 2.25042e-05
	LOSS [training: 3.0351721128202818 | validation: 1.375920633110127]
	TIME [epoch: 8.6 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0399004361502135		[learning rate: 2.2423e-05]
	Learning Rate: 2.24225e-05
	LOSS [training: 3.0399004361502135 | validation: 1.3768226772717114]
	TIME [epoch: 8.61 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0390902173410117		[learning rate: 2.2341e-05]
	Learning Rate: 2.23411e-05
	LOSS [training: 3.0390902173410117 | validation: 1.3750347155066922]
	TIME [epoch: 8.63 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.040428884860897		[learning rate: 2.226e-05]
	Learning Rate: 2.22601e-05
	LOSS [training: 3.040428884860897 | validation: 1.3723312188341776]
	TIME [epoch: 8.6 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0402009672476975		[learning rate: 2.2179e-05]
	Learning Rate: 2.21793e-05
	LOSS [training: 3.0402009672476975 | validation: 1.3659123182403978]
	TIME [epoch: 8.61 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0315824820673916		[learning rate: 2.2099e-05]
	Learning Rate: 2.20988e-05
	LOSS [training: 3.0315824820673916 | validation: 1.3678412253229424]
	TIME [epoch: 8.62 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0354849219317876		[learning rate: 2.2019e-05]
	Learning Rate: 2.20186e-05
	LOSS [training: 3.0354849219317876 | validation: 1.3688345880808228]
	TIME [epoch: 8.61 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0379691321009967		[learning rate: 2.1939e-05]
	Learning Rate: 2.19387e-05
	LOSS [training: 3.0379691321009967 | validation: 1.3743876848860808]
	TIME [epoch: 8.61 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.032882946426342		[learning rate: 2.1859e-05]
	Learning Rate: 2.18591e-05
	LOSS [training: 3.032882946426342 | validation: 1.3713080673366758]
	TIME [epoch: 8.61 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039300078959539		[learning rate: 2.178e-05]
	Learning Rate: 2.17797e-05
	LOSS [training: 3.039300078959539 | validation: 1.3703219939704359]
	TIME [epoch: 8.62 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0382744075200003		[learning rate: 2.1701e-05]
	Learning Rate: 2.17007e-05
	LOSS [training: 3.0382744075200003 | validation: 1.3742326548533137]
	TIME [epoch: 8.59 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0389143951538506		[learning rate: 2.1622e-05]
	Learning Rate: 2.16219e-05
	LOSS [training: 3.0389143951538506 | validation: 1.362459277490041]
	TIME [epoch: 8.6 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038193840363365		[learning rate: 2.1543e-05]
	Learning Rate: 2.15435e-05
	LOSS [training: 3.038193840363365 | validation: 1.364311423398239]
	TIME [epoch: 8.61 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0367021087994104		[learning rate: 2.1465e-05]
	Learning Rate: 2.14653e-05
	LOSS [training: 3.0367021087994104 | validation: 1.3597014185143332]
	TIME [epoch: 8.62 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0346998853250278		[learning rate: 2.1387e-05]
	Learning Rate: 2.13874e-05
	LOSS [training: 3.0346998853250278 | validation: 1.3641435093074894]
	TIME [epoch: 8.59 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0352265376204		[learning rate: 2.131e-05]
	Learning Rate: 2.13098e-05
	LOSS [training: 3.0352265376204 | validation: 1.3656376594074329]
	TIME [epoch: 8.6 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036778172566586		[learning rate: 2.1232e-05]
	Learning Rate: 2.12325e-05
	LOSS [training: 3.036778172566586 | validation: 1.3585315843917622]
	TIME [epoch: 8.61 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0424939315049895		[learning rate: 2.1155e-05]
	Learning Rate: 2.11554e-05
	LOSS [training: 3.0424939315049895 | validation: 1.3719790425843619]
	TIME [epoch: 8.62 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037583991759189		[learning rate: 2.1079e-05]
	Learning Rate: 2.10786e-05
	LOSS [training: 3.037583991759189 | validation: 1.353426588177079]
	TIME [epoch: 8.61 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0391388000065076		[learning rate: 2.1002e-05]
	Learning Rate: 2.10021e-05
	LOSS [training: 3.0391388000065076 | validation: 1.3657053262305885]
	TIME [epoch: 8.6 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0348172543404774		[learning rate: 2.0926e-05]
	Learning Rate: 2.09259e-05
	LOSS [training: 3.0348172543404774 | validation: 1.369306448343649]
	TIME [epoch: 8.62 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0352514992401387		[learning rate: 2.085e-05]
	Learning Rate: 2.085e-05
	LOSS [training: 3.0352514992401387 | validation: 1.3683378731279183]
	TIME [epoch: 8.62 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.031842168248501		[learning rate: 2.0774e-05]
	Learning Rate: 2.07743e-05
	LOSS [training: 3.031842168248501 | validation: 1.3638555481930121]
	TIME [epoch: 8.61 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035152119679416		[learning rate: 2.0699e-05]
	Learning Rate: 2.06989e-05
	LOSS [training: 3.035152119679416 | validation: 1.3696287796200395]
	TIME [epoch: 8.59 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0368485358691037		[learning rate: 2.0624e-05]
	Learning Rate: 2.06238e-05
	LOSS [training: 3.0368485358691037 | validation: 1.3662311007631254]
	TIME [epoch: 8.6 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036635243080732		[learning rate: 2.0549e-05]
	Learning Rate: 2.05489e-05
	LOSS [training: 3.036635243080732 | validation: 1.3745875905361649]
	TIME [epoch: 8.61 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0415418320385608		[learning rate: 2.0474e-05]
	Learning Rate: 2.04744e-05
	LOSS [training: 3.0415418320385608 | validation: 1.3670165923488082]
	TIME [epoch: 8.6 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0318130148822453		[learning rate: 2.04e-05]
	Learning Rate: 2.04001e-05
	LOSS [training: 3.0318130148822453 | validation: 1.364546298648399]
	TIME [epoch: 8.6 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037286231105613		[learning rate: 2.0326e-05]
	Learning Rate: 2.0326e-05
	LOSS [training: 3.037286231105613 | validation: 1.3721195543739129]
	TIME [epoch: 8.62 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0353124714272814		[learning rate: 2.0252e-05]
	Learning Rate: 2.02523e-05
	LOSS [training: 3.0353124714272814 | validation: 1.3616283429006972]
	TIME [epoch: 8.6 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0358453903656395		[learning rate: 2.0179e-05]
	Learning Rate: 2.01788e-05
	LOSS [training: 3.0358453903656395 | validation: 1.364712243778079]
	TIME [epoch: 8.6 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0375905158680117		[learning rate: 2.0106e-05]
	Learning Rate: 2.01055e-05
	LOSS [training: 3.0375905158680117 | validation: 1.36459487728698]
	TIME [epoch: 8.6 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036192680835219		[learning rate: 2.0033e-05]
	Learning Rate: 2.00326e-05
	LOSS [training: 3.036192680835219 | validation: 1.3726304232036368]
	TIME [epoch: 8.62 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036717300007445		[learning rate: 1.996e-05]
	Learning Rate: 1.99599e-05
	LOSS [training: 3.036717300007445 | validation: 1.3644895156215526]
	TIME [epoch: 8.6 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034378032883988		[learning rate: 1.9887e-05]
	Learning Rate: 1.98874e-05
	LOSS [training: 3.034378032883988 | validation: 1.3700986489262927]
	TIME [epoch: 8.6 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0380596428024917		[learning rate: 1.9815e-05]
	Learning Rate: 1.98153e-05
	LOSS [training: 3.0380596428024917 | validation: 1.3684667633761982]
	TIME [epoch: 8.6 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034168271840695		[learning rate: 1.9743e-05]
	Learning Rate: 1.97434e-05
	LOSS [training: 3.034168271840695 | validation: 1.3719498220090265]
	TIME [epoch: 8.63 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0358483396346947		[learning rate: 1.9672e-05]
	Learning Rate: 1.96717e-05
	LOSS [training: 3.0358483396346947 | validation: 1.3610864033886578]
	TIME [epoch: 8.6 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035783737664948		[learning rate: 1.96e-05]
	Learning Rate: 1.96003e-05
	LOSS [training: 3.035783737664948 | validation: 1.367115265141781]
	TIME [epoch: 8.6 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037138014368265		[learning rate: 1.9529e-05]
	Learning Rate: 1.95292e-05
	LOSS [training: 3.037138014368265 | validation: 1.3705736601090845]
	TIME [epoch: 8.6 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0327540054342594		[learning rate: 1.9458e-05]
	Learning Rate: 1.94583e-05
	LOSS [training: 3.0327540054342594 | validation: 1.3615238513694172]
	TIME [epoch: 8.63 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036279334902294		[learning rate: 1.9388e-05]
	Learning Rate: 1.93877e-05
	LOSS [training: 3.036279334902294 | validation: 1.3684921368524214]
	TIME [epoch: 8.6 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0348534884876863		[learning rate: 1.9317e-05]
	Learning Rate: 1.93173e-05
	LOSS [training: 3.0348534884876863 | validation: 1.358539499893802]
	TIME [epoch: 8.6 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0350904495905846		[learning rate: 1.9247e-05]
	Learning Rate: 1.92472e-05
	LOSS [training: 3.0350904495905846 | validation: 1.3650810575867058]
	TIME [epoch: 8.6 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0395552182916052		[learning rate: 1.9177e-05]
	Learning Rate: 1.91774e-05
	LOSS [training: 3.0395552182916052 | validation: 1.3798102846070395]
	TIME [epoch: 8.61 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035889468181815		[learning rate: 1.9108e-05]
	Learning Rate: 1.91078e-05
	LOSS [training: 3.035889468181815 | validation: 1.3696072222544942]
	TIME [epoch: 8.6 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0341422506830087		[learning rate: 1.9038e-05]
	Learning Rate: 1.90384e-05
	LOSS [training: 3.0341422506830087 | validation: 1.3646576937495005]
	TIME [epoch: 8.61 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037549208792395		[learning rate: 1.8969e-05]
	Learning Rate: 1.89694e-05
	LOSS [training: 3.037549208792395 | validation: 1.36320387198786]
	TIME [epoch: 8.61 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0381992160020013		[learning rate: 1.8901e-05]
	Learning Rate: 1.89005e-05
	LOSS [training: 3.0381992160020013 | validation: 1.368207704889507]
	TIME [epoch: 8.62 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.031628864171464		[learning rate: 1.8832e-05]
	Learning Rate: 1.88319e-05
	LOSS [training: 3.031628864171464 | validation: 1.368626145893886]
	TIME [epoch: 8.6 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037311030992905		[learning rate: 1.8764e-05]
	Learning Rate: 1.87636e-05
	LOSS [training: 3.037311030992905 | validation: 1.3614362813299619]
	TIME [epoch: 8.61 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0374704112647284		[learning rate: 1.8695e-05]
	Learning Rate: 1.86955e-05
	LOSS [training: 3.0374704112647284 | validation: 1.3614422321639479]
	TIME [epoch: 8.62 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.040485337217678		[learning rate: 1.8628e-05]
	Learning Rate: 1.86276e-05
	LOSS [training: 3.040485337217678 | validation: 1.362864248492847]
	TIME [epoch: 8.61 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036033957841295		[learning rate: 1.856e-05]
	Learning Rate: 1.856e-05
	LOSS [training: 3.036033957841295 | validation: 1.3668505923386605]
	TIME [epoch: 8.6 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0349540934382397		[learning rate: 1.8493e-05]
	Learning Rate: 1.84927e-05
	LOSS [training: 3.0349540934382397 | validation: 1.365752139269692]
	TIME [epoch: 8.6 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0346515447014357		[learning rate: 1.8426e-05]
	Learning Rate: 1.84256e-05
	LOSS [training: 3.0346515447014357 | validation: 1.3693906181956508]
	TIME [epoch: 8.61 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037458179855727		[learning rate: 1.8359e-05]
	Learning Rate: 1.83587e-05
	LOSS [training: 3.037458179855727 | validation: 1.3575236027310718]
	TIME [epoch: 8.61 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039622456861896		[learning rate: 1.8292e-05]
	Learning Rate: 1.82921e-05
	LOSS [training: 3.039622456861896 | validation: 1.3577983060027523]
	TIME [epoch: 8.6 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0353154282219377		[learning rate: 1.8226e-05]
	Learning Rate: 1.82257e-05
	LOSS [training: 3.0353154282219377 | validation: 1.3629943793827266]
	TIME [epoch: 8.59 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035287949400191		[learning rate: 1.816e-05]
	Learning Rate: 1.81596e-05
	LOSS [training: 3.035287949400191 | validation: 1.375645907473726]
	TIME [epoch: 8.61 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0353498625255897		[learning rate: 1.8094e-05]
	Learning Rate: 1.80937e-05
	LOSS [training: 3.0353498625255897 | validation: 1.3694875546079395]
	TIME [epoch: 8.59 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0362796216150856		[learning rate: 1.8028e-05]
	Learning Rate: 1.8028e-05
	LOSS [training: 3.0362796216150856 | validation: 1.3647319614656686]
	TIME [epoch: 8.6 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.032995688776535		[learning rate: 1.7963e-05]
	Learning Rate: 1.79626e-05
	LOSS [training: 3.032995688776535 | validation: 1.3642496980668986]
	TIME [epoch: 8.59 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036386596433592		[learning rate: 1.7897e-05]
	Learning Rate: 1.78974e-05
	LOSS [training: 3.036386596433592 | validation: 1.3752257728653074]
	TIME [epoch: 8.62 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.040014595212129		[learning rate: 1.7832e-05]
	Learning Rate: 1.78324e-05
	LOSS [training: 3.040014595212129 | validation: 1.3603451372789646]
	TIME [epoch: 8.59 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03726966084537		[learning rate: 1.7768e-05]
	Learning Rate: 1.77677e-05
	LOSS [training: 3.03726966084537 | validation: 1.3728205866402794]
	TIME [epoch: 8.6 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035291089224711		[learning rate: 1.7703e-05]
	Learning Rate: 1.77032e-05
	LOSS [training: 3.035291089224711 | validation: 1.351713716326051]
	TIME [epoch: 8.59 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034253482623936		[learning rate: 1.7639e-05]
	Learning Rate: 1.7639e-05
	LOSS [training: 3.034253482623936 | validation: 1.3630040074550394]
	TIME [epoch: 8.61 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038323442967649		[learning rate: 1.7575e-05]
	Learning Rate: 1.7575e-05
	LOSS [training: 3.038323442967649 | validation: 1.3704555411023658]
	TIME [epoch: 8.59 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0385553817564617		[learning rate: 1.7511e-05]
	Learning Rate: 1.75112e-05
	LOSS [training: 3.0385553817564617 | validation: 1.3668676847732892]
	TIME [epoch: 8.6 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034189221919285		[learning rate: 1.7448e-05]
	Learning Rate: 1.74476e-05
	LOSS [training: 3.034189221919285 | validation: 1.3619834984237418]
	TIME [epoch: 8.6 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034251548927298		[learning rate: 1.7384e-05]
	Learning Rate: 1.73843e-05
	LOSS [training: 3.034251548927298 | validation: 1.3651797691010623]
	TIME [epoch: 8.6 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0369544290747728		[learning rate: 1.7321e-05]
	Learning Rate: 1.73212e-05
	LOSS [training: 3.0369544290747728 | validation: 1.3624038486820131]
	TIME [epoch: 8.6 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0360793956753342		[learning rate: 1.7258e-05]
	Learning Rate: 1.72584e-05
	LOSS [training: 3.0360793956753342 | validation: 1.356134995042759]
	TIME [epoch: 8.6 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0348412401016613		[learning rate: 1.7196e-05]
	Learning Rate: 1.71957e-05
	LOSS [training: 3.0348412401016613 | validation: 1.3671971406770005]
	TIME [epoch: 8.6 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0373875359345903		[learning rate: 1.7133e-05]
	Learning Rate: 1.71333e-05
	LOSS [training: 3.0373875359345903 | validation: 1.3722685992725498]
	TIME [epoch: 8.6 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038140542248332		[learning rate: 1.7071e-05]
	Learning Rate: 1.70712e-05
	LOSS [training: 3.038140542248332 | validation: 1.3613370322278242]
	TIME [epoch: 8.6 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0343846375372134		[learning rate: 1.7009e-05]
	Learning Rate: 1.70092e-05
	LOSS [training: 3.0343846375372134 | validation: 1.363667566326252]
	TIME [epoch: 8.6 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039577005365781		[learning rate: 1.6947e-05]
	Learning Rate: 1.69475e-05
	LOSS [training: 3.039577005365781 | validation: 1.366656300398385]
	TIME [epoch: 8.61 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0303545831238505		[learning rate: 1.6886e-05]
	Learning Rate: 1.6886e-05
	LOSS [training: 3.0303545831238505 | validation: 1.3669058313179319]
	TIME [epoch: 8.6 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0409830685203545		[learning rate: 1.6825e-05]
	Learning Rate: 1.68247e-05
	LOSS [training: 3.0409830685203545 | validation: 1.3678208870843511]
	TIME [epoch: 8.6 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0342776503878346		[learning rate: 1.6764e-05]
	Learning Rate: 1.67636e-05
	LOSS [training: 3.0342776503878346 | validation: 1.3790430932039968]
	TIME [epoch: 8.59 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0379148979520836		[learning rate: 1.6703e-05]
	Learning Rate: 1.67028e-05
	LOSS [training: 3.0379148979520836 | validation: 1.3705230198429639]
	TIME [epoch: 8.61 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035527718677585		[learning rate: 1.6642e-05]
	Learning Rate: 1.66422e-05
	LOSS [training: 3.035527718677585 | validation: 1.3666587322195816]
	TIME [epoch: 8.6 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0390092472840573		[learning rate: 1.6582e-05]
	Learning Rate: 1.65818e-05
	LOSS [training: 3.0390092472840573 | validation: 1.3716501856395766]
	TIME [epoch: 8.6 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0352397263272435		[learning rate: 1.6522e-05]
	Learning Rate: 1.65216e-05
	LOSS [training: 3.0352397263272435 | validation: 1.3554925143609833]
	TIME [epoch: 8.59 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0327838033786447		[learning rate: 1.6462e-05]
	Learning Rate: 1.64617e-05
	LOSS [training: 3.0327838033786447 | validation: 1.370842794300372]
	TIME [epoch: 8.62 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0347892584855103		[learning rate: 1.6402e-05]
	Learning Rate: 1.64019e-05
	LOSS [training: 3.0347892584855103 | validation: 1.3684866906487239]
	TIME [epoch: 8.59 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0364939663127073		[learning rate: 1.6342e-05]
	Learning Rate: 1.63424e-05
	LOSS [training: 3.0364939663127073 | validation: 1.362342907816133]
	TIME [epoch: 8.59 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0391635315341947		[learning rate: 1.6283e-05]
	Learning Rate: 1.62831e-05
	LOSS [training: 3.0391635315341947 | validation: 1.3676853430821396]
	TIME [epoch: 8.59 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0317190716413256		[learning rate: 1.6224e-05]
	Learning Rate: 1.6224e-05
	LOSS [training: 3.0317190716413256 | validation: 1.3636236816360732]
	TIME [epoch: 8.62 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035494169788963		[learning rate: 1.6165e-05]
	Learning Rate: 1.61651e-05
	LOSS [training: 3.035494169788963 | validation: 1.365194355234921]
	TIME [epoch: 8.6 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037250447675267		[learning rate: 1.6106e-05]
	Learning Rate: 1.61065e-05
	LOSS [training: 3.037250447675267 | validation: 1.3762594885575097]
	TIME [epoch: 8.59 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0377315977394335		[learning rate: 1.6048e-05]
	Learning Rate: 1.6048e-05
	LOSS [training: 3.0377315977394335 | validation: 1.3712006786603737]
	TIME [epoch: 8.6 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0339514854860097		[learning rate: 1.599e-05]
	Learning Rate: 1.59898e-05
	LOSS [training: 3.0339514854860097 | validation: 1.3656487310334842]
	TIME [epoch: 8.61 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0347585008020994		[learning rate: 1.5932e-05]
	Learning Rate: 1.59317e-05
	LOSS [training: 3.0347585008020994 | validation: 1.3638468383714908]
	TIME [epoch: 8.61 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034106069468803		[learning rate: 1.5874e-05]
	Learning Rate: 1.58739e-05
	LOSS [training: 3.034106069468803 | validation: 1.3723275324026516]
	TIME [epoch: 8.59 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039362725655207		[learning rate: 1.5816e-05]
	Learning Rate: 1.58163e-05
	LOSS [training: 3.039362725655207 | validation: 1.379372804170252]
	TIME [epoch: 8.6 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0414043383945093		[learning rate: 1.5759e-05]
	Learning Rate: 1.57589e-05
	LOSS [training: 3.0414043383945093 | validation: 1.3688704303362231]
	TIME [epoch: 8.61 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0399497374613116		[learning rate: 1.5702e-05]
	Learning Rate: 1.57017e-05
	LOSS [training: 3.0399497374613116 | validation: 1.3686327286965634]
	TIME [epoch: 8.6 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0337449911808383		[learning rate: 1.5645e-05]
	Learning Rate: 1.56447e-05
	LOSS [training: 3.0337449911808383 | validation: 1.3678611088653378]
	TIME [epoch: 8.6 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0375411773346657		[learning rate: 1.5588e-05]
	Learning Rate: 1.5588e-05
	LOSS [training: 3.0375411773346657 | validation: 1.3656584854283738]
	TIME [epoch: 8.62 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036776924890126		[learning rate: 1.5531e-05]
	Learning Rate: 1.55314e-05
	LOSS [training: 3.036776924890126 | validation: 1.3694573082568733]
	TIME [epoch: 8.61 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034381292356287		[learning rate: 1.5475e-05]
	Learning Rate: 1.5475e-05
	LOSS [training: 3.034381292356287 | validation: 1.37666194277282]
	TIME [epoch: 8.59 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035905545296289		[learning rate: 1.5419e-05]
	Learning Rate: 1.54189e-05
	LOSS [training: 3.035905545296289 | validation: 1.3716302266402038]
	TIME [epoch: 8.6 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035010415605493		[learning rate: 1.5363e-05]
	Learning Rate: 1.53629e-05
	LOSS [training: 3.035010415605493 | validation: 1.3720692749605699]
	TIME [epoch: 8.62 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037479752505503		[learning rate: 1.5307e-05]
	Learning Rate: 1.53072e-05
	LOSS [training: 3.037479752505503 | validation: 1.376399923353321]
	TIME [epoch: 8.6 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0354527408363228		[learning rate: 1.5252e-05]
	Learning Rate: 1.52516e-05
	LOSS [training: 3.0354527408363228 | validation: 1.376302592973739]
	TIME [epoch: 8.6 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036781516694634		[learning rate: 1.5196e-05]
	Learning Rate: 1.51963e-05
	LOSS [training: 3.036781516694634 | validation: 1.3713081233071964]
	TIME [epoch: 8.6 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0407748318981302		[learning rate: 1.5141e-05]
	Learning Rate: 1.51411e-05
	LOSS [training: 3.0407748318981302 | validation: 1.3743120809400056]
	TIME [epoch: 8.62 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0374854805172276		[learning rate: 1.5086e-05]
	Learning Rate: 1.50862e-05
	LOSS [training: 3.0374854805172276 | validation: 1.3728963604766422]
	TIME [epoch: 8.6 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.040081329754288		[learning rate: 1.5031e-05]
	Learning Rate: 1.50314e-05
	LOSS [training: 3.040081329754288 | validation: 1.3635895711281005]
	TIME [epoch: 8.61 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0337575196424087		[learning rate: 1.4977e-05]
	Learning Rate: 1.49769e-05
	LOSS [training: 3.0337575196424087 | validation: 1.365000016839026]
	TIME [epoch: 8.6 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0348342667296686		[learning rate: 1.4923e-05]
	Learning Rate: 1.49225e-05
	LOSS [training: 3.0348342667296686 | validation: 1.3585875108277992]
	TIME [epoch: 8.63 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0323553573569324		[learning rate: 1.4868e-05]
	Learning Rate: 1.48684e-05
	LOSS [training: 3.0323553573569324 | validation: 1.367240030337381]
	TIME [epoch: 8.61 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039055829449592		[learning rate: 1.4814e-05]
	Learning Rate: 1.48144e-05
	LOSS [training: 3.039055829449592 | validation: 1.360005878378584]
	TIME [epoch: 8.6 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0380587981192617		[learning rate: 1.4761e-05]
	Learning Rate: 1.47606e-05
	LOSS [training: 3.0380587981192617 | validation: 1.3613356199066653]
	TIME [epoch: 8.6 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0372329027002936		[learning rate: 1.4707e-05]
	Learning Rate: 1.47071e-05
	LOSS [training: 3.0372329027002936 | validation: 1.3730826239678569]
	TIME [epoch: 8.63 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034231587296934		[learning rate: 1.4654e-05]
	Learning Rate: 1.46537e-05
	LOSS [training: 3.034231587296934 | validation: 1.370519442463434]
	TIME [epoch: 8.6 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034489765865743		[learning rate: 1.4601e-05]
	Learning Rate: 1.46005e-05
	LOSS [training: 3.034489765865743 | validation: 1.3674893784625948]
	TIME [epoch: 8.6 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035712793336639		[learning rate: 1.4548e-05]
	Learning Rate: 1.45475e-05
	LOSS [training: 3.035712793336639 | validation: 1.3742236568595692]
	TIME [epoch: 8.6 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0358416252046467		[learning rate: 1.4495e-05]
	Learning Rate: 1.44947e-05
	LOSS [training: 3.0358416252046467 | validation: 1.3699731741358858]
	TIME [epoch: 8.6 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0313798336493383		[learning rate: 1.4442e-05]
	Learning Rate: 1.44421e-05
	LOSS [training: 3.0313798336493383 | validation: 1.362882508220048]
	TIME [epoch: 8.6 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0326370562600826		[learning rate: 1.439e-05]
	Learning Rate: 1.43897e-05
	LOSS [training: 3.0326370562600826 | validation: 1.362736498144082]
	TIME [epoch: 8.6 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0351088894575406		[learning rate: 1.4338e-05]
	Learning Rate: 1.43375e-05
	LOSS [training: 3.0351088894575406 | validation: 1.3760056927789526]
	TIME [epoch: 8.62 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0375261935872957		[learning rate: 1.4285e-05]
	Learning Rate: 1.42855e-05
	LOSS [training: 3.0375261935872957 | validation: 1.3669080369300846]
	TIME [epoch: 8.61 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034507748236322		[learning rate: 1.4234e-05]
	Learning Rate: 1.42336e-05
	LOSS [training: 3.034507748236322 | validation: 1.3626669255527681]
	TIME [epoch: 8.62 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0381078331900033		[learning rate: 1.4182e-05]
	Learning Rate: 1.4182e-05
	LOSS [training: 3.0381078331900033 | validation: 1.3649596234626926]
	TIME [epoch: 8.6 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0373696742345486		[learning rate: 1.4131e-05]
	Learning Rate: 1.41305e-05
	LOSS [training: 3.0373696742345486 | validation: 1.3643339536514885]
	TIME [epoch: 8.63 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0319841923401745		[learning rate: 1.4079e-05]
	Learning Rate: 1.40792e-05
	LOSS [training: 3.0319841923401745 | validation: 1.3706714377709455]
	TIME [epoch: 8.6 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0324752952923366		[learning rate: 1.4028e-05]
	Learning Rate: 1.40281e-05
	LOSS [training: 3.0324752952923366 | validation: 1.3662513095965574]
	TIME [epoch: 8.61 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0353414656766935		[learning rate: 1.3977e-05]
	Learning Rate: 1.39772e-05
	LOSS [training: 3.0353414656766935 | validation: 1.3673014728905657]
	TIME [epoch: 8.61 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037942439924705		[learning rate: 1.3927e-05]
	Learning Rate: 1.39265e-05
	LOSS [training: 3.037942439924705 | validation: 1.3578167025509609]
	TIME [epoch: 8.62 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0390782546653963		[learning rate: 1.3876e-05]
	Learning Rate: 1.3876e-05
	LOSS [training: 3.0390782546653963 | validation: 1.3601238933396218]
	TIME [epoch: 8.6 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0324859405734115		[learning rate: 1.3826e-05]
	Learning Rate: 1.38256e-05
	LOSS [training: 3.0324859405734115 | validation: 1.358103113957961]
	TIME [epoch: 8.6 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0364189422859034		[learning rate: 1.3775e-05]
	Learning Rate: 1.37754e-05
	LOSS [training: 3.0364189422859034 | validation: 1.3576470816782138]
	TIME [epoch: 8.61 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0356937148486627		[learning rate: 1.3725e-05]
	Learning Rate: 1.37254e-05
	LOSS [training: 3.0356937148486627 | validation: 1.3601247000800862]
	TIME [epoch: 8.63 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0404511380719947		[learning rate: 1.3676e-05]
	Learning Rate: 1.36756e-05
	LOSS [training: 3.0404511380719947 | validation: 1.3591160354331944]
	TIME [epoch: 8.6 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038591385906772		[learning rate: 1.3626e-05]
	Learning Rate: 1.3626e-05
	LOSS [training: 3.038591385906772 | validation: 1.370157670067515]
	TIME [epoch: 8.6 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038513862469724		[learning rate: 1.3577e-05]
	Learning Rate: 1.35766e-05
	LOSS [training: 3.038513862469724 | validation: 1.3694625509615006]
	TIME [epoch: 8.6 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038214858840965		[learning rate: 1.3527e-05]
	Learning Rate: 1.35273e-05
	LOSS [training: 3.038214858840965 | validation: 1.3665675136965345]
	TIME [epoch: 8.62 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0359303955727173		[learning rate: 1.3478e-05]
	Learning Rate: 1.34782e-05
	LOSS [training: 3.0359303955727173 | validation: 1.3548086674161575]
	TIME [epoch: 8.61 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03303849564586		[learning rate: 1.3429e-05]
	Learning Rate: 1.34293e-05
	LOSS [training: 3.03303849564586 | validation: 1.3608721331367486]
	TIME [epoch: 8.6 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034892036300982		[learning rate: 1.3381e-05]
	Learning Rate: 1.33805e-05
	LOSS [training: 3.034892036300982 | validation: 1.3579728209756805]
	TIME [epoch: 8.61 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036837094363736		[learning rate: 1.3332e-05]
	Learning Rate: 1.3332e-05
	LOSS [training: 3.036837094363736 | validation: 1.3679992093842552]
	TIME [epoch: 8.6 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034300981523914		[learning rate: 1.3284e-05]
	Learning Rate: 1.32836e-05
	LOSS [training: 3.034300981523914 | validation: 1.3654469406418688]
	TIME [epoch: 8.61 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0349191475179014		[learning rate: 1.3235e-05]
	Learning Rate: 1.32354e-05
	LOSS [training: 3.0349191475179014 | validation: 1.3704108465798854]
	TIME [epoch: 8.61 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03743699073791		[learning rate: 1.3187e-05]
	Learning Rate: 1.31874e-05
	LOSS [training: 3.03743699073791 | validation: 1.367759977948301]
	TIME [epoch: 8.62 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0393882059589536		[learning rate: 1.314e-05]
	Learning Rate: 1.31395e-05
	LOSS [training: 3.0393882059589536 | validation: 1.363840893345263]
	TIME [epoch: 8.61 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0309323380240825		[learning rate: 1.3092e-05]
	Learning Rate: 1.30918e-05
	LOSS [training: 3.0309323380240825 | validation: 1.3638905702934971]
	TIME [epoch: 8.61 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034547518212885		[learning rate: 1.3044e-05]
	Learning Rate: 1.30443e-05
	LOSS [training: 3.034547518212885 | validation: 1.3689628732433843]
	TIME [epoch: 8.61 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0332462972555936		[learning rate: 1.2997e-05]
	Learning Rate: 1.2997e-05
	LOSS [training: 3.0332462972555936 | validation: 1.3657540022807784]
	TIME [epoch: 8.62 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0408585184408485		[learning rate: 1.295e-05]
	Learning Rate: 1.29498e-05
	LOSS [training: 3.0408585184408485 | validation: 1.3583872914931978]
	TIME [epoch: 8.61 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.033580397150577		[learning rate: 1.2903e-05]
	Learning Rate: 1.29028e-05
	LOSS [training: 3.033580397150577 | validation: 1.365926853360406]
	TIME [epoch: 8.61 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.032094486937008		[learning rate: 1.2856e-05]
	Learning Rate: 1.2856e-05
	LOSS [training: 3.032094486937008 | validation: 1.372996792833486]
	TIME [epoch: 8.6 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0307393255223323		[learning rate: 1.2809e-05]
	Learning Rate: 1.28093e-05
	LOSS [training: 3.0307393255223323 | validation: 1.3645096311888132]
	TIME [epoch: 8.62 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036302250811264		[learning rate: 1.2763e-05]
	Learning Rate: 1.27628e-05
	LOSS [training: 3.036302250811264 | validation: 1.3615093349121519]
	TIME [epoch: 8.61 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0355761765285885		[learning rate: 1.2717e-05]
	Learning Rate: 1.27165e-05
	LOSS [training: 3.0355761765285885 | validation: 1.3669210226329467]
	TIME [epoch: 8.6 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038455223756822		[learning rate: 1.267e-05]
	Learning Rate: 1.26704e-05
	LOSS [training: 3.038455223756822 | validation: 1.3724835016420314]
	TIME [epoch: 8.6 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036042570602438		[learning rate: 1.2624e-05]
	Learning Rate: 1.26244e-05
	LOSS [training: 3.036042570602438 | validation: 1.365607666155827]
	TIME [epoch: 8.61 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0375097185009006		[learning rate: 1.2579e-05]
	Learning Rate: 1.25786e-05
	LOSS [training: 3.0375097185009006 | validation: 1.3723613330801978]
	TIME [epoch: 8.6 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0348577254410847		[learning rate: 1.2533e-05]
	Learning Rate: 1.25329e-05
	LOSS [training: 3.0348577254410847 | validation: 1.3688334981945716]
	TIME [epoch: 8.6 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03643159050219		[learning rate: 1.2487e-05]
	Learning Rate: 1.24874e-05
	LOSS [training: 3.03643159050219 | validation: 1.364282306021393]
	TIME [epoch: 8.59 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0363232260167443		[learning rate: 1.2442e-05]
	Learning Rate: 1.24421e-05
	LOSS [training: 3.0363232260167443 | validation: 1.36772859441679]
	TIME [epoch: 8.62 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039629370356084		[learning rate: 1.2397e-05]
	Learning Rate: 1.2397e-05
	LOSS [training: 3.039629370356084 | validation: 1.3670328012134565]
	TIME [epoch: 8.6 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035193577489205		[learning rate: 1.2352e-05]
	Learning Rate: 1.2352e-05
	LOSS [training: 3.035193577489205 | validation: 1.3513470132309044]
	TIME [epoch: 8.6 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036428363242209		[learning rate: 1.2307e-05]
	Learning Rate: 1.23072e-05
	LOSS [training: 3.036428363242209 | validation: 1.3633038307923184]
	TIME [epoch: 8.61 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0386216394435506		[learning rate: 1.2263e-05]
	Learning Rate: 1.22625e-05
	LOSS [training: 3.0386216394435506 | validation: 1.372961250224531]
	TIME [epoch: 8.63 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0389547696558785		[learning rate: 1.2218e-05]
	Learning Rate: 1.2218e-05
	LOSS [training: 3.0389547696558785 | validation: 1.3783787870227524]
	TIME [epoch: 8.6 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035345202897373		[learning rate: 1.2174e-05]
	Learning Rate: 1.21737e-05
	LOSS [training: 3.035345202897373 | validation: 1.36012997580302]
	TIME [epoch: 8.6 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0388353271903474		[learning rate: 1.2129e-05]
	Learning Rate: 1.21295e-05
	LOSS [training: 3.0388353271903474 | validation: 1.3562252738133753]
	TIME [epoch: 8.61 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0363776975033323		[learning rate: 1.2085e-05]
	Learning Rate: 1.20855e-05
	LOSS [training: 3.0363776975033323 | validation: 1.3547260761708122]
	TIME [epoch: 8.62 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.032735017610073		[learning rate: 1.2042e-05]
	Learning Rate: 1.20416e-05
	LOSS [training: 3.032735017610073 | validation: 1.3575171680695894]
	TIME [epoch: 8.6 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.033386441016975		[learning rate: 1.1998e-05]
	Learning Rate: 1.19979e-05
	LOSS [training: 3.033386441016975 | validation: 1.3706284893562306]
	TIME [epoch: 8.59 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036372424731302		[learning rate: 1.1954e-05]
	Learning Rate: 1.19544e-05
	LOSS [training: 3.036372424731302 | validation: 1.368062683889319]
	TIME [epoch: 8.61 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036560868811575		[learning rate: 1.1911e-05]
	Learning Rate: 1.1911e-05
	LOSS [training: 3.036560868811575 | validation: 1.3570229863939827]
	TIME [epoch: 8.6 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036741818054848		[learning rate: 1.1868e-05]
	Learning Rate: 1.18678e-05
	LOSS [training: 3.036741818054848 | validation: 1.3592148082219135]
	TIME [epoch: 8.59 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0381312045307443		[learning rate: 1.1825e-05]
	Learning Rate: 1.18247e-05
	LOSS [training: 3.0381312045307443 | validation: 1.3712692582385877]
	TIME [epoch: 8.59 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.033000835673974		[learning rate: 1.1782e-05]
	Learning Rate: 1.17818e-05
	LOSS [training: 3.033000835673974 | validation: 1.3646516376003432]
	TIME [epoch: 8.61 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036214000808873		[learning rate: 1.1739e-05]
	Learning Rate: 1.1739e-05
	LOSS [training: 3.036214000808873 | validation: 1.365079585792738]
	TIME [epoch: 8.59 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034125335131158		[learning rate: 1.1696e-05]
	Learning Rate: 1.16964e-05
	LOSS [training: 3.034125335131158 | validation: 1.3658407432882478]
	TIME [epoch: 8.59 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0358781455960804		[learning rate: 1.1654e-05]
	Learning Rate: 1.1654e-05
	LOSS [training: 3.0358781455960804 | validation: 1.3777338256384581]
	TIME [epoch: 8.6 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0368546271686965		[learning rate: 1.1612e-05]
	Learning Rate: 1.16117e-05
	LOSS [training: 3.0368546271686965 | validation: 1.380478763390351]
	TIME [epoch: 8.61 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035634192354564		[learning rate: 1.157e-05]
	Learning Rate: 1.15695e-05
	LOSS [training: 3.035634192354564 | validation: 1.376901477533814]
	TIME [epoch: 8.59 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0404389533155287		[learning rate: 1.1528e-05]
	Learning Rate: 1.15275e-05
	LOSS [training: 3.0404389533155287 | validation: 1.3661195609600754]
	TIME [epoch: 8.59 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03350401406875		[learning rate: 1.1486e-05]
	Learning Rate: 1.14857e-05
	LOSS [training: 3.03350401406875 | validation: 1.366203127481913]
	TIME [epoch: 8.59 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03759825062133		[learning rate: 1.1444e-05]
	Learning Rate: 1.1444e-05
	LOSS [training: 3.03759825062133 | validation: 1.377651022616924]
	TIME [epoch: 8.62 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0378350655826556		[learning rate: 1.1402e-05]
	Learning Rate: 1.14025e-05
	LOSS [training: 3.0378350655826556 | validation: 1.3603629603640541]
	TIME [epoch: 8.58 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0326583695852265		[learning rate: 1.1361e-05]
	Learning Rate: 1.13611e-05
	LOSS [training: 3.0326583695852265 | validation: 1.3597821228925708]
	TIME [epoch: 8.6 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0353705672929707		[learning rate: 1.132e-05]
	Learning Rate: 1.13199e-05
	LOSS [training: 3.0353705672929707 | validation: 1.3598666289920414]
	TIME [epoch: 8.59 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0395738396387872		[learning rate: 1.1279e-05]
	Learning Rate: 1.12788e-05
	LOSS [training: 3.0395738396387872 | validation: 1.3740254771637184]
	TIME [epoch: 8.61 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0374639960519128		[learning rate: 1.1238e-05]
	Learning Rate: 1.12379e-05
	LOSS [training: 3.0374639960519128 | validation: 1.3634531671111598]
	TIME [epoch: 8.59 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0374565593210687		[learning rate: 1.1197e-05]
	Learning Rate: 1.11971e-05
	LOSS [training: 3.0374565593210687 | validation: 1.3816208678461126]
	TIME [epoch: 8.59 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0342736483990853		[learning rate: 1.1156e-05]
	Learning Rate: 1.11565e-05
	LOSS [training: 3.0342736483990853 | validation: 1.3638008909838661]
	TIME [epoch: 8.6 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0369867853347983		[learning rate: 1.1116e-05]
	Learning Rate: 1.1116e-05
	LOSS [training: 3.0369867853347983 | validation: 1.3664286516095723]
	TIME [epoch: 8.63 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0375350925201476		[learning rate: 1.1076e-05]
	Learning Rate: 1.10756e-05
	LOSS [training: 3.0375350925201476 | validation: 1.3691085226642234]
	TIME [epoch: 8.59 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0344243155779553		[learning rate: 1.1035e-05]
	Learning Rate: 1.10354e-05
	LOSS [training: 3.0344243155779553 | validation: 1.366479758536586]
	TIME [epoch: 8.6 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036463778907633		[learning rate: 1.0995e-05]
	Learning Rate: 1.09954e-05
	LOSS [training: 3.036463778907633 | validation: 1.3623027197402757]
	TIME [epoch: 8.62 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035730275252788		[learning rate: 1.0955e-05]
	Learning Rate: 1.09555e-05
	LOSS [training: 3.035730275252788 | validation: 1.36563056964245]
	TIME [epoch: 8.61 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0369974124078927		[learning rate: 1.0916e-05]
	Learning Rate: 1.09157e-05
	LOSS [training: 3.0369974124078927 | validation: 1.3693176927502269]
	TIME [epoch: 8.6 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037442303137836		[learning rate: 1.0876e-05]
	Learning Rate: 1.08761e-05
	LOSS [training: 3.037442303137836 | validation: 1.3597632502330141]
	TIME [epoch: 8.6 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034298852585296		[learning rate: 1.0837e-05]
	Learning Rate: 1.08366e-05
	LOSS [training: 3.034298852585296 | validation: 1.3638318800098526]
	TIME [epoch: 8.62 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.033433212885203		[learning rate: 1.0797e-05]
	Learning Rate: 1.07973e-05
	LOSS [training: 3.033433212885203 | validation: 1.3533299135911188]
	TIME [epoch: 8.6 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0311806794161185		[learning rate: 1.0758e-05]
	Learning Rate: 1.07581e-05
	LOSS [training: 3.0311806794161185 | validation: 1.3701200802058193]
	TIME [epoch: 8.61 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0335279083923177		[learning rate: 1.0719e-05]
	Learning Rate: 1.07191e-05
	LOSS [training: 3.0335279083923177 | validation: 1.3711311161018112]
	TIME [epoch: 8.6 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034800606390147		[learning rate: 1.068e-05]
	Learning Rate: 1.06802e-05
	LOSS [training: 3.034800606390147 | validation: 1.3662289438444184]
	TIME [epoch: 8.62 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037773668675728		[learning rate: 1.0641e-05]
	Learning Rate: 1.06414e-05
	LOSS [training: 3.037773668675728 | validation: 1.3695983674916135]
	TIME [epoch: 8.59 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038132390537562		[learning rate: 1.0603e-05]
	Learning Rate: 1.06028e-05
	LOSS [training: 3.038132390537562 | validation: 1.3616960529930997]
	TIME [epoch: 8.59 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036614751556554		[learning rate: 1.0564e-05]
	Learning Rate: 1.05643e-05
	LOSS [training: 3.036614751556554 | validation: 1.3597821783189543]
	TIME [epoch: 8.6 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034803425726983		[learning rate: 1.0526e-05]
	Learning Rate: 1.0526e-05
	LOSS [training: 3.034803425726983 | validation: 1.3635784453456279]
	TIME [epoch: 8.62 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0363233730942363		[learning rate: 1.0488e-05]
	Learning Rate: 1.04878e-05
	LOSS [training: 3.0363233730942363 | validation: 1.3611091766065881]
	TIME [epoch: 8.6 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.038387980772945		[learning rate: 1.045e-05]
	Learning Rate: 1.04497e-05
	LOSS [training: 3.038387980772945 | validation: 1.3558819353071654]
	TIME [epoch: 8.6 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035896845340163		[learning rate: 1.0412e-05]
	Learning Rate: 1.04118e-05
	LOSS [training: 3.035896845340163 | validation: 1.3665519405920978]
	TIME [epoch: 8.6 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034296428757482		[learning rate: 1.0374e-05]
	Learning Rate: 1.0374e-05
	LOSS [training: 3.034296428757482 | validation: 1.3586774835319135]
	TIME [epoch: 8.62 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037663901420774		[learning rate: 1.0336e-05]
	Learning Rate: 1.03364e-05
	LOSS [training: 3.037663901420774 | validation: 1.3604058413959126]
	TIME [epoch: 8.6 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0400296613508897		[learning rate: 1.0299e-05]
	Learning Rate: 1.02989e-05
	LOSS [training: 3.0400296613508897 | validation: 1.3628059205831988]
	TIME [epoch: 8.6 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0340080126472424		[learning rate: 1.0261e-05]
	Learning Rate: 1.02615e-05
	LOSS [training: 3.0340080126472424 | validation: 1.361744273189612]
	TIME [epoch: 8.61 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0393074453928053		[learning rate: 1.0224e-05]
	Learning Rate: 1.02243e-05
	LOSS [training: 3.0393074453928053 | validation: 1.3693185538312467]
	TIME [epoch: 8.61 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.032409037993368		[learning rate: 1.0187e-05]
	Learning Rate: 1.01872e-05
	LOSS [training: 3.032409037993368 | validation: 1.3659144772680065]
	TIME [epoch: 8.59 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.034440228127962		[learning rate: 1.015e-05]
	Learning Rate: 1.01502e-05
	LOSS [training: 3.034440228127962 | validation: 1.3558967960168669]
	TIME [epoch: 8.6 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036807102868071		[learning rate: 1.0113e-05]
	Learning Rate: 1.01133e-05
	LOSS [training: 3.036807102868071 | validation: 1.362048613941566]
	TIME [epoch: 8.61 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0422531362675653		[learning rate: 1.0077e-05]
	Learning Rate: 1.00766e-05
	LOSS [training: 3.0422531362675653 | validation: 1.3655951086490217]
	TIME [epoch: 8.61 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0329208066428532		[learning rate: 1.004e-05]
	Learning Rate: 1.00401e-05
	LOSS [training: 3.0329208066428532 | validation: 1.3624991132146507]
	TIME [epoch: 8.6 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.03188511184286		[learning rate: 1.0004e-05]
	Learning Rate: 1.00036e-05
	LOSS [training: 3.03188511184286 | validation: 1.3710620168343888]
	TIME [epoch: 8.6 sec]
Finished training in 17384.377 seconds.
