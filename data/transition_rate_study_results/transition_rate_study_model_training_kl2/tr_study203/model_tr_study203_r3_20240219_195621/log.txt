Args:
Namespace(name='model_tr_study203', outdir='out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3', training_data='data/transition_rate_studies/tr_study203/tr_study203_training/r3', validation_data='data/transition_rate_studies/tr_study203/tr_study203_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3715449681

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 10.648934089531565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.648934089531565 | validation: 10.727654509227614]
	TIME [epoch: 54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.682193567263422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.682193567263422 | validation: 7.377291866048377]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.976372548212884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.976372548212884 | validation: 7.636956033314423]
	TIME [epoch: 8.45 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.696643365208126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.696643365208126 | validation: 5.390552149455568]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.3752045864508196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3752045864508196 | validation: 5.631017078461778]
	TIME [epoch: 8.48 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.232178361661967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.232178361661967 | validation: 5.42030259201074]
	TIME [epoch: 8.46 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.27347283782206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.27347283782206 | validation: 4.795294107620755]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.11469995734092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.11469995734092 | validation: 4.69676537544481]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.88826416203842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.88826416203842 | validation: 4.8831260065240425]
	TIME [epoch: 8.47 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.8769776675472025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8769776675472025 | validation: 4.584160660748029]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.663321957108799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.663321957108799 | validation: 4.8503176816402185]
	TIME [epoch: 8.46 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.734993632826564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.734993632826564 | validation: 4.636580955807451]
	TIME [epoch: 8.47 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.615711933854504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.615711933854504 | validation: 4.309937470067065]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.543990739360264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.543990739360264 | validation: 4.706053286162874]
	TIME [epoch: 8.44 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.760085336856638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.760085336856638 | validation: 4.113586356807557]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.5406059831721475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5406059831721475 | validation: 4.27600382480671]
	TIME [epoch: 8.45 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.341566749890453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.341566749890453 | validation: 5.717942243752233]
	TIME [epoch: 8.47 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.474546906287543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.474546906287543 | validation: 4.173903736142516]
	TIME [epoch: 8.46 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.281491291468495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.281491291468495 | validation: 4.2542363149317275]
	TIME [epoch: 8.45 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.240793977072743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.240793977072743 | validation: 3.9589224299873154]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.128415919492755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.128415919492755 | validation: 3.835966670066358]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.141908233721074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.141908233721074 | validation: 4.3407215296386426]
	TIME [epoch: 8.45 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.135632207960595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.135632207960595 | validation: 3.708351376408144]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.015268820850847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.015268820850847 | validation: 3.7709236609837515]
	TIME [epoch: 8.46 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.942392610361515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.942392610361515 | validation: 4.395776067618707]
	TIME [epoch: 8.44 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.927568360368414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.927568360368414 | validation: 4.0824813333728445]
	TIME [epoch: 8.46 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8850498121168733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8850498121168733 | validation: 3.5951583773704296]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7861244516336305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7861244516336305 | validation: 3.6390084806478136]
	TIME [epoch: 8.47 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8389209099058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8389209099058 | validation: 4.409781858833268]
	TIME [epoch: 8.45 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8980771425166645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8980771425166645 | validation: 4.012973324485238]
	TIME [epoch: 8.46 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.778638960744695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.778638960744695 | validation: 3.7482077624199013]
	TIME [epoch: 8.52 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.727884776920026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.727884776920026 | validation: 3.919884390976783]
	TIME [epoch: 8.47 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7357434393094273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7357434393094273 | validation: 3.85273989657807]
	TIME [epoch: 8.45 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6679986683773693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6679986683773693 | validation: 3.3196795160212855]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6789893329532903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6789893329532903 | validation: 3.318485669296545]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8124714896875807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8124714896875807 | validation: 3.3284066052731447]
	TIME [epoch: 8.5 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.755141012005848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.755141012005848 | validation: 3.5135634255537074]
	TIME [epoch: 8.45 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.603730854068766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.603730854068766 | validation: 3.42219985510663]
	TIME [epoch: 8.45 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6840851100096836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6840851100096836 | validation: 3.3593246435497734]
	TIME [epoch: 8.47 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6046983232416148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6046983232416148 | validation: 3.3231607029980994]
	TIME [epoch: 8.46 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5766494177618235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5766494177618235 | validation: 3.363706072672132]
	TIME [epoch: 8.45 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.553443920231441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.553443920231441 | validation: 3.6395288499495195]
	TIME [epoch: 8.44 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6159697162550835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6159697162550835 | validation: 3.3003170906898784]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6797483656894032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6797483656894032 | validation: 3.4758590884828555]
	TIME [epoch: 8.44 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5931975886001384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5931975886001384 | validation: 3.466108168167887]
	TIME [epoch: 8.45 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4808088145542486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4808088145542486 | validation: 4.365724799097995]
	TIME [epoch: 8.46 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5791105555352765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5791105555352765 | validation: 3.4547250091757187]
	TIME [epoch: 8.5 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.625881992414783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.625881992414783 | validation: 3.075552465295302]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5005376623269804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5005376623269804 | validation: 3.340218559292117]
	TIME [epoch: 8.45 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.442359897835888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.442359897835888 | validation: 3.293368587617505]
	TIME [epoch: 8.46 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4515077125871896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4515077125871896 | validation: 3.3959820052731367]
	TIME [epoch: 8.49 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.511292952851781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.511292952851781 | validation: 3.098114200752935]
	TIME [epoch: 8.46 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.520288292525496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.520288292525496 | validation: 3.213800845321502]
	TIME [epoch: 8.46 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6509164958278326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6509164958278326 | validation: 3.7050542087236673]
	TIME [epoch: 8.45 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.621521141856911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.621521141856911 | validation: 3.188596306295742]
	TIME [epoch: 8.48 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.511490759453481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.511490759453481 | validation: 3.71140402572745]
	TIME [epoch: 8.47 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5405101441581897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5405101441581897 | validation: 3.220482996509779]
	TIME [epoch: 8.45 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4972704753113133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4972704753113133 | validation: 3.2188708402326482]
	TIME [epoch: 8.46 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.426591554848547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.426591554848547 | validation: 3.3350515376627747]
	TIME [epoch: 8.47 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5741682806620583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5741682806620583 | validation: 3.493325816608502]
	TIME [epoch: 8.46 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4434394146861558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4434394146861558 | validation: 3.724188702301248]
	TIME [epoch: 8.46 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6004826040964226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6004826040964226 | validation: 3.240373230979105]
	TIME [epoch: 8.45 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5529205994034436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5529205994034436 | validation: 3.366193445399837]
	TIME [epoch: 8.52 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7343743042761957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7343743042761957 | validation: 4.314669717369132]
	TIME [epoch: 8.47 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6083871845150393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6083871845150393 | validation: 3.8022002246622524]
	TIME [epoch: 8.45 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6164457059744537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6164457059744537 | validation: 3.368882241028707]
	TIME [epoch: 8.45 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.540998738285689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.540998738285689 | validation: 3.370510324922149]
	TIME [epoch: 8.53 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4571967747821355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4571967747821355 | validation: 3.1293375336560088]
	TIME [epoch: 8.45 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3893006907016536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3893006907016536 | validation: 3.5263338236299493]
	TIME [epoch: 8.45 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4888158909966847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4888158909966847 | validation: 3.2716999081351643]
	TIME [epoch: 8.47 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.468562095239313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.468562095239313 | validation: 3.576106736756147]
	TIME [epoch: 8.46 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6489021824191163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6489021824191163 | validation: 4.02583569243599]
	TIME [epoch: 8.47 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.585548793689379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.585548793689379 | validation: 3.275973983901629]
	TIME [epoch: 8.46 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.405377436964072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.405377436964072 | validation: 3.3778328382864897]
	TIME [epoch: 8.48 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4794899940837416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4794899940837416 | validation: 3.3101164233490135]
	TIME [epoch: 8.45 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3674966518305545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3674966518305545 | validation: 3.5559843554298136]
	TIME [epoch: 8.48 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5744121926821757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5744121926821757 | validation: 3.8409340249164465]
	TIME [epoch: 8.47 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4235721990607253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4235721990607253 | validation: 3.4725483143621787]
	TIME [epoch: 8.48 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3586522328571093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3586522328571093 | validation: 3.272453816516972]
	TIME [epoch: 8.44 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4195210715994775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4195210715994775 | validation: 3.058893107828448]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.33814821938767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.33814821938767 | validation: 3.046646100347749]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.404367716891219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.404367716891219 | validation: 3.5937036579150865]
	TIME [epoch: 8.46 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3416304297186565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3416304297186565 | validation: 3.4822869533548806]
	TIME [epoch: 8.46 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.398653227467456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.398653227467456 | validation: 3.189537054635145]
	TIME [epoch: 8.48 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3782250079790814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3782250079790814 | validation: 3.1969622516096594]
	TIME [epoch: 8.44 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3969136955937373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3969136955937373 | validation: 3.5946505667827413]
	TIME [epoch: 8.45 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.520013215945543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.520013215945543 | validation: 3.384748207583451]
	TIME [epoch: 8.44 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3501497498980632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3501497498980632 | validation: 3.232522104449911]
	TIME [epoch: 8.47 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.440102614993699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.440102614993699 | validation: 3.2324417897531452]
	TIME [epoch: 8.49 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2753836023234677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2753836023234677 | validation: 3.1125723004707]
	TIME [epoch: 8.45 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3205522723224625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3205522723224625 | validation: 3.567176595699162]
	TIME [epoch: 8.47 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.336634476530199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.336634476530199 | validation: 3.417834384107995]
	TIME [epoch: 8.46 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3483012226239213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3483012226239213 | validation: 3.159863105242989]
	TIME [epoch: 8.48 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3291285595483338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3291285595483338 | validation: 3.0676291521666257]
	TIME [epoch: 8.45 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3709326910178006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3709326910178006 | validation: 3.149493187647448]
	TIME [epoch: 8.46 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.275962898533289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.275962898533289 | validation: 3.0258353692550877]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3997830210410553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3997830210410553 | validation: 3.160927377542299]
	TIME [epoch: 8.48 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.283957549665183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.283957549665183 | validation: 3.619449814191029]
	TIME [epoch: 8.45 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3184096287470117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3184096287470117 | validation: 3.1733256448580023]
	TIME [epoch: 8.45 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1968307593386323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1968307593386323 | validation: 3.2575330342636777]
	TIME [epoch: 8.45 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2750049849547422		[learning rate: 0.0099673]
	Learning Rate: 0.00996733
	LOSS [training: 3.2750049849547422 | validation: 3.2628520939614627]
	TIME [epoch: 8.47 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4002275973147		[learning rate: 0.0099312]
	Learning Rate: 0.00993116
	LOSS [training: 3.4002275973147 | validation: 3.6915653568648827]
	TIME [epoch: 8.46 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5211044907712092		[learning rate: 0.0098951]
	Learning Rate: 0.00989512
	LOSS [training: 3.5211044907712092 | validation: 3.1253340984606988]
	TIME [epoch: 8.45 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.174861790126241		[learning rate: 0.0098592]
	Learning Rate: 0.00985921
	LOSS [training: 3.174861790126241 | validation: 3.0914885511650687]
	TIME [epoch: 8.45 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.351226708305912		[learning rate: 0.0098234]
	Learning Rate: 0.00982343
	LOSS [training: 3.351226708305912 | validation: 3.0241555166973]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.306845766663969		[learning rate: 0.0097878]
	Learning Rate: 0.00978778
	LOSS [training: 3.306845766663969 | validation: 3.26560824614433]
	TIME [epoch: 8.44 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2327630281575255		[learning rate: 0.0097523]
	Learning Rate: 0.00975226
	LOSS [training: 3.2327630281575255 | validation: 3.267306619392172]
	TIME [epoch: 8.45 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2562415020123345		[learning rate: 0.0097169]
	Learning Rate: 0.00971687
	LOSS [training: 3.2562415020123345 | validation: 3.541239270620554]
	TIME [epoch: 8.5 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2366021395192432		[learning rate: 0.0096816]
	Learning Rate: 0.0096816
	LOSS [training: 3.2366021395192432 | validation: 3.017853997698288]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.199110930821056		[learning rate: 0.0096465]
	Learning Rate: 0.00964647
	LOSS [training: 3.199110930821056 | validation: 3.1159563671414228]
	TIME [epoch: 8.47 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2788541760778704		[learning rate: 0.0096115]
	Learning Rate: 0.00961146
	LOSS [training: 3.2788541760778704 | validation: 3.0448208184278194]
	TIME [epoch: 8.45 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1463227246748806		[learning rate: 0.0095766]
	Learning Rate: 0.00957658
	LOSS [training: 3.1463227246748806 | validation: 2.981274199608894]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3238681084664647		[learning rate: 0.0095418]
	Learning Rate: 0.00954183
	LOSS [training: 3.3238681084664647 | validation: 3.297353485949275]
	TIME [epoch: 8.46 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.247790342002694		[learning rate: 0.0095072]
	Learning Rate: 0.0095072
	LOSS [training: 3.247790342002694 | validation: 2.9089961785327008]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2048235852834694		[learning rate: 0.0094727]
	Learning Rate: 0.0094727
	LOSS [training: 3.2048235852834694 | validation: 3.0133099001004764]
	TIME [epoch: 8.44 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1519201601388405		[learning rate: 0.0094383]
	Learning Rate: 0.00943832
	LOSS [training: 3.1519201601388405 | validation: 3.1559771784915776]
	TIME [epoch: 8.45 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3067068253283276		[learning rate: 0.0094041]
	Learning Rate: 0.00940407
	LOSS [training: 3.3067068253283276 | validation: 2.9298354411465812]
	TIME [epoch: 8.44 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.19149359094033		[learning rate: 0.0093699]
	Learning Rate: 0.00936994
	LOSS [training: 3.19149359094033 | validation: 2.968812779023707]
	TIME [epoch: 8.46 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.30576523507404		[learning rate: 0.0093359]
	Learning Rate: 0.00933594
	LOSS [training: 3.30576523507404 | validation: 3.539745489506464]
	TIME [epoch: 8.46 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.32738550010174		[learning rate: 0.0093021]
	Learning Rate: 0.00930206
	LOSS [training: 3.32738550010174 | validation: 3.0663738330871575]
	TIME [epoch: 8.45 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1036310789409085		[learning rate: 0.0092683]
	Learning Rate: 0.0092683
	LOSS [training: 3.1036310789409085 | validation: 3.1417372452603076]
	TIME [epoch: 8.49 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1568208599171084		[learning rate: 0.0092347]
	Learning Rate: 0.00923466
	LOSS [training: 3.1568208599171084 | validation: 3.0693299053374097]
	TIME [epoch: 8.44 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.199143983261659		[learning rate: 0.0092011]
	Learning Rate: 0.00920115
	LOSS [training: 3.199143983261659 | validation: 3.0761277654118437]
	TIME [epoch: 8.46 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.188491304374716		[learning rate: 0.0091678]
	Learning Rate: 0.00916776
	LOSS [training: 3.188491304374716 | validation: 3.267659643654409]
	TIME [epoch: 8.45 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.273183183682781		[learning rate: 0.0091345]
	Learning Rate: 0.00913449
	LOSS [training: 3.273183183682781 | validation: 3.3277004226117084]
	TIME [epoch: 8.46 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1589790549648122		[learning rate: 0.0091013]
	Learning Rate: 0.00910134
	LOSS [training: 3.1589790549648122 | validation: 3.510741547108296]
	TIME [epoch: 8.45 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.162236109990618		[learning rate: 0.0090683]
	Learning Rate: 0.00906831
	LOSS [training: 3.162236109990618 | validation: 2.925179980707483]
	TIME [epoch: 8.47 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.277297896875619		[learning rate: 0.0090354]
	Learning Rate: 0.0090354
	LOSS [training: 3.277297896875619 | validation: 3.17810109958544]
	TIME [epoch: 8.45 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1369561943677824		[learning rate: 0.0090026]
	Learning Rate: 0.00900261
	LOSS [training: 3.1369561943677824 | validation: 3.1245208528364787]
	TIME [epoch: 8.48 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1959990097193063		[learning rate: 0.0089699]
	Learning Rate: 0.00896994
	LOSS [training: 3.1959990097193063 | validation: 3.253774477510233]
	TIME [epoch: 8.46 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.19167103374335		[learning rate: 0.0089374]
	Learning Rate: 0.00893739
	LOSS [training: 3.19167103374335 | validation: 2.914272406111076]
	TIME [epoch: 8.47 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.170304960498127		[learning rate: 0.008905]
	Learning Rate: 0.00890495
	LOSS [training: 3.170304960498127 | validation: 3.1227637420576477]
	TIME [epoch: 8.45 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1479598843713203		[learning rate: 0.0088726]
	Learning Rate: 0.00887263
	LOSS [training: 3.1479598843713203 | validation: 3.3807938074493036]
	TIME [epoch: 8.45 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.267008835117937		[learning rate: 0.0088404]
	Learning Rate: 0.00884044
	LOSS [training: 3.267008835117937 | validation: 3.909628102115744]
	TIME [epoch: 8.46 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2992190311565084		[learning rate: 0.0088084]
	Learning Rate: 0.00880835
	LOSS [training: 3.2992190311565084 | validation: 2.9169356381040643]
	TIME [epoch: 8.46 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1032302324780927		[learning rate: 0.0087764]
	Learning Rate: 0.00877639
	LOSS [training: 3.1032302324780927 | validation: 3.9115972396229086]
	TIME [epoch: 8.46 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.187157508912519		[learning rate: 0.0087445]
	Learning Rate: 0.00874454
	LOSS [training: 3.187157508912519 | validation: 3.1171252743081066]
	TIME [epoch: 8.45 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.119086799312437		[learning rate: 0.0087128]
	Learning Rate: 0.0087128
	LOSS [training: 3.119086799312437 | validation: 3.048266849848196]
	TIME [epoch: 8.45 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.069721747409743		[learning rate: 0.0086812]
	Learning Rate: 0.00868118
	LOSS [training: 3.069721747409743 | validation: 3.209100841362449]
	TIME [epoch: 8.46 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1582844245569768		[learning rate: 0.0086497]
	Learning Rate: 0.00864968
	LOSS [training: 3.1582844245569768 | validation: 3.678708645781758]
	TIME [epoch: 8.46 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.265901345162323		[learning rate: 0.0086183]
	Learning Rate: 0.00861829
	LOSS [training: 3.265901345162323 | validation: 3.1825088428023696]
	TIME [epoch: 8.45 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.180514818788698		[learning rate: 0.008587]
	Learning Rate: 0.00858701
	LOSS [training: 3.180514818788698 | validation: 3.0107958901084517]
	TIME [epoch: 8.45 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.291725819061263		[learning rate: 0.0085559]
	Learning Rate: 0.00855585
	LOSS [training: 3.291725819061263 | validation: 3.0476167813040345]
	TIME [epoch: 8.44 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.155023595318528		[learning rate: 0.0085248]
	Learning Rate: 0.0085248
	LOSS [training: 3.155023595318528 | validation: 3.4704231373476038]
	TIME [epoch: 8.47 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.264915784037947		[learning rate: 0.0084939]
	Learning Rate: 0.00849386
	LOSS [training: 3.264915784037947 | validation: 3.1039296033194157]
	TIME [epoch: 8.44 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.132125414690828		[learning rate: 0.008463]
	Learning Rate: 0.00846304
	LOSS [training: 3.132125414690828 | validation: 3.104352465619137]
	TIME [epoch: 8.45 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2111342912595533		[learning rate: 0.0084323]
	Learning Rate: 0.00843233
	LOSS [training: 3.2111342912595533 | validation: 3.1084343153723637]
	TIME [epoch: 8.45 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1685995105637486		[learning rate: 0.0084017]
	Learning Rate: 0.00840172
	LOSS [training: 3.1685995105637486 | validation: 3.015814790558366]
	TIME [epoch: 8.48 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1344125918311025		[learning rate: 0.0083712]
	Learning Rate: 0.00837123
	LOSS [training: 3.1344125918311025 | validation: 3.5695641548298482]
	TIME [epoch: 8.45 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3964010599206214		[learning rate: 0.0083409]
	Learning Rate: 0.00834085
	LOSS [training: 3.3964010599206214 | validation: 3.0312983571641183]
	TIME [epoch: 8.45 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1452527915109227		[learning rate: 0.0083106]
	Learning Rate: 0.00831059
	LOSS [training: 3.1452527915109227 | validation: 2.9575625187206565]
	TIME [epoch: 8.46 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.217614619370791		[learning rate: 0.0082804]
	Learning Rate: 0.00828043
	LOSS [training: 3.217614619370791 | validation: 3.1982680997698125]
	TIME [epoch: 8.5 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.129723679723681		[learning rate: 0.0082504]
	Learning Rate: 0.00825037
	LOSS [training: 3.129723679723681 | validation: 3.0679324730890087]
	TIME [epoch: 8.43 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0834906554427817		[learning rate: 0.0082204]
	Learning Rate: 0.00822043
	LOSS [training: 3.0834906554427817 | validation: 3.0815415972283953]
	TIME [epoch: 8.46 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.084888873932033		[learning rate: 0.0081906]
	Learning Rate: 0.0081906
	LOSS [training: 3.084888873932033 | validation: 2.9620786976608264]
	TIME [epoch: 8.5 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.116377412266446		[learning rate: 0.0081609]
	Learning Rate: 0.00816088
	LOSS [training: 3.116377412266446 | validation: 3.3539425465078385]
	TIME [epoch: 8.47 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1317861975256		[learning rate: 0.0081313]
	Learning Rate: 0.00813126
	LOSS [training: 3.1317861975256 | validation: 3.3231377409315486]
	TIME [epoch: 8.45 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1495078638248524		[learning rate: 0.0081018]
	Learning Rate: 0.00810175
	LOSS [training: 3.1495078638248524 | validation: 2.946874897424072]
	TIME [epoch: 8.45 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2386352313921924		[learning rate: 0.0080724]
	Learning Rate: 0.00807235
	LOSS [training: 3.2386352313921924 | validation: 3.183798014510181]
	TIME [epoch: 8.45 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.116005491504397		[learning rate: 0.0080431]
	Learning Rate: 0.00804305
	LOSS [training: 3.116005491504397 | validation: 3.217639447916226]
	TIME [epoch: 8.49 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0920710075843223		[learning rate: 0.0080139]
	Learning Rate: 0.00801387
	LOSS [training: 3.0920710075843223 | validation: 2.959012137791648]
	TIME [epoch: 8.48 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1287191713486875		[learning rate: 0.0079848]
	Learning Rate: 0.00798478
	LOSS [training: 3.1287191713486875 | validation: 2.884276689154898]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.077728896866044		[learning rate: 0.0079558]
	Learning Rate: 0.00795581
	LOSS [training: 3.077728896866044 | validation: 4.160475006042013]
	TIME [epoch: 8.45 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.187790240597184		[learning rate: 0.0079269]
	Learning Rate: 0.00792693
	LOSS [training: 3.187790240597184 | validation: 2.908257900750545]
	TIME [epoch: 8.48 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.133942895699458		[learning rate: 0.0078982]
	Learning Rate: 0.00789817
	LOSS [training: 3.133942895699458 | validation: 3.1668425626176933]
	TIME [epoch: 8.45 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.094092284473835		[learning rate: 0.0078695]
	Learning Rate: 0.0078695
	LOSS [training: 3.094092284473835 | validation: 2.8599124713463246]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.036108647295555		[learning rate: 0.0078409]
	Learning Rate: 0.00784094
	LOSS [training: 3.036108647295555 | validation: 3.0305539767247387]
	TIME [epoch: 8.46 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.073669040244197		[learning rate: 0.0078125]
	Learning Rate: 0.00781249
	LOSS [training: 3.073669040244197 | validation: 2.916603437518945]
	TIME [epoch: 8.48 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0397378371143793		[learning rate: 0.0077841]
	Learning Rate: 0.00778414
	LOSS [training: 3.0397378371143793 | validation: 3.018984381956814]
	TIME [epoch: 8.45 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.111196643710616		[learning rate: 0.0077559]
	Learning Rate: 0.00775589
	LOSS [training: 3.111196643710616 | validation: 3.3168692262504633]
	TIME [epoch: 8.45 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.340494463651173		[learning rate: 0.0077277]
	Learning Rate: 0.00772774
	LOSS [training: 3.340494463651173 | validation: 3.471663675898859]
	TIME [epoch: 8.43 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.285355644852968		[learning rate: 0.0076997]
	Learning Rate: 0.0076997
	LOSS [training: 3.285355644852968 | validation: 3.0193544700442994]
	TIME [epoch: 8.44 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1093685307593084		[learning rate: 0.0076718]
	Learning Rate: 0.00767176
	LOSS [training: 3.1093685307593084 | validation: 3.0679452719651783]
	TIME [epoch: 8.47 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0991310178341616		[learning rate: 0.0076439]
	Learning Rate: 0.00764391
	LOSS [training: 3.0991310178341616 | validation: 3.0448867842840572]
	TIME [epoch: 8.45 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.175778314605296		[learning rate: 0.0076162]
	Learning Rate: 0.00761617
	LOSS [training: 3.175778314605296 | validation: 3.094718696355731]
	TIME [epoch: 8.45 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.321519295744783		[learning rate: 0.0075885]
	Learning Rate: 0.00758853
	LOSS [training: 3.321519295744783 | validation: 2.9546787443942253]
	TIME [epoch: 8.45 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1957156828127005		[learning rate: 0.007561]
	Learning Rate: 0.00756099
	LOSS [training: 3.1957156828127005 | validation: 3.1181559617640215]
	TIME [epoch: 8.47 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1142183490274173		[learning rate: 0.0075336]
	Learning Rate: 0.00753356
	LOSS [training: 3.1142183490274173 | validation: 2.890343782687019]
	TIME [epoch: 8.45 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.017209664053113		[learning rate: 0.0075062]
	Learning Rate: 0.00750622
	LOSS [training: 3.017209664053113 | validation: 2.835867525040377]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0841407257151827		[learning rate: 0.007479]
	Learning Rate: 0.00747897
	LOSS [training: 3.0841407257151827 | validation: 3.0613251967021076]
	TIME [epoch: 8.46 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1559901488841406		[learning rate: 0.0074518]
	Learning Rate: 0.00745183
	LOSS [training: 3.1559901488841406 | validation: 3.079648455871051]
	TIME [epoch: 8.48 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037190660208727		[learning rate: 0.0074248]
	Learning Rate: 0.00742479
	LOSS [training: 3.037190660208727 | validation: 3.33628866786684]
	TIME [epoch: 8.46 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0860499561258052		[learning rate: 0.0073978]
	Learning Rate: 0.00739784
	LOSS [training: 3.0860499561258052 | validation: 3.432647950173219]
	TIME [epoch: 8.46 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1335940048844777		[learning rate: 0.007371]
	Learning Rate: 0.007371
	LOSS [training: 3.1335940048844777 | validation: 3.2896250033788084]
	TIME [epoch: 8.46 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.251783879798841		[learning rate: 0.0073442]
	Learning Rate: 0.00734425
	LOSS [training: 3.251783879798841 | validation: 2.926467471198439]
	TIME [epoch: 8.45 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.049842758584168		[learning rate: 0.0073176]
	Learning Rate: 0.0073176
	LOSS [training: 3.049842758584168 | validation: 2.9707552158927366]
	TIME [epoch: 8.48 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0022738350744613		[learning rate: 0.007291]
	Learning Rate: 0.00729104
	LOSS [training: 3.0022738350744613 | validation: 3.030387239429694]
	TIME [epoch: 8.46 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.118701598555055		[learning rate: 0.0072646]
	Learning Rate: 0.00726458
	LOSS [training: 3.118701598555055 | validation: 2.890451145020575]
	TIME [epoch: 8.45 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.990276324477642		[learning rate: 0.0072382]
	Learning Rate: 0.00723822
	LOSS [training: 2.990276324477642 | validation: 3.088420032235611]
	TIME [epoch: 8.46 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.986790385350889		[learning rate: 0.0072119]
	Learning Rate: 0.00721195
	LOSS [training: 2.986790385350889 | validation: 3.069919943521252]
	TIME [epoch: 8.48 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9379373287074046		[learning rate: 0.0071858]
	Learning Rate: 0.00718578
	LOSS [training: 2.9379373287074046 | validation: 2.9097354693344037]
	TIME [epoch: 8.46 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.943505971730607		[learning rate: 0.0071597]
	Learning Rate: 0.0071597
	LOSS [training: 2.943505971730607 | validation: 2.9663662775432815]
	TIME [epoch: 8.45 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0093110805283776		[learning rate: 0.0071337]
	Learning Rate: 0.00713371
	LOSS [training: 3.0093110805283776 | validation: 2.924937024750699]
	TIME [epoch: 8.46 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9120072983328464		[learning rate: 0.0071078]
	Learning Rate: 0.00710783
	LOSS [training: 2.9120072983328464 | validation: 3.204873502959985]
	TIME [epoch: 8.47 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.018759411292275		[learning rate: 0.007082]
	Learning Rate: 0.00708203
	LOSS [training: 3.018759411292275 | validation: 3.127868898912144]
	TIME [epoch: 8.47 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9881058087344643		[learning rate: 0.0070563]
	Learning Rate: 0.00705633
	LOSS [training: 2.9881058087344643 | validation: 2.795525495099671]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.015331966946338		[learning rate: 0.0070307]
	Learning Rate: 0.00703072
	LOSS [training: 3.015331966946338 | validation: 2.8710818360528862]
	TIME [epoch: 8.45 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.998082365963362		[learning rate: 0.0070052]
	Learning Rate: 0.00700521
	LOSS [training: 2.998082365963362 | validation: 3.455537667912907]
	TIME [epoch: 8.46 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0446271421938738		[learning rate: 0.0069798]
	Learning Rate: 0.00697979
	LOSS [training: 3.0446271421938738 | validation: 3.3349302045102824]
	TIME [epoch: 8.52 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2617212468199903		[learning rate: 0.0069545]
	Learning Rate: 0.00695446
	LOSS [training: 3.2617212468199903 | validation: 2.9234497110366844]
	TIME [epoch: 8.44 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9747246341149944		[learning rate: 0.0069292]
	Learning Rate: 0.00692922
	LOSS [training: 2.9747246341149944 | validation: 2.8775658593466593]
	TIME [epoch: 8.45 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.948400926443533		[learning rate: 0.0069041]
	Learning Rate: 0.00690407
	LOSS [training: 2.948400926443533 | validation: 3.0615630698033254]
	TIME [epoch: 8.46 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0172895048219175		[learning rate: 0.006879]
	Learning Rate: 0.00687902
	LOSS [training: 3.0172895048219175 | validation: 3.3486838155189202]
	TIME [epoch: 8.48 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9811138075313446		[learning rate: 0.0068541]
	Learning Rate: 0.00685405
	LOSS [training: 2.9811138075313446 | validation: 2.884816063091771]
	TIME [epoch: 8.5 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0494300330648842		[learning rate: 0.0068292]
	Learning Rate: 0.00682918
	LOSS [training: 3.0494300330648842 | validation: 3.099741230848476]
	TIME [epoch: 8.47 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.953465984891585		[learning rate: 0.0068044]
	Learning Rate: 0.00680439
	LOSS [training: 2.953465984891585 | validation: 2.808884566077368]
	TIME [epoch: 8.45 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2007895698407722		[learning rate: 0.0067797]
	Learning Rate: 0.0067797
	LOSS [training: 3.2007895698407722 | validation: 2.8616645025455614]
	TIME [epoch: 8.48 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.039668892404389		[learning rate: 0.0067551]
	Learning Rate: 0.0067551
	LOSS [training: 3.039668892404389 | validation: 2.876670303404133]
	TIME [epoch: 8.47 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9709977035120247		[learning rate: 0.0067306]
	Learning Rate: 0.00673058
	LOSS [training: 2.9709977035120247 | validation: 2.8413945244348087]
	TIME [epoch: 8.45 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.928359679788967		[learning rate: 0.0067062]
	Learning Rate: 0.00670616
	LOSS [training: 2.928359679788967 | validation: 2.9501426499951533]
	TIME [epoch: 8.45 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9606079123940274		[learning rate: 0.0066818]
	Learning Rate: 0.00668182
	LOSS [training: 2.9606079123940274 | validation: 3.0189436478089897]
	TIME [epoch: 8.51 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.986076807266884		[learning rate: 0.0066576]
	Learning Rate: 0.00665757
	LOSS [training: 2.986076807266884 | validation: 2.929179665555787]
	TIME [epoch: 8.47 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9752963368413603		[learning rate: 0.0066334]
	Learning Rate: 0.00663341
	LOSS [training: 2.9752963368413603 | validation: 3.0684259346014073]
	TIME [epoch: 8.44 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9395182154887918		[learning rate: 0.0066093]
	Learning Rate: 0.00660934
	LOSS [training: 2.9395182154887918 | validation: 2.9765654051237345]
	TIME [epoch: 8.45 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.045624640577797		[learning rate: 0.0065854]
	Learning Rate: 0.00658535
	LOSS [training: 3.045624640577797 | validation: 2.779787431047238]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.979236542661864		[learning rate: 0.0065615]
	Learning Rate: 0.00656145
	LOSS [training: 2.979236542661864 | validation: 3.1834609641353486]
	TIME [epoch: 8.48 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.114093819042633		[learning rate: 0.0065376]
	Learning Rate: 0.00653764
	LOSS [training: 3.114093819042633 | validation: 2.8836158886818923]
	TIME [epoch: 8.44 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1137651771884984		[learning rate: 0.0065139]
	Learning Rate: 0.00651392
	LOSS [training: 3.1137651771884984 | validation: 2.8875001027667073]
	TIME [epoch: 8.46 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9852529891596133		[learning rate: 0.0064903]
	Learning Rate: 0.00649028
	LOSS [training: 2.9852529891596133 | validation: 2.9034246705066726]
	TIME [epoch: 8.46 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0601384059742402		[learning rate: 0.0064667]
	Learning Rate: 0.00646672
	LOSS [training: 3.0601384059742402 | validation: 2.832488865880856]
	TIME [epoch: 8.47 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.013952645507396		[learning rate: 0.0064433]
	Learning Rate: 0.00644325
	LOSS [training: 3.013952645507396 | validation: 3.202935566139362]
	TIME [epoch: 8.46 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.024482588775183		[learning rate: 0.0064199]
	Learning Rate: 0.00641987
	LOSS [training: 3.024482588775183 | validation: 2.9880215517990187]
	TIME [epoch: 8.49 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.94260260598754		[learning rate: 0.0063966]
	Learning Rate: 0.00639657
	LOSS [training: 2.94260260598754 | validation: 2.8725941259178995]
	TIME [epoch: 8.44 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9842016708992505		[learning rate: 0.0063734]
	Learning Rate: 0.00637336
	LOSS [training: 2.9842016708992505 | validation: 3.3479344052430564]
	TIME [epoch: 8.47 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0055195143716755		[learning rate: 0.0063502]
	Learning Rate: 0.00635023
	LOSS [training: 3.0055195143716755 | validation: 2.8579807314228294]
	TIME [epoch: 8.45 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.960717004044713		[learning rate: 0.0063272]
	Learning Rate: 0.00632718
	LOSS [training: 2.960717004044713 | validation: 3.0421950720651423]
	TIME [epoch: 8.45 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9542216534513637		[learning rate: 0.0063042]
	Learning Rate: 0.00630422
	LOSS [training: 2.9542216534513637 | validation: 2.788412023046348]
	TIME [epoch: 8.46 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.851679344124146		[learning rate: 0.0062813]
	Learning Rate: 0.00628134
	LOSS [training: 2.851679344124146 | validation: 2.7440517780126354]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9999431336480784		[learning rate: 0.0062585]
	Learning Rate: 0.00625855
	LOSS [training: 2.9999431336480784 | validation: 3.0043072800153734]
	TIME [epoch: 8.48 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.921803375258181		[learning rate: 0.0062358]
	Learning Rate: 0.00623584
	LOSS [training: 2.921803375258181 | validation: 2.754700342947436]
	TIME [epoch: 8.47 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.962823200808088		[learning rate: 0.0062132]
	Learning Rate: 0.00621321
	LOSS [training: 2.962823200808088 | validation: 2.8418779950624815]
	TIME [epoch: 8.45 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.922986794662908		[learning rate: 0.0061907]
	Learning Rate: 0.00619066
	LOSS [training: 2.922986794662908 | validation: 3.3010488441789185]
	TIME [epoch: 8.47 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.999902331837507		[learning rate: 0.0061682]
	Learning Rate: 0.00616819
	LOSS [training: 2.999902331837507 | validation: 2.8857161314309967]
	TIME [epoch: 8.48 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9346338267995686		[learning rate: 0.0061458]
	Learning Rate: 0.00614581
	LOSS [training: 2.9346338267995686 | validation: 3.422056422485799]
	TIME [epoch: 8.47 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.018932132666289		[learning rate: 0.0061235]
	Learning Rate: 0.0061235
	LOSS [training: 3.018932132666289 | validation: 2.7985229691249973]
	TIME [epoch: 8.46 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.897379297809424		[learning rate: 0.0061013]
	Learning Rate: 0.00610128
	LOSS [training: 2.897379297809424 | validation: 2.716660850476197]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.884137483297248		[learning rate: 0.0060791]
	Learning Rate: 0.00607914
	LOSS [training: 2.884137483297248 | validation: 3.0971998863450474]
	TIME [epoch: 8.48 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8678084556305357		[learning rate: 0.0060571]
	Learning Rate: 0.00605708
	LOSS [training: 2.8678084556305357 | validation: 2.707078873895257]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9179602227635617		[learning rate: 0.0060351]
	Learning Rate: 0.0060351
	LOSS [training: 2.9179602227635617 | validation: 3.092154678962724]
	TIME [epoch: 8.44 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.888084928442347		[learning rate: 0.0060132]
	Learning Rate: 0.00601319
	LOSS [training: 2.888084928442347 | validation: 2.8627097355311975]
	TIME [epoch: 8.46 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.939895993948075		[learning rate: 0.0059914]
	Learning Rate: 0.00599137
	LOSS [training: 2.939895993948075 | validation: 2.735039799730975]
	TIME [epoch: 8.45 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9463646361879676		[learning rate: 0.0059696]
	Learning Rate: 0.00596963
	LOSS [training: 2.9463646361879676 | validation: 2.813734105832357]
	TIME [epoch: 8.46 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.02146844600301		[learning rate: 0.005948]
	Learning Rate: 0.00594797
	LOSS [training: 3.02146844600301 | validation: 2.8350995425830274]
	TIME [epoch: 8.42 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8719715258675125		[learning rate: 0.0059264]
	Learning Rate: 0.00592638
	LOSS [training: 2.8719715258675125 | validation: 2.93132929549108]
	TIME [epoch: 8.42 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.959041245038125		[learning rate: 0.0059049]
	Learning Rate: 0.00590487
	LOSS [training: 2.959041245038125 | validation: 2.808821781799237]
	TIME [epoch: 8.43 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.829854969514768		[learning rate: 0.0058834]
	Learning Rate: 0.00588344
	LOSS [training: 2.829854969514768 | validation: 2.792253295670548]
	TIME [epoch: 8.45 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.912166161785573		[learning rate: 0.0058621]
	Learning Rate: 0.00586209
	LOSS [training: 2.912166161785573 | validation: 2.712089922241201]
	TIME [epoch: 8.42 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.365679932653502		[learning rate: 0.0058408]
	Learning Rate: 0.00584082
	LOSS [training: 3.365679932653502 | validation: 2.73253450193865]
	TIME [epoch: 8.42 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9295331773670124		[learning rate: 0.0058196]
	Learning Rate: 0.00581962
	LOSS [training: 2.9295331773670124 | validation: 2.8224946340078656]
	TIME [epoch: 8.42 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.901607979761209		[learning rate: 0.0057985]
	Learning Rate: 0.0057985
	LOSS [training: 2.901607979761209 | validation: 2.766095303066039]
	TIME [epoch: 8.44 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.883378401545565		[learning rate: 0.0057775]
	Learning Rate: 0.00577746
	LOSS [training: 2.883378401545565 | validation: 2.989494016779153]
	TIME [epoch: 8.42 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.86952530810515		[learning rate: 0.0057565]
	Learning Rate: 0.00575649
	LOSS [training: 2.86952530810515 | validation: 2.6608750533010443]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8279299624303738		[learning rate: 0.0057356]
	Learning Rate: 0.0057356
	LOSS [training: 2.8279299624303738 | validation: 3.1479877047729063]
	TIME [epoch: 8.42 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.866259283606077		[learning rate: 0.0057148]
	Learning Rate: 0.00571479
	LOSS [training: 2.866259283606077 | validation: 2.6757445355105327]
	TIME [epoch: 8.45 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8628458423379333		[learning rate: 0.005694]
	Learning Rate: 0.00569405
	LOSS [training: 2.8628458423379333 | validation: 2.738902960225308]
	TIME [epoch: 8.43 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9711891693382255		[learning rate: 0.0056734]
	Learning Rate: 0.00567338
	LOSS [training: 2.9711891693382255 | validation: 2.7202088907308646]
	TIME [epoch: 8.42 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.818662992245662		[learning rate: 0.0056528]
	Learning Rate: 0.00565279
	LOSS [training: 2.818662992245662 | validation: 2.6581309736642753]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9218483978432133		[learning rate: 0.0056323]
	Learning Rate: 0.00563228
	LOSS [training: 2.9218483978432133 | validation: 2.8291729034073447]
	TIME [epoch: 8.42 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8444109934637676		[learning rate: 0.0056118]
	Learning Rate: 0.00561184
	LOSS [training: 2.8444109934637676 | validation: 2.8974500805110797]
	TIME [epoch: 8.46 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8983661909493827		[learning rate: 0.0055915]
	Learning Rate: 0.00559147
	LOSS [training: 2.8983661909493827 | validation: 2.9034473421586817]
	TIME [epoch: 8.42 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.853547726878905		[learning rate: 0.0055712]
	Learning Rate: 0.00557118
	LOSS [training: 2.853547726878905 | validation: 2.815947507917935]
	TIME [epoch: 8.41 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9811279889281534		[learning rate: 0.005551]
	Learning Rate: 0.00555096
	LOSS [training: 2.9811279889281534 | validation: 2.981650331191662]
	TIME [epoch: 8.42 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8655379677437187		[learning rate: 0.0055308]
	Learning Rate: 0.00553082
	LOSS [training: 2.8655379677437187 | validation: 2.7240119647656296]
	TIME [epoch: 8.47 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8980732859167566		[learning rate: 0.0055107]
	Learning Rate: 0.00551075
	LOSS [training: 2.8980732859167566 | validation: 3.193365317764279]
	TIME [epoch: 8.46 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.055016929404924		[learning rate: 0.0054907]
	Learning Rate: 0.00549075
	LOSS [training: 3.055016929404924 | validation: 2.833997039238089]
	TIME [epoch: 8.42 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.894927904280653		[learning rate: 0.0054708]
	Learning Rate: 0.00547082
	LOSS [training: 2.894927904280653 | validation: 2.960828126577261]
	TIME [epoch: 8.42 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8548659375401897		[learning rate: 0.005451]
	Learning Rate: 0.00545097
	LOSS [training: 2.8548659375401897 | validation: 2.7613980678570282]
	TIME [epoch: 8.46 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.894430758540715		[learning rate: 0.0054312]
	Learning Rate: 0.00543119
	LOSS [training: 2.894430758540715 | validation: 2.918620395850854]
	TIME [epoch: 8.45 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.959339889638848		[learning rate: 0.0054115]
	Learning Rate: 0.00541148
	LOSS [training: 2.959339889638848 | validation: 2.9175340649307255]
	TIME [epoch: 8.44 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8335431512980813		[learning rate: 0.0053918]
	Learning Rate: 0.00539184
	LOSS [training: 2.8335431512980813 | validation: 2.8520784063308344]
	TIME [epoch: 8.44 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9018411904486854		[learning rate: 0.0053723]
	Learning Rate: 0.00537227
	LOSS [training: 2.9018411904486854 | validation: 2.8410801895858357]
	TIME [epoch: 8.45 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8940362902554915		[learning rate: 0.0053528]
	Learning Rate: 0.00535277
	LOSS [training: 2.8940362902554915 | validation: 2.9916367209155545]
	TIME [epoch: 8.49 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9579792957790785		[learning rate: 0.0053333]
	Learning Rate: 0.00533335
	LOSS [training: 2.9579792957790785 | validation: 2.68554296916232]
	TIME [epoch: 8.45 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8160615967379123		[learning rate: 0.005314]
	Learning Rate: 0.00531399
	LOSS [training: 2.8160615967379123 | validation: 2.9151509087169885]
	TIME [epoch: 8.44 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8314293861278346		[learning rate: 0.0052947]
	Learning Rate: 0.00529471
	LOSS [training: 2.8314293861278346 | validation: 2.8829083105961044]
	TIME [epoch: 8.43 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7872243293249617		[learning rate: 0.0052755]
	Learning Rate: 0.00527549
	LOSS [training: 2.7872243293249617 | validation: 3.1286224162154075]
	TIME [epoch: 8.48 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.050797446604633		[learning rate: 0.0052563]
	Learning Rate: 0.00525635
	LOSS [training: 3.050797446604633 | validation: 2.7808115182311806]
	TIME [epoch: 8.44 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7607754118332206		[learning rate: 0.0052373]
	Learning Rate: 0.00523727
	LOSS [training: 2.7607754118332206 | validation: 2.845199009491054]
	TIME [epoch: 8.44 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.875656872517561		[learning rate: 0.0052183]
	Learning Rate: 0.00521827
	LOSS [training: 2.875656872517561 | validation: 3.440124569481575]
	TIME [epoch: 8.43 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.949154068399227		[learning rate: 0.0051993]
	Learning Rate: 0.00519933
	LOSS [training: 2.949154068399227 | validation: 2.816648094664606]
	TIME [epoch: 8.45 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9393119840426833		[learning rate: 0.0051805]
	Learning Rate: 0.00518046
	LOSS [training: 2.9393119840426833 | validation: 2.846098284750217]
	TIME [epoch: 8.43 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7425128482283254		[learning rate: 0.0051617]
	Learning Rate: 0.00516166
	LOSS [training: 2.7425128482283254 | validation: 3.2460321421409217]
	TIME [epoch: 8.42 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.834579718104402		[learning rate: 0.0051429]
	Learning Rate: 0.00514293
	LOSS [training: 2.834579718104402 | validation: 3.0588519416387885]
	TIME [epoch: 8.45 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8605136235797497		[learning rate: 0.0051243]
	Learning Rate: 0.00512426
	LOSS [training: 2.8605136235797497 | validation: 3.1765864563880886]
	TIME [epoch: 8.45 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8816208510193775		[learning rate: 0.0051057]
	Learning Rate: 0.00510567
	LOSS [training: 2.8816208510193775 | validation: 2.891597600104241]
	TIME [epoch: 8.45 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.89701108789403		[learning rate: 0.0050871]
	Learning Rate: 0.00508714
	LOSS [training: 2.89701108789403 | validation: 2.7638789280449254]
	TIME [epoch: 8.43 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7935827290163147		[learning rate: 0.0050687]
	Learning Rate: 0.00506868
	LOSS [training: 2.7935827290163147 | validation: 2.7336031451545253]
	TIME [epoch: 8.44 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.816595316625329		[learning rate: 0.0050503]
	Learning Rate: 0.00505028
	LOSS [training: 2.816595316625329 | validation: 2.6921123962591356]
	TIME [epoch: 8.47 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7701884148982754		[learning rate: 0.005032]
	Learning Rate: 0.00503196
	LOSS [training: 2.7701884148982754 | validation: 2.718918093625841]
	TIME [epoch: 8.44 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.818057546632006		[learning rate: 0.0050137]
	Learning Rate: 0.00501369
	LOSS [training: 2.818057546632006 | validation: 2.907795501766856]
	TIME [epoch: 8.44 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8038143334518972		[learning rate: 0.0049955]
	Learning Rate: 0.0049955
	LOSS [training: 2.8038143334518972 | validation: 2.9425045811239343]
	TIME [epoch: 8.44 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.868684446149653		[learning rate: 0.0049774]
	Learning Rate: 0.00497737
	LOSS [training: 2.868684446149653 | validation: 2.770871237066597]
	TIME [epoch: 8.49 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8727580129483927		[learning rate: 0.0049593]
	Learning Rate: 0.00495931
	LOSS [training: 2.8727580129483927 | validation: 3.426330151773194]
	TIME [epoch: 8.45 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8969410405645477		[learning rate: 0.0049413]
	Learning Rate: 0.00494131
	LOSS [training: 2.8969410405645477 | validation: 3.0564488395991853]
	TIME [epoch: 8.43 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.879458355175852		[learning rate: 0.0049234]
	Learning Rate: 0.00492338
	LOSS [training: 2.879458355175852 | validation: 2.8323543155593205]
	TIME [epoch: 8.44 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.824635974635604		[learning rate: 0.0049055]
	Learning Rate: 0.00490551
	LOSS [training: 2.824635974635604 | validation: 2.9599220833602633]
	TIME [epoch: 8.51 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.886625540329264		[learning rate: 0.0048877]
	Learning Rate: 0.00488771
	LOSS [training: 2.886625540329264 | validation: 2.812048695430901]
	TIME [epoch: 8.44 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9364594383289333		[learning rate: 0.00487]
	Learning Rate: 0.00486997
	LOSS [training: 2.9364594383289333 | validation: 2.8738608590226065]
	TIME [epoch: 8.43 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.935930062928934		[learning rate: 0.0048523]
	Learning Rate: 0.0048523
	LOSS [training: 2.935930062928934 | validation: 2.9656191519296167]
	TIME [epoch: 8.45 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8149680922602918		[learning rate: 0.0048347]
	Learning Rate: 0.00483469
	LOSS [training: 2.8149680922602918 | validation: 2.868223367139941]
	TIME [epoch: 8.44 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.899050006678814		[learning rate: 0.0048171]
	Learning Rate: 0.00481714
	LOSS [training: 2.899050006678814 | validation: 2.701123813449488]
	TIME [epoch: 8.46 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9101318676512653		[learning rate: 0.0047997]
	Learning Rate: 0.00479966
	LOSS [training: 2.9101318676512653 | validation: 2.681665515727378]
	TIME [epoch: 8.46 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.858855635806246		[learning rate: 0.0047822]
	Learning Rate: 0.00478224
	LOSS [training: 2.858855635806246 | validation: 2.939854092109962]
	TIME [epoch: 8.42 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8482351766410314		[learning rate: 0.0047649]
	Learning Rate: 0.00476489
	LOSS [training: 2.8482351766410314 | validation: 2.9235529587650246]
	TIME [epoch: 8.44 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8821791259701666		[learning rate: 0.0047476]
	Learning Rate: 0.00474759
	LOSS [training: 2.8821791259701666 | validation: 2.9407099905270964]
	TIME [epoch: 8.46 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8021541547553097		[learning rate: 0.0047304]
	Learning Rate: 0.00473037
	LOSS [training: 2.8021541547553097 | validation: 2.7425094163484376]
	TIME [epoch: 8.42 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8675595418505884		[learning rate: 0.0047132]
	Learning Rate: 0.0047132
	LOSS [training: 2.8675595418505884 | validation: 2.642380056506865]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8361468286429576		[learning rate: 0.0046961]
	Learning Rate: 0.00469609
	LOSS [training: 2.8361468286429576 | validation: 2.9830498846561717]
	TIME [epoch: 8.44 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.869424208433217		[learning rate: 0.0046791]
	Learning Rate: 0.00467905
	LOSS [training: 2.869424208433217 | validation: 3.181720254792827]
	TIME [epoch: 8.46 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8532242171721567		[learning rate: 0.0046621]
	Learning Rate: 0.00466207
	LOSS [training: 2.8532242171721567 | validation: 2.9156264431285726]
	TIME [epoch: 8.47 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.881284354881843		[learning rate: 0.0046452]
	Learning Rate: 0.00464515
	LOSS [training: 2.881284354881843 | validation: 2.6595063319598804]
	TIME [epoch: 8.47 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.841951795925869		[learning rate: 0.0046283]
	Learning Rate: 0.0046283
	LOSS [training: 2.841951795925869 | validation: 2.662830295592351]
	TIME [epoch: 8.43 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.833020360951661		[learning rate: 0.0046115]
	Learning Rate: 0.0046115
	LOSS [training: 2.833020360951661 | validation: 2.679556693521042]
	TIME [epoch: 8.45 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7673131464993284		[learning rate: 0.0045948]
	Learning Rate: 0.00459476
	LOSS [training: 2.7673131464993284 | validation: 3.1024276527003964]
	TIME [epoch: 8.44 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.845415719705689		[learning rate: 0.0045781]
	Learning Rate: 0.00457809
	LOSS [training: 2.845415719705689 | validation: 2.7269075758190366]
	TIME [epoch: 8.44 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.842356865235549		[learning rate: 0.0045615]
	Learning Rate: 0.00456148
	LOSS [training: 2.842356865235549 | validation: 2.6695123036146793]
	TIME [epoch: 8.44 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8101563408158503		[learning rate: 0.0045449]
	Learning Rate: 0.00454492
	LOSS [training: 2.8101563408158503 | validation: 2.701338766312367]
	TIME [epoch: 8.45 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7994775610970626		[learning rate: 0.0045284]
	Learning Rate: 0.00452843
	LOSS [training: 2.7994775610970626 | validation: 2.703297868636369]
	TIME [epoch: 8.43 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9458939739378036		[learning rate: 0.004512]
	Learning Rate: 0.00451199
	LOSS [training: 2.9458939739378036 | validation: 2.9489846396004773]
	TIME [epoch: 8.44 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.946365878957024		[learning rate: 0.0044956]
	Learning Rate: 0.00449562
	LOSS [training: 2.946365878957024 | validation: 2.7128069251537736]
	TIME [epoch: 8.43 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7753248760915437		[learning rate: 0.0044793]
	Learning Rate: 0.0044793
	LOSS [training: 2.7753248760915437 | validation: 2.80806444264515]
	TIME [epoch: 8.45 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8550398207685825		[learning rate: 0.004463]
	Learning Rate: 0.00446305
	LOSS [training: 2.8550398207685825 | validation: 2.7279157951642636]
	TIME [epoch: 8.49 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8306542281994327		[learning rate: 0.0044469]
	Learning Rate: 0.00444685
	LOSS [training: 2.8306542281994327 | validation: 3.071923395468585]
	TIME [epoch: 8.42 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.781214304097164		[learning rate: 0.0044307]
	Learning Rate: 0.00443071
	LOSS [training: 2.781214304097164 | validation: 2.8851872592470733]
	TIME [epoch: 8.43 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7653867927382483		[learning rate: 0.0044146]
	Learning Rate: 0.00441463
	LOSS [training: 2.7653867927382483 | validation: 2.8523536559995737]
	TIME [epoch: 8.43 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.760717042019506		[learning rate: 0.0043986]
	Learning Rate: 0.00439861
	LOSS [training: 2.760717042019506 | validation: 2.606161105100976]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7635121034003074		[learning rate: 0.0043827]
	Learning Rate: 0.00438265
	LOSS [training: 2.7635121034003074 | validation: 3.0598544704503476]
	TIME [epoch: 8.44 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.815849227967389		[learning rate: 0.0043667]
	Learning Rate: 0.00436675
	LOSS [training: 2.815849227967389 | validation: 2.8227832493698264]
	TIME [epoch: 8.45 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7867859709168368		[learning rate: 0.0043509]
	Learning Rate: 0.0043509
	LOSS [training: 2.7867859709168368 | validation: 2.79775954933496]
	TIME [epoch: 8.46 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7493899629444734		[learning rate: 0.0043351]
	Learning Rate: 0.00433511
	LOSS [training: 2.7493899629444734 | validation: 2.6247619912343962]
	TIME [epoch: 8.45 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.855278690324451		[learning rate: 0.0043194]
	Learning Rate: 0.00431938
	LOSS [training: 2.855278690324451 | validation: 2.6585272160408926]
	TIME [epoch: 8.46 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7030954924501627		[learning rate: 0.0043037]
	Learning Rate: 0.0043037
	LOSS [training: 2.7030954924501627 | validation: 2.6749171452236142]
	TIME [epoch: 8.45 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8447425421391515		[learning rate: 0.0042881]
	Learning Rate: 0.00428808
	LOSS [training: 2.8447425421391515 | validation: 2.680934838833407]
	TIME [epoch: 8.45 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7282538369568963		[learning rate: 0.0042725]
	Learning Rate: 0.00427252
	LOSS [training: 2.7282538369568963 | validation: 2.97231056678052]
	TIME [epoch: 8.45 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7842171781699845		[learning rate: 0.004257]
	Learning Rate: 0.00425702
	LOSS [training: 2.7842171781699845 | validation: 2.7369671510036824]
	TIME [epoch: 8.47 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8264300643976217		[learning rate: 0.0042416]
	Learning Rate: 0.00424157
	LOSS [training: 2.8264300643976217 | validation: 2.6607187842364066]
	TIME [epoch: 8.43 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8119159633520168		[learning rate: 0.0042262]
	Learning Rate: 0.00422617
	LOSS [training: 2.8119159633520168 | validation: 2.6725478417828086]
	TIME [epoch: 8.46 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8469440746684027		[learning rate: 0.0042108]
	Learning Rate: 0.00421084
	LOSS [training: 2.8469440746684027 | validation: 2.6260086594687353]
	TIME [epoch: 8.46 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7784468726905236		[learning rate: 0.0041956]
	Learning Rate: 0.00419556
	LOSS [training: 2.7784468726905236 | validation: 2.6816445968638973]
	TIME [epoch: 8.47 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.832070496249628		[learning rate: 0.0041803]
	Learning Rate: 0.00418033
	LOSS [training: 2.832070496249628 | validation: 3.1889211371324153]
	TIME [epoch: 8.45 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8627580071878		[learning rate: 0.0041652]
	Learning Rate: 0.00416516
	LOSS [training: 2.8627580071878 | validation: 2.6419684282670293]
	TIME [epoch: 8.47 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8303800039177807		[learning rate: 0.00415]
	Learning Rate: 0.00415004
	LOSS [training: 2.8303800039177807 | validation: 2.8591703932373953]
	TIME [epoch: 8.44 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8402233453083285		[learning rate: 0.004135]
	Learning Rate: 0.00413498
	LOSS [training: 2.8402233453083285 | validation: 2.6639926849385835]
	TIME [epoch: 8.45 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.762919159299722		[learning rate: 0.00412]
	Learning Rate: 0.00411998
	LOSS [training: 2.762919159299722 | validation: 2.7007494526458884]
	TIME [epoch: 8.44 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7352641357676695		[learning rate: 0.004105]
	Learning Rate: 0.00410502
	LOSS [training: 2.7352641357676695 | validation: 2.5995077767170476]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.842353715262308		[learning rate: 0.0040901]
	Learning Rate: 0.00409013
	LOSS [training: 2.842353715262308 | validation: 2.8581954456681835]
	TIME [epoch: 8.48 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9507774982019948		[learning rate: 0.0040753]
	Learning Rate: 0.00407528
	LOSS [training: 2.9507774982019948 | validation: 2.9409948820481038]
	TIME [epoch: 8.46 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8374193414468114		[learning rate: 0.0040605]
	Learning Rate: 0.00406049
	LOSS [training: 2.8374193414468114 | validation: 2.7083209964871706]
	TIME [epoch: 8.5 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.826142745946769		[learning rate: 0.0040458]
	Learning Rate: 0.00404576
	LOSS [training: 2.826142745946769 | validation: 2.755562494935229]
	TIME [epoch: 8.49 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.753284904823923		[learning rate: 0.0040311]
	Learning Rate: 0.00403108
	LOSS [training: 2.753284904823923 | validation: 3.2839876222822806]
	TIME [epoch: 8.45 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.832734119212272		[learning rate: 0.0040164]
	Learning Rate: 0.00401645
	LOSS [training: 2.832734119212272 | validation: 2.7572937725215585]
	TIME [epoch: 8.5 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8003670474324123		[learning rate: 0.0040019]
	Learning Rate: 0.00400187
	LOSS [training: 2.8003670474324123 | validation: 2.900407510160238]
	TIME [epoch: 8.46 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7856731382119966		[learning rate: 0.0039873]
	Learning Rate: 0.00398735
	LOSS [training: 2.7856731382119966 | validation: 2.709663353919847]
	TIME [epoch: 8.46 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.769583254887506		[learning rate: 0.0039729]
	Learning Rate: 0.00397288
	LOSS [training: 2.769583254887506 | validation: 2.618994611005272]
	TIME [epoch: 8.46 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.749402497159187		[learning rate: 0.0039585]
	Learning Rate: 0.00395846
	LOSS [training: 2.749402497159187 | validation: 2.8021132310304813]
	TIME [epoch: 8.49 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8161962638297737		[learning rate: 0.0039441]
	Learning Rate: 0.00394409
	LOSS [training: 2.8161962638297737 | validation: 2.6906064994048697]
	TIME [epoch: 8.46 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7910649411426114		[learning rate: 0.0039298]
	Learning Rate: 0.00392978
	LOSS [training: 2.7910649411426114 | validation: 2.793437556200896]
	TIME [epoch: 8.52 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7818301529757625		[learning rate: 0.0039155]
	Learning Rate: 0.00391552
	LOSS [training: 2.7818301529757625 | validation: 2.7351708444176586]
	TIME [epoch: 8.49 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.777771052470791		[learning rate: 0.0039013]
	Learning Rate: 0.00390131
	LOSS [training: 2.777771052470791 | validation: 2.627960659332638]
	TIME [epoch: 8.47 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7580028478814693		[learning rate: 0.0038872]
	Learning Rate: 0.00388715
	LOSS [training: 2.7580028478814693 | validation: 2.8286086582109213]
	TIME [epoch: 8.46 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8256047574794785		[learning rate: 0.003873]
	Learning Rate: 0.00387305
	LOSS [training: 2.8256047574794785 | validation: 2.807932596672967]
	TIME [epoch: 8.46 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.760845089701457		[learning rate: 0.003859]
	Learning Rate: 0.00385899
	LOSS [training: 2.760845089701457 | validation: 2.6427145561744596]
	TIME [epoch: 8.47 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7870768150581333		[learning rate: 0.003845]
	Learning Rate: 0.00384499
	LOSS [training: 2.7870768150581333 | validation: 2.689434567503209]
	TIME [epoch: 8.47 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7416750576451077		[learning rate: 0.003831]
	Learning Rate: 0.00383103
	LOSS [training: 2.7416750576451077 | validation: 2.757727029348496]
	TIME [epoch: 8.46 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.804123703120105		[learning rate: 0.0038171]
	Learning Rate: 0.00381713
	LOSS [training: 2.804123703120105 | validation: 2.6783340937827678]
	TIME [epoch: 8.45 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.755940891021864		[learning rate: 0.0038033]
	Learning Rate: 0.00380328
	LOSS [training: 2.755940891021864 | validation: 2.8258447733305854]
	TIME [epoch: 8.48 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7509096392866885		[learning rate: 0.0037895]
	Learning Rate: 0.00378947
	LOSS [training: 2.7509096392866885 | validation: 2.6685817817715565]
	TIME [epoch: 8.46 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.801568040082615		[learning rate: 0.0037757]
	Learning Rate: 0.00377572
	LOSS [training: 2.801568040082615 | validation: 2.715911045063506]
	TIME [epoch: 8.45 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7820242835430613		[learning rate: 0.003762]
	Learning Rate: 0.00376202
	LOSS [training: 2.7820242835430613 | validation: 2.7820700917779018]
	TIME [epoch: 8.47 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7824016622009022		[learning rate: 0.0037484]
	Learning Rate: 0.00374837
	LOSS [training: 2.7824016622009022 | validation: 2.6827745563366396]
	TIME [epoch: 8.47 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8160805299325644		[learning rate: 0.0037348]
	Learning Rate: 0.00373476
	LOSS [training: 2.8160805299325644 | validation: 2.666422016934296]
	TIME [epoch: 8.47 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.770686154220292		[learning rate: 0.0037212]
	Learning Rate: 0.00372121
	LOSS [training: 2.770686154220292 | validation: 2.6471978599459653]
	TIME [epoch: 8.46 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.744703497090722		[learning rate: 0.0037077]
	Learning Rate: 0.00370771
	LOSS [training: 2.744703497090722 | validation: 2.665337736817343]
	TIME [epoch: 8.46 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.927363465396426		[learning rate: 0.0036943]
	Learning Rate: 0.00369425
	LOSS [training: 2.927363465396426 | validation: 2.9008191910035457]
	TIME [epoch: 8.52 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.805329568688849		[learning rate: 0.0036808]
	Learning Rate: 0.00368084
	LOSS [training: 2.805329568688849 | validation: 2.662693265737551]
	TIME [epoch: 8.49 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.773542385046283		[learning rate: 0.0036675]
	Learning Rate: 0.00366749
	LOSS [training: 2.773542385046283 | validation: 2.933676750170701]
	TIME [epoch: 8.46 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8750691520530807		[learning rate: 0.0036542]
	Learning Rate: 0.00365418
	LOSS [training: 2.8750691520530807 | validation: 2.889224269526748]
	TIME [epoch: 8.46 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.819782799320445		[learning rate: 0.0036409]
	Learning Rate: 0.00364092
	LOSS [training: 2.819782799320445 | validation: 2.761147257542458]
	TIME [epoch: 8.49 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.768740024825694		[learning rate: 0.0036277]
	Learning Rate: 0.0036277
	LOSS [training: 2.768740024825694 | validation: 2.6107198710006365]
	TIME [epoch: 8.47 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.75213558921754		[learning rate: 0.0036145]
	Learning Rate: 0.00361454
	LOSS [training: 2.75213558921754 | validation: 2.6640616469536154]
	TIME [epoch: 8.47 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7912280090634556		[learning rate: 0.0036014]
	Learning Rate: 0.00360142
	LOSS [training: 2.7912280090634556 | validation: 2.6751501824707833]
	TIME [epoch: 8.45 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.771701927008137		[learning rate: 0.0035883]
	Learning Rate: 0.00358835
	LOSS [training: 2.771701927008137 | validation: 2.704799760564816]
	TIME [epoch: 8.48 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.763205587579512		[learning rate: 0.0035753]
	Learning Rate: 0.00357533
	LOSS [training: 2.763205587579512 | validation: 2.6859842517352326]
	TIME [epoch: 8.48 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8325668428358366		[learning rate: 0.0035624]
	Learning Rate: 0.00356235
	LOSS [training: 2.8325668428358366 | validation: 2.739637569899343]
	TIME [epoch: 8.47 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.796493524094396		[learning rate: 0.0035494]
	Learning Rate: 0.00354942
	LOSS [training: 2.796493524094396 | validation: 2.8278536317172307]
	TIME [epoch: 8.46 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.752089062016197		[learning rate: 0.0035365]
	Learning Rate: 0.00353654
	LOSS [training: 2.752089062016197 | validation: 2.825035532369143]
	TIME [epoch: 8.5 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7619551373894735		[learning rate: 0.0035237]
	Learning Rate: 0.00352371
	LOSS [training: 2.7619551373894735 | validation: 2.628089358382791]
	TIME [epoch: 8.46 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7432450435524025		[learning rate: 0.0035109]
	Learning Rate: 0.00351092
	LOSS [training: 2.7432450435524025 | validation: 2.720048185360021]
	TIME [epoch: 8.47 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.751533458770042		[learning rate: 0.0034982]
	Learning Rate: 0.00349818
	LOSS [training: 2.751533458770042 | validation: 2.6373252185530847]
	TIME [epoch: 8.46 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7241585620768305		[learning rate: 0.0034855]
	Learning Rate: 0.00348548
	LOSS [training: 2.7241585620768305 | validation: 2.8165012747726066]
	TIME [epoch: 8.49 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7745696165700933		[learning rate: 0.0034728]
	Learning Rate: 0.00347284
	LOSS [training: 2.7745696165700933 | validation: 2.6650463359721446]
	TIME [epoch: 8.47 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.79359101587128		[learning rate: 0.0034602]
	Learning Rate: 0.00346023
	LOSS [training: 2.79359101587128 | validation: 2.6387924727553917]
	TIME [epoch: 8.46 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.733434597506526		[learning rate: 0.0034477]
	Learning Rate: 0.00344767
	LOSS [training: 2.733434597506526 | validation: 2.796356260663531]
	TIME [epoch: 8.46 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7498477218821966		[learning rate: 0.0034352]
	Learning Rate: 0.00343516
	LOSS [training: 2.7498477218821966 | validation: 2.666070522995846]
	TIME [epoch: 8.49 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.718375885035564		[learning rate: 0.0034227]
	Learning Rate: 0.0034227
	LOSS [training: 2.718375885035564 | validation: 3.3401847316848134]
	TIME [epoch: 8.52 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8221494891267507		[learning rate: 0.0034103]
	Learning Rate: 0.00341028
	LOSS [training: 2.8221494891267507 | validation: 2.662949713497381]
	TIME [epoch: 8.45 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7566328691929614		[learning rate: 0.0033979]
	Learning Rate: 0.0033979
	LOSS [training: 2.7566328691929614 | validation: 2.7004689009531533]
	TIME [epoch: 8.44 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7929928970499533		[learning rate: 0.0033856]
	Learning Rate: 0.00338557
	LOSS [training: 2.7929928970499533 | validation: 2.740593620135191]
	TIME [epoch: 8.53 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.730173767677464		[learning rate: 0.0033733]
	Learning Rate: 0.00337328
	LOSS [training: 2.730173767677464 | validation: 2.6570564353735424]
	TIME [epoch: 8.47 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.780539519366824		[learning rate: 0.003361]
	Learning Rate: 0.00336104
	LOSS [training: 2.780539519366824 | validation: 2.6052804954594695]
	TIME [epoch: 8.45 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.711573860495132		[learning rate: 0.0033488]
	Learning Rate: 0.00334884
	LOSS [training: 2.711573860495132 | validation: 2.6551860399635325]
	TIME [epoch: 8.46 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7102766346532112		[learning rate: 0.0033367]
	Learning Rate: 0.00333669
	LOSS [training: 2.7102766346532112 | validation: 2.8014889702804746]
	TIME [epoch: 8.48 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8579912140657364		[learning rate: 0.0033246]
	Learning Rate: 0.00332458
	LOSS [training: 2.8579912140657364 | validation: 2.690856412739694]
	TIME [epoch: 8.46 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7976851233775353		[learning rate: 0.0033125]
	Learning Rate: 0.00331252
	LOSS [training: 2.7976851233775353 | validation: 2.698437588085583]
	TIME [epoch: 8.46 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.78291850522413		[learning rate: 0.0033005]
	Learning Rate: 0.00330049
	LOSS [training: 2.78291850522413 | validation: 2.7578351481712664]
	TIME [epoch: 8.46 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.870426545499266		[learning rate: 0.0032885]
	Learning Rate: 0.00328852
	LOSS [training: 2.870426545499266 | validation: 2.7652569598758494]
	TIME [epoch: 8.53 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7955462542911844		[learning rate: 0.0032766]
	Learning Rate: 0.00327658
	LOSS [training: 2.7955462542911844 | validation: 2.80923514919773]
	TIME [epoch: 8.45 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.713744783879727		[learning rate: 0.0032647]
	Learning Rate: 0.00326469
	LOSS [training: 2.713744783879727 | validation: 2.65292887824573]
	TIME [epoch: 8.44 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.807220781024476		[learning rate: 0.0032528]
	Learning Rate: 0.00325284
	LOSS [training: 2.807220781024476 | validation: 2.6332075346013535]
	TIME [epoch: 8.48 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7057547224159437		[learning rate: 0.003241]
	Learning Rate: 0.00324104
	LOSS [training: 2.7057547224159437 | validation: 2.606551368001906]
	TIME [epoch: 8.46 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.751984920412742		[learning rate: 0.0032293]
	Learning Rate: 0.00322928
	LOSS [training: 2.751984920412742 | validation: 2.8772157645505123]
	TIME [epoch: 8.45 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7330449474904763		[learning rate: 0.0032176]
	Learning Rate: 0.00321756
	LOSS [training: 2.7330449474904763 | validation: 2.5875887666168684]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7029607376318565		[learning rate: 0.0032059]
	Learning Rate: 0.00320588
	LOSS [training: 2.7029607376318565 | validation: 2.759332133656665]
	TIME [epoch: 8.48 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7148816070658497		[learning rate: 0.0031942]
	Learning Rate: 0.00319425
	LOSS [training: 2.7148816070658497 | validation: 2.5972480804957283]
	TIME [epoch: 8.45 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7527294911438576		[learning rate: 0.0031827]
	Learning Rate: 0.00318265
	LOSS [training: 2.7527294911438576 | validation: 2.92014077484363]
	TIME [epoch: 8.44 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7402274293428093		[learning rate: 0.0031711]
	Learning Rate: 0.0031711
	LOSS [training: 2.7402274293428093 | validation: 2.6368373201974133]
	TIME [epoch: 8.5 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7211031063840037		[learning rate: 0.0031596]
	Learning Rate: 0.0031596
	LOSS [training: 2.7211031063840037 | validation: 2.6138962272265687]
	TIME [epoch: 8.43 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.721965401041205		[learning rate: 0.0031481]
	Learning Rate: 0.00314813
	LOSS [training: 2.721965401041205 | validation: 2.653631520294645]
	TIME [epoch: 8.47 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9869625295826165		[learning rate: 0.0031367]
	Learning Rate: 0.00313671
	LOSS [training: 2.9869625295826165 | validation: 2.63464062196877]
	TIME [epoch: 8.44 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7196266184522977		[learning rate: 0.0031253]
	Learning Rate: 0.00312532
	LOSS [training: 2.7196266184522977 | validation: 2.7031950796098156]
	TIME [epoch: 8.43 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6804802191731016		[learning rate: 0.003114]
	Learning Rate: 0.00311398
	LOSS [training: 2.6804802191731016 | validation: 2.621818926825966]
	TIME [epoch: 8.45 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7181820456795855		[learning rate: 0.0031027]
	Learning Rate: 0.00310268
	LOSS [training: 2.7181820456795855 | validation: 2.676125020844182]
	TIME [epoch: 8.47 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7944668807623443		[learning rate: 0.0030914]
	Learning Rate: 0.00309142
	LOSS [training: 2.7944668807623443 | validation: 2.9636565775983827]
	TIME [epoch: 8.46 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.782656542067462		[learning rate: 0.0030802]
	Learning Rate: 0.0030802
	LOSS [training: 2.782656542067462 | validation: 2.740555346869861]
	TIME [epoch: 8.44 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7427070405608154		[learning rate: 0.003069]
	Learning Rate: 0.00306902
	LOSS [training: 2.7427070405608154 | validation: 2.571540348699868]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.714115303042116		[learning rate: 0.0030579]
	Learning Rate: 0.00305788
	LOSS [training: 2.714115303042116 | validation: 2.648719757997097]
	TIME [epoch: 8.47 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.722436432952199		[learning rate: 0.0030468]
	Learning Rate: 0.00304679
	LOSS [training: 2.722436432952199 | validation: 2.6373100591944603]
	TIME [epoch: 8.45 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8309153127900246		[learning rate: 0.0030357]
	Learning Rate: 0.00303573
	LOSS [training: 2.8309153127900246 | validation: 2.643221624281123]
	TIME [epoch: 8.46 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7077970877497015		[learning rate: 0.0030247]
	Learning Rate: 0.00302471
	LOSS [training: 2.7077970877497015 | validation: 2.6613696152494244]
	TIME [epoch: 8.44 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.70879725209399		[learning rate: 0.0030137]
	Learning Rate: 0.00301374
	LOSS [training: 2.70879725209399 | validation: 2.6758192644902734]
	TIME [epoch: 8.48 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.771512184218772		[learning rate: 0.0030028]
	Learning Rate: 0.0030028
	LOSS [training: 2.771512184218772 | validation: 2.615294734581846]
	TIME [epoch: 8.44 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6661511664595277		[learning rate: 0.0029919]
	Learning Rate: 0.0029919
	LOSS [training: 2.6661511664595277 | validation: 2.6204559775070333]
	TIME [epoch: 8.45 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7206688182338543		[learning rate: 0.002981]
	Learning Rate: 0.00298104
	LOSS [training: 2.7206688182338543 | validation: 2.772125130779428]
	TIME [epoch: 8.45 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7076040679997226		[learning rate: 0.0029702]
	Learning Rate: 0.00297023
	LOSS [training: 2.7076040679997226 | validation: 2.6039990174530026]
	TIME [epoch: 8.47 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7229659982632386		[learning rate: 0.0029594]
	Learning Rate: 0.00295945
	LOSS [training: 2.7229659982632386 | validation: 2.689809693465495]
	TIME [epoch: 8.45 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.736388601519031		[learning rate: 0.0029487]
	Learning Rate: 0.00294871
	LOSS [training: 2.736388601519031 | validation: 2.717308633415203]
	TIME [epoch: 8.43 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7555129650988768		[learning rate: 0.002938]
	Learning Rate: 0.00293801
	LOSS [training: 2.7555129650988768 | validation: 2.6001675767621]
	TIME [epoch: 8.5 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7271459990529285		[learning rate: 0.0029273]
	Learning Rate: 0.00292734
	LOSS [training: 2.7271459990529285 | validation: 2.644400467561703]
	TIME [epoch: 8.45 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6973120016858694		[learning rate: 0.0029167]
	Learning Rate: 0.00291672
	LOSS [training: 2.6973120016858694 | validation: 2.709450689969858]
	TIME [epoch: 8.45 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.712061597931005		[learning rate: 0.0029061]
	Learning Rate: 0.00290614
	LOSS [training: 2.712061597931005 | validation: 2.6414493315210903]
	TIME [epoch: 8.45 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.712275291934374		[learning rate: 0.0028956]
	Learning Rate: 0.00289559
	LOSS [training: 2.712275291934374 | validation: 2.6234924915555675]
	TIME [epoch: 8.45 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.689062292515314		[learning rate: 0.0028851]
	Learning Rate: 0.00288508
	LOSS [training: 2.689062292515314 | validation: 2.5921537347171597]
	TIME [epoch: 8.47 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6881438908332096		[learning rate: 0.0028746]
	Learning Rate: 0.00287461
	LOSS [training: 2.6881438908332096 | validation: 2.589664628413292]
	TIME [epoch: 8.45 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7949901336661607		[learning rate: 0.0028642]
	Learning Rate: 0.00286418
	LOSS [training: 2.7949901336661607 | validation: 2.8760429726170016]
	TIME [epoch: 8.44 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.888062989946222		[learning rate: 0.0028538]
	Learning Rate: 0.00285378
	LOSS [training: 2.888062989946222 | validation: 2.770546481341489]
	TIME [epoch: 8.45 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.748964402832087		[learning rate: 0.0028434]
	Learning Rate: 0.00284343
	LOSS [training: 2.748964402832087 | validation: 2.647404314874398]
	TIME [epoch: 8.47 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7471177545783685		[learning rate: 0.0028331]
	Learning Rate: 0.00283311
	LOSS [training: 2.7471177545783685 | validation: 2.652626401391694]
	TIME [epoch: 8.45 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.708002624569664		[learning rate: 0.0028228]
	Learning Rate: 0.00282283
	LOSS [training: 2.708002624569664 | validation: 2.5847686560767826]
	TIME [epoch: 8.45 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.739706198185471		[learning rate: 0.0028126]
	Learning Rate: 0.00281258
	LOSS [training: 2.739706198185471 | validation: 2.569718934017735]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7136029634291257		[learning rate: 0.0028024]
	Learning Rate: 0.00280238
	LOSS [training: 2.7136029634291257 | validation: 2.6180394175922204]
	TIME [epoch: 8.47 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6872650788214143		[learning rate: 0.0027922]
	Learning Rate: 0.00279221
	LOSS [training: 2.6872650788214143 | validation: 2.6636096244593057]
	TIME [epoch: 8.44 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.691008420261763		[learning rate: 0.0027821]
	Learning Rate: 0.00278207
	LOSS [training: 2.691008420261763 | validation: 2.6586004461372337]
	TIME [epoch: 8.44 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7177048914071227		[learning rate: 0.002772]
	Learning Rate: 0.00277198
	LOSS [training: 2.7177048914071227 | validation: 2.7386891517311582]
	TIME [epoch: 8.45 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7326635408716307		[learning rate: 0.0027619]
	Learning Rate: 0.00276192
	LOSS [training: 2.7326635408716307 | validation: 2.6402461255392984]
	TIME [epoch: 8.46 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7278686887432095		[learning rate: 0.0027519]
	Learning Rate: 0.00275189
	LOSS [training: 2.7278686887432095 | validation: 2.8499176832619426]
	TIME [epoch: 8.45 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7185528713474		[learning rate: 0.0027419]
	Learning Rate: 0.00274191
	LOSS [training: 2.7185528713474 | validation: 2.7424463971433237]
	TIME [epoch: 8.49 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.679735860225855		[learning rate: 0.002732]
	Learning Rate: 0.00273196
	LOSS [training: 2.679735860225855 | validation: 2.7469074629926755]
	TIME [epoch: 8.45 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7192142779799724		[learning rate: 0.002722]
	Learning Rate: 0.00272204
	LOSS [training: 2.7192142779799724 | validation: 2.637756172594307]
	TIME [epoch: 8.46 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6670827240409283		[learning rate: 0.0027122]
	Learning Rate: 0.00271216
	LOSS [training: 2.6670827240409283 | validation: 2.6656893058400577]
	TIME [epoch: 8.46 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7167040773114137		[learning rate: 0.0027023]
	Learning Rate: 0.00270232
	LOSS [training: 2.7167040773114137 | validation: 2.681785121508941]
	TIME [epoch: 8.47 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7221325835648833		[learning rate: 0.0026925]
	Learning Rate: 0.00269251
	LOSS [training: 2.7221325835648833 | validation: 2.6070167597430416]
	TIME [epoch: 8.44 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7103738164922064		[learning rate: 0.0026827]
	Learning Rate: 0.00268274
	LOSS [training: 2.7103738164922064 | validation: 2.576489868350701]
	TIME [epoch: 8.46 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7240656652703437		[learning rate: 0.002673]
	Learning Rate: 0.00267301
	LOSS [training: 2.7240656652703437 | validation: 2.581861664243238]
	TIME [epoch: 8.43 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7217210825180476		[learning rate: 0.0026633]
	Learning Rate: 0.00266331
	LOSS [training: 2.7217210825180476 | validation: 2.5754149544628575]
	TIME [epoch: 8.44 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6961701270473104		[learning rate: 0.0026536]
	Learning Rate: 0.00265364
	LOSS [training: 2.6961701270473104 | validation: 2.5496554675055076]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_465.pth
	Model improved!!!
EPOCH 466/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7318076552658175		[learning rate: 0.002644]
	Learning Rate: 0.00264401
	LOSS [training: 2.7318076552658175 | validation: 2.5964635417089577]
	TIME [epoch: 8.47 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.715480157702571		[learning rate: 0.0026344]
	Learning Rate: 0.00263441
	LOSS [training: 2.715480157702571 | validation: 2.618024992438121]
	TIME [epoch: 8.43 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.71240736945524		[learning rate: 0.0026249]
	Learning Rate: 0.00262485
	LOSS [training: 2.71240736945524 | validation: 2.638120905822711]
	TIME [epoch: 8.44 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.744207815878646		[learning rate: 0.0026153]
	Learning Rate: 0.00261533
	LOSS [training: 2.744207815878646 | validation: 2.621615071496173]
	TIME [epoch: 8.43 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7478030442741384		[learning rate: 0.0026058]
	Learning Rate: 0.00260584
	LOSS [training: 2.7478030442741384 | validation: 2.6182305347700985]
	TIME [epoch: 8.46 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.680288626783343		[learning rate: 0.0025964]
	Learning Rate: 0.00259638
	LOSS [training: 2.680288626783343 | validation: 2.71926468114453]
	TIME [epoch: 8.44 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6975564723419945		[learning rate: 0.002587]
	Learning Rate: 0.00258696
	LOSS [training: 2.6975564723419945 | validation: 2.6286081324003874]
	TIME [epoch: 8.43 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.751143974866369		[learning rate: 0.0025776]
	Learning Rate: 0.00257757
	LOSS [training: 2.751143974866369 | validation: 2.6356693060790617]
	TIME [epoch: 8.46 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6599180635015234		[learning rate: 0.0025682]
	Learning Rate: 0.00256822
	LOSS [training: 2.6599180635015234 | validation: 2.5813823128018756]
	TIME [epoch: 8.47 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6816301665562494		[learning rate: 0.0025589]
	Learning Rate: 0.0025589
	LOSS [training: 2.6816301665562494 | validation: 2.6132190196972322]
	TIME [epoch: 8.43 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.675526612997494		[learning rate: 0.0025496]
	Learning Rate: 0.00254961
	LOSS [training: 2.675526612997494 | validation: 2.5810323210425983]
	TIME [epoch: 8.43 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6734668565015314		[learning rate: 0.0025404]
	Learning Rate: 0.00254036
	LOSS [training: 2.6734668565015314 | validation: 2.6036111707889202]
	TIME [epoch: 8.43 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7385463908220267		[learning rate: 0.0025311]
	Learning Rate: 0.00253114
	LOSS [training: 2.7385463908220267 | validation: 2.6223228736496655]
	TIME [epoch: 8.46 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.678731797342714		[learning rate: 0.002522]
	Learning Rate: 0.00252195
	LOSS [training: 2.678731797342714 | validation: 2.6015924602113385]
	TIME [epoch: 8.42 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6754999382878966		[learning rate: 0.0025128]
	Learning Rate: 0.0025128
	LOSS [training: 2.6754999382878966 | validation: 2.5923998053431285]
	TIME [epoch: 8.44 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.662909233595326		[learning rate: 0.0025037]
	Learning Rate: 0.00250368
	LOSS [training: 2.662909233595326 | validation: 2.6097574808214214]
	TIME [epoch: 8.44 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.651251809517544		[learning rate: 0.0024946]
	Learning Rate: 0.00249459
	LOSS [training: 2.651251809517544 | validation: 2.8561770369962387]
	TIME [epoch: 8.45 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.705165425266906		[learning rate: 0.0024855]
	Learning Rate: 0.00248554
	LOSS [training: 2.705165425266906 | validation: 2.75342222808636]
	TIME [epoch: 8.45 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7872399662194036		[learning rate: 0.0024765]
	Learning Rate: 0.00247652
	LOSS [training: 2.7872399662194036 | validation: 2.829100177869984]
	TIME [epoch: 8.45 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.805395778389399		[learning rate: 0.0024675]
	Learning Rate: 0.00246753
	LOSS [training: 2.805395778389399 | validation: 2.5610421535844883]
	TIME [epoch: 8.44 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6804032533466104		[learning rate: 0.0024586]
	Learning Rate: 0.00245858
	LOSS [training: 2.6804032533466104 | validation: 2.6592081492178794]
	TIME [epoch: 8.47 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7029795648429458		[learning rate: 0.0024497]
	Learning Rate: 0.00244966
	LOSS [training: 2.7029795648429458 | validation: 2.589544026613207]
	TIME [epoch: 8.44 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7113360409267986		[learning rate: 0.0024408]
	Learning Rate: 0.00244077
	LOSS [training: 2.7113360409267986 | validation: 2.5721900320555906]
	TIME [epoch: 8.47 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.657065853925458		[learning rate: 0.0024319]
	Learning Rate: 0.00243191
	LOSS [training: 2.657065853925458 | validation: 2.6109657969010502]
	TIME [epoch: 8.46 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.652492800162746		[learning rate: 0.0024231]
	Learning Rate: 0.00242308
	LOSS [training: 2.652492800162746 | validation: 2.6512776803807783]
	TIME [epoch: 8.46 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6880152032375593		[learning rate: 0.0024143]
	Learning Rate: 0.00241429
	LOSS [training: 2.6880152032375593 | validation: 2.7110901733106987]
	TIME [epoch: 8.43 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7059249314571896		[learning rate: 0.0024055]
	Learning Rate: 0.00240553
	LOSS [training: 2.7059249314571896 | validation: 2.619734676408814]
	TIME [epoch: 8.45 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6943418572645124		[learning rate: 0.0023968]
	Learning Rate: 0.0023968
	LOSS [training: 2.6943418572645124 | validation: 2.6600914656687684]
	TIME [epoch: 8.46 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.720619129914568		[learning rate: 0.0023881]
	Learning Rate: 0.0023881
	LOSS [training: 2.720619129914568 | validation: 2.6081808768772845]
	TIME [epoch: 8.46 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.663521281136705		[learning rate: 0.0023794]
	Learning Rate: 0.00237943
	LOSS [training: 2.663521281136705 | validation: 2.5798255466211364]
	TIME [epoch: 8.44 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.632860208550756		[learning rate: 0.0023708]
	Learning Rate: 0.0023708
	LOSS [training: 2.632860208550756 | validation: 2.7406341946429524]
	TIME [epoch: 8.44 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6662266720235572		[learning rate: 0.0023622]
	Learning Rate: 0.0023622
	LOSS [training: 2.6662266720235572 | validation: 2.6189183435077954]
	TIME [epoch: 8.45 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6814957539352444		[learning rate: 0.0023536]
	Learning Rate: 0.00235362
	LOSS [training: 2.6814957539352444 | validation: 2.5890644219585357]
	TIME [epoch: 8.45 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.678074084017873		[learning rate: 0.0023451]
	Learning Rate: 0.00234508
	LOSS [training: 2.678074084017873 | validation: 2.575020153892791]
	TIME [epoch: 8.43 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6726378665747523		[learning rate: 0.0023366]
	Learning Rate: 0.00233657
	LOSS [training: 2.6726378665747523 | validation: 2.666569759068336]
	TIME [epoch: 8.44 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.687569089947663		[learning rate: 0.0023281]
	Learning Rate: 0.00232809
	LOSS [training: 2.687569089947663 | validation: 2.5635745245021284]
	TIME [epoch: 8.43 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6574409759591457		[learning rate: 0.0023196]
	Learning Rate: 0.00231964
	LOSS [training: 2.6574409759591457 | validation: 2.6221987602000576]
	TIME [epoch: 8.47 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.675147161097531		[learning rate: 0.0023112]
	Learning Rate: 0.00231122
	LOSS [training: 2.675147161097531 | validation: 2.6135261380317645]
	TIME [epoch: 8.43 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.681676343689508		[learning rate: 0.0023028]
	Learning Rate: 0.00230284
	LOSS [training: 2.681676343689508 | validation: 2.573917170255161]
	TIME [epoch: 8.43 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.725061407497548		[learning rate: 0.0022945]
	Learning Rate: 0.00229448
	LOSS [training: 2.725061407497548 | validation: 2.653447756140028]
	TIME [epoch: 8.43 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6560935822576393		[learning rate: 0.0022862]
	Learning Rate: 0.00228615
	LOSS [training: 2.6560935822576393 | validation: 2.564174125855799]
	TIME [epoch: 8.46 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.681278202502407		[learning rate: 0.0022779]
	Learning Rate: 0.00227786
	LOSS [training: 2.681278202502407 | validation: 2.5688368396265817]
	TIME [epoch: 8.44 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6535294860133867		[learning rate: 0.0022696]
	Learning Rate: 0.00226959
	LOSS [training: 2.6535294860133867 | validation: 2.6299135976073895]
	TIME [epoch: 8.43 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.658786417968389		[learning rate: 0.0022614]
	Learning Rate: 0.00226135
	LOSS [training: 2.658786417968389 | validation: 2.5861956354060363]
	TIME [epoch: 8.45 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6886887089755085		[learning rate: 0.0022531]
	Learning Rate: 0.00225315
	LOSS [training: 2.6886887089755085 | validation: 2.6630585526391046]
	TIME [epoch: 8.46 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.702752465678044		[learning rate: 0.002245]
	Learning Rate: 0.00224497
	LOSS [training: 2.702752465678044 | validation: 2.599837536932571]
	TIME [epoch: 8.44 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.651541859441129		[learning rate: 0.0022368]
	Learning Rate: 0.00223682
	LOSS [training: 2.651541859441129 | validation: 2.601886568173084]
	TIME [epoch: 8.47 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8156102764863937		[learning rate: 0.0022287]
	Learning Rate: 0.00222871
	LOSS [training: 2.8156102764863937 | validation: 2.8204592024348143]
	TIME [epoch: 8.44 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.697512094968555		[learning rate: 0.0022206]
	Learning Rate: 0.00222062
	LOSS [training: 2.697512094968555 | validation: 2.554561538403652]
	TIME [epoch: 8.45 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6615145715511037		[learning rate: 0.0022126]
	Learning Rate: 0.00221256
	LOSS [training: 2.6615145715511037 | validation: 2.5511714915953063]
	TIME [epoch: 8.46 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6849140651347083		[learning rate: 0.0022045]
	Learning Rate: 0.00220453
	LOSS [training: 2.6849140651347083 | validation: 2.5580937887171755]
	TIME [epoch: 8.46 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.683645206965516		[learning rate: 0.0021965]
	Learning Rate: 0.00219653
	LOSS [training: 2.683645206965516 | validation: 2.5692230231792]
	TIME [epoch: 8.45 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6669805388415804		[learning rate: 0.0021886]
	Learning Rate: 0.00218856
	LOSS [training: 2.6669805388415804 | validation: 2.5850152061795932]
	TIME [epoch: 8.44 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.666116624642206		[learning rate: 0.0021806]
	Learning Rate: 0.00218061
	LOSS [training: 2.666116624642206 | validation: 2.6143322500462194]
	TIME [epoch: 8.45 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7040428910488936		[learning rate: 0.0021727]
	Learning Rate: 0.0021727
	LOSS [training: 2.7040428910488936 | validation: 2.6166813368723814]
	TIME [epoch: 8.42 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6643063517770553		[learning rate: 0.0021648]
	Learning Rate: 0.00216482
	LOSS [training: 2.6643063517770553 | validation: 2.588546398746862]
	TIME [epoch: 8.51 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6691970250018597		[learning rate: 0.002157]
	Learning Rate: 0.00215696
	LOSS [training: 2.6691970250018597 | validation: 2.6309552596509365]
	TIME [epoch: 8.42 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.679432757894225		[learning rate: 0.0021491]
	Learning Rate: 0.00214913
	LOSS [training: 2.679432757894225 | validation: 2.63034519732314]
	TIME [epoch: 8.43 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6635488745019287		[learning rate: 0.0021413]
	Learning Rate: 0.00214133
	LOSS [training: 2.6635488745019287 | validation: 2.594227435932403]
	TIME [epoch: 8.43 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6793615947099347		[learning rate: 0.0021336]
	Learning Rate: 0.00213356
	LOSS [training: 2.6793615947099347 | validation: 2.556973587618634]
	TIME [epoch: 8.46 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6345351604591682		[learning rate: 0.0021258]
	Learning Rate: 0.00212582
	LOSS [training: 2.6345351604591682 | validation: 2.7536343857604866]
	TIME [epoch: 8.49 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.737150115976415		[learning rate: 0.0021181]
	Learning Rate: 0.0021181
	LOSS [training: 2.737150115976415 | validation: 2.555026359727564]
	TIME [epoch: 8.43 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6659260748054043		[learning rate: 0.0021104]
	Learning Rate: 0.00211042
	LOSS [training: 2.6659260748054043 | validation: 2.551763910213963]
	TIME [epoch: 8.43 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6520920558545447		[learning rate: 0.0021028]
	Learning Rate: 0.00210276
	LOSS [training: 2.6520920558545447 | validation: 2.549654920507426]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6444504579078036		[learning rate: 0.0020951]
	Learning Rate: 0.00209513
	LOSS [training: 2.6444504579078036 | validation: 2.6801920325169446]
	TIME [epoch: 8.45 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6637093369989935		[learning rate: 0.0020875]
	Learning Rate: 0.00208752
	LOSS [training: 2.6637093369989935 | validation: 2.6130530057302988]
	TIME [epoch: 8.44 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.661606956959257		[learning rate: 0.0020799]
	Learning Rate: 0.00207995
	LOSS [training: 2.661606956959257 | validation: 2.5668781701555057]
	TIME [epoch: 8.44 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.69964869703898		[learning rate: 0.0020724]
	Learning Rate: 0.0020724
	LOSS [training: 2.69964869703898 | validation: 2.586640064843338]
	TIME [epoch: 8.45 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.695685724606512		[learning rate: 0.0020649]
	Learning Rate: 0.00206488
	LOSS [training: 2.695685724606512 | validation: 2.559442962991415]
	TIME [epoch: 8.45 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.670163396171464		[learning rate: 0.0020574]
	Learning Rate: 0.00205739
	LOSS [training: 2.670163396171464 | validation: 2.595374999417544]
	TIME [epoch: 8.44 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.69117122376079		[learning rate: 0.0020499]
	Learning Rate: 0.00204992
	LOSS [training: 2.69117122376079 | validation: 2.7030034647899397]
	TIME [epoch: 8.44 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6566097290451993		[learning rate: 0.0020425]
	Learning Rate: 0.00204248
	LOSS [training: 2.6566097290451993 | validation: 2.6390967477860987]
	TIME [epoch: 8.46 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7002546928715545		[learning rate: 0.0020351]
	Learning Rate: 0.00203507
	LOSS [training: 2.7002546928715545 | validation: 2.5824241538361474]
	TIME [epoch: 8.45 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6415568134971723		[learning rate: 0.0020277]
	Learning Rate: 0.00202768
	LOSS [training: 2.6415568134971723 | validation: 2.6139554096033484]
	TIME [epoch: 8.49 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.681480427465448		[learning rate: 0.0020203]
	Learning Rate: 0.00202032
	LOSS [training: 2.681480427465448 | validation: 2.594809161038656]
	TIME [epoch: 8.44 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6592379195389575		[learning rate: 0.002013]
	Learning Rate: 0.00201299
	LOSS [training: 2.6592379195389575 | validation: 2.643221887666721]
	TIME [epoch: 8.46 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.700899256774013		[learning rate: 0.0020057]
	Learning Rate: 0.00200569
	LOSS [training: 2.700899256774013 | validation: 2.607291373346797]
	TIME [epoch: 8.45 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6515860026558604		[learning rate: 0.0019984]
	Learning Rate: 0.00199841
	LOSS [training: 2.6515860026558604 | validation: 2.7718675882433375]
	TIME [epoch: 8.44 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.710611352554344		[learning rate: 0.0019912]
	Learning Rate: 0.00199116
	LOSS [training: 2.710611352554344 | validation: 2.5636267728998083]
	TIME [epoch: 8.45 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.64610994938529		[learning rate: 0.0019839]
	Learning Rate: 0.00198393
	LOSS [training: 2.64610994938529 | validation: 2.5760463640533464]
	TIME [epoch: 8.46 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6959005070130013		[learning rate: 0.0019767]
	Learning Rate: 0.00197673
	LOSS [training: 2.6959005070130013 | validation: 2.5967341410779436]
	TIME [epoch: 8.45 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.73526511728168		[learning rate: 0.0019696]
	Learning Rate: 0.00196956
	LOSS [training: 2.73526511728168 | validation: 2.6361234111019423]
	TIME [epoch: 8.45 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.657228119154226		[learning rate: 0.0019624]
	Learning Rate: 0.00196241
	LOSS [training: 2.657228119154226 | validation: 2.6219121552259854]
	TIME [epoch: 8.43 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6837648732916852		[learning rate: 0.0019553]
	Learning Rate: 0.00195529
	LOSS [training: 2.6837648732916852 | validation: 2.8910088818039563]
	TIME [epoch: 8.47 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.689053608701369		[learning rate: 0.0019482]
	Learning Rate: 0.00194819
	LOSS [training: 2.689053608701369 | validation: 2.590422205140559]
	TIME [epoch: 8.45 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.665431456359974		[learning rate: 0.0019411]
	Learning Rate: 0.00194112
	LOSS [training: 2.665431456359974 | validation: 2.691282517776636]
	TIME [epoch: 8.44 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.673136021107513		[learning rate: 0.0019341]
	Learning Rate: 0.00193408
	LOSS [training: 2.673136021107513 | validation: 2.5649315634384253]
	TIME [epoch: 8.45 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.689751393286		[learning rate: 0.0019271]
	Learning Rate: 0.00192706
	LOSS [training: 2.689751393286 | validation: 2.556177673439862]
	TIME [epoch: 8.47 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6520170280111586		[learning rate: 0.0019201]
	Learning Rate: 0.00192006
	LOSS [training: 2.6520170280111586 | validation: 2.684014835841226]
	TIME [epoch: 8.45 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6516638013313045		[learning rate: 0.0019131]
	Learning Rate: 0.0019131
	LOSS [training: 2.6516638013313045 | validation: 2.578830574986683]
	TIME [epoch: 8.44 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.728722892185499		[learning rate: 0.0019062]
	Learning Rate: 0.00190615
	LOSS [training: 2.728722892185499 | validation: 2.626559935029637]
	TIME [epoch: 8.44 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6705887150658123		[learning rate: 0.0018992]
	Learning Rate: 0.00189924
	LOSS [training: 2.6705887150658123 | validation: 2.6395349839242503]
	TIME [epoch: 8.47 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6437442338458013		[learning rate: 0.0018923]
	Learning Rate: 0.00189234
	LOSS [training: 2.6437442338458013 | validation: 2.5908430886806526]
	TIME [epoch: 8.46 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.647532171593996		[learning rate: 0.0018855]
	Learning Rate: 0.00188548
	LOSS [training: 2.647532171593996 | validation: 2.584136434267001]
	TIME [epoch: 8.47 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.679592438261599		[learning rate: 0.0018786]
	Learning Rate: 0.00187863
	LOSS [training: 2.679592438261599 | validation: 2.5959988730058914]
	TIME [epoch: 8.44 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.673132936353892		[learning rate: 0.0018718]
	Learning Rate: 0.00187182
	LOSS [training: 2.673132936353892 | validation: 2.7582483099647686]
	TIME [epoch: 8.48 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.703854937656039		[learning rate: 0.001865]
	Learning Rate: 0.00186502
	LOSS [training: 2.703854937656039 | validation: 2.596441902183477]
	TIME [epoch: 8.46 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6395802370986017		[learning rate: 0.0018583]
	Learning Rate: 0.00185825
	LOSS [training: 2.6395802370986017 | validation: 2.6183973982123367]
	TIME [epoch: 8.43 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.671489663257943		[learning rate: 0.0018515]
	Learning Rate: 0.00185151
	LOSS [training: 2.671489663257943 | validation: 2.556828379159348]
	TIME [epoch: 8.44 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6408927827196864		[learning rate: 0.0018448]
	Learning Rate: 0.00184479
	LOSS [training: 2.6408927827196864 | validation: 2.5662585623350553]
	TIME [epoch: 8.46 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.641066444965183		[learning rate: 0.0018381]
	Learning Rate: 0.0018381
	LOSS [training: 2.641066444965183 | validation: 2.5558003295216167]
	TIME [epoch: 8.45 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6517629341412197		[learning rate: 0.0018314]
	Learning Rate: 0.00183143
	LOSS [training: 2.6517629341412197 | validation: 2.553922109767826]
	TIME [epoch: 8.48 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6589901147065604		[learning rate: 0.0018248]
	Learning Rate: 0.00182478
	LOSS [training: 2.6589901147065604 | validation: 2.6505104572358507]
	TIME [epoch: 8.43 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.648822527205879		[learning rate: 0.0018182]
	Learning Rate: 0.00181816
	LOSS [training: 2.648822527205879 | validation: 2.581845294657307]
	TIME [epoch: 8.45 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6442645577915562		[learning rate: 0.0018116]
	Learning Rate: 0.00181156
	LOSS [training: 2.6442645577915562 | validation: 2.571222317297966]
	TIME [epoch: 8.44 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6538447902158793		[learning rate: 0.001805]
	Learning Rate: 0.00180499
	LOSS [training: 2.6538447902158793 | validation: 2.5657783268652437]
	TIME [epoch: 8.43 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6324579126288072		[learning rate: 0.0017984]
	Learning Rate: 0.00179844
	LOSS [training: 2.6324579126288072 | validation: 2.586820075254893]
	TIME [epoch: 8.47 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6994050349460204		[learning rate: 0.0017919]
	Learning Rate: 0.00179191
	LOSS [training: 2.6994050349460204 | validation: 2.565474021763247]
	TIME [epoch: 8.48 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7246416157001354		[learning rate: 0.0017854]
	Learning Rate: 0.00178541
	LOSS [training: 2.7246416157001354 | validation: 2.662988731576273]
	TIME [epoch: 8.43 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.701990475960522		[learning rate: 0.0017789]
	Learning Rate: 0.00177893
	LOSS [training: 2.701990475960522 | validation: 2.662315295616951]
	TIME [epoch: 8.43 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6587283642425175		[learning rate: 0.0017725]
	Learning Rate: 0.00177247
	LOSS [training: 2.6587283642425175 | validation: 2.5662834884525987]
	TIME [epoch: 8.45 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.634511751871854		[learning rate: 0.001766]
	Learning Rate: 0.00176604
	LOSS [training: 2.634511751871854 | validation: 2.685057986181]
	TIME [epoch: 8.45 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6745805848167357		[learning rate: 0.0017596]
	Learning Rate: 0.00175963
	LOSS [training: 2.6745805848167357 | validation: 2.5892834963740894]
	TIME [epoch: 8.44 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.666940458168998		[learning rate: 0.0017532]
	Learning Rate: 0.00175324
	LOSS [training: 2.666940458168998 | validation: 2.66154440229814]
	TIME [epoch: 8.43 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6542131316814888		[learning rate: 0.0017469]
	Learning Rate: 0.00174688
	LOSS [training: 2.6542131316814888 | validation: 2.546500536216457]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6552627545756335		[learning rate: 0.0017405]
	Learning Rate: 0.00174054
	LOSS [training: 2.6552627545756335 | validation: 2.5880927638401046]
	TIME [epoch: 8.45 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.637783821074435		[learning rate: 0.0017342]
	Learning Rate: 0.00173422
	LOSS [training: 2.637783821074435 | validation: 2.608949021271812]
	TIME [epoch: 8.44 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6397211510312992		[learning rate: 0.0017279]
	Learning Rate: 0.00172793
	LOSS [training: 2.6397211510312992 | validation: 2.545706975363654]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_583.pth
	Model improved!!!
EPOCH 584/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6465622871035563		[learning rate: 0.0017217]
	Learning Rate: 0.00172166
	LOSS [training: 2.6465622871035563 | validation: 2.5911619124401697]
	TIME [epoch: 8.43 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.651888102304798		[learning rate: 0.0017154]
	Learning Rate: 0.00171541
	LOSS [training: 2.651888102304798 | validation: 2.5690879958449435]
	TIME [epoch: 8.45 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.680649725281343		[learning rate: 0.0017092]
	Learning Rate: 0.00170919
	LOSS [training: 2.680649725281343 | validation: 2.562804112464679]
	TIME [epoch: 8.49 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6433134873824535		[learning rate: 0.001703]
	Learning Rate: 0.00170298
	LOSS [training: 2.6433134873824535 | validation: 2.6744950781942474]
	TIME [epoch: 8.42 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6615547197292146		[learning rate: 0.0016968]
	Learning Rate: 0.0016968
	LOSS [training: 2.6615547197292146 | validation: 2.56628507812572]
	TIME [epoch: 8.43 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6267588986431494		[learning rate: 0.0016906]
	Learning Rate: 0.00169065
	LOSS [training: 2.6267588986431494 | validation: 2.5618765446989427]
	TIME [epoch: 8.45 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.63410197233815		[learning rate: 0.0016845]
	Learning Rate: 0.00168451
	LOSS [training: 2.63410197233815 | validation: 2.5605582874361033]
	TIME [epoch: 8.44 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.65916870936958		[learning rate: 0.0016784]
	Learning Rate: 0.0016784
	LOSS [training: 2.65916870936958 | validation: 2.5709972947752293]
	TIME [epoch: 8.44 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.669564220212604		[learning rate: 0.0016723]
	Learning Rate: 0.00167231
	LOSS [training: 2.669564220212604 | validation: 2.635603675615009]
	TIME [epoch: 8.44 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6354816638202285		[learning rate: 0.0016662]
	Learning Rate: 0.00166624
	LOSS [training: 2.6354816638202285 | validation: 2.610939589134101]
	TIME [epoch: 8.46 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6360351380709113		[learning rate: 0.0016602]
	Learning Rate: 0.00166019
	LOSS [training: 2.6360351380709113 | validation: 2.570046245619361]
	TIME [epoch: 8.43 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6322552700910373		[learning rate: 0.0016542]
	Learning Rate: 0.00165417
	LOSS [training: 2.6322552700910373 | validation: 2.6352719605899138]
	TIME [epoch: 8.44 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6684880250408556		[learning rate: 0.0016482]
	Learning Rate: 0.00164816
	LOSS [training: 2.6684880250408556 | validation: 2.6499431371322157]
	TIME [epoch: 8.49 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6401708970048157		[learning rate: 0.0016422]
	Learning Rate: 0.00164218
	LOSS [training: 2.6401708970048157 | validation: 2.5506432079823664]
	TIME [epoch: 8.46 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.626529446110319		[learning rate: 0.0016362]
	Learning Rate: 0.00163622
	LOSS [training: 2.626529446110319 | validation: 2.558064764872846]
	TIME [epoch: 8.44 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6701563312319516		[learning rate: 0.0016303]
	Learning Rate: 0.00163028
	LOSS [training: 2.6701563312319516 | validation: 2.587868511724015]
	TIME [epoch: 8.46 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.639365314222353		[learning rate: 0.0016244]
	Learning Rate: 0.00162437
	LOSS [training: 2.639365314222353 | validation: 2.586691200982565]
	TIME [epoch: 8.45 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7031300586578295		[learning rate: 0.0016185]
	Learning Rate: 0.00161847
	LOSS [training: 2.7031300586578295 | validation: 2.66551492751437]
	TIME [epoch: 8.45 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.632434389011787		[learning rate: 0.0016126]
	Learning Rate: 0.0016126
	LOSS [training: 2.632434389011787 | validation: 2.557710197638264]
	TIME [epoch: 8.45 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6458250533144385		[learning rate: 0.0016067]
	Learning Rate: 0.00160675
	LOSS [training: 2.6458250533144385 | validation: 2.606710213743421]
	TIME [epoch: 8.43 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6488298505716967		[learning rate: 0.0016009]
	Learning Rate: 0.00160092
	LOSS [training: 2.6488298505716967 | validation: 2.728491402872563]
	TIME [epoch: 8.45 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.712330113946424		[learning rate: 0.0015951]
	Learning Rate: 0.00159511
	LOSS [training: 2.712330113946424 | validation: 2.5783544862191112]
	TIME [epoch: 8.48 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.659289166388261		[learning rate: 0.0015893]
	Learning Rate: 0.00158932
	LOSS [training: 2.659289166388261 | validation: 2.548939508948153]
	TIME [epoch: 8.47 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6338835010693624		[learning rate: 0.0015835]
	Learning Rate: 0.00158355
	LOSS [training: 2.6338835010693624 | validation: 2.5969913349915568]
	TIME [epoch: 8.42 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6462077755270994		[learning rate: 0.0015778]
	Learning Rate: 0.0015778
	LOSS [training: 2.6462077755270994 | validation: 2.5918421549820185]
	TIME [epoch: 8.44 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.645937969397864		[learning rate: 0.0015721]
	Learning Rate: 0.00157208
	LOSS [training: 2.645937969397864 | validation: 2.5960844428290852]
	TIME [epoch: 8.45 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6945084196030025		[learning rate: 0.0015664]
	Learning Rate: 0.00156637
	LOSS [training: 2.6945084196030025 | validation: 2.5831650861906112]
	TIME [epoch: 8.44 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.626363791005979		[learning rate: 0.0015607]
	Learning Rate: 0.00156069
	LOSS [training: 2.626363791005979 | validation: 2.5506004756352425]
	TIME [epoch: 8.42 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.666263170559541		[learning rate: 0.001555]
	Learning Rate: 0.00155502
	LOSS [training: 2.666263170559541 | validation: 2.5552994256840096]
	TIME [epoch: 8.48 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.636669922244934		[learning rate: 0.0015494]
	Learning Rate: 0.00154938
	LOSS [training: 2.636669922244934 | validation: 2.6219071770280165]
	TIME [epoch: 8.46 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.664916744064326		[learning rate: 0.0015438]
	Learning Rate: 0.00154376
	LOSS [training: 2.664916744064326 | validation: 2.5879975397197157]
	TIME [epoch: 8.43 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6637421271732804		[learning rate: 0.0015382]
	Learning Rate: 0.00153815
	LOSS [training: 2.6637421271732804 | validation: 2.5641649624389258]
	TIME [epoch: 8.44 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6433859493995113		[learning rate: 0.0015326]
	Learning Rate: 0.00153257
	LOSS [training: 2.6433859493995113 | validation: 2.570376971740428]
	TIME [epoch: 8.45 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6486351439442384		[learning rate: 0.001527]
	Learning Rate: 0.00152701
	LOSS [training: 2.6486351439442384 | validation: 2.5491187686208416]
	TIME [epoch: 8.45 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.641434146458595		[learning rate: 0.0015215]
	Learning Rate: 0.00152147
	LOSS [training: 2.641434146458595 | validation: 2.583697274095187]
	TIME [epoch: 8.43 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.641276866929336		[learning rate: 0.0015159]
	Learning Rate: 0.00151595
	LOSS [training: 2.641276866929336 | validation: 2.55660090400938]
	TIME [epoch: 8.45 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.631623731467633		[learning rate: 0.0015104]
	Learning Rate: 0.00151045
	LOSS [training: 2.631623731467633 | validation: 2.535669924728896]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6232767644947037		[learning rate: 0.001505]
	Learning Rate: 0.00150496
	LOSS [training: 2.6232767644947037 | validation: 2.750711111852354]
	TIME [epoch: 8.46 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6577858601387434		[learning rate: 0.0014995]
	Learning Rate: 0.0014995
	LOSS [training: 2.6577858601387434 | validation: 2.6419730012720968]
	TIME [epoch: 8.43 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6699628146762313		[learning rate: 0.0014941]
	Learning Rate: 0.00149406
	LOSS [training: 2.6699628146762313 | validation: 2.5966217178301845]
	TIME [epoch: 8.44 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6250196160056336		[learning rate: 0.0014886]
	Learning Rate: 0.00148864
	LOSS [training: 2.6250196160056336 | validation: 2.6788263546913313]
	TIME [epoch: 8.47 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6657529322849554		[learning rate: 0.0014832]
	Learning Rate: 0.00148324
	LOSS [training: 2.6657529322849554 | validation: 2.7760582952371506]
	TIME [epoch: 8.48 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.657118755065212		[learning rate: 0.0014779]
	Learning Rate: 0.00147785
	LOSS [training: 2.657118755065212 | validation: 2.6364623619795164]
	TIME [epoch: 8.43 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6541784162567046		[learning rate: 0.0014725]
	Learning Rate: 0.00147249
	LOSS [training: 2.6541784162567046 | validation: 2.5718966820987923]
	TIME [epoch: 8.44 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6327402329829606		[learning rate: 0.0014671]
	Learning Rate: 0.00146715
	LOSS [training: 2.6327402329829606 | validation: 2.7635583671468047]
	TIME [epoch: 8.45 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6600777819986		[learning rate: 0.0014618]
	Learning Rate: 0.00146182
	LOSS [training: 2.6600777819986 | validation: 2.6011325536990295]
	TIME [epoch: 8.46 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6560211149054305		[learning rate: 0.0014565]
	Learning Rate: 0.00145652
	LOSS [training: 2.6560211149054305 | validation: 2.546627713188373]
	TIME [epoch: 8.44 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7660134664800213		[learning rate: 0.0014512]
	Learning Rate: 0.00145123
	LOSS [training: 2.7660134664800213 | validation: 2.555091631392772]
	TIME [epoch: 8.44 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.619697454590207		[learning rate: 0.001446]
	Learning Rate: 0.00144597
	LOSS [training: 2.619697454590207 | validation: 2.6011803426690046]
	TIME [epoch: 8.45 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.635501566702931		[learning rate: 0.0014407]
	Learning Rate: 0.00144072
	LOSS [training: 2.635501566702931 | validation: 2.581581585522941]
	TIME [epoch: 8.46 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6372801218855417		[learning rate: 0.0014355]
	Learning Rate: 0.00143549
	LOSS [training: 2.6372801218855417 | validation: 2.562632917406217]
	TIME [epoch: 8.43 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.631637794231531		[learning rate: 0.0014303]
	Learning Rate: 0.00143028
	LOSS [training: 2.631637794231531 | validation: 2.6444192822456616]
	TIME [epoch: 8.44 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6380825410012974		[learning rate: 0.0014251]
	Learning Rate: 0.00142509
	LOSS [training: 2.6380825410012974 | validation: 2.5714166171863404]
	TIME [epoch: 8.45 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.632741399811362		[learning rate: 0.0014199]
	Learning Rate: 0.00141992
	LOSS [training: 2.632741399811362 | validation: 2.5813193767833633]
	TIME [epoch: 8.46 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6273078186806726		[learning rate: 0.0014148]
	Learning Rate: 0.00141476
	LOSS [training: 2.6273078186806726 | validation: 2.574208176182701]
	TIME [epoch: 8.43 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6310439134894796		[learning rate: 0.0014096]
	Learning Rate: 0.00140963
	LOSS [training: 2.6310439134894796 | validation: 2.559934880328825]
	TIME [epoch: 8.44 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.628157915284695		[learning rate: 0.0014045]
	Learning Rate: 0.00140451
	LOSS [training: 2.628157915284695 | validation: 2.566794974173173]
	TIME [epoch: 8.47 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6438103966599784		[learning rate: 0.0013994]
	Learning Rate: 0.00139942
	LOSS [training: 2.6438103966599784 | validation: 2.537656671773603]
	TIME [epoch: 8.48 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.646100890051067		[learning rate: 0.0013943]
	Learning Rate: 0.00139434
	LOSS [training: 2.646100890051067 | validation: 2.571860198072007]
	TIME [epoch: 8.46 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606170158621759		[learning rate: 0.0013893]
	Learning Rate: 0.00138928
	LOSS [training: 2.606170158621759 | validation: 2.5481382627853364]
	TIME [epoch: 8.43 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6391850137263537		[learning rate: 0.0013842]
	Learning Rate: 0.00138424
	LOSS [training: 2.6391850137263537 | validation: 2.567298325201557]
	TIME [epoch: 8.46 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6454737684040124		[learning rate: 0.0013792]
	Learning Rate: 0.00137921
	LOSS [training: 2.6454737684040124 | validation: 2.5625703030853755]
	TIME [epoch: 8.47 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.630324294104896		[learning rate: 0.0013742]
	Learning Rate: 0.00137421
	LOSS [training: 2.630324294104896 | validation: 2.540700928550829]
	TIME [epoch: 8.46 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6335746850250437		[learning rate: 0.0013692]
	Learning Rate: 0.00136922
	LOSS [training: 2.6335746850250437 | validation: 2.5912505937706136]
	TIME [epoch: 8.43 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6422818225240414		[learning rate: 0.0013643]
	Learning Rate: 0.00136425
	LOSS [training: 2.6422818225240414 | validation: 2.60436535269608]
	TIME [epoch: 8.44 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.656807148681044		[learning rate: 0.0013593]
	Learning Rate: 0.0013593
	LOSS [training: 2.656807148681044 | validation: 2.546595723028851]
	TIME [epoch: 8.45 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6352215081731045		[learning rate: 0.0013544]
	Learning Rate: 0.00135437
	LOSS [training: 2.6352215081731045 | validation: 2.5715557037535044]
	TIME [epoch: 8.43 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6223502289529304		[learning rate: 0.0013495]
	Learning Rate: 0.00134945
	LOSS [training: 2.6223502289529304 | validation: 2.547963836895179]
	TIME [epoch: 8.47 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.641344995454266		[learning rate: 0.0013446]
	Learning Rate: 0.00134456
	LOSS [training: 2.641344995454266 | validation: 2.5474012614428494]
	TIME [epoch: 8.46 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6175825604064236		[learning rate: 0.0013397]
	Learning Rate: 0.00133968
	LOSS [training: 2.6175825604064236 | validation: 2.5381299268232134]
	TIME [epoch: 8.44 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.623520334408098		[learning rate: 0.0013348]
	Learning Rate: 0.00133481
	LOSS [training: 2.623520334408098 | validation: 2.529751734754685]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_654.pth
	Model improved!!!
EPOCH 655/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6204336028248276		[learning rate: 0.00133]
	Learning Rate: 0.00132997
	LOSS [training: 2.6204336028248276 | validation: 2.5779835654828958]
	TIME [epoch: 8.46 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6240500833930676		[learning rate: 0.0013251]
	Learning Rate: 0.00132514
	LOSS [training: 2.6240500833930676 | validation: 2.5493375358961776]
	TIME [epoch: 8.47 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.627949852084445		[learning rate: 0.0013203]
	Learning Rate: 0.00132034
	LOSS [training: 2.627949852084445 | validation: 2.5787117712082526]
	TIME [epoch: 8.48 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.637427830923518		[learning rate: 0.0013155]
	Learning Rate: 0.00131554
	LOSS [training: 2.637427830923518 | validation: 2.5519000981297983]
	TIME [epoch: 8.52 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6159953904462996		[learning rate: 0.0013108]
	Learning Rate: 0.00131077
	LOSS [training: 2.6159953904462996 | validation: 2.5371083939811774]
	TIME [epoch: 8.44 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.623148007801093		[learning rate: 0.001306]
	Learning Rate: 0.00130601
	LOSS [training: 2.623148007801093 | validation: 2.556294499971473]
	TIME [epoch: 8.45 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6249698760206988		[learning rate: 0.0013013]
	Learning Rate: 0.00130127
	LOSS [training: 2.6249698760206988 | validation: 2.5449196300090158]
	TIME [epoch: 8.49 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6187756259813275		[learning rate: 0.0012966]
	Learning Rate: 0.00129655
	LOSS [training: 2.6187756259813275 | validation: 2.5285077683333936]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6180583936999002		[learning rate: 0.0012918]
	Learning Rate: 0.00129185
	LOSS [training: 2.6180583936999002 | validation: 2.5402205645144478]
	TIME [epoch: 8.47 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6173727172475996		[learning rate: 0.0012872]
	Learning Rate: 0.00128716
	LOSS [training: 2.6173727172475996 | validation: 2.596204126563019]
	TIME [epoch: 8.46 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.655532348631108		[learning rate: 0.0012825]
	Learning Rate: 0.00128249
	LOSS [training: 2.655532348631108 | validation: 2.554876857743247]
	TIME [epoch: 8.53 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6206305746828744		[learning rate: 0.0012778]
	Learning Rate: 0.00127783
	LOSS [training: 2.6206305746828744 | validation: 2.572439787702148]
	TIME [epoch: 8.46 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6362537723738813		[learning rate: 0.0012732]
	Learning Rate: 0.00127319
	LOSS [training: 2.6362537723738813 | validation: 2.5937776633900445]
	TIME [epoch: 8.45 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613623506241025		[learning rate: 0.0012686]
	Learning Rate: 0.00126857
	LOSS [training: 2.613623506241025 | validation: 2.5704040666437162]
	TIME [epoch: 8.46 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6256515688452446		[learning rate: 0.001264]
	Learning Rate: 0.00126397
	LOSS [training: 2.6256515688452446 | validation: 2.546770784832937]
	TIME [epoch: 8.47 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6403307955418507		[learning rate: 0.0012594]
	Learning Rate: 0.00125938
	LOSS [training: 2.6403307955418507 | validation: 2.5961785547103933]
	TIME [epoch: 8.46 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.644783261396146		[learning rate: 0.0012548]
	Learning Rate: 0.00125481
	LOSS [training: 2.644783261396146 | validation: 2.6164200163097187]
	TIME [epoch: 8.46 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6218461088382874		[learning rate: 0.0012503]
	Learning Rate: 0.00125026
	LOSS [training: 2.6218461088382874 | validation: 2.5819137573769417]
	TIME [epoch: 8.46 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.637006015522548		[learning rate: 0.0012457]
	Learning Rate: 0.00124572
	LOSS [training: 2.637006015522548 | validation: 2.5608608103643196]
	TIME [epoch: 8.48 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6347678387966225		[learning rate: 0.0012412]
	Learning Rate: 0.0012412
	LOSS [training: 2.6347678387966225 | validation: 2.596106661753212]
	TIME [epoch: 8.46 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6297054031845515		[learning rate: 0.0012367]
	Learning Rate: 0.0012367
	LOSS [training: 2.6297054031845515 | validation: 2.567230942460136]
	TIME [epoch: 8.47 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6589734708916315		[learning rate: 0.0012322]
	Learning Rate: 0.00123221
	LOSS [training: 2.6589734708916315 | validation: 2.615999474627272]
	TIME [epoch: 8.45 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.62177360039881		[learning rate: 0.0012277]
	Learning Rate: 0.00122774
	LOSS [training: 2.62177360039881 | validation: 2.5629748822063254]
	TIME [epoch: 8.48 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.657140574584314		[learning rate: 0.0012233]
	Learning Rate: 0.00122328
	LOSS [training: 2.657140574584314 | validation: 2.7049256042825363]
	TIME [epoch: 8.45 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6337351828249114		[learning rate: 0.0012188]
	Learning Rate: 0.00121884
	LOSS [training: 2.6337351828249114 | validation: 2.5929729825922556]
	TIME [epoch: 8.45 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.622679380894861		[learning rate: 0.0012144]
	Learning Rate: 0.00121442
	LOSS [training: 2.622679380894861 | validation: 2.58124994735668]
	TIME [epoch: 8.5 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.639039585389713		[learning rate: 0.00121]
	Learning Rate: 0.00121001
	LOSS [training: 2.639039585389713 | validation: 2.567855722545292]
	TIME [epoch: 8.5 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6604295972487786		[learning rate: 0.0012056]
	Learning Rate: 0.00120562
	LOSS [training: 2.6604295972487786 | validation: 2.6859992592859827]
	TIME [epoch: 8.44 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.64048872069844		[learning rate: 0.0012012]
	Learning Rate: 0.00120125
	LOSS [training: 2.64048872069844 | validation: 2.5544338169779905]
	TIME [epoch: 8.47 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6200780636587333		[learning rate: 0.0011969]
	Learning Rate: 0.00119689
	LOSS [training: 2.6200780636587333 | validation: 2.5916040914932106]
	TIME [epoch: 8.54 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6691882600441605		[learning rate: 0.0011925]
	Learning Rate: 0.00119254
	LOSS [training: 2.6691882600441605 | validation: 2.5814280326436774]
	TIME [epoch: 8.46 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6441327509000003		[learning rate: 0.0011882]
	Learning Rate: 0.00118821
	LOSS [training: 2.6441327509000003 | validation: 2.535872269588276]
	TIME [epoch: 8.46 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6451653753617625		[learning rate: 0.0011839]
	Learning Rate: 0.0011839
	LOSS [training: 2.6451653753617625 | validation: 2.555646940024525]
	TIME [epoch: 8.44 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6340718701956063		[learning rate: 0.0011796]
	Learning Rate: 0.00117961
	LOSS [training: 2.6340718701956063 | validation: 2.640652037597702]
	TIME [epoch: 8.51 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6410960997619557		[learning rate: 0.0011753]
	Learning Rate: 0.00117532
	LOSS [training: 2.6410960997619557 | validation: 2.545256372513629]
	TIME [epoch: 8.48 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.641630505587533		[learning rate: 0.0011711]
	Learning Rate: 0.00117106
	LOSS [training: 2.641630505587533 | validation: 2.5910889866621054]
	TIME [epoch: 8.46 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6116559798603305		[learning rate: 0.0011668]
	Learning Rate: 0.00116681
	LOSS [training: 2.6116559798603305 | validation: 2.7056190615483646]
	TIME [epoch: 8.46 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6443756289600318		[learning rate: 0.0011626]
	Learning Rate: 0.00116258
	LOSS [training: 2.6443756289600318 | validation: 2.611419367214613]
	TIME [epoch: 8.49 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6877950874137424		[learning rate: 0.0011584]
	Learning Rate: 0.00115836
	LOSS [training: 2.6877950874137424 | validation: 2.558496341406996]
	TIME [epoch: 8.5 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.623468875473806		[learning rate: 0.0011542]
	Learning Rate: 0.00115415
	LOSS [training: 2.623468875473806 | validation: 2.559918656750023]
	TIME [epoch: 8.48 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.620757792421883		[learning rate: 0.00115]
	Learning Rate: 0.00114996
	LOSS [training: 2.620757792421883 | validation: 2.5502634280581864]
	TIME [epoch: 8.45 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6204693850193683		[learning rate: 0.0011458]
	Learning Rate: 0.00114579
	LOSS [training: 2.6204693850193683 | validation: 2.5473569305529526]
	TIME [epoch: 8.48 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6204767053707507		[learning rate: 0.0011416]
	Learning Rate: 0.00114163
	LOSS [training: 2.6204767053707507 | validation: 2.547891341586091]
	TIME [epoch: 8.46 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6173925250611663		[learning rate: 0.0011375]
	Learning Rate: 0.00113749
	LOSS [training: 2.6173925250611663 | validation: 2.6282039014995537]
	TIME [epoch: 8.45 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.672130623678833		[learning rate: 0.0011334]
	Learning Rate: 0.00113336
	LOSS [training: 2.672130623678833 | validation: 2.5984372749002365]
	TIME [epoch: 8.46 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.622960716853617		[learning rate: 0.0011292]
	Learning Rate: 0.00112925
	LOSS [training: 2.622960716853617 | validation: 2.548227252155323]
	TIME [epoch: 8.47 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.628525708618675		[learning rate: 0.0011252]
	Learning Rate: 0.00112515
	LOSS [training: 2.628525708618675 | validation: 2.574514265607408]
	TIME [epoch: 8.47 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.657461536448938		[learning rate: 0.0011211]
	Learning Rate: 0.00112107
	LOSS [training: 2.657461536448938 | validation: 2.791808087837823]
	TIME [epoch: 8.45 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6543503428692894		[learning rate: 0.001117]
	Learning Rate: 0.001117
	LOSS [training: 2.6543503428692894 | validation: 2.594674160077674]
	TIME [epoch: 8.47 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6483478529799283		[learning rate: 0.0011129]
	Learning Rate: 0.00111295
	LOSS [training: 2.6483478529799283 | validation: 2.5524374985084606]
	TIME [epoch: 8.54 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6056204177687063		[learning rate: 0.0011089]
	Learning Rate: 0.00110891
	LOSS [training: 2.6056204177687063 | validation: 2.556600559441368]
	TIME [epoch: 8.45 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6121675366036996		[learning rate: 0.0011049]
	Learning Rate: 0.00110488
	LOSS [training: 2.6121675366036996 | validation: 2.564533485935583]
	TIME [epoch: 8.45 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.611203942499322		[learning rate: 0.0011009]
	Learning Rate: 0.00110087
	LOSS [training: 2.611203942499322 | validation: 2.6178168542924194]
	TIME [epoch: 8.46 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6175840926308593		[learning rate: 0.0010969]
	Learning Rate: 0.00109688
	LOSS [training: 2.6175840926308593 | validation: 2.542380468752632]
	TIME [epoch: 8.47 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600954859530384		[learning rate: 0.0010929]
	Learning Rate: 0.0010929
	LOSS [training: 2.600954859530384 | validation: 2.540160932574307]
	TIME [epoch: 8.46 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.628092843096353		[learning rate: 0.0010889]
	Learning Rate: 0.00108893
	LOSS [training: 2.628092843096353 | validation: 2.5514047049590554]
	TIME [epoch: 8.46 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6125694421831365		[learning rate: 0.001085]
	Learning Rate: 0.00108498
	LOSS [training: 2.6125694421831365 | validation: 2.5414421832514327]
	TIME [epoch: 8.46 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.619802377476661		[learning rate: 0.001081]
	Learning Rate: 0.00108104
	LOSS [training: 2.619802377476661 | validation: 2.592873959756411]
	TIME [epoch: 8.48 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.633023264229581		[learning rate: 0.0010771]
	Learning Rate: 0.00107712
	LOSS [training: 2.633023264229581 | validation: 2.5381888502531647]
	TIME [epoch: 8.46 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.620408165920862		[learning rate: 0.0010732]
	Learning Rate: 0.00107321
	LOSS [training: 2.620408165920862 | validation: 2.5638308481975085]
	TIME [epoch: 8.47 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6273001985292748		[learning rate: 0.0010693]
	Learning Rate: 0.00106931
	LOSS [training: 2.6273001985292748 | validation: 2.521290850370864]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_715.pth
	Model improved!!!
EPOCH 716/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6709928051191354		[learning rate: 0.0010654]
	Learning Rate: 0.00106543
	LOSS [training: 2.6709928051191354 | validation: 2.5383792117433437]
	TIME [epoch: 8.47 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6453906062317936		[learning rate: 0.0010616]
	Learning Rate: 0.00106157
	LOSS [training: 2.6453906062317936 | validation: 2.59948823108156]
	TIME [epoch: 8.45 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613700037398215		[learning rate: 0.0010577]
	Learning Rate: 0.00105771
	LOSS [training: 2.613700037398215 | validation: 2.5497516643015805]
	TIME [epoch: 8.45 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.628807400542181		[learning rate: 0.0010539]
	Learning Rate: 0.00105388
	LOSS [training: 2.628807400542181 | validation: 2.5618131759120812]
	TIME [epoch: 8.45 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6130382251754947		[learning rate: 0.0010501]
	Learning Rate: 0.00105005
	LOSS [training: 2.6130382251754947 | validation: 2.539397548183709]
	TIME [epoch: 8.46 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6053717342219187		[learning rate: 0.0010462]
	Learning Rate: 0.00104624
	LOSS [training: 2.6053717342219187 | validation: 2.551502080164895]
	TIME [epoch: 8.47 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.611629958250405		[learning rate: 0.0010424]
	Learning Rate: 0.00104244
	LOSS [training: 2.611629958250405 | validation: 2.5338665790056103]
	TIME [epoch: 8.45 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.625086586361417		[learning rate: 0.0010387]
	Learning Rate: 0.00103866
	LOSS [training: 2.625086586361417 | validation: 2.5481007503180995]
	TIME [epoch: 8.52 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6207773368503267		[learning rate: 0.0010349]
	Learning Rate: 0.00103489
	LOSS [training: 2.6207773368503267 | validation: 2.533873409401325]
	TIME [epoch: 8.48 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6256772211619657		[learning rate: 0.0010311]
	Learning Rate: 0.00103114
	LOSS [training: 2.6256772211619657 | validation: 2.544768280194507]
	TIME [epoch: 8.45 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6295786927954703		[learning rate: 0.0010274]
	Learning Rate: 0.00102739
	LOSS [training: 2.6295786927954703 | validation: 2.6274841045723223]
	TIME [epoch: 8.52 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6113253600293898		[learning rate: 0.0010237]
	Learning Rate: 0.00102367
	LOSS [training: 2.6113253600293898 | validation: 2.5450314048535034]
	TIME [epoch: 8.45 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60735324303391		[learning rate: 0.00102]
	Learning Rate: 0.00101995
	LOSS [training: 2.60735324303391 | validation: 2.558523916065343]
	TIME [epoch: 8.47 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6136224996743818		[learning rate: 0.0010162]
	Learning Rate: 0.00101625
	LOSS [training: 2.6136224996743818 | validation: 2.5318788679092066]
	TIME [epoch: 8.47 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6272976218217226		[learning rate: 0.0010126]
	Learning Rate: 0.00101256
	LOSS [training: 2.6272976218217226 | validation: 2.633543012929323]
	TIME [epoch: 8.46 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.628211395002507		[learning rate: 0.0010089]
	Learning Rate: 0.00100889
	LOSS [training: 2.628211395002507 | validation: 2.5406326037184437]
	TIME [epoch: 8.51 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6096454315791275		[learning rate: 0.0010052]
	Learning Rate: 0.00100522
	LOSS [training: 2.6096454315791275 | validation: 2.545618888638023]
	TIME [epoch: 8.47 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6090220826756676		[learning rate: 0.0010016]
	Learning Rate: 0.00100158
	LOSS [training: 2.6090220826756676 | validation: 2.6987272300295686]
	TIME [epoch: 8.47 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6407390996772393		[learning rate: 0.00099794]
	Learning Rate: 0.000997942
	LOSS [training: 2.6407390996772393 | validation: 2.5542230334817324]
	TIME [epoch: 8.46 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.632593690110741		[learning rate: 0.00099432]
	Learning Rate: 0.00099432
	LOSS [training: 2.632593690110741 | validation: 2.6071671158467757]
	TIME [epoch: 8.47 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.642816327399182		[learning rate: 0.00099071]
	Learning Rate: 0.000990712
	LOSS [training: 2.642816327399182 | validation: 2.5428355060194656]
	TIME [epoch: 8.49 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6243192536920232		[learning rate: 0.00098712]
	Learning Rate: 0.000987116
	LOSS [training: 2.6243192536920232 | validation: 2.550919842813893]
	TIME [epoch: 8.45 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6296379553603515		[learning rate: 0.00098353]
	Learning Rate: 0.000983534
	LOSS [training: 2.6296379553603515 | validation: 2.540722320943824]
	TIME [epoch: 8.47 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6187810598859764		[learning rate: 0.00097996]
	Learning Rate: 0.000979965
	LOSS [training: 2.6187810598859764 | validation: 2.539933830660418]
	TIME [epoch: 8.5 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6303273459130785		[learning rate: 0.00097641]
	Learning Rate: 0.000976409
	LOSS [training: 2.6303273459130785 | validation: 2.5498156036193764]
	TIME [epoch: 8.45 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6250214103192984		[learning rate: 0.00097287]
	Learning Rate: 0.000972865
	LOSS [training: 2.6250214103192984 | validation: 2.585224824146297]
	TIME [epoch: 8.44 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6174919566773363		[learning rate: 0.00096933]
	Learning Rate: 0.000969335
	LOSS [training: 2.6174919566773363 | validation: 2.545289252713905]
	TIME [epoch: 8.45 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6186913522873607		[learning rate: 0.00096582]
	Learning Rate: 0.000965817
	LOSS [training: 2.6186913522873607 | validation: 2.669626792305773]
	TIME [epoch: 8.46 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.625674335309261		[learning rate: 0.00096231]
	Learning Rate: 0.000962312
	LOSS [training: 2.625674335309261 | validation: 2.551880745878881]
	TIME [epoch: 8.45 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6144076915782084		[learning rate: 0.00095882]
	Learning Rate: 0.00095882
	LOSS [training: 2.6144076915782084 | validation: 2.6047503450487826]
	TIME [epoch: 8.45 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.617925169309308		[learning rate: 0.00095534]
	Learning Rate: 0.00095534
	LOSS [training: 2.617925169309308 | validation: 2.5738960021320016]
	TIME [epoch: 8.44 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6525446666807575		[learning rate: 0.00095187]
	Learning Rate: 0.000951873
	LOSS [training: 2.6525446666807575 | validation: 2.5457588797180057]
	TIME [epoch: 8.47 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6135906367142616		[learning rate: 0.00094842]
	Learning Rate: 0.000948419
	LOSS [training: 2.6135906367142616 | validation: 2.583718565171016]
	TIME [epoch: 8.45 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.620299009307573		[learning rate: 0.00094498]
	Learning Rate: 0.000944977
	LOSS [training: 2.620299009307573 | validation: 2.6023881751895432]
	TIME [epoch: 8.45 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600903676083189		[learning rate: 0.00094155]
	Learning Rate: 0.000941547
	LOSS [training: 2.600903676083189 | validation: 2.5396565733990277]
	TIME [epoch: 8.47 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.645516467363187		[learning rate: 0.00093813]
	Learning Rate: 0.00093813
	LOSS [training: 2.645516467363187 | validation: 2.548297812056001]
	TIME [epoch: 8.49 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6257851355739787		[learning rate: 0.00093473]
	Learning Rate: 0.000934726
	LOSS [training: 2.6257851355739787 | validation: 2.612894979774877]
	TIME [epoch: 8.44 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6403076958344696		[learning rate: 0.00093133]
	Learning Rate: 0.000931334
	LOSS [training: 2.6403076958344696 | validation: 2.5464337398668366]
	TIME [epoch: 8.43 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.617282499152169		[learning rate: 0.00092795]
	Learning Rate: 0.000927954
	LOSS [training: 2.617282499152169 | validation: 2.57246009727897]
	TIME [epoch: 8.45 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6082903074295096		[learning rate: 0.00092459]
	Learning Rate: 0.000924586
	LOSS [training: 2.6082903074295096 | validation: 2.561193148910045]
	TIME [epoch: 8.46 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.611561361941991		[learning rate: 0.00092123]
	Learning Rate: 0.000921231
	LOSS [training: 2.611561361941991 | validation: 2.5707271588383813]
	TIME [epoch: 8.45 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6277679391668975		[learning rate: 0.00091789]
	Learning Rate: 0.000917888
	LOSS [training: 2.6277679391668975 | validation: 2.5607398883732326]
	TIME [epoch: 8.45 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6264325373616226		[learning rate: 0.00091456]
	Learning Rate: 0.000914556
	LOSS [training: 2.6264325373616226 | validation: 2.627042636943223]
	TIME [epoch: 8.44 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6059927911660425		[learning rate: 0.00091124]
	Learning Rate: 0.000911237
	LOSS [training: 2.6059927911660425 | validation: 2.54015093185287]
	TIME [epoch: 8.46 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.617716649018237		[learning rate: 0.00090793]
	Learning Rate: 0.000907931
	LOSS [training: 2.617716649018237 | validation: 2.629556173337855]
	TIME [epoch: 8.45 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.639917851089383		[learning rate: 0.00090464]
	Learning Rate: 0.000904636
	LOSS [training: 2.639917851089383 | validation: 2.6259334024298706]
	TIME [epoch: 8.45 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6025781725144888		[learning rate: 0.00090135]
	Learning Rate: 0.000901353
	LOSS [training: 2.6025781725144888 | validation: 2.5801725919127807]
	TIME [epoch: 8.44 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610781677278534		[learning rate: 0.00089808]
	Learning Rate: 0.000898082
	LOSS [training: 2.610781677278534 | validation: 2.579029247559078]
	TIME [epoch: 8.46 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.618863041778133		[learning rate: 0.00089482]
	Learning Rate: 0.000894822
	LOSS [training: 2.618863041778133 | validation: 2.5449392117691767]
	TIME [epoch: 8.45 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606998725831618		[learning rate: 0.00089157]
	Learning Rate: 0.000891575
	LOSS [training: 2.606998725831618 | validation: 2.561858536251479]
	TIME [epoch: 8.46 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.631087575380957		[learning rate: 0.00088834]
	Learning Rate: 0.000888339
	LOSS [training: 2.631087575380957 | validation: 2.5966425436263427]
	TIME [epoch: 8.44 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6428333009594605		[learning rate: 0.00088512]
	Learning Rate: 0.000885116
	LOSS [training: 2.6428333009594605 | validation: 2.579506371673233]
	TIME [epoch: 8.47 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6068928412693504		[learning rate: 0.0008819]
	Learning Rate: 0.000881903
	LOSS [training: 2.6068928412693504 | validation: 2.596393019342644]
	TIME [epoch: 8.46 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6130217038898818		[learning rate: 0.0008787]
	Learning Rate: 0.000878703
	LOSS [training: 2.6130217038898818 | validation: 2.5390858627109587]
	TIME [epoch: 8.44 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.622510591371636		[learning rate: 0.00087551]
	Learning Rate: 0.000875514
	LOSS [training: 2.622510591371636 | validation: 2.5793344583058273]
	TIME [epoch: 8.44 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.620654767831213		[learning rate: 0.00087234]
	Learning Rate: 0.000872337
	LOSS [training: 2.620654767831213 | validation: 2.5483239901008585]
	TIME [epoch: 8.52 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615719012333188		[learning rate: 0.00086917]
	Learning Rate: 0.000869171
	LOSS [training: 2.615719012333188 | validation: 2.546020043958774]
	TIME [epoch: 8.45 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.609206151155435		[learning rate: 0.00086602]
	Learning Rate: 0.000866017
	LOSS [training: 2.609206151155435 | validation: 2.5576043446047114]
	TIME [epoch: 8.44 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6047516353826436		[learning rate: 0.00086287]
	Learning Rate: 0.000862874
	LOSS [training: 2.6047516353826436 | validation: 2.5496414013322086]
	TIME [epoch: 8.49 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613093969823699		[learning rate: 0.00085974]
	Learning Rate: 0.000859743
	LOSS [training: 2.613093969823699 | validation: 2.5429536861624547]
	TIME [epoch: 8.47 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6239111263957744		[learning rate: 0.00085662]
	Learning Rate: 0.000856623
	LOSS [training: 2.6239111263957744 | validation: 2.544537935220495]
	TIME [epoch: 8.45 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.650907427277165		[learning rate: 0.00085351]
	Learning Rate: 0.000853514
	LOSS [training: 2.650907427277165 | validation: 2.5421977042089487]
	TIME [epoch: 8.46 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6136167051020833		[learning rate: 0.00085042]
	Learning Rate: 0.000850416
	LOSS [training: 2.6136167051020833 | validation: 2.5562937085588846]
	TIME [epoch: 8.49 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.621313110410592		[learning rate: 0.00084733]
	Learning Rate: 0.00084733
	LOSS [training: 2.621313110410592 | validation: 2.5650097828323712]
	TIME [epoch: 8.49 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6234364913886616		[learning rate: 0.00084426]
	Learning Rate: 0.000844255
	LOSS [training: 2.6234364913886616 | validation: 2.563070520738914]
	TIME [epoch: 8.45 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6061643465379745		[learning rate: 0.00084119]
	Learning Rate: 0.000841191
	LOSS [training: 2.6061643465379745 | validation: 2.53717735943425]
	TIME [epoch: 8.43 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6235153278884313		[learning rate: 0.00083814]
	Learning Rate: 0.000838139
	LOSS [training: 2.6235153278884313 | validation: 2.54255977453937]
	TIME [epoch: 8.46 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613824119753018		[learning rate: 0.0008351]
	Learning Rate: 0.000835097
	LOSS [training: 2.613824119753018 | validation: 2.5595793214885463]
	TIME [epoch: 8.46 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615948700276905		[learning rate: 0.00083207]
	Learning Rate: 0.000832066
	LOSS [training: 2.615948700276905 | validation: 2.537005230501711]
	TIME [epoch: 8.45 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6047469566903825		[learning rate: 0.00082905]
	Learning Rate: 0.000829046
	LOSS [training: 2.6047469566903825 | validation: 2.535547352998137]
	TIME [epoch: 8.44 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6054345747087435		[learning rate: 0.00082604]
	Learning Rate: 0.000826038
	LOSS [training: 2.6054345747087435 | validation: 2.5502104999915494]
	TIME [epoch: 8.47 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615748334855088		[learning rate: 0.00082304]
	Learning Rate: 0.00082304
	LOSS [training: 2.615748334855088 | validation: 2.5614848138029593]
	TIME [epoch: 8.5 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6294965377125767		[learning rate: 0.00082005]
	Learning Rate: 0.000820053
	LOSS [training: 2.6294965377125767 | validation: 2.5665468294474505]
	TIME [epoch: 8.46 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6094090005004533		[learning rate: 0.00081708]
	Learning Rate: 0.000817077
	LOSS [training: 2.6094090005004533 | validation: 2.5255600576925112]
	TIME [epoch: 8.44 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6120697888959192		[learning rate: 0.00081411]
	Learning Rate: 0.000814112
	LOSS [training: 2.6120697888959192 | validation: 2.538019775721529]
	TIME [epoch: 8.46 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6020337878573327		[learning rate: 0.00081116]
	Learning Rate: 0.000811158
	LOSS [training: 2.6020337878573327 | validation: 2.5616911665518742]
	TIME [epoch: 8.45 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604193553158479		[learning rate: 0.00080821]
	Learning Rate: 0.000808214
	LOSS [training: 2.604193553158479 | validation: 2.5418905042089373]
	TIME [epoch: 8.45 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60779475957208		[learning rate: 0.00080528]
	Learning Rate: 0.000805281
	LOSS [training: 2.60779475957208 | validation: 2.541269949991272]
	TIME [epoch: 8.44 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.649576403623872		[learning rate: 0.00080236]
	Learning Rate: 0.000802358
	LOSS [training: 2.649576403623872 | validation: 2.5490796697796148]
	TIME [epoch: 8.46 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.627962207721135		[learning rate: 0.00079945]
	Learning Rate: 0.000799447
	LOSS [training: 2.627962207721135 | validation: 2.5626332426720957]
	TIME [epoch: 8.45 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603313471461633		[learning rate: 0.00079655]
	Learning Rate: 0.000796545
	LOSS [training: 2.603313471461633 | validation: 2.5620493109049276]
	TIME [epoch: 8.45 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6325966338729296		[learning rate: 0.00079365]
	Learning Rate: 0.000793655
	LOSS [training: 2.6325966338729296 | validation: 2.5507421301357596]
	TIME [epoch: 8.44 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6091724263928375		[learning rate: 0.00079077]
	Learning Rate: 0.000790775
	LOSS [training: 2.6091724263928375 | validation: 2.5404078542692723]
	TIME [epoch: 8.47 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6071982565912997		[learning rate: 0.0007879]
	Learning Rate: 0.000787905
	LOSS [training: 2.6071982565912997 | validation: 2.536315950546773]
	TIME [epoch: 8.44 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607623499649385		[learning rate: 0.00078505]
	Learning Rate: 0.000785045
	LOSS [training: 2.607623499649385 | validation: 2.5384919592784474]
	TIME [epoch: 8.43 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6197215089425665		[learning rate: 0.0007822]
	Learning Rate: 0.000782196
	LOSS [training: 2.6197215089425665 | validation: 2.549602114795667]
	TIME [epoch: 8.45 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615487388730833		[learning rate: 0.00077936]
	Learning Rate: 0.000779358
	LOSS [training: 2.615487388730833 | validation: 2.545968835744284]
	TIME [epoch: 8.48 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.631791487423616		[learning rate: 0.00077653]
	Learning Rate: 0.000776529
	LOSS [training: 2.631791487423616 | validation: 2.5925081163098396]
	TIME [epoch: 8.47 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6234319314234136		[learning rate: 0.00077371]
	Learning Rate: 0.000773711
	LOSS [training: 2.6234319314234136 | validation: 2.537990065141477]
	TIME [epoch: 8.48 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607605409787676		[learning rate: 0.0007709]
	Learning Rate: 0.000770903
	LOSS [training: 2.607605409787676 | validation: 2.5388647598595804]
	TIME [epoch: 8.43 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607458840688233		[learning rate: 0.00076811]
	Learning Rate: 0.000768106
	LOSS [training: 2.607458840688233 | validation: 2.549867574070794]
	TIME [epoch: 8.47 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60125392777514		[learning rate: 0.00076532]
	Learning Rate: 0.000765318
	LOSS [training: 2.60125392777514 | validation: 2.571531990356259]
	TIME [epoch: 8.45 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6018799376647896		[learning rate: 0.00076254]
	Learning Rate: 0.000762541
	LOSS [training: 2.6018799376647896 | validation: 2.550275296570296]
	TIME [epoch: 8.46 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6351666590080107		[learning rate: 0.00075977]
	Learning Rate: 0.000759774
	LOSS [training: 2.6351666590080107 | validation: 2.555799119544168]
	TIME [epoch: 8.45 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6019635065891946		[learning rate: 0.00075702]
	Learning Rate: 0.000757016
	LOSS [training: 2.6019635065891946 | validation: 2.56688889785068]
	TIME [epoch: 8.48 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6007705338101026		[learning rate: 0.00075427]
	Learning Rate: 0.000754269
	LOSS [training: 2.6007705338101026 | validation: 2.5466371233483693]
	TIME [epoch: 8.45 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6155319105692145		[learning rate: 0.00075153]
	Learning Rate: 0.000751532
	LOSS [training: 2.6155319105692145 | validation: 2.577618348235449]
	TIME [epoch: 8.46 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.608866083854722		[learning rate: 0.0007488]
	Learning Rate: 0.000748804
	LOSS [training: 2.608866083854722 | validation: 2.5471850662450173]
	TIME [epoch: 8.45 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6094227411706448		[learning rate: 0.00074609]
	Learning Rate: 0.000746087
	LOSS [training: 2.6094227411706448 | validation: 2.5621047617566224]
	TIME [epoch: 8.48 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6291044940655324		[learning rate: 0.00074338]
	Learning Rate: 0.000743379
	LOSS [training: 2.6291044940655324 | validation: 2.5864786651109215]
	TIME [epoch: 8.46 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6027559828593603		[learning rate: 0.00074068]
	Learning Rate: 0.000740682
	LOSS [training: 2.6027559828593603 | validation: 2.562356200820938]
	TIME [epoch: 8.47 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6259062058623814		[learning rate: 0.00073799]
	Learning Rate: 0.000737994
	LOSS [training: 2.6259062058623814 | validation: 2.56030713969013]
	TIME [epoch: 8.46 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6241550553929085		[learning rate: 0.00073532]
	Learning Rate: 0.000735315
	LOSS [training: 2.6241550553929085 | validation: 2.5506965850412326]
	TIME [epoch: 8.49 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5990093521268456		[learning rate: 0.00073265]
	Learning Rate: 0.000732647
	LOSS [training: 2.5990093521268456 | validation: 2.5676529543620368]
	TIME [epoch: 8.45 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6122914537329462		[learning rate: 0.00072999]
	Learning Rate: 0.000729988
	LOSS [training: 2.6122914537329462 | validation: 2.568269857232366]
	TIME [epoch: 8.44 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.609357774573929		[learning rate: 0.00072734]
	Learning Rate: 0.000727339
	LOSS [training: 2.609357774573929 | validation: 2.5980259301634625]
	TIME [epoch: 8.47 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604475481670154		[learning rate: 0.0007247]
	Learning Rate: 0.000724699
	LOSS [training: 2.604475481670154 | validation: 2.538534083354886]
	TIME [epoch: 8.49 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6021537833546105		[learning rate: 0.00072207]
	Learning Rate: 0.000722069
	LOSS [training: 2.6021537833546105 | validation: 2.5467271587163474]
	TIME [epoch: 8.48 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603975165652115		[learning rate: 0.00071945]
	Learning Rate: 0.000719449
	LOSS [training: 2.603975165652115 | validation: 2.558240343375937]
	TIME [epoch: 8.45 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6027707545059453		[learning rate: 0.00071684]
	Learning Rate: 0.000716838
	LOSS [training: 2.6027707545059453 | validation: 2.54081406841728]
	TIME [epoch: 8.43 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613548911796866		[learning rate: 0.00071424]
	Learning Rate: 0.000714237
	LOSS [training: 2.613548911796866 | validation: 2.5486560875132196]
	TIME [epoch: 8.49 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598000868983118		[learning rate: 0.00071164]
	Learning Rate: 0.000711645
	LOSS [training: 2.598000868983118 | validation: 2.5767668624189897]
	TIME [epoch: 8.49 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6126017031224307		[learning rate: 0.00070906]
	Learning Rate: 0.000709062
	LOSS [training: 2.6126017031224307 | validation: 2.5411834947248817]
	TIME [epoch: 8.45 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6130752751383626		[learning rate: 0.00070649]
	Learning Rate: 0.000706489
	LOSS [training: 2.6130752751383626 | validation: 2.532057836541276]
	TIME [epoch: 8.44 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.612925612389517		[learning rate: 0.00070392]
	Learning Rate: 0.000703925
	LOSS [training: 2.612925612389517 | validation: 2.547116400066466]
	TIME [epoch: 8.47 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596670821180886		[learning rate: 0.00070137]
	Learning Rate: 0.00070137
	LOSS [training: 2.596670821180886 | validation: 2.564238429444636]
	TIME [epoch: 8.49 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6192375576165583		[learning rate: 0.00069883]
	Learning Rate: 0.000698825
	LOSS [training: 2.6192375576165583 | validation: 2.592496542971668]
	TIME [epoch: 8.47 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6040433647412327		[learning rate: 0.00069629]
	Learning Rate: 0.000696289
	LOSS [training: 2.6040433647412327 | validation: 2.5996087432687847]
	TIME [epoch: 8.46 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605280599643659		[learning rate: 0.00069376]
	Learning Rate: 0.000693762
	LOSS [training: 2.605280599643659 | validation: 2.5732978252274106]
	TIME [epoch: 8.46 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59800187444936		[learning rate: 0.00069124]
	Learning Rate: 0.000691244
	LOSS [training: 2.59800187444936 | validation: 2.5430295165338825]
	TIME [epoch: 8.45 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6093170693071386		[learning rate: 0.00068874]
	Learning Rate: 0.000688736
	LOSS [training: 2.6093170693071386 | validation: 2.569125377748223]
	TIME [epoch: 8.46 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6292828378278923		[learning rate: 0.00068624]
	Learning Rate: 0.000686236
	LOSS [training: 2.6292828378278923 | validation: 2.5639269037719137]
	TIME [epoch: 8.45 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6254846561040215		[learning rate: 0.00068375]
	Learning Rate: 0.000683746
	LOSS [training: 2.6254846561040215 | validation: 2.5425615083035944]
	TIME [epoch: 8.51 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6053043148215713		[learning rate: 0.00068126]
	Learning Rate: 0.000681265
	LOSS [training: 2.6053043148215713 | validation: 2.5663592375478563]
	TIME [epoch: 8.48 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601398768430597		[learning rate: 0.00067879]
	Learning Rate: 0.000678792
	LOSS [training: 2.601398768430597 | validation: 2.5441715012962214]
	TIME [epoch: 8.45 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.611201699446473		[learning rate: 0.00067633]
	Learning Rate: 0.000676329
	LOSS [training: 2.611201699446473 | validation: 2.531696109135524]
	TIME [epoch: 8.47 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.595651129748289		[learning rate: 0.00067387]
	Learning Rate: 0.000673874
	LOSS [training: 2.595651129748289 | validation: 2.538174430298599]
	TIME [epoch: 8.47 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6204804586783808		[learning rate: 0.00067143]
	Learning Rate: 0.000671429
	LOSS [training: 2.6204804586783808 | validation: 2.6348058243314547]
	TIME [epoch: 8.45 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.608995229728533		[learning rate: 0.00066899]
	Learning Rate: 0.000668992
	LOSS [training: 2.608995229728533 | validation: 2.560790506259888]
	TIME [epoch: 8.46 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6180586583571106		[learning rate: 0.00066656]
	Learning Rate: 0.000666564
	LOSS [training: 2.6180586583571106 | validation: 2.5520837379801335]
	TIME [epoch: 8.48 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602089843706046		[learning rate: 0.00066415]
	Learning Rate: 0.000664145
	LOSS [training: 2.602089843706046 | validation: 2.557195786004751]
	TIME [epoch: 8.46 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6183716566659925		[learning rate: 0.00066174]
	Learning Rate: 0.000661735
	LOSS [training: 2.6183716566659925 | validation: 2.6949814563266705]
	TIME [epoch: 8.46 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6439687307579134		[learning rate: 0.00065933]
	Learning Rate: 0.000659334
	LOSS [training: 2.6439687307579134 | validation: 2.54204831638614]
	TIME [epoch: 8.46 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6014610605023423		[learning rate: 0.00065694]
	Learning Rate: 0.000656941
	LOSS [training: 2.6014610605023423 | validation: 2.5424144657747956]
	TIME [epoch: 8.47 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606051369050036		[learning rate: 0.00065456]
	Learning Rate: 0.000654557
	LOSS [training: 2.606051369050036 | validation: 2.5552596330584665]
	TIME [epoch: 8.46 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6016474793328443		[learning rate: 0.00065218]
	Learning Rate: 0.000652182
	LOSS [training: 2.6016474793328443 | validation: 2.53436630734058]
	TIME [epoch: 8.46 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6077421080352527		[learning rate: 0.00064981]
	Learning Rate: 0.000649815
	LOSS [training: 2.6077421080352527 | validation: 2.5520476893865345]
	TIME [epoch: 8.45 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596142836271603		[learning rate: 0.00064746]
	Learning Rate: 0.000647456
	LOSS [training: 2.596142836271603 | validation: 2.5381316732301826]
	TIME [epoch: 8.48 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5960240728910913		[learning rate: 0.00064511]
	Learning Rate: 0.000645107
	LOSS [training: 2.5960240728910913 | validation: 2.5856764836045243]
	TIME [epoch: 8.46 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6153958162193947		[learning rate: 0.00064277]
	Learning Rate: 0.000642766
	LOSS [training: 2.6153958162193947 | validation: 2.5690971043598125]
	TIME [epoch: 8.52 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6217962298191346		[learning rate: 0.00064043]
	Learning Rate: 0.000640433
	LOSS [training: 2.6217962298191346 | validation: 2.5453518276430396]
	TIME [epoch: 8.45 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6014078641530483		[learning rate: 0.00063811]
	Learning Rate: 0.000638109
	LOSS [training: 2.6014078641530483 | validation: 2.555260787490321]
	TIME [epoch: 8.46 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603161951693719		[learning rate: 0.00063579]
	Learning Rate: 0.000635793
	LOSS [training: 2.603161951693719 | validation: 2.543909675398486]
	TIME [epoch: 8.46 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5957448724474985		[learning rate: 0.00063349]
	Learning Rate: 0.000633486
	LOSS [training: 2.5957448724474985 | validation: 2.5329677413013973]
	TIME [epoch: 8.47 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.614367940537151		[learning rate: 0.00063119]
	Learning Rate: 0.000631187
	LOSS [training: 2.614367940537151 | validation: 2.581830854631056]
	TIME [epoch: 8.46 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6100026612234144		[learning rate: 0.0006289]
	Learning Rate: 0.000628896
	LOSS [training: 2.6100026612234144 | validation: 2.5390989598060685]
	TIME [epoch: 8.48 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6107119909014456		[learning rate: 0.00062661]
	Learning Rate: 0.000626614
	LOSS [training: 2.6107119909014456 | validation: 2.559174972287605]
	TIME [epoch: 8.47 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6163732508731132		[learning rate: 0.00062434]
	Learning Rate: 0.00062434
	LOSS [training: 2.6163732508731132 | validation: 2.5751097878777776]
	TIME [epoch: 8.45 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5964912438774412		[learning rate: 0.00062207]
	Learning Rate: 0.000622074
	LOSS [training: 2.5964912438774412 | validation: 2.547507966211009]
	TIME [epoch: 8.45 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6106646723441473		[learning rate: 0.00061982]
	Learning Rate: 0.000619816
	LOSS [training: 2.6106646723441473 | validation: 2.530721971471161]
	TIME [epoch: 8.49 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601775632420668		[learning rate: 0.00061757]
	Learning Rate: 0.000617567
	LOSS [training: 2.601775632420668 | validation: 2.5492241974799272]
	TIME [epoch: 8.47 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.62445367113421		[learning rate: 0.00061533]
	Learning Rate: 0.000615326
	LOSS [training: 2.62445367113421 | validation: 2.5984130703736943]
	TIME [epoch: 8.45 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605256157718685		[learning rate: 0.00061309]
	Learning Rate: 0.000613093
	LOSS [training: 2.605256157718685 | validation: 2.5585567660881976]
	TIME [epoch: 8.47 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610806678781098		[learning rate: 0.00061087]
	Learning Rate: 0.000610868
	LOSS [training: 2.610806678781098 | validation: 2.569395210460855]
	TIME [epoch: 8.49 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601838013990841		[learning rate: 0.00060865]
	Learning Rate: 0.000608651
	LOSS [training: 2.601838013990841 | validation: 2.55438078082289]
	TIME [epoch: 8.46 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.595692576061973		[learning rate: 0.00060644]
	Learning Rate: 0.000606442
	LOSS [training: 2.595692576061973 | validation: 2.5378308301333687]
	TIME [epoch: 8.46 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6194443062426647		[learning rate: 0.00060424]
	Learning Rate: 0.000604242
	LOSS [training: 2.6194443062426647 | validation: 2.563456914405662]
	TIME [epoch: 8.45 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6006269706905245		[learning rate: 0.00060205]
	Learning Rate: 0.000602049
	LOSS [training: 2.6006269706905245 | validation: 2.5364314242245194]
	TIME [epoch: 8.48 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605893565146438		[learning rate: 0.00059986]
	Learning Rate: 0.000599864
	LOSS [training: 2.605893565146438 | validation: 2.5796273849450655]
	TIME [epoch: 8.45 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6025322502288573		[learning rate: 0.00059769]
	Learning Rate: 0.000597687
	LOSS [training: 2.6025322502288573 | validation: 2.601268903418171]
	TIME [epoch: 8.46 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610363934999893		[learning rate: 0.00059552]
	Learning Rate: 0.000595518
	LOSS [training: 2.610363934999893 | validation: 2.535893785777379]
	TIME [epoch: 8.46 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598138472174168		[learning rate: 0.00059336]
	Learning Rate: 0.000593357
	LOSS [training: 2.598138472174168 | validation: 2.540995729682767]
	TIME [epoch: 8.49 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6050059631658877		[learning rate: 0.0005912]
	Learning Rate: 0.000591203
	LOSS [training: 2.6050059631658877 | validation: 2.558717554676972]
	TIME [epoch: 8.5 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605707215180243		[learning rate: 0.00058906]
	Learning Rate: 0.000589058
	LOSS [training: 2.605707215180243 | validation: 2.536521143815609]
	TIME [epoch: 8.47 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598275097876799		[learning rate: 0.00058692]
	Learning Rate: 0.00058692
	LOSS [training: 2.598275097876799 | validation: 2.5424162157154817]
	TIME [epoch: 8.46 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6141184880817154		[learning rate: 0.00058479]
	Learning Rate: 0.00058479
	LOSS [training: 2.6141184880817154 | validation: 2.5505566955458914]
	TIME [epoch: 8.46 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6019219123066804		[learning rate: 0.00058267]
	Learning Rate: 0.000582668
	LOSS [training: 2.6019219123066804 | validation: 2.6497617610693034]
	TIME [epoch: 8.52 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6370722319784616		[learning rate: 0.00058055]
	Learning Rate: 0.000580553
	LOSS [training: 2.6370722319784616 | validation: 2.5790091390719647]
	TIME [epoch: 8.44 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597466437470439		[learning rate: 0.00057845]
	Learning Rate: 0.000578446
	LOSS [training: 2.597466437470439 | validation: 2.534239133274558]
	TIME [epoch: 8.46 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596913898905563		[learning rate: 0.00057635]
	Learning Rate: 0.000576347
	LOSS [training: 2.596913898905563 | validation: 2.5351558852768514]
	TIME [epoch: 8.46 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596050101798876		[learning rate: 0.00057426]
	Learning Rate: 0.000574256
	LOSS [training: 2.596050101798876 | validation: 2.533887721592456]
	TIME [epoch: 8.46 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.617311234533358		[learning rate: 0.00057217]
	Learning Rate: 0.000572172
	LOSS [training: 2.617311234533358 | validation: 2.5401967072795646]
	TIME [epoch: 8.45 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6109306043884697		[learning rate: 0.0005701]
	Learning Rate: 0.000570095
	LOSS [training: 2.6109306043884697 | validation: 2.5796352361343784]
	TIME [epoch: 8.54 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6104947106938523		[learning rate: 0.00056803]
	Learning Rate: 0.000568026
	LOSS [training: 2.6104947106938523 | validation: 2.5377057512598684]
	TIME [epoch: 8.45 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6203537411959283		[learning rate: 0.00056596]
	Learning Rate: 0.000565965
	LOSS [training: 2.6203537411959283 | validation: 2.541285501194907]
	TIME [epoch: 8.45 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6103556399491277		[learning rate: 0.00056391]
	Learning Rate: 0.000563911
	LOSS [training: 2.6103556399491277 | validation: 2.5458148041467443]
	TIME [epoch: 8.45 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6007416859204135		[learning rate: 0.00056186]
	Learning Rate: 0.000561864
	LOSS [training: 2.6007416859204135 | validation: 2.5323077492854993]
	TIME [epoch: 8.48 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.595613667004933		[learning rate: 0.00055983]
	Learning Rate: 0.000559825
	LOSS [training: 2.595613667004933 | validation: 2.5510963676601923]
	TIME [epoch: 8.45 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603558742503252		[learning rate: 0.00055779]
	Learning Rate: 0.000557794
	LOSS [training: 2.603558742503252 | validation: 2.5390331353658198]
	TIME [epoch: 8.46 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602312091192199		[learning rate: 0.00055577]
	Learning Rate: 0.00055577
	LOSS [training: 2.602312091192199 | validation: 2.559580182940647]
	TIME [epoch: 8.45 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6248668204017016		[learning rate: 0.00055375]
	Learning Rate: 0.000553753
	LOSS [training: 2.6248668204017016 | validation: 2.570477563576249]
	TIME [epoch: 8.49 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602787140685982		[learning rate: 0.00055174]
	Learning Rate: 0.000551743
	LOSS [training: 2.602787140685982 | validation: 2.5437792728621194]
	TIME [epoch: 8.49 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5985757353375503		[learning rate: 0.00054974]
	Learning Rate: 0.000549741
	LOSS [training: 2.5985757353375503 | validation: 2.545357514825278]
	TIME [epoch: 8.44 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6086754783493507		[learning rate: 0.00054775]
	Learning Rate: 0.000547746
	LOSS [training: 2.6086754783493507 | validation: 2.548875359784849]
	TIME [epoch: 8.43 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5986161472507363		[learning rate: 0.00054576]
	Learning Rate: 0.000545758
	LOSS [training: 2.5986161472507363 | validation: 2.5278885665813267]
	TIME [epoch: 8.48 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5947107527191338		[learning rate: 0.00054378]
	Learning Rate: 0.000543777
	LOSS [training: 2.5947107527191338 | validation: 2.538063535958004]
	TIME [epoch: 8.46 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598904266159143		[learning rate: 0.0005418]
	Learning Rate: 0.000541804
	LOSS [training: 2.598904266159143 | validation: 2.5533871702719972]
	TIME [epoch: 8.45 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6133923917214483		[learning rate: 0.00053984]
	Learning Rate: 0.000539838
	LOSS [training: 2.6133923917214483 | validation: 2.5297933355720454]
	TIME [epoch: 8.45 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6076460185873382		[learning rate: 0.00053788]
	Learning Rate: 0.000537879
	LOSS [training: 2.6076460185873382 | validation: 2.5451732286282525]
	TIME [epoch: 8.47 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.608332157492907		[learning rate: 0.00053593]
	Learning Rate: 0.000535926
	LOSS [training: 2.608332157492907 | validation: 2.5683821819871184]
	TIME [epoch: 8.45 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6031341945400013		[learning rate: 0.00053398]
	Learning Rate: 0.000533982
	LOSS [training: 2.6031341945400013 | validation: 2.64087275364986]
	TIME [epoch: 8.45 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.628511095830597		[learning rate: 0.00053204]
	Learning Rate: 0.000532044
	LOSS [training: 2.628511095830597 | validation: 2.534431879884445]
	TIME [epoch: 8.44 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5995599491023915		[learning rate: 0.00053011]
	Learning Rate: 0.000530113
	LOSS [training: 2.5995599491023915 | validation: 2.5338833037349415]
	TIME [epoch: 8.46 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5917225648995643		[learning rate: 0.00052819]
	Learning Rate: 0.000528189
	LOSS [training: 2.5917225648995643 | validation: 2.5284265524549756]
	TIME [epoch: 8.45 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5946961691950143		[learning rate: 0.00052627]
	Learning Rate: 0.000526272
	LOSS [training: 2.5946961691950143 | validation: 2.538331074414743]
	TIME [epoch: 8.48 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6040154733023853		[learning rate: 0.00052436]
	Learning Rate: 0.000524362
	LOSS [training: 2.6040154733023853 | validation: 2.5285411285887687]
	TIME [epoch: 8.47 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601197933712281		[learning rate: 0.00052246]
	Learning Rate: 0.00052246
	LOSS [training: 2.601197933712281 | validation: 2.561307212122787]
	TIME [epoch: 8.46 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615839812612995		[learning rate: 0.00052056]
	Learning Rate: 0.000520563
	LOSS [training: 2.615839812612995 | validation: 2.569647438498042]
	TIME [epoch: 8.44 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601499996152671		[learning rate: 0.00051867]
	Learning Rate: 0.000518674
	LOSS [training: 2.601499996152671 | validation: 2.583051492448642]
	TIME [epoch: 8.45 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6221825028427874		[learning rate: 0.00051679]
	Learning Rate: 0.000516792
	LOSS [training: 2.6221825028427874 | validation: 2.5524726695616624]
	TIME [epoch: 8.44 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598829417210777		[learning rate: 0.00051492]
	Learning Rate: 0.000514917
	LOSS [training: 2.598829417210777 | validation: 2.533770021890888]
	TIME [epoch: 8.46 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6054769394525668		[learning rate: 0.00051305]
	Learning Rate: 0.000513048
	LOSS [training: 2.6054769394525668 | validation: 2.612817017311327]
	TIME [epoch: 8.44 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6200090177751667		[learning rate: 0.00051119]
	Learning Rate: 0.000511186
	LOSS [training: 2.6200090177751667 | validation: 2.529660371915633]
	TIME [epoch: 8.44 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598791912144407		[learning rate: 0.00050933]
	Learning Rate: 0.000509331
	LOSS [training: 2.598791912144407 | validation: 2.5729791247344656]
	TIME [epoch: 8.45 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596677974987441		[learning rate: 0.00050748]
	Learning Rate: 0.000507483
	LOSS [training: 2.596677974987441 | validation: 2.5511254276987696]
	TIME [epoch: 8.46 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6027483371535216		[learning rate: 0.00050564]
	Learning Rate: 0.000505641
	LOSS [training: 2.6027483371535216 | validation: 2.554587892132297]
	TIME [epoch: 8.44 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6009152237588165		[learning rate: 0.00050381]
	Learning Rate: 0.000503806
	LOSS [training: 2.6009152237588165 | validation: 2.534734933511663]
	TIME [epoch: 8.43 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.620655314040513		[learning rate: 0.00050198]
	Learning Rate: 0.000501977
	LOSS [training: 2.620655314040513 | validation: 2.5309274045697676]
	TIME [epoch: 8.45 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603334401930985		[learning rate: 0.00050016]
	Learning Rate: 0.000500156
	LOSS [training: 2.603334401930985 | validation: 2.5183683517926134]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_924.pth
	Model improved!!!
EPOCH 925/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5996952737432126		[learning rate: 0.00049834]
	Learning Rate: 0.000498341
	LOSS [training: 2.5996952737432126 | validation: 2.5345461739117874]
	TIME [epoch: 8.43 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605393117974144		[learning rate: 0.00049653]
	Learning Rate: 0.000496532
	LOSS [training: 2.605393117974144 | validation: 2.554285980808469]
	TIME [epoch: 8.44 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5956292257039175		[learning rate: 0.00049473]
	Learning Rate: 0.00049473
	LOSS [training: 2.5956292257039175 | validation: 2.548097045048163]
	TIME [epoch: 8.44 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5995849181237576		[learning rate: 0.00049293]
	Learning Rate: 0.000492935
	LOSS [training: 2.5995849181237576 | validation: 2.534882235752895]
	TIME [epoch: 8.44 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5976952109480362		[learning rate: 0.00049115]
	Learning Rate: 0.000491146
	LOSS [training: 2.5976952109480362 | validation: 2.562537437467707]
	TIME [epoch: 8.44 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610634124887668		[learning rate: 0.00048936]
	Learning Rate: 0.000489364
	LOSS [training: 2.610634124887668 | validation: 2.5331149639947723]
	TIME [epoch: 8.44 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601942004065653		[learning rate: 0.00048759]
	Learning Rate: 0.000487588
	LOSS [training: 2.601942004065653 | validation: 2.5763776895321557]
	TIME [epoch: 8.43 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.616754526995938		[learning rate: 0.00048582]
	Learning Rate: 0.000485818
	LOSS [training: 2.616754526995938 | validation: 2.593700237072601]
	TIME [epoch: 8.45 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615144379547047		[learning rate: 0.00048406]
	Learning Rate: 0.000484055
	LOSS [training: 2.615144379547047 | validation: 2.558530655198208]
	TIME [epoch: 8.47 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.595526575748525		[learning rate: 0.0004823]
	Learning Rate: 0.000482298
	LOSS [training: 2.595526575748525 | validation: 2.5429689413608036]
	TIME [epoch: 8.45 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.594637557718103		[learning rate: 0.00048055]
	Learning Rate: 0.000480548
	LOSS [training: 2.594637557718103 | validation: 2.537309557053659]
	TIME [epoch: 8.43 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6015023884459545		[learning rate: 0.0004788]
	Learning Rate: 0.000478804
	LOSS [training: 2.6015023884459545 | validation: 2.5418721878423627]
	TIME [epoch: 8.46 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597145989399093		[learning rate: 0.00047707]
	Learning Rate: 0.000477067
	LOSS [training: 2.597145989399093 | validation: 2.5317007942458707]
	TIME [epoch: 8.49 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588458799230291		[learning rate: 0.00047534]
	Learning Rate: 0.000475335
	LOSS [training: 2.588458799230291 | validation: 2.5355668309095476]
	TIME [epoch: 8.43 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.595784243636529		[learning rate: 0.00047361]
	Learning Rate: 0.00047361
	LOSS [training: 2.595784243636529 | validation: 2.572856238715139]
	TIME [epoch: 8.43 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5916442935616746		[learning rate: 0.00047189]
	Learning Rate: 0.000471891
	LOSS [training: 2.5916442935616746 | validation: 2.5256874220858663]
	TIME [epoch: 8.46 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5908942907869212		[learning rate: 0.00047018]
	Learning Rate: 0.000470179
	LOSS [training: 2.5908942907869212 | validation: 2.5524784406002388]
	TIME [epoch: 8.44 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603495426824144		[learning rate: 0.00046847]
	Learning Rate: 0.000468473
	LOSS [training: 2.603495426824144 | validation: 2.529934656734232]
	TIME [epoch: 8.44 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600558871708968		[learning rate: 0.00046677]
	Learning Rate: 0.000466773
	LOSS [training: 2.600558871708968 | validation: 2.5330614102420634]
	TIME [epoch: 8.51 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5942437739934396		[learning rate: 0.00046508]
	Learning Rate: 0.000465079
	LOSS [training: 2.5942437739934396 | validation: 2.537257748542662]
	TIME [epoch: 8.44 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6071040827286227		[learning rate: 0.00046339]
	Learning Rate: 0.000463391
	LOSS [training: 2.6071040827286227 | validation: 2.542884915497985]
	TIME [epoch: 8.43 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6147430995506444		[learning rate: 0.00046171]
	Learning Rate: 0.000461709
	LOSS [training: 2.6147430995506444 | validation: 2.53646844645732]
	TIME [epoch: 8.43 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5985600095693973		[learning rate: 0.00046003]
	Learning Rate: 0.000460034
	LOSS [training: 2.5985600095693973 | validation: 2.540649696952151]
	TIME [epoch: 8.47 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597323842829627		[learning rate: 0.00045836]
	Learning Rate: 0.000458364
	LOSS [training: 2.597323842829627 | validation: 2.575158446992994]
	TIME [epoch: 8.44 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604703022154111		[learning rate: 0.0004567]
	Learning Rate: 0.000456701
	LOSS [training: 2.604703022154111 | validation: 2.541671193545897]
	TIME [epoch: 8.44 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602571294754669		[learning rate: 0.00045504]
	Learning Rate: 0.000455043
	LOSS [training: 2.602571294754669 | validation: 2.58727434241924]
	TIME [epoch: 8.49 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605273216229402		[learning rate: 0.00045339]
	Learning Rate: 0.000453392
	LOSS [training: 2.605273216229402 | validation: 2.5322481491807523]
	TIME [epoch: 8.44 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603237622402298		[learning rate: 0.00045175]
	Learning Rate: 0.000451746
	LOSS [training: 2.603237622402298 | validation: 2.5619467776854603]
	TIME [epoch: 8.43 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6020697764974945		[learning rate: 0.00045011]
	Learning Rate: 0.000450107
	LOSS [training: 2.6020697764974945 | validation: 2.5414734971131736]
	TIME [epoch: 8.44 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5999240962623245		[learning rate: 0.00044847]
	Learning Rate: 0.000448474
	LOSS [training: 2.5999240962623245 | validation: 2.534721089456184]
	TIME [epoch: 8.44 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6082417301831406		[learning rate: 0.00044685]
	Learning Rate: 0.000446846
	LOSS [training: 2.6082417301831406 | validation: 2.559362569618769]
	TIME [epoch: 8.42 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598504650207222		[learning rate: 0.00044522]
	Learning Rate: 0.000445224
	LOSS [training: 2.598504650207222 | validation: 2.535491693249302]
	TIME [epoch: 8.47 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6169056324225246		[learning rate: 0.00044361]
	Learning Rate: 0.000443609
	LOSS [training: 2.6169056324225246 | validation: 2.5691414883483104]
	TIME [epoch: 8.43 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602407685710097		[learning rate: 0.000442]
	Learning Rate: 0.000441999
	LOSS [training: 2.602407685710097 | validation: 2.552745776754519]
	TIME [epoch: 8.43 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602245546847943		[learning rate: 0.00044039]
	Learning Rate: 0.000440395
	LOSS [training: 2.602245546847943 | validation: 2.5685539875380377]
	TIME [epoch: 8.44 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598615340210408		[learning rate: 0.0004388]
	Learning Rate: 0.000438797
	LOSS [training: 2.598615340210408 | validation: 2.532940496239523]
	TIME [epoch: 8.44 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600431989557639		[learning rate: 0.0004372]
	Learning Rate: 0.000437204
	LOSS [training: 2.600431989557639 | validation: 2.5504334640415873]
	TIME [epoch: 8.44 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597535388502821		[learning rate: 0.00043562]
	Learning Rate: 0.000435617
	LOSS [training: 2.597535388502821 | validation: 2.5505780043917383]
	TIME [epoch: 8.42 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5985947551462694		[learning rate: 0.00043404]
	Learning Rate: 0.000434037
	LOSS [training: 2.5985947551462694 | validation: 2.5972322401823886]
	TIME [epoch: 8.49 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610160879089872		[learning rate: 0.00043246]
	Learning Rate: 0.000432461
	LOSS [training: 2.610160879089872 | validation: 2.558590719465485]
	TIME [epoch: 8.44 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5937761555211005		[learning rate: 0.00043089]
	Learning Rate: 0.000430892
	LOSS [training: 2.5937761555211005 | validation: 2.558524281997493]
	TIME [epoch: 8.43 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602614672938006		[learning rate: 0.00042933]
	Learning Rate: 0.000429328
	LOSS [training: 2.602614672938006 | validation: 2.555755247031767]
	TIME [epoch: 8.42 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5923908227470145		[learning rate: 0.00042777]
	Learning Rate: 0.00042777
	LOSS [training: 2.5923908227470145 | validation: 2.55732039646077]
	TIME [epoch: 8.45 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5999341499560726		[learning rate: 0.00042622]
	Learning Rate: 0.000426218
	LOSS [training: 2.5999341499560726 | validation: 2.542524767420824]
	TIME [epoch: 8.43 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.592228239066421		[learning rate: 0.00042467]
	Learning Rate: 0.000424671
	LOSS [training: 2.592228239066421 | validation: 2.547761897734893]
	TIME [epoch: 8.43 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597437517961027		[learning rate: 0.00042313]
	Learning Rate: 0.00042313
	LOSS [training: 2.597437517961027 | validation: 2.5412103731807925]
	TIME [epoch: 8.43 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5959256741353243		[learning rate: 0.00042159]
	Learning Rate: 0.000421594
	LOSS [training: 2.5959256741353243 | validation: 2.5290953771662568]
	TIME [epoch: 8.45 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596343317841115		[learning rate: 0.00042006]
	Learning Rate: 0.000420064
	LOSS [training: 2.596343317841115 | validation: 2.5775455838792434]
	TIME [epoch: 8.43 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600260580027158		[learning rate: 0.00041854]
	Learning Rate: 0.00041854
	LOSS [training: 2.600260580027158 | validation: 2.5274202122733724]
	TIME [epoch: 8.44 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587252734357621		[learning rate: 0.00041702]
	Learning Rate: 0.000417021
	LOSS [training: 2.587252734357621 | validation: 2.5360421862165605]
	TIME [epoch: 8.43 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6193261788905455		[learning rate: 0.00041551]
	Learning Rate: 0.000415508
	LOSS [training: 2.6193261788905455 | validation: 2.5489672202914155]
	TIME [epoch: 8.44 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6012365324036244		[learning rate: 0.000414]
	Learning Rate: 0.000414
	LOSS [training: 2.6012365324036244 | validation: 2.5243011721021364]
	TIME [epoch: 8.43 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603042798332381		[learning rate: 0.0004125]
	Learning Rate: 0.000412497
	LOSS [training: 2.603042798332381 | validation: 2.521584875625278]
	TIME [epoch: 8.42 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5993926755222065		[learning rate: 0.000411]
	Learning Rate: 0.000411
	LOSS [training: 2.5993926755222065 | validation: 2.529081835813787]
	TIME [epoch: 8.44 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602677808012654		[learning rate: 0.00040951]
	Learning Rate: 0.000409509
	LOSS [training: 2.602677808012654 | validation: 2.5378483699113463]
	TIME [epoch: 8.44 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.590452554831905		[learning rate: 0.00040802]
	Learning Rate: 0.000408023
	LOSS [training: 2.590452554831905 | validation: 2.5226744353708472]
	TIME [epoch: 8.47 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6023118498394635		[learning rate: 0.00040654]
	Learning Rate: 0.000406542
	LOSS [training: 2.6023118498394635 | validation: 2.538360389563369]
	TIME [epoch: 8.44 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597458477703511		[learning rate: 0.00040507]
	Learning Rate: 0.000405067
	LOSS [training: 2.597458477703511 | validation: 2.5281487631229753]
	TIME [epoch: 8.42 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5978155458750374		[learning rate: 0.0004036]
	Learning Rate: 0.000403597
	LOSS [training: 2.5978155458750374 | validation: 2.548033904458417]
	TIME [epoch: 8.45 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603274170326646		[learning rate: 0.00040213]
	Learning Rate: 0.000402132
	LOSS [training: 2.603274170326646 | validation: 2.5549222630893222]
	TIME [epoch: 8.48 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587964103319922		[learning rate: 0.00040067]
	Learning Rate: 0.000400672
	LOSS [training: 2.587964103319922 | validation: 2.528585042856039]
	TIME [epoch: 8.43 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.592479204035409		[learning rate: 0.00039922]
	Learning Rate: 0.000399218
	LOSS [training: 2.592479204035409 | validation: 2.565437735684842]
	TIME [epoch: 8.43 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6007108596787156		[learning rate: 0.00039777]
	Learning Rate: 0.00039777
	LOSS [training: 2.6007108596787156 | validation: 2.5514283283520336]
	TIME [epoch: 8.44 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599501326991392		[learning rate: 0.00039633]
	Learning Rate: 0.000396326
	LOSS [training: 2.599501326991392 | validation: 2.5386102409927993]
	TIME [epoch: 8.43 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5982894374332797		[learning rate: 0.00039489]
	Learning Rate: 0.000394888
	LOSS [training: 2.5982894374332797 | validation: 2.530956270999413]
	TIME [epoch: 8.42 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5912555533286783		[learning rate: 0.00039345]
	Learning Rate: 0.000393455
	LOSS [training: 2.5912555533286783 | validation: 2.531675154117365]
	TIME [epoch: 8.45 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6003479504702414		[learning rate: 0.00039203]
	Learning Rate: 0.000392027
	LOSS [training: 2.6003479504702414 | validation: 2.5526022468185623]
	TIME [epoch: 8.47 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6001007504202702		[learning rate: 0.0003906]
	Learning Rate: 0.000390604
	LOSS [training: 2.6001007504202702 | validation: 2.5377307329404823]
	TIME [epoch: 8.43 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6050131446861284		[learning rate: 0.00038919]
	Learning Rate: 0.000389187
	LOSS [training: 2.6050131446861284 | validation: 2.5366772020761204]
	TIME [epoch: 8.43 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.595748366634415		[learning rate: 0.00038777]
	Learning Rate: 0.000387774
	LOSS [training: 2.595748366634415 | validation: 2.550536503405327]
	TIME [epoch: 8.44 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6111236537781237		[learning rate: 0.00038637]
	Learning Rate: 0.000386367
	LOSS [training: 2.6111236537781237 | validation: 2.533443794284046]
	TIME [epoch: 8.44 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5897624800820065		[learning rate: 0.00038496]
	Learning Rate: 0.000384965
	LOSS [training: 2.5897624800820065 | validation: 2.52704923902508]
	TIME [epoch: 8.47 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5886078231144345		[learning rate: 0.00038357]
	Learning Rate: 0.000383568
	LOSS [training: 2.5886078231144345 | validation: 2.5359307271360647]
	TIME [epoch: 8.43 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5969801323167983		[learning rate: 0.00038218]
	Learning Rate: 0.000382176
	LOSS [training: 2.5969801323167983 | validation: 2.5678107630184774]
	TIME [epoch: 8.43 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6105826900322358		[learning rate: 0.00038079]
	Learning Rate: 0.000380789
	LOSS [training: 2.6105826900322358 | validation: 2.576872398988836]
	TIME [epoch: 8.45 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5943108231394367		[learning rate: 0.00037941]
	Learning Rate: 0.000379407
	LOSS [training: 2.5943108231394367 | validation: 2.5282493969679214]
	TIME [epoch: 8.45 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606203395022429		[learning rate: 0.00037803]
	Learning Rate: 0.00037803
	LOSS [training: 2.606203395022429 | validation: 2.566133823209521]
	TIME [epoch: 8.43 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6009197480559023		[learning rate: 0.00037666]
	Learning Rate: 0.000376658
	LOSS [training: 2.6009197480559023 | validation: 2.5383115790267587]
	TIME [epoch: 8.44 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5954499229108814		[learning rate: 0.00037529]
	Learning Rate: 0.000375291
	LOSS [training: 2.5954499229108814 | validation: 2.5302829973914687]
	TIME [epoch: 8.45 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6142023382664443		[learning rate: 0.00037393]
	Learning Rate: 0.000373929
	LOSS [training: 2.6142023382664443 | validation: 2.5407138445711803]
	TIME [epoch: 8.43 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597794036614923		[learning rate: 0.00037257]
	Learning Rate: 0.000372572
	LOSS [training: 2.597794036614923 | validation: 2.5339497019873716]
	TIME [epoch: 8.43 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5901385572954547		[learning rate: 0.00037122]
	Learning Rate: 0.00037122
	LOSS [training: 2.5901385572954547 | validation: 2.5434595511787097]
	TIME [epoch: 8.43 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5968423923615465		[learning rate: 0.00036987]
	Learning Rate: 0.000369873
	LOSS [training: 2.5968423923615465 | validation: 2.5465724653706743]
	TIME [epoch: 8.45 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6067791995307905		[learning rate: 0.00036853]
	Learning Rate: 0.000368531
	LOSS [training: 2.6067791995307905 | validation: 2.535043013272512]
	TIME [epoch: 8.44 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5881836364174826		[learning rate: 0.00036719]
	Learning Rate: 0.000367193
	LOSS [training: 2.5881836364174826 | validation: 2.5415525488389825]
	TIME [epoch: 8.42 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.591043164281016		[learning rate: 0.00036586]
	Learning Rate: 0.000365861
	LOSS [training: 2.591043164281016 | validation: 2.530450615914199]
	TIME [epoch: 8.47 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5947937412197457		[learning rate: 0.00036453]
	Learning Rate: 0.000364533
	LOSS [training: 2.5947937412197457 | validation: 2.53549779468229]
	TIME [epoch: 8.45 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58880736551167		[learning rate: 0.00036321]
	Learning Rate: 0.00036321
	LOSS [training: 2.58880736551167 | validation: 2.5319046534506398]
	TIME [epoch: 8.43 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5899726202346343		[learning rate: 0.00036189]
	Learning Rate: 0.000361892
	LOSS [training: 2.5899726202346343 | validation: 2.5408981996071214]
	TIME [epoch: 8.43 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59393204517027		[learning rate: 0.00036058]
	Learning Rate: 0.000360579
	LOSS [training: 2.59393204517027 | validation: 2.5224354507715523]
	TIME [epoch: 8.44 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600510754401063		[learning rate: 0.00035927]
	Learning Rate: 0.00035927
	LOSS [training: 2.600510754401063 | validation: 2.544484692814841]
	TIME [epoch: 8.44 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.593132481894321		[learning rate: 0.00035797]
	Learning Rate: 0.000357966
	LOSS [training: 2.593132481894321 | validation: 2.5329193587919274]
	TIME [epoch: 8.43 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.590591968349532		[learning rate: 0.00035667]
	Learning Rate: 0.000356667
	LOSS [training: 2.590591968349532 | validation: 2.5382234512442468]
	TIME [epoch: 8.43 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5958953292994753		[learning rate: 0.00035537]
	Learning Rate: 0.000355373
	LOSS [training: 2.5958953292994753 | validation: 2.5343809422457473]
	TIME [epoch: 8.45 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605659262822695		[learning rate: 0.00035408]
	Learning Rate: 0.000354083
	LOSS [training: 2.605659262822695 | validation: 2.53121960371217]
	TIME [epoch: 8.44 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5973353225893723		[learning rate: 0.0003528]
	Learning Rate: 0.000352798
	LOSS [training: 2.5973353225893723 | validation: 2.546210600971226]
	TIME [epoch: 8.43 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6003593220582495		[learning rate: 0.00035152]
	Learning Rate: 0.000351518
	LOSS [training: 2.6003593220582495 | validation: 2.544094371884003]
	TIME [epoch: 8.43 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598401697905311		[learning rate: 0.00035024]
	Learning Rate: 0.000350242
	LOSS [training: 2.598401697905311 | validation: 2.5362655355399335]
	TIME [epoch: 8.45 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583617340522261		[learning rate: 0.00034897]
	Learning Rate: 0.000348971
	LOSS [training: 2.583617340522261 | validation: 2.522832708427902]
	TIME [epoch: 8.43 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5977400763032814		[learning rate: 0.0003477]
	Learning Rate: 0.000347705
	LOSS [training: 2.5977400763032814 | validation: 2.590105656552125]
	TIME [epoch: 8.43 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6087662388662816		[learning rate: 0.00034644]
	Learning Rate: 0.000346443
	LOSS [training: 2.6087662388662816 | validation: 2.5244819500146756]
	TIME [epoch: 8.43 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5927010117618856		[learning rate: 0.00034519]
	Learning Rate: 0.000345186
	LOSS [training: 2.5927010117618856 | validation: 2.5253944471835803]
	TIME [epoch: 8.45 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5898044828682716		[learning rate: 0.00034393]
	Learning Rate: 0.000343933
	LOSS [training: 2.5898044828682716 | validation: 2.5363945108872867]
	TIME [epoch: 8.43 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603662883760311		[learning rate: 0.00034268]
	Learning Rate: 0.000342685
	LOSS [training: 2.603662883760311 | validation: 2.535562085796017]
	TIME [epoch: 8.45 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.594182140969218		[learning rate: 0.00034144]
	Learning Rate: 0.000341441
	LOSS [training: 2.594182140969218 | validation: 2.525584550837311]
	TIME [epoch: 8.47 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.592275373700544		[learning rate: 0.0003402]
	Learning Rate: 0.000340202
	LOSS [training: 2.592275373700544 | validation: 2.534131721239202]
	TIME [epoch: 8.44 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5981469382093367		[learning rate: 0.00033897]
	Learning Rate: 0.000338967
	LOSS [training: 2.5981469382093367 | validation: 2.574720854713002]
	TIME [epoch: 8.44 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5970478775869745		[learning rate: 0.00033774]
	Learning Rate: 0.000337737
	LOSS [training: 2.5970478775869745 | validation: 2.5461225980352227]
	TIME [epoch: 8.45 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5877440349480354		[learning rate: 0.00033651]
	Learning Rate: 0.000336512
	LOSS [training: 2.5877440349480354 | validation: 2.5299699122247983]
	TIME [epoch: 8.46 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5900866101756828		[learning rate: 0.00033529]
	Learning Rate: 0.00033529
	LOSS [training: 2.5900866101756828 | validation: 2.5313506293032626]
	TIME [epoch: 8.45 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59348038721206		[learning rate: 0.00033407]
	Learning Rate: 0.000334074
	LOSS [training: 2.59348038721206 | validation: 2.5921892486263616]
	TIME [epoch: 8.44 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6028969547082488		[learning rate: 0.00033286]
	Learning Rate: 0.000332861
	LOSS [training: 2.6028969547082488 | validation: 2.5388531925004023]
	TIME [epoch: 8.43 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5980818320280052		[learning rate: 0.00033165]
	Learning Rate: 0.000331653
	LOSS [training: 2.5980818320280052 | validation: 2.536724884365786]
	TIME [epoch: 8.44 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.593502700383893		[learning rate: 0.00033045]
	Learning Rate: 0.00033045
	LOSS [training: 2.593502700383893 | validation: 2.5304137886869]
	TIME [epoch: 8.49 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5911416046511944		[learning rate: 0.00032925]
	Learning Rate: 0.00032925
	LOSS [training: 2.5911416046511944 | validation: 2.5348842674445673]
	TIME [epoch: 8.44 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589175948479294		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 2.589175948479294 | validation: 2.530544162904558]
	TIME [epoch: 8.43 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5953313793086243		[learning rate: 0.00032686]
	Learning Rate: 0.000326865
	LOSS [training: 2.5953313793086243 | validation: 2.560780606668356]
	TIME [epoch: 8.44 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598330127501291		[learning rate: 0.00032568]
	Learning Rate: 0.000325679
	LOSS [training: 2.598330127501291 | validation: 2.552706986639307]
	TIME [epoch: 8.45 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5994416295441494		[learning rate: 0.0003245]
	Learning Rate: 0.000324497
	LOSS [training: 2.5994416295441494 | validation: 2.534986496217883]
	TIME [epoch: 8.48 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5936798336215157		[learning rate: 0.00032332]
	Learning Rate: 0.000323319
	LOSS [training: 2.5936798336215157 | validation: 2.531171636735678]
	TIME [epoch: 8.44 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6001152848450513		[learning rate: 0.00032215]
	Learning Rate: 0.000322146
	LOSS [training: 2.6001152848450513 | validation: 2.553455124091264]
	TIME [epoch: 8.42 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.595503226227777		[learning rate: 0.00032098]
	Learning Rate: 0.000320977
	LOSS [training: 2.595503226227777 | validation: 2.528125194659593]
	TIME [epoch: 8.46 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.594444022494618		[learning rate: 0.00031981]
	Learning Rate: 0.000319812
	LOSS [training: 2.594444022494618 | validation: 2.5417644650526823]
	TIME [epoch: 8.43 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6047812690161725		[learning rate: 0.00031865]
	Learning Rate: 0.000318651
	LOSS [training: 2.6047812690161725 | validation: 2.5229492214766283]
	TIME [epoch: 8.44 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6004278934387868		[learning rate: 0.0003175]
	Learning Rate: 0.000317495
	LOSS [training: 2.6004278934387868 | validation: 2.541080233076441]
	TIME [epoch: 8.44 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6042895306335394		[learning rate: 0.00031634]
	Learning Rate: 0.000316343
	LOSS [training: 2.6042895306335394 | validation: 2.545510591196617]
	TIME [epoch: 8.45 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601520563913519		[learning rate: 0.00031519]
	Learning Rate: 0.000315195
	LOSS [training: 2.601520563913519 | validation: 2.530733564370147]
	TIME [epoch: 8.44 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597449831793401		[learning rate: 0.00031405]
	Learning Rate: 0.000314051
	LOSS [training: 2.597449831793401 | validation: 2.5297392327432853]
	TIME [epoch: 8.43 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589755000761762		[learning rate: 0.00031291]
	Learning Rate: 0.000312911
	LOSS [training: 2.589755000761762 | validation: 2.5424888796311516]
	TIME [epoch: 8.44 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587979386704893		[learning rate: 0.00031178]
	Learning Rate: 0.000311776
	LOSS [training: 2.587979386704893 | validation: 2.552051264776283]
	TIME [epoch: 8.46 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5968745365610264		[learning rate: 0.00031064]
	Learning Rate: 0.000310644
	LOSS [training: 2.5968745365610264 | validation: 2.5192862370823437]
	TIME [epoch: 8.44 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5860931912121403		[learning rate: 0.00030952]
	Learning Rate: 0.000309517
	LOSS [training: 2.5860931912121403 | validation: 2.531066184896555]
	TIME [epoch: 8.44 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5947122296479557		[learning rate: 0.00030839]
	Learning Rate: 0.000308394
	LOSS [training: 2.5947122296479557 | validation: 2.5290040978959922]
	TIME [epoch: 8.46 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5961194801798624		[learning rate: 0.00030727]
	Learning Rate: 0.000307274
	LOSS [training: 2.5961194801798624 | validation: 2.5311990547961374]
	TIME [epoch: 8.48 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5895481957475206		[learning rate: 0.00030616]
	Learning Rate: 0.000306159
	LOSS [training: 2.5895481957475206 | validation: 2.5424149190072765]
	TIME [epoch: 8.43 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597590034434602		[learning rate: 0.00030505]
	Learning Rate: 0.000305048
	LOSS [training: 2.597590034434602 | validation: 2.5303734886324047]
	TIME [epoch: 8.43 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5844602875558556		[learning rate: 0.00030394]
	Learning Rate: 0.000303941
	LOSS [training: 2.5844602875558556 | validation: 2.5419777402669474]
	TIME [epoch: 8.44 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.591955136894235		[learning rate: 0.00030284]
	Learning Rate: 0.000302838
	LOSS [training: 2.591955136894235 | validation: 2.5305477087525663]
	TIME [epoch: 8.45 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589810800581737		[learning rate: 0.00030174]
	Learning Rate: 0.000301739
	LOSS [training: 2.589810800581737 | validation: 2.5290510189736297]
	TIME [epoch: 8.44 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5969568498238864		[learning rate: 0.00030064]
	Learning Rate: 0.000300644
	LOSS [training: 2.5969568498238864 | validation: 2.542427046952721]
	TIME [epoch: 8.43 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59115610948054		[learning rate: 0.00029955]
	Learning Rate: 0.000299553
	LOSS [training: 2.59115610948054 | validation: 2.516406723848851]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_1065.pth
	Model improved!!!
EPOCH 1066/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59893546981976		[learning rate: 0.00029847]
	Learning Rate: 0.000298466
	LOSS [training: 2.59893546981976 | validation: 2.530115216732863]
	TIME [epoch: 8.44 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.594011036989814		[learning rate: 0.00029738]
	Learning Rate: 0.000297383
	LOSS [training: 2.594011036989814 | validation: 2.5337506589065057]
	TIME [epoch: 8.43 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5855570986269134		[learning rate: 0.0002963]
	Learning Rate: 0.000296304
	LOSS [training: 2.5855570986269134 | validation: 2.5379154744448615]
	TIME [epoch: 8.43 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582360292361815		[learning rate: 0.00029523]
	Learning Rate: 0.000295228
	LOSS [training: 2.582360292361815 | validation: 2.5267868913611693]
	TIME [epoch: 8.43 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5971640413880097		[learning rate: 0.00029416]
	Learning Rate: 0.000294157
	LOSS [training: 2.5971640413880097 | validation: 2.5433913057284974]
	TIME [epoch: 8.46 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600319743420049		[learning rate: 0.00029309]
	Learning Rate: 0.000293089
	LOSS [training: 2.600319743420049 | validation: 2.5462483363982704]
	TIME [epoch: 8.43 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5908131088397974		[learning rate: 0.00029203]
	Learning Rate: 0.000292026
	LOSS [training: 2.5908131088397974 | validation: 2.5900124026503613]
	TIME [epoch: 8.42 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.608194598398107		[learning rate: 0.00029097]
	Learning Rate: 0.000290966
	LOSS [training: 2.608194598398107 | validation: 2.5329402276941027]
	TIME [epoch: 8.43 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5887229718161806		[learning rate: 0.00028991]
	Learning Rate: 0.00028991
	LOSS [training: 2.5887229718161806 | validation: 2.552188270002791]
	TIME [epoch: 8.46 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5966229678166632		[learning rate: 0.00028886]
	Learning Rate: 0.000288858
	LOSS [training: 2.5966229678166632 | validation: 2.5592390695338088]
	TIME [epoch: 8.44 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.594904060624412		[learning rate: 0.00028781]
	Learning Rate: 0.00028781
	LOSS [training: 2.594904060624412 | validation: 2.5432013124261132]
	TIME [epoch: 8.44 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59018280785946		[learning rate: 0.00028677]
	Learning Rate: 0.000286765
	LOSS [training: 2.59018280785946 | validation: 2.5589274282243943]
	TIME [epoch: 8.44 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5929794220460147		[learning rate: 0.00028572]
	Learning Rate: 0.000285724
	LOSS [training: 2.5929794220460147 | validation: 2.528347626594698]
	TIME [epoch: 8.46 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5901599002251885		[learning rate: 0.00028469]
	Learning Rate: 0.000284688
	LOSS [training: 2.5901599002251885 | validation: 2.5254165726863302]
	TIME [epoch: 8.45 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5872599754111967		[learning rate: 0.00028365]
	Learning Rate: 0.000283654
	LOSS [training: 2.5872599754111967 | validation: 2.52566464861788]
	TIME [epoch: 8.44 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5943716363863514		[learning rate: 0.00028263]
	Learning Rate: 0.000282625
	LOSS [training: 2.5943716363863514 | validation: 2.5249965917069286]
	TIME [epoch: 8.44 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5987663201022904		[learning rate: 0.0002816]
	Learning Rate: 0.000281599
	LOSS [training: 2.5987663201022904 | validation: 2.5309899877821342]
	TIME [epoch: 8.46 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.592287558946393		[learning rate: 0.00028058]
	Learning Rate: 0.000280577
	LOSS [training: 2.592287558946393 | validation: 2.554446573645782]
	TIME [epoch: 8.43 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5873868934432673		[learning rate: 0.00027956]
	Learning Rate: 0.000279559
	LOSS [training: 2.5873868934432673 | validation: 2.5285227607190484]
	TIME [epoch: 8.45 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5838132210353857		[learning rate: 0.00027854]
	Learning Rate: 0.000278545
	LOSS [training: 2.5838132210353857 | validation: 2.5230226152430486]
	TIME [epoch: 8.44 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5866816382037756		[learning rate: 0.00027753]
	Learning Rate: 0.000277534
	LOSS [training: 2.5866816382037756 | validation: 2.538047535525947]
	TIME [epoch: 8.46 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587867906220027		[learning rate: 0.00027653]
	Learning Rate: 0.000276527
	LOSS [training: 2.587867906220027 | validation: 2.552711328058542]
	TIME [epoch: 8.45 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.595512135398175		[learning rate: 0.00027552]
	Learning Rate: 0.000275523
	LOSS [training: 2.595512135398175 | validation: 2.5295537079735038]
	TIME [epoch: 8.43 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5844250187746716		[learning rate: 0.00027452]
	Learning Rate: 0.000274523
	LOSS [training: 2.5844250187746716 | validation: 2.5349131569552306]
	TIME [epoch: 8.49 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5969283087580175		[learning rate: 0.00027353]
	Learning Rate: 0.000273527
	LOSS [training: 2.5969283087580175 | validation: 2.526511899483048]
	TIME [epoch: 8.44 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588876010060626		[learning rate: 0.00027253]
	Learning Rate: 0.000272534
	LOSS [training: 2.588876010060626 | validation: 2.541295415806773]
	TIME [epoch: 8.43 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6019092447943777		[learning rate: 0.00027155]
	Learning Rate: 0.000271545
	LOSS [training: 2.6019092447943777 | validation: 2.5266024320614724]
	TIME [epoch: 8.44 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5958163276683384		[learning rate: 0.00027056]
	Learning Rate: 0.00027056
	LOSS [training: 2.5958163276683384 | validation: 2.5436471714653255]
	TIME [epoch: 8.49 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5893251024220376		[learning rate: 0.00026958]
	Learning Rate: 0.000269578
	LOSS [training: 2.5893251024220376 | validation: 2.5274362739080347]
	TIME [epoch: 8.45 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5863235343414104		[learning rate: 0.0002686]
	Learning Rate: 0.0002686
	LOSS [training: 2.5863235343414104 | validation: 2.527750027490667]
	TIME [epoch: 8.44 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5897227493658868		[learning rate: 0.00026762]
	Learning Rate: 0.000267625
	LOSS [training: 2.5897227493658868 | validation: 2.541404573264959]
	TIME [epoch: 8.44 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5902684847361956		[learning rate: 0.00026665]
	Learning Rate: 0.000266654
	LOSS [training: 2.5902684847361956 | validation: 2.536878534462364]
	TIME [epoch: 8.47 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5858350574110456		[learning rate: 0.00026569]
	Learning Rate: 0.000265686
	LOSS [training: 2.5858350574110456 | validation: 2.5403536551679187]
	TIME [epoch: 8.48 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599538099474049		[learning rate: 0.00026472]
	Learning Rate: 0.000264722
	LOSS [training: 2.599538099474049 | validation: 2.5678728956976933]
	TIME [epoch: 8.43 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597669177249433		[learning rate: 0.00026376]
	Learning Rate: 0.000263761
	LOSS [training: 2.597669177249433 | validation: 2.5369357761993347]
	TIME [epoch: 8.43 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.593687265612987		[learning rate: 0.0002628]
	Learning Rate: 0.000262804
	LOSS [training: 2.593687265612987 | validation: 2.5298236248744295]
	TIME [epoch: 8.45 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59378538025107		[learning rate: 0.00026185]
	Learning Rate: 0.00026185
	LOSS [training: 2.59378538025107 | validation: 2.549143173831392]
	TIME [epoch: 8.44 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5895703136698884		[learning rate: 0.0002609]
	Learning Rate: 0.0002609
	LOSS [training: 2.5895703136698884 | validation: 2.543597727136829]
	TIME [epoch: 8.43 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5900426488064427		[learning rate: 0.00025995]
	Learning Rate: 0.000259953
	LOSS [training: 2.5900426488064427 | validation: 2.525873718422722]
	TIME [epoch: 8.43 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5940574808017547		[learning rate: 0.00025901]
	Learning Rate: 0.00025901
	LOSS [training: 2.5940574808017547 | validation: 2.5600965391166017]
	TIME [epoch: 8.46 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59506444178369		[learning rate: 0.00025807]
	Learning Rate: 0.00025807
	LOSS [training: 2.59506444178369 | validation: 2.526806121954143]
	TIME [epoch: 8.44 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6008830198985367		[learning rate: 0.00025713]
	Learning Rate: 0.000257133
	LOSS [training: 2.6008830198985367 | validation: 2.550411549901199]
	TIME [epoch: 8.47 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.593311963486401		[learning rate: 0.0002562]
	Learning Rate: 0.0002562
	LOSS [training: 2.593311963486401 | validation: 2.538991922600461]
	TIME [epoch: 8.44 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5872909526155907		[learning rate: 0.00025527]
	Learning Rate: 0.00025527
	LOSS [training: 2.5872909526155907 | validation: 2.549230392854123]
	TIME [epoch: 8.45 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589788830897555		[learning rate: 0.00025434]
	Learning Rate: 0.000254344
	LOSS [training: 2.589788830897555 | validation: 2.527493588338144]
	TIME [epoch: 8.43 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596070771795814		[learning rate: 0.00025342]
	Learning Rate: 0.000253421
	LOSS [training: 2.596070771795814 | validation: 2.5515742441837053]
	TIME [epoch: 8.44 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6014392130085366		[learning rate: 0.0002525]
	Learning Rate: 0.000252501
	LOSS [training: 2.6014392130085366 | validation: 2.5384150079473478]
	TIME [epoch: 8.44 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5880832020082263		[learning rate: 0.00025158]
	Learning Rate: 0.000251585
	LOSS [training: 2.5880832020082263 | validation: 2.5368453941747204]
	TIME [epoch: 8.45 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.592963022245388		[learning rate: 0.00025067]
	Learning Rate: 0.000250672
	LOSS [training: 2.592963022245388 | validation: 2.513007280493526]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_1114.pth
	Model improved!!!
EPOCH 1115/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5891459502762286		[learning rate: 0.00024976]
	Learning Rate: 0.000249762
	LOSS [training: 2.5891459502762286 | validation: 2.5301700643465614]
	TIME [epoch: 8.42 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5889873340437215		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 2.5889873340437215 | validation: 2.5324112510072596]
	TIME [epoch: 8.43 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5852533197392065		[learning rate: 0.00024795]
	Learning Rate: 0.000247952
	LOSS [training: 2.5852533197392065 | validation: 2.529740787773462]
	TIME [epoch: 8.45 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5864644211938836		[learning rate: 0.00024705]
	Learning Rate: 0.000247053
	LOSS [training: 2.5864644211938836 | validation: 2.5242602993867056]
	TIME [epoch: 8.43 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597193001844869		[learning rate: 0.00024616]
	Learning Rate: 0.000246156
	LOSS [training: 2.597193001844869 | validation: 2.530959104623405]
	TIME [epoch: 8.44 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5891174609080396		[learning rate: 0.00024526]
	Learning Rate: 0.000245263
	LOSS [training: 2.5891174609080396 | validation: 2.5241538052228325]
	TIME [epoch: 8.43 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.590536823887025		[learning rate: 0.00024437]
	Learning Rate: 0.000244373
	LOSS [training: 2.590536823887025 | validation: 2.534094746140913]
	TIME [epoch: 8.45 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5985784871475746		[learning rate: 0.00024349]
	Learning Rate: 0.000243486
	LOSS [training: 2.5985784871475746 | validation: 2.5560430300911805]
	TIME [epoch: 8.43 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5860609972024475		[learning rate: 0.0002426]
	Learning Rate: 0.000242602
	LOSS [training: 2.5860609972024475 | validation: 2.5263278669372253]
	TIME [epoch: 8.44 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5936835384035137		[learning rate: 0.00024172]
	Learning Rate: 0.000241722
	LOSS [training: 2.5936835384035137 | validation: 2.5563460983496746]
	TIME [epoch: 8.46 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599673801370132		[learning rate: 0.00024084]
	Learning Rate: 0.000240845
	LOSS [training: 2.599673801370132 | validation: 2.5376404671064714]
	TIME [epoch: 8.47 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5884897683660717		[learning rate: 0.00023997]
	Learning Rate: 0.000239971
	LOSS [training: 2.5884897683660717 | validation: 2.533849047936428]
	TIME [epoch: 8.43 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602463864422927		[learning rate: 0.0002391]
	Learning Rate: 0.0002391
	LOSS [training: 2.602463864422927 | validation: 2.5468789692527882]
	TIME [epoch: 8.43 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588584218914719		[learning rate: 0.00023823]
	Learning Rate: 0.000238232
	LOSS [training: 2.588584218914719 | validation: 2.534527078337418]
	TIME [epoch: 8.44 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5914105923829522		[learning rate: 0.00023737]
	Learning Rate: 0.000237367
	LOSS [training: 2.5914105923829522 | validation: 2.5349558335002103]
	TIME [epoch: 8.46 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5991615827046513		[learning rate: 0.00023651]
	Learning Rate: 0.000236506
	LOSS [training: 2.5991615827046513 | validation: 2.5401248113239703]
	TIME [epoch: 8.44 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583074931551403		[learning rate: 0.00023565]
	Learning Rate: 0.000235648
	LOSS [training: 2.583074931551403 | validation: 2.534101433301978]
	TIME [epoch: 8.44 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589781360806522		[learning rate: 0.00023479]
	Learning Rate: 0.000234793
	LOSS [training: 2.589781360806522 | validation: 2.528209927651374]
	TIME [epoch: 8.43 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588158112064787		[learning rate: 0.00023394]
	Learning Rate: 0.00023394
	LOSS [training: 2.588158112064787 | validation: 2.534545468131305]
	TIME [epoch: 8.45 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599243167797135		[learning rate: 0.00023309]
	Learning Rate: 0.000233091
	LOSS [training: 2.599243167797135 | validation: 2.5377565062147296]
	TIME [epoch: 8.44 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.591902510576285		[learning rate: 0.00023225]
	Learning Rate: 0.000232246
	LOSS [training: 2.591902510576285 | validation: 2.5558055244335653]
	TIME [epoch: 8.43 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598319848762858		[learning rate: 0.0002314]
	Learning Rate: 0.000231403
	LOSS [training: 2.598319848762858 | validation: 2.5349714539337724]
	TIME [epoch: 8.43 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6009512334249325		[learning rate: 0.00023056]
	Learning Rate: 0.000230563
	LOSS [training: 2.6009512334249325 | validation: 2.529683206580577]
	TIME [epoch: 8.45 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5958446316746673		[learning rate: 0.00022973]
	Learning Rate: 0.000229726
	LOSS [training: 2.5958446316746673 | validation: 2.5289307773241267]
	TIME [epoch: 8.44 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6027758301644606		[learning rate: 0.00022889]
	Learning Rate: 0.000228893
	LOSS [training: 2.6027758301644606 | validation: 2.520985941364792]
	TIME [epoch: 8.44 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586150571050523		[learning rate: 0.00022806]
	Learning Rate: 0.000228062
	LOSS [training: 2.586150571050523 | validation: 2.534649023431907]
	TIME [epoch: 8.43 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58517260215853		[learning rate: 0.00022723]
	Learning Rate: 0.000227234
	LOSS [training: 2.58517260215853 | validation: 2.531324599928299]
	TIME [epoch: 8.46 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5932173637398805		[learning rate: 0.00022641]
	Learning Rate: 0.00022641
	LOSS [training: 2.5932173637398805 | validation: 2.5268483170361717]
	TIME [epoch: 8.44 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.592882873461422		[learning rate: 0.00022559]
	Learning Rate: 0.000225588
	LOSS [training: 2.592882873461422 | validation: 2.5484854994151602]
	TIME [epoch: 8.44 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58839932301909		[learning rate: 0.00022477]
	Learning Rate: 0.000224769
	LOSS [training: 2.58839932301909 | validation: 2.5412407398989454]
	TIME [epoch: 8.43 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5883831366097985		[learning rate: 0.00022395]
	Learning Rate: 0.000223954
	LOSS [training: 2.5883831366097985 | validation: 2.5201616211139655]
	TIME [epoch: 8.46 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5883582255523083		[learning rate: 0.00022314]
	Learning Rate: 0.000223141
	LOSS [training: 2.5883582255523083 | validation: 2.530934280007512]
	TIME [epoch: 8.46 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.594018804921819		[learning rate: 0.00022233]
	Learning Rate: 0.000222331
	LOSS [training: 2.594018804921819 | validation: 2.5430107203619654]
	TIME [epoch: 8.46 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602325149969753		[learning rate: 0.00022152]
	Learning Rate: 0.000221524
	LOSS [training: 2.602325149969753 | validation: 2.521983747313987]
	TIME [epoch: 8.43 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5866291656384317		[learning rate: 0.00022072]
	Learning Rate: 0.00022072
	LOSS [training: 2.5866291656384317 | validation: 2.5296673170433315]
	TIME [epoch: 8.45 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5863673030691303		[learning rate: 0.00021992]
	Learning Rate: 0.000219919
	LOSS [training: 2.5863673030691303 | validation: 2.5286838762239237]
	TIME [epoch: 8.44 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5877571219386355		[learning rate: 0.00021912]
	Learning Rate: 0.000219121
	LOSS [training: 2.5877571219386355 | validation: 2.5452857198064875]
	TIME [epoch: 8.48 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5949071731136004		[learning rate: 0.00021833]
	Learning Rate: 0.000218326
	LOSS [training: 2.5949071731136004 | validation: 2.537292222756663]
	TIME [epoch: 8.42 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589172907585124		[learning rate: 0.00021753]
	Learning Rate: 0.000217534
	LOSS [training: 2.589172907585124 | validation: 2.5245794379399458]
	TIME [epoch: 8.45 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589341483611645		[learning rate: 0.00021674]
	Learning Rate: 0.000216744
	LOSS [training: 2.589341483611645 | validation: 2.5232294734225214]
	TIME [epoch: 8.43 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587912343059943		[learning rate: 0.00021596]
	Learning Rate: 0.000215958
	LOSS [training: 2.587912343059943 | validation: 2.5241130916345247]
	TIME [epoch: 8.48 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5983937952741147		[learning rate: 0.00021517]
	Learning Rate: 0.000215174
	LOSS [training: 2.5983937952741147 | validation: 2.537536969198717]
	TIME [epoch: 8.44 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588691260357309		[learning rate: 0.00021439]
	Learning Rate: 0.000214393
	LOSS [training: 2.588691260357309 | validation: 2.5326712498691792]
	TIME [epoch: 8.45 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5874907051136686		[learning rate: 0.00021361]
	Learning Rate: 0.000213615
	LOSS [training: 2.5874907051136686 | validation: 2.5323305759469252]
	TIME [epoch: 8.44 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5867468605720596		[learning rate: 0.00021284]
	Learning Rate: 0.00021284
	LOSS [training: 2.5867468605720596 | validation: 2.5338585399851232]
	TIME [epoch: 8.43 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589697742608224		[learning rate: 0.00021207]
	Learning Rate: 0.000212067
	LOSS [training: 2.589697742608224 | validation: 2.5209247119875497]
	TIME [epoch: 8.47 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583465133879586		[learning rate: 0.0002113]
	Learning Rate: 0.000211298
	LOSS [training: 2.583465133879586 | validation: 2.534405321838758]
	TIME [epoch: 8.45 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584257516888289		[learning rate: 0.00021053]
	Learning Rate: 0.000210531
	LOSS [training: 2.584257516888289 | validation: 2.552427545957289]
	TIME [epoch: 8.43 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59346880434445		[learning rate: 0.00020977]
	Learning Rate: 0.000209767
	LOSS [training: 2.59346880434445 | validation: 2.5540637256353604]
	TIME [epoch: 8.43 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5874989946880262		[learning rate: 0.00020901]
	Learning Rate: 0.000209006
	LOSS [training: 2.5874989946880262 | validation: 2.527793725708576]
	TIME [epoch: 8.45 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.590523197540773		[learning rate: 0.00020825]
	Learning Rate: 0.000208247
	LOSS [training: 2.590523197540773 | validation: 2.5317405007414937]
	TIME [epoch: 8.44 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586983692918568		[learning rate: 0.00020749]
	Learning Rate: 0.000207491
	LOSS [training: 2.586983692918568 | validation: 2.531992950319718]
	TIME [epoch: 8.43 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5860497720359374		[learning rate: 0.00020674]
	Learning Rate: 0.000206738
	LOSS [training: 2.5860497720359374 | validation: 2.5229698937221587]
	TIME [epoch: 8.44 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588759183826796		[learning rate: 0.00020599]
	Learning Rate: 0.000205988
	LOSS [training: 2.588759183826796 | validation: 2.5212199350011013]
	TIME [epoch: 8.45 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5966038661008413		[learning rate: 0.00020524]
	Learning Rate: 0.000205241
	LOSS [training: 2.5966038661008413 | validation: 2.5302300034889478]
	TIME [epoch: 8.44 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585011754177981		[learning rate: 0.0002045]
	Learning Rate: 0.000204496
	LOSS [training: 2.585011754177981 | validation: 2.5284372529869534]
	TIME [epoch: 8.42 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5866631813480176		[learning rate: 0.00020375]
	Learning Rate: 0.000203754
	LOSS [training: 2.5866631813480176 | validation: 2.535955070737539]
	TIME [epoch: 8.43 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585627060640133		[learning rate: 0.00020301]
	Learning Rate: 0.000203014
	LOSS [training: 2.585627060640133 | validation: 2.555234306965405]
	TIME [epoch: 8.46 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6166282549621114		[learning rate: 0.00020228]
	Learning Rate: 0.000202277
	LOSS [training: 2.6166282549621114 | validation: 2.5583206843848547]
	TIME [epoch: 8.44 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.590454597840361		[learning rate: 0.00020154]
	Learning Rate: 0.000201543
	LOSS [training: 2.590454597840361 | validation: 2.5236087051827196]
	TIME [epoch: 8.43 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58363702588031		[learning rate: 0.00020081]
	Learning Rate: 0.000200812
	LOSS [training: 2.58363702588031 | validation: 2.520368869140374]
	TIME [epoch: 8.46 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.592475354234741		[learning rate: 0.00020008]
	Learning Rate: 0.000200083
	LOSS [training: 2.592475354234741 | validation: 2.530953638260082]
	TIME [epoch: 8.48 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588719118035958		[learning rate: 0.00019936]
	Learning Rate: 0.000199357
	LOSS [training: 2.588719118035958 | validation: 2.525953846112678]
	TIME [epoch: 8.43 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5870515527564595		[learning rate: 0.00019863]
	Learning Rate: 0.000198634
	LOSS [training: 2.5870515527564595 | validation: 2.532597033706108]
	TIME [epoch: 8.43 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6027327373367544		[learning rate: 0.00019791]
	Learning Rate: 0.000197913
	LOSS [training: 2.6027327373367544 | validation: 2.5323190471145867]
	TIME [epoch: 8.43 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589746137710361		[learning rate: 0.00019719]
	Learning Rate: 0.000197194
	LOSS [training: 2.589746137710361 | validation: 2.5277013587746167]
	TIME [epoch: 8.45 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583977744675406		[learning rate: 0.00019648]
	Learning Rate: 0.000196479
	LOSS [training: 2.583977744675406 | validation: 2.538627662738577]
	TIME [epoch: 8.44 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58702183840417		[learning rate: 0.00019577]
	Learning Rate: 0.000195766
	LOSS [training: 2.58702183840417 | validation: 2.5411557680178083]
	TIME [epoch: 8.44 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588061200010217		[learning rate: 0.00019506]
	Learning Rate: 0.000195055
	LOSS [training: 2.588061200010217 | validation: 2.536336915128839]
	TIME [epoch: 8.44 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5910779933581125		[learning rate: 0.00019435]
	Learning Rate: 0.000194347
	LOSS [training: 2.5910779933581125 | validation: 2.525673159388048]
	TIME [epoch: 8.46 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59806298077399		[learning rate: 0.00019364]
	Learning Rate: 0.000193642
	LOSS [training: 2.59806298077399 | validation: 2.5670166337726217]
	TIME [epoch: 8.44 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5986923057736573		[learning rate: 0.00019294]
	Learning Rate: 0.000192939
	LOSS [training: 2.5986923057736573 | validation: 2.516923473045774]
	TIME [epoch: 8.44 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5895806389020497		[learning rate: 0.00019224]
	Learning Rate: 0.000192239
	LOSS [training: 2.5895806389020497 | validation: 2.530886134851424]
	TIME [epoch: 8.44 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5845049847693877		[learning rate: 0.00019154]
	Learning Rate: 0.000191542
	LOSS [training: 2.5845049847693877 | validation: 2.5260645035473184]
	TIME [epoch: 8.45 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5907678577012874		[learning rate: 0.00019085]
	Learning Rate: 0.000190846
	LOSS [training: 2.5907678577012874 | validation: 2.535350053500813]
	TIME [epoch: 8.44 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5958485151558763		[learning rate: 0.00019015]
	Learning Rate: 0.000190154
	LOSS [training: 2.5958485151558763 | validation: 2.522931098239174]
	TIME [epoch: 8.44 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.590574949141155		[learning rate: 0.00018946]
	Learning Rate: 0.000189464
	LOSS [training: 2.590574949141155 | validation: 2.529498560628564]
	TIME [epoch: 8.44 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5906765154437945		[learning rate: 0.00018878]
	Learning Rate: 0.000188776
	LOSS [training: 2.5906765154437945 | validation: 2.5230244602894416]
	TIME [epoch: 8.5 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5896122394853953		[learning rate: 0.00018809]
	Learning Rate: 0.000188091
	LOSS [training: 2.5896122394853953 | validation: 2.52115153448097]
	TIME [epoch: 8.43 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5921569356402285		[learning rate: 0.00018741]
	Learning Rate: 0.000187409
	LOSS [training: 2.5921569356402285 | validation: 2.546380628508339]
	TIME [epoch: 8.43 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5925641401280846		[learning rate: 0.00018673]
	Learning Rate: 0.000186729
	LOSS [training: 2.5925641401280846 | validation: 2.544968971343752]
	TIME [epoch: 8.43 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587565557013412		[learning rate: 0.00018605]
	Learning Rate: 0.000186051
	LOSS [training: 2.587565557013412 | validation: 2.529069263590169]
	TIME [epoch: 8.48 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5868550608111542		[learning rate: 0.00018538]
	Learning Rate: 0.000185376
	LOSS [training: 2.5868550608111542 | validation: 2.525174450688494]
	TIME [epoch: 8.45 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5979144799612577		[learning rate: 0.0001847]
	Learning Rate: 0.000184703
	LOSS [training: 2.5979144799612577 | validation: 2.531241835165142]
	TIME [epoch: 8.42 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.591226370099271		[learning rate: 0.00018403]
	Learning Rate: 0.000184033
	LOSS [training: 2.591226370099271 | validation: 2.543188408219595]
	TIME [epoch: 8.44 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5929630158981274		[learning rate: 0.00018336]
	Learning Rate: 0.000183365
	LOSS [training: 2.5929630158981274 | validation: 2.528323724742868]
	TIME [epoch: 8.46 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585374101142072		[learning rate: 0.0001827]
	Learning Rate: 0.000182699
	LOSS [training: 2.585374101142072 | validation: 2.5110785002483844]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_1201.pth
	Model improved!!!
EPOCH 1202/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5839102685279185		[learning rate: 0.00018204]
	Learning Rate: 0.000182036
	LOSS [training: 2.5839102685279185 | validation: 2.5344348474888063]
	TIME [epoch: 8.48 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.592900391353427		[learning rate: 0.00018138]
	Learning Rate: 0.000181376
	LOSS [training: 2.592900391353427 | validation: 2.5503530958011993]
	TIME [epoch: 8.42 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5994915593532006		[learning rate: 0.00018072]
	Learning Rate: 0.000180717
	LOSS [training: 2.5994915593532006 | validation: 2.539141051702021]
	TIME [epoch: 8.45 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5836786890611423		[learning rate: 0.00018006]
	Learning Rate: 0.000180062
	LOSS [training: 2.5836786890611423 | validation: 2.524148656366334]
	TIME [epoch: 8.43 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583657257200815		[learning rate: 0.00017941]
	Learning Rate: 0.000179408
	LOSS [training: 2.583657257200815 | validation: 2.5337386221412013]
	TIME [epoch: 8.43 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5866394873950576		[learning rate: 0.00017876]
	Learning Rate: 0.000178757
	LOSS [training: 2.5866394873950576 | validation: 2.529653678528148]
	TIME [epoch: 8.44 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598416557691651		[learning rate: 0.00017811]
	Learning Rate: 0.000178108
	LOSS [training: 2.598416557691651 | validation: 2.539076644224754]
	TIME [epoch: 8.48 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.591487951069193		[learning rate: 0.00017746]
	Learning Rate: 0.000177462
	LOSS [training: 2.591487951069193 | validation: 2.5232394371528963]
	TIME [epoch: 8.43 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583340052316025		[learning rate: 0.00017682]
	Learning Rate: 0.000176818
	LOSS [training: 2.583340052316025 | validation: 2.5232916297043797]
	TIME [epoch: 8.43 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5868866736958673		[learning rate: 0.00017618]
	Learning Rate: 0.000176176
	LOSS [training: 2.5868866736958673 | validation: 2.5259616633839688]
	TIME [epoch: 8.44 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5918585619166796		[learning rate: 0.00017554]
	Learning Rate: 0.000175537
	LOSS [training: 2.5918585619166796 | validation: 2.5415069143257716]
	TIME [epoch: 8.45 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585845133919793		[learning rate: 0.0001749]
	Learning Rate: 0.0001749
	LOSS [training: 2.585845133919793 | validation: 2.532088368053595]
	TIME [epoch: 8.44 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5938235058579098		[learning rate: 0.00017427]
	Learning Rate: 0.000174265
	LOSS [training: 2.5938235058579098 | validation: 2.5256811575667895]
	TIME [epoch: 8.43 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5852075856431833		[learning rate: 0.00017363]
	Learning Rate: 0.000173633
	LOSS [training: 2.5852075856431833 | validation: 2.5420294915852244]
	TIME [epoch: 8.43 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5885407272201153		[learning rate: 0.000173]
	Learning Rate: 0.000173003
	LOSS [training: 2.5885407272201153 | validation: 2.545419338065675]
	TIME [epoch: 8.46 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5925348543446503		[learning rate: 0.00017237]
	Learning Rate: 0.000172375
	LOSS [training: 2.5925348543446503 | validation: 2.533800325654159]
	TIME [epoch: 8.43 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58439196085492		[learning rate: 0.00017175]
	Learning Rate: 0.000171749
	LOSS [training: 2.58439196085492 | validation: 2.528889445424064]
	TIME [epoch: 8.43 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5874419009804845		[learning rate: 0.00017113]
	Learning Rate: 0.000171126
	LOSS [training: 2.5874419009804845 | validation: 2.544441164361215]
	TIME [epoch: 8.45 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5923058142059316		[learning rate: 0.0001705]
	Learning Rate: 0.000170505
	LOSS [training: 2.5923058142059316 | validation: 2.5401771836252998]
	TIME [epoch: 8.46 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5899559711491253		[learning rate: 0.00016989]
	Learning Rate: 0.000169886
	LOSS [training: 2.5899559711491253 | validation: 2.520320657589547]
	TIME [epoch: 8.44 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5851214705651455		[learning rate: 0.00016927]
	Learning Rate: 0.00016927
	LOSS [training: 2.5851214705651455 | validation: 2.5332867662049967]
	TIME [epoch: 8.43 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5854362826997153		[learning rate: 0.00016866]
	Learning Rate: 0.000168655
	LOSS [training: 2.5854362826997153 | validation: 2.524657209762065]
	TIME [epoch: 8.43 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586219684474469		[learning rate: 0.00016804]
	Learning Rate: 0.000168043
	LOSS [training: 2.586219684474469 | validation: 2.5277646816712203]
	TIME [epoch: 8.48 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585000983279896		[learning rate: 0.00016743]
	Learning Rate: 0.000167433
	LOSS [training: 2.585000983279896 | validation: 2.5405071393876772]
	TIME [epoch: 8.46 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58608536934194		[learning rate: 0.00016683]
	Learning Rate: 0.000166826
	LOSS [training: 2.58608536934194 | validation: 2.525780696017998]
	TIME [epoch: 8.43 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5833416831196283		[learning rate: 0.00016622]
	Learning Rate: 0.00016622
	LOSS [training: 2.5833416831196283 | validation: 2.530391449255085]
	TIME [epoch: 8.45 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5878059786942957		[learning rate: 0.00016562]
	Learning Rate: 0.000165617
	LOSS [training: 2.5878059786942957 | validation: 2.529958600900134]
	TIME [epoch: 8.45 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583654942489725		[learning rate: 0.00016502]
	Learning Rate: 0.000165016
	LOSS [training: 2.583654942489725 | validation: 2.522584735233532]
	TIME [epoch: 8.44 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587487755922747		[learning rate: 0.00016442]
	Learning Rate: 0.000164417
	LOSS [training: 2.587487755922747 | validation: 2.5237103560885745]
	TIME [epoch: 8.43 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586104344844661		[learning rate: 0.00016382]
	Learning Rate: 0.000163821
	LOSS [training: 2.586104344844661 | validation: 2.521917782260637]
	TIME [epoch: 8.45 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613264117635406		[learning rate: 0.00016323]
	Learning Rate: 0.000163226
	LOSS [training: 2.613264117635406 | validation: 2.5226383359780553]
	TIME [epoch: 8.44 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5881231957766166		[learning rate: 0.00016263]
	Learning Rate: 0.000162634
	LOSS [training: 2.5881231957766166 | validation: 2.5252035837317344]
	TIME [epoch: 8.44 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5869757035482026		[learning rate: 0.00016204]
	Learning Rate: 0.000162043
	LOSS [training: 2.5869757035482026 | validation: 2.522631594864836]
	TIME [epoch: 8.43 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58506667569202		[learning rate: 0.00016146]
	Learning Rate: 0.000161455
	LOSS [training: 2.58506667569202 | validation: 2.5338610120080576]
	TIME [epoch: 8.46 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58772231154592		[learning rate: 0.00016087]
	Learning Rate: 0.000160869
	LOSS [training: 2.58772231154592 | validation: 2.528965857478412]
	TIME [epoch: 8.44 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5919816682042613		[learning rate: 0.00016029]
	Learning Rate: 0.000160286
	LOSS [training: 2.5919816682042613 | validation: 2.5412987410680867]
	TIME [epoch: 8.44 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585270725831032		[learning rate: 0.0001597]
	Learning Rate: 0.000159704
	LOSS [training: 2.585270725831032 | validation: 2.533133402764361]
	TIME [epoch: 8.43 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589201857962167		[learning rate: 0.00015912]
	Learning Rate: 0.000159124
	LOSS [training: 2.589201857962167 | validation: 2.542123351664528]
	TIME [epoch: 8.46 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5894787556791328		[learning rate: 0.00015855]
	Learning Rate: 0.000158547
	LOSS [training: 2.5894787556791328 | validation: 2.5319140028504528]
	TIME [epoch: 8.44 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.593257786935461		[learning rate: 0.00015797]
	Learning Rate: 0.000157972
	LOSS [training: 2.593257786935461 | validation: 2.5230266130988555]
	TIME [epoch: 8.45 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58727459341332		[learning rate: 0.0001574]
	Learning Rate: 0.000157398
	LOSS [training: 2.58727459341332 | validation: 2.5348173703978816]
	TIME [epoch: 8.43 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5843285652072963		[learning rate: 0.00015683]
	Learning Rate: 0.000156827
	LOSS [training: 2.5843285652072963 | validation: 2.5408863823262244]
	TIME [epoch: 8.47 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.592510092909873		[learning rate: 0.00015626]
	Learning Rate: 0.000156258
	LOSS [training: 2.592510092909873 | validation: 2.5280874183619195]
	TIME [epoch: 8.44 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5882135546804745		[learning rate: 0.00015569]
	Learning Rate: 0.000155691
	LOSS [training: 2.5882135546804745 | validation: 2.531256185303752]
	TIME [epoch: 8.44 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587860079006183		[learning rate: 0.00015513]
	Learning Rate: 0.000155126
	LOSS [training: 2.587860079006183 | validation: 2.519602430010358]
	TIME [epoch: 8.43 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58546258389666		[learning rate: 0.00015456]
	Learning Rate: 0.000154563
	LOSS [training: 2.58546258389666 | validation: 2.5393213731418713]
	TIME [epoch: 8.45 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579023078622732		[learning rate: 0.000154]
	Learning Rate: 0.000154002
	LOSS [training: 2.579023078622732 | validation: 2.5269335931462336]
	TIME [epoch: 8.44 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5880806126738776		[learning rate: 0.00015344]
	Learning Rate: 0.000153443
	LOSS [training: 2.5880806126738776 | validation: 2.538351513099067]
	TIME [epoch: 8.44 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5891260672766103		[learning rate: 0.00015289]
	Learning Rate: 0.000152886
	LOSS [training: 2.5891260672766103 | validation: 2.540521721407824]
	TIME [epoch: 8.44 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5940772534313923		[learning rate: 0.00015233]
	Learning Rate: 0.000152331
	LOSS [training: 2.5940772534313923 | validation: 2.5299333111871274]
	TIME [epoch: 8.46 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58730382478066		[learning rate: 0.00015178]
	Learning Rate: 0.000151779
	LOSS [training: 2.58730382478066 | validation: 2.5489991411260746]
	TIME [epoch: 8.45 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5839831281628336		[learning rate: 0.00015123]
	Learning Rate: 0.000151228
	LOSS [training: 2.5839831281628336 | validation: 2.5284651173275323]
	TIME [epoch: 8.43 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58370875256549		[learning rate: 0.00015068]
	Learning Rate: 0.000150679
	LOSS [training: 2.58370875256549 | validation: 2.533025945157676]
	TIME [epoch: 8.47 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5875129109050308		[learning rate: 0.00015013]
	Learning Rate: 0.000150132
	LOSS [training: 2.5875129109050308 | validation: 2.526216735497053]
	TIME [epoch: 8.47 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586709318295492		[learning rate: 0.00014959]
	Learning Rate: 0.000149587
	LOSS [training: 2.586709318295492 | validation: 2.5207808868756274]
	TIME [epoch: 8.44 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5847836584301165		[learning rate: 0.00014904]
	Learning Rate: 0.000149044
	LOSS [training: 2.5847836584301165 | validation: 2.5393220797940557]
	TIME [epoch: 8.46 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588746353858332		[learning rate: 0.0001485]
	Learning Rate: 0.000148504
	LOSS [training: 2.588746353858332 | validation: 2.5337533111107433]
	TIME [epoch: 8.45 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5802616150892392		[learning rate: 0.00014796]
	Learning Rate: 0.000147965
	LOSS [training: 2.5802616150892392 | validation: 2.5435259292181103]
	TIME [epoch: 8.44 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5875864142468528		[learning rate: 0.00014743]
	Learning Rate: 0.000147428
	LOSS [training: 2.5875864142468528 | validation: 2.530435925938443]
	TIME [epoch: 8.44 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5923009619259965		[learning rate: 0.00014689]
	Learning Rate: 0.000146893
	LOSS [training: 2.5923009619259965 | validation: 2.5553981803800605]
	TIME [epoch: 8.42 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.591956321953538		[learning rate: 0.00014636]
	Learning Rate: 0.00014636
	LOSS [training: 2.591956321953538 | validation: 2.534229433825809]
	TIME [epoch: 8.45 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588861606739831		[learning rate: 0.00014583]
	Learning Rate: 0.000145828
	LOSS [training: 2.588861606739831 | validation: 2.5588501564613586]
	TIME [epoch: 8.45 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.590996985092358		[learning rate: 0.0001453]
	Learning Rate: 0.000145299
	LOSS [training: 2.590996985092358 | validation: 2.5438316641689225]
	TIME [epoch: 8.42 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5831689129223965		[learning rate: 0.00014477]
	Learning Rate: 0.000144772
	LOSS [training: 2.5831689129223965 | validation: 2.5313106791361086]
	TIME [epoch: 8.48 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58693980346745		[learning rate: 0.00014425]
	Learning Rate: 0.000144247
	LOSS [training: 2.58693980346745 | validation: 2.530712286425555]
	TIME [epoch: 8.44 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.590804456707058		[learning rate: 0.00014372]
	Learning Rate: 0.000143723
	LOSS [training: 2.590804456707058 | validation: 2.532520024010477]
	TIME [epoch: 8.45 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585567289412295		[learning rate: 0.0001432]
	Learning Rate: 0.000143201
	LOSS [training: 2.585567289412295 | validation: 2.5324643074364612]
	TIME [epoch: 8.43 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5864025389529517		[learning rate: 0.00014268]
	Learning Rate: 0.000142682
	LOSS [training: 2.5864025389529517 | validation: 2.5315861683054974]
	TIME [epoch: 8.44 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5848130697342593		[learning rate: 0.00014216]
	Learning Rate: 0.000142164
	LOSS [training: 2.5848130697342593 | validation: 2.5292861106135973]
	TIME [epoch: 8.44 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587327532489827		[learning rate: 0.00014165]
	Learning Rate: 0.000141648
	LOSS [training: 2.587327532489827 | validation: 2.5251791368369756]
	TIME [epoch: 8.45 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5929099062390852		[learning rate: 0.00014113]
	Learning Rate: 0.000141134
	LOSS [training: 2.5929099062390852 | validation: 2.530754182981119]
	TIME [epoch: 8.44 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5855725112916548		[learning rate: 0.00014062]
	Learning Rate: 0.000140622
	LOSS [training: 2.5855725112916548 | validation: 2.539312511677478]
	TIME [epoch: 8.46 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5954038459880158		[learning rate: 0.00014011]
	Learning Rate: 0.000140112
	LOSS [training: 2.5954038459880158 | validation: 2.5479735394062697]
	TIME [epoch: 8.45 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589514660978328		[learning rate: 0.0001396]
	Learning Rate: 0.000139603
	LOSS [training: 2.589514660978328 | validation: 2.5328126462073306]
	TIME [epoch: 8.45 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581851484023209		[learning rate: 0.0001391]
	Learning Rate: 0.000139096
	LOSS [training: 2.581851484023209 | validation: 2.5375043119776106]
	TIME [epoch: 8.44 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.590435458950909		[learning rate: 0.00013859]
	Learning Rate: 0.000138592
	LOSS [training: 2.590435458950909 | validation: 2.5298934819091503]
	TIME [epoch: 8.43 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5900039375892057		[learning rate: 0.00013809]
	Learning Rate: 0.000138089
	LOSS [training: 2.5900039375892057 | validation: 2.526483714389096]
	TIME [epoch: 8.45 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588875214711834		[learning rate: 0.00013759]
	Learning Rate: 0.000137587
	LOSS [training: 2.588875214711834 | validation: 2.5338314764991448]
	TIME [epoch: 8.44 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5894337339283013		[learning rate: 0.00013709]
	Learning Rate: 0.000137088
	LOSS [training: 2.5894337339283013 | validation: 2.529522902158576]
	TIME [epoch: 8.44 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5844360217427047		[learning rate: 0.00013659]
	Learning Rate: 0.000136591
	LOSS [training: 2.5844360217427047 | validation: 2.522265212496668]
	TIME [epoch: 8.43 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58095780268049		[learning rate: 0.00013609]
	Learning Rate: 0.000136095
	LOSS [training: 2.58095780268049 | validation: 2.528567323570798]
	TIME [epoch: 8.45 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5888648650887545		[learning rate: 0.0001356]
	Learning Rate: 0.000135601
	LOSS [training: 2.5888648650887545 | validation: 2.5327545980799524]
	TIME [epoch: 8.44 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59978068784849		[learning rate: 0.00013511]
	Learning Rate: 0.000135109
	LOSS [training: 2.59978068784849 | validation: 2.518514826294647]
	TIME [epoch: 8.43 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5826270524275365		[learning rate: 0.00013462]
	Learning Rate: 0.000134619
	LOSS [training: 2.5826270524275365 | validation: 2.52667488078358]
	TIME [epoch: 8.43 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5866455068221494		[learning rate: 0.00013413]
	Learning Rate: 0.00013413
	LOSS [training: 2.5866455068221494 | validation: 2.53164384233854]
	TIME [epoch: 8.45 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585252107287845		[learning rate: 0.00013364]
	Learning Rate: 0.000133643
	LOSS [training: 2.585252107287845 | validation: 2.5345996882257222]
	TIME [epoch: 8.45 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5894003402656685		[learning rate: 0.00013316]
	Learning Rate: 0.000133158
	LOSS [training: 2.5894003402656685 | validation: 2.5281461580497107]
	TIME [epoch: 8.47 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5808617975047774		[learning rate: 0.00013268]
	Learning Rate: 0.000132675
	LOSS [training: 2.5808617975047774 | validation: 2.5239651574921655]
	TIME [epoch: 8.43 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587967299311402		[learning rate: 0.00013219]
	Learning Rate: 0.000132194
	LOSS [training: 2.587967299311402 | validation: 2.523634709170387]
	TIME [epoch: 8.45 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583998671918786		[learning rate: 0.00013171]
	Learning Rate: 0.000131714
	LOSS [training: 2.583998671918786 | validation: 2.52829465324743]
	TIME [epoch: 8.44 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5829021529164344		[learning rate: 0.00013124]
	Learning Rate: 0.000131236
	LOSS [training: 2.5829021529164344 | validation: 2.520478320530297]
	TIME [epoch: 8.43 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5876919099615563		[learning rate: 0.00013076]
	Learning Rate: 0.00013076
	LOSS [training: 2.5876919099615563 | validation: 2.526011832287626]
	TIME [epoch: 8.43 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.590014713286804		[learning rate: 0.00013029]
	Learning Rate: 0.000130285
	LOSS [training: 2.590014713286804 | validation: 2.527365770065405]
	TIME [epoch: 8.45 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585669774098581		[learning rate: 0.00012981]
	Learning Rate: 0.000129812
	LOSS [training: 2.585669774098581 | validation: 2.527215816240628]
	TIME [epoch: 8.44 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5851245080600784		[learning rate: 0.00012934]
	Learning Rate: 0.000129341
	LOSS [training: 2.5851245080600784 | validation: 2.5230155036572235]
	TIME [epoch: 8.43 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5856287507645535		[learning rate: 0.00012887]
	Learning Rate: 0.000128872
	LOSS [training: 2.5856287507645535 | validation: 2.5291669372693653]
	TIME [epoch: 8.43 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5839159031035424		[learning rate: 0.0001284]
	Learning Rate: 0.000128404
	LOSS [training: 2.5839159031035424 | validation: 2.535597882155007]
	TIME [epoch: 8.47 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5867167216004985		[learning rate: 0.00012794]
	Learning Rate: 0.000127938
	LOSS [training: 2.5867167216004985 | validation: 2.515773549753421]
	TIME [epoch: 8.44 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5923463331275345		[learning rate: 0.00012747]
	Learning Rate: 0.000127474
	LOSS [training: 2.5923463331275345 | validation: 2.5254342109151935]
	TIME [epoch: 8.44 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5857346613339787		[learning rate: 0.00012701]
	Learning Rate: 0.000127011
	LOSS [training: 2.5857346613339787 | validation: 2.5240819493305295]
	TIME [epoch: 8.43 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5845452021556974		[learning rate: 0.00012655]
	Learning Rate: 0.00012655
	LOSS [training: 2.5845452021556974 | validation: 2.5257799303819133]
	TIME [epoch: 8.46 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5836112943948706		[learning rate: 0.00012609]
	Learning Rate: 0.000126091
	LOSS [training: 2.5836112943948706 | validation: 2.518142064451015]
	TIME [epoch: 8.44 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5867986168384447		[learning rate: 0.00012563]
	Learning Rate: 0.000125633
	LOSS [training: 2.5867986168384447 | validation: 2.525329375043931]
	TIME [epoch: 8.45 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5861153705852007		[learning rate: 0.00012518]
	Learning Rate: 0.000125178
	LOSS [training: 2.5861153705852007 | validation: 2.5240609418243745]
	TIME [epoch: 8.44 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588311638214875		[learning rate: 0.00012472]
	Learning Rate: 0.000124723
	LOSS [training: 2.588311638214875 | validation: 2.522761260783347]
	TIME [epoch: 8.46 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588338776080651		[learning rate: 0.00012427]
	Learning Rate: 0.000124271
	LOSS [training: 2.588338776080651 | validation: 2.522786596679325]
	TIME [epoch: 8.46 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5957384481482197		[learning rate: 0.00012382]
	Learning Rate: 0.00012382
	LOSS [training: 2.5957384481482197 | validation: 2.52007218902916]
	TIME [epoch: 8.48 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589412375144928		[learning rate: 0.00012337]
	Learning Rate: 0.00012337
	LOSS [training: 2.589412375144928 | validation: 2.556545947497008]
	TIME [epoch: 8.43 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5877477395842265		[learning rate: 0.00012292]
	Learning Rate: 0.000122923
	LOSS [training: 2.5877477395842265 | validation: 2.5278436982797787]
	TIME [epoch: 8.45 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5842627312048605		[learning rate: 0.00012248]
	Learning Rate: 0.000122476
	LOSS [training: 2.5842627312048605 | validation: 2.5277355028154656]
	TIME [epoch: 8.44 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5865175169464276		[learning rate: 0.00012203]
	Learning Rate: 0.000122032
	LOSS [training: 2.5865175169464276 | validation: 2.5293649218043512]
	TIME [epoch: 8.44 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5861772996536945		[learning rate: 0.00012159]
	Learning Rate: 0.000121589
	LOSS [training: 2.5861772996536945 | validation: 2.535390915946452]
	TIME [epoch: 8.48 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589619940770107		[learning rate: 0.00012115]
	Learning Rate: 0.000121148
	LOSS [training: 2.589619940770107 | validation: 2.5303351503563776]
	TIME [epoch: 8.46 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5846217207246447		[learning rate: 0.00012071]
	Learning Rate: 0.000120708
	LOSS [training: 2.5846217207246447 | validation: 2.5165136362869838]
	TIME [epoch: 8.42 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5836277829348235		[learning rate: 0.00012027]
	Learning Rate: 0.00012027
	LOSS [training: 2.5836277829348235 | validation: 2.5402779956272097]
	TIME [epoch: 8.44 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585875642479695		[learning rate: 0.00011983]
	Learning Rate: 0.000119834
	LOSS [training: 2.585875642479695 | validation: 2.5200253550321365]
	TIME [epoch: 8.44 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5874647758682423		[learning rate: 0.0001194]
	Learning Rate: 0.000119399
	LOSS [training: 2.5874647758682423 | validation: 2.528110584524665]
	TIME [epoch: 8.46 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583761645184397		[learning rate: 0.00011897]
	Learning Rate: 0.000118966
	LOSS [training: 2.583761645184397 | validation: 2.52907339939417]
	TIME [epoch: 8.43 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586609963908142		[learning rate: 0.00011853]
	Learning Rate: 0.000118534
	LOSS [training: 2.586609963908142 | validation: 2.520014275674786]
	TIME [epoch: 8.43 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5866444735259244		[learning rate: 0.0001181]
	Learning Rate: 0.000118104
	LOSS [training: 2.5866444735259244 | validation: 2.5291318668040477]
	TIME [epoch: 8.46 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585741555959178		[learning rate: 0.00011767]
	Learning Rate: 0.000117675
	LOSS [training: 2.585741555959178 | validation: 2.532821469955824]
	TIME [epoch: 8.49 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5792214179230633		[learning rate: 0.00011725]
	Learning Rate: 0.000117248
	LOSS [training: 2.5792214179230633 | validation: 2.5212190236888565]
	TIME [epoch: 8.43 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5875273906541154		[learning rate: 0.00011682]
	Learning Rate: 0.000116822
	LOSS [training: 2.5875273906541154 | validation: 2.541463363651929]
	TIME [epoch: 8.43 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5940159826481093		[learning rate: 0.0001164]
	Learning Rate: 0.000116398
	LOSS [training: 2.5940159826481093 | validation: 2.5423078902859775]
	TIME [epoch: 8.45 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588351695830744		[learning rate: 0.00011598]
	Learning Rate: 0.000115976
	LOSS [training: 2.588351695830744 | validation: 2.53512024821143]
	TIME [epoch: 8.44 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5826512691064436		[learning rate: 0.00011556]
	Learning Rate: 0.000115555
	LOSS [training: 2.5826512691064436 | validation: 2.5247237983424133]
	TIME [epoch: 8.43 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5893900921301265		[learning rate: 0.00011514]
	Learning Rate: 0.000115136
	LOSS [training: 2.5893900921301265 | validation: 2.523705143784323]
	TIME [epoch: 8.44 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5842773912612733		[learning rate: 0.00011472]
	Learning Rate: 0.000114718
	LOSS [training: 2.5842773912612733 | validation: 2.5181187854218483]
	TIME [epoch: 8.45 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5873174389278706		[learning rate: 0.0001143]
	Learning Rate: 0.000114302
	LOSS [training: 2.5873174389278706 | validation: 2.527676147730931]
	TIME [epoch: 8.44 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582945872636208		[learning rate: 0.00011389]
	Learning Rate: 0.000113887
	LOSS [training: 2.582945872636208 | validation: 2.514766991073895]
	TIME [epoch: 8.44 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5840187324172215		[learning rate: 0.00011347]
	Learning Rate: 0.000113474
	LOSS [training: 2.5840187324172215 | validation: 2.5215230864747813]
	TIME [epoch: 8.43 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5895174946020756		[learning rate: 0.00011306]
	Learning Rate: 0.000113062
	LOSS [training: 2.5895174946020756 | validation: 2.5317442968907784]
	TIME [epoch: 8.47 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588884513640881		[learning rate: 0.00011265]
	Learning Rate: 0.000112651
	LOSS [training: 2.588884513640881 | validation: 2.5320051602662943]
	TIME [epoch: 8.46 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.591493249357403		[learning rate: 0.00011224]
	Learning Rate: 0.000112243
	LOSS [training: 2.591493249357403 | validation: 2.533939701168487]
	TIME [epoch: 8.47 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5921207691046777		[learning rate: 0.00011184]
	Learning Rate: 0.000111835
	LOSS [training: 2.5921207691046777 | validation: 2.53560266410862]
	TIME [epoch: 8.44 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5923458666527854		[learning rate: 0.00011143]
	Learning Rate: 0.000111429
	LOSS [training: 2.5923458666527854 | validation: 2.533487699229755]
	TIME [epoch: 8.45 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589366662119963		[learning rate: 0.00011103]
	Learning Rate: 0.000111025
	LOSS [training: 2.589366662119963 | validation: 2.539904803481103]
	TIME [epoch: 8.44 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585957635091053		[learning rate: 0.00011062]
	Learning Rate: 0.000110622
	LOSS [training: 2.585957635091053 | validation: 2.5156327173951696]
	TIME [epoch: 8.44 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584463809835989		[learning rate: 0.00011022]
	Learning Rate: 0.000110221
	LOSS [training: 2.584463809835989 | validation: 2.526894428221927]
	TIME [epoch: 8.44 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5890040862292873		[learning rate: 0.00010982]
	Learning Rate: 0.000109821
	LOSS [training: 2.5890040862292873 | validation: 2.5313666974938993]
	TIME [epoch: 8.46 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5850115167554915		[learning rate: 0.00010942]
	Learning Rate: 0.000109422
	LOSS [training: 2.5850115167554915 | validation: 2.5370920312798413]
	TIME [epoch: 8.44 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583675307320535		[learning rate: 0.00010903]
	Learning Rate: 0.000109025
	LOSS [training: 2.583675307320535 | validation: 2.530580379814301]
	TIME [epoch: 8.44 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584295689450893		[learning rate: 0.00010863]
	Learning Rate: 0.000108629
	LOSS [training: 2.584295689450893 | validation: 2.533418411865675]
	TIME [epoch: 8.43 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5894306881938967		[learning rate: 0.00010824]
	Learning Rate: 0.000108235
	LOSS [training: 2.5894306881938967 | validation: 2.5251771783274006]
	TIME [epoch: 8.46 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5857019105436727		[learning rate: 0.00010784]
	Learning Rate: 0.000107842
	LOSS [training: 2.5857019105436727 | validation: 2.5291935233195915]
	TIME [epoch: 8.44 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587700824214145		[learning rate: 0.00010745]
	Learning Rate: 0.000107451
	LOSS [training: 2.587700824214145 | validation: 2.51765266698431]
	TIME [epoch: 8.45 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5858370499944536		[learning rate: 0.00010706]
	Learning Rate: 0.000107061
	LOSS [training: 2.5858370499944536 | validation: 2.521891861791447]
	TIME [epoch: 8.43 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5895333391067563		[learning rate: 0.00010667]
	Learning Rate: 0.000106673
	LOSS [training: 2.5895333391067563 | validation: 2.5333815947766642]
	TIME [epoch: 8.46 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589040531603019		[learning rate: 0.00010629]
	Learning Rate: 0.000106285
	LOSS [training: 2.589040531603019 | validation: 2.5346660054879653]
	TIME [epoch: 8.43 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5854881289763973		[learning rate: 0.0001059]
	Learning Rate: 0.0001059
	LOSS [training: 2.5854881289763973 | validation: 2.5264218996590913]
	TIME [epoch: 8.44 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5839394999763448		[learning rate: 0.00010552]
	Learning Rate: 0.000105515
	LOSS [training: 2.5839394999763448 | validation: 2.5216330432363763]
	TIME [epoch: 8.43 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5809061378199867		[learning rate: 0.00010513]
	Learning Rate: 0.000105132
	LOSS [training: 2.5809061378199867 | validation: 2.5308157300023337]
	TIME [epoch: 8.46 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583784436998804		[learning rate: 0.00010475]
	Learning Rate: 0.000104751
	LOSS [training: 2.583784436998804 | validation: 2.520963596261268]
	TIME [epoch: 8.43 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58782088711516		[learning rate: 0.00010437]
	Learning Rate: 0.000104371
	LOSS [training: 2.58782088711516 | validation: 2.524467002396342]
	TIME [epoch: 8.44 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5854934010617825		[learning rate: 0.00010399]
	Learning Rate: 0.000103992
	LOSS [training: 2.5854934010617825 | validation: 2.5275811944932522]
	TIME [epoch: 8.43 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5880789980663956		[learning rate: 0.00010361]
	Learning Rate: 0.000103615
	LOSS [training: 2.5880789980663956 | validation: 2.5331185318709135]
	TIME [epoch: 8.46 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.590508063596611		[learning rate: 0.00010324]
	Learning Rate: 0.000103239
	LOSS [training: 2.590508063596611 | validation: 2.519165365152677]
	TIME [epoch: 8.43 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587177101852046		[learning rate: 0.00010286]
	Learning Rate: 0.000102864
	LOSS [training: 2.587177101852046 | validation: 2.5365793912809744]
	TIME [epoch: 8.43 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.590557832847746		[learning rate: 0.00010249]
	Learning Rate: 0.000102491
	LOSS [training: 2.590557832847746 | validation: 2.5220762262516345]
	TIME [epoch: 8.43 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5867018706789087		[learning rate: 0.00010212]
	Learning Rate: 0.000102119
	LOSS [training: 2.5867018706789087 | validation: 2.5280626948327223]
	TIME [epoch: 8.46 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5886588185333688		[learning rate: 0.00010175]
	Learning Rate: 0.000101748
	LOSS [training: 2.5886588185333688 | validation: 2.5248872586135773]
	TIME [epoch: 8.47 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582854292044151		[learning rate: 0.00010138]
	Learning Rate: 0.000101379
	LOSS [training: 2.582854292044151 | validation: 2.528306050206881]
	TIME [epoch: 8.45 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5876435128772006		[learning rate: 0.00010101]
	Learning Rate: 0.000101011
	LOSS [training: 2.5876435128772006 | validation: 2.5465943870492485]
	TIME [epoch: 8.43 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.592952057845941		[learning rate: 0.00010064]
	Learning Rate: 0.000100644
	LOSS [training: 2.592952057845941 | validation: 2.5241583940287224]
	TIME [epoch: 8.45 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5862941255666314		[learning rate: 0.00010028]
	Learning Rate: 0.000100279
	LOSS [training: 2.5862941255666314 | validation: 2.5298534067252225]
	TIME [epoch: 8.43 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5828963178951705		[learning rate: 9.9915e-05]
	Learning Rate: 9.99152e-05
	LOSS [training: 2.5828963178951705 | validation: 2.5349300856575603]
	TIME [epoch: 8.48 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5874372227666553		[learning rate: 9.9553e-05]
	Learning Rate: 9.95526e-05
	LOSS [training: 2.5874372227666553 | validation: 2.528780614214406]
	TIME [epoch: 8.45 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5850278599588647		[learning rate: 9.9191e-05]
	Learning Rate: 9.91913e-05
	LOSS [training: 2.5850278599588647 | validation: 2.524108506605721]
	TIME [epoch: 8.44 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5841639297634202		[learning rate: 9.8831e-05]
	Learning Rate: 9.88314e-05
	LOSS [training: 2.5841639297634202 | validation: 2.520522693612478]
	TIME [epoch: 8.42 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589165033909704		[learning rate: 9.8473e-05]
	Learning Rate: 9.84727e-05
	LOSS [training: 2.589165033909704 | validation: 2.5299510411371466]
	TIME [epoch: 8.44 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5850109271243182		[learning rate: 9.8115e-05]
	Learning Rate: 9.81153e-05
	LOSS [training: 2.5850109271243182 | validation: 2.53380099443049]
	TIME [epoch: 8.44 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5850226814895705		[learning rate: 9.7759e-05]
	Learning Rate: 9.77592e-05
	LOSS [training: 2.5850226814895705 | validation: 2.541914625786725]
	TIME [epoch: 8.49 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58445156337548		[learning rate: 9.7404e-05]
	Learning Rate: 9.74045e-05
	LOSS [training: 2.58445156337548 | validation: 2.523989124427189]
	TIME [epoch: 8.45 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5822665987692077		[learning rate: 9.7051e-05]
	Learning Rate: 9.7051e-05
	LOSS [training: 2.5822665987692077 | validation: 2.526295885793279]
	TIME [epoch: 8.43 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585243358025453		[learning rate: 9.6699e-05]
	Learning Rate: 9.66988e-05
	LOSS [training: 2.585243358025453 | validation: 2.538131863220622]
	TIME [epoch: 8.44 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585789852038957		[learning rate: 9.6348e-05]
	Learning Rate: 9.63479e-05
	LOSS [training: 2.585789852038957 | validation: 2.52783268024014]
	TIME [epoch: 8.44 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584374887207743		[learning rate: 9.5998e-05]
	Learning Rate: 9.59982e-05
	LOSS [training: 2.584374887207743 | validation: 2.54975223682282]
	TIME [epoch: 8.43 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5936212816158926		[learning rate: 9.565e-05]
	Learning Rate: 9.56499e-05
	LOSS [training: 2.5936212816158926 | validation: 2.543917268746573]
	TIME [epoch: 8.44 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5853664924311155		[learning rate: 9.5303e-05]
	Learning Rate: 9.53027e-05
	LOSS [training: 2.5853664924311155 | validation: 2.5486743720400145]
	TIME [epoch: 8.45 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5845446863135852		[learning rate: 9.4957e-05]
	Learning Rate: 9.49569e-05
	LOSS [training: 2.5845446863135852 | validation: 2.52790569829694]
	TIME [epoch: 8.46 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585144030560561		[learning rate: 9.4612e-05]
	Learning Rate: 9.46122e-05
	LOSS [training: 2.585144030560561 | validation: 2.529632653149906]
	TIME [epoch: 8.47 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581187425603435		[learning rate: 9.4269e-05]
	Learning Rate: 9.42689e-05
	LOSS [training: 2.581187425603435 | validation: 2.5249246592197654]
	TIME [epoch: 8.42 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5859998499174153		[learning rate: 9.3927e-05]
	Learning Rate: 9.39268e-05
	LOSS [training: 2.5859998499174153 | validation: 2.529850598949862]
	TIME [epoch: 8.44 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5868611764462393		[learning rate: 9.3586e-05]
	Learning Rate: 9.35859e-05
	LOSS [training: 2.5868611764462393 | validation: 2.529432406129292]
	TIME [epoch: 8.43 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5955003509817294		[learning rate: 9.3246e-05]
	Learning Rate: 9.32463e-05
	LOSS [training: 2.5955003509817294 | validation: 2.528525530494293]
	TIME [epoch: 8.45 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5861590210814844		[learning rate: 9.2908e-05]
	Learning Rate: 9.29079e-05
	LOSS [training: 2.5861590210814844 | validation: 2.537651928686638]
	TIME [epoch: 8.43 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5849961605931524		[learning rate: 9.2571e-05]
	Learning Rate: 9.25707e-05
	LOSS [training: 2.5849961605931524 | validation: 2.5237754886457378]
	TIME [epoch: 8.45 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585514455651789		[learning rate: 9.2235e-05]
	Learning Rate: 9.22348e-05
	LOSS [training: 2.585514455651789 | validation: 2.5276661537165825]
	TIME [epoch: 8.43 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584476353432323		[learning rate: 9.19e-05]
	Learning Rate: 9.19001e-05
	LOSS [training: 2.584476353432323 | validation: 2.52373534587975]
	TIME [epoch: 8.44 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5875031080348028		[learning rate: 9.1567e-05]
	Learning Rate: 9.15665e-05
	LOSS [training: 2.5875031080348028 | validation: 2.5280392572611783]
	TIME [epoch: 8.44 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5853290227612615		[learning rate: 9.1234e-05]
	Learning Rate: 9.12343e-05
	LOSS [training: 2.5853290227612615 | validation: 2.5418058635332255]
	TIME [epoch: 8.47 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5826242207157417		[learning rate: 9.0903e-05]
	Learning Rate: 9.09031e-05
	LOSS [training: 2.5826242207157417 | validation: 2.533613525345335]
	TIME [epoch: 8.44 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58709208256108		[learning rate: 9.0573e-05]
	Learning Rate: 9.05733e-05
	LOSS [training: 2.58709208256108 | validation: 2.5358311219269005]
	TIME [epoch: 8.43 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5821954599574615		[learning rate: 9.0245e-05]
	Learning Rate: 9.02446e-05
	LOSS [training: 2.5821954599574615 | validation: 2.5289176769227772]
	TIME [epoch: 8.43 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5834979417362134		[learning rate: 8.9917e-05]
	Learning Rate: 8.99171e-05
	LOSS [training: 2.5834979417362134 | validation: 2.537963450017143]
	TIME [epoch: 8.45 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5867657328994555		[learning rate: 8.9591e-05]
	Learning Rate: 8.95908e-05
	LOSS [training: 2.5867657328994555 | validation: 2.5324120621964807]
	TIME [epoch: 8.43 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589720089104023		[learning rate: 8.9266e-05]
	Learning Rate: 8.92656e-05
	LOSS [training: 2.589720089104023 | validation: 2.5222511538441976]
	TIME [epoch: 8.44 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587742835457817		[learning rate: 8.8942e-05]
	Learning Rate: 8.89417e-05
	LOSS [training: 2.587742835457817 | validation: 2.5433017565349196]
	TIME [epoch: 8.43 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5841608409135914		[learning rate: 8.8619e-05]
	Learning Rate: 8.86189e-05
	LOSS [training: 2.5841608409135914 | validation: 2.522443763042559]
	TIME [epoch: 8.45 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5878157848068093		[learning rate: 8.8297e-05]
	Learning Rate: 8.82973e-05
	LOSS [training: 2.5878157848068093 | validation: 2.525788219499942]
	TIME [epoch: 8.43 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5827894687569724		[learning rate: 8.7977e-05]
	Learning Rate: 8.79769e-05
	LOSS [training: 2.5827894687569724 | validation: 2.5282268165651205]
	TIME [epoch: 8.46 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.591434161111782		[learning rate: 8.7658e-05]
	Learning Rate: 8.76576e-05
	LOSS [training: 2.591434161111782 | validation: 2.5376059000460858]
	TIME [epoch: 8.45 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5890250086373734		[learning rate: 8.7339e-05]
	Learning Rate: 8.73395e-05
	LOSS [training: 2.5890250086373734 | validation: 2.523410781354122]
	TIME [epoch: 8.45 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5831184545207755		[learning rate: 8.7023e-05]
	Learning Rate: 8.70225e-05
	LOSS [training: 2.5831184545207755 | validation: 2.527328472846045]
	TIME [epoch: 8.43 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5826803333197237		[learning rate: 8.6707e-05]
	Learning Rate: 8.67067e-05
	LOSS [training: 2.5826803333197237 | validation: 2.5357359088819376]
	TIME [epoch: 8.44 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5835278555536347		[learning rate: 8.6392e-05]
	Learning Rate: 8.63921e-05
	LOSS [training: 2.5835278555536347 | validation: 2.530671963990737]
	TIME [epoch: 8.43 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586422255511574		[learning rate: 8.6079e-05]
	Learning Rate: 8.60785e-05
	LOSS [training: 2.586422255511574 | validation: 2.544430849571329]
	TIME [epoch: 8.45 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581366714017228		[learning rate: 8.5766e-05]
	Learning Rate: 8.57661e-05
	LOSS [training: 2.581366714017228 | validation: 2.5275913447263734]
	TIME [epoch: 8.43 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5871964104661775		[learning rate: 8.5455e-05]
	Learning Rate: 8.54549e-05
	LOSS [training: 2.5871964104661775 | validation: 2.522960217120395]
	TIME [epoch: 8.43 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5843902766180364		[learning rate: 8.5145e-05]
	Learning Rate: 8.51448e-05
	LOSS [training: 2.5843902766180364 | validation: 2.525719851571689]
	TIME [epoch: 8.44 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5799519129862105		[learning rate: 8.4836e-05]
	Learning Rate: 8.48358e-05
	LOSS [training: 2.5799519129862105 | validation: 2.523971755023764]
	TIME [epoch: 8.46 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5843843037925103		[learning rate: 8.4528e-05]
	Learning Rate: 8.45279e-05
	LOSS [training: 2.5843843037925103 | validation: 2.5292033294227574]
	TIME [epoch: 8.44 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583151341158097		[learning rate: 8.4221e-05]
	Learning Rate: 8.42211e-05
	LOSS [training: 2.583151341158097 | validation: 2.5307727416930126]
	TIME [epoch: 8.44 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5865144757554304		[learning rate: 8.3916e-05]
	Learning Rate: 8.39155e-05
	LOSS [training: 2.5865144757554304 | validation: 2.5415864248094646]
	TIME [epoch: 8.44 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585236544867023		[learning rate: 8.3611e-05]
	Learning Rate: 8.3611e-05
	LOSS [training: 2.585236544867023 | validation: 2.5255691214176874]
	TIME [epoch: 8.45 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5859427338971464		[learning rate: 8.3308e-05]
	Learning Rate: 8.33075e-05
	LOSS [training: 2.5859427338971464 | validation: 2.5177596257743335]
	TIME [epoch: 8.44 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5856933643866986		[learning rate: 8.3005e-05]
	Learning Rate: 8.30052e-05
	LOSS [training: 2.5856933643866986 | validation: 2.5237419683247326]
	TIME [epoch: 8.43 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5817742613219674		[learning rate: 8.2704e-05]
	Learning Rate: 8.2704e-05
	LOSS [training: 2.5817742613219674 | validation: 2.5259304257296957]
	TIME [epoch: 8.44 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58544412137518		[learning rate: 8.2404e-05]
	Learning Rate: 8.24038e-05
	LOSS [training: 2.58544412137518 | validation: 2.531569557769113]
	TIME [epoch: 8.45 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5782489894969887		[learning rate: 8.2105e-05]
	Learning Rate: 8.21048e-05
	LOSS [training: 2.5782489894969887 | validation: 2.515291072151102]
	TIME [epoch: 8.44 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5857391844751696		[learning rate: 8.1807e-05]
	Learning Rate: 8.18068e-05
	LOSS [training: 2.5857391844751696 | validation: 2.528399740330996]
	TIME [epoch: 8.44 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5889434509862683		[learning rate: 8.151e-05]
	Learning Rate: 8.15099e-05
	LOSS [training: 2.5889434509862683 | validation: 2.52783276586825]
	TIME [epoch: 8.45 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579480295703668		[learning rate: 8.1214e-05]
	Learning Rate: 8.12142e-05
	LOSS [training: 2.579480295703668 | validation: 2.529913486642334]
	TIME [epoch: 8.44 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5905265042057555		[learning rate: 8.0919e-05]
	Learning Rate: 8.09194e-05
	LOSS [training: 2.5905265042057555 | validation: 2.533702715430959]
	TIME [epoch: 8.47 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583404005380635		[learning rate: 8.0626e-05]
	Learning Rate: 8.06257e-05
	LOSS [training: 2.583404005380635 | validation: 2.5339226941954403]
	TIME [epoch: 8.45 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587292229345519		[learning rate: 8.0333e-05]
	Learning Rate: 8.03331e-05
	LOSS [training: 2.587292229345519 | validation: 2.5301828072653993]
	TIME [epoch: 8.44 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5796201087704143		[learning rate: 8.0042e-05]
	Learning Rate: 8.00416e-05
	LOSS [training: 2.5796201087704143 | validation: 2.5231634227133286]
	TIME [epoch: 8.44 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585014359181467		[learning rate: 7.9751e-05]
	Learning Rate: 7.97511e-05
	LOSS [training: 2.585014359181467 | validation: 2.527031190051341]
	TIME [epoch: 8.45 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584005238072799		[learning rate: 7.9462e-05]
	Learning Rate: 7.94617e-05
	LOSS [training: 2.584005238072799 | validation: 2.5305188925024353]
	TIME [epoch: 8.48 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5874586598729827		[learning rate: 7.9173e-05]
	Learning Rate: 7.91733e-05
	LOSS [training: 2.5874586598729827 | validation: 2.5418805590144515]
	TIME [epoch: 8.45 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579235226367524		[learning rate: 7.8886e-05]
	Learning Rate: 7.8886e-05
	LOSS [training: 2.579235226367524 | validation: 2.528451783390665]
	TIME [epoch: 8.44 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583997593544862		[learning rate: 7.86e-05]
	Learning Rate: 7.85997e-05
	LOSS [training: 2.583997593544862 | validation: 2.5255187580880474]
	TIME [epoch: 8.43 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584732080598853		[learning rate: 7.8315e-05]
	Learning Rate: 7.83145e-05
	LOSS [training: 2.584732080598853 | validation: 2.526542384946481]
	TIME [epoch: 8.44 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586275236126254		[learning rate: 7.803e-05]
	Learning Rate: 7.80303e-05
	LOSS [training: 2.586275236126254 | validation: 2.5389211034496784]
	TIME [epoch: 8.46 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5854718570461515		[learning rate: 7.7747e-05]
	Learning Rate: 7.77471e-05
	LOSS [training: 2.5854718570461515 | validation: 2.546715999523583]
	TIME [epoch: 8.45 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5825220186835436		[learning rate: 7.7465e-05]
	Learning Rate: 7.7465e-05
	LOSS [training: 2.5825220186835436 | validation: 2.5193487015128566]
	TIME [epoch: 8.48 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583665005389615		[learning rate: 7.7184e-05]
	Learning Rate: 7.71838e-05
	LOSS [training: 2.583665005389615 | validation: 2.5268710679092736]
	TIME [epoch: 8.42 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5846179529745594		[learning rate: 7.6904e-05]
	Learning Rate: 7.69037e-05
	LOSS [training: 2.5846179529745594 | validation: 2.5198842136934774]
	TIME [epoch: 8.45 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5814901556080256		[learning rate: 7.6625e-05]
	Learning Rate: 7.66246e-05
	LOSS [training: 2.5814901556080256 | validation: 2.5362978464646613]
	TIME [epoch: 8.43 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5825509322179734		[learning rate: 7.6347e-05]
	Learning Rate: 7.63466e-05
	LOSS [training: 2.5825509322179734 | validation: 2.51981293212054]
	TIME [epoch: 8.44 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5908543168558165		[learning rate: 7.607e-05]
	Learning Rate: 7.60695e-05
	LOSS [training: 2.5908543168558165 | validation: 2.5387953434615387]
	TIME [epoch: 8.45 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586232302124462		[learning rate: 7.5793e-05]
	Learning Rate: 7.57935e-05
	LOSS [training: 2.586232302124462 | validation: 2.5309846058967684]
	TIME [epoch: 8.45 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582982471440232		[learning rate: 7.5518e-05]
	Learning Rate: 7.55184e-05
	LOSS [training: 2.582982471440232 | validation: 2.523442193412013]
	TIME [epoch: 8.45 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5846511517120954		[learning rate: 7.5244e-05]
	Learning Rate: 7.52443e-05
	LOSS [training: 2.5846511517120954 | validation: 2.539179550714435]
	TIME [epoch: 8.42 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587883642003501		[learning rate: 7.4971e-05]
	Learning Rate: 7.49712e-05
	LOSS [training: 2.587883642003501 | validation: 2.5440407578125814]
	TIME [epoch: 8.44 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5838560840077003		[learning rate: 7.4699e-05]
	Learning Rate: 7.46992e-05
	LOSS [training: 2.5838560840077003 | validation: 2.529639029940576]
	TIME [epoch: 8.43 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5874139972901236		[learning rate: 7.4428e-05]
	Learning Rate: 7.44281e-05
	LOSS [training: 2.5874139972901236 | validation: 2.5147910001515448]
	TIME [epoch: 8.45 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58480688202577		[learning rate: 7.4158e-05]
	Learning Rate: 7.4158e-05
	LOSS [training: 2.58480688202577 | validation: 2.5246983405448726]
	TIME [epoch: 8.47 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58743862366917		[learning rate: 7.3889e-05]
	Learning Rate: 7.38889e-05
	LOSS [training: 2.58743862366917 | validation: 2.522829535960282]
	TIME [epoch: 8.45 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5847799562943035		[learning rate: 7.3621e-05]
	Learning Rate: 7.36207e-05
	LOSS [training: 2.5847799562943035 | validation: 2.522380039840317]
	TIME [epoch: 8.44 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5853822489834046		[learning rate: 7.3354e-05]
	Learning Rate: 7.33536e-05
	LOSS [training: 2.5853822489834046 | validation: 2.5342009604954443]
	TIME [epoch: 8.44 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5851712989876363		[learning rate: 7.3087e-05]
	Learning Rate: 7.30873e-05
	LOSS [training: 2.5851712989876363 | validation: 2.531460205593338]
	TIME [epoch: 8.43 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5826641032410618		[learning rate: 7.2822e-05]
	Learning Rate: 7.28221e-05
	LOSS [training: 2.5826641032410618 | validation: 2.5269869637608835]
	TIME [epoch: 8.43 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583141586477239		[learning rate: 7.2558e-05]
	Learning Rate: 7.25578e-05
	LOSS [training: 2.583141586477239 | validation: 2.5320346916840677]
	TIME [epoch: 8.45 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584032989030237		[learning rate: 7.2295e-05]
	Learning Rate: 7.22945e-05
	LOSS [training: 2.584032989030237 | validation: 2.5345982879856077]
	TIME [epoch: 8.44 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585780343717766		[learning rate: 7.2032e-05]
	Learning Rate: 7.20321e-05
	LOSS [training: 2.585780343717766 | validation: 2.5361194619098786]
	TIME [epoch: 8.43 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586477695343314		[learning rate: 7.1771e-05]
	Learning Rate: 7.17707e-05
	LOSS [training: 2.586477695343314 | validation: 2.5316277406382115]
	TIME [epoch: 8.44 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587197996340244		[learning rate: 7.151e-05]
	Learning Rate: 7.15103e-05
	LOSS [training: 2.587197996340244 | validation: 2.5369727068300505]
	TIME [epoch: 8.46 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5874512416218183		[learning rate: 7.1251e-05]
	Learning Rate: 7.12508e-05
	LOSS [training: 2.5874512416218183 | validation: 2.534796717890666]
	TIME [epoch: 8.43 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5862749982600355		[learning rate: 7.0992e-05]
	Learning Rate: 7.09922e-05
	LOSS [training: 2.5862749982600355 | validation: 2.5367326507812358]
	TIME [epoch: 8.47 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582850680904559		[learning rate: 7.0735e-05]
	Learning Rate: 7.07345e-05
	LOSS [training: 2.582850680904559 | validation: 2.524168682226092]
	TIME [epoch: 8.45 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5787046841912398		[learning rate: 7.0478e-05]
	Learning Rate: 7.04778e-05
	LOSS [training: 2.5787046841912398 | validation: 2.5297700469833515]
	TIME [epoch: 8.46 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5841264880948205		[learning rate: 7.0222e-05]
	Learning Rate: 7.02221e-05
	LOSS [training: 2.5841264880948205 | validation: 2.522100505135299]
	TIME [epoch: 8.43 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582554242177726		[learning rate: 6.9967e-05]
	Learning Rate: 6.99672e-05
	LOSS [training: 2.582554242177726 | validation: 2.5302026758445333]
	TIME [epoch: 8.44 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5922466944082663		[learning rate: 6.9713e-05]
	Learning Rate: 6.97133e-05
	LOSS [training: 2.5922466944082663 | validation: 2.528661652707215]
	TIME [epoch: 8.43 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5863094848114834		[learning rate: 6.946e-05]
	Learning Rate: 6.94603e-05
	LOSS [training: 2.5863094848114834 | validation: 2.524529221150769]
	TIME [epoch: 8.46 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5895226801345705		[learning rate: 6.9208e-05]
	Learning Rate: 6.92083e-05
	LOSS [training: 2.5895226801345705 | validation: 2.527450731973982]
	TIME [epoch: 8.43 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5901087085848067		[learning rate: 6.8957e-05]
	Learning Rate: 6.89571e-05
	LOSS [training: 2.5901087085848067 | validation: 2.5338606451168193]
	TIME [epoch: 8.43 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5841484729977915		[learning rate: 6.8707e-05]
	Learning Rate: 6.87068e-05
	LOSS [training: 2.5841484729977915 | validation: 2.5281415823890994]
	TIME [epoch: 8.45 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586042246087035		[learning rate: 6.8457e-05]
	Learning Rate: 6.84575e-05
	LOSS [training: 2.586042246087035 | validation: 2.5226685604082313]
	TIME [epoch: 8.44 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5819093006568385		[learning rate: 6.8209e-05]
	Learning Rate: 6.82091e-05
	LOSS [training: 2.5819093006568385 | validation: 2.5300283042251586]
	TIME [epoch: 8.44 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5842413773094473		[learning rate: 6.7962e-05]
	Learning Rate: 6.79615e-05
	LOSS [training: 2.5842413773094473 | validation: 2.5280694198818825]
	TIME [epoch: 8.44 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5791597202977785		[learning rate: 6.7715e-05]
	Learning Rate: 6.77149e-05
	LOSS [training: 2.5791597202977785 | validation: 2.525570351388095]
	TIME [epoch: 8.44 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5840967246880533		[learning rate: 6.7469e-05]
	Learning Rate: 6.74692e-05
	LOSS [training: 2.5840967246880533 | validation: 2.5225774366841884]
	TIME [epoch: 8.44 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588696929911494		[learning rate: 6.7224e-05]
	Learning Rate: 6.72243e-05
	LOSS [training: 2.588696929911494 | validation: 2.5333889191486083]
	TIME [epoch: 8.43 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582232597312217		[learning rate: 6.698e-05]
	Learning Rate: 6.69804e-05
	LOSS [training: 2.582232597312217 | validation: 2.5345987551064297]
	TIME [epoch: 8.44 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5848620486120955		[learning rate: 6.6737e-05]
	Learning Rate: 6.67373e-05
	LOSS [training: 2.5848620486120955 | validation: 2.531835941332064]
	TIME [epoch: 8.47 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5853630966594894		[learning rate: 6.6495e-05]
	Learning Rate: 6.64951e-05
	LOSS [training: 2.5853630966594894 | validation: 2.5137753674155356]
	TIME [epoch: 8.44 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581883659897397		[learning rate: 6.6254e-05]
	Learning Rate: 6.62538e-05
	LOSS [training: 2.581883659897397 | validation: 2.5251028215203606]
	TIME [epoch: 8.44 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587625101382353		[learning rate: 6.6013e-05]
	Learning Rate: 6.60133e-05
	LOSS [training: 2.587625101382353 | validation: 2.52231466149729]
	TIME [epoch: 8.43 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585964687948594		[learning rate: 6.5774e-05]
	Learning Rate: 6.57738e-05
	LOSS [training: 2.585964687948594 | validation: 2.5350883304239513]
	TIME [epoch: 8.47 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5859326954958197		[learning rate: 6.5535e-05]
	Learning Rate: 6.55351e-05
	LOSS [training: 2.5859326954958197 | validation: 2.535573317596586]
	TIME [epoch: 8.43 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5828115656910753		[learning rate: 6.5297e-05]
	Learning Rate: 6.52972e-05
	LOSS [training: 2.5828115656910753 | validation: 2.520277041187115]
	TIME [epoch: 8.46 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5837982915663527		[learning rate: 6.506e-05]
	Learning Rate: 6.50603e-05
	LOSS [training: 2.5837982915663527 | validation: 2.5238768604637514]
	TIME [epoch: 8.46 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5844746944584744		[learning rate: 6.4824e-05]
	Learning Rate: 6.48242e-05
	LOSS [training: 2.5844746944584744 | validation: 2.5262224459061606]
	TIME [epoch: 8.45 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5825628175356248		[learning rate: 6.4589e-05]
	Learning Rate: 6.45889e-05
	LOSS [training: 2.5825628175356248 | validation: 2.52519839548592]
	TIME [epoch: 8.43 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58553102449811		[learning rate: 6.4354e-05]
	Learning Rate: 6.43545e-05
	LOSS [training: 2.58553102449811 | validation: 2.521808268297778]
	TIME [epoch: 8.43 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5848867227362997		[learning rate: 6.4121e-05]
	Learning Rate: 6.4121e-05
	LOSS [training: 2.5848867227362997 | validation: 2.5244174918697038]
	TIME [epoch: 8.47 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585554445014187		[learning rate: 6.3888e-05]
	Learning Rate: 6.38883e-05
	LOSS [training: 2.585554445014187 | validation: 2.530313087858599]
	TIME [epoch: 8.47 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5839238206660946		[learning rate: 6.3656e-05]
	Learning Rate: 6.36564e-05
	LOSS [training: 2.5839238206660946 | validation: 2.5356060351039282]
	TIME [epoch: 8.44 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5879896695834086		[learning rate: 6.3425e-05]
	Learning Rate: 6.34254e-05
	LOSS [training: 2.5879896695834086 | validation: 2.5388677152199834]
	TIME [epoch: 8.42 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5884445648027095		[learning rate: 6.3195e-05]
	Learning Rate: 6.31952e-05
	LOSS [training: 2.5884445648027095 | validation: 2.5235239358761574]
	TIME [epoch: 8.44 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5810157016306348		[learning rate: 6.2966e-05]
	Learning Rate: 6.29659e-05
	LOSS [training: 2.5810157016306348 | validation: 2.526339589172535]
	TIME [epoch: 8.45 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583737453937466		[learning rate: 6.2737e-05]
	Learning Rate: 6.27374e-05
	LOSS [training: 2.583737453937466 | validation: 2.534937873388715]
	TIME [epoch: 8.43 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586449270785252		[learning rate: 6.251e-05]
	Learning Rate: 6.25097e-05
	LOSS [training: 2.586449270785252 | validation: 2.5202805817996756]
	TIME [epoch: 8.43 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583345538066408		[learning rate: 6.2283e-05]
	Learning Rate: 6.22828e-05
	LOSS [training: 2.583345538066408 | validation: 2.51423154669187]
	TIME [epoch: 8.43 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583130692729731		[learning rate: 6.2057e-05]
	Learning Rate: 6.20568e-05
	LOSS [training: 2.583130692729731 | validation: 2.531450588863133]
	TIME [epoch: 8.49 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5811721172302384		[learning rate: 6.1832e-05]
	Learning Rate: 6.18316e-05
	LOSS [training: 2.5811721172302384 | validation: 2.528669715137395]
	TIME [epoch: 8.46 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588648936404275		[learning rate: 6.1607e-05]
	Learning Rate: 6.16072e-05
	LOSS [training: 2.588648936404275 | validation: 2.5302914350308874]
	TIME [epoch: 8.43 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5879404963459463		[learning rate: 6.1384e-05]
	Learning Rate: 6.13836e-05
	LOSS [training: 2.5879404963459463 | validation: 2.54201784915704]
	TIME [epoch: 8.43 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58578997172187		[learning rate: 6.1161e-05]
	Learning Rate: 6.11609e-05
	LOSS [training: 2.58578997172187 | validation: 2.5320751666122057]
	TIME [epoch: 8.45 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5829923335921263		[learning rate: 6.0939e-05]
	Learning Rate: 6.09389e-05
	LOSS [training: 2.5829923335921263 | validation: 2.525270316150514]
	TIME [epoch: 8.45 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5831546578272397		[learning rate: 6.0718e-05]
	Learning Rate: 6.07178e-05
	LOSS [training: 2.5831546578272397 | validation: 2.5305329257729396]
	TIME [epoch: 8.42 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5894775109796853		[learning rate: 6.0497e-05]
	Learning Rate: 6.04974e-05
	LOSS [training: 2.5894775109796853 | validation: 2.5291535017585396]
	TIME [epoch: 8.44 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5807881895241067		[learning rate: 6.0278e-05]
	Learning Rate: 6.02779e-05
	LOSS [training: 2.5807881895241067 | validation: 2.530415123849693]
	TIME [epoch: 8.47 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586516889969001		[learning rate: 6.0059e-05]
	Learning Rate: 6.00591e-05
	LOSS [training: 2.586516889969001 | validation: 2.528938920716736]
	TIME [epoch: 8.47 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589795174386009		[learning rate: 5.9841e-05]
	Learning Rate: 5.98412e-05
	LOSS [training: 2.589795174386009 | validation: 2.528596071076949]
	TIME [epoch: 8.43 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58787757129096		[learning rate: 5.9624e-05]
	Learning Rate: 5.9624e-05
	LOSS [training: 2.58787757129096 | validation: 2.52181785045007]
	TIME [epoch: 8.42 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5814524912156323		[learning rate: 5.9408e-05]
	Learning Rate: 5.94076e-05
	LOSS [training: 2.5814524912156323 | validation: 2.5268130246497136]
	TIME [epoch: 8.46 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580587222294349		[learning rate: 5.9192e-05]
	Learning Rate: 5.9192e-05
	LOSS [training: 2.580587222294349 | validation: 2.528572219531571]
	TIME [epoch: 8.43 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585315885192366		[learning rate: 5.8977e-05]
	Learning Rate: 5.89772e-05
	LOSS [training: 2.585315885192366 | validation: 2.5227015780032804]
	TIME [epoch: 8.44 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5849149245798784		[learning rate: 5.8763e-05]
	Learning Rate: 5.87632e-05
	LOSS [training: 2.5849149245798784 | validation: 2.5329196640338214]
	TIME [epoch: 8.44 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5812787988650583		[learning rate: 5.855e-05]
	Learning Rate: 5.85499e-05
	LOSS [training: 2.5812787988650583 | validation: 2.530489993240919]
	TIME [epoch: 8.46 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583715138046059		[learning rate: 5.8337e-05]
	Learning Rate: 5.83374e-05
	LOSS [training: 2.583715138046059 | validation: 2.533732235682554]
	TIME [epoch: 8.44 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584337526399802		[learning rate: 5.8126e-05]
	Learning Rate: 5.81257e-05
	LOSS [training: 2.584337526399802 | validation: 2.524015695097317]
	TIME [epoch: 8.44 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581865909509963		[learning rate: 5.7915e-05]
	Learning Rate: 5.79148e-05
	LOSS [training: 2.581865909509963 | validation: 2.5151186039417026]
	TIME [epoch: 8.45 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5863695497404864		[learning rate: 5.7705e-05]
	Learning Rate: 5.77046e-05
	LOSS [training: 2.5863695497404864 | validation: 2.541547413916144]
	TIME [epoch: 8.46 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587464855340126		[learning rate: 5.7495e-05]
	Learning Rate: 5.74952e-05
	LOSS [training: 2.587464855340126 | validation: 2.525039838858504]
	TIME [epoch: 8.44 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58325187977644		[learning rate: 5.7287e-05]
	Learning Rate: 5.72866e-05
	LOSS [training: 2.58325187977644 | validation: 2.5188435933883397]
	TIME [epoch: 8.43 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5817987862226905		[learning rate: 5.7079e-05]
	Learning Rate: 5.70787e-05
	LOSS [training: 2.5817987862226905 | validation: 2.5238856686408946]
	TIME [epoch: 8.46 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5825276881219428		[learning rate: 5.6872e-05]
	Learning Rate: 5.68715e-05
	LOSS [training: 2.5825276881219428 | validation: 2.5189638241188126]
	TIME [epoch: 8.45 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.577364038709142		[learning rate: 5.6665e-05]
	Learning Rate: 5.66651e-05
	LOSS [training: 2.577364038709142 | validation: 2.5334321403100395]
	TIME [epoch: 8.48 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5816280275776764		[learning rate: 5.6459e-05]
	Learning Rate: 5.64595e-05
	LOSS [training: 2.5816280275776764 | validation: 2.529067859775365]
	TIME [epoch: 8.46 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5810800793460658		[learning rate: 5.6255e-05]
	Learning Rate: 5.62546e-05
	LOSS [training: 2.5810800793460658 | validation: 2.5224225046483086]
	TIME [epoch: 8.45 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5820947786111237		[learning rate: 5.605e-05]
	Learning Rate: 5.60504e-05
	LOSS [training: 2.5820947786111237 | validation: 2.5211209789200515]
	TIME [epoch: 8.44 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5829599757850685		[learning rate: 5.5847e-05]
	Learning Rate: 5.5847e-05
	LOSS [training: 2.5829599757850685 | validation: 2.5263947158743028]
	TIME [epoch: 8.43 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5821220870157306		[learning rate: 5.5644e-05]
	Learning Rate: 5.56444e-05
	LOSS [training: 2.5821220870157306 | validation: 2.5283387199110283]
	TIME [epoch: 8.43 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579624516901371		[learning rate: 5.5442e-05]
	Learning Rate: 5.54424e-05
	LOSS [training: 2.579624516901371 | validation: 2.5239167949501393]
	TIME [epoch: 8.46 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5880203732955165		[learning rate: 5.5241e-05]
	Learning Rate: 5.52412e-05
	LOSS [training: 2.5880203732955165 | validation: 2.5244558877664853]
	TIME [epoch: 8.44 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582106170597654		[learning rate: 5.5041e-05]
	Learning Rate: 5.50407e-05
	LOSS [training: 2.582106170597654 | validation: 2.528714118866594]
	TIME [epoch: 8.44 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583761951486257		[learning rate: 5.4841e-05]
	Learning Rate: 5.4841e-05
	LOSS [training: 2.583761951486257 | validation: 2.5296580507102675]
	TIME [epoch: 8.43 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5805621519951076		[learning rate: 5.4642e-05]
	Learning Rate: 5.4642e-05
	LOSS [training: 2.5805621519951076 | validation: 2.5301819083564236]
	TIME [epoch: 8.46 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5815553296582427		[learning rate: 5.4444e-05]
	Learning Rate: 5.44437e-05
	LOSS [training: 2.5815553296582427 | validation: 2.529032802457559]
	TIME [epoch: 8.43 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5827371106727424		[learning rate: 5.4246e-05]
	Learning Rate: 5.42461e-05
	LOSS [training: 2.5827371106727424 | validation: 2.537949823421326]
	TIME [epoch: 8.44 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585802876926937		[learning rate: 5.4049e-05]
	Learning Rate: 5.40492e-05
	LOSS [training: 2.585802876926937 | validation: 2.5402698697547694]
	TIME [epoch: 8.44 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5848980436630056		[learning rate: 5.3853e-05]
	Learning Rate: 5.38531e-05
	LOSS [training: 2.5848980436630056 | validation: 2.5286759069629947]
	TIME [epoch: 8.46 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5848206138492453		[learning rate: 5.3658e-05]
	Learning Rate: 5.36577e-05
	LOSS [training: 2.5848206138492453 | validation: 2.5321546314243846]
	TIME [epoch: 8.43 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585158274254416		[learning rate: 5.3463e-05]
	Learning Rate: 5.34629e-05
	LOSS [training: 2.585158274254416 | validation: 2.5257297290878835]
	TIME [epoch: 8.44 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584434794346091		[learning rate: 5.3269e-05]
	Learning Rate: 5.32689e-05
	LOSS [training: 2.584434794346091 | validation: 2.536699151285095]
	TIME [epoch: 8.43 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581264082617134		[learning rate: 5.3076e-05]
	Learning Rate: 5.30756e-05
	LOSS [training: 2.581264082617134 | validation: 2.52764058766296]
	TIME [epoch: 8.46 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5842192404806346		[learning rate: 5.2883e-05]
	Learning Rate: 5.2883e-05
	LOSS [training: 2.5842192404806346 | validation: 2.524482576875516]
	TIME [epoch: 8.43 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5844258322810747		[learning rate: 5.2691e-05]
	Learning Rate: 5.26911e-05
	LOSS [training: 2.5844258322810747 | validation: 2.5256577720438087]
	TIME [epoch: 8.47 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583541036401323		[learning rate: 5.25e-05]
	Learning Rate: 5.24998e-05
	LOSS [training: 2.583541036401323 | validation: 2.5224370831810177]
	TIME [epoch: 8.45 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581434792912091		[learning rate: 5.2309e-05]
	Learning Rate: 5.23093e-05
	LOSS [training: 2.581434792912091 | validation: 2.5325110537473776]
	TIME [epoch: 8.44 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578903974301642		[learning rate: 5.2119e-05]
	Learning Rate: 5.21195e-05
	LOSS [training: 2.578903974301642 | validation: 2.523183578060939]
	TIME [epoch: 8.45 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5798270223930215		[learning rate: 5.193e-05]
	Learning Rate: 5.19303e-05
	LOSS [training: 2.5798270223930215 | validation: 2.539672343674559]
	TIME [epoch: 8.47 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5821519979859504		[learning rate: 5.1742e-05]
	Learning Rate: 5.17419e-05
	LOSS [training: 2.5821519979859504 | validation: 2.5164088132726983]
	TIME [epoch: 8.45 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581255459176515		[learning rate: 5.1554e-05]
	Learning Rate: 5.15541e-05
	LOSS [training: 2.581255459176515 | validation: 2.526659332423949]
	TIME [epoch: 8.45 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579652054548067		[learning rate: 5.1367e-05]
	Learning Rate: 5.1367e-05
	LOSS [training: 2.579652054548067 | validation: 2.5300895229328364]
	TIME [epoch: 8.43 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582302371380367		[learning rate: 5.1181e-05]
	Learning Rate: 5.11806e-05
	LOSS [training: 2.582302371380367 | validation: 2.5152813707497557]
	TIME [epoch: 8.44 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58497777849497		[learning rate: 5.0995e-05]
	Learning Rate: 5.09949e-05
	LOSS [training: 2.58497777849497 | validation: 2.5203091816269305]
	TIME [epoch: 8.45 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5838641510929294		[learning rate: 5.081e-05]
	Learning Rate: 5.08098e-05
	LOSS [training: 2.5838641510929294 | validation: 2.5250497617412755]
	TIME [epoch: 8.49 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581756614264152		[learning rate: 5.0625e-05]
	Learning Rate: 5.06254e-05
	LOSS [training: 2.581756614264152 | validation: 2.5236304663882043]
	TIME [epoch: 8.42 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580344547627613		[learning rate: 5.0442e-05]
	Learning Rate: 5.04417e-05
	LOSS [training: 2.580344547627613 | validation: 2.523526190834701]
	TIME [epoch: 8.43 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5857607732287358		[learning rate: 5.0259e-05]
	Learning Rate: 5.02586e-05
	LOSS [training: 2.5857607732287358 | validation: 2.533020984792583]
	TIME [epoch: 8.43 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5831405542025987		[learning rate: 5.0076e-05]
	Learning Rate: 5.00762e-05
	LOSS [training: 2.5831405542025987 | validation: 2.5331034038832727]
	TIME [epoch: 8.46 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5818730295698313		[learning rate: 4.9894e-05]
	Learning Rate: 4.98945e-05
	LOSS [training: 2.5818730295698313 | validation: 2.528893704330616]
	TIME [epoch: 8.46 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584710025732126		[learning rate: 4.9713e-05]
	Learning Rate: 4.97134e-05
	LOSS [training: 2.584710025732126 | validation: 2.536067549864158]
	TIME [epoch: 8.47 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584528136861374		[learning rate: 4.9533e-05]
	Learning Rate: 4.9533e-05
	LOSS [training: 2.584528136861374 | validation: 2.5302099940887226]
	TIME [epoch: 8.44 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5861167211398692		[learning rate: 4.9353e-05]
	Learning Rate: 4.93533e-05
	LOSS [training: 2.5861167211398692 | validation: 2.526076064891984]
	TIME [epoch: 8.45 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5859027287597725		[learning rate: 4.9174e-05]
	Learning Rate: 4.91742e-05
	LOSS [training: 2.5859027287597725 | validation: 2.5348696231577215]
	TIME [epoch: 8.43 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5842037609441015		[learning rate: 4.8996e-05]
	Learning Rate: 4.89957e-05
	LOSS [training: 2.5842037609441015 | validation: 2.5208973179521577]
	TIME [epoch: 8.43 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583321461101886		[learning rate: 4.8818e-05]
	Learning Rate: 4.88179e-05
	LOSS [training: 2.583321461101886 | validation: 2.527279025756161]
	TIME [epoch: 8.44 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5816456128342016		[learning rate: 4.8641e-05]
	Learning Rate: 4.86407e-05
	LOSS [training: 2.5816456128342016 | validation: 2.533652039553923]
	TIME [epoch: 8.45 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584234680383057		[learning rate: 4.8464e-05]
	Learning Rate: 4.84642e-05
	LOSS [training: 2.584234680383057 | validation: 2.5200239843081502]
	TIME [epoch: 8.44 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582631606924576		[learning rate: 4.8288e-05]
	Learning Rate: 4.82883e-05
	LOSS [training: 2.582631606924576 | validation: 2.5217605017070186]
	TIME [epoch: 8.43 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583002969327743		[learning rate: 4.8113e-05]
	Learning Rate: 4.81131e-05
	LOSS [training: 2.583002969327743 | validation: 2.527649318713409]
	TIME [epoch: 8.46 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5855027364699774		[learning rate: 4.7938e-05]
	Learning Rate: 4.79385e-05
	LOSS [training: 2.5855027364699774 | validation: 2.528026817707212]
	TIME [epoch: 8.45 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5811387641590837		[learning rate: 4.7765e-05]
	Learning Rate: 4.77645e-05
	LOSS [training: 2.5811387641590837 | validation: 2.5246026553863423]
	TIME [epoch: 8.43 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582427065910769		[learning rate: 4.7591e-05]
	Learning Rate: 4.75912e-05
	LOSS [training: 2.582427065910769 | validation: 2.5331961237703364]
	TIME [epoch: 8.44 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581717854178738		[learning rate: 4.7418e-05]
	Learning Rate: 4.74185e-05
	LOSS [training: 2.581717854178738 | validation: 2.5311754373385735]
	TIME [epoch: 8.45 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583669118202222		[learning rate: 4.7246e-05]
	Learning Rate: 4.72464e-05
	LOSS [training: 2.583669118202222 | validation: 2.5369607140319057]
	TIME [epoch: 8.44 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585709655582917		[learning rate: 4.7075e-05]
	Learning Rate: 4.70749e-05
	LOSS [training: 2.585709655582917 | validation: 2.527426615767289]
	TIME [epoch: 8.43 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584425449528289		[learning rate: 4.6904e-05]
	Learning Rate: 4.69041e-05
	LOSS [training: 2.584425449528289 | validation: 2.5230476149133967]
	TIME [epoch: 8.44 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582016878763342		[learning rate: 4.6734e-05]
	Learning Rate: 4.67339e-05
	LOSS [training: 2.582016878763342 | validation: 2.5236574682930835]
	TIME [epoch: 8.46 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5834849953539774		[learning rate: 4.6564e-05]
	Learning Rate: 4.65643e-05
	LOSS [training: 2.5834849953539774 | validation: 2.5268410979665026]
	TIME [epoch: 8.47 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5849659422817224		[learning rate: 4.6395e-05]
	Learning Rate: 4.63953e-05
	LOSS [training: 2.5849659422817224 | validation: 2.5234777958887387]
	TIME [epoch: 8.45 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5849337151953025		[learning rate: 4.6227e-05]
	Learning Rate: 4.62269e-05
	LOSS [training: 2.5849337151953025 | validation: 2.5108085240733518]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study203/model_tr_study203_r3_20240219_195621/states/model_tr_study203_1579.pth
	Model improved!!!
EPOCH 1580/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582940619021353		[learning rate: 4.6059e-05]
	Learning Rate: 4.60591e-05
	LOSS [training: 2.582940619021353 | validation: 2.5246406919784983]
	TIME [epoch: 8.45 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5838217692947714		[learning rate: 4.5892e-05]
	Learning Rate: 4.5892e-05
	LOSS [training: 2.5838217692947714 | validation: 2.536855791041952]
	TIME [epoch: 8.42 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5853470765204736		[learning rate: 4.5725e-05]
	Learning Rate: 4.57254e-05
	LOSS [training: 2.5853470765204736 | validation: 2.530343963362273]
	TIME [epoch: 8.43 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581337289624099		[learning rate: 4.556e-05]
	Learning Rate: 4.55595e-05
	LOSS [training: 2.581337289624099 | validation: 2.530432771410606]
	TIME [epoch: 8.42 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579232520638075		[learning rate: 4.5394e-05]
	Learning Rate: 4.53942e-05
	LOSS [training: 2.579232520638075 | validation: 2.5272751154691866]
	TIME [epoch: 8.44 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5822180694503936		[learning rate: 4.5229e-05]
	Learning Rate: 4.52294e-05
	LOSS [training: 2.5822180694503936 | validation: 2.525087156608639]
	TIME [epoch: 8.44 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5891601189016082		[learning rate: 4.5065e-05]
	Learning Rate: 4.50653e-05
	LOSS [training: 2.5891601189016082 | validation: 2.524978428903597]
	TIME [epoch: 8.43 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5818423667418378		[learning rate: 4.4902e-05]
	Learning Rate: 4.49017e-05
	LOSS [training: 2.5818423667418378 | validation: 2.5308752131261567]
	TIME [epoch: 8.43 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5820005389733858		[learning rate: 4.4739e-05]
	Learning Rate: 4.47388e-05
	LOSS [training: 2.5820005389733858 | validation: 2.5314008722827337]
	TIME [epoch: 8.44 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585975389720923		[learning rate: 4.4576e-05]
	Learning Rate: 4.45764e-05
	LOSS [training: 2.585975389720923 | validation: 2.5390096202554284]
	TIME [epoch: 8.43 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5849593614486674		[learning rate: 4.4415e-05]
	Learning Rate: 4.44147e-05
	LOSS [training: 2.5849593614486674 | validation: 2.528968177641155]
	TIME [epoch: 8.43 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5799742098071774		[learning rate: 4.4253e-05]
	Learning Rate: 4.42535e-05
	LOSS [training: 2.5799742098071774 | validation: 2.5246851296041593]
	TIME [epoch: 8.42 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581842723632031		[learning rate: 4.4093e-05]
	Learning Rate: 4.40929e-05
	LOSS [training: 2.581842723632031 | validation: 2.525679308218016]
	TIME [epoch: 8.45 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584237636567029		[learning rate: 4.3933e-05]
	Learning Rate: 4.39329e-05
	LOSS [training: 2.584237636567029 | validation: 2.534116859592279]
	TIME [epoch: 8.43 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583842472125604		[learning rate: 4.3773e-05]
	Learning Rate: 4.37734e-05
	LOSS [training: 2.583842472125604 | validation: 2.518496084562442]
	TIME [epoch: 8.43 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5815951978336376		[learning rate: 4.3615e-05]
	Learning Rate: 4.36146e-05
	LOSS [training: 2.5815951978336376 | validation: 2.520166745317892]
	TIME [epoch: 8.43 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5840301039072457		[learning rate: 4.3456e-05]
	Learning Rate: 4.34563e-05
	LOSS [training: 2.5840301039072457 | validation: 2.5275621248778464]
	TIME [epoch: 8.45 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582555414657285		[learning rate: 4.3299e-05]
	Learning Rate: 4.32986e-05
	LOSS [training: 2.582555414657285 | validation: 2.5191254484504486]
	TIME [epoch: 8.43 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5842776842281396		[learning rate: 4.3141e-05]
	Learning Rate: 4.31415e-05
	LOSS [training: 2.5842776842281396 | validation: 2.5219334966933555]
	TIME [epoch: 8.42 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58085591705331		[learning rate: 4.2985e-05]
	Learning Rate: 4.29849e-05
	LOSS [training: 2.58085591705331 | validation: 2.527963891817028]
	TIME [epoch: 8.42 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5824496487490136		[learning rate: 4.2829e-05]
	Learning Rate: 4.28289e-05
	LOSS [training: 2.5824496487490136 | validation: 2.5297826244084365]
	TIME [epoch: 8.47 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5883998649768936		[learning rate: 4.2673e-05]
	Learning Rate: 4.26735e-05
	LOSS [training: 2.5883998649768936 | validation: 2.5305347791411883]
	TIME [epoch: 8.47 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5839441026250642		[learning rate: 4.2519e-05]
	Learning Rate: 4.25186e-05
	LOSS [training: 2.5839441026250642 | validation: 2.523137423569592]
	TIME [epoch: 8.42 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582580348346121		[learning rate: 4.2364e-05]
	Learning Rate: 4.23643e-05
	LOSS [training: 2.582580348346121 | validation: 2.5222487439421224]
	TIME [epoch: 8.42 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5832179651053173		[learning rate: 4.2211e-05]
	Learning Rate: 4.22106e-05
	LOSS [training: 2.5832179651053173 | validation: 2.5259620653675974]
	TIME [epoch: 8.46 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5821316805205954		[learning rate: 4.2057e-05]
	Learning Rate: 4.20574e-05
	LOSS [training: 2.5821316805205954 | validation: 2.526187383470021]
	TIME [epoch: 8.47 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5862000829935305		[learning rate: 4.1905e-05]
	Learning Rate: 4.19047e-05
	LOSS [training: 2.5862000829935305 | validation: 2.515749147958212]
	TIME [epoch: 8.42 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5867860969506573		[learning rate: 4.1753e-05]
	Learning Rate: 4.17527e-05
	LOSS [training: 2.5867860969506573 | validation: 2.522409469202557]
	TIME [epoch: 8.42 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588924556421622		[learning rate: 4.1601e-05]
	Learning Rate: 4.16012e-05
	LOSS [training: 2.588924556421622 | validation: 2.5226886401632624]
	TIME [epoch: 8.44 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5803700525644193		[learning rate: 4.145e-05]
	Learning Rate: 4.14502e-05
	LOSS [training: 2.5803700525644193 | validation: 2.524448985738588]
	TIME [epoch: 8.44 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5838720045441326		[learning rate: 4.13e-05]
	Learning Rate: 4.12998e-05
	LOSS [training: 2.5838720045441326 | validation: 2.524449251730737]
	TIME [epoch: 8.43 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5863044167674483		[learning rate: 4.115e-05]
	Learning Rate: 4.11499e-05
	LOSS [training: 2.5863044167674483 | validation: 2.5366268909816356]
	TIME [epoch: 8.44 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5869316177271893		[learning rate: 4.1001e-05]
	Learning Rate: 4.10005e-05
	LOSS [training: 2.5869316177271893 | validation: 2.5328804706101846]
	TIME [epoch: 8.47 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5855966533859274		[learning rate: 4.0852e-05]
	Learning Rate: 4.08517e-05
	LOSS [training: 2.5855966533859274 | validation: 2.5225738709361596]
	TIME [epoch: 8.46 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5833418307512823		[learning rate: 4.0703e-05]
	Learning Rate: 4.07035e-05
	LOSS [training: 2.5833418307512823 | validation: 2.539557930967934]
	TIME [epoch: 8.42 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5907614691362193		[learning rate: 4.0556e-05]
	Learning Rate: 4.05558e-05
	LOSS [training: 2.5907614691362193 | validation: 2.536462824358245]
	TIME [epoch: 8.42 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5863464904464117		[learning rate: 4.0409e-05]
	Learning Rate: 4.04086e-05
	LOSS [training: 2.5863464904464117 | validation: 2.5381186761517647]
	TIME [epoch: 8.45 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581704783571068		[learning rate: 4.0262e-05]
	Learning Rate: 4.0262e-05
	LOSS [training: 2.581704783571068 | validation: 2.5353741452067755]
	TIME [epoch: 8.43 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5827452972705625		[learning rate: 4.0116e-05]
	Learning Rate: 4.01158e-05
	LOSS [training: 2.5827452972705625 | validation: 2.530875300660628]
	TIME [epoch: 8.43 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5854646850162704		[learning rate: 3.997e-05]
	Learning Rate: 3.99703e-05
	LOSS [training: 2.5854646850162704 | validation: 2.524777554152381]
	TIME [epoch: 8.45 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5841632592711963		[learning rate: 3.9825e-05]
	Learning Rate: 3.98252e-05
	LOSS [training: 2.5841632592711963 | validation: 2.5261181500366208]
	TIME [epoch: 8.49 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5881216293080773		[learning rate: 3.9681e-05]
	Learning Rate: 3.96807e-05
	LOSS [training: 2.5881216293080773 | validation: 2.529967484748874]
	TIME [epoch: 8.41 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580746315586561		[learning rate: 3.9537e-05]
	Learning Rate: 3.95367e-05
	LOSS [training: 2.580746315586561 | validation: 2.524964969397372]
	TIME [epoch: 8.42 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5829900182851446		[learning rate: 3.9393e-05]
	Learning Rate: 3.93932e-05
	LOSS [training: 2.5829900182851446 | validation: 2.5268171614663437]
	TIME [epoch: 8.42 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.57848602637318		[learning rate: 3.925e-05]
	Learning Rate: 3.92502e-05
	LOSS [training: 2.57848602637318 | validation: 2.528592702040559]
	TIME [epoch: 8.46 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5819567937349213		[learning rate: 3.9108e-05]
	Learning Rate: 3.91078e-05
	LOSS [training: 2.5819567937349213 | validation: 2.52712627266967]
	TIME [epoch: 8.43 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579303116543947		[learning rate: 3.8966e-05]
	Learning Rate: 3.89659e-05
	LOSS [training: 2.579303116543947 | validation: 2.534114864429762]
	TIME [epoch: 8.43 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5826482288378525		[learning rate: 3.8824e-05]
	Learning Rate: 3.88245e-05
	LOSS [training: 2.5826482288378525 | validation: 2.522477393262384]
	TIME [epoch: 8.43 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5863066370936205		[learning rate: 3.8684e-05]
	Learning Rate: 3.86836e-05
	LOSS [training: 2.5863066370936205 | validation: 2.5297602969686643]
	TIME [epoch: 8.45 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587946676643572		[learning rate: 3.8543e-05]
	Learning Rate: 3.85432e-05
	LOSS [training: 2.587946676643572 | validation: 2.5207226229777886]
	TIME [epoch: 8.43 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5838831634936352		[learning rate: 3.8403e-05]
	Learning Rate: 3.84033e-05
	LOSS [training: 2.5838831634936352 | validation: 2.524611612383035]
	TIME [epoch: 8.43 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582103242531022		[learning rate: 3.8264e-05]
	Learning Rate: 3.82639e-05
	LOSS [training: 2.582103242531022 | validation: 2.5246091163818614]
	TIME [epoch: 8.44 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5840315934757596		[learning rate: 3.8125e-05]
	Learning Rate: 3.81251e-05
	LOSS [training: 2.5840315934757596 | validation: 2.535275455925163]
	TIME [epoch: 8.44 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580234716864031		[learning rate: 3.7987e-05]
	Learning Rate: 3.79867e-05
	LOSS [training: 2.580234716864031 | validation: 2.5354030281721247]
	TIME [epoch: 8.42 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584467486496266		[learning rate: 3.7849e-05]
	Learning Rate: 3.78489e-05
	LOSS [training: 2.584467486496266 | validation: 2.530316399348517]
	TIME [epoch: 8.43 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581375740311252		[learning rate: 3.7711e-05]
	Learning Rate: 3.77115e-05
	LOSS [training: 2.581375740311252 | validation: 2.530717340268673]
	TIME [epoch: 8.43 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5810369664835227		[learning rate: 3.7575e-05]
	Learning Rate: 3.75746e-05
	LOSS [training: 2.5810369664835227 | validation: 2.5270850683687853]
	TIME [epoch: 8.47 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5824004784414027		[learning rate: 3.7438e-05]
	Learning Rate: 3.74383e-05
	LOSS [training: 2.5824004784414027 | validation: 2.5240425242797544]
	TIME [epoch: 8.46 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5788960191126717		[learning rate: 3.7302e-05]
	Learning Rate: 3.73024e-05
	LOSS [training: 2.5788960191126717 | validation: 2.523550885862091]
	TIME [epoch: 8.42 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582610565490003		[learning rate: 3.7167e-05]
	Learning Rate: 3.7167e-05
	LOSS [training: 2.582610565490003 | validation: 2.5308382800693203]
	TIME [epoch: 8.44 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5804482256946115		[learning rate: 3.7032e-05]
	Learning Rate: 3.70322e-05
	LOSS [training: 2.5804482256946115 | validation: 2.528282794619672]
	TIME [epoch: 8.43 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580707079374547		[learning rate: 3.6898e-05]
	Learning Rate: 3.68978e-05
	LOSS [training: 2.580707079374547 | validation: 2.526096922145676]
	TIME [epoch: 8.42 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585211161167549		[learning rate: 3.6764e-05]
	Learning Rate: 3.67639e-05
	LOSS [training: 2.585211161167549 | validation: 2.523896037038692]
	TIME [epoch: 8.43 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58916285457597		[learning rate: 3.663e-05]
	Learning Rate: 3.66304e-05
	LOSS [training: 2.58916285457597 | validation: 2.529005760706336]
	TIME [epoch: 8.44 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.589482518770283		[learning rate: 3.6498e-05]
	Learning Rate: 3.64975e-05
	LOSS [training: 2.589482518770283 | validation: 2.5158876138741637]
	TIME [epoch: 8.43 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5798135152789867		[learning rate: 3.6365e-05]
	Learning Rate: 3.63651e-05
	LOSS [training: 2.5798135152789867 | validation: 2.5392355658698182]
	TIME [epoch: 8.43 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578922062851946		[learning rate: 3.6233e-05]
	Learning Rate: 3.62331e-05
	LOSS [training: 2.578922062851946 | validation: 2.5225622652089092]
	TIME [epoch: 8.43 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5809107064430576		[learning rate: 3.6102e-05]
	Learning Rate: 3.61016e-05
	LOSS [training: 2.5809107064430576 | validation: 2.533004916639314]
	TIME [epoch: 8.46 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5809030071167607		[learning rate: 3.5971e-05]
	Learning Rate: 3.59706e-05
	LOSS [training: 2.5809030071167607 | validation: 2.5262879765922106]
	TIME [epoch: 8.43 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579914861402511		[learning rate: 3.584e-05]
	Learning Rate: 3.584e-05
	LOSS [training: 2.579914861402511 | validation: 2.5295102079132574]
	TIME [epoch: 8.44 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5786288303058846		[learning rate: 3.571e-05]
	Learning Rate: 3.571e-05
	LOSS [training: 2.5786288303058846 | validation: 2.524414301400855]
	TIME [epoch: 8.42 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581054164354249		[learning rate: 3.558e-05]
	Learning Rate: 3.55804e-05
	LOSS [training: 2.581054164354249 | validation: 2.5280404361967364]
	TIME [epoch: 8.46 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582248982087927		[learning rate: 3.5451e-05]
	Learning Rate: 3.54513e-05
	LOSS [training: 2.582248982087927 | validation: 2.5300870574210714]
	TIME [epoch: 8.43 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5841496567064874		[learning rate: 3.5323e-05]
	Learning Rate: 3.53226e-05
	LOSS [training: 2.5841496567064874 | validation: 2.5280838239063805]
	TIME [epoch: 8.43 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5812715097844006		[learning rate: 3.5194e-05]
	Learning Rate: 3.51944e-05
	LOSS [training: 2.5812715097844006 | validation: 2.5413579741445296]
	TIME [epoch: 8.42 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5850681343772797		[learning rate: 3.5067e-05]
	Learning Rate: 3.50667e-05
	LOSS [training: 2.5850681343772797 | validation: 2.529009123613868]
	TIME [epoch: 8.47 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582662396153959		[learning rate: 3.4939e-05]
	Learning Rate: 3.49394e-05
	LOSS [training: 2.582662396153959 | validation: 2.524336143785095]
	TIME [epoch: 8.43 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5820528212465828		[learning rate: 3.4813e-05]
	Learning Rate: 3.48126e-05
	LOSS [training: 2.5820528212465828 | validation: 2.5185323313950017]
	TIME [epoch: 8.44 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5811471733137195		[learning rate: 3.4686e-05]
	Learning Rate: 3.46863e-05
	LOSS [training: 2.5811471733137195 | validation: 2.522083498249039]
	TIME [epoch: 8.43 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582177057758458		[learning rate: 3.456e-05]
	Learning Rate: 3.45604e-05
	LOSS [training: 2.582177057758458 | validation: 2.525376272816833]
	TIME [epoch: 8.45 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5843238003913926		[learning rate: 3.4435e-05]
	Learning Rate: 3.4435e-05
	LOSS [training: 2.5843238003913926 | validation: 2.5237826057783685]
	TIME [epoch: 8.43 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5874881028468213		[learning rate: 3.431e-05]
	Learning Rate: 3.431e-05
	LOSS [training: 2.5874881028468213 | validation: 2.5175496020484096]
	TIME [epoch: 8.44 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5861199391428156		[learning rate: 3.4186e-05]
	Learning Rate: 3.41855e-05
	LOSS [training: 2.5861199391428156 | validation: 2.530549772621221]
	TIME [epoch: 8.43 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5787158490969384		[learning rate: 3.4061e-05]
	Learning Rate: 3.40615e-05
	LOSS [training: 2.5787158490969384 | validation: 2.5280644782906077]
	TIME [epoch: 8.5 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.576957832925887		[learning rate: 3.3938e-05]
	Learning Rate: 3.39378e-05
	LOSS [training: 2.576957832925887 | validation: 2.518202431934209]
	TIME [epoch: 8.45 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58183077963406		[learning rate: 3.3815e-05]
	Learning Rate: 3.38147e-05
	LOSS [training: 2.58183077963406 | validation: 2.515679244340888]
	TIME [epoch: 8.43 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5813303416309976		[learning rate: 3.3692e-05]
	Learning Rate: 3.3692e-05
	LOSS [training: 2.5813303416309976 | validation: 2.5285592804069195]
	TIME [epoch: 8.42 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5826048946797746		[learning rate: 3.357e-05]
	Learning Rate: 3.35697e-05
	LOSS [training: 2.5826048946797746 | validation: 2.5204896824613114]
	TIME [epoch: 8.5 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5805536510356797		[learning rate: 3.3448e-05]
	Learning Rate: 3.34479e-05
	LOSS [training: 2.5805536510356797 | validation: 2.529886370695253]
	TIME [epoch: 8.45 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5829779783173694		[learning rate: 3.3326e-05]
	Learning Rate: 3.33265e-05
	LOSS [training: 2.5829779783173694 | validation: 2.528694830601725]
	TIME [epoch: 8.43 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5847851559795036		[learning rate: 3.3206e-05]
	Learning Rate: 3.32055e-05
	LOSS [training: 2.5847851559795036 | validation: 2.524891799536256]
	TIME [epoch: 8.43 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581019737686331		[learning rate: 3.3085e-05]
	Learning Rate: 3.3085e-05
	LOSS [training: 2.581019737686331 | validation: 2.5353538412988237]
	TIME [epoch: 8.45 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5802246181496216		[learning rate: 3.2965e-05]
	Learning Rate: 3.2965e-05
	LOSS [training: 2.5802246181496216 | validation: 2.525128540215153]
	TIME [epoch: 8.44 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5780044286327324		[learning rate: 3.2845e-05]
	Learning Rate: 3.28453e-05
	LOSS [training: 2.5780044286327324 | validation: 2.530926757063427]
	TIME [epoch: 8.43 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581466643651223		[learning rate: 3.2726e-05]
	Learning Rate: 3.27261e-05
	LOSS [training: 2.581466643651223 | validation: 2.5333565175953687]
	TIME [epoch: 8.44 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5849503437034484		[learning rate: 3.2607e-05]
	Learning Rate: 3.26074e-05
	LOSS [training: 2.5849503437034484 | validation: 2.5270628336414025]
	TIME [epoch: 8.44 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5811730668036934		[learning rate: 3.2489e-05]
	Learning Rate: 3.2489e-05
	LOSS [training: 2.5811730668036934 | validation: 2.5247217863936933]
	TIME [epoch: 8.48 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5808188878827556		[learning rate: 3.2371e-05]
	Learning Rate: 3.23711e-05
	LOSS [training: 2.5808188878827556 | validation: 2.529124431329664]
	TIME [epoch: 8.44 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5824123176744913		[learning rate: 3.2254e-05]
	Learning Rate: 3.22537e-05
	LOSS [training: 2.5824123176744913 | validation: 2.523244059060576]
	TIME [epoch: 8.43 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5846903215988464		[learning rate: 3.2137e-05]
	Learning Rate: 3.21366e-05
	LOSS [training: 2.5846903215988464 | validation: 2.5200179336109896]
	TIME [epoch: 8.46 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5828151712899876		[learning rate: 3.202e-05]
	Learning Rate: 3.202e-05
	LOSS [training: 2.5828151712899876 | validation: 2.52148888830063]
	TIME [epoch: 8.44 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5802070547107983		[learning rate: 3.1904e-05]
	Learning Rate: 3.19038e-05
	LOSS [training: 2.5802070547107983 | validation: 2.5220600842943885]
	TIME [epoch: 8.45 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582906469694169		[learning rate: 3.1788e-05]
	Learning Rate: 3.1788e-05
	LOSS [training: 2.582906469694169 | validation: 2.523352472030715]
	TIME [epoch: 8.44 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579428527327784		[learning rate: 3.1673e-05]
	Learning Rate: 3.16726e-05
	LOSS [training: 2.579428527327784 | validation: 2.5298645832833158]
	TIME [epoch: 8.46 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58100298216309		[learning rate: 3.1558e-05]
	Learning Rate: 3.15577e-05
	LOSS [training: 2.58100298216309 | validation: 2.537889238129348]
	TIME [epoch: 8.43 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5830266556225716		[learning rate: 3.1443e-05]
	Learning Rate: 3.14432e-05
	LOSS [training: 2.5830266556225716 | validation: 2.5259497301273055]
	TIME [epoch: 8.45 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580290721203743		[learning rate: 3.1329e-05]
	Learning Rate: 3.13291e-05
	LOSS [training: 2.580290721203743 | validation: 2.525243099130172]
	TIME [epoch: 8.48 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580249114058297		[learning rate: 3.1215e-05]
	Learning Rate: 3.12154e-05
	LOSS [training: 2.580249114058297 | validation: 2.5241195729656645]
	TIME [epoch: 8.45 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5790566961644306		[learning rate: 3.1102e-05]
	Learning Rate: 3.11021e-05
	LOSS [training: 2.5790566961644306 | validation: 2.5296670261160825]
	TIME [epoch: 8.44 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5822440834083062		[learning rate: 3.0989e-05]
	Learning Rate: 3.09892e-05
	LOSS [training: 2.5822440834083062 | validation: 2.530460815487516]
	TIME [epoch: 8.43 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5853608215863804		[learning rate: 3.0877e-05]
	Learning Rate: 3.08768e-05
	LOSS [training: 2.5853608215863804 | validation: 2.5270083578915754]
	TIME [epoch: 8.44 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5795812226813952		[learning rate: 3.0765e-05]
	Learning Rate: 3.07647e-05
	LOSS [training: 2.5795812226813952 | validation: 2.5231158389250896]
	TIME [epoch: 8.46 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5787960106603407		[learning rate: 3.0653e-05]
	Learning Rate: 3.0653e-05
	LOSS [training: 2.5787960106603407 | validation: 2.531563211584861]
	TIME [epoch: 8.44 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578967886223197		[learning rate: 3.0542e-05]
	Learning Rate: 3.05418e-05
	LOSS [training: 2.578967886223197 | validation: 2.524161164565978]
	TIME [epoch: 8.44 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580979089002838		[learning rate: 3.0431e-05]
	Learning Rate: 3.0431e-05
	LOSS [training: 2.580979089002838 | validation: 2.5266855239779193]
	TIME [epoch: 8.43 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5792142252568655		[learning rate: 3.0321e-05]
	Learning Rate: 3.03205e-05
	LOSS [training: 2.5792142252568655 | validation: 2.526878488184553]
	TIME [epoch: 8.47 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5821423966151653		[learning rate: 3.0211e-05]
	Learning Rate: 3.02105e-05
	LOSS [training: 2.5821423966151653 | validation: 2.5239787100024405]
	TIME [epoch: 8.44 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5775496876119175		[learning rate: 3.0101e-05]
	Learning Rate: 3.01009e-05
	LOSS [training: 2.5775496876119175 | validation: 2.5151943875189793]
	TIME [epoch: 8.44 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584981021450125		[learning rate: 2.9992e-05]
	Learning Rate: 2.99916e-05
	LOSS [training: 2.584981021450125 | validation: 2.523847341088947]
	TIME [epoch: 8.45 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5858687708616452		[learning rate: 2.9883e-05]
	Learning Rate: 2.98828e-05
	LOSS [training: 2.5858687708616452 | validation: 2.51842624437345]
	TIME [epoch: 8.46 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5859496171727527		[learning rate: 2.9774e-05]
	Learning Rate: 2.97743e-05
	LOSS [training: 2.5859496171727527 | validation: 2.523454884151352]
	TIME [epoch: 8.48 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583962631301955		[learning rate: 2.9666e-05]
	Learning Rate: 2.96663e-05
	LOSS [training: 2.583962631301955 | validation: 2.51686757010311]
	TIME [epoch: 8.43 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5866682209575225		[learning rate: 2.9559e-05]
	Learning Rate: 2.95586e-05
	LOSS [training: 2.5866682209575225 | validation: 2.5174213130754017]
	TIME [epoch: 8.45 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58459681025192		[learning rate: 2.9451e-05]
	Learning Rate: 2.94514e-05
	LOSS [training: 2.58459681025192 | validation: 2.5209991649905974]
	TIME [epoch: 8.44 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5811413140785637		[learning rate: 2.9344e-05]
	Learning Rate: 2.93445e-05
	LOSS [training: 2.5811413140785637 | validation: 2.524789493760725]
	TIME [epoch: 8.44 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5819081624046385		[learning rate: 2.9238e-05]
	Learning Rate: 2.9238e-05
	LOSS [training: 2.5819081624046385 | validation: 2.525067443228795]
	TIME [epoch: 8.43 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5797594566767543		[learning rate: 2.9132e-05]
	Learning Rate: 2.91319e-05
	LOSS [training: 2.5797594566767543 | validation: 2.5264862840056637]
	TIME [epoch: 8.46 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578809720950444		[learning rate: 2.9026e-05]
	Learning Rate: 2.90262e-05
	LOSS [training: 2.578809720950444 | validation: 2.531275598375795]
	TIME [epoch: 8.44 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5789321902913604		[learning rate: 2.8921e-05]
	Learning Rate: 2.89208e-05
	LOSS [training: 2.5789321902913604 | validation: 2.53358838931985]
	TIME [epoch: 8.43 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5776958197166273		[learning rate: 2.8816e-05]
	Learning Rate: 2.88159e-05
	LOSS [training: 2.5776958197166273 | validation: 2.524242648646868]
	TIME [epoch: 8.44 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5802358246574215		[learning rate: 2.8711e-05]
	Learning Rate: 2.87113e-05
	LOSS [training: 2.5802358246574215 | validation: 2.5342093710322002]
	TIME [epoch: 8.45 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583998277123645		[learning rate: 2.8607e-05]
	Learning Rate: 2.86071e-05
	LOSS [training: 2.583998277123645 | validation: 2.530166850913078]
	TIME [epoch: 8.45 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5784893046100246		[learning rate: 2.8503e-05]
	Learning Rate: 2.85033e-05
	LOSS [training: 2.5784893046100246 | validation: 2.5346589414217533]
	TIME [epoch: 8.43 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5847215303427		[learning rate: 2.84e-05]
	Learning Rate: 2.83998e-05
	LOSS [training: 2.5847215303427 | validation: 2.5258062961618166]
	TIME [epoch: 8.44 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5821347215930377		[learning rate: 2.8297e-05]
	Learning Rate: 2.82968e-05
	LOSS [training: 2.5821347215930377 | validation: 2.516739282069471]
	TIME [epoch: 8.45 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585518702975758		[learning rate: 2.8194e-05]
	Learning Rate: 2.81941e-05
	LOSS [training: 2.585518702975758 | validation: 2.527655830674467]
	TIME [epoch: 8.44 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582491714088568		[learning rate: 2.8092e-05]
	Learning Rate: 2.80918e-05
	LOSS [training: 2.582491714088568 | validation: 2.524604864839862]
	TIME [epoch: 8.44 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582644509887693		[learning rate: 2.799e-05]
	Learning Rate: 2.79898e-05
	LOSS [training: 2.582644509887693 | validation: 2.5321445174369126]
	TIME [epoch: 8.43 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5827318328717777		[learning rate: 2.7888e-05]
	Learning Rate: 2.78882e-05
	LOSS [training: 2.5827318328717777 | validation: 2.519745026598333]
	TIME [epoch: 8.48 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581505113822012		[learning rate: 2.7787e-05]
	Learning Rate: 2.7787e-05
	LOSS [training: 2.581505113822012 | validation: 2.5226894882967033]
	TIME [epoch: 8.47 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578419318923198		[learning rate: 2.7686e-05]
	Learning Rate: 2.76862e-05
	LOSS [training: 2.578419318923198 | validation: 2.5256107426639325]
	TIME [epoch: 8.43 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582670728443857		[learning rate: 2.7586e-05]
	Learning Rate: 2.75857e-05
	LOSS [training: 2.582670728443857 | validation: 2.5280666411354984]
	TIME [epoch: 8.43 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5811804986948146		[learning rate: 2.7486e-05]
	Learning Rate: 2.74856e-05
	LOSS [training: 2.5811804986948146 | validation: 2.528089390654]
	TIME [epoch: 8.47 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5801839256538117		[learning rate: 2.7386e-05]
	Learning Rate: 2.73859e-05
	LOSS [training: 2.5801839256538117 | validation: 2.5181442199168327]
	TIME [epoch: 8.48 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5808466568110866		[learning rate: 2.7286e-05]
	Learning Rate: 2.72865e-05
	LOSS [training: 2.5808466568110866 | validation: 2.521234936500738]
	TIME [epoch: 8.44 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5843280023758926		[learning rate: 2.7187e-05]
	Learning Rate: 2.71875e-05
	LOSS [training: 2.5843280023758926 | validation: 2.5324217934417605]
	TIME [epoch: 8.43 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582066201788819		[learning rate: 2.7089e-05]
	Learning Rate: 2.70888e-05
	LOSS [training: 2.582066201788819 | validation: 2.5218313080665604]
	TIME [epoch: 8.46 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5828327137633633		[learning rate: 2.699e-05]
	Learning Rate: 2.69905e-05
	LOSS [training: 2.5828327137633633 | validation: 2.5356411005576684]
	TIME [epoch: 8.44 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581715278637681		[learning rate: 2.6893e-05]
	Learning Rate: 2.68925e-05
	LOSS [training: 2.581715278637681 | validation: 2.5229334169793507]
	TIME [epoch: 8.48 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5831582528762085		[learning rate: 2.6795e-05]
	Learning Rate: 2.67949e-05
	LOSS [training: 2.5831582528762085 | validation: 2.534069792785588]
	TIME [epoch: 8.44 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58072723523341		[learning rate: 2.6698e-05]
	Learning Rate: 2.66977e-05
	LOSS [training: 2.58072723523341 | validation: 2.5256034427809055]
	TIME [epoch: 8.45 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580206331888659		[learning rate: 2.6601e-05]
	Learning Rate: 2.66008e-05
	LOSS [training: 2.580206331888659 | validation: 2.527589019769461]
	TIME [epoch: 8.44 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585738554236532		[learning rate: 2.6504e-05]
	Learning Rate: 2.65043e-05
	LOSS [training: 2.585738554236532 | validation: 2.5245014794245115]
	TIME [epoch: 8.43 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5820701675924815		[learning rate: 2.6408e-05]
	Learning Rate: 2.64081e-05
	LOSS [training: 2.5820701675924815 | validation: 2.533019733720044]
	TIME [epoch: 8.46 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5816755315347866		[learning rate: 2.6312e-05]
	Learning Rate: 2.63123e-05
	LOSS [training: 2.5816755315347866 | validation: 2.5256368194799004]
	TIME [epoch: 8.49 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581735913564448		[learning rate: 2.6217e-05]
	Learning Rate: 2.62168e-05
	LOSS [training: 2.581735913564448 | validation: 2.523417110720624]
	TIME [epoch: 8.43 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584592698006166		[learning rate: 2.6122e-05]
	Learning Rate: 2.61216e-05
	LOSS [training: 2.584592698006166 | validation: 2.5239309488050417]
	TIME [epoch: 8.42 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5808249322711476		[learning rate: 2.6027e-05]
	Learning Rate: 2.60268e-05
	LOSS [training: 2.5808249322711476 | validation: 2.5293037814484167]
	TIME [epoch: 8.43 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5837702065807706		[learning rate: 2.5932e-05]
	Learning Rate: 2.59324e-05
	LOSS [training: 2.5837702065807706 | validation: 2.5238518616116474]
	TIME [epoch: 8.46 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5790393357790404		[learning rate: 2.5838e-05]
	Learning Rate: 2.58383e-05
	LOSS [training: 2.5790393357790404 | validation: 2.521620745306627]
	TIME [epoch: 8.44 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583417856881991		[learning rate: 2.5744e-05]
	Learning Rate: 2.57445e-05
	LOSS [training: 2.583417856881991 | validation: 2.524934436297431]
	TIME [epoch: 8.43 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5787250618648745		[learning rate: 2.5651e-05]
	Learning Rate: 2.56511e-05
	LOSS [training: 2.5787250618648745 | validation: 2.536433812610507]
	TIME [epoch: 8.44 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5808758887605334		[learning rate: 2.5558e-05]
	Learning Rate: 2.5558e-05
	LOSS [training: 2.5808758887605334 | validation: 2.525145367715628]
	TIME [epoch: 8.46 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5789842772245053		[learning rate: 2.5465e-05]
	Learning Rate: 2.54652e-05
	LOSS [training: 2.5789842772245053 | validation: 2.5317988035781624]
	TIME [epoch: 8.42 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5818657674416112		[learning rate: 2.5373e-05]
	Learning Rate: 2.53728e-05
	LOSS [training: 2.5818657674416112 | validation: 2.525997431630905]
	TIME [epoch: 8.44 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5821925225618925		[learning rate: 2.5281e-05]
	Learning Rate: 2.52807e-05
	LOSS [training: 2.5821925225618925 | validation: 2.532857556213239]
	TIME [epoch: 8.44 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5804974001600014		[learning rate: 2.5189e-05]
	Learning Rate: 2.5189e-05
	LOSS [training: 2.5804974001600014 | validation: 2.522700272536671]
	TIME [epoch: 8.44 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580599311916502		[learning rate: 2.5098e-05]
	Learning Rate: 2.50976e-05
	LOSS [training: 2.580599311916502 | validation: 2.524880122321594]
	TIME [epoch: 8.43 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5798594346868233		[learning rate: 2.5006e-05]
	Learning Rate: 2.50065e-05
	LOSS [training: 2.5798594346868233 | validation: 2.5256276653077365]
	TIME [epoch: 8.48 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582000562692282		[learning rate: 2.4916e-05]
	Learning Rate: 2.49157e-05
	LOSS [training: 2.582000562692282 | validation: 2.524347085486397]
	TIME [epoch: 8.48 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5841094284215997		[learning rate: 2.4825e-05]
	Learning Rate: 2.48253e-05
	LOSS [training: 2.5841094284215997 | validation: 2.5324643776016593]
	TIME [epoch: 8.43 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5830849312941058		[learning rate: 2.4735e-05]
	Learning Rate: 2.47352e-05
	LOSS [training: 2.5830849312941058 | validation: 2.522377187352177]
	TIME [epoch: 8.43 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5862969523857133		[learning rate: 2.4645e-05]
	Learning Rate: 2.46455e-05
	LOSS [training: 2.5862969523857133 | validation: 2.5276391144673283]
	TIME [epoch: 8.43 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579733624647731		[learning rate: 2.4556e-05]
	Learning Rate: 2.4556e-05
	LOSS [training: 2.579733624647731 | validation: 2.5235903426469415]
	TIME [epoch: 8.45 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5827166155389527		[learning rate: 2.4467e-05]
	Learning Rate: 2.44669e-05
	LOSS [training: 2.5827166155389527 | validation: 2.52644034538942]
	TIME [epoch: 8.43 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580556957408711		[learning rate: 2.4378e-05]
	Learning Rate: 2.43781e-05
	LOSS [training: 2.580556957408711 | validation: 2.524559432030421]
	TIME [epoch: 8.42 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5822654918017998		[learning rate: 2.429e-05]
	Learning Rate: 2.42896e-05
	LOSS [training: 2.5822654918017998 | validation: 2.5273888877727444]
	TIME [epoch: 8.44 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5827497966172195		[learning rate: 2.4201e-05]
	Learning Rate: 2.42015e-05
	LOSS [training: 2.5827497966172195 | validation: 2.528765611669458]
	TIME [epoch: 8.45 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5794984991758856		[learning rate: 2.4114e-05]
	Learning Rate: 2.41137e-05
	LOSS [training: 2.5794984991758856 | validation: 2.528358453368042]
	TIME [epoch: 8.45 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5826285344761373		[learning rate: 2.4026e-05]
	Learning Rate: 2.40262e-05
	LOSS [training: 2.5826285344761373 | validation: 2.518042818723269]
	TIME [epoch: 8.42 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5818520789676227		[learning rate: 2.3939e-05]
	Learning Rate: 2.3939e-05
	LOSS [training: 2.5818520789676227 | validation: 2.5231294556350274]
	TIME [epoch: 8.44 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5780722041442354		[learning rate: 2.3852e-05]
	Learning Rate: 2.38521e-05
	LOSS [training: 2.5780722041442354 | validation: 2.5234641219783067]
	TIME [epoch: 8.45 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5847048435520708		[learning rate: 2.3766e-05]
	Learning Rate: 2.37655e-05
	LOSS [training: 2.5847048435520708 | validation: 2.5293889350682965]
	TIME [epoch: 8.44 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582991411934824		[learning rate: 2.3679e-05]
	Learning Rate: 2.36793e-05
	LOSS [training: 2.582991411934824 | validation: 2.525042091079515]
	TIME [epoch: 8.43 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581539845291142		[learning rate: 2.3593e-05]
	Learning Rate: 2.35933e-05
	LOSS [training: 2.581539845291142 | validation: 2.5229134900965784]
	TIME [epoch: 8.43 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587608519063271		[learning rate: 2.3508e-05]
	Learning Rate: 2.35077e-05
	LOSS [training: 2.587608519063271 | validation: 2.5323802272211147]
	TIME [epoch: 8.45 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582996355987861		[learning rate: 2.3422e-05]
	Learning Rate: 2.34224e-05
	LOSS [training: 2.582996355987861 | validation: 2.523810416910628]
	TIME [epoch: 8.44 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5816813973921198		[learning rate: 2.3337e-05]
	Learning Rate: 2.33374e-05
	LOSS [training: 2.5816813973921198 | validation: 2.518281005705984]
	TIME [epoch: 8.42 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5849757764123193		[learning rate: 2.3253e-05]
	Learning Rate: 2.32527e-05
	LOSS [training: 2.5849757764123193 | validation: 2.524596399870859]
	TIME [epoch: 8.44 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5848226923822533		[learning rate: 2.3168e-05]
	Learning Rate: 2.31683e-05
	LOSS [training: 2.5848226923822533 | validation: 2.5259443246513658]
	TIME [epoch: 8.49 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580089092043686		[learning rate: 2.3084e-05]
	Learning Rate: 2.30843e-05
	LOSS [training: 2.580089092043686 | validation: 2.530662180546323]
	TIME [epoch: 8.46 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585535331178443		[learning rate: 2.3e-05]
	Learning Rate: 2.30005e-05
	LOSS [training: 2.585535331178443 | validation: 2.528649238472041]
	TIME [epoch: 8.42 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5891316775979663		[learning rate: 2.2917e-05]
	Learning Rate: 2.2917e-05
	LOSS [training: 2.5891316775979663 | validation: 2.530810714563773]
	TIME [epoch: 8.42 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58384365745285		[learning rate: 2.2834e-05]
	Learning Rate: 2.28338e-05
	LOSS [training: 2.58384365745285 | validation: 2.524711027835555]
	TIME [epoch: 8.46 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5880083333670205		[learning rate: 2.2751e-05]
	Learning Rate: 2.2751e-05
	LOSS [training: 2.5880083333670205 | validation: 2.522970947526234]
	TIME [epoch: 8.48 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5799322029762184		[learning rate: 2.2668e-05]
	Learning Rate: 2.26684e-05
	LOSS [training: 2.5799322029762184 | validation: 2.5249489370764673]
	TIME [epoch: 8.43 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581564950788409		[learning rate: 2.2586e-05]
	Learning Rate: 2.25862e-05
	LOSS [training: 2.581564950788409 | validation: 2.5209864401696986]
	TIME [epoch: 8.42 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5860440229162216		[learning rate: 2.2504e-05]
	Learning Rate: 2.25042e-05
	LOSS [training: 2.5860440229162216 | validation: 2.522367244511439]
	TIME [epoch: 8.45 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583862561593708		[learning rate: 2.2423e-05]
	Learning Rate: 2.24225e-05
	LOSS [training: 2.583862561593708 | validation: 2.5275101132425295]
	TIME [epoch: 8.43 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5850004657046464		[learning rate: 2.2341e-05]
	Learning Rate: 2.23411e-05
	LOSS [training: 2.5850004657046464 | validation: 2.5246920478073043]
	TIME [epoch: 8.44 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578515201369152		[learning rate: 2.226e-05]
	Learning Rate: 2.22601e-05
	LOSS [training: 2.578515201369152 | validation: 2.5249212907824976]
	TIME [epoch: 8.48 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5822033344447974		[learning rate: 2.2179e-05]
	Learning Rate: 2.21793e-05
	LOSS [training: 2.5822033344447974 | validation: 2.52589285895484]
	TIME [epoch: 8.46 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583345258617414		[learning rate: 2.2099e-05]
	Learning Rate: 2.20988e-05
	LOSS [training: 2.583345258617414 | validation: 2.5226538223565647]
	TIME [epoch: 8.43 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5831362497408588		[learning rate: 2.2019e-05]
	Learning Rate: 2.20186e-05
	LOSS [training: 2.5831362497408588 | validation: 2.535712613175482]
	TIME [epoch: 8.43 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5812282021324466		[learning rate: 2.1939e-05]
	Learning Rate: 2.19387e-05
	LOSS [training: 2.5812282021324466 | validation: 2.529748675299942]
	TIME [epoch: 8.45 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5826503570129185		[learning rate: 2.1859e-05]
	Learning Rate: 2.18591e-05
	LOSS [training: 2.5826503570129185 | validation: 2.515607888142303]
	TIME [epoch: 8.46 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5819166549932953		[learning rate: 2.178e-05]
	Learning Rate: 2.17797e-05
	LOSS [training: 2.5819166549932953 | validation: 2.529311834117652]
	TIME [epoch: 8.44 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580148862436464		[learning rate: 2.1701e-05]
	Learning Rate: 2.17007e-05
	LOSS [training: 2.580148862436464 | validation: 2.530334231488477]
	TIME [epoch: 8.44 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582918820659394		[learning rate: 2.1622e-05]
	Learning Rate: 2.16219e-05
	LOSS [training: 2.582918820659394 | validation: 2.530640531718227]
	TIME [epoch: 8.48 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579699976935449		[learning rate: 2.1543e-05]
	Learning Rate: 2.15435e-05
	LOSS [training: 2.579699976935449 | validation: 2.5238452773905404]
	TIME [epoch: 8.46 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580271922841415		[learning rate: 2.1465e-05]
	Learning Rate: 2.14653e-05
	LOSS [training: 2.580271922841415 | validation: 2.527659776400575]
	TIME [epoch: 8.43 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580757088691799		[learning rate: 2.1387e-05]
	Learning Rate: 2.13874e-05
	LOSS [training: 2.580757088691799 | validation: 2.518347571549953]
	TIME [epoch: 8.43 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581151558541851		[learning rate: 2.131e-05]
	Learning Rate: 2.13098e-05
	LOSS [training: 2.581151558541851 | validation: 2.5144664227501696]
	TIME [epoch: 8.45 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5823522223074007		[learning rate: 2.1232e-05]
	Learning Rate: 2.12325e-05
	LOSS [training: 2.5823522223074007 | validation: 2.527400817346898]
	TIME [epoch: 8.45 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5835862767931284		[learning rate: 2.1155e-05]
	Learning Rate: 2.11554e-05
	LOSS [training: 2.5835862767931284 | validation: 2.530885042956516]
	TIME [epoch: 8.43 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.588281741437224		[learning rate: 2.1079e-05]
	Learning Rate: 2.10786e-05
	LOSS [training: 2.588281741437224 | validation: 2.51729141840843]
	TIME [epoch: 8.44 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579409484809207		[learning rate: 2.1002e-05]
	Learning Rate: 2.10021e-05
	LOSS [training: 2.579409484809207 | validation: 2.5248274579652334]
	TIME [epoch: 8.45 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5802858931138153		[learning rate: 2.0926e-05]
	Learning Rate: 2.09259e-05
	LOSS [training: 2.5802858931138153 | validation: 2.5263586024723033]
	TIME [epoch: 8.44 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580369733522466		[learning rate: 2.085e-05]
	Learning Rate: 2.085e-05
	LOSS [training: 2.580369733522466 | validation: 2.530970053364677]
	TIME [epoch: 8.43 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5824760151500192		[learning rate: 2.0774e-05]
	Learning Rate: 2.07743e-05
	LOSS [training: 2.5824760151500192 | validation: 2.5355001937533075]
	TIME [epoch: 8.45 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5821872236185275		[learning rate: 2.0699e-05]
	Learning Rate: 2.06989e-05
	LOSS [training: 2.5821872236185275 | validation: 2.5326161421656117]
	TIME [epoch: 8.46 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581672526795289		[learning rate: 2.0624e-05]
	Learning Rate: 2.06238e-05
	LOSS [training: 2.581672526795289 | validation: 2.531537319344528]
	TIME [epoch: 8.46 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585224738163527		[learning rate: 2.0549e-05]
	Learning Rate: 2.05489e-05
	LOSS [training: 2.585224738163527 | validation: 2.5248569870258555]
	TIME [epoch: 8.47 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582566821862044		[learning rate: 2.0474e-05]
	Learning Rate: 2.04744e-05
	LOSS [training: 2.582566821862044 | validation: 2.5307871911774447]
	TIME [epoch: 8.42 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579785130349049		[learning rate: 2.04e-05]
	Learning Rate: 2.04001e-05
	LOSS [training: 2.579785130349049 | validation: 2.520102374814199]
	TIME [epoch: 8.45 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5803548726071086		[learning rate: 2.0326e-05]
	Learning Rate: 2.0326e-05
	LOSS [training: 2.5803548726071086 | validation: 2.5201833003096894]
	TIME [epoch: 8.44 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5813889825970255		[learning rate: 2.0252e-05]
	Learning Rate: 2.02523e-05
	LOSS [training: 2.5813889825970255 | validation: 2.527867941343274]
	TIME [epoch: 8.45 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5774241759609104		[learning rate: 2.0179e-05]
	Learning Rate: 2.01788e-05
	LOSS [training: 2.5774241759609104 | validation: 2.5178929547065296]
	TIME [epoch: 8.43 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580214731960697		[learning rate: 2.0106e-05]
	Learning Rate: 2.01055e-05
	LOSS [training: 2.580214731960697 | validation: 2.530248388503453]
	TIME [epoch: 8.46 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583237486154636		[learning rate: 2.0033e-05]
	Learning Rate: 2.00326e-05
	LOSS [training: 2.583237486154636 | validation: 2.5246117818028444]
	TIME [epoch: 8.44 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5817054714875494		[learning rate: 1.996e-05]
	Learning Rate: 1.99599e-05
	LOSS [training: 2.5817054714875494 | validation: 2.531022535216267]
	TIME [epoch: 8.44 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5783947034252983		[learning rate: 1.9887e-05]
	Learning Rate: 1.98874e-05
	LOSS [training: 2.5783947034252983 | validation: 2.516990006441487]
	TIME [epoch: 8.44 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579867759332709		[learning rate: 1.9815e-05]
	Learning Rate: 1.98153e-05
	LOSS [training: 2.579867759332709 | validation: 2.5275581678526624]
	TIME [epoch: 8.46 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582991463021883		[learning rate: 1.9743e-05]
	Learning Rate: 1.97434e-05
	LOSS [training: 2.582991463021883 | validation: 2.520924200496757]
	TIME [epoch: 8.44 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5824900627000327		[learning rate: 1.9672e-05]
	Learning Rate: 1.96717e-05
	LOSS [training: 2.5824900627000327 | validation: 2.530448234953829]
	TIME [epoch: 8.43 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582647528131605		[learning rate: 1.96e-05]
	Learning Rate: 1.96003e-05
	LOSS [training: 2.582647528131605 | validation: 2.5216687120996064]
	TIME [epoch: 8.43 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5837720173545877		[learning rate: 1.9529e-05]
	Learning Rate: 1.95292e-05
	LOSS [training: 2.5837720173545877 | validation: 2.5222526570175683]
	TIME [epoch: 8.46 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584689324522768		[learning rate: 1.9458e-05]
	Learning Rate: 1.94583e-05
	LOSS [training: 2.584689324522768 | validation: 2.5236669109853374]
	TIME [epoch: 8.44 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584674433947845		[learning rate: 1.9388e-05]
	Learning Rate: 1.93877e-05
	LOSS [training: 2.584674433947845 | validation: 2.520819188446306]
	TIME [epoch: 8.43 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584829861086365		[learning rate: 1.9317e-05]
	Learning Rate: 1.93173e-05
	LOSS [training: 2.584829861086365 | validation: 2.5237563119165194]
	TIME [epoch: 8.44 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5784589439998054		[learning rate: 1.9247e-05]
	Learning Rate: 1.92472e-05
	LOSS [training: 2.5784589439998054 | validation: 2.5235565196652905]
	TIME [epoch: 8.45 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582945167138976		[learning rate: 1.9177e-05]
	Learning Rate: 1.91774e-05
	LOSS [training: 2.582945167138976 | validation: 2.526648868818927]
	TIME [epoch: 8.43 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579392981234082		[learning rate: 1.9108e-05]
	Learning Rate: 1.91078e-05
	LOSS [training: 2.579392981234082 | validation: 2.5342889387224794]
	TIME [epoch: 8.44 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579873206823611		[learning rate: 1.9038e-05]
	Learning Rate: 1.90384e-05
	LOSS [training: 2.579873206823611 | validation: 2.525172618999404]
	TIME [epoch: 8.46 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5830658963399626		[learning rate: 1.8969e-05]
	Learning Rate: 1.89694e-05
	LOSS [training: 2.5830658963399626 | validation: 2.5252324382630458]
	TIME [epoch: 8.5 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5789671732899566		[learning rate: 1.8901e-05]
	Learning Rate: 1.89005e-05
	LOSS [training: 2.5789671732899566 | validation: 2.5262038696904865]
	TIME [epoch: 8.42 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5813403752507273		[learning rate: 1.8832e-05]
	Learning Rate: 1.88319e-05
	LOSS [training: 2.5813403752507273 | validation: 2.521632099669356]
	TIME [epoch: 8.42 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583503219142116		[learning rate: 1.8764e-05]
	Learning Rate: 1.87636e-05
	LOSS [training: 2.583503219142116 | validation: 2.5293791155003724]
	TIME [epoch: 8.46 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5854174288114757		[learning rate: 1.8695e-05]
	Learning Rate: 1.86955e-05
	LOSS [training: 2.5854174288114757 | validation: 2.523703845144113]
	TIME [epoch: 8.5 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5828751634788096		[learning rate: 1.8628e-05]
	Learning Rate: 1.86276e-05
	LOSS [training: 2.5828751634788096 | validation: 2.5195712266774457]
	TIME [epoch: 8.43 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586579337002484		[learning rate: 1.856e-05]
	Learning Rate: 1.856e-05
	LOSS [training: 2.586579337002484 | validation: 2.5136446337457627]
	TIME [epoch: 8.43 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5835051971232916		[learning rate: 1.8493e-05]
	Learning Rate: 1.84927e-05
	LOSS [training: 2.5835051971232916 | validation: 2.530208079761306]
	TIME [epoch: 8.44 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580759945015133		[learning rate: 1.8426e-05]
	Learning Rate: 1.84256e-05
	LOSS [training: 2.580759945015133 | validation: 2.5257392357241106]
	TIME [epoch: 8.46 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5841865640733297		[learning rate: 1.8359e-05]
	Learning Rate: 1.83587e-05
	LOSS [training: 2.5841865640733297 | validation: 2.531911065000602]
	TIME [epoch: 8.44 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5796625864309353		[learning rate: 1.8292e-05]
	Learning Rate: 1.82921e-05
	LOSS [training: 2.5796625864309353 | validation: 2.5250248851626518]
	TIME [epoch: 8.47 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5825982628531285		[learning rate: 1.8226e-05]
	Learning Rate: 1.82257e-05
	LOSS [training: 2.5825982628531285 | validation: 2.523630261864221]
	TIME [epoch: 8.46 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580176783949768		[learning rate: 1.816e-05]
	Learning Rate: 1.81596e-05
	LOSS [training: 2.580176783949768 | validation: 2.5277750385348137]
	TIME [epoch: 8.44 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58267835101073		[learning rate: 1.8094e-05]
	Learning Rate: 1.80937e-05
	LOSS [training: 2.58267835101073 | validation: 2.525935202701569]
	TIME [epoch: 8.43 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582588960095543		[learning rate: 1.8028e-05]
	Learning Rate: 1.8028e-05
	LOSS [training: 2.582588960095543 | validation: 2.523034659395644]
	TIME [epoch: 8.43 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578042547347212		[learning rate: 1.7963e-05]
	Learning Rate: 1.79626e-05
	LOSS [training: 2.578042547347212 | validation: 2.523917448039767]
	TIME [epoch: 8.45 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585155443023292		[learning rate: 1.7897e-05]
	Learning Rate: 1.78974e-05
	LOSS [training: 2.585155443023292 | validation: 2.529315317203859]
	TIME [epoch: 8.45 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5804494564401055		[learning rate: 1.7832e-05]
	Learning Rate: 1.78324e-05
	LOSS [training: 2.5804494564401055 | validation: 2.532228769062217]
	TIME [epoch: 8.44 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5824573486902915		[learning rate: 1.7768e-05]
	Learning Rate: 1.77677e-05
	LOSS [training: 2.5824573486902915 | validation: 2.5213784266292736]
	TIME [epoch: 8.45 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5775396663028407		[learning rate: 1.7703e-05]
	Learning Rate: 1.77032e-05
	LOSS [training: 2.5775396663028407 | validation: 2.5304288228197205]
	TIME [epoch: 8.44 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581759882303903		[learning rate: 1.7639e-05]
	Learning Rate: 1.7639e-05
	LOSS [training: 2.581759882303903 | validation: 2.5245562800143864]
	TIME [epoch: 8.46 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583539266028832		[learning rate: 1.7575e-05]
	Learning Rate: 1.7575e-05
	LOSS [training: 2.583539266028832 | validation: 2.53296144046503]
	TIME [epoch: 8.48 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.587982765693783		[learning rate: 1.7511e-05]
	Learning Rate: 1.75112e-05
	LOSS [training: 2.587982765693783 | validation: 2.524492754588506]
	TIME [epoch: 8.43 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583442469578498		[learning rate: 1.7448e-05]
	Learning Rate: 1.74476e-05
	LOSS [training: 2.583442469578498 | validation: 2.5247931635932117]
	TIME [epoch: 8.45 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584023272814504		[learning rate: 1.7384e-05]
	Learning Rate: 1.73843e-05
	LOSS [training: 2.584023272814504 | validation: 2.538784503566234]
	TIME [epoch: 8.44 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5836328026766107		[learning rate: 1.7321e-05]
	Learning Rate: 1.73212e-05
	LOSS [training: 2.5836328026766107 | validation: 2.5382935825655775]
	TIME [epoch: 8.43 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581063786179685		[learning rate: 1.7258e-05]
	Learning Rate: 1.72584e-05
	LOSS [training: 2.581063786179685 | validation: 2.5374735582755505]
	TIME [epoch: 8.44 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578429988624646		[learning rate: 1.7196e-05]
	Learning Rate: 1.71957e-05
	LOSS [training: 2.578429988624646 | validation: 2.5366273181059587]
	TIME [epoch: 8.46 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584441588728974		[learning rate: 1.7133e-05]
	Learning Rate: 1.71333e-05
	LOSS [training: 2.584441588728974 | validation: 2.5251326212235634]
	TIME [epoch: 8.44 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5793276382511108		[learning rate: 1.7071e-05]
	Learning Rate: 1.70712e-05
	LOSS [training: 2.5793276382511108 | validation: 2.5284990480556657]
	TIME [epoch: 8.45 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5836233642344277		[learning rate: 1.7009e-05]
	Learning Rate: 1.70092e-05
	LOSS [training: 2.5836233642344277 | validation: 2.5336654393271916]
	TIME [epoch: 8.44 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5826511340449003		[learning rate: 1.6947e-05]
	Learning Rate: 1.69475e-05
	LOSS [training: 2.5826511340449003 | validation: 2.5283613841992256]
	TIME [epoch: 8.46 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5798252154875216		[learning rate: 1.6886e-05]
	Learning Rate: 1.6886e-05
	LOSS [training: 2.5798252154875216 | validation: 2.5203900485629633]
	TIME [epoch: 8.45 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5783726057673375		[learning rate: 1.6825e-05]
	Learning Rate: 1.68247e-05
	LOSS [training: 2.5783726057673375 | validation: 2.515749137538685]
	TIME [epoch: 8.43 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5789338766643337		[learning rate: 1.6764e-05]
	Learning Rate: 1.67636e-05
	LOSS [training: 2.5789338766643337 | validation: 2.520531638140949]
	TIME [epoch: 8.44 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580640104040986		[learning rate: 1.6703e-05]
	Learning Rate: 1.67028e-05
	LOSS [training: 2.580640104040986 | validation: 2.5194330162225382]
	TIME [epoch: 8.46 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580545438125881		[learning rate: 1.6642e-05]
	Learning Rate: 1.66422e-05
	LOSS [training: 2.580545438125881 | validation: 2.5262884879624403]
	TIME [epoch: 8.44 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.577728508186038		[learning rate: 1.6582e-05]
	Learning Rate: 1.65818e-05
	LOSS [training: 2.577728508186038 | validation: 2.52311492390854]
	TIME [epoch: 8.44 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5840868747249095		[learning rate: 1.6522e-05]
	Learning Rate: 1.65216e-05
	LOSS [training: 2.5840868747249095 | validation: 2.5257304359090114]
	TIME [epoch: 8.47 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578113258290127		[learning rate: 1.6462e-05]
	Learning Rate: 1.64617e-05
	LOSS [training: 2.578113258290127 | validation: 2.522033479734037]
	TIME [epoch: 8.47 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579290513635443		[learning rate: 1.6402e-05]
	Learning Rate: 1.64019e-05
	LOSS [training: 2.579290513635443 | validation: 2.5217017413208516]
	TIME [epoch: 8.44 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5784347165818002		[learning rate: 1.6342e-05]
	Learning Rate: 1.63424e-05
	LOSS [training: 2.5784347165818002 | validation: 2.530279941416173]
	TIME [epoch: 8.43 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582527995563903		[learning rate: 1.6283e-05]
	Learning Rate: 1.62831e-05
	LOSS [training: 2.582527995563903 | validation: 2.5383648700986807]
	TIME [epoch: 8.45 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582031676686274		[learning rate: 1.6224e-05]
	Learning Rate: 1.6224e-05
	LOSS [training: 2.582031676686274 | validation: 2.5321383672015663]
	TIME [epoch: 8.46 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581377679516311		[learning rate: 1.6165e-05]
	Learning Rate: 1.61651e-05
	LOSS [training: 2.581377679516311 | validation: 2.5255406952137385]
	TIME [epoch: 8.44 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581266442559495		[learning rate: 1.6106e-05]
	Learning Rate: 1.61065e-05
	LOSS [training: 2.581266442559495 | validation: 2.5290710574486157]
	TIME [epoch: 8.42 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583432750601357		[learning rate: 1.6048e-05]
	Learning Rate: 1.6048e-05
	LOSS [training: 2.583432750601357 | validation: 2.52592259695252]
	TIME [epoch: 8.43 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578892505218155		[learning rate: 1.599e-05]
	Learning Rate: 1.59898e-05
	LOSS [training: 2.578892505218155 | validation: 2.5257491270544343]
	TIME [epoch: 8.46 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5821912910783014		[learning rate: 1.5932e-05]
	Learning Rate: 1.59317e-05
	LOSS [training: 2.5821912910783014 | validation: 2.5196457380307504]
	TIME [epoch: 8.44 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5803438019420577		[learning rate: 1.5874e-05]
	Learning Rate: 1.58739e-05
	LOSS [training: 2.5803438019420577 | validation: 2.524969985136361]
	TIME [epoch: 8.44 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580897192218475		[learning rate: 1.5816e-05]
	Learning Rate: 1.58163e-05
	LOSS [training: 2.580897192218475 | validation: 2.5369198858481026]
	TIME [epoch: 8.44 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578323779010103		[learning rate: 1.5759e-05]
	Learning Rate: 1.57589e-05
	LOSS [training: 2.578323779010103 | validation: 2.5314530091143914]
	TIME [epoch: 8.45 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5820245508940998		[learning rate: 1.5702e-05]
	Learning Rate: 1.57017e-05
	LOSS [training: 2.5820245508940998 | validation: 2.533334994932253]
	TIME [epoch: 8.43 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582645595961117		[learning rate: 1.5645e-05]
	Learning Rate: 1.56447e-05
	LOSS [training: 2.582645595961117 | validation: 2.5212739049980577]
	TIME [epoch: 8.44 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579041896075502		[learning rate: 1.5588e-05]
	Learning Rate: 1.5588e-05
	LOSS [training: 2.579041896075502 | validation: 2.5282273308933636]
	TIME [epoch: 8.43 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583674003184667		[learning rate: 1.5531e-05]
	Learning Rate: 1.55314e-05
	LOSS [training: 2.583674003184667 | validation: 2.5230066825271895]
	TIME [epoch: 8.46 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5785004452412315		[learning rate: 1.5475e-05]
	Learning Rate: 1.5475e-05
	LOSS [training: 2.5785004452412315 | validation: 2.5225221598300784]
	TIME [epoch: 8.45 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580722992914823		[learning rate: 1.5419e-05]
	Learning Rate: 1.54189e-05
	LOSS [training: 2.580722992914823 | validation: 2.5212185245533707]
	TIME [epoch: 8.43 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5828749507551714		[learning rate: 1.5363e-05]
	Learning Rate: 1.53629e-05
	LOSS [training: 2.5828749507551714 | validation: 2.5299439929250633]
	TIME [epoch: 8.45 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584543648468558		[learning rate: 1.5307e-05]
	Learning Rate: 1.53072e-05
	LOSS [training: 2.584543648468558 | validation: 2.5284119823032905]
	TIME [epoch: 8.45 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5784229621426062		[learning rate: 1.5252e-05]
	Learning Rate: 1.52516e-05
	LOSS [training: 2.5784229621426062 | validation: 2.5190916553275047]
	TIME [epoch: 8.47 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5847330068968724		[learning rate: 1.5196e-05]
	Learning Rate: 1.51963e-05
	LOSS [training: 2.5847330068968724 | validation: 2.5300426306111854]
	TIME [epoch: 8.45 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5793634482692207		[learning rate: 1.5141e-05]
	Learning Rate: 1.51411e-05
	LOSS [training: 2.5793634482692207 | validation: 2.525671356902232]
	TIME [epoch: 8.44 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5809297511804563		[learning rate: 1.5086e-05]
	Learning Rate: 1.50862e-05
	LOSS [training: 2.5809297511804563 | validation: 2.527265950201803]
	TIME [epoch: 8.44 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583616292487721		[learning rate: 1.5031e-05]
	Learning Rate: 1.50314e-05
	LOSS [training: 2.583616292487721 | validation: 2.5256235295594323]
	TIME [epoch: 8.44 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5836841508237423		[learning rate: 1.4977e-05]
	Learning Rate: 1.49769e-05
	LOSS [training: 2.5836841508237423 | validation: 2.5196760381083103]
	TIME [epoch: 8.47 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583602727818267		[learning rate: 1.4923e-05]
	Learning Rate: 1.49225e-05
	LOSS [training: 2.583602727818267 | validation: 2.5260501377592286]
	TIME [epoch: 8.45 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579717000512451		[learning rate: 1.4868e-05]
	Learning Rate: 1.48684e-05
	LOSS [training: 2.579717000512451 | validation: 2.5342097714297296]
	TIME [epoch: 8.44 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5817714543995285		[learning rate: 1.4814e-05]
	Learning Rate: 1.48144e-05
	LOSS [training: 2.5817714543995285 | validation: 2.528674434559532]
	TIME [epoch: 8.44 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5839668946687957		[learning rate: 1.4761e-05]
	Learning Rate: 1.47606e-05
	LOSS [training: 2.5839668946687957 | validation: 2.5311088974706513]
	TIME [epoch: 8.44 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5823037580343877		[learning rate: 1.4707e-05]
	Learning Rate: 1.47071e-05
	LOSS [training: 2.5823037580343877 | validation: 2.525657089563886]
	TIME [epoch: 8.49 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583185476583025		[learning rate: 1.4654e-05]
	Learning Rate: 1.46537e-05
	LOSS [training: 2.583185476583025 | validation: 2.539621283788149]
	TIME [epoch: 8.45 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580416826035691		[learning rate: 1.4601e-05]
	Learning Rate: 1.46005e-05
	LOSS [training: 2.580416826035691 | validation: 2.5270596611185683]
	TIME [epoch: 8.42 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5799689022931838		[learning rate: 1.4548e-05]
	Learning Rate: 1.45475e-05
	LOSS [training: 2.5799689022931838 | validation: 2.5261595523752094]
	TIME [epoch: 8.43 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58353866191553		[learning rate: 1.4495e-05]
	Learning Rate: 1.44947e-05
	LOSS [training: 2.58353866191553 | validation: 2.5251086050492466]
	TIME [epoch: 8.45 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58396073197556		[learning rate: 1.4442e-05]
	Learning Rate: 1.44421e-05
	LOSS [training: 2.58396073197556 | validation: 2.5275817379348693]
	TIME [epoch: 8.44 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5789795821312955		[learning rate: 1.439e-05]
	Learning Rate: 1.43897e-05
	LOSS [training: 2.5789795821312955 | validation: 2.5186483351391473]
	TIME [epoch: 8.43 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581592892433899		[learning rate: 1.4338e-05]
	Learning Rate: 1.43375e-05
	LOSS [training: 2.581592892433899 | validation: 2.531188391508726]
	TIME [epoch: 8.44 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5831737831391		[learning rate: 1.4285e-05]
	Learning Rate: 1.42855e-05
	LOSS [training: 2.5831737831391 | validation: 2.527013663594183]
	TIME [epoch: 8.49 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5796226883363955		[learning rate: 1.4234e-05]
	Learning Rate: 1.42336e-05
	LOSS [training: 2.5796226883363955 | validation: 2.5222292538019024]
	TIME [epoch: 8.44 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.586785323691212		[learning rate: 1.4182e-05]
	Learning Rate: 1.4182e-05
	LOSS [training: 2.586785323691212 | validation: 2.5277383268002396]
	TIME [epoch: 8.43 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581980320369322		[learning rate: 1.4131e-05]
	Learning Rate: 1.41305e-05
	LOSS [training: 2.581980320369322 | validation: 2.5275449622753685]
	TIME [epoch: 8.43 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.584864278855226		[learning rate: 1.4079e-05]
	Learning Rate: 1.40792e-05
	LOSS [training: 2.584864278855226 | validation: 2.5234492337003767]
	TIME [epoch: 8.46 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5800007455795413		[learning rate: 1.4028e-05]
	Learning Rate: 1.40281e-05
	LOSS [training: 2.5800007455795413 | validation: 2.519994190768732]
	TIME [epoch: 8.43 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578614643819135		[learning rate: 1.3977e-05]
	Learning Rate: 1.39772e-05
	LOSS [training: 2.578614643819135 | validation: 2.523500569961538]
	TIME [epoch: 8.44 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585997353110246		[learning rate: 1.3927e-05]
	Learning Rate: 1.39265e-05
	LOSS [training: 2.585997353110246 | validation: 2.517063061586522]
	TIME [epoch: 8.43 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5840385362835585		[learning rate: 1.3876e-05]
	Learning Rate: 1.3876e-05
	LOSS [training: 2.5840385362835585 | validation: 2.531758966368975]
	TIME [epoch: 8.45 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5839835834822646		[learning rate: 1.3826e-05]
	Learning Rate: 1.38256e-05
	LOSS [training: 2.5839835834822646 | validation: 2.53077609769624]
	TIME [epoch: 8.45 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579590742520086		[learning rate: 1.3775e-05]
	Learning Rate: 1.37754e-05
	LOSS [training: 2.579590742520086 | validation: 2.52738203657119]
	TIME [epoch: 8.43 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5802221517623556		[learning rate: 1.3725e-05]
	Learning Rate: 1.37254e-05
	LOSS [training: 2.5802221517623556 | validation: 2.526508599289378]
	TIME [epoch: 8.43 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5845459061609306		[learning rate: 1.3676e-05]
	Learning Rate: 1.36756e-05
	LOSS [training: 2.5845459061609306 | validation: 2.5300551491619916]
	TIME [epoch: 8.45 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581324425178029		[learning rate: 1.3626e-05]
	Learning Rate: 1.3626e-05
	LOSS [training: 2.581324425178029 | validation: 2.5242144150037946]
	TIME [epoch: 8.43 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585452466054767		[learning rate: 1.3577e-05]
	Learning Rate: 1.35766e-05
	LOSS [training: 2.585452466054767 | validation: 2.5167014889149746]
	TIME [epoch: 8.44 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5856231995048993		[learning rate: 1.3527e-05]
	Learning Rate: 1.35273e-05
	LOSS [training: 2.5856231995048993 | validation: 2.524598596928005]
	TIME [epoch: 8.44 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5843293203660274		[learning rate: 1.3478e-05]
	Learning Rate: 1.34782e-05
	LOSS [training: 2.5843293203660274 | validation: 2.52932411590973]
	TIME [epoch: 8.45 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5817066646385736		[learning rate: 1.3429e-05]
	Learning Rate: 1.34293e-05
	LOSS [training: 2.5817066646385736 | validation: 2.532701691840531]
	TIME [epoch: 8.43 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5859133495436497		[learning rate: 1.3381e-05]
	Learning Rate: 1.33805e-05
	LOSS [training: 2.5859133495436497 | validation: 2.5291629884758304]
	TIME [epoch: 8.43 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.577983351200127		[learning rate: 1.3332e-05]
	Learning Rate: 1.3332e-05
	LOSS [training: 2.577983351200127 | validation: 2.5235380599674717]
	TIME [epoch: 8.44 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5838853301537483		[learning rate: 1.3284e-05]
	Learning Rate: 1.32836e-05
	LOSS [training: 2.5838853301537483 | validation: 2.5290357540296418]
	TIME [epoch: 8.46 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580767247637631		[learning rate: 1.3235e-05]
	Learning Rate: 1.32354e-05
	LOSS [training: 2.580767247637631 | validation: 2.5295840184467044]
	TIME [epoch: 8.43 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5819504338698005		[learning rate: 1.3187e-05]
	Learning Rate: 1.31874e-05
	LOSS [training: 2.5819504338698005 | validation: 2.515252648292119]
	TIME [epoch: 8.47 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5836875578492107		[learning rate: 1.314e-05]
	Learning Rate: 1.31395e-05
	LOSS [training: 2.5836875578492107 | validation: 2.527131417818176]
	TIME [epoch: 8.46 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579676905676037		[learning rate: 1.3092e-05]
	Learning Rate: 1.30918e-05
	LOSS [training: 2.579676905676037 | validation: 2.533601191312436]
	TIME [epoch: 8.45 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5825568380268997		[learning rate: 1.3044e-05]
	Learning Rate: 1.30443e-05
	LOSS [training: 2.5825568380268997 | validation: 2.5274557543285527]
	TIME [epoch: 8.43 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5818330872992186		[learning rate: 1.2997e-05]
	Learning Rate: 1.2997e-05
	LOSS [training: 2.5818330872992186 | validation: 2.519435185348777]
	TIME [epoch: 8.42 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5817471357974466		[learning rate: 1.295e-05]
	Learning Rate: 1.29498e-05
	LOSS [training: 2.5817471357974466 | validation: 2.534886268922696]
	TIME [epoch: 8.45 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5801014341781263		[learning rate: 1.2903e-05]
	Learning Rate: 1.29028e-05
	LOSS [training: 2.5801014341781263 | validation: 2.5329938347598753]
	TIME [epoch: 8.45 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5874511157643028		[learning rate: 1.2856e-05]
	Learning Rate: 1.2856e-05
	LOSS [training: 2.5874511157643028 | validation: 2.5212651158310386]
	TIME [epoch: 8.43 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5815649723904825		[learning rate: 1.2809e-05]
	Learning Rate: 1.28093e-05
	LOSS [training: 2.5815649723904825 | validation: 2.529709774988598]
	TIME [epoch: 8.43 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578664167068006		[learning rate: 1.2763e-05]
	Learning Rate: 1.27628e-05
	LOSS [training: 2.578664167068006 | validation: 2.522150663831681]
	TIME [epoch: 8.45 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580219471566026		[learning rate: 1.2717e-05]
	Learning Rate: 1.27165e-05
	LOSS [training: 2.580219471566026 | validation: 2.525612053699355]
	TIME [epoch: 8.43 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580725033640534		[learning rate: 1.267e-05]
	Learning Rate: 1.26704e-05
	LOSS [training: 2.580725033640534 | validation: 2.530766346371518]
	TIME [epoch: 8.44 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581900752392656		[learning rate: 1.2624e-05]
	Learning Rate: 1.26244e-05
	LOSS [training: 2.581900752392656 | validation: 2.5327652902884408]
	TIME [epoch: 8.43 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580764769860335		[learning rate: 1.2579e-05]
	Learning Rate: 1.25786e-05
	LOSS [training: 2.580764769860335 | validation: 2.524619215754777]
	TIME [epoch: 8.45 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5784025734704787		[learning rate: 1.2533e-05]
	Learning Rate: 1.25329e-05
	LOSS [training: 2.5784025734704787 | validation: 2.5198759065059173]
	TIME [epoch: 8.43 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5817367504553674		[learning rate: 1.2487e-05]
	Learning Rate: 1.24874e-05
	LOSS [training: 2.5817367504553674 | validation: 2.526179341703936]
	TIME [epoch: 8.44 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5824849087684987		[learning rate: 1.2442e-05]
	Learning Rate: 1.24421e-05
	LOSS [training: 2.5824849087684987 | validation: 2.5211930980126214]
	TIME [epoch: 8.43 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5824344302634774		[learning rate: 1.2397e-05]
	Learning Rate: 1.2397e-05
	LOSS [training: 2.5824344302634774 | validation: 2.5191463596820785]
	TIME [epoch: 8.45 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58073726099168		[learning rate: 1.2352e-05]
	Learning Rate: 1.2352e-05
	LOSS [training: 2.58073726099168 | validation: 2.523256667625507]
	TIME [epoch: 8.44 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579738192788492		[learning rate: 1.2307e-05]
	Learning Rate: 1.23072e-05
	LOSS [training: 2.579738192788492 | validation: 2.528219575496445]
	TIME [epoch: 8.43 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582717992476475		[learning rate: 1.2263e-05]
	Learning Rate: 1.22625e-05
	LOSS [training: 2.582717992476475 | validation: 2.523276169471033]
	TIME [epoch: 8.44 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582407554126732		[learning rate: 1.2218e-05]
	Learning Rate: 1.2218e-05
	LOSS [training: 2.582407554126732 | validation: 2.5243412060538915]
	TIME [epoch: 8.45 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581192778157197		[learning rate: 1.2174e-05]
	Learning Rate: 1.21737e-05
	LOSS [training: 2.581192778157197 | validation: 2.5232191551884107]
	TIME [epoch: 8.44 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.585762936539944		[learning rate: 1.2129e-05]
	Learning Rate: 1.21295e-05
	LOSS [training: 2.585762936539944 | validation: 2.5319951932332607]
	TIME [epoch: 8.44 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5784415898195987		[learning rate: 1.2085e-05]
	Learning Rate: 1.20855e-05
	LOSS [training: 2.5784415898195987 | validation: 2.535832757781273]
	TIME [epoch: 8.48 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5760748953775505		[learning rate: 1.2042e-05]
	Learning Rate: 1.20416e-05
	LOSS [training: 2.5760748953775505 | validation: 2.535557563141873]
	TIME [epoch: 8.47 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580966148057482		[learning rate: 1.1998e-05]
	Learning Rate: 1.19979e-05
	LOSS [training: 2.580966148057482 | validation: 2.5183975405136514]
	TIME [epoch: 8.43 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5850044640670395		[learning rate: 1.1954e-05]
	Learning Rate: 1.19544e-05
	LOSS [training: 2.5850044640670395 | validation: 2.5244006612330847]
	TIME [epoch: 8.43 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5806603929203944		[learning rate: 1.1911e-05]
	Learning Rate: 1.1911e-05
	LOSS [training: 2.5806603929203944 | validation: 2.535556665434064]
	TIME [epoch: 8.43 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581042047970773		[learning rate: 1.1868e-05]
	Learning Rate: 1.18678e-05
	LOSS [training: 2.581042047970773 | validation: 2.527576280369405]
	TIME [epoch: 8.49 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5840798700041248		[learning rate: 1.1825e-05]
	Learning Rate: 1.18247e-05
	LOSS [training: 2.5840798700041248 | validation: 2.5291719796854935]
	TIME [epoch: 8.46 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5808675793146287		[learning rate: 1.1782e-05]
	Learning Rate: 1.17818e-05
	LOSS [training: 2.5808675793146287 | validation: 2.53444153746874]
	TIME [epoch: 8.44 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5804026421296067		[learning rate: 1.1739e-05]
	Learning Rate: 1.1739e-05
	LOSS [training: 2.5804026421296067 | validation: 2.5200792720460425]
	TIME [epoch: 8.43 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5817762225920973		[learning rate: 1.1696e-05]
	Learning Rate: 1.16964e-05
	LOSS [training: 2.5817762225920973 | validation: 2.5249008732125215]
	TIME [epoch: 8.46 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582128347874302		[learning rate: 1.1654e-05]
	Learning Rate: 1.1654e-05
	LOSS [training: 2.582128347874302 | validation: 2.527777261322705]
	TIME [epoch: 8.43 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5806798107777116		[learning rate: 1.1612e-05]
	Learning Rate: 1.16117e-05
	LOSS [training: 2.5806798107777116 | validation: 2.525124587205571]
	TIME [epoch: 8.47 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580691189628373		[learning rate: 1.157e-05]
	Learning Rate: 1.15695e-05
	LOSS [training: 2.580691189628373 | validation: 2.5332627595664046]
	TIME [epoch: 8.47 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5823729873120067		[learning rate: 1.1528e-05]
	Learning Rate: 1.15275e-05
	LOSS [training: 2.5823729873120067 | validation: 2.5314880685270245]
	TIME [epoch: 8.45 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583104673419969		[learning rate: 1.1486e-05]
	Learning Rate: 1.14857e-05
	LOSS [training: 2.583104673419969 | validation: 2.5297645480921163]
	TIME [epoch: 8.43 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5815934459013534		[learning rate: 1.1444e-05]
	Learning Rate: 1.1444e-05
	LOSS [training: 2.5815934459013534 | validation: 2.5213607134167613]
	TIME [epoch: 8.44 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.583905265369603		[learning rate: 1.1402e-05]
	Learning Rate: 1.14025e-05
	LOSS [training: 2.583905265369603 | validation: 2.5235627631982314]
	TIME [epoch: 8.43 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5798304659536453		[learning rate: 1.1361e-05]
	Learning Rate: 1.13611e-05
	LOSS [training: 2.5798304659536453 | validation: 2.5337374984551557]
	TIME [epoch: 8.46 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58391920647123		[learning rate: 1.132e-05]
	Learning Rate: 1.13199e-05
	LOSS [training: 2.58391920647123 | validation: 2.5314136206936197]
	TIME [epoch: 8.43 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580486655028578		[learning rate: 1.1279e-05]
	Learning Rate: 1.12788e-05
	LOSS [training: 2.580486655028578 | validation: 2.5315148797262568]
	TIME [epoch: 8.43 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579833976036666		[learning rate: 1.1238e-05]
	Learning Rate: 1.12379e-05
	LOSS [training: 2.579833976036666 | validation: 2.527934888420059]
	TIME [epoch: 8.44 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5808787925629733		[learning rate: 1.1197e-05]
	Learning Rate: 1.11971e-05
	LOSS [training: 2.5808787925629733 | validation: 2.5319765392980536]
	TIME [epoch: 8.45 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.579724123969833		[learning rate: 1.1156e-05]
	Learning Rate: 1.11565e-05
	LOSS [training: 2.579724123969833 | validation: 2.53152416443396]
	TIME [epoch: 8.43 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5837258107512944		[learning rate: 1.1116e-05]
	Learning Rate: 1.1116e-05
	LOSS [training: 2.5837258107512944 | validation: 2.5249727803393163]
	TIME [epoch: 8.47 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5797802350420627		[learning rate: 1.1076e-05]
	Learning Rate: 1.10756e-05
	LOSS [training: 2.5797802350420627 | validation: 2.526190246423571]
	TIME [epoch: 8.45 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5825095383298953		[learning rate: 1.1035e-05]
	Learning Rate: 1.10354e-05
	LOSS [training: 2.5825095383298953 | validation: 2.5328144643564707]
	TIME [epoch: 8.45 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5835520664452876		[learning rate: 1.0995e-05]
	Learning Rate: 1.09954e-05
	LOSS [training: 2.5835520664452876 | validation: 2.5282822449238784]
	TIME [epoch: 8.43 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.578803731125284		[learning rate: 1.0955e-05]
	Learning Rate: 1.09555e-05
	LOSS [training: 2.578803731125284 | validation: 2.534076819591589]
	TIME [epoch: 8.45 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5808911682464997		[learning rate: 1.0916e-05]
	Learning Rate: 1.09157e-05
	LOSS [training: 2.5808911682464997 | validation: 2.5259168050167347]
	TIME [epoch: 8.44 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5815990616126903		[learning rate: 1.0876e-05]
	Learning Rate: 1.08761e-05
	LOSS [training: 2.5815990616126903 | validation: 2.518782058383376]
	TIME [epoch: 8.45 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5840726413339654		[learning rate: 1.0837e-05]
	Learning Rate: 1.08366e-05
	LOSS [training: 2.5840726413339654 | validation: 2.512313343444085]
	TIME [epoch: 8.43 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5806643713395254		[learning rate: 1.0797e-05]
	Learning Rate: 1.07973e-05
	LOSS [training: 2.5806643713395254 | validation: 2.5241358892598487]
	TIME [epoch: 8.44 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5853280329610078		[learning rate: 1.0758e-05]
	Learning Rate: 1.07581e-05
	LOSS [training: 2.5853280329610078 | validation: 2.5342866459966418]
	TIME [epoch: 8.45 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5768036443133107		[learning rate: 1.0719e-05]
	Learning Rate: 1.07191e-05
	LOSS [training: 2.5768036443133107 | validation: 2.519715676891861]
	TIME [epoch: 8.44 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5808583763128787		[learning rate: 1.068e-05]
	Learning Rate: 1.06802e-05
	LOSS [training: 2.5808583763128787 | validation: 2.5255866582550706]
	TIME [epoch: 8.43 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582786460290614		[learning rate: 1.0641e-05]
	Learning Rate: 1.06414e-05
	LOSS [training: 2.582786460290614 | validation: 2.5294487521547104]
	TIME [epoch: 8.43 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5807689008228056		[learning rate: 1.0603e-05]
	Learning Rate: 1.06028e-05
	LOSS [training: 2.5807689008228056 | validation: 2.5213624315423466]
	TIME [epoch: 8.46 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5851470735550213		[learning rate: 1.0564e-05]
	Learning Rate: 1.05643e-05
	LOSS [training: 2.5851470735550213 | validation: 2.5263838283125892]
	TIME [epoch: 8.44 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580732652069923		[learning rate: 1.0526e-05]
	Learning Rate: 1.0526e-05
	LOSS [training: 2.580732652069923 | validation: 2.5215432778790063]
	TIME [epoch: 8.44 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5804307301103044		[learning rate: 1.0488e-05]
	Learning Rate: 1.04878e-05
	LOSS [training: 2.5804307301103044 | validation: 2.525185259448943]
	TIME [epoch: 8.44 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.581235477012476		[learning rate: 1.045e-05]
	Learning Rate: 1.04497e-05
	LOSS [training: 2.581235477012476 | validation: 2.5310742313632777]
	TIME [epoch: 8.46 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5850263608276496		[learning rate: 1.0412e-05]
	Learning Rate: 1.04118e-05
	LOSS [training: 2.5850263608276496 | validation: 2.5265822208373434]
	TIME [epoch: 8.44 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5786478336987995		[learning rate: 1.0374e-05]
	Learning Rate: 1.0374e-05
	LOSS [training: 2.5786478336987995 | validation: 2.533626263470052]
	TIME [epoch: 8.44 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5791099730764975		[learning rate: 1.0336e-05]
	Learning Rate: 1.03364e-05
	LOSS [training: 2.5791099730764975 | validation: 2.531946838234065]
	TIME [epoch: 8.44 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5785850700061497		[learning rate: 1.0299e-05]
	Learning Rate: 1.02989e-05
	LOSS [training: 2.5785850700061497 | validation: 2.527472625025421]
	TIME [epoch: 8.45 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.580222581751668		[learning rate: 1.0261e-05]
	Learning Rate: 1.02615e-05
	LOSS [training: 2.580222581751668 | validation: 2.525438205302515]
	TIME [epoch: 8.44 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5827142312889135		[learning rate: 1.0224e-05]
	Learning Rate: 1.02243e-05
	LOSS [training: 2.5827142312889135 | validation: 2.524641672378544]
	TIME [epoch: 8.43 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.58160282491049		[learning rate: 1.0187e-05]
	Learning Rate: 1.01872e-05
	LOSS [training: 2.58160282491049 | validation: 2.525205206244979]
	TIME [epoch: 8.47 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5781695134415736		[learning rate: 1.015e-05]
	Learning Rate: 1.01502e-05
	LOSS [training: 2.5781695134415736 | validation: 2.5233697438069456]
	TIME [epoch: 8.49 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5804096182342495		[learning rate: 1.0113e-05]
	Learning Rate: 1.01133e-05
	LOSS [training: 2.5804096182342495 | validation: 2.5225335514704526]
	TIME [epoch: 8.43 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582833979266108		[learning rate: 1.0077e-05]
	Learning Rate: 1.00766e-05
	LOSS [training: 2.582833979266108 | validation: 2.527619032250917]
	TIME [epoch: 8.43 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.582051963766714		[learning rate: 1.004e-05]
	Learning Rate: 1.00401e-05
	LOSS [training: 2.582051963766714 | validation: 2.533326372381203]
	TIME [epoch: 8.45 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5821951524320563		[learning rate: 1.0004e-05]
	Learning Rate: 1.00036e-05
	LOSS [training: 2.5821951524320563 | validation: 2.526558027289016]
	TIME [epoch: 8.47 sec]
Finished training in 17028.294 seconds.
