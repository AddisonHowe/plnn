Args:
Namespace(name='model_tr_study201', outdir='out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1', training_data='data/transition_rate_studies/tr_study201/tr_study201_training/r1', validation_data='data/transition_rate_studies/tr_study201/tr_study201_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1552073424

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.369511696181036		[learning rate: 0.01]
		[batch 20/20] avg loss: 11.600443353168078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.984977524674557 | validation: 12.361485724329103]
	TIME [epoch: 53.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.642460487859612		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.278655531878718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.960558009869166 | validation: 4.174138873614898]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.902231517890604		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.877948001691419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.890089759791012 | validation: 3.4085187659909018]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.986190148479741		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.578762112786907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7824761306333246 | validation: 4.846215985021381]
	TIME [epoch: 8.41 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.80861569761027		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.4891461502245935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.648880923917432 | validation: 3.6760998336928905]
	TIME [epoch: 8.42 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.520123659831405		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.7557760605534565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.63794986019243 | validation: 3.3652409874488662]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.398452804583832		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.238339840520704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3183963225522675 | validation: 2.9198802356837597]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.16134417839844		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.064986952676027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1131655655372334 | validation: 3.533521429733935]
	TIME [epoch: 8.42 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.079943691268758		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8428561591433286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.961399925206044 | validation: 2.859496685154683]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9565465560327455		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.984482100335078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9705143281839126 | validation: 2.9016622281781697]
	TIME [epoch: 8.43 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.553397563794956		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.518260556540388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5358290601676723 | validation: 3.0886256716083706]
	TIME [epoch: 8.42 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4540183596075416		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3690557809686545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.411537070288098 | validation: 2.5657045451291545]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1976762214614474		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3804660037634817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.289071112612464 | validation: 2.579493245777173]
	TIME [epoch: 8.46 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3021171216185143		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.308237778159275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3051774498888946 | validation: 3.042821989736312]
	TIME [epoch: 8.43 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2172453922570567		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2708132130989176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.244029302677988 | validation: 2.576224200981202]
	TIME [epoch: 8.43 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9986166230669324		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2405459458361614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.119581284451547 | validation: 2.7727826368112036]
	TIME [epoch: 8.44 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.040199950262561		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.993475780678122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0168378654703414 | validation: 2.131776653613296]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8993511009983974		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8508008584994884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.875075979748943 | validation: 3.235064167535541]
	TIME [epoch: 8.44 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.972241714705702		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.858945794694681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9155937547001916 | validation: 2.069224047607283]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9361428014939		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8882233435111866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9121830725025437 | validation: 2.0071204820414477]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8294469225207264		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6231310740394145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.72628899828007 | validation: 1.8811813362392351]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7013760540717553		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7721281645494593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7367521093106073 | validation: 2.664812269530511]
	TIME [epoch: 8.43 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.682006343126477		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4552995631133063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.568652953119892 | validation: 2.1162054685240346]
	TIME [epoch: 8.43 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.768413498355407		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.566898359650033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.66765592900272 | validation: 1.9940999949597504]
	TIME [epoch: 8.45 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.706321684732451		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5337543597888796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6200380222606654 | validation: 2.0704423926767093]
	TIME [epoch: 8.43 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5776843962417595		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6591546326700928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6184195144559257 | validation: 1.934883949661841]
	TIME [epoch: 8.43 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4337289348723026		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3225688205203956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3781488776963493 | validation: 1.8494232578067769]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5034922451328456		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5097676061750858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.506629925653965 | validation: 2.2844037994997977]
	TIME [epoch: 8.46 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3608130247924977		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3171652385328887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.338989131662693 | validation: 1.532993627955773]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2648949177984243		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2755671134311397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.270231015614782 | validation: 2.204059759092848]
	TIME [epoch: 8.43 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2651877851356255		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0317175143745825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1484526497551037 | validation: 2.119610546597129]
	TIME [epoch: 8.47 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2673596492994275		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.296486915513766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2819232824065967 | validation: 1.4406086696387241]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9271002311784762		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.893423362406206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9102617967923414 | validation: 1.8238840421405174]
	TIME [epoch: 8.43 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8639217113672852		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.127106773978408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9955142426728465 | validation: 1.921132083825639]
	TIME [epoch: 8.44 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.953073652093305		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8943688637457345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9237212579195202 | validation: 1.8380015795903963]
	TIME [epoch: 8.5 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9129808733804032		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8828505359825076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8979157046814552 | validation: 1.126971724061606]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8432912779774953		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8673343756279455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8553128268027201 | validation: 1.621148923027981]
	TIME [epoch: 8.43 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7516638594074698		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7965135123245406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7740886858660054 | validation: 1.2134253226417318]
	TIME [epoch: 8.43 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6695731159509903		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6563922023420652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6629826591465282 | validation: 0.9642812854943619]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.786315274851742		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6766779649505483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7314966199011448 | validation: 1.6018908297776435]
	TIME [epoch: 8.47 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.634335270950066		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8165477907804057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7254415308652358 | validation: 1.671746199941699]
	TIME [epoch: 8.45 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6003341970646499		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7090103071909142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6546722521277826 | validation: 0.9650352110727141]
	TIME [epoch: 8.46 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.526655060094355		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7338361188951008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6302455894947279 | validation: 1.1533526264402179]
	TIME [epoch: 8.44 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7798806767251072		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.696992532530437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.738436604627772 | validation: 1.2557138353844557]
	TIME [epoch: 8.48 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.660039104516733		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5855839988382687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6228115516775012 | validation: 1.5903379263835762]
	TIME [epoch: 8.5 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.626552981383479		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4153363979282985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5209446896558887 | validation: 1.2510316861133217]
	TIME [epoch: 8.46 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5193202208369523		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5875652082000729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5534427145185128 | validation: 1.2194287220324003]
	TIME [epoch: 8.45 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.65013735668053		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5389698536317362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.594553605156133 | validation: 2.150873093799066]
	TIME [epoch: 8.47 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5845255730536647		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6195849145205383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6020552437871018 | validation: 1.1859551183286228]
	TIME [epoch: 8.45 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5276335113495807		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.586358617836601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5569960645930911 | validation: 1.460288121193594]
	TIME [epoch: 8.45 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6127205785296461		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4789377960854597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.545829187307553 | validation: 0.9214984103743253]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5576513221077088		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4479522968040528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5028018094558808 | validation: 1.0696332614873998]
	TIME [epoch: 8.46 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.623858740604092		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.569266093113921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5965624168590065 | validation: 1.1513731773051976]
	TIME [epoch: 8.44 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5604800518708906		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5037547453056714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5321173985882812 | validation: 1.0365728101138223]
	TIME [epoch: 8.44 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4518277833182252		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6083883443977722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5301080638579985 | validation: 1.0354740804518288]
	TIME [epoch: 8.47 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5811664476269762		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5788313805569552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5799989140919655 | validation: 2.246929832555419]
	TIME [epoch: 8.44 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.63917428947268		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3380852442426199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4886297668576496 | validation: 1.2232314776021314]
	TIME [epoch: 8.45 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5695602272399494		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6474304527795323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6084953400097408 | validation: 1.9117697002492902]
	TIME [epoch: 8.44 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5426952347492986		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4746546650512111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5086749499002547 | validation: 1.8210341552130158]
	TIME [epoch: 8.47 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5559542562746134		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.514072576274558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5350134162745857 | validation: 1.5382536520677692]
	TIME [epoch: 8.45 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.003166042281822		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7128927263612692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8580293843215454 | validation: 1.0720043902786314]
	TIME [epoch: 8.45 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.460533152167961		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5270849934055266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4938090727867441 | validation: 0.8430598001122672]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.386115850054165		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5634387345038294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.474777292278997 | validation: 0.9253008692692466]
	TIME [epoch: 8.48 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3166364114320381		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5300686670194863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.423352539225762 | validation: 0.889784581233973]
	TIME [epoch: 8.45 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4324108783713818		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.472925537894691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4526682081330367 | validation: 1.0750830956257449]
	TIME [epoch: 8.43 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4347559836962103		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.441520233241625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4381381084689175 | validation: 2.1141987153619555]
	TIME [epoch: 8.42 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.678909151244826		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5416530377892534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6102810945170396 | validation: 0.9690047823814504]
	TIME [epoch: 8.46 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.39177493737284		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.476630927185647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4342029322792436 | validation: 1.065316951827386]
	TIME [epoch: 8.43 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5449898038379675		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5110715599381357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5280306818880516 | validation: 2.1392063905729626]
	TIME [epoch: 8.43 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3652790108701072		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.454245040116397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4097620254932524 | validation: 1.1154621324455785]
	TIME [epoch: 8.43 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.29977527105153		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.426837844649635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3633065578505825 | validation: 1.8345320625439387]
	TIME [epoch: 8.47 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5794093528509903		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3534009418327184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4664051473418542 | validation: 1.0058786810015095]
	TIME [epoch: 8.43 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3765758390620058		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.414959278986915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3957675590244605 | validation: 0.9502146501831255]
	TIME [epoch: 8.44 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2884439764041622		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2158293852327913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2521366808184768 | validation: 0.7476836027611136]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2240092071822635		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5511639517253777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3875865794538202 | validation: 1.1286865518552696]
	TIME [epoch: 8.45 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4528883066347835		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2939817372158895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3734350219253366 | validation: 0.8416514702969049]
	TIME [epoch: 8.43 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4348783215867886		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5375591953902021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4862187584884956 | validation: 0.871817709600229]
	TIME [epoch: 8.43 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3487771849804835		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3217766508669058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3352769179236945 | validation: 0.8202736112922686]
	TIME [epoch: 8.44 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3603314270227023		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.250556598264992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3054440126438471 | validation: 1.3090532760661817]
	TIME [epoch: 8.49 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4242539075797316		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4378800755545895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4310669915671608 | validation: 0.7784599802305182]
	TIME [epoch: 8.42 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3910593138709753		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3462128015108947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3686360576909353 | validation: 1.5328258137077597]
	TIME [epoch: 8.46 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.432642788665637		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.507448043895095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.970045416280366 | validation: 0.8826150775163562]
	TIME [epoch: 8.44 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4047150904388341		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3323689252916011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.368542007865218 | validation: 1.1857543541532436]
	TIME [epoch: 8.45 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4544530454536793		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.483051837114329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.468752441284004 | validation: 0.8026971632784514]
	TIME [epoch: 8.43 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.498810712596966		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.171958547600373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8353846300986691 | validation: 1.2342123809221937]
	TIME [epoch: 8.42 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.624701128147692		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.578378125029084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6015396265883879 | validation: 1.7877158512200362]
	TIME [epoch: 8.43 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4560216972995597		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4142966388093734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4351591680544662 | validation: 0.8870281704617125]
	TIME [epoch: 8.49 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.333874074609774		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2600689370795979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.296971505844686 | validation: 1.5578558967553493]
	TIME [epoch: 8.43 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2219727303111596		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3089351857831253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2654539580471424 | validation: 1.4792061268295065]
	TIME [epoch: 8.42 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4107260584153747		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.201971219236549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.306348638825962 | validation: 0.722977501661923]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.471733117387341		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2360355649995507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.353884341193446 | validation: 1.10493035097721]
	TIME [epoch: 8.46 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.386481332611697		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3213382449587874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.353909788785242 | validation: 0.7109891806926435]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r1_20240219_191309/states/model_tr_study201_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.327793097929375		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.456773139806994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3922831188681843 | validation: 0.802632067714786]
	TIME [epoch: 8.42 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2965828259471028		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5245544622690026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.410568644108053 | validation: 0.7695373023311173]
	TIME [epoch: 8.44 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3234954384997308		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2702869385863003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2968911885430154 | validation: 0.8354263475317725]
	TIME [epoch: 8.49 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3772413275773765		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5216358937634535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.449438610670415 | validation: 1.0449832109153077]
	TIME [epoch: 8.41 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.614106354159849		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.258153025802668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4361296899812586 | validation: 1.2116267504327412]
	TIME [epoch: 8.43 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6975712383835293		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3491981367180326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.523384687550781 | validation: 2.9086253509112043]
	TIME [epoch: 8.42 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4371080429418082		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3190815303047516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3780947866232798 | validation: 0.9077914827151317]
	TIME [epoch: 8.46 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4374823949887057		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2230661820033109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3302742884960082 | validation: 1.4179440711393698]
	TIME [epoch: 8.42 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6837891829695102		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 1.7255062980573825		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 1.7046477405134464 | validation: 1.308444825635252]
	TIME [epoch: 8.44 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3950823744347294		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 1.3371868747791136		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 1.3661346246069215 | validation: 0.8704519132809366]
	TIME [epoch: 8.44 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3470330464310674		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 1.380426621612061		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 1.3637298340215642 | validation: 0.7573337067222696]
	TIME [epoch: 8.45 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2950902165946352		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 1.3984272268071578		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 1.3467587217008963 | validation: 0.8571282580496514]
	TIME [epoch: 8.42 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4589611760338108		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 1.1971254900855932		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 1.3280433330597021 | validation: 0.7500817046672211]
	TIME [epoch: 8.44 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6032965928891354		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 1.2381758159999696		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 1.4207362044445528 | validation: 1.039719662873123]
	TIME [epoch: 8.47 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4875765575268143		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 1.468699055370426		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 1.4781378064486206 | validation: 1.2944204748647206]
	TIME [epoch: 8.45 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4338263110176963		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 1.3999323776518557		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 1.4168793443347754 | validation: 1.0912238233878944]
	TIME [epoch: 8.42 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.337694146051495		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 1.57426003157664		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 1.4559770888140675 | validation: 1.0196406010513333]
	TIME [epoch: 8.44 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5412211269521854		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 1.2428164090461211		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 1.3920187679991531 | validation: 1.209318279784173]
	TIME [epoch: 8.46 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2423976802787005		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 1.3337495940066026		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 1.2880736371426516 | validation: 1.0592626664668745]
	TIME [epoch: 8.44 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2961304216537692		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 1.3046210354360226		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 1.3003757285448958 | validation: 0.7865776297311569]
	TIME [epoch: 8.43 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3896165234928428		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 1.4032682622361774		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 1.39644239286451 | validation: 1.017960104370411]
	TIME [epoch: 8.44 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.134741436559854		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 2.1229450904096643		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 1.6288432634847592 | validation: 0.9741232066095562]
	TIME [epoch: 8.46 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4453679137618458		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 1.5816800746356776		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 1.5135239941987617 | validation: 1.7749474222412163]
	TIME [epoch: 8.45 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4231846102308279		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 1.354743560438156		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 1.3889640853344918 | validation: 1.0152788001741515]
	TIME [epoch: 8.44 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6669000422320817		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 1.460650417910274		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 1.5637752300711778 | validation: 1.9945556419775237]
	TIME [epoch: 8.43 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0080517243199703		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 1.5201932698681795		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 1.7641224970940748 | validation: 1.333476919092207]
	TIME [epoch: 8.45 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5292487451728134		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 1.410051732390722		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 1.4696502387817674 | validation: 1.2772708178303103]
	TIME [epoch: 8.44 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6580812229516393		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 1.5151926183266033		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 1.586636920639121 | validation: 2.3128970707143752]
	TIME [epoch: 8.44 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4275114067372514		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 1.7464915496464168		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 2.087001478191834 | validation: 1.3045253037818194]
	TIME [epoch: 8.44 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4768757451310248		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 1.587450107600792		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 1.5321629263659085 | validation: 1.3041679208843315]
	TIME [epoch: 8.46 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5168673589097135		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 1.3974078846155287		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 1.457137621762621 | validation: 1.086955621695258]
	TIME [epoch: 8.44 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3041660747726305		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 1.5355875385510767		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 1.4198768066618537 | validation: 1.36706281497111]
	TIME [epoch: 8.43 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3641552039156792		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 1.7671499067391523		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 1.5656525553274159 | validation: 0.9427993813079263]
	TIME [epoch: 8.44 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3121863562367913		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 1.3219943182458775		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 1.3170903372413343 | validation: 0.9353768537740713]
	TIME [epoch: 8.5 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.137865725547599		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 1.2909487282503487		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 1.2144072268989736 | validation: 0.7998191456565591]
	TIME [epoch: 8.43 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2197258702569487		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 1.1420147072332503		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 1.1808702887450995 | validation: 1.9527257685541812]
	TIME [epoch: 8.44 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.300844978671581		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 1.7590073597177072		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 1.5299261691946442 | validation: 1.9928347742795918]
	TIME [epoch: 8.47 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4073931677114984		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 1.7122822453126383		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 2.0598377065120683 | validation: 0.9750750115476273]
	TIME [epoch: 8.45 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3479256045246317		[learning rate: 0.008952]
		[batch 20/20] avg loss: 2.199045375459989		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 1.77348548999231 | validation: 0.8791289085926581]
	TIME [epoch: 8.44 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.384203789370294		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 1.225570706398064		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 1.3048872478841793 | validation: 0.7687567295085851]
	TIME [epoch: 8.43 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3191140709403477		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 1.2817884927397643		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 1.8004512818400555 | validation: 1.3348160491920202]
	TIME [epoch: 8.43 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4132593643803155		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 1.4209225277064437		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 1.4170909460433792 | validation: 1.0514365080162607]
	TIME [epoch: 8.51 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1920042162704179		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 1.80751930349204		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 1.499761759881229 | validation: 0.7962751528445051]
	TIME [epoch: 8.43 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.791297571318734		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 2.3007392367495685		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 2.046018404034151 | validation: 1.3980101346892913]
	TIME [epoch: 8.43 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5288124257481575		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 1.257124892021325		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 1.3929686588847408 | validation: 1.2247260275066234]
	TIME [epoch: 8.44 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4877655049010563		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 1.282679884669917		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 1.3852226947854869 | validation: 1.1767247353517751]
	TIME [epoch: 8.46 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.464394368372457		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 1.8590995787397664		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 1.6617469735561123 | validation: 0.9672455189034418]
	TIME [epoch: 8.44 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.301886348271502		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 1.5162560639689082		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 1.409071206120205 | validation: 0.8926412747150416]
	TIME [epoch: 8.44 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2050408934855394		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 1.4382370857080953		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 1.3216389895968172 | validation: 1.1232664758551358]
	TIME [epoch: 8.48 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.134346325335858		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 2.7520729431786912		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 1.9432096342572742 | validation: 1.2894539927944322]
	TIME [epoch: 8.45 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3115095243250066		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 1.4834254180533895		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 1.397467471189198 | validation: 1.0254479940251549]
	TIME [epoch: 8.44 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3443654842096966		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 1.5425320545730354		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 1.443448769391366 | validation: 1.2090103538932375]
	TIME [epoch: 8.44 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2828850296468035		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 1.2842810811059953		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 1.2835830553763992 | validation: 1.220603302763755]
	TIME [epoch: 8.44 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.510986756280851		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 1.490324194337383		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 1.500655475309117 | validation: 1.2201000771480452]
	TIME [epoch: 8.46 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4680382377554495		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 1.3770011402843454		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 1.4225196890198974 | validation: 0.7188206346566519]
	TIME [epoch: 8.44 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3251932413805985		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 1.2288573533862603		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 1.2770252973834295 | validation: 1.0774406894098314]
	TIME [epoch: 8.45 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3900281150236093		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 1.2508766207712012		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 1.320452367897405 | validation: 0.843541526626091]
	TIME [epoch: 8.43 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.025307851015521		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 1.1968598677937732		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 1.1110838594046468 | validation: 1.5153041265026992]
	TIME [epoch: 8.47 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3386832196610863		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 1.2378777422366005		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 1.2882804809488433 | validation: 0.7420346955113475]
	TIME [epoch: 8.44 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4741827182638187		[learning rate: 0.008294]
		[batch 20/20] avg loss: 1.2154215709166678		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 1.3448021445902434 | validation: 0.765680210523628]
	TIME [epoch: 8.44 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.837004059213286		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 1.2338815362791675		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 1.5354427977462266 | validation: 1.6747480596705522]
	TIME [epoch: 8.48 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0661456830998346		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 1.8874832445463987		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 1.4768144638231167 | validation: 1.490178284252643]
	TIME [epoch: 8.46 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1914646846069281		[learning rate: 0.008204]
		[batch 20/20] avg loss: 1.26359913320788		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 1.2275319089074042 | validation: 1.0531711568946849]
	TIME [epoch: 8.44 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3531691914290267		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 1.7653692889829955		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 1.559269240206011 | validation: 0.8429334654135149]
	TIME [epoch: 8.44 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4089322323787818		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 1.1773653124794705		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 1.293148772429126 | validation: 0.888182231015276]
	TIME [epoch: 8.44 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.177884813998569		[learning rate: 0.008115]
		[batch 20/20] avg loss: 1.2706568156682219		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 1.224270814833395 | validation: 2.1118022048144587]
	TIME [epoch: 8.48 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2365428070663533		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 1.5782345788683734		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 1.4073886929673631 | validation: 1.0311452949859339]
	TIME [epoch: 8.44 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.310607310603068		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 1.9913850882450563		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 1.6509961994240618 | validation: 5.697449890387101]
	TIME [epoch: 8.45 sec]
EPOCH 161/2000:
	Training over batches...
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.05
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.025
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.0125
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.00625
ERROR:
Encountered nan in loss and reached the maximum number of model alterations: 4.
