Args:
Namespace(name='model_tr_study201', outdir='out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5', training_data='data/transition_rate_studies/tr_study201/tr_study201_training/r5', validation_data='data/transition_rate_studies/tr_study201/tr_study201_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3231744212

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.929016712532292		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.475172852622592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.20209478257744 | validation: 8.096558508479308]
	TIME [epoch: 47.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.044312052005157		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.603977831041079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.824144941523119 | validation: 4.960677157282035]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.211478128573025		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.010117133733798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.11079763115341 | validation: 4.017666175590685]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.915419695351354		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.887337987798177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.901378841574766 | validation: 4.424484082958675]
	TIME [epoch: 8.87 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.773419055574871		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.641037277384717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.707228166479792 | validation: 3.630311008734387]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.678729191748557		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.6173666611219435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.64804792643525 | validation: 3.4707094480386944]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.441530354230207		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.514287054093815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.47790870416201 | validation: 4.256779704069492]
	TIME [epoch: 8.89 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.613699339459336		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.387935613152598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.500817476305967 | validation: 3.7199521785953373]
	TIME [epoch: 8.87 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.363095885148205		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.370302508919738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.866699197033972 | validation: 5.442968165284844]
	TIME [epoch: 8.86 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.759127622163693		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.431099384924115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.595113503543905 | validation: 3.3799647342583214]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.644946263695703		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.321727818076827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.483337040886264 | validation: 3.432105265525538]
	TIME [epoch: 8.87 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.298398273019486		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.2943253997180895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.296361836368788 | validation: 3.3912049778206996]
	TIME [epoch: 8.89 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.230102777401024		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.2927162438755655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.261409510638295 | validation: 3.10332736961326]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.668009919165678		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.361269877276709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.014639898221192 | validation: 3.9181766174372696]
	TIME [epoch: 8.88 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.611207863197859		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.775920122800164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.693563992999012 | validation: 3.2008356438558097]
	TIME [epoch: 8.88 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.514539437811625		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.851245095077766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.682892266444695 | validation: 3.762993153252564]
	TIME [epoch: 8.9 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.5067633696070155		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.9949986413403176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.250881005473665 | validation: 3.8659417114364025]
	TIME [epoch: 8.89 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.301081114346593		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.1705065467587605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.235793830552675 | validation: 3.4742091677913827]
	TIME [epoch: 8.88 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.237694136323477		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.206234533084951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.221964334704213 | validation: 3.1932427037778424]
	TIME [epoch: 8.88 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.058272387793677		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.118505369043492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.088388878418584 | validation: 3.32687443953934]
	TIME [epoch: 8.88 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.922519617376628		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.191729537562684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.057124577469655 | validation: 3.195839825165269]
	TIME [epoch: 8.9 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.94165053589447		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.063225623895126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.002438079894796 | validation: 2.645704175363442]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.169988268652039		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.332530190088402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.251259229370222 | validation: 3.659196583857801]
	TIME [epoch: 8.88 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9476781208956657		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.18206732098073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0648727209381965 | validation: 4.238936208267708]
	TIME [epoch: 8.89 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.035412174351149		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.891772743114249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.963592458732699 | validation: 3.0094164318164776]
	TIME [epoch: 8.88 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.051040410733494		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.812562302579605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.931801356656549 | validation: 3.1078098223109127]
	TIME [epoch: 8.89 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7476839903986687		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.34344293605091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.045563463224788 | validation: 3.4255245824869403]
	TIME [epoch: 8.87 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.02982681260917		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.198027987540657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.113927400074912 | validation: 3.2016405288195857]
	TIME [epoch: 8.87 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.066510202183328		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.493059761773219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.279784981978272 | validation: 3.5487129649045066]
	TIME [epoch: 8.88 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.709778311994866		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.9985766779641394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3541774949795045 | validation: 2.9398582575123324]
	TIME [epoch: 8.89 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7385479312855656		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.290610053901711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0145789925936395 | validation: 3.467642266895349]
	TIME [epoch: 8.88 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.934846596047163		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.30635099085015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.120598793448657 | validation: 3.1084013012499576]
	TIME [epoch: 8.88 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.298253960769673		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6232664376513255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9607601992104997 | validation: 3.2285782981246225]
	TIME [epoch: 8.87 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.057429403522299		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7369666978041827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8971980506632407 | validation: 2.9097358351827425]
	TIME [epoch: 8.88 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7003944148393835		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.023780593159896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.862087503999639 | validation: 3.580881742832786]
	TIME [epoch: 8.9 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.101801009964406		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.110018433743986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1059097218541964 | validation: 3.9904671331417028]
	TIME [epoch: 8.88 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.078226720170088		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.943535053367804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0108808867689465 | validation: 3.829163698918504]
	TIME [epoch: 8.87 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.1542963554492305		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.6738763430819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.414086349265565 | validation: 2.9732176326034363]
	TIME [epoch: 8.87 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8424030164126295		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.173075907991899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.007739462202264 | validation: 3.7263535157855032]
	TIME [epoch: 8.87 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.931506893693883		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8813609290691424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9064339113815123 | validation: 3.9577670909162443]
	TIME [epoch: 8.89 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.224839935394833		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7906806400835364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.007760287739185 | validation: 3.6958651309184307]
	TIME [epoch: 8.87 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.950513973922393		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8595474595907886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.905030716756591 | validation: 3.1357477215509877]
	TIME [epoch: 8.87 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5720410345902813		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.332964932793905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.952502983692093 | validation: 3.1576234750675347]
	TIME [epoch: 8.88 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.906119650083652		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7019643731496514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8040420116166516 | validation: 2.8672859352790994]
	TIME [epoch: 8.9 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7752644552449475		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8348143538164075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.805039404530677 | validation: 2.9866912930121794]
	TIME [epoch: 8.88 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7304316712726626		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6391707702531826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6848012207629224 | validation: 2.6399271527019987]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8421134071212037		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.62334377659624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7327285918587223 | validation: 2.855824192979209]
	TIME [epoch: 8.87 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.519567193415532		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.644954973990214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5822610837028734 | validation: 2.606914255630475]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4241304280539793		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7893149787059435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.606722703379962 | validation: 2.8257961797140436]
	TIME [epoch: 8.9 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8937029572967163		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5908127656356763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7422578614661965 | validation: 3.738542467872472]
	TIME [epoch: 8.86 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8419784092375244		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.764027783061929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.803003096149726 | validation: 2.675303328910012]
	TIME [epoch: 8.85 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.783970658183643		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.874446104566868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8292083813752553 | validation: 2.950717368358839]
	TIME [epoch: 8.86 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9153273937361504		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5672799705802793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7413036821582155 | validation: 2.7957919140967227]
	TIME [epoch: 8.86 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.16698062619749		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.181410802242812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.174195714220152 | validation: 2.8308966184259146]
	TIME [epoch: 8.88 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.687119858502469		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.566642548553575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.626881203528022 | validation: 2.816282438875842]
	TIME [epoch: 8.86 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.72228893321772		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.9763243299940605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8493066316058906 | validation: 2.823564069702617]
	TIME [epoch: 8.86 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.571272995495277		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.711587314884606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6414301551899415 | validation: 2.723376946017607]
	TIME [epoch: 8.86 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4592179640748575		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.834980975287087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6470994696809726 | validation: 2.7254531819111207]
	TIME [epoch: 8.87 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.561473707797159		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.794234330790996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6778540192940774 | validation: 2.9284008021390266]
	TIME [epoch: 8.87 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4714886544337027		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7662982834451695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6188934689394365 | validation: 4.0852261419491676]
	TIME [epoch: 8.86 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.762627840536768		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5129524747509797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.637790157643874 | validation: 2.9158846190004915]
	TIME [epoch: 8.87 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6087424313529213		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6712727754976426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.640007603425281 | validation: 2.7191620330954245]
	TIME [epoch: 8.85 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4342125210855543		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.886864543494787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.660538532290171 | validation: 3.303598875750836]
	TIME [epoch: 8.88 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4571112151920147		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5955955660671863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5263533906295996 | validation: 3.014355286137263]
	TIME [epoch: 8.86 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6317907690519036		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5495050966601815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5906479328560423 | validation: 2.6029195489302936]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.546589386472108		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6271423645218177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.586865875496964 | validation: 3.0897158281811734]
	TIME [epoch: 8.86 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4853426358826285		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.723970459082872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.60465654748275 | validation: 2.602527553247169]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.586357907969822		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.519906596762155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5531322523659887 | validation: 3.168409012566329]
	TIME [epoch: 8.89 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.687905304740204		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.511539668049069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5997224863946373 | validation: 3.3014133114293567]
	TIME [epoch: 8.87 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5409804299748076		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5948586443047383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5679195371397734 | validation: 2.71494625459071]
	TIME [epoch: 8.86 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5610473210224365		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5785912862689626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5698193036457 | validation: 2.7352163847443016]
	TIME [epoch: 8.87 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6060058487753155		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3709330663684085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4884694575718624 | validation: 3.0300465262398504]
	TIME [epoch: 8.89 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4841776803464852		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.485463762760994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4848207215537386 | validation: 2.779166344252888]
	TIME [epoch: 8.87 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5820243161746697		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.4061201137104433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4940722149425563 | validation: 2.8229650576389727]
	TIME [epoch: 8.87 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.496161246299534		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3059497136881197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.401055479993827 | validation: 2.6995627712210863]
	TIME [epoch: 8.86 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4864080173049494		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.475028056226404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4807180367656763 | validation: 2.723534127381963]
	TIME [epoch: 8.87 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4958709541043644		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.623016357837404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5594436559708833 | validation: 2.778004007986821]
	TIME [epoch: 8.88 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.553844691093601		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.4279862854602663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.490915488276933 | validation: 3.72223357891494]
	TIME [epoch: 8.87 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5984884362446707		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5431791519862914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.570833794115481 | validation: 2.732844792512968]
	TIME [epoch: 8.86 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3734449968084674		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7924197508806246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.582932373844545 | validation: 2.498219898519773]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.453509375141759		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.4796482819492374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.466578828545498 | validation: 2.4464061190771846]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.063997248115833		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5937511770455814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8288742125807067 | validation: 2.514952939330857]
	TIME [epoch: 8.86 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.46812809740186		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6507384098645765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5594332536332187 | validation: 2.7207749190623343]
	TIME [epoch: 8.85 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4829209725438885		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.4633813352248453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.473151153884367 | validation: 3.890332469853976]
	TIME [epoch: 8.85 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.646105572953787		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3078291078003637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.476967340377075 | validation: 2.7148671292194915]
	TIME [epoch: 8.84 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6473628738010766		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1626847279333083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4050238008671925 | validation: 2.5665117111790323]
	TIME [epoch: 8.85 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.382407214150453		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.598136401902939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4902718080266957 | validation: 2.7860225488122436]
	TIME [epoch: 8.85 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3444878339843513		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.642298252271356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4933930431278535 | validation: 2.455953219166533]
	TIME [epoch: 8.84 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.515963960957314		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.253158016273202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.384560988615258 | validation: 2.988971658083245]
	TIME [epoch: 8.84 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.474070639595169		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8543486929332103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6642096662641896 | validation: 2.4570936435755213]
	TIME [epoch: 8.83 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5882368705301317		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3516595187024807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.469948194616306 | validation: 2.315784860874912]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7395819447415932		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.739817075354437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7396995100480153 | validation: 2.415329924530279]
	TIME [epoch: 8.85 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.595177576115147		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.60347883517683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.599328205645988 | validation: 3.104228009202589]
	TIME [epoch: 8.85 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.392172105100935		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6430969538773503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5176345294891425 | validation: 2.588695473741625]
	TIME [epoch: 8.84 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.617436698081163		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.228807839343731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.423122268712447 | validation: 2.9700538531519176]
	TIME [epoch: 8.85 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.444590572603734		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.231803113038225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3381968428209796 | validation: 2.491111625660654]
	TIME [epoch: 8.87 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.403289261349957		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3761376198544775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3897134406022182 | validation: 2.4354668386682867]
	TIME [epoch: 8.85 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2314655249220516		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.346878591291491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2891720581067716 | validation: 2.163745336570756]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5489872926271895		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6242325905666393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5866099415969144 | validation: 2.236325928980464]
	TIME [epoch: 8.86 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2415896886440208		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5578045936749234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3996971411594723 | validation: 2.4272542486119715]
	TIME [epoch: 8.87 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.298799583646081		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 3.264298162142393		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 3.281548872894237 | validation: 2.2784168971489196]
	TIME [epoch: 8.86 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1070776927107238		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 3.604256466501911		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 3.3556670796063166 | validation: 2.35970943519415]
	TIME [epoch: 8.85 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3540889790005695		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 3.373759502420159		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 3.3639242407103636 | validation: 2.613645457804619]
	TIME [epoch: 8.85 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3414614085048284		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 3.150877925251764		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 3.2461696668782962 | validation: 2.289426825121507]
	TIME [epoch: 8.85 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2050313994413457		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 3.535676704687898		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 3.3703540520646222 | validation: 3.2610472551915852]
	TIME [epoch: 8.87 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.358126305515089		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 3.073743187047711		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 3.2159347462813996 | validation: 3.2423601050629602]
	TIME [epoch: 8.85 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1684344398264868		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 3.1548116495244174		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 3.1616230446754514 | validation: 2.5742034797527986]
	TIME [epoch: 8.85 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.143470236909189		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 3.2459764066050014		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 3.194723321757095 | validation: 2.289814461906682]
	TIME [epoch: 8.85 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.004061646006521		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 3.2542694568176396		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 3.12916555141208 | validation: 2.4008941904665524]
	TIME [epoch: 8.86 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9558296765389		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 2.8801180970849787		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 2.9179738868119394 | validation: 1.7625242437360034]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9012753822095823		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 3.0977550207927185		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 2.9995152015011515 | validation: 2.0833463487856867]
	TIME [epoch: 8.86 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7282867870873466		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 2.9105214621522886		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 2.8194041246198176 | validation: 1.8503313181706422]
	TIME [epoch: 8.85 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8229193538539645		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 2.7740819211597687		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 2.798500637506866 | validation: 1.777008901390421]
	TIME [epoch: 8.85 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.769949626177497		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 2.645218242706508		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 2.707583934442002 | validation: 2.3410941160381475]
	TIME [epoch: 8.87 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8124654967905145		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 2.7875700030472665		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 2.80001774991889 | validation: 2.690689774086078]
	TIME [epoch: 8.85 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.95206164211024		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 2.9990671851337622		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 2.975564413622001 | validation: 2.169193559157705]
	TIME [epoch: 8.85 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.767246541138805		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 2.8303183826743377		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 2.7987824619065713 | validation: 1.8838780097381669]
	TIME [epoch: 8.85 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.72941159348372		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 2.8116449074530463		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 2.770528250468382 | validation: 2.9595730424569795]
	TIME [epoch: 8.85 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.674908613599432		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 2.978045161746794		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 2.8264768876731123 | validation: 2.2016842584175675]
	TIME [epoch: 8.87 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.907932704006623		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 2.618565270089273		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 2.7632489870479473 | validation: 2.225785886284397]
	TIME [epoch: 8.85 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8308036095399958		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 2.4693962689009754		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 2.650099939220486 | validation: 2.247843526248286]
	TIME [epoch: 8.85 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5721207885287765		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 2.770898304712758		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 2.6715095466207672 | validation: 3.020361772059179]
	TIME [epoch: 8.85 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.945836437820989		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 2.496525363794338		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 2.7211809008076635 | validation: 2.147474030842533]
	TIME [epoch: 8.86 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6733047447251566		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 2.922458383617503		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 2.7978815641713295 | validation: 2.492992355808275]
	TIME [epoch: 8.87 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7529997278736547		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 2.8165593706423016		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 2.7847795492579785 | validation: 2.5820293403560135]
	TIME [epoch: 8.86 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.889305461558072		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 2.7345848658970295		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 2.811945163727551 | validation: 1.8541662018769003]
	TIME [epoch: 8.86 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.651939135013838		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 2.834021149184948		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 2.742980142099393 | validation: 2.6547449335360613]
	TIME [epoch: 8.85 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.746536865623124		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 2.677057746180032		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 2.711797305901578 | validation: 2.448381714013872]
	TIME [epoch: 8.86 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.651071159406399		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 2.993262569274804		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 2.822166864340601 | validation: 2.7239942269281867]
	TIME [epoch: 8.86 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.941992191158947		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 2.3516091884616857		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 2.646800689810316 | validation: 1.6421160130389587]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6747930545820333		[learning rate: 0.008952]
		[batch 20/20] avg loss: 2.57929090374344		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 2.6270419791627364 | validation: 1.809547184025241]
	TIME [epoch: 8.86 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5672544654535274		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 2.8495284001265118		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 2.708391432790019 | validation: 1.6577679249568065]
	TIME [epoch: 8.84 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.806704244454006		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 2.5959483365707454		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 2.701326290512376 | validation: 2.4139482855046683]
	TIME [epoch: 8.87 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7967185688619116		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 2.680187398149216		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 2.7384529835055638 | validation: 2.3550749597671343]
	TIME [epoch: 8.85 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8966160701361234		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 2.474473246092876		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 2.685544658114499 | validation: 1.8859118421160792]
	TIME [epoch: 8.85 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6628233759553677		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 2.851417582835474		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 2.7571204793954207 | validation: 1.7079563500036672]
	TIME [epoch: 8.85 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6540270278827642		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 2.5933343670096827		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 2.6236806974462237 | validation: 1.720225270113945]
	TIME [epoch: 8.85 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7977324971718494		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 2.7228209707937276		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 2.760276733982788 | validation: 1.678177313685858]
	TIME [epoch: 8.87 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4948134391328134		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 2.653983526642592		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 2.574398482887703 | validation: 2.34335462905982]
	TIME [epoch: 8.85 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.572106832815672		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 2.7198905666941804		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 2.6459986997549265 | validation: 2.3201751873403103]
	TIME [epoch: 8.85 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.616281040245105		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 2.748060006659029		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 2.6821705234520676 | validation: 1.879430739190195]
	TIME [epoch: 8.85 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.482956253293212		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 2.776706223720079		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 2.629831238506645 | validation: 1.6948492177055114]
	TIME [epoch: 8.85 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4906528880930554		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 2.778537783573215		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 2.6345953358331355 | validation: 1.7331725914399292]
	TIME [epoch: 8.87 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.563312182254574		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 2.6975350907499918		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 2.630423636502283 | validation: 2.0890790403181287]
	TIME [epoch: 8.85 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3494620213721227		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 2.805983026231009		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 2.5777225238015657 | validation: 1.6923785115036227]
	TIME [epoch: 8.85 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6865994672704483		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 2.832817103390984		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 2.759708285330716 | validation: 1.682176783604606]
	TIME [epoch: 8.84 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.740689209325958		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 2.620166164984777		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 2.680427687155367 | validation: 1.5976000910863117]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6396444884849837		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 2.7232886718491343		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 2.681466580167059 | validation: 1.6543054628928724]
	TIME [epoch: 8.85 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.594031687573242		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 2.804196686220533		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 2.699114186896888 | validation: 1.7675389327416282]
	TIME [epoch: 8.84 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.619785247430854		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 2.389669196886321		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 2.504727222158588 | validation: 1.661373008255383]
	TIME [epoch: 8.85 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.47349747247923		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 2.5342492261455103		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 2.5038733493123706 | validation: 1.7654203933569912]
	TIME [epoch: 8.85 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5371268923244337		[learning rate: 0.008294]
		[batch 20/20] avg loss: 2.603893135709932		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 2.570510014017183 | validation: 2.1935680114013945]
	TIME [epoch: 8.86 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5327361993807522		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 2.5520852983499123		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 2.542410748865332 | validation: 1.6776475251499288]
	TIME [epoch: 8.85 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6433170705850144		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 2.632797353641121		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 2.6380572121130674 | validation: 1.9831103661659064]
	TIME [epoch: 8.84 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6672176983533555		[learning rate: 0.008204]
		[batch 20/20] avg loss: 2.2861258401315725		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 2.4766717692424636 | validation: 2.2185038044234195]
	TIME [epoch: 8.84 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5289639508090005		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 2.5183049129212174		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 2.5236344318651094 | validation: 1.6033280313670015]
	TIME [epoch: 8.84 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5260439355459248		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 2.506835100466985		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 2.516439518006455 | validation: 1.9344386716312927]
	TIME [epoch: 8.87 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.657335555888099		[learning rate: 0.008115]
		[batch 20/20] avg loss: 2.5174401894833265		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 2.5873878726857127 | validation: 2.3457142880631143]
	TIME [epoch: 8.85 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9711529343169683		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 2.5350533959100083		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 2.753103165113488 | validation: 1.7417662598661123]
	TIME [epoch: 8.84 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4055865070341698		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 2.6075932174178513		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 2.50658986222601 | validation: 2.0987517468893504]
	TIME [epoch: 8.85 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.610510084326071		[learning rate: 0.008027]
		[batch 20/20] avg loss: 2.313938622031412		[learning rate: 0.0080124]
	Learning Rate: 0.00801241
	LOSS [training: 2.4622243531787413 | validation: 1.6092574986017891]
	TIME [epoch: 8.86 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4553319233465416		[learning rate: 0.0079979]
		[batch 20/20] avg loss: 2.522497614325233		[learning rate: 0.0079833]
	Learning Rate: 0.00798333
	LOSS [training: 2.4889147688358877 | validation: 2.4029399348943206]
	TIME [epoch: 8.85 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6144607134665163		[learning rate: 0.0079688]
		[batch 20/20] avg loss: 2.488444725468441		[learning rate: 0.0079544]
	Learning Rate: 0.00795436
	LOSS [training: 2.551452719467479 | validation: 1.893301863643849]
	TIME [epoch: 8.85 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.49928759238718		[learning rate: 0.0079399]
		[batch 20/20] avg loss: 2.961270786164183		[learning rate: 0.0079255]
	Learning Rate: 0.00792549
	LOSS [training: 2.7302791892756813 | validation: 1.6031747072661668]
	TIME [epoch: 8.86 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.139383791816358		[learning rate: 0.0079111]
		[batch 20/20] avg loss: 2.412762930842931		[learning rate: 0.0078967]
	Learning Rate: 0.00789673
	LOSS [training: 2.2760733613296447 | validation: 2.928844448482814]
	TIME [epoch: 8.85 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.749918362309864		[learning rate: 0.0078824]
		[batch 20/20] avg loss: 2.630777039726987		[learning rate: 0.0078681]
	Learning Rate: 0.00786807
	LOSS [training: 2.6903477010184256 | validation: 1.85238442614737]
	TIME [epoch: 8.87 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.372952159304754		[learning rate: 0.0078538]
		[batch 20/20] avg loss: 2.7766994362811186		[learning rate: 0.0078395]
	Learning Rate: 0.00783952
	LOSS [training: 2.5748257977929367 | validation: 1.6714172766181021]
	TIME [epoch: 8.85 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.694388312210651		[learning rate: 0.0078253]
		[batch 20/20] avg loss: 2.590342465349509		[learning rate: 0.0078111]
	Learning Rate: 0.00781107
	LOSS [training: 2.6423653887800804 | validation: 1.9298868898154313]
	TIME [epoch: 8.84 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6492099275844496		[learning rate: 0.0077969]
		[batch 20/20] avg loss: 2.483396921036054		[learning rate: 0.0077827]
	Learning Rate: 0.00778272
	LOSS [training: 2.566303424310252 | validation: 2.34058772783215]
	TIME [epoch: 8.85 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.483754350179738		[learning rate: 0.0077686]
		[batch 20/20] avg loss: 2.2735990673099726		[learning rate: 0.0077545]
	Learning Rate: 0.00775448
	LOSS [training: 2.378676708744855 | validation: 2.856853868073721]
	TIME [epoch: 8.85 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4279805130685714		[learning rate: 0.0077404]
		[batch 20/20] avg loss: 2.7361016460346144		[learning rate: 0.0077263]
	Learning Rate: 0.00772634
	LOSS [training: 2.5820410795515927 | validation: 2.0879249188183913]
	TIME [epoch: 8.87 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4968483708459916		[learning rate: 0.0077123]
		[batch 20/20] avg loss: 2.554678400720495		[learning rate: 0.0076983]
	Learning Rate: 0.0076983
	LOSS [training: 2.5257633857832436 | validation: 2.082600636002929]
	TIME [epoch: 8.85 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3516153975903826		[learning rate: 0.0076843]
		[batch 20/20] avg loss: 2.4545319955912133		[learning rate: 0.0076704]
	Learning Rate: 0.00767036
	LOSS [training: 2.403073696590798 | validation: 1.6332498728692244]
	TIME [epoch: 8.85 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3058073700952866		[learning rate: 0.0076564]
		[batch 20/20] avg loss: 2.434766948055762		[learning rate: 0.0076425]
	Learning Rate: 0.00764252
	LOSS [training: 2.3702871590755237 | validation: 2.5903007308416006]
	TIME [epoch: 8.85 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.618676447864435		[learning rate: 0.0076286]
		[batch 20/20] avg loss: 2.231844202266479		[learning rate: 0.0076148]
	Learning Rate: 0.00761479
	LOSS [training: 2.4252603250654565 | validation: 1.6550859542907164]
	TIME [epoch: 8.87 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.075190517916287		[learning rate: 0.007601]
		[batch 20/20] avg loss: 2.417768003129366		[learning rate: 0.0075872]
	Learning Rate: 0.00758715
	LOSS [training: 2.2464792605228263 | validation: 1.5337326514559249]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.262393536893671		[learning rate: 0.0075734]
		[batch 20/20] avg loss: 2.708833195574023		[learning rate: 0.0075596]
	Learning Rate: 0.00755962
	LOSS [training: 2.485613366233847 | validation: 1.5281601836802794]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4372916313717363		[learning rate: 0.0075459]
		[batch 20/20] avg loss: 2.5562073754067263		[learning rate: 0.0075322]
	Learning Rate: 0.00753219
	LOSS [training: 2.496749503389231 | validation: 1.7509282644704949]
	TIME [epoch: 8.85 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.62580928448845		[learning rate: 0.0075185]
		[batch 20/20] avg loss: 2.3949401408543194		[learning rate: 0.0075049]
	Learning Rate: 0.00750485
	LOSS [training: 2.5103747126713847 | validation: 1.6558600721827845]
	TIME [epoch: 8.85 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.277846324842822		[learning rate: 0.0074912]
		[batch 20/20] avg loss: 2.8130991554915097		[learning rate: 0.0074776]
	Learning Rate: 0.00747762
	LOSS [training: 2.5454727401671655 | validation: 1.6723554849527913]
	TIME [epoch: 8.87 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.330261921569775		[learning rate: 0.007464]
		[batch 20/20] avg loss: 2.6814104398461045		[learning rate: 0.0074505]
	Learning Rate: 0.00745048
	LOSS [training: 2.5058361807079397 | validation: 4.67889252899631]
	TIME [epoch: 8.85 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6347221014616338		[learning rate: 0.0074369]
		[batch 20/20] avg loss: 2.26937941373335		[learning rate: 0.0074234]
	Learning Rate: 0.00742344
	LOSS [training: 2.4520507575974912 | validation: 1.6589589375358071]
	TIME [epoch: 8.84 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.296420409906053		[learning rate: 0.00741]
		[batch 20/20] avg loss: 2.720717526715707		[learning rate: 0.0073965]
	Learning Rate: 0.0073965
	LOSS [training: 2.50856896831088 | validation: 1.6143605626849975]
	TIME [epoch: 8.84 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5138896006864737		[learning rate: 0.0073831]
		[batch 20/20] avg loss: 2.4245051128901625		[learning rate: 0.0073697]
	Learning Rate: 0.00736966
	LOSS [training: 2.4691973567883183 | validation: 1.5648092595728051]
	TIME [epoch: 8.84 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4272813903747825		[learning rate: 0.0073563]
		[batch 20/20] avg loss: 2.4965804181709097		[learning rate: 0.0073429]
	Learning Rate: 0.00734291
	LOSS [training: 2.4619309042728466 | validation: 1.539765476508319]
	TIME [epoch: 8.87 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.624635025289728		[learning rate: 0.0073296]
		[batch 20/20] avg loss: 2.40042964314804		[learning rate: 0.0073163]
	Learning Rate: 0.00731627
	LOSS [training: 2.5125323342188843 | validation: 2.0041551151993433]
	TIME [epoch: 8.85 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.40708603453052		[learning rate: 0.007303]
		[batch 20/20] avg loss: 2.6971259723357646		[learning rate: 0.0072897]
	Learning Rate: 0.00728971
	LOSS [training: 2.552106003433142 | validation: 1.6506986910549664]
	TIME [epoch: 8.85 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.142863294477093		[learning rate: 0.0072765]
		[batch 20/20] avg loss: 2.3568037402339037		[learning rate: 0.0072633]
	Learning Rate: 0.00726326
	LOSS [training: 2.2498335173554986 | validation: 1.699827323599797]
	TIME [epoch: 8.84 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.498595933331417		[learning rate: 0.0072501]
		[batch 20/20] avg loss: 2.477532207576098		[learning rate: 0.0072369]
	Learning Rate: 0.0072369
	LOSS [training: 2.4880640704537575 | validation: 2.3082742614088163]
	TIME [epoch: 8.87 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3199408418568686		[learning rate: 0.0072238]
		[batch 20/20] avg loss: 2.7871537479464323		[learning rate: 0.0072106]
	Learning Rate: 0.00721064
	LOSS [training: 2.55354729490165 | validation: 2.084923193895938]
	TIME [epoch: 8.86 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.647774644236638		[learning rate: 0.0071975]
		[batch 20/20] avg loss: 2.5815800070780694		[learning rate: 0.0071845]
	Learning Rate: 0.00718447
	LOSS [training: 2.614677325657354 | validation: 1.753198524066854]
	TIME [epoch: 8.85 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4171625380343875		[learning rate: 0.0071714]
		[batch 20/20] avg loss: 2.6409545674370185		[learning rate: 0.0071584]
	Learning Rate: 0.0071584
	LOSS [training: 2.5290585527357035 | validation: 1.7858503829339498]
	TIME [epoch: 8.85 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4790445149267524		[learning rate: 0.0071454]
		[batch 20/20] avg loss: 2.444967263443347		[learning rate: 0.0071324]
	Learning Rate: 0.00713242
	LOSS [training: 2.46200588918505 | validation: 1.5370737348803496]
	TIME [epoch: 8.84 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1772689030206616		[learning rate: 0.0071195]
		[batch 20/20] avg loss: 2.34008644402302		[learning rate: 0.0071065]
	Learning Rate: 0.00710653
	LOSS [training: 2.2586776735218406 | validation: 1.650037578514962]
	TIME [epoch: 8.87 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2127815673853695		[learning rate: 0.0070936]
		[batch 20/20] avg loss: 2.430252739908793		[learning rate: 0.0070807]
	Learning Rate: 0.00708074
	LOSS [training: 2.3215171536470818 | validation: 2.7462965020226577]
	TIME [epoch: 8.85 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5007816571772357		[learning rate: 0.0070679]
		[batch 20/20] avg loss: 2.5226182903266667		[learning rate: 0.007055]
	Learning Rate: 0.00705505
	LOSS [training: 2.5116999737519508 | validation: 2.4102909351135096]
	TIME [epoch: 8.84 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4685616103890835		[learning rate: 0.0070422]
		[batch 20/20] avg loss: 2.5748021773629515		[learning rate: 0.0070294]
	Learning Rate: 0.00702945
	LOSS [training: 2.521681893876017 | validation: 2.118629114784803]
	TIME [epoch: 8.84 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.464611725238433		[learning rate: 0.0070167]
		[batch 20/20] avg loss: 2.5348447428831813		[learning rate: 0.0070039]
	Learning Rate: 0.00700394
	LOSS [training: 2.4997282340608065 | validation: 2.1037218182465454]
	TIME [epoch: 8.84 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5042470263477172		[learning rate: 0.0069912]
		[batch 20/20] avg loss: 2.463806355681569		[learning rate: 0.0069785]
	Learning Rate: 0.00697852
	LOSS [training: 2.484026691014643 | validation: 2.3751945650418818]
	TIME [epoch: 8.87 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.626649598873221		[learning rate: 0.0069658]
		[batch 20/20] avg loss: 2.421943360129835		[learning rate: 0.0069532]
	Learning Rate: 0.00695319
	LOSS [training: 2.5242964795015284 | validation: 1.6605180262086787]
	TIME [epoch: 8.85 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.385769279341942		[learning rate: 0.0069406]
		[batch 20/20] avg loss: 2.4978298782375368		[learning rate: 0.006928]
	Learning Rate: 0.00692796
	LOSS [training: 2.441799578789739 | validation: 2.479955719811785]
	TIME [epoch: 8.84 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5237903232401906		[learning rate: 0.0069154]
		[batch 20/20] avg loss: 2.514167132115502		[learning rate: 0.0069028]
	Learning Rate: 0.00690282
	LOSS [training: 2.518978727677846 | validation: 1.9535692202529518]
	TIME [epoch: 8.85 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8368478236499457		[learning rate: 0.0068903]
		[batch 20/20] avg loss: 2.5047532646047452		[learning rate: 0.0068778]
	Learning Rate: 0.00687777
	LOSS [training: 2.6708005441273457 | validation: 1.5426116174582554]
	TIME [epoch: 8.85 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.584025544609709		[learning rate: 0.0068653]
		[batch 20/20] avg loss: 2.320668763878305		[learning rate: 0.0068528]
	Learning Rate: 0.00685281
	LOSS [training: 2.4523471542440074 | validation: 1.6765666124522576]
	TIME [epoch: 8.87 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.398430248722776		[learning rate: 0.0068404]
		[batch 20/20] avg loss: 2.108202117560917		[learning rate: 0.0068279]
	Learning Rate: 0.00682794
	LOSS [training: 2.2533161831418465 | validation: 1.5853828706926036]
	TIME [epoch: 8.85 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5749327517997402		[learning rate: 0.0068155]
		[batch 20/20] avg loss: 2.4582646855647585		[learning rate: 0.0068032]
	Learning Rate: 0.00680316
	LOSS [training: 2.5165987186822485 | validation: 2.4714449617320597]
	TIME [epoch: 8.86 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.691682551846489		[learning rate: 0.0067908]
		[batch 20/20] avg loss: 2.138770851488938		[learning rate: 0.0067785]
	Learning Rate: 0.00677847
	LOSS [training: 2.4152267016677134 | validation: 1.5635011450091527]
	TIME [epoch: 8.85 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2677731709477413		[learning rate: 0.0067662]
		[batch 20/20] avg loss: 2.5120901464198453		[learning rate: 0.0067539]
	Learning Rate: 0.00675387
	LOSS [training: 2.3899316586837926 | validation: 1.8981072007273547]
	TIME [epoch: 8.87 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4511720957615553		[learning rate: 0.0067416]
		[batch 20/20] avg loss: 2.4319826923690657		[learning rate: 0.0067294]
	Learning Rate: 0.00672936
	LOSS [training: 2.4415773940653103 | validation: 1.9257865563818952]
	TIME [epoch: 8.86 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3776828442225995		[learning rate: 0.0067171]
		[batch 20/20] avg loss: 2.515071100033281		[learning rate: 0.0067049]
	Learning Rate: 0.00670494
	LOSS [training: 2.4463769721279403 | validation: 2.1323415390794573]
	TIME [epoch: 8.85 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.345674538546484		[learning rate: 0.0066928]
		[batch 20/20] avg loss: 2.3155142572955882		[learning rate: 0.0066806]
	Learning Rate: 0.0066806
	LOSS [training: 2.3305943979210357 | validation: 1.4589817877709588]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4102841878215955		[learning rate: 0.0066685]
		[batch 20/20] avg loss: 2.2846988935260337		[learning rate: 0.0066564]
	Learning Rate: 0.00665636
	LOSS [training: 2.3474915406738153 | validation: 1.5907798482137898]
	TIME [epoch: 8.85 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3244898359861015		[learning rate: 0.0066443]
		[batch 20/20] avg loss: 2.350809293829307		[learning rate: 0.0066322]
	Learning Rate: 0.0066322
	LOSS [training: 2.337649564907704 | validation: 2.2460643664063586]
	TIME [epoch: 8.87 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2353485540900877		[learning rate: 0.0066202]
		[batch 20/20] avg loss: 2.1725134039208505		[learning rate: 0.0066081]
	Learning Rate: 0.00660814
	LOSS [training: 2.2039309790054693 | validation: 1.5010900026575325]
	TIME [epoch: 8.86 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3630439592028205		[learning rate: 0.0065961]
		[batch 20/20] avg loss: 2.381915176364968		[learning rate: 0.0065842]
	Learning Rate: 0.00658415
	LOSS [training: 2.372479567783894 | validation: 1.7238419439494292]
	TIME [epoch: 8.84 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2383951750542224		[learning rate: 0.0065722]
		[batch 20/20] avg loss: 3.3449463774124624		[learning rate: 0.0065603]
	Learning Rate: 0.00656026
	LOSS [training: 2.7916707762333424 | validation: 1.7206981440147737]
	TIME [epoch: 8.85 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.211314932131404		[learning rate: 0.0065483]
		[batch 20/20] avg loss: 2.136875701415909		[learning rate: 0.0065365]
	Learning Rate: 0.00653645
	LOSS [training: 2.174095316773656 | validation: 1.8332455947421291]
	TIME [epoch: 8.85 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0530731618886824		[learning rate: 0.0065246]
		[batch 20/20] avg loss: 2.406826394414451		[learning rate: 0.0065127]
	Learning Rate: 0.00651273
	LOSS [training: 2.2299497781515667 | validation: 1.6365633378473063]
	TIME [epoch: 8.86 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6829848127383973		[learning rate: 0.0065009]
		[batch 20/20] avg loss: 2.3178059598938736		[learning rate: 0.0064891]
	Learning Rate: 0.0064891
	LOSS [training: 2.5003953863161352 | validation: 1.5177343180685785]
	TIME [epoch: 8.85 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4825978678132996		[learning rate: 0.0064773]
		[batch 20/20] avg loss: 2.4319048272342223		[learning rate: 0.0064655]
	Learning Rate: 0.00646555
	LOSS [training: 2.4572513475237607 | validation: 1.5455820564263556]
	TIME [epoch: 8.85 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.360355468389815		[learning rate: 0.0064538]
		[batch 20/20] avg loss: 2.1975004670628486		[learning rate: 0.0064421]
	Learning Rate: 0.00644208
	LOSS [training: 2.278927967726332 | validation: 1.457108892018623]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.158764448959597		[learning rate: 0.0064304]
		[batch 20/20] avg loss: 2.194916071427351		[learning rate: 0.0064187]
	Learning Rate: 0.0064187
	LOSS [training: 2.176840260193474 | validation: 1.9428389007619713]
	TIME [epoch: 8.87 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3708003973294307		[learning rate: 0.006407]
		[batch 20/20] avg loss: 2.4102411779127957		[learning rate: 0.0063954]
	Learning Rate: 0.00639541
	LOSS [training: 2.3905207876211128 | validation: 1.5156391309941921]
	TIME [epoch: 8.85 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3712051227225253		[learning rate: 0.0063838]
		[batch 20/20] avg loss: 2.3020252107785537		[learning rate: 0.0063722]
	Learning Rate: 0.0063722
	LOSS [training: 2.3366151667505397 | validation: 1.8539964007528045]
	TIME [epoch: 8.85 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.213339336762027		[learning rate: 0.0063606]
		[batch 20/20] avg loss: 2.0218824427121134		[learning rate: 0.0063491]
	Learning Rate: 0.00634908
	LOSS [training: 2.1176108897370702 | validation: 1.505600983162027]
	TIME [epoch: 8.84 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.290904910040118		[learning rate: 0.0063375]
		[batch 20/20] avg loss: 2.028537000964275		[learning rate: 0.006326]
	Learning Rate: 0.00632603
	LOSS [training: 2.159720955502196 | validation: 1.6172014426621089]
	TIME [epoch: 8.85 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.102048305696606		[learning rate: 0.0063145]
		[batch 20/20] avg loss: 2.3210442597785006		[learning rate: 0.0063031]
	Learning Rate: 0.00630308
	LOSS [training: 2.211546282737553 | validation: 3.6487606071980876]
	TIME [epoch: 8.87 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.642134239204651		[learning rate: 0.0062916]
		[batch 20/20] avg loss: 2.5815047071157933		[learning rate: 0.0062802]
	Learning Rate: 0.0062802
	LOSS [training: 2.6118194731602222 | validation: 2.0834246611486495]
	TIME [epoch: 8.86 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5310641804917733		[learning rate: 0.0062688]
		[batch 20/20] avg loss: 2.48158608976243		[learning rate: 0.0062574]
	Learning Rate: 0.00625741
	LOSS [training: 2.5063251351271014 | validation: 2.084224165285167]
	TIME [epoch: 8.85 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5178649959553794		[learning rate: 0.006246]
		[batch 20/20] avg loss: 2.419664583796096		[learning rate: 0.0062347]
	Learning Rate: 0.0062347
	LOSS [training: 2.4687647898757374 | validation: 2.3775894920417624]
	TIME [epoch: 8.85 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.38346827421731		[learning rate: 0.0062234]
		[batch 20/20] avg loss: 2.5085528407297915		[learning rate: 0.0062121]
	Learning Rate: 0.00621208
	LOSS [training: 2.4460105574735507 | validation: 2.1816851812251667]
	TIME [epoch: 8.86 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6684615231864197		[learning rate: 0.0062008]
		[batch 20/20] avg loss: 2.30241505711523		[learning rate: 0.0061895]
	Learning Rate: 0.00618953
	LOSS [training: 2.4854382901508254 | validation: 2.5351156219579494]
	TIME [epoch: 8.88 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3666914914872854		[learning rate: 0.0061783]
		[batch 20/20] avg loss: 2.219421265910383		[learning rate: 0.0061671]
	Learning Rate: 0.00616707
	LOSS [training: 2.2930563786988345 | validation: 1.5191098198381334]
	TIME [epoch: 8.87 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0922771188352236		[learning rate: 0.0061559]
		[batch 20/20] avg loss: 2.431748180683262		[learning rate: 0.0061447]
	Learning Rate: 0.00614469
	LOSS [training: 2.2620126497592423 | validation: 1.7519063128188443]
	TIME [epoch: 8.85 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.569358551329933		[learning rate: 0.0061335]
		[batch 20/20] avg loss: 2.491709889701401		[learning rate: 0.0061224]
	Learning Rate: 0.00612239
	LOSS [training: 2.5305342205156665 | validation: 1.7307067599654302]
	TIME [epoch: 8.86 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.363127500427903		[learning rate: 0.0061113]
		[batch 20/20] avg loss: 2.5566790040190597		[learning rate: 0.0061002]
	Learning Rate: 0.00610017
	LOSS [training: 2.4599032522234814 | validation: 1.6758535984102898]
	TIME [epoch: 8.89 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4307906446330483		[learning rate: 0.0060891]
		[batch 20/20] avg loss: 2.4337101670559402		[learning rate: 0.006078]
	Learning Rate: 0.00607803
	LOSS [training: 2.4322504058444947 | validation: 1.5249476501556292]
	TIME [epoch: 8.86 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.40627575108609		[learning rate: 0.006067]
		[batch 20/20] avg loss: 2.493541802705004		[learning rate: 0.006056]
	Learning Rate: 0.00605598
	LOSS [training: 2.4499087768955468 | validation: 1.6780505260917644]
	TIME [epoch: 8.86 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.316726652792621		[learning rate: 0.006045]
		[batch 20/20] avg loss: 2.5898737626967487		[learning rate: 0.006034]
	Learning Rate: 0.006034
	LOSS [training: 2.453300207744685 | validation: 1.4902913723750337]
	TIME [epoch: 8.86 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4668657530062363		[learning rate: 0.006023]
		[batch 20/20] avg loss: 2.2357411517437553		[learning rate: 0.0060121]
	Learning Rate: 0.0060121
	LOSS [training: 2.351303452374996 | validation: 4.22342131003501]
	TIME [epoch: 8.84 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.898463271401586		[learning rate: 0.0060012]
		[batch 20/20] avg loss: 2.39343494327822		[learning rate: 0.0059903]
	Learning Rate: 0.00599028
	LOSS [training: 2.6459491073399035 | validation: 2.597801666710688]
	TIME [epoch: 8.89 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5803456000914387		[learning rate: 0.0059794]
		[batch 20/20] avg loss: 2.192769843467123		[learning rate: 0.0059685]
	Learning Rate: 0.00596854
	LOSS [training: 2.3865577217792806 | validation: 1.6223980770106952]
	TIME [epoch: 8.85 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0929730018766777		[learning rate: 0.0059577]
		[batch 20/20] avg loss: 2.272705498473433		[learning rate: 0.0059469]
	Learning Rate: 0.00594688
	LOSS [training: 2.182839250175056 | validation: 1.5844007730478278]
	TIME [epoch: 8.86 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.25632940126856		[learning rate: 0.0059361]
		[batch 20/20] avg loss: 2.2784555404753126		[learning rate: 0.0059253]
	Learning Rate: 0.0059253
	LOSS [training: 2.2673924708719366 | validation: 1.4153004308804802]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3435121373736605		[learning rate: 0.0059145]
		[batch 20/20] avg loss: 2.550463713819001		[learning rate: 0.0059038]
	Learning Rate: 0.0059038
	LOSS [training: 2.446987925596331 | validation: 2.1497281245488984]
	TIME [epoch: 8.85 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4771597786941975		[learning rate: 0.0058931]
		[batch 20/20] avg loss: 2.504899102990273		[learning rate: 0.0058824]
	Learning Rate: 0.00588237
	LOSS [training: 2.4910294408422353 | validation: 2.066546843149742]
	TIME [epoch: 8.86 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3715086459855845		[learning rate: 0.0058717]
		[batch 20/20] avg loss: 2.5536225747389274		[learning rate: 0.005861]
	Learning Rate: 0.00586103
	LOSS [training: 2.4625656103622555 | validation: 2.202512345110459]
	TIME [epoch: 8.84 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.305964732611416		[learning rate: 0.0058504]
		[batch 20/20] avg loss: 2.612655464657627		[learning rate: 0.0058398]
	Learning Rate: 0.00583976
	LOSS [training: 2.4593100986345213 | validation: 2.083619036409629]
	TIME [epoch: 8.84 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5264800901401174		[learning rate: 0.0058291]
		[batch 20/20] avg loss: 2.5043014193700555		[learning rate: 0.0058186]
	Learning Rate: 0.00581856
	LOSS [training: 2.515390754755086 | validation: 1.6858843229778995]
	TIME [epoch: 8.83 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5589687524048674		[learning rate: 0.005808]
		[batch 20/20] avg loss: 2.3559752996044034		[learning rate: 0.0057974]
	Learning Rate: 0.00579745
	LOSS [training: 2.457472026004636 | validation: 1.5141705115343989]
	TIME [epoch: 8.86 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5841105115847847		[learning rate: 0.0057869]
		[batch 20/20] avg loss: 2.2951700280777567		[learning rate: 0.0057764]
	Learning Rate: 0.00577641
	LOSS [training: 2.4396402698312705 | validation: 1.6350695504635988]
	TIME [epoch: 8.85 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5683242352873497		[learning rate: 0.0057659]
		[batch 20/20] avg loss: 2.2976620360476634		[learning rate: 0.0057554]
	Learning Rate: 0.00575545
	LOSS [training: 2.4329931356675067 | validation: 1.6784038359488838]
	TIME [epoch: 8.84 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4035099716197346		[learning rate: 0.005745]
		[batch 20/20] avg loss: 2.5402926945541866		[learning rate: 0.0057346]
	Learning Rate: 0.00573456
	LOSS [training: 2.4719013330869606 | validation: 1.607497033088703]
	TIME [epoch: 8.83 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3604394952505308		[learning rate: 0.0057241]
		[batch 20/20] avg loss: 2.317152498284046		[learning rate: 0.0057137]
	Learning Rate: 0.00571375
	LOSS [training: 2.338795996767289 | validation: 1.4989809111574417]
	TIME [epoch: 8.85 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9411013902654457		[learning rate: 0.0057034]
		[batch 20/20] avg loss: 2.3390087806307878		[learning rate: 0.005693]
	Learning Rate: 0.00569301
	LOSS [training: 2.1400550854481173 | validation: 1.7471213863000665]
	TIME [epoch: 8.87 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.218793799583252		[learning rate: 0.0056827]
		[batch 20/20] avg loss: 2.0786533051035994		[learning rate: 0.0056724]
	Learning Rate: 0.00567235
	LOSS [training: 2.1487235523434256 | validation: 1.904129314117008]
	TIME [epoch: 8.85 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.304889401965172		[learning rate: 0.005662]
		[batch 20/20] avg loss: 2.0323950615470805		[learning rate: 0.0056518]
	Learning Rate: 0.00565177
	LOSS [training: 2.1686422317561265 | validation: 1.662137079783117]
	TIME [epoch: 8.84 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2688981165686664		[learning rate: 0.0056415]
		[batch 20/20] avg loss: 2.0964853982130935		[learning rate: 0.0056313]
	Learning Rate: 0.00563126
	LOSS [training: 2.18269175739088 | validation: 3.1359633100419932]
	TIME [epoch: 8.86 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.63415341423675		[learning rate: 0.005621]
		[batch 20/20] avg loss: 2.349428388856379		[learning rate: 0.0056108]
	Learning Rate: 0.00561082
	LOSS [training: 2.4917909015465645 | validation: 2.117093695370035]
	TIME [epoch: 8.86 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6715833961822435		[learning rate: 0.0056006]
		[batch 20/20] avg loss: 2.813660964601009		[learning rate: 0.0055905]
	Learning Rate: 0.00559046
	LOSS [training: 2.742622180391627 | validation: 2.747259148817805]
	TIME [epoch: 8.87 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4788775853734415		[learning rate: 0.0055803]
		[batch 20/20] avg loss: 2.3971070442217415		[learning rate: 0.0055702]
	Learning Rate: 0.00557017
	LOSS [training: 2.4379923147975915 | validation: 2.0707020769472946]
	TIME [epoch: 8.85 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.464181883967398		[learning rate: 0.0055601]
		[batch 20/20] avg loss: 2.2923270039808568		[learning rate: 0.00555]
	Learning Rate: 0.00554996
	LOSS [training: 2.3782544439741278 | validation: 2.2495577517454244]
	TIME [epoch: 8.85 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.465852129523223		[learning rate: 0.0055399]
		[batch 20/20] avg loss: 2.4223936655119696		[learning rate: 0.0055298]
	Learning Rate: 0.00552981
	LOSS [training: 2.444122897517596 | validation: 2.168478275359155]
	TIME [epoch: 8.85 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1812428186276986		[learning rate: 0.0055198]
		[batch 20/20] avg loss: 2.540079144845192		[learning rate: 0.0055097]
	Learning Rate: 0.00550975
	LOSS [training: 2.3606609817364452 | validation: 2.1103304508391028]
	TIME [epoch: 8.87 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.390783014428057		[learning rate: 0.0054997]
		[batch 20/20] avg loss: 2.347954560616269		[learning rate: 0.0054898]
	Learning Rate: 0.00548975
	LOSS [training: 2.369368787522163 | validation: 2.1386237337304568]
	TIME [epoch: 8.88 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4137081676801673		[learning rate: 0.0054798]
		[batch 20/20] avg loss: 2.3746674428783794		[learning rate: 0.0054698]
	Learning Rate: 0.00546983
	LOSS [training: 2.394187805279273 | validation: 2.7130582281684976]
	TIME [epoch: 8.86 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2988883467072503		[learning rate: 0.0054599]
		[batch 20/20] avg loss: 2.4469212479490836		[learning rate: 0.00545]
	Learning Rate: 0.00544998
	LOSS [training: 2.372904797328167 | validation: 1.5090457467634382]
	TIME [epoch: 8.85 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.424795271564001		[learning rate: 0.0054401]
		[batch 20/20] avg loss: 2.5869971871584134		[learning rate: 0.0054302]
	Learning Rate: 0.0054302
	LOSS [training: 2.5058962293612073 | validation: 1.6553757276373677]
	TIME [epoch: 8.86 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2828584008510195		[learning rate: 0.0054203]
		[batch 20/20] avg loss: 2.515838542798188		[learning rate: 0.0054105]
	Learning Rate: 0.00541049
	LOSS [training: 2.3993484718246036 | validation: 1.4702823105701541]
	TIME [epoch: 8.87 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.596034075769078		[learning rate: 0.0054007]
		[batch 20/20] avg loss: 2.3991249905053795		[learning rate: 0.0053909]
	Learning Rate: 0.00539086
	LOSS [training: 2.4975795331372295 | validation: 1.7223026655175047]
	TIME [epoch: 8.86 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3625770089017477		[learning rate: 0.0053811]
		[batch 20/20] avg loss: 2.5403733644065283		[learning rate: 0.0053713]
	Learning Rate: 0.00537129
	LOSS [training: 2.451475186654138 | validation: 1.605457800927424]
	TIME [epoch: 8.86 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.457410104458829		[learning rate: 0.0053615]
		[batch 20/20] avg loss: 2.493746935240062		[learning rate: 0.0053518]
	Learning Rate: 0.0053518
	LOSS [training: 2.475578519849445 | validation: 1.6461439282201047]
	TIME [epoch: 8.86 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.408737048654925		[learning rate: 0.0053421]
		[batch 20/20] avg loss: 2.205860322033811		[learning rate: 0.0053324]
	Learning Rate: 0.00533238
	LOSS [training: 2.307298685344368 | validation: 1.5143066387737822]
	TIME [epoch: 8.86 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9834903561608717		[learning rate: 0.0053227]
		[batch 20/20] avg loss: 2.302388946628416		[learning rate: 0.005313]
	Learning Rate: 0.00531303
	LOSS [training: 2.142939651394644 | validation: 1.537226081967259]
	TIME [epoch: 8.88 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1575411164739737		[learning rate: 0.0053034]
		[batch 20/20] avg loss: 2.1184945346149338		[learning rate: 0.0052937]
	Learning Rate: 0.00529375
	LOSS [training: 2.1380178255444533 | validation: 1.4407968819842354]
	TIME [epoch: 8.86 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4089876793746945		[learning rate: 0.0052841]
		[batch 20/20] avg loss: 2.376763122041804		[learning rate: 0.0052745]
	Learning Rate: 0.00527454
	LOSS [training: 2.3928754007082498 | validation: 1.445525672962323]
	TIME [epoch: 8.86 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4482933965281197		[learning rate: 0.005265]
		[batch 20/20] avg loss: 2.408760385815269		[learning rate: 0.0052554]
	Learning Rate: 0.00525539
	LOSS [training: 2.428526891171695 | validation: 1.6994366125066125]
	TIME [epoch: 8.86 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.412603148427766		[learning rate: 0.0052458]
		[batch 20/20] avg loss: 2.5078670463798223		[learning rate: 0.0052363]
	Learning Rate: 0.00523632
	LOSS [training: 2.4602350974037948 | validation: 2.6871092360081144]
	TIME [epoch: 8.87 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1799848177026786		[learning rate: 0.0052268]
		[batch 20/20] avg loss: 2.3636046903598578		[learning rate: 0.0052173]
	Learning Rate: 0.00521732
	LOSS [training: 2.771794754031268 | validation: 2.114551643972303]
	TIME [epoch: 8.89 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3225141538688674		[learning rate: 0.0052078]
		[batch 20/20] avg loss: 2.5060163399565756		[learning rate: 0.0051984]
	Learning Rate: 0.00519839
	LOSS [training: 2.4142652469127217 | validation: 2.1206038364067403]
	TIME [epoch: 8.87 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4258579714012596		[learning rate: 0.0051889]
		[batch 20/20] avg loss: 2.5295277079353835		[learning rate: 0.0051795]
	Learning Rate: 0.00517952
	LOSS [training: 2.477692839668321 | validation: 2.1660870592619785]
	TIME [epoch: 8.86 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4938535994607482		[learning rate: 0.0051701]
		[batch 20/20] avg loss: 2.127177660373003		[learning rate: 0.0051607]
	Learning Rate: 0.00516072
	LOSS [training: 2.3105156299168756 | validation: 1.5335300257962177]
	TIME [epoch: 8.86 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2441569283861673		[learning rate: 0.0051513]
		[batch 20/20] avg loss: 2.3895555290123562		[learning rate: 0.005142]
	Learning Rate: 0.00514199
	LOSS [training: 2.3168562286992618 | validation: 1.6007652562736865]
	TIME [epoch: 8.88 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3210001351837115		[learning rate: 0.0051327]
		[batch 20/20] avg loss: 2.3792800414905098		[learning rate: 0.0051233]
	Learning Rate: 0.00512333
	LOSS [training: 2.3501400883371106 | validation: 2.1459284436111004]
	TIME [epoch: 8.87 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4962596398757433		[learning rate: 0.005114]
		[batch 20/20] avg loss: 2.2296547202701382		[learning rate: 0.0051047]
	Learning Rate: 0.00510474
	LOSS [training: 2.3629571800729403 | validation: 2.1235369763903478]
	TIME [epoch: 8.87 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3679287193398304		[learning rate: 0.0050955]
		[batch 20/20] avg loss: 2.3431929796419437		[learning rate: 0.0050862]
	Learning Rate: 0.00508622
	LOSS [training: 2.3555608494908875 | validation: 1.8667826554485956]
	TIME [epoch: 8.86 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.362299142257788		[learning rate: 0.005077]
		[batch 20/20] avg loss: 2.3701831254591594		[learning rate: 0.0050678]
	Learning Rate: 0.00506776
	LOSS [training: 2.366241133858474 | validation: 1.5100078493862368]
	TIME [epoch: 8.87 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.467575254058648		[learning rate: 0.0050586]
		[batch 20/20] avg loss: 2.403364700852503		[learning rate: 0.0050494]
	Learning Rate: 0.00504937
	LOSS [training: 2.435469977455575 | validation: 1.6010047163884358]
	TIME [epoch: 8.89 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.405498082477225		[learning rate: 0.0050402]
		[batch 20/20] avg loss: 2.337655379230161		[learning rate: 0.005031]
	Learning Rate: 0.00503104
	LOSS [training: 2.3715767308536924 | validation: 1.529297694761438]
	TIME [epoch: 8.85 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2244211945054806		[learning rate: 0.0050219]
		[batch 20/20] avg loss: 2.516997391355464		[learning rate: 0.0050128]
	Learning Rate: 0.00501278
	LOSS [training: 2.370709292930473 | validation: 1.5437179764342737]
	TIME [epoch: 8.86 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3262286608685536		[learning rate: 0.0050037]
		[batch 20/20] avg loss: 2.358200327558188		[learning rate: 0.0049946]
	Learning Rate: 0.00499459
	LOSS [training: 2.3422144942133705 | validation: 1.6078403735103484]
	TIME [epoch: 8.84 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.377428516265895		[learning rate: 0.0049855]
		[batch 20/20] avg loss: 2.310759180676896		[learning rate: 0.0049765]
	Learning Rate: 0.00497647
	LOSS [training: 2.3440938484713953 | validation: 1.570801004335981]
	TIME [epoch: 8.86 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.25055117265959		[learning rate: 0.0049674]
		[batch 20/20] avg loss: 2.509685416482885		[learning rate: 0.0049584]
	Learning Rate: 0.00495841
	LOSS [training: 2.3801182945712376 | validation: 1.530861953035704]
	TIME [epoch: 8.88 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3651272338175686		[learning rate: 0.0049494]
		[batch 20/20] avg loss: 2.379634992866227		[learning rate: 0.0049404]
	Learning Rate: 0.00494041
	LOSS [training: 2.372381113341898 | validation: 1.6767264709145988]
	TIME [epoch: 8.87 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3201906564846624		[learning rate: 0.0049314]
		[batch 20/20] avg loss: 2.382824100622002		[learning rate: 0.0049225]
	Learning Rate: 0.00492248
	LOSS [training: 2.3515073785533316 | validation: 1.4910314564258145]
	TIME [epoch: 8.86 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.459584721896472		[learning rate: 0.0049135]
		[batch 20/20] avg loss: 2.1499017584696807		[learning rate: 0.0049046]
	Learning Rate: 0.00490462
	LOSS [training: 2.3047432401830763 | validation: 1.895644352522083]
	TIME [epoch: 8.85 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.322215179295731		[learning rate: 0.0048957]
		[batch 20/20] avg loss: 2.240302818849502		[learning rate: 0.0048868]
	Learning Rate: 0.00488682
	LOSS [training: 2.281258999072617 | validation: 1.4240536552259782]
	TIME [epoch: 8.88 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0414754634606345		[learning rate: 0.0048779]
		[batch 20/20] avg loss: 2.1662026901920415		[learning rate: 0.0048691]
	Learning Rate: 0.00486909
	LOSS [training: 2.103839076826338 | validation: 1.6495653350656994]
	TIME [epoch: 8.86 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3574003703537616		[learning rate: 0.0048602]
		[batch 20/20] avg loss: 2.1273013443020163		[learning rate: 0.0048514]
	Learning Rate: 0.00485141
	LOSS [training: 2.2423508573278887 | validation: 2.048824908278656]
	TIME [epoch: 8.86 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1807460982690623		[learning rate: 0.0048426]
		[batch 20/20] avg loss: 2.1108797155113637		[learning rate: 0.0048338]
	Learning Rate: 0.00483381
	LOSS [training: 2.1458129068902134 | validation: 1.4417682389307847]
	TIME [epoch: 8.87 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4306928847758114		[learning rate: 0.004825]
		[batch 20/20] avg loss: 2.0533213124770087		[learning rate: 0.0048163]
	Learning Rate: 0.00481627
	LOSS [training: 2.24200709862641 | validation: 1.8770602110800652]
	TIME [epoch: 8.86 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.744966841264744		[learning rate: 0.0048075]
		[batch 20/20] avg loss: 2.3425970241012934		[learning rate: 0.0047988]
	Learning Rate: 0.00479879
	LOSS [training: 2.543781932683019 | validation: 1.5811738496481693]
	TIME [epoch: 8.9 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3097053165720176		[learning rate: 0.0047901]
		[batch 20/20] avg loss: 2.1602473925242647		[learning rate: 0.0047814]
	Learning Rate: 0.00478137
	LOSS [training: 2.2349763545481407 | validation: 1.778270354758417]
	TIME [epoch: 8.86 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.260175410217106		[learning rate: 0.0047727]
		[batch 20/20] avg loss: 2.265618076483185		[learning rate: 0.004764]
	Learning Rate: 0.00476402
	LOSS [training: 2.262896743350145 | validation: 1.8582159946111434]
	TIME [epoch: 8.86 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3060537459401376		[learning rate: 0.0047554]
		[batch 20/20] avg loss: 2.2082228929180525		[learning rate: 0.0047467]
	Learning Rate: 0.00474673
	LOSS [training: 2.257138319429095 | validation: 1.6434945473647427]
	TIME [epoch: 8.86 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1635289748833353		[learning rate: 0.0047381]
		[batch 20/20] avg loss: 2.2346371318716582		[learning rate: 0.0047295]
	Learning Rate: 0.00472951
	LOSS [training: 2.199083053377497 | validation: 1.8591035377013603]
	TIME [epoch: 8.85 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.305150403148736		[learning rate: 0.0047209]
		[batch 20/20] avg loss: 2.07870667277142		[learning rate: 0.0047123]
	Learning Rate: 0.00471234
	LOSS [training: 2.1919285379600777 | validation: 1.7797613014436418]
	TIME [epoch: 8.87 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.286456517188688		[learning rate: 0.0047038]
		[batch 20/20] avg loss: 2.1034943577791028		[learning rate: 0.0046952]
	Learning Rate: 0.00469524
	LOSS [training: 2.194975437483895 | validation: 2.638169707432999]
	TIME [epoch: 8.85 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2952550783699044		[learning rate: 0.0046867]
		[batch 20/20] avg loss: 2.259685163577709		[learning rate: 0.0046782]
	Learning Rate: 0.0046782
	LOSS [training: 2.277470120973807 | validation: 1.684122755984062]
	TIME [epoch: 8.85 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.11557670066428		[learning rate: 0.0046697]
		[batch 20/20] avg loss: 2.3304220291119995		[learning rate: 0.0046612]
	Learning Rate: 0.00466122
	LOSS [training: 2.22299936488814 | validation: 1.8640927798526372]
	TIME [epoch: 8.85 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.04977675062997		[learning rate: 0.0046528]
		[batch 20/20] avg loss: 2.365402267889848		[learning rate: 0.0046443]
	Learning Rate: 0.00464431
	LOSS [training: 2.207589509259909 | validation: 2.004006336573863]
	TIME [epoch: 8.87 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0244424887132046		[learning rate: 0.0046359]
		[batch 20/20] avg loss: 2.1617826107029643		[learning rate: 0.0046275]
	Learning Rate: 0.00462745
	LOSS [training: 2.093112549708084 | validation: 1.4135112885272514]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2394912575229724		[learning rate: 0.004619]
		[batch 20/20] avg loss: 2.308883575763251		[learning rate: 0.0046107]
	Learning Rate: 0.00461066
	LOSS [training: 2.274187416643111 | validation: 1.5657112446813455]
	TIME [epoch: 8.85 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0374372648353494		[learning rate: 0.0046023]
		[batch 20/20] avg loss: 2.1864428688947815		[learning rate: 0.0045939]
	Learning Rate: 0.00459393
	LOSS [training: 2.1119400668650656 | validation: 1.5875872310957724]
	TIME [epoch: 8.84 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.019211726509029		[learning rate: 0.0045856]
		[batch 20/20] avg loss: 2.0649470563146677		[learning rate: 0.0045773]
	Learning Rate: 0.00457726
	LOSS [training: 2.0420793914118476 | validation: 1.3700837632854685]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.481809018879909		[learning rate: 0.0045689]
		[batch 20/20] avg loss: 2.1261046143175273		[learning rate: 0.0045606]
	Learning Rate: 0.00456065
	LOSS [training: 2.303956816598718 | validation: 1.4425476940698476]
	TIME [epoch: 8.88 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2048644573535316		[learning rate: 0.0045524]
		[batch 20/20] avg loss: 2.119875488702678		[learning rate: 0.0045441]
	Learning Rate: 0.00454409
	LOSS [training: 2.1623699730281047 | validation: 2.129397400318096]
	TIME [epoch: 8.88 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.187730640604511		[learning rate: 0.0045358]
		[batch 20/20] avg loss: 2.1843709892680208		[learning rate: 0.0045276]
	Learning Rate: 0.0045276
	LOSS [training: 2.1860508149362663 | validation: 1.3763583223708555]
	TIME [epoch: 8.87 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.08426640595928		[learning rate: 0.0045194]
		[batch 20/20] avg loss: 2.299483029876786		[learning rate: 0.0045112]
	Learning Rate: 0.00451117
	LOSS [training: 2.191874717918033 | validation: 2.019285301232882]
	TIME [epoch: 8.87 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1474506383919305		[learning rate: 0.004503]
		[batch 20/20] avg loss: 2.3355760924453848		[learning rate: 0.0044948]
	Learning Rate: 0.0044948
	LOSS [training: 2.2415133654186574 | validation: 1.8289515384145045]
	TIME [epoch: 8.87 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0924139829521153		[learning rate: 0.0044866]
		[batch 20/20] avg loss: 2.433956508839079		[learning rate: 0.0044785]
	Learning Rate: 0.00447849
	LOSS [training: 2.263185245895597 | validation: 1.514840157961367]
	TIME [epoch: 8.89 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.09663635998668		[learning rate: 0.0044704]
		[batch 20/20] avg loss: 2.3559824245170278		[learning rate: 0.0044622]
	Learning Rate: 0.00446224
	LOSS [training: 2.2263093922518538 | validation: 1.4216959399713107]
	TIME [epoch: 8.87 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2985733853630537		[learning rate: 0.0044541]
		[batch 20/20] avg loss: 2.2753829043589646		[learning rate: 0.004446]
	Learning Rate: 0.00444604
	LOSS [training: 2.286978144861009 | validation: 1.4380205817889564]
	TIME [epoch: 8.87 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.16180104586959		[learning rate: 0.004438]
		[batch 20/20] avg loss: 2.333358587683034		[learning rate: 0.0044299]
	Learning Rate: 0.00442991
	LOSS [training: 2.2475798167763124 | validation: 1.6895217137847967]
	TIME [epoch: 8.87 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.162930569243643		[learning rate: 0.0044219]
		[batch 20/20] avg loss: 2.2928929353738066		[learning rate: 0.0044138]
	Learning Rate: 0.00441383
	LOSS [training: 2.2279117523087253 | validation: 1.617797413863991]
	TIME [epoch: 8.92 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2514147127133257		[learning rate: 0.0044058]
		[batch 20/20] avg loss: 2.1011874551100123		[learning rate: 0.0043978]
	Learning Rate: 0.00439781
	LOSS [training: 2.1763010839116688 | validation: 1.4933540056124033]
	TIME [epoch: 8.87 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.28650770972145		[learning rate: 0.0043898]
		[batch 20/20] avg loss: 2.1908482437693273		[learning rate: 0.0043819]
	Learning Rate: 0.00438185
	LOSS [training: 2.2386779767453886 | validation: 1.4426923313999909]
	TIME [epoch: 8.88 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1590283218298802		[learning rate: 0.0043739]
		[batch 20/20] avg loss: 2.2658031553004685		[learning rate: 0.004366]
	Learning Rate: 0.00436595
	LOSS [training: 2.2124157385651744 | validation: 1.451115040526769]
	TIME [epoch: 8.87 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2563236698606253		[learning rate: 0.004358]
		[batch 20/20] avg loss: 2.1609567294667444		[learning rate: 0.0043501]
	Learning Rate: 0.00435011
	LOSS [training: 2.208640199663685 | validation: 1.5176093372348842]
	TIME [epoch: 8.87 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2694719555148484		[learning rate: 0.0043422]
		[batch 20/20] avg loss: 2.1086863420249693		[learning rate: 0.0043343]
	Learning Rate: 0.00433432
	LOSS [training: 2.189079148769909 | validation: 1.39874398740718]
	TIME [epoch: 8.89 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.06061608805482		[learning rate: 0.0043264]
		[batch 20/20] avg loss: 2.316875886993266		[learning rate: 0.0043186]
	Learning Rate: 0.00431859
	LOSS [training: 2.188745987524043 | validation: 1.6747306805929714]
	TIME [epoch: 8.87 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0664469008836326		[learning rate: 0.0043107]
		[batch 20/20] avg loss: 2.2555497225481878		[learning rate: 0.0043029]
	Learning Rate: 0.00430292
	LOSS [training: 2.16099831171591 | validation: 1.5061215486340742]
	TIME [epoch: 8.87 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.112014992106899		[learning rate: 0.0042951]
		[batch 20/20] avg loss: 2.156836660172797		[learning rate: 0.0042873]
	Learning Rate: 0.0042873
	LOSS [training: 2.1344258261398483 | validation: 1.4938237121999458]
	TIME [epoch: 8.87 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.055822658320973		[learning rate: 0.0042795]
		[batch 20/20] avg loss: 2.29318627450522		[learning rate: 0.0042717]
	Learning Rate: 0.00427174
	LOSS [training: 2.1745044664130964 | validation: 1.4459475856190487]
	TIME [epoch: 8.88 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0740503529735985		[learning rate: 0.004264]
		[batch 20/20] avg loss: 2.3117620099915044		[learning rate: 0.0042562]
	Learning Rate: 0.00425624
	LOSS [training: 2.1929061814825515 | validation: 1.514480326420034]
	TIME [epoch: 8.89 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2568768604691902		[learning rate: 0.0042485]
		[batch 20/20] avg loss: 2.130139083829514		[learning rate: 0.0042408]
	Learning Rate: 0.0042408
	LOSS [training: 2.1935079721493516 | validation: 1.3589925229210937]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2528760383575994		[learning rate: 0.0042331]
		[batch 20/20] avg loss: 2.129655872794337		[learning rate: 0.0042254]
	Learning Rate: 0.00422541
	LOSS [training: 2.191265955575968 | validation: 1.3634832897075673]
	TIME [epoch: 8.87 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.015379084666859		[learning rate: 0.0042177]
		[batch 20/20] avg loss: 2.324844931136426		[learning rate: 0.0042101]
	Learning Rate: 0.00421007
	LOSS [training: 2.1701120079016425 | validation: 1.5352363085998302]
	TIME [epoch: 8.86 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9247039924041913		[learning rate: 0.0042024]
		[batch 20/20] avg loss: 2.268237244556911		[learning rate: 0.0041948]
	Learning Rate: 0.00419479
	LOSS [training: 2.0964706184805513 | validation: 1.3515233386612364]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1563569555482913		[learning rate: 0.0041872]
		[batch 20/20] avg loss: 2.2897811988895294		[learning rate: 0.0041796]
	Learning Rate: 0.00417957
	LOSS [training: 2.22306907721891 | validation: 1.5643023375978997]
	TIME [epoch: 8.88 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6366328621570227		[learning rate: 0.004172]
		[batch 20/20] avg loss: 2.2287253359629795		[learning rate: 0.0041644]
	Learning Rate: 0.0041644
	LOSS [training: 2.4326790990600005 | validation: 1.7954272199537629]
	TIME [epoch: 8.87 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.066487289892799		[learning rate: 0.0041568]
		[batch 20/20] avg loss: 2.350179415555279		[learning rate: 0.0041493]
	Learning Rate: 0.00414929
	LOSS [training: 2.208333352724039 | validation: 1.8090091616624904]
	TIME [epoch: 8.87 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0853264317783955		[learning rate: 0.0041418]
		[batch 20/20] avg loss: 2.283646862168852		[learning rate: 0.0041342]
	Learning Rate: 0.00413423
	LOSS [training: 2.1844866469736246 | validation: 2.4793565042656756]
	TIME [epoch: 8.86 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.086885397825214		[learning rate: 0.0041267]
		[batch 20/20] avg loss: 2.0614896059730823		[learning rate: 0.0041192]
	Learning Rate: 0.00411923
	LOSS [training: 2.0741875018991482 | validation: 1.4393061986440192]
	TIME [epoch: 8.88 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1778679655987316		[learning rate: 0.0041117]
		[batch 20/20] avg loss: 2.1088132625818377		[learning rate: 0.0041043]
	Learning Rate: 0.00410428
	LOSS [training: 2.143340614090284 | validation: 2.0462867747212723]
	TIME [epoch: 8.87 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.220090031855831		[learning rate: 0.0040968]
		[batch 20/20] avg loss: 2.134146648777776		[learning rate: 0.0040894]
	Learning Rate: 0.00408938
	LOSS [training: 2.1771183403168033 | validation: 1.6379083170441353]
	TIME [epoch: 8.86 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.040039780620213		[learning rate: 0.004082]
		[batch 20/20] avg loss: 2.2615770350155615		[learning rate: 0.0040745]
	Learning Rate: 0.00407454
	LOSS [training: 2.150808407817887 | validation: 3.866058913553205]
	TIME [epoch: 8.87 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.311790935276801		[learning rate: 0.0040671]
		[batch 20/20] avg loss: 2.4676615007378158		[learning rate: 0.0040598]
	Learning Rate: 0.00405976
	LOSS [training: 3.389726218007308 | validation: 1.9443832415240767]
	TIME [epoch: 8.86 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7755120640358912		[learning rate: 0.0040524]
		[batch 20/20] avg loss: 3.641147832245371		[learning rate: 0.004045]
	Learning Rate: 0.00404502
	LOSS [training: 3.2083299481406313 | validation: 2.125644864236806]
	TIME [epoch: 8.89 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4028292420848545		[learning rate: 0.0040377]
		[batch 20/20] avg loss: 1.5371315298127366		[learning rate: 0.0040303]
	Learning Rate: 0.00403034
	LOSS [training: 1.9699803859487954 | validation: 1.1143064611206905]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4551784336969873		[learning rate: 0.004023]
		[batch 20/20] avg loss: 2.398689022667029		[learning rate: 0.0040157]
	Learning Rate: 0.00401572
	LOSS [training: 1.9269337281820078 | validation: 1.9386446134098303]
	TIME [epoch: 8.87 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5843786063868173		[learning rate: 0.0040084]
		[batch 20/20] avg loss: 1.20771581616996		[learning rate: 0.0040011]
	Learning Rate: 0.00400114
	LOSS [training: 1.3960472112783886 | validation: 0.8812891207400124]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.242672745489454		[learning rate: 0.0039939]
		[batch 20/20] avg loss: 1.2540291899537164		[learning rate: 0.0039866]
	Learning Rate: 0.00398662
	LOSS [training: 1.2483509677215854 | validation: 1.0378866648454157]
	TIME [epoch: 8.89 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.299920857012353		[learning rate: 0.0039794]
		[batch 20/20] avg loss: 1.2218871908175477		[learning rate: 0.0039722]
	Learning Rate: 0.00397216
	LOSS [training: 1.2609040239149503 | validation: 0.8036627000524399]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6406866504073097		[learning rate: 0.0039649]
		[batch 20/20] avg loss: 1.4182021049538256		[learning rate: 0.0039577]
	Learning Rate: 0.00395774
	LOSS [training: 1.529444377680568 | validation: 1.096175158411145]
	TIME [epoch: 8.86 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3793280610626826		[learning rate: 0.0039506]
		[batch 20/20] avg loss: 1.4606532850763247		[learning rate: 0.0039434]
	Learning Rate: 0.00394338
	LOSS [training: 1.4199906730695038 | validation: 0.6170230743990784]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2606885423941092		[learning rate: 0.0039362]
		[batch 20/20] avg loss: 1.1330111176343398		[learning rate: 0.0039291]
	Learning Rate: 0.00392907
	LOSS [training: 1.1968498300142247 | validation: 1.4356067463202897]
	TIME [epoch: 8.86 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0487904205175		[learning rate: 0.0039219]
		[batch 20/20] avg loss: 1.2833303753818082		[learning rate: 0.0039148]
	Learning Rate: 0.00391481
	LOSS [training: 1.6660603979496535 | validation: 0.7522501125867485]
	TIME [epoch: 8.88 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1841517720397325		[learning rate: 0.0039077]
		[batch 20/20] avg loss: 1.0402228171346217		[learning rate: 0.0039006]
	Learning Rate: 0.0039006
	LOSS [training: 1.1121872945871771 | validation: 0.757454084554276]
	TIME [epoch: 8.87 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.170040953665163		[learning rate: 0.0038935]
		[batch 20/20] avg loss: 1.1992417087497904		[learning rate: 0.0038864]
	Learning Rate: 0.00388645
	LOSS [training: 1.1846413312074766 | validation: 1.27884234351135]
	TIME [epoch: 8.86 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0777472021809231		[learning rate: 0.0038794]
		[batch 20/20] avg loss: 1.1656809845480844		[learning rate: 0.0038723]
	Learning Rate: 0.00387234
	LOSS [training: 1.121714093364504 | validation: 0.8403132729670144]
	TIME [epoch: 8.86 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.102005837486549		[learning rate: 0.0038653]
		[batch 20/20] avg loss: 1.1760852881980244		[learning rate: 0.0038583]
	Learning Rate: 0.00385829
	LOSS [training: 1.1390455628422864 | validation: 0.5625053622767324]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0947735181621434		[learning rate: 0.0038513]
		[batch 20/20] avg loss: 1.1973514244976937		[learning rate: 0.0038443]
	Learning Rate: 0.00384429
	LOSS [training: 1.1460624713299186 | validation: 0.5306506502892689]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1499819311043014		[learning rate: 0.0038373]
		[batch 20/20] avg loss: 1.1512286315748805		[learning rate: 0.0038303]
	Learning Rate: 0.00383034
	LOSS [training: 1.1506052813395908 | validation: 0.9035806680117162]
	TIME [epoch: 8.86 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1913580956644534		[learning rate: 0.0038234]
		[batch 20/20] avg loss: 1.0738298457958244		[learning rate: 0.0038164]
	Learning Rate: 0.00381644
	LOSS [training: 1.132593970730139 | validation: 1.338012192230529]
	TIME [epoch: 8.85 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9972430555828128		[learning rate: 0.0038095]
		[batch 20/20] avg loss: 0.8841484647180092		[learning rate: 0.0038026]
	Learning Rate: 0.00380258
	LOSS [training: 0.9406957601504109 | validation: 0.5246309892510463]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0100967749379062		[learning rate: 0.0037957]
		[batch 20/20] avg loss: 1.1559442621744427		[learning rate: 0.0037888]
	Learning Rate: 0.00378879
	LOSS [training: 1.0830205185561743 | validation: 0.7641661067886968]
	TIME [epoch: 8.89 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0863757190240166		[learning rate: 0.0037819]
		[batch 20/20] avg loss: 1.383826616412304		[learning rate: 0.003775]
	Learning Rate: 0.00377504
	LOSS [training: 1.2351011677181607 | validation: 1.1582609137374766]
	TIME [epoch: 8.87 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0675847043066322		[learning rate: 0.0037682]
		[batch 20/20] avg loss: 0.9434355344014888		[learning rate: 0.0037613]
	Learning Rate: 0.00376134
	LOSS [training: 1.0055101193540605 | validation: 0.5243683761185268]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1704469644207376		[learning rate: 0.0037545]
		[batch 20/20] avg loss: 0.9862938234850219		[learning rate: 0.0037477]
	Learning Rate: 0.00374769
	LOSS [training: 1.0783703939528797 | validation: 0.8353093553263594]
	TIME [epoch: 8.87 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1282995901819084		[learning rate: 0.0037409]
		[batch 20/20] avg loss: 1.167055047265432		[learning rate: 0.0037341]
	Learning Rate: 0.00373408
	LOSS [training: 1.1476773187236702 | validation: 0.6496069644107731]
	TIME [epoch: 8.86 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1654650199511614		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 1.096006218756897		[learning rate: 0.0037205]
	Learning Rate: 0.00372053
	LOSS [training: 1.1307356193540292 | validation: 0.5831564474854587]
	TIME [epoch: 8.89 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9088843906615521		[learning rate: 0.0037138]
		[batch 20/20] avg loss: 1.1255995895663637		[learning rate: 0.003707]
	Learning Rate: 0.00370703
	LOSS [training: 1.017241990113958 | validation: 1.2276804595456414]
	TIME [epoch: 8.86 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9553919494800047		[learning rate: 0.0037003]
		[batch 20/20] avg loss: 0.9794163304379223		[learning rate: 0.0036936]
	Learning Rate: 0.00369358
	LOSS [training: 0.9674041399589635 | validation: 0.7871307960491107]
	TIME [epoch: 8.87 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9919238412218968		[learning rate: 0.0036869]
		[batch 20/20] avg loss: 1.1471424104131909		[learning rate: 0.0036802]
	Learning Rate: 0.00368017
	LOSS [training: 1.069533125817544 | validation: 1.4678453295841336]
	TIME [epoch: 8.86 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.057487599063426		[learning rate: 0.0036735]
		[batch 20/20] avg loss: 0.8325802870038718		[learning rate: 0.0036668]
	Learning Rate: 0.00366682
	LOSS [training: 0.9450339430336486 | validation: 0.5328863763954753]
	TIME [epoch: 8.88 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8846757995003103		[learning rate: 0.0036602]
		[batch 20/20] avg loss: 1.04723999249505		[learning rate: 0.0036535]
	Learning Rate: 0.00365351
	LOSS [training: 0.96595789599768 | validation: 0.5802612033800212]
	TIME [epoch: 8.87 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1667863089501673		[learning rate: 0.0036469]
		[batch 20/20] avg loss: 1.1495687152175988		[learning rate: 0.0036403]
	Learning Rate: 0.00364025
	LOSS [training: 1.158177512083883 | validation: 0.47249679681389906]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9176625156224784		[learning rate: 0.0036336]
		[batch 20/20] avg loss: 1.4829021689922706		[learning rate: 0.003627]
	Learning Rate: 0.00362704
	LOSS [training: 1.2002823423073745 | validation: 1.3740927678196924]
	TIME [epoch: 8.87 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.146656329250861		[learning rate: 0.0036205]
		[batch 20/20] avg loss: 0.8627071722084503		[learning rate: 0.0036139]
	Learning Rate: 0.00361388
	LOSS [training: 1.0046817507296557 | validation: 0.8805987905942085]
	TIME [epoch: 8.87 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8502537565484397		[learning rate: 0.0036073]
		[batch 20/20] avg loss: 0.9948663088659654		[learning rate: 0.0036008]
	Learning Rate: 0.00360076
	LOSS [training: 0.9225600327072028 | validation: 0.4829934005430375]
	TIME [epoch: 8.88 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0312795161431534		[learning rate: 0.0035942]
		[batch 20/20] avg loss: 0.9827980369014865		[learning rate: 0.0035877]
	Learning Rate: 0.0035877
	LOSS [training: 1.0070387765223203 | validation: 1.2477577619512474]
	TIME [epoch: 8.87 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0608283164446006		[learning rate: 0.0035812]
		[batch 20/20] avg loss: 0.9593942796438245		[learning rate: 0.0035747]
	Learning Rate: 0.00357468
	LOSS [training: 1.0101112980442124 | validation: 0.8617758219646859]
	TIME [epoch: 8.86 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.986804076011396		[learning rate: 0.0035682]
		[batch 20/20] avg loss: 0.8043837527498109		[learning rate: 0.0035617]
	Learning Rate: 0.0035617
	LOSS [training: 0.8955939143806037 | validation: 0.772901196262793]
	TIME [epoch: 8.87 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9676041536065216		[learning rate: 0.0035552]
		[batch 20/20] avg loss: 0.8772274776671665		[learning rate: 0.0035488]
	Learning Rate: 0.00354878
	LOSS [training: 0.9224158156368439 | validation: 1.2861916296587843]
	TIME [epoch: 8.86 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8998028225479409		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 1.0669100382653942		[learning rate: 0.0035359]
	Learning Rate: 0.0035359
	LOSS [training: 0.9833564304066676 | validation: 0.6938164731579163]
	TIME [epoch: 8.88 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8840676361548464		[learning rate: 0.0035295]
		[batch 20/20] avg loss: 1.0553972755723546		[learning rate: 0.0035231]
	Learning Rate: 0.00352307
	LOSS [training: 0.9697324558636005 | validation: 0.8575740131711347]
	TIME [epoch: 8.86 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8351384386110551		[learning rate: 0.0035167]
		[batch 20/20] avg loss: 1.0316888893286702		[learning rate: 0.0035103]
	Learning Rate: 0.00351028
	LOSS [training: 0.9334136639698627 | validation: 0.7879920015701777]
	TIME [epoch: 8.85 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9077600905114975		[learning rate: 0.0035039]
		[batch 20/20] avg loss: 1.0782131446372636		[learning rate: 0.0034975]
	Learning Rate: 0.00349754
	LOSS [training: 0.9929866175743804 | validation: 0.7636459477668514]
	TIME [epoch: 8.86 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9014783909888617		[learning rate: 0.0034912]
		[batch 20/20] avg loss: 0.9645897899276399		[learning rate: 0.0034849]
	Learning Rate: 0.00348485
	LOSS [training: 0.933034090458251 | validation: 0.5816108561630735]
	TIME [epoch: 8.87 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7680810336887101		[learning rate: 0.0034785]
		[batch 20/20] avg loss: 0.7324432799560996		[learning rate: 0.0034722]
	Learning Rate: 0.0034722
	LOSS [training: 0.7502621568224049 | validation: 0.5727597346182693]
	TIME [epoch: 8.87 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8843974544946891		[learning rate: 0.0034659]
		[batch 20/20] avg loss: 1.0862338385750043		[learning rate: 0.0034596]
	Learning Rate: 0.0034596
	LOSS [training: 0.9853156465348469 | validation: 0.6027557610031626]
	TIME [epoch: 8.86 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8416024516853821		[learning rate: 0.0034533]
		[batch 20/20] avg loss: 0.7489198634866977		[learning rate: 0.003447]
	Learning Rate: 0.00344705
	LOSS [training: 0.7952611575860398 | validation: 0.5105264765364259]
	TIME [epoch: 8.86 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7427973912612655		[learning rate: 0.0034408]
		[batch 20/20] avg loss: 0.9186497386399519		[learning rate: 0.0034345]
	Learning Rate: 0.00343454
	LOSS [training: 0.8307235649506088 | validation: 0.3709036172845962]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9817138219614291		[learning rate: 0.0034283]
		[batch 20/20] avg loss: 0.8814943468833414		[learning rate: 0.0034221]
	Learning Rate: 0.00342207
	LOSS [training: 0.9316040844223854 | validation: 0.8764498565530658]
	TIME [epoch: 8.88 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9174669894056464		[learning rate: 0.0034159]
		[batch 20/20] avg loss: 0.9074102219008399		[learning rate: 0.0034097]
	Learning Rate: 0.00340966
	LOSS [training: 0.912438605653243 | validation: 0.8783638839815509]
	TIME [epoch: 8.86 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7752617374072767		[learning rate: 0.0034035]
		[batch 20/20] avg loss: 0.9740786955658957		[learning rate: 0.0033973]
	Learning Rate: 0.00339728
	LOSS [training: 0.8746702164865863 | validation: 1.0484274418033095]
	TIME [epoch: 8.85 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8589036972171895		[learning rate: 0.0033911]
		[batch 20/20] avg loss: 0.964798727684167		[learning rate: 0.003385]
	Learning Rate: 0.00338495
	LOSS [training: 0.9118512124506782 | validation: 0.6853448771097668]
	TIME [epoch: 8.86 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.799655098051835		[learning rate: 0.0033788]
		[batch 20/20] avg loss: 0.930281071660208		[learning rate: 0.0033727]
	Learning Rate: 0.00337267
	LOSS [training: 0.8649680848560214 | validation: 1.2744363414026711]
	TIME [epoch: 8.86 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0244293040045531		[learning rate: 0.0033665]
		[batch 20/20] avg loss: 0.7527696524199962		[learning rate: 0.0033604]
	Learning Rate: 0.00336043
	LOSS [training: 0.8885994782122746 | validation: 0.4953103074013956]
	TIME [epoch: 8.88 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8929974513711002		[learning rate: 0.0033543]
		[batch 20/20] avg loss: 0.9066597680045099		[learning rate: 0.0033482]
	Learning Rate: 0.00334823
	LOSS [training: 0.8998286096878051 | validation: 1.6328974949098485]
	TIME [epoch: 8.86 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1427503302531994		[learning rate: 0.0033422]
		[batch 20/20] avg loss: 2.0544157318983314		[learning rate: 0.0033361]
	Learning Rate: 0.00333608
	LOSS [training: 2.598583031075765 | validation: 0.8555651107852889]
	TIME [epoch: 8.85 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1235784655686154		[learning rate: 0.00333]
		[batch 20/20] avg loss: 0.9534781053604698		[learning rate: 0.003324]
	Learning Rate: 0.00332398
	LOSS [training: 1.0385282854645426 | validation: 0.5449603644256997]
	TIME [epoch: 8.86 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.883458135556468		[learning rate: 0.0033179]
		[batch 20/20] avg loss: 1.0044990221161805		[learning rate: 0.0033119]
	Learning Rate: 0.00331191
	LOSS [training: 0.9439785788363239 | validation: 0.6152652123278222]
	TIME [epoch: 8.87 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1331932542939924		[learning rate: 0.0033059]
		[batch 20/20] avg loss: 0.9811376467671671		[learning rate: 0.0032999]
	Learning Rate: 0.00329989
	LOSS [training: 1.0571654505305799 | validation: 0.9595543995168877]
	TIME [epoch: 8.87 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8773824895670708		[learning rate: 0.0032939]
		[batch 20/20] avg loss: 1.9636600677213831		[learning rate: 0.0032879]
	Learning Rate: 0.00328792
	LOSS [training: 2.920521278644227 | validation: 1.3499873199887562]
	TIME [epoch: 8.86 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0933487004645812		[learning rate: 0.0032819]
		[batch 20/20] avg loss: 1.3074885640468035		[learning rate: 0.003276]
	Learning Rate: 0.00327599
	LOSS [training: 1.2004186322556922 | validation: 2.1121146188396005]
	TIME [epoch: 8.86 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.216166548655792		[learning rate: 0.00327]
		[batch 20/20] avg loss: 1.059710584600153		[learning rate: 0.0032641]
	Learning Rate: 0.0032641
	LOSS [training: 2.1379385666279727 | validation: 0.5724268652007991]
	TIME [epoch: 8.86 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0013647132557661		[learning rate: 0.0032582]
		[batch 20/20] avg loss: 1.025823539352862		[learning rate: 0.0032523]
	Learning Rate: 0.00325225
	LOSS [training: 1.0135941263043138 | validation: 1.0225539445279996]
	TIME [epoch: 8.88 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2557951192897563		[learning rate: 0.0032463]
		[batch 20/20] avg loss: 2.158699374009386		[learning rate: 0.0032404]
	Learning Rate: 0.00324045
	LOSS [training: 1.7072472466495712 | validation: 1.3495586031259834]
	TIME [epoch: 8.86 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5792313877409607		[learning rate: 0.0032346]
		[batch 20/20] avg loss: 1.5156088677676476		[learning rate: 0.0032287]
	Learning Rate: 0.00322869
	LOSS [training: 1.547420127754304 | validation: 1.0796950624740769]
	TIME [epoch: 8.86 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3292472623564922		[learning rate: 0.0032228]
		[batch 20/20] avg loss: 1.0185777494703623		[learning rate: 0.003217]
	Learning Rate: 0.00321697
	LOSS [training: 1.173912505913427 | validation: 0.6645491136466071]
	TIME [epoch: 8.85 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2468644814790733		[learning rate: 0.0032111]
		[batch 20/20] avg loss: 1.6407859918178485		[learning rate: 0.0032053]
	Learning Rate: 0.0032053
	LOSS [training: 1.4438252366484607 | validation: 4.338593706012267]
	TIME [epoch: 8.86 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.645515015383685		[learning rate: 0.0031995]
		[batch 20/20] avg loss: 8.048275369014878		[learning rate: 0.0031937]
	Learning Rate: 0.00319367
	LOSS [training: 7.846895192199281 | validation: 4.314172397994694]
	TIME [epoch: 8.88 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9219102581549752		[learning rate: 0.0031879]
		[batch 20/20] avg loss: 0.9105647747699553		[learning rate: 0.0031821]
	Learning Rate: 0.00318208
	LOSS [training: 1.4162375164624652 | validation: 0.8929766781384795]
	TIME [epoch: 8.86 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8052379222408257		[learning rate: 0.0031763]
		[batch 20/20] avg loss: 0.9418289568069275		[learning rate: 0.0031705]
	Learning Rate: 0.00317053
	LOSS [training: 0.8735334395238766 | validation: 0.5555784540189436]
	TIME [epoch: 8.86 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7923251841893383		[learning rate: 0.0031648]
		[batch 20/20] avg loss: 0.8812274408736812		[learning rate: 0.003159]
	Learning Rate: 0.00315902
	LOSS [training: 0.8367763125315097 | validation: 0.4737130094994117]
	TIME [epoch: 8.85 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.881511206389311		[learning rate: 0.0031533]
		[batch 20/20] avg loss: 0.7721104221225304		[learning rate: 0.0031476]
	Learning Rate: 0.00314756
	LOSS [training: 0.8268108142559207 | validation: 0.48886808212145605]
	TIME [epoch: 8.86 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7455464187244831		[learning rate: 0.0031418]
		[batch 20/20] avg loss: 0.7977043812873075		[learning rate: 0.0031361]
	Learning Rate: 0.00313613
	LOSS [training: 0.7716254000058953 | validation: 1.3781130328821498]
	TIME [epoch: 8.88 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8854164285600982		[learning rate: 0.0031304]
		[batch 20/20] avg loss: 0.8041052619387493		[learning rate: 0.0031248]
	Learning Rate: 0.00312475
	LOSS [training: 0.8447608452494239 | validation: 0.9033844419997834]
	TIME [epoch: 8.86 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8211310082061123		[learning rate: 0.0031191]
		[batch 20/20] avg loss: 0.6846596473325978		[learning rate: 0.0031134]
	Learning Rate: 0.00311341
	LOSS [training: 0.752895327769355 | validation: 0.4708553776168793]
	TIME [epoch: 8.87 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6634332124511997		[learning rate: 0.0031078]
		[batch 20/20] avg loss: 0.7765639142691542		[learning rate: 0.0031021]
	Learning Rate: 0.00310212
	LOSS [training: 0.7199985633601769 | validation: 0.7699909602788666]
	TIME [epoch: 8.86 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7246001419973858		[learning rate: 0.0030965]
		[batch 20/20] avg loss: 0.8610268167746777		[learning rate: 0.0030909]
	Learning Rate: 0.00309086
	LOSS [training: 0.7928134793860319 | validation: 0.7519214762970566]
	TIME [epoch: 8.88 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7802883440339496		[learning rate: 0.0030852]
		[batch 20/20] avg loss: 0.7819169928745485		[learning rate: 0.0030796]
	Learning Rate: 0.00307964
	LOSS [training: 0.7811026684542492 | validation: 0.7234636632587251]
	TIME [epoch: 8.87 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.595484873345977		[learning rate: 0.003074]
		[batch 20/20] avg loss: 0.7462578879642108		[learning rate: 0.0030685]
	Learning Rate: 0.00306846
	LOSS [training: 0.6708713806550939 | validation: 0.63722605867796]
	TIME [epoch: 8.86 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6616203401043583		[learning rate: 0.0030629]
		[batch 20/20] avg loss: 0.6325462096861757		[learning rate: 0.0030573]
	Learning Rate: 0.00305733
	LOSS [training: 0.647083274895267 | validation: 0.4358354618720427]
	TIME [epoch: 8.86 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.672045823990749		[learning rate: 0.0030518]
		[batch 20/20] avg loss: 0.7665147626641317		[learning rate: 0.0030462]
	Learning Rate: 0.00304623
	LOSS [training: 0.7192802933274403 | validation: 0.8334780866584075]
	TIME [epoch: 8.86 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8430741322519213		[learning rate: 0.0030407]
		[batch 20/20] avg loss: 0.6629144468052415		[learning rate: 0.0030352]
	Learning Rate: 0.00303518
	LOSS [training: 0.7529942895285814 | validation: 0.3203304227454657]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3633600839853326		[learning rate: 0.0030297]
		[batch 20/20] avg loss: 0.6183596672958125		[learning rate: 0.0030242]
	Learning Rate: 0.00302416
	LOSS [training: 0.9908598756405727 | validation: 0.6100193909091426]
	TIME [epoch: 8.86 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6843160392859058		[learning rate: 0.0030187]
		[batch 20/20] avg loss: 0.7847814375833737		[learning rate: 0.0030132]
	Learning Rate: 0.00301319
	LOSS [training: 0.7345487384346396 | validation: 0.3667129743435995]
	TIME [epoch: 8.85 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7585662775322654		[learning rate: 0.0030077]
		[batch 20/20] avg loss: 0.9486024824628165		[learning rate: 0.0030023]
	Learning Rate: 0.00300225
	LOSS [training: 0.853584379997541 | validation: 2.2353677794540463]
	TIME [epoch: 8.86 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.384106596470041		[learning rate: 0.0029968]
		[batch 20/20] avg loss: 4.884974661433503		[learning rate: 0.0029914]
	Learning Rate: 0.00299136
	LOSS [training: 4.634540628951771 | validation: 4.569602425722559]
	TIME [epoch: 8.85 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.373323555092544		[learning rate: 0.0029859]
		[batch 20/20] avg loss: 9.192892810039236		[learning rate: 0.0029805]
	Learning Rate: 0.0029805
	LOSS [training: 8.78310818256589 | validation: 13.198542836844295]
	TIME [epoch: 8.88 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.330265907209935		[learning rate: 0.0029751]
		[batch 20/20] avg loss: 8.60785511834883		[learning rate: 0.0029697]
	Learning Rate: 0.00296969
	LOSS [training: 10.469060512779384 | validation: 10.363386798848612]
	TIME [epoch: 8.85 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.486034811687818		[learning rate: 0.0029643]
		[batch 20/20] avg loss: 2.805927903748511		[learning rate: 0.0029589]
	Learning Rate: 0.00295891
	LOSS [training: 5.145981357718165 | validation: 2.2853883634768004]
	TIME [epoch: 8.85 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.33036984309353		[learning rate: 0.0029535]
		[batch 20/20] avg loss: 2.9799196778939154		[learning rate: 0.0029482]
	Learning Rate: 0.00294817
	LOSS [training: 2.655144760493722 | validation: 7.126195132588832]
	TIME [epoch: 8.86 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.903691685191254		[learning rate: 0.0029428]
		[batch 20/20] avg loss: 1.3779823248648875		[learning rate: 0.0029375]
	Learning Rate: 0.00293747
	LOSS [training: 3.6408370050280707 | validation: 1.2994610300811666]
	TIME [epoch: 8.87 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.01186026774867		[learning rate: 0.0029321]
		[batch 20/20] avg loss: 1.0280005628444617		[learning rate: 0.0029268]
	Learning Rate: 0.00292681
	LOSS [training: 1.519930415296566 | validation: 2.341494932156066]
	TIME [epoch: 8.86 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3785636476769185		[learning rate: 0.0029215]
		[batch 20/20] avg loss: 1.4813096535672066		[learning rate: 0.0029162]
	Learning Rate: 0.00291619
	LOSS [training: 2.4299366506220625 | validation: 1.2888420899087565]
	TIME [epoch: 8.86 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2094497358591039		[learning rate: 0.0029109]
		[batch 20/20] avg loss: 0.8200358088984936		[learning rate: 0.0029056]
	Learning Rate: 0.00290561
	LOSS [training: 1.0147427723787987 | validation: 0.8839613381081592]
	TIME [epoch: 8.86 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0305586202624553		[learning rate: 0.0029003]
		[batch 20/20] avg loss: 2.145335336595596		[learning rate: 0.0028951]
	Learning Rate: 0.00289506
	LOSS [training: 1.5879469784290259 | validation: 3.8109228766449057]
	TIME [epoch: 8.85 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.924922359096631		[learning rate: 0.0028898]
		[batch 20/20] avg loss: 5.537150373374638		[learning rate: 0.0028846]
	Learning Rate: 0.00288456
	LOSS [training: 5.231036366235634 | validation: 2.9088093616918766]
	TIME [epoch: 8.87 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7600611993897837		[learning rate: 0.0028793]
		[batch 20/20] avg loss: 2.194890065390432		[learning rate: 0.0028741]
	Learning Rate: 0.00287409
	LOSS [training: 2.4774756323901084 | validation: 2.53219424874373]
	TIME [epoch: 8.86 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.888501366746334		[learning rate: 0.0028689]
		[batch 20/20] avg loss: 2.841886919087563		[learning rate: 0.0028637]
	Learning Rate: 0.00286366
	LOSS [training: 3.8651941429169483 | validation: 1.8606685819959]
	TIME [epoch: 8.85 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.985666704626729		[learning rate: 0.0028585]
		[batch 20/20] avg loss: 4.2376118309139335		[learning rate: 0.0028533]
	Learning Rate: 0.00285326
	LOSS [training: 3.611639267770331 | validation: 6.388646129937573]
	TIME [epoch: 8.85 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.997835703548387		[learning rate: 0.0028481]
		[batch 20/20] avg loss: 3.453547647852063		[learning rate: 0.0028429]
	Learning Rate: 0.00284291
	LOSS [training: 4.225691675700225 | validation: 2.5787089426177547]
	TIME [epoch: 8.86 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.828036528317382		[learning rate: 0.0028377]
		[batch 20/20] avg loss: 7.0261804701511235		[learning rate: 0.0028326]
	Learning Rate: 0.00283259
	LOSS [training: 6.4271084992342535 | validation: 3.8047029859407737]
	TIME [epoch: 8.87 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7775632627106384		[learning rate: 0.0028274]
		[batch 20/20] avg loss: 4.992716429125886		[learning rate: 0.0028223]
	Learning Rate: 0.00282231
	LOSS [training: 3.885139845918262 | validation: 5.874332947355291]
	TIME [epoch: 8.85 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.893269017612019		[learning rate: 0.0028172]
		[batch 20/20] avg loss: 6.824692578944807		[learning rate: 0.0028121]
	Learning Rate: 0.00281207
	LOSS [training: 6.858980798278412 | validation: 9.706351915789439]
	TIME [epoch: 8.85 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.330402668353582		[learning rate: 0.002807]
		[batch 20/20] avg loss: 6.497834876944272		[learning rate: 0.0028019]
	Learning Rate: 0.00280187
	LOSS [training: 7.414118772648926 | validation: 7.9601354469792085]
	TIME [epoch: 8.85 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.015608900440377		[learning rate: 0.0027968]
		[batch 20/20] avg loss: 3.6956307460631495		[learning rate: 0.0027917]
	Learning Rate: 0.0027917
	LOSS [training: 4.855619823251763 | validation: 2.198463501021552]
	TIME [epoch: 8.87 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3798511927736854		[learning rate: 0.0027866]
		[batch 20/20] avg loss: 3.6266310720012016		[learning rate: 0.0027816]
	Learning Rate: 0.00278157
	LOSS [training: 3.503241132387444 | validation: 4.466978745687668]
	TIME [epoch: 8.86 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.836231239578359		[learning rate: 0.0027765]
		[batch 20/20] avg loss: 3.4582777994326412		[learning rate: 0.0027715]
	Learning Rate: 0.00277147
	LOSS [training: 4.1472545195055 | validation: 2.0457278069458957]
	TIME [epoch: 8.86 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7842006167171176		[learning rate: 0.0027664]
		[batch 20/20] avg loss: 3.695089714814099		[learning rate: 0.0027614]
	Learning Rate: 0.00276141
	LOSS [training: 3.2396451657656087 | validation: 4.507926911628563]
	TIME [epoch: 8.85 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.613560647249462		[learning rate: 0.0027564]
		[batch 20/20] avg loss: 3.1450839434907176		[learning rate: 0.0027514]
	Learning Rate: 0.00275139
	LOSS [training: 3.3793222953700903 | validation: 2.347580557744589]
	TIME [epoch: 8.85 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4169641318753454		[learning rate: 0.0027464]
		[batch 20/20] avg loss: 6.960318289680875		[learning rate: 0.0027414]
	Learning Rate: 0.00274141
	LOSS [training: 5.18864121077811 | validation: 3.8816164194111766]
	TIME [epoch: 8.88 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.860209975194752		[learning rate: 0.0027364]
		[batch 20/20] avg loss: 2.2860665392045383		[learning rate: 0.0027315]
	Learning Rate: 0.00273146
	LOSS [training: 2.573138257199645 | validation: 1.9508992968897072]
	TIME [epoch: 8.86 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.526356036081296		[learning rate: 0.0027265]
		[batch 20/20] avg loss: 2.6404738204753775		[learning rate: 0.0027215]
	Learning Rate: 0.00272155
	LOSS [training: 2.5834149282783376 | validation: 2.2937104837799955]
	TIME [epoch: 8.85 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4206122234258407		[learning rate: 0.0027166]
		[batch 20/20] avg loss: 4.93095607407482		[learning rate: 0.0027117]
	Learning Rate: 0.00271167
	LOSS [training: 4.1757841487503295 | validation: 6.255473544151883]
	TIME [epoch: 8.86 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.732202894758918		[learning rate: 0.0027067]
		[batch 20/20] avg loss: 5.5958172663960815		[learning rate: 0.0027018]
	Learning Rate: 0.00270183
	LOSS [training: 6.1640100805774996 | validation: 1.9907738870219314]
	TIME [epoch: 8.86 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.859568069620631		[learning rate: 0.0026969]
		[batch 20/20] avg loss: 5.842198427352254		[learning rate: 0.002692]
	Learning Rate: 0.00269202
	LOSS [training: 5.3508832484864435 | validation: 4.458955577340048]
	TIME [epoch: 8.88 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.937598110533358		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 4.535671339215679		[learning rate: 0.0026823]
	Learning Rate: 0.00268225
	LOSS [training: 4.236634724874518 | validation: 2.7387801712635698]
	TIME [epoch: 8.86 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5957063508505813		[learning rate: 0.0026774]
		[batch 20/20] avg loss: 3.184250829706104		[learning rate: 0.0026725]
	Learning Rate: 0.00267252
	LOSS [training: 3.389978590278343 | validation: 2.388904192316473]
	TIME [epoch: 8.86 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.438751400059773		[learning rate: 0.0026677]
		[batch 20/20] avg loss: 10.707222410377785		[learning rate: 0.0026628]
	Learning Rate: 0.00266282
	LOSS [training: 8.07298690521878 | validation: 11.605294026620268]
	TIME [epoch: 8.85 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.424681433860751		[learning rate: 0.002658]
		[batch 20/20] avg loss: 11.458446057607338		[learning rate: 0.0026532]
	Learning Rate: 0.00265316
	LOSS [training: 11.441563745734046 | validation: 12.073353175162676]
	TIME [epoch: 8.87 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.962531081003903		[learning rate: 0.0026483]
		[batch 20/20] avg loss: 10.214688413862913		[learning rate: 0.0026435]
	Learning Rate: 0.00264353
	LOSS [training: 11.088609747433406 | validation: 10.427772234851194]
	TIME [epoch: 8.87 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.60851602463254		[learning rate: 0.0026387]
		[batch 20/20] avg loss: 10.993417005677022		[learning rate: 0.0026339]
	Learning Rate: 0.00263394
	LOSS [training: 10.300966515154776 | validation: 11.986002528963112]
	TIME [epoch: 8.85 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.226189045241131		[learning rate: 0.0026292]
		[batch 20/20] avg loss: 12.168267948514687		[learning rate: 0.0026244]
	Learning Rate: 0.00262438
	LOSS [training: 12.197228496877912 | validation: 11.689522217027044]
	TIME [epoch: 8.86 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.909696630312682		[learning rate: 0.0026196]
		[batch 20/20] avg loss: 10.903335175399185		[learning rate: 0.0026149]
	Learning Rate: 0.00261485
	LOSS [training: 10.906515902855933 | validation: 11.39872914939574]
	TIME [epoch: 8.86 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.598632065400661		[learning rate: 0.0026101]
		[batch 20/20] avg loss: 12.034373907617095		[learning rate: 0.0026054]
	Learning Rate: 0.00260536
	LOSS [training: 11.816502986508878 | validation: 11.52135796037079]
	TIME [epoch: 8.88 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.957268924859136		[learning rate: 0.0026006]
		[batch 20/20] avg loss: 10.106185561071943		[learning rate: 0.0025959]
	Learning Rate: 0.00259591
	LOSS [training: 11.031727242965541 | validation: 6.847312530355356]
	TIME [epoch: 8.85 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.902274265438436		[learning rate: 0.0025912]
		[batch 20/20] avg loss: 4.145663005360177		[learning rate: 0.0025865]
	Learning Rate: 0.00258649
	LOSS [training: 4.5239686353993065 | validation: 3.444668710153487]
	TIME [epoch: 8.85 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5343782401031136		[learning rate: 0.0025818]
		[batch 20/20] avg loss: 3.3131171022661947		[learning rate: 0.0025771]
	Learning Rate: 0.0025771
	LOSS [training: 3.4237476711846533 | validation: 2.689249712064273]
	TIME [epoch: 8.86 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.017499677534224		[learning rate: 0.0025724]
		[batch 20/20] avg loss: 3.6233201223056155		[learning rate: 0.0025677]
	Learning Rate: 0.00256775
	LOSS [training: 3.8204098999199205 | validation: 2.496111119188876]
	TIME [epoch: 8.86 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3454729417386915		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 2.9701036740468414		[learning rate: 0.0025584]
	Learning Rate: 0.00255843
	LOSS [training: 3.1577883078927664 | validation: 2.323114937926864]
	TIME [epoch: 8.88 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7628010204250026		[learning rate: 0.0025538]
		[batch 20/20] avg loss: 3.267077170488418		[learning rate: 0.0025491]
	Learning Rate: 0.00254915
	LOSS [training: 3.01493909545671 | validation: 2.1190271805810044]
	TIME [epoch: 8.86 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2081067395553076		[learning rate: 0.0025445]
		[batch 20/20] avg loss: 3.4099273954288956		[learning rate: 0.0025399]
	Learning Rate: 0.0025399
	LOSS [training: 3.309017067492101 | validation: 2.2386263182992288]
	TIME [epoch: 8.86 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9368042051948904		[learning rate: 0.0025353]
		[batch 20/20] avg loss: 3.195948076114774		[learning rate: 0.0025307]
	Learning Rate: 0.00253068
	LOSS [training: 3.5663761406548318 | validation: 2.462935954528417]
	TIME [epoch: 8.85 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.434553186014867		[learning rate: 0.0025261]
		[batch 20/20] avg loss: 3.5950429051556227		[learning rate: 0.0025215]
	Learning Rate: 0.00252149
	LOSS [training: 3.5147980455852457 | validation: 2.5885065273414]
	TIME [epoch: 8.85 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.057011034493698		[learning rate: 0.0025169]
		[batch 20/20] avg loss: 4.0715382159476174		[learning rate: 0.0025123]
	Learning Rate: 0.00251234
	LOSS [training: 4.564274625220658 | validation: 2.36339917337633]
	TIME [epoch: 8.88 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7650891162252806		[learning rate: 0.0025078]
		[batch 20/20] avg loss: 7.602069141242933		[learning rate: 0.0025032]
	Learning Rate: 0.00250323
	LOSS [training: 5.683579128734108 | validation: 9.611132368645565]
	TIME [epoch: 8.85 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.58410837459321		[learning rate: 0.0024987]
		[batch 20/20] avg loss: 8.434228156238293		[learning rate: 0.0024941]
	Learning Rate: 0.00249414
	LOSS [training: 9.00916826541575 | validation: 6.319252082884634]
	TIME [epoch: 8.86 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7582853759934		[learning rate: 0.0024896]
		[batch 20/20] avg loss: 3.6899532513369038		[learning rate: 0.0024851]
	Learning Rate: 0.00248509
	LOSS [training: 5.724119313665152 | validation: 2.53644976080898]
	TIME [epoch: 8.85 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.432786058826848		[learning rate: 0.0024806]
		[batch 20/20] avg loss: 3.073250462829919		[learning rate: 0.0024761]
	Learning Rate: 0.00247607
	LOSS [training: 3.253018260828383 | validation: 2.743105011138069]
	TIME [epoch: 8.87 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.389950684880661		[learning rate: 0.0024716]
		[batch 20/20] avg loss: 3.416492047597681		[learning rate: 0.0024671]
	Learning Rate: 0.00246709
	LOSS [training: 3.4032213662391713 | validation: 2.3078506897739377]
	TIME [epoch: 8.86 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1066427671893555		[learning rate: 0.0024626]
		[batch 20/20] avg loss: 3.226869925162839		[learning rate: 0.0024581]
	Learning Rate: 0.00245813
	LOSS [training: 3.166756346176098 | validation: 2.2271268331720524]
	TIME [epoch: 8.85 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1577012910694817		[learning rate: 0.0024537]
		[batch 20/20] avg loss: 3.0403282846311246		[learning rate: 0.0024492]
	Learning Rate: 0.00244921
	LOSS [training: 3.0990147878503027 | validation: 2.2541237866289165]
	TIME [epoch: 8.86 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.881530659918694		[learning rate: 0.0024448]
		[batch 20/20] avg loss: 5.887346316541405		[learning rate: 0.0024403]
	Learning Rate: 0.00244032
	LOSS [training: 5.384438488230051 | validation: 2.6410851021294026]
	TIME [epoch: 8.86 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4229122051181577		[learning rate: 0.0024359]
		[batch 20/20] avg loss: 3.7917096548453975		[learning rate: 0.0024315]
	Learning Rate: 0.00243147
	LOSS [training: 3.607310929981778 | validation: 2.2265212296357584]
	TIME [epoch: 8.88 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1603748615437817		[learning rate: 0.0024271]
		[batch 20/20] avg loss: 3.1030417957040237		[learning rate: 0.0024226]
	Learning Rate: 0.00242264
	LOSS [training: 3.131708328623902 | validation: 2.258012109549092]
	TIME [epoch: 8.87 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6281901004203525		[learning rate: 0.0024182]
		[batch 20/20] avg loss: 6.391302836876312		[learning rate: 0.0024139]
	Learning Rate: 0.00241385
	LOSS [training: 5.009746468648332 | validation: 8.685928941300096]
	TIME [epoch: 8.85 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.5937184942088		[learning rate: 0.0024095]
		[batch 20/20] avg loss: 5.0024158232856255		[learning rate: 0.0024051]
	Learning Rate: 0.00240509
	LOSS [training: 5.298067158747213 | validation: 8.319044263906127]
	TIME [epoch: 8.85 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.283760963175899		[learning rate: 0.0024007]
		[batch 20/20] avg loss: 6.269877867339361		[learning rate: 0.0023964]
	Learning Rate: 0.00239636
	LOSS [training: 7.276819415257629 | validation: 3.9632634374281634]
	TIME [epoch: 8.86 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2708340216756815		[learning rate: 0.002392]
		[batch 20/20] avg loss: 3.113555195052697		[learning rate: 0.0023877]
	Learning Rate: 0.00238767
	LOSS [training: 3.192194608364189 | validation: 2.35393319969139]
	TIME [epoch: 8.88 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3284129848250394		[learning rate: 0.0023833]
		[batch 20/20] avg loss: 2.819515514922489		[learning rate: 0.002379]
	Learning Rate: 0.002379
	LOSS [training: 3.0739642498737636 | validation: 2.228584145820364]
	TIME [epoch: 8.86 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.151335625020178		[learning rate: 0.0023747]
		[batch 20/20] avg loss: 3.4539284117120177		[learning rate: 0.0023704]
	Learning Rate: 0.00237037
	LOSS [training: 3.302632018366097 | validation: 3.2164571765797834]
	TIME [epoch: 8.85 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.591155918912826		[learning rate: 0.0023661]
		[batch 20/20] avg loss: 4.166737088911377		[learning rate: 0.0023618]
	Learning Rate: 0.00236177
	LOSS [training: 4.878946503912101 | validation: 2.6614562383916383]
	TIME [epoch: 8.86 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.3764494375509795		[learning rate: 0.0023575]
		[batch 20/20] avg loss: 7.392040526912794		[learning rate: 0.0023532]
	Learning Rate: 0.00235319
	LOSS [training: 5.884244982231887 | validation: 10.499694075612219]
	TIME [epoch: 8.86 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.690061318843291		[learning rate: 0.0023489]
		[batch 20/20] avg loss: 11.113936453589753		[learning rate: 0.0023447]
	Learning Rate: 0.00234465
	LOSS [training: 10.901998886216521 | validation: 11.204286561971456]
	TIME [epoch: 8.86 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.36821923448679		[learning rate: 0.0023404]
		[batch 20/20] avg loss: 11.640880638382969		[learning rate: 0.0023361]
	Learning Rate: 0.00233615
	LOSS [training: 11.504549936434879 | validation: 11.566008488543673]
	TIME [epoch: 8.86 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.819771981519732		[learning rate: 0.0023319]
		[batch 20/20] avg loss: 12.114502056450021		[learning rate: 0.0023277]
	Learning Rate: 0.00232767
	LOSS [training: 11.967137018984877 | validation: 11.730960953630388]
	TIME [epoch: 8.85 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.961061358404127		[learning rate: 0.0023234]
		[batch 20/20] avg loss: 11.575458990298912		[learning rate: 0.0023192]
	Learning Rate: 0.00231922
	LOSS [training: 11.768260174351521 | validation: 10.854827381572658]
	TIME [epoch: 8.86 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.558129031263536		[learning rate: 0.002315]
		[batch 20/20] avg loss: 3.2887788161314497		[learning rate: 0.0023108]
	Learning Rate: 0.0023108
	LOSS [training: 5.423453923697494 | validation: 2.3286319509223437]
	TIME [epoch: 8.87 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0479625505073438		[learning rate: 0.0023066]
		[batch 20/20] avg loss: 4.209488366373056		[learning rate: 0.0023024]
	Learning Rate: 0.00230242
	LOSS [training: 3.6287254584401993 | validation: 2.70686786233593]
	TIME [epoch: 8.86 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.220728688228956		[learning rate: 0.0022982]
		[batch 20/20] avg loss: 2.815340721147394		[learning rate: 0.0022941]
	Learning Rate: 0.00229406
	LOSS [training: 3.018034704688175 | validation: 2.2245307192438823]
	TIME [epoch: 8.85 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.06130896497629		[learning rate: 0.0022899]
		[batch 20/20] avg loss: 2.7512651373416794		[learning rate: 0.0022857]
	Learning Rate: 0.00228574
	LOSS [training: 2.9062870511589844 | validation: 2.06330385075128]
	TIME [epoch: 8.85 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7748093087637704		[learning rate: 0.0022816]
		[batch 20/20] avg loss: 3.0220202488286976		[learning rate: 0.0022774]
	Learning Rate: 0.00227744
	LOSS [training: 2.8984147787962344 | validation: 1.9951326621723002]
	TIME [epoch: 8.85 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.877316700058462		[learning rate: 0.0022733]
		[batch 20/20] avg loss: 2.763394807461535		[learning rate: 0.0022692]
	Learning Rate: 0.00226918
	LOSS [training: 2.8203557537599986 | validation: 2.1946406962719376]
	TIME [epoch: 8.87 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0180639710329484		[learning rate: 0.0022651]
		[batch 20/20] avg loss: 2.6763804217179077		[learning rate: 0.0022609]
	Learning Rate: 0.00226094
	LOSS [training: 2.8472221963754274 | validation: 2.0700228461853953]
	TIME [epoch: 8.85 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.901097026365542		[learning rate: 0.0022568]
		[batch 20/20] avg loss: 2.7152604208952837		[learning rate: 0.0022527]
	Learning Rate: 0.00225274
	LOSS [training: 2.8081787236304128 | validation: 1.947863617302238]
	TIME [epoch: 8.84 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.837592096329803		[learning rate: 0.0022486]
		[batch 20/20] avg loss: 2.838146880387879		[learning rate: 0.0022446]
	Learning Rate: 0.00224456
	LOSS [training: 2.837869488358842 | validation: 1.9821545476501479]
	TIME [epoch: 8.85 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.776809411261896		[learning rate: 0.0022405]
		[batch 20/20] avg loss: 3.9461471764668885		[learning rate: 0.0022364]
	Learning Rate: 0.00223642
	LOSS [training: 3.361478293864393 | validation: 3.634320671258971]
	TIME [epoch: 8.87 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.986743092137739		[learning rate: 0.0022324]
		[batch 20/20] avg loss: 2.821658360932738		[learning rate: 0.0022283]
	Learning Rate: 0.0022283
	LOSS [training: 2.9042007265352385 | validation: 1.9133829665463862]
	TIME [epoch: 8.85 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7867056158635624		[learning rate: 0.0022243]
		[batch 20/20] avg loss: 2.8003061637781435		[learning rate: 0.0022202]
	Learning Rate: 0.00222021
	LOSS [training: 2.793505889820853 | validation: 1.9317079046880452]
	TIME [epoch: 9.1 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7638564789437394		[learning rate: 0.0022162]
		[batch 20/20] avg loss: 2.840430125504106		[learning rate: 0.0022122]
	Learning Rate: 0.00221216
	LOSS [training: 2.8021433022239224 | validation: 1.9959148239483742]
	TIME [epoch: 8.84 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8895010525899654		[learning rate: 0.0022081]
		[batch 20/20] avg loss: 2.7121084313116794		[learning rate: 0.0022041]
	Learning Rate: 0.00220413
	LOSS [training: 2.8008047419508224 | validation: 1.9206357588875786]
	TIME [epoch: 8.85 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.813782949377903		[learning rate: 0.0022001]
		[batch 20/20] avg loss: 2.6761156252167617		[learning rate: 0.0021961]
	Learning Rate: 0.00219613
	LOSS [training: 2.7449492872973327 | validation: 2.227752883097236]
	TIME [epoch: 8.86 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4046684402547647		[learning rate: 0.0021921]
		[batch 20/20] avg loss: 2.679924683958971		[learning rate: 0.0021882]
	Learning Rate: 0.00218816
	LOSS [training: 3.042296562106868 | validation: 1.7725347018494646]
	TIME [epoch: 8.85 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4311979025476758		[learning rate: 0.0021842]
		[batch 20/20] avg loss: 2.5958778603815063		[learning rate: 0.0021802]
	Learning Rate: 0.00218022
	LOSS [training: 2.513537881464591 | validation: 1.6465851321635012]
	TIME [epoch: 8.84 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1567141792959648		[learning rate: 0.0021763]
		[batch 20/20] avg loss: 2.1303220608302285		[learning rate: 0.0021723]
	Learning Rate: 0.00217231
	LOSS [training: 2.1435181200630966 | validation: 1.337685105592088]
	TIME [epoch: 8.84 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.207969380633442		[learning rate: 0.0021684]
		[batch 20/20] avg loss: 2.073583998643433		[learning rate: 0.0021644]
	Learning Rate: 0.00216442
	LOSS [training: 2.1407766896384373 | validation: 1.7942410931878074]
	TIME [epoch: 8.85 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.277578928171851		[learning rate: 0.0021605]
		[batch 20/20] avg loss: 3.8233629605810746		[learning rate: 0.0021566]
	Learning Rate: 0.00215657
	LOSS [training: 3.050470944376463 | validation: 6.373530049641594]
	TIME [epoch: 8.87 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.904615210962999		[learning rate: 0.0021527]
		[batch 20/20] avg loss: 5.91219300223776		[learning rate: 0.0021487]
	Learning Rate: 0.00214874
	LOSS [training: 6.408404106600379 | validation: 8.064505052439518]
	TIME [epoch: 8.85 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.989662823566822		[learning rate: 0.0021448]
		[batch 20/20] avg loss: 4.207455104175475		[learning rate: 0.0021409]
	Learning Rate: 0.00214094
	LOSS [training: 5.098558963871149 | validation: 4.45744387151184]
	TIME [epoch: 8.84 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5267235591457995		[learning rate: 0.0021371]
		[batch 20/20] avg loss: 3.0879239224496957		[learning rate: 0.0021332]
	Learning Rate: 0.00213317
	LOSS [training: 3.3073237407977487 | validation: 2.0250123426306694]
	TIME [epoch: 8.83 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.943272645958765		[learning rate: 0.0021293]
		[batch 20/20] avg loss: 3.2629504710775734		[learning rate: 0.0021254]
	Learning Rate: 0.00212543
	LOSS [training: 3.103111558518169 | validation: 2.9054872693892513]
	TIME [epoch: 8.85 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0371337942630054		[learning rate: 0.0021216]
		[batch 20/20] avg loss: 7.754173603307213		[learning rate: 0.0021177]
	Learning Rate: 0.00211772
	LOSS [training: 5.395653698785109 | validation: 10.737603955763495]
	TIME [epoch: 8.86 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.350381329979914		[learning rate: 0.0021139]
		[batch 20/20] avg loss: 8.708638675990375		[learning rate: 0.00211]
	Learning Rate: 0.00211003
	LOSS [training: 9.029510002985145 | validation: 9.506223582275117]
	TIME [epoch: 8.84 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.775138253886933		[learning rate: 0.0021062]
		[batch 20/20] avg loss: 11.826676802366546		[learning rate: 0.0021024]
	Learning Rate: 0.00210238
	LOSS [training: 10.80090752812674 | validation: 12.186525874760012]
	TIME [epoch: 8.85 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.338099846164585		[learning rate: 0.0020986]
		[batch 20/20] avg loss: 12.186780883722253		[learning rate: 0.0020947]
	Learning Rate: 0.00209475
	LOSS [training: 12.26244036494342 | validation: 12.145226053311504]
	TIME [epoch: 8.85 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.30301837818313		[learning rate: 0.0020909]
		[batch 20/20] avg loss: 12.41750588768505		[learning rate: 0.0020871]
	Learning Rate: 0.00208714
	LOSS [training: 12.36026213293409 | validation: 12.336378880484077]
	TIME [epoch: 8.86 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.422999922917162		[learning rate: 0.0020834]
		[batch 20/20] avg loss: 12.396772803257189		[learning rate: 0.0020796]
	Learning Rate: 0.00207957
	LOSS [training: 12.409886363087173 | validation: 12.335251516726869]
	TIME [epoch: 8.84 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.442940348568303		[learning rate: 0.0020758]
		[batch 20/20] avg loss: 12.450019406958038		[learning rate: 0.002072]
	Learning Rate: 0.00207202
	LOSS [training: 12.446479877763172 | validation: 12.349236442853597]
	TIME [epoch: 8.85 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.46205826022069		[learning rate: 0.0020683]
		[batch 20/20] avg loss: 12.255290501614196		[learning rate: 0.0020645]
	Learning Rate: 0.0020645
	LOSS [training: 12.358674380917442 | validation: 12.142198834411765]
	TIME [epoch: 8.85 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.221987942603771		[learning rate: 0.0020608]
		[batch 20/20] avg loss: 12.401289738627272		[learning rate: 0.002057]
	Learning Rate: 0.00205701
	LOSS [training: 12.311638840615522 | validation: 12.343519322448564]
	TIME [epoch: 8.85 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.475099673073345		[learning rate: 0.0020533]
		[batch 20/20] avg loss: 12.452544478125251		[learning rate: 0.0020495]
	Learning Rate: 0.00204955
	LOSS [training: 12.463822075599298 | validation: 12.390077944792415]
	TIME [epoch: 8.87 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.448500279539196		[learning rate: 0.0020458]
		[batch 20/20] avg loss: 12.473690364467654		[learning rate: 0.0020421]
	Learning Rate: 0.00204211
	LOSS [training: 12.461095322003422 | validation: 12.429658971929978]
	TIME [epoch: 8.86 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.495050884252816		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 12.413566075066422		[learning rate: 0.0020347]
	Learning Rate: 0.0020347
	LOSS [training: 12.45430847965962 | validation: 12.37925386946352]
	TIME [epoch: 8.85 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.30556397022549		[learning rate: 0.002031]
		[batch 20/20] avg loss: 11.661572194452393		[learning rate: 0.0020273]
	Learning Rate: 0.00202731
	LOSS [training: 11.983568082338941 | validation: 11.636730636648194]
	TIME [epoch: 8.85 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.673366787670014		[learning rate: 0.0020236]
		[batch 20/20] avg loss: 9.725042760645483		[learning rate: 0.00202]
	Learning Rate: 0.00201996
	LOSS [training: 10.19920477415775 | validation: 10.030690053005081]
	TIME [epoch: 8.85 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.283925092417027		[learning rate: 0.0020163]
		[batch 20/20] avg loss: 10.558427723061316		[learning rate: 0.0020126]
	Learning Rate: 0.00201263
	LOSS [training: 10.421176407739171 | validation: 10.956364560900454]
	TIME [epoch: 8.88 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.015333077209586		[learning rate: 0.002009]
		[batch 20/20] avg loss: 11.952655828602394		[learning rate: 0.0020053]
	Learning Rate: 0.00200532
	LOSS [training: 11.483994452905991 | validation: 12.106064769359197]
	TIME [epoch: 8.85 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.888116678563957		[learning rate: 0.0020017]
		[batch 20/20] avg loss: 11.945515804368105		[learning rate: 0.001998]
	Learning Rate: 0.00199805
	LOSS [training: 11.91681624146603 | validation: 11.967251508736231]
	TIME [epoch: 8.86 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.693826916793281		[learning rate: 0.0019944]
		[batch 20/20] avg loss: 11.767248652493286		[learning rate: 0.0019908]
	Learning Rate: 0.00199079
	LOSS [training: 11.730537784643284 | validation: 12.009672317659659]
	TIME [epoch: 8.85 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.788491648972		[learning rate: 0.0019872]
		[batch 20/20] avg loss: 12.039035901920368		[learning rate: 0.0019836]
	Learning Rate: 0.00198357
	LOSS [training: 11.913763775446185 | validation: 11.97341503042125]
	TIME [epoch: 8.87 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.569161888330054		[learning rate: 0.00198]
		[batch 20/20] avg loss: 11.054160942360495		[learning rate: 0.0019764]
	Learning Rate: 0.00197637
	LOSS [training: 11.311661415345274 | validation: 10.870504013856655]
	TIME [epoch: 8.86 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.927224781891727		[learning rate: 0.0019728]
		[batch 20/20] avg loss: 11.419166404828708		[learning rate: 0.0019692]
	Learning Rate: 0.0019692
	LOSS [training: 11.173195593360216 | validation: 11.424734565865958]
	TIME [epoch: 8.85 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.265592708411637		[learning rate: 0.0019656]
		[batch 20/20] avg loss: 11.91176470308633		[learning rate: 0.0019621]
	Learning Rate: 0.00196205
	LOSS [training: 11.588678705748984 | validation: 12.167692835640619]
	TIME [epoch: 8.85 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.86770958036065		[learning rate: 0.0019585]
		[batch 20/20] avg loss: 10.705981530948982		[learning rate: 0.0019549]
	Learning Rate: 0.00195493
	LOSS [training: 11.286845555654816 | validation: 10.159346013885035]
	TIME [epoch: 8.84 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.15978147405005		[learning rate: 0.0019514]
		[batch 20/20] avg loss: 6.983045074349046		[learning rate: 0.0019478]
	Learning Rate: 0.00194784
	LOSS [training: 8.071413274199548 | validation: 6.623767550236273]
	TIME [epoch: 8.86 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.273066387489663		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 9.817024134174082		[learning rate: 0.0019408]
	Learning Rate: 0.00194077
	LOSS [training: 9.045045260831873 | validation: 9.781515440829278]
	TIME [epoch: 8.85 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.67757121250801		[learning rate: 0.0019372]
		[batch 20/20] avg loss: 9.45568912724344		[learning rate: 0.0019337]
	Learning Rate: 0.00193373
	LOSS [training: 9.566630169875724 | validation: 10.144800623686102]
	TIME [epoch: 8.84 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.31445946466353		[learning rate: 0.0019302]
		[batch 20/20] avg loss: 6.294668097726743		[learning rate: 0.0019267]
	Learning Rate: 0.00192671
	LOSS [training: 7.804563781195137 | validation: 3.7748269134338344]
	TIME [epoch: 8.85 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.671954663415698		[learning rate: 0.0019232]
		[batch 20/20] avg loss: 4.236126040538577		[learning rate: 0.0019197]
	Learning Rate: 0.00191972
	LOSS [training: 4.454040351977138 | validation: 2.7644101558982026]
	TIME [epoch: 8.85 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3052637087565033		[learning rate: 0.0019162]
		[batch 20/20] avg loss: 2.9859773037450257		[learning rate: 0.0019127]
	Learning Rate: 0.00191275
	LOSS [training: 3.145620506250764 | validation: 2.2838349709601924]
	TIME [epoch: 8.87 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7251060950185164		[learning rate: 0.0019093]
		[batch 20/20] avg loss: 3.1769572629660794		[learning rate: 0.0019058]
	Learning Rate: 0.00190581
	LOSS [training: 3.4510316789922975 | validation: 2.3576962797030765]
	TIME [epoch: 8.84 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2191317603834477		[learning rate: 0.0019023]
		[batch 20/20] avg loss: 3.034260349103327		[learning rate: 0.0018989]
	Learning Rate: 0.00189889
	LOSS [training: 3.126696054743387 | validation: 2.1190451820904146]
	TIME [epoch: 8.85 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7331748532491966		[learning rate: 0.0018954]
		[batch 20/20] avg loss: 3.2957429269069856		[learning rate: 0.001892]
	Learning Rate: 0.001892
	LOSS [training: 3.0144588900780906 | validation: 2.3884339170681916]
	TIME [epoch: 8.85 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.989029824124102		[learning rate: 0.0018886]
		[batch 20/20] avg loss: 2.997011872566078		[learning rate: 0.0018851]
	Learning Rate: 0.00188513
	LOSS [training: 2.99302084834509 | validation: 2.063918565525207]
	TIME [epoch: 8.87 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9932396883208874		[learning rate: 0.0018817]
		[batch 20/20] avg loss: 2.894726327080705		[learning rate: 0.0018783]
	Learning Rate: 0.00187829
	LOSS [training: 2.9439830077007954 | validation: 2.093393159005689]
	TIME [epoch: 8.85 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.172313698821856		[learning rate: 0.0018749]
		[batch 20/20] avg loss: 6.943845698187788		[learning rate: 0.0018715]
	Learning Rate: 0.00187148
	LOSS [training: 5.058079698504821 | validation: 7.595823612710946]
	TIME [epoch: 8.85 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.024813226384332		[learning rate: 0.0018681]
		[batch 20/20] avg loss: 8.115931942589757		[learning rate: 0.0018647]
	Learning Rate: 0.00186468
	LOSS [training: 8.070372584487043 | validation: 7.084749048463998]
	TIME [epoch: 8.86 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.853948866856467		[learning rate: 0.0018613]
		[batch 20/20] avg loss: 9.842592544977716		[learning rate: 0.0018579]
	Learning Rate: 0.00185792
	LOSS [training: 9.348270705917091 | validation: 10.049653633348079]
	TIME [epoch: 8.85 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.97838275416107		[learning rate: 0.0018545]
		[batch 20/20] avg loss: 9.915335471024385		[learning rate: 0.0018512]
	Learning Rate: 0.00185117
	LOSS [training: 9.946859112592724 | validation: 9.98644162802833]
	TIME [epoch: 8.87 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.009671725072625		[learning rate: 0.0018478]
		[batch 20/20] avg loss: 3.432357430199924		[learning rate: 0.0018445]
	Learning Rate: 0.00184446
	LOSS [training: 6.221014577636274 | validation: 3.84089234656196]
	TIME [epoch: 8.85 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.984407104831944		[learning rate: 0.0018411]
		[batch 20/20] avg loss: 3.0683379095729735		[learning rate: 0.0018378]
	Learning Rate: 0.00183776
	LOSS [training: 4.026372507202458 | validation: 3.202126326523569]
	TIME [epoch: 8.85 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.426643849709573		[learning rate: 0.0018344]
		[batch 20/20] avg loss: 8.442241039184605		[learning rate: 0.0018311]
	Learning Rate: 0.00183109
	LOSS [training: 7.4344424444470905 | validation: 8.569695717955199]
	TIME [epoch: 8.85 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.336290872351978		[learning rate: 0.0018278]
		[batch 20/20] avg loss: 8.271061214733558		[learning rate: 0.0018244]
	Learning Rate: 0.00182445
	LOSS [training: 7.303676043542768 | validation: 10.263558308247696]
	TIME [epoch: 8.85 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.096176798856394		[learning rate: 0.0018211]
		[batch 20/20] avg loss: 8.887125949723055		[learning rate: 0.0018178]
	Learning Rate: 0.00181783
	LOSS [training: 9.491651374289722 | validation: 9.765846976753988]
	TIME [epoch: 8.86 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.001024471982983		[learning rate: 0.0018145]
		[batch 20/20] avg loss: 10.672424645113011		[learning rate: 0.0018112]
	Learning Rate: 0.00181123
	LOSS [training: 10.336724558547996 | validation: 11.067382814112385]
	TIME [epoch: 8.85 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.12256343207039		[learning rate: 0.0018079]
		[batch 20/20] avg loss: 9.092449687323398		[learning rate: 0.0018047]
	Learning Rate: 0.00180466
	LOSS [training: 9.607506559696894 | validation: 7.184898862913996]
	TIME [epoch: 8.85 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.0167054385701375		[learning rate: 0.0018014]
		[batch 20/20] avg loss: 3.1501867338686105		[learning rate: 0.0017981]
	Learning Rate: 0.00179811
	LOSS [training: 3.5834460862193738 | validation: 2.1775229181371434]
	TIME [epoch: 8.85 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.908306383413997		[learning rate: 0.0017948]
		[batch 20/20] avg loss: 3.048805649149996		[learning rate: 0.0017916]
	Learning Rate: 0.00179158
	LOSS [training: 2.978556016281997 | validation: 2.7575454913948754]
	TIME [epoch: 8.86 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.215422016838277		[learning rate: 0.0017883]
		[batch 20/20] avg loss: 8.7154129866416		[learning rate: 0.0017851]
	Learning Rate: 0.00178508
	LOSS [training: 7.465417501739937 | validation: 8.996826489095085]
	TIME [epoch: 8.87 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.135944882620807		[learning rate: 0.0017818]
		[batch 20/20] avg loss: 11.101690834076136		[learning rate: 0.0017786]
	Learning Rate: 0.0017786
	LOSS [training: 10.618817858348471 | validation: 11.658306717165212]
	TIME [epoch: 8.85 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.846781889753176		[learning rate: 0.0017754]
		[batch 20/20] avg loss: 11.378460011625197		[learning rate: 0.0017721]
	Learning Rate: 0.00177215
	LOSS [training: 11.612620950689188 | validation: 11.09283800723937]
	TIME [epoch: 8.85 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.487883293526982		[learning rate: 0.0017689]
		[batch 20/20] avg loss: 10.326479737641066		[learning rate: 0.0017657]
	Learning Rate: 0.00176572
	LOSS [training: 10.407181515584025 | validation: 10.478033904794666]
	TIME [epoch: 8.84 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.210271888345828		[learning rate: 0.0017625]
		[batch 20/20] avg loss: 8.275874187301092		[learning rate: 0.0017593]
	Learning Rate: 0.00175931
	LOSS [training: 9.243073037823459 | validation: 6.9774037122386305]
	TIME [epoch: 8.86 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.633056703822016		[learning rate: 0.0017561]
		[batch 20/20] avg loss: 6.793215734461668		[learning rate: 0.0017529]
	Learning Rate: 0.00175292
	LOSS [training: 7.213136219141842 | validation: 3.19867929328702]
	TIME [epoch: 8.84 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.07125009716903		[learning rate: 0.0017497]
		[batch 20/20] avg loss: 2.899482391823085		[learning rate: 0.0017466]
	Learning Rate: 0.00174656
	LOSS [training: 2.9853662444960576 | validation: 1.9157976500812792]
	TIME [epoch: 8.84 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.81668651558227		[learning rate: 0.0017434]
		[batch 20/20] avg loss: 2.6742635523149696		[learning rate: 0.0017402]
	Learning Rate: 0.00174022
	LOSS [training: 2.7454750339486194 | validation: 1.8765680186177842]
	TIME [epoch: 8.84 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.408032601668005		[learning rate: 0.0017371]
		[batch 20/20] avg loss: 2.0320075303155707		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 2.2200200659917884 | validation: 1.1745188530467718]
	TIME [epoch: 8.85 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5530320211283102		[learning rate: 0.0017308]
		[batch 20/20] avg loss: 1.8685125695005418		[learning rate: 0.0017276]
	Learning Rate: 0.00172762
	LOSS [training: 1.7107722953144262 | validation: 1.079730201710371]
	TIME [epoch: 8.87 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6665404724655928		[learning rate: 0.0017245]
		[batch 20/20] avg loss: 1.5782999913239908		[learning rate: 0.0017213]
	Learning Rate: 0.00172135
	LOSS [training: 1.6224202318947918 | validation: 1.1486388973389081]
	TIME [epoch: 8.85 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6836564716647522		[learning rate: 0.0017182]
		[batch 20/20] avg loss: 1.5953713068025839		[learning rate: 0.0017151]
	Learning Rate: 0.0017151
	LOSS [training: 1.6395138892336678 | validation: 1.2267900739222515]
	TIME [epoch: 8.93 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4564434042360168		[learning rate: 0.001712]
		[batch 20/20] avg loss: 1.7395356639077615		[learning rate: 0.0017089]
	Learning Rate: 0.00170888
	LOSS [training: 1.597989534071889 | validation: 1.1983744013642046]
	TIME [epoch: 8.83 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5104951639732027		[learning rate: 0.0017058]
		[batch 20/20] avg loss: 1.6511703746703301		[learning rate: 0.0017027]
	Learning Rate: 0.00170267
	LOSS [training: 1.5808327693217663 | validation: 1.1479694470077373]
	TIME [epoch: 8.84 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.616701459675354		[learning rate: 0.0016996]
		[batch 20/20] avg loss: 1.6028327470976866		[learning rate: 0.0016965]
	Learning Rate: 0.0016965
	LOSS [training: 1.60976710338652 | validation: 1.3493947305590088]
	TIME [epoch: 8.86 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4226655543836684		[learning rate: 0.0016934]
		[batch 20/20] avg loss: 1.6858406255878606		[learning rate: 0.0016903]
	Learning Rate: 0.00169034
	LOSS [training: 1.5542530899857647 | validation: 1.3267043289510798]
	TIME [epoch: 8.84 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6027764014773407		[learning rate: 0.0016873]
		[batch 20/20] avg loss: 1.6208809966959734		[learning rate: 0.0016842]
	Learning Rate: 0.0016842
	LOSS [training: 1.6118286990866568 | validation: 1.2355873417281802]
	TIME [epoch: 8.84 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.652681308956429		[learning rate: 0.0016811]
		[batch 20/20] avg loss: 1.7733201234588098		[learning rate: 0.0016781]
	Learning Rate: 0.00167809
	LOSS [training: 1.7130007162076193 | validation: 1.460602154544927]
	TIME [epoch: 8.85 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.182254712031516		[learning rate: 0.001675]
		[batch 20/20] avg loss: 1.5445561763645688		[learning rate: 0.001672]
	Learning Rate: 0.001672
	LOSS [training: 1.863405444198042 | validation: 1.2204916414870666]
	TIME [epoch: 8.87 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4282441377211508		[learning rate: 0.001669]
		[batch 20/20] avg loss: 1.676110198812065		[learning rate: 0.0016659]
	Learning Rate: 0.00166593
	LOSS [training: 1.5521771682666081 | validation: 1.143690769702823]
	TIME [epoch: 8.85 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.589710415996121		[learning rate: 0.0016629]
		[batch 20/20] avg loss: 1.7815535084960854		[learning rate: 0.0016599]
	Learning Rate: 0.00165989
	LOSS [training: 1.6856319622461033 | validation: 1.201183677944092]
	TIME [epoch: 8.84 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.494206792359658		[learning rate: 0.0016569]
		[batch 20/20] avg loss: 1.659916012424405		[learning rate: 0.0016539]
	Learning Rate: 0.00165387
	LOSS [training: 1.5770614023920317 | validation: 1.3983854839789993]
	TIME [epoch: 8.85 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7558494713747437		[learning rate: 0.0016509]
		[batch 20/20] avg loss: 1.3908232589863276		[learning rate: 0.0016479]
	Learning Rate: 0.00164786
	LOSS [training: 1.5733363651805359 | validation: 1.2068561116195538]
	TIME [epoch: 8.85 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5698691499265731		[learning rate: 0.0016449]
		[batch 20/20] avg loss: 1.4920793516259416		[learning rate: 0.0016419]
	Learning Rate: 0.00164188
	LOSS [training: 1.5309742507762574 | validation: 1.1531617805316368]
	TIME [epoch: 8.86 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4167970378112391		[learning rate: 0.0016389]
		[batch 20/20] avg loss: 1.6291840253950518		[learning rate: 0.0016359]
	Learning Rate: 0.00163592
	LOSS [training: 1.5229905316031453 | validation: 1.1735392467515293]
	TIME [epoch: 8.85 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5330111284469405		[learning rate: 0.001633]
		[batch 20/20] avg loss: 1.4395690524021232		[learning rate: 0.00163]
	Learning Rate: 0.00162999
	LOSS [training: 1.4862900904245318 | validation: 1.121028404001692]
	TIME [epoch: 8.83 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3273852337218295		[learning rate: 0.001627]
		[batch 20/20] avg loss: 1.6548234322859479		[learning rate: 0.0016241]
	Learning Rate: 0.00162407
	LOSS [training: 1.491104333003889 | validation: 1.0648534696082002]
	TIME [epoch: 8.85 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4452913731409764		[learning rate: 0.0016211]
		[batch 20/20] avg loss: 1.6097875748661914		[learning rate: 0.0016182]
	Learning Rate: 0.00161818
	LOSS [training: 1.5275394740035837 | validation: 1.1103708711740479]
	TIME [epoch: 8.85 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5744391953428838		[learning rate: 0.0016152]
		[batch 20/20] avg loss: 1.3884459589210805		[learning rate: 0.0016123]
	Learning Rate: 0.00161231
	LOSS [training: 1.4814425771319821 | validation: 1.440456597871725]
	TIME [epoch: 8.87 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6067596526145362		[learning rate: 0.0016094]
		[batch 20/20] avg loss: 1.5463504976863593		[learning rate: 0.0016065]
	Learning Rate: 0.00160645
	LOSS [training: 1.5765550751504478 | validation: 1.3416099681394955]
	TIME [epoch: 8.84 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5277434364029552		[learning rate: 0.0016035]
		[batch 20/20] avg loss: 1.5483111303135564		[learning rate: 0.0016006]
	Learning Rate: 0.00160062
	LOSS [training: 1.5380272833582558 | validation: 1.144165713787987]
	TIME [epoch: 8.85 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6805481169794028		[learning rate: 0.0015977]
		[batch 20/20] avg loss: 1.4191111380389334		[learning rate: 0.0015948]
	Learning Rate: 0.00159482
	LOSS [training: 1.5498296275091679 | validation: 1.4714660562580288]
	TIME [epoch: 8.84 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5098046720355751		[learning rate: 0.0015919]
		[batch 20/20] avg loss: 1.618213386878447		[learning rate: 0.001589]
	Learning Rate: 0.00158903
	LOSS [training: 1.5640090294570108 | validation: 1.7263671305786379]
	TIME [epoch: 8.87 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.554005980008419		[learning rate: 0.0015861]
		[batch 20/20] avg loss: 1.5791889128998027		[learning rate: 0.0015833]
	Learning Rate: 0.00158326
	LOSS [training: 1.5665974464541108 | validation: 1.1050971523327495]
	TIME [epoch: 8.84 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5783692374198446		[learning rate: 0.0015804]
		[batch 20/20] avg loss: 1.4305345841436274		[learning rate: 0.0015775]
	Learning Rate: 0.00157752
	LOSS [training: 1.5044519107817362 | validation: 1.354022394585074]
	TIME [epoch: 8.85 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5246728854911409		[learning rate: 0.0015747]
		[batch 20/20] avg loss: 1.4534146635605123		[learning rate: 0.0015718]
	Learning Rate: 0.00157179
	LOSS [training: 1.4890437745258267 | validation: 1.3253596369005334]
	TIME [epoch: 8.84 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4317284031215443		[learning rate: 0.0015689]
		[batch 20/20] avg loss: 1.6129845454075853		[learning rate: 0.0015661]
	Learning Rate: 0.00156609
	LOSS [training: 1.5223564742645643 | validation: 1.2208267373766508]
	TIME [epoch: 8.84 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6399368350041468		[learning rate: 0.0015632]
		[batch 20/20] avg loss: 1.4410601956734985		[learning rate: 0.0015604]
	Learning Rate: 0.0015604
	LOSS [training: 1.5404985153388226 | validation: 1.1401639208159364]
	TIME [epoch: 8.86 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5707456413960368		[learning rate: 0.0015576]
		[batch 20/20] avg loss: 1.4480874604228762		[learning rate: 0.0015547]
	Learning Rate: 0.00155474
	LOSS [training: 1.5094165509094566 | validation: 1.0592158744815787]
	TIME [epoch: 8.85 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3616824155697542		[learning rate: 0.0015519]
		[batch 20/20] avg loss: 1.529007688467879		[learning rate: 0.0015491]
	Learning Rate: 0.0015491
	LOSS [training: 1.445345052018817 | validation: 1.1355547428560158]
	TIME [epoch: 8.85 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4097379421561755		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 1.568790224282491		[learning rate: 0.0015435]
	Learning Rate: 0.00154348
	LOSS [training: 1.4892640832193333 | validation: 1.1195000419850933]
	TIME [epoch: 8.84 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4015844206497388		[learning rate: 0.0015407]
		[batch 20/20] avg loss: 1.4834775795927002		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 1.4425310001212193 | validation: 1.073128239927001]
	TIME [epoch: 8.85 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5925472435339416		[learning rate: 0.0015351]
		[batch 20/20] avg loss: 1.2697345140714362		[learning rate: 0.0015323]
	Learning Rate: 0.00153229
	LOSS [training: 1.4311408788026891 | validation: 1.1379121275643602]
	TIME [epoch: 8.86 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5451623044011547		[learning rate: 0.0015295]
		[batch 20/20] avg loss: 1.4479037493183355		[learning rate: 0.0015267]
	Learning Rate: 0.00152673
	LOSS [training: 1.4965330268597448 | validation: 1.2433556406932884]
	TIME [epoch: 8.85 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3934512413993412		[learning rate: 0.001524]
		[batch 20/20] avg loss: 1.5172843558213795		[learning rate: 0.0015212]
	Learning Rate: 0.00152119
	LOSS [training: 1.4553677986103604 | validation: 1.3825230849072376]
	TIME [epoch: 8.85 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6224358985054999		[learning rate: 0.0015184]
		[batch 20/20] avg loss: 1.4916212180665722		[learning rate: 0.0015157]
	Learning Rate: 0.00151567
	LOSS [training: 1.557028558286036 | validation: 1.052209633990235]
	TIME [epoch: 8.85 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8541105974930896		[learning rate: 0.0015129]
		[batch 20/20] avg loss: 1.583741752471225		[learning rate: 0.0015102]
	Learning Rate: 0.00151017
	LOSS [training: 1.7189261749821576 | validation: 1.1417364034260202]
	TIME [epoch: 8.85 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.417248450971929		[learning rate: 0.0015074]
		[batch 20/20] avg loss: 1.4776827754472377		[learning rate: 0.0015047]
	Learning Rate: 0.00150469
	LOSS [training: 1.4474656132095833 | validation: 1.0662583914624453]
	TIME [epoch: 8.85 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.363529366577436		[learning rate: 0.001502]
		[batch 20/20] avg loss: 1.5466356248649327		[learning rate: 0.0014992]
	Learning Rate: 0.00149923
	LOSS [training: 1.455082495721184 | validation: 1.3163732368581273]
	TIME [epoch: 8.85 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5674444064066706		[learning rate: 0.0014965]
		[batch 20/20] avg loss: 1.5687464697128086		[learning rate: 0.0014938]
	Learning Rate: 0.00149379
	LOSS [training: 1.5680954380597396 | validation: 1.8341280738460988]
	TIME [epoch: 8.84 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8685085571303222		[learning rate: 0.0014911]
		[batch 20/20] avg loss: 1.5405001892642909		[learning rate: 0.0014884]
	Learning Rate: 0.00148837
	LOSS [training: 1.7045043731973064 | validation: 1.0872995668012482]
	TIME [epoch: 8.84 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4851432896148071		[learning rate: 0.0014857]
		[batch 20/20] avg loss: 1.500229578984608		[learning rate: 0.001483]
	Learning Rate: 0.00148297
	LOSS [training: 1.4926864342997073 | validation: 1.205993347840296]
	TIME [epoch: 8.86 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.508586990073123		[learning rate: 0.0014803]
		[batch 20/20] avg loss: 1.5422655340336218		[learning rate: 0.0014776]
	Learning Rate: 0.00147759
	LOSS [training: 1.5254262620533723 | validation: 1.0244887156969358]
	TIME [epoch: 8.84 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.357475241250936		[learning rate: 0.0014749]
		[batch 20/20] avg loss: 1.6007500427346375		[learning rate: 0.0014722]
	Learning Rate: 0.00147222
	LOSS [training: 1.4791126419927871 | validation: 1.0097418064299308]
	TIME [epoch: 8.83 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4416967291723115		[learning rate: 0.0014695]
		[batch 20/20] avg loss: 1.4285819350330848		[learning rate: 0.0014669]
	Learning Rate: 0.00146688
	LOSS [training: 1.4351393321026982 | validation: 1.4848992184871714]
	TIME [epoch: 8.83 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.563981782358392		[learning rate: 0.0014642]
		[batch 20/20] avg loss: 1.440766450480944		[learning rate: 0.0014616]
	Learning Rate: 0.00146156
	LOSS [training: 1.502374116419668 | validation: 1.066479087165268]
	TIME [epoch: 8.84 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.641707285623839		[learning rate: 0.0014589]
		[batch 20/20] avg loss: 1.6242870346740435		[learning rate: 0.0014563]
	Learning Rate: 0.00145625
	LOSS [training: 1.6329971601489415 | validation: 1.5634064060580015]
	TIME [epoch: 8.86 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6340617792821284		[learning rate: 0.0014536]
		[batch 20/20] avg loss: 1.4097519975140125		[learning rate: 0.001451]
	Learning Rate: 0.00145097
	LOSS [training: 1.5219068883980706 | validation: 1.2184708750810738]
	TIME [epoch: 8.83 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5831948782512537		[learning rate: 0.0014483]
		[batch 20/20] avg loss: 1.6782001200709828		[learning rate: 0.0014457]
	Learning Rate: 0.0014457
	LOSS [training: 1.630697499161118 | validation: 1.3463931636708026]
	TIME [epoch: 8.85 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6664869278570236		[learning rate: 0.0014431]
		[batch 20/20] avg loss: 1.4386507211140354		[learning rate: 0.0014405]
	Learning Rate: 0.00144046
	LOSS [training: 1.5525688244855294 | validation: 1.0801894563809147]
	TIME [epoch: 8.82 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4984749947800604		[learning rate: 0.0014378]
		[batch 20/20] avg loss: 1.6383283359216185		[learning rate: 0.0014352]
	Learning Rate: 0.00143523
	LOSS [training: 1.5684016653508395 | validation: 2.5159643881117106]
	TIME [epoch: 8.84 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6703299495167745		[learning rate: 0.0014326]
		[batch 20/20] avg loss: 1.767289749270099		[learning rate: 0.00143]
	Learning Rate: 0.00143002
	LOSS [training: 1.7188098493934372 | validation: 1.079268653104757]
	TIME [epoch: 8.86 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4970692704523072		[learning rate: 0.0014274]
		[batch 20/20] avg loss: 1.4756480746587588		[learning rate: 0.0014248]
	Learning Rate: 0.00142483
	LOSS [training: 1.4863586725555331 | validation: 1.1197967120555228]
	TIME [epoch: 8.84 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6131164004211445		[learning rate: 0.0014222]
		[batch 20/20] avg loss: 1.529696111452988		[learning rate: 0.0014197]
	Learning Rate: 0.00141966
	LOSS [training: 1.5714062559370663 | validation: 1.0415640020930956]
	TIME [epoch: 8.84 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.404740791267591		[learning rate: 0.0014171]
		[batch 20/20] avg loss: 1.5210858513883383		[learning rate: 0.0014145]
	Learning Rate: 0.00141451
	LOSS [training: 1.4629133213279648 | validation: 1.664307330069452]
	TIME [epoch: 8.85 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0012179282294964		[learning rate: 0.0014119]
		[batch 20/20] avg loss: 1.4658008883947358		[learning rate: 0.0014094]
	Learning Rate: 0.00140937
	LOSS [training: 1.7335094083121168 | validation: 1.0627513420987285]
	TIME [epoch: 8.86 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4082354953199938		[learning rate: 0.0014068]
		[batch 20/20] avg loss: 1.706091197494008		[learning rate: 0.0014043]
	Learning Rate: 0.00140426
	LOSS [training: 1.5571633464070007 | validation: 1.14717978547012]
	TIME [epoch: 8.85 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5349653309084441		[learning rate: 0.0014017]
		[batch 20/20] avg loss: 1.5255105276072123		[learning rate: 0.0013992]
	Learning Rate: 0.00139916
	LOSS [training: 1.530237929257828 | validation: 1.0620999318972095]
	TIME [epoch: 8.85 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.502055835383519		[learning rate: 0.0013966]
		[batch 20/20] avg loss: 1.5096731291333203		[learning rate: 0.0013941]
	Learning Rate: 0.00139409
	LOSS [training: 1.5058644822584197 | validation: 1.336501726588254]
	TIME [epoch: 8.84 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.538446421592008		[learning rate: 0.0013916]
		[batch 20/20] avg loss: 1.5032964758339415		[learning rate: 0.001389]
	Learning Rate: 0.00138903
	LOSS [training: 1.5208714487129746 | validation: 1.1574480231768018]
	TIME [epoch: 8.84 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5427773382916468		[learning rate: 0.0013865]
		[batch 20/20] avg loss: 1.5194160605761877		[learning rate: 0.001384]
	Learning Rate: 0.00138399
	LOSS [training: 1.5310966994339168 | validation: 1.046072921931087]
	TIME [epoch: 8.87 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4896524471028763		[learning rate: 0.0013815]
		[batch 20/20] avg loss: 1.4651859861564953		[learning rate: 0.001379]
	Learning Rate: 0.00137896
	LOSS [training: 1.4774192166296858 | validation: 1.1899841075494724]
	TIME [epoch: 8.85 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3592853415836843		[learning rate: 0.0013765]
		[batch 20/20] avg loss: 1.477933085054411		[learning rate: 0.001374]
	Learning Rate: 0.00137396
	LOSS [training: 1.4186092133190478 | validation: 1.037587695775604]
	TIME [epoch: 8.84 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4877801982223722		[learning rate: 0.0013715]
		[batch 20/20] avg loss: 1.3934799321200593		[learning rate: 0.001369]
	Learning Rate: 0.00136897
	LOSS [training: 1.4406300651712158 | validation: 1.1443835960107935]
	TIME [epoch: 8.85 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.33750005715607		[learning rate: 0.0013665]
		[batch 20/20] avg loss: 1.4446651611934187		[learning rate: 0.001364]
	Learning Rate: 0.001364
	LOSS [training: 1.3910826091747441 | validation: 1.1643611657390893]
	TIME [epoch: 8.85 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4650660503014066		[learning rate: 0.0013615]
		[batch 20/20] avg loss: 1.3951420639455905		[learning rate: 0.0013591]
	Learning Rate: 0.00135905
	LOSS [training: 1.4301040571234984 | validation: 1.0686143077241712]
	TIME [epoch: 8.87 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3683941898840444		[learning rate: 0.0013566]
		[batch 20/20] avg loss: 1.3678102560358838		[learning rate: 0.0013541]
	Learning Rate: 0.00135412
	LOSS [training: 1.368102222959964 | validation: 1.0462988462626754]
	TIME [epoch: 8.85 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6311572222344168		[learning rate: 0.0013517]
		[batch 20/20] avg loss: 2.1625326501011264		[learning rate: 0.0013492]
	Learning Rate: 0.00134921
	LOSS [training: 1.8968449361677713 | validation: 2.934915291420172]
	TIME [epoch: 8.84 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9427814816191744		[learning rate: 0.0013468]
		[batch 20/20] avg loss: 1.7441139530435883		[learning rate: 0.0013443]
	Learning Rate: 0.00134431
	LOSS [training: 1.8434477173313817 | validation: 1.232803872102839]
	TIME [epoch: 8.84 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.696721514426325		[learning rate: 0.0013419]
		[batch 20/20] avg loss: 1.4090371480086639		[learning rate: 0.0013394]
	Learning Rate: 0.00133943
	LOSS [training: 1.5528793312174947 | validation: 1.3614027332247147]
	TIME [epoch: 8.86 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6341085000645799		[learning rate: 0.001337]
		[batch 20/20] avg loss: 1.3822332797840584		[learning rate: 0.0013346]
	Learning Rate: 0.00133457
	LOSS [training: 1.508170889924319 | validation: 1.109833947039543]
	TIME [epoch: 8.84 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.150100738361872		[learning rate: 0.0013321]
		[batch 20/20] avg loss: 3.2749380940634665		[learning rate: 0.0013297]
	Learning Rate: 0.00132973
	LOSS [training: 2.71251941621267 | validation: 2.1478658536514805]
	TIME [epoch: 8.84 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8315242744517066		[learning rate: 0.0013273]
		[batch 20/20] avg loss: 2.9346522599713194		[learning rate: 0.0013249]
	Learning Rate: 0.0013249
	LOSS [training: 2.8830882672115132 | validation: 2.180217166761185]
	TIME [epoch: 8.84 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6330893216019517		[learning rate: 0.0013225]
		[batch 20/20] avg loss: 2.402397280084772		[learning rate: 0.0013201]
	Learning Rate: 0.0013201
	LOSS [training: 2.517743300843362 | validation: 1.7321236312263881]
	TIME [epoch: 8.84 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.859552094244231		[learning rate: 0.0013177]
		[batch 20/20] avg loss: 1.5203046500358233		[learning rate: 0.0013153]
	Learning Rate: 0.0013153
	LOSS [training: 1.689928372140027 | validation: 1.082692176976071]
	TIME [epoch: 8.86 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3978727113367644		[learning rate: 0.0013129]
		[batch 20/20] avg loss: 2.7554202694844343		[learning rate: 0.0013105]
	Learning Rate: 0.00131053
	LOSS [training: 2.076646490410599 | validation: 3.3939161277013232]
	TIME [epoch: 8.85 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.470425296192024		[learning rate: 0.0013082]
		[batch 20/20] avg loss: 9.293177133114181		[learning rate: 0.0013058]
	Learning Rate: 0.00130578
	LOSS [training: 6.381801214653103 | validation: 11.518677999163657]
	TIME [epoch: 8.84 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.10969261846804		[learning rate: 0.0013034]
		[batch 20/20] avg loss: 11.785962352041407		[learning rate: 0.001301]
	Learning Rate: 0.00130104
	LOSS [training: 11.447827485254724 | validation: 11.600740908203836]
	TIME [epoch: 8.84 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.565888804142151		[learning rate: 0.0012987]
		[batch 20/20] avg loss: 11.116935522559723		[learning rate: 0.0012963]
	Learning Rate: 0.00129631
	LOSS [training: 11.341412163350935 | validation: 10.798466006972276]
	TIME [epoch: 8.84 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.57094847306771		[learning rate: 0.001294]
		[batch 20/20] avg loss: 10.348389090806197		[learning rate: 0.0012916]
	Learning Rate: 0.00129161
	LOSS [training: 10.459668781936955 | validation: 10.770270065877451]
	TIME [epoch: 8.86 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.337898786279407		[learning rate: 0.0012893]
		[batch 20/20] avg loss: 10.723522806696579		[learning rate: 0.0012869]
	Learning Rate: 0.00128692
	LOSS [training: 10.530710796487991 | validation: 11.163951199695378]
	TIME [epoch: 8.84 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.153529200874207		[learning rate: 0.0012846]
		[batch 20/20] avg loss: 11.198346480251729		[learning rate: 0.0012823]
	Learning Rate: 0.00128225
	LOSS [training: 11.17593784056297 | validation: 11.281213329330035]
	TIME [epoch: 8.84 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.003454669722855		[learning rate: 0.0012799]
		[batch 20/20] avg loss: 10.951373246807227		[learning rate: 0.0012776]
	Learning Rate: 0.0012776
	LOSS [training: 10.977413958265043 | validation: 11.306543575126284]
	TIME [epoch: 8.83 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.033210866973143		[learning rate: 0.0012753]
		[batch 20/20] avg loss: 10.951804453515898		[learning rate: 0.001273]
	Learning Rate: 0.00127296
	LOSS [training: 10.992507660244522 | validation: 11.1483672424951]
	TIME [epoch: 8.85 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.501928496180298		[learning rate: 0.0012707]
		[batch 20/20] avg loss: 10.069459246802413		[learning rate: 0.0012683]
	Learning Rate: 0.00126834
	LOSS [training: 10.285693871491357 | validation: 10.51203186130077]
	TIME [epoch: 8.83 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.38525565836505		[learning rate: 0.001266]
		[batch 20/20] avg loss: 9.688419904934602		[learning rate: 0.0012637]
	Learning Rate: 0.00126374
	LOSS [training: 10.036837781649824 | validation: 10.083468151474316]
	TIME [epoch: 8.84 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.210403218184755		[learning rate: 0.0012614]
		[batch 20/20] avg loss: 6.430874045930969		[learning rate: 0.0012592]
	Learning Rate: 0.00125915
	LOSS [training: 7.820638632057862 | validation: 7.005844625263781]
	TIME [epoch: 8.84 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.458434865404497		[learning rate: 0.0012569]
		[batch 20/20] avg loss: 4.584455630353652		[learning rate: 0.0012546]
	Learning Rate: 0.00125458
	LOSS [training: 5.0214452478790745 | validation: 5.056637262215501]
	TIME [epoch: 8.84 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.192244220811848		[learning rate: 0.0012523]
		[batch 20/20] avg loss: 3.9552430648034593		[learning rate: 0.00125]
	Learning Rate: 0.00125003
	LOSS [training: 4.073743642807653 | validation: 4.5182600768244425]
	TIME [epoch: 8.86 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.733884112632994		[learning rate: 0.0012478]
		[batch 20/20] avg loss: 3.9233307172637937		[learning rate: 0.0012455]
	Learning Rate: 0.0012455
	LOSS [training: 3.8286074149483937 | validation: 3.767859373860947]
	TIME [epoch: 8.84 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8003106207962363		[learning rate: 0.0012432]
		[batch 20/20] avg loss: 4.309786844229158		[learning rate: 0.001241]
	Learning Rate: 0.00124098
	LOSS [training: 4.055048732512697 | validation: 5.808225499062172]
	TIME [epoch: 8.84 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4083308078691132		[learning rate: 0.0012387]
		[batch 20/20] avg loss: 4.379358879561584		[learning rate: 0.0012365]
	Learning Rate: 0.00123647
	LOSS [training: 3.8938448437153497 | validation: 4.946047001724336]
	TIME [epoch: 8.84 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.057222720106876		[learning rate: 0.0012342]
		[batch 20/20] avg loss: 3.512624566736035		[learning rate: 0.001232]
	Learning Rate: 0.00123198
	LOSS [training: 3.7849236434214553 | validation: 4.81548881261217]
	TIME [epoch: 8.85 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.656899685788093		[learning rate: 0.0012297]
		[batch 20/20] avg loss: 4.352701084424829		[learning rate: 0.0012275]
	Learning Rate: 0.00122751
	LOSS [training: 4.00480038510646 | validation: 6.597049645075764]
	TIME [epoch: 8.87 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.267857112165208		[learning rate: 0.0012253]
		[batch 20/20] avg loss: 3.5117760605898356		[learning rate: 0.0012231]
	Learning Rate: 0.00122306
	LOSS [training: 3.889816586377522 | validation: 4.477612517594948]
	TIME [epoch: 8.85 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6426857388520104		[learning rate: 0.0012208]
		[batch 20/20] avg loss: 3.326780004550789		[learning rate: 0.0012186]
	Learning Rate: 0.00121862
	LOSS [training: 3.4847328717013992 | validation: 4.638765141438004]
	TIME [epoch: 8.85 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4694994270436665		[learning rate: 0.0012164]
		[batch 20/20] avg loss: 3.1745433029961747		[learning rate: 0.0012142]
	Learning Rate: 0.0012142
	LOSS [training: 3.3220213650199213 | validation: 4.574486590851405]
	TIME [epoch: 8.84 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.325039254132329		[learning rate: 0.001212]
		[batch 20/20] avg loss: 2.96038649627971		[learning rate: 0.0012098]
	Learning Rate: 0.00120979
	LOSS [training: 3.14271287520602 | validation: 2.7025454948167766]
	TIME [epoch: 8.84 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.310576535671346		[learning rate: 0.0012076]
		[batch 20/20] avg loss: 3.2646397051149245		[learning rate: 0.0012054]
	Learning Rate: 0.0012054
	LOSS [training: 3.287608120393135 | validation: 4.057329410584629]
	TIME [epoch: 8.86 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2452890562723047		[learning rate: 0.0012032]
		[batch 20/20] avg loss: 3.4238792670216456		[learning rate: 0.001201]
	Learning Rate: 0.00120103
	LOSS [training: 3.334584161646975 | validation: 3.939380764008853]
	TIME [epoch: 8.85 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.935686082193736		[learning rate: 0.0011988]
		[batch 20/20] avg loss: 3.155018657451429		[learning rate: 0.0011967]
	Learning Rate: 0.00119667
	LOSS [training: 3.045352369822582 | validation: 2.859819283177342]
	TIME [epoch: 8.84 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.648660461905203		[learning rate: 0.0011945]
		[batch 20/20] avg loss: 2.792517721203727		[learning rate: 0.0011923]
	Learning Rate: 0.00119233
	LOSS [training: 2.720589091554465 | validation: 3.1564491356220157]
	TIME [epoch: 8.84 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6841002154808127		[learning rate: 0.0011902]
		[batch 20/20] avg loss: 3.367424109047743		[learning rate: 0.001188]
	Learning Rate: 0.001188
	LOSS [training: 3.025762162264278 | validation: 4.682451424146308]
	TIME [epoch: 8.86 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6605139440774837		[learning rate: 0.0011858]
		[batch 20/20] avg loss: 3.9746036010406636		[learning rate: 0.0011837]
	Learning Rate: 0.00118369
	LOSS [training: 3.8175587725590736 | validation: 5.096547804705664]
	TIME [epoch: 8.84 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.694233904718144		[learning rate: 0.0011815]
		[batch 20/20] avg loss: 3.3749421308786935		[learning rate: 0.0011794]
	Learning Rate: 0.00117939
	LOSS [training: 3.53458801779842 | validation: 4.399770091240647]
	TIME [epoch: 8.84 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.481878417973024		[learning rate: 0.0011772]
		[batch 20/20] avg loss: 2.773077123847275		[learning rate: 0.0011751]
	Learning Rate: 0.00117511
	LOSS [training: 3.12747777091015 | validation: 3.767423894623157]
	TIME [epoch: 8.84 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.915913913590652		[learning rate: 0.001173]
		[batch 20/20] avg loss: 2.9415700321685323		[learning rate: 0.0011708]
	Learning Rate: 0.00117085
	LOSS [training: 2.9287419728795925 | validation: 2.856160749374769]
	TIME [epoch: 8.84 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.910478505424977		[learning rate: 0.0011687]
		[batch 20/20] avg loss: 2.434709060170232		[learning rate: 0.0011666]
	Learning Rate: 0.0011666
	LOSS [training: 2.6725937827976045 | validation: 2.6512236922581267]
	TIME [epoch: 8.86 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.59120152787444		[learning rate: 0.0011645]
		[batch 20/20] avg loss: 2.914677458514828		[learning rate: 0.0011624]
	Learning Rate: 0.00116236
	LOSS [training: 2.752939493194634 | validation: 3.4712824228658854]
	TIME [epoch: 8.85 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.592111875110029		[learning rate: 0.0011603]
		[batch 20/20] avg loss: 2.969105749581692		[learning rate: 0.0011581]
	Learning Rate: 0.00115815
	LOSS [training: 2.7806088123458608 | validation: 3.095449691892741]
	TIME [epoch: 8.84 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8075940239659887		[learning rate: 0.001156]
		[batch 20/20] avg loss: 2.8604084344574914		[learning rate: 0.0011539]
	Learning Rate: 0.00115394
	LOSS [training: 2.8340012292117396 | validation: 2.728620400971901]
	TIME [epoch: 8.84 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4905355272025522		[learning rate: 0.0011518]
		[batch 20/20] avg loss: 2.27132253050557		[learning rate: 0.0011498]
	Learning Rate: 0.00114975
	LOSS [training: 2.3809290288540605 | validation: 2.63708660325659]
	TIME [epoch: 8.84 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.370388106949704		[learning rate: 0.0011477]
		[batch 20/20] avg loss: 2.4166303488734786		[learning rate: 0.0011456]
	Learning Rate: 0.00114558
	LOSS [training: 2.3935092279115913 | validation: 2.3044389571889505]
	TIME [epoch: 8.86 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.53229926735407		[learning rate: 0.0011435]
		[batch 20/20] avg loss: 2.078128113604806		[learning rate: 0.0011414]
	Learning Rate: 0.00114142
	LOSS [training: 2.305213690479438 | validation: 1.9271905799270554]
	TIME [epoch: 8.83 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0956350247709445		[learning rate: 0.0011394]
		[batch 20/20] avg loss: 2.063339202530008		[learning rate: 0.0011373]
	Learning Rate: 0.00113728
	LOSS [training: 2.0794871136504764 | validation: 1.9245331551341678]
	TIME [epoch: 8.84 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3021063016704986		[learning rate: 0.0011352]
		[batch 20/20] avg loss: 2.1139509905728917		[learning rate: 0.0011332]
	Learning Rate: 0.00113316
	LOSS [training: 2.208028646121695 | validation: 2.0209854792349837]
	TIME [epoch: 8.83 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5331152833242307		[learning rate: 0.0011311]
		[batch 20/20] avg loss: 2.3422810156446126		[learning rate: 0.001129]
	Learning Rate: 0.00112904
	LOSS [training: 2.4376981494844214 | validation: 2.0347630296266184]
	TIME [epoch: 8.86 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1336012237581605		[learning rate: 0.001127]
		[batch 20/20] avg loss: 2.048916045499928		[learning rate: 0.0011249]
	Learning Rate: 0.00112495
	LOSS [training: 2.091258634629044 | validation: 1.9590007286808464]
	TIME [epoch: 8.83 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.994875849471287		[learning rate: 0.0011229]
		[batch 20/20] avg loss: 2.08400696254743		[learning rate: 0.0011209]
	Learning Rate: 0.00112086
	LOSS [training: 2.0394414060093586 | validation: 1.8245912383001857]
	TIME [epoch: 8.83 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2290579159954538		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 2.260453159006118		[learning rate: 0.0011168]
	Learning Rate: 0.0011168
	LOSS [training: 2.2447555375007857 | validation: 2.793810432021217]
	TIME [epoch: 8.84 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.060410654888544		[learning rate: 0.0011148]
		[batch 20/20] avg loss: 2.2284937770641475		[learning rate: 0.0011127]
	Learning Rate: 0.00111274
	LOSS [training: 2.144452215976346 | validation: 1.959396552622114]
	TIME [epoch: 8.83 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.162619296988487		[learning rate: 0.0011107]
		[batch 20/20] avg loss: 2.056228658435417		[learning rate: 0.0011087]
	Learning Rate: 0.0011087
	LOSS [training: 2.1094239777119514 | validation: 1.7573194651812656]
	TIME [epoch: 8.86 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.083334435092099		[learning rate: 0.0011067]
		[batch 20/20] avg loss: 1.9410028879966597		[learning rate: 0.0011047]
	Learning Rate: 0.00110468
	LOSS [training: 2.012168661544379 | validation: 1.7590371735838113]
	TIME [epoch: 8.84 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1241691840005457		[learning rate: 0.0011027]
		[batch 20/20] avg loss: 1.9810735417768832		[learning rate: 0.0011007]
	Learning Rate: 0.00110067
	LOSS [training: 2.052621362888715 | validation: 1.6803303279612296]
	TIME [epoch: 8.83 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9971478795192952		[learning rate: 0.0010987]
		[batch 20/20] avg loss: 2.295060209523909		[learning rate: 0.0010967]
	Learning Rate: 0.00109668
	LOSS [training: 2.146104044521602 | validation: 1.8968892211641426]
	TIME [epoch: 8.84 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0447097775504686		[learning rate: 0.0010947]
		[batch 20/20] avg loss: 1.850758776545834		[learning rate: 0.0010927]
	Learning Rate: 0.0010927
	LOSS [training: 1.9477342770481514 | validation: 1.5059879128840405]
	TIME [epoch: 8.84 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.781742732794558		[learning rate: 0.0010907]
		[batch 20/20] avg loss: 1.890655225594405		[learning rate: 0.0010887]
	Learning Rate: 0.00108873
	LOSS [training: 1.8361989791944815 | validation: 1.8253598064009393]
	TIME [epoch: 8.85 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.036238406407453		[learning rate: 0.0010868]
		[batch 20/20] avg loss: 2.007210298389176		[learning rate: 0.0010848]
	Learning Rate: 0.00108478
	LOSS [training: 2.021724352398315 | validation: 2.5920567287539575]
	TIME [epoch: 8.84 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.056685774904855		[learning rate: 0.0010828]
		[batch 20/20] avg loss: 1.9126829856089418		[learning rate: 0.0010808]
	Learning Rate: 0.00108084
	LOSS [training: 1.984684380256899 | validation: 1.605180300439295]
	TIME [epoch: 8.84 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2564666341768556		[learning rate: 0.0010789]
		[batch 20/20] avg loss: 2.9502736135925756		[learning rate: 0.0010769]
	Learning Rate: 0.00107692
	LOSS [training: 2.603370123884716 | validation: 2.365107613408627]
	TIME [epoch: 8.84 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0471149711888232		[learning rate: 0.001075]
		[batch 20/20] avg loss: 2.0833130843432217		[learning rate: 0.001073]
	Learning Rate: 0.00107301
	LOSS [training: 2.0652140277660225 | validation: 1.501490758370782]
	TIME [epoch: 8.85 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9062958484092647		[learning rate: 0.0010711]
		[batch 20/20] avg loss: 1.9871755570851968		[learning rate: 0.0010691]
	Learning Rate: 0.00106912
	LOSS [training: 1.9467357027472303 | validation: 1.580818291950543]
	TIME [epoch: 8.85 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0270541718339854		[learning rate: 0.0010672]
		[batch 20/20] avg loss: 1.8958035293607665		[learning rate: 0.0010652]
	Learning Rate: 0.00106524
	LOSS [training: 1.9614288505973758 | validation: 1.6563239592030192]
	TIME [epoch: 8.84 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8051224315460441		[learning rate: 0.0010633]
		[batch 20/20] avg loss: 1.9732216544336647		[learning rate: 0.0010614]
	Learning Rate: 0.00106137
	LOSS [training: 1.8891720429898544 | validation: 1.5455659883600013]
	TIME [epoch: 8.84 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0132134476948775		[learning rate: 0.0010594]
		[batch 20/20] avg loss: 1.7926364065108502		[learning rate: 0.0010575]
	Learning Rate: 0.00105752
	LOSS [training: 1.9029249271028639 | validation: 1.5518237782656217]
	TIME [epoch: 8.85 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8068968184787195		[learning rate: 0.0010556]
		[batch 20/20] avg loss: 1.8571362921310677		[learning rate: 0.0010537]
	Learning Rate: 0.00105368
	LOSS [training: 1.8320165553048935 | validation: 1.560691756411409]
	TIME [epoch: 8.86 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9027568751311812		[learning rate: 0.0010518]
		[batch 20/20] avg loss: 1.853454661895715		[learning rate: 0.0010499]
	Learning Rate: 0.00104986
	LOSS [training: 1.8781057685134481 | validation: 1.4865032898868036]
	TIME [epoch: 8.84 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7180456593379585		[learning rate: 0.001048]
		[batch 20/20] avg loss: 1.8600100068807108		[learning rate: 0.0010461]
	Learning Rate: 0.00104605
	LOSS [training: 1.7890278331093346 | validation: 1.5680331100823464]
	TIME [epoch: 8.84 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8950254399467177		[learning rate: 0.0010442]
		[batch 20/20] avg loss: 1.7895697118878797		[learning rate: 0.0010423]
	Learning Rate: 0.00104225
	LOSS [training: 1.8422975759172986 | validation: 1.7394585378305367]
	TIME [epoch: 8.83 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8252923387571751		[learning rate: 0.0010404]
		[batch 20/20] avg loss: 1.6947671483715578		[learning rate: 0.0010385]
	Learning Rate: 0.00103847
	LOSS [training: 1.7600297435643661 | validation: 1.4852197802105684]
	TIME [epoch: 8.85 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8000105590897486		[learning rate: 0.0010366]
		[batch 20/20] avg loss: 1.9160289199676388		[learning rate: 0.0010347]
	Learning Rate: 0.0010347
	LOSS [training: 1.8580197395286935 | validation: 1.6202829076113152]
	TIME [epoch: 8.86 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8163966672983016		[learning rate: 0.0010328]
		[batch 20/20] avg loss: 1.9145953006346041		[learning rate: 0.0010309]
	Learning Rate: 0.00103095
	LOSS [training: 1.8654959839664524 | validation: 1.6404747590664406]
	TIME [epoch: 8.85 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.925423455802513		[learning rate: 0.0010291]
		[batch 20/20] avg loss: 1.664827471012351		[learning rate: 0.0010272]
	Learning Rate: 0.00102721
	LOSS [training: 1.7951254634074316 | validation: 1.4201757490092448]
	TIME [epoch: 8.85 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5802826551745632		[learning rate: 0.0010253]
		[batch 20/20] avg loss: 1.8843050246867534		[learning rate: 0.0010235]
	Learning Rate: 0.00102348
	LOSS [training: 1.7322938399306582 | validation: 1.5006527054112828]
	TIME [epoch: 8.84 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.735981053002148		[learning rate: 0.0010216]
		[batch 20/20] avg loss: 1.7386836357857138		[learning rate: 0.0010198]
	Learning Rate: 0.00101976
	LOSS [training: 1.737332344393931 | validation: 1.53192256863641]
	TIME [epoch: 8.84 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8732408065990582		[learning rate: 0.0010179]
		[batch 20/20] avg loss: 1.8858467911885417		[learning rate: 0.0010161]
	Learning Rate: 0.00101606
	LOSS [training: 1.8795437988938002 | validation: 1.6540957231065092]
	TIME [epoch: 8.86 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6690871493265491		[learning rate: 0.0010142]
		[batch 20/20] avg loss: 1.847890128804039		[learning rate: 0.0010124]
	Learning Rate: 0.00101238
	LOSS [training: 1.758488639065294 | validation: 1.4486078347514628]
	TIME [epoch: 8.84 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6746506079165524		[learning rate: 0.0010105]
		[batch 20/20] avg loss: 1.7845355880022602		[learning rate: 0.0010087]
	Learning Rate: 0.0010087
	LOSS [training: 1.7295930979594065 | validation: 1.4527068453099796]
	TIME [epoch: 8.84 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7226590138441473		[learning rate: 0.0010069]
		[batch 20/20] avg loss: 1.8716867956587813		[learning rate: 0.001005]
	Learning Rate: 0.00100504
	LOSS [training: 1.7971729047514642 | validation: 1.7277413629936007]
	TIME [epoch: 8.84 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9373403582548172		[learning rate: 0.0010032]
		[batch 20/20] avg loss: 1.8249010398254442		[learning rate: 0.0010014]
	Learning Rate: 0.00100139
	LOSS [training: 1.881120699040131 | validation: 1.861335168833587]
	TIME [epoch: 8.86 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6107258173000876		[learning rate: 0.00099958]
		[batch 20/20] avg loss: 1.8440784024103216		[learning rate: 0.00099776]
	Learning Rate: 0.00099776
	LOSS [training: 1.7274021098552041 | validation: 1.6073789959329776]
	TIME [epoch: 8.85 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8158540398939693		[learning rate: 0.00099595]
		[batch 20/20] avg loss: 1.5622756315296156		[learning rate: 0.00099414]
	Learning Rate: 0.00099414
	LOSS [training: 1.6890648357117926 | validation: 1.4901091867993768]
	TIME [epoch: 8.84 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8173049032994169		[learning rate: 0.00099233]
		[batch 20/20] avg loss: 1.5958728405409484		[learning rate: 0.00099053]
	Learning Rate: 0.000990532
	LOSS [training: 1.7065888719201827 | validation: 1.5154739659438015]
	TIME [epoch: 8.84 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7068989664570182		[learning rate: 0.00098873]
		[batch 20/20] avg loss: 1.6063580210928468		[learning rate: 0.00098694]
	Learning Rate: 0.000986937
	LOSS [training: 1.6566284937749327 | validation: 1.3752698561403864]
	TIME [epoch: 8.84 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5065860865070495		[learning rate: 0.00098514]
		[batch 20/20] avg loss: 1.7909766007549834		[learning rate: 0.00098336]
	Learning Rate: 0.000983355
	LOSS [training: 1.648781343631017 | validation: 1.5397864666679097]
	TIME [epoch: 8.86 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7821660841549527		[learning rate: 0.00098157]
		[batch 20/20] avg loss: 1.517807302325151		[learning rate: 0.00097979]
	Learning Rate: 0.000979787
	LOSS [training: 1.6499866932400518 | validation: 1.3859710213712426]
	TIME [epoch: 8.84 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6311160952442858		[learning rate: 0.00097801]
		[batch 20/20] avg loss: 1.6313775897079634		[learning rate: 0.00097623]
	Learning Rate: 0.000976231
	LOSS [training: 1.6312468424761242 | validation: 1.3154825218490769]
	TIME [epoch: 8.84 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6535505173748621		[learning rate: 0.00097446]
		[batch 20/20] avg loss: 1.5846762289574279		[learning rate: 0.00097269]
	Learning Rate: 0.000972688
	LOSS [training: 1.619113373166145 | validation: 1.3688822662849534]
	TIME [epoch: 8.82 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.654745201332349		[learning rate: 0.00097092]
		[batch 20/20] avg loss: 1.7025074877420991		[learning rate: 0.00096916]
	Learning Rate: 0.000969158
	LOSS [training: 1.6786263445372243 | validation: 1.4346027875865113]
	TIME [epoch: 8.84 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5965711580056225		[learning rate: 0.0009674]
		[batch 20/20] avg loss: 1.5625352822869762		[learning rate: 0.00096564]
	Learning Rate: 0.000965641
	LOSS [training: 1.5795532201462992 | validation: 1.3379911065340955]
	TIME [epoch: 8.86 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6254042562547284		[learning rate: 0.00096389]
		[batch 20/20] avg loss: 1.589030627009339		[learning rate: 0.00096214]
	Learning Rate: 0.000962137
	LOSS [training: 1.607217441632034 | validation: 1.2697937104009227]
	TIME [epoch: 8.84 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6734717686535334		[learning rate: 0.00096039]
		[batch 20/20] avg loss: 1.5218493031943754		[learning rate: 0.00095865]
	Learning Rate: 0.000958645
	LOSS [training: 1.5976605359239544 | validation: 1.406467479679005]
	TIME [epoch: 8.84 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5936059144408277		[learning rate: 0.0009569]
		[batch 20/20] avg loss: 1.7644003801132229		[learning rate: 0.00095517]
	Learning Rate: 0.000955166
	LOSS [training: 1.6790031472770253 | validation: 1.4271226061282472]
	TIME [epoch: 8.84 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.582299891555781		[learning rate: 0.00095343]
		[batch 20/20] avg loss: 1.5789672740768066		[learning rate: 0.0009517]
	Learning Rate: 0.0009517
	LOSS [training: 1.5806335828162938 | validation: 1.2792359042361083]
	TIME [epoch: 8.86 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.55390816740221		[learning rate: 0.00094997]
		[batch 20/20] avg loss: 1.5752580531461675		[learning rate: 0.00094825]
	Learning Rate: 0.000948246
	LOSS [training: 1.5645831102741885 | validation: 1.2293177494404413]
	TIME [epoch: 8.84 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6290355732052813		[learning rate: 0.00094652]
		[batch 20/20] avg loss: 1.438916071860012		[learning rate: 0.0009448]
	Learning Rate: 0.000944805
	LOSS [training: 1.5339758225326467 | validation: 1.2258974553827573]
	TIME [epoch: 8.83 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5000394500718452		[learning rate: 0.00094309]
		[batch 20/20] avg loss: 1.5988065451826476		[learning rate: 0.00094138]
	Learning Rate: 0.000941376
	LOSS [training: 1.5494229976272462 | validation: 1.2903725911417478]
	TIME [epoch: 8.84 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3926120274116656		[learning rate: 0.00093967]
		[batch 20/20] avg loss: 1.647296548592425		[learning rate: 0.00093796]
	Learning Rate: 0.00093796
	LOSS [training: 1.5199542880020454 | validation: 1.1844919747230862]
	TIME [epoch: 8.83 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5014161943477153		[learning rate: 0.00093626]
		[batch 20/20] avg loss: 1.526273538725729		[learning rate: 0.00093456]
	Learning Rate: 0.000934556
	LOSS [training: 1.5138448665367221 | validation: 1.1560790070777327]
	TIME [epoch: 8.86 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.537610221282826		[learning rate: 0.00093286]
		[batch 20/20] avg loss: 1.566801430259723		[learning rate: 0.00093116]
	Learning Rate: 0.000931164
	LOSS [training: 1.5522058257712745 | validation: 1.1557537484141491]
	TIME [epoch: 8.83 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.693934748253557		[learning rate: 0.00092947]
		[batch 20/20] avg loss: 1.4099747287040796		[learning rate: 0.00092779]
	Learning Rate: 0.000927785
	LOSS [training: 1.5519547384788186 | validation: 1.4609067095536739]
	TIME [epoch: 8.84 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4539705981000302		[learning rate: 0.0009261]
		[batch 20/20] avg loss: 1.5782704645318903		[learning rate: 0.00092442]
	Learning Rate: 0.000924418
	LOSS [training: 1.5161205313159598 | validation: 1.254961502746436]
	TIME [epoch: 8.84 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.607223538524884		[learning rate: 0.00092274]
		[batch 20/20] avg loss: 1.4603763150214655		[learning rate: 0.00092106]
	Learning Rate: 0.000921063
	LOSS [training: 1.5337999267731748 | validation: 1.187906479737978]
	TIME [epoch: 8.83 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5723414745598048		[learning rate: 0.00091939]
		[batch 20/20] avg loss: 1.490730394783154		[learning rate: 0.00091772]
	Learning Rate: 0.000917721
	LOSS [training: 1.5315359346714796 | validation: 1.2362218248336165]
	TIME [epoch: 8.85 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.464677709491159		[learning rate: 0.00091605]
		[batch 20/20] avg loss: 1.4876504658537164		[learning rate: 0.00091439]
	Learning Rate: 0.00091439
	LOSS [training: 1.4761640876724376 | validation: 1.3391070063431267]
	TIME [epoch: 8.84 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7102955808053526		[learning rate: 0.00091273]
		[batch 20/20] avg loss: 1.2330003506832017		[learning rate: 0.00091107]
	Learning Rate: 0.000911072
	LOSS [training: 1.471647965744277 | validation: 1.1399421950756652]
	TIME [epoch: 8.84 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4865543038591436		[learning rate: 0.00090942]
		[batch 20/20] avg loss: 1.403531101032867		[learning rate: 0.00090777]
	Learning Rate: 0.000907766
	LOSS [training: 1.4450427024460053 | validation: 1.2977064690842748]
	TIME [epoch: 8.84 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.368856163559317		[learning rate: 0.00090612]
		[batch 20/20] avg loss: 1.5235549703516256		[learning rate: 0.00090447]
	Learning Rate: 0.000904471
	LOSS [training: 1.4462055669554714 | validation: 1.679540120708442]
	TIME [epoch: 8.85 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5201075768367875		[learning rate: 0.00090283]
		[batch 20/20] avg loss: 1.4099145529800394		[learning rate: 0.00090119]
	Learning Rate: 0.000901189
	LOSS [training: 1.4650110649084134 | validation: 1.145669709078052]
	TIME [epoch: 8.85 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.475070943481985		[learning rate: 0.00089955]
		[batch 20/20] avg loss: 1.3529063013092986		[learning rate: 0.00089792]
	Learning Rate: 0.000897918
	LOSS [training: 1.413988622395642 | validation: 1.1049705336059805]
	TIME [epoch: 8.82 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.405245834047271		[learning rate: 0.00089629]
		[batch 20/20] avg loss: 1.3956777849137398		[learning rate: 0.00089466]
	Learning Rate: 0.00089466
	LOSS [training: 1.4004618094805057 | validation: 1.061141469863138]
	TIME [epoch: 8.83 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3334946955591978		[learning rate: 0.00089303]
		[batch 20/20] avg loss: 1.4579467064837497		[learning rate: 0.00089141]
	Learning Rate: 0.000891413
	LOSS [training: 1.3957207010214734 | validation: 1.1091816570705233]
	TIME [epoch: 8.83 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3425487853341835		[learning rate: 0.00088979]
		[batch 20/20] avg loss: 1.5007433956606884		[learning rate: 0.00088818]
	Learning Rate: 0.000888178
	LOSS [training: 1.4216460904974357 | validation: 1.0222463728817281]
	TIME [epoch: 8.85 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3961492864924065		[learning rate: 0.00088656]
		[batch 20/20] avg loss: 1.356106665692532		[learning rate: 0.00088495]
	Learning Rate: 0.000884955
	LOSS [training: 1.3761279760924694 | validation: 1.1280964010831722]
	TIME [epoch: 8.84 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6159809790367388		[learning rate: 0.00088335]
		[batch 20/20] avg loss: 1.3113515845405204		[learning rate: 0.00088174]
	Learning Rate: 0.000881743
	LOSS [training: 1.4636662817886295 | validation: 1.0283570736491152]
	TIME [epoch: 8.83 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.413478609432581		[learning rate: 0.00088014]
		[batch 20/20] avg loss: 1.3434162932843456		[learning rate: 0.00087854]
	Learning Rate: 0.000878543
	LOSS [training: 1.3784474513584632 | validation: 1.1860273585820384]
	TIME [epoch: 8.84 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3349621796645192		[learning rate: 0.00087695]
		[batch 20/20] avg loss: 1.4810709430433577		[learning rate: 0.00087536]
	Learning Rate: 0.000875355
	LOSS [training: 1.4080165613539388 | validation: 1.2667749601103553]
	TIME [epoch: 8.84 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.166393414813274		[learning rate: 0.00087377]
		[batch 20/20] avg loss: 1.521665482569016		[learning rate: 0.00087218]
	Learning Rate: 0.000872178
	LOSS [training: 1.344029448691145 | validation: 1.0895318995853132]
	TIME [epoch: 8.86 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.423556625629905		[learning rate: 0.00087059]
		[batch 20/20] avg loss: 1.2733298209470503		[learning rate: 0.00086901]
	Learning Rate: 0.000869013
	LOSS [training: 1.3484432232884775 | validation: 1.0137128765383538]
	TIME [epoch: 8.84 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3416523812214138		[learning rate: 0.00086743]
		[batch 20/20] avg loss: 1.2821816740042071		[learning rate: 0.00086586]
	Learning Rate: 0.000865859
	LOSS [training: 1.3119170276128105 | validation: 1.018302715978903]
	TIME [epoch: 8.83 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2226345659782938		[learning rate: 0.00086429]
		[batch 20/20] avg loss: 1.3337119271041975		[learning rate: 0.00086272]
	Learning Rate: 0.000862717
	LOSS [training: 1.2781732465412459 | validation: 1.1480427328234162]
	TIME [epoch: 8.84 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.255483469009874		[learning rate: 0.00086115]
		[batch 20/20] avg loss: 1.3053234959190296		[learning rate: 0.00085959]
	Learning Rate: 0.000859586
	LOSS [training: 1.2804034824644521 | validation: 1.045086032509935]
	TIME [epoch: 8.83 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2332628089761832		[learning rate: 0.00085803]
		[batch 20/20] avg loss: 1.3044421441049008		[learning rate: 0.00085647]
	Learning Rate: 0.000856467
	LOSS [training: 1.2688524765405418 | validation: 0.989266580081183]
	TIME [epoch: 8.85 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3851345393119048		[learning rate: 0.00085491]
		[batch 20/20] avg loss: 1.1787160558920244		[learning rate: 0.00085336]
	Learning Rate: 0.000853359
	LOSS [training: 1.2819252976019648 | validation: 1.2065283920648442]
	TIME [epoch: 8.83 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.187285298357176		[learning rate: 0.00085181]
		[batch 20/20] avg loss: 1.2806903627923483		[learning rate: 0.00085026]
	Learning Rate: 0.000850262
	LOSS [training: 1.233987830574762 | validation: 0.9349179972108395]
	TIME [epoch: 8.82 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1770900448625192		[learning rate: 0.00084872]
		[batch 20/20] avg loss: 1.2053590130724345		[learning rate: 0.00084718]
	Learning Rate: 0.000847176
	LOSS [training: 1.1912245289674768 | validation: 1.1012434281939711]
	TIME [epoch: 8.85 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.124811906723888		[learning rate: 0.00084564]
		[batch 20/20] avg loss: 1.1731389495674815		[learning rate: 0.0008441]
	Learning Rate: 0.000844102
	LOSS [training: 1.1489754281456848 | validation: 0.8292180169104658]
	TIME [epoch: 8.85 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0093444777338827		[learning rate: 0.00084257]
		[batch 20/20] avg loss: 1.194479019714921		[learning rate: 0.00084104]
	Learning Rate: 0.000841038
	LOSS [training: 1.101911748724402 | validation: 1.018729468227834]
	TIME [epoch: 8.84 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9758120127097161		[learning rate: 0.00083951]
		[batch 20/20] avg loss: 0.9256158425629513		[learning rate: 0.00083799]
	Learning Rate: 0.000837986
	LOSS [training: 0.9507139276363337 | validation: 0.6104343788826657]
	TIME [epoch: 8.83 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8078298611315583		[learning rate: 0.00083646]
		[batch 20/20] avg loss: 0.7170391275863723		[learning rate: 0.00083495]
	Learning Rate: 0.000834945
	LOSS [training: 0.7624344943589654 | validation: 0.6373936281595869]
	TIME [epoch: 8.83 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.661220533024115		[learning rate: 0.00083343]
		[batch 20/20] avg loss: 0.7237445669536713		[learning rate: 0.00083192]
	Learning Rate: 0.000831915
	LOSS [training: 0.6924825499888934 | validation: 0.9845506786982198]
	TIME [epoch: 8.84 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7914039241637989		[learning rate: 0.0008304]
		[batch 20/20] avg loss: 0.6886057463965258		[learning rate: 0.0008289]
	Learning Rate: 0.000828896
	LOSS [training: 0.7400048352801624 | validation: 0.695909044996927]
	TIME [epoch: 8.86 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7797849406477494		[learning rate: 0.00082739]
		[batch 20/20] avg loss: 0.6766385164247991		[learning rate: 0.00082589]
	Learning Rate: 0.000825888
	LOSS [training: 0.7282117285362744 | validation: 0.7447372129495471]
	TIME [epoch: 8.84 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7668863953777951		[learning rate: 0.00082439]
		[batch 20/20] avg loss: 0.7424104061248865		[learning rate: 0.00082289]
	Learning Rate: 0.000822891
	LOSS [training: 0.7546484007513408 | validation: 0.6895417662681473]
	TIME [epoch: 8.83 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7053545788230466		[learning rate: 0.0008214]
		[batch 20/20] avg loss: 0.7188535863841554		[learning rate: 0.0008199]
	Learning Rate: 0.000819904
	LOSS [training: 0.712104082603601 | validation: 0.5223415734511725]
	TIME [epoch: 8.83 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.69286534797302		[learning rate: 0.00081842]
		[batch 20/20] avg loss: 0.8345169123338158		[learning rate: 0.00081693]
	Learning Rate: 0.000816929
	LOSS [training: 0.7636911301534177 | validation: 0.5011764674630302]
	TIME [epoch: 8.84 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7040109922797348		[learning rate: 0.00081545]
		[batch 20/20] avg loss: 0.7792119649697135		[learning rate: 0.00081396]
	Learning Rate: 0.000813964
	LOSS [training: 0.7416114786247242 | validation: 0.5294857406251814]
	TIME [epoch: 8.84 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6864250190734584		[learning rate: 0.00081249]
		[batch 20/20] avg loss: 0.7465505926270766		[learning rate: 0.00081101]
	Learning Rate: 0.00081101
	LOSS [training: 0.7164878058502675 | validation: 0.5113945725170577]
	TIME [epoch: 8.85 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6278115464366117		[learning rate: 0.00080954]
		[batch 20/20] avg loss: 0.7779668086188594		[learning rate: 0.00080807]
	Learning Rate: 0.000808067
	LOSS [training: 0.7028891775277356 | validation: 0.7444403917954342]
	TIME [epoch: 8.83 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8325654866667778		[learning rate: 0.0008066]
		[batch 20/20] avg loss: 0.8291200717320532		[learning rate: 0.00080513]
	Learning Rate: 0.000805135
	LOSS [training: 0.8308427791994155 | validation: 0.5331202779981309]
	TIME [epoch: 8.84 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7617035698798647		[learning rate: 0.00080367]
		[batch 20/20] avg loss: 0.646960977417253		[learning rate: 0.00080221]
	Learning Rate: 0.000802213
	LOSS [training: 0.7043322736485588 | validation: 0.5547822841310222]
	TIME [epoch: 8.86 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.794068133531379		[learning rate: 0.00080076]
		[batch 20/20] avg loss: 0.6956399584778643		[learning rate: 0.0007993]
	Learning Rate: 0.000799301
	LOSS [training: 0.7448540460046216 | validation: 0.4759115575306861]
	TIME [epoch: 8.83 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7118301402267522		[learning rate: 0.00079785]
		[batch 20/20] avg loss: 0.6707885659530775		[learning rate: 0.0007964]
	Learning Rate: 0.000796401
	LOSS [training: 0.6913093530899148 | validation: 0.44775639525324573]
	TIME [epoch: 8.84 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8365418553957105		[learning rate: 0.00079495]
		[batch 20/20] avg loss: 1.1276406914962036		[learning rate: 0.00079351]
	Learning Rate: 0.000793511
	LOSS [training: 0.9820912734459573 | validation: 0.8148749822457414]
	TIME [epoch: 8.83 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8846519803721706		[learning rate: 0.00079207]
		[batch 20/20] avg loss: 0.7236395757957097		[learning rate: 0.00079063]
	Learning Rate: 0.000790631
	LOSS [training: 0.8041457780839403 | validation: 0.436764536735261]
	TIME [epoch: 8.84 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6681936356504423		[learning rate: 0.00078919]
		[batch 20/20] avg loss: 0.6392397908101948		[learning rate: 0.00078776]
	Learning Rate: 0.000787761
	LOSS [training: 0.6537167132303187 | validation: 0.5846504345599784]
	TIME [epoch: 8.87 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.694004739429816		[learning rate: 0.00078633]
		[batch 20/20] avg loss: 0.6793783163464692		[learning rate: 0.0007849]
	Learning Rate: 0.000784903
	LOSS [training: 0.6866915278881425 | validation: 0.6305053979764533]
	TIME [epoch: 8.84 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6415842157378184		[learning rate: 0.00078348]
		[batch 20/20] avg loss: 0.6874793447309723		[learning rate: 0.00078205]
	Learning Rate: 0.000782054
	LOSS [training: 0.6645317802343953 | validation: 0.5209359892443602]
	TIME [epoch: 8.84 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8138256403901224		[learning rate: 0.00078063]
		[batch 20/20] avg loss: 0.6105117902784438		[learning rate: 0.00077922]
	Learning Rate: 0.000779216
	LOSS [training: 0.712168715334283 | validation: 0.6041083761581013]
	TIME [epoch: 8.84 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7324895775508965		[learning rate: 0.0007778]
		[batch 20/20] avg loss: 3.1455239533998043		[learning rate: 0.00077639]
	Learning Rate: 0.000776388
	LOSS [training: 1.9390067654753502 | validation: 1.6965173282515746]
	TIME [epoch: 8.83 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4657415067409574		[learning rate: 0.00077498]
		[batch 20/20] avg loss: 1.4936147322950455		[learning rate: 0.00077357]
	Learning Rate: 0.000773571
	LOSS [training: 1.4796781195180015 | validation: 2.593249138534744]
	TIME [epoch: 8.86 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9769040273226004		[learning rate: 0.00077217]
		[batch 20/20] avg loss: 0.8912153119692569		[learning rate: 0.00077076]
	Learning Rate: 0.000770763
	LOSS [training: 1.434059669645929 | validation: 0.48967792996980425]
	TIME [epoch: 8.83 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6280940947357508		[learning rate: 0.00076936]
		[batch 20/20] avg loss: 0.6332992501778667		[learning rate: 0.00076797]
	Learning Rate: 0.000767966
	LOSS [training: 0.6306966724568088 | validation: 0.49739646798372295]
	TIME [epoch: 8.83 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5313629912403827		[learning rate: 0.00076657]
		[batch 20/20] avg loss: 0.6900998655147849		[learning rate: 0.00076518]
	Learning Rate: 0.000765179
	LOSS [training: 0.6107314283775838 | validation: 0.5624111805717174]
	TIME [epoch: 8.83 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.668422534653884		[learning rate: 0.00076379]
		[batch 20/20] avg loss: 0.5913739270267644		[learning rate: 0.0007624]
	Learning Rate: 0.000762402
	LOSS [training: 0.629898230840324 | validation: 0.4740090114547779]
	TIME [epoch: 8.84 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7080229262226978		[learning rate: 0.00076102]
		[batch 20/20] avg loss: 0.5161582499709221		[learning rate: 0.00075964]
	Learning Rate: 0.000759636
	LOSS [training: 0.6120905880968099 | validation: 0.4083946031406572]
	TIME [epoch: 8.86 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5688502987522841		[learning rate: 0.00075826]
		[batch 20/20] avg loss: 0.6544482040602755		[learning rate: 0.00075688]
	Learning Rate: 0.000756879
	LOSS [training: 0.6116492514062798 | validation: 0.394209465016717]
	TIME [epoch: 8.83 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6124698152023467		[learning rate: 0.0007555]
		[batch 20/20] avg loss: 0.639196709085962		[learning rate: 0.00075413]
	Learning Rate: 0.000754132
	LOSS [training: 0.6258332621441542 | validation: 0.5743163976525175]
	TIME [epoch: 8.83 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.639652839278814		[learning rate: 0.00075276]
		[batch 20/20] avg loss: 0.6727251302664137		[learning rate: 0.0007514]
	Learning Rate: 0.000751395
	LOSS [training: 0.6561889847726138 | validation: 0.46293647202696847]
	TIME [epoch: 8.83 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6087102599694718		[learning rate: 0.00075003]
		[batch 20/20] avg loss: 0.560087048416791		[learning rate: 0.00074867]
	Learning Rate: 0.000748668
	LOSS [training: 0.5843986541931315 | validation: 0.38283778770872035]
	TIME [epoch: 8.87 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5181889155395024		[learning rate: 0.00074731]
		[batch 20/20] avg loss: 0.6454688934273104		[learning rate: 0.00074595]
	Learning Rate: 0.000745951
	LOSS [training: 0.5818289044834064 | validation: 0.4525182740579818]
	TIME [epoch: 8.84 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5295238523596695		[learning rate: 0.0007446]
		[batch 20/20] avg loss: 0.5966167676656258		[learning rate: 0.00074324]
	Learning Rate: 0.000743244
	LOSS [training: 0.5630703100126477 | validation: 0.42226491492022133]
	TIME [epoch: 8.84 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5337424936389261		[learning rate: 0.00074189]
		[batch 20/20] avg loss: 0.6244116221568002		[learning rate: 0.00074055]
	Learning Rate: 0.000740547
	LOSS [training: 0.5790770578978632 | validation: 0.43200013857071223]
	TIME [epoch: 8.85 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5973793788706673		[learning rate: 0.0007392]
		[batch 20/20] avg loss: 0.5580717836774173		[learning rate: 0.00073786]
	Learning Rate: 0.00073786
	LOSS [training: 0.5777255812740423 | validation: 0.357379364771643]
	TIME [epoch: 8.83 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5801732955092841		[learning rate: 0.00073652]
		[batch 20/20] avg loss: 0.5311130830417823		[learning rate: 0.00073518]
	Learning Rate: 0.000735182
	LOSS [training: 0.5556431892755331 | validation: 0.38369637894922975]
	TIME [epoch: 8.86 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5942327657852708		[learning rate: 0.00073385]
		[batch 20/20] avg loss: 0.4988485926782831		[learning rate: 0.00073251]
	Learning Rate: 0.000732514
	LOSS [training: 0.5465406792317767 | validation: 0.4144531716549752]
	TIME [epoch: 8.83 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48725149450064953		[learning rate: 0.00073118]
		[batch 20/20] avg loss: 0.5928426817438537		[learning rate: 0.00072986]
	Learning Rate: 0.000729855
	LOSS [training: 0.5400470881222514 | validation: 0.4176116369341566]
	TIME [epoch: 8.83 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5985265063734794		[learning rate: 0.00072853]
		[batch 20/20] avg loss: 0.5635503773435813		[learning rate: 0.00072721]
	Learning Rate: 0.000727207
	LOSS [training: 0.5810384418585304 | validation: 0.4446268964885951]
	TIME [epoch: 8.83 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5406920792317514		[learning rate: 0.00072589]
		[batch 20/20] avg loss: 0.509677651070654		[learning rate: 0.00072457]
	Learning Rate: 0.000724568
	LOSS [training: 0.5251848651512028 | validation: 0.3974911335303079]
	TIME [epoch: 8.83 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4862093740286674		[learning rate: 0.00072325]
		[batch 20/20] avg loss: 0.5580095617494205		[learning rate: 0.00072194]
	Learning Rate: 0.000721938
	LOSS [training: 0.522109467889044 | validation: 0.45539497591156153]
	TIME [epoch: 8.86 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5833046932358921		[learning rate: 0.00072063]
		[batch 20/20] avg loss: 0.46008553071244834		[learning rate: 0.00071932]
	Learning Rate: 0.000719318
	LOSS [training: 0.5216951119741702 | validation: 0.3872643652843171]
	TIME [epoch: 8.83 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5477772904602135		[learning rate: 0.00071801]
		[batch 20/20] avg loss: 0.4598397463405517		[learning rate: 0.00071671]
	Learning Rate: 0.000716708
	LOSS [training: 0.5038085184003827 | validation: 0.37496772563308406]
	TIME [epoch: 8.84 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48405474655653025		[learning rate: 0.00071541]
		[batch 20/20] avg loss: 0.4967989268311374		[learning rate: 0.00071411]
	Learning Rate: 0.000714107
	LOSS [training: 0.49042683669383386 | validation: 0.34725656353969814]
	TIME [epoch: 8.85 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5178313501719181		[learning rate: 0.00071281]
		[batch 20/20] avg loss: 0.5227868603929198		[learning rate: 0.00071152]
	Learning Rate: 0.000711515
	LOSS [training: 0.5203091052824188 | validation: 0.4577871938722176]
	TIME [epoch: 8.85 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49062198692406234		[learning rate: 0.00071022]
		[batch 20/20] avg loss: 0.51831826375856		[learning rate: 0.00070893]
	Learning Rate: 0.000708933
	LOSS [training: 0.5044701253413112 | validation: 0.31734044635066444]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_828.pth
	Model improved!!!
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5045403351864566		[learning rate: 0.00070765]
		[batch 20/20] avg loss: 0.4718274217109589		[learning rate: 0.00070636]
	Learning Rate: 0.00070636
	LOSS [training: 0.4881838784487078 | validation: 0.3739617023313471]
	TIME [epoch: 8.85 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5223716557347791		[learning rate: 0.00070508]
		[batch 20/20] avg loss: 0.45027098656461007		[learning rate: 0.0007038]
	Learning Rate: 0.000703797
	LOSS [training: 0.4863213211496946 | validation: 0.38607385264694627]
	TIME [epoch: 8.84 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48446130283928107		[learning rate: 0.00070252]
		[batch 20/20] avg loss: 0.4675149470533759		[learning rate: 0.00070124]
	Learning Rate: 0.000701243
	LOSS [training: 0.47598812494632836 | validation: 0.2825054651185638]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_831.pth
	Model improved!!!
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4939133291407979		[learning rate: 0.00069997]
		[batch 20/20] avg loss: 0.483898781022012		[learning rate: 0.0006987]
	Learning Rate: 0.000698698
	LOSS [training: 0.488906055081405 | validation: 0.3186644342365273]
	TIME [epoch: 8.86 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.441306010758975		[learning rate: 0.00069743]
		[batch 20/20] avg loss: 0.47393353741406524		[learning rate: 0.00069616]
	Learning Rate: 0.000696162
	LOSS [training: 0.4576197740865201 | validation: 0.2930903067667455]
	TIME [epoch: 8.85 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4315206007283886		[learning rate: 0.0006949]
		[batch 20/20] avg loss: 0.46633892673552574		[learning rate: 0.00069364]
	Learning Rate: 0.000693636
	LOSS [training: 0.4489297637319572 | validation: 0.43648705506637797]
	TIME [epoch: 8.84 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4280410431693265		[learning rate: 0.00069238]
		[batch 20/20] avg loss: 0.43781502822857854		[learning rate: 0.00069112]
	Learning Rate: 0.000691119
	LOSS [training: 0.4329280356989525 | validation: 0.3000892428748412]
	TIME [epoch: 8.84 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4412854905013015		[learning rate: 0.00068986]
		[batch 20/20] avg loss: 0.4281805077609143		[learning rate: 0.00068861]
	Learning Rate: 0.000688611
	LOSS [training: 0.43473299913110797 | validation: 0.36891519467593314]
	TIME [epoch: 8.84 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4136756609170976		[learning rate: 0.00068736]
		[batch 20/20] avg loss: 0.4197371225690322		[learning rate: 0.00068611]
	Learning Rate: 0.000686112
	LOSS [training: 0.41670639174306484 | validation: 0.28350221021275357]
	TIME [epoch: 8.86 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5740397171337241		[learning rate: 0.00068487]
		[batch 20/20] avg loss: 0.4169628758927608		[learning rate: 0.00068362]
	Learning Rate: 0.000683622
	LOSS [training: 0.4955012965132425 | validation: 0.333840902694579]
	TIME [epoch: 8.84 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46106854138147835		[learning rate: 0.00068238]
		[batch 20/20] avg loss: 0.4028948829510089		[learning rate: 0.00068114]
	Learning Rate: 0.000681141
	LOSS [training: 0.4319817121662436 | validation: 0.32035835160967857]
	TIME [epoch: 8.84 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4024872376260841		[learning rate: 0.0006799]
		[batch 20/20] avg loss: 0.4423807388382513		[learning rate: 0.00067867]
	Learning Rate: 0.000678669
	LOSS [training: 0.42243398823216766 | validation: 0.266731793952717]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_840.pth
	Model improved!!!
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42365923326965316		[learning rate: 0.00067744]
		[batch 20/20] avg loss: 0.389979977963233		[learning rate: 0.00067621]
	Learning Rate: 0.000676206
	LOSS [training: 0.4068196056164431 | validation: 0.26943936328238405]
	TIME [epoch: 8.87 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3759720139059079		[learning rate: 0.00067498]
		[batch 20/20] avg loss: 0.4197787290047499		[learning rate: 0.00067375]
	Learning Rate: 0.000673752
	LOSS [training: 0.3978753714553289 | validation: 0.30575178536891606]
	TIME [epoch: 8.85 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40633102904051616		[learning rate: 0.00067253]
		[batch 20/20] avg loss: 0.40955159450114903		[learning rate: 0.00067131]
	Learning Rate: 0.000671307
	LOSS [training: 0.40794131177083265 | validation: 0.5054117531988479]
	TIME [epoch: 8.85 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39402987174243176		[learning rate: 0.00067009]
		[batch 20/20] avg loss: 0.3941994685720617		[learning rate: 0.00066887]
	Learning Rate: 0.000668871
	LOSS [training: 0.3941146701572467 | validation: 0.23191270287655888]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_844.pth
	Model improved!!!
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40351510940623514		[learning rate: 0.00066766]
		[batch 20/20] avg loss: 0.46255826984764603		[learning rate: 0.00066644]
	Learning Rate: 0.000666443
	LOSS [training: 0.4330366896269406 | validation: 0.29886182913456727]
	TIME [epoch: 8.84 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3986693159960965		[learning rate: 0.00066523]
		[batch 20/20] avg loss: 0.3827911360374169		[learning rate: 0.00066402]
	Learning Rate: 0.000664025
	LOSS [training: 0.3907302260167566 | validation: 0.48763398378316475]
	TIME [epoch: 8.86 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3610236771807466		[learning rate: 0.00066282]
		[batch 20/20] avg loss: 0.4455640274418348		[learning rate: 0.00066161]
	Learning Rate: 0.000661615
	LOSS [training: 0.40329385231129067 | validation: 0.27974421168861785]
	TIME [epoch: 8.85 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4584101982352758		[learning rate: 0.00066041]
		[batch 20/20] avg loss: 0.38980902536416534		[learning rate: 0.00065921]
	Learning Rate: 0.000659214
	LOSS [training: 0.42410961179972057 | validation: 0.34695638686071106]
	TIME [epoch: 8.85 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3725411110518951		[learning rate: 0.00065802]
		[batch 20/20] avg loss: 0.37532852748258866		[learning rate: 0.00065682]
	Learning Rate: 0.000656822
	LOSS [training: 0.3739348192672418 | validation: 0.24882626760409704]
	TIME [epoch: 8.84 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3449856035208193		[learning rate: 0.00065563]
		[batch 20/20] avg loss: 0.3548308735196483		[learning rate: 0.00065444]
	Learning Rate: 0.000654438
	LOSS [training: 0.3499082385202339 | validation: 0.4039158607452677]
	TIME [epoch: 8.84 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4279931750139756		[learning rate: 0.00065325]
		[batch 20/20] avg loss: 0.3635946182964973		[learning rate: 0.00065206]
	Learning Rate: 0.000652063
	LOSS [training: 0.3957938966552364 | validation: 0.3541258393673873]
	TIME [epoch: 8.86 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33730215325665003		[learning rate: 0.00065088]
		[batch 20/20] avg loss: 0.34107476156269356		[learning rate: 0.0006497]
	Learning Rate: 0.000649697
	LOSS [training: 0.3391884574096718 | validation: 0.22536040630296592]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_852.pth
	Model improved!!!
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3839100631624358		[learning rate: 0.00064852]
		[batch 20/20] avg loss: 0.40683669768524816		[learning rate: 0.00064734]
	Learning Rate: 0.000647339
	LOSS [training: 0.39537338042384207 | validation: 0.27459556838161514]
	TIME [epoch: 8.86 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3287318948175303		[learning rate: 0.00064616]
		[batch 20/20] avg loss: 0.33725860972116756		[learning rate: 0.00064499]
	Learning Rate: 0.00064499
	LOSS [training: 0.3329952522693489 | validation: 0.23034010832745963]
	TIME [epoch: 8.85 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37037342125332173		[learning rate: 0.00064382]
		[batch 20/20] avg loss: 0.3837732764134318		[learning rate: 0.00064265]
	Learning Rate: 0.000642649
	LOSS [training: 0.37707334883337673 | validation: 0.25077573988611074]
	TIME [epoch: 8.86 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36728361442734947		[learning rate: 0.00064148]
		[batch 20/20] avg loss: 0.30647744673340255		[learning rate: 0.00064032]
	Learning Rate: 0.000640317
	LOSS [training: 0.33688053058037604 | validation: 0.30312747492541264]
	TIME [epoch: 8.85 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3215506070658257		[learning rate: 0.00063915]
		[batch 20/20] avg loss: 0.33181080060751267		[learning rate: 0.00063799]
	Learning Rate: 0.000637993
	LOSS [training: 0.3266807038366692 | validation: 0.21077060245983756]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37280422284481307		[learning rate: 0.00063683]
		[batch 20/20] avg loss: 0.3728192160649122		[learning rate: 0.00063568]
	Learning Rate: 0.000635677
	LOSS [training: 0.3728117194548627 | validation: 0.24904554225217723]
	TIME [epoch: 8.85 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3463078778598582		[learning rate: 0.00063452]
		[batch 20/20] avg loss: 0.3343109249121152		[learning rate: 0.00063337]
	Learning Rate: 0.000633371
	LOSS [training: 0.3403094013859867 | validation: 0.2188918978803243]
	TIME [epoch: 8.85 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3014365605988768		[learning rate: 0.00063222]
		[batch 20/20] avg loss: 0.3705771470827503		[learning rate: 0.00063107]
	Learning Rate: 0.000631072
	LOSS [training: 0.3360068538408135 | validation: 0.2292703049261089]
	TIME [epoch: 8.87 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3047022607730187		[learning rate: 0.00062993]
		[batch 20/20] avg loss: 0.35770691654533504		[learning rate: 0.00062878]
	Learning Rate: 0.000628782
	LOSS [training: 0.33120458865917685 | validation: 0.209287184564674]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_861.pth
	Model improved!!!
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3540276252752139		[learning rate: 0.00062764]
		[batch 20/20] avg loss: 0.32044791475730344		[learning rate: 0.0006265]
	Learning Rate: 0.0006265
	LOSS [training: 0.33723777001625865 | validation: 0.2413731872875837]
	TIME [epoch: 8.85 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32108094397976605		[learning rate: 0.00062536]
		[batch 20/20] avg loss: 0.3111168814763627		[learning rate: 0.00062423]
	Learning Rate: 0.000624226
	LOSS [training: 0.31609891272806434 | validation: 0.29562552980079254]
	TIME [epoch: 8.84 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3209814698496325		[learning rate: 0.00062309]
		[batch 20/20] avg loss: 0.34109958523318773		[learning rate: 0.00062196]
	Learning Rate: 0.000621961
	LOSS [training: 0.33104052754141 | validation: 0.25211018646153166]
	TIME [epoch: 8.85 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30555067849891077		[learning rate: 0.00062083]
		[batch 20/20] avg loss: 0.31310594670717656		[learning rate: 0.0006197]
	Learning Rate: 0.000619704
	LOSS [training: 0.3093283126030437 | validation: 0.21317387562209683]
	TIME [epoch: 8.88 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32090907906191185		[learning rate: 0.00061858]
		[batch 20/20] avg loss: 0.31088873681868834		[learning rate: 0.00061745]
	Learning Rate: 0.000617455
	LOSS [training: 0.3158989079403002 | validation: 0.24403730188266026]
	TIME [epoch: 8.85 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2685634256856082		[learning rate: 0.00061633]
		[batch 20/20] avg loss: 0.3299098141109599		[learning rate: 0.00061521]
	Learning Rate: 0.000615214
	LOSS [training: 0.2992366198982841 | validation: 0.2333139440009417]
	TIME [epoch: 8.85 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28090102792829963		[learning rate: 0.0006141]
		[batch 20/20] avg loss: 0.3249145474848886		[learning rate: 0.00061298]
	Learning Rate: 0.000612982
	LOSS [training: 0.3029077877065941 | validation: 0.3016912128283929]
	TIME [epoch: 8.85 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3712422772655381		[learning rate: 0.00061187]
		[batch 20/20] avg loss: 0.28686958678912794		[learning rate: 0.00061076]
	Learning Rate: 0.000610757
	LOSS [training: 0.3290559320273331 | validation: 0.3087025135495178]
	TIME [epoch: 8.86 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2620301704842477		[learning rate: 0.00060965]
		[batch 20/20] avg loss: 0.43192419767195		[learning rate: 0.00060854]
	Learning Rate: 0.00060854
	LOSS [training: 0.34697718407809885 | validation: 0.2905342282270784]
	TIME [epoch: 8.85 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4322555428904303		[learning rate: 0.00060744]
		[batch 20/20] avg loss: 0.2904882312201472		[learning rate: 0.00060633]
	Learning Rate: 0.000606332
	LOSS [training: 0.3613718870552887 | validation: 0.4028174815595578]
	TIME [epoch: 8.85 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31174697942262586		[learning rate: 0.00060523]
		[batch 20/20] avg loss: 0.28277227572764013		[learning rate: 0.00060413]
	Learning Rate: 0.000604132
	LOSS [training: 0.29725962757513297 | validation: 0.21382831884888912]
	TIME [epoch: 8.85 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31982136593377064		[learning rate: 0.00060303]
		[batch 20/20] avg loss: 0.3471207286110606		[learning rate: 0.00060194]
	Learning Rate: 0.000601939
	LOSS [training: 0.3334710472724157 | validation: 0.27342713349139763]
	TIME [epoch: 8.85 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3135864990000996		[learning rate: 0.00060085]
		[batch 20/20] avg loss: 0.29759048463026533		[learning rate: 0.00059975]
	Learning Rate: 0.000599755
	LOSS [training: 0.30558849181518244 | validation: 0.26442033899231854]
	TIME [epoch: 8.86 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30631832550929455		[learning rate: 0.00059867]
		[batch 20/20] avg loss: 0.2825298027723455		[learning rate: 0.00059758]
	Learning Rate: 0.000597578
	LOSS [training: 0.2944240641408201 | validation: 0.20318404790754518]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_875.pth
	Model improved!!!
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3057535967211536		[learning rate: 0.00059649]
		[batch 20/20] avg loss: 0.28780126236001596		[learning rate: 0.00059541]
	Learning Rate: 0.00059541
	LOSS [training: 0.29677742954058467 | validation: 0.2562919446439723]
	TIME [epoch: 8.84 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32941104678112476		[learning rate: 0.00059433]
		[batch 20/20] avg loss: 0.27249241594591805		[learning rate: 0.00059325]
	Learning Rate: 0.000593249
	LOSS [training: 0.30095173136352144 | validation: 0.24760379039934097]
	TIME [epoch: 8.84 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32739668334747696		[learning rate: 0.00059217]
		[batch 20/20] avg loss: 0.3498678507591365		[learning rate: 0.0005911]
	Learning Rate: 0.000591096
	LOSS [training: 0.33863226705330673 | validation: 0.2655976674134369]
	TIME [epoch: 8.85 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32357812121302554		[learning rate: 0.00059002]
		[batch 20/20] avg loss: 0.34079720725551355		[learning rate: 0.00058895]
	Learning Rate: 0.000588951
	LOSS [training: 0.33218766423426954 | validation: 0.214189950739189]
	TIME [epoch: 8.87 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3449804444987743		[learning rate: 0.00058788]
		[batch 20/20] avg loss: 0.2882131406061776		[learning rate: 0.00058681]
	Learning Rate: 0.000586813
	LOSS [training: 0.31659679255247597 | validation: 0.4170956714435028]
	TIME [epoch: 8.85 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3657152207066403		[learning rate: 0.00058575]
		[batch 20/20] avg loss: 0.254291221585426		[learning rate: 0.00058468]
	Learning Rate: 0.000584684
	LOSS [training: 0.3100032211460332 | validation: 0.23775591069461194]
	TIME [epoch: 8.84 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2963454010688569		[learning rate: 0.00058362]
		[batch 20/20] avg loss: 0.2927676758988614		[learning rate: 0.00058256]
	Learning Rate: 0.000582562
	LOSS [training: 0.2945565384838591 | validation: 0.3167186879668013]
	TIME [epoch: 8.85 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2943475053808847		[learning rate: 0.0005815]
		[batch 20/20] avg loss: 0.2758279384363568		[learning rate: 0.00058045]
	Learning Rate: 0.000580448
	LOSS [training: 0.28508772190862064 | validation: 0.25103634320219675]
	TIME [epoch: 8.86 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27682369326893114		[learning rate: 0.00057939]
		[batch 20/20] avg loss: 0.3014485899736471		[learning rate: 0.00057834]
	Learning Rate: 0.000578341
	LOSS [training: 0.2891361416212891 | validation: 0.24293948039736135]
	TIME [epoch: 8.84 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29376152338071704		[learning rate: 0.00057729]
		[batch 20/20] avg loss: 0.2658817581821301		[learning rate: 0.00057624]
	Learning Rate: 0.000576243
	LOSS [training: 0.2798216407814236 | validation: 0.2063633280247873]
	TIME [epoch: 8.84 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3129519333187218		[learning rate: 0.0005752]
		[batch 20/20] avg loss: 0.274239326473016		[learning rate: 0.00057415]
	Learning Rate: 0.000574151
	LOSS [training: 0.2935956298958689 | validation: 0.2625610779756894]
	TIME [epoch: 8.84 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28972196044205645		[learning rate: 0.00057311]
		[batch 20/20] avg loss: 0.2702940254608309		[learning rate: 0.00057207]
	Learning Rate: 0.000572068
	LOSS [training: 0.2800079929514437 | validation: 0.2168894061507752]
	TIME [epoch: 8.84 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.353225107204908		[learning rate: 0.00057103]
		[batch 20/20] avg loss: 0.24999868277803294		[learning rate: 0.00056999]
	Learning Rate: 0.000569992
	LOSS [training: 0.30161189499147045 | validation: 0.21798063145996568]
	TIME [epoch: 8.86 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2784363157625399		[learning rate: 0.00056896]
		[batch 20/20] avg loss: 0.28925812753044217		[learning rate: 0.00056792]
	Learning Rate: 0.000567923
	LOSS [training: 0.283847221646491 | validation: 0.22719137553869018]
	TIME [epoch: 8.84 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38081157101887275		[learning rate: 0.00056689]
		[batch 20/20] avg loss: 0.2995204323999295		[learning rate: 0.00056586]
	Learning Rate: 0.000565862
	LOSS [training: 0.34016600170940114 | validation: 0.23339477438753425]
	TIME [epoch: 8.84 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30415749539516707		[learning rate: 0.00056483]
		[batch 20/20] avg loss: 0.30040078449382124		[learning rate: 0.00056381]
	Learning Rate: 0.000563808
	LOSS [training: 0.3022791399444942 | validation: 0.21394436754739343]
	TIME [epoch: 8.84 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27105215786884707		[learning rate: 0.00056278]
		[batch 20/20] avg loss: 0.2744885720893409		[learning rate: 0.00056176]
	Learning Rate: 0.000561762
	LOSS [training: 0.2727703649790939 | validation: 0.2321678716649609]
	TIME [epoch: 8.85 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.325633616332111		[learning rate: 0.00056074]
		[batch 20/20] avg loss: 0.32638169221267044		[learning rate: 0.00055972]
	Learning Rate: 0.000559724
	LOSS [training: 0.32600765427239065 | validation: 0.20453235693699734]
	TIME [epoch: 8.86 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30164197556581673		[learning rate: 0.00055871]
		[batch 20/20] avg loss: 0.26058133986409654		[learning rate: 0.00055769]
	Learning Rate: 0.000557692
	LOSS [training: 0.28111165771495666 | validation: 0.31342237793572425]
	TIME [epoch: 8.85 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30939600210954366		[learning rate: 0.00055668]
		[batch 20/20] avg loss: 0.2905177710957001		[learning rate: 0.00055567]
	Learning Rate: 0.000555669
	LOSS [training: 0.2999568866026219 | validation: 0.2046243219670893]
	TIME [epoch: 8.84 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30398080273051964		[learning rate: 0.00055466]
		[batch 20/20] avg loss: 0.27197076201495884		[learning rate: 0.00055365]
	Learning Rate: 0.000553652
	LOSS [training: 0.2879757823727392 | validation: 0.2079252621869578]
	TIME [epoch: 8.84 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2615719424122184		[learning rate: 0.00055265]
		[batch 20/20] avg loss: 0.3013282510320871		[learning rate: 0.00055164]
	Learning Rate: 0.000551643
	LOSS [training: 0.2814500967221528 | validation: 0.23792593263846407]
	TIME [epoch: 8.86 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.271714503297256		[learning rate: 0.00055064]
		[batch 20/20] avg loss: 0.30801413846849063		[learning rate: 0.00054964]
	Learning Rate: 0.000549641
	LOSS [training: 0.2898643208828732 | validation: 0.19665486838986385]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_898.pth
	Model improved!!!
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22848173737915872		[learning rate: 0.00054864]
		[batch 20/20] avg loss: 0.34124727709685654		[learning rate: 0.00054765]
	Learning Rate: 0.000547646
	LOSS [training: 0.2848645072380075 | validation: 0.18671184532752114]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_899.pth
	Model improved!!!
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26043435686676225		[learning rate: 0.00054665]
		[batch 20/20] avg loss: 0.26553253472458527		[learning rate: 0.00054566]
	Learning Rate: 0.000545659
	LOSS [training: 0.2629834457956738 | validation: 0.22279719284684654]
	TIME [epoch: 8.84 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26058100236679843		[learning rate: 0.00054467]
		[batch 20/20] avg loss: 0.2883518991453001		[learning rate: 0.00054368]
	Learning Rate: 0.000543678
	LOSS [training: 0.27446645075604925 | validation: 0.16645427094051407]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_901.pth
	Model improved!!!
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2653052172899996		[learning rate: 0.00054269]
		[batch 20/20] avg loss: 0.2719171986587325		[learning rate: 0.00054171]
	Learning Rate: 0.000541705
	LOSS [training: 0.268611207974366 | validation: 0.2718267263442737]
	TIME [epoch: 8.87 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3507947225137404		[learning rate: 0.00054072]
		[batch 20/20] avg loss: 0.25547837630464876		[learning rate: 0.00053974]
	Learning Rate: 0.00053974
	LOSS [training: 0.30313654940919454 | validation: 0.31911186177216705]
	TIME [epoch: 8.84 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2969748318701003		[learning rate: 0.00053876]
		[batch 20/20] avg loss: 0.2436806345794274		[learning rate: 0.00053778]
	Learning Rate: 0.000537781
	LOSS [training: 0.27032773322476383 | validation: 0.24485710841170624]
	TIME [epoch: 8.85 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2786094236309693		[learning rate: 0.0005368]
		[batch 20/20] avg loss: 0.2659542519926241		[learning rate: 0.00053583]
	Learning Rate: 0.000535829
	LOSS [training: 0.2722818378117967 | validation: 0.2920980783391769]
	TIME [epoch: 8.84 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26670528621274897		[learning rate: 0.00053486]
		[batch 20/20] avg loss: 0.28456110864149353		[learning rate: 0.00053388]
	Learning Rate: 0.000533885
	LOSS [training: 0.2756331974271213 | validation: 0.31414442537133336]
	TIME [epoch: 8.84 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28727567844929525		[learning rate: 0.00053291]
		[batch 20/20] avg loss: 0.25981515228455937		[learning rate: 0.00053195]
	Learning Rate: 0.000531947
	LOSS [training: 0.27354541536692734 | validation: 0.25653551105364036]
	TIME [epoch: 8.87 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2510751197815482		[learning rate: 0.00053098]
		[batch 20/20] avg loss: 0.2822459865507239		[learning rate: 0.00053002]
	Learning Rate: 0.000530017
	LOSS [training: 0.26666055316613607 | validation: 0.2192012171221804]
	TIME [epoch: 8.84 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2643854259692365		[learning rate: 0.00052905]
		[batch 20/20] avg loss: 0.2637294395802246		[learning rate: 0.00052809]
	Learning Rate: 0.000528093
	LOSS [training: 0.26405743277473054 | validation: 0.2922831649353534]
	TIME [epoch: 8.84 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30022922021812093		[learning rate: 0.00052713]
		[batch 20/20] avg loss: 0.25168184048722175		[learning rate: 0.00052618]
	Learning Rate: 0.000526177
	LOSS [training: 0.2759555303526713 | validation: 0.20490753372771398]
	TIME [epoch: 8.84 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.252866751672839		[learning rate: 0.00052522]
		[batch 20/20] avg loss: 0.2625183824304421		[learning rate: 0.00052427]
	Learning Rate: 0.000524267
	LOSS [training: 0.2576925670516405 | validation: 0.2879904390234068]
	TIME [epoch: 8.85 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2811019831696112		[learning rate: 0.00052332]
		[batch 20/20] avg loss: 0.29432400566138706		[learning rate: 0.00052236]
	Learning Rate: 0.000522365
	LOSS [training: 0.28771299441549913 | validation: 0.27486433954636286]
	TIME [epoch: 8.84 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25734530560312674		[learning rate: 0.00052142]
		[batch 20/20] avg loss: 0.2637023503207373		[learning rate: 0.00052047]
	Learning Rate: 0.000520469
	LOSS [training: 0.260523827961932 | validation: 0.226685694040476]
	TIME [epoch: 8.84 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2777062918430141		[learning rate: 0.00051952]
		[batch 20/20] avg loss: 0.2760723309537549		[learning rate: 0.00051858]
	Learning Rate: 0.00051858
	LOSS [training: 0.27688931139838446 | validation: 0.1751764963713865]
	TIME [epoch: 8.84 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23349321115793442		[learning rate: 0.00051764]
		[batch 20/20] avg loss: 0.24377286111682026		[learning rate: 0.0005167]
	Learning Rate: 0.000516698
	LOSS [training: 0.23863303613737732 | validation: 0.2562023480228412]
	TIME [epoch: 8.83 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26874339210696696		[learning rate: 0.00051576]
		[batch 20/20] avg loss: 0.2582977169954626		[learning rate: 0.00051482]
	Learning Rate: 0.000514823
	LOSS [training: 0.26352055455121476 | validation: 0.20111504453383952]
	TIME [epoch: 8.85 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2850948738749521		[learning rate: 0.00051389]
		[batch 20/20] avg loss: 0.274714849765268		[learning rate: 0.00051295]
	Learning Rate: 0.000512955
	LOSS [training: 0.27990486182011 | validation: 0.2500223984038332]
	TIME [epoch: 8.84 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2904467915784556		[learning rate: 0.00051202]
		[batch 20/20] avg loss: 0.27923520750689074		[learning rate: 0.00051109]
	Learning Rate: 0.000511093
	LOSS [training: 0.28484099954267317 | validation: 0.17870644367257732]
	TIME [epoch: 8.83 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2766479821382192		[learning rate: 0.00051016]
		[batch 20/20] avg loss: 0.28634678000347225		[learning rate: 0.00050924]
	Learning Rate: 0.000509238
	LOSS [training: 0.28149738107084576 | validation: 0.1827212374482073]
	TIME [epoch: 8.84 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23619405926513223		[learning rate: 0.00050831]
		[batch 20/20] avg loss: 0.2765786179179592		[learning rate: 0.00050739]
	Learning Rate: 0.00050739
	LOSS [training: 0.2563863385915457 | validation: 0.2065871797557118]
	TIME [epoch: 8.84 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25483642294683284		[learning rate: 0.00050647]
		[batch 20/20] avg loss: 0.2903845240113962		[learning rate: 0.00050555]
	Learning Rate: 0.000505549
	LOSS [training: 0.2726104734791145 | validation: 0.21979045938261396]
	TIME [epoch: 8.86 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2456383389816653		[learning rate: 0.00050463]
		[batch 20/20] avg loss: 0.24798716194420098		[learning rate: 0.00050371]
	Learning Rate: 0.000503714
	LOSS [training: 0.2468127504629331 | validation: 0.24460973474593595]
	TIME [epoch: 8.85 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2728235064068899		[learning rate: 0.0005028]
		[batch 20/20] avg loss: 0.3040032368201046		[learning rate: 0.00050189]
	Learning Rate: 0.000501886
	LOSS [training: 0.2884133716134972 | validation: 0.18893163769521568]
	TIME [epoch: 8.84 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24297463426756125		[learning rate: 0.00050097]
		[batch 20/20] avg loss: 0.27441354628664466		[learning rate: 0.00050006]
	Learning Rate: 0.000500065
	LOSS [training: 0.258694090277103 | validation: 0.30606090904252775]
	TIME [epoch: 8.84 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23132946790098657		[learning rate: 0.00049916]
		[batch 20/20] avg loss: 0.2738205245052381		[learning rate: 0.00049825]
	Learning Rate: 0.00049825
	LOSS [training: 0.25257499620311236 | validation: 0.2187595331336231]
	TIME [epoch: 8.86 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25598873110727893		[learning rate: 0.00049735]
		[batch 20/20] avg loss: 0.2736516250643767		[learning rate: 0.00049644]
	Learning Rate: 0.000496442
	LOSS [training: 0.2648201780858278 | validation: 0.20262957550232258]
	TIME [epoch: 8.83 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29030888224797435		[learning rate: 0.00049554]
		[batch 20/20] avg loss: 0.29931969731516916		[learning rate: 0.00049464]
	Learning Rate: 0.00049464
	LOSS [training: 0.2948142897815717 | validation: 0.3051764623478308]
	TIME [epoch: 8.84 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24419048133505297		[learning rate: 0.00049374]
		[batch 20/20] avg loss: 0.22912713440580718		[learning rate: 0.00049285]
	Learning Rate: 0.000492845
	LOSS [training: 0.2366588078704301 | validation: 0.23772965926131892]
	TIME [epoch: 8.85 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23960166432836977		[learning rate: 0.00049195]
		[batch 20/20] avg loss: 0.27857435702385525		[learning rate: 0.00049106]
	Learning Rate: 0.000491057
	LOSS [training: 0.25908801067611253 | validation: 0.16977520735880836]
	TIME [epoch: 8.83 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24125506471037342		[learning rate: 0.00049016]
		[batch 20/20] avg loss: 0.2600311836901107		[learning rate: 0.00048927]
	Learning Rate: 0.000489275
	LOSS [training: 0.25064312420024204 | validation: 0.21903167308540977]
	TIME [epoch: 8.86 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27612849876842194		[learning rate: 0.00048839]
		[batch 20/20] avg loss: 0.23414356835234917		[learning rate: 0.0004875]
	Learning Rate: 0.000487499
	LOSS [training: 0.2551360335603855 | validation: 0.4071772133821982]
	TIME [epoch: 8.84 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29323958874478506		[learning rate: 0.00048661]
		[batch 20/20] avg loss: 0.24167625799287012		[learning rate: 0.00048573]
	Learning Rate: 0.00048573
	LOSS [training: 0.26745792336882757 | validation: 0.20322337094536522]
	TIME [epoch: 8.85 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24048566529945944		[learning rate: 0.00048485]
		[batch 20/20] avg loss: 0.32268002131746215		[learning rate: 0.00048397]
	Learning Rate: 0.000483967
	LOSS [training: 0.2815828433084608 | validation: 0.21525176361426074]
	TIME [epoch: 8.84 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2932513514120359		[learning rate: 0.00048309]
		[batch 20/20] avg loss: 0.22633840973654426		[learning rate: 0.00048221]
	Learning Rate: 0.000482211
	LOSS [training: 0.2597948805742901 | validation: 0.24629310016393618]
	TIME [epoch: 8.84 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2814533910988877		[learning rate: 0.00048133]
		[batch 20/20] avg loss: 0.26235201360396		[learning rate: 0.00048046]
	Learning Rate: 0.000480461
	LOSS [training: 0.2719027023514239 | validation: 0.1883728542434373]
	TIME [epoch: 8.86 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2302824672995111		[learning rate: 0.00047959]
		[batch 20/20] avg loss: 0.26668343757622004		[learning rate: 0.00047872]
	Learning Rate: 0.000478717
	LOSS [training: 0.24848295243786556 | validation: 0.1702070052119739]
	TIME [epoch: 8.85 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3071208056173693		[learning rate: 0.00047785]
		[batch 20/20] avg loss: 0.25564180386051877		[learning rate: 0.00047698]
	Learning Rate: 0.00047698
	LOSS [training: 0.28138130473894407 | validation: 0.26620638310989697]
	TIME [epoch: 8.84 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2528109030866701		[learning rate: 0.00047611]
		[batch 20/20] avg loss: 0.26390575671522937		[learning rate: 0.00047525]
	Learning Rate: 0.000475249
	LOSS [training: 0.2583583299009497 | validation: 0.2798137983963462]
	TIME [epoch: 8.84 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24856747440684943		[learning rate: 0.00047439]
		[batch 20/20] avg loss: 0.24318136796271045		[learning rate: 0.00047352]
	Learning Rate: 0.000473524
	LOSS [training: 0.24587442118477995 | validation: 0.24161556358180952]
	TIME [epoch: 8.85 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27706298855633643		[learning rate: 0.00047266]
		[batch 20/20] avg loss: 0.3110506054794874		[learning rate: 0.00047181]
	Learning Rate: 0.000471806
	LOSS [training: 0.2940567970179119 | validation: 0.20722988788838864]
	TIME [epoch: 8.85 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26760637375183344		[learning rate: 0.00047095]
		[batch 20/20] avg loss: 0.24327698787040863		[learning rate: 0.00047009]
	Learning Rate: 0.000470093
	LOSS [training: 0.25544168081112106 | validation: 0.21534526116748512]
	TIME [epoch: 8.84 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2264142913560625		[learning rate: 0.00046924]
		[batch 20/20] avg loss: 0.2653990832397516		[learning rate: 0.00046839]
	Learning Rate: 0.000468388
	LOSS [training: 0.24590668729790707 | validation: 0.2832599852830165]
	TIME [epoch: 8.85 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27676350026013996		[learning rate: 0.00046754]
		[batch 20/20] avg loss: 0.2823556896229067		[learning rate: 0.00046669]
	Learning Rate: 0.000466688
	LOSS [training: 0.2795595949415234 | validation: 0.19897750413311815]
	TIME [epoch: 8.84 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25897659819614716		[learning rate: 0.00046584]
		[batch 20/20] avg loss: 0.24587286044208462		[learning rate: 0.00046499]
	Learning Rate: 0.000464994
	LOSS [training: 0.25242472931911586 | validation: 0.18504826639358227]
	TIME [epoch: 8.86 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21197230396501937		[learning rate: 0.00046415]
		[batch 20/20] avg loss: 0.2522623652639723		[learning rate: 0.00046331]
	Learning Rate: 0.000463307
	LOSS [training: 0.2321173346144958 | validation: 0.22053108531459545]
	TIME [epoch: 8.85 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2429462302129582		[learning rate: 0.00046247]
		[batch 20/20] avg loss: 0.22891551822303677		[learning rate: 0.00046163]
	Learning Rate: 0.000461625
	LOSS [training: 0.23593087421799744 | validation: 0.3156650580604706]
	TIME [epoch: 8.84 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.289679465642081		[learning rate: 0.00046079]
		[batch 20/20] avg loss: 0.26530694443670283		[learning rate: 0.00045995]
	Learning Rate: 0.00045995
	LOSS [training: 0.27749320503939195 | validation: 0.18329663627819806]
	TIME [epoch: 8.84 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2397881901432871		[learning rate: 0.00045911]
		[batch 20/20] avg loss: 0.2518060867641675		[learning rate: 0.00045828]
	Learning Rate: 0.000458281
	LOSS [training: 0.24579713845372725 | validation: 0.19082007898116451]
	TIME [epoch: 8.85 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25005705680016604		[learning rate: 0.00045745]
		[batch 20/20] avg loss: 0.2516356767568192		[learning rate: 0.00045662]
	Learning Rate: 0.000456618
	LOSS [training: 0.2508463667784926 | validation: 0.19664390936800583]
	TIME [epoch: 8.87 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2291257227328502		[learning rate: 0.00045579]
		[batch 20/20] avg loss: 0.28683225916183		[learning rate: 0.00045496]
	Learning Rate: 0.000454961
	LOSS [training: 0.2579789909473401 | validation: 0.39638305567226934]
	TIME [epoch: 8.84 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2988045426861012		[learning rate: 0.00045413]
		[batch 20/20] avg loss: 0.2704271414719462		[learning rate: 0.00045331]
	Learning Rate: 0.000453309
	LOSS [training: 0.2846158420790237 | validation: 0.20878761661992878]
	TIME [epoch: 8.85 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25619982464838165		[learning rate: 0.00045249]
		[batch 20/20] avg loss: 0.25815742486803506		[learning rate: 0.00045166]
	Learning Rate: 0.000451664
	LOSS [training: 0.2571786247582083 | validation: 0.23402248741127857]
	TIME [epoch: 8.83 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21258034698191625		[learning rate: 0.00045084]
		[batch 20/20] avg loss: 0.2834559679314191		[learning rate: 0.00045003]
	Learning Rate: 0.000450025
	LOSS [training: 0.24801815745666764 | validation: 0.2226568208119025]
	TIME [epoch: 8.84 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27142862189561245		[learning rate: 0.00044921]
		[batch 20/20] avg loss: 0.23666812906622656		[learning rate: 0.00044839]
	Learning Rate: 0.000448392
	LOSS [training: 0.2540483754809195 | validation: 0.2512592832009176]
	TIME [epoch: 8.86 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24308137478914632		[learning rate: 0.00044758]
		[batch 20/20] avg loss: 0.26747087769229644		[learning rate: 0.00044676]
	Learning Rate: 0.000446765
	LOSS [training: 0.2552761262407214 | validation: 0.19000769573495296]
	TIME [epoch: 8.84 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23580518248247842		[learning rate: 0.00044595]
		[batch 20/20] avg loss: 0.25402092900025725		[learning rate: 0.00044514]
	Learning Rate: 0.000445143
	LOSS [training: 0.24491305574136785 | validation: 0.20485978018438178]
	TIME [epoch: 8.85 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3016952549794044		[learning rate: 0.00044434]
		[batch 20/20] avg loss: 0.23431890161786964		[learning rate: 0.00044353]
	Learning Rate: 0.000443528
	LOSS [training: 0.26800707829863707 | validation: 0.2076650417508759]
	TIME [epoch: 8.84 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2505116323986377		[learning rate: 0.00044272]
		[batch 20/20] avg loss: 0.24137313901770896		[learning rate: 0.00044192]
	Learning Rate: 0.000441918
	LOSS [training: 0.24594238570817328 | validation: 0.23287353885733508]
	TIME [epoch: 8.86 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24673984673892538		[learning rate: 0.00044112]
		[batch 20/20] avg loss: 0.2285307278396853		[learning rate: 0.00044031]
	Learning Rate: 0.000440315
	LOSS [training: 0.23763528728930533 | validation: 0.15354459253877206]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_959.pth
	Model improved!!!
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24281329843976915		[learning rate: 0.00043952]
		[batch 20/20] avg loss: 0.2649156044347084		[learning rate: 0.00043872]
	Learning Rate: 0.000438717
	LOSS [training: 0.25386445143723874 | validation: 0.1640968609900859]
	TIME [epoch: 8.84 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22521314927612188		[learning rate: 0.00043792]
		[batch 20/20] avg loss: 0.21781814789877624		[learning rate: 0.00043712]
	Learning Rate: 0.000437125
	LOSS [training: 0.22151564858744907 | validation: 0.18943193555600407]
	TIME [epoch: 8.84 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21480054049568106		[learning rate: 0.00043633]
		[batch 20/20] avg loss: 0.24852262384244367		[learning rate: 0.00043554]
	Learning Rate: 0.000435538
	LOSS [training: 0.23166158216906235 | validation: 0.17413434969257677]
	TIME [epoch: 8.84 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2556373253639242		[learning rate: 0.00043475]
		[batch 20/20] avg loss: 0.24451649448235896		[learning rate: 0.00043396]
	Learning Rate: 0.000433958
	LOSS [training: 0.2500769099231416 | validation: 0.20292318135375648]
	TIME [epoch: 8.86 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26927693932438773		[learning rate: 0.00043317]
		[batch 20/20] avg loss: 0.2104299732163662		[learning rate: 0.00043238]
	Learning Rate: 0.000432383
	LOSS [training: 0.23985345627037696 | validation: 0.16309678200527353]
	TIME [epoch: 8.84 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21904161930530677		[learning rate: 0.0004316]
		[batch 20/20] avg loss: 0.2626028250062028		[learning rate: 0.00043081]
	Learning Rate: 0.000430814
	LOSS [training: 0.24082222215575483 | validation: 0.19419179153393049]
	TIME [epoch: 8.84 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24730830633600576		[learning rate: 0.00043003]
		[batch 20/20] avg loss: 0.2473279172777028		[learning rate: 0.00042925]
	Learning Rate: 0.00042925
	LOSS [training: 0.2473181118068543 | validation: 0.30416361358036437]
	TIME [epoch: 8.84 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25112799501052846		[learning rate: 0.00042847]
		[batch 20/20] avg loss: 0.26460839006037984		[learning rate: 0.00042769]
	Learning Rate: 0.000427692
	LOSS [training: 0.25786819253545423 | validation: 0.17066203543609312]
	TIME [epoch: 8.85 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23416315224332368		[learning rate: 0.00042692]
		[batch 20/20] avg loss: 0.27413416886864317		[learning rate: 0.00042614]
	Learning Rate: 0.00042614
	LOSS [training: 0.2541486605559834 | validation: 0.16810272359911813]
	TIME [epoch: 8.86 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2299980984619217		[learning rate: 0.00042537]
		[batch 20/20] avg loss: 0.24497875137429478		[learning rate: 0.00042459]
	Learning Rate: 0.000424594
	LOSS [training: 0.23748842491810823 | validation: 0.40707575603869456]
	TIME [epoch: 8.85 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25630700227461406		[learning rate: 0.00042382]
		[batch 20/20] avg loss: 0.2486362756328607		[learning rate: 0.00042305]
	Learning Rate: 0.000423053
	LOSS [training: 0.2524716389537373 | validation: 0.18115856228926708]
	TIME [epoch: 8.84 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2465784948496576		[learning rate: 0.00042228]
		[batch 20/20] avg loss: 0.23733435702033567		[learning rate: 0.00042152]
	Learning Rate: 0.000421518
	LOSS [training: 0.24195642593499667 | validation: 0.1939103064129619]
	TIME [epoch: 8.84 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23070369811999067		[learning rate: 0.00042075]
		[batch 20/20] avg loss: 0.21632222949252672		[learning rate: 0.00041999]
	Learning Rate: 0.000419988
	LOSS [training: 0.22351296380625868 | validation: 0.21007786307218104]
	TIME [epoch: 8.86 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2545429565668454		[learning rate: 0.00041923]
		[batch 20/20] avg loss: 0.2308976414982138		[learning rate: 0.00041846]
	Learning Rate: 0.000418464
	LOSS [training: 0.24272029903252954 | validation: 0.16779201422364515]
	TIME [epoch: 8.84 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3384576809023253		[learning rate: 0.0004177]
		[batch 20/20] avg loss: 0.21944103186125238		[learning rate: 0.00041695]
	Learning Rate: 0.000416945
	LOSS [training: 0.2789493563817889 | validation: 0.24065162560528824]
	TIME [epoch: 8.85 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25198213379261986		[learning rate: 0.00041619]
		[batch 20/20] avg loss: 0.236440692382263		[learning rate: 0.00041543]
	Learning Rate: 0.000415432
	LOSS [training: 0.2442114130874414 | validation: 0.19353390046430605]
	TIME [epoch: 8.84 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2684244691343969		[learning rate: 0.00041468]
		[batch 20/20] avg loss: 0.21916720454306443		[learning rate: 0.00041392]
	Learning Rate: 0.000413924
	LOSS [training: 0.24379583683873066 | validation: 0.1882455842147684]
	TIME [epoch: 8.85 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19677852666537057		[learning rate: 0.00041317]
		[batch 20/20] avg loss: 0.26900206237410806		[learning rate: 0.00041242]
	Learning Rate: 0.000412422
	LOSS [training: 0.23289029451973936 | validation: 0.19440752583334736]
	TIME [epoch: 8.86 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2594888035674525		[learning rate: 0.00041167]
		[batch 20/20] avg loss: 0.23233201324326638		[learning rate: 0.00041093]
	Learning Rate: 0.000410926
	LOSS [training: 0.24591040840535947 | validation: 0.16675558468330173]
	TIME [epoch: 8.84 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2216848271605481		[learning rate: 0.00041018]
		[batch 20/20] avg loss: 0.2952317767480381		[learning rate: 0.00040943]
	Learning Rate: 0.000409434
	LOSS [training: 0.2584583019542931 | validation: 0.2488331316427204]
	TIME [epoch: 8.83 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27183004186444604		[learning rate: 0.00040869]
		[batch 20/20] avg loss: 0.22909183722139229		[learning rate: 0.00040795]
	Learning Rate: 0.000407948
	LOSS [training: 0.2504609395429192 | validation: 0.18449483439552936]
	TIME [epoch: 8.85 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21843228968549783		[learning rate: 0.00040721]
		[batch 20/20] avg loss: 0.23260436726419081		[learning rate: 0.00040647]
	Learning Rate: 0.000406468
	LOSS [training: 0.22551832847484438 | validation: 0.2509391541811992]
	TIME [epoch: 8.84 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22938799937187535		[learning rate: 0.00040573]
		[batch 20/20] avg loss: 0.22542204644820635		[learning rate: 0.00040499]
	Learning Rate: 0.000404993
	LOSS [training: 0.2274050229100409 | validation: 0.25947151422110093]
	TIME [epoch: 8.86 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25121120241753986		[learning rate: 0.00040426]
		[batch 20/20] avg loss: 0.19865190184381437		[learning rate: 0.00040352]
	Learning Rate: 0.000403523
	LOSS [training: 0.22493155213067711 | validation: 0.24812940397870065]
	TIME [epoch: 8.84 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2143696167923892		[learning rate: 0.00040279]
		[batch 20/20] avg loss: 0.2243649360352193		[learning rate: 0.00040206]
	Learning Rate: 0.000402059
	LOSS [training: 0.2193672764138043 | validation: 0.23536150046486892]
	TIME [epoch: 8.87 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22045649353619856		[learning rate: 0.00040133]
		[batch 20/20] avg loss: 0.19195322006412216		[learning rate: 0.0004006]
	Learning Rate: 0.0004006
	LOSS [training: 0.20620485680016035 | validation: 0.22885337897977406]
	TIME [epoch: 8.84 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24012506143853746		[learning rate: 0.00039987]
		[batch 20/20] avg loss: 0.2402224384747782		[learning rate: 0.00039915]
	Learning Rate: 0.000399146
	LOSS [training: 0.24017374995665786 | validation: 0.17181590421496126]
	TIME [epoch: 8.86 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23687080459208487		[learning rate: 0.00039842]
		[batch 20/20] avg loss: 0.2254586508555702		[learning rate: 0.0003977]
	Learning Rate: 0.000397697
	LOSS [training: 0.23116472772382757 | validation: 0.20355931102207767]
	TIME [epoch: 8.85 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2184948552381966		[learning rate: 0.00039698]
		[batch 20/20] avg loss: 0.23583956082264343		[learning rate: 0.00039625]
	Learning Rate: 0.000396254
	LOSS [training: 0.22716720803042004 | validation: 0.16873139228876666]
	TIME [epoch: 8.85 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20753593291561917		[learning rate: 0.00039553]
		[batch 20/20] avg loss: 0.22780490126173597		[learning rate: 0.00039482]
	Learning Rate: 0.000394816
	LOSS [training: 0.21767041708867763 | validation: 0.16917862865254746]
	TIME [epoch: 8.85 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24825655363713048		[learning rate: 0.0003941]
		[batch 20/20] avg loss: 0.2581351529924164		[learning rate: 0.00039338]
	Learning Rate: 0.000393383
	LOSS [training: 0.25319585331477346 | validation: 0.20093962483608038]
	TIME [epoch: 8.84 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25193944976800575		[learning rate: 0.00039267]
		[batch 20/20] avg loss: 0.2168805656205634		[learning rate: 0.00039196]
	Learning Rate: 0.000391956
	LOSS [training: 0.23441000769428463 | validation: 0.27472950955075004]
	TIME [epoch: 8.87 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27983205931965877		[learning rate: 0.00039124]
		[batch 20/20] avg loss: 0.2152873610316234		[learning rate: 0.00039053]
	Learning Rate: 0.000390533
	LOSS [training: 0.2475597101756411 | validation: 0.1951900301275777]
	TIME [epoch: 8.85 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22239003599291718		[learning rate: 0.00038982]
		[batch 20/20] avg loss: 0.22966272716061056		[learning rate: 0.00038912]
	Learning Rate: 0.000389116
	LOSS [training: 0.22602638157676386 | validation: 0.23518451823069886]
	TIME [epoch: 8.84 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25932339964558904		[learning rate: 0.00038841]
		[batch 20/20] avg loss: 0.20315336328231542		[learning rate: 0.0003877]
	Learning Rate: 0.000387704
	LOSS [training: 0.23123838146395218 | validation: 0.14403026768698815]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_994.pth
	Model improved!!!
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2308015574766646		[learning rate: 0.000387]
		[batch 20/20] avg loss: 0.2184455042950586		[learning rate: 0.0003863]
	Learning Rate: 0.000386297
	LOSS [training: 0.2246235308858616 | validation: 0.16710571886912265]
	TIME [epoch: 8.84 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20072487450883186		[learning rate: 0.0003856]
		[batch 20/20] avg loss: 0.25848549271447685		[learning rate: 0.00038489]
	Learning Rate: 0.000384895
	LOSS [training: 0.2296051836116543 | validation: 0.16353110953386887]
	TIME [epoch: 8.86 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22111182250495703		[learning rate: 0.0003842]
		[batch 20/20] avg loss: 0.23297266771325736		[learning rate: 0.0003835]
	Learning Rate: 0.000383498
	LOSS [training: 0.2270422451091072 | validation: 0.23856498293952305]
	TIME [epoch: 8.84 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2319551833681691		[learning rate: 0.0003828]
		[batch 20/20] avg loss: 0.2271025818627702		[learning rate: 0.00038211]
	Learning Rate: 0.000382106
	LOSS [training: 0.22952888261546964 | validation: 0.18246273075503197]
	TIME [epoch: 8.84 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24881637676188434		[learning rate: 0.00038141]
		[batch 20/20] avg loss: 0.20318099192552058		[learning rate: 0.00038072]
	Learning Rate: 0.00038072
	LOSS [training: 0.22599868434370246 | validation: 0.2263328936082067]
	TIME [epoch: 8.84 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24232657022908555		[learning rate: 0.00038003]
		[batch 20/20] avg loss: 0.2076340999785824		[learning rate: 0.00037934]
	Learning Rate: 0.000379338
	LOSS [training: 0.22498033510383403 | validation: 0.20387797163043647]
	TIME [epoch: 8.85 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21910811244127		[learning rate: 0.00037865]
		[batch 20/20] avg loss: 0.23288864030193673		[learning rate: 0.00037796]
	Learning Rate: 0.000377961
	LOSS [training: 0.22599837637160336 | validation: 0.22176454008670143]
	TIME [epoch: 8.85 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23721807786372082		[learning rate: 0.00037727]
		[batch 20/20] avg loss: 0.21324426911967867		[learning rate: 0.00037659]
	Learning Rate: 0.00037659
	LOSS [training: 0.22523117349169972 | validation: 0.18744540519248334]
	TIME [epoch: 8.84 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23510231875358972		[learning rate: 0.00037591]
		[batch 20/20] avg loss: 0.21764994538150026		[learning rate: 0.00037522]
	Learning Rate: 0.000375223
	LOSS [training: 0.22637613206754498 | validation: 0.17374357838335386]
	TIME [epoch: 8.84 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22352377272406865		[learning rate: 0.00037454]
		[batch 20/20] avg loss: 0.22838215530687528		[learning rate: 0.00037386]
	Learning Rate: 0.000373861
	LOSS [training: 0.225952964015472 | validation: 0.17642366164067383]
	TIME [epoch: 8.83 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23589707852656133		[learning rate: 0.00037318]
		[batch 20/20] avg loss: 0.28307573814051284		[learning rate: 0.0003725]
	Learning Rate: 0.000372505
	LOSS [training: 0.2594864083335371 | validation: 0.20288349943239]
	TIME [epoch: 8.86 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22355936637900897		[learning rate: 0.00037183]
		[batch 20/20] avg loss: 0.1976148606027051		[learning rate: 0.00037115]
	Learning Rate: 0.000371153
	LOSS [training: 0.21058711349085701 | validation: 0.17614787237423224]
	TIME [epoch: 8.84 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2136769834208796		[learning rate: 0.00037048]
		[batch 20/20] avg loss: 0.2163540900927718		[learning rate: 0.00036981]
	Learning Rate: 0.000369806
	LOSS [training: 0.2150155367568257 | validation: 0.2146747619189021]
	TIME [epoch: 8.83 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4611043889983		[learning rate: 0.00036913]
		[batch 20/20] avg loss: 0.34814042873993467		[learning rate: 0.00036846]
	Learning Rate: 0.000368464
	LOSS [training: 0.4046224088691173 | validation: 0.18621501061321105]
	TIME [epoch: 8.84 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2265128540089274		[learning rate: 0.00036779]
		[batch 20/20] avg loss: 0.22056878890176454		[learning rate: 0.00036713]
	Learning Rate: 0.000367127
	LOSS [training: 0.22354082145534596 | validation: 0.16721505756197416]
	TIME [epoch: 8.83 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22027291110257946		[learning rate: 0.00036646]
		[batch 20/20] avg loss: 0.2242015765334267		[learning rate: 0.00036579]
	Learning Rate: 0.000365794
	LOSS [training: 0.22223724381800308 | validation: 0.2311909089181217]
	TIME [epoch: 8.85 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2454951841662611		[learning rate: 0.00036513]
		[batch 20/20] avg loss: 0.19915164700393678		[learning rate: 0.00036447]
	Learning Rate: 0.000364467
	LOSS [training: 0.22232341558509897 | validation: 0.18850836636417206]
	TIME [epoch: 8.84 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26047668253147493		[learning rate: 0.0003638]
		[batch 20/20] avg loss: 0.20173232810221245		[learning rate: 0.00036314]
	Learning Rate: 0.000363144
	LOSS [training: 0.23110450531684362 | validation: 0.26115547360003405]
	TIME [epoch: 8.84 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22897553167363066		[learning rate: 0.00036248]
		[batch 20/20] avg loss: 0.24328341498666842		[learning rate: 0.00036183]
	Learning Rate: 0.000361826
	LOSS [training: 0.2361294733301495 | validation: 0.15800139964003662]
	TIME [epoch: 8.84 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22716713850517262		[learning rate: 0.00036117]
		[batch 20/20] avg loss: 0.22554281615716562		[learning rate: 0.00036051]
	Learning Rate: 0.000360513
	LOSS [training: 0.22635497733116905 | validation: 0.1591345111108762]
	TIME [epoch: 8.84 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23401565314806833		[learning rate: 0.00035986]
		[batch 20/20] avg loss: 0.2009994567085441		[learning rate: 0.0003592]
	Learning Rate: 0.000359205
	LOSS [training: 0.21750755492830626 | validation: 0.24275437792426158]
	TIME [epoch: 8.86 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2345916253430763		[learning rate: 0.00035855]
		[batch 20/20] avg loss: 0.19973198594032798		[learning rate: 0.0003579]
	Learning Rate: 0.000357901
	LOSS [training: 0.21716180564170212 | validation: 0.1540085527340548]
	TIME [epoch: 8.84 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18337668969167625		[learning rate: 0.00035725]
		[batch 20/20] avg loss: 0.2536011593661492		[learning rate: 0.0003566]
	Learning Rate: 0.000356602
	LOSS [training: 0.21848892452891272 | validation: 0.16878030700452756]
	TIME [epoch: 8.83 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19324878350585556		[learning rate: 0.00035595]
		[batch 20/20] avg loss: 0.25356987451927704		[learning rate: 0.00035531]
	Learning Rate: 0.000355308
	LOSS [training: 0.22340932901256627 | validation: 0.1882450834922092]
	TIME [epoch: 8.82 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23548030275059717		[learning rate: 0.00035466]
		[batch 20/20] avg loss: 0.260882857766988		[learning rate: 0.00035402]
	Learning Rate: 0.000354019
	LOSS [training: 0.24818158025879264 | validation: 0.17893423706726916]
	TIME [epoch: 8.86 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2046308616046142		[learning rate: 0.00035338]
		[batch 20/20] avg loss: 0.22469482321501189		[learning rate: 0.00035273]
	Learning Rate: 0.000352734
	LOSS [training: 0.21466284240981306 | validation: 0.21071415550108014]
	TIME [epoch: 8.85 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22719697572614553		[learning rate: 0.00035209]
		[batch 20/20] avg loss: 0.22966951180063053		[learning rate: 0.00035145]
	Learning Rate: 0.000351454
	LOSS [training: 0.22843324376338803 | validation: 0.15574143649600247]
	TIME [epoch: 8.85 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22511111571703135		[learning rate: 0.00035082]
		[batch 20/20] avg loss: 0.2067037411984935		[learning rate: 0.00035018]
	Learning Rate: 0.000350179
	LOSS [training: 0.2159074284577624 | validation: 0.1584459713988585]
	TIME [epoch: 8.84 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19948140181780533		[learning rate: 0.00034954]
		[batch 20/20] avg loss: 0.2593919763911817		[learning rate: 0.00034891]
	Learning Rate: 0.000348908
	LOSS [training: 0.2294366891044935 | validation: 0.1654276111846224]
	TIME [epoch: 8.84 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2099041992271325		[learning rate: 0.00034827]
		[batch 20/20] avg loss: 0.19712623259093673		[learning rate: 0.00034764]
	Learning Rate: 0.000347641
	LOSS [training: 0.20351521590903462 | validation: 0.2413146257756581]
	TIME [epoch: 8.87 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3351304174986923		[learning rate: 0.00034701]
		[batch 20/20] avg loss: 0.1957143667222819		[learning rate: 0.00034638]
	Learning Rate: 0.00034638
	LOSS [training: 0.2654223921104871 | validation: 0.18757032271462318]
	TIME [epoch: 8.85 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23464630549162196		[learning rate: 0.00034575]
		[batch 20/20] avg loss: 0.2222012960437801		[learning rate: 0.00034512]
	Learning Rate: 0.000345123
	LOSS [training: 0.228423800767701 | validation: 0.22416651092200288]
	TIME [epoch: 8.84 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20463103505223845		[learning rate: 0.0003445]
		[batch 20/20] avg loss: 0.20911505058865218		[learning rate: 0.00034387]
	Learning Rate: 0.00034387
	LOSS [training: 0.20687304282044527 | validation: 0.2643147521862849]
	TIME [epoch: 8.84 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22226125153111392		[learning rate: 0.00034325]
		[batch 20/20] avg loss: 0.2268288457396282		[learning rate: 0.00034262]
	Learning Rate: 0.000342622
	LOSS [training: 0.22454504863537106 | validation: 0.18187836006525726]
	TIME [epoch: 8.84 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29344800289596124		[learning rate: 0.000342]
		[batch 20/20] avg loss: 0.20629232283353		[learning rate: 0.00034138]
	Learning Rate: 0.000341379
	LOSS [training: 0.2498701628647456 | validation: 0.22657112026237908]
	TIME [epoch: 8.87 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23496047630764125		[learning rate: 0.00034076]
		[batch 20/20] avg loss: 0.22401902309791186		[learning rate: 0.00034014]
	Learning Rate: 0.00034014
	LOSS [training: 0.22948974970277652 | validation: 0.2617633438641562]
	TIME [epoch: 8.85 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21612287258154977		[learning rate: 0.00033952]
		[batch 20/20] avg loss: 0.239258564616763		[learning rate: 0.00033891]
	Learning Rate: 0.000338906
	LOSS [training: 0.22769071859915635 | validation: 0.18063988157719127]
	TIME [epoch: 8.85 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23662544824071366		[learning rate: 0.00033829]
		[batch 20/20] avg loss: 0.20656065922433203		[learning rate: 0.00033768]
	Learning Rate: 0.000337676
	LOSS [training: 0.22159305373252286 | validation: 0.1329981981499723]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1032.pth
	Model improved!!!
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2127508026122705		[learning rate: 0.00033706]
		[batch 20/20] avg loss: 0.20708799578316378		[learning rate: 0.00033645]
	Learning Rate: 0.00033645
	LOSS [training: 0.20991939919771713 | validation: 0.148529995243408]
	TIME [epoch: 8.87 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2047618972646279		[learning rate: 0.00033584]
		[batch 20/20] avg loss: 0.19332351339690484		[learning rate: 0.00033523]
	Learning Rate: 0.000335229
	LOSS [training: 0.19904270533076635 | validation: 0.1530849276127636]
	TIME [epoch: 8.86 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22021685778980454		[learning rate: 0.00033462]
		[batch 20/20] avg loss: 0.21722184274352202		[learning rate: 0.00033401]
	Learning Rate: 0.000334013
	LOSS [training: 0.21871935026666328 | validation: 0.1415165158692328]
	TIME [epoch: 8.86 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22167666583193216		[learning rate: 0.00033341]
		[batch 20/20] avg loss: 0.2235078974722083		[learning rate: 0.0003328]
	Learning Rate: 0.000332801
	LOSS [training: 0.22259228165207018 | validation: 0.19216193591916872]
	TIME [epoch: 8.85 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21993377453463842		[learning rate: 0.0003322]
		[batch 20/20] avg loss: 0.21616001220756448		[learning rate: 0.00033159]
	Learning Rate: 0.000331593
	LOSS [training: 0.21804689337110142 | validation: 0.19379792797560613]
	TIME [epoch: 8.86 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22092155501483104		[learning rate: 0.00033099]
		[batch 20/20] avg loss: 0.21296835381024365		[learning rate: 0.00033039]
	Learning Rate: 0.00033039
	LOSS [training: 0.2169449544125373 | validation: 0.12813874895296057]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1038.pth
	Model improved!!!
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23656429827328732		[learning rate: 0.00032979]
		[batch 20/20] avg loss: 0.20487936816692226		[learning rate: 0.00032919]
	Learning Rate: 0.000329191
	LOSS [training: 0.22072183322010477 | validation: 0.16539956758664698]
	TIME [epoch: 8.86 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20770863463947942		[learning rate: 0.00032859]
		[batch 20/20] avg loss: 0.21730763194251512		[learning rate: 0.000328]
	Learning Rate: 0.000327996
	LOSS [training: 0.21250813329099727 | validation: 0.19705561776746408]
	TIME [epoch: 8.86 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2021711405766891		[learning rate: 0.0003274]
		[batch 20/20] avg loss: 0.20529656800731785		[learning rate: 0.00032681]
	Learning Rate: 0.000326806
	LOSS [training: 0.20373385429200347 | validation: 0.1759914250333687]
	TIME [epoch: 8.86 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21686667948626331		[learning rate: 0.00032621]
		[batch 20/20] avg loss: 0.23433921483078718		[learning rate: 0.00032562]
	Learning Rate: 0.00032562
	LOSS [training: 0.2256029471585253 | validation: 0.12231919666323234]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1042.pth
	Model improved!!!
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23643989094955642		[learning rate: 0.00032503]
		[batch 20/20] avg loss: 0.17533388018966192		[learning rate: 0.00032444]
	Learning Rate: 0.000324438
	LOSS [training: 0.20588688556960913 | validation: 0.14777910235656577]
	TIME [epoch: 8.88 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1958751004137193		[learning rate: 0.00032385]
		[batch 20/20] avg loss: 0.26063448295243025		[learning rate: 0.00032326]
	Learning Rate: 0.00032326
	LOSS [training: 0.22825479168307478 | validation: 0.16005709531632986]
	TIME [epoch: 8.86 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21329179439481516		[learning rate: 0.00032267]
		[batch 20/20] avg loss: 0.22744815106959723		[learning rate: 0.00032209]
	Learning Rate: 0.000322087
	LOSS [training: 0.2203699727322062 | validation: 0.13715725506282353]
	TIME [epoch: 8.86 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22983228209013568		[learning rate: 0.0003215]
		[batch 20/20] avg loss: 0.24006443588381532		[learning rate: 0.00032092]
	Learning Rate: 0.000320918
	LOSS [training: 0.23494835898697555 | validation: 0.2307228623587134]
	TIME [epoch: 8.86 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19514911727863968		[learning rate: 0.00032034]
		[batch 20/20] avg loss: 0.22700687816653548		[learning rate: 0.00031975]
	Learning Rate: 0.000319754
	LOSS [training: 0.21107799772258753 | validation: 0.16337965775577812]
	TIME [epoch: 8.88 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18268910374494646		[learning rate: 0.00031917]
		[batch 20/20] avg loss: 0.21835045357924637		[learning rate: 0.00031859]
	Learning Rate: 0.000318593
	LOSS [training: 0.20051977866209642 | validation: 0.17529542343758608]
	TIME [epoch: 8.86 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2222601396142873		[learning rate: 0.00031801]
		[batch 20/20] avg loss: 0.18795342180971103		[learning rate: 0.00031744]
	Learning Rate: 0.000317437
	LOSS [training: 0.2051067807119992 | validation: 0.16030987577339573]
	TIME [epoch: 8.86 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19130148787847873		[learning rate: 0.00031686]
		[batch 20/20] avg loss: 0.24035647381407882		[learning rate: 0.00031629]
	Learning Rate: 0.000316285
	LOSS [training: 0.21582898084627872 | validation: 0.2460168098974799]
	TIME [epoch: 8.85 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20761151766874986		[learning rate: 0.00031571]
		[batch 20/20] avg loss: 0.21908971095005453		[learning rate: 0.00031514]
	Learning Rate: 0.000315137
	LOSS [training: 0.21335061430940216 | validation: 0.1776664786565914]
	TIME [epoch: 8.86 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19797309162872376		[learning rate: 0.00031457]
		[batch 20/20] avg loss: 0.17866410148052195		[learning rate: 0.00031399]
	Learning Rate: 0.000313994
	LOSS [training: 0.18831859655462285 | validation: 0.13373496847059865]
	TIME [epoch: 8.87 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20336169183741548		[learning rate: 0.00031342]
		[batch 20/20] avg loss: 0.1802954919180094		[learning rate: 0.00031285]
	Learning Rate: 0.000312854
	LOSS [training: 0.19182859187771245 | validation: 0.16612516687233125]
	TIME [epoch: 8.86 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26459220264341254		[learning rate: 0.00031229]
		[batch 20/20] avg loss: 0.19563807884845272		[learning rate: 0.00031172]
	Learning Rate: 0.000311719
	LOSS [training: 0.2301151407459326 | validation: 0.12941522263706318]
	TIME [epoch: 8.86 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2084182779379627		[learning rate: 0.00031115]
		[batch 20/20] avg loss: 0.2087008184445774		[learning rate: 0.00031059]
	Learning Rate: 0.000310588
	LOSS [training: 0.20855954819127004 | validation: 0.20647885957988302]
	TIME [epoch: 8.86 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22603420263497198		[learning rate: 0.00031002]
		[batch 20/20] avg loss: 0.19115297663763733		[learning rate: 0.00030946]
	Learning Rate: 0.000309461
	LOSS [training: 0.20859358963630464 | validation: 0.14950911221247093]
	TIME [epoch: 8.86 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20581088962751962		[learning rate: 0.0003089]
		[batch 20/20] avg loss: 0.17517651566701167		[learning rate: 0.00030834]
	Learning Rate: 0.000308338
	LOSS [training: 0.19049370264726562 | validation: 0.13663092810688537]
	TIME [epoch: 8.88 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1803221763196837		[learning rate: 0.00030778]
		[batch 20/20] avg loss: 0.19854566590745976		[learning rate: 0.00030722]
	Learning Rate: 0.000307219
	LOSS [training: 0.1894339211135717 | validation: 0.17406047153245371]
	TIME [epoch: 8.86 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1805037887527774		[learning rate: 0.00030666]
		[batch 20/20] avg loss: 0.20347178424520473		[learning rate: 0.0003061]
	Learning Rate: 0.000306104
	LOSS [training: 0.19198778649899106 | validation: 0.1566156118014732]
	TIME [epoch: 8.86 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19267480813551194		[learning rate: 0.00030555]
		[batch 20/20] avg loss: 0.20626932300198045		[learning rate: 0.00030499]
	Learning Rate: 0.000304993
	LOSS [training: 0.19947206556874617 | validation: 0.25163428832796414]
	TIME [epoch: 8.86 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21956411324635322		[learning rate: 0.00030444]
		[batch 20/20] avg loss: 0.21905049607995636		[learning rate: 0.00030389]
	Learning Rate: 0.000303886
	LOSS [training: 0.2193073046631548 | validation: 0.18074242330897217]
	TIME [epoch: 8.88 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1974008187215094		[learning rate: 0.00030333]
		[batch 20/20] avg loss: 0.19355462387913608		[learning rate: 0.00030278]
	Learning Rate: 0.000302783
	LOSS [training: 0.19547772130032276 | validation: 0.207190194364548]
	TIME [epoch: 8.87 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18369495809197203		[learning rate: 0.00030223]
		[batch 20/20] avg loss: 0.21431092261992646		[learning rate: 0.00030168]
	Learning Rate: 0.000301684
	LOSS [training: 0.19900294035594926 | validation: 0.18242347040098425]
	TIME [epoch: 8.86 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21805057413885426		[learning rate: 0.00030114]
		[batch 20/20] avg loss: 0.1963939167342592		[learning rate: 0.00030059]
	Learning Rate: 0.000300589
	LOSS [training: 0.20722224543655673 | validation: 0.16782841633549414]
	TIME [epoch: 8.86 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23310113103346689		[learning rate: 0.00030004]
		[batch 20/20] avg loss: 0.1900924089277864		[learning rate: 0.0002995]
	Learning Rate: 0.000299499
	LOSS [training: 0.21159676998062665 | validation: 0.18905122712347944]
	TIME [epoch: 8.85 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.174798684453885		[learning rate: 0.00029895]
		[batch 20/20] avg loss: 0.21142876553563011		[learning rate: 0.00029841]
	Learning Rate: 0.000298412
	LOSS [training: 0.19311372499475757 | validation: 0.12495172875598676]
	TIME [epoch: 8.88 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1947800386919407		[learning rate: 0.00029787]
		[batch 20/20] avg loss: 0.19970411987177536		[learning rate: 0.00029733]
	Learning Rate: 0.000297329
	LOSS [training: 0.197242079281858 | validation: 0.17561705664737098]
	TIME [epoch: 8.86 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18065562757701914		[learning rate: 0.00029679]
		[batch 20/20] avg loss: 0.21675871599462077		[learning rate: 0.00029625]
	Learning Rate: 0.00029625
	LOSS [training: 0.19870717178581992 | validation: 0.16058122766273752]
	TIME [epoch: 8.86 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18130405154510532		[learning rate: 0.00029571]
		[batch 20/20] avg loss: 0.199632608334656		[learning rate: 0.00029517]
	Learning Rate: 0.000295175
	LOSS [training: 0.19046832993988067 | validation: 0.14869079180213712]
	TIME [epoch: 8.86 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19217652814178712		[learning rate: 0.00029464]
		[batch 20/20] avg loss: 0.18001445980152966		[learning rate: 0.0002941]
	Learning Rate: 0.000294103
	LOSS [training: 0.1860954939716584 | validation: 0.13244434158812649]
	TIME [epoch: 8.86 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16871795412313623		[learning rate: 0.00029357]
		[batch 20/20] avg loss: 0.20715160262784288		[learning rate: 0.00029304]
	Learning Rate: 0.000293036
	LOSS [training: 0.18793477837548953 | validation: 0.20436824561302258]
	TIME [epoch: 8.88 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19485997402766544		[learning rate: 0.0002925]
		[batch 20/20] avg loss: 0.19634404600554883		[learning rate: 0.00029197]
	Learning Rate: 0.000291973
	LOSS [training: 0.19560201001660715 | validation: 0.11998324804244298]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1072.pth
	Model improved!!!
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1939144871204061		[learning rate: 0.00029144]
		[batch 20/20] avg loss: 0.20447011483781505		[learning rate: 0.00029091]
	Learning Rate: 0.000290913
	LOSS [training: 0.1991923009791106 | validation: 0.18200494328056452]
	TIME [epoch: 8.85 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17988471152927116		[learning rate: 0.00029038]
		[batch 20/20] avg loss: 0.2203075904641903		[learning rate: 0.00028986]
	Learning Rate: 0.000289857
	LOSS [training: 0.20009615099673078 | validation: 0.1999316398827025]
	TIME [epoch: 8.85 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2009893859767879		[learning rate: 0.00028933]
		[batch 20/20] avg loss: 0.17313947589615147		[learning rate: 0.00028881]
	Learning Rate: 0.000288805
	LOSS [training: 0.18706443093646974 | validation: 0.12251913765618075]
	TIME [epoch: 8.87 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18753040150120875		[learning rate: 0.00028828]
		[batch 20/20] avg loss: 0.1706293117263769		[learning rate: 0.00028776]
	Learning Rate: 0.000287757
	LOSS [training: 0.17907985661379283 | validation: 0.13913787445386938]
	TIME [epoch: 8.86 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17866449210196492		[learning rate: 0.00028723]
		[batch 20/20] avg loss: 0.1924939647274442		[learning rate: 0.00028671]
	Learning Rate: 0.000286713
	LOSS [training: 0.18557922841470456 | validation: 0.13270508340764103]
	TIME [epoch: 8.85 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20963918540679694		[learning rate: 0.00028619]
		[batch 20/20] avg loss: 0.18466838765141097		[learning rate: 0.00028567]
	Learning Rate: 0.000285672
	LOSS [training: 0.19715378652910395 | validation: 0.15540325360746748]
	TIME [epoch: 8.85 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19734604013323295		[learning rate: 0.00028515]
		[batch 20/20] avg loss: 0.18851677022839133		[learning rate: 0.00028464]
	Learning Rate: 0.000284636
	LOSS [training: 0.19293140518081212 | validation: 0.18897724496226415]
	TIME [epoch: 8.86 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20613679385298028		[learning rate: 0.00028412]
		[batch 20/20] avg loss: 0.20224937460269063		[learning rate: 0.0002836]
	Learning Rate: 0.000283603
	LOSS [training: 0.20419308422783544 | validation: 0.1723004450613614]
	TIME [epoch: 8.87 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19877253391919245		[learning rate: 0.00028309]
		[batch 20/20] avg loss: 0.1966199623848243		[learning rate: 0.00028257]
	Learning Rate: 0.000282574
	LOSS [training: 0.19769624815200837 | validation: 0.1435901116550099]
	TIME [epoch: 8.86 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.183835216628148		[learning rate: 0.00028206]
		[batch 20/20] avg loss: 0.19109917602565368		[learning rate: 0.00028155]
	Learning Rate: 0.000281548
	LOSS [training: 0.18746719632690084 | validation: 0.2643959038195157]
	TIME [epoch: 8.86 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19012342867859858		[learning rate: 0.00028104]
		[batch 20/20] avg loss: 0.22256470528134065		[learning rate: 0.00028053]
	Learning Rate: 0.000280526
	LOSS [training: 0.20634406697996957 | validation: 0.1581209731817271]
	TIME [epoch: 8.85 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18621284694327833		[learning rate: 0.00028002]
		[batch 20/20] avg loss: 0.20109748536014033		[learning rate: 0.00027951]
	Learning Rate: 0.000279508
	LOSS [training: 0.19365516615170936 | validation: 0.19034186712836973]
	TIME [epoch: 8.86 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20996290698749737		[learning rate: 0.000279]
		[batch 20/20] avg loss: 0.1820366435149608		[learning rate: 0.00027849]
	Learning Rate: 0.000278494
	LOSS [training: 0.19599977525122908 | validation: 0.18052931067838135]
	TIME [epoch: 8.87 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20252805742871188		[learning rate: 0.00027799]
		[batch 20/20] avg loss: 0.18829262365948857		[learning rate: 0.00027748]
	Learning Rate: 0.000277483
	LOSS [training: 0.19541034054410017 | validation: 0.16478073331262957]
	TIME [epoch: 8.86 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17417425148395752		[learning rate: 0.00027698]
		[batch 20/20] avg loss: 0.18774630348285354		[learning rate: 0.00027648]
	Learning Rate: 0.000276476
	LOSS [training: 0.1809602774834055 | validation: 0.13575797580966584]
	TIME [epoch: 8.84 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18410188282611553		[learning rate: 0.00027597]
		[batch 20/20] avg loss: 0.18209694334497828		[learning rate: 0.00027547]
	Learning Rate: 0.000275473
	LOSS [training: 0.1830994130855469 | validation: 0.1517828840797936]
	TIME [epoch: 8.85 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21083047536619035		[learning rate: 0.00027497]
		[batch 20/20] avg loss: 0.23866048708019733		[learning rate: 0.00027447]
	Learning Rate: 0.000274473
	LOSS [training: 0.22474548122319385 | validation: 0.1396166842740421]
	TIME [epoch: 8.87 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1896730367650874		[learning rate: 0.00027397]
		[batch 20/20] avg loss: 0.20219165235470454		[learning rate: 0.00027348]
	Learning Rate: 0.000273477
	LOSS [training: 0.19593234455989594 | validation: 0.15232895535582672]
	TIME [epoch: 8.85 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2102722087726608		[learning rate: 0.00027298]
		[batch 20/20] avg loss: 0.17303810403226927		[learning rate: 0.00027248]
	Learning Rate: 0.000272485
	LOSS [training: 0.19165515640246505 | validation: 0.14374303453669793]
	TIME [epoch: 8.85 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1664814321227123		[learning rate: 0.00027199]
		[batch 20/20] avg loss: 0.1946204528186137		[learning rate: 0.0002715]
	Learning Rate: 0.000271496
	LOSS [training: 0.180550942470663 | validation: 0.13880257041842134]
	TIME [epoch: 8.84 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17963921113410933		[learning rate: 0.000271]
		[batch 20/20] avg loss: 0.17232547740923915		[learning rate: 0.00027051]
	Learning Rate: 0.000270511
	LOSS [training: 0.17598234427167425 | validation: 0.17833119686561533]
	TIME [epoch: 8.84 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19793476855157005		[learning rate: 0.00027002]
		[batch 20/20] avg loss: 0.18394501657116244		[learning rate: 0.00026953]
	Learning Rate: 0.000269529
	LOSS [training: 0.19093989256136623 | validation: 0.18079568573552318]
	TIME [epoch: 8.87 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16379291073768423		[learning rate: 0.00026904]
		[batch 20/20] avg loss: 0.20942688068984858		[learning rate: 0.00026855]
	Learning Rate: 0.000268551
	LOSS [training: 0.18660989571376643 | validation: 0.19117931925556114]
	TIME [epoch: 8.86 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1897784886956782		[learning rate: 0.00026806]
		[batch 20/20] avg loss: 0.19578385166491002		[learning rate: 0.00026758]
	Learning Rate: 0.000267576
	LOSS [training: 0.19278117018029411 | validation: 0.19403986118623862]
	TIME [epoch: 8.85 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19534486053659036		[learning rate: 0.00026709]
		[batch 20/20] avg loss: 0.19236060632803767		[learning rate: 0.00026661]
	Learning Rate: 0.000266605
	LOSS [training: 0.19385273343231402 | validation: 0.18047075850403396]
	TIME [epoch: 8.85 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19260295110090658		[learning rate: 0.00026612]
		[batch 20/20] avg loss: 0.20667130115013058		[learning rate: 0.00026564]
	Learning Rate: 0.000265638
	LOSS [training: 0.1996371261255186 | validation: 0.1322311276592089]
	TIME [epoch: 8.85 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17643238830072958		[learning rate: 0.00026516]
		[batch 20/20] avg loss: 0.18309254020078874		[learning rate: 0.00026467]
	Learning Rate: 0.000264674
	LOSS [training: 0.1797624642507592 | validation: 0.14771514855157342]
	TIME [epoch: 8.87 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16518170497640336		[learning rate: 0.00026419]
		[batch 20/20] avg loss: 0.18676592520597413		[learning rate: 0.00026371]
	Learning Rate: 0.000263713
	LOSS [training: 0.17597381509118876 | validation: 0.2003166072661338]
	TIME [epoch: 8.85 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18833067716323879		[learning rate: 0.00026323]
		[batch 20/20] avg loss: 0.19222249969235677		[learning rate: 0.00026276]
	Learning Rate: 0.000262756
	LOSS [training: 0.19027658842779777 | validation: 0.2174280084514862]
	TIME [epoch: 8.85 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1774071490594157		[learning rate: 0.00026228]
		[batch 20/20] avg loss: 0.1954684427757214		[learning rate: 0.0002618]
	Learning Rate: 0.000261802
	LOSS [training: 0.18643779591756854 | validation: 0.10947324872086403]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1102.pth
	Model improved!!!
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1715735111066595		[learning rate: 0.00026133]
		[batch 20/20] avg loss: 0.1975101331937233		[learning rate: 0.00026085]
	Learning Rate: 0.000260852
	LOSS [training: 0.18454182215019138 | validation: 0.12345016802885903]
	TIME [epoch: 8.87 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17573025365466305		[learning rate: 0.00026038]
		[batch 20/20] avg loss: 0.18740661049673846		[learning rate: 0.00025991]
	Learning Rate: 0.000259906
	LOSS [training: 0.18156843207570078 | validation: 0.1565492132242866]
	TIME [epoch: 8.85 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19928811198374916		[learning rate: 0.00025943]
		[batch 20/20] avg loss: 0.16139169530683		[learning rate: 0.00025896]
	Learning Rate: 0.000258962
	LOSS [training: 0.18033990364528957 | validation: 0.1654243508713451]
	TIME [epoch: 8.84 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18594529210739233		[learning rate: 0.00025849]
		[batch 20/20] avg loss: 0.19532889140022747		[learning rate: 0.00025802]
	Learning Rate: 0.000258023
	LOSS [training: 0.19063709175380988 | validation: 0.14262629062766488]
	TIME [epoch: 8.85 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19157883510223922		[learning rate: 0.00025755]
		[batch 20/20] avg loss: 0.1802971015153087		[learning rate: 0.00025709]
	Learning Rate: 0.000257086
	LOSS [training: 0.185937968308774 | validation: 0.14339106571947224]
	TIME [epoch: 8.84 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1918551092770663		[learning rate: 0.00025662]
		[batch 20/20] avg loss: 0.17661570832812773		[learning rate: 0.00025615]
	Learning Rate: 0.000256153
	LOSS [training: 0.18423540880259698 | validation: 0.13488183205969753]
	TIME [epoch: 8.86 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19293699622329313		[learning rate: 0.00025569]
		[batch 20/20] avg loss: 0.17967393834269807		[learning rate: 0.00025522]
	Learning Rate: 0.000255224
	LOSS [training: 0.1863054672829956 | validation: 0.1327886233369792]
	TIME [epoch: 8.85 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1625514794373955		[learning rate: 0.00025476]
		[batch 20/20] avg loss: 0.21001969980754565		[learning rate: 0.0002543]
	Learning Rate: 0.000254298
	LOSS [training: 0.1862855896224706 | validation: 0.13500083523715034]
	TIME [epoch: 8.84 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18358706629353633		[learning rate: 0.00025384]
		[batch 20/20] avg loss: 0.2153642974924809		[learning rate: 0.00025337]
	Learning Rate: 0.000253375
	LOSS [training: 0.1994756818930086 | validation: 0.304467249690325]
	TIME [epoch: 8.84 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19346597382414898		[learning rate: 0.00025291]
		[batch 20/20] avg loss: 0.19888886068090889		[learning rate: 0.00025246]
	Learning Rate: 0.000252455
	LOSS [training: 0.19617741725252894 | validation: 0.14629078544858815]
	TIME [epoch: 8.84 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17015551134420526		[learning rate: 0.000252]
		[batch 20/20] avg loss: 0.17950128570369747		[learning rate: 0.00025154]
	Learning Rate: 0.000251539
	LOSS [training: 0.17482839852395135 | validation: 0.17036058455387215]
	TIME [epoch: 8.86 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1682753245454138		[learning rate: 0.00025108]
		[batch 20/20] avg loss: 0.19097959577142257		[learning rate: 0.00025063]
	Learning Rate: 0.000250626
	LOSS [training: 0.17962746015841818 | validation: 0.2441215865814167]
	TIME [epoch: 8.85 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20735907920280136		[learning rate: 0.00025017]
		[batch 20/20] avg loss: 0.20221601052663257		[learning rate: 0.00024972]
	Learning Rate: 0.000249717
	LOSS [training: 0.20478754486471695 | validation: 0.14238794581850425]
	TIME [epoch: 8.84 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17146022535880895		[learning rate: 0.00024926]
		[batch 20/20] avg loss: 0.18994124519471045		[learning rate: 0.00024881]
	Learning Rate: 0.00024881
	LOSS [training: 0.18070073527675967 | validation: 0.15302519803345377]
	TIME [epoch: 8.84 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1727634184921248		[learning rate: 0.00024836]
		[batch 20/20] avg loss: 0.20714729466073697		[learning rate: 0.00024791]
	Learning Rate: 0.000247907
	LOSS [training: 0.18995535657643087 | validation: 0.22680304141945004]
	TIME [epoch: 8.85 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20059473009164225		[learning rate: 0.00024746]
		[batch 20/20] avg loss: 0.1679974476975356		[learning rate: 0.00024701]
	Learning Rate: 0.000247008
	LOSS [training: 0.18429608889458893 | validation: 0.13950945431231696]
	TIME [epoch: 8.85 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17823960784419995		[learning rate: 0.00024656]
		[batch 20/20] avg loss: 0.18571499018897925		[learning rate: 0.00024611]
	Learning Rate: 0.000246111
	LOSS [training: 0.18197729901658963 | validation: 0.1257604950526181]
	TIME [epoch: 8.84 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1737974364638712		[learning rate: 0.00024566]
		[batch 20/20] avg loss: 0.17849119160345522		[learning rate: 0.00024522]
	Learning Rate: 0.000245218
	LOSS [training: 0.1761443140336632 | validation: 0.1194452816948966]
	TIME [epoch: 8.84 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17342560484050093		[learning rate: 0.00024477]
		[batch 20/20] avg loss: 0.18357432603451906		[learning rate: 0.00024433]
	Learning Rate: 0.000244328
	LOSS [training: 0.17849996543750998 | validation: 0.20659244033165375]
	TIME [epoch: 8.84 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21818647652253573		[learning rate: 0.00024388]
		[batch 20/20] avg loss: 0.20817150927074352		[learning rate: 0.00024344]
	Learning Rate: 0.000243442
	LOSS [training: 0.2131789928966396 | validation: 0.1858803172806194]
	TIME [epoch: 8.86 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1637859844742155		[learning rate: 0.000243]
		[batch 20/20] avg loss: 0.2076289471912935		[learning rate: 0.00024256]
	Learning Rate: 0.000242558
	LOSS [training: 0.18570746583275446 | validation: 0.1697005104359312]
	TIME [epoch: 8.84 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18035573411480874		[learning rate: 0.00024212]
		[batch 20/20] avg loss: 0.1800679666696134		[learning rate: 0.00024168]
	Learning Rate: 0.000241678
	LOSS [training: 0.18021185039221102 | validation: 0.2433283494525721]
	TIME [epoch: 8.85 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17904378651104377		[learning rate: 0.00024124]
		[batch 20/20] avg loss: 0.1948149144116836		[learning rate: 0.0002408]
	Learning Rate: 0.000240801
	LOSS [training: 0.18692935046136372 | validation: 0.14523825184444128]
	TIME [epoch: 8.84 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16677327535815223		[learning rate: 0.00024036]
		[batch 20/20] avg loss: 0.18559084373579932		[learning rate: 0.00023993]
	Learning Rate: 0.000239927
	LOSS [training: 0.17618205954697577 | validation: 0.11988154166950417]
	TIME [epoch: 8.85 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16659950253494404		[learning rate: 0.00023949]
		[batch 20/20] avg loss: 0.19352607969034663		[learning rate: 0.00023906]
	Learning Rate: 0.000239056
	LOSS [training: 0.18006279111264534 | validation: 0.13248912525254825]
	TIME [epoch: 8.86 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16279501899480328		[learning rate: 0.00023862]
		[batch 20/20] avg loss: 0.1669623811689474		[learning rate: 0.00023819]
	Learning Rate: 0.000238189
	LOSS [training: 0.16487870008187533 | validation: 0.16701068329913502]
	TIME [epoch: 8.85 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21758689387055669		[learning rate: 0.00023776]
		[batch 20/20] avg loss: 0.1636294417387351		[learning rate: 0.00023732]
	Learning Rate: 0.000237324
	LOSS [training: 0.1906081678046459 | validation: 0.1439974985379004]
	TIME [epoch: 8.84 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16850982763618638		[learning rate: 0.00023689]
		[batch 20/20] avg loss: 0.17271060114789327		[learning rate: 0.00023646]
	Learning Rate: 0.000236463
	LOSS [training: 0.1706102143920398 | validation: 0.14300760734179518]
	TIME [epoch: 8.84 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14851480608716858		[learning rate: 0.00023603]
		[batch 20/20] avg loss: 0.21866108893691721		[learning rate: 0.0002356]
	Learning Rate: 0.000235605
	LOSS [training: 0.1835879475120429 | validation: 0.14601628440124903]
	TIME [epoch: 8.84 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16524562559951492		[learning rate: 0.00023518]
		[batch 20/20] avg loss: 0.19177758042296372		[learning rate: 0.00023475]
	Learning Rate: 0.00023475
	LOSS [training: 0.1785116030112393 | validation: 0.1596488747973116]
	TIME [epoch: 8.86 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17259756267113593		[learning rate: 0.00023432]
		[batch 20/20] avg loss: 0.18431248314803356		[learning rate: 0.0002339]
	Learning Rate: 0.000233898
	LOSS [training: 0.17845502290958476 | validation: 0.1500762716986429]
	TIME [epoch: 8.84 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19066404332638787		[learning rate: 0.00023347]
		[batch 20/20] avg loss: 0.17449408273093578		[learning rate: 0.00023305]
	Learning Rate: 0.000233049
	LOSS [training: 0.18257906302866178 | validation: 0.15021829539591192]
	TIME [epoch: 8.84 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18591850844050953		[learning rate: 0.00023263]
		[batch 20/20] avg loss: 0.18833131169257383		[learning rate: 0.0002322]
	Learning Rate: 0.000232203
	LOSS [training: 0.1871249100665417 | validation: 0.14370410458581653]
	TIME [epoch: 8.84 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19079751761968664		[learning rate: 0.00023178]
		[batch 20/20] avg loss: 0.2476702306947404		[learning rate: 0.00023136]
	Learning Rate: 0.000231361
	LOSS [training: 0.21923387415721352 | validation: 0.1290738218656196]
	TIME [epoch: 8.87 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18781813291775878		[learning rate: 0.00023094]
		[batch 20/20] avg loss: 0.16148125536332486		[learning rate: 0.00023052]
	Learning Rate: 0.000230521
	LOSS [training: 0.17464969414054182 | validation: 0.15723704616177717]
	TIME [epoch: 8.84 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18302777034818365		[learning rate: 0.0002301]
		[batch 20/20] avg loss: 0.17389133221004185		[learning rate: 0.00022968]
	Learning Rate: 0.000229685
	LOSS [training: 0.17845955127911275 | validation: 0.19039768665675602]
	TIME [epoch: 8.84 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1699572870049328		[learning rate: 0.00022927]
		[batch 20/20] avg loss: 0.17607140867005175		[learning rate: 0.00022885]
	Learning Rate: 0.000228851
	LOSS [training: 0.17301434783749228 | validation: 0.1089205618905564]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1139.pth
	Model improved!!!
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18160181608725454		[learning rate: 0.00022844]
		[batch 20/20] avg loss: 0.18099490865681633		[learning rate: 0.00022802]
	Learning Rate: 0.00022802
	LOSS [training: 0.18129836237203542 | validation: 0.13957924589496237]
	TIME [epoch: 8.85 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16801866829794584		[learning rate: 0.00022761]
		[batch 20/20] avg loss: 0.1843426028913773		[learning rate: 0.00022719]
	Learning Rate: 0.000227193
	LOSS [training: 0.1761806355946616 | validation: 0.14154466543134772]
	TIME [epoch: 8.87 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15527578931882074		[learning rate: 0.00022678]
		[batch 20/20] avg loss: 0.17894837761709964		[learning rate: 0.00022637]
	Learning Rate: 0.000226368
	LOSS [training: 0.16711208346796017 | validation: 0.19983637056222753]
	TIME [epoch: 8.84 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16211291012101553		[learning rate: 0.00022596]
		[batch 20/20] avg loss: 0.18896365863163		[learning rate: 0.00022555]
	Learning Rate: 0.000225547
	LOSS [training: 0.17553828437632274 | validation: 0.14115144374967062]
	TIME [epoch: 8.85 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1627167955433003		[learning rate: 0.00022514]
		[batch 20/20] avg loss: 0.19451410259946286		[learning rate: 0.00022473]
	Learning Rate: 0.000224728
	LOSS [training: 0.17861544907138158 | validation: 0.17832028170927208]
	TIME [epoch: 8.84 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17632662012090067		[learning rate: 0.00022432]
		[batch 20/20] avg loss: 0.1611927903845883		[learning rate: 0.00022391]
	Learning Rate: 0.000223913
	LOSS [training: 0.16875970525274447 | validation: 0.16350479475103166]
	TIME [epoch: 8.85 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16984910371109158		[learning rate: 0.00022351]
		[batch 20/20] avg loss: 0.18612179512636287		[learning rate: 0.0002231]
	Learning Rate: 0.0002231
	LOSS [training: 0.17798544941872718 | validation: 0.1819737399980336]
	TIME [epoch: 8.86 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16983136114099479		[learning rate: 0.0002227]
		[batch 20/20] avg loss: 0.1913415583171019		[learning rate: 0.00022229]
	Learning Rate: 0.000222291
	LOSS [training: 0.18058645972904835 | validation: 0.18855541397811115]
	TIME [epoch: 8.84 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18801532797453496		[learning rate: 0.00022189]
		[batch 20/20] avg loss: 0.19723105467289748		[learning rate: 0.00022148]
	Learning Rate: 0.000221484
	LOSS [training: 0.19262319132371622 | validation: 0.13893621257705066]
	TIME [epoch: 8.85 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17066944941496415		[learning rate: 0.00022108]
		[batch 20/20] avg loss: 0.184310200952774		[learning rate: 0.00022068]
	Learning Rate: 0.00022068
	LOSS [training: 0.17748982518386913 | validation: 0.13318252520428533]
	TIME [epoch: 8.84 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1678281461123281		[learning rate: 0.00022028]
		[batch 20/20] avg loss: 0.1856814051348055		[learning rate: 0.00021988]
	Learning Rate: 0.000219879
	LOSS [training: 0.1767547756235668 | validation: 0.19798637477070755]
	TIME [epoch: 8.86 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.169575669273291		[learning rate: 0.00021948]
		[batch 20/20] avg loss: 0.18025726843878506		[learning rate: 0.00021908]
	Learning Rate: 0.000219081
	LOSS [training: 0.174916468856038 | validation: 0.12838974577857293]
	TIME [epoch: 8.84 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19427992592671642		[learning rate: 0.00021868]
		[batch 20/20] avg loss: 0.16025929888175033		[learning rate: 0.00021829]
	Learning Rate: 0.000218286
	LOSS [training: 0.1772696124042334 | validation: 0.19426678079812287]
	TIME [epoch: 8.84 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17393748430657233		[learning rate: 0.00021789]
		[batch 20/20] avg loss: 0.26248298104152545		[learning rate: 0.00021749]
	Learning Rate: 0.000217494
	LOSS [training: 0.21821023267404885 | validation: 0.13980732777039945]
	TIME [epoch: 8.84 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1781041398702676		[learning rate: 0.0002171]
		[batch 20/20] avg loss: 0.18387062334000745		[learning rate: 0.0002167]
	Learning Rate: 0.000216705
	LOSS [training: 0.18098738160513753 | validation: 0.14596939342958015]
	TIME [epoch: 8.83 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1555080788239312		[learning rate: 0.00021631]
		[batch 20/20] avg loss: 0.18551866566414232		[learning rate: 0.00021592]
	Learning Rate: 0.000215918
	LOSS [training: 0.1705133722440368 | validation: 0.16948090914586744]
	TIME [epoch: 8.86 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17857849100392847		[learning rate: 0.00021553]
		[batch 20/20] avg loss: 0.19870470391590342		[learning rate: 0.00021513]
	Learning Rate: 0.000215135
	LOSS [training: 0.1886415974599159 | validation: 0.17560987619717394]
	TIME [epoch: 8.83 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17684481944214187		[learning rate: 0.00021474]
		[batch 20/20] avg loss: 0.16719573866745863		[learning rate: 0.00021435]
	Learning Rate: 0.000214354
	LOSS [training: 0.17202027905480025 | validation: 0.168557600163335]
	TIME [epoch: 8.84 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18284463835452153		[learning rate: 0.00021396]
		[batch 20/20] avg loss: 0.17255493975418013		[learning rate: 0.00021358]
	Learning Rate: 0.000213576
	LOSS [training: 0.17769978905435083 | validation: 0.11984869759711735]
	TIME [epoch: 8.83 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18043882010939216		[learning rate: 0.00021319]
		[batch 20/20] avg loss: 0.1441426463680946		[learning rate: 0.0002128]
	Learning Rate: 0.000212801
	LOSS [training: 0.16229073323874338 | validation: 0.13729549878855218]
	TIME [epoch: 8.84 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.186023074122651		[learning rate: 0.00021241]
		[batch 20/20] avg loss: 0.16130896654234522		[learning rate: 0.00021203]
	Learning Rate: 0.000212029
	LOSS [training: 0.1736660203324981 | validation: 0.14534539822902975]
	TIME [epoch: 8.86 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16514419572740602		[learning rate: 0.00021164]
		[batch 20/20] avg loss: 0.17702577561313673		[learning rate: 0.00021126]
	Learning Rate: 0.000211259
	LOSS [training: 0.17108498567027136 | validation: 0.1394343791055656]
	TIME [epoch: 8.85 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1849912375435047		[learning rate: 0.00021088]
		[batch 20/20] avg loss: 0.16979114413621382		[learning rate: 0.00021049]
	Learning Rate: 0.000210493
	LOSS [training: 0.17739119083985924 | validation: 0.11198861098611554]
	TIME [epoch: 8.84 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21593029736652752		[learning rate: 0.00021011]
		[batch 20/20] avg loss: 0.18474761815546478		[learning rate: 0.00020973]
	Learning Rate: 0.000209729
	LOSS [training: 0.2003389577609962 | validation: 0.12726006848267388]
	TIME [epoch: 8.85 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18369378583572282		[learning rate: 0.00020935]
		[batch 20/20] avg loss: 0.19964549789191863		[learning rate: 0.00020897]
	Learning Rate: 0.000208968
	LOSS [training: 0.1916696418638207 | validation: 0.14163809095951616]
	TIME [epoch: 8.86 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15500360811188293		[learning rate: 0.00020859]
		[batch 20/20] avg loss: 0.17211851323713281		[learning rate: 0.00020821]
	Learning Rate: 0.000208209
	LOSS [training: 0.16356106067450785 | validation: 0.16375249278619847]
	TIME [epoch: 8.85 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17290669738997913		[learning rate: 0.00020783]
		[batch 20/20] avg loss: 0.2009646138834619		[learning rate: 0.00020745]
	Learning Rate: 0.000207454
	LOSS [training: 0.1869356556367205 | validation: 0.14443014529341583]
	TIME [epoch: 8.84 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16019349454026474		[learning rate: 0.00020708]
		[batch 20/20] avg loss: 0.1929898497277977		[learning rate: 0.0002067]
	Learning Rate: 0.000206701
	LOSS [training: 0.17659167213403124 | validation: 0.15165500984778216]
	TIME [epoch: 8.85 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16687998317278604		[learning rate: 0.00020633]
		[batch 20/20] avg loss: 0.1537982824235661		[learning rate: 0.00020595]
	Learning Rate: 0.000205951
	LOSS [training: 0.16033913279817608 | validation: 0.14510552691537748]
	TIME [epoch: 8.84 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16277668305949525		[learning rate: 0.00020558]
		[batch 20/20] avg loss: 0.18800147082310908		[learning rate: 0.0002052]
	Learning Rate: 0.000205203
	LOSS [training: 0.17538907694130212 | validation: 0.18036428020858172]
	TIME [epoch: 8.86 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18677631547252513		[learning rate: 0.00020483]
		[batch 20/20] avg loss: 0.16157868620587568		[learning rate: 0.00020446]
	Learning Rate: 0.000204459
	LOSS [training: 0.17417750083920047 | validation: 0.13380312949966058]
	TIME [epoch: 8.85 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17148439763906653		[learning rate: 0.00020409]
		[batch 20/20] avg loss: 0.16601030917285214		[learning rate: 0.00020372]
	Learning Rate: 0.000203717
	LOSS [training: 0.16874735340595934 | validation: 0.13953011845558805]
	TIME [epoch: 8.84 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16553591852788502		[learning rate: 0.00020335]
		[batch 20/20] avg loss: 0.17639039247229157		[learning rate: 0.00020298]
	Learning Rate: 0.000202977
	LOSS [training: 0.17096315550008828 | validation: 0.15833412126994745]
	TIME [epoch: 8.85 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17145451502484654		[learning rate: 0.00020261]
		[batch 20/20] avg loss: 0.16570934419956485		[learning rate: 0.00020224]
	Learning Rate: 0.000202241
	LOSS [training: 0.16858192961220567 | validation: 0.12111480293258982]
	TIME [epoch: 8.83 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17293224676853036		[learning rate: 0.00020187]
		[batch 20/20] avg loss: 0.18098393407805163		[learning rate: 0.00020151]
	Learning Rate: 0.000201507
	LOSS [training: 0.17695809042329097 | validation: 0.11670217053785424]
	TIME [epoch: 8.87 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17977136532614263		[learning rate: 0.00020114]
		[batch 20/20] avg loss: 0.1607431883465734		[learning rate: 0.00020078]
	Learning Rate: 0.000200775
	LOSS [training: 0.17025727683635802 | validation: 0.14872938698207988]
	TIME [epoch: 8.84 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1624740263833906		[learning rate: 0.00020041]
		[batch 20/20] avg loss: 0.18410605094145177		[learning rate: 0.00020005]
	Learning Rate: 0.000200047
	LOSS [training: 0.17329003866242118 | validation: 0.1492701881202126]
	TIME [epoch: 8.85 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17817007240438523		[learning rate: 0.00019968]
		[batch 20/20] avg loss: 0.15629639275920462		[learning rate: 0.00019932]
	Learning Rate: 0.000199321
	LOSS [training: 0.16723323258179484 | validation: 0.14172361407554068]
	TIME [epoch: 8.84 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18758451258589615		[learning rate: 0.00019896]
		[batch 20/20] avg loss: 0.14753619887625194		[learning rate: 0.0001986]
	Learning Rate: 0.000198597
	LOSS [training: 0.16756035573107403 | validation: 0.1525289753340435]
	TIME [epoch: 8.86 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17022366263622565		[learning rate: 0.00019824]
		[batch 20/20] avg loss: 0.15936494540392898		[learning rate: 0.00019788]
	Learning Rate: 0.000197877
	LOSS [training: 0.16479430402007728 | validation: 0.15187809432650107]
	TIME [epoch: 8.86 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.182920734927216		[learning rate: 0.00019752]
		[batch 20/20] avg loss: 0.15181792640268316		[learning rate: 0.00019716]
	Learning Rate: 0.000197159
	LOSS [training: 0.16736933066494958 | validation: 0.10736187247005556]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1180.pth
	Model improved!!!
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1728543198077043		[learning rate: 0.0001968]
		[batch 20/20] avg loss: 0.1499076716770357		[learning rate: 0.00019644]
	Learning Rate: 0.000196443
	LOSS [training: 0.16138099574237 | validation: 0.16902737209225244]
	TIME [epoch: 8.84 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17971578483945394		[learning rate: 0.00019609]
		[batch 20/20] avg loss: 0.157820645229519		[learning rate: 0.00019573]
	Learning Rate: 0.00019573
	LOSS [training: 0.16876821503448647 | validation: 0.1342703355318358]
	TIME [epoch: 8.84 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18827013538624054		[learning rate: 0.00019537]
		[batch 20/20] avg loss: 0.17288205126902512		[learning rate: 0.00019502]
	Learning Rate: 0.00019502
	LOSS [training: 0.18057609332763286 | validation: 0.15978611115586736]
	TIME [epoch: 8.86 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16003067793894327		[learning rate: 0.00019467]
		[batch 20/20] avg loss: 0.1778044085215106		[learning rate: 0.00019431]
	Learning Rate: 0.000194312
	LOSS [training: 0.16891754323022692 | validation: 0.16178419060448068]
	TIME [epoch: 8.85 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16956624589075797		[learning rate: 0.00019396]
		[batch 20/20] avg loss: 0.15719978319071706		[learning rate: 0.00019361]
	Learning Rate: 0.000193607
	LOSS [training: 0.1633830145407375 | validation: 0.17115655294592744]
	TIME [epoch: 8.85 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17433578753519086		[learning rate: 0.00019326]
		[batch 20/20] avg loss: 0.169132544583778		[learning rate: 0.0001929]
	Learning Rate: 0.000192904
	LOSS [training: 0.1717341660594844 | validation: 0.12545182279410805]
	TIME [epoch: 8.84 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16037126997478185		[learning rate: 0.00019255]
		[batch 20/20] avg loss: 0.164567537674033		[learning rate: 0.0001922]
	Learning Rate: 0.000192204
	LOSS [training: 0.16246940382440744 | validation: 0.15070140119233483]
	TIME [epoch: 8.85 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17517380603158		[learning rate: 0.00019186]
		[batch 20/20] avg loss: 0.17324903553309473		[learning rate: 0.00019151]
	Learning Rate: 0.000191507
	LOSS [training: 0.17421142078233737 | validation: 0.12015476990559835]
	TIME [epoch: 8.86 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16061901408099905		[learning rate: 0.00019116]
		[batch 20/20] avg loss: 0.18095429594009857		[learning rate: 0.00019081]
	Learning Rate: 0.000190812
	LOSS [training: 0.17078665501054882 | validation: 0.13830893267165667]
	TIME [epoch: 8.85 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1780791498109383		[learning rate: 0.00019047]
		[batch 20/20] avg loss: 0.17421981064097697		[learning rate: 0.00019012]
	Learning Rate: 0.000190119
	LOSS [training: 0.17614948022595764 | validation: 0.1341494205343346]
	TIME [epoch: 8.84 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1622691388182639		[learning rate: 0.00018977]
		[batch 20/20] avg loss: 0.1523386055069984		[learning rate: 0.00018943]
	Learning Rate: 0.000189429
	LOSS [training: 0.15730387216263117 | validation: 0.1758767742506934]
	TIME [epoch: 8.85 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1788154322633719		[learning rate: 0.00018909]
		[batch 20/20] avg loss: 0.17065521833673097		[learning rate: 0.00018874]
	Learning Rate: 0.000188742
	LOSS [training: 0.1747353253000514 | validation: 0.19829246440655762]
	TIME [epoch: 8.86 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16800230871002342		[learning rate: 0.0001884]
		[batch 20/20] avg loss: 0.1635954018313734		[learning rate: 0.00018806]
	Learning Rate: 0.000188057
	LOSS [training: 0.16579885527069838 | validation: 0.12887792336054643]
	TIME [epoch: 8.85 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13969924041451579		[learning rate: 0.00018772]
		[batch 20/20] avg loss: 0.16707557469717166		[learning rate: 0.00018737]
	Learning Rate: 0.000187375
	LOSS [training: 0.15338740755584368 | validation: 0.1512193881437976]
	TIME [epoch: 8.84 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1665454009208892		[learning rate: 0.00018703]
		[batch 20/20] avg loss: 0.1651667588642354		[learning rate: 0.00018669]
	Learning Rate: 0.000186695
	LOSS [training: 0.16585607989256226 | validation: 0.15350843020044308]
	TIME [epoch: 8.85 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14393825774774655		[learning rate: 0.00018636]
		[batch 20/20] avg loss: 0.18262086481460843		[learning rate: 0.00018602]
	Learning Rate: 0.000186017
	LOSS [training: 0.16327956128117752 | validation: 0.12034441378042977]
	TIME [epoch: 8.84 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1491856173533914		[learning rate: 0.00018568]
		[batch 20/20] avg loss: 0.15435040437658848		[learning rate: 0.00018534]
	Learning Rate: 0.000185342
	LOSS [training: 0.15176801086498995 | validation: 0.12463544330219654]
	TIME [epoch: 8.86 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17941435019858543		[learning rate: 0.00018501]
		[batch 20/20] avg loss: 0.16044006481427062		[learning rate: 0.00018467]
	Learning Rate: 0.000184669
	LOSS [training: 0.16992720750642804 | validation: 0.13303204067639496]
	TIME [epoch: 8.84 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.164030813035876		[learning rate: 0.00018433]
		[batch 20/20] avg loss: 0.1608018882346351		[learning rate: 0.000184]
	Learning Rate: 0.000183999
	LOSS [training: 0.16241635063525556 | validation: 0.20591231462826345]
	TIME [epoch: 8.85 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1803750654033194		[learning rate: 0.00018367]
		[batch 20/20] avg loss: 0.17135684805618803		[learning rate: 0.00018333]
	Learning Rate: 0.000183331
	LOSS [training: 0.17586595672975372 | validation: 0.1321447179308864]
	TIME [epoch: 8.84 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18876501272816032		[learning rate: 0.000183]
		[batch 20/20] avg loss: 0.15288595673186262		[learning rate: 0.00018267]
	Learning Rate: 0.000182666
	LOSS [training: 0.17082548473001144 | validation: 0.12872959982318435]
	TIME [epoch: 8.85 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1547434137630282		[learning rate: 0.00018233]
		[batch 20/20] avg loss: 0.14909319660514414		[learning rate: 0.000182]
	Learning Rate: 0.000182003
	LOSS [training: 0.15191830518408614 | validation: 0.1697235010871827]
	TIME [epoch: 8.86 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16697486761740804		[learning rate: 0.00018167]
		[batch 20/20] avg loss: 0.17563056837203644		[learning rate: 0.00018134]
	Learning Rate: 0.000181343
	LOSS [training: 0.17130271799472224 | validation: 0.16932243980215325]
	TIME [epoch: 8.85 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15647750753444037		[learning rate: 0.00018101]
		[batch 20/20] avg loss: 0.17941443838967247		[learning rate: 0.00018068]
	Learning Rate: 0.000180685
	LOSS [training: 0.16794597296205643 | validation: 0.14785653112173666]
	TIME [epoch: 8.84 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17757615260787804		[learning rate: 0.00018036]
		[batch 20/20] avg loss: 0.14847336771670083		[learning rate: 0.00018003]
	Learning Rate: 0.000180029
	LOSS [training: 0.16302476016228945 | validation: 0.14704292889899925]
	TIME [epoch: 8.84 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1658438743848516		[learning rate: 0.0001797]
		[batch 20/20] avg loss: 0.15601760402568302		[learning rate: 0.00017938]
	Learning Rate: 0.000179376
	LOSS [training: 0.1609307392052673 | validation: 0.1365446061184211]
	TIME [epoch: 8.85 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1803693325035493		[learning rate: 0.00017905]
		[batch 20/20] avg loss: 0.17421576659541		[learning rate: 0.00017872]
	Learning Rate: 0.000178725
	LOSS [training: 0.17729254954947965 | validation: 0.11949197229939856]
	TIME [epoch: 8.86 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1656647536015061		[learning rate: 0.0001784]
		[batch 20/20] avg loss: 0.15965882632069944		[learning rate: 0.00017808]
	Learning Rate: 0.000178076
	LOSS [training: 0.16266178996110275 | validation: 0.11838828605947772]
	TIME [epoch: 8.84 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16243145874844828		[learning rate: 0.00017775]
		[batch 20/20] avg loss: 0.18111408240412213		[learning rate: 0.00017743]
	Learning Rate: 0.00017743
	LOSS [training: 0.1717727705762852 | validation: 0.15746571474213114]
	TIME [epoch: 8.84 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15049431412062259		[learning rate: 0.00017711]
		[batch 20/20] avg loss: 0.17663754501953377		[learning rate: 0.00017679]
	Learning Rate: 0.000176786
	LOSS [training: 0.16356592957007818 | validation: 0.142116244881829]
	TIME [epoch: 8.84 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15800434063263197		[learning rate: 0.00017646]
		[batch 20/20] avg loss: 0.1869589364723266		[learning rate: 0.00017614]
	Learning Rate: 0.000176144
	LOSS [training: 0.17248163855247928 | validation: 0.16173228469610376]
	TIME [epoch: 8.87 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14511739250563357		[learning rate: 0.00017582]
		[batch 20/20] avg loss: 0.15867629892343152		[learning rate: 0.0001755]
	Learning Rate: 0.000175505
	LOSS [training: 0.15189684571453252 | validation: 0.1398039525860775]
	TIME [epoch: 8.85 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16903613408050194		[learning rate: 0.00017519]
		[batch 20/20] avg loss: 0.16215369923761216		[learning rate: 0.00017487]
	Learning Rate: 0.000174868
	LOSS [training: 0.16559491665905707 | validation: 0.16685514046653604]
	TIME [epoch: 8.84 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18577133185694095		[learning rate: 0.00017455]
		[batch 20/20] avg loss: 0.18722722816055876		[learning rate: 0.00017423]
	Learning Rate: 0.000174233
	LOSS [training: 0.18649928000874982 | validation: 0.11018268920402871]
	TIME [epoch: 8.85 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1438431422912477		[learning rate: 0.00017392]
		[batch 20/20] avg loss: 0.16069272042183097		[learning rate: 0.0001736]
	Learning Rate: 0.000173601
	LOSS [training: 0.15226793135653932 | validation: 0.1531998197256073]
	TIME [epoch: 8.84 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1816141790572647		[learning rate: 0.00017329]
		[batch 20/20] avg loss: 0.1819799755390906		[learning rate: 0.00017297]
	Learning Rate: 0.000172971
	LOSS [training: 0.18179707729817768 | validation: 0.1435492676119173]
	TIME [epoch: 8.87 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1852848298316469		[learning rate: 0.00017266]
		[batch 20/20] avg loss: 0.16242691173149815		[learning rate: 0.00017234]
	Learning Rate: 0.000172343
	LOSS [training: 0.17385587078157252 | validation: 0.15689111156398744]
	TIME [epoch: 8.85 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16517245455402046		[learning rate: 0.00017203]
		[batch 20/20] avg loss: 0.16909115805196212		[learning rate: 0.00017172]
	Learning Rate: 0.000171718
	LOSS [training: 0.16713180630299124 | validation: 0.11506219045688702]
	TIME [epoch: 8.85 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14874068525911338		[learning rate: 0.00017141]
		[batch 20/20] avg loss: 0.16305609364763748		[learning rate: 0.00017109]
	Learning Rate: 0.000171095
	LOSS [training: 0.15589838945337542 | validation: 0.11804426857052347]
	TIME [epoch: 8.84 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1652084718635762		[learning rate: 0.00017078]
		[batch 20/20] avg loss: 0.1862186651924826		[learning rate: 0.00017047]
	Learning Rate: 0.000170474
	LOSS [training: 0.17571356852802938 | validation: 0.1439407428710143]
	TIME [epoch: 8.84 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1795616926822034		[learning rate: 0.00017016]
		[batch 20/20] avg loss: 0.20283672828906982		[learning rate: 0.00016986]
	Learning Rate: 0.000169855
	LOSS [training: 0.19119921048563662 | validation: 0.19453156647716607]
	TIME [epoch: 8.86 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19216616758597985		[learning rate: 0.00016955]
		[batch 20/20] avg loss: 0.17611099675470115		[learning rate: 0.00016924]
	Learning Rate: 0.000169239
	LOSS [training: 0.18413858217034046 | validation: 0.2327366968156355]
	TIME [epoch: 8.84 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19284456839194916		[learning rate: 0.00016893]
		[batch 20/20] avg loss: 0.16443923323712986		[learning rate: 0.00016862]
	Learning Rate: 0.000168625
	LOSS [training: 0.17864190081453948 | validation: 0.12483855459094591]
	TIME [epoch: 8.84 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16250036385658992		[learning rate: 0.00016832]
		[batch 20/20] avg loss: 0.149208353078292		[learning rate: 0.00016801]
	Learning Rate: 0.000168013
	LOSS [training: 0.15585435846744097 | validation: 0.12093347301032015]
	TIME [epoch: 8.84 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1546213296438281		[learning rate: 0.00016771]
		[batch 20/20] avg loss: 0.16797896298421539		[learning rate: 0.0001674]
	Learning Rate: 0.000167403
	LOSS [training: 0.16130014631402173 | validation: 0.11419971849671454]
	TIME [epoch: 8.86 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14193894911368843		[learning rate: 0.0001671]
		[batch 20/20] avg loss: 0.18093329472066141		[learning rate: 0.0001668]
	Learning Rate: 0.000166795
	LOSS [training: 0.1614361219171749 | validation: 0.1638441732405003]
	TIME [epoch: 8.85 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1740537981512848		[learning rate: 0.00016649]
		[batch 20/20] avg loss: 0.13096317376302488		[learning rate: 0.00016619]
	Learning Rate: 0.00016619
	LOSS [training: 0.15250848595715483 | validation: 0.1251454783014617]
	TIME [epoch: 8.83 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16385257197951292		[learning rate: 0.00016589]
		[batch 20/20] avg loss: 0.15774928547876838		[learning rate: 0.00016559]
	Learning Rate: 0.000165587
	LOSS [training: 0.16080092872914065 | validation: 0.13469157524167552]
	TIME [epoch: 8.84 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14731137963287944		[learning rate: 0.00016529]
		[batch 20/20] avg loss: 0.17258743112933342		[learning rate: 0.00016499]
	Learning Rate: 0.000164986
	LOSS [training: 0.1599494053811064 | validation: 0.1384243996379807]
	TIME [epoch: 8.84 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16430414311809624		[learning rate: 0.00016469]
		[batch 20/20] avg loss: 0.16593125503362446		[learning rate: 0.00016439]
	Learning Rate: 0.000164387
	LOSS [training: 0.16511769907586035 | validation: 0.15092269465850755]
	TIME [epoch: 8.86 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14550089413517703		[learning rate: 0.00016409]
		[batch 20/20] avg loss: 0.16616360091466906		[learning rate: 0.00016379]
	Learning Rate: 0.000163791
	LOSS [training: 0.15583224752492303 | validation: 0.13421046026405406]
	TIME [epoch: 8.85 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16655042402664227		[learning rate: 0.00016349]
		[batch 20/20] avg loss: 0.156200120089991		[learning rate: 0.0001632]
	Learning Rate: 0.000163196
	LOSS [training: 0.1613752720583166 | validation: 0.12696223276685015]
	TIME [epoch: 8.84 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1740920650707073		[learning rate: 0.0001629]
		[batch 20/20] avg loss: 0.15357485400082857		[learning rate: 0.0001626]
	Learning Rate: 0.000162604
	LOSS [training: 0.16383345953576792 | validation: 0.15204221560981168]
	TIME [epoch: 8.85 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15586862551512146		[learning rate: 0.00016231]
		[batch 20/20] avg loss: 0.14731792368746036		[learning rate: 0.00016201]
	Learning Rate: 0.000162014
	LOSS [training: 0.1515932746012909 | validation: 0.1497441973138116]
	TIME [epoch: 8.84 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15342247561698988		[learning rate: 0.00016172]
		[batch 20/20] avg loss: 0.16563466450569558		[learning rate: 0.00016143]
	Learning Rate: 0.000161426
	LOSS [training: 0.15952857006134272 | validation: 0.13962657529491362]
	TIME [epoch: 8.86 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15668658828733065		[learning rate: 0.00016113]
		[batch 20/20] avg loss: 0.1537248306486794		[learning rate: 0.00016084]
	Learning Rate: 0.00016084
	LOSS [training: 0.15520570946800502 | validation: 0.14930784000087724]
	TIME [epoch: 8.84 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15271018082279383		[learning rate: 0.00016055]
		[batch 20/20] avg loss: 0.17372219358129445		[learning rate: 0.00016026]
	Learning Rate: 0.000160257
	LOSS [training: 0.16321618720204414 | validation: 0.12231280975033368]
	TIME [epoch: 8.84 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14422339373093954		[learning rate: 0.00015997]
		[batch 20/20] avg loss: 0.17485554753848637		[learning rate: 0.00015967]
	Learning Rate: 0.000159675
	LOSS [training: 0.15953947063471294 | validation: 0.11305991460311163]
	TIME [epoch: 8.84 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16241996117829266		[learning rate: 0.00015938]
		[batch 20/20] avg loss: 0.15443118604563139		[learning rate: 0.0001591]
	Learning Rate: 0.000159096
	LOSS [training: 0.15842557361196202 | validation: 0.1277367103770171]
	TIME [epoch: 8.85 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14876310165029913		[learning rate: 0.00015881]
		[batch 20/20] avg loss: 0.16439002234218927		[learning rate: 0.00015852]
	Learning Rate: 0.000158518
	LOSS [training: 0.15657656199624423 | validation: 0.12149715336304752]
	TIME [epoch: 8.85 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1590993844734496		[learning rate: 0.00015823]
		[batch 20/20] avg loss: 0.1508231251572259		[learning rate: 0.00015794]
	Learning Rate: 0.000157943
	LOSS [training: 0.15496125481533776 | validation: 0.13290095826242604]
	TIME [epoch: 8.85 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17424185831980799		[learning rate: 0.00015766]
		[batch 20/20] avg loss: 0.157232678125041		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.16573726822242452 | validation: 0.16052838278939238]
	TIME [epoch: 8.84 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1587894348067219		[learning rate: 0.00015708]
		[batch 20/20] avg loss: 0.1573010798995086		[learning rate: 0.0001568]
	Learning Rate: 0.000156799
	LOSS [training: 0.15804525735311525 | validation: 0.17439527510579528]
	TIME [epoch: 8.84 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15418643020772826		[learning rate: 0.00015651]
		[batch 20/20] avg loss: 0.14968178007585486		[learning rate: 0.00015623]
	Learning Rate: 0.00015623
	LOSS [training: 0.1519341051417916 | validation: 0.13663964580187637]
	TIME [epoch: 8.86 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15163716644673927		[learning rate: 0.00015595]
		[batch 20/20] avg loss: 0.16371165499971133		[learning rate: 0.00015566]
	Learning Rate: 0.000155663
	LOSS [training: 0.15767441072322533 | validation: 0.12444283093584024]
	TIME [epoch: 8.85 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15779017409272944		[learning rate: 0.00015538]
		[batch 20/20] avg loss: 0.1674754461419169		[learning rate: 0.0001551]
	Learning Rate: 0.000155098
	LOSS [training: 0.16263281011732317 | validation: 0.09950083019844418]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1246.pth
	Model improved!!!
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1534351147621562		[learning rate: 0.00015482]
		[batch 20/20] avg loss: 0.1696863838827362		[learning rate: 0.00015453]
	Learning Rate: 0.000154535
	LOSS [training: 0.1615607493224462 | validation: 0.11171472633659653]
	TIME [epoch: 8.84 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16848571312269484		[learning rate: 0.00015425]
		[batch 20/20] avg loss: 0.1455581585469442		[learning rate: 0.00015397]
	Learning Rate: 0.000153974
	LOSS [training: 0.15702193583481952 | validation: 0.16440596259015985]
	TIME [epoch: 8.84 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15018000738007434		[learning rate: 0.00015369]
		[batch 20/20] avg loss: 0.16584031819772602		[learning rate: 0.00015342]
	Learning Rate: 0.000153415
	LOSS [training: 0.15801016278890018 | validation: 0.15547550874522031]
	TIME [epoch: 8.85 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18512116091301536		[learning rate: 0.00015314]
		[batch 20/20] avg loss: 0.17741042224758052		[learning rate: 0.00015286]
	Learning Rate: 0.000152858
	LOSS [training: 0.18126579158029793 | validation: 0.1268778273817917]
	TIME [epoch: 8.84 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16133534689184206		[learning rate: 0.00015258]
		[batch 20/20] avg loss: 0.13728821298270696		[learning rate: 0.0001523]
	Learning Rate: 0.000152304
	LOSS [training: 0.14931177993727446 | validation: 0.11597081001858366]
	TIME [epoch: 8.84 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16517151029447524		[learning rate: 0.00015203]
		[batch 20/20] avg loss: 0.16437457352739662		[learning rate: 0.00015175]
	Learning Rate: 0.000151751
	LOSS [training: 0.16477304191093597 | validation: 0.12804389505068203]
	TIME [epoch: 8.84 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13770998448510147		[learning rate: 0.00015148]
		[batch 20/20] avg loss: 0.1583480957421701		[learning rate: 0.0001512]
	Learning Rate: 0.0001512
	LOSS [training: 0.14802904011363577 | validation: 0.1208764243712733]
	TIME [epoch: 8.85 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14321933709821286		[learning rate: 0.00015093]
		[batch 20/20] avg loss: 0.16513472414736347		[learning rate: 0.00015065]
	Learning Rate: 0.000150652
	LOSS [training: 0.15417703062278815 | validation: 0.1814590074521887]
	TIME [epoch: 8.85 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1759094940042047		[learning rate: 0.00015038]
		[batch 20/20] avg loss: 0.16166630192296666		[learning rate: 0.0001501]
	Learning Rate: 0.000150105
	LOSS [training: 0.16878789796358573 | validation: 0.11507555693133929]
	TIME [epoch: 8.84 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15321414317972373		[learning rate: 0.00014983]
		[batch 20/20] avg loss: 0.17472329864793743		[learning rate: 0.00014956]
	Learning Rate: 0.00014956
	LOSS [training: 0.1639687209138306 | validation: 0.15145357582451216]
	TIME [epoch: 8.84 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13692291010307403		[learning rate: 0.00014929]
		[batch 20/20] avg loss: 0.1770026609269813		[learning rate: 0.00014902]
	Learning Rate: 0.000149017
	LOSS [training: 0.15696278551502768 | validation: 0.14439015358601562]
	TIME [epoch: 8.84 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1496387209061926		[learning rate: 0.00014875]
		[batch 20/20] avg loss: 0.14896792037818302		[learning rate: 0.00014848]
	Learning Rate: 0.000148477
	LOSS [training: 0.1493033206421878 | validation: 0.1363730294472545]
	TIME [epoch: 8.87 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17672829023509695		[learning rate: 0.00014821]
		[batch 20/20] avg loss: 0.16778895038769231		[learning rate: 0.00014794]
	Learning Rate: 0.000147938
	LOSS [training: 0.17225862031139466 | validation: 0.1385092383747392]
	TIME [epoch: 8.85 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15722739090902887		[learning rate: 0.00014767]
		[batch 20/20] avg loss: 0.16900521760857964		[learning rate: 0.0001474]
	Learning Rate: 0.000147401
	LOSS [training: 0.16311630425880425 | validation: 0.13627459491577842]
	TIME [epoch: 8.84 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15509420890421488		[learning rate: 0.00014713]
		[batch 20/20] avg loss: 0.15285481830535075		[learning rate: 0.00014687]
	Learning Rate: 0.000146866
	LOSS [training: 0.15397451360478281 | validation: 0.11613861554349279]
	TIME [epoch: 8.84 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17052382550504278		[learning rate: 0.0001466]
		[batch 20/20] avg loss: 0.1655528167294282		[learning rate: 0.00014633]
	Learning Rate: 0.000146333
	LOSS [training: 0.16803832111723546 | validation: 0.10187889515163621]
	TIME [epoch: 8.84 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14659760008836004		[learning rate: 0.00014607]
		[batch 20/20] avg loss: 0.1577615573223194		[learning rate: 0.0001458]
	Learning Rate: 0.000145802
	LOSS [training: 0.15217957870533974 | validation: 0.15405783880966445]
	TIME [epoch: 8.86 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1618633289703953		[learning rate: 0.00014554]
		[batch 20/20] avg loss: 0.15020384550332708		[learning rate: 0.00014527]
	Learning Rate: 0.000145273
	LOSS [training: 0.1560335872368612 | validation: 0.153190985493147]
	TIME [epoch: 8.84 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18458373818363147		[learning rate: 0.00014501]
		[batch 20/20] avg loss: 0.12932452132289465		[learning rate: 0.00014475]
	Learning Rate: 0.000144746
	LOSS [training: 0.15695412975326303 | validation: 0.13770723594890819]
	TIME [epoch: 8.84 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14243341066297402		[learning rate: 0.00014448]
		[batch 20/20] avg loss: 0.1807778538448221		[learning rate: 0.00014422]
	Learning Rate: 0.00014422
	LOSS [training: 0.161605632253898 | validation: 0.1432305022934822]
	TIME [epoch: 8.84 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16744649673537343		[learning rate: 0.00014396]
		[batch 20/20] avg loss: 0.16373658430433108		[learning rate: 0.0001437]
	Learning Rate: 0.000143697
	LOSS [training: 0.16559154051985228 | validation: 0.131465925158222]
	TIME [epoch: 8.84 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1824216101458272		[learning rate: 0.00014344]
		[batch 20/20] avg loss: 0.1579641288647733		[learning rate: 0.00014318]
	Learning Rate: 0.000143175
	LOSS [training: 0.17019286950530021 | validation: 0.15397565750600783]
	TIME [epoch: 8.86 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15534312410560033		[learning rate: 0.00014292]
		[batch 20/20] avg loss: 0.15147090617665154		[learning rate: 0.00014266]
	Learning Rate: 0.000142656
	LOSS [training: 0.15340701514112598 | validation: 0.1239504143983152]
	TIME [epoch: 8.83 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15162355800529953		[learning rate: 0.0001424]
		[batch 20/20] avg loss: 0.15729312838571533		[learning rate: 0.00014214]
	Learning Rate: 0.000142138
	LOSS [training: 0.15445834319550744 | validation: 0.14381337225124008]
	TIME [epoch: 8.84 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1591304408153587		[learning rate: 0.00014188]
		[batch 20/20] avg loss: 0.1472803693363118		[learning rate: 0.00014162]
	Learning Rate: 0.000141622
	LOSS [training: 0.15320540507583522 | validation: 0.13391579991186192]
	TIME [epoch: 8.85 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1535137603018845		[learning rate: 0.00014137]
		[batch 20/20] avg loss: 0.1419598961380114		[learning rate: 0.00014111]
	Learning Rate: 0.000141108
	LOSS [training: 0.14773682821994796 | validation: 0.12116369166906653]
	TIME [epoch: 8.86 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1836384539844969		[learning rate: 0.00014085]
		[batch 20/20] avg loss: 0.1770177399273726		[learning rate: 0.0001406]
	Learning Rate: 0.000140596
	LOSS [training: 0.18032809695593474 | validation: 0.14526578883154748]
	TIME [epoch: 8.85 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14770469767020833		[learning rate: 0.00014034]
		[batch 20/20] avg loss: 0.16466342874155104		[learning rate: 0.00014009]
	Learning Rate: 0.000140086
	LOSS [training: 0.1561840632058797 | validation: 0.11456012117183836]
	TIME [epoch: 8.84 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15185368088942944		[learning rate: 0.00013983]
		[batch 20/20] avg loss: 0.15021500097358448		[learning rate: 0.00013958]
	Learning Rate: 0.000139578
	LOSS [training: 0.15103434093150697 | validation: 0.1304594575983231]
	TIME [epoch: 8.84 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1464791216286561		[learning rate: 0.00013932]
		[batch 20/20] avg loss: 0.14694712353008826		[learning rate: 0.00013907]
	Learning Rate: 0.000139071
	LOSS [training: 0.14671312257937216 | validation: 0.13094904941885244]
	TIME [epoch: 8.84 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16429145716508006		[learning rate: 0.00013882]
		[batch 20/20] avg loss: 0.15069796242898142		[learning rate: 0.00013857]
	Learning Rate: 0.000138566
	LOSS [training: 0.15749470979703076 | validation: 0.1556611064943278]
	TIME [epoch: 8.86 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16456604530472124		[learning rate: 0.00013831]
		[batch 20/20] avg loss: 0.16058373353352834		[learning rate: 0.00013806]
	Learning Rate: 0.000138064
	LOSS [training: 0.16257488941912476 | validation: 0.16378435820755577]
	TIME [epoch: 8.84 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1486907267762185		[learning rate: 0.00013781]
		[batch 20/20] avg loss: 0.15589237528841288		[learning rate: 0.00013756]
	Learning Rate: 0.000137562
	LOSS [training: 0.1522915510323157 | validation: 0.11099604423720565]
	TIME [epoch: 8.84 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1788871425624889		[learning rate: 0.00013731]
		[batch 20/20] avg loss: 0.17389964084213022		[learning rate: 0.00013706]
	Learning Rate: 0.000137063
	LOSS [training: 0.17639339170230955 | validation: 0.1349998758893676]
	TIME [epoch: 8.84 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16477869227536762		[learning rate: 0.00013681]
		[batch 20/20] avg loss: 0.15865434930131178		[learning rate: 0.00013657]
	Learning Rate: 0.000136566
	LOSS [training: 0.16171652078833973 | validation: 0.14614473266165567]
	TIME [epoch: 8.91 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1712097269091692		[learning rate: 0.00013632]
		[batch 20/20] avg loss: 0.15913511682930018		[learning rate: 0.00013607]
	Learning Rate: 0.00013607
	LOSS [training: 0.16517242186923473 | validation: 0.1189936070050436]
	TIME [epoch: 8.86 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1812168212586452		[learning rate: 0.00013582]
		[batch 20/20] avg loss: 0.1795732339604182		[learning rate: 0.00013558]
	Learning Rate: 0.000135576
	LOSS [training: 0.1803950276095317 | validation: 0.1414857662791218]
	TIME [epoch: 8.85 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17168819196138413		[learning rate: 0.00013533]
		[batch 20/20] avg loss: 0.15659637828742218		[learning rate: 0.00013508]
	Learning Rate: 0.000135084
	LOSS [training: 0.16414228512440315 | validation: 0.12646214408553513]
	TIME [epoch: 8.84 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15541756828705294		[learning rate: 0.00013484]
		[batch 20/20] avg loss: 0.1634883573484856		[learning rate: 0.00013459]
	Learning Rate: 0.000134594
	LOSS [training: 0.15945296281776927 | validation: 0.16141965262369742]
	TIME [epoch: 8.85 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17391402891416058		[learning rate: 0.00013435]
		[batch 20/20] avg loss: 0.16670201273660185		[learning rate: 0.00013411]
	Learning Rate: 0.000134106
	LOSS [training: 0.17030802082538118 | validation: 0.11258583556725177]
	TIME [epoch: 8.86 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14041242910432777		[learning rate: 0.00013386]
		[batch 20/20] avg loss: 0.15685172227794272		[learning rate: 0.00013362]
	Learning Rate: 0.000133619
	LOSS [training: 0.1486320756911353 | validation: 0.14416559877783938]
	TIME [epoch: 8.84 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15511913699855842		[learning rate: 0.00013338]
		[batch 20/20] avg loss: 0.15454359452102845		[learning rate: 0.00013313]
	Learning Rate: 0.000133134
	LOSS [training: 0.1548313657597934 | validation: 0.13896014743038487]
	TIME [epoch: 8.84 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1633753441891148		[learning rate: 0.00013289]
		[batch 20/20] avg loss: 0.1386510302701645		[learning rate: 0.00013265]
	Learning Rate: 0.000132651
	LOSS [training: 0.15101318722963966 | validation: 0.13109482636341552]
	TIME [epoch: 8.84 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.159431502029242		[learning rate: 0.00013241]
		[batch 20/20] avg loss: 0.15414611502605374		[learning rate: 0.00013217]
	Learning Rate: 0.00013217
	LOSS [training: 0.15678880852764793 | validation: 0.10875466846460241]
	TIME [epoch: 8.84 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15413252817110798		[learning rate: 0.00013193]
		[batch 20/20] avg loss: 0.14965775743415388		[learning rate: 0.00013169]
	Learning Rate: 0.00013169
	LOSS [training: 0.15189514280263092 | validation: 0.14022591845098095]
	TIME [epoch: 8.87 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15420135342707467		[learning rate: 0.00013145]
		[batch 20/20] avg loss: 0.16260574573052475		[learning rate: 0.00013121]
	Learning Rate: 0.000131212
	LOSS [training: 0.1584035495787997 | validation: 0.11047687275972996]
	TIME [epoch: 8.84 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15430402974503205		[learning rate: 0.00013097]
		[batch 20/20] avg loss: 0.167271908909235		[learning rate: 0.00013074]
	Learning Rate: 0.000130736
	LOSS [training: 0.16078796932713352 | validation: 0.1276626517663836]
	TIME [epoch: 8.84 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16659373185898013		[learning rate: 0.0001305]
		[batch 20/20] avg loss: 0.15194351816861393		[learning rate: 0.00013026]
	Learning Rate: 0.000130261
	LOSS [training: 0.15926862501379704 | validation: 0.12328910316091027]
	TIME [epoch: 8.84 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18827077240061524		[learning rate: 0.00013002]
		[batch 20/20] avg loss: 0.16147555036749023		[learning rate: 0.00012979]
	Learning Rate: 0.000129789
	LOSS [training: 0.17487316138405276 | validation: 0.13133089004850507]
	TIME [epoch: 8.84 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16091345888015085		[learning rate: 0.00012955]
		[batch 20/20] avg loss: 0.14734654678905035		[learning rate: 0.00012932]
	Learning Rate: 0.000129318
	LOSS [training: 0.1541300028346006 | validation: 0.13306661034899459]
	TIME [epoch: 8.86 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14032047929406583		[learning rate: 0.00012908]
		[batch 20/20] avg loss: 0.15699934855885483		[learning rate: 0.00012885]
	Learning Rate: 0.000128848
	LOSS [training: 0.14865991392646036 | validation: 0.11755631370304356]
	TIME [epoch: 8.85 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13849317650745502		[learning rate: 0.00012861]
		[batch 20/20] avg loss: 0.15816887682005548		[learning rate: 0.00012838]
	Learning Rate: 0.000128381
	LOSS [training: 0.14833102666375525 | validation: 0.11329967172484731]
	TIME [epoch: 8.83 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1454893365780738		[learning rate: 0.00012815]
		[batch 20/20] avg loss: 0.16595669410633976		[learning rate: 0.00012791]
	Learning Rate: 0.000127915
	LOSS [training: 0.15572301534220676 | validation: 0.12365688596733138]
	TIME [epoch: 8.84 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1428321597238613		[learning rate: 0.00012768]
		[batch 20/20] avg loss: 0.1589724679828866		[learning rate: 0.00012745]
	Learning Rate: 0.000127451
	LOSS [training: 0.15090231385337394 | validation: 0.13407282125157333]
	TIME [epoch: 8.85 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15360880471377986		[learning rate: 0.00012722]
		[batch 20/20] avg loss: 0.1656319678639365		[learning rate: 0.00012699]
	Learning Rate: 0.000126988
	LOSS [training: 0.15962038628885816 | validation: 0.1277622961753379]
	TIME [epoch: 8.85 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1652047511129855		[learning rate: 0.00012676]
		[batch 20/20] avg loss: 0.13887927519469856		[learning rate: 0.00012653]
	Learning Rate: 0.000126527
	LOSS [training: 0.15204201315384203 | validation: 0.14377091502808073]
	TIME [epoch: 8.83 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14200880142989397		[learning rate: 0.0001263]
		[batch 20/20] avg loss: 0.16768003646740504		[learning rate: 0.00012607]
	Learning Rate: 0.000126068
	LOSS [training: 0.15484441894864948 | validation: 0.12298255693305311]
	TIME [epoch: 8.84 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15816900985150703		[learning rate: 0.00012584]
		[batch 20/20] avg loss: 0.14673753276342497		[learning rate: 0.00012561]
	Learning Rate: 0.000125611
	LOSS [training: 0.152453271307466 | validation: 0.1312683069973548]
	TIME [epoch: 8.84 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14692206563238294		[learning rate: 0.00012538]
		[batch 20/20] avg loss: 0.16621262082338134		[learning rate: 0.00012515]
	Learning Rate: 0.000125155
	LOSS [training: 0.15656734322788213 | validation: 0.1296165547480001]
	TIME [epoch: 8.85 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1602148166340493		[learning rate: 0.00012493]
		[batch 20/20] avg loss: 0.1269275105398661		[learning rate: 0.0001247]
	Learning Rate: 0.000124701
	LOSS [training: 0.1435711635869577 | validation: 0.13834322917526531]
	TIME [epoch: 8.83 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14347945807191387		[learning rate: 0.00012447]
		[batch 20/20] avg loss: 0.1616292436201221		[learning rate: 0.00012425]
	Learning Rate: 0.000124248
	LOSS [training: 0.15255435084601798 | validation: 0.14205184270466137]
	TIME [epoch: 8.82 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14630547152203033		[learning rate: 0.00012402]
		[batch 20/20] avg loss: 0.15870481633799355		[learning rate: 0.0001238]
	Learning Rate: 0.000123797
	LOSS [training: 0.15250514393001188 | validation: 0.12427999032405738]
	TIME [epoch: 8.83 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14967562149272576		[learning rate: 0.00012357]
		[batch 20/20] avg loss: 0.1550554244680257		[learning rate: 0.00012335]
	Learning Rate: 0.000123348
	LOSS [training: 0.1523655229803757 | validation: 0.12000886286225963]
	TIME [epoch: 8.84 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16734004695101595		[learning rate: 0.00012312]
		[batch 20/20] avg loss: 0.157464447407517		[learning rate: 0.0001229]
	Learning Rate: 0.0001229
	LOSS [training: 0.16240224717926646 | validation: 0.1120646557269667]
	TIME [epoch: 8.86 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13955094883267213		[learning rate: 0.00012268]
		[batch 20/20] avg loss: 0.13983608382704457		[learning rate: 0.00012245]
	Learning Rate: 0.000122454
	LOSS [training: 0.13969351632985835 | validation: 0.13055425423725825]
	TIME [epoch: 8.84 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14571277962969728		[learning rate: 0.00012223]
		[batch 20/20] avg loss: 0.1448249234966674		[learning rate: 0.00012201]
	Learning Rate: 0.00012201
	LOSS [training: 0.1452688515631824 | validation: 0.17491773716962628]
	TIME [epoch: 8.84 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15541725394617553		[learning rate: 0.00012179]
		[batch 20/20] avg loss: 0.14565600818425412		[learning rate: 0.00012157]
	Learning Rate: 0.000121567
	LOSS [training: 0.15053663106521487 | validation: 0.12443719959683078]
	TIME [epoch: 8.84 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15123573661171508		[learning rate: 0.00012135]
		[batch 20/20] avg loss: 0.1402110660409614		[learning rate: 0.00012113]
	Learning Rate: 0.000121126
	LOSS [training: 0.14572340132633826 | validation: 0.18871757227269567]
	TIME [epoch: 8.83 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14906574328646843		[learning rate: 0.00012091]
		[batch 20/20] avg loss: 0.14710350814804013		[learning rate: 0.00012069]
	Learning Rate: 0.000120686
	LOSS [training: 0.1480846257172543 | validation: 0.10414318627243986]
	TIME [epoch: 8.86 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14056287383930707		[learning rate: 0.00012047]
		[batch 20/20] avg loss: 0.17269351632692914		[learning rate: 0.00012025]
	Learning Rate: 0.000120248
	LOSS [training: 0.15662819508311812 | validation: 0.1336031107731225]
	TIME [epoch: 8.83 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15487218092871977		[learning rate: 0.00012003]
		[batch 20/20] avg loss: 0.15626346033384483		[learning rate: 0.00011981]
	Learning Rate: 0.000119812
	LOSS [training: 0.15556782063128227 | validation: 0.12232651611860908]
	TIME [epoch: 8.84 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13674553812148543		[learning rate: 0.00011959]
		[batch 20/20] avg loss: 0.15442925817343522		[learning rate: 0.00011938]
	Learning Rate: 0.000119377
	LOSS [training: 0.14558739814746033 | validation: 0.15911297463147753]
	TIME [epoch: 8.84 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14042510134866157		[learning rate: 0.00011916]
		[batch 20/20] avg loss: 0.1457601494391578		[learning rate: 0.00011894]
	Learning Rate: 0.000118944
	LOSS [training: 0.1430926253939097 | validation: 0.1260658857956012]
	TIME [epoch: 8.85 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14082401231252023		[learning rate: 0.00011873]
		[batch 20/20] avg loss: 0.15003238339511407		[learning rate: 0.00011851]
	Learning Rate: 0.000118512
	LOSS [training: 0.14542819785381716 | validation: 0.1581149077875768]
	TIME [epoch: 8.83 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14910016997200937		[learning rate: 0.0001183]
		[batch 20/20] avg loss: 0.1654543682328392		[learning rate: 0.00011808]
	Learning Rate: 0.000118082
	LOSS [training: 0.1572772691024243 | validation: 0.12842483873383095]
	TIME [epoch: 8.83 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14863561837580724		[learning rate: 0.00011787]
		[batch 20/20] avg loss: 0.1530843452522437		[learning rate: 0.00011765]
	Learning Rate: 0.000117654
	LOSS [training: 0.1508599818140255 | validation: 0.11775259090062082]
	TIME [epoch: 8.84 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1488170461450131		[learning rate: 0.00011744]
		[batch 20/20] avg loss: 0.14820093288543776		[learning rate: 0.00011723]
	Learning Rate: 0.000117227
	LOSS [training: 0.14850898951522545 | validation: 0.11024910933094986]
	TIME [epoch: 8.83 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15409590026200196		[learning rate: 0.00011701]
		[batch 20/20] avg loss: 0.13009852642413514		[learning rate: 0.0001168]
	Learning Rate: 0.000116801
	LOSS [training: 0.14209721334306855 | validation: 0.1346674525852073]
	TIME [epoch: 8.86 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14085796560759195		[learning rate: 0.00011659]
		[batch 20/20] avg loss: 0.14822730959375244		[learning rate: 0.00011638]
	Learning Rate: 0.000116377
	LOSS [training: 0.1445426376006722 | validation: 0.10929547271600397]
	TIME [epoch: 8.84 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1484449082154672		[learning rate: 0.00011617]
		[batch 20/20] avg loss: 0.16201669711462813		[learning rate: 0.00011595]
	Learning Rate: 0.000115955
	LOSS [training: 0.15523080266504766 | validation: 0.12784362026623453]
	TIME [epoch: 8.83 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1600735871246858		[learning rate: 0.00011574]
		[batch 20/20] avg loss: 0.14479571129061117		[learning rate: 0.00011553]
	Learning Rate: 0.000115534
	LOSS [training: 0.1524346492076485 | validation: 0.1470397632851654]
	TIME [epoch: 8.82 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14345048866620674		[learning rate: 0.00011532]
		[batch 20/20] avg loss: 0.13760648910015522		[learning rate: 0.00011511]
	Learning Rate: 0.000115115
	LOSS [training: 0.14052848888318098 | validation: 0.18707917879781155]
	TIME [epoch: 8.82 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15044403464702216		[learning rate: 0.00011491]
		[batch 20/20] avg loss: 0.1601881718955867		[learning rate: 0.0001147]
	Learning Rate: 0.000114697
	LOSS [training: 0.15531610327130443 | validation: 0.11271194423206794]
	TIME [epoch: 8.86 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1252494342876383		[learning rate: 0.00011449]
		[batch 20/20] avg loss: 0.15975675401149028		[learning rate: 0.00011428]
	Learning Rate: 0.000114281
	LOSS [training: 0.14250309414956425 | validation: 0.13542421750907313]
	TIME [epoch: 8.83 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16418802200522345		[learning rate: 0.00011407]
		[batch 20/20] avg loss: 0.13188571424589046		[learning rate: 0.00011387]
	Learning Rate: 0.000113866
	LOSS [training: 0.14803686812555694 | validation: 0.11935424067531844]
	TIME [epoch: 8.82 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1656631878179308		[learning rate: 0.00011366]
		[batch 20/20] avg loss: 0.13832528133691183		[learning rate: 0.00011345]
	Learning Rate: 0.000113453
	LOSS [training: 0.1519942345774213 | validation: 0.11118846482726934]
	TIME [epoch: 8.84 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13690590096684363		[learning rate: 0.00011325]
		[batch 20/20] avg loss: 0.14793472445053676		[learning rate: 0.00011304]
	Learning Rate: 0.000113041
	LOSS [training: 0.1424203127086902 | validation: 0.1299450933687398]
	TIME [epoch: 8.85 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1415312500523262		[learning rate: 0.00011284]
		[batch 20/20] avg loss: 0.1525601375882737		[learning rate: 0.00011263]
	Learning Rate: 0.000112631
	LOSS [training: 0.14704569382029994 | validation: 0.11934436563372905]
	TIME [epoch: 8.84 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14608388382830537		[learning rate: 0.00011243]
		[batch 20/20] avg loss: 0.16347516232720613		[learning rate: 0.00011222]
	Learning Rate: 0.000112222
	LOSS [training: 0.15477952307775572 | validation: 0.12054442546429515]
	TIME [epoch: 8.83 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16426202287022557		[learning rate: 0.00011202]
		[batch 20/20] avg loss: 0.12300979024523975		[learning rate: 0.00011181]
	Learning Rate: 0.000111815
	LOSS [training: 0.14363590655773265 | validation: 0.11950070468244224]
	TIME [epoch: 8.83 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16653456161962096		[learning rate: 0.00011161]
		[batch 20/20] avg loss: 0.13125522583239685		[learning rate: 0.00011141]
	Learning Rate: 0.000111409
	LOSS [training: 0.1488948937260089 | validation: 0.13657135030734585]
	TIME [epoch: 8.84 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14584053084988322		[learning rate: 0.00011121]
		[batch 20/20] avg loss: 0.13929666350473566		[learning rate: 0.000111]
	Learning Rate: 0.000111005
	LOSS [training: 0.14256859717730946 | validation: 0.10445692970224582]
	TIME [epoch: 8.85 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14662713760794563		[learning rate: 0.0001108]
		[batch 20/20] avg loss: 0.1479559451344224		[learning rate: 0.0001106]
	Learning Rate: 0.000110602
	LOSS [training: 0.14729154137118403 | validation: 0.11201482808875327]
	TIME [epoch: 8.84 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14482942038050642		[learning rate: 0.0001104]
		[batch 20/20] avg loss: 0.14099776175413606		[learning rate: 0.0001102]
	Learning Rate: 0.000110201
	LOSS [training: 0.14291359106732124 | validation: 0.11257900441078332]
	TIME [epoch: 8.83 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14383480575761634		[learning rate: 0.00011]
		[batch 20/20] avg loss: 0.14981499493039602		[learning rate: 0.0001098]
	Learning Rate: 0.000109801
	LOSS [training: 0.14682490034400617 | validation: 0.12043369214139034]
	TIME [epoch: 8.84 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13713110707156706		[learning rate: 0.0001096]
		[batch 20/20] avg loss: 0.15537473341021918		[learning rate: 0.0001094]
	Learning Rate: 0.000109402
	LOSS [training: 0.14625292024089312 | validation: 0.11364088066126547]
	TIME [epoch: 8.84 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.138412301346557		[learning rate: 0.0001092]
		[batch 20/20] avg loss: 0.15502168718912007		[learning rate: 0.00010901]
	Learning Rate: 0.000109005
	LOSS [training: 0.14671699426783852 | validation: 0.11257980108800843]
	TIME [epoch: 8.86 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14572176021588765		[learning rate: 0.00010881]
		[batch 20/20] avg loss: 0.15718690415677		[learning rate: 0.00010861]
	Learning Rate: 0.00010861
	LOSS [training: 0.1514543321863288 | validation: 0.18927205178233053]
	TIME [epoch: 8.84 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1688371571862715		[learning rate: 0.00010841]
		[batch 20/20] avg loss: 0.13289740789798382		[learning rate: 0.00010822]
	Learning Rate: 0.000108215
	LOSS [training: 0.15086728254212764 | validation: 0.10353698802920205]
	TIME [epoch: 8.84 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14023538450405304		[learning rate: 0.00010802]
		[batch 20/20] avg loss: 0.14928218778156518		[learning rate: 0.00010782]
	Learning Rate: 0.000107823
	LOSS [training: 0.14475878614280913 | validation: 0.10940621198633602]
	TIME [epoch: 8.84 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13596896463554628		[learning rate: 0.00010763]
		[batch 20/20] avg loss: 0.15388103481014992		[learning rate: 0.00010743]
	Learning Rate: 0.000107432
	LOSS [training: 0.1449249997228481 | validation: 0.11263661768411164]
	TIME [epoch: 8.85 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1567559607864794		[learning rate: 0.00010724]
		[batch 20/20] avg loss: 0.15548985060398512		[learning rate: 0.00010704]
	Learning Rate: 0.000107042
	LOSS [training: 0.1561229056952323 | validation: 0.12698581996964464]
	TIME [epoch: 8.85 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16595411963196938		[learning rate: 0.00010685]
		[batch 20/20] avg loss: 0.12971536488697494		[learning rate: 0.00010665]
	Learning Rate: 0.000106653
	LOSS [training: 0.14783474225947218 | validation: 0.147011974405594]
	TIME [epoch: 8.84 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15065755243917417		[learning rate: 0.00010646]
		[batch 20/20] avg loss: 0.16376300537681762		[learning rate: 0.00010627]
	Learning Rate: 0.000106266
	LOSS [training: 0.15721027890799588 | validation: 0.13212959990204226]
	TIME [epoch: 8.84 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14538723345884358		[learning rate: 0.00010607]
		[batch 20/20] avg loss: 0.14859021996297833		[learning rate: 0.00010588]
	Learning Rate: 0.00010588
	LOSS [training: 0.14698872671091096 | validation: 0.12654869866657162]
	TIME [epoch: 8.84 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14060591814248485		[learning rate: 0.00010569]
		[batch 20/20] avg loss: 0.1431992479881635		[learning rate: 0.0001055]
	Learning Rate: 0.000105496
	LOSS [training: 0.1419025830653242 | validation: 0.11617869077514392]
	TIME [epoch: 8.86 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15296221975651267		[learning rate: 0.0001053]
		[batch 20/20] avg loss: 0.13121113451668506		[learning rate: 0.00010511]
	Learning Rate: 0.000105113
	LOSS [training: 0.1420866771365989 | validation: 0.13163299485315952]
	TIME [epoch: 8.84 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15184467029458942		[learning rate: 0.00010492]
		[batch 20/20] avg loss: 0.1346350112525066		[learning rate: 0.00010473]
	Learning Rate: 0.000104732
	LOSS [training: 0.14323984077354798 | validation: 0.10552932137452121]
	TIME [epoch: 8.84 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13427315974928386		[learning rate: 0.00010454]
		[batch 20/20] avg loss: 0.1726894416118154		[learning rate: 0.00010435]
	Learning Rate: 0.000104352
	LOSS [training: 0.1534813006805496 | validation: 0.12403839237577485]
	TIME [epoch: 8.84 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1327486274136494		[learning rate: 0.00010416]
		[batch 20/20] avg loss: 0.1499982439588344		[learning rate: 0.00010397]
	Learning Rate: 0.000103973
	LOSS [training: 0.1413734356862419 | validation: 0.1039143434997054]
	TIME [epoch: 8.84 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15334691262765815		[learning rate: 0.00010378]
		[batch 20/20] avg loss: 0.1448322266252533		[learning rate: 0.0001036]
	Learning Rate: 0.000103596
	LOSS [training: 0.14908956962645575 | validation: 0.1428898916032102]
	TIME [epoch: 8.85 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15194905949770074		[learning rate: 0.00010341]
		[batch 20/20] avg loss: 0.13123660780470123		[learning rate: 0.00010322]
	Learning Rate: 0.00010322
	LOSS [training: 0.14159283365120098 | validation: 0.10925003396789043]
	TIME [epoch: 8.83 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14714431587631735		[learning rate: 0.00010303]
		[batch 20/20] avg loss: 0.12076370263101224		[learning rate: 0.00010285]
	Learning Rate: 0.000102845
	LOSS [training: 0.13395400925366482 | validation: 0.18386185394426333]
	TIME [epoch: 8.84 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16264519185896412		[learning rate: 0.00010266]
		[batch 20/20] avg loss: 0.15999694508436682		[learning rate: 0.00010247]
	Learning Rate: 0.000102472
	LOSS [training: 0.16132106847166547 | validation: 0.14136833510030042]
	TIME [epoch: 8.84 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13673586973751942		[learning rate: 0.00010229]
		[batch 20/20] avg loss: 0.155441984313671		[learning rate: 0.0001021]
	Learning Rate: 0.0001021
	LOSS [training: 0.1460889270255952 | validation: 0.1186765316681949]
	TIME [epoch: 8.84 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1489287233851274		[learning rate: 0.00010191]
		[batch 20/20] avg loss: 0.1751006760994475		[learning rate: 0.00010173]
	Learning Rate: 0.00010173
	LOSS [training: 0.16201469974228744 | validation: 0.11697904016340296]
	TIME [epoch: 8.86 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13985642597296327		[learning rate: 0.00010154]
		[batch 20/20] avg loss: 0.14143709998047357		[learning rate: 0.00010136]
	Learning Rate: 0.00010136
	LOSS [training: 0.14064676297671844 | validation: 0.11468488443102916]
	TIME [epoch: 8.84 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14478874855291965		[learning rate: 0.00010118]
		[batch 20/20] avg loss: 0.1440805625179284		[learning rate: 0.00010099]
	Learning Rate: 0.000100993
	LOSS [training: 0.14443465553542398 | validation: 0.1314430626981734]
	TIME [epoch: 8.84 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14888059865376838		[learning rate: 0.00010081]
		[batch 20/20] avg loss: 0.1435058192394295		[learning rate: 0.00010063]
	Learning Rate: 0.000100626
	LOSS [training: 0.14619320894659893 | validation: 0.1113705364649826]
	TIME [epoch: 8.83 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14348290650353318		[learning rate: 0.00010044]
		[batch 20/20] avg loss: 0.13201043514594074		[learning rate: 0.00010026]
	Learning Rate: 0.000100261
	LOSS [training: 0.13774667082473696 | validation: 0.12125612659020997]
	TIME [epoch: 8.86 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13456330045548054		[learning rate: 0.00010008]
		[batch 20/20] avg loss: 0.1551200611499586		[learning rate: 9.9897e-05]
	Learning Rate: 9.98971e-05
	LOSS [training: 0.14484168080271956 | validation: 0.1264695740798006]
	TIME [epoch: 8.85 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1454610153155661		[learning rate: 9.9716e-05]
		[batch 20/20] avg loss: 0.15111530723460154		[learning rate: 9.9535e-05]
	Learning Rate: 9.95345e-05
	LOSS [training: 0.14828816127508385 | validation: 0.11740887364650633]
	TIME [epoch: 8.84 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14662683066983784		[learning rate: 9.9354e-05]
		[batch 20/20] avg loss: 0.13479899628061529		[learning rate: 9.9173e-05]
	Learning Rate: 9.91733e-05
	LOSS [training: 0.14071291347522655 | validation: 0.11519986572333504]
	TIME [epoch: 8.83 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1499523503138481		[learning rate: 9.8993e-05]
		[batch 20/20] avg loss: 0.1367066627086394		[learning rate: 9.8813e-05]
	Learning Rate: 9.88134e-05
	LOSS [training: 0.14332950651124376 | validation: 0.10445363218289218]
	TIME [epoch: 8.83 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16319538489611393		[learning rate: 9.8634e-05]
		[batch 20/20] avg loss: 0.11878590369058908		[learning rate: 9.8455e-05]
	Learning Rate: 9.84548e-05
	LOSS [training: 0.1409906442933515 | validation: 0.11787788457627389]
	TIME [epoch: 8.85 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15223241698593007		[learning rate: 9.8276e-05]
		[batch 20/20] avg loss: 0.14293461826502657		[learning rate: 9.8098e-05]
	Learning Rate: 9.80975e-05
	LOSS [training: 0.14758351762547833 | validation: 0.13483960726671484]
	TIME [epoch: 8.83 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16298027898116002		[learning rate: 9.7919e-05]
		[batch 20/20] avg loss: 0.13231796005829896		[learning rate: 9.7742e-05]
	Learning Rate: 9.77415e-05
	LOSS [training: 0.14764911951972948 | validation: 0.1368574353754071]
	TIME [epoch: 8.84 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15648453344559904		[learning rate: 9.7564e-05]
		[batch 20/20] avg loss: 0.13501583192207736		[learning rate: 9.7387e-05]
	Learning Rate: 9.73868e-05
	LOSS [training: 0.14575018268383824 | validation: 0.1310708009502268]
	TIME [epoch: 8.84 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1537353012471869		[learning rate: 9.721e-05]
		[batch 20/20] avg loss: 0.14026387782345023		[learning rate: 9.7033e-05]
	Learning Rate: 9.70334e-05
	LOSS [training: 0.14699958953531855 | validation: 0.10914005315706132]
	TIME [epoch: 8.83 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14527453809943708		[learning rate: 9.6857e-05]
		[batch 20/20] avg loss: 0.14558685326553353		[learning rate: 9.6681e-05]
	Learning Rate: 9.66812e-05
	LOSS [training: 0.14543069568248534 | validation: 0.19753817520981215]
	TIME [epoch: 8.85 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17768195204648823		[learning rate: 9.6506e-05]
		[batch 20/20] avg loss: 0.1460840458502523		[learning rate: 9.633e-05]
	Learning Rate: 9.63304e-05
	LOSS [training: 0.16188299894837027 | validation: 0.12321265547999288]
	TIME [epoch: 8.84 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15361607061034224		[learning rate: 9.6155e-05]
		[batch 20/20] avg loss: 0.1455641624348947		[learning rate: 9.5981e-05]
	Learning Rate: 9.59808e-05
	LOSS [training: 0.14959011652261842 | validation: 0.19054759685430556]
	TIME [epoch: 8.84 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15478655322359378		[learning rate: 9.5806e-05]
		[batch 20/20] avg loss: 0.14560282450646977		[learning rate: 9.5632e-05]
	Learning Rate: 9.56324e-05
	LOSS [training: 0.1501946888650318 | validation: 0.12996916306594228]
	TIME [epoch: 8.83 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1473795199959698		[learning rate: 9.5459e-05]
		[batch 20/20] avg loss: 0.14325027325776643		[learning rate: 9.5285e-05]
	Learning Rate: 9.52854e-05
	LOSS [training: 0.1453148966268681 | validation: 0.12333686106657603]
	TIME [epoch: 8.84 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14631319502182055		[learning rate: 9.5112e-05]
		[batch 20/20] avg loss: 0.15629037601712376		[learning rate: 9.494e-05]
	Learning Rate: 9.49396e-05
	LOSS [training: 0.15130178551947213 | validation: 0.10954328021600082]
	TIME [epoch: 8.83 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14111627020286452		[learning rate: 9.4767e-05]
		[batch 20/20] avg loss: 0.14491707220918126		[learning rate: 9.4595e-05]
	Learning Rate: 9.45951e-05
	LOSS [training: 0.14301667120602285 | validation: 0.11258866059398154]
	TIME [epoch: 8.83 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12974053679680392		[learning rate: 9.4423e-05]
		[batch 20/20] avg loss: 0.15328778861722575		[learning rate: 9.4252e-05]
	Learning Rate: 9.42518e-05
	LOSS [training: 0.1415141627070148 | validation: 0.12190565866934938]
	TIME [epoch: 8.83 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14536893739226564		[learning rate: 9.4081e-05]
		[batch 20/20] avg loss: 0.1452283868321131		[learning rate: 9.391e-05]
	Learning Rate: 9.39097e-05
	LOSS [training: 0.1452986621121894 | validation: 0.1379846858160887]
	TIME [epoch: 8.83 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14999088085031276		[learning rate: 9.3739e-05]
		[batch 20/20] avg loss: 0.1342327006650928		[learning rate: 9.3569e-05]
	Learning Rate: 9.35689e-05
	LOSS [training: 0.14211179075770275 | validation: 0.1271510514088393]
	TIME [epoch: 8.85 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16553573998157747		[learning rate: 9.3399e-05]
		[batch 20/20] avg loss: 0.14019255839461772		[learning rate: 9.3229e-05]
	Learning Rate: 9.32294e-05
	LOSS [training: 0.1528641491880976 | validation: 0.12778722124184372]
	TIME [epoch: 8.83 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1501331804965363		[learning rate: 9.306e-05]
		[batch 20/20] avg loss: 0.1373700412504713		[learning rate: 9.2891e-05]
	Learning Rate: 9.2891e-05
	LOSS [training: 0.14375161087350383 | validation: 0.12500271186053313]
	TIME [epoch: 8.82 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14511165134881523		[learning rate: 9.2722e-05]
		[batch 20/20] avg loss: 0.13836339902998931		[learning rate: 9.2554e-05]
	Learning Rate: 9.25539e-05
	LOSS [training: 0.14173752518940227 | validation: 0.13502602805648947]
	TIME [epoch: 8.82 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14804536277532312		[learning rate: 9.2386e-05]
		[batch 20/20] avg loss: 0.14869251467312436		[learning rate: 9.2218e-05]
	Learning Rate: 9.2218e-05
	LOSS [training: 0.14836893872422374 | validation: 0.12068437380137009]
	TIME [epoch: 8.83 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15269402685872158		[learning rate: 9.2051e-05]
		[batch 20/20] avg loss: 0.13888546731570528		[learning rate: 9.1883e-05]
	Learning Rate: 9.18834e-05
	LOSS [training: 0.1457897470872134 | validation: 0.12141339532824409]
	TIME [epoch: 8.85 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14502980610323543		[learning rate: 9.1716e-05]
		[batch 20/20] avg loss: 0.13608744939018974		[learning rate: 9.155e-05]
	Learning Rate: 9.15499e-05
	LOSS [training: 0.14055862774671257 | validation: 0.1283819643363199]
	TIME [epoch: 8.83 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13491229678818506		[learning rate: 9.1384e-05]
		[batch 20/20] avg loss: 0.14943842569812243		[learning rate: 9.1218e-05]
	Learning Rate: 9.12177e-05
	LOSS [training: 0.14217536124315377 | validation: 0.12114460927096934]
	TIME [epoch: 8.84 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13143745983122426		[learning rate: 9.1052e-05]
		[batch 20/20] avg loss: 0.14843952812801536		[learning rate: 9.0887e-05]
	Learning Rate: 9.08866e-05
	LOSS [training: 0.1399384939796198 | validation: 0.11869798275926578]
	TIME [epoch: 8.84 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13601614465829828		[learning rate: 9.0722e-05]
		[batch 20/20] avg loss: 0.15244250548022228		[learning rate: 9.0557e-05]
	Learning Rate: 9.05568e-05
	LOSS [training: 0.1442293250692603 | validation: 0.11397022322334506]
	TIME [epoch: 8.85 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15116147791180806		[learning rate: 9.0392e-05]
		[batch 20/20] avg loss: 0.1419784967146678		[learning rate: 9.0228e-05]
	Learning Rate: 9.02281e-05
	LOSS [training: 0.14656998731323795 | validation: 0.12170752172099855]
	TIME [epoch: 8.83 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16632824259663384		[learning rate: 9.0064e-05]
		[batch 20/20] avg loss: 0.14479754513933224		[learning rate: 8.9901e-05]
	Learning Rate: 8.99007e-05
	LOSS [training: 0.15556289386798305 | validation: 0.12783537812515564]
	TIME [epoch: 8.82 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14435247474930918		[learning rate: 8.9737e-05]
		[batch 20/20] avg loss: 0.145251869951428		[learning rate: 8.9574e-05]
	Learning Rate: 8.95745e-05
	LOSS [training: 0.14480217235036857 | validation: 0.14312648220913574]
	TIME [epoch: 8.83 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15432387409073173		[learning rate: 8.9412e-05]
		[batch 20/20] avg loss: 0.15089205128359137		[learning rate: 8.9249e-05]
	Learning Rate: 8.92494e-05
	LOSS [training: 0.15260796268716154 | validation: 0.143835253940509]
	TIME [epoch: 8.84 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13889929034047382		[learning rate: 8.9087e-05]
		[batch 20/20] avg loss: 0.1555005898996323		[learning rate: 8.8926e-05]
	Learning Rate: 8.89255e-05
	LOSS [training: 0.14719994012005305 | validation: 0.12545963700738383]
	TIME [epoch: 8.86 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17426114539948742		[learning rate: 8.8764e-05]
		[batch 20/20] avg loss: 0.142204936581461		[learning rate: 8.8603e-05]
	Learning Rate: 8.86028e-05
	LOSS [training: 0.1582330409904742 | validation: 0.1139049590082]
	TIME [epoch: 8.84 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1326721682760028		[learning rate: 8.8442e-05]
		[batch 20/20] avg loss: 0.15172145266203613		[learning rate: 8.8281e-05]
	Learning Rate: 8.82813e-05
	LOSS [training: 0.1421968104690195 | validation: 0.13435897922535206]
	TIME [epoch: 8.84 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.179974511924819		[learning rate: 8.8121e-05]
		[batch 20/20] avg loss: 0.1351521493926166		[learning rate: 8.7961e-05]
	Learning Rate: 8.79609e-05
	LOSS [training: 0.1575633306587178 | validation: 0.132324664491001]
	TIME [epoch: 8.83 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1493093137638805		[learning rate: 8.7801e-05]
		[batch 20/20] avg loss: 0.12668327531913948		[learning rate: 8.7642e-05]
	Learning Rate: 8.76417e-05
	LOSS [training: 0.13799629454151 | validation: 0.11893712807187098]
	TIME [epoch: 8.84 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14652252689174958		[learning rate: 8.7482e-05]
		[batch 20/20] avg loss: 0.14723043059872715		[learning rate: 8.7324e-05]
	Learning Rate: 8.73236e-05
	LOSS [training: 0.1468764787452384 | validation: 0.12776036720248438]
	TIME [epoch: 8.85 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1470866306442125		[learning rate: 8.7165e-05]
		[batch 20/20] avg loss: 0.14227779333448606		[learning rate: 8.7007e-05]
	Learning Rate: 8.70067e-05
	LOSS [training: 0.14468221198934933 | validation: 0.14079912747326792]
	TIME [epoch: 8.85 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1443023442259553		[learning rate: 8.6849e-05]
		[batch 20/20] avg loss: 0.15847135211800456		[learning rate: 8.6691e-05]
	Learning Rate: 8.66909e-05
	LOSS [training: 0.15138684817197992 | validation: 0.11423373356641801]
	TIME [epoch: 8.83 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15167919033427674		[learning rate: 8.6533e-05]
		[batch 20/20] avg loss: 0.14603952226071742		[learning rate: 8.6376e-05]
	Learning Rate: 8.63763e-05
	LOSS [training: 0.14885935629749708 | validation: 0.12043936695265495]
	TIME [epoch: 8.83 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1256900543801063		[learning rate: 8.6219e-05]
		[batch 20/20] avg loss: 0.141959559887107		[learning rate: 8.6063e-05]
	Learning Rate: 8.60629e-05
	LOSS [training: 0.13382480713360667 | validation: 0.11944041698557271]
	TIME [epoch: 8.85 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1565665679761729		[learning rate: 8.5907e-05]
		[batch 20/20] avg loss: 0.13444756345845985		[learning rate: 8.5751e-05]
	Learning Rate: 8.57505e-05
	LOSS [training: 0.1455070657173164 | validation: 0.12138982556010335]
	TIME [epoch: 8.84 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13768990541606194		[learning rate: 8.5595e-05]
		[batch 20/20] avg loss: 0.14826828940068723		[learning rate: 8.5439e-05]
	Learning Rate: 8.54394e-05
	LOSS [training: 0.1429790974083746 | validation: 0.16250638533310366]
	TIME [epoch: 8.83 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14745430760186926		[learning rate: 8.5284e-05]
		[batch 20/20] avg loss: 0.138379698217235		[learning rate: 8.5129e-05]
	Learning Rate: 8.51293e-05
	LOSS [training: 0.14291700290955217 | validation: 0.1053981515323248]
	TIME [epoch: 8.84 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1306683814764696		[learning rate: 8.4975e-05]
		[batch 20/20] avg loss: 0.16075795380473687		[learning rate: 8.482e-05]
	Learning Rate: 8.48204e-05
	LOSS [training: 0.14571316764060324 | validation: 0.15482390032755014]
	TIME [epoch: 8.83 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15209328666063016		[learning rate: 8.4666e-05]
		[batch 20/20] avg loss: 0.13772411454356823		[learning rate: 8.4513e-05]
	Learning Rate: 8.45125e-05
	LOSS [training: 0.1449087006020992 | validation: 0.12176683438268146]
	TIME [epoch: 8.85 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13298726319801588		[learning rate: 8.4359e-05]
		[batch 20/20] avg loss: 0.1453281072719118		[learning rate: 8.4206e-05]
	Learning Rate: 8.42058e-05
	LOSS [training: 0.13915768523496386 | validation: 0.1306874731617553]
	TIME [epoch: 8.83 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12495517422758302		[learning rate: 8.4053e-05]
		[batch 20/20] avg loss: 0.14706020044522145		[learning rate: 8.39e-05]
	Learning Rate: 8.39002e-05
	LOSS [training: 0.13600768733640217 | validation: 0.10364203765417553]
	TIME [epoch: 8.83 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14543246458590253		[learning rate: 8.3748e-05]
		[batch 20/20] avg loss: 0.12812068668313775		[learning rate: 8.3596e-05]
	Learning Rate: 8.35958e-05
	LOSS [training: 0.13677657563452011 | validation: 0.1357045513531826]
	TIME [epoch: 8.83 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15740267098830305		[learning rate: 8.3444e-05]
		[batch 20/20] avg loss: 0.12836309323903686		[learning rate: 8.3292e-05]
	Learning Rate: 8.32924e-05
	LOSS [training: 0.14288288211366995 | validation: 0.11948803473258661]
	TIME [epoch: 8.83 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14018600897016895		[learning rate: 8.3141e-05]
		[batch 20/20] avg loss: 0.1362226779333921		[learning rate: 8.299e-05]
	Learning Rate: 8.29901e-05
	LOSS [training: 0.13820434345178056 | validation: 0.13309587890936753]
	TIME [epoch: 8.86 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14077089939673199		[learning rate: 8.2839e-05]
		[batch 20/20] avg loss: 0.1455927123190475		[learning rate: 8.2689e-05]
	Learning Rate: 8.26889e-05
	LOSS [training: 0.1431818058578897 | validation: 0.13016043651867112]
	TIME [epoch: 8.83 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14964806326391225		[learning rate: 8.2539e-05]
		[batch 20/20] avg loss: 0.125536030868148		[learning rate: 8.2389e-05]
	Learning Rate: 8.23889e-05
	LOSS [training: 0.13759204706603012 | validation: 0.11210145920119671]
	TIME [epoch: 8.84 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1466260398708128		[learning rate: 8.2239e-05]
		[batch 20/20] avg loss: 0.12968134893324507		[learning rate: 8.209e-05]
	Learning Rate: 8.20899e-05
	LOSS [training: 0.13815369440202893 | validation: 0.12039439991632324]
	TIME [epoch: 8.84 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14272143465443146		[learning rate: 8.1941e-05]
		[batch 20/20] avg loss: 0.1393230326238004		[learning rate: 8.1792e-05]
	Learning Rate: 8.17919e-05
	LOSS [training: 0.14102223363911592 | validation: 0.12015530571909959]
	TIME [epoch: 8.83 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13710342419045599		[learning rate: 8.1643e-05]
		[batch 20/20] avg loss: 0.13794023906021016		[learning rate: 8.1495e-05]
	Learning Rate: 8.14951e-05
	LOSS [training: 0.1375218316253331 | validation: 0.12105966859411321]
	TIME [epoch: 8.85 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14111981954501102		[learning rate: 8.1347e-05]
		[batch 20/20] avg loss: 0.1437171004559829		[learning rate: 8.1199e-05]
	Learning Rate: 8.11994e-05
	LOSS [training: 0.142418460000497 | validation: 0.11837360235060022]
	TIME [epoch: 8.82 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13065672501443282		[learning rate: 8.1052e-05]
		[batch 20/20] avg loss: 0.14000768662315538		[learning rate: 8.0905e-05]
	Learning Rate: 8.09047e-05
	LOSS [training: 0.1353322058187941 | validation: 0.11615667849759999]
	TIME [epoch: 8.84 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13968497610986774		[learning rate: 8.0758e-05]
		[batch 20/20] avg loss: 0.13078367232939256		[learning rate: 8.0611e-05]
	Learning Rate: 8.06111e-05
	LOSS [training: 0.13523432421963016 | validation: 0.1182823608549857]
	TIME [epoch: 8.84 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13493869212131618		[learning rate: 8.0465e-05]
		[batch 20/20] avg loss: 0.15242320605312704		[learning rate: 8.0319e-05]
	Learning Rate: 8.03186e-05
	LOSS [training: 0.1436809490872216 | validation: 0.11680778512044082]
	TIME [epoch: 8.86 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15090083735255613		[learning rate: 8.0173e-05]
		[batch 20/20] avg loss: 0.13362135353271637		[learning rate: 8.0027e-05]
	Learning Rate: 8.00271e-05
	LOSS [training: 0.14226109544263627 | validation: 0.1208470663027442]
	TIME [epoch: 8.84 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13864291519926913		[learning rate: 7.9882e-05]
		[batch 20/20] avg loss: 0.1445052525382572		[learning rate: 7.9737e-05]
	Learning Rate: 7.97367e-05
	LOSS [training: 0.1415740838687632 | validation: 0.12450478893442984]
	TIME [epoch: 8.83 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14274497538497496		[learning rate: 7.9592e-05]
		[batch 20/20] avg loss: 0.12521381713807553		[learning rate: 7.9447e-05]
	Learning Rate: 7.94473e-05
	LOSS [training: 0.13397939626152527 | validation: 0.1220523295162955]
	TIME [epoch: 8.84 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13204835927241015		[learning rate: 7.9303e-05]
		[batch 20/20] avg loss: 0.13756607859863726		[learning rate: 7.9159e-05]
	Learning Rate: 7.91589e-05
	LOSS [training: 0.13480721893552366 | validation: 0.11720185419867177]
	TIME [epoch: 8.83 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1439038812484737		[learning rate: 7.9015e-05]
		[batch 20/20] avg loss: 0.13963161202227223		[learning rate: 7.8872e-05]
	Learning Rate: 7.88717e-05
	LOSS [training: 0.14176774663537292 | validation: 0.15775450009820882]
	TIME [epoch: 8.86 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1457302294039679		[learning rate: 7.8728e-05]
		[batch 20/20] avg loss: 0.13951417751838543		[learning rate: 7.8585e-05]
	Learning Rate: 7.85854e-05
	LOSS [training: 0.14262220346117668 | validation: 0.11508014132505598]
	TIME [epoch: 8.84 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1286797097877301		[learning rate: 7.8443e-05]
		[batch 20/20] avg loss: 0.14748254385048978		[learning rate: 7.83e-05]
	Learning Rate: 7.83003e-05
	LOSS [training: 0.13808112681910995 | validation: 0.1905276732741814]
	TIME [epoch: 8.84 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1682962216856654		[learning rate: 7.8158e-05]
		[batch 20/20] avg loss: 0.14055303429482788		[learning rate: 7.8016e-05]
	Learning Rate: 7.80161e-05
	LOSS [training: 0.1544246279902466 | validation: 0.10681799727933909]
	TIME [epoch: 8.84 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13866015706531104		[learning rate: 7.7874e-05]
		[batch 20/20] avg loss: 0.13416829397560742		[learning rate: 7.7733e-05]
	Learning Rate: 7.7733e-05
	LOSS [training: 0.1364142255204592 | validation: 0.10077563409918178]
	TIME [epoch: 8.84 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14236848563886764		[learning rate: 7.7592e-05]
		[batch 20/20] avg loss: 0.12666040361623673		[learning rate: 7.7451e-05]
	Learning Rate: 7.74509e-05
	LOSS [training: 0.1345144446275522 | validation: 0.14064279072774413]
	TIME [epoch: 8.86 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1464343321396489		[learning rate: 7.731e-05]
		[batch 20/20] avg loss: 0.15075209171791942		[learning rate: 7.717e-05]
	Learning Rate: 7.71698e-05
	LOSS [training: 0.1485932119287842 | validation: 0.11074406224297562]
	TIME [epoch: 8.83 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12712774539389576		[learning rate: 7.703e-05]
		[batch 20/20] avg loss: 0.149082117193989		[learning rate: 7.689e-05]
	Learning Rate: 7.68897e-05
	LOSS [training: 0.13810493129394238 | validation: 0.11764623474968904]
	TIME [epoch: 8.83 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1388246949283481		[learning rate: 7.675e-05]
		[batch 20/20] avg loss: 0.145951653830482		[learning rate: 7.6611e-05]
	Learning Rate: 7.66107e-05
	LOSS [training: 0.14238817437941506 | validation: 0.12012677105900747]
	TIME [epoch: 8.84 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14149164506001008		[learning rate: 7.6472e-05]
		[batch 20/20] avg loss: 0.1374387723572847		[learning rate: 7.6333e-05]
	Learning Rate: 7.63327e-05
	LOSS [training: 0.13946520870864737 | validation: 0.11073308036992047]
	TIME [epoch: 8.85 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12627857602870096		[learning rate: 7.6194e-05]
		[batch 20/20] avg loss: 0.1470622411049965		[learning rate: 7.6056e-05]
	Learning Rate: 7.60557e-05
	LOSS [training: 0.13667040856684873 | validation: 0.11060754670177536]
	TIME [epoch: 8.84 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13905398231863203		[learning rate: 7.5918e-05]
		[batch 20/20] avg loss: 0.12909474556664952		[learning rate: 7.578e-05]
	Learning Rate: 7.57797e-05
	LOSS [training: 0.13407436394264077 | validation: 0.11722428019360856]
	TIME [epoch: 8.83 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13852252762008122		[learning rate: 7.5642e-05]
		[batch 20/20] avg loss: 0.1434558628228718		[learning rate: 7.5505e-05]
	Learning Rate: 7.55047e-05
	LOSS [training: 0.14098919522147652 | validation: 0.12108617031102714]
	TIME [epoch: 8.84 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1382124470802031		[learning rate: 7.5368e-05]
		[batch 20/20] avg loss: 0.14079082401183482		[learning rate: 7.5231e-05]
	Learning Rate: 7.52307e-05
	LOSS [training: 0.13950163554601894 | validation: 0.11251878986138165]
	TIME [epoch: 8.83 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13771556714749145		[learning rate: 7.5094e-05]
		[batch 20/20] avg loss: 0.1486536369205512		[learning rate: 7.4958e-05]
	Learning Rate: 7.49576e-05
	LOSS [training: 0.14318460203402134 | validation: 0.11330469753105359]
	TIME [epoch: 8.86 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12859032310474308		[learning rate: 7.4822e-05]
		[batch 20/20] avg loss: 0.14193240125035594		[learning rate: 7.4686e-05]
	Learning Rate: 7.46856e-05
	LOSS [training: 0.13526136217754953 | validation: 0.10659874360349918]
	TIME [epoch: 8.84 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1504882303724921		[learning rate: 7.455e-05]
		[batch 20/20] avg loss: 0.12655091728338635		[learning rate: 7.4415e-05]
	Learning Rate: 7.44146e-05
	LOSS [training: 0.13851957382793925 | validation: 0.12413703728606061]
	TIME [epoch: 8.83 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1327823305858427		[learning rate: 7.4279e-05]
		[batch 20/20] avg loss: 0.1617245156199147		[learning rate: 7.4144e-05]
	Learning Rate: 7.41445e-05
	LOSS [training: 0.14725342310287873 | validation: 0.10739343964492738]
	TIME [epoch: 8.84 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12585957322322383		[learning rate: 7.401e-05]
		[batch 20/20] avg loss: 0.1461823120966823		[learning rate: 7.3875e-05]
	Learning Rate: 7.38754e-05
	LOSS [training: 0.1360209426599531 | validation: 0.10870935272919224]
	TIME [epoch: 8.83 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13970223885689767		[learning rate: 7.3741e-05]
		[batch 20/20] avg loss: 0.1446753529150011		[learning rate: 7.3607e-05]
	Learning Rate: 7.36073e-05
	LOSS [training: 0.14218879588594938 | validation: 0.12779548615716305]
	TIME [epoch: 8.85 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12359407197303449		[learning rate: 7.3474e-05]
		[batch 20/20] avg loss: 0.15126124154249987		[learning rate: 7.334e-05]
	Learning Rate: 7.33402e-05
	LOSS [training: 0.1374276567577672 | validation: 0.11081806425848956]
	TIME [epoch: 8.83 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15326120322480896		[learning rate: 7.3207e-05]
		[batch 20/20] avg loss: 0.13361452608525964		[learning rate: 7.3074e-05]
	Learning Rate: 7.30741e-05
	LOSS [training: 0.14343786465503428 | validation: 0.12455142484151957]
	TIME [epoch: 8.84 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13209194393994655		[learning rate: 7.2941e-05]
		[batch 20/20] avg loss: 0.14856893604347604		[learning rate: 7.2809e-05]
	Learning Rate: 7.28089e-05
	LOSS [training: 0.1403304399917113 | validation: 0.11313010398762072]
	TIME [epoch: 8.83 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12576046667853272		[learning rate: 7.2677e-05]
		[batch 20/20] avg loss: 0.14294828939781562		[learning rate: 7.2545e-05]
	Learning Rate: 7.25447e-05
	LOSS [training: 0.13435437803817413 | validation: 0.10960840537499189]
	TIME [epoch: 8.85 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14364172905850428		[learning rate: 7.2413e-05]
		[batch 20/20] avg loss: 0.14169015955863168		[learning rate: 7.2281e-05]
	Learning Rate: 7.22814e-05
	LOSS [training: 0.142665944308568 | validation: 0.15993953679439052]
	TIME [epoch: 8.85 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14011371271863682		[learning rate: 7.215e-05]
		[batch 20/20] avg loss: 0.13902394716499433		[learning rate: 7.2019e-05]
	Learning Rate: 7.2019e-05
	LOSS [training: 0.13956882994181558 | validation: 0.11462711730205054]
	TIME [epoch: 8.85 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13263386884201686		[learning rate: 7.1888e-05]
		[batch 20/20] avg loss: 0.1406342665663173		[learning rate: 7.1758e-05]
	Learning Rate: 7.17577e-05
	LOSS [training: 0.13663406770416708 | validation: 0.11447244702688496]
	TIME [epoch: 8.83 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12627254278243977		[learning rate: 7.1627e-05]
		[batch 20/20] avg loss: 0.15132043645150633		[learning rate: 7.1497e-05]
	Learning Rate: 7.14973e-05
	LOSS [training: 0.13879648961697305 | validation: 0.12060527509206499]
	TIME [epoch: 8.83 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1322696242554576		[learning rate: 7.1367e-05]
		[batch 20/20] avg loss: 0.13632391657594028		[learning rate: 7.1238e-05]
	Learning Rate: 7.12378e-05
	LOSS [training: 0.13429677041569898 | validation: 0.12206718562668202]
	TIME [epoch: 8.86 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15745418643185288		[learning rate: 7.1108e-05]
		[batch 20/20] avg loss: 0.12400549376230566		[learning rate: 7.0979e-05]
	Learning Rate: 7.09793e-05
	LOSS [training: 0.14072984009707926 | validation: 0.09728993204884123]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1461.pth
	Model improved!!!
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13602615560425818		[learning rate: 7.085e-05]
		[batch 20/20] avg loss: 0.13915578598711792		[learning rate: 7.0722e-05]
	Learning Rate: 7.07217e-05
	LOSS [training: 0.13759097079568805 | validation: 0.11451745005296185]
	TIME [epoch: 8.84 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12968871027970788		[learning rate: 7.0593e-05]
		[batch 20/20] avg loss: 0.14778578895907693		[learning rate: 7.0465e-05]
	Learning Rate: 7.0465e-05
	LOSS [training: 0.13873724961939243 | validation: 0.11643410338191225]
	TIME [epoch: 8.83 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14737491257484198		[learning rate: 7.0337e-05]
		[batch 20/20] avg loss: 0.13177650676360808		[learning rate: 7.0209e-05]
	Learning Rate: 7.02093e-05
	LOSS [training: 0.13957570966922503 | validation: 0.1028445489981411]
	TIME [epoch: 8.83 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13651220066270733		[learning rate: 7.0082e-05]
		[batch 20/20] avg loss: 0.15179963229456325		[learning rate: 6.9955e-05]
	Learning Rate: 6.99545e-05
	LOSS [training: 0.1441559164786353 | validation: 0.11750089970378849]
	TIME [epoch: 8.85 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13837032504520835		[learning rate: 6.9827e-05]
		[batch 20/20] avg loss: 0.13499834093148852		[learning rate: 6.9701e-05]
	Learning Rate: 6.97006e-05
	LOSS [training: 0.13668433298834842 | validation: 0.13900046197103622]
	TIME [epoch: 8.84 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14130456038856748		[learning rate: 6.9574e-05]
		[batch 20/20] avg loss: 0.13432371566150722		[learning rate: 6.9448e-05]
	Learning Rate: 6.94477e-05
	LOSS [training: 0.13781413802503734 | validation: 0.1254578241187475]
	TIME [epoch: 8.83 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13107053336709915		[learning rate: 6.9322e-05]
		[batch 20/20] avg loss: 0.13973140401885328		[learning rate: 6.9196e-05]
	Learning Rate: 6.91957e-05
	LOSS [training: 0.13540096869297621 | validation: 0.12201721876528136]
	TIME [epoch: 8.83 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13550960468787226		[learning rate: 6.907e-05]
		[batch 20/20] avg loss: 0.1352513297292369		[learning rate: 6.8945e-05]
	Learning Rate: 6.89446e-05
	LOSS [training: 0.13538046720855462 | validation: 0.1060540528999297]
	TIME [epoch: 8.83 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15180959731378318		[learning rate: 6.8819e-05]
		[batch 20/20] avg loss: 0.13421247290069876		[learning rate: 6.8694e-05]
	Learning Rate: 6.86944e-05
	LOSS [training: 0.14301103510724097 | validation: 0.11981356550435686]
	TIME [epoch: 8.85 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13141235219019334		[learning rate: 6.857e-05]
		[batch 20/20] avg loss: 0.1359975600885935		[learning rate: 6.8445e-05]
	Learning Rate: 6.84451e-05
	LOSS [training: 0.13370495613939343 | validation: 0.12486185444297071]
	TIME [epoch: 8.83 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14292910683995144		[learning rate: 6.8321e-05]
		[batch 20/20] avg loss: 0.1256110936711682		[learning rate: 6.8197e-05]
	Learning Rate: 6.81967e-05
	LOSS [training: 0.1342701002555598 | validation: 0.10446152494825581]
	TIME [epoch: 8.83 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1388176446231687		[learning rate: 6.8073e-05]
		[batch 20/20] avg loss: 0.136954408081425		[learning rate: 6.7949e-05]
	Learning Rate: 6.79492e-05
	LOSS [training: 0.13788602635229688 | validation: 0.11416356098250635]
	TIME [epoch: 8.83 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13537297352365374		[learning rate: 6.7826e-05]
		[batch 20/20] avg loss: 0.137470681627624		[learning rate: 6.7703e-05]
	Learning Rate: 6.77026e-05
	LOSS [training: 0.13642182757563887 | validation: 0.11922609776031812]
	TIME [epoch: 8.85 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13493140785276705		[learning rate: 6.758e-05]
		[batch 20/20] avg loss: 0.14256790583143927		[learning rate: 6.7457e-05]
	Learning Rate: 6.74569e-05
	LOSS [training: 0.13874965684210316 | validation: 0.12927970417400278]
	TIME [epoch: 8.84 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14419699662372235		[learning rate: 6.7334e-05]
		[batch 20/20] avg loss: 0.12896369053805562		[learning rate: 6.7212e-05]
	Learning Rate: 6.72121e-05
	LOSS [training: 0.13658034358088894 | validation: 0.14331792897295234]
	TIME [epoch: 8.84 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14911424026441583		[learning rate: 6.709e-05]
		[batch 20/20] avg loss: 0.12275063876459284		[learning rate: 6.6968e-05]
	Learning Rate: 6.69682e-05
	LOSS [training: 0.13593243951450432 | validation: 0.11596257396687912]
	TIME [epoch: 8.83 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13803687711784882		[learning rate: 6.6847e-05]
		[batch 20/20] avg loss: 0.13186190784341606		[learning rate: 6.6725e-05]
	Learning Rate: 6.67251e-05
	LOSS [training: 0.13494939248063242 | validation: 0.10897475180839214]
	TIME [epoch: 8.83 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12822338206703218		[learning rate: 6.6604e-05]
		[batch 20/20] avg loss: 0.1394750631960751		[learning rate: 6.6483e-05]
	Learning Rate: 6.6483e-05
	LOSS [training: 0.1338492226315536 | validation: 0.1097096653617469]
	TIME [epoch: 8.84 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1455439121892611		[learning rate: 6.6362e-05]
		[batch 20/20] avg loss: 0.13884167489900545		[learning rate: 6.6242e-05]
	Learning Rate: 6.62417e-05
	LOSS [training: 0.14219279354413328 | validation: 0.11645401606117027]
	TIME [epoch: 8.82 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12722246120457315		[learning rate: 6.6121e-05]
		[batch 20/20] avg loss: 0.15927437870914998		[learning rate: 6.6001e-05]
	Learning Rate: 6.60013e-05
	LOSS [training: 0.14324841995686152 | validation: 0.10902504681744717]
	TIME [epoch: 8.83 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14771911939247334		[learning rate: 6.5881e-05]
		[batch 20/20] avg loss: 0.12266832833313175		[learning rate: 6.5762e-05]
	Learning Rate: 6.57618e-05
	LOSS [training: 0.13519372386280254 | validation: 0.11239646116580566]
	TIME [epoch: 8.83 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13601803249048874		[learning rate: 6.5642e-05]
		[batch 20/20] avg loss: 0.13503169109213192		[learning rate: 6.5523e-05]
	Learning Rate: 6.55232e-05
	LOSS [training: 0.1355248617913103 | validation: 0.11114548183789379]
	TIME [epoch: 8.83 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13144492186923928		[learning rate: 6.5404e-05]
		[batch 20/20] avg loss: 0.13753759573832514		[learning rate: 6.5285e-05]
	Learning Rate: 6.52854e-05
	LOSS [training: 0.13449125880378218 | validation: 0.11240512507825239]
	TIME [epoch: 8.85 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1346032437599905		[learning rate: 6.5167e-05]
		[batch 20/20] avg loss: 0.13565282950458527		[learning rate: 6.5048e-05]
	Learning Rate: 6.50484e-05
	LOSS [training: 0.1351280366322879 | validation: 0.12112673149669564]
	TIME [epoch: 8.83 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1368408133748004		[learning rate: 6.493e-05]
		[batch 20/20] avg loss: 0.14554367080719835		[learning rate: 6.4812e-05]
	Learning Rate: 6.48124e-05
	LOSS [training: 0.14119224209099937 | validation: 0.1297073802972569]
	TIME [epoch: 8.84 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12353963040920105		[learning rate: 6.4695e-05]
		[batch 20/20] avg loss: 0.15191512534302581		[learning rate: 6.4577e-05]
	Learning Rate: 6.45772e-05
	LOSS [training: 0.13772737787611344 | validation: 0.1462864822768077]
	TIME [epoch: 8.84 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15262287307351938		[learning rate: 6.446e-05]
		[batch 20/20] avg loss: 0.13932213095140517		[learning rate: 6.4343e-05]
	Learning Rate: 6.43428e-05
	LOSS [training: 0.14597250201246226 | validation: 0.10776421316276598]
	TIME [epoch: 8.85 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14418275480185616		[learning rate: 6.4226e-05]
		[batch 20/20] avg loss: 0.1394789149119663		[learning rate: 6.4109e-05]
	Learning Rate: 6.41093e-05
	LOSS [training: 0.14183083485691123 | validation: 0.12753951118258017]
	TIME [epoch: 8.83 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12432206171375612		[learning rate: 6.3993e-05]
		[batch 20/20] avg loss: 0.14587338041071884		[learning rate: 6.3877e-05]
	Learning Rate: 6.38767e-05
	LOSS [training: 0.13509772106223752 | validation: 0.10494122703541205]
	TIME [epoch: 8.83 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15449497932565398		[learning rate: 6.3761e-05]
		[batch 20/20] avg loss: 0.11122210634368039		[learning rate: 6.3645e-05]
	Learning Rate: 6.36449e-05
	LOSS [training: 0.1328585428346672 | validation: 0.12783970461219174]
	TIME [epoch: 8.83 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14793946035240774		[learning rate: 6.3529e-05]
		[batch 20/20] avg loss: 0.11743003798810998		[learning rate: 6.3414e-05]
	Learning Rate: 6.34139e-05
	LOSS [training: 0.1326847491702589 | validation: 0.11179473282367239]
	TIME [epoch: 8.82 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12629833845465988		[learning rate: 6.3299e-05]
		[batch 20/20] avg loss: 0.16772170339718379		[learning rate: 6.3184e-05]
	Learning Rate: 6.31837e-05
	LOSS [training: 0.14701002092592189 | validation: 0.13641127817699053]
	TIME [epoch: 8.85 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1261069397101742		[learning rate: 6.3069e-05]
		[batch 20/20] avg loss: 0.14762739961564167		[learning rate: 6.2954e-05]
	Learning Rate: 6.29544e-05
	LOSS [training: 0.1368671696629079 | validation: 0.10473498524139535]
	TIME [epoch: 8.84 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12205171082294439		[learning rate: 6.284e-05]
		[batch 20/20] avg loss: 0.15067956079810868		[learning rate: 6.2726e-05]
	Learning Rate: 6.2726e-05
	LOSS [training: 0.13636563581052658 | validation: 0.159730751915396]
	TIME [epoch: 8.83 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1450435625806578		[learning rate: 6.2612e-05]
		[batch 20/20] avg loss: 0.13808731281798142		[learning rate: 6.2498e-05]
	Learning Rate: 6.24983e-05
	LOSS [training: 0.14156543769931962 | validation: 0.1185334515655892]
	TIME [epoch: 8.83 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12781087547882758		[learning rate: 6.2385e-05]
		[batch 20/20] avg loss: 0.13663064231837369		[learning rate: 6.2272e-05]
	Learning Rate: 6.22715e-05
	LOSS [training: 0.13222075889860063 | validation: 0.1080195230363293]
	TIME [epoch: 8.83 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13263502443250094		[learning rate: 6.2158e-05]
		[batch 20/20] avg loss: 0.12918595404509836		[learning rate: 6.2046e-05]
	Learning Rate: 6.20455e-05
	LOSS [training: 0.13091048923879964 | validation: 0.10674134895707006]
	TIME [epoch: 8.85 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13588824226346283		[learning rate: 6.1933e-05]
		[batch 20/20] avg loss: 0.1378572264425862		[learning rate: 6.182e-05]
	Learning Rate: 6.18204e-05
	LOSS [training: 0.13687273435302455 | validation: 0.11516696987961343]
	TIME [epoch: 8.83 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11826117908186133		[learning rate: 6.1708e-05]
		[batch 20/20] avg loss: 0.15135279866278092		[learning rate: 6.1596e-05]
	Learning Rate: 6.1596e-05
	LOSS [training: 0.13480698887232115 | validation: 0.11435075996831699]
	TIME [epoch: 8.84 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1269857124316225		[learning rate: 6.1484e-05]
		[batch 20/20] avg loss: 0.13768285373578731		[learning rate: 6.1372e-05]
	Learning Rate: 6.13725e-05
	LOSS [training: 0.1323342830837049 | validation: 0.11226988267395996]
	TIME [epoch: 8.82 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14478787848767555		[learning rate: 6.1261e-05]
		[batch 20/20] avg loss: 0.128204837965642		[learning rate: 6.115e-05]
	Learning Rate: 6.11498e-05
	LOSS [training: 0.13649635822665873 | validation: 0.11818178249859959]
	TIME [epoch: 8.85 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13733979601192398		[learning rate: 6.1039e-05]
		[batch 20/20] avg loss: 0.15249872222806762		[learning rate: 6.0928e-05]
	Learning Rate: 6.09278e-05
	LOSS [training: 0.14491925911999579 | validation: 0.11901178883342396]
	TIME [epoch: 8.83 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13546534180205932		[learning rate: 6.0817e-05]
		[batch 20/20] avg loss: 0.130294581889628		[learning rate: 6.0707e-05]
	Learning Rate: 6.07067e-05
	LOSS [training: 0.13287996184584364 | validation: 0.11967952428243993]
	TIME [epoch: 8.83 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13924589181441377		[learning rate: 6.0596e-05]
		[batch 20/20] avg loss: 0.1308673389225082		[learning rate: 6.0486e-05]
	Learning Rate: 6.04864e-05
	LOSS [training: 0.13505661536846103 | validation: 0.11651347642515605]
	TIME [epoch: 8.82 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11772799813003276		[learning rate: 6.0377e-05]
		[batch 20/20] avg loss: 0.1501190739865852		[learning rate: 6.0267e-05]
	Learning Rate: 6.02669e-05
	LOSS [training: 0.13392353605830898 | validation: 0.12432292728697512]
	TIME [epoch: 8.83 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15319721468028163		[learning rate: 6.0157e-05]
		[batch 20/20] avg loss: 0.14098166719537467		[learning rate: 6.0048e-05]
	Learning Rate: 6.00482e-05
	LOSS [training: 0.1470894409378282 | validation: 0.13440754501662983]
	TIME [epoch: 8.85 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14491603977603387		[learning rate: 5.9939e-05]
		[batch 20/20] avg loss: 0.1238167316421849		[learning rate: 5.983e-05]
	Learning Rate: 5.98303e-05
	LOSS [training: 0.13436638570910936 | validation: 0.12218587405405508]
	TIME [epoch: 8.84 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1312818285689332		[learning rate: 5.9722e-05]
		[batch 20/20] avg loss: 0.13547765285457863		[learning rate: 5.9613e-05]
	Learning Rate: 5.96132e-05
	LOSS [training: 0.1333797407117559 | validation: 0.11369514055671848]
	TIME [epoch: 8.83 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1380277454584497		[learning rate: 5.9505e-05]
		[batch 20/20] avg loss: 0.1329845612062574		[learning rate: 5.9397e-05]
	Learning Rate: 5.93968e-05
	LOSS [training: 0.13550615333235355 | validation: 0.1055556900962465]
	TIME [epoch: 8.83 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1383535712152711		[learning rate: 5.9289e-05]
		[batch 20/20] avg loss: 0.1350242799370227		[learning rate: 5.9181e-05]
	Learning Rate: 5.91813e-05
	LOSS [training: 0.13668892557614692 | validation: 0.10373865897887058]
	TIME [epoch: 8.83 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13508595237754367		[learning rate: 5.9074e-05]
		[batch 20/20] avg loss: 0.1390688560103309		[learning rate: 5.8966e-05]
	Learning Rate: 5.89665e-05
	LOSS [training: 0.1370774041939373 | validation: 0.1399002094579729]
	TIME [epoch: 8.85 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14764389761067137		[learning rate: 5.8859e-05]
		[batch 20/20] avg loss: 0.13198465475441049		[learning rate: 5.8752e-05]
	Learning Rate: 5.87525e-05
	LOSS [training: 0.13981427618254094 | validation: 0.11472549183000198]
	TIME [epoch: 8.84 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13496776399969085		[learning rate: 5.8646e-05]
		[batch 20/20] avg loss: 0.1383737302713463		[learning rate: 5.8539e-05]
	Learning Rate: 5.85393e-05
	LOSS [training: 0.13667074713551855 | validation: 0.11403149944897227]
	TIME [epoch: 8.83 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12538135931055008		[learning rate: 5.8433e-05]
		[batch 20/20] avg loss: 0.13919389187252373		[learning rate: 5.8327e-05]
	Learning Rate: 5.83268e-05
	LOSS [training: 0.1322876255915369 | validation: 0.12031644428750143]
	TIME [epoch: 8.83 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13946464276788081		[learning rate: 5.8221e-05]
		[batch 20/20] avg loss: 0.1280554564748445		[learning rate: 5.8115e-05]
	Learning Rate: 5.81152e-05
	LOSS [training: 0.13376004962136268 | validation: 0.12037851322321162]
	TIME [epoch: 8.84 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14250082829908833		[learning rate: 5.801e-05]
		[batch 20/20] avg loss: 0.13360656950485372		[learning rate: 5.7904e-05]
	Learning Rate: 5.79043e-05
	LOSS [training: 0.13805369890197106 | validation: 0.10101068637569695]
	TIME [epoch: 8.84 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1259018547351942		[learning rate: 5.7799e-05]
		[batch 20/20] avg loss: 0.13899382244816674		[learning rate: 5.7694e-05]
	Learning Rate: 5.76941e-05
	LOSS [training: 0.13244783859168047 | validation: 0.12343425787349929]
	TIME [epoch: 8.83 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13264773618159903		[learning rate: 5.7589e-05]
		[batch 20/20] avg loss: 0.13422961219197602		[learning rate: 5.7485e-05]
	Learning Rate: 5.74847e-05
	LOSS [training: 0.13343867418678754 | validation: 0.09377761891122678]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1519.pth
	Model improved!!!
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15333775439361824		[learning rate: 5.738e-05]
		[batch 20/20] avg loss: 0.12535810030556882		[learning rate: 5.7276e-05]
	Learning Rate: 5.72761e-05
	LOSS [training: 0.13934792734959356 | validation: 0.10988141375664787]
	TIME [epoch: 8.84 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13005322274226772		[learning rate: 5.7172e-05]
		[batch 20/20] avg loss: 0.13461152370566903		[learning rate: 5.7068e-05]
	Learning Rate: 5.70683e-05
	LOSS [training: 0.13233237322396837 | validation: 0.11458692314245707]
	TIME [epoch: 8.86 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13325373472861687		[learning rate: 5.6965e-05]
		[batch 20/20] avg loss: 0.13670601896610593		[learning rate: 5.6861e-05]
	Learning Rate: 5.68612e-05
	LOSS [training: 0.13497987684736137 | validation: 0.1108811232345952]
	TIME [epoch: 8.84 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13734777791468425		[learning rate: 5.6758e-05]
		[batch 20/20] avg loss: 0.134501954950961		[learning rate: 5.6655e-05]
	Learning Rate: 5.66548e-05
	LOSS [training: 0.13592486643282262 | validation: 0.12462266726879317]
	TIME [epoch: 8.84 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12131132987514229		[learning rate: 5.6552e-05]
		[batch 20/20] avg loss: 0.14386162869673566		[learning rate: 5.6449e-05]
	Learning Rate: 5.64492e-05
	LOSS [training: 0.13258647928593897 | validation: 0.10681268607883768]
	TIME [epoch: 8.84 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14247345661192776		[learning rate: 5.6347e-05]
		[batch 20/20] avg loss: 0.13553905472054434		[learning rate: 5.6244e-05]
	Learning Rate: 5.62444e-05
	LOSS [training: 0.13900625566623606 | validation: 0.10326459475065237]
	TIME [epoch: 8.84 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1360038574000943		[learning rate: 5.6142e-05]
		[batch 20/20] avg loss: 0.12872952690397507		[learning rate: 5.604e-05]
	Learning Rate: 5.60403e-05
	LOSS [training: 0.13236669215203467 | validation: 0.10420015940177373]
	TIME [epoch: 8.87 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13072867645596645		[learning rate: 5.5938e-05]
		[batch 20/20] avg loss: 0.13617292333600595		[learning rate: 5.5837e-05]
	Learning Rate: 5.58369e-05
	LOSS [training: 0.1334507998959862 | validation: 0.10525321865470261]
	TIME [epoch: 8.85 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12311081929683225		[learning rate: 5.5735e-05]
		[batch 20/20] avg loss: 0.13803230276980077		[learning rate: 5.5634e-05]
	Learning Rate: 5.56342e-05
	LOSS [training: 0.13057156103331652 | validation: 0.10833320584680083]
	TIME [epoch: 8.85 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13363702319108545		[learning rate: 5.5533e-05]
		[batch 20/20] avg loss: 0.14234941421589736		[learning rate: 5.5432e-05]
	Learning Rate: 5.54323e-05
	LOSS [training: 0.1379932187034914 | validation: 0.10111296275157845]
	TIME [epoch: 8.84 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12761351679896804		[learning rate: 5.5332e-05]
		[batch 20/20] avg loss: 0.13337912703834767		[learning rate: 5.5231e-05]
	Learning Rate: 5.52312e-05
	LOSS [training: 0.13049632191865784 | validation: 0.1139611465128818]
	TIME [epoch: 8.86 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13516080548305717		[learning rate: 5.5131e-05]
		[batch 20/20] avg loss: 0.13704492919929773		[learning rate: 5.5031e-05]
	Learning Rate: 5.50307e-05
	LOSS [training: 0.1361028673411775 | validation: 0.10953664384666348]
	TIME [epoch: 8.86 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12480161561547749		[learning rate: 5.4931e-05]
		[batch 20/20] avg loss: 0.14666231120248957		[learning rate: 5.4831e-05]
	Learning Rate: 5.4831e-05
	LOSS [training: 0.13573196340898352 | validation: 0.10495927655364139]
	TIME [epoch: 8.84 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12278596508894879		[learning rate: 5.4731e-05]
		[batch 20/20] avg loss: 0.14792084834825717		[learning rate: 5.4632e-05]
	Learning Rate: 5.4632e-05
	LOSS [training: 0.13535340671860296 | validation: 0.11126271959725842]
	TIME [epoch: 8.85 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13439795658561907		[learning rate: 5.4533e-05]
		[batch 20/20] avg loss: 0.14731881578360018		[learning rate: 5.4434e-05]
	Learning Rate: 5.44338e-05
	LOSS [training: 0.1408583861846096 | validation: 0.10963091845062181]
	TIME [epoch: 8.85 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1363696158089087		[learning rate: 5.4335e-05]
		[batch 20/20] avg loss: 0.1283368387370149		[learning rate: 5.4236e-05]
	Learning Rate: 5.42362e-05
	LOSS [training: 0.1323532272729618 | validation: 0.11371911650721898]
	TIME [epoch: 8.87 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13938983237475694		[learning rate: 5.4138e-05]
		[batch 20/20] avg loss: 0.133325833199949		[learning rate: 5.4039e-05]
	Learning Rate: 5.40394e-05
	LOSS [training: 0.13635783278735297 | validation: 0.10718954551616806]
	TIME [epoch: 8.86 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13743431899541753		[learning rate: 5.3941e-05]
		[batch 20/20] avg loss: 0.12709796778432195		[learning rate: 5.3843e-05]
	Learning Rate: 5.38433e-05
	LOSS [training: 0.1322661433898697 | validation: 0.12750705406788462]
	TIME [epoch: 8.85 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14808112925816116		[learning rate: 5.3746e-05]
		[batch 20/20] avg loss: 0.13641538184767832		[learning rate: 5.3648e-05]
	Learning Rate: 5.36479e-05
	LOSS [training: 0.14224825555291976 | validation: 0.11397652117360509]
	TIME [epoch: 8.85 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1227943793123271		[learning rate: 5.355e-05]
		[batch 20/20] avg loss: 0.1380910749451963		[learning rate: 5.3453e-05]
	Learning Rate: 5.34532e-05
	LOSS [training: 0.1304427271287617 | validation: 0.1087468353258627]
	TIME [epoch: 8.85 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1291564601339391		[learning rate: 5.3356e-05]
		[batch 20/20] avg loss: 0.13234265136236525		[learning rate: 5.3259e-05]
	Learning Rate: 5.32592e-05
	LOSS [training: 0.13074955574815217 | validation: 0.1377837715464245]
	TIME [epoch: 8.88 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13414653523625994		[learning rate: 5.3162e-05]
		[batch 20/20] avg loss: 0.12871221402186048		[learning rate: 5.3066e-05]
	Learning Rate: 5.30659e-05
	LOSS [training: 0.13142937462906024 | validation: 0.11090268467369772]
	TIME [epoch: 8.86 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1456160082611386		[learning rate: 5.297e-05]
		[batch 20/20] avg loss: 0.11730918696190265		[learning rate: 5.2873e-05]
	Learning Rate: 5.28734e-05
	LOSS [training: 0.13146259761152063 | validation: 0.11363822088972926]
	TIME [epoch: 8.86 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13722016993913408		[learning rate: 5.2777e-05]
		[batch 20/20] avg loss: 0.14026373797148475		[learning rate: 5.2681e-05]
	Learning Rate: 5.26815e-05
	LOSS [training: 0.13874195395530942 | validation: 0.13522404276289848]
	TIME [epoch: 8.86 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14409256977069862		[learning rate: 5.2586e-05]
		[batch 20/20] avg loss: 0.12071521916586248		[learning rate: 5.249e-05]
	Learning Rate: 5.24903e-05
	LOSS [training: 0.13240389446828055 | validation: 0.09920925581831345]
	TIME [epoch: 8.86 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12010883969341164		[learning rate: 5.2395e-05]
		[batch 20/20] avg loss: 0.1508873644676961		[learning rate: 5.23e-05]
	Learning Rate: 5.22998e-05
	LOSS [training: 0.1354981020805539 | validation: 0.12232024722526168]
	TIME [epoch: 8.87 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1409557430678428		[learning rate: 5.2205e-05]
		[batch 20/20] avg loss: 0.12553753693045444		[learning rate: 5.211e-05]
	Learning Rate: 5.211e-05
	LOSS [training: 0.13324663999914862 | validation: 0.11443329161656528]
	TIME [epoch: 8.85 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12435039324794869		[learning rate: 5.2015e-05]
		[batch 20/20] avg loss: 0.1354975267887602		[learning rate: 5.1921e-05]
	Learning Rate: 5.19209e-05
	LOSS [training: 0.12992396001835443 | validation: 0.1096233702948066]
	TIME [epoch: 8.85 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13788400320522107		[learning rate: 5.1827e-05]
		[batch 20/20] avg loss: 0.12685313636477297		[learning rate: 5.1732e-05]
	Learning Rate: 5.17325e-05
	LOSS [training: 0.132368569784997 | validation: 0.1157468721231453]
	TIME [epoch: 8.86 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14010356694978138		[learning rate: 5.1639e-05]
		[batch 20/20] avg loss: 0.1477550186064513		[learning rate: 5.1545e-05]
	Learning Rate: 5.15447e-05
	LOSS [training: 0.14392929277811634 | validation: 0.11794974535851026]
	TIME [epoch: 8.87 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13366691226336974		[learning rate: 5.1451e-05]
		[batch 20/20] avg loss: 0.1359933421170799		[learning rate: 5.1358e-05]
	Learning Rate: 5.13577e-05
	LOSS [training: 0.13483012719022486 | validation: 0.12123379400969345]
	TIME [epoch: 8.86 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1341357719002943		[learning rate: 5.1264e-05]
		[batch 20/20] avg loss: 0.14436166009078374		[learning rate: 5.1171e-05]
	Learning Rate: 5.11713e-05
	LOSS [training: 0.139248715995539 | validation: 0.11630197399797351]
	TIME [epoch: 8.85 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12714943067153195		[learning rate: 5.1078e-05]
		[batch 20/20] avg loss: 0.14703128521669723		[learning rate: 5.0986e-05]
	Learning Rate: 5.09856e-05
	LOSS [training: 0.13709035794411456 | validation: 0.09802014471804824]
	TIME [epoch: 8.84 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13980880138710391		[learning rate: 5.0893e-05]
		[batch 20/20] avg loss: 0.13748161110284085		[learning rate: 5.0801e-05]
	Learning Rate: 5.08006e-05
	LOSS [training: 0.13864520624497237 | validation: 0.107360039973639]
	TIME [epoch: 8.85 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.125317425758751		[learning rate: 5.0708e-05]
		[batch 20/20] avg loss: 0.1344099563947548		[learning rate: 5.0616e-05]
	Learning Rate: 5.06162e-05
	LOSS [training: 0.12986369107675289 | validation: 0.1163243102412455]
	TIME [epoch: 8.87 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12703834716000845		[learning rate: 5.0524e-05]
		[batch 20/20] avg loss: 0.1432637195387767		[learning rate: 5.0433e-05]
	Learning Rate: 5.04325e-05
	LOSS [training: 0.13515103334939257 | validation: 0.12363651673724421]
	TIME [epoch: 8.86 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13194263359673258		[learning rate: 5.0341e-05]
		[batch 20/20] avg loss: 0.1359432397627392		[learning rate: 5.0249e-05]
	Learning Rate: 5.02495e-05
	LOSS [training: 0.13394293667973586 | validation: 0.11389158271076101]
	TIME [epoch: 8.85 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13882255819246442		[learning rate: 5.0158e-05]
		[batch 20/20] avg loss: 0.12739451612672437		[learning rate: 5.0067e-05]
	Learning Rate: 5.00671e-05
	LOSS [training: 0.13310853715959436 | validation: 0.11520849106871871]
	TIME [epoch: 8.85 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14836173802219638		[learning rate: 4.9976e-05]
		[batch 20/20] avg loss: 0.11584087401631331		[learning rate: 4.9885e-05]
	Learning Rate: 4.98854e-05
	LOSS [training: 0.13210130601925488 | validation: 0.12553768540572074]
	TIME [epoch: 8.85 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14225481926276048		[learning rate: 4.9795e-05]
		[batch 20/20] avg loss: 0.12651428066545709		[learning rate: 4.9704e-05]
	Learning Rate: 4.97044e-05
	LOSS [training: 0.1343845499641088 | validation: 0.12250784430692946]
	TIME [epoch: 8.86 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13328764351947106		[learning rate: 4.9614e-05]
		[batch 20/20] avg loss: 0.12325371574789679		[learning rate: 4.9524e-05]
	Learning Rate: 4.9524e-05
	LOSS [training: 0.12827067963368394 | validation: 0.1079876442809341]
	TIME [epoch: 8.85 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1253204269655318		[learning rate: 4.9434e-05]
		[batch 20/20] avg loss: 0.1407929906823148		[learning rate: 4.9344e-05]
	Learning Rate: 4.93443e-05
	LOSS [training: 0.1330567088239233 | validation: 0.12627688735210063]
	TIME [epoch: 8.84 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1355994679725883		[learning rate: 4.9255e-05]
		[batch 20/20] avg loss: 0.12135971087828731		[learning rate: 4.9165e-05]
	Learning Rate: 4.91652e-05
	LOSS [training: 0.1284795894254378 | validation: 0.1075248746267562]
	TIME [epoch: 8.85 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12255650257603048		[learning rate: 4.9076e-05]
		[batch 20/20] avg loss: 0.13734053214350145		[learning rate: 4.8987e-05]
	Learning Rate: 4.89868e-05
	LOSS [training: 0.12994851735976595 | validation: 0.11714084225829541]
	TIME [epoch: 8.87 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14767449319783837		[learning rate: 4.8898e-05]
		[batch 20/20] avg loss: 0.12897726612576094		[learning rate: 4.8809e-05]
	Learning Rate: 4.8809e-05
	LOSS [training: 0.13832587966179966 | validation: 0.11028318163523668]
	TIME [epoch: 8.85 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1283809205949678		[learning rate: 4.872e-05]
		[batch 20/20] avg loss: 0.13600739462046854		[learning rate: 4.8632e-05]
	Learning Rate: 4.86319e-05
	LOSS [training: 0.1321941576077182 | validation: 0.10531312393821696]
	TIME [epoch: 8.85 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12111005176281657		[learning rate: 4.8544e-05]
		[batch 20/20] avg loss: 0.14041324279373618		[learning rate: 4.8455e-05]
	Learning Rate: 4.84554e-05
	LOSS [training: 0.13076164727827636 | validation: 0.11505086609958538]
	TIME [epoch: 8.84 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1284114943465998		[learning rate: 4.8367e-05]
		[batch 20/20] avg loss: 0.13689745039165033		[learning rate: 4.828e-05]
	Learning Rate: 4.82795e-05
	LOSS [training: 0.13265447236912506 | validation: 0.13141269056917884]
	TIME [epoch: 8.85 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1316692072653871		[learning rate: 4.8192e-05]
		[batch 20/20] avg loss: 0.13812724676272772		[learning rate: 4.8104e-05]
	Learning Rate: 4.81043e-05
	LOSS [training: 0.13489822701405743 | validation: 0.09273051032191225]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1568.pth
	Model improved!!!
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12074886105115756		[learning rate: 4.8017e-05]
		[batch 20/20] avg loss: 0.14188695811210286		[learning rate: 4.793e-05]
	Learning Rate: 4.79298e-05
	LOSS [training: 0.13131790958163025 | validation: 0.09867357710778754]
	TIME [epoch: 8.85 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13387652774801967		[learning rate: 4.7843e-05]
		[batch 20/20] avg loss: 0.1307297374031629		[learning rate: 4.7756e-05]
	Learning Rate: 4.77558e-05
	LOSS [training: 0.1323031325755913 | validation: 0.11202769195716297]
	TIME [epoch: 8.84 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1407913310829256		[learning rate: 4.7669e-05]
		[batch 20/20] avg loss: 0.11775796834478007		[learning rate: 4.7583e-05]
	Learning Rate: 4.75825e-05
	LOSS [training: 0.12927464971385286 | validation: 0.1099446434161217]
	TIME [epoch: 8.84 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13717739800052747		[learning rate: 4.7496e-05]
		[batch 20/20] avg loss: 0.1283453977173344		[learning rate: 4.741e-05]
	Learning Rate: 4.74098e-05
	LOSS [training: 0.13276139785893093 | validation: 0.11097603228141367]
	TIME [epoch: 8.84 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1403913220202629		[learning rate: 4.7324e-05]
		[batch 20/20] avg loss: 0.12830641842772278		[learning rate: 4.7238e-05]
	Learning Rate: 4.72378e-05
	LOSS [training: 0.13434887022399283 | validation: 0.10544051463664442]
	TIME [epoch: 8.86 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13196133690829523		[learning rate: 4.7152e-05]
		[batch 20/20] avg loss: 0.128835365306676		[learning rate: 4.7066e-05]
	Learning Rate: 4.70664e-05
	LOSS [training: 0.13039835110748557 | validation: 0.10563719245337992]
	TIME [epoch: 8.84 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12998620147390005		[learning rate: 4.6981e-05]
		[batch 20/20] avg loss: 0.13224884431740844		[learning rate: 4.6896e-05]
	Learning Rate: 4.68955e-05
	LOSS [training: 0.13111752289565423 | validation: 0.11214455127783893]
	TIME [epoch: 8.84 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12870136070113866		[learning rate: 4.681e-05]
		[batch 20/20] avg loss: 0.1266470509956993		[learning rate: 4.6725e-05]
	Learning Rate: 4.67254e-05
	LOSS [training: 0.127674205848419 | validation: 0.11180796190313305]
	TIME [epoch: 8.84 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1313491292397413		[learning rate: 4.6641e-05]
		[batch 20/20] avg loss: 0.1283317237410237		[learning rate: 4.6556e-05]
	Learning Rate: 4.65558e-05
	LOSS [training: 0.12984042649038247 | validation: 0.11318645377039377]
	TIME [epoch: 8.86 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12882653330392696		[learning rate: 4.6471e-05]
		[batch 20/20] avg loss: 0.14573510843300425		[learning rate: 4.6387e-05]
	Learning Rate: 4.63868e-05
	LOSS [training: 0.1372808208684656 | validation: 0.12949706093800095]
	TIME [epoch: 8.85 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1289852554337035		[learning rate: 4.6303e-05]
		[batch 20/20] avg loss: 0.135367185394394		[learning rate: 4.6219e-05]
	Learning Rate: 4.62185e-05
	LOSS [training: 0.13217622041404875 | validation: 0.10847653543410922]
	TIME [epoch: 8.84 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12711646726429165		[learning rate: 4.6135e-05]
		[batch 20/20] avg loss: 0.13622781462326125		[learning rate: 4.6051e-05]
	Learning Rate: 4.60508e-05
	LOSS [training: 0.13167214094377647 | validation: 0.11206935044457414]
	TIME [epoch: 8.84 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11072967868688907		[learning rate: 4.5967e-05]
		[batch 20/20] avg loss: 0.13827292062281793		[learning rate: 4.5884e-05]
	Learning Rate: 4.58836e-05
	LOSS [training: 0.1245012996548535 | validation: 0.10859002045641011]
	TIME [epoch: 8.84 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1260185191677316		[learning rate: 4.58e-05]
		[batch 20/20] avg loss: 0.13769773770673172		[learning rate: 4.5717e-05]
	Learning Rate: 4.57171e-05
	LOSS [training: 0.13185812843723166 | validation: 0.12990065510779739]
	TIME [epoch: 8.86 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12332288537426024		[learning rate: 4.5634e-05]
		[batch 20/20] avg loss: 0.15010532784246092		[learning rate: 4.5551e-05]
	Learning Rate: 4.55512e-05
	LOSS [training: 0.13671410660836056 | validation: 0.10047076131692977]
	TIME [epoch: 8.84 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12340052550544353		[learning rate: 4.5468e-05]
		[batch 20/20] avg loss: 0.13498400286383846		[learning rate: 4.5386e-05]
	Learning Rate: 4.53859e-05
	LOSS [training: 0.12919226418464097 | validation: 0.10985283863374945]
	TIME [epoch: 8.84 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12208932544329225		[learning rate: 4.5303e-05]
		[batch 20/20] avg loss: 0.13514957237174213		[learning rate: 4.5221e-05]
	Learning Rate: 4.52212e-05
	LOSS [training: 0.1286194489075172 | validation: 0.1182157776475505]
	TIME [epoch: 8.85 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14117304587872717		[learning rate: 4.5139e-05]
		[batch 20/20] avg loss: 0.11611152932331392		[learning rate: 4.5057e-05]
	Learning Rate: 4.50571e-05
	LOSS [training: 0.1286422876010205 | validation: 0.1072848950223016]
	TIME [epoch: 8.84 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12707280733010734		[learning rate: 4.4975e-05]
		[batch 20/20] avg loss: 0.13251895216980636		[learning rate: 4.4894e-05]
	Learning Rate: 4.48936e-05
	LOSS [training: 0.12979587974995688 | validation: 0.09448932120607581]
	TIME [epoch: 8.86 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12577374659448068		[learning rate: 4.4812e-05]
		[batch 20/20] avg loss: 0.14119771386408275		[learning rate: 4.4731e-05]
	Learning Rate: 4.47307e-05
	LOSS [training: 0.13348573022928173 | validation: 0.09446473174604036]
	TIME [epoch: 8.84 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1432224388318569		[learning rate: 4.4649e-05]
		[batch 20/20] avg loss: 0.12314694767096837		[learning rate: 4.4568e-05]
	Learning Rate: 4.45683e-05
	LOSS [training: 0.13318469325141263 | validation: 0.0983012553613818]
	TIME [epoch: 8.84 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14548048581396478		[learning rate: 4.4487e-05]
		[batch 20/20] avg loss: 0.12267353804188356		[learning rate: 4.4407e-05]
	Learning Rate: 4.44066e-05
	LOSS [training: 0.13407701192792415 | validation: 0.11666360048578316]
	TIME [epoch: 8.84 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13977605267862503		[learning rate: 4.4326e-05]
		[batch 20/20] avg loss: 0.11750396228516387		[learning rate: 4.4245e-05]
	Learning Rate: 4.42454e-05
	LOSS [training: 0.12864000748189441 | validation: 0.11505457784952072]
	TIME [epoch: 8.86 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1343252336233412		[learning rate: 4.4165e-05]
		[batch 20/20] avg loss: 0.1335562505081755		[learning rate: 4.4085e-05]
	Learning Rate: 4.40849e-05
	LOSS [training: 0.13394074206575832 | validation: 0.10812514843758192]
	TIME [epoch: 8.85 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12799126413651873		[learning rate: 4.4005e-05]
		[batch 20/20] avg loss: 0.1379685094954891		[learning rate: 4.3925e-05]
	Learning Rate: 4.39249e-05
	LOSS [training: 0.13297988681600392 | validation: 0.1097607009383659]
	TIME [epoch: 8.84 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12343920187383924		[learning rate: 4.3845e-05]
		[batch 20/20] avg loss: 0.13994393565927196		[learning rate: 4.3765e-05]
	Learning Rate: 4.37655e-05
	LOSS [training: 0.13169156876655558 | validation: 0.10768395256958266]
	TIME [epoch: 8.85 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14012064316912176		[learning rate: 4.3686e-05]
		[batch 20/20] avg loss: 0.11933078927566101		[learning rate: 4.3607e-05]
	Learning Rate: 4.36067e-05
	LOSS [training: 0.12972571622239137 | validation: 0.09506483581366502]
	TIME [epoch: 8.84 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13252434241591665		[learning rate: 4.3527e-05]
		[batch 20/20] avg loss: 0.13023573377079317		[learning rate: 4.3448e-05]
	Learning Rate: 4.34484e-05
	LOSS [training: 0.13138003809335488 | validation: 0.09954546630208966]
	TIME [epoch: 8.86 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13052993374056734		[learning rate: 4.3369e-05]
		[batch 20/20] avg loss: 0.1265252228354155		[learning rate: 4.3291e-05]
	Learning Rate: 4.32907e-05
	LOSS [training: 0.1285275782879914 | validation: 0.10139174842120385]
	TIME [epoch: 8.85 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1319935157347934		[learning rate: 4.3212e-05]
		[batch 20/20] avg loss: 0.13664547647941547		[learning rate: 4.3134e-05]
	Learning Rate: 4.31336e-05
	LOSS [training: 0.13431949610710445 | validation: 0.12006001389111379]
	TIME [epoch: 8.84 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13416672687064365		[learning rate: 4.3055e-05]
		[batch 20/20] avg loss: 0.12704618393730252		[learning rate: 4.2977e-05]
	Learning Rate: 4.29771e-05
	LOSS [training: 0.1306064554039731 | validation: 0.11925412553308137]
	TIME [epoch: 8.84 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1367657498469697		[learning rate: 4.2899e-05]
		[batch 20/20] avg loss: 0.12178172885071945		[learning rate: 4.2821e-05]
	Learning Rate: 4.28211e-05
	LOSS [training: 0.1292737393488446 | validation: 0.10690306343913471]
	TIME [epoch: 8.85 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1406775672614088		[learning rate: 4.2743e-05]
		[batch 20/20] avg loss: 0.12762177351360832		[learning rate: 4.2666e-05]
	Learning Rate: 4.26657e-05
	LOSS [training: 0.13414967038750855 | validation: 0.11244508957266709]
	TIME [epoch: 8.86 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12965591114797242		[learning rate: 4.2588e-05]
		[batch 20/20] avg loss: 0.13118255909318555		[learning rate: 4.2511e-05]
	Learning Rate: 4.25109e-05
	LOSS [training: 0.13041923512057893 | validation: 0.10987770196548978]
	TIME [epoch: 8.85 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13205950335688804		[learning rate: 4.2434e-05]
		[batch 20/20] avg loss: 0.12799428340941682		[learning rate: 4.2357e-05]
	Learning Rate: 4.23566e-05
	LOSS [training: 0.1300268933831524 | validation: 0.10419995726789492]
	TIME [epoch: 8.84 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14429029142275518		[learning rate: 4.228e-05]
		[batch 20/20] avg loss: 0.1164950909881878		[learning rate: 4.2203e-05]
	Learning Rate: 4.22029e-05
	LOSS [training: 0.13039269120547148 | validation: 0.10634477536392653]
	TIME [epoch: 8.85 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1403802934163783		[learning rate: 4.2126e-05]
		[batch 20/20] avg loss: 0.11925845573609675		[learning rate: 4.205e-05]
	Learning Rate: 4.20497e-05
	LOSS [training: 0.12981937457623755 | validation: 0.11157209672395839]
	TIME [epoch: 8.86 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11828455220084892		[learning rate: 4.1973e-05]
		[batch 20/20] avg loss: 0.13784710500689484		[learning rate: 4.1897e-05]
	Learning Rate: 4.18971e-05
	LOSS [training: 0.1280658286038719 | validation: 0.11849344424060317]
	TIME [epoch: 8.86 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12983002355152104		[learning rate: 4.1821e-05]
		[batch 20/20] avg loss: 0.13292699466826982		[learning rate: 4.1745e-05]
	Learning Rate: 4.17451e-05
	LOSS [training: 0.13137850910989543 | validation: 0.10510996147142546]
	TIME [epoch: 8.85 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1400303828445552		[learning rate: 4.1669e-05]
		[batch 20/20] avg loss: 0.12528634928406707		[learning rate: 4.1594e-05]
	Learning Rate: 4.15936e-05
	LOSS [training: 0.13265836606431117 | validation: 0.11137477931252526]
	TIME [epoch: 8.84 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13928760424680176		[learning rate: 4.1518e-05]
		[batch 20/20] avg loss: 0.1269942491262827		[learning rate: 4.1443e-05]
	Learning Rate: 4.14426e-05
	LOSS [training: 0.13314092668654226 | validation: 0.10672146687972077]
	TIME [epoch: 8.85 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13039434758313667		[learning rate: 4.1367e-05]
		[batch 20/20] avg loss: 0.13659797675925694		[learning rate: 4.1292e-05]
	Learning Rate: 4.12922e-05
	LOSS [training: 0.13349616217119678 | validation: 0.11933851172439634]
	TIME [epoch: 8.86 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11697173693685148		[learning rate: 4.1217e-05]
		[batch 20/20] avg loss: 0.14558969216184253		[learning rate: 4.1142e-05]
	Learning Rate: 4.11424e-05
	LOSS [training: 0.13128071454934698 | validation: 0.10785660069593851]
	TIME [epoch: 8.85 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1274727799273458		[learning rate: 4.1068e-05]
		[batch 20/20] avg loss: 0.14032572918989722		[learning rate: 4.0993e-05]
	Learning Rate: 4.09931e-05
	LOSS [training: 0.13389925455862148 | validation: 0.10215423896877915]
	TIME [epoch: 8.85 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1275346668904032		[learning rate: 4.0919e-05]
		[batch 20/20] avg loss: 0.13294377728583623		[learning rate: 4.0844e-05]
	Learning Rate: 4.08443e-05
	LOSS [training: 0.1302392220881197 | validation: 0.11029243115069173]
	TIME [epoch: 8.85 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13741673802955878		[learning rate: 4.077e-05]
		[batch 20/20] avg loss: 0.12306732796843248		[learning rate: 4.0696e-05]
	Learning Rate: 4.06961e-05
	LOSS [training: 0.13024203299899564 | validation: 0.12157037514559998]
	TIME [epoch: 8.85 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12471557698683274		[learning rate: 4.0622e-05]
		[batch 20/20] avg loss: 0.14471066688299108		[learning rate: 4.0548e-05]
	Learning Rate: 4.05484e-05
	LOSS [training: 0.1347131219349119 | validation: 0.11059328734183332]
	TIME [epoch: 8.86 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13505788376194688		[learning rate: 4.0475e-05]
		[batch 20/20] avg loss: 0.12603117536591257		[learning rate: 4.0401e-05]
	Learning Rate: 4.04012e-05
	LOSS [training: 0.13054452956392976 | validation: 0.09557639769845264]
	TIME [epoch: 8.85 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12588171409362106		[learning rate: 4.0328e-05]
		[batch 20/20] avg loss: 0.13462476103186208		[learning rate: 4.0255e-05]
	Learning Rate: 4.02546e-05
	LOSS [training: 0.1302532375627416 | validation: 0.10898592406625109]
	TIME [epoch: 8.85 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13218077398587819		[learning rate: 4.0182e-05]
		[batch 20/20] avg loss: 0.14178904646823426		[learning rate: 4.0109e-05]
	Learning Rate: 4.01085e-05
	LOSS [training: 0.13698491022705622 | validation: 0.10225903795261607]
	TIME [epoch: 8.84 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13190874131210886		[learning rate: 4.0036e-05]
		[batch 20/20] avg loss: 0.1300732627015843		[learning rate: 3.9963e-05]
	Learning Rate: 3.9963e-05
	LOSS [training: 0.1309910020068466 | validation: 0.10599781601739344]
	TIME [epoch: 8.85 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13891422473746196		[learning rate: 3.989e-05]
		[batch 20/20] avg loss: 0.12047954733527269		[learning rate: 3.9818e-05]
	Learning Rate: 3.9818e-05
	LOSS [training: 0.12969688603636734 | validation: 0.09722622909153941]
	TIME [epoch: 8.87 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14497188649226445		[learning rate: 3.9746e-05]
		[batch 20/20] avg loss: 0.11820485807332617		[learning rate: 3.9673e-05]
	Learning Rate: 3.96735e-05
	LOSS [training: 0.1315883722827953 | validation: 0.11953494304693148]
	TIME [epoch: 8.85 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12935348600972296		[learning rate: 3.9601e-05]
		[batch 20/20] avg loss: 0.13556240029777275		[learning rate: 3.9529e-05]
	Learning Rate: 3.95295e-05
	LOSS [training: 0.13245794315374787 | validation: 0.10643336696313668]
	TIME [epoch: 8.86 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13795627732494037		[learning rate: 3.9458e-05]
		[batch 20/20] avg loss: 0.1268067732846602		[learning rate: 3.9386e-05]
	Learning Rate: 3.9386e-05
	LOSS [training: 0.13238152530480024 | validation: 0.10353991772518818]
	TIME [epoch: 8.85 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14025518674716397		[learning rate: 3.9315e-05]
		[batch 20/20] avg loss: 0.1183168896484671		[learning rate: 3.9243e-05]
	Learning Rate: 3.92431e-05
	LOSS [training: 0.12928603819781553 | validation: 0.11001867805732665]
	TIME [epoch: 8.87 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1203991651521698		[learning rate: 3.9172e-05]
		[batch 20/20] avg loss: 0.13960558402496734		[learning rate: 3.9101e-05]
	Learning Rate: 3.91007e-05
	LOSS [training: 0.13000237458856856 | validation: 0.1061930569353053]
	TIME [epoch: 8.85 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12927627436879882		[learning rate: 3.903e-05]
		[batch 20/20] avg loss: 0.14209480043989497		[learning rate: 3.8959e-05]
	Learning Rate: 3.89588e-05
	LOSS [training: 0.13568553740434688 | validation: 0.10131343800789934]
	TIME [epoch: 8.84 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1333155080156242		[learning rate: 3.8888e-05]
		[batch 20/20] avg loss: 0.13420187281931611		[learning rate: 3.8817e-05]
	Learning Rate: 3.88174e-05
	LOSS [training: 0.13375869041747016 | validation: 0.10716722859412973]
	TIME [epoch: 8.84 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1369897143115109		[learning rate: 3.8747e-05]
		[batch 20/20] avg loss: 0.12565703292821087		[learning rate: 3.8677e-05]
	Learning Rate: 3.86765e-05
	LOSS [training: 0.13132337361986088 | validation: 0.10730045866209877]
	TIME [epoch: 8.84 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13060126241571085		[learning rate: 3.8606e-05]
		[batch 20/20] avg loss: 0.12776816083063883		[learning rate: 3.8536e-05]
	Learning Rate: 3.85362e-05
	LOSS [training: 0.1291847116231748 | validation: 0.1091879753728434]
	TIME [epoch: 8.87 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14465800877226853		[learning rate: 3.8466e-05]
		[batch 20/20] avg loss: 0.12666510373604134		[learning rate: 3.8396e-05]
	Learning Rate: 3.83963e-05
	LOSS [training: 0.13566155625415494 | validation: 0.10640149274399875]
	TIME [epoch: 8.85 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12618747865939323		[learning rate: 3.8327e-05]
		[batch 20/20] avg loss: 0.13208050242776745		[learning rate: 3.8257e-05]
	Learning Rate: 3.8257e-05
	LOSS [training: 0.12913399054358035 | validation: 0.10501766779028812]
	TIME [epoch: 8.85 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1327249947561345		[learning rate: 3.8187e-05]
		[batch 20/20] avg loss: 0.13514796884234428		[learning rate: 3.8118e-05]
	Learning Rate: 3.81181e-05
	LOSS [training: 0.13393648179923937 | validation: 0.11615320633788777]
	TIME [epoch: 8.84 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14532286471832725		[learning rate: 3.8049e-05]
		[batch 20/20] avg loss: 0.11812276586350053		[learning rate: 3.798e-05]
	Learning Rate: 3.79798e-05
	LOSS [training: 0.1317228152909139 | validation: 0.10405884118583963]
	TIME [epoch: 8.85 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12903907443543242		[learning rate: 3.7911e-05]
		[batch 20/20] avg loss: 0.13541178620510883		[learning rate: 3.7842e-05]
	Learning Rate: 3.7842e-05
	LOSS [training: 0.13222543032027062 | validation: 0.10501953812953903]
	TIME [epoch: 8.87 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11778258044348618		[learning rate: 3.7773e-05]
		[batch 20/20] avg loss: 0.13882089019692453		[learning rate: 3.7705e-05]
	Learning Rate: 3.77046e-05
	LOSS [training: 0.12830173532020536 | validation: 0.12023801344652807]
	TIME [epoch: 8.84 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13201592728465897		[learning rate: 3.7636e-05]
		[batch 20/20] avg loss: 0.12257644362227862		[learning rate: 3.7568e-05]
	Learning Rate: 3.75678e-05
	LOSS [training: 0.1272961854534688 | validation: 0.10521154904892471]
	TIME [epoch: 8.85 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14909446218470152		[learning rate: 3.75e-05]
		[batch 20/20] avg loss: 0.11834865009935529		[learning rate: 3.7431e-05]
	Learning Rate: 3.74315e-05
	LOSS [training: 0.13372155614202838 | validation: 0.11165857109166585]
	TIME [epoch: 8.84 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14020436678653664		[learning rate: 3.7363e-05]
		[batch 20/20] avg loss: 0.11845778656144283		[learning rate: 3.7296e-05]
	Learning Rate: 3.72956e-05
	LOSS [training: 0.12933107667398974 | validation: 0.09860775578424048]
	TIME [epoch: 8.87 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1239887052913496		[learning rate: 3.7228e-05]
		[batch 20/20] avg loss: 0.13861189553004682		[learning rate: 3.716e-05]
	Learning Rate: 3.71603e-05
	LOSS [training: 0.13130030041069823 | validation: 0.10896948390626053]
	TIME [epoch: 8.85 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12650360165510932		[learning rate: 3.7093e-05]
		[batch 20/20] avg loss: 0.14700242392801302		[learning rate: 3.7025e-05]
	Learning Rate: 3.70254e-05
	LOSS [training: 0.13675301279156116 | validation: 0.11489581685607712]
	TIME [epoch: 8.84 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13224671604031413		[learning rate: 3.6958e-05]
		[batch 20/20] avg loss: 0.13376814571229023		[learning rate: 3.6891e-05]
	Learning Rate: 3.68911e-05
	LOSS [training: 0.1330074308763022 | validation: 0.11827314772893605]
	TIME [epoch: 8.84 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12746943675497752		[learning rate: 3.6824e-05]
		[batch 20/20] avg loss: 0.1416152829586544		[learning rate: 3.6757e-05]
	Learning Rate: 3.67572e-05
	LOSS [training: 0.13454235985681592 | validation: 0.11917894469419878]
	TIME [epoch: 8.84 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13493079216059112		[learning rate: 3.669e-05]
		[batch 20/20] avg loss: 0.1297622780154601		[learning rate: 3.6624e-05]
	Learning Rate: 3.66238e-05
	LOSS [training: 0.13234653508802557 | validation: 0.10173736590643345]
	TIME [epoch: 8.87 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11772583299090014		[learning rate: 3.6557e-05]
		[batch 20/20] avg loss: 0.14318810633184978		[learning rate: 3.6491e-05]
	Learning Rate: 3.64909e-05
	LOSS [training: 0.13045696966137496 | validation: 0.09599929051449807]
	TIME [epoch: 8.85 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1362720965059228		[learning rate: 3.6425e-05]
		[batch 20/20] avg loss: 0.13046606972843738		[learning rate: 3.6358e-05]
	Learning Rate: 3.63584e-05
	LOSS [training: 0.13336908311718013 | validation: 0.12597046356705546]
	TIME [epoch: 8.86 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12614553418757105		[learning rate: 3.6292e-05]
		[batch 20/20] avg loss: 0.13526404274345913		[learning rate: 3.6226e-05]
	Learning Rate: 3.62265e-05
	LOSS [training: 0.1307047884655151 | validation: 0.1048992957183524]
	TIME [epoch: 8.85 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1358445267646178		[learning rate: 3.6161e-05]
		[batch 20/20] avg loss: 0.12503216322771715		[learning rate: 3.6095e-05]
	Learning Rate: 3.6095e-05
	LOSS [training: 0.13043834499616744 | validation: 0.1063548481996546]
	TIME [epoch: 8.84 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13308833247477922		[learning rate: 3.6029e-05]
		[batch 20/20] avg loss: 0.13125388201106572		[learning rate: 3.5964e-05]
	Learning Rate: 3.5964e-05
	LOSS [training: 0.13217110724292244 | validation: 0.10071858140409008]
	TIME [epoch: 8.86 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13890175388539205		[learning rate: 3.5899e-05]
		[batch 20/20] avg loss: 0.11827303920966348		[learning rate: 3.5834e-05]
	Learning Rate: 3.58335e-05
	LOSS [training: 0.12858739654752777 | validation: 0.10543417555306533]
	TIME [epoch: 8.85 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12160976926928457		[learning rate: 3.5768e-05]
		[batch 20/20] avg loss: 0.1413139420993458		[learning rate: 3.5703e-05]
	Learning Rate: 3.57035e-05
	LOSS [training: 0.13146185568431518 | validation: 0.10753217611776208]
	TIME [epoch: 8.84 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12373335644929406		[learning rate: 3.5639e-05]
		[batch 20/20] avg loss: 0.13748990912412576		[learning rate: 3.5574e-05]
	Learning Rate: 3.55739e-05
	LOSS [training: 0.13061163278670992 | validation: 0.11120205063904733]
	TIME [epoch: 8.85 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11915797278204754		[learning rate: 3.5509e-05]
		[batch 20/20] avg loss: 0.1336602098789011		[learning rate: 3.5445e-05]
	Learning Rate: 3.54448e-05
	LOSS [training: 0.12640909133047434 | validation: 0.10520011255432654]
	TIME [epoch: 8.86 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12255305858138979		[learning rate: 3.538e-05]
		[batch 20/20] avg loss: 0.13766466178846454		[learning rate: 3.5316e-05]
	Learning Rate: 3.53162e-05
	LOSS [training: 0.13010886018492715 | validation: 0.10200893881105688]
	TIME [epoch: 8.85 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1484355127813016		[learning rate: 3.5252e-05]
		[batch 20/20] avg loss: 0.10730639767417124		[learning rate: 3.5188e-05]
	Learning Rate: 3.5188e-05
	LOSS [training: 0.12787095522773645 | validation: 0.10258509709962202]
	TIME [epoch: 8.84 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12255099446930329		[learning rate: 3.5124e-05]
		[batch 20/20] avg loss: 0.1308076587017431		[learning rate: 3.506e-05]
	Learning Rate: 3.50603e-05
	LOSS [training: 0.1266793265855232 | validation: 0.10465953880612143]
	TIME [epoch: 8.84 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13391824075265224		[learning rate: 3.4997e-05]
		[batch 20/20] avg loss: 0.12491379506923543		[learning rate: 3.4933e-05]
	Learning Rate: 3.49331e-05
	LOSS [training: 0.12941601791094384 | validation: 0.1030585160565399]
	TIME [epoch: 8.84 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13805335734775573		[learning rate: 3.487e-05]
		[batch 20/20] avg loss: 0.1167602488241728		[learning rate: 3.4806e-05]
	Learning Rate: 3.48063e-05
	LOSS [training: 0.12740680308596425 | validation: 0.10589566455094168]
	TIME [epoch: 8.86 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12057712041347242		[learning rate: 3.4743e-05]
		[batch 20/20] avg loss: 0.1330491990165245		[learning rate: 3.468e-05]
	Learning Rate: 3.468e-05
	LOSS [training: 0.12681315971499846 | validation: 0.10663804036275294]
	TIME [epoch: 8.85 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1265399713324577		[learning rate: 3.4617e-05]
		[batch 20/20] avg loss: 0.12545109164273768		[learning rate: 3.4554e-05]
	Learning Rate: 3.45541e-05
	LOSS [training: 0.12599553148759768 | validation: 0.10372615596979098]
	TIME [epoch: 8.85 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.119373779453085		[learning rate: 3.4491e-05]
		[batch 20/20] avg loss: 0.1388424171722309		[learning rate: 3.4429e-05]
	Learning Rate: 3.44287e-05
	LOSS [training: 0.12910809831265796 | validation: 0.11457688563912481]
	TIME [epoch: 8.85 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14019713250446794		[learning rate: 3.4366e-05]
		[batch 20/20] avg loss: 0.1288464054116013		[learning rate: 3.4304e-05]
	Learning Rate: 3.43038e-05
	LOSS [training: 0.13452176895803464 | validation: 0.10634388299080581]
	TIME [epoch: 8.85 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12020556807248028		[learning rate: 3.4241e-05]
		[batch 20/20] avg loss: 0.13472586929841476		[learning rate: 3.4179e-05]
	Learning Rate: 3.41793e-05
	LOSS [training: 0.12746571868544754 | validation: 0.10429563200815062]
	TIME [epoch: 8.86 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12110255695820574		[learning rate: 3.4117e-05]
		[batch 20/20] avg loss: 0.14760419757834425		[learning rate: 3.4055e-05]
	Learning Rate: 3.40553e-05
	LOSS [training: 0.134353377268275 | validation: 0.10532766760712993]
	TIME [epoch: 8.86 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13576815275063459		[learning rate: 3.3993e-05]
		[batch 20/20] avg loss: 0.12143475168990511		[learning rate: 3.3932e-05]
	Learning Rate: 3.39317e-05
	LOSS [training: 0.12860145222026984 | validation: 0.10073352131983188]
	TIME [epoch: 8.84 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12906556663315835		[learning rate: 3.387e-05]
		[batch 20/20] avg loss: 0.12319807883830522		[learning rate: 3.3809e-05]
	Learning Rate: 3.38085e-05
	LOSS [training: 0.1261318227357318 | validation: 0.11593223968341913]
	TIME [epoch: 8.85 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.133309734169154		[learning rate: 3.3747e-05]
		[batch 20/20] avg loss: 0.135282540418067		[learning rate: 3.3686e-05]
	Learning Rate: 3.36858e-05
	LOSS [training: 0.1342961372936105 | validation: 0.11594707557759608]
	TIME [epoch: 8.86 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1392058232189142		[learning rate: 3.3625e-05]
		[batch 20/20] avg loss: 0.12068261323516032		[learning rate: 3.3564e-05]
	Learning Rate: 3.35636e-05
	LOSS [training: 0.12994421822703728 | validation: 0.10291903364229076]
	TIME [epoch: 8.86 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1341572100544522		[learning rate: 3.3503e-05]
		[batch 20/20] avg loss: 0.11812960490356236		[learning rate: 3.3442e-05]
	Learning Rate: 3.34418e-05
	LOSS [training: 0.1261434074790073 | validation: 0.11250606110202316]
	TIME [epoch: 8.85 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13725445522193372		[learning rate: 3.3381e-05]
		[batch 20/20] avg loss: 0.11821386227246486		[learning rate: 3.332e-05]
	Learning Rate: 3.33204e-05
	LOSS [training: 0.12773415874719926 | validation: 0.10156161418388997]
	TIME [epoch: 8.84 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11905689612762775		[learning rate: 3.326e-05]
		[batch 20/20] avg loss: 0.15080981916154337		[learning rate: 3.32e-05]
	Learning Rate: 3.31995e-05
	LOSS [training: 0.13493335764458556 | validation: 0.10480066260390399]
	TIME [epoch: 8.85 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11909116374398995		[learning rate: 3.3139e-05]
		[batch 20/20] avg loss: 0.12918728874737054		[learning rate: 3.3079e-05]
	Learning Rate: 3.3079e-05
	LOSS [training: 0.12413922624568026 | validation: 0.11026512625372491]
	TIME [epoch: 8.86 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1291104831001766		[learning rate: 3.3019e-05]
		[batch 20/20] avg loss: 0.12904156560002972		[learning rate: 3.2959e-05]
	Learning Rate: 3.2959e-05
	LOSS [training: 0.12907602435010315 | validation: 0.1138361751171555]
	TIME [epoch: 8.85 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13097366496389254		[learning rate: 3.2899e-05]
		[batch 20/20] avg loss: 0.12994395153100513		[learning rate: 3.2839e-05]
	Learning Rate: 3.28394e-05
	LOSS [training: 0.1304588082474488 | validation: 0.1115293009404714]
	TIME [epoch: 8.85 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12274689920889843		[learning rate: 3.278e-05]
		[batch 20/20] avg loss: 0.13572392953569629		[learning rate: 3.272e-05]
	Learning Rate: 3.27202e-05
	LOSS [training: 0.12923541437229735 | validation: 0.10278314574418145]
	TIME [epoch: 8.85 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12334120420095507		[learning rate: 3.2661e-05]
		[batch 20/20] avg loss: 0.1239961245542008		[learning rate: 3.2601e-05]
	Learning Rate: 3.26014e-05
	LOSS [training: 0.12366866437757792 | validation: 0.11033871688469295]
	TIME [epoch: 8.84 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12067969272612758		[learning rate: 3.2542e-05]
		[batch 20/20] avg loss: 0.1358240161074075		[learning rate: 3.2483e-05]
	Learning Rate: 3.24831e-05
	LOSS [training: 0.12825185441676754 | validation: 0.11474189470427032]
	TIME [epoch: 8.86 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13039466455799523		[learning rate: 3.2424e-05]
		[batch 20/20] avg loss: 0.12958473706518325		[learning rate: 3.2365e-05]
	Learning Rate: 3.23652e-05
	LOSS [training: 0.12998970081158925 | validation: 0.109172791471282]
	TIME [epoch: 8.84 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12478325633979041		[learning rate: 3.2306e-05]
		[batch 20/20] avg loss: 0.13224593697898457		[learning rate: 3.2248e-05]
	Learning Rate: 3.22478e-05
	LOSS [training: 0.1285145966593875 | validation: 0.11530850235325053]
	TIME [epoch: 8.85 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13246305824822874		[learning rate: 3.2189e-05]
		[batch 20/20] avg loss: 0.13383994853239717		[learning rate: 3.2131e-05]
	Learning Rate: 3.21308e-05
	LOSS [training: 0.13315150339031293 | validation: 0.11268172210051436]
	TIME [epoch: 8.84 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12470892554371786		[learning rate: 3.2072e-05]
		[batch 20/20] avg loss: 0.12463274523765969		[learning rate: 3.2014e-05]
	Learning Rate: 3.20142e-05
	LOSS [training: 0.12467083539068877 | validation: 0.1090328739340778]
	TIME [epoch: 8.85 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12994819458697673		[learning rate: 3.1956e-05]
		[batch 20/20] avg loss: 0.13276431313309367		[learning rate: 3.1898e-05]
	Learning Rate: 3.1898e-05
	LOSS [training: 0.13135625386003522 | validation: 0.10854676815104312]
	TIME [epoch: 8.86 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12497511709091176		[learning rate: 3.184e-05]
		[batch 20/20] avg loss: 0.13083492177750233		[learning rate: 3.1782e-05]
	Learning Rate: 3.17822e-05
	LOSS [training: 0.12790501943420704 | validation: 0.10293952365534426]
	TIME [epoch: 8.85 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1333925551033892		[learning rate: 3.1725e-05]
		[batch 20/20] avg loss: 0.11721280774496064		[learning rate: 3.1667e-05]
	Learning Rate: 3.16669e-05
	LOSS [training: 0.12530268142417494 | validation: 0.12493721702974848]
	TIME [epoch: 8.85 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1405780190327306		[learning rate: 3.1609e-05]
		[batch 20/20] avg loss: 0.12026302854990698		[learning rate: 3.1552e-05]
	Learning Rate: 3.1552e-05
	LOSS [training: 0.1304205237913188 | validation: 0.11280095257271541]
	TIME [epoch: 8.84 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1252023738990356		[learning rate: 3.1495e-05]
		[batch 20/20] avg loss: 0.1288672170418242		[learning rate: 3.1437e-05]
	Learning Rate: 3.14375e-05
	LOSS [training: 0.1270347954704299 | validation: 0.10176444738510587]
	TIME [epoch: 8.87 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11634761359119761		[learning rate: 3.138e-05]
		[batch 20/20] avg loss: 0.13423773524735588		[learning rate: 3.1323e-05]
	Learning Rate: 3.13234e-05
	LOSS [training: 0.12529267441927674 | validation: 0.11501625280628162]
	TIME [epoch: 8.85 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12002603761448269		[learning rate: 3.1266e-05]
		[batch 20/20] avg loss: 0.14393669737060055		[learning rate: 3.121e-05]
	Learning Rate: 3.12097e-05
	LOSS [training: 0.1319813674925416 | validation: 0.10682830539656359]
	TIME [epoch: 8.85 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12098887997328325		[learning rate: 3.1153e-05]
		[batch 20/20] avg loss: 0.15027849108958571		[learning rate: 3.1096e-05]
	Learning Rate: 3.10964e-05
	LOSS [training: 0.13563368553143446 | validation: 0.13326668705982886]
	TIME [epoch: 8.84 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13741548449887356		[learning rate: 3.104e-05]
		[batch 20/20] avg loss: 0.12397289704625396		[learning rate: 3.0984e-05]
	Learning Rate: 3.09836e-05
	LOSS [training: 0.13069419077256378 | validation: 0.11343893128802916]
	TIME [epoch: 8.85 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13151822285626205		[learning rate: 3.0927e-05]
		[batch 20/20] avg loss: 0.12271097797037889		[learning rate: 3.0871e-05]
	Learning Rate: 3.08711e-05
	LOSS [training: 0.12711460041332046 | validation: 0.10534072141222046]
	TIME [epoch: 8.87 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13487669183765455		[learning rate: 3.0815e-05]
		[batch 20/20] avg loss: 0.1253559778164392		[learning rate: 3.0759e-05]
	Learning Rate: 3.07591e-05
	LOSS [training: 0.13011633482704688 | validation: 0.10837773380741496]
	TIME [epoch: 8.84 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14318266950857506		[learning rate: 3.0703e-05]
		[batch 20/20] avg loss: 0.14088483664657211		[learning rate: 3.0647e-05]
	Learning Rate: 3.06475e-05
	LOSS [training: 0.1420337530775736 | validation: 0.11456364316941021]
	TIME [epoch: 8.84 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12419281261955625		[learning rate: 3.0592e-05]
		[batch 20/20] avg loss: 0.1307541513458375		[learning rate: 3.0536e-05]
	Learning Rate: 3.05363e-05
	LOSS [training: 0.1274734819826969 | validation: 0.11067786197597952]
	TIME [epoch: 8.84 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12007126587289577		[learning rate: 3.0481e-05]
		[batch 20/20] avg loss: 0.13310315356678185		[learning rate: 3.0425e-05]
	Learning Rate: 3.04254e-05
	LOSS [training: 0.1265872097198388 | validation: 0.1137897426776091]
	TIME [epoch: 8.84 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1271960049070763		[learning rate: 3.037e-05]
		[batch 20/20] avg loss: 0.11982963345584571		[learning rate: 3.0315e-05]
	Learning Rate: 3.0315e-05
	LOSS [training: 0.12351281918146104 | validation: 0.116118563350504]
	TIME [epoch: 8.86 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1247479954887599		[learning rate: 3.026e-05]
		[batch 20/20] avg loss: 0.1264307012909732		[learning rate: 3.0205e-05]
	Learning Rate: 3.0205e-05
	LOSS [training: 0.12558934838986657 | validation: 0.1114604403931005]
	TIME [epoch: 8.84 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12992790355054812		[learning rate: 3.015e-05]
		[batch 20/20] avg loss: 0.12915690870625043		[learning rate: 3.0095e-05]
	Learning Rate: 3.00954e-05
	LOSS [training: 0.12954240612839926 | validation: 0.10924980419640218]
	TIME [epoch: 8.84 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1422477808558589		[learning rate: 3.0041e-05]
		[batch 20/20] avg loss: 0.1256426763904222		[learning rate: 2.9986e-05]
	Learning Rate: 2.99862e-05
	LOSS [training: 0.13394522862314054 | validation: 0.11285884906810362]
	TIME [epoch: 8.84 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14599869193072545		[learning rate: 2.9932e-05]
		[batch 20/20] avg loss: 0.1199532607355925		[learning rate: 2.9877e-05]
	Learning Rate: 2.98774e-05
	LOSS [training: 0.132975976333159 | validation: 0.10642525764189405]
	TIME [epoch: 8.86 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.126146169102805		[learning rate: 2.9823e-05]
		[batch 20/20] avg loss: 0.13522443844775348		[learning rate: 2.9769e-05]
	Learning Rate: 2.97689e-05
	LOSS [training: 0.13068530377527923 | validation: 0.11306897026054703]
	TIME [epoch: 8.85 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11215795394336674		[learning rate: 2.9715e-05]
		[batch 20/20] avg loss: 0.1463977750093302		[learning rate: 2.9661e-05]
	Learning Rate: 2.96609e-05
	LOSS [training: 0.12927786447634845 | validation: 0.10714871172147351]
	TIME [epoch: 8.84 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11877811782322707		[learning rate: 2.9607e-05]
		[batch 20/20] avg loss: 0.13374631210071924		[learning rate: 2.9553e-05]
	Learning Rate: 2.95533e-05
	LOSS [training: 0.12626221496197315 | validation: 0.11161692471930452]
	TIME [epoch: 8.84 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12861122360887542		[learning rate: 2.95e-05]
		[batch 20/20] avg loss: 0.120130984332333		[learning rate: 2.9446e-05]
	Learning Rate: 2.9446e-05
	LOSS [training: 0.12437110397060422 | validation: 0.10462864632001409]
	TIME [epoch: 8.85 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12216194519238888		[learning rate: 2.9393e-05]
		[batch 20/20] avg loss: 0.1285597609007995		[learning rate: 2.9339e-05]
	Learning Rate: 2.93391e-05
	LOSS [training: 0.1253608530465942 | validation: 0.10869057727810241]
	TIME [epoch: 8.86 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12289620954084757		[learning rate: 2.9286e-05]
		[batch 20/20] avg loss: 0.12760813218115688		[learning rate: 2.9233e-05]
	Learning Rate: 2.92327e-05
	LOSS [training: 0.1252521708610022 | validation: 0.11107321093914689]
	TIME [epoch: 8.84 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1318790886207431		[learning rate: 2.918e-05]
		[batch 20/20] avg loss: 0.11811020573673162		[learning rate: 2.9127e-05]
	Learning Rate: 2.91266e-05
	LOSS [training: 0.12499464717873736 | validation: 0.1021613830685151]
	TIME [epoch: 8.84 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12478538915202701		[learning rate: 2.9074e-05]
		[batch 20/20] avg loss: 0.12884510720455247		[learning rate: 2.9021e-05]
	Learning Rate: 2.90209e-05
	LOSS [training: 0.1268152481782897 | validation: 0.09925938589192551]
	TIME [epoch: 8.85 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12619889262635417		[learning rate: 2.8968e-05]
		[batch 20/20] avg loss: 0.126150034846532		[learning rate: 2.8916e-05]
	Learning Rate: 2.89156e-05
	LOSS [training: 0.1261744637364431 | validation: 0.1163320180469254]
	TIME [epoch: 8.84 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11540774656771631		[learning rate: 2.8863e-05]
		[batch 20/20] avg loss: 0.1368327686222492		[learning rate: 2.8811e-05]
	Learning Rate: 2.88106e-05
	LOSS [training: 0.12612025759498274 | validation: 0.10092783519310775]
	TIME [epoch: 8.86 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1283251285550593		[learning rate: 2.8758e-05]
		[batch 20/20] avg loss: 0.13118722063550903		[learning rate: 2.8706e-05]
	Learning Rate: 2.87061e-05
	LOSS [training: 0.12975617459528416 | validation: 0.10826212370542412]
	TIME [epoch: 8.85 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12907109826460056		[learning rate: 2.8654e-05]
		[batch 20/20] avg loss: 0.13511908252967914		[learning rate: 2.8602e-05]
	Learning Rate: 2.86019e-05
	LOSS [training: 0.13209509039713982 | validation: 0.11195027183768182]
	TIME [epoch: 8.83 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13068082782375864		[learning rate: 2.855e-05]
		[batch 20/20] avg loss: 0.12388565800316416		[learning rate: 2.8498e-05]
	Learning Rate: 2.84981e-05
	LOSS [training: 0.12728324291346144 | validation: 0.10614728540885547]
	TIME [epoch: 8.84 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1299357707551479		[learning rate: 2.8446e-05]
		[batch 20/20] avg loss: 0.12255202651103392		[learning rate: 2.8395e-05]
	Learning Rate: 2.83947e-05
	LOSS [training: 0.1262438986330909 | validation: 0.10103934718109694]
	TIME [epoch: 8.86 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12585875248337774		[learning rate: 2.8343e-05]
		[batch 20/20] avg loss: 0.125467259221978		[learning rate: 2.8292e-05]
	Learning Rate: 2.82916e-05
	LOSS [training: 0.12566300585267787 | validation: 0.11225755657828691]
	TIME [epoch: 8.84 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13242044691666968		[learning rate: 2.824e-05]
		[batch 20/20] avg loss: 0.12078573213999869		[learning rate: 2.8189e-05]
	Learning Rate: 2.8189e-05
	LOSS [training: 0.12660308952833416 | validation: 0.10601978239963392]
	TIME [epoch: 8.84 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1347333039302865		[learning rate: 2.8138e-05]
		[batch 20/20] avg loss: 0.12178661082027746		[learning rate: 2.8087e-05]
	Learning Rate: 2.80867e-05
	LOSS [training: 0.128259957375282 | validation: 0.09578734144564079]
	TIME [epoch: 8.84 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13356430037218645		[learning rate: 2.8036e-05]
		[batch 20/20] avg loss: 0.11381680088456261		[learning rate: 2.7985e-05]
	Learning Rate: 2.79847e-05
	LOSS [training: 0.12369055062837453 | validation: 0.10926492720356633]
	TIME [epoch: 8.84 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13103416161743586		[learning rate: 2.7934e-05]
		[batch 20/20] avg loss: 0.11867370040246579		[learning rate: 2.7883e-05]
	Learning Rate: 2.78832e-05
	LOSS [training: 0.12485393100995079 | validation: 0.1104904767808102]
	TIME [epoch: 8.86 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13358278056564668		[learning rate: 2.7833e-05]
		[batch 20/20] avg loss: 0.1237101491996615		[learning rate: 2.7782e-05]
	Learning Rate: 2.7782e-05
	LOSS [training: 0.12864646488265408 | validation: 0.1051508046944927]
	TIME [epoch: 8.84 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11345590285075616		[learning rate: 2.7732e-05]
		[batch 20/20] avg loss: 0.13361166990699483		[learning rate: 2.7681e-05]
	Learning Rate: 2.76812e-05
	LOSS [training: 0.12353378637887547 | validation: 0.11176844188315388]
	TIME [epoch: 8.84 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12623599862342863		[learning rate: 2.7631e-05]
		[batch 20/20] avg loss: 0.12691407366454732		[learning rate: 2.7581e-05]
	Learning Rate: 2.75807e-05
	LOSS [training: 0.12657503614398796 | validation: 0.10423663615282305]
	TIME [epoch: 8.84 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12180884306335985		[learning rate: 2.7531e-05]
		[batch 20/20] avg loss: 0.13405412406563913		[learning rate: 2.7481e-05]
	Learning Rate: 2.74806e-05
	LOSS [training: 0.12793148356449952 | validation: 0.11027722468974961]
	TIME [epoch: 8.84 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13194860424268479		[learning rate: 2.7431e-05]
		[batch 20/20] avg loss: 0.11534090819775863		[learning rate: 2.7381e-05]
	Learning Rate: 2.73809e-05
	LOSS [training: 0.12364475622022168 | validation: 0.10393160658453843]
	TIME [epoch: 8.86 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13520743248290515		[learning rate: 2.7331e-05]
		[batch 20/20] avg loss: 0.11526268351917976		[learning rate: 2.7282e-05]
	Learning Rate: 2.72815e-05
	LOSS [training: 0.12523505800104245 | validation: 0.10735728661099356]
	TIME [epoch: 8.84 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11973796373827117		[learning rate: 2.7232e-05]
		[batch 20/20] avg loss: 0.12827338215562784		[learning rate: 2.7183e-05]
	Learning Rate: 2.71825e-05
	LOSS [training: 0.1240056729469495 | validation: 0.11881652050596678]
	TIME [epoch: 8.84 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12998658369775767		[learning rate: 2.7133e-05]
		[batch 20/20] avg loss: 0.12786668598692302		[learning rate: 2.7084e-05]
	Learning Rate: 2.70839e-05
	LOSS [training: 0.12892663484234032 | validation: 0.10417706045621097]
	TIME [epoch: 8.83 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13147658346039254		[learning rate: 2.7035e-05]
		[batch 20/20] avg loss: 0.11166162477701544		[learning rate: 2.6986e-05]
	Learning Rate: 2.69856e-05
	LOSS [training: 0.12156910411870399 | validation: 0.10840372274936025]
	TIME [epoch: 8.85 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13655780423972047		[learning rate: 2.6937e-05]
		[batch 20/20] avg loss: 0.11665740204419599		[learning rate: 2.6888e-05]
	Learning Rate: 2.68876e-05
	LOSS [training: 0.12660760314195824 | validation: 0.11169242231594545]
	TIME [epoch: 8.85 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12780920190425998		[learning rate: 2.6839e-05]
		[batch 20/20] avg loss: 0.1239383254656095		[learning rate: 2.679e-05]
	Learning Rate: 2.67901e-05
	LOSS [training: 0.12587376368493478 | validation: 0.0962627222702538]
	TIME [epoch: 8.85 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11826987771944722		[learning rate: 2.6741e-05]
		[batch 20/20] avg loss: 0.13746402077790146		[learning rate: 2.6693e-05]
	Learning Rate: 2.66928e-05
	LOSS [training: 0.12786694924867434 | validation: 0.09951027008619938]
	TIME [epoch: 8.85 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12671129995181896		[learning rate: 2.6644e-05]
		[batch 20/20] avg loss: 0.12435069279291014		[learning rate: 2.6596e-05]
	Learning Rate: 2.6596e-05
	LOSS [training: 0.12553099637236453 | validation: 0.10535404032496823]
	TIME [epoch: 8.84 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12478217969888275		[learning rate: 2.6548e-05]
		[batch 20/20] avg loss: 0.1266833490994561		[learning rate: 2.6499e-05]
	Learning Rate: 2.64994e-05
	LOSS [training: 0.12573276439916942 | validation: 0.10399035939652077]
	TIME [epoch: 8.87 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1275581257764229		[learning rate: 2.6451e-05]
		[batch 20/20] avg loss: 0.12188034541722055		[learning rate: 2.6403e-05]
	Learning Rate: 2.64033e-05
	LOSS [training: 0.12471923559682171 | validation: 0.10882682514458208]
	TIME [epoch: 8.84 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13249420520430769		[learning rate: 2.6355e-05]
		[batch 20/20] avg loss: 0.11943191056577936		[learning rate: 2.6307e-05]
	Learning Rate: 2.63075e-05
	LOSS [training: 0.1259630578850435 | validation: 0.0922523547426249]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1734.pth
	Model improved!!!
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12659385018750172		[learning rate: 2.626e-05]
		[batch 20/20] avg loss: 0.1287170658969749		[learning rate: 2.6212e-05]
	Learning Rate: 2.6212e-05
	LOSS [training: 0.12765545804223835 | validation: 0.12416779655651769]
	TIME [epoch: 8.84 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1222015394198808		[learning rate: 2.6164e-05]
		[batch 20/20] avg loss: 0.1277507271920101		[learning rate: 2.6117e-05]
	Learning Rate: 2.61169e-05
	LOSS [training: 0.12497613330594544 | validation: 0.10929186591638138]
	TIME [epoch: 8.84 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12870844679607804		[learning rate: 2.6069e-05]
		[batch 20/20] avg loss: 0.11870055099393126		[learning rate: 2.6022e-05]
	Learning Rate: 2.60221e-05
	LOSS [training: 0.12370449889500464 | validation: 0.12418150326346811]
	TIME [epoch: 8.86 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12502106697912624		[learning rate: 2.5975e-05]
		[batch 20/20] avg loss: 0.12567898464585467		[learning rate: 2.5928e-05]
	Learning Rate: 2.59277e-05
	LOSS [training: 0.12535002581249044 | validation: 0.11661060000051018]
	TIME [epoch: 8.85 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1188167530767102		[learning rate: 2.5881e-05]
		[batch 20/20] avg loss: 0.13279866502020157		[learning rate: 2.5834e-05]
	Learning Rate: 2.58336e-05
	LOSS [training: 0.1258077090484559 | validation: 0.10034808317394625]
	TIME [epoch: 8.84 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13957216763583066		[learning rate: 2.5787e-05]
		[batch 20/20] avg loss: 0.10605483764088539		[learning rate: 2.574e-05]
	Learning Rate: 2.57398e-05
	LOSS [training: 0.12281350263835802 | validation: 0.10926886725958453]
	TIME [epoch: 8.84 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13282861383514682		[learning rate: 2.5693e-05]
		[batch 20/20] avg loss: 0.12287305850605916		[learning rate: 2.5646e-05]
	Learning Rate: 2.56464e-05
	LOSS [training: 0.12785083617060303 | validation: 0.0958458837185769]
	TIME [epoch: 8.86 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12032758091406512		[learning rate: 2.56e-05]
		[batch 20/20] avg loss: 0.14988249090633565		[learning rate: 2.5553e-05]
	Learning Rate: 2.55533e-05
	LOSS [training: 0.13510503591020034 | validation: 0.10082553116151487]
	TIME [epoch: 8.86 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12236629646835946		[learning rate: 2.5507e-05]
		[batch 20/20] avg loss: 0.13414718567640996		[learning rate: 2.5461e-05]
	Learning Rate: 2.54606e-05
	LOSS [training: 0.1282567410723847 | validation: 0.10699493872764108]
	TIME [epoch: 8.84 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14035146674805013		[learning rate: 2.5414e-05]
		[batch 20/20] avg loss: 0.12354874614288132		[learning rate: 2.5368e-05]
	Learning Rate: 2.53682e-05
	LOSS [training: 0.1319501064454657 | validation: 0.09956388357289221]
	TIME [epoch: 8.85 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12611272110186142		[learning rate: 2.5322e-05]
		[batch 20/20] avg loss: 0.13202399773644033		[learning rate: 2.5276e-05]
	Learning Rate: 2.52761e-05
	LOSS [training: 0.1290683594191509 | validation: 0.1224402040566814]
	TIME [epoch: 8.84 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12480675457240191		[learning rate: 2.523e-05]
		[batch 20/20] avg loss: 0.13916207918944484		[learning rate: 2.5184e-05]
	Learning Rate: 2.51844e-05
	LOSS [training: 0.13198441688092338 | validation: 0.09925160652342761]
	TIME [epoch: 8.87 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13382181582465616		[learning rate: 2.5139e-05]
		[batch 20/20] avg loss: 0.11537646990585171		[learning rate: 2.5093e-05]
	Learning Rate: 2.5093e-05
	LOSS [training: 0.12459914286525395 | validation: 0.10503924637242078]
	TIME [epoch: 8.85 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1263988121250685		[learning rate: 2.5047e-05]
		[batch 20/20] avg loss: 0.1285110586346917		[learning rate: 2.5002e-05]
	Learning Rate: 2.50019e-05
	LOSS [training: 0.12745493537988012 | validation: 0.09876095487394498]
	TIME [epoch: 8.84 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1313655736367232		[learning rate: 2.4957e-05]
		[batch 20/20] avg loss: 0.11214185600877373		[learning rate: 2.4911e-05]
	Learning Rate: 2.49112e-05
	LOSS [training: 0.12175371482274844 | validation: 0.10360134243011263]
	TIME [epoch: 8.84 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12783934404777322		[learning rate: 2.4866e-05]
		[batch 20/20] avg loss: 0.13190613783937438		[learning rate: 2.4821e-05]
	Learning Rate: 2.48208e-05
	LOSS [training: 0.12987274094357382 | validation: 0.10432371022816797]
	TIME [epoch: 8.84 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12167820307890094		[learning rate: 2.4776e-05]
		[batch 20/20] avg loss: 0.1292417685246657		[learning rate: 2.4731e-05]
	Learning Rate: 2.47307e-05
	LOSS [training: 0.1254599858017833 | validation: 0.1073073087850775]
	TIME [epoch: 8.86 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12598164598762052		[learning rate: 2.4686e-05]
		[batch 20/20] avg loss: 0.11742601443668813		[learning rate: 2.4641e-05]
	Learning Rate: 2.4641e-05
	LOSS [training: 0.12170383021215433 | validation: 0.10856482921725485]
	TIME [epoch: 8.85 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12226623515303596		[learning rate: 2.4596e-05]
		[batch 20/20] avg loss: 0.1303089328007654		[learning rate: 2.4552e-05]
	Learning Rate: 2.45516e-05
	LOSS [training: 0.12628758397690068 | validation: 0.11983675124052579]
	TIME [epoch: 8.84 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13815248051022716		[learning rate: 2.4507e-05]
		[batch 20/20] avg loss: 0.1228230740717895		[learning rate: 2.4462e-05]
	Learning Rate: 2.44625e-05
	LOSS [training: 0.13048777729100833 | validation: 0.1065088647950017]
	TIME [epoch: 8.85 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1310077924410667		[learning rate: 2.4418e-05]
		[batch 20/20] avg loss: 0.11888787798751044		[learning rate: 2.4374e-05]
	Learning Rate: 2.43737e-05
	LOSS [training: 0.12494783521428857 | validation: 0.09702115095410982]
	TIME [epoch: 8.84 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12789170359195634		[learning rate: 2.4329e-05]
		[batch 20/20] avg loss: 0.1280541476849832		[learning rate: 2.4285e-05]
	Learning Rate: 2.42852e-05
	LOSS [training: 0.1279729256384698 | validation: 0.10921509762588225]
	TIME [epoch: 8.87 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.129762977140056		[learning rate: 2.4241e-05]
		[batch 20/20] avg loss: 0.12409072733654955		[learning rate: 2.4197e-05]
	Learning Rate: 2.41971e-05
	LOSS [training: 0.1269268522383028 | validation: 0.11243661692875208]
	TIME [epoch: 8.85 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13145623354804775		[learning rate: 2.4153e-05]
		[batch 20/20] avg loss: 0.11518852525117332		[learning rate: 2.4109e-05]
	Learning Rate: 2.41093e-05
	LOSS [training: 0.12332237939961051 | validation: 0.1123937990911638]
	TIME [epoch: 8.85 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13493500803533937		[learning rate: 2.4065e-05]
		[batch 20/20] avg loss: 0.11520816738302483		[learning rate: 2.4022e-05]
	Learning Rate: 2.40218e-05
	LOSS [training: 0.1250715877091821 | validation: 0.11426864755946843]
	TIME [epoch: 8.85 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11722577071731717		[learning rate: 2.3978e-05]
		[batch 20/20] avg loss: 0.1306314714186259		[learning rate: 2.3935e-05]
	Learning Rate: 2.39346e-05
	LOSS [training: 0.12392862106797149 | validation: 0.10898832275493617]
	TIME [epoch: 8.86 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11805672530260052		[learning rate: 2.3891e-05]
		[batch 20/20] avg loss: 0.13024412696923396		[learning rate: 2.3848e-05]
	Learning Rate: 2.38477e-05
	LOSS [training: 0.12415042613591724 | validation: 0.10280315086422774]
	TIME [epoch: 8.85 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13898095307620614		[learning rate: 2.3804e-05]
		[batch 20/20] avg loss: 0.12315851715838637		[learning rate: 2.3761e-05]
	Learning Rate: 2.37612e-05
	LOSS [training: 0.1310697351172963 | validation: 0.09940421933020237]
	TIME [epoch: 8.84 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12023097091025206		[learning rate: 2.3718e-05]
		[batch 20/20] avg loss: 0.13253234064336422		[learning rate: 2.3675e-05]
	Learning Rate: 2.3675e-05
	LOSS [training: 0.12638165577680813 | validation: 0.11711144723425326]
	TIME [epoch: 8.84 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13695353725860676		[learning rate: 2.3632e-05]
		[batch 20/20] avg loss: 0.12086071835117491		[learning rate: 2.3589e-05]
	Learning Rate: 2.35891e-05
	LOSS [training: 0.12890712780489083 | validation: 0.1103228736589397]
	TIME [epoch: 8.84 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12391169529993533		[learning rate: 2.3546e-05]
		[batch 20/20] avg loss: 0.12120367646831029		[learning rate: 2.3503e-05]
	Learning Rate: 2.35034e-05
	LOSS [training: 0.12255768588412283 | validation: 0.1005623917075739]
	TIME [epoch: 8.86 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13133904864530174		[learning rate: 2.3461e-05]
		[batch 20/20] avg loss: 0.12457138599225615		[learning rate: 2.3418e-05]
	Learning Rate: 2.34182e-05
	LOSS [training: 0.12795521731877893 | validation: 0.10446229502684613]
	TIME [epoch: 8.85 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.131768416332975		[learning rate: 2.3376e-05]
		[batch 20/20] avg loss: 0.1188160149013853		[learning rate: 2.3333e-05]
	Learning Rate: 2.33332e-05
	LOSS [training: 0.12529221561718012 | validation: 0.1092133109145564]
	TIME [epoch: 8.84 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11844678047535588		[learning rate: 2.3291e-05]
		[batch 20/20] avg loss: 0.12907439059240802		[learning rate: 2.3248e-05]
	Learning Rate: 2.32485e-05
	LOSS [training: 0.12376058553388194 | validation: 0.10907849471550354]
	TIME [epoch: 8.85 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12719252348367358		[learning rate: 2.3206e-05]
		[batch 20/20] avg loss: 0.1317861514390471		[learning rate: 2.3164e-05]
	Learning Rate: 2.31641e-05
	LOSS [training: 0.12948933746136032 | validation: 0.10472687650526188]
	TIME [epoch: 8.85 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.136414585396758		[learning rate: 2.3122e-05]
		[batch 20/20] avg loss: 0.11815186906358514		[learning rate: 2.308e-05]
	Learning Rate: 2.30801e-05
	LOSS [training: 0.12728322723017155 | validation: 0.0908066834306262]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1770.pth
	Model improved!!!
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12550183171043497		[learning rate: 2.3038e-05]
		[batch 20/20] avg loss: 0.12955210544002654		[learning rate: 2.2996e-05]
	Learning Rate: 2.29963e-05
	LOSS [training: 0.12752696857523077 | validation: 0.10011860534731766]
	TIME [epoch: 8.85 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14215789771391427		[learning rate: 2.2955e-05]
		[batch 20/20] avg loss: 0.1081787668426621		[learning rate: 2.2913e-05]
	Learning Rate: 2.29128e-05
	LOSS [training: 0.1251683322782882 | validation: 0.09195317629253873]
	TIME [epoch: 8.84 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1254976524493488		[learning rate: 2.2871e-05]
		[batch 20/20] avg loss: 0.125525997025833		[learning rate: 2.283e-05]
	Learning Rate: 2.28297e-05
	LOSS [training: 0.12551182473759087 | validation: 0.10794188944542832]
	TIME [epoch: 8.84 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12639422319602922		[learning rate: 2.2788e-05]
		[batch 20/20] avg loss: 0.12069004979235834		[learning rate: 2.2747e-05]
	Learning Rate: 2.27468e-05
	LOSS [training: 0.12354213649419377 | validation: 0.11104233276830813]
	TIME [epoch: 8.87 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13084638966322032		[learning rate: 2.2706e-05]
		[batch 20/20] avg loss: 0.11273461338323063		[learning rate: 2.2664e-05]
	Learning Rate: 2.26643e-05
	LOSS [training: 0.12179050152322549 | validation: 0.11846909237685084]
	TIME [epoch: 8.84 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12999330004946855		[learning rate: 2.2623e-05]
		[batch 20/20] avg loss: 0.12343135564671223		[learning rate: 2.2582e-05]
	Learning Rate: 2.2582e-05
	LOSS [training: 0.1267123278480904 | validation: 0.10462673325374734]
	TIME [epoch: 8.84 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12692640486696033		[learning rate: 2.2541e-05]
		[batch 20/20] avg loss: 0.1207474053082517		[learning rate: 2.25e-05]
	Learning Rate: 2.25001e-05
	LOSS [training: 0.12383690508760606 | validation: 0.10467567559147205]
	TIME [epoch: 8.84 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13400561733105945		[learning rate: 2.2459e-05]
		[batch 20/20] avg loss: 0.11517128238560208		[learning rate: 2.2418e-05]
	Learning Rate: 2.24184e-05
	LOSS [training: 0.12458844985833076 | validation: 0.11002600106574249]
	TIME [epoch: 8.84 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12147040866833377		[learning rate: 2.2378e-05]
		[batch 20/20] avg loss: 0.12189563665024442		[learning rate: 2.2337e-05]
	Learning Rate: 2.23371e-05
	LOSS [training: 0.1216830226592891 | validation: 0.11396140369019805]
	TIME [epoch: 8.86 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1267896336118269		[learning rate: 2.2297e-05]
		[batch 20/20] avg loss: 0.12376117772822637		[learning rate: 2.2256e-05]
	Learning Rate: 2.2256e-05
	LOSS [training: 0.12527540567002665 | validation: 0.11036144687534133]
	TIME [epoch: 8.84 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12574893068910717		[learning rate: 2.2216e-05]
		[batch 20/20] avg loss: 0.12705440273839236		[learning rate: 2.2175e-05]
	Learning Rate: 2.21753e-05
	LOSS [training: 0.12640166671374978 | validation: 0.10699993040040603]
	TIME [epoch: 8.84 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14067036875162714		[learning rate: 2.2135e-05]
		[batch 20/20] avg loss: 0.11508370827978069		[learning rate: 2.2095e-05]
	Learning Rate: 2.20948e-05
	LOSS [training: 0.12787703851570392 | validation: 0.1026467069838501]
	TIME [epoch: 8.83 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12299504248291449		[learning rate: 2.2055e-05]
		[batch 20/20] avg loss: 0.1310402390025878		[learning rate: 2.2015e-05]
	Learning Rate: 2.20146e-05
	LOSS [training: 0.12701764074275115 | validation: 0.09825058994441721]
	TIME [epoch: 8.84 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12388139284429234		[learning rate: 2.1975e-05]
		[batch 20/20] avg loss: 0.13356379241646832		[learning rate: 2.1935e-05]
	Learning Rate: 2.19347e-05
	LOSS [training: 0.12872259263038033 | validation: 0.1040980402631906]
	TIME [epoch: 8.86 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12105551820450514		[learning rate: 2.1895e-05]
		[batch 20/20] avg loss: 0.1287775231929466		[learning rate: 2.1855e-05]
	Learning Rate: 2.18551e-05
	LOSS [training: 0.12491652069872587 | validation: 0.1056489322920634]
	TIME [epoch: 8.84 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12674514131215067		[learning rate: 2.1815e-05]
		[batch 20/20] avg loss: 0.12262039405929503		[learning rate: 2.1776e-05]
	Learning Rate: 2.17758e-05
	LOSS [training: 0.12468276768572288 | validation: 0.12218422102303533]
	TIME [epoch: 8.84 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13904645972098584		[learning rate: 2.1736e-05]
		[batch 20/20] avg loss: 0.11262280385941421		[learning rate: 2.1697e-05]
	Learning Rate: 2.16968e-05
	LOSS [training: 0.12583463179020005 | validation: 0.11151024718199061]
	TIME [epoch: 8.83 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12210098404544381		[learning rate: 2.1657e-05]
		[batch 20/20] avg loss: 0.13548634080698946		[learning rate: 2.1618e-05]
	Learning Rate: 2.1618e-05
	LOSS [training: 0.12879366242621665 | validation: 0.10723364069616012]
	TIME [epoch: 8.85 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12286410131471873		[learning rate: 2.1579e-05]
		[batch 20/20] avg loss: 0.12540649764307032		[learning rate: 2.154e-05]
	Learning Rate: 2.15396e-05
	LOSS [training: 0.12413529947889454 | validation: 0.10792861625645113]
	TIME [epoch: 8.85 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11340035716028929		[learning rate: 2.15e-05]
		[batch 20/20] avg loss: 0.14011631997209387		[learning rate: 2.1461e-05]
	Learning Rate: 2.14614e-05
	LOSS [training: 0.1267583385661916 | validation: 0.09664945746036596]
	TIME [epoch: 8.84 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14276238505810068		[learning rate: 2.1422e-05]
		[batch 20/20] avg loss: 0.11247705110176068		[learning rate: 2.1384e-05]
	Learning Rate: 2.13835e-05
	LOSS [training: 0.12761971807993067 | validation: 0.10094762777704952]
	TIME [epoch: 8.83 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12309105973924135		[learning rate: 2.1345e-05]
		[batch 20/20] avg loss: 0.1307054285181038		[learning rate: 2.1306e-05]
	Learning Rate: 2.13059e-05
	LOSS [training: 0.12689824412867257 | validation: 0.0993870425273481]
	TIME [epoch: 8.83 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13524716397840503		[learning rate: 2.1267e-05]
		[batch 20/20] avg loss: 0.11158533019258547		[learning rate: 2.1229e-05]
	Learning Rate: 2.12286e-05
	LOSS [training: 0.12341624708549521 | validation: 0.09628835936011265]
	TIME [epoch: 8.86 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1231389547920132		[learning rate: 2.119e-05]
		[batch 20/20] avg loss: 0.12469316220821532		[learning rate: 2.1152e-05]
	Learning Rate: 2.11515e-05
	LOSS [training: 0.12391605850011425 | validation: 0.10692502285779966]
	TIME [epoch: 8.84 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1291326021855351		[learning rate: 2.1113e-05]
		[batch 20/20] avg loss: 0.12117795482682885		[learning rate: 2.1075e-05]
	Learning Rate: 2.10748e-05
	LOSS [training: 0.125155278506182 | validation: 0.09793270946550986]
	TIME [epoch: 8.84 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13169820570702878		[learning rate: 2.1037e-05]
		[batch 20/20] avg loss: 0.12613975162179167		[learning rate: 2.0998e-05]
	Learning Rate: 2.09983e-05
	LOSS [training: 0.1289189786644102 | validation: 0.10181036962679595]
	TIME [epoch: 8.84 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11020162701558618		[learning rate: 2.096e-05]
		[batch 20/20] avg loss: 0.1323361006273463		[learning rate: 2.0922e-05]
	Learning Rate: 2.09221e-05
	LOSS [training: 0.12126886382146622 | validation: 0.10655283970943287]
	TIME [epoch: 8.84 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1286692006859877		[learning rate: 2.0884e-05]
		[batch 20/20] avg loss: 0.11829243425228113		[learning rate: 2.0846e-05]
	Learning Rate: 2.08462e-05
	LOSS [training: 0.12348081746913442 | validation: 0.10217939063264932]
	TIME [epoch: 8.87 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12272299851076666		[learning rate: 2.0808e-05]
		[batch 20/20] avg loss: 0.13107726747416026		[learning rate: 2.0771e-05]
	Learning Rate: 2.07705e-05
	LOSS [training: 0.12690013299246347 | validation: 0.10915663855561371]
	TIME [epoch: 8.83 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1259854017458995		[learning rate: 2.0733e-05]
		[batch 20/20] avg loss: 0.1289015153862453		[learning rate: 2.0695e-05]
	Learning Rate: 2.06951e-05
	LOSS [training: 0.12744345856607242 | validation: 0.10209730801508674]
	TIME [epoch: 8.83 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12308654251946764		[learning rate: 2.0658e-05]
		[batch 20/20] avg loss: 0.12898045633491506		[learning rate: 2.062e-05]
	Learning Rate: 2.062e-05
	LOSS [training: 0.12603349942719136 | validation: 0.09883191365379837]
	TIME [epoch: 8.83 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1367025979306143		[learning rate: 2.0583e-05]
		[batch 20/20] avg loss: 0.12301116480006176		[learning rate: 2.0545e-05]
	Learning Rate: 2.05452e-05
	LOSS [training: 0.12985688136533802 | validation: 0.10540562445155208]
	TIME [epoch: 8.86 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1284757947840424		[learning rate: 2.0508e-05]
		[batch 20/20] avg loss: 0.1280291004710527		[learning rate: 2.0471e-05]
	Learning Rate: 2.04706e-05
	LOSS [training: 0.12825244762754756 | validation: 0.10876142215289214]
	TIME [epoch: 8.83 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1262343594885896		[learning rate: 2.0433e-05]
		[batch 20/20] avg loss: 0.12372314079221218		[learning rate: 2.0396e-05]
	Learning Rate: 2.03964e-05
	LOSS [training: 0.1249787501404009 | validation: 0.10863895561592019]
	TIME [epoch: 8.82 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11894034060312882		[learning rate: 2.0359e-05]
		[batch 20/20] avg loss: 0.13153154902016495		[learning rate: 2.0322e-05]
	Learning Rate: 2.03223e-05
	LOSS [training: 0.1252359448116469 | validation: 0.10199820136360975]
	TIME [epoch: 8.84 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11090922366766327		[learning rate: 2.0285e-05]
		[batch 20/20] avg loss: 0.13568200455876286		[learning rate: 2.0249e-05]
	Learning Rate: 2.02486e-05
	LOSS [training: 0.12329561411321306 | validation: 0.11190755987880659]
	TIME [epoch: 8.83 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11621636479490627		[learning rate: 2.0212e-05]
		[batch 20/20] avg loss: 0.13087268862330687		[learning rate: 2.0175e-05]
	Learning Rate: 2.01751e-05
	LOSS [training: 0.12354452670910657 | validation: 0.09787358073316113]
	TIME [epoch: 8.86 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1169055224971641		[learning rate: 2.0138e-05]
		[batch 20/20] avg loss: 0.13409085857586797		[learning rate: 2.0102e-05]
	Learning Rate: 2.01019e-05
	LOSS [training: 0.12549819053651604 | validation: 0.09750366869914953]
	TIME [epoch: 8.84 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11725320860204547		[learning rate: 2.0065e-05]
		[batch 20/20] avg loss: 0.13101576359885198		[learning rate: 2.0029e-05]
	Learning Rate: 2.00289e-05
	LOSS [training: 0.12413448610044875 | validation: 0.11144820990691666]
	TIME [epoch: 8.84 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13841276966444405		[learning rate: 1.9993e-05]
		[batch 20/20] avg loss: 0.11299153785478133		[learning rate: 1.9956e-05]
	Learning Rate: 1.99563e-05
	LOSS [training: 0.12570215375961266 | validation: 0.09609678650910826]
	TIME [epoch: 8.84 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12097400212938654		[learning rate: 1.992e-05]
		[batch 20/20] avg loss: 0.13745086326750688		[learning rate: 1.9884e-05]
	Learning Rate: 1.98838e-05
	LOSS [training: 0.1292124326984467 | validation: 0.10566474220159627]
	TIME [epoch: 8.84 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1284816367280412		[learning rate: 1.9848e-05]
		[batch 20/20] avg loss: 0.12865819986968294		[learning rate: 1.9812e-05]
	Learning Rate: 1.98117e-05
	LOSS [training: 0.12856991829886202 | validation: 0.09974777136948501]
	TIME [epoch: 8.85 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1322075421917785		[learning rate: 1.9776e-05]
		[batch 20/20] avg loss: 0.11703508313063407		[learning rate: 1.974e-05]
	Learning Rate: 1.97398e-05
	LOSS [training: 0.12462131266120627 | validation: 0.11466392738995486]
	TIME [epoch: 8.84 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14037723836048868		[learning rate: 1.9704e-05]
		[batch 20/20] avg loss: 0.1179587533005508		[learning rate: 1.9668e-05]
	Learning Rate: 1.96681e-05
	LOSS [training: 0.12916799583051972 | validation: 0.1079108187320963]
	TIME [epoch: 8.84 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13045263859406067		[learning rate: 1.9632e-05]
		[batch 20/20] avg loss: 0.12209305300307949		[learning rate: 1.9597e-05]
	Learning Rate: 1.95968e-05
	LOSS [training: 0.12627284579857007 | validation: 0.10346954784758987]
	TIME [epoch: 8.84 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14066961707601577		[learning rate: 1.9561e-05]
		[batch 20/20] avg loss: 0.10669721408243019		[learning rate: 1.9526e-05]
	Learning Rate: 1.95256e-05
	LOSS [training: 0.12368341557922298 | validation: 0.09762525368115933]
	TIME [epoch: 8.85 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12762553486673378		[learning rate: 1.949e-05]
		[batch 20/20] avg loss: 0.1146728826078316		[learning rate: 1.9455e-05]
	Learning Rate: 1.94548e-05
	LOSS [training: 0.12114920873728267 | validation: 0.10292153274727388]
	TIME [epoch: 8.85 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13521020085172925		[learning rate: 1.9419e-05]
		[batch 20/20] avg loss: 0.11745886990667691		[learning rate: 1.9384e-05]
	Learning Rate: 1.93842e-05
	LOSS [training: 0.12633453537920308 | validation: 0.10320144369642542]
	TIME [epoch: 8.84 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12742620511261737		[learning rate: 1.9349e-05]
		[batch 20/20] avg loss: 0.11984043845037769		[learning rate: 1.9314e-05]
	Learning Rate: 1.93138e-05
	LOSS [training: 0.12363332178149752 | validation: 0.10571705575335369]
	TIME [epoch: 8.84 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1146766541401311		[learning rate: 1.9279e-05]
		[batch 20/20] avg loss: 0.13208573760287118		[learning rate: 1.9244e-05]
	Learning Rate: 1.92437e-05
	LOSS [training: 0.12338119587150113 | validation: 0.109325285006811]
	TIME [epoch: 8.84 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13284491013853494		[learning rate: 1.9209e-05]
		[batch 20/20] avg loss: 0.1356203431733108		[learning rate: 1.9174e-05]
	Learning Rate: 1.91739e-05
	LOSS [training: 0.1342326266559229 | validation: 0.10564680632502038]
	TIME [epoch: 8.85 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13381465298787545		[learning rate: 1.9139e-05]
		[batch 20/20] avg loss: 0.1293635672196008		[learning rate: 1.9104e-05]
	Learning Rate: 1.91043e-05
	LOSS [training: 0.13158911010373808 | validation: 0.0975229859177436]
	TIME [epoch: 8.84 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12997731249321148		[learning rate: 1.907e-05]
		[batch 20/20] avg loss: 0.12505795664943758		[learning rate: 1.9035e-05]
	Learning Rate: 1.9035e-05
	LOSS [training: 0.12751763457132453 | validation: 0.1068562837155442]
	TIME [epoch: 8.84 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13306864742158872		[learning rate: 1.9e-05]
		[batch 20/20] avg loss: 0.11703229445183091		[learning rate: 1.8966e-05]
	Learning Rate: 1.89659e-05
	LOSS [training: 0.12505047093670982 | validation: 0.10578719611624647]
	TIME [epoch: 8.84 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11593413791327041		[learning rate: 1.8931e-05]
		[batch 20/20] avg loss: 0.13174780117132742		[learning rate: 1.8897e-05]
	Learning Rate: 1.88971e-05
	LOSS [training: 0.12384096954229892 | validation: 0.08948158663725513]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r5_20240219_192256/states/model_tr_study201_1825.pth
	Model improved!!!
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11998316422651079		[learning rate: 1.8863e-05]
		[batch 20/20] avg loss: 0.1329580828841645		[learning rate: 1.8829e-05]
	Learning Rate: 1.88285e-05
	LOSS [training: 0.1264706235553376 | validation: 0.10553469566165706]
	TIME [epoch: 8.86 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11997090917101656		[learning rate: 1.8794e-05]
		[batch 20/20] avg loss: 0.12996643854011625		[learning rate: 1.876e-05]
	Learning Rate: 1.87602e-05
	LOSS [training: 0.12496867385556638 | validation: 0.10558347995686675]
	TIME [epoch: 8.84 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12571253547418162		[learning rate: 1.8726e-05]
		[batch 20/20] avg loss: 0.1273336328923702		[learning rate: 1.8692e-05]
	Learning Rate: 1.86921e-05
	LOSS [training: 0.1265230841832759 | validation: 0.09621452528703439]
	TIME [epoch: 8.84 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12276578385767771		[learning rate: 1.8658e-05]
		[batch 20/20] avg loss: 0.13177617631066604		[learning rate: 1.8624e-05]
	Learning Rate: 1.86243e-05
	LOSS [training: 0.12727098008417187 | validation: 0.10706943987885012]
	TIME [epoch: 8.84 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12137375113567768		[learning rate: 1.859e-05]
		[batch 20/20] avg loss: 0.1349130942084985		[learning rate: 1.8557e-05]
	Learning Rate: 1.85567e-05
	LOSS [training: 0.1281434226720881 | validation: 0.10926319410338937]
	TIME [epoch: 8.85 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12513305643639716		[learning rate: 1.8523e-05]
		[batch 20/20] avg loss: 0.1286067950375556		[learning rate: 1.8489e-05]
	Learning Rate: 1.84893e-05
	LOSS [training: 0.12686992573697636 | validation: 0.10239833810052054]
	TIME [epoch: 8.85 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12827256555889344		[learning rate: 1.8456e-05]
		[batch 20/20] avg loss: 0.12324887315861485		[learning rate: 1.8422e-05]
	Learning Rate: 1.84222e-05
	LOSS [training: 0.12576071935875416 | validation: 0.09468940657626801]
	TIME [epoch: 8.83 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12502446384588217		[learning rate: 1.8389e-05]
		[batch 20/20] avg loss: 0.12426098345019065		[learning rate: 1.8355e-05]
	Learning Rate: 1.83554e-05
	LOSS [training: 0.12464272364803639 | validation: 0.10367138072204826]
	TIME [epoch: 8.84 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12075743151649068		[learning rate: 1.8322e-05]
		[batch 20/20] avg loss: 0.1256180241302637		[learning rate: 1.8289e-05]
	Learning Rate: 1.82888e-05
	LOSS [training: 0.1231877278233772 | validation: 0.10737842544279369]
	TIME [epoch: 8.83 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12381316918652618		[learning rate: 1.8256e-05]
		[batch 20/20] avg loss: 0.11851262478364522		[learning rate: 1.8222e-05]
	Learning Rate: 1.82224e-05
	LOSS [training: 0.12116289698508571 | validation: 0.10871282325549184]
	TIME [epoch: 8.86 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12023258733384587		[learning rate: 1.8189e-05]
		[batch 20/20] avg loss: 0.12828538316573865		[learning rate: 1.8156e-05]
	Learning Rate: 1.81563e-05
	LOSS [training: 0.12425898524979226 | validation: 0.10175177037722136]
	TIME [epoch: 8.83 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1309002564026521		[learning rate: 1.8123e-05]
		[batch 20/20] avg loss: 0.12214998828969889		[learning rate: 1.809e-05]
	Learning Rate: 1.80904e-05
	LOSS [training: 0.12652512234617547 | validation: 0.10763387489991869]
	TIME [epoch: 8.83 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12356513985269332		[learning rate: 1.8058e-05]
		[batch 20/20] avg loss: 0.12153212952518203		[learning rate: 1.8025e-05]
	Learning Rate: 1.80247e-05
	LOSS [training: 0.12254863468893769 | validation: 0.10137499207924143]
	TIME [epoch: 8.84 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11671733702322536		[learning rate: 1.7992e-05]
		[batch 20/20] avg loss: 0.1270240867706845		[learning rate: 1.7959e-05]
	Learning Rate: 1.79593e-05
	LOSS [training: 0.12187071189695493 | validation: 0.09886657076015255]
	TIME [epoch: 8.84 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12293566564683345		[learning rate: 1.7927e-05]
		[batch 20/20] avg loss: 0.12751356560438826		[learning rate: 1.7894e-05]
	Learning Rate: 1.78941e-05
	LOSS [training: 0.12522461562561088 | validation: 0.10921498861549857]
	TIME [epoch: 8.84 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1373452612711575		[learning rate: 1.7862e-05]
		[batch 20/20] avg loss: 0.12024308413271592		[learning rate: 1.7829e-05]
	Learning Rate: 1.78292e-05
	LOSS [training: 0.12879417270193672 | validation: 0.1117513231783727]
	TIME [epoch: 8.84 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12029528740706222		[learning rate: 1.7797e-05]
		[batch 20/20] avg loss: 0.13005512836017014		[learning rate: 1.7764e-05]
	Learning Rate: 1.77645e-05
	LOSS [training: 0.1251752078836162 | validation: 0.0971313822116458]
	TIME [epoch: 8.84 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12338344740334573		[learning rate: 1.7732e-05]
		[batch 20/20] avg loss: 0.13326737601101465		[learning rate: 1.77e-05]
	Learning Rate: 1.77e-05
	LOSS [training: 0.12832541170718018 | validation: 0.11206519873238235]
	TIME [epoch: 8.84 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1304843817396065		[learning rate: 1.7668e-05]
		[batch 20/20] avg loss: 0.12451973452202576		[learning rate: 1.7636e-05]
	Learning Rate: 1.76358e-05
	LOSS [training: 0.12750205813081614 | validation: 0.10446342098797172]
	TIME [epoch: 8.84 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13248926575822534		[learning rate: 1.7604e-05]
		[batch 20/20] avg loss: 0.11226388800364251		[learning rate: 1.7572e-05]
	Learning Rate: 1.75718e-05
	LOSS [training: 0.12237657688093395 | validation: 0.10246327806465873]
	TIME [epoch: 8.86 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12634049236798		[learning rate: 1.754e-05]
		[batch 20/20] avg loss: 0.1256329209716887		[learning rate: 1.7508e-05]
	Learning Rate: 1.7508e-05
	LOSS [training: 0.12598670666983436 | validation: 0.11487246197282534]
	TIME [epoch: 8.84 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11899431655366047		[learning rate: 1.7476e-05]
		[batch 20/20] avg loss: 0.13832215188669159		[learning rate: 1.7444e-05]
	Learning Rate: 1.74445e-05
	LOSS [training: 0.12865823422017605 | validation: 0.11051278289094552]
	TIME [epoch: 8.83 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13177847853170493		[learning rate: 1.7413e-05]
		[batch 20/20] avg loss: 0.11795617165365528		[learning rate: 1.7381e-05]
	Learning Rate: 1.73812e-05
	LOSS [training: 0.1248673250926801 | validation: 0.10545303218234763]
	TIME [epoch: 8.84 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11437249207429563		[learning rate: 1.735e-05]
		[batch 20/20] avg loss: 0.1365283669984564		[learning rate: 1.7318e-05]
	Learning Rate: 1.73181e-05
	LOSS [training: 0.12545042953637603 | validation: 0.1109651381455399]
	TIME [epoch: 8.85 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12480551483637028		[learning rate: 1.7287e-05]
		[batch 20/20] avg loss: 0.12814396081326476		[learning rate: 1.7255e-05]
	Learning Rate: 1.72552e-05
	LOSS [training: 0.1264747378248175 | validation: 0.11274165200807838]
	TIME [epoch: 8.84 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13082931280455673		[learning rate: 1.7224e-05]
		[batch 20/20] avg loss: 0.12160640982885515		[learning rate: 1.7193e-05]
	Learning Rate: 1.71926e-05
	LOSS [training: 0.12621786131670593 | validation: 0.10954821589163893]
	TIME [epoch: 8.83 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12825223110063858		[learning rate: 1.7161e-05]
		[batch 20/20] avg loss: 0.1242131235410792		[learning rate: 1.713e-05]
	Learning Rate: 1.71302e-05
	LOSS [training: 0.1262326773208589 | validation: 0.10528437455030058]
	TIME [epoch: 8.84 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12533841760726044		[learning rate: 1.7099e-05]
		[batch 20/20] avg loss: 0.12561334875148464		[learning rate: 1.7068e-05]
	Learning Rate: 1.70681e-05
	LOSS [training: 0.1254758831793725 | validation: 0.10853697063401907]
	TIME [epoch: 8.83 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12652232598338398		[learning rate: 1.7037e-05]
		[batch 20/20] avg loss: 0.12339356417864047		[learning rate: 1.7006e-05]
	Learning Rate: 1.70061e-05
	LOSS [training: 0.12495794508101224 | validation: 0.10402112385193205]
	TIME [epoch: 8.86 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1261070994697572		[learning rate: 1.6975e-05]
		[batch 20/20] avg loss: 0.12932396768258902		[learning rate: 1.6944e-05]
	Learning Rate: 1.69444e-05
	LOSS [training: 0.12771553357617313 | validation: 0.10738259795270327]
	TIME [epoch: 8.84 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11766414341637055		[learning rate: 1.6914e-05]
		[batch 20/20] avg loss: 0.1309231191410944		[learning rate: 1.6883e-05]
	Learning Rate: 1.68829e-05
	LOSS [training: 0.12429363127873246 | validation: 0.0978310987225115]
	TIME [epoch: 8.83 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12332289560944346		[learning rate: 1.6852e-05]
		[batch 20/20] avg loss: 0.12404162625080564		[learning rate: 1.6822e-05]
	Learning Rate: 1.68216e-05
	LOSS [training: 0.12368226093012455 | validation: 0.10362321233836022]
	TIME [epoch: 8.84 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12787848957241643		[learning rate: 1.6791e-05]
		[batch 20/20] avg loss: 0.11804010028057506		[learning rate: 1.6761e-05]
	Learning Rate: 1.67606e-05
	LOSS [training: 0.12295929492649574 | validation: 0.09745020790371507]
	TIME [epoch: 8.84 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11897476720284224		[learning rate: 1.673e-05]
		[batch 20/20] avg loss: 0.1280870006982649		[learning rate: 1.67e-05]
	Learning Rate: 1.66998e-05
	LOSS [training: 0.12353088395055356 | validation: 0.10809390891259738]
	TIME [epoch: 8.86 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13010148239721997		[learning rate: 1.6669e-05]
		[batch 20/20] avg loss: 0.11394030305683756		[learning rate: 1.6639e-05]
	Learning Rate: 1.66392e-05
	LOSS [training: 0.12202089272702872 | validation: 0.09891003377738117]
	TIME [epoch: 8.84 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12415898182661518		[learning rate: 1.6609e-05]
		[batch 20/20] avg loss: 0.12657906860881485		[learning rate: 1.6579e-05]
	Learning Rate: 1.65788e-05
	LOSS [training: 0.12536902521771504 | validation: 0.10193774420055243]
	TIME [epoch: 8.83 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12293856854917615		[learning rate: 1.6549e-05]
		[batch 20/20] avg loss: 0.12398297447296003		[learning rate: 1.6519e-05]
	Learning Rate: 1.65186e-05
	LOSS [training: 0.12346077151106809 | validation: 0.10725262814136141]
	TIME [epoch: 8.84 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12009867043326386		[learning rate: 1.6489e-05]
		[batch 20/20] avg loss: 0.12435004195234452		[learning rate: 1.6459e-05]
	Learning Rate: 1.64587e-05
	LOSS [training: 0.12222435619280418 | validation: 0.09457231005181042]
	TIME [epoch: 8.86 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12161496034053154		[learning rate: 1.6429e-05]
		[batch 20/20] avg loss: 0.12750361678582517		[learning rate: 1.6399e-05]
	Learning Rate: 1.63989e-05
	LOSS [training: 0.12455928856317833 | validation: 0.10535036314018036]
	TIME [epoch: 8.85 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13125929156386307		[learning rate: 1.6369e-05]
		[batch 20/20] avg loss: 0.11646476953287052		[learning rate: 1.6339e-05]
	Learning Rate: 1.63394e-05
	LOSS [training: 0.12386203054836678 | validation: 0.10551349834416064]
	TIME [epoch: 8.84 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10392425561478058		[learning rate: 1.631e-05]
		[batch 20/20] avg loss: 0.1429259139622858		[learning rate: 1.628e-05]
	Learning Rate: 1.62801e-05
	LOSS [training: 0.1234250847885332 | validation: 0.10275551408691612]
	TIME [epoch: 8.84 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1333816720503565		[learning rate: 1.6251e-05]
		[batch 20/20] avg loss: 0.11490801541571907		[learning rate: 1.6221e-05]
	Learning Rate: 1.62211e-05
	LOSS [training: 0.12414484373303777 | validation: 0.10164663472466869]
	TIME [epoch: 8.84 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1253227693093128		[learning rate: 1.6192e-05]
		[batch 20/20] avg loss: 0.12819599050193634		[learning rate: 1.6162e-05]
	Learning Rate: 1.61622e-05
	LOSS [training: 0.12675937990562458 | validation: 0.10110472557099745]
	TIME [epoch: 8.86 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13109830870125577		[learning rate: 1.6133e-05]
		[batch 20/20] avg loss: 0.11871420863457031		[learning rate: 1.6104e-05]
	Learning Rate: 1.61035e-05
	LOSS [training: 0.12490625866791302 | validation: 0.0981611243294588]
	TIME [epoch: 8.85 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12183824188699313		[learning rate: 1.6074e-05]
		[batch 20/20] avg loss: 0.12438398595616176		[learning rate: 1.6045e-05]
	Learning Rate: 1.60451e-05
	LOSS [training: 0.12311111392157745 | validation: 0.09331666316795077]
	TIME [epoch: 8.84 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11993215123527859		[learning rate: 1.6016e-05]
		[batch 20/20] avg loss: 0.1276410529650419		[learning rate: 1.5987e-05]
	Learning Rate: 1.59869e-05
	LOSS [training: 0.12378660210016028 | validation: 0.10005146225635884]
	TIME [epoch: 8.84 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1157212600517071		[learning rate: 1.5958e-05]
		[batch 20/20] avg loss: 0.12360651232396365		[learning rate: 1.5929e-05]
	Learning Rate: 1.59288e-05
	LOSS [training: 0.11966388618783537 | validation: 0.09785907138639738]
	TIME [epoch: 8.85 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11941361813900189		[learning rate: 1.59e-05]
		[batch 20/20] avg loss: 0.12359693599812879		[learning rate: 1.5871e-05]
	Learning Rate: 1.5871e-05
	LOSS [training: 0.12150527706856533 | validation: 0.09313385378446545]
	TIME [epoch: 8.85 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13128595714293675		[learning rate: 1.5842e-05]
		[batch 20/20] avg loss: 0.11319239340510209		[learning rate: 1.5813e-05]
	Learning Rate: 1.58134e-05
	LOSS [training: 0.12223917527401944 | validation: 0.09887429062829166]
	TIME [epoch: 8.85 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12709953647778516		[learning rate: 1.5785e-05]
		[batch 20/20] avg loss: 0.12241975784138281		[learning rate: 1.5756e-05]
	Learning Rate: 1.57561e-05
	LOSS [training: 0.12475964715958396 | validation: 0.10843157657839458]
	TIME [epoch: 8.84 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1151544788027071		[learning rate: 1.5727e-05]
		[batch 20/20] avg loss: 0.12922735046109635		[learning rate: 1.5699e-05]
	Learning Rate: 1.56989e-05
	LOSS [training: 0.12219091463190172 | validation: 0.09805793239771833]
	TIME [epoch: 8.85 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12005819108381377		[learning rate: 1.567e-05]
		[batch 20/20] avg loss: 0.13032904457622524		[learning rate: 1.5642e-05]
	Learning Rate: 1.56419e-05
	LOSS [training: 0.12519361783001945 | validation: 0.10619010170872065]
	TIME [epoch: 8.86 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1285855169620465		[learning rate: 1.5613e-05]
		[batch 20/20] avg loss: 0.12180137668982746		[learning rate: 1.5585e-05]
	Learning Rate: 1.55851e-05
	LOSS [training: 0.12519344682593697 | validation: 0.12311113780318392]
	TIME [epoch: 8.85 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13669498482488776		[learning rate: 1.5557e-05]
		[batch 20/20] avg loss: 0.11487124682929095		[learning rate: 1.5529e-05]
	Learning Rate: 1.55286e-05
	LOSS [training: 0.12578311582708937 | validation: 0.10202402747383164]
	TIME [epoch: 8.85 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.116988244800767		[learning rate: 1.55e-05]
		[batch 20/20] avg loss: 0.13063997600989857		[learning rate: 1.5472e-05]
	Learning Rate: 1.54722e-05
	LOSS [training: 0.12381411040533277 | validation: 0.10983715560290078]
	TIME [epoch: 8.85 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1306583414748505		[learning rate: 1.5444e-05]
		[batch 20/20] avg loss: 0.12900466634203667		[learning rate: 1.5416e-05]
	Learning Rate: 1.54161e-05
	LOSS [training: 0.12983150390844359 | validation: 0.10756998839244489]
	TIME [epoch: 8.84 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12342124032505646		[learning rate: 1.5388e-05]
		[batch 20/20] avg loss: 0.12915213279397825		[learning rate: 1.536e-05]
	Learning Rate: 1.53601e-05
	LOSS [training: 0.12628668655951736 | validation: 0.12364961649316324]
	TIME [epoch: 8.87 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12819775006566486		[learning rate: 1.5332e-05]
		[batch 20/20] avg loss: 0.12281662572843215		[learning rate: 1.5304e-05]
	Learning Rate: 1.53044e-05
	LOSS [training: 0.12550718789704846 | validation: 0.10241687040265063]
	TIME [epoch: 8.85 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12398738359473978		[learning rate: 1.5277e-05]
		[batch 20/20] avg loss: 0.1264095403604221		[learning rate: 1.5249e-05]
	Learning Rate: 1.52488e-05
	LOSS [training: 0.12519846197758094 | validation: 0.09862460069281273]
	TIME [epoch: 8.84 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11753559917611855		[learning rate: 1.5221e-05]
		[batch 20/20] avg loss: 0.1273237972480363		[learning rate: 1.5194e-05]
	Learning Rate: 1.51935e-05
	LOSS [training: 0.12242969821207741 | validation: 0.09814476528929152]
	TIME [epoch: 8.83 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10606390469861252		[learning rate: 1.5166e-05]
		[batch 20/20] avg loss: 0.13878996146530892		[learning rate: 1.5138e-05]
	Learning Rate: 1.51384e-05
	LOSS [training: 0.1224269330819607 | validation: 0.10900454776400988]
	TIME [epoch: 8.84 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13711150137851183		[learning rate: 1.5111e-05]
		[batch 20/20] avg loss: 0.11275656598665051		[learning rate: 1.5083e-05]
	Learning Rate: 1.50834e-05
	LOSS [training: 0.12493403368258116 | validation: 0.10568739891653428]
	TIME [epoch: 8.86 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13233952925741974		[learning rate: 1.5056e-05]
		[batch 20/20] avg loss: 0.11471631633983899		[learning rate: 1.5029e-05]
	Learning Rate: 1.50287e-05
	LOSS [training: 0.12352792279862937 | validation: 0.10295558622206882]
	TIME [epoch: 8.84 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12204769473308448		[learning rate: 1.5001e-05]
		[batch 20/20] avg loss: 0.1335842746811096		[learning rate: 1.4974e-05]
	Learning Rate: 1.49741e-05
	LOSS [training: 0.12781598470709704 | validation: 0.10344210802231976]
	TIME [epoch: 8.84 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12807030645500525		[learning rate: 1.4947e-05]
		[batch 20/20] avg loss: 0.1199724841032713		[learning rate: 1.492e-05]
	Learning Rate: 1.49198e-05
	LOSS [training: 0.12402139527913827 | validation: 0.10697295786153178]
	TIME [epoch: 8.83 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13207699003008944		[learning rate: 1.4893e-05]
		[batch 20/20] avg loss: 0.11993218483090703		[learning rate: 1.4866e-05]
	Learning Rate: 1.48657e-05
	LOSS [training: 0.12600458743049825 | validation: 0.10662979006367622]
	TIME [epoch: 8.85 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.107516327659217		[learning rate: 1.4839e-05]
		[batch 20/20] avg loss: 0.13263469351501783		[learning rate: 1.4812e-05]
	Learning Rate: 1.48117e-05
	LOSS [training: 0.1200755105871174 | validation: 0.11614118171007234]
	TIME [epoch: 8.85 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10567254832659172		[learning rate: 1.4785e-05]
		[batch 20/20] avg loss: 0.13704414454755637		[learning rate: 1.4758e-05]
	Learning Rate: 1.4758e-05
	LOSS [training: 0.12135834643707406 | validation: 0.11097266204024739]
	TIME [epoch: 8.85 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12688101433189494		[learning rate: 1.4731e-05]
		[batch 20/20] avg loss: 0.12488267443251382		[learning rate: 1.4704e-05]
	Learning Rate: 1.47044e-05
	LOSS [training: 0.12588184438220437 | validation: 0.09987569323526739]
	TIME [epoch: 8.85 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11308391259889339		[learning rate: 1.4678e-05]
		[batch 20/20] avg loss: 0.14262046288616428		[learning rate: 1.4651e-05]
	Learning Rate: 1.4651e-05
	LOSS [training: 0.12785218774252885 | validation: 0.10644308418296258]
	TIME [epoch: 8.84 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.126520097905831		[learning rate: 1.4624e-05]
		[batch 20/20] avg loss: 0.12942811068310475		[learning rate: 1.4598e-05]
	Learning Rate: 1.45979e-05
	LOSS [training: 0.1279741042944679 | validation: 0.11241891957980711]
	TIME [epoch: 8.87 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11965074201530665		[learning rate: 1.4571e-05]
		[batch 20/20] avg loss: 0.13122722305456508		[learning rate: 1.4545e-05]
	Learning Rate: 1.45449e-05
	LOSS [training: 0.12543898253493585 | validation: 0.09814432071041146]
	TIME [epoch: 8.85 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1222381385164798		[learning rate: 1.4518e-05]
		[batch 20/20] avg loss: 0.1305459360031989		[learning rate: 1.4492e-05]
	Learning Rate: 1.44921e-05
	LOSS [training: 0.12639203725983933 | validation: 0.10480845776761026]
	TIME [epoch: 8.85 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12156961332791309		[learning rate: 1.4466e-05]
		[batch 20/20] avg loss: 0.13055903527479304		[learning rate: 1.444e-05]
	Learning Rate: 1.44395e-05
	LOSS [training: 0.12606432430135306 | validation: 0.09784212328144415]
	TIME [epoch: 8.85 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12091608878998983		[learning rate: 1.4413e-05]
		[batch 20/20] avg loss: 0.12401822280822775		[learning rate: 1.4387e-05]
	Learning Rate: 1.43871e-05
	LOSS [training: 0.12246715579910879 | validation: 0.10289820392892471]
	TIME [epoch: 8.85 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.127244682082869		[learning rate: 1.4361e-05]
		[batch 20/20] avg loss: 0.12108333389571886		[learning rate: 1.4335e-05]
	Learning Rate: 1.43349e-05
	LOSS [training: 0.12416400798929392 | validation: 0.10167842590174324]
	TIME [epoch: 8.87 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12486229250756861		[learning rate: 1.4309e-05]
		[batch 20/20] avg loss: 0.11863980023538896		[learning rate: 1.4283e-05]
	Learning Rate: 1.42829e-05
	LOSS [training: 0.12175104637147878 | validation: 0.09988435464110035]
	TIME [epoch: 8.85 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1375766809880837		[learning rate: 1.4257e-05]
		[batch 20/20] avg loss: 0.11375013617624567		[learning rate: 1.4231e-05]
	Learning Rate: 1.4231e-05
	LOSS [training: 0.1256634085821647 | validation: 0.1097229648024361]
	TIME [epoch: 8.85 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11878062395566007		[learning rate: 1.4205e-05]
		[batch 20/20] avg loss: 0.12610721574152878		[learning rate: 1.4179e-05]
	Learning Rate: 1.41794e-05
	LOSS [training: 0.12244391984859442 | validation: 0.10213547587857297]
	TIME [epoch: 8.85 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11894143333990548		[learning rate: 1.4154e-05]
		[batch 20/20] avg loss: 0.13430220493867956		[learning rate: 1.4128e-05]
	Learning Rate: 1.41279e-05
	LOSS [training: 0.12662181913929255 | validation: 0.10506353721090063]
	TIME [epoch: 8.85 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12325859568786042		[learning rate: 1.4102e-05]
		[batch 20/20] avg loss: 0.12332518828345895		[learning rate: 1.4077e-05]
	Learning Rate: 1.40767e-05
	LOSS [training: 0.12329189198565968 | validation: 0.10447269908352218]
	TIME [epoch: 8.88 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11250907704830689		[learning rate: 1.4051e-05]
		[batch 20/20] avg loss: 0.13252651313906874		[learning rate: 1.4026e-05]
	Learning Rate: 1.40256e-05
	LOSS [training: 0.12251779509368779 | validation: 0.10612502844946957]
	TIME [epoch: 8.84 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12369261487209449		[learning rate: 1.4e-05]
		[batch 20/20] avg loss: 0.12426094954697157		[learning rate: 1.3975e-05]
	Learning Rate: 1.39747e-05
	LOSS [training: 0.12397678220953301 | validation: 0.10099254603192402]
	TIME [epoch: 8.85 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12569266485341854		[learning rate: 1.3949e-05]
		[batch 20/20] avg loss: 0.11920303029360102		[learning rate: 1.3924e-05]
	Learning Rate: 1.3924e-05
	LOSS [training: 0.12244784757350975 | validation: 0.10734946944925747]
	TIME [epoch: 8.85 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12846183618850393		[learning rate: 1.3899e-05]
		[batch 20/20] avg loss: 0.12413868187287813		[learning rate: 1.3873e-05]
	Learning Rate: 1.38734e-05
	LOSS [training: 0.12630025903069103 | validation: 0.10860086869462264]
	TIME [epoch: 8.86 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12056806317210116		[learning rate: 1.3848e-05]
		[batch 20/20] avg loss: 0.12795949315395616		[learning rate: 1.3823e-05]
	Learning Rate: 1.38231e-05
	LOSS [training: 0.12426377816302867 | validation: 0.10693871422837418]
	TIME [epoch: 8.85 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12198658571174859		[learning rate: 1.3798e-05]
		[batch 20/20] avg loss: 0.1295520437627462		[learning rate: 1.3773e-05]
	Learning Rate: 1.37729e-05
	LOSS [training: 0.1257693147372474 | validation: 0.10349404094098058]
	TIME [epoch: 8.84 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12841363928109822		[learning rate: 1.3748e-05]
		[batch 20/20] avg loss: 0.12278008318039102		[learning rate: 1.3723e-05]
	Learning Rate: 1.37229e-05
	LOSS [training: 0.1255968612307446 | validation: 0.10007816162640437]
	TIME [epoch: 8.84 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13482714841252494		[learning rate: 1.3698e-05]
		[batch 20/20] avg loss: 0.11597328203633599		[learning rate: 1.3673e-05]
	Learning Rate: 1.36731e-05
	LOSS [training: 0.12540021522443046 | validation: 0.10920977213449809]
	TIME [epoch: 8.85 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13248165119169372		[learning rate: 1.3648e-05]
		[batch 20/20] avg loss: 0.10959790922699324		[learning rate: 1.3624e-05]
	Learning Rate: 1.36235e-05
	LOSS [training: 0.12103978020934347 | validation: 0.10095432597044626]
	TIME [epoch: 8.86 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11729379119827084		[learning rate: 1.3599e-05]
		[batch 20/20] avg loss: 0.1174463275550676		[learning rate: 1.3574e-05]
	Learning Rate: 1.35741e-05
	LOSS [training: 0.11737005937666922 | validation: 0.10197276486372084]
	TIME [epoch: 8.84 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1310857916378793		[learning rate: 1.3549e-05]
		[batch 20/20] avg loss: 0.11791557295014157		[learning rate: 1.3525e-05]
	Learning Rate: 1.35248e-05
	LOSS [training: 0.12450068229401041 | validation: 0.1101299074547216]
	TIME [epoch: 8.84 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11672546879706333		[learning rate: 1.35e-05]
		[batch 20/20] avg loss: 0.12889590542615673		[learning rate: 1.3476e-05]
	Learning Rate: 1.34757e-05
	LOSS [training: 0.12281068711161 | validation: 0.10482934121043376]
	TIME [epoch: 8.85 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12552215047202617		[learning rate: 1.3451e-05]
		[batch 20/20] avg loss: 0.12563177193934033		[learning rate: 1.3427e-05]
	Learning Rate: 1.34268e-05
	LOSS [training: 0.12557696120568324 | validation: 0.10774032715901213]
	TIME [epoch: 8.84 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12603578826233156		[learning rate: 1.3402e-05]
		[batch 20/20] avg loss: 0.11797826214328307		[learning rate: 1.3378e-05]
	Learning Rate: 1.33781e-05
	LOSS [training: 0.12200702520280729 | validation: 0.10632636135572615]
	TIME [epoch: 8.86 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1226600737439908		[learning rate: 1.3354e-05]
		[batch 20/20] avg loss: 0.1258784019834201		[learning rate: 1.333e-05]
	Learning Rate: 1.33296e-05
	LOSS [training: 0.12426923786370545 | validation: 0.09529509209064574]
	TIME [epoch: 8.84 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13607572668238324		[learning rate: 1.3305e-05]
		[batch 20/20] avg loss: 0.11341449775758401		[learning rate: 1.3281e-05]
	Learning Rate: 1.32812e-05
	LOSS [training: 0.12474511221998363 | validation: 0.10535736361544028]
	TIME [epoch: 8.83 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12875378647383945		[learning rate: 1.3257e-05]
		[batch 20/20] avg loss: 0.12178921102798276		[learning rate: 1.3233e-05]
	Learning Rate: 1.3233e-05
	LOSS [training: 0.12527149875091112 | validation: 0.10100049213049452]
	TIME [epoch: 8.85 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12241972161838141		[learning rate: 1.3209e-05]
		[batch 20/20] avg loss: 0.12172833329381214		[learning rate: 1.3185e-05]
	Learning Rate: 1.3185e-05
	LOSS [training: 0.12207402745609677 | validation: 0.10545938883612213]
	TIME [epoch: 8.85 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13018051987700904		[learning rate: 1.3161e-05]
		[batch 20/20] avg loss: 0.11907044357104886		[learning rate: 1.3137e-05]
	Learning Rate: 1.31371e-05
	LOSS [training: 0.12462548172402896 | validation: 0.10571347753139748]
	TIME [epoch: 8.84 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1265356154193757		[learning rate: 1.3113e-05]
		[batch 20/20] avg loss: 0.11938110344168318		[learning rate: 1.3089e-05]
	Learning Rate: 1.30894e-05
	LOSS [training: 0.12295835943052942 | validation: 0.09961535185565162]
	TIME [epoch: 8.84 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11550866305414705		[learning rate: 1.3066e-05]
		[batch 20/20] avg loss: 0.13004463356198837		[learning rate: 1.3042e-05]
	Learning Rate: 1.30419e-05
	LOSS [training: 0.1227766483080677 | validation: 0.1024327184263005]
	TIME [epoch: 8.84 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11377684023524288		[learning rate: 1.3018e-05]
		[batch 20/20] avg loss: 0.13216036376504872		[learning rate: 1.2995e-05]
	Learning Rate: 1.29946e-05
	LOSS [training: 0.12296860200014581 | validation: 0.10165638707697501]
	TIME [epoch: 8.85 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1184013833506216		[learning rate: 1.2971e-05]
		[batch 20/20] avg loss: 0.12902704889650596		[learning rate: 1.2947e-05]
	Learning Rate: 1.29475e-05
	LOSS [training: 0.12371421612356379 | validation: 0.10786837243747029]
	TIME [epoch: 8.86 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12214307532572843		[learning rate: 1.2924e-05]
		[batch 20/20] avg loss: 0.12307508328189425		[learning rate: 1.29e-05]
	Learning Rate: 1.29005e-05
	LOSS [training: 0.12260907930381137 | validation: 0.10825829653782669]
	TIME [epoch: 8.84 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11584410245277618		[learning rate: 1.2877e-05]
		[batch 20/20] avg loss: 0.1256557235710169		[learning rate: 1.2854e-05]
	Learning Rate: 1.28536e-05
	LOSS [training: 0.12074991301189655 | validation: 0.10243103366978953]
	TIME [epoch: 8.84 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11757731019851173		[learning rate: 1.283e-05]
		[batch 20/20] avg loss: 0.12507233568445472		[learning rate: 1.2807e-05]
	Learning Rate: 1.2807e-05
	LOSS [training: 0.12132482294148321 | validation: 0.10010359233385985]
	TIME [epoch: 8.84 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13990145829796094		[learning rate: 1.2784e-05]
		[batch 20/20] avg loss: 0.10731648954140409		[learning rate: 1.2761e-05]
	Learning Rate: 1.27605e-05
	LOSS [training: 0.12360897391968251 | validation: 0.10594274207428354]
	TIME [epoch: 8.85 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.134708547582185		[learning rate: 1.2737e-05]
		[batch 20/20] avg loss: 0.11097139376414064		[learning rate: 1.2714e-05]
	Learning Rate: 1.27142e-05
	LOSS [training: 0.12283997067316281 | validation: 0.09880822501979236]
	TIME [epoch: 8.86 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11260268573677842		[learning rate: 1.2691e-05]
		[batch 20/20] avg loss: 0.13350828503313275		[learning rate: 1.2668e-05]
	Learning Rate: 1.26681e-05
	LOSS [training: 0.12305548538495556 | validation: 0.1010991380785843]
	TIME [epoch: 8.84 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12370351973660558		[learning rate: 1.2645e-05]
		[batch 20/20] avg loss: 0.12208921638337118		[learning rate: 1.2622e-05]
	Learning Rate: 1.26221e-05
	LOSS [training: 0.12289636805998841 | validation: 0.10127860295856494]
	TIME [epoch: 8.84 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1154141204696247		[learning rate: 1.2599e-05]
		[batch 20/20] avg loss: 0.12456122618829421		[learning rate: 1.2576e-05]
	Learning Rate: 1.25763e-05
	LOSS [training: 0.11998767332895946 | validation: 0.10104634612487823]
	TIME [epoch: 8.84 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12900123497026128		[learning rate: 1.2553e-05]
		[batch 20/20] avg loss: 0.1182786252810816		[learning rate: 1.2531e-05]
	Learning Rate: 1.25307e-05
	LOSS [training: 0.12363993012567145 | validation: 0.10999285605981592]
	TIME [epoch: 8.87 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11778210421590203		[learning rate: 1.2508e-05]
		[batch 20/20] avg loss: 0.12602814983881747		[learning rate: 1.2485e-05]
	Learning Rate: 1.24852e-05
	LOSS [training: 0.12190512702735976 | validation: 0.10208007073411338]
	TIME [epoch: 8.84 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11701857370537203		[learning rate: 1.2463e-05]
		[batch 20/20] avg loss: 0.12640748199418306		[learning rate: 1.244e-05]
	Learning Rate: 1.24399e-05
	LOSS [training: 0.12171302784977756 | validation: 0.10087446845372271]
	TIME [epoch: 8.85 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1269043500198627		[learning rate: 1.2417e-05]
		[batch 20/20] avg loss: 0.12201369155515285		[learning rate: 1.2395e-05]
	Learning Rate: 1.23947e-05
	LOSS [training: 0.12445902078750781 | validation: 0.10150917147863044]
	TIME [epoch: 8.84 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11927221070280522		[learning rate: 1.2372e-05]
		[batch 20/20] avg loss: 0.12849425295929479		[learning rate: 1.235e-05]
	Learning Rate: 1.23497e-05
	LOSS [training: 0.12388323183105002 | validation: 0.09639572127968882]
	TIME [epoch: 8.83 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12966318520635328		[learning rate: 1.2327e-05]
		[batch 20/20] avg loss: 0.1234601326376811		[learning rate: 1.2305e-05]
	Learning Rate: 1.23049e-05
	LOSS [training: 0.12656165892201718 | validation: 0.09733974922211602]
	TIME [epoch: 8.87 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11725883006721056		[learning rate: 1.2283e-05]
		[batch 20/20] avg loss: 0.13303136395010998		[learning rate: 1.226e-05]
	Learning Rate: 1.22603e-05
	LOSS [training: 0.12514509700866025 | validation: 0.1130732268084494]
	TIME [epoch: 8.84 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13692310038555044		[learning rate: 1.2238e-05]
		[batch 20/20] avg loss: 0.10986551481039963		[learning rate: 1.2216e-05]
	Learning Rate: 1.22158e-05
	LOSS [training: 0.12339430759797505 | validation: 0.10516349075229656]
	TIME [epoch: 8.84 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11435286649880827		[learning rate: 1.2194e-05]
		[batch 20/20] avg loss: 0.12954180889085176		[learning rate: 1.2171e-05]
	Learning Rate: 1.21714e-05
	LOSS [training: 0.12194733769483004 | validation: 0.10911846617555936]
	TIME [epoch: 8.84 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11567636668438037		[learning rate: 1.2149e-05]
		[batch 20/20] avg loss: 0.1325623014044235		[learning rate: 1.2127e-05]
	Learning Rate: 1.21273e-05
	LOSS [training: 0.12411933404440192 | validation: 0.10618494228290841]
	TIME [epoch: 8.82 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12011875123799129		[learning rate: 1.2105e-05]
		[batch 20/20] avg loss: 0.12516281993462935		[learning rate: 1.2083e-05]
	Learning Rate: 1.20833e-05
	LOSS [training: 0.12264078558631031 | validation: 0.10649901130133885]
	TIME [epoch: 8.85 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12352541559160475		[learning rate: 1.2061e-05]
		[batch 20/20] avg loss: 0.1203109613532261		[learning rate: 1.2039e-05]
	Learning Rate: 1.20394e-05
	LOSS [training: 0.1219181884724154 | validation: 0.10630810568329892]
	TIME [epoch: 8.84 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12184754569365594		[learning rate: 1.2018e-05]
		[batch 20/20] avg loss: 0.12039975027213545		[learning rate: 1.1996e-05]
	Learning Rate: 1.19957e-05
	LOSS [training: 0.1211236479828957 | validation: 0.10717648474319261]
	TIME [epoch: 8.84 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12827130267049203		[learning rate: 1.1974e-05]
		[batch 20/20] avg loss: 0.12182132840929069		[learning rate: 1.1952e-05]
	Learning Rate: 1.19522e-05
	LOSS [training: 0.12504631553989137 | validation: 0.10541211254338018]
	TIME [epoch: 8.83 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.112404341144266		[learning rate: 1.193e-05]
		[batch 20/20] avg loss: 0.13292084244911845		[learning rate: 1.1909e-05]
	Learning Rate: 1.19088e-05
	LOSS [training: 0.12266259179669221 | validation: 0.0975781012041646]
	TIME [epoch: 8.85 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11110665931112788		[learning rate: 1.1887e-05]
		[batch 20/20] avg loss: 0.13059876315952654		[learning rate: 1.1866e-05]
	Learning Rate: 1.18656e-05
	LOSS [training: 0.12085271123532719 | validation: 0.10235587016861972]
	TIME [epoch: 8.85 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12986782635512775		[learning rate: 1.1844e-05]
		[batch 20/20] avg loss: 0.11850070308341679		[learning rate: 1.1823e-05]
	Learning Rate: 1.18225e-05
	LOSS [training: 0.12418426471927228 | validation: 0.10344144295360802]
	TIME [epoch: 8.84 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12767932327777068		[learning rate: 1.1801e-05]
		[batch 20/20] avg loss: 0.11309438297844068		[learning rate: 1.178e-05]
	Learning Rate: 1.17796e-05
	LOSS [training: 0.12038685312810568 | validation: 0.11490591074290023]
	TIME [epoch: 8.83 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12534471265661562		[learning rate: 1.1758e-05]
		[batch 20/20] avg loss: 0.12135382474359506		[learning rate: 1.1737e-05]
	Learning Rate: 1.17369e-05
	LOSS [training: 0.12334926870010532 | validation: 0.11551797141944259]
	TIME [epoch: 8.83 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12331259753198273		[learning rate: 1.1716e-05]
		[batch 20/20] avg loss: 0.11352812501773386		[learning rate: 1.1694e-05]
	Learning Rate: 1.16943e-05
	LOSS [training: 0.11842036127485826 | validation: 0.10513787995849971]
	TIME [epoch: 8.86 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1357791356223273		[learning rate: 1.1673e-05]
		[batch 20/20] avg loss: 0.10751015433771205		[learning rate: 1.1652e-05]
	Learning Rate: 1.16518e-05
	LOSS [training: 0.12164464498001963 | validation: 0.10780023901201734]
	TIME [epoch: 8.83 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12349396864976488		[learning rate: 1.1631e-05]
		[batch 20/20] avg loss: 0.115833802302667		[learning rate: 1.161e-05]
	Learning Rate: 1.16096e-05
	LOSS [training: 0.11966388547621595 | validation: 0.10518322956949613]
	TIME [epoch: 8.84 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12599012337732182		[learning rate: 1.1588e-05]
		[batch 20/20] avg loss: 0.11871660756972953		[learning rate: 1.1567e-05]
	Learning Rate: 1.15674e-05
	LOSS [training: 0.12235336547352568 | validation: 0.10348416176056449]
	TIME [epoch: 8.84 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11960362843751937		[learning rate: 1.1546e-05]
		[batch 20/20] avg loss: 0.12342065017259096		[learning rate: 1.1525e-05]
	Learning Rate: 1.15255e-05
	LOSS [training: 0.12151213930505518 | validation: 0.09546238066373225]
	TIME [epoch: 8.84 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11296187383908052		[learning rate: 1.1505e-05]
		[batch 20/20] avg loss: 0.13210644709018984		[learning rate: 1.1484e-05]
	Learning Rate: 1.14836e-05
	LOSS [training: 0.12253416046463517 | validation: 0.0989154823160643]
	TIME [epoch: 8.85 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12429120315320312		[learning rate: 1.1463e-05]
		[batch 20/20] avg loss: 0.11721016134139248		[learning rate: 1.1442e-05]
	Learning Rate: 1.1442e-05
	LOSS [training: 0.12075068224729782 | validation: 0.09756378012988129]
	TIME [epoch: 8.84 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1253253234568497		[learning rate: 1.1421e-05]
		[batch 20/20] avg loss: 0.12298199386065738		[learning rate: 1.14e-05]
	Learning Rate: 1.14004e-05
	LOSS [training: 0.12415365865875354 | validation: 0.10355220963469947]
	TIME [epoch: 8.84 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12987437830930532		[learning rate: 1.138e-05]
		[batch 20/20] avg loss: 0.11405626403807023		[learning rate: 1.1359e-05]
	Learning Rate: 1.13591e-05
	LOSS [training: 0.12196532117368775 | validation: 0.11286034632546577]
	TIME [epoch: 8.83 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13025722567872194		[learning rate: 1.1338e-05]
		[batch 20/20] avg loss: 0.12095817881463097		[learning rate: 1.1318e-05]
	Learning Rate: 1.13178e-05
	LOSS [training: 0.12560770224667644 | validation: 0.11704252259907096]
	TIME [epoch: 8.84 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12481373272699252		[learning rate: 1.1297e-05]
		[batch 20/20] avg loss: 0.12278380172492913		[learning rate: 1.1277e-05]
	Learning Rate: 1.12768e-05
	LOSS [training: 0.12379876722596084 | validation: 0.10680299769947693]
	TIME [epoch: 8.86 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1254679316438905		[learning rate: 1.1256e-05]
		[batch 20/20] avg loss: 0.11994279422765544		[learning rate: 1.1236e-05]
	Learning Rate: 1.12358e-05
	LOSS [training: 0.12270536293577297 | validation: 0.10322464111084811]
	TIME [epoch: 8.84 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12698877569428613		[learning rate: 1.1215e-05]
		[batch 20/20] avg loss: 0.11978803903064399		[learning rate: 1.1195e-05]
	Learning Rate: 1.11951e-05
	LOSS [training: 0.12338840736246504 | validation: 0.0920537981827365]
	TIME [epoch: 8.83 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12528760773217434		[learning rate: 1.1175e-05]
		[batch 20/20] avg loss: 0.1211703338310666		[learning rate: 1.1154e-05]
	Learning Rate: 1.11544e-05
	LOSS [training: 0.12322897078162047 | validation: 0.09456727949160376]
	TIME [epoch: 8.84 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11698004144270646		[learning rate: 1.1134e-05]
		[batch 20/20] avg loss: 0.1301483983070889		[learning rate: 1.1114e-05]
	Learning Rate: 1.1114e-05
	LOSS [training: 0.12356421987489766 | validation: 0.10627038909825193]
	TIME [epoch: 8.85 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12655882357123638		[learning rate: 1.1094e-05]
		[batch 20/20] avg loss: 0.12352437465648616		[learning rate: 1.1074e-05]
	Learning Rate: 1.10736e-05
	LOSS [training: 0.12504159911386126 | validation: 0.10360819851730518]
	TIME [epoch: 8.85 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13431611111832803		[learning rate: 1.1054e-05]
		[batch 20/20] avg loss: 0.11742011521382896		[learning rate: 1.1033e-05]
	Learning Rate: 1.10334e-05
	LOSS [training: 0.12586811316607846 | validation: 0.10804409187366272]
	TIME [epoch: 8.85 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11816878275688321		[learning rate: 1.1013e-05]
		[batch 20/20] avg loss: 0.13458082253746512		[learning rate: 1.0993e-05]
	Learning Rate: 1.09934e-05
	LOSS [training: 0.12637480264717413 | validation: 0.09663382100390636]
	TIME [epoch: 8.83 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1186871245419312		[learning rate: 1.0973e-05]
		[batch 20/20] avg loss: 0.12873888469692157		[learning rate: 1.0953e-05]
	Learning Rate: 1.09535e-05
	LOSS [training: 0.1237130046194264 | validation: 0.10109446975458868]
	TIME [epoch: 8.84 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12234536520595451		[learning rate: 1.0934e-05]
		[batch 20/20] avg loss: 0.1268292706871101		[learning rate: 1.0914e-05]
	Learning Rate: 1.09137e-05
	LOSS [training: 0.12458731794653233 | validation: 0.09978144635455966]
	TIME [epoch: 8.85 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11822084523844736		[learning rate: 1.0894e-05]
		[batch 20/20] avg loss: 0.12204630348759456		[learning rate: 1.0874e-05]
	Learning Rate: 1.08741e-05
	LOSS [training: 0.12013357436302097 | validation: 0.1072754826142763]
	TIME [epoch: 8.84 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11739449766280767		[learning rate: 1.0854e-05]
		[batch 20/20] avg loss: 0.127459408449264		[learning rate: 1.0835e-05]
	Learning Rate: 1.08347e-05
	LOSS [training: 0.12242695305603583 | validation: 0.11138175359958248]
	TIME [epoch: 8.84 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11464555772798275		[learning rate: 1.0815e-05]
		[batch 20/20] avg loss: 0.13642936650978016		[learning rate: 1.0795e-05]
	Learning Rate: 1.07954e-05
	LOSS [training: 0.12553746211888145 | validation: 0.11423975431696798]
	TIME [epoch: 8.84 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12535581978111154		[learning rate: 1.0776e-05]
		[batch 20/20] avg loss: 0.11743196005408445		[learning rate: 1.0756e-05]
	Learning Rate: 1.07562e-05
	LOSS [training: 0.12139388991759803 | validation: 0.09852379340683294]
	TIME [epoch: 8.84 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12370762520981995		[learning rate: 1.0737e-05]
		[batch 20/20] avg loss: 0.12170424894015655		[learning rate: 1.0717e-05]
	Learning Rate: 1.07171e-05
	LOSS [training: 0.12270593707498822 | validation: 0.10279504754757506]
	TIME [epoch: 8.86 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12927758316194546		[learning rate: 1.0698e-05]
		[batch 20/20] avg loss: 0.11393753240791382		[learning rate: 1.0678e-05]
	Learning Rate: 1.06782e-05
	LOSS [training: 0.12160755778492964 | validation: 0.10659941312038045]
	TIME [epoch: 8.84 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13176905155132215		[learning rate: 1.0659e-05]
		[batch 20/20] avg loss: 0.11670823473233176		[learning rate: 1.0639e-05]
	Learning Rate: 1.06395e-05
	LOSS [training: 0.12423864314182695 | validation: 0.09758948532306527]
	TIME [epoch: 8.83 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12150933284164463		[learning rate: 1.062e-05]
		[batch 20/20] avg loss: 0.11915223849712878		[learning rate: 1.0601e-05]
	Learning Rate: 1.06009e-05
	LOSS [training: 0.12033078566938668 | validation: 0.10812065228750325]
	TIME [epoch: 8.82 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11714271547245925		[learning rate: 1.0582e-05]
		[batch 20/20] avg loss: 0.1280717950634374		[learning rate: 1.0562e-05]
	Learning Rate: 1.05624e-05
	LOSS [training: 0.12260725526794831 | validation: 0.10714855552262756]
	TIME [epoch: 8.86 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10679115690986503		[learning rate: 1.0543e-05]
		[batch 20/20] avg loss: 0.1327869019596628		[learning rate: 1.0524e-05]
	Learning Rate: 1.05241e-05
	LOSS [training: 0.11978902943476388 | validation: 0.10591368809478921]
	TIME [epoch: 8.84 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12223221981816083		[learning rate: 1.0505e-05]
		[batch 20/20] avg loss: 0.11633892897003309		[learning rate: 1.0486e-05]
	Learning Rate: 1.04859e-05
	LOSS [training: 0.11928557439409695 | validation: 0.10171913296128204]
	TIME [epoch: 8.85 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12355991196439164		[learning rate: 1.0467e-05]
		[batch 20/20] avg loss: 0.12042289705927334		[learning rate: 1.0448e-05]
	Learning Rate: 1.04478e-05
	LOSS [training: 0.1219914045118325 | validation: 0.10761758151641676]
	TIME [epoch: 8.84 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11931081314709831		[learning rate: 1.0429e-05]
		[batch 20/20] avg loss: 0.12448438968475091		[learning rate: 1.041e-05]
	Learning Rate: 1.04099e-05
	LOSS [training: 0.12189760141592462 | validation: 0.09400074151810997]
	TIME [epoch: 8.84 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12838040552082028		[learning rate: 1.0391e-05]
		[batch 20/20] avg loss: 0.11824341301397405		[learning rate: 1.0372e-05]
	Learning Rate: 1.03721e-05
	LOSS [training: 0.12331190926739717 | validation: 0.09864881235068661]
	TIME [epoch: 8.86 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11993837841565047		[learning rate: 1.0353e-05]
		[batch 20/20] avg loss: 0.12622635503570795		[learning rate: 1.0335e-05]
	Learning Rate: 1.03345e-05
	LOSS [training: 0.12308236672567921 | validation: 0.09816891516134833]
	TIME [epoch: 8.84 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12338730076981164		[learning rate: 1.0316e-05]
		[batch 20/20] avg loss: 0.12429266989615986		[learning rate: 1.0297e-05]
	Learning Rate: 1.0297e-05
	LOSS [training: 0.12383998533298575 | validation: 0.10592257171454196]
	TIME [epoch: 8.84 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13573792679199453		[learning rate: 1.0278e-05]
		[batch 20/20] avg loss: 0.1052531999592307		[learning rate: 1.026e-05]
	Learning Rate: 1.02596e-05
	LOSS [training: 0.12049556337561262 | validation: 0.09820975273085343]
	TIME [epoch: 8.84 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12522267152190691		[learning rate: 1.0241e-05]
		[batch 20/20] avg loss: 0.12027806819405577		[learning rate: 1.0222e-05]
	Learning Rate: 1.02224e-05
	LOSS [training: 0.12275036985798135 | validation: 0.10582021988466005]
	TIME [epoch: 8.84 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11696736638029279		[learning rate: 1.0204e-05]
		[batch 20/20] avg loss: 0.12951562225795585		[learning rate: 1.0185e-05]
	Learning Rate: 1.01853e-05
	LOSS [training: 0.12324149431912432 | validation: 0.10856260358836176]
	TIME [epoch: 8.86 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1280195896437626		[learning rate: 1.0167e-05]
		[batch 20/20] avg loss: 0.11938728611355301		[learning rate: 1.0148e-05]
	Learning Rate: 1.01483e-05
	LOSS [training: 0.12370343787865781 | validation: 0.10998468854641644]
	TIME [epoch: 8.83 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1268342999110838		[learning rate: 1.013e-05]
		[batch 20/20] avg loss: 0.11618146324112252		[learning rate: 1.0112e-05]
	Learning Rate: 1.01115e-05
	LOSS [training: 0.12150788157610318 | validation: 0.10255536085297094]
	TIME [epoch: 8.83 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13537298678589899		[learning rate: 1.0093e-05]
		[batch 20/20] avg loss: 0.11289459430566975		[learning rate: 1.0075e-05]
	Learning Rate: 1.00748e-05
	LOSS [training: 0.12413379054578433 | validation: 0.10415709244059194]
	TIME [epoch: 8.84 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10965393313560931		[learning rate: 1.0057e-05]
		[batch 20/20] avg loss: 0.14416884088555265		[learning rate: 1.0038e-05]
	Learning Rate: 1.00382e-05
	LOSS [training: 0.12691138701058097 | validation: 0.10705209188495199]
	TIME [epoch: 8.86 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11519091188825785		[learning rate: 1.002e-05]
		[batch 20/20] avg loss: 0.13223700659117693		[learning rate: 1.0002e-05]
	Learning Rate: 1.00018e-05
	LOSS [training: 0.1237139592397174 | validation: 0.1004036711564349]
	TIME [epoch: 8.84 sec]
Finished training in 17831.171 seconds.
