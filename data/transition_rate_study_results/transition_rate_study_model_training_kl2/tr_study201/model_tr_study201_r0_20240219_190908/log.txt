Args:
Namespace(name='model_tr_study201', outdir='out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0', training_data='data/transition_rate_studies/tr_study201/tr_study201_training/r0', validation_data='data/transition_rate_studies/tr_study201/tr_study201_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3594851673

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.045774758700803		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.65643837952081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.851106569110804 | validation: 8.568699484892058]
	TIME [epoch: 52.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.4090236601028225		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.783123374387397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.596073517245111 | validation: 6.051792308335115]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.66707951640453		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.4877385007873345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.577409008595932 | validation: 5.422244981838931]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.305646625065111		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.171957718250885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.238802171657998 | validation: 4.7025947532900325]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.92590330084307		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.028196205618816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9770497532309443 | validation: 5.369541787731858]
	TIME [epoch: 8.4 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8657838425855937		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.94354249036123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.904663166473412 | validation: 4.211462343944412]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.883334633870999		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.636949776929768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7601422054003835 | validation: 4.476684240729633]
	TIME [epoch: 8.4 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.672003845680284		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8019121627684216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.736958004224353 | validation: 4.460238023207806]
	TIME [epoch: 8.39 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.558200594750479		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.48341986048602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5208102276182487 | validation: 4.300639148243951]
	TIME [epoch: 8.4 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3650147492776625		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6463261114710077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.505670430374335 | validation: 4.664324356113552]
	TIME [epoch: 8.41 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4080805509011527		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.366112375732386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3870964633167686 | validation: 4.0642724060504145]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4315478998160898		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1847386342121107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.308143267014099 | validation: 4.1124824737643975]
	TIME [epoch: 8.39 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3889439186606536		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.135572551211938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2622582349362963 | validation: 4.118204802038678]
	TIME [epoch: 8.41 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.307300241265726		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1921253703858614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2497128058257934 | validation: 3.71596132112992]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.05399321160185		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.210327180776872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.132160196189361 | validation: 4.4562465197123675]
	TIME [epoch: 8.41 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1296735732815266		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.199512378303307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1645929757924174 | validation: 3.9288315071793103]
	TIME [epoch: 8.41 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.146053925554688		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0101012492503374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.078077587402513 | validation: 4.133979438891708]
	TIME [epoch: 8.44 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0006568092716632		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2218092429867062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.111233026129184 | validation: 3.756773936624481]
	TIME [epoch: 8.42 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.127852137259847		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0211820680084513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0745171026341493 | validation: 3.5760869607878494]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8987490696641793		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.235416122621826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0670825961430026 | validation: 3.623053189625838]
	TIME [epoch: 8.41 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.189796325328283		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.89229190467552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0410441150019016 | validation: 3.885745243367319]
	TIME [epoch: 8.42 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0336196566647673		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0260073812488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.029813518956783 | validation: 3.7153574760485366]
	TIME [epoch: 8.41 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.31066812467974		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0886299427838235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1996490337317818 | validation: 3.7144333864322414]
	TIME [epoch: 8.4 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.000689830897616		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9442755064120782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.972482668654847 | validation: 3.928945506925414]
	TIME [epoch: 8.41 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9532719327995762		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3458744742989617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.149573203549269 | validation: 3.5167002797540317]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.85295656644433		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8846121008812924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8687843336628114 | validation: 3.710042184114719]
	TIME [epoch: 8.43 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8699058502563015		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.934180863173343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.902043356714823 | validation: 4.074632453137271]
	TIME [epoch: 8.41 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9773148719849383		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.893788723231123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9355517976080305 | validation: 3.8070044731685115]
	TIME [epoch: 8.41 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.013951059848721		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7623381247132888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.888144592281005 | validation: 3.5470696228198415]
	TIME [epoch: 8.41 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1439296141883735		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.748165927027632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9460477706080024 | validation: 3.331354058855083]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4931625302690867		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.4557550504863657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4744587903777258 | validation: 3.6992829777050122]
	TIME [epoch: 8.4 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2635425309917245		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.008963357082336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.13625294403703 | validation: 4.047874020196153]
	TIME [epoch: 8.4 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.881717749276623		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7670155750736174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8243666621751204 | validation: 3.199933052900299]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7180316484109204		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.841536152265909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7797839003384146 | validation: 3.449685443266259]
	TIME [epoch: 8.42 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.757445405195443		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9724497079386767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.864947556567059 | validation: 3.71968186232079]
	TIME [epoch: 8.39 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9320305260799264		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.271308507468942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1016695167744346 | validation: 3.3534508990871705]
	TIME [epoch: 8.39 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.890254673452128		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.613264444169263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.751759558810696 | validation: 3.421605898216961]
	TIME [epoch: 8.39 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6461718715302154		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9558730119123227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.801022441721269 | validation: 3.2196073472886737]
	TIME [epoch: 8.41 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.900266110117603		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6565407688168796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.778403439467241 | validation: 3.4761793127471616]
	TIME [epoch: 8.39 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.694254838786731		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.635851311463697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.665053075125214 | validation: 3.1234194947051668]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6451759779996165		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.807348832702874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.726262405351245 | validation: 3.7007043737460537]
	TIME [epoch: 8.39 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6694383511000983		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.839610960945364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.754524656022731 | validation: 3.1608682662784306]
	TIME [epoch: 8.42 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5937274837608704		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.664818963144001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.629273223452436 | validation: 3.906245980846248]
	TIME [epoch: 8.39 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.449717216692268		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.040027069320877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7448721430065732 | validation: 3.168822689224047]
	TIME [epoch: 8.39 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5748324549666135		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5037649556126285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.539298705289621 | validation: 3.080876549623862]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.459080683700842		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7453836535941276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.602232168647485 | validation: 3.8522070906200008]
	TIME [epoch: 8.42 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.586536772175417		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5296214797936867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.558079125984552 | validation: 3.981796637166558]
	TIME [epoch: 8.4 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.511035516578006		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.630425362685778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5707304396318924 | validation: 3.3663965733995025]
	TIME [epoch: 8.39 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6869949662555164		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6783427502424546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.682668858248985 | validation: 3.2586542493032384]
	TIME [epoch: 8.39 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5412925026062667		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.66632281684884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6038076597275532 | validation: 3.4330816890343616]
	TIME [epoch: 8.41 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4929380682567706		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.625845842013236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.559391955135003 | validation: 3.435423499771198]
	TIME [epoch: 8.4 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8207191675656182		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1811528524371098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0009360100013645 | validation: 3.8546179115319825]
	TIME [epoch: 8.4 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9151413861234605		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.610739989019031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7629406875712457 | validation: 3.357653212938616]
	TIME [epoch: 8.39 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.615840920473306		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.751620986078624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6837309532759654 | validation: 4.004539853743825]
	TIME [epoch: 8.42 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.523061592767024		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.50239461174191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5127281022544667 | validation: 2.951559589798512]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4454319348630595		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9693459439942425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.70738893942865 | validation: 3.284449444361141]
	TIME [epoch: 8.39 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.514986307680312		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5353394608314748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5251628842558937 | validation: 3.743611827816273]
	TIME [epoch: 8.38 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3801064625686155		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7296316180164846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.554869040292549 | validation: 3.232921326213186]
	TIME [epoch: 8.41 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.585818770829516		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5001446793000985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.542981725064807 | validation: 3.0517099181994705]
	TIME [epoch: 8.39 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3838139937210747		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6029008161752984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4933574049481857 | validation: 3.4434032184305026]
	TIME [epoch: 8.39 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4315658764051973		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4641540894613985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4478599829332977 | validation: 3.2305371934641918]
	TIME [epoch: 8.38 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.339553824116245		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.459815476548477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.399684650332362 | validation: 4.234800826544221]
	TIME [epoch: 8.41 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7567017472024555		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.380568008515421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.568634877858939 | validation: 3.465424958939851]
	TIME [epoch: 8.39 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.417163280872975		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.377197329986441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3971803054297083 | validation: 3.327538388115259]
	TIME [epoch: 8.39 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.347444190190242		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.440348469108383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.393896329649313 | validation: 3.3140867671428795]
	TIME [epoch: 8.38 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.390080765375191		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6140597172214317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5020702412983113 | validation: 3.1006247146262087]
	TIME [epoch: 8.39 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4285140881856146		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4666368304952018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.447575459340408 | validation: 3.328564499630025]
	TIME [epoch: 8.42 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.436773886433458		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.689223673590381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5629987800119185 | validation: 3.5578384332193593]
	TIME [epoch: 8.39 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5649992577508955		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.423098923540025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4940490906454604 | validation: 3.2334605068819924]
	TIME [epoch: 8.38 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3389210366043884		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.608526791211653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.473723913908021 | validation: 3.055803348565525]
	TIME [epoch: 8.39 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5773824112285615		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2187969190584895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3980896651435253 | validation: 3.303459864255844]
	TIME [epoch: 8.42 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4182989428328177		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3027546647081345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.360526803770477 | validation: 3.097229649344621]
	TIME [epoch: 8.39 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.664316781828071		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3994884529845413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.531902617406306 | validation: 3.2095097973045097]
	TIME [epoch: 8.39 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7124841234513513		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4213942686816496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5669391960665005 | validation: 3.2393817253795483]
	TIME [epoch: 8.39 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4495078992126933		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9613592895594585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.705433594386075 | validation: 3.041114554543662]
	TIME [epoch: 8.41 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.277948324426638		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.364149971787911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.321049148107274 | validation: 2.969720420116882]
	TIME [epoch: 8.4 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.178836105989054		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4457812162043084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3123086610966817 | validation: 2.9842616169940546]
	TIME [epoch: 8.39 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.426252031408469		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2260756024947073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.326163816951588 | validation: 3.7327254350140815]
	TIME [epoch: 8.39 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.28888277884504		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.863685120596317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.576283949720679 | validation: 4.256520754679124]
	TIME [epoch: 8.39 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6791038020660607		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8301968558798256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.754650328972943 | validation: 2.9193307962359825]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0962914604698235		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.573394356784103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.334842908626963 | validation: 2.873820748795338]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3533734847712857		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.295600858145274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3244871714582795 | validation: 3.1170896336023888]
	TIME [epoch: 8.39 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.284271052566504		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2215320034202013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2529015279933526 | validation: 2.8084651773289675]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9269865492408726		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6025803168542305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2647834330475516 | validation: 3.150294871471775]
	TIME [epoch: 8.44 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.469666005357714		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4207639739487288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.445214989653221 | validation: 3.373006309506814]
	TIME [epoch: 8.41 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4259863748734043		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.361158396091181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.393572385482293 | validation: 3.0765610306132327]
	TIME [epoch: 8.4 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3861909796663143		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3268071833295196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3564990814979168 | validation: 3.2704864089368892]
	TIME [epoch: 8.4 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.137604670120427		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0015756902544317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5695901801874292 | validation: 3.019612273478163]
	TIME [epoch: 8.43 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7109005696839814		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7703939229243866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7406472463041838 | validation: 3.2950193209073935]
	TIME [epoch: 8.41 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7029564507209933		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.462354780230428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5826556154757108 | validation: 3.1136404390767565]
	TIME [epoch: 8.41 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3992738467167065		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4441441029385005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4217089748276037 | validation: 3.356099018759169]
	TIME [epoch: 8.41 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.187336319741014		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.36986197292335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.278599146332182 | validation: 3.1471036135312613]
	TIME [epoch: 8.43 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.303295597841126		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.467237789393308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3852666936172175 | validation: 2.981210370874031]
	TIME [epoch: 8.41 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1282209980510536		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3354636344328537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.231842316241954 | validation: 3.08365561410917]
	TIME [epoch: 8.4 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.26631008771497		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.468724347165487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.367517217440228 | validation: 2.9091370969891543]
	TIME [epoch: 8.41 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4648766535951294		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3814285072607446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.423152580427937 | validation: 3.0557270491636634]
	TIME [epoch: 8.41 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.38533807816565		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6649969845731176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5251675313693838 | validation: 2.959728762341915]
	TIME [epoch: 8.43 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.964217091942715		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4030757652559993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1836464285993578 | validation: 2.870830670841019]
	TIME [epoch: 8.41 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.221826655056331		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2446389158264592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.233232785441395 | validation: 2.7698957640548287]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.258780905252054		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.21540896988389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.237094937567972 | validation: 3.266357312046218]
	TIME [epoch: 8.41 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.391653944425997		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 2.155475128244695		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 2.273564536335346 | validation: 2.9631356300196026]
	TIME [epoch: 8.43 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1767389516480136		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 2.208696539047221		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 2.1927177453476174 | validation: 2.794714168792392]
	TIME [epoch: 8.41 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.202236966769102		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 2.161644090325064		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 2.181940528547083 | validation: 3.011295415982987]
	TIME [epoch: 8.41 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2905478737266147		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 2.4612299808121385		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 2.375888927269377 | validation: 2.8682931606516004]
	TIME [epoch: 8.41 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.183354446808915		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 2.4071557502510776		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 2.2952550985299967 | validation: 3.307341556855996]
	TIME [epoch: 8.43 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2898954145506947		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 2.431957025679937		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 2.360926220115316 | validation: 2.9078578987021806]
	TIME [epoch: 8.41 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4176270991904665		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 2.0689825157884076		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 2.243304807489437 | validation: 2.9491957010248004]
	TIME [epoch: 8.4 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3871292114194906		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 2.2869648550898214		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 2.3370470332546556 | validation: 3.4503051518320063]
	TIME [epoch: 8.41 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.140973953691641		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 2.788330918036076		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 2.4646524358638584 | validation: 3.1668501382155463]
	TIME [epoch: 8.41 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7833855125792697		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 2.1965737384122495		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 2.48997962549576 | validation: 3.8263651043736666]
	TIME [epoch: 8.43 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4459634128254235		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 2.3856665035231743		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 2.4158149581742987 | validation: 3.655951099157363]
	TIME [epoch: 8.4 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2755210625147746		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 2.1794297796936073		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 2.2274754211041907 | validation: 4.086964451334643]
	TIME [epoch: 8.4 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3895848611218384		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 2.30510260526476		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 2.347343733193299 | validation: 3.185139463561677]
	TIME [epoch: 8.41 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.302554192978083		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 2.374447163426931		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 2.3385006782025073 | validation: 4.138557881769495]
	TIME [epoch: 8.43 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.560128521958469		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 2.375608307944924		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 2.4678684149516963 | validation: 2.907642687344006]
	TIME [epoch: 8.41 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2002443265659104		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 2.3503088134547503		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 2.2752765700103303 | validation: 2.939159503046432]
	TIME [epoch: 8.4 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3220917701829604		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 2.318134931718236		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 2.3201133509505985 | validation: 2.812737387016125]
	TIME [epoch: 8.41 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5045524729352016		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 2.238412538538101		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 2.3714825057366515 | validation: 3.3538957838169416]
	TIME [epoch: 8.43 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2520305188606153		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 2.214123514101827		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 2.2330770164812215 | validation: 2.8446041801730515]
	TIME [epoch: 8.41 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3936414090715523		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 2.339490756938747		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 2.366566083005149 | validation: 2.8312222285229587]
	TIME [epoch: 8.4 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.229310114318031		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 2.748856403601193		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 2.4890832589596124 | validation: 3.0012075476504796]
	TIME [epoch: 8.4 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.345228663088078		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 2.3276890575756877		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 2.3364588603318825 | validation: 2.822889010852661]
	TIME [epoch: 8.42 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1765658587634094		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 2.1652336470331868		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 2.170899752898298 | validation: 3.125996313758228]
	TIME [epoch: 8.41 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5049354362505794		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 2.3323598350899504		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 2.418647635670265 | validation: 3.644837818554522]
	TIME [epoch: 8.41 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5245277184664063		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 2.0902223240714517		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 2.3073750212689284 | validation: 2.8775754250232026]
	TIME [epoch: 8.4 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1521237850961414		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 2.30797265341206		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 2.230048219254101 | validation: 2.8247352514839683]
	TIME [epoch: 8.41 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.46483992197869		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 2.070815049715005		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 2.2678274858468477 | validation: 3.2042317841112706]
	TIME [epoch: 8.43 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1162554928141204		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 2.297848970770685		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 2.207052231792403 | validation: 2.817604497671063]
	TIME [epoch: 8.41 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.321036230794186		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 1.9638376787512049		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 2.142436954772695 | validation: 2.9128840278958963]
	TIME [epoch: 8.4 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.171640805631653		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 2.4133989818877		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 2.2925198937596765 | validation: 2.949987866827347]
	TIME [epoch: 8.4 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0761206637233864		[learning rate: 0.008952]
		[batch 20/20] avg loss: 2.187350290725542		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 2.1317354772244643 | validation: 3.0522370412254416]
	TIME [epoch: 8.42 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.144396150178601		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 2.1875696413773524		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 2.1659828957779763 | validation: 3.18076774079225]
	TIME [epoch: 8.4 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3475942424351963		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 2.392446938098997		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 2.3700205902670968 | validation: 2.8422496475647407]
	TIME [epoch: 8.4 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2181393324703453		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 2.1344921732091993		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 2.176315752839772 | validation: 3.0384105858148884]
	TIME [epoch: 8.4 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.193154849496106		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 2.0414869298907714		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 2.117320889693439 | validation: 2.8059259143660062]
	TIME [epoch: 8.42 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4091650244991873		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 2.125781418108113		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 2.267473221303651 | validation: 3.343875517275118]
	TIME [epoch: 8.4 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4519599906896716		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 2.457525171301352		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 2.454742580995512 | validation: 3.160399114694231]
	TIME [epoch: 8.4 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1801138276323355		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 2.3842170867122756		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 2.2821654571723053 | validation: 3.0669703921456373]
	TIME [epoch: 8.4 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.227108382526053		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 2.2900679375804023		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 2.258588160053227 | validation: 3.2024962288166408]
	TIME [epoch: 8.4 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.111784384525838		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 2.1431604729849805		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 2.12747242875541 | validation: 2.7458193942580547]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.182970326183349		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 2.077250679463703		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 2.130110502823526 | validation: 3.0667527214709946]
	TIME [epoch: 8.39 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.011689535696685		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 2.5198323948431205		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 2.2657609652699024 | validation: 3.0122138658254936]
	TIME [epoch: 8.39 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.129575183126413		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 2.1135860413430696		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 2.121580612234741 | validation: 2.8302345820105996]
	TIME [epoch: 8.39 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0982574220613666		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 2.329647967867015		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 2.213952694964191 | validation: 2.8565929651002437]
	TIME [epoch: 8.41 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.131976110432887		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 2.2246969450693794		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 2.178336527751133 | validation: 2.902243768371848]
	TIME [epoch: 8.39 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.005564449927066		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 2.4880182640778274		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 2.2467913570024463 | validation: 3.254593952064965]
	TIME [epoch: 8.39 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1187191811469583		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 2.227696566005402		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 2.1732078735761804 | validation: 3.0935596752278935]
	TIME [epoch: 8.38 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.001382289412551		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 2.2158338032859017		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 2.1086080463492265 | validation: 2.8792360055397563]
	TIME [epoch: 8.4 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1538997720054995		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 2.200634721227813		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 2.1772672466166556 | validation: 2.750860289060328]
	TIME [epoch: 8.39 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3091748323398935		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 2.0062475460490368		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 2.1577111891944654 | validation: 2.7715955857820136]
	TIME [epoch: 8.38 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.193950319042482		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 2.232707254998301		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 2.2133287870203917 | validation: 3.0726169978902633]
	TIME [epoch: 8.39 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9949274087065512		[learning rate: 0.008294]
		[batch 20/20] avg loss: 2.350916407021145		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 2.1729219078638478 | validation: 2.706959252806461]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.019766405231962		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 2.247704148319409		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 2.133735276775685 | validation: 2.823047471099989]
	TIME [epoch: 8.41 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.162010961748714		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 1.9502039743717865		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 2.0561074680602505 | validation: 2.8972332850096563]
	TIME [epoch: 8.38 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.053519335972699		[learning rate: 0.008204]
		[batch 20/20] avg loss: 2.3670293073235373		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 2.2102743216481175 | validation: 2.8124360135930067]
	TIME [epoch: 8.38 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1686432989975524		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 1.9414333988593504		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 2.0550383489284516 | validation: 2.733457521473936]
	TIME [epoch: 8.39 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.218145939779485		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 1.9966151568468793		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 2.107380548313182 | validation: 2.8821746050748964]
	TIME [epoch: 8.4 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.331043621854813		[learning rate: 0.008115]
		[batch 20/20] avg loss: 2.2419858047701924		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 2.286514713312503 | validation: 2.7641265729243933]
	TIME [epoch: 8.39 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.191753437794321		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 1.8859395216200743		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 2.0388464797071983 | validation: 2.752389885107267]
	TIME [epoch: 8.38 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1665085197053804		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 2.007667749732783		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 2.0870881347190813 | validation: 2.8028361948566585]
	TIME [epoch: 8.39 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.967195260761122		[learning rate: 0.008027]
		[batch 20/20] avg loss: 2.0192886739361366		[learning rate: 0.0080124]
	Learning Rate: 0.00801241
	LOSS [training: 1.9932419673486297 | validation: 2.772083265601608]
	TIME [epoch: 8.41 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1184347625260336		[learning rate: 0.0079979]
		[batch 20/20] avg loss: 2.354927180578897		[learning rate: 0.0079833]
	Learning Rate: 0.00798333
	LOSS [training: 2.236680971552465 | validation: 2.8129340765949893]
	TIME [epoch: 8.38 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2717077147702636		[learning rate: 0.0079688]
		[batch 20/20] avg loss: 2.03942177093628		[learning rate: 0.0079544]
	Learning Rate: 0.00795436
	LOSS [training: 2.155564742853272 | validation: 3.217616556779965]
	TIME [epoch: 8.39 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2014064971022678		[learning rate: 0.0079399]
		[batch 20/20] avg loss: 2.126902943383515		[learning rate: 0.0079255]
	Learning Rate: 0.00792549
	LOSS [training: 2.1641547202428915 | validation: 2.9930717645475333]
	TIME [epoch: 8.39 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.290039584071377		[learning rate: 0.0079111]
		[batch 20/20] avg loss: 1.9114211598733732		[learning rate: 0.0078967]
	Learning Rate: 0.00789673
	LOSS [training: 2.100730371972375 | validation: 2.9570090564221276]
	TIME [epoch: 8.39 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1321650587078738		[learning rate: 0.0078824]
		[batch 20/20] avg loss: 2.210479383334814		[learning rate: 0.0078681]
	Learning Rate: 0.00786807
	LOSS [training: 2.1713222210213443 | validation: 2.7662882916748814]
	TIME [epoch: 8.41 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.923057268318225		[learning rate: 0.0078538]
		[batch 20/20] avg loss: 2.017334153537642		[learning rate: 0.0078395]
	Learning Rate: 0.00783952
	LOSS [training: 1.970195710927933 | validation: 2.7473530863367084]
	TIME [epoch: 8.38 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.020553509832687		[learning rate: 0.0078253]
		[batch 20/20] avg loss: 2.126369901341982		[learning rate: 0.0078111]
	Learning Rate: 0.00781107
	LOSS [training: 2.073461705587334 | validation: 2.726849132502112]
	TIME [epoch: 8.38 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0543336379573685		[learning rate: 0.0077969]
		[batch 20/20] avg loss: 2.146449539128805		[learning rate: 0.0077827]
	Learning Rate: 0.00778272
	LOSS [training: 2.100391588543087 | validation: 3.06075041161077]
	TIME [epoch: 8.39 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0998093631533594		[learning rate: 0.0077686]
		[batch 20/20] avg loss: 2.0368353895037563		[learning rate: 0.0077545]
	Learning Rate: 0.00775448
	LOSS [training: 2.0683223763285583 | validation: 2.985112197304415]
	TIME [epoch: 8.4 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3038651082009514		[learning rate: 0.0077404]
		[batch 20/20] avg loss: 1.9060331215312847		[learning rate: 0.0077263]
	Learning Rate: 0.00772634
	LOSS [training: 2.1049491148661175 | validation: 2.6958930507828156]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.003525303731652		[learning rate: 0.0077123]
		[batch 20/20] avg loss: 1.8479462888444604		[learning rate: 0.0076983]
	Learning Rate: 0.0076983
	LOSS [training: 1.9257357962880561 | validation: 2.7152587174331804]
	TIME [epoch: 8.38 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.881052854294624		[learning rate: 0.0076843]
		[batch 20/20] avg loss: 2.01650440876792		[learning rate: 0.0076704]
	Learning Rate: 0.00767036
	LOSS [training: 1.9487786315312718 | validation: 2.7243936226368426]
	TIME [epoch: 8.38 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9928201776191106		[learning rate: 0.0076564]
		[batch 20/20] avg loss: 2.106490781251118		[learning rate: 0.0076425]
	Learning Rate: 0.00764252
	LOSS [training: 2.0496554794351143 | validation: 2.823058172958242]
	TIME [epoch: 8.41 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.116847733080965		[learning rate: 0.0076286]
		[batch 20/20] avg loss: 2.047774970211556		[learning rate: 0.0076148]
	Learning Rate: 0.00761479
	LOSS [training: 2.0823113516462604 | validation: 2.816837047944973]
	TIME [epoch: 8.38 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0189297418888		[learning rate: 0.007601]
		[batch 20/20] avg loss: 2.0000707645991476		[learning rate: 0.0075872]
	Learning Rate: 0.00758715
	LOSS [training: 2.0095002532439743 | validation: 3.124853835774481]
	TIME [epoch: 8.39 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.02176511536875		[learning rate: 0.0075734]
		[batch 20/20] avg loss: 1.978637840648106		[learning rate: 0.0075596]
	Learning Rate: 0.00755962
	LOSS [training: 2.000201478008427 | validation: 2.73076548152624]
	TIME [epoch: 8.38 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0113307199172974		[learning rate: 0.0075459]
		[batch 20/20] avg loss: 2.1421868712174352		[learning rate: 0.0075322]
	Learning Rate: 0.00753219
	LOSS [training: 2.0767587955673665 | validation: 2.6727434949230893]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.13285085409769		[learning rate: 0.0075185]
		[batch 20/20] avg loss: 2.036757239939473		[learning rate: 0.0075049]
	Learning Rate: 0.00750485
	LOSS [training: 2.0848040470185807 | validation: 3.1321956159829254]
	TIME [epoch: 8.39 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7909087481178223		[learning rate: 0.0074912]
		[batch 20/20] avg loss: 2.0599612365105116		[learning rate: 0.0074776]
	Learning Rate: 0.00747762
	LOSS [training: 2.4254349923141674 | validation: 2.871930985962739]
	TIME [epoch: 8.38 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2647499703593788		[learning rate: 0.007464]
		[batch 20/20] avg loss: 1.8898148601069338		[learning rate: 0.0074505]
	Learning Rate: 0.00745048
	LOSS [training: 2.077282415233156 | validation: 2.7432501187315252]
	TIME [epoch: 8.39 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.018963557050097		[learning rate: 0.0074369]
		[batch 20/20] avg loss: 2.001012912158269		[learning rate: 0.0074234]
	Learning Rate: 0.00742344
	LOSS [training: 2.009988234604183 | validation: 2.7170024878457593]
	TIME [epoch: 8.39 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0793122424569375		[learning rate: 0.00741]
		[batch 20/20] avg loss: 1.9218985454874116		[learning rate: 0.0073965]
	Learning Rate: 0.0073965
	LOSS [training: 2.000605393972175 | validation: 2.6816364141681754]
	TIME [epoch: 8.42 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.935902357242234		[learning rate: 0.0073831]
		[batch 20/20] avg loss: 2.1871721143799965		[learning rate: 0.0073697]
	Learning Rate: 0.00736966
	LOSS [training: 2.0615372358111155 | validation: 2.883860846229601]
	TIME [epoch: 8.38 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9260842354147374		[learning rate: 0.0073563]
		[batch 20/20] avg loss: 2.0855903946179755		[learning rate: 0.0073429]
	Learning Rate: 0.00734291
	LOSS [training: 2.0058373150163566 | validation: 2.833682601779033]
	TIME [epoch: 8.4 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.004378813023977		[learning rate: 0.0073296]
		[batch 20/20] avg loss: 2.1304863567587247		[learning rate: 0.0073163]
	Learning Rate: 0.00731627
	LOSS [training: 2.0674325848913506 | validation: 2.93828499342588]
	TIME [epoch: 8.38 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1797716613291107		[learning rate: 0.007303]
		[batch 20/20] avg loss: 1.9819099269621816		[learning rate: 0.0072897]
	Learning Rate: 0.00728971
	LOSS [training: 2.0808407941456464 | validation: 2.719251277480212]
	TIME [epoch: 8.41 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.038283317918685		[learning rate: 0.0072765]
		[batch 20/20] avg loss: 1.9087797940203288		[learning rate: 0.0072633]
	Learning Rate: 0.00726326
	LOSS [training: 1.973531555969507 | validation: 2.737778373356423]
	TIME [epoch: 8.39 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0375681947923034		[learning rate: 0.0072501]
		[batch 20/20] avg loss: 1.964121613757855		[learning rate: 0.0072369]
	Learning Rate: 0.0072369
	LOSS [training: 2.0008449042750795 | validation: 2.659443775980348]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1615788187450646		[learning rate: 0.0072238]
		[batch 20/20] avg loss: 1.7915487095203524		[learning rate: 0.0072106]
	Learning Rate: 0.00721064
	LOSS [training: 1.976563764132709 | validation: 2.742157620725174]
	TIME [epoch: 8.42 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.115780026133274		[learning rate: 0.0071975]
		[batch 20/20] avg loss: 2.045098575989606		[learning rate: 0.0071845]
	Learning Rate: 0.00718447
	LOSS [training: 2.0804393010614404 | validation: 2.731567275073302]
	TIME [epoch: 8.44 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8877804103812685		[learning rate: 0.0071714]
		[batch 20/20] avg loss: 2.1196343893038874		[learning rate: 0.0071584]
	Learning Rate: 0.0071584
	LOSS [training: 2.003707399842578 | validation: 2.707554193776076]
	TIME [epoch: 8.41 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0597909154366665		[learning rate: 0.0071454]
		[batch 20/20] avg loss: 1.8787800939655035		[learning rate: 0.0071324]
	Learning Rate: 0.00713242
	LOSS [training: 1.9692855047010849 | validation: 2.6929375853383757]
	TIME [epoch: 8.41 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8747578779281349		[learning rate: 0.0071195]
		[batch 20/20] avg loss: 2.12537806009653		[learning rate: 0.0071065]
	Learning Rate: 0.00710653
	LOSS [training: 2.000067969012332 | validation: 2.637896885676843]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1277912399005205		[learning rate: 0.0070936]
		[batch 20/20] avg loss: 2.069945667656603		[learning rate: 0.0070807]
	Learning Rate: 0.00708074
	LOSS [training: 2.098868453778562 | validation: 2.7796430709159625]
	TIME [epoch: 8.41 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9049036411512972		[learning rate: 0.0070679]
		[batch 20/20] avg loss: 2.052291261275193		[learning rate: 0.007055]
	Learning Rate: 0.00705505
	LOSS [training: 1.9785974512132447 | validation: 2.657088259773958]
	TIME [epoch: 8.43 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.039709415702185		[learning rate: 0.0070422]
		[batch 20/20] avg loss: 2.009660262071711		[learning rate: 0.0070294]
	Learning Rate: 0.00702945
	LOSS [training: 2.0246848388869476 | validation: 2.7111729960411157]
	TIME [epoch: 8.4 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9791642032628154		[learning rate: 0.0070167]
		[batch 20/20] avg loss: 1.9074375808694797		[learning rate: 0.0070039]
	Learning Rate: 0.00700394
	LOSS [training: 1.9433008920661474 | validation: 3.045450307877843]
	TIME [epoch: 8.4 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0719875711337643		[learning rate: 0.0069912]
		[batch 20/20] avg loss: 1.87533471761461		[learning rate: 0.0069785]
	Learning Rate: 0.00697852
	LOSS [training: 1.9736611443741867 | validation: 3.045549470057515]
	TIME [epoch: 8.41 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.016436234282012		[learning rate: 0.0069658]
		[batch 20/20] avg loss: 1.9929237722592723		[learning rate: 0.0069532]
	Learning Rate: 0.00695319
	LOSS [training: 2.004680003270642 | validation: 2.678993642402595]
	TIME [epoch: 8.43 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8934925186691163		[learning rate: 0.0069406]
		[batch 20/20] avg loss: 1.9319072079834263		[learning rate: 0.006928]
	Learning Rate: 0.00692796
	LOSS [training: 1.9126998633262715 | validation: 2.6483580098258956]
	TIME [epoch: 8.42 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8356937973519476		[learning rate: 0.0069154]
		[batch 20/20] avg loss: 2.021439634799553		[learning rate: 0.0069028]
	Learning Rate: 0.00690282
	LOSS [training: 1.9285667160757505 | validation: 2.705432705851636]
	TIME [epoch: 8.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9064289508062355		[learning rate: 0.0068903]
		[batch 20/20] avg loss: 1.9390403732462786		[learning rate: 0.0068778]
	Learning Rate: 0.00687777
	LOSS [training: 1.9227346620262573 | validation: 2.6793055782571207]
	TIME [epoch: 8.43 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.029742530291203		[learning rate: 0.0068653]
		[batch 20/20] avg loss: 1.8799855025469714		[learning rate: 0.0068528]
	Learning Rate: 0.00685281
	LOSS [training: 1.9548640164190871 | validation: 2.7668262296684505]
	TIME [epoch: 8.45 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8024195559220815		[learning rate: 0.0068404]
		[batch 20/20] avg loss: 2.1071369890639753		[learning rate: 0.0068279]
	Learning Rate: 0.00682794
	LOSS [training: 1.9547782724930287 | validation: 2.7360034212382027]
	TIME [epoch: 8.41 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9885944930827175		[learning rate: 0.0068155]
		[batch 20/20] avg loss: 2.1190642534439412		[learning rate: 0.0068032]
	Learning Rate: 0.00680316
	LOSS [training: 2.053829373263329 | validation: 2.7018641461419732]
	TIME [epoch: 8.41 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.097027403789636		[learning rate: 0.0067908]
		[batch 20/20] avg loss: 1.8637667658529153		[learning rate: 0.0067785]
	Learning Rate: 0.00677847
	LOSS [training: 1.9803970848212757 | validation: 3.2086393422271318]
	TIME [epoch: 8.4 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0317289311163567		[learning rate: 0.0067662]
		[batch 20/20] avg loss: 1.9571288997606917		[learning rate: 0.0067539]
	Learning Rate: 0.00675387
	LOSS [training: 1.9944289154385242 | validation: 2.7901816627762823]
	TIME [epoch: 8.42 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.981129421040653		[learning rate: 0.0067416]
		[batch 20/20] avg loss: 2.0531113145572926		[learning rate: 0.0067294]
	Learning Rate: 0.00672936
	LOSS [training: 2.0171203677989724 | validation: 2.672584912889567]
	TIME [epoch: 8.43 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8885321063293237		[learning rate: 0.0067171]
		[batch 20/20] avg loss: 2.047777110666948		[learning rate: 0.0067049]
	Learning Rate: 0.00670494
	LOSS [training: 1.9681546084981356 | validation: 2.6616803414553427]
	TIME [epoch: 8.41 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9834978432700119		[learning rate: 0.0066928]
		[batch 20/20] avg loss: 1.8814178847575498		[learning rate: 0.0066806]
	Learning Rate: 0.0066806
	LOSS [training: 1.9324578640137804 | validation: 2.634118892227008]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8896532050323234		[learning rate: 0.0066685]
		[batch 20/20] avg loss: 2.103701185846394		[learning rate: 0.0066564]
	Learning Rate: 0.00665636
	LOSS [training: 1.9966771954393585 | validation: 2.9281805425525445]
	TIME [epoch: 8.39 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7845644812801713		[learning rate: 0.0066443]
		[batch 20/20] avg loss: 2.174442244573542		[learning rate: 0.0066322]
	Learning Rate: 0.0066322
	LOSS [training: 1.9795033629268566 | validation: 2.9447111600844917]
	TIME [epoch: 8.42 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.078176065526176		[learning rate: 0.0066202]
		[batch 20/20] avg loss: 1.9718186104189157		[learning rate: 0.0066081]
	Learning Rate: 0.00660814
	LOSS [training: 2.024997337972546 | validation: 2.6914951556964835]
	TIME [epoch: 8.39 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.838538328163827		[learning rate: 0.0065961]
		[batch 20/20] avg loss: 1.856687741577884		[learning rate: 0.0065842]
	Learning Rate: 0.00658415
	LOSS [training: 1.8476130348708555 | validation: 2.6186790115439353]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9361895937506906		[learning rate: 0.0065722]
		[batch 20/20] avg loss: 1.9715288036232779		[learning rate: 0.0065603]
	Learning Rate: 0.00656026
	LOSS [training: 1.9538591986869835 | validation: 3.0059594962256915]
	TIME [epoch: 8.42 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9163996674312949		[learning rate: 0.0065483]
		[batch 20/20] avg loss: 2.1404971036938196		[learning rate: 0.0065365]
	Learning Rate: 0.00653645
	LOSS [training: 2.0284483855625575 | validation: 2.995180711642389]
	TIME [epoch: 8.44 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.030058980533279		[learning rate: 0.0065246]
		[batch 20/20] avg loss: 1.9601960568371564		[learning rate: 0.0065127]
	Learning Rate: 0.00651273
	LOSS [training: 1.995127518685218 | validation: 2.783510505513289]
	TIME [epoch: 8.41 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0574323757793347		[learning rate: 0.0065009]
		[batch 20/20] avg loss: 1.9295714906839367		[learning rate: 0.0064891]
	Learning Rate: 0.0064891
	LOSS [training: 1.9935019332316357 | validation: 2.6657091242950672]
	TIME [epoch: 8.41 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8181953019278345		[learning rate: 0.0064773]
		[batch 20/20] avg loss: 2.050246197427085		[learning rate: 0.0064655]
	Learning Rate: 0.00646555
	LOSS [training: 1.93422074967746 | validation: 2.6844529798622023]
	TIME [epoch: 8.41 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9639396674619423		[learning rate: 0.0064538]
		[batch 20/20] avg loss: 1.9571236187305694		[learning rate: 0.0064421]
	Learning Rate: 0.00644208
	LOSS [training: 1.9605316430962556 | validation: 2.6878643705029606]
	TIME [epoch: 8.44 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8128568351036076		[learning rate: 0.0064304]
		[batch 20/20] avg loss: 2.0296571062319337		[learning rate: 0.0064187]
	Learning Rate: 0.0064187
	LOSS [training: 1.9212569706677711 | validation: 2.670431456531973]
	TIME [epoch: 8.42 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8928153415278755		[learning rate: 0.006407]
		[batch 20/20] avg loss: 2.0578347406154114		[learning rate: 0.0063954]
	Learning Rate: 0.00639541
	LOSS [training: 1.9753250410716432 | validation: 2.6255497340561433]
	TIME [epoch: 8.41 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7274525027932905		[learning rate: 0.0063838]
		[batch 20/20] avg loss: 2.01053860209164		[learning rate: 0.0063722]
	Learning Rate: 0.0063722
	LOSS [training: 1.8689955524424655 | validation: 2.700472070499207]
	TIME [epoch: 8.41 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0079805879903354		[learning rate: 0.0063606]
		[batch 20/20] avg loss: 1.8508522295722554		[learning rate: 0.0063491]
	Learning Rate: 0.00634908
	LOSS [training: 1.9294164087812955 | validation: 2.603717748977758]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.946387649799592		[learning rate: 0.0063375]
		[batch 20/20] avg loss: 1.9742810503610286		[learning rate: 0.006326]
	Learning Rate: 0.00632603
	LOSS [training: 1.9603343500803103 | validation: 2.6377041873877354]
	TIME [epoch: 8.44 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8671112420982745		[learning rate: 0.0063145]
		[batch 20/20] avg loss: 1.877719942855839		[learning rate: 0.0063031]
	Learning Rate: 0.00630308
	LOSS [training: 1.8724155924770565 | validation: 2.684828681847653]
	TIME [epoch: 8.41 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9830493700680805		[learning rate: 0.0062916]
		[batch 20/20] avg loss: 1.8491177307827187		[learning rate: 0.0062802]
	Learning Rate: 0.0062802
	LOSS [training: 1.9160835504253995 | validation: 2.8995453008544327]
	TIME [epoch: 8.41 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.974275075674313		[learning rate: 0.0062688]
		[batch 20/20] avg loss: 1.966085340419859		[learning rate: 0.0062574]
	Learning Rate: 0.00625741
	LOSS [training: 1.9701802080470863 | validation: 2.6464996370263467]
	TIME [epoch: 8.41 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.034404842599887		[learning rate: 0.006246]
		[batch 20/20] avg loss: 1.8519069061714124		[learning rate: 0.0062347]
	Learning Rate: 0.0062347
	LOSS [training: 1.9431558743856496 | validation: 2.7485324656796015]
	TIME [epoch: 8.43 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.931749480045615		[learning rate: 0.0062234]
		[batch 20/20] avg loss: 1.9645038061124793		[learning rate: 0.0062121]
	Learning Rate: 0.00621208
	LOSS [training: 1.948126643079047 | validation: 3.095225359902148]
	TIME [epoch: 8.41 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.948255846574759		[learning rate: 0.0062008]
		[batch 20/20] avg loss: 2.1564877068979276		[learning rate: 0.0061895]
	Learning Rate: 0.00618953
	LOSS [training: 2.052371776736343 | validation: 2.936651027544614]
	TIME [epoch: 8.41 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.77761825959632		[learning rate: 0.0061783]
		[batch 20/20] avg loss: 2.1059510873031737		[learning rate: 0.0061671]
	Learning Rate: 0.00616707
	LOSS [training: 1.941784673449747 | validation: 2.8631038934512167]
	TIME [epoch: 8.41 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.04384695325725		[learning rate: 0.0061559]
		[batch 20/20] avg loss: 1.846064904354197		[learning rate: 0.0061447]
	Learning Rate: 0.00614469
	LOSS [training: 1.9449559288057237 | validation: 3.3565228791916963]
	TIME [epoch: 8.44 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.043967522372716		[learning rate: 0.0061335]
		[batch 20/20] avg loss: 1.7981283630007276		[learning rate: 0.0061224]
	Learning Rate: 0.00612239
	LOSS [training: 1.9210479426867217 | validation: 2.6823500048259135]
	TIME [epoch: 8.41 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.236936248768284		[learning rate: 0.0061113]
		[batch 20/20] avg loss: 1.7191568265142323		[learning rate: 0.0061002]
	Learning Rate: 0.00610017
	LOSS [training: 1.978046537641258 | validation: 2.5990371841793314]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9127761501163083		[learning rate: 0.0060891]
		[batch 20/20] avg loss: 2.010197653167518		[learning rate: 0.006078]
	Learning Rate: 0.00607803
	LOSS [training: 1.9614869016419128 | validation: 2.727966770358364]
	TIME [epoch: 8.41 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8242950358096077		[learning rate: 0.006067]
		[batch 20/20] avg loss: 2.059137220975073		[learning rate: 0.006056]
	Learning Rate: 0.00605598
	LOSS [training: 1.9417161283923405 | validation: 2.9969411587554804]
	TIME [epoch: 8.43 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9649555097520277		[learning rate: 0.006045]
		[batch 20/20] avg loss: 1.8638224856687113		[learning rate: 0.006034]
	Learning Rate: 0.006034
	LOSS [training: 1.9143889977103696 | validation: 2.747447379025782]
	TIME [epoch: 8.41 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0427480219137313		[learning rate: 0.006023]
		[batch 20/20] avg loss: 2.0527084312233983		[learning rate: 0.0060121]
	Learning Rate: 0.0060121
	LOSS [training: 2.0477282265685646 | validation: 2.675495690802438]
	TIME [epoch: 8.4 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7683389397365015		[learning rate: 0.0060012]
		[batch 20/20] avg loss: 2.1265941071471017		[learning rate: 0.0059903]
	Learning Rate: 0.00599028
	LOSS [training: 1.9474665234418016 | validation: 2.624443107452673]
	TIME [epoch: 8.41 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8633019084629545		[learning rate: 0.0059794]
		[batch 20/20] avg loss: 1.8101572020190044		[learning rate: 0.0059685]
	Learning Rate: 0.00596854
	LOSS [training: 1.8367295552409797 | validation: 2.6412412307412123]
	TIME [epoch: 8.41 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8808764356786412		[learning rate: 0.0059577]
		[batch 20/20] avg loss: 1.7722040995756498		[learning rate: 0.0059469]
	Learning Rate: 0.00594688
	LOSS [training: 1.8265402676271456 | validation: 2.6504908689164255]
	TIME [epoch: 8.43 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6468338425748423		[learning rate: 0.0059361]
		[batch 20/20] avg loss: 2.17122388518848		[learning rate: 0.0059253]
	Learning Rate: 0.0059253
	LOSS [training: 1.9090288638816613 | validation: 2.6595371673359223]
	TIME [epoch: 8.41 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8030707042766108		[learning rate: 0.0059145]
		[batch 20/20] avg loss: 1.9657409457911483		[learning rate: 0.0059038]
	Learning Rate: 0.0059038
	LOSS [training: 1.8844058250338798 | validation: 2.6570343072763025]
	TIME [epoch: 8.4 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9158849007574728		[learning rate: 0.0058931]
		[batch 20/20] avg loss: 2.080030441193873		[learning rate: 0.0058824]
	Learning Rate: 0.00588237
	LOSS [training: 1.9979576709756732 | validation: 2.671607650352309]
	TIME [epoch: 8.4 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1520037992860828		[learning rate: 0.0058717]
		[batch 20/20] avg loss: 2.1251364933156167		[learning rate: 0.005861]
	Learning Rate: 0.00586103
	LOSS [training: 2.1385701463008493 | validation: 2.9003841652775724]
	TIME [epoch: 8.42 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.843170203337359		[learning rate: 0.0058504]
		[batch 20/20] avg loss: 2.0240053724847824		[learning rate: 0.0058398]
	Learning Rate: 0.00583976
	LOSS [training: 1.9335877879110703 | validation: 2.647271171444256]
	TIME [epoch: 8.41 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8335900471975772		[learning rate: 0.0058291]
		[batch 20/20] avg loss: 1.9717638663356591		[learning rate: 0.0058186]
	Learning Rate: 0.00581856
	LOSS [training: 1.9026769567666182 | validation: 2.735137340596382]
	TIME [epoch: 8.4 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7398951007919734		[learning rate: 0.005808]
		[batch 20/20] avg loss: 2.039262454924355		[learning rate: 0.0057974]
	Learning Rate: 0.00579745
	LOSS [training: 1.8895787778581643 | validation: 2.719857040521469]
	TIME [epoch: 8.4 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9223929654913223		[learning rate: 0.0057869]
		[batch 20/20] avg loss: 1.9214298148958218		[learning rate: 0.0057764]
	Learning Rate: 0.00577641
	LOSS [training: 1.921911390193572 | validation: 2.67563877231292]
	TIME [epoch: 8.42 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.752233890608947		[learning rate: 0.0057659]
		[batch 20/20] avg loss: 1.9077217686846992		[learning rate: 0.0057554]
	Learning Rate: 0.00575545
	LOSS [training: 1.8299778296468232 | validation: 2.599987094023528]
	TIME [epoch: 8.4 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.922049904092051		[learning rate: 0.005745]
		[batch 20/20] avg loss: 1.8214910198562517		[learning rate: 0.0057346]
	Learning Rate: 0.00573456
	LOSS [training: 1.8717704619741515 | validation: 2.9635213517238754]
	TIME [epoch: 8.4 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0796383156246017		[learning rate: 0.0057241]
		[batch 20/20] avg loss: 1.8736548138559381		[learning rate: 0.0057137]
	Learning Rate: 0.00571375
	LOSS [training: 1.9766465647402693 | validation: 2.8481563124940124]
	TIME [epoch: 8.39 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0475412407212317		[learning rate: 0.0057034]
		[batch 20/20] avg loss: 1.7864797014195557		[learning rate: 0.005693]
	Learning Rate: 0.00569301
	LOSS [training: 1.917010471070394 | validation: 2.682376903704424]
	TIME [epoch: 8.39 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7983149324431456		[learning rate: 0.0056827]
		[batch 20/20] avg loss: 2.0856269627226585		[learning rate: 0.0056724]
	Learning Rate: 0.00567235
	LOSS [training: 1.941970947582902 | validation: 3.1401423696988675]
	TIME [epoch: 8.42 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8354331392961658		[learning rate: 0.005662]
		[batch 20/20] avg loss: 1.9995138872267002		[learning rate: 0.0056518]
	Learning Rate: 0.00565177
	LOSS [training: 1.9174735132614331 | validation: 2.7722929274536554]
	TIME [epoch: 8.39 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9670798051142253		[learning rate: 0.0056415]
		[batch 20/20] avg loss: 1.8448064882843718		[learning rate: 0.0056313]
	Learning Rate: 0.00563126
	LOSS [training: 1.9059431466992982 | validation: 2.6510948152326765]
	TIME [epoch: 8.39 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9261420043333264		[learning rate: 0.005621]
		[batch 20/20] avg loss: 1.8678186120001843		[learning rate: 0.0056108]
	Learning Rate: 0.00561082
	LOSS [training: 1.8969803081667553 | validation: 2.7970457554062618]
	TIME [epoch: 8.39 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7765572769159668		[learning rate: 0.0056006]
		[batch 20/20] avg loss: 1.9584607945798034		[learning rate: 0.0055905]
	Learning Rate: 0.00559046
	LOSS [training: 1.8675090357478852 | validation: 2.7076581561866697]
	TIME [epoch: 8.42 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.992057356203987		[learning rate: 0.0055803]
		[batch 20/20] avg loss: 1.9589793610708182		[learning rate: 0.0055702]
	Learning Rate: 0.00557017
	LOSS [training: 1.9755183586374023 | validation: 2.616362827593494]
	TIME [epoch: 8.39 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.941927768109537		[learning rate: 0.0055601]
		[batch 20/20] avg loss: 1.8593035443975936		[learning rate: 0.00555]
	Learning Rate: 0.00554996
	LOSS [training: 1.900615656253565 | validation: 2.621609015163513]
	TIME [epoch: 8.38 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.768129836197693		[learning rate: 0.0055399]
		[batch 20/20] avg loss: 1.8511192523288407		[learning rate: 0.0055298]
	Learning Rate: 0.00552981
	LOSS [training: 1.8096245442632672 | validation: 2.579653019685763]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9641215350081045		[learning rate: 0.0055198]
		[batch 20/20] avg loss: 1.9840302294612795		[learning rate: 0.0055097]
	Learning Rate: 0.00550975
	LOSS [training: 1.974075882234692 | validation: 2.689071703214149]
	TIME [epoch: 8.41 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9394985671784748		[learning rate: 0.0054997]
		[batch 20/20] avg loss: 1.817684773171657		[learning rate: 0.0054898]
	Learning Rate: 0.00548975
	LOSS [training: 1.8785916701750662 | validation: 2.6776476380090273]
	TIME [epoch: 8.39 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8131130604618222		[learning rate: 0.0054798]
		[batch 20/20] avg loss: 1.9880624700767666		[learning rate: 0.0054698]
	Learning Rate: 0.00546983
	LOSS [training: 1.9005877652692942 | validation: 2.6067625566547328]
	TIME [epoch: 8.39 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8262454463493776		[learning rate: 0.0054599]
		[batch 20/20] avg loss: 1.9588611182234739		[learning rate: 0.00545]
	Learning Rate: 0.00544998
	LOSS [training: 1.8925532822864262 | validation: 2.7261695902501812]
	TIME [epoch: 8.38 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7733917305109468		[learning rate: 0.0054401]
		[batch 20/20] avg loss: 1.9903195759267842		[learning rate: 0.0054302]
	Learning Rate: 0.0054302
	LOSS [training: 1.881855653218866 | validation: 2.663255956709288]
	TIME [epoch: 8.39 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9534016781336228		[learning rate: 0.0054203]
		[batch 20/20] avg loss: 1.8219604787006614		[learning rate: 0.0054105]
	Learning Rate: 0.00541049
	LOSS [training: 1.8876810784171423 | validation: 2.6209894729555696]
	TIME [epoch: 8.41 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8514079199171154		[learning rate: 0.0054007]
		[batch 20/20] avg loss: 1.8587790875772199		[learning rate: 0.0053909]
	Learning Rate: 0.00539086
	LOSS [training: 1.8550935037471679 | validation: 3.0531618250396]
	TIME [epoch: 8.39 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7062164415113468		[learning rate: 0.0053811]
		[batch 20/20] avg loss: 2.118661243315813		[learning rate: 0.0053713]
	Learning Rate: 0.00537129
	LOSS [training: 1.91243884241358 | validation: 2.5991078788686086]
	TIME [epoch: 8.38 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8840148816926785		[learning rate: 0.0053615]
		[batch 20/20] avg loss: 1.9157318880131737		[learning rate: 0.0053518]
	Learning Rate: 0.0053518
	LOSS [training: 1.8998733848529263 | validation: 2.6581242831425143]
	TIME [epoch: 8.39 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.811208618759125		[learning rate: 0.0053421]
		[batch 20/20] avg loss: 1.9248516024153008		[learning rate: 0.0053324]
	Learning Rate: 0.00533238
	LOSS [training: 1.8680301105872126 | validation: 2.655571425705054]
	TIME [epoch: 8.41 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8865207640208586		[learning rate: 0.0053227]
		[batch 20/20] avg loss: 1.8477204001781835		[learning rate: 0.005313]
	Learning Rate: 0.00531303
	LOSS [training: 1.8671205820995211 | validation: 2.839637465647823]
	TIME [epoch: 8.39 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.783920826641968		[learning rate: 0.0053034]
		[batch 20/20] avg loss: 2.004179684900034		[learning rate: 0.0052937]
	Learning Rate: 0.00529375
	LOSS [training: 1.8940502557710008 | validation: 2.7295159950083803]
	TIME [epoch: 8.39 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6809749243583916		[learning rate: 0.0052841]
		[batch 20/20] avg loss: 2.0053095045470033		[learning rate: 0.0052745]
	Learning Rate: 0.00527454
	LOSS [training: 1.8431422144526977 | validation: 2.6076969627066533]
	TIME [epoch: 8.38 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8732546383373454		[learning rate: 0.005265]
		[batch 20/20] avg loss: 1.9088030195859627		[learning rate: 0.0052554]
	Learning Rate: 0.00525539
	LOSS [training: 1.8910288289616541 | validation: 2.6153648912890164]
	TIME [epoch: 8.41 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.953954203566243		[learning rate: 0.0052458]
		[batch 20/20] avg loss: 1.8690027390575583		[learning rate: 0.0052363]
	Learning Rate: 0.00523632
	LOSS [training: 1.9114784713119009 | validation: 2.784288213496653]
	TIME [epoch: 8.38 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8814495813171255		[learning rate: 0.0052268]
		[batch 20/20] avg loss: 1.9206727663758598		[learning rate: 0.0052173]
	Learning Rate: 0.00521732
	LOSS [training: 1.901061173846493 | validation: 2.917008424589818]
	TIME [epoch: 8.39 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.844716067042055		[learning rate: 0.0052078]
		[batch 20/20] avg loss: 1.9288052810054186		[learning rate: 0.0051984]
	Learning Rate: 0.00519839
	LOSS [training: 1.886760674023737 | validation: 2.6148508198196474]
	TIME [epoch: 8.39 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8719524095155127		[learning rate: 0.0051889]
		[batch 20/20] avg loss: 1.894872813142396		[learning rate: 0.0051795]
	Learning Rate: 0.00517952
	LOSS [training: 1.883412611328954 | validation: 2.670060157765106]
	TIME [epoch: 8.41 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9021899208320463		[learning rate: 0.0051701]
		[batch 20/20] avg loss: 1.8580879788903093		[learning rate: 0.0051607]
	Learning Rate: 0.00516072
	LOSS [training: 1.880138949861178 | validation: 2.621419362691423]
	TIME [epoch: 8.4 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9984874346321846		[learning rate: 0.0051513]
		[batch 20/20] avg loss: 1.8093452668207253		[learning rate: 0.005142]
	Learning Rate: 0.00514199
	LOSS [training: 1.9039163507264552 | validation: 2.6991587899376928]
	TIME [epoch: 8.39 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7118836277625633		[learning rate: 0.0051327]
		[batch 20/20] avg loss: 1.951810736019714		[learning rate: 0.0051233]
	Learning Rate: 0.00512333
	LOSS [training: 1.8318471818911388 | validation: 2.79561594238924]
	TIME [epoch: 8.39 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7596687136148295		[learning rate: 0.005114]
		[batch 20/20] avg loss: 1.8636370200779666		[learning rate: 0.0051047]
	Learning Rate: 0.00510474
	LOSS [training: 1.8116528668463978 | validation: 2.7014200505981836]
	TIME [epoch: 8.39 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6116037379660408		[learning rate: 0.0050955]
		[batch 20/20] avg loss: 2.0470786868545288		[learning rate: 0.0050862]
	Learning Rate: 0.00508622
	LOSS [training: 1.8293412124102855 | validation: 2.782092525913703]
	TIME [epoch: 8.42 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8063803694071336		[learning rate: 0.005077]
		[batch 20/20] avg loss: 1.965809879990069		[learning rate: 0.0050678]
	Learning Rate: 0.00506776
	LOSS [training: 1.8860951246986013 | validation: 2.591272684826591]
	TIME [epoch: 8.39 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.975398575154302		[learning rate: 0.0050586]
		[batch 20/20] avg loss: 1.7446221391341301		[learning rate: 0.0050494]
	Learning Rate: 0.00504937
	LOSS [training: 1.860010357144216 | validation: 2.890316403932734]
	TIME [epoch: 8.4 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8603477364484644		[learning rate: 0.0050402]
		[batch 20/20] avg loss: 1.9334055762588864		[learning rate: 0.005031]
	Learning Rate: 0.00503104
	LOSS [training: 1.896876656353675 | validation: 2.627844550227525]
	TIME [epoch: 8.4 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.853469796299585		[learning rate: 0.0050219]
		[batch 20/20] avg loss: 1.8706832920892946		[learning rate: 0.0050128]
	Learning Rate: 0.00501278
	LOSS [training: 1.86207654419444 | validation: 2.636220647555631]
	TIME [epoch: 8.41 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7920120520160232		[learning rate: 0.0050037]
		[batch 20/20] avg loss: 1.8543016344166499		[learning rate: 0.0049946]
	Learning Rate: 0.00499459
	LOSS [training: 1.8231568432163368 | validation: 2.724005056431812]
	TIME [epoch: 8.4 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0290999117203095		[learning rate: 0.0049855]
		[batch 20/20] avg loss: 1.7586608926717744		[learning rate: 0.0049765]
	Learning Rate: 0.00497647
	LOSS [training: 1.8938804021960418 | validation: 2.7912501163156764]
	TIME [epoch: 8.39 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7360188383682154		[learning rate: 0.0049674]
		[batch 20/20] avg loss: 1.9944485005643084		[learning rate: 0.0049584]
	Learning Rate: 0.00495841
	LOSS [training: 1.865233669466262 | validation: 2.8543497107358187]
	TIME [epoch: 8.39 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9647287427679558		[learning rate: 0.0049494]
		[batch 20/20] avg loss: 1.7142123433590857		[learning rate: 0.0049404]
	Learning Rate: 0.00494041
	LOSS [training: 1.839470543063521 | validation: 2.6510822029852505]
	TIME [epoch: 8.41 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8654110064941758		[learning rate: 0.0049314]
		[batch 20/20] avg loss: 1.7977705803515132		[learning rate: 0.0049225]
	Learning Rate: 0.00492248
	LOSS [training: 1.8315907934228441 | validation: 2.6822574065758484]
	TIME [epoch: 8.4 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8865140264895455		[learning rate: 0.0049135]
		[batch 20/20] avg loss: 1.9211520217006792		[learning rate: 0.0049046]
	Learning Rate: 0.00490462
	LOSS [training: 1.9038330240951127 | validation: 2.65506159436965]
	TIME [epoch: 8.39 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7620979964294534		[learning rate: 0.0048957]
		[batch 20/20] avg loss: 1.9181828563719088		[learning rate: 0.0048868]
	Learning Rate: 0.00488682
	LOSS [training: 1.840140426400681 | validation: 2.5984622268689006]
	TIME [epoch: 8.39 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8439161710057195		[learning rate: 0.0048779]
		[batch 20/20] avg loss: 1.694319275361694		[learning rate: 0.0048691]
	Learning Rate: 0.00486909
	LOSS [training: 1.7691177231837067 | validation: 2.9689679726040388]
	TIME [epoch: 8.4 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9937991872086134		[learning rate: 0.0048602]
		[batch 20/20] avg loss: 1.695676219349887		[learning rate: 0.0048514]
	Learning Rate: 0.00485141
	LOSS [training: 1.8447377032792502 | validation: 2.566913546148072]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.654258333145722		[learning rate: 0.0048426]
		[batch 20/20] avg loss: 2.021167412412421		[learning rate: 0.0048338]
	Learning Rate: 0.00483381
	LOSS [training: 1.837712872779072 | validation: 2.674748122157726]
	TIME [epoch: 8.4 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9000977082133121		[learning rate: 0.004825]
		[batch 20/20] avg loss: 1.8843809599440213		[learning rate: 0.0048163]
	Learning Rate: 0.00481627
	LOSS [training: 1.8922393340786665 | validation: 2.751534802894467]
	TIME [epoch: 8.39 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8254576464975163		[learning rate: 0.0048075]
		[batch 20/20] avg loss: 1.9310650617421594		[learning rate: 0.0047988]
	Learning Rate: 0.00479879
	LOSS [training: 1.878261354119838 | validation: 2.9968525695953643]
	TIME [epoch: 8.39 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8383547718404594		[learning rate: 0.0047901]
		[batch 20/20] avg loss: 1.8561352233974961		[learning rate: 0.0047814]
	Learning Rate: 0.00478137
	LOSS [training: 1.847244997618978 | validation: 3.0609672722600205]
	TIME [epoch: 8.42 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9623822809893796		[learning rate: 0.0047727]
		[batch 20/20] avg loss: 1.80566792903395		[learning rate: 0.004764]
	Learning Rate: 0.00476402
	LOSS [training: 1.8840251050116645 | validation: 2.639837333777707]
	TIME [epoch: 8.39 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7375707243548646		[learning rate: 0.0047554]
		[batch 20/20] avg loss: 1.940153185002663		[learning rate: 0.0047467]
	Learning Rate: 0.00474673
	LOSS [training: 1.8388619546787637 | validation: 2.5975457452515354]
	TIME [epoch: 8.39 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.826512015267539		[learning rate: 0.0047381]
		[batch 20/20] avg loss: 1.7059231025986246		[learning rate: 0.0047295]
	Learning Rate: 0.00472951
	LOSS [training: 1.7662175589330817 | validation: 2.566491483484531]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8465587886115944		[learning rate: 0.0047209]
		[batch 20/20] avg loss: 2.061195736652267		[learning rate: 0.0047123]
	Learning Rate: 0.00471234
	LOSS [training: 1.9538772626319303 | validation: 2.7210970122288636]
	TIME [epoch: 8.41 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.805302223010139		[learning rate: 0.0047038]
		[batch 20/20] avg loss: 1.962251858605964		[learning rate: 0.0046952]
	Learning Rate: 0.00469524
	LOSS [training: 1.8837770408080516 | validation: 2.80389882020866]
	TIME [epoch: 8.39 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.805369003398519		[learning rate: 0.0046867]
		[batch 20/20] avg loss: 1.7624700595356146		[learning rate: 0.0046782]
	Learning Rate: 0.0046782
	LOSS [training: 1.7839195314670668 | validation: 2.7657606992155697]
	TIME [epoch: 8.37 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7407183423104773		[learning rate: 0.0046697]
		[batch 20/20] avg loss: 1.8891192261670795		[learning rate: 0.0046612]
	Learning Rate: 0.00466122
	LOSS [training: 1.8149187842387786 | validation: 2.6735412643173997]
	TIME [epoch: 8.37 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.753690292585007		[learning rate: 0.0046528]
		[batch 20/20] avg loss: 1.9371826434677686		[learning rate: 0.0046443]
	Learning Rate: 0.00464431
	LOSS [training: 1.8454364680263875 | validation: 2.612922036882032]
	TIME [epoch: 8.38 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0181530906137928		[learning rate: 0.0046359]
		[batch 20/20] avg loss: 1.577100365720343		[learning rate: 0.0046275]
	Learning Rate: 0.00462745
	LOSS [training: 1.7976267281670677 | validation: 2.5608607852142797]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.938209991909679		[learning rate: 0.004619]
		[batch 20/20] avg loss: 1.8046993751233376		[learning rate: 0.0046107]
	Learning Rate: 0.00461066
	LOSS [training: 1.871454683516508 | validation: 2.8731452155852675]
	TIME [epoch: 8.38 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0026517581825205		[learning rate: 0.0046023]
		[batch 20/20] avg loss: 1.712267920343471		[learning rate: 0.0045939]
	Learning Rate: 0.00459393
	LOSS [training: 1.8574598392629962 | validation: 2.854664764398453]
	TIME [epoch: 8.39 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7077055119079485		[learning rate: 0.0045856]
		[batch 20/20] avg loss: 2.0701174172558727		[learning rate: 0.0045773]
	Learning Rate: 0.00457726
	LOSS [training: 1.888911464581911 | validation: 2.6394990498265942]
	TIME [epoch: 8.39 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.806057628576378		[learning rate: 0.0045689]
		[batch 20/20] avg loss: 1.9206319248828865		[learning rate: 0.0045606]
	Learning Rate: 0.00456065
	LOSS [training: 1.863344776729632 | validation: 2.708819678902299]
	TIME [epoch: 8.4 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8183058570586852		[learning rate: 0.0045524]
		[batch 20/20] avg loss: 1.8126658993852984		[learning rate: 0.0045441]
	Learning Rate: 0.00454409
	LOSS [training: 1.8154858782219914 | validation: 2.6529463608741035]
	TIME [epoch: 8.38 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9590386226746077		[learning rate: 0.0045358]
		[batch 20/20] avg loss: 1.751601838006895		[learning rate: 0.0045276]
	Learning Rate: 0.0045276
	LOSS [training: 1.8553202303407512 | validation: 2.5808731008849977]
	TIME [epoch: 8.37 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8139796972879978		[learning rate: 0.0045194]
		[batch 20/20] avg loss: 1.81594033239648		[learning rate: 0.0045112]
	Learning Rate: 0.00451117
	LOSS [training: 1.8149600148422387 | validation: 2.9052309476784464]
	TIME [epoch: 8.38 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7996553100468298		[learning rate: 0.004503]
		[batch 20/20] avg loss: 1.8566756045700956		[learning rate: 0.0044948]
	Learning Rate: 0.0044948
	LOSS [training: 1.8281654573084627 | validation: 2.6718523766732067]
	TIME [epoch: 8.41 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8371210945702885		[learning rate: 0.0044866]
		[batch 20/20] avg loss: 1.8232495402385087		[learning rate: 0.0044785]
	Learning Rate: 0.00447849
	LOSS [training: 1.8301853174043985 | validation: 3.0073951908981815]
	TIME [epoch: 8.39 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7815658563704169		[learning rate: 0.0044704]
		[batch 20/20] avg loss: 1.9016961746495853		[learning rate: 0.0044622]
	Learning Rate: 0.00446224
	LOSS [training: 1.841631015510001 | validation: 2.655354124976867]
	TIME [epoch: 8.39 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6609534795955174		[learning rate: 0.0044541]
		[batch 20/20] avg loss: 1.905942592505557		[learning rate: 0.004446]
	Learning Rate: 0.00444604
	LOSS [training: 1.7834480360505371 | validation: 2.5655587539018803]
	TIME [epoch: 8.38 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7108316165429733		[learning rate: 0.004438]
		[batch 20/20] avg loss: 1.8724652259557495		[learning rate: 0.0044299]
	Learning Rate: 0.00442991
	LOSS [training: 1.7916484212493615 | validation: 2.8575472876652337]
	TIME [epoch: 8.4 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7659946131715496		[learning rate: 0.0044219]
		[batch 20/20] avg loss: 1.9086371699285505		[learning rate: 0.0044138]
	Learning Rate: 0.00441383
	LOSS [training: 1.8373158915500496 | validation: 2.585603960586652]
	TIME [epoch: 8.41 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.012152880812222		[learning rate: 0.0044058]
		[batch 20/20] avg loss: 1.6261813620706622		[learning rate: 0.0043978]
	Learning Rate: 0.00439781
	LOSS [training: 1.819167121441442 | validation: 3.1331269491796454]
	TIME [epoch: 8.39 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7378715328732945		[learning rate: 0.0043898]
		[batch 20/20] avg loss: 1.9609670491540225		[learning rate: 0.0043819]
	Learning Rate: 0.00438185
	LOSS [training: 1.849419291013659 | validation: 2.65161178730872]
	TIME [epoch: 8.39 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9180340939236735		[learning rate: 0.0043739]
		[batch 20/20] avg loss: 1.7665517852211832		[learning rate: 0.004366]
	Learning Rate: 0.00436595
	LOSS [training: 1.8422929395724288 | validation: 2.8917209139696514]
	TIME [epoch: 8.4 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7760619520130603		[learning rate: 0.004358]
		[batch 20/20] avg loss: 1.946005076095647		[learning rate: 0.0043501]
	Learning Rate: 0.00435011
	LOSS [training: 1.8610335140543537 | validation: 2.6793964468661176]
	TIME [epoch: 8.41 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7092698790204888		[learning rate: 0.0043422]
		[batch 20/20] avg loss: 1.8570909383915317		[learning rate: 0.0043343]
	Learning Rate: 0.00433432
	LOSS [training: 1.7831804087060104 | validation: 2.6285038385335513]
	TIME [epoch: 8.4 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.829876714416263		[learning rate: 0.0043264]
		[batch 20/20] avg loss: 1.7667853457668552		[learning rate: 0.0043186]
	Learning Rate: 0.00431859
	LOSS [training: 1.7983310300915594 | validation: 2.5988340274813577]
	TIME [epoch: 8.39 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8536180291590878		[learning rate: 0.0043107]
		[batch 20/20] avg loss: 1.739458009779786		[learning rate: 0.0043029]
	Learning Rate: 0.00430292
	LOSS [training: 1.7965380194694365 | validation: 2.598631635345898]
	TIME [epoch: 8.4 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.871170709736592		[learning rate: 0.0042951]
		[batch 20/20] avg loss: 1.7857696805318242		[learning rate: 0.0042873]
	Learning Rate: 0.0042873
	LOSS [training: 1.828470195134208 | validation: 2.764284998336006]
	TIME [epoch: 8.41 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8076128536916625		[learning rate: 0.0042795]
		[batch 20/20] avg loss: 1.8102999736907819		[learning rate: 0.0042717]
	Learning Rate: 0.00427174
	LOSS [training: 1.8089564136912224 | validation: 2.5914723150579757]
	TIME [epoch: 8.4 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9430796173940454		[learning rate: 0.004264]
		[batch 20/20] avg loss: 1.6278100284613841		[learning rate: 0.0042562]
	Learning Rate: 0.00425624
	LOSS [training: 1.7854448229277147 | validation: 2.548319741107682]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6915046035554724		[learning rate: 0.0042485]
		[batch 20/20] avg loss: 1.8764367072176682		[learning rate: 0.0042408]
	Learning Rate: 0.0042408
	LOSS [training: 1.78397065538657 | validation: 2.7131098469970096]
	TIME [epoch: 8.39 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7310712124171157		[learning rate: 0.0042331]
		[batch 20/20] avg loss: 1.917310621738214		[learning rate: 0.0042254]
	Learning Rate: 0.00422541
	LOSS [training: 1.8241909170776647 | validation: 2.706517239718633]
	TIME [epoch: 8.41 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8360051753406992		[learning rate: 0.0042177]
		[batch 20/20] avg loss: 1.7304494962273573		[learning rate: 0.0042101]
	Learning Rate: 0.00421007
	LOSS [training: 1.7832273357840287 | validation: 2.5601489623749156]
	TIME [epoch: 8.39 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8200073926407232		[learning rate: 0.0042024]
		[batch 20/20] avg loss: 1.7805926827759833		[learning rate: 0.0041948]
	Learning Rate: 0.00419479
	LOSS [training: 1.8003000377083533 | validation: 2.6701216712308096]
	TIME [epoch: 8.38 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6327234866431453		[learning rate: 0.0041872]
		[batch 20/20] avg loss: 1.9162667805136888		[learning rate: 0.0041796]
	Learning Rate: 0.00417957
	LOSS [training: 1.774495133578417 | validation: 2.7597874166974865]
	TIME [epoch: 8.38 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.687820995172825		[learning rate: 0.004172]
		[batch 20/20] avg loss: 1.9000689686660892		[learning rate: 0.0041644]
	Learning Rate: 0.0041644
	LOSS [training: 1.793944981919457 | validation: 2.5872999549055766]
	TIME [epoch: 8.38 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8606956470766263		[learning rate: 0.0041568]
		[batch 20/20] avg loss: 1.724596512833021		[learning rate: 0.0041493]
	Learning Rate: 0.00414929
	LOSS [training: 1.7926460799548238 | validation: 2.7249935352071333]
	TIME [epoch: 8.41 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7380609008887302		[learning rate: 0.0041418]
		[batch 20/20] avg loss: 1.8488189667416275		[learning rate: 0.0041342]
	Learning Rate: 0.00413423
	LOSS [training: 1.7934399338151785 | validation: 2.6521400853207533]
	TIME [epoch: 8.38 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8812364365212915		[learning rate: 0.0041267]
		[batch 20/20] avg loss: 1.7704499296943361		[learning rate: 0.0041192]
	Learning Rate: 0.00411923
	LOSS [training: 1.8258431831078137 | validation: 2.60482390788642]
	TIME [epoch: 8.38 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8215359898898797		[learning rate: 0.0041117]
		[batch 20/20] avg loss: 1.7678279601758926		[learning rate: 0.0041043]
	Learning Rate: 0.00410428
	LOSS [training: 1.7946819750328864 | validation: 2.6177884612196953]
	TIME [epoch: 8.38 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.739808776333253		[learning rate: 0.0040968]
		[batch 20/20] avg loss: 1.8247318071291256		[learning rate: 0.0040894]
	Learning Rate: 0.00408938
	LOSS [training: 1.7822702917311894 | validation: 2.554845648238535]
	TIME [epoch: 8.41 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7492166099900992		[learning rate: 0.004082]
		[batch 20/20] avg loss: 1.8086131549339561		[learning rate: 0.0040745]
	Learning Rate: 0.00407454
	LOSS [training: 1.7789148824620278 | validation: 2.9274796054920547]
	TIME [epoch: 8.39 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9273296543655956		[learning rate: 0.0040671]
		[batch 20/20] avg loss: 1.719024267774127		[learning rate: 0.0040598]
	Learning Rate: 0.00405976
	LOSS [training: 1.8231769610698616 | validation: 2.5689698423931837]
	TIME [epoch: 8.39 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.065154387292128		[learning rate: 0.0040524]
		[batch 20/20] avg loss: 1.5490172529787136		[learning rate: 0.004045]
	Learning Rate: 0.00404502
	LOSS [training: 1.807085820135421 | validation: 3.5298885908688056]
	TIME [epoch: 8.39 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9586971486771205		[learning rate: 0.0040377]
		[batch 20/20] avg loss: 1.895824167823438		[learning rate: 0.0040303]
	Learning Rate: 0.00403034
	LOSS [training: 1.9272606582502796 | validation: 2.7696552163410004]
	TIME [epoch: 8.42 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9127034498977225		[learning rate: 0.004023]
		[batch 20/20] avg loss: 1.664268841695733		[learning rate: 0.0040157]
	Learning Rate: 0.00401572
	LOSS [training: 1.7884861457967272 | validation: 2.6464332139305373]
	TIME [epoch: 8.39 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.863851213180412		[learning rate: 0.0040084]
		[batch 20/20] avg loss: 1.6769641814755172		[learning rate: 0.0040011]
	Learning Rate: 0.00400114
	LOSS [training: 1.7704076973279643 | validation: 2.6708860145091773]
	TIME [epoch: 8.39 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7704284596456177		[learning rate: 0.0039939]
		[batch 20/20] avg loss: 1.8823244683909823		[learning rate: 0.0039866]
	Learning Rate: 0.00398662
	LOSS [training: 1.8263764640182998 | validation: 2.7250282103572996]
	TIME [epoch: 8.38 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8170756806307495		[learning rate: 0.0039794]
		[batch 20/20] avg loss: 1.8158076442463158		[learning rate: 0.0039722]
	Learning Rate: 0.00397216
	LOSS [training: 1.816441662438533 | validation: 2.6584016620181052]
	TIME [epoch: 8.38 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.633512009635035		[learning rate: 0.0039649]
		[batch 20/20] avg loss: 1.9141801975521333		[learning rate: 0.0039577]
	Learning Rate: 0.00395774
	LOSS [training: 1.7738461035935842 | validation: 2.608857708104126]
	TIME [epoch: 8.41 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6523466093360475		[learning rate: 0.0039506]
		[batch 20/20] avg loss: 1.880661994101968		[learning rate: 0.0039434]
	Learning Rate: 0.00394338
	LOSS [training: 1.7665043017190079 | validation: 2.641811800614935]
	TIME [epoch: 8.38 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6240459572568888		[learning rate: 0.0039362]
		[batch 20/20] avg loss: 1.9667959130436017		[learning rate: 0.0039291]
	Learning Rate: 0.00392907
	LOSS [training: 1.7954209351502448 | validation: 2.6585872833866824]
	TIME [epoch: 8.38 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0156805218564156		[learning rate: 0.0039219]
		[batch 20/20] avg loss: 1.4850165797131658		[learning rate: 0.0039148]
	Learning Rate: 0.00391481
	LOSS [training: 1.750348550784791 | validation: 2.8242347125322476]
	TIME [epoch: 8.37 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9156371139729564		[learning rate: 0.0039077]
		[batch 20/20] avg loss: 1.645816199885323		[learning rate: 0.0039006]
	Learning Rate: 0.0039006
	LOSS [training: 1.78072665692914 | validation: 2.622552351018447]
	TIME [epoch: 8.4 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.707902036662174		[learning rate: 0.0038935]
		[batch 20/20] avg loss: 1.798479714121914		[learning rate: 0.0038864]
	Learning Rate: 0.00388645
	LOSS [training: 1.753190875392044 | validation: 2.643088691658354]
	TIME [epoch: 8.38 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9828639243755344		[learning rate: 0.0038794]
		[batch 20/20] avg loss: 1.7967041879095547		[learning rate: 0.0038723]
	Learning Rate: 0.00387234
	LOSS [training: 1.8897840561425443 | validation: 2.632916482406247]
	TIME [epoch: 8.38 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.882719897461104		[learning rate: 0.0038653]
		[batch 20/20] avg loss: 1.7281192016973925		[learning rate: 0.0038583]
	Learning Rate: 0.00385829
	LOSS [training: 1.805419549579248 | validation: 2.5864356887968194]
	TIME [epoch: 8.38 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7143752036786644		[learning rate: 0.0038513]
		[batch 20/20] avg loss: 1.8241294598259272		[learning rate: 0.0038443]
	Learning Rate: 0.00384429
	LOSS [training: 1.7692523317522961 | validation: 2.6362327508315295]
	TIME [epoch: 8.39 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8738647085676505		[learning rate: 0.0038373]
		[batch 20/20] avg loss: 2.00078549732016		[learning rate: 0.0038303]
	Learning Rate: 0.00383034
	LOSS [training: 1.9373251029439058 | validation: 2.661882321120232]
	TIME [epoch: 8.38 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8392936293364162		[learning rate: 0.0038234]
		[batch 20/20] avg loss: 1.7540782689740961		[learning rate: 0.0038164]
	Learning Rate: 0.00381644
	LOSS [training: 1.7966859491552563 | validation: 2.6363974084857364]
	TIME [epoch: 8.37 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8478820192416585		[learning rate: 0.0038095]
		[batch 20/20] avg loss: 1.7082049615039545		[learning rate: 0.0038026]
	Learning Rate: 0.00380258
	LOSS [training: 1.778043490372807 | validation: 2.6459655997906983]
	TIME [epoch: 8.37 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8896541351047784		[learning rate: 0.0037957]
		[batch 20/20] avg loss: 1.6415065466389875		[learning rate: 0.0037888]
	Learning Rate: 0.00378879
	LOSS [training: 1.7655803408718829 | validation: 2.6816857110105414]
	TIME [epoch: 8.39 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7861029786714884		[learning rate: 0.0037819]
		[batch 20/20] avg loss: 1.7485941412633152		[learning rate: 0.003775]
	Learning Rate: 0.00377504
	LOSS [training: 1.767348559967402 | validation: 2.689006700141693]
	TIME [epoch: 8.41 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7574255236534042		[learning rate: 0.0037682]
		[batch 20/20] avg loss: 1.7238801612291805		[learning rate: 0.0037613]
	Learning Rate: 0.00376134
	LOSS [training: 1.7406528424412921 | validation: 2.7827716334539296]
	TIME [epoch: 8.38 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8150044792342843		[learning rate: 0.0037545]
		[batch 20/20] avg loss: 1.7554534262808907		[learning rate: 0.0037477]
	Learning Rate: 0.00374769
	LOSS [training: 1.7852289527575878 | validation: 2.6824045036742517]
	TIME [epoch: 8.38 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.874009682320618		[learning rate: 0.0037409]
		[batch 20/20] avg loss: 1.7047701401628015		[learning rate: 0.0037341]
	Learning Rate: 0.00373408
	LOSS [training: 1.78938991124171 | validation: 2.8514374876316264]
	TIME [epoch: 8.38 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6215447748579646		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 1.9122032864153489		[learning rate: 0.0037205]
	Learning Rate: 0.00372053
	LOSS [training: 1.7668740306366568 | validation: 2.5779620807559787]
	TIME [epoch: 8.4 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8171569265600158		[learning rate: 0.0037138]
		[batch 20/20] avg loss: 1.7206626597150465		[learning rate: 0.003707]
	Learning Rate: 0.00370703
	LOSS [training: 1.7689097931375315 | validation: 2.7952806288530554]
	TIME [epoch: 8.38 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.802343174021369		[learning rate: 0.0037003]
		[batch 20/20] avg loss: 1.7196294491813329		[learning rate: 0.0036936]
	Learning Rate: 0.00369358
	LOSS [training: 1.7609863116013507 | validation: 2.5517429282147956]
	TIME [epoch: 8.38 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7826392439930001		[learning rate: 0.0036869]
		[batch 20/20] avg loss: 1.721312604385083		[learning rate: 0.0036802]
	Learning Rate: 0.00368017
	LOSS [training: 1.751975924189042 | validation: 2.7209859104472307]
	TIME [epoch: 8.38 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7120149853290532		[learning rate: 0.0036735]
		[batch 20/20] avg loss: 1.7884277766271552		[learning rate: 0.0036668]
	Learning Rate: 0.00366682
	LOSS [training: 1.750221380978104 | validation: 2.854535806349868]
	TIME [epoch: 8.4 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7777294444153864		[learning rate: 0.0036602]
		[batch 20/20] avg loss: 1.8672817350887514		[learning rate: 0.0036535]
	Learning Rate: 0.00365351
	LOSS [training: 1.8225055897520683 | validation: 2.734446041071187]
	TIME [epoch: 8.38 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8838084925916039		[learning rate: 0.0036469]
		[batch 20/20] avg loss: 1.68655240129754		[learning rate: 0.0036403]
	Learning Rate: 0.00364025
	LOSS [training: 1.7851804469445725 | validation: 2.5738609661769964]
	TIME [epoch: 8.39 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9889987641390612		[learning rate: 0.0036336]
		[batch 20/20] avg loss: 1.5178886175135589		[learning rate: 0.003627]
	Learning Rate: 0.00362704
	LOSS [training: 1.75344369082631 | validation: 2.7128091373823464]
	TIME [epoch: 8.38 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9783910428943252		[learning rate: 0.0036205]
		[batch 20/20] avg loss: 1.5537943949445487		[learning rate: 0.0036139]
	Learning Rate: 0.00361388
	LOSS [training: 1.7660927189194369 | validation: 2.665217178793763]
	TIME [epoch: 8.39 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8180090468164203		[learning rate: 0.0036073]
		[batch 20/20] avg loss: 1.7461339958938815		[learning rate: 0.0036008]
	Learning Rate: 0.00360076
	LOSS [training: 1.7820715213551508 | validation: 2.695254278623404]
	TIME [epoch: 8.41 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9509245436742906		[learning rate: 0.0035942]
		[batch 20/20] avg loss: 1.9120373893322118		[learning rate: 0.0035877]
	Learning Rate: 0.0035877
	LOSS [training: 1.9314809665032513 | validation: 2.625585476153792]
	TIME [epoch: 8.39 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8876750266780338		[learning rate: 0.0035812]
		[batch 20/20] avg loss: 1.5913571704512641		[learning rate: 0.0035747]
	Learning Rate: 0.00357468
	LOSS [training: 1.7395160985646492 | validation: 2.5715789342826323]
	TIME [epoch: 8.38 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8416897859700831		[learning rate: 0.0035682]
		[batch 20/20] avg loss: 1.6081808218688338		[learning rate: 0.0035617]
	Learning Rate: 0.0035617
	LOSS [training: 1.7249353039194584 | validation: 2.7314743842847684]
	TIME [epoch: 8.37 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7336307067014638		[learning rate: 0.0035552]
		[batch 20/20] avg loss: 1.7728285735003866		[learning rate: 0.0035488]
	Learning Rate: 0.00354878
	LOSS [training: 1.7532296401009255 | validation: 2.547053348992334]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.687276491626872		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 1.801832730492133		[learning rate: 0.0035359]
	Learning Rate: 0.0035359
	LOSS [training: 1.744554611059503 | validation: 2.5492178061831097]
	TIME [epoch: 8.39 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7814774986433033		[learning rate: 0.0035295]
		[batch 20/20] avg loss: 1.7469172955650856		[learning rate: 0.0035231]
	Learning Rate: 0.00352307
	LOSS [training: 1.7641973971041942 | validation: 2.686498440564765]
	TIME [epoch: 8.38 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5404380236834405		[learning rate: 0.0035167]
		[batch 20/20] avg loss: 1.9622410580470138		[learning rate: 0.0035103]
	Learning Rate: 0.00351028
	LOSS [training: 1.7513395408652273 | validation: 2.669372961162441]
	TIME [epoch: 8.38 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.669892414388665		[learning rate: 0.0035039]
		[batch 20/20] avg loss: 1.728124270449228		[learning rate: 0.0034975]
	Learning Rate: 0.00349754
	LOSS [training: 1.6990083424189464 | validation: 2.5885728408289506]
	TIME [epoch: 8.4 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.754754568674778		[learning rate: 0.0034912]
		[batch 20/20] avg loss: 1.7877431662993502		[learning rate: 0.0034849]
	Learning Rate: 0.00348485
	LOSS [training: 1.771248867487064 | validation: 2.580086528240435]
	TIME [epoch: 8.38 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6830996113189023		[learning rate: 0.0034785]
		[batch 20/20] avg loss: 1.8539482330737997		[learning rate: 0.0034722]
	Learning Rate: 0.0034722
	LOSS [training: 1.7685239221963514 | validation: 2.6316289451602652]
	TIME [epoch: 8.38 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7800059754530793		[learning rate: 0.0034659]
		[batch 20/20] avg loss: 1.8019651985433505		[learning rate: 0.0034596]
	Learning Rate: 0.0034596
	LOSS [training: 1.790985586998215 | validation: 2.6151809117916462]
	TIME [epoch: 8.38 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8168324944933758		[learning rate: 0.0034533]
		[batch 20/20] avg loss: 1.7298197428882394		[learning rate: 0.003447]
	Learning Rate: 0.00344705
	LOSS [training: 1.7733261186908074 | validation: 2.6552323278506345]
	TIME [epoch: 8.38 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6272408766116755		[learning rate: 0.0034408]
		[batch 20/20] avg loss: 1.8522588990485551		[learning rate: 0.0034345]
	Learning Rate: 0.00343454
	LOSS [training: 1.7397498878301154 | validation: 2.8249551083679805]
	TIME [epoch: 8.41 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.72129931440552		[learning rate: 0.0034283]
		[batch 20/20] avg loss: 1.7948130874799308		[learning rate: 0.0034221]
	Learning Rate: 0.00342207
	LOSS [training: 1.7580562009427254 | validation: 2.6792621268665293]
	TIME [epoch: 8.38 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8269773343607223		[learning rate: 0.0034159]
		[batch 20/20] avg loss: 1.8296189695919125		[learning rate: 0.0034097]
	Learning Rate: 0.00340966
	LOSS [training: 1.8282981519763175 | validation: 2.594930649345512]
	TIME [epoch: 8.39 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7465231615450896		[learning rate: 0.0034035]
		[batch 20/20] avg loss: 1.7434834195513862		[learning rate: 0.0033973]
	Learning Rate: 0.00339728
	LOSS [training: 1.7450032905482378 | validation: 2.5440969568447644]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.979785485300151		[learning rate: 0.0033911]
		[batch 20/20] avg loss: 1.5243882168237386		[learning rate: 0.003385]
	Learning Rate: 0.00338495
	LOSS [training: 1.7520868510619447 | validation: 2.562427042947663]
	TIME [epoch: 8.42 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8040223108839624		[learning rate: 0.0033788]
		[batch 20/20] avg loss: 1.7248364626359407		[learning rate: 0.0033727]
	Learning Rate: 0.00337267
	LOSS [training: 1.7644293867599514 | validation: 2.7207667864530625]
	TIME [epoch: 8.38 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7728365124109906		[learning rate: 0.0033665]
		[batch 20/20] avg loss: 1.723622039619731		[learning rate: 0.0033604]
	Learning Rate: 0.00336043
	LOSS [training: 1.7482292760153608 | validation: 2.5552396039832703]
	TIME [epoch: 8.38 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8856121730903403		[learning rate: 0.0033543]
		[batch 20/20] avg loss: 1.632068926134269		[learning rate: 0.0033482]
	Learning Rate: 0.00334823
	LOSS [training: 1.7588405496123047 | validation: 2.787183782155507]
	TIME [epoch: 8.38 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6786773802112904		[learning rate: 0.0033422]
		[batch 20/20] avg loss: 1.801840325407353		[learning rate: 0.0033361]
	Learning Rate: 0.00333608
	LOSS [training: 1.740258852809322 | validation: 2.659709893478891]
	TIME [epoch: 8.41 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8028450109889647		[learning rate: 0.00333]
		[batch 20/20] avg loss: 1.732278296697481		[learning rate: 0.003324]
	Learning Rate: 0.00332398
	LOSS [training: 1.7675616538432226 | validation: 2.5913370203437665]
	TIME [epoch: 8.38 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.872044904178691		[learning rate: 0.0033179]
		[batch 20/20] avg loss: 1.7231850792875125		[learning rate: 0.0033119]
	Learning Rate: 0.00331191
	LOSS [training: 1.7976149917331017 | validation: 2.593161008651543]
	TIME [epoch: 8.38 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6877389930126796		[learning rate: 0.0033059]
		[batch 20/20] avg loss: 1.7898134605697766		[learning rate: 0.0032999]
	Learning Rate: 0.00329989
	LOSS [training: 1.738776226791228 | validation: 2.5975156461535653]
	TIME [epoch: 8.38 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6699608211876218		[learning rate: 0.0032939]
		[batch 20/20] avg loss: 1.8493415912061462		[learning rate: 0.0032879]
	Learning Rate: 0.00328792
	LOSS [training: 1.759651206196884 | validation: 2.7218672852606343]
	TIME [epoch: 8.39 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7370317773272586		[learning rate: 0.0032819]
		[batch 20/20] avg loss: 1.7575973213130047		[learning rate: 0.003276]
	Learning Rate: 0.00327599
	LOSS [training: 1.7473145493201323 | validation: 2.576086779935764]
	TIME [epoch: 8.39 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8137423418099945		[learning rate: 0.00327]
		[batch 20/20] avg loss: 1.6521181703278427		[learning rate: 0.0032641]
	Learning Rate: 0.0032641
	LOSS [training: 1.7329302560689182 | validation: 2.5878710464785706]
	TIME [epoch: 8.38 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6754463399378736		[learning rate: 0.0032582]
		[batch 20/20] avg loss: 1.7911907366916047		[learning rate: 0.0032523]
	Learning Rate: 0.00325225
	LOSS [training: 1.7333185383147391 | validation: 2.6298826385135072]
	TIME [epoch: 8.38 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8140086731985459		[learning rate: 0.0032463]
		[batch 20/20] avg loss: 1.7241701218450423		[learning rate: 0.0032404]
	Learning Rate: 0.00324045
	LOSS [training: 1.7690893975217938 | validation: 2.7111799695769667]
	TIME [epoch: 8.38 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.754068464078377		[learning rate: 0.0032346]
		[batch 20/20] avg loss: 1.6534780129559636		[learning rate: 0.0032287]
	Learning Rate: 0.00322869
	LOSS [training: 1.7037732385171704 | validation: 2.6811830615871477]
	TIME [epoch: 8.4 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8336165305828591		[learning rate: 0.0032228]
		[batch 20/20] avg loss: 1.6890141347853702		[learning rate: 0.003217]
	Learning Rate: 0.00321697
	LOSS [training: 1.7613153326841147 | validation: 2.6770430570763084]
	TIME [epoch: 8.39 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8651011861775015		[learning rate: 0.0032111]
		[batch 20/20] avg loss: 1.6203080962483074		[learning rate: 0.0032053]
	Learning Rate: 0.0032053
	LOSS [training: 1.7427046412129048 | validation: 2.595352575064833]
	TIME [epoch: 8.39 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7742530150705957		[learning rate: 0.0031995]
		[batch 20/20] avg loss: 1.7157995144479106		[learning rate: 0.0031937]
	Learning Rate: 0.00319367
	LOSS [training: 1.7450262647592534 | validation: 2.5559303690489568]
	TIME [epoch: 8.39 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7574050027551344		[learning rate: 0.0031879]
		[batch 20/20] avg loss: 1.6928075307139345		[learning rate: 0.0031821]
	Learning Rate: 0.00318208
	LOSS [training: 1.7251062667345347 | validation: 2.65938147901648]
	TIME [epoch: 8.41 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7471798219077956		[learning rate: 0.0031763]
		[batch 20/20] avg loss: 1.6819106928948167		[learning rate: 0.0031705]
	Learning Rate: 0.00317053
	LOSS [training: 1.7145452574013063 | validation: 2.5253807111248565]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5698453812291064		[learning rate: 0.0031648]
		[batch 20/20] avg loss: 1.925342689882731		[learning rate: 0.003159]
	Learning Rate: 0.00315902
	LOSS [training: 1.747594035555919 | validation: 2.5480433332616212]
	TIME [epoch: 8.41 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7457404531757459		[learning rate: 0.0031533]
		[batch 20/20] avg loss: 1.7728816143344872		[learning rate: 0.0031476]
	Learning Rate: 0.00314756
	LOSS [training: 1.7593110337551168 | validation: 2.6509751684202882]
	TIME [epoch: 8.4 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7322858388434295		[learning rate: 0.0031418]
		[batch 20/20] avg loss: 1.7177860747762002		[learning rate: 0.0031361]
	Learning Rate: 0.00313613
	LOSS [training: 1.7250359568098148 | validation: 2.665787033470476]
	TIME [epoch: 8.43 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8747280662168482		[learning rate: 0.0031304]
		[batch 20/20] avg loss: 1.5941980189508187		[learning rate: 0.0031248]
	Learning Rate: 0.00312475
	LOSS [training: 1.7344630425838339 | validation: 2.5857121533122407]
	TIME [epoch: 8.4 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7453972540103564		[learning rate: 0.0031191]
		[batch 20/20] avg loss: 1.724198226739616		[learning rate: 0.0031134]
	Learning Rate: 0.00311341
	LOSS [training: 1.734797740374986 | validation: 2.58733662605157]
	TIME [epoch: 8.41 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5854958581292142		[learning rate: 0.0031078]
		[batch 20/20] avg loss: 1.8657627107759112		[learning rate: 0.0031021]
	Learning Rate: 0.00310212
	LOSS [training: 1.7256292844525631 | validation: 2.635463501991354]
	TIME [epoch: 8.4 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.637942582372348		[learning rate: 0.0030965]
		[batch 20/20] avg loss: 1.7479451529821202		[learning rate: 0.0030909]
	Learning Rate: 0.00309086
	LOSS [training: 1.6929438676772341 | validation: 2.781944507622654]
	TIME [epoch: 8.4 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9799697779394727		[learning rate: 0.0030852]
		[batch 20/20] avg loss: 1.543394260631253		[learning rate: 0.0030796]
	Learning Rate: 0.00307964
	LOSS [training: 1.7616820192853626 | validation: 2.777861540644388]
	TIME [epoch: 8.43 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6555788689638404		[learning rate: 0.003074]
		[batch 20/20] avg loss: 1.9236090146004667		[learning rate: 0.0030685]
	Learning Rate: 0.00306846
	LOSS [training: 1.7895939417821534 | validation: 2.5791987716930755]
	TIME [epoch: 8.4 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5493037500843976		[learning rate: 0.0030629]
		[batch 20/20] avg loss: 1.8804869114620186		[learning rate: 0.0030573]
	Learning Rate: 0.00305733
	LOSS [training: 1.7148953307732078 | validation: 2.560178545960439]
	TIME [epoch: 8.4 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.675925007248771		[learning rate: 0.0030518]
		[batch 20/20] avg loss: 1.806095426486688		[learning rate: 0.0030462]
	Learning Rate: 0.00304623
	LOSS [training: 1.7410102168677297 | validation: 2.5787225563389624]
	TIME [epoch: 8.4 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6653048989375292		[learning rate: 0.0030407]
		[batch 20/20] avg loss: 1.8079898996620791		[learning rate: 0.0030352]
	Learning Rate: 0.00303518
	LOSS [training: 1.7366473992998042 | validation: 2.587878546801828]
	TIME [epoch: 8.43 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8323231009062042		[learning rate: 0.0030297]
		[batch 20/20] avg loss: 1.6187694137280204		[learning rate: 0.0030242]
	Learning Rate: 0.00302416
	LOSS [training: 1.7255462573171123 | validation: 2.557609269760982]
	TIME [epoch: 8.41 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7403331948709102		[learning rate: 0.0030187]
		[batch 20/20] avg loss: 1.7306141400086361		[learning rate: 0.0030132]
	Learning Rate: 0.00301319
	LOSS [training: 1.7354736674397735 | validation: 2.6529251643227636]
	TIME [epoch: 8.41 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8337997441599083		[learning rate: 0.0030077]
		[batch 20/20] avg loss: 1.6750190131872338		[learning rate: 0.0030023]
	Learning Rate: 0.00300225
	LOSS [training: 1.754409378673571 | validation: 2.872790936123882]
	TIME [epoch: 8.4 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8743192170955905		[learning rate: 0.0029968]
		[batch 20/20] avg loss: 1.668006906462065		[learning rate: 0.0029914]
	Learning Rate: 0.00299136
	LOSS [training: 1.7711630617788274 | validation: 2.554402907988464]
	TIME [epoch: 8.43 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7442414573457377		[learning rate: 0.0029859]
		[batch 20/20] avg loss: 1.7263938824672842		[learning rate: 0.0029805]
	Learning Rate: 0.0029805
	LOSS [training: 1.735317669906511 | validation: 2.7799759350500977]
	TIME [epoch: 8.41 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.740905247227118		[learning rate: 0.0029751]
		[batch 20/20] avg loss: 1.8029924428484982		[learning rate: 0.0029697]
	Learning Rate: 0.00296969
	LOSS [training: 1.7719488450378083 | validation: 2.550738543156528]
	TIME [epoch: 8.4 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7463262500310672		[learning rate: 0.0029643]
		[batch 20/20] avg loss: 1.7226123702511107		[learning rate: 0.0029589]
	Learning Rate: 0.00295891
	LOSS [training: 1.734469310141089 | validation: 2.687511532586126]
	TIME [epoch: 8.4 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8452045783317808		[learning rate: 0.0029535]
		[batch 20/20] avg loss: 1.7010263746537468		[learning rate: 0.0029482]
	Learning Rate: 0.00294817
	LOSS [training: 1.7731154764927635 | validation: 2.5331777834828086]
	TIME [epoch: 8.41 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7441118403990725		[learning rate: 0.0029428]
		[batch 20/20] avg loss: 1.749949737411029		[learning rate: 0.0029375]
	Learning Rate: 0.00293747
	LOSS [training: 1.7470307889050507 | validation: 2.6241429968531196]
	TIME [epoch: 8.43 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6942297300022688		[learning rate: 0.0029321]
		[batch 20/20] avg loss: 1.7628956917406924		[learning rate: 0.0029268]
	Learning Rate: 0.00292681
	LOSS [training: 1.7285627108714805 | validation: 2.623472765339614]
	TIME [epoch: 8.4 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.637830619001741		[learning rate: 0.0029215]
		[batch 20/20] avg loss: 1.8284772397795883		[learning rate: 0.0029162]
	Learning Rate: 0.00291619
	LOSS [training: 1.7331539293906648 | validation: 2.613503402399494]
	TIME [epoch: 8.4 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7617104907607495		[learning rate: 0.0029109]
		[batch 20/20] avg loss: 1.6877551192435447		[learning rate: 0.0029056]
	Learning Rate: 0.00290561
	LOSS [training: 1.7247328050021467 | validation: 2.5365489417034324]
	TIME [epoch: 8.41 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5929937095181024		[learning rate: 0.0029003]
		[batch 20/20] avg loss: 1.8015685104053702		[learning rate: 0.0028951]
	Learning Rate: 0.00289506
	LOSS [training: 1.6972811099617364 | validation: 2.856660176203879]
	TIME [epoch: 8.43 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7969797109861143		[learning rate: 0.0028898]
		[batch 20/20] avg loss: 1.6811050564887569		[learning rate: 0.0028846]
	Learning Rate: 0.00288456
	LOSS [training: 1.7390423837374356 | validation: 2.5705395146119248]
	TIME [epoch: 8.41 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.696210192073884		[learning rate: 0.0028793]
		[batch 20/20] avg loss: 1.7672794080294802		[learning rate: 0.0028741]
	Learning Rate: 0.00287409
	LOSS [training: 1.731744800051682 | validation: 2.644552293455103]
	TIME [epoch: 8.4 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6252097995852413		[learning rate: 0.0028689]
		[batch 20/20] avg loss: 1.8357044867295982		[learning rate: 0.0028637]
	Learning Rate: 0.00286366
	LOSS [training: 1.7304571431574196 | validation: 2.5242159567166977]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6452706910758157		[learning rate: 0.0028585]
		[batch 20/20] avg loss: 1.783059954544186		[learning rate: 0.0028533]
	Learning Rate: 0.00285326
	LOSS [training: 1.7141653228100004 | validation: 2.531285276731899]
	TIME [epoch: 8.42 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6800426017573877		[learning rate: 0.0028481]
		[batch 20/20] avg loss: 1.7183969401053765		[learning rate: 0.0028429]
	Learning Rate: 0.00284291
	LOSS [training: 1.6992197709313825 | validation: 2.5641570964327465]
	TIME [epoch: 8.4 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7998443596302245		[learning rate: 0.0028377]
		[batch 20/20] avg loss: 1.7621799846544763		[learning rate: 0.0028326]
	Learning Rate: 0.00283259
	LOSS [training: 1.7810121721423502 | validation: 2.5247188132887386]
	TIME [epoch: 8.4 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7369951776655714		[learning rate: 0.0028274]
		[batch 20/20] avg loss: 1.657179918454133		[learning rate: 0.0028223]
	Learning Rate: 0.00282231
	LOSS [training: 1.6970875480598522 | validation: 2.6707541608466654]
	TIME [epoch: 8.4 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8537946363491937		[learning rate: 0.0028172]
		[batch 20/20] avg loss: 1.6168522427908676		[learning rate: 0.0028121]
	Learning Rate: 0.00281207
	LOSS [training: 1.7353234395700308 | validation: 2.549949918958866]
	TIME [epoch: 8.42 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9135279846443893		[learning rate: 0.002807]
		[batch 20/20] avg loss: 1.5017491189360601		[learning rate: 0.0028019]
	Learning Rate: 0.00280187
	LOSS [training: 1.7076385517902246 | validation: 2.6315115791842434]
	TIME [epoch: 8.4 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6832887877801113		[learning rate: 0.0027968]
		[batch 20/20] avg loss: 1.758440330118179		[learning rate: 0.0027917]
	Learning Rate: 0.0027917
	LOSS [training: 1.7208645589491454 | validation: 2.5402177309401677]
	TIME [epoch: 8.4 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.859268320347626		[learning rate: 0.0027866]
		[batch 20/20] avg loss: 1.6925274213462167		[learning rate: 0.0027816]
	Learning Rate: 0.00278157
	LOSS [training: 1.775897870846921 | validation: 2.586033719328327]
	TIME [epoch: 8.39 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6469902100864922		[learning rate: 0.0027765]
		[batch 20/20] avg loss: 1.8825456461557288		[learning rate: 0.0027715]
	Learning Rate: 0.00277147
	LOSS [training: 1.7647679281211104 | validation: 2.6173742410928433]
	TIME [epoch: 8.4 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.777589408738255		[learning rate: 0.0027664]
		[batch 20/20] avg loss: 1.8338320768854657		[learning rate: 0.0027614]
	Learning Rate: 0.00276141
	LOSS [training: 1.8057107428118606 | validation: 2.5586355831119785]
	TIME [epoch: 8.42 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.730440216099209		[learning rate: 0.0027564]
		[batch 20/20] avg loss: 1.6778747707637016		[learning rate: 0.0027514]
	Learning Rate: 0.00275139
	LOSS [training: 1.7041574934314554 | validation: 2.647581650360076]
	TIME [epoch: 8.4 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5135863578026931		[learning rate: 0.0027464]
		[batch 20/20] avg loss: 1.922811732271859		[learning rate: 0.0027414]
	Learning Rate: 0.00274141
	LOSS [training: 1.718199045037276 | validation: 2.6236919334427045]
	TIME [epoch: 8.39 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5631500917443308		[learning rate: 0.0027364]
		[batch 20/20] avg loss: 1.9490002820547319		[learning rate: 0.0027315]
	Learning Rate: 0.00273146
	LOSS [training: 1.7560751868995315 | validation: 2.5836937760928107]
	TIME [epoch: 8.4 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.853292210089266		[learning rate: 0.0027265]
		[batch 20/20] avg loss: 1.5850051455863272		[learning rate: 0.0027215]
	Learning Rate: 0.00272155
	LOSS [training: 1.7191486778377965 | validation: 2.5931906917749883]
	TIME [epoch: 8.42 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6182883860245298		[learning rate: 0.0027166]
		[batch 20/20] avg loss: 1.8108375818676117		[learning rate: 0.0027117]
	Learning Rate: 0.00271167
	LOSS [training: 1.7145629839460708 | validation: 2.6464169003601823]
	TIME [epoch: 8.4 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6613397766507023		[learning rate: 0.0027067]
		[batch 20/20] avg loss: 1.768346760170985		[learning rate: 0.0027018]
	Learning Rate: 0.00270183
	LOSS [training: 1.7148432684108434 | validation: 2.6866171114867337]
	TIME [epoch: 8.39 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7842942400855528		[learning rate: 0.0026969]
		[batch 20/20] avg loss: 1.7188635614115797		[learning rate: 0.002692]
	Learning Rate: 0.00269202
	LOSS [training: 1.7515789007485663 | validation: 2.562653721146251]
	TIME [epoch: 8.39 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7084134816131047		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 1.6703264884036066		[learning rate: 0.0026823]
	Learning Rate: 0.00268225
	LOSS [training: 1.6893699850083557 | validation: 2.637296713436661]
	TIME [epoch: 8.42 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.78297920467468		[learning rate: 0.0026774]
		[batch 20/20] avg loss: 1.6526795627712818		[learning rate: 0.0026725]
	Learning Rate: 0.00267252
	LOSS [training: 1.717829383722981 | validation: 2.631892629022606]
	TIME [epoch: 8.41 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6518002702765817		[learning rate: 0.0026677]
		[batch 20/20] avg loss: 1.7518964490068825		[learning rate: 0.0026628]
	Learning Rate: 0.00266282
	LOSS [training: 1.701848359641732 | validation: 2.5469017485670364]
	TIME [epoch: 8.4 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6742284567999228		[learning rate: 0.002658]
		[batch 20/20] avg loss: 1.765541835166291		[learning rate: 0.0026532]
	Learning Rate: 0.00265316
	LOSS [training: 1.7198851459831068 | validation: 2.5623365208094384]
	TIME [epoch: 8.39 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7372098745186701		[learning rate: 0.0026483]
		[batch 20/20] avg loss: 1.6601825656618847		[learning rate: 0.0026435]
	Learning Rate: 0.00264353
	LOSS [training: 1.6986962200902778 | validation: 2.5397189712589574]
	TIME [epoch: 8.39 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.717385370583638		[learning rate: 0.0026387]
		[batch 20/20] avg loss: 1.7017248902424407		[learning rate: 0.0026339]
	Learning Rate: 0.00263394
	LOSS [training: 1.7095551304130392 | validation: 2.782830870555439]
	TIME [epoch: 8.42 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8040161728192605		[learning rate: 0.0026292]
		[batch 20/20] avg loss: 1.6515357266488067		[learning rate: 0.0026244]
	Learning Rate: 0.00262438
	LOSS [training: 1.7277759497340335 | validation: 2.607221056969408]
	TIME [epoch: 8.39 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.614293674852465		[learning rate: 0.0026196]
		[batch 20/20] avg loss: 1.7812831348484501		[learning rate: 0.0026149]
	Learning Rate: 0.00261485
	LOSS [training: 1.697788404850457 | validation: 2.566089092379972]
	TIME [epoch: 8.39 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6060459623673964		[learning rate: 0.0026101]
		[batch 20/20] avg loss: 1.8209375170012472		[learning rate: 0.0026054]
	Learning Rate: 0.00260536
	LOSS [training: 1.7134917396843221 | validation: 2.627142961190253]
	TIME [epoch: 8.39 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5793409393877305		[learning rate: 0.0026006]
		[batch 20/20] avg loss: 1.865063198975966		[learning rate: 0.0025959]
	Learning Rate: 0.00259591
	LOSS [training: 1.7222020691818485 | validation: 2.6069629894012936]
	TIME [epoch: 8.43 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8747079728925495		[learning rate: 0.0025912]
		[batch 20/20] avg loss: 1.5415565620880778		[learning rate: 0.0025865]
	Learning Rate: 0.00258649
	LOSS [training: 1.7081322674903139 | validation: 2.81856404013533]
	TIME [epoch: 8.4 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.761602683117698		[learning rate: 0.0025818]
		[batch 20/20] avg loss: 1.7757171840174526		[learning rate: 0.0025771]
	Learning Rate: 0.0025771
	LOSS [training: 1.7686599335675754 | validation: 2.542005880311102]
	TIME [epoch: 8.4 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8766558793702628		[learning rate: 0.0025724]
		[batch 20/20] avg loss: 1.6797482814715974		[learning rate: 0.0025677]
	Learning Rate: 0.00256775
	LOSS [training: 1.7782020804209302 | validation: 2.593525103515364]
	TIME [epoch: 8.39 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6236311109325279		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 1.7617133261703806		[learning rate: 0.0025584]
	Learning Rate: 0.00255843
	LOSS [training: 1.6926722185514547 | validation: 2.520950046502279]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8042519594340685		[learning rate: 0.0025538]
		[batch 20/20] avg loss: 1.5791781749524243		[learning rate: 0.0025491]
	Learning Rate: 0.00254915
	LOSS [training: 1.6917150671932462 | validation: 2.5716283924257906]
	TIME [epoch: 8.4 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7157585660775236		[learning rate: 0.0025445]
		[batch 20/20] avg loss: 1.7082898373034006		[learning rate: 0.0025399]
	Learning Rate: 0.0025399
	LOSS [training: 1.7120242016904623 | validation: 2.651813672598599]
	TIME [epoch: 8.4 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6857345948985802		[learning rate: 0.0025353]
		[batch 20/20] avg loss: 1.7629167123391831		[learning rate: 0.0025307]
	Learning Rate: 0.00253068
	LOSS [training: 1.7243256536188816 | validation: 2.5494752677223778]
	TIME [epoch: 8.39 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7958922140339801		[learning rate: 0.0025261]
		[batch 20/20] avg loss: 1.5502552290722875		[learning rate: 0.0025215]
	Learning Rate: 0.00252149
	LOSS [training: 1.6730737215531337 | validation: 2.7435636171673776]
	TIME [epoch: 8.38 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7240304734735132		[learning rate: 0.0025169]
		[batch 20/20] avg loss: 1.7395051485092345		[learning rate: 0.0025123]
	Learning Rate: 0.00251234
	LOSS [training: 1.7317678109913743 | validation: 2.5535487700355457]
	TIME [epoch: 8.4 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7587076475417653		[learning rate: 0.0025078]
		[batch 20/20] avg loss: 1.6962327183157329		[learning rate: 0.0025032]
	Learning Rate: 0.00250323
	LOSS [training: 1.727470182928749 | validation: 2.5437667459435636]
	TIME [epoch: 8.39 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6826154149458443		[learning rate: 0.0024987]
		[batch 20/20] avg loss: 1.6982260202925883		[learning rate: 0.0024941]
	Learning Rate: 0.00249414
	LOSS [training: 1.6904207176192163 | validation: 2.606190819439035]
	TIME [epoch: 8.39 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6945651876783567		[learning rate: 0.0024896]
		[batch 20/20] avg loss: 1.6816620509308944		[learning rate: 0.0024851]
	Learning Rate: 0.00248509
	LOSS [training: 1.6881136193046256 | validation: 2.598807798350881]
	TIME [epoch: 8.39 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7118867229899533		[learning rate: 0.0024806]
		[batch 20/20] avg loss: 1.7144714035130069		[learning rate: 0.0024761]
	Learning Rate: 0.00247607
	LOSS [training: 1.71317906325148 | validation: 2.5697038248585127]
	TIME [epoch: 8.41 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7467087768089182		[learning rate: 0.0024716]
		[batch 20/20] avg loss: 1.6699092730936553		[learning rate: 0.0024671]
	Learning Rate: 0.00246709
	LOSS [training: 1.7083090249512867 | validation: 2.5676695720041827]
	TIME [epoch: 8.39 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6096496997032688		[learning rate: 0.0024626]
		[batch 20/20] avg loss: 1.8227493558501269		[learning rate: 0.0024581]
	Learning Rate: 0.00245813
	LOSS [training: 1.7161995277766984 | validation: 2.6080048478321833]
	TIME [epoch: 8.37 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8239325518040566		[learning rate: 0.0024537]
		[batch 20/20] avg loss: 1.5825207304423516		[learning rate: 0.0024492]
	Learning Rate: 0.00244921
	LOSS [training: 1.7032266411232044 | validation: 2.575970129042473]
	TIME [epoch: 8.39 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7048786009146717		[learning rate: 0.0024448]
		[batch 20/20] avg loss: 1.7031412048348948		[learning rate: 0.0024403]
	Learning Rate: 0.00244032
	LOSS [training: 1.7040099028747833 | validation: 2.5487710331309756]
	TIME [epoch: 8.41 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6265148995919105		[learning rate: 0.0024359]
		[batch 20/20] avg loss: 1.7935460768510167		[learning rate: 0.0024315]
	Learning Rate: 0.00243147
	LOSS [training: 1.710030488221464 | validation: 2.530926849414935]
	TIME [epoch: 8.39 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6415255943732547		[learning rate: 0.0024271]
		[batch 20/20] avg loss: 1.8831430940621836		[learning rate: 0.0024226]
	Learning Rate: 0.00242264
	LOSS [training: 1.762334344217719 | validation: 2.5299171964639444]
	TIME [epoch: 8.39 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9357031700451675		[learning rate: 0.0024182]
		[batch 20/20] avg loss: 1.4569914874949426		[learning rate: 0.0024139]
	Learning Rate: 0.00241385
	LOSS [training: 1.6963473287700552 | validation: 2.7254807254628752]
	TIME [epoch: 8.38 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6686271918725613		[learning rate: 0.0024095]
		[batch 20/20] avg loss: 1.7413410334792707		[learning rate: 0.0024051]
	Learning Rate: 0.00240509
	LOSS [training: 1.7049841126759158 | validation: 2.768451702283018]
	TIME [epoch: 8.4 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7373671631687995		[learning rate: 0.0024007]
		[batch 20/20] avg loss: 1.7417703800579827		[learning rate: 0.0023964]
	Learning Rate: 0.00239636
	LOSS [training: 1.7395687716133914 | validation: 4.027039681722468]
	TIME [epoch: 8.4 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0287090693957675		[learning rate: 0.002392]
		[batch 20/20] avg loss: 1.6609131184438009		[learning rate: 0.0023877]
	Learning Rate: 0.00238767
	LOSS [training: 1.844811093919784 | validation: 2.530334981749196]
	TIME [epoch: 8.39 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6844364348018068		[learning rate: 0.0023833]
		[batch 20/20] avg loss: 1.725308051837618		[learning rate: 0.002379]
	Learning Rate: 0.002379
	LOSS [training: 1.7048722433197123 | validation: 2.5325073453906013]
	TIME [epoch: 8.39 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6608556750987344		[learning rate: 0.0023747]
		[batch 20/20] avg loss: 1.730588969419291		[learning rate: 0.0023704]
	Learning Rate: 0.00237037
	LOSS [training: 1.695722322259013 | validation: 2.573913679272491]
	TIME [epoch: 8.39 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5613300276647546		[learning rate: 0.0023661]
		[batch 20/20] avg loss: 1.8737516710994904		[learning rate: 0.0023618]
	Learning Rate: 0.00236177
	LOSS [training: 1.7175408493821227 | validation: 2.562421553299386]
	TIME [epoch: 8.42 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8591214474431634		[learning rate: 0.0023575]
		[batch 20/20] avg loss: 1.58925490024239		[learning rate: 0.0023532]
	Learning Rate: 0.00235319
	LOSS [training: 1.7241881738427765 | validation: 2.903065222777654]
	TIME [epoch: 8.4 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6757627543754725		[learning rate: 0.0023489]
		[batch 20/20] avg loss: 1.759820012322488		[learning rate: 0.0023447]
	Learning Rate: 0.00234465
	LOSS [training: 1.71779138334898 | validation: 2.533121539319912]
	TIME [epoch: 8.39 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6529661538283862		[learning rate: 0.0023404]
		[batch 20/20] avg loss: 1.7130624938139931		[learning rate: 0.0023361]
	Learning Rate: 0.00233615
	LOSS [training: 1.6830143238211897 | validation: 2.545596831316374]
	TIME [epoch: 8.39 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.613934623984272		[learning rate: 0.0023319]
		[batch 20/20] avg loss: 1.9162257639798412		[learning rate: 0.0023277]
	Learning Rate: 0.00232767
	LOSS [training: 1.7650801939820564 | validation: 2.565710330472452]
	TIME [epoch: 8.42 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7301317649920858		[learning rate: 0.0023234]
		[batch 20/20] avg loss: 1.640116637237145		[learning rate: 0.0023192]
	Learning Rate: 0.00231922
	LOSS [training: 1.6851242011146148 | validation: 2.6153451157715635]
	TIME [epoch: 8.39 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.709555990822097		[learning rate: 0.002315]
		[batch 20/20] avg loss: 1.649252866506129		[learning rate: 0.0023108]
	Learning Rate: 0.0023108
	LOSS [training: 1.6794044286641125 | validation: 2.5176543424606486]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_503.pth
	Model improved!!!
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6840631104826596		[learning rate: 0.0023066]
		[batch 20/20] avg loss: 1.6677375230892948		[learning rate: 0.0023024]
	Learning Rate: 0.00230242
	LOSS [training: 1.6759003167859774 | validation: 2.5384614538524173]
	TIME [epoch: 8.38 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7723062421568556		[learning rate: 0.0022982]
		[batch 20/20] avg loss: 1.6383043668364732		[learning rate: 0.0022941]
	Learning Rate: 0.00229406
	LOSS [training: 1.7053053044966642 | validation: 2.5556897958326403]
	TIME [epoch: 8.41 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7285379515196375		[learning rate: 0.0022899]
		[batch 20/20] avg loss: 1.6867625257370897		[learning rate: 0.0022857]
	Learning Rate: 0.00228574
	LOSS [training: 1.7076502386283636 | validation: 2.6658632918289715]
	TIME [epoch: 8.39 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6697895720911844		[learning rate: 0.0022816]
		[batch 20/20] avg loss: 1.7478853004957222		[learning rate: 0.0022774]
	Learning Rate: 0.00227744
	LOSS [training: 1.708837436293453 | validation: 2.5692550717651796]
	TIME [epoch: 8.38 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5509761527089077		[learning rate: 0.0022733]
		[batch 20/20] avg loss: 1.8900393116572864		[learning rate: 0.0022692]
	Learning Rate: 0.00226918
	LOSS [training: 1.720507732183097 | validation: 2.52128207596331]
	TIME [epoch: 8.39 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6381249306280203		[learning rate: 0.0022651]
		[batch 20/20] avg loss: 1.7511185498997275		[learning rate: 0.0022609]
	Learning Rate: 0.00226094
	LOSS [training: 1.6946217402638737 | validation: 2.5315848625029487]
	TIME [epoch: 8.38 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8293475549196039		[learning rate: 0.0022568]
		[batch 20/20] avg loss: 1.5358446550899518		[learning rate: 0.0022527]
	Learning Rate: 0.00225274
	LOSS [training: 1.6825961050047777 | validation: 2.572295278062583]
	TIME [epoch: 8.4 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6660453675030467		[learning rate: 0.0022486]
		[batch 20/20] avg loss: 1.8819526493304664		[learning rate: 0.0022446]
	Learning Rate: 0.00224456
	LOSS [training: 1.7739990084167565 | validation: 2.584208086798959]
	TIME [epoch: 8.37 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7426615431288213		[learning rate: 0.0022405]
		[batch 20/20] avg loss: 1.889500401982429		[learning rate: 0.0022364]
	Learning Rate: 0.00223642
	LOSS [training: 1.8160809725556248 | validation: 2.569919940522647]
	TIME [epoch: 8.38 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8027598110879555		[learning rate: 0.0022324]
		[batch 20/20] avg loss: 1.6564460898788753		[learning rate: 0.0022283]
	Learning Rate: 0.0022283
	LOSS [training: 1.7296029504834152 | validation: 2.5347419491071634]
	TIME [epoch: 8.38 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7455104117784408		[learning rate: 0.0022243]
		[batch 20/20] avg loss: 1.6521635392544787		[learning rate: 0.0022202]
	Learning Rate: 0.00222021
	LOSS [training: 1.6988369755164594 | validation: 2.7130776609157126]
	TIME [epoch: 8.41 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7221428828997152		[learning rate: 0.0022162]
		[batch 20/20] avg loss: 1.6922029964634686		[learning rate: 0.0022122]
	Learning Rate: 0.00221216
	LOSS [training: 1.7071729396815918 | validation: 2.7655693666224477]
	TIME [epoch: 8.4 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6111874929733143		[learning rate: 0.0022081]
		[batch 20/20] avg loss: 1.7899959550754379		[learning rate: 0.0022041]
	Learning Rate: 0.00220413
	LOSS [training: 1.7005917240243762 | validation: 2.5311509213506427]
	TIME [epoch: 8.38 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6752879859618646		[learning rate: 0.0022001]
		[batch 20/20] avg loss: 1.741498946036403		[learning rate: 0.0021961]
	Learning Rate: 0.00219613
	LOSS [training: 1.7083934659991336 | validation: 2.593320287230986]
	TIME [epoch: 8.39 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6992231800677167		[learning rate: 0.0021921]
		[batch 20/20] avg loss: 1.649171761820321		[learning rate: 0.0021882]
	Learning Rate: 0.00218816
	LOSS [training: 1.6741974709440188 | validation: 2.5818677347275854]
	TIME [epoch: 8.41 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6626919053216196		[learning rate: 0.0021842]
		[batch 20/20] avg loss: 1.693325425697059		[learning rate: 0.0021802]
	Learning Rate: 0.00218022
	LOSS [training: 1.6780086655093391 | validation: 2.5131825652264266]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6457601354307207		[learning rate: 0.0021763]
		[batch 20/20] avg loss: 1.745338562611702		[learning rate: 0.0021723]
	Learning Rate: 0.00217231
	LOSS [training: 1.6955493490212112 | validation: 2.5482818192816494]
	TIME [epoch: 8.41 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6521836621279422		[learning rate: 0.0021684]
		[batch 20/20] avg loss: 1.7326355591891094		[learning rate: 0.0021644]
	Learning Rate: 0.00216442
	LOSS [training: 1.6924096106585254 | validation: 2.5717887844084464]
	TIME [epoch: 8.39 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5062957938773693		[learning rate: 0.0021605]
		[batch 20/20] avg loss: 1.8692519527811968		[learning rate: 0.0021566]
	Learning Rate: 0.00215657
	LOSS [training: 1.6877738733292833 | validation: 2.522436950431465]
	TIME [epoch: 8.4 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.559219430249445		[learning rate: 0.0021527]
		[batch 20/20] avg loss: 1.8238781496797771		[learning rate: 0.0021487]
	Learning Rate: 0.00214874
	LOSS [training: 1.6915487899646113 | validation: 2.52624964194923]
	TIME [epoch: 8.42 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.747206150910095		[learning rate: 0.0021448]
		[batch 20/20] avg loss: 1.7202966377767708		[learning rate: 0.0021409]
	Learning Rate: 0.00214094
	LOSS [training: 1.7337513943434328 | validation: 2.5502254240771536]
	TIME [epoch: 8.38 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5055764539933358		[learning rate: 0.0021371]
		[batch 20/20] avg loss: 1.8564563036378126		[learning rate: 0.0021332]
	Learning Rate: 0.00213317
	LOSS [training: 1.681016378815574 | validation: 2.688016690675112]
	TIME [epoch: 8.4 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8761098008908703		[learning rate: 0.0021293]
		[batch 20/20] avg loss: 1.5248064928680083		[learning rate: 0.0021254]
	Learning Rate: 0.00212543
	LOSS [training: 1.7004581468794395 | validation: 2.5607154458004855]
	TIME [epoch: 8.4 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6186829680333013		[learning rate: 0.0021216]
		[batch 20/20] avg loss: 1.8096623114164043		[learning rate: 0.0021177]
	Learning Rate: 0.00211772
	LOSS [training: 1.714172639724853 | validation: 2.5657977535044854]
	TIME [epoch: 8.4 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7671590758881304		[learning rate: 0.0021139]
		[batch 20/20] avg loss: 1.620788994198173		[learning rate: 0.00211]
	Learning Rate: 0.00211003
	LOSS [training: 1.6939740350431518 | validation: 2.526701777759018]
	TIME [epoch: 8.38 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8006387635088543		[learning rate: 0.0021062]
		[batch 20/20] avg loss: 1.6106149116672046		[learning rate: 0.0021024]
	Learning Rate: 0.00210238
	LOSS [training: 1.7056268375880297 | validation: 2.5708233493346127]
	TIME [epoch: 8.38 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8819839367570297		[learning rate: 0.0020986]
		[batch 20/20] avg loss: 1.4920327398019988		[learning rate: 0.0020947]
	Learning Rate: 0.00209475
	LOSS [training: 1.6870083382795145 | validation: 2.563166132229978]
	TIME [epoch: 8.38 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7493209159468908		[learning rate: 0.0020909]
		[batch 20/20] avg loss: 1.6092689998499838		[learning rate: 0.0020871]
	Learning Rate: 0.00208714
	LOSS [training: 1.6792949578984373 | validation: 2.5697746951798326]
	TIME [epoch: 8.39 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7521694350325892		[learning rate: 0.0020834]
		[batch 20/20] avg loss: 1.6163743150579026		[learning rate: 0.0020796]
	Learning Rate: 0.00207957
	LOSS [training: 1.6842718750452454 | validation: 2.805601369715891]
	TIME [epoch: 8.38 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7197326669123154		[learning rate: 0.0020758]
		[batch 20/20] avg loss: 1.6776352760454103		[learning rate: 0.002072]
	Learning Rate: 0.00207202
	LOSS [training: 1.698683971478863 | validation: 2.52236985752291]
	TIME [epoch: 8.37 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5561053243271552		[learning rate: 0.0020683]
		[batch 20/20] avg loss: 1.8429948097842215		[learning rate: 0.0020645]
	Learning Rate: 0.0020645
	LOSS [training: 1.6995500670556882 | validation: 2.6833609533709737]
	TIME [epoch: 8.37 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7296550694350619		[learning rate: 0.0020608]
		[batch 20/20] avg loss: 1.665881315384023		[learning rate: 0.002057]
	Learning Rate: 0.00205701
	LOSS [training: 1.6977681924095425 | validation: 2.517513093210344]
	TIME [epoch: 8.37 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5563788793374138		[learning rate: 0.0020533]
		[batch 20/20] avg loss: 1.8201954848090096		[learning rate: 0.0020495]
	Learning Rate: 0.00204955
	LOSS [training: 1.6882871820732117 | validation: 2.6373049645683535]
	TIME [epoch: 8.4 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.787800947468424		[learning rate: 0.0020458]
		[batch 20/20] avg loss: 1.570762661606212		[learning rate: 0.0020421]
	Learning Rate: 0.00204211
	LOSS [training: 1.6792818045373181 | validation: 2.540712525158234]
	TIME [epoch: 8.38 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7320354469540589		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 1.6327247691863014		[learning rate: 0.0020347]
	Learning Rate: 0.0020347
	LOSS [training: 1.68238010807018 | validation: 2.537054466172862]
	TIME [epoch: 8.37 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7265386905675328		[learning rate: 0.002031]
		[batch 20/20] avg loss: 1.695296523410163		[learning rate: 0.0020273]
	Learning Rate: 0.00202731
	LOSS [training: 1.7109176069888477 | validation: 2.621967752256565]
	TIME [epoch: 8.37 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3427113526243026		[learning rate: 0.0020236]
		[batch 20/20] avg loss: 2.0168994055522704		[learning rate: 0.00202]
	Learning Rate: 0.00201996
	LOSS [training: 1.6798053790882865 | validation: 2.5745026755946636]
	TIME [epoch: 8.4 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8396518545272245		[learning rate: 0.0020163]
		[batch 20/20] avg loss: 1.5958521991095052		[learning rate: 0.0020126]
	Learning Rate: 0.00201263
	LOSS [training: 1.7177520268183648 | validation: 2.632192677026416]
	TIME [epoch: 8.37 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9221756784714337		[learning rate: 0.002009]
		[batch 20/20] avg loss: 1.4615905883124722		[learning rate: 0.0020053]
	Learning Rate: 0.00200532
	LOSS [training: 1.691883133391953 | validation: 2.999111423220568]
	TIME [epoch: 8.37 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7172191041902358		[learning rate: 0.0020017]
		[batch 20/20] avg loss: 1.7187970279358626		[learning rate: 0.001998]
	Learning Rate: 0.00199805
	LOSS [training: 1.7180080660630495 | validation: 2.5899366592647173]
	TIME [epoch: 8.37 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6599379663844473		[learning rate: 0.0019944]
		[batch 20/20] avg loss: 1.6883695856403995		[learning rate: 0.0019908]
	Learning Rate: 0.00199079
	LOSS [training: 1.6741537760124234 | validation: 2.586416976893876]
	TIME [epoch: 8.4 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6518195899399175		[learning rate: 0.0019872]
		[batch 20/20] avg loss: 1.6877124400267292		[learning rate: 0.0019836]
	Learning Rate: 0.00198357
	LOSS [training: 1.6697660149833233 | validation: 2.6594296032363696]
	TIME [epoch: 8.38 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.673116177287259		[learning rate: 0.00198]
		[batch 20/20] avg loss: 1.707150371520512		[learning rate: 0.0019764]
	Learning Rate: 0.00197637
	LOSS [training: 1.6901332744038853 | validation: 2.5710414663799273]
	TIME [epoch: 8.37 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6782062450039543		[learning rate: 0.0019728]
		[batch 20/20] avg loss: 1.6545475437079993		[learning rate: 0.0019692]
	Learning Rate: 0.0019692
	LOSS [training: 1.666376894355977 | validation: 2.630017681731746]
	TIME [epoch: 8.37 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7484115715490227		[learning rate: 0.0019656]
		[batch 20/20] avg loss: 1.6230512759564903		[learning rate: 0.0019621]
	Learning Rate: 0.00196205
	LOSS [training: 1.685731423752756 | validation: 2.635061118932221]
	TIME [epoch: 8.37 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7277082836602424		[learning rate: 0.0019585]
		[batch 20/20] avg loss: 1.6246960044498593		[learning rate: 0.0019549]
	Learning Rate: 0.00195493
	LOSS [training: 1.6762021440550505 | validation: 2.5269286132202122]
	TIME [epoch: 8.4 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6852581601890413		[learning rate: 0.0019514]
		[batch 20/20] avg loss: 1.7713887146372471		[learning rate: 0.0019478]
	Learning Rate: 0.00194784
	LOSS [training: 1.7283234374131442 | validation: 2.6861715513165554]
	TIME [epoch: 8.37 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7080107164806937		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 1.7141249921202237		[learning rate: 0.0019408]
	Learning Rate: 0.00194077
	LOSS [training: 1.7110678543004585 | validation: 2.526165097827686]
	TIME [epoch: 8.38 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6447844903775817		[learning rate: 0.0019372]
		[batch 20/20] avg loss: 1.6989659944789035		[learning rate: 0.0019337]
	Learning Rate: 0.00193373
	LOSS [training: 1.6718752424282428 | validation: 2.5709805220947684]
	TIME [epoch: 8.37 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7875825650779116		[learning rate: 0.0019302]
		[batch 20/20] avg loss: 1.6568566824485536		[learning rate: 0.0019267]
	Learning Rate: 0.00192671
	LOSS [training: 1.7222196237632332 | validation: 2.7617380902189335]
	TIME [epoch: 8.4 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6531751434799618		[learning rate: 0.0019232]
		[batch 20/20] avg loss: 1.7354148908351852		[learning rate: 0.0019197]
	Learning Rate: 0.00191972
	LOSS [training: 1.694295017157574 | validation: 2.555633045969177]
	TIME [epoch: 8.38 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5466879025607745		[learning rate: 0.0019162]
		[batch 20/20] avg loss: 1.8313974758743843		[learning rate: 0.0019127]
	Learning Rate: 0.00191275
	LOSS [training: 1.6890426892175796 | validation: 2.5394735191229483]
	TIME [epoch: 8.39 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6199901181358698		[learning rate: 0.0019093]
		[batch 20/20] avg loss: 1.694754997334377		[learning rate: 0.0019058]
	Learning Rate: 0.00190581
	LOSS [training: 1.6573725577351233 | validation: 2.600695839627841]
	TIME [epoch: 8.38 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7409518522922678		[learning rate: 0.0019023]
		[batch 20/20] avg loss: 1.6240798087775066		[learning rate: 0.0018989]
	Learning Rate: 0.00189889
	LOSS [training: 1.6825158305348868 | validation: 2.5812444030005235]
	TIME [epoch: 8.4 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.765422189331997		[learning rate: 0.0018954]
		[batch 20/20] avg loss: 1.6079200252953363		[learning rate: 0.001892]
	Learning Rate: 0.001892
	LOSS [training: 1.6866711073136664 | validation: 2.5300404116163766]
	TIME [epoch: 8.39 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.580199516479377		[learning rate: 0.0018886]
		[batch 20/20] avg loss: 1.7321288448321124		[learning rate: 0.0018851]
	Learning Rate: 0.00188513
	LOSS [training: 1.6561641806557446 | validation: 2.5448136917017976]
	TIME [epoch: 8.38 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6902342724183363		[learning rate: 0.0018817]
		[batch 20/20] avg loss: 1.643216725407655		[learning rate: 0.0018783]
	Learning Rate: 0.00187829
	LOSS [training: 1.6667254989129954 | validation: 2.7877597432825985]
	TIME [epoch: 8.38 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6782417827120277		[learning rate: 0.0018749]
		[batch 20/20] avg loss: 1.6974835387007468		[learning rate: 0.0018715]
	Learning Rate: 0.00187148
	LOSS [training: 1.6878626607063876 | validation: 2.5805507120711075]
	TIME [epoch: 8.38 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6448063524302516		[learning rate: 0.0018681]
		[batch 20/20] avg loss: 1.7017976060767146		[learning rate: 0.0018647]
	Learning Rate: 0.00186468
	LOSS [training: 1.6733019792534833 | validation: 2.568065523957002]
	TIME [epoch: 8.4 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7479509617866598		[learning rate: 0.0018613]
		[batch 20/20] avg loss: 1.5902767223639052		[learning rate: 0.0018579]
	Learning Rate: 0.00185792
	LOSS [training: 1.6691138420752822 | validation: 2.5627486704894427]
	TIME [epoch: 8.38 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7647857395022168		[learning rate: 0.0018545]
		[batch 20/20] avg loss: 1.59529789610417		[learning rate: 0.0018512]
	Learning Rate: 0.00185117
	LOSS [training: 1.6800418178031933 | validation: 2.5066974699360625]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6770726614342604		[learning rate: 0.0018478]
		[batch 20/20] avg loss: 1.6727188750608395		[learning rate: 0.0018445]
	Learning Rate: 0.00184446
	LOSS [training: 1.6748957682475503 | validation: 2.6029023480142337]
	TIME [epoch: 8.39 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7704759755456998		[learning rate: 0.0018411]
		[batch 20/20] avg loss: 1.611940545101073		[learning rate: 0.0018378]
	Learning Rate: 0.00183776
	LOSS [training: 1.6912082603233862 | validation: 2.5818767443464927]
	TIME [epoch: 8.41 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6096764110715216		[learning rate: 0.0018344]
		[batch 20/20] avg loss: 1.7268446310289736		[learning rate: 0.0018311]
	Learning Rate: 0.00183109
	LOSS [training: 1.6682605210502472 | validation: 2.5886471685769727]
	TIME [epoch: 8.37 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7047280226314339		[learning rate: 0.0018278]
		[batch 20/20] avg loss: 1.6197902460429223		[learning rate: 0.0018244]
	Learning Rate: 0.00182445
	LOSS [training: 1.662259134337178 | validation: 2.518689887695542]
	TIME [epoch: 8.38 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.658388239484563		[learning rate: 0.0018211]
		[batch 20/20] avg loss: 1.7106789944036316		[learning rate: 0.0018178]
	Learning Rate: 0.00181783
	LOSS [training: 1.6845336169440972 | validation: 2.5449258094536407]
	TIME [epoch: 8.39 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.733144263434855		[learning rate: 0.0018145]
		[batch 20/20] avg loss: 1.62412207435865		[learning rate: 0.0018112]
	Learning Rate: 0.00181123
	LOSS [training: 1.6786331688967526 | validation: 2.724608944427934]
	TIME [epoch: 8.41 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6880625344914884		[learning rate: 0.0018079]
		[batch 20/20] avg loss: 1.7224206284852088		[learning rate: 0.0018047]
	Learning Rate: 0.00180466
	LOSS [training: 1.7052415814883486 | validation: 2.606824910851743]
	TIME [epoch: 8.39 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.713808266515024		[learning rate: 0.0018014]
		[batch 20/20] avg loss: 1.6318253429107983		[learning rate: 0.0017981]
	Learning Rate: 0.00179811
	LOSS [training: 1.6728168047129113 | validation: 2.5338552360047757]
	TIME [epoch: 8.39 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7133862367604291		[learning rate: 0.0017948]
		[batch 20/20] avg loss: 1.6913356553182797		[learning rate: 0.0017916]
	Learning Rate: 0.00179158
	LOSS [training: 1.7023609460393545 | validation: 2.518273010974535]
	TIME [epoch: 8.37 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6712943677035175		[learning rate: 0.0017883]
		[batch 20/20] avg loss: 1.660895625568973		[learning rate: 0.0017851]
	Learning Rate: 0.00178508
	LOSS [training: 1.6660949966362455 | validation: 2.584048858925405]
	TIME [epoch: 8.39 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7127835497879702		[learning rate: 0.0017818]
		[batch 20/20] avg loss: 1.69259011821249		[learning rate: 0.0017786]
	Learning Rate: 0.0017786
	LOSS [training: 1.7026868340002301 | validation: 2.8104526555745277]
	TIME [epoch: 8.39 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6796986735702206		[learning rate: 0.0017754]
		[batch 20/20] avg loss: 1.8861866726871348		[learning rate: 0.0017721]
	Learning Rate: 0.00177215
	LOSS [training: 1.7829426731286773 | validation: 2.6990946435816108]
	TIME [epoch: 8.38 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6291959320141598		[learning rate: 0.0017689]
		[batch 20/20] avg loss: 1.738902307303714		[learning rate: 0.0017657]
	Learning Rate: 0.00176572
	LOSS [training: 1.6840491196589364 | validation: 2.575532361476227]
	TIME [epoch: 8.38 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6578444692784593		[learning rate: 0.0017625]
		[batch 20/20] avg loss: 1.6853557543597488		[learning rate: 0.0017593]
	Learning Rate: 0.00175931
	LOSS [training: 1.6716001118191042 | validation: 2.705310670863785]
	TIME [epoch: 8.38 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7560046199695052		[learning rate: 0.0017561]
		[batch 20/20] avg loss: 1.6385338250030557		[learning rate: 0.0017529]
	Learning Rate: 0.00175292
	LOSS [training: 1.69726922248628 | validation: 2.5509746719185955]
	TIME [epoch: 8.41 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4996159517743786		[learning rate: 0.0017497]
		[batch 20/20] avg loss: 1.8506129005521856		[learning rate: 0.0017466]
	Learning Rate: 0.00174656
	LOSS [training: 1.6751144261632824 | validation: 2.5164892683952145]
	TIME [epoch: 8.39 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7362760047706438		[learning rate: 0.0017434]
		[batch 20/20] avg loss: 1.5834489541862957		[learning rate: 0.0017402]
	Learning Rate: 0.00174022
	LOSS [training: 1.6598624794784698 | validation: 2.5509999469694997]
	TIME [epoch: 8.38 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.578522036534244		[learning rate: 0.0017371]
		[batch 20/20] avg loss: 1.7860296429866538		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 1.682275839760449 | validation: 2.5359446174646036]
	TIME [epoch: 8.38 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5754338064239255		[learning rate: 0.0017308]
		[batch 20/20] avg loss: 1.7603597674446856		[learning rate: 0.0017276]
	Learning Rate: 0.00172762
	LOSS [training: 1.6678967869343055 | validation: 2.60144059147174]
	TIME [epoch: 8.41 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6450864741858253		[learning rate: 0.0017245]
		[batch 20/20] avg loss: 1.7308891954253816		[learning rate: 0.0017213]
	Learning Rate: 0.00172135
	LOSS [training: 1.6879878348056032 | validation: 2.5499548274142296]
	TIME [epoch: 8.39 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5554695992233327		[learning rate: 0.0017182]
		[batch 20/20] avg loss: 1.8394794636135523		[learning rate: 0.0017151]
	Learning Rate: 0.0017151
	LOSS [training: 1.6974745314184425 | validation: 2.640482956495589]
	TIME [epoch: 8.39 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6690444896011116		[learning rate: 0.001712]
		[batch 20/20] avg loss: 1.6833007515552396		[learning rate: 0.0017089]
	Learning Rate: 0.00170888
	LOSS [training: 1.676172620578176 | validation: 2.509978504668556]
	TIME [epoch: 8.39 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7314334301172216		[learning rate: 0.0017058]
		[batch 20/20] avg loss: 1.6379355487120395		[learning rate: 0.0017027]
	Learning Rate: 0.00170267
	LOSS [training: 1.6846844894146304 | validation: 2.5206908999999973]
	TIME [epoch: 8.42 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6117858159745118		[learning rate: 0.0016996]
		[batch 20/20] avg loss: 1.7238589400037891		[learning rate: 0.0016965]
	Learning Rate: 0.0016965
	LOSS [training: 1.6678223779891503 | validation: 2.5198537125131244]
	TIME [epoch: 8.4 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.691252506854585		[learning rate: 0.0016934]
		[batch 20/20] avg loss: 1.6413934246055633		[learning rate: 0.0016903]
	Learning Rate: 0.00169034
	LOSS [training: 1.666322965730074 | validation: 2.5234038051821654]
	TIME [epoch: 8.38 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7436215620254973		[learning rate: 0.0016873]
		[batch 20/20] avg loss: 1.6063054832770547		[learning rate: 0.0016842]
	Learning Rate: 0.0016842
	LOSS [training: 1.6749635226512762 | validation: 2.5282451218667283]
	TIME [epoch: 8.39 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4778312706005052		[learning rate: 0.0016811]
		[batch 20/20] avg loss: 1.839023884087992		[learning rate: 0.0016781]
	Learning Rate: 0.00167809
	LOSS [training: 1.6584275773442485 | validation: 2.545269429322258]
	TIME [epoch: 8.39 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6396933415651724		[learning rate: 0.001675]
		[batch 20/20] avg loss: 1.69492757387559		[learning rate: 0.001672]
	Learning Rate: 0.001672
	LOSS [training: 1.6673104577203812 | validation: 2.524366219313729]
	TIME [epoch: 8.41 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7301566839340055		[learning rate: 0.001669]
		[batch 20/20] avg loss: 1.6278377500203303		[learning rate: 0.0016659]
	Learning Rate: 0.00166593
	LOSS [training: 1.6789972169771676 | validation: 2.574826735454703]
	TIME [epoch: 8.39 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.708850004427298		[learning rate: 0.0016629]
		[batch 20/20] avg loss: 1.6633486491583198		[learning rate: 0.0016599]
	Learning Rate: 0.00165989
	LOSS [training: 1.686099326792809 | validation: 2.511704622371706]
	TIME [epoch: 8.38 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6515326617831778		[learning rate: 0.0016569]
		[batch 20/20] avg loss: 1.7445969460404824		[learning rate: 0.0016539]
	Learning Rate: 0.00165387
	LOSS [training: 1.69806480391183 | validation: 2.525488970013603]
	TIME [epoch: 8.39 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7254839662079902		[learning rate: 0.0016509]
		[batch 20/20] avg loss: 1.622249575407698		[learning rate: 0.0016479]
	Learning Rate: 0.00164786
	LOSS [training: 1.6738667708078439 | validation: 2.561819937595191]
	TIME [epoch: 8.41 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.607488098121997		[learning rate: 0.0016449]
		[batch 20/20] avg loss: 1.7657976495395051		[learning rate: 0.0016419]
	Learning Rate: 0.00164188
	LOSS [training: 1.6866428738307513 | validation: 2.544053096996341]
	TIME [epoch: 8.39 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7601333895491778		[learning rate: 0.0016389]
		[batch 20/20] avg loss: 1.554227675758151		[learning rate: 0.0016359]
	Learning Rate: 0.00163592
	LOSS [training: 1.6571805326536644 | validation: 2.5717257420834403]
	TIME [epoch: 8.39 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6542829226066178		[learning rate: 0.001633]
		[batch 20/20] avg loss: 1.6773945129060455		[learning rate: 0.00163]
	Learning Rate: 0.00162999
	LOSS [training: 1.6658387177563316 | validation: 2.5513857443008137]
	TIME [epoch: 8.39 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7535972404046452		[learning rate: 0.001627]
		[batch 20/20] avg loss: 1.6360804820439292		[learning rate: 0.0016241]
	Learning Rate: 0.00162407
	LOSS [training: 1.6948388612242873 | validation: 2.538556054323051]
	TIME [epoch: 8.41 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5725734364216555		[learning rate: 0.0016211]
		[batch 20/20] avg loss: 1.797996464034498		[learning rate: 0.0016182]
	Learning Rate: 0.00161818
	LOSS [training: 1.685284950228077 | validation: 2.5222087732542624]
	TIME [epoch: 8.39 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7681094629703022		[learning rate: 0.0016152]
		[batch 20/20] avg loss: 1.5430948468858499		[learning rate: 0.0016123]
	Learning Rate: 0.00161231
	LOSS [training: 1.6556021549280762 | validation: 2.794873901942561]
	TIME [epoch: 8.39 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7137194659253088		[learning rate: 0.0016094]
		[batch 20/20] avg loss: 1.7005934459939354		[learning rate: 0.0016065]
	Learning Rate: 0.00160645
	LOSS [training: 1.707156455959622 | validation: 2.5241794663044193]
	TIME [epoch: 8.4 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7107568535168443		[learning rate: 0.0016035]
		[batch 20/20] avg loss: 1.6727277357702228		[learning rate: 0.0016006]
	Learning Rate: 0.00160062
	LOSS [training: 1.6917422946435334 | validation: 2.513388222368911]
	TIME [epoch: 8.39 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6319383240707548		[learning rate: 0.0015977]
		[batch 20/20] avg loss: 1.6813110202441888		[learning rate: 0.0015948]
	Learning Rate: 0.00159482
	LOSS [training: 1.6566246721574718 | validation: 2.523331047334462]
	TIME [epoch: 8.42 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6192823820510884		[learning rate: 0.0015919]
		[batch 20/20] avg loss: 1.7363233657577108		[learning rate: 0.001589]
	Learning Rate: 0.00158903
	LOSS [training: 1.6778028739043997 | validation: 2.639565735597743]
	TIME [epoch: 8.38 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7667723612592101		[learning rate: 0.0015861]
		[batch 20/20] avg loss: 1.6005461465372697		[learning rate: 0.0015833]
	Learning Rate: 0.00158326
	LOSS [training: 1.6836592538982398 | validation: 2.5487323877117722]
	TIME [epoch: 8.39 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.651077253156183		[learning rate: 0.0015804]
		[batch 20/20] avg loss: 1.695600270694991		[learning rate: 0.0015775]
	Learning Rate: 0.00157752
	LOSS [training: 1.6733387619255873 | validation: 2.52312015349264]
	TIME [epoch: 8.39 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7241731276693684		[learning rate: 0.0015747]
		[batch 20/20] avg loss: 1.6260942442132347		[learning rate: 0.0015718]
	Learning Rate: 0.00157179
	LOSS [training: 1.6751336859413015 | validation: 2.5084729690133285]
	TIME [epoch: 8.41 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5653951497065683		[learning rate: 0.0015689]
		[batch 20/20] avg loss: 1.7667465541216452		[learning rate: 0.0015661]
	Learning Rate: 0.00156609
	LOSS [training: 1.6660708519141068 | validation: 2.5553730235418746]
	TIME [epoch: 8.4 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.529045294324913		[learning rate: 0.0015632]
		[batch 20/20] avg loss: 1.791341791131164		[learning rate: 0.0015604]
	Learning Rate: 0.0015604
	LOSS [training: 1.6601935427280385 | validation: 2.5407702023563763]
	TIME [epoch: 8.39 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5992136742840128		[learning rate: 0.0015576]
		[batch 20/20] avg loss: 1.704385523447477		[learning rate: 0.0015547]
	Learning Rate: 0.00155474
	LOSS [training: 1.6517995988657446 | validation: 2.5393013736533474]
	TIME [epoch: 8.39 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7278777230839828		[learning rate: 0.0015519]
		[batch 20/20] avg loss: 1.6269866283141092		[learning rate: 0.0015491]
	Learning Rate: 0.0015491
	LOSS [training: 1.677432175699046 | validation: 2.5296751903864934]
	TIME [epoch: 8.41 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.686785727295043		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 1.63415123975997		[learning rate: 0.0015435]
	Learning Rate: 0.00154348
	LOSS [training: 1.6604684835275065 | validation: 2.588965634861132]
	TIME [epoch: 8.39 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.669373122503535		[learning rate: 0.0015407]
		[batch 20/20] avg loss: 1.69693205339927		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 1.6831525879514024 | validation: 2.5556655480259645]
	TIME [epoch: 8.39 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6441322311910382		[learning rate: 0.0015351]
		[batch 20/20] avg loss: 1.7205234799807219		[learning rate: 0.0015323]
	Learning Rate: 0.00153229
	LOSS [training: 1.6823278555858803 | validation: 2.514460160603006]
	TIME [epoch: 8.39 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6936878689097683		[learning rate: 0.0015295]
		[batch 20/20] avg loss: 1.6536595165767047		[learning rate: 0.0015267]
	Learning Rate: 0.00152673
	LOSS [training: 1.6736736927432365 | validation: 2.5446130519827768]
	TIME [epoch: 8.38 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.813490780484281		[learning rate: 0.001524]
		[batch 20/20] avg loss: 1.5205865117982007		[learning rate: 0.0015212]
	Learning Rate: 0.00152119
	LOSS [training: 1.6670386461412405 | validation: 2.5192983574464534]
	TIME [epoch: 8.41 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.752887995801179		[learning rate: 0.0015184]
		[batch 20/20] avg loss: 1.6505906632551521		[learning rate: 0.0015157]
	Learning Rate: 0.00151567
	LOSS [training: 1.7017393295281658 | validation: 2.552478603303153]
	TIME [epoch: 8.38 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.623496761034308		[learning rate: 0.0015129]
		[batch 20/20] avg loss: 1.7207567963114638		[learning rate: 0.0015102]
	Learning Rate: 0.00151017
	LOSS [training: 1.672126778672886 | validation: 2.54836721648746]
	TIME [epoch: 8.38 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.829191994785809		[learning rate: 0.0015074]
		[batch 20/20] avg loss: 1.553104911432662		[learning rate: 0.0015047]
	Learning Rate: 0.00150469
	LOSS [training: 1.6911484531092356 | validation: 2.5386697478381572]
	TIME [epoch: 8.39 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6724524181247837		[learning rate: 0.001502]
		[batch 20/20] avg loss: 1.6800096775125404		[learning rate: 0.0014992]
	Learning Rate: 0.00149923
	LOSS [training: 1.676231047818662 | validation: 2.58364609150309]
	TIME [epoch: 8.42 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6655811227043587		[learning rate: 0.0014965]
		[batch 20/20] avg loss: 1.6576820369219405		[learning rate: 0.0014938]
	Learning Rate: 0.00149379
	LOSS [training: 1.6616315798131491 | validation: 2.5629577404632404]
	TIME [epoch: 8.39 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6944879423212664		[learning rate: 0.0014911]
		[batch 20/20] avg loss: 1.7594726912141432		[learning rate: 0.0014884]
	Learning Rate: 0.00148837
	LOSS [training: 1.7269803167677051 | validation: 2.56936921714314]
	TIME [epoch: 8.39 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4700148628029681		[learning rate: 0.0014857]
		[batch 20/20] avg loss: 1.8280970466986726		[learning rate: 0.001483]
	Learning Rate: 0.00148297
	LOSS [training: 1.6490559547508201 | validation: 2.578747387924785]
	TIME [epoch: 8.39 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6366739923976148		[learning rate: 0.0014803]
		[batch 20/20] avg loss: 1.662133385973916		[learning rate: 0.0014776]
	Learning Rate: 0.00147759
	LOSS [training: 1.6494036891857653 | validation: 2.5158605347133847]
	TIME [epoch: 8.41 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6065181810149873		[learning rate: 0.0014749]
		[batch 20/20] avg loss: 1.723957987411504		[learning rate: 0.0014722]
	Learning Rate: 0.00147222
	LOSS [training: 1.6652380842132455 | validation: 2.5150894446086594]
	TIME [epoch: 8.39 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6474261711747271		[learning rate: 0.0014695]
		[batch 20/20] avg loss: 1.6528086833237616		[learning rate: 0.0014669]
	Learning Rate: 0.00146688
	LOSS [training: 1.6501174272492445 | validation: 2.538636437490796]
	TIME [epoch: 8.39 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6255414955596272		[learning rate: 0.0014642]
		[batch 20/20] avg loss: 1.6940695675969297		[learning rate: 0.0014616]
	Learning Rate: 0.00146156
	LOSS [training: 1.6598055315782787 | validation: 2.5245902783890304]
	TIME [epoch: 8.39 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8298993394195715		[learning rate: 0.0014589]
		[batch 20/20] avg loss: 1.466964740512086		[learning rate: 0.0014563]
	Learning Rate: 0.00145625
	LOSS [training: 1.6484320399658283 | validation: 2.5149213583320265]
	TIME [epoch: 8.39 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7912991514857475		[learning rate: 0.0014536]
		[batch 20/20] avg loss: 1.5341973026231266		[learning rate: 0.001451]
	Learning Rate: 0.00145097
	LOSS [training: 1.662748227054437 | validation: 2.522202785105668]
	TIME [epoch: 8.41 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.686013147293696		[learning rate: 0.0014483]
		[batch 20/20] avg loss: 1.6579915991588134		[learning rate: 0.0014457]
	Learning Rate: 0.0014457
	LOSS [training: 1.6720023732262548 | validation: 2.632877919434364]
	TIME [epoch: 8.39 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5915018094216555		[learning rate: 0.0014431]
		[batch 20/20] avg loss: 1.7667660498494815		[learning rate: 0.0014405]
	Learning Rate: 0.00144046
	LOSS [training: 1.679133929635568 | validation: 2.542148043953759]
	TIME [epoch: 8.39 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8403483056575538		[learning rate: 0.0014378]
		[batch 20/20] avg loss: 1.5091940245701299		[learning rate: 0.0014352]
	Learning Rate: 0.00143523
	LOSS [training: 1.6747711651138417 | validation: 2.551949269015624]
	TIME [epoch: 8.37 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.747360341631958		[learning rate: 0.0014326]
		[batch 20/20] avg loss: 1.5599624310245874		[learning rate: 0.00143]
	Learning Rate: 0.00143002
	LOSS [training: 1.6536613863282725 | validation: 2.545063955845462]
	TIME [epoch: 8.41 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.660024662334402		[learning rate: 0.0014274]
		[batch 20/20] avg loss: 1.6886435681274719		[learning rate: 0.0014248]
	Learning Rate: 0.00142483
	LOSS [training: 1.674334115230937 | validation: 2.545042937109861]
	TIME [epoch: 8.39 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6783111735495393		[learning rate: 0.0014222]
		[batch 20/20] avg loss: 1.622725650704827		[learning rate: 0.0014197]
	Learning Rate: 0.00141966
	LOSS [training: 1.6505184121271832 | validation: 2.5333615099357885]
	TIME [epoch: 8.39 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6439185854632377		[learning rate: 0.0014171]
		[batch 20/20] avg loss: 1.7427812363350739		[learning rate: 0.0014145]
	Learning Rate: 0.00141451
	LOSS [training: 1.6933499108991559 | validation: 2.514794508986488]
	TIME [epoch: 8.39 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5397783935378224		[learning rate: 0.0014119]
		[batch 20/20] avg loss: 1.8066974993904321		[learning rate: 0.0014094]
	Learning Rate: 0.00140937
	LOSS [training: 1.6732379464641274 | validation: 2.5289705605443515]
	TIME [epoch: 8.41 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.67586398290174		[learning rate: 0.0014068]
		[batch 20/20] avg loss: 1.6719067322641472		[learning rate: 0.0014043]
	Learning Rate: 0.00140426
	LOSS [training: 1.6738853575829438 | validation: 2.520229149143208]
	TIME [epoch: 8.39 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7820668391490038		[learning rate: 0.0014017]
		[batch 20/20] avg loss: 1.5521915580765246		[learning rate: 0.0013992]
	Learning Rate: 0.00139916
	LOSS [training: 1.667129198612764 | validation: 2.529921887184548]
	TIME [epoch: 8.38 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7924268343138636		[learning rate: 0.0013966]
		[batch 20/20] avg loss: 1.5781163579289097		[learning rate: 0.0013941]
	Learning Rate: 0.00139409
	LOSS [training: 1.6852715961213867 | validation: 2.5661227784858385]
	TIME [epoch: 8.39 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.719826252663657		[learning rate: 0.0013916]
		[batch 20/20] avg loss: 1.6803084627709814		[learning rate: 0.001389]
	Learning Rate: 0.00138903
	LOSS [training: 1.700067357717319 | validation: 2.69061595926729]
	TIME [epoch: 8.4 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7385345789628175		[learning rate: 0.0013865]
		[batch 20/20] avg loss: 1.586736167528415		[learning rate: 0.001384]
	Learning Rate: 0.00138399
	LOSS [training: 1.6626353732456163 | validation: 2.5198896364515604]
	TIME [epoch: 8.41 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6928711814195665		[learning rate: 0.0013815]
		[batch 20/20] avg loss: 1.6482288400189016		[learning rate: 0.001379]
	Learning Rate: 0.00137896
	LOSS [training: 1.6705500107192344 | validation: 2.526959494256368]
	TIME [epoch: 8.39 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6846960470544439		[learning rate: 0.0013765]
		[batch 20/20] avg loss: 1.7174753255838886		[learning rate: 0.001374]
	Learning Rate: 0.00137396
	LOSS [training: 1.7010856863191663 | validation: 2.5858585685057798]
	TIME [epoch: 8.39 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5547951172859134		[learning rate: 0.0013715]
		[batch 20/20] avg loss: 1.7505751448500682		[learning rate: 0.001369]
	Learning Rate: 0.00136897
	LOSS [training: 1.6526851310679909 | validation: 2.542438283956847]
	TIME [epoch: 8.4 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5765411397845577		[learning rate: 0.0013665]
		[batch 20/20] avg loss: 1.7372183864887902		[learning rate: 0.001364]
	Learning Rate: 0.001364
	LOSS [training: 1.6568797631366743 | validation: 2.528472759068671]
	TIME [epoch: 8.42 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6891986118368998		[learning rate: 0.0013615]
		[batch 20/20] avg loss: 1.6028365018591142		[learning rate: 0.0013591]
	Learning Rate: 0.00135905
	LOSS [training: 1.6460175568480069 | validation: 2.5154974348831374]
	TIME [epoch: 8.41 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6150595457122965		[learning rate: 0.0013566]
		[batch 20/20] avg loss: 1.7017946646284288		[learning rate: 0.0013541]
	Learning Rate: 0.00135412
	LOSS [training: 1.6584271051703627 | validation: 2.52649247467773]
	TIME [epoch: 8.39 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5449927996163573		[learning rate: 0.0013517]
		[batch 20/20] avg loss: 1.809203937193929		[learning rate: 0.0013492]
	Learning Rate: 0.00134921
	LOSS [training: 1.677098368405143 | validation: 2.534920028602239]
	TIME [epoch: 8.39 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6823805237678315		[learning rate: 0.0013468]
		[batch 20/20] avg loss: 1.625119063000896		[learning rate: 0.0013443]
	Learning Rate: 0.00134431
	LOSS [training: 1.6537497933843635 | validation: 2.587699608036329]
	TIME [epoch: 8.41 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5340684429542464		[learning rate: 0.0013419]
		[batch 20/20] avg loss: 1.8199918739546113		[learning rate: 0.0013394]
	Learning Rate: 0.00133943
	LOSS [training: 1.6770301584544285 | validation: 2.5378770645556346]
	TIME [epoch: 8.39 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5638697608334504		[learning rate: 0.001337]
		[batch 20/20] avg loss: 1.7459072458734215		[learning rate: 0.0013346]
	Learning Rate: 0.00133457
	LOSS [training: 1.6548885033534357 | validation: 2.5156159708573087]
	TIME [epoch: 8.39 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6529807944980892		[learning rate: 0.0013321]
		[batch 20/20] avg loss: 1.627262978582932		[learning rate: 0.0013297]
	Learning Rate: 0.00132973
	LOSS [training: 1.6401218865405105 | validation: 2.500312929005416]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_655.pth
	Model improved!!!
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5684166385831753		[learning rate: 0.0013273]
		[batch 20/20] avg loss: 1.7535287025382018		[learning rate: 0.0013249]
	Learning Rate: 0.0013249
	LOSS [training: 1.6609726705606884 | validation: 2.5197518420165883]
	TIME [epoch: 8.41 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8253751406289098		[learning rate: 0.0013225]
		[batch 20/20] avg loss: 1.5202328957250422		[learning rate: 0.0013201]
	Learning Rate: 0.0013201
	LOSS [training: 1.6728040181769757 | validation: 2.566418778673062]
	TIME [epoch: 8.38 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.779140630349994		[learning rate: 0.0013177]
		[batch 20/20] avg loss: 1.5684926908770378		[learning rate: 0.0013153]
	Learning Rate: 0.0013153
	LOSS [training: 1.6738166606135163 | validation: 2.619193826067055]
	TIME [epoch: 8.53 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.67425979442022		[learning rate: 0.0013129]
		[batch 20/20] avg loss: 1.6652597714671835		[learning rate: 0.0013105]
	Learning Rate: 0.00131053
	LOSS [training: 1.6697597829437019 | validation: 2.571971429297171]
	TIME [epoch: 8.38 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5824334036766063		[learning rate: 0.0013082]
		[batch 20/20] avg loss: 1.7679205650581007		[learning rate: 0.0013058]
	Learning Rate: 0.00130578
	LOSS [training: 1.6751769843673536 | validation: 2.5913586533960555]
	TIME [epoch: 8.38 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4713410641057731		[learning rate: 0.0013034]
		[batch 20/20] avg loss: 1.8561047562920536		[learning rate: 0.001301]
	Learning Rate: 0.00130104
	LOSS [training: 1.6637229101989135 | validation: 2.549787788937767]
	TIME [epoch: 8.41 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5663995353458997		[learning rate: 0.0012987]
		[batch 20/20] avg loss: 1.7382832096941594		[learning rate: 0.0012963]
	Learning Rate: 0.00129631
	LOSS [training: 1.65234137252003 | validation: 2.5204980902622784]
	TIME [epoch: 8.38 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.679227907914757		[learning rate: 0.001294]
		[batch 20/20] avg loss: 1.6319890028476016		[learning rate: 0.0012916]
	Learning Rate: 0.00129161
	LOSS [training: 1.6556084553811796 | validation: 2.5625944159012364]
	TIME [epoch: 8.39 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3703688594626184		[learning rate: 0.0012893]
		[batch 20/20] avg loss: 1.9324993958300964		[learning rate: 0.0012869]
	Learning Rate: 0.00128692
	LOSS [training: 1.651434127646357 | validation: 2.578866025843665]
	TIME [epoch: 8.38 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.688806665286382		[learning rate: 0.0012846]
		[batch 20/20] avg loss: 1.5824002774835475		[learning rate: 0.0012823]
	Learning Rate: 0.00128225
	LOSS [training: 1.635603471384965 | validation: 2.5178111731204713]
	TIME [epoch: 8.41 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5802143444468493		[learning rate: 0.0012799]
		[batch 20/20] avg loss: 1.7051568173028044		[learning rate: 0.0012776]
	Learning Rate: 0.0012776
	LOSS [training: 1.6426855808748269 | validation: 2.5299944901986593]
	TIME [epoch: 8.38 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.658257574644646		[learning rate: 0.0012753]
		[batch 20/20] avg loss: 1.641386127163597		[learning rate: 0.001273]
	Learning Rate: 0.00127296
	LOSS [training: 1.6498218509041211 | validation: 2.5402532848038373]
	TIME [epoch: 8.37 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.65066406844655		[learning rate: 0.0012707]
		[batch 20/20] avg loss: 1.667138936980934		[learning rate: 0.0012683]
	Learning Rate: 0.00126834
	LOSS [training: 1.6589015027137421 | validation: 2.5120629269894112]
	TIME [epoch: 8.39 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.408826296626047		[learning rate: 0.001266]
		[batch 20/20] avg loss: 1.8760687056478909		[learning rate: 0.0012637]
	Learning Rate: 0.00126374
	LOSS [training: 1.6424475011369686 | validation: 2.518406782718095]
	TIME [epoch: 8.41 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.625488867030926		[learning rate: 0.0012614]
		[batch 20/20] avg loss: 1.649266600084977		[learning rate: 0.0012592]
	Learning Rate: 0.00125915
	LOSS [training: 1.6373777335579516 | validation: 2.520489300524981]
	TIME [epoch: 8.39 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7006485902142257		[learning rate: 0.0012569]
		[batch 20/20] avg loss: 1.645994512856359		[learning rate: 0.0012546]
	Learning Rate: 0.00125458
	LOSS [training: 1.6733215515352924 | validation: 2.6319704767400536]
	TIME [epoch: 8.38 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6227387061315048		[learning rate: 0.0012523]
		[batch 20/20] avg loss: 1.7063389603558545		[learning rate: 0.00125]
	Learning Rate: 0.00125003
	LOSS [training: 1.66453883324368 | validation: 2.528551335211855]
	TIME [epoch: 8.38 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6212291733089252		[learning rate: 0.0012478]
		[batch 20/20] avg loss: 1.6630526932783076		[learning rate: 0.0012455]
	Learning Rate: 0.0012455
	LOSS [training: 1.6421409332936165 | validation: 2.520725829413294]
	TIME [epoch: 8.38 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5004124064416442		[learning rate: 0.0012432]
		[batch 20/20] avg loss: 1.8329164005515157		[learning rate: 0.001241]
	Learning Rate: 0.00124098
	LOSS [training: 1.6666644034965798 | validation: 2.5796944738196643]
	TIME [epoch: 8.4 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7473550929750796		[learning rate: 0.0012387]
		[batch 20/20] avg loss: 1.5707330438037488		[learning rate: 0.0012365]
	Learning Rate: 0.00123647
	LOSS [training: 1.659044068389414 | validation: 2.551122325590466]
	TIME [epoch: 8.38 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6423123440657412		[learning rate: 0.0012342]
		[batch 20/20] avg loss: 1.6871148998731598		[learning rate: 0.001232]
	Learning Rate: 0.00123198
	LOSS [training: 1.664713621969451 | validation: 2.5559368466988954]
	TIME [epoch: 8.38 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5349283317049853		[learning rate: 0.0012297]
		[batch 20/20] avg loss: 1.7775974877665395		[learning rate: 0.0012275]
	Learning Rate: 0.00122751
	LOSS [training: 1.6562629097357626 | validation: 2.574623542627962]
	TIME [epoch: 8.39 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6889972929032395		[learning rate: 0.0012253]
		[batch 20/20] avg loss: 1.630001178699023		[learning rate: 0.0012231]
	Learning Rate: 0.00122306
	LOSS [training: 1.6594992358011311 | validation: 2.5610326858060777]
	TIME [epoch: 8.41 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.627430618387887		[learning rate: 0.0012208]
		[batch 20/20] avg loss: 1.66666901667763		[learning rate: 0.0012186]
	Learning Rate: 0.00121862
	LOSS [training: 1.6470498175327584 | validation: 2.5376786626147743]
	TIME [epoch: 8.37 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.557318577307633		[learning rate: 0.0012164]
		[batch 20/20] avg loss: 1.7418838187946648		[learning rate: 0.0012142]
	Learning Rate: 0.0012142
	LOSS [training: 1.6496011980511487 | validation: 2.5320252871838194]
	TIME [epoch: 8.39 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6092346148390078		[learning rate: 0.001212]
		[batch 20/20] avg loss: 1.7118360487193756		[learning rate: 0.0012098]
	Learning Rate: 0.00120979
	LOSS [training: 1.6605353317791915 | validation: 2.5427853652796135]
	TIME [epoch: 8.38 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6026750031281858		[learning rate: 0.0012076]
		[batch 20/20] avg loss: 1.7468046022264005		[learning rate: 0.0012054]
	Learning Rate: 0.0012054
	LOSS [training: 1.674739802677293 | validation: 2.5762528222814773]
	TIME [epoch: 8.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6006068729731744		[learning rate: 0.0012032]
		[batch 20/20] avg loss: 1.7748675949851038		[learning rate: 0.001201]
	Learning Rate: 0.00120103
	LOSS [training: 1.687737233979139 | validation: 2.7265558846899167]
	TIME [epoch: 8.39 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7631743309164343		[learning rate: 0.0011988]
		[batch 20/20] avg loss: 1.6249324177616997		[learning rate: 0.0011967]
	Learning Rate: 0.00119667
	LOSS [training: 1.6940533743390667 | validation: 2.519763263317633]
	TIME [epoch: 8.38 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7042912873218783		[learning rate: 0.0011945]
		[batch 20/20] avg loss: 1.5869041627269078		[learning rate: 0.0011923]
	Learning Rate: 0.00119233
	LOSS [training: 1.6455977250243934 | validation: 2.5134623581118936]
	TIME [epoch: 8.4 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7373253545605203		[learning rate: 0.0011902]
		[batch 20/20] avg loss: 1.5660700421138964		[learning rate: 0.001188]
	Learning Rate: 0.001188
	LOSS [training: 1.6516976983372083 | validation: 2.5421115793940112]
	TIME [epoch: 8.39 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6896081255450894		[learning rate: 0.0011858]
		[batch 20/20] avg loss: 1.584162394420388		[learning rate: 0.0011837]
	Learning Rate: 0.00118369
	LOSS [training: 1.6368852599827388 | validation: 2.546414484746802]
	TIME [epoch: 8.41 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6053310539651418		[learning rate: 0.0011815]
		[batch 20/20] avg loss: 1.7154429106888365		[learning rate: 0.0011794]
	Learning Rate: 0.00117939
	LOSS [training: 1.6603869823269886 | validation: 2.6405459911955953]
	TIME [epoch: 8.38 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6828396346621095		[learning rate: 0.0011772]
		[batch 20/20] avg loss: 1.6193364893165534		[learning rate: 0.0011751]
	Learning Rate: 0.00117511
	LOSS [training: 1.6510880619893313 | validation: 2.523201662653776]
	TIME [epoch: 8.38 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4318598096826325		[learning rate: 0.001173]
		[batch 20/20] avg loss: 1.860408164060146		[learning rate: 0.0011708]
	Learning Rate: 0.00117085
	LOSS [training: 1.646133986871389 | validation: 2.5054170634486317]
	TIME [epoch: 8.38 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7145194975628097		[learning rate: 0.0011687]
		[batch 20/20] avg loss: 1.6025369593565473		[learning rate: 0.0011666]
	Learning Rate: 0.0011666
	LOSS [training: 1.6585282284596787 | validation: 2.5669258068266334]
	TIME [epoch: 8.4 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7516171241175307		[learning rate: 0.0011645]
		[batch 20/20] avg loss: 1.5281865642885095		[learning rate: 0.0011624]
	Learning Rate: 0.00116236
	LOSS [training: 1.6399018442030198 | validation: 2.6500850309485546]
	TIME [epoch: 8.39 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8949239435925918		[learning rate: 0.0011603]
		[batch 20/20] avg loss: 1.4688069377502517		[learning rate: 0.0011581]
	Learning Rate: 0.00115815
	LOSS [training: 1.6818654406714217 | validation: 2.527068962447574]
	TIME [epoch: 8.39 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6046115611103438		[learning rate: 0.001156]
		[batch 20/20] avg loss: 1.6906360477995865		[learning rate: 0.0011539]
	Learning Rate: 0.00115394
	LOSS [training: 1.6476238044549654 | validation: 2.660412874266274]
	TIME [epoch: 8.38 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7141707322359643		[learning rate: 0.0011518]
		[batch 20/20] avg loss: 1.6312091414167018		[learning rate: 0.0011498]
	Learning Rate: 0.00114975
	LOSS [training: 1.6726899368263333 | validation: 2.5630157918613885]
	TIME [epoch: 8.41 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.775339330597913		[learning rate: 0.0011477]
		[batch 20/20] avg loss: 1.535901984901423		[learning rate: 0.0011456]
	Learning Rate: 0.00114558
	LOSS [training: 1.6556206577496677 | validation: 2.5712765181114428]
	TIME [epoch: 8.38 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5863931176575885		[learning rate: 0.0011435]
		[batch 20/20] avg loss: 1.7239196497368698		[learning rate: 0.0011414]
	Learning Rate: 0.00114142
	LOSS [training: 1.6551563836972292 | validation: 2.5318764153021043]
	TIME [epoch: 8.38 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6257792664567343		[learning rate: 0.0011394]
		[batch 20/20] avg loss: 1.6736350698496036		[learning rate: 0.0011373]
	Learning Rate: 0.00113728
	LOSS [training: 1.649707168153169 | validation: 2.5402666421403137]
	TIME [epoch: 8.38 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5394200896359993		[learning rate: 0.0011352]
		[batch 20/20] avg loss: 1.7701762872001023		[learning rate: 0.0011332]
	Learning Rate: 0.00113316
	LOSS [training: 1.654798188418051 | validation: 2.532956205623923]
	TIME [epoch: 8.38 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6523427082821986		[learning rate: 0.0011311]
		[batch 20/20] avg loss: 1.6953166336995618		[learning rate: 0.001129]
	Learning Rate: 0.00112904
	LOSS [training: 1.6738296709908802 | validation: 2.5203761053669695]
	TIME [epoch: 8.39 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5470858007578556		[learning rate: 0.001127]
		[batch 20/20] avg loss: 1.7990991912250731		[learning rate: 0.0011249]
	Learning Rate: 0.00112495
	LOSS [training: 1.6730924959914641 | validation: 2.5193417648621725]
	TIME [epoch: 8.38 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5979009950703253		[learning rate: 0.0011229]
		[batch 20/20] avg loss: 1.7064120497052238		[learning rate: 0.0011209]
	Learning Rate: 0.00112086
	LOSS [training: 1.6521565223877748 | validation: 2.530133843497304]
	TIME [epoch: 8.37 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6059453128137515		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 1.6916520565940563		[learning rate: 0.0011168]
	Learning Rate: 0.0011168
	LOSS [training: 1.6487986847039036 | validation: 2.5236234139384672]
	TIME [epoch: 8.37 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7247429750524241		[learning rate: 0.0011148]
		[batch 20/20] avg loss: 1.6033579575063928		[learning rate: 0.0011127]
	Learning Rate: 0.00111274
	LOSS [training: 1.6640504662794082 | validation: 2.5751118018551638]
	TIME [epoch: 8.41 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.657563895953445		[learning rate: 0.0011107]
		[batch 20/20] avg loss: 1.6235037497165508		[learning rate: 0.0011087]
	Learning Rate: 0.0011087
	LOSS [training: 1.640533822834998 | validation: 2.5273843503927225]
	TIME [epoch: 8.39 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8963850489597267		[learning rate: 0.0011067]
		[batch 20/20] avg loss: 1.3833718676345372		[learning rate: 0.0011047]
	Learning Rate: 0.00110468
	LOSS [training: 1.639878458297132 | validation: 2.5131067622046945]
	TIME [epoch: 8.39 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6769132653433094		[learning rate: 0.0011027]
		[batch 20/20] avg loss: 1.618362328689391		[learning rate: 0.0011007]
	Learning Rate: 0.00110067
	LOSS [training: 1.6476377970163498 | validation: 2.534055469305901]
	TIME [epoch: 8.38 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.52157124910088		[learning rate: 0.0010987]
		[batch 20/20] avg loss: 1.7757794308445938		[learning rate: 0.0010967]
	Learning Rate: 0.00109668
	LOSS [training: 1.648675339972737 | validation: 2.5475988208081146]
	TIME [epoch: 8.4 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.651945920734707		[learning rate: 0.0010947]
		[batch 20/20] avg loss: 1.6910424533168573		[learning rate: 0.0010927]
	Learning Rate: 0.0010927
	LOSS [training: 1.671494187025782 | validation: 2.544684350173719]
	TIME [epoch: 8.39 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6148585857528654		[learning rate: 0.0010907]
		[batch 20/20] avg loss: 1.6509183922340775		[learning rate: 0.0010887]
	Learning Rate: 0.00108873
	LOSS [training: 1.6328884889934714 | validation: 2.526338432239522]
	TIME [epoch: 8.38 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.59286282115283		[learning rate: 0.0010868]
		[batch 20/20] avg loss: 1.671990766701121		[learning rate: 0.0010848]
	Learning Rate: 0.00108478
	LOSS [training: 1.6324267939269756 | validation: 2.5456096376415847]
	TIME [epoch: 8.38 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6599033813985062		[learning rate: 0.0010828]
		[batch 20/20] avg loss: 1.6184395681823154		[learning rate: 0.0010808]
	Learning Rate: 0.00108084
	LOSS [training: 1.6391714747904111 | validation: 2.552699238119796]
	TIME [epoch: 8.4 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.582416569312916		[learning rate: 0.0010789]
		[batch 20/20] avg loss: 1.6795215759886726		[learning rate: 0.0010769]
	Learning Rate: 0.00107692
	LOSS [training: 1.6309690726507942 | validation: 2.5352132655543347]
	TIME [epoch: 8.4 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.594546867494138		[learning rate: 0.001075]
		[batch 20/20] avg loss: 1.718471192062627		[learning rate: 0.001073]
	Learning Rate: 0.00107301
	LOSS [training: 1.6565090297783827 | validation: 2.515242505729013]
	TIME [epoch: 8.39 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6898147863779287		[learning rate: 0.0010711]
		[batch 20/20] avg loss: 1.6130571076524844		[learning rate: 0.0010691]
	Learning Rate: 0.00106912
	LOSS [training: 1.6514359470152065 | validation: 2.578497575248111]
	TIME [epoch: 8.39 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.64314974648634		[learning rate: 0.0010672]
		[batch 20/20] avg loss: 1.6456390301854724		[learning rate: 0.0010652]
	Learning Rate: 0.00106524
	LOSS [training: 1.6443943883359062 | validation: 2.5227965534684946]
	TIME [epoch: 8.38 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6575743330342405		[learning rate: 0.0010633]
		[batch 20/20] avg loss: 1.6662190670356203		[learning rate: 0.0010614]
	Learning Rate: 0.00106137
	LOSS [training: 1.6618967000349305 | validation: 2.512012731759227]
	TIME [epoch: 8.41 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5142328957668325		[learning rate: 0.0010594]
		[batch 20/20] avg loss: 1.7576964750641895		[learning rate: 0.0010575]
	Learning Rate: 0.00105752
	LOSS [training: 1.6359646854155112 | validation: 2.637979351169662]
	TIME [epoch: 8.39 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7675025104509785		[learning rate: 0.0010556]
		[batch 20/20] avg loss: 1.5361001588985672		[learning rate: 0.0010537]
	Learning Rate: 0.00105368
	LOSS [training: 1.6518013346747729 | validation: 2.5431617708611527]
	TIME [epoch: 8.39 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8974660384309854		[learning rate: 0.0010518]
		[batch 20/20] avg loss: 1.4508648987541957		[learning rate: 0.0010499]
	Learning Rate: 0.00104986
	LOSS [training: 1.6741654685925902 | validation: 2.5129374758317335]
	TIME [epoch: 8.38 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5474178648376162		[learning rate: 0.001048]
		[batch 20/20] avg loss: 1.7397928635207633		[learning rate: 0.0010461]
	Learning Rate: 0.00104605
	LOSS [training: 1.6436053641791901 | validation: 2.527555168707306]
	TIME [epoch: 8.41 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5929035832328642		[learning rate: 0.0010442]
		[batch 20/20] avg loss: 1.6839503299526553		[learning rate: 0.0010423]
	Learning Rate: 0.00104225
	LOSS [training: 1.63842695659276 | validation: 2.820246065552715]
	TIME [epoch: 8.39 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7389741359739328		[learning rate: 0.0010404]
		[batch 20/20] avg loss: 1.6040477808873335		[learning rate: 0.0010385]
	Learning Rate: 0.00103847
	LOSS [training: 1.6715109584306334 | validation: 2.5038535146633802]
	TIME [epoch: 8.39 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6832998991487607		[learning rate: 0.0010366]
		[batch 20/20] avg loss: 1.6092266985126717		[learning rate: 0.0010347]
	Learning Rate: 0.0010347
	LOSS [training: 1.6462632988307164 | validation: 2.637785832956119]
	TIME [epoch: 8.39 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5662007943563996		[learning rate: 0.0010328]
		[batch 20/20] avg loss: 1.7489488384582295		[learning rate: 0.0010309]
	Learning Rate: 0.00103095
	LOSS [training: 1.6575748164073143 | validation: 2.5380500265522743]
	TIME [epoch: 8.41 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6688048942686753		[learning rate: 0.0010291]
		[batch 20/20] avg loss: 1.5949483470070869		[learning rate: 0.0010272]
	Learning Rate: 0.00102721
	LOSS [training: 1.6318766206378807 | validation: 2.5257991907204524]
	TIME [epoch: 8.39 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6475686644763834		[learning rate: 0.0010253]
		[batch 20/20] avg loss: 1.6283920556274551		[learning rate: 0.0010235]
	Learning Rate: 0.00102348
	LOSS [training: 1.6379803600519192 | validation: 2.520490383648628]
	TIME [epoch: 8.38 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5250095907563712		[learning rate: 0.0010216]
		[batch 20/20] avg loss: 1.7441132677767566		[learning rate: 0.0010198]
	Learning Rate: 0.00101976
	LOSS [training: 1.634561429266564 | validation: 2.5299925334329925]
	TIME [epoch: 8.38 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6827366295903192		[learning rate: 0.0010179]
		[batch 20/20] avg loss: 1.5902541723151058		[learning rate: 0.0010161]
	Learning Rate: 0.00101606
	LOSS [training: 1.6364954009527124 | validation: 2.5392764917519273]
	TIME [epoch: 8.38 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5672425182826226		[learning rate: 0.0010142]
		[batch 20/20] avg loss: 1.7729430688648986		[learning rate: 0.0010124]
	Learning Rate: 0.00101238
	LOSS [training: 1.6700927935737606 | validation: 2.578908426998448]
	TIME [epoch: 8.41 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6299283341104445		[learning rate: 0.0010105]
		[batch 20/20] avg loss: 1.6600465317852517		[learning rate: 0.0010087]
	Learning Rate: 0.0010087
	LOSS [training: 1.6449874329478482 | validation: 2.545458191823561]
	TIME [epoch: 8.4 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7618425835033225		[learning rate: 0.0010069]
		[batch 20/20] avg loss: 1.5419895097036258		[learning rate: 0.001005]
	Learning Rate: 0.00100504
	LOSS [training: 1.6519160466034741 | validation: 2.531763721328205]
	TIME [epoch: 8.39 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5499594249878774		[learning rate: 0.0010032]
		[batch 20/20] avg loss: 1.7336801722234774		[learning rate: 0.0010014]
	Learning Rate: 0.00100139
	LOSS [training: 1.6418197986056775 | validation: 2.5177413905924655]
	TIME [epoch: 8.38 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6008699619983264		[learning rate: 0.00099958]
		[batch 20/20] avg loss: 1.710108606945918		[learning rate: 0.00099776]
	Learning Rate: 0.00099776
	LOSS [training: 1.6554892844721223 | validation: 2.5627233493589148]
	TIME [epoch: 8.41 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6563409519018806		[learning rate: 0.00099595]
		[batch 20/20] avg loss: 1.6249429590600308		[learning rate: 0.00099414]
	Learning Rate: 0.00099414
	LOSS [training: 1.6406419554809557 | validation: 2.5339066158674175]
	TIME [epoch: 8.39 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6419655389686618		[learning rate: 0.00099233]
		[batch 20/20] avg loss: 1.68002243747973		[learning rate: 0.00099053]
	Learning Rate: 0.000990532
	LOSS [training: 1.660993988224196 | validation: 2.5223278565575464]
	TIME [epoch: 8.39 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3822776876533769		[learning rate: 0.00098873]
		[batch 20/20] avg loss: 1.891009898952273		[learning rate: 0.00098694]
	Learning Rate: 0.000986937
	LOSS [training: 1.636643793302825 | validation: 2.515655986504208]
	TIME [epoch: 8.39 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4936772866182455		[learning rate: 0.00098514]
		[batch 20/20] avg loss: 1.7859375275702938		[learning rate: 0.00098336]
	Learning Rate: 0.000983355
	LOSS [training: 1.63980740709427 | validation: 2.517053500230429]
	TIME [epoch: 8.41 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.707884562297335		[learning rate: 0.00098157]
		[batch 20/20] avg loss: 1.5487003299966913		[learning rate: 0.00097979]
	Learning Rate: 0.000979787
	LOSS [training: 1.6282924461470132 | validation: 2.5221709795811065]
	TIME [epoch: 8.38 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6309132094793006		[learning rate: 0.00097801]
		[batch 20/20] avg loss: 1.641138596247536		[learning rate: 0.00097623]
	Learning Rate: 0.000976231
	LOSS [training: 1.6360259028634185 | validation: 2.516305152064745]
	TIME [epoch: 8.39 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.709557306930585		[learning rate: 0.00097446]
		[batch 20/20] avg loss: 1.6332106224959066		[learning rate: 0.00097269]
	Learning Rate: 0.000972688
	LOSS [training: 1.6713839647132454 | validation: 2.528424823940911]
	TIME [epoch: 8.39 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6955458943245927		[learning rate: 0.00097092]
		[batch 20/20] avg loss: 1.5775474545998853		[learning rate: 0.00096916]
	Learning Rate: 0.000969158
	LOSS [training: 1.6365466744622388 | validation: 2.514970466682673]
	TIME [epoch: 8.38 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5872549958230375		[learning rate: 0.0009674]
		[batch 20/20] avg loss: 1.7638822530930227		[learning rate: 0.00096564]
	Learning Rate: 0.000965641
	LOSS [training: 1.6755686244580303 | validation: 2.5400558030980314]
	TIME [epoch: 8.41 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.656160338343205		[learning rate: 0.00096389]
		[batch 20/20] avg loss: 1.6564833702637862		[learning rate: 0.00096214]
	Learning Rate: 0.000962137
	LOSS [training: 1.6563218543034957 | validation: 2.588116758314892]
	TIME [epoch: 8.38 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7486221915996913		[learning rate: 0.00096039]
		[batch 20/20] avg loss: 1.552433419741361		[learning rate: 0.00095865]
	Learning Rate: 0.000958645
	LOSS [training: 1.6505278056705261 | validation: 2.575599464141959]
	TIME [epoch: 8.38 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.644619420667007		[learning rate: 0.0009569]
		[batch 20/20] avg loss: 1.650869412482145		[learning rate: 0.00095517]
	Learning Rate: 0.000955166
	LOSS [training: 1.6477444165745763 | validation: 2.5449777893142462]
	TIME [epoch: 8.38 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6047386700833772		[learning rate: 0.00095343]
		[batch 20/20] avg loss: 1.6975469001515786		[learning rate: 0.0009517]
	Learning Rate: 0.0009517
	LOSS [training: 1.6511427851174776 | validation: 2.5479519209051786]
	TIME [epoch: 8.41 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5451805278827249		[learning rate: 0.00094997]
		[batch 20/20] avg loss: 1.7236596648133204		[learning rate: 0.00094825]
	Learning Rate: 0.000948246
	LOSS [training: 1.6344200963480222 | validation: 2.50765976195444]
	TIME [epoch: 8.37 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6274054805388356		[learning rate: 0.00094652]
		[batch 20/20] avg loss: 1.6423682305634557		[learning rate: 0.0009448]
	Learning Rate: 0.000944805
	LOSS [training: 1.6348868555511458 | validation: 2.548407644541288]
	TIME [epoch: 8.39 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8262793927031649		[learning rate: 0.00094309]
		[batch 20/20] avg loss: 1.454725434574221		[learning rate: 0.00094138]
	Learning Rate: 0.000941376
	LOSS [training: 1.6405024136386928 | validation: 2.555076227641685]
	TIME [epoch: 8.38 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6573860228321258		[learning rate: 0.00093967]
		[batch 20/20] avg loss: 1.677651345672017		[learning rate: 0.00093796]
	Learning Rate: 0.00093796
	LOSS [training: 1.6675186842520717 | validation: 2.523701861814252]
	TIME [epoch: 8.4 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6708195894423734		[learning rate: 0.00093626]
		[batch 20/20] avg loss: 1.6135340913677532		[learning rate: 0.00093456]
	Learning Rate: 0.000934556
	LOSS [training: 1.6421768404050632 | validation: 2.5585555788212426]
	TIME [epoch: 8.38 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7806513001710012		[learning rate: 0.00093286]
		[batch 20/20] avg loss: 1.4993121103625893		[learning rate: 0.00093116]
	Learning Rate: 0.000931164
	LOSS [training: 1.6399817052667953 | validation: 2.545800574214443]
	TIME [epoch: 8.38 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5335940544620903		[learning rate: 0.00092947]
		[batch 20/20] avg loss: 1.7340663153878573		[learning rate: 0.00092779]
	Learning Rate: 0.000927785
	LOSS [training: 1.633830184924974 | validation: 2.550019600455837]
	TIME [epoch: 8.38 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7237945138258817		[learning rate: 0.0009261]
		[batch 20/20] avg loss: 1.5679078917933384		[learning rate: 0.00092442]
	Learning Rate: 0.000924418
	LOSS [training: 1.6458512028096102 | validation: 2.543648251984587]
	TIME [epoch: 8.38 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5439350614146057		[learning rate: 0.00092274]
		[batch 20/20] avg loss: 1.7329971087714136		[learning rate: 0.00092106]
	Learning Rate: 0.000921063
	LOSS [training: 1.6384660850930097 | validation: 2.546116005067963]
	TIME [epoch: 8.4 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.597695876965871		[learning rate: 0.00091939]
		[batch 20/20] avg loss: 1.6677838281528672		[learning rate: 0.00091772]
	Learning Rate: 0.000917721
	LOSS [training: 1.632739852559369 | validation: 2.5194958563189433]
	TIME [epoch: 8.39 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5866845180269844		[learning rate: 0.00091605]
		[batch 20/20] avg loss: 1.6897083391765615		[learning rate: 0.00091439]
	Learning Rate: 0.00091439
	LOSS [training: 1.638196428601773 | validation: 2.5417820415976236]
	TIME [epoch: 8.37 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6011091333000187		[learning rate: 0.00091273]
		[batch 20/20] avg loss: 1.7117245701782067		[learning rate: 0.00091107]
	Learning Rate: 0.000911072
	LOSS [training: 1.6564168517391127 | validation: 2.5303962992737628]
	TIME [epoch: 8.37 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.562730050148409		[learning rate: 0.00090942]
		[batch 20/20] avg loss: 1.7304294340717192		[learning rate: 0.00090777]
	Learning Rate: 0.000907766
	LOSS [training: 1.6465797421100639 | validation: 2.551082489550338]
	TIME [epoch: 8.4 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7544813956092717		[learning rate: 0.00090612]
		[batch 20/20] avg loss: 1.5363789192563821		[learning rate: 0.00090447]
	Learning Rate: 0.000904471
	LOSS [training: 1.645430157432827 | validation: 2.6193333558762544]
	TIME [epoch: 8.38 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4959931249541507		[learning rate: 0.00090283]
		[batch 20/20] avg loss: 1.7941916410085406		[learning rate: 0.00090119]
	Learning Rate: 0.000901189
	LOSS [training: 1.6450923829813455 | validation: 2.5329751746053013]
	TIME [epoch: 8.38 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7138642744617436		[learning rate: 0.00089955]
		[batch 20/20] avg loss: 1.6129501609950185		[learning rate: 0.00089792]
	Learning Rate: 0.000897918
	LOSS [training: 1.6634072177283805 | validation: 2.5536830485376045]
	TIME [epoch: 8.37 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7023842925530677		[learning rate: 0.00089629]
		[batch 20/20] avg loss: 1.5716001184406871		[learning rate: 0.00089466]
	Learning Rate: 0.00089466
	LOSS [training: 1.636992205496877 | validation: 2.5186662627362923]
	TIME [epoch: 8.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6680963905248312		[learning rate: 0.00089303]
		[batch 20/20] avg loss: 1.5994447013980924		[learning rate: 0.00089141]
	Learning Rate: 0.000891413
	LOSS [training: 1.6337705459614618 | validation: 2.5083932691267155]
	TIME [epoch: 8.38 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4989688471343379		[learning rate: 0.00088979]
		[batch 20/20] avg loss: 1.808899557192652		[learning rate: 0.00088818]
	Learning Rate: 0.000888178
	LOSS [training: 1.653934202163495 | validation: 2.544363629762886]
	TIME [epoch: 8.38 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5551397122261095		[learning rate: 0.00088656]
		[batch 20/20] avg loss: 1.7356169495678997		[learning rate: 0.00088495]
	Learning Rate: 0.000884955
	LOSS [training: 1.6453783308970045 | validation: 2.6300071912516483]
	TIME [epoch: 8.37 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.686943463965044		[learning rate: 0.00088335]
		[batch 20/20] avg loss: 1.6536590263861732		[learning rate: 0.00088174]
	Learning Rate: 0.000881743
	LOSS [training: 1.6703012451756087 | validation: 2.5072896988400895]
	TIME [epoch: 8.37 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6514023666439919		[learning rate: 0.00088014]
		[batch 20/20] avg loss: 1.6021802515182066		[learning rate: 0.00087854]
	Learning Rate: 0.000878543
	LOSS [training: 1.6267913090810988 | validation: 2.534711765741081]
	TIME [epoch: 8.4 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.47968518762152		[learning rate: 0.00087695]
		[batch 20/20] avg loss: 1.7915870965055976		[learning rate: 0.00087536]
	Learning Rate: 0.000875355
	LOSS [training: 1.635636142063559 | validation: 2.5107945917335184]
	TIME [epoch: 8.37 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5954957935231486		[learning rate: 0.00087377]
		[batch 20/20] avg loss: 1.7021916439623261		[learning rate: 0.00087218]
	Learning Rate: 0.000872178
	LOSS [training: 1.6488437187427372 | validation: 2.507980723683693]
	TIME [epoch: 8.38 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7749473351450586		[learning rate: 0.00087059]
		[batch 20/20] avg loss: 1.4934743636050167		[learning rate: 0.00086901]
	Learning Rate: 0.000869013
	LOSS [training: 1.6342108493750378 | validation: 2.522079115389076]
	TIME [epoch: 8.37 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6669110668732032		[learning rate: 0.00086743]
		[batch 20/20] avg loss: 1.628497649722916		[learning rate: 0.00086586]
	Learning Rate: 0.000865859
	LOSS [training: 1.64770435829806 | validation: 2.6462118763650198]
	TIME [epoch: 8.39 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.671797839795731		[learning rate: 0.00086429]
		[batch 20/20] avg loss: 1.6076063716146218		[learning rate: 0.00086272]
	Learning Rate: 0.000862717
	LOSS [training: 1.6397021057051764 | validation: 2.539987807657682]
	TIME [epoch: 8.37 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5604292400380686		[learning rate: 0.00086115]
		[batch 20/20] avg loss: 1.724822957766126		[learning rate: 0.00085959]
	Learning Rate: 0.000859586
	LOSS [training: 1.642626098902097 | validation: 2.6089745436886354]
	TIME [epoch: 8.37 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5977287286484292		[learning rate: 0.00085803]
		[batch 20/20] avg loss: 1.7168599782618852		[learning rate: 0.00085647]
	Learning Rate: 0.000856467
	LOSS [training: 1.6572943534551574 | validation: 2.520713836560182]
	TIME [epoch: 8.38 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5986344130403225		[learning rate: 0.00085491]
		[batch 20/20] avg loss: 1.7083839863220345		[learning rate: 0.00085336]
	Learning Rate: 0.000853359
	LOSS [training: 1.6535091996811786 | validation: 2.5274096433638946]
	TIME [epoch: 8.4 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.651907973955146		[learning rate: 0.00085181]
		[batch 20/20] avg loss: 1.6589703592040865		[learning rate: 0.00085026]
	Learning Rate: 0.000850262
	LOSS [training: 1.6554391665796164 | validation: 2.5234368689494726]
	TIME [epoch: 8.38 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.757292192490229		[learning rate: 0.00084872]
		[batch 20/20] avg loss: 1.5194555526173756		[learning rate: 0.00084718]
	Learning Rate: 0.000847176
	LOSS [training: 1.638373872553802 | validation: 2.5265167302676623]
	TIME [epoch: 8.38 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6274802376108604		[learning rate: 0.00084564]
		[batch 20/20] avg loss: 1.6537174784843685		[learning rate: 0.0008441]
	Learning Rate: 0.000844102
	LOSS [training: 1.6405988580476145 | validation: 2.5650801175358997]
	TIME [epoch: 8.38 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6522476769816765		[learning rate: 0.00084257]
		[batch 20/20] avg loss: 1.619042432308241		[learning rate: 0.00084104]
	Learning Rate: 0.000841038
	LOSS [training: 1.635645054644959 | validation: 2.512236129714061]
	TIME [epoch: 8.37 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6466921564342318		[learning rate: 0.00083951]
		[batch 20/20] avg loss: 1.6255438258464836		[learning rate: 0.00083799]
	Learning Rate: 0.000837986
	LOSS [training: 1.6361179911403578 | validation: 2.5123236925125014]
	TIME [epoch: 8.41 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6518252605386263		[learning rate: 0.00083646]
		[batch 20/20] avg loss: 1.615535951846533		[learning rate: 0.00083495]
	Learning Rate: 0.000834945
	LOSS [training: 1.63368060619258 | validation: 2.5008518912241353]
	TIME [epoch: 8.38 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6262075703878633		[learning rate: 0.00083343]
		[batch 20/20] avg loss: 1.6609115952954305		[learning rate: 0.00083192]
	Learning Rate: 0.000831915
	LOSS [training: 1.6435595828416472 | validation: 2.523846002453835]
	TIME [epoch: 8.38 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.462546126568816		[learning rate: 0.0008304]
		[batch 20/20] avg loss: 1.8161015197225914		[learning rate: 0.0008289]
	Learning Rate: 0.000828896
	LOSS [training: 1.6393238231457037 | validation: 2.5058218497980027]
	TIME [epoch: 8.38 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5948675337483909		[learning rate: 0.00082739]
		[batch 20/20] avg loss: 1.671073498347674		[learning rate: 0.00082589]
	Learning Rate: 0.000825888
	LOSS [training: 1.6329705160480323 | validation: 2.5255874999838284]
	TIME [epoch: 8.41 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5500327986845939		[learning rate: 0.00082439]
		[batch 20/20] avg loss: 1.723303261024018		[learning rate: 0.00082289]
	Learning Rate: 0.000822891
	LOSS [training: 1.636668029854306 | validation: 2.510991820294122]
	TIME [epoch: 8.39 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.491014116357214		[learning rate: 0.0008214]
		[batch 20/20] avg loss: 1.8012192913040757		[learning rate: 0.0008199]
	Learning Rate: 0.000819904
	LOSS [training: 1.646116703830645 | validation: 2.5316813524718214]
	TIME [epoch: 8.38 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.571362484682169		[learning rate: 0.00081842]
		[batch 20/20] avg loss: 1.6958576507735965		[learning rate: 0.00081693]
	Learning Rate: 0.000816929
	LOSS [training: 1.6336100677278829 | validation: 2.5226174894951847]
	TIME [epoch: 8.38 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7395737644261646		[learning rate: 0.00081545]
		[batch 20/20] avg loss: 1.538267148780248		[learning rate: 0.00081396]
	Learning Rate: 0.000813964
	LOSS [training: 1.6389204566032063 | validation: 2.5076069651298862]
	TIME [epoch: 8.4 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5870022786925437		[learning rate: 0.00081249]
		[batch 20/20] avg loss: 1.7061395660827223		[learning rate: 0.00081101]
	Learning Rate: 0.00081101
	LOSS [training: 1.646570922387633 | validation: 2.5072417419104958]
	TIME [epoch: 8.38 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.58116188001175		[learning rate: 0.00080954]
		[batch 20/20] avg loss: 1.6795079689658352		[learning rate: 0.00080807]
	Learning Rate: 0.000808067
	LOSS [training: 1.6303349244887921 | validation: 2.565331915312583]
	TIME [epoch: 8.38 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6596929365219304		[learning rate: 0.0008066]
		[batch 20/20] avg loss: 1.6185019589582457		[learning rate: 0.00080513]
	Learning Rate: 0.000805135
	LOSS [training: 1.6390974477400881 | validation: 2.592125244873288]
	TIME [epoch: 8.38 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5807898403380267		[learning rate: 0.00080367]
		[batch 20/20] avg loss: 1.7311387202915207		[learning rate: 0.00080221]
	Learning Rate: 0.000802213
	LOSS [training: 1.6559642803147736 | validation: 2.5360696116067345]
	TIME [epoch: 8.39 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5941411174771698		[learning rate: 0.00080076]
		[batch 20/20] avg loss: 1.6824433260343994		[learning rate: 0.0007993]
	Learning Rate: 0.000799301
	LOSS [training: 1.6382922217557847 | validation: 2.50978164610016]
	TIME [epoch: 8.39 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6556912963286465		[learning rate: 0.00079785]
		[batch 20/20] avg loss: 1.6070106244792324		[learning rate: 0.0007964]
	Learning Rate: 0.000796401
	LOSS [training: 1.6313509604039396 | validation: 2.5203744256602962]
	TIME [epoch: 8.38 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6567710817251124		[learning rate: 0.00079495]
		[batch 20/20] avg loss: 1.6238696762352998		[learning rate: 0.00079351]
	Learning Rate: 0.000793511
	LOSS [training: 1.6403203789802059 | validation: 2.5635153101706623]
	TIME [epoch: 8.39 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5931895447651823		[learning rate: 0.00079207]
		[batch 20/20] avg loss: 1.6708558935577105		[learning rate: 0.00079063]
	Learning Rate: 0.000790631
	LOSS [training: 1.6320227191614465 | validation: 2.513973208304513]
	TIME [epoch: 8.39 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7945404696774445		[learning rate: 0.00078919]
		[batch 20/20] avg loss: 1.5009555657155644		[learning rate: 0.00078776]
	Learning Rate: 0.000787761
	LOSS [training: 1.6477480176965045 | validation: 2.533816190193367]
	TIME [epoch: 8.41 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6211031246887881		[learning rate: 0.00078633]
		[batch 20/20] avg loss: 1.6618121038462248		[learning rate: 0.0007849]
	Learning Rate: 0.000784903
	LOSS [training: 1.6414576142675066 | validation: 2.50412665970127]
	TIME [epoch: 8.38 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7922661804646378		[learning rate: 0.00078348]
		[batch 20/20] avg loss: 1.4745315483744224		[learning rate: 0.00078205]
	Learning Rate: 0.000782054
	LOSS [training: 1.63339886441953 | validation: 2.5451707534275907]
	TIME [epoch: 8.39 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5763389370698193		[learning rate: 0.00078063]
		[batch 20/20] avg loss: 1.6994499535609904		[learning rate: 0.00077922]
	Learning Rate: 0.000779216
	LOSS [training: 1.6378944453154047 | validation: 2.524074496342419]
	TIME [epoch: 8.38 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5467993808740783		[learning rate: 0.0007778]
		[batch 20/20] avg loss: 1.7117690583109813		[learning rate: 0.00077639]
	Learning Rate: 0.000776388
	LOSS [training: 1.6292842195925297 | validation: 2.5181139945193087]
	TIME [epoch: 8.4 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7145506072135663		[learning rate: 0.00077498]
		[batch 20/20] avg loss: 1.5970585784143159		[learning rate: 0.00077357]
	Learning Rate: 0.000773571
	LOSS [training: 1.6558045928139413 | validation: 2.5021296177199335]
	TIME [epoch: 8.38 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7379433527449926		[learning rate: 0.00077217]
		[batch 20/20] avg loss: 1.5559684703170809		[learning rate: 0.00077076]
	Learning Rate: 0.000770763
	LOSS [training: 1.6469559115310364 | validation: 2.5358583407646047]
	TIME [epoch: 8.38 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6739709598726094		[learning rate: 0.00076936]
		[batch 20/20] avg loss: 1.605157723048753		[learning rate: 0.00076797]
	Learning Rate: 0.000767966
	LOSS [training: 1.6395643414606809 | validation: 2.5381274704013626]
	TIME [epoch: 8.37 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6478297395569506		[learning rate: 0.00076657]
		[batch 20/20] avg loss: 1.6413147101499828		[learning rate: 0.00076518]
	Learning Rate: 0.000765179
	LOSS [training: 1.6445722248534669 | validation: 2.515311522107396]
	TIME [epoch: 8.39 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7195136918342082		[learning rate: 0.00076379]
		[batch 20/20] avg loss: 1.5530346416398682		[learning rate: 0.0007624]
	Learning Rate: 0.000762402
	LOSS [training: 1.636274166737038 | validation: 2.5371845056017124]
	TIME [epoch: 8.39 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4352933238413235		[learning rate: 0.00076102]
		[batch 20/20] avg loss: 1.8333705324608858		[learning rate: 0.00075964]
	Learning Rate: 0.000759636
	LOSS [training: 1.6343319281511044 | validation: 2.503798512285527]
	TIME [epoch: 8.38 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5849021339822793		[learning rate: 0.00075826]
		[batch 20/20] avg loss: 1.6936884431439831		[learning rate: 0.00075688]
	Learning Rate: 0.000756879
	LOSS [training: 1.6392952885631318 | validation: 2.521423476544034]
	TIME [epoch: 8.38 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7233223553874417		[learning rate: 0.0007555]
		[batch 20/20] avg loss: 1.5466893942224578		[learning rate: 0.00075413]
	Learning Rate: 0.000754132
	LOSS [training: 1.6350058748049499 | validation: 2.518834686500891]
	TIME [epoch: 8.38 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7114207792262544		[learning rate: 0.00075276]
		[batch 20/20] avg loss: 1.5907717092611906		[learning rate: 0.0007514]
	Learning Rate: 0.000751395
	LOSS [training: 1.6510962442437225 | validation: 2.5268439910765075]
	TIME [epoch: 8.41 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6539533314058563		[learning rate: 0.00075003]
		[batch 20/20] avg loss: 1.6321848752900219		[learning rate: 0.00074867]
	Learning Rate: 0.000748668
	LOSS [training: 1.6430691033479394 | validation: 2.582695118194936]
	TIME [epoch: 8.38 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6511173479401697		[learning rate: 0.00074731]
		[batch 20/20] avg loss: 1.6376764132605603		[learning rate: 0.00074595]
	Learning Rate: 0.000745951
	LOSS [training: 1.644396880600365 | validation: 2.5091241617495177]
	TIME [epoch: 8.38 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7403998233131528		[learning rate: 0.0007446]
		[batch 20/20] avg loss: 1.552535726555335		[learning rate: 0.00074324]
	Learning Rate: 0.000743244
	LOSS [training: 1.6464677749342438 | validation: 2.5734232554598204]
	TIME [epoch: 8.38 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4991047038025083		[learning rate: 0.00074189]
		[batch 20/20] avg loss: 1.7661262964185422		[learning rate: 0.00074055]
	Learning Rate: 0.000740547
	LOSS [training: 1.6326155001105256 | validation: 2.509340663686025]
	TIME [epoch: 8.41 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5234255334812135		[learning rate: 0.0007392]
		[batch 20/20] avg loss: 1.7288527327382817		[learning rate: 0.00073786]
	Learning Rate: 0.00073786
	LOSS [training: 1.6261391331097474 | validation: 2.5092875988028562]
	TIME [epoch: 8.38 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6795727447059505		[learning rate: 0.00073652]
		[batch 20/20] avg loss: 1.5975020711476258		[learning rate: 0.00073518]
	Learning Rate: 0.000735182
	LOSS [training: 1.638537407926788 | validation: 2.5238389817492033]
	TIME [epoch: 8.38 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7819274900692406		[learning rate: 0.00073385]
		[batch 20/20] avg loss: 1.4960200917343234		[learning rate: 0.00073251]
	Learning Rate: 0.000732514
	LOSS [training: 1.638973790901782 | validation: 2.5063968345260923]
	TIME [epoch: 8.38 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6013034835614193		[learning rate: 0.00073118]
		[batch 20/20] avg loss: 1.6653189668005228		[learning rate: 0.00072986]
	Learning Rate: 0.000729855
	LOSS [training: 1.633311225180971 | validation: 2.5025835940595735]
	TIME [epoch: 8.39 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5822666917874637		[learning rate: 0.00072853]
		[batch 20/20] avg loss: 1.7043713469768083		[learning rate: 0.00072721]
	Learning Rate: 0.000727207
	LOSS [training: 1.643319019382136 | validation: 2.568484753404411]
	TIME [epoch: 8.38 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.629521026699899		[learning rate: 0.00072589]
		[batch 20/20] avg loss: 1.6582251671200379		[learning rate: 0.00072457]
	Learning Rate: 0.000724568
	LOSS [training: 1.6438730969099686 | validation: 2.507856948674146]
	TIME [epoch: 8.38 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6261602945907054		[learning rate: 0.00072325]
		[batch 20/20] avg loss: 1.6469370770734557		[learning rate: 0.00072194]
	Learning Rate: 0.000721938
	LOSS [training: 1.6365486858320804 | validation: 2.5045114054597564]
	TIME [epoch: 8.37 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5379598114692987		[learning rate: 0.00072063]
		[batch 20/20] avg loss: 1.740792017452015		[learning rate: 0.00071932]
	Learning Rate: 0.000719318
	LOSS [training: 1.639375914460657 | validation: 2.516375207114493]
	TIME [epoch: 8.38 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.640129331687092		[learning rate: 0.00071801]
		[batch 20/20] avg loss: 1.6663028649128595		[learning rate: 0.00071671]
	Learning Rate: 0.000716708
	LOSS [training: 1.6532160982999755 | validation: 2.5602975182862173]
	TIME [epoch: 8.4 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7779670915444723		[learning rate: 0.00071541]
		[batch 20/20] avg loss: 1.5110769452367276		[learning rate: 0.00071411]
	Learning Rate: 0.000714107
	LOSS [training: 1.6445220183906 | validation: 2.5307226743934104]
	TIME [epoch: 8.38 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5755102185480787		[learning rate: 0.00071281]
		[batch 20/20] avg loss: 1.6897007665405563		[learning rate: 0.00071152]
	Learning Rate: 0.000711515
	LOSS [training: 1.6326054925443176 | validation: 2.588734163269103]
	TIME [epoch: 8.38 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5392148941536365		[learning rate: 0.00071022]
		[batch 20/20] avg loss: 1.7325830997544007		[learning rate: 0.00070893]
	Learning Rate: 0.000708933
	LOSS [training: 1.6358989969540183 | validation: 2.557204355791175]
	TIME [epoch: 8.38 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.561642814734905		[learning rate: 0.00070765]
		[batch 20/20] avg loss: 1.6990417562998725		[learning rate: 0.00070636]
	Learning Rate: 0.00070636
	LOSS [training: 1.6303422855173886 | validation: 2.5198746759862596]
	TIME [epoch: 8.4 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6305735017208676		[learning rate: 0.00070508]
		[batch 20/20] avg loss: 1.6253522836269183		[learning rate: 0.0007038]
	Learning Rate: 0.000703797
	LOSS [training: 1.627962892673893 | validation: 2.5151840211469794]
	TIME [epoch: 8.39 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.667586718035898		[learning rate: 0.00070252]
		[batch 20/20] avg loss: 1.6341422114132567		[learning rate: 0.00070124]
	Learning Rate: 0.000701243
	LOSS [training: 1.650864464724577 | validation: 2.508939384158761]
	TIME [epoch: 8.39 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5112760990615162		[learning rate: 0.00069997]
		[batch 20/20] avg loss: 1.7536340596961857		[learning rate: 0.0006987]
	Learning Rate: 0.000698698
	LOSS [training: 1.6324550793788508 | validation: 2.5079740419544696]
	TIME [epoch: 8.39 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7682744489669653		[learning rate: 0.00069743]
		[batch 20/20] avg loss: 1.496610299892049		[learning rate: 0.00069616]
	Learning Rate: 0.000696162
	LOSS [training: 1.6324423744295067 | validation: 2.5165775640684624]
	TIME [epoch: 8.4 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.555943666655919		[learning rate: 0.0006949]
		[batch 20/20] avg loss: 1.7214454314266323		[learning rate: 0.00069364]
	Learning Rate: 0.000693636
	LOSS [training: 1.6386945490412756 | validation: 2.536616111173175]
	TIME [epoch: 8.38 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7286983971606111		[learning rate: 0.00069238]
		[batch 20/20] avg loss: 1.5381234897197875		[learning rate: 0.00069112]
	Learning Rate: 0.000691119
	LOSS [training: 1.6334109434401998 | validation: 2.722586607611591]
	TIME [epoch: 8.38 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6444134456737984		[learning rate: 0.00068986]
		[batch 20/20] avg loss: 1.6547535568148448		[learning rate: 0.00068861]
	Learning Rate: 0.000688611
	LOSS [training: 1.6495835012443216 | validation: 2.5139245841292692]
	TIME [epoch: 8.38 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.666483526964575		[learning rate: 0.00068736]
		[batch 20/20] avg loss: 1.6000519194286849		[learning rate: 0.00068611]
	Learning Rate: 0.000686112
	LOSS [training: 1.6332677231966302 | validation: 2.5168537682562535]
	TIME [epoch: 8.38 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7166420240945033		[learning rate: 0.00068487]
		[batch 20/20] avg loss: 1.5903896927654078		[learning rate: 0.00068362]
	Learning Rate: 0.000683622
	LOSS [training: 1.6535158584299556 | validation: 2.5115287419624375]
	TIME [epoch: 8.4 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6521576936571005		[learning rate: 0.00068238]
		[batch 20/20] avg loss: 1.6010022514119704		[learning rate: 0.00068114]
	Learning Rate: 0.000681141
	LOSS [training: 1.6265799725345356 | validation: 2.5377967976984097]
	TIME [epoch: 8.37 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6596910879118638		[learning rate: 0.0006799]
		[batch 20/20] avg loss: 1.5993588241530854		[learning rate: 0.00067867]
	Learning Rate: 0.000678669
	LOSS [training: 1.629524956032475 | validation: 2.525891109796259]
	TIME [epoch: 8.38 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7411268868861989		[learning rate: 0.00067744]
		[batch 20/20] avg loss: 1.5095812505085004		[learning rate: 0.00067621]
	Learning Rate: 0.000676206
	LOSS [training: 1.6253540686973498 | validation: 2.515254752827511]
	TIME [epoch: 8.38 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.544133388327495		[learning rate: 0.00067498]
		[batch 20/20] avg loss: 1.7159216059139824		[learning rate: 0.00067375]
	Learning Rate: 0.000673752
	LOSS [training: 1.6300274971207387 | validation: 2.5496819961050714]
	TIME [epoch: 8.4 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7058505366102978		[learning rate: 0.00067253]
		[batch 20/20] avg loss: 1.553601637968581		[learning rate: 0.00067131]
	Learning Rate: 0.000671307
	LOSS [training: 1.62972608728944 | validation: 2.5463056558822816]
	TIME [epoch: 8.39 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7669564175349433		[learning rate: 0.00067009]
		[batch 20/20] avg loss: 1.5084608258579062		[learning rate: 0.00066887]
	Learning Rate: 0.000668871
	LOSS [training: 1.6377086216964247 | validation: 2.552214486692914]
	TIME [epoch: 8.39 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8136293810254736		[learning rate: 0.00066766]
		[batch 20/20] avg loss: 1.4450993367467768		[learning rate: 0.00066644]
	Learning Rate: 0.000666443
	LOSS [training: 1.629364358886125 | validation: 2.5222302169229582]
	TIME [epoch: 8.38 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.550879714464411		[learning rate: 0.00066523]
		[batch 20/20] avg loss: 1.707560304333803		[learning rate: 0.00066402]
	Learning Rate: 0.000664025
	LOSS [training: 1.629220009399107 | validation: 2.5052054368364436]
	TIME [epoch: 8.4 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6964417836536405		[learning rate: 0.00066282]
		[batch 20/20] avg loss: 1.5595549921678313		[learning rate: 0.00066161]
	Learning Rate: 0.000661615
	LOSS [training: 1.6279983879107356 | validation: 2.507697464244809]
	TIME [epoch: 8.38 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5912988450129801		[learning rate: 0.00066041]
		[batch 20/20] avg loss: 1.6747862047855098		[learning rate: 0.00065921]
	Learning Rate: 0.000659214
	LOSS [training: 1.6330425248992448 | validation: 2.5172113946072026]
	TIME [epoch: 8.38 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6844281973055324		[learning rate: 0.00065802]
		[batch 20/20] avg loss: 1.6060862979659205		[learning rate: 0.00065682]
	Learning Rate: 0.000656822
	LOSS [training: 1.6452572476357266 | validation: 2.513702117979057]
	TIME [epoch: 8.38 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.574625512965717		[learning rate: 0.00065563]
		[batch 20/20] avg loss: 1.721436456059233		[learning rate: 0.00065444]
	Learning Rate: 0.000654438
	LOSS [training: 1.6480309845124748 | validation: 2.498957116509981]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_850.pth
	Model improved!!!
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8210487697992588		[learning rate: 0.00065325]
		[batch 20/20] avg loss: 1.457468799599927		[learning rate: 0.00065206]
	Learning Rate: 0.000652063
	LOSS [training: 1.639258784699593 | validation: 2.5034274408406048]
	TIME [epoch: 8.4 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.702762861686083		[learning rate: 0.00065088]
		[batch 20/20] avg loss: 1.5693192750189802		[learning rate: 0.0006497]
	Learning Rate: 0.000649697
	LOSS [training: 1.6360410683525317 | validation: 2.501563600485872]
	TIME [epoch: 8.37 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7805019003459823		[learning rate: 0.00064852]
		[batch 20/20] avg loss: 1.4814856935062015		[learning rate: 0.00064734]
	Learning Rate: 0.000647339
	LOSS [training: 1.630993796926092 | validation: 2.5177323833053276]
	TIME [epoch: 8.37 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.465481233488442		[learning rate: 0.00064616]
		[batch 20/20] avg loss: 1.8090509066361005		[learning rate: 0.00064499]
	Learning Rate: 0.00064499
	LOSS [training: 1.6372660700622714 | validation: 2.519030928055452]
	TIME [epoch: 8.37 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5091785741108374		[learning rate: 0.00064382]
		[batch 20/20] avg loss: 1.783955441885866		[learning rate: 0.00064265]
	Learning Rate: 0.000642649
	LOSS [training: 1.6465670079983514 | validation: 2.534978846988743]
	TIME [epoch: 8.39 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.51173267500551		[learning rate: 0.00064148]
		[batch 20/20] avg loss: 1.7940010088588778		[learning rate: 0.00064032]
	Learning Rate: 0.000640317
	LOSS [training: 1.6528668419321941 | validation: 2.635975234325609]
	TIME [epoch: 8.37 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6058208832061815		[learning rate: 0.00063915]
		[batch 20/20] avg loss: 1.6811127666783654		[learning rate: 0.00063799]
	Learning Rate: 0.000637993
	LOSS [training: 1.6434668249422735 | validation: 2.5112560514164004]
	TIME [epoch: 8.37 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4901085003239811		[learning rate: 0.00063683]
		[batch 20/20] avg loss: 1.759637613439385		[learning rate: 0.00063568]
	Learning Rate: 0.000635677
	LOSS [training: 1.624873056881683 | validation: 2.58786145716258]
	TIME [epoch: 8.36 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6449467138083187		[learning rate: 0.00063452]
		[batch 20/20] avg loss: 1.614131071375348		[learning rate: 0.00063337]
	Learning Rate: 0.000633371
	LOSS [training: 1.629538892591833 | validation: 2.5045475064599825]
	TIME [epoch: 8.39 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6030458001457586		[learning rate: 0.00063222]
		[batch 20/20] avg loss: 1.6388311705253866		[learning rate: 0.00063107]
	Learning Rate: 0.000631072
	LOSS [training: 1.6209384853355726 | validation: 2.5454287616814084]
	TIME [epoch: 8.37 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7420301921391925		[learning rate: 0.00062993]
		[batch 20/20] avg loss: 1.5438283719107688		[learning rate: 0.00062878]
	Learning Rate: 0.000628782
	LOSS [training: 1.6429292820249806 | validation: 2.524648621138388]
	TIME [epoch: 8.37 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6435691331234572		[learning rate: 0.00062764]
		[batch 20/20] avg loss: 1.6024354468927193		[learning rate: 0.0006265]
	Learning Rate: 0.0006265
	LOSS [training: 1.6230022900080883 | validation: 2.510219999734384]
	TIME [epoch: 8.37 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.62037193649277		[learning rate: 0.00062536]
		[batch 20/20] avg loss: 1.6500922498867545		[learning rate: 0.00062423]
	Learning Rate: 0.000624226
	LOSS [training: 1.6352320931897624 | validation: 2.5087484405602094]
	TIME [epoch: 8.37 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5001600212338653		[learning rate: 0.00062309]
		[batch 20/20] avg loss: 1.7546763481820868		[learning rate: 0.00062196]
	Learning Rate: 0.000621961
	LOSS [training: 1.6274181847079763 | validation: 2.5396400462806303]
	TIME [epoch: 8.37 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6231595885570922		[learning rate: 0.00062083]
		[batch 20/20] avg loss: 1.6287970500569027		[learning rate: 0.0006197]
	Learning Rate: 0.000619704
	LOSS [training: 1.6259783193069972 | validation: 2.5114473294100836]
	TIME [epoch: 8.37 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7980651593466448		[learning rate: 0.00061858]
		[batch 20/20] avg loss: 1.463830891010484		[learning rate: 0.00061745]
	Learning Rate: 0.000617455
	LOSS [training: 1.630948025178564 | validation: 2.5130125386442]
	TIME [epoch: 8.36 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.653003789854528		[learning rate: 0.00061633]
		[batch 20/20] avg loss: 1.6046209124218993		[learning rate: 0.00061521]
	Learning Rate: 0.000615214
	LOSS [training: 1.628812351138214 | validation: 2.5058372760130103]
	TIME [epoch: 8.37 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6308017570841578		[learning rate: 0.0006141]
		[batch 20/20] avg loss: 1.650053785218453		[learning rate: 0.00061298]
	Learning Rate: 0.000612982
	LOSS [training: 1.640427771151305 | validation: 2.5062053400164546]
	TIME [epoch: 8.39 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5653182002117771		[learning rate: 0.00061187]
		[batch 20/20] avg loss: 1.6797331799753785		[learning rate: 0.00061076]
	Learning Rate: 0.000610757
	LOSS [training: 1.6225256900935776 | validation: 2.506204809884244]
	TIME [epoch: 8.37 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7381162864491926		[learning rate: 0.00060965]
		[batch 20/20] avg loss: 1.5137031942575314		[learning rate: 0.00060854]
	Learning Rate: 0.00060854
	LOSS [training: 1.6259097403533622 | validation: 2.531854583369264]
	TIME [epoch: 8.37 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8146266268192506		[learning rate: 0.00060744]
		[batch 20/20] avg loss: 1.4455985817105357		[learning rate: 0.00060633]
	Learning Rate: 0.000606332
	LOSS [training: 1.6301126042648935 | validation: 2.5205284741001166]
	TIME [epoch: 8.36 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6017351191627014		[learning rate: 0.00060523]
		[batch 20/20] avg loss: 1.6580472010829053		[learning rate: 0.00060413]
	Learning Rate: 0.000604132
	LOSS [training: 1.6298911601228034 | validation: 2.522195575774212]
	TIME [epoch: 8.38 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.633490143404724		[learning rate: 0.00060303]
		[batch 20/20] avg loss: 1.6202330299132286		[learning rate: 0.00060194]
	Learning Rate: 0.000601939
	LOSS [training: 1.626861586658976 | validation: 2.507272298968876]
	TIME [epoch: 8.38 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5423054628303408		[learning rate: 0.00060085]
		[batch 20/20] avg loss: 1.7020227003693744		[learning rate: 0.00059975]
	Learning Rate: 0.000599755
	LOSS [training: 1.6221640815998573 | validation: 2.5122127927689384]
	TIME [epoch: 8.36 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.531973790782393		[learning rate: 0.00059867]
		[batch 20/20] avg loss: 1.7208911383960472		[learning rate: 0.00059758]
	Learning Rate: 0.000597578
	LOSS [training: 1.6264324645892203 | validation: 2.516016204391888]
	TIME [epoch: 8.37 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4932964717244404		[learning rate: 0.00059649]
		[batch 20/20] avg loss: 1.7584614337583706		[learning rate: 0.00059541]
	Learning Rate: 0.00059541
	LOSS [training: 1.6258789527414053 | validation: 2.507855766093722]
	TIME [epoch: 8.38 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7658666672826484		[learning rate: 0.00059433]
		[batch 20/20] avg loss: 1.5078697358294935		[learning rate: 0.00059325]
	Learning Rate: 0.000593249
	LOSS [training: 1.6368682015560712 | validation: 2.5320505598051746]
	TIME [epoch: 8.37 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5124760108754685		[learning rate: 0.00059217]
		[batch 20/20] avg loss: 1.741918847664099		[learning rate: 0.0005911]
	Learning Rate: 0.000591096
	LOSS [training: 1.6271974292697837 | validation: 2.570019090843913]
	TIME [epoch: 8.36 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5423524451774067		[learning rate: 0.00059002]
		[batch 20/20] avg loss: 1.7441596341618446		[learning rate: 0.00058895]
	Learning Rate: 0.000588951
	LOSS [training: 1.643256039669626 | validation: 2.5657009200151966]
	TIME [epoch: 8.37 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5857544920757127		[learning rate: 0.00058788]
		[batch 20/20] avg loss: 1.696480226438362		[learning rate: 0.00058681]
	Learning Rate: 0.000586813
	LOSS [training: 1.6411173592570374 | validation: 2.562144549514233]
	TIME [epoch: 8.38 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6429306653362523		[learning rate: 0.00058575]
		[batch 20/20] avg loss: 1.615110918041745		[learning rate: 0.00058468]
	Learning Rate: 0.000584684
	LOSS [training: 1.629020791688999 | validation: 2.5256875809917356]
	TIME [epoch: 8.39 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5281362655355148		[learning rate: 0.00058362]
		[batch 20/20] avg loss: 1.7667472534989734		[learning rate: 0.00058256]
	Learning Rate: 0.000582562
	LOSS [training: 1.647441759517244 | validation: 2.523201278032539]
	TIME [epoch: 8.37 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.543900096792155		[learning rate: 0.0005815]
		[batch 20/20] avg loss: 1.714641117329761		[learning rate: 0.00058045]
	Learning Rate: 0.000580448
	LOSS [training: 1.629270607060958 | validation: 2.5115382098035246]
	TIME [epoch: 8.37 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4942790116646854		[learning rate: 0.00057939]
		[batch 20/20] avg loss: 1.78748364354757		[learning rate: 0.00057834]
	Learning Rate: 0.000578341
	LOSS [training: 1.6408813276061274 | validation: 2.5136946286156663]
	TIME [epoch: 8.36 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6123644984363543		[learning rate: 0.00057729]
		[batch 20/20] avg loss: 1.6466009922203004		[learning rate: 0.00057624]
	Learning Rate: 0.000576243
	LOSS [training: 1.6294827453283272 | validation: 2.543550869365217]
	TIME [epoch: 8.39 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6040028678459932		[learning rate: 0.0005752]
		[batch 20/20] avg loss: 1.659090630636467		[learning rate: 0.00057415]
	Learning Rate: 0.000574151
	LOSS [training: 1.6315467492412299 | validation: 2.509515587429399]
	TIME [epoch: 8.37 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.625379698104282		[learning rate: 0.00057311]
		[batch 20/20] avg loss: 1.6431161622948927		[learning rate: 0.00057207]
	Learning Rate: 0.000572068
	LOSS [training: 1.6342479301995874 | validation: 2.508203183939159]
	TIME [epoch: 8.36 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6563425253150144		[learning rate: 0.00057103]
		[batch 20/20] avg loss: 1.6019879557854686		[learning rate: 0.00056999]
	Learning Rate: 0.000569992
	LOSS [training: 1.6291652405502415 | validation: 2.5212773151047627]
	TIME [epoch: 8.36 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5908836834413207		[learning rate: 0.00056896]
		[batch 20/20] avg loss: 1.6597051848540079		[learning rate: 0.00056792]
	Learning Rate: 0.000567923
	LOSS [training: 1.6252944341476645 | validation: 2.528811214139565]
	TIME [epoch: 8.37 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5941336246245434		[learning rate: 0.00056689]
		[batch 20/20] avg loss: 1.6810943811969217		[learning rate: 0.00056586]
	Learning Rate: 0.000565862
	LOSS [training: 1.637614002910733 | validation: 2.5226618967367234]
	TIME [epoch: 8.38 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6327506766690818		[learning rate: 0.00056483]
		[batch 20/20] avg loss: 1.6286374000570594		[learning rate: 0.00056381]
	Learning Rate: 0.000563808
	LOSS [training: 1.6306940383630706 | validation: 2.509741031202699]
	TIME [epoch: 8.38 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5513315616957981		[learning rate: 0.00056278]
		[batch 20/20] avg loss: 1.689766682083258		[learning rate: 0.00056176]
	Learning Rate: 0.000561762
	LOSS [training: 1.6205491218895283 | validation: 2.526966009115049]
	TIME [epoch: 8.38 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.635243053472579		[learning rate: 0.00056074]
		[batch 20/20] avg loss: 1.6251954076692492		[learning rate: 0.00055972]
	Learning Rate: 0.000559724
	LOSS [training: 1.6302192305709142 | validation: 2.5334645212669997]
	TIME [epoch: 8.38 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5998733784814116		[learning rate: 0.00055871]
		[batch 20/20] avg loss: 1.6709253664895858		[learning rate: 0.00055769]
	Learning Rate: 0.000557692
	LOSS [training: 1.6353993724854987 | validation: 2.5166309469104675]
	TIME [epoch: 8.39 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5512408310352437		[learning rate: 0.00055668]
		[batch 20/20] avg loss: 1.6948831737768937		[learning rate: 0.00055567]
	Learning Rate: 0.000555669
	LOSS [training: 1.6230620024060687 | validation: 2.5258834349433528]
	TIME [epoch: 8.37 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7271304538835142		[learning rate: 0.00055466]
		[batch 20/20] avg loss: 1.5140385293307324		[learning rate: 0.00055365]
	Learning Rate: 0.000553652
	LOSS [training: 1.6205844916071233 | validation: 2.527274837986705]
	TIME [epoch: 8.37 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6730080218656636		[learning rate: 0.00055265]
		[batch 20/20] avg loss: 1.573231844455703		[learning rate: 0.00055164]
	Learning Rate: 0.000551643
	LOSS [training: 1.6231199331606831 | validation: 2.510085286332117]
	TIME [epoch: 8.36 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5963353642523634		[learning rate: 0.00055064]
		[batch 20/20] avg loss: 1.6482752396575542		[learning rate: 0.00054964]
	Learning Rate: 0.000549641
	LOSS [training: 1.622305301954959 | validation: 2.506159196478547]
	TIME [epoch: 8.38 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6390591951611069		[learning rate: 0.00054864]
		[batch 20/20] avg loss: 1.6086035202267843		[learning rate: 0.00054765]
	Learning Rate: 0.000547646
	LOSS [training: 1.6238313576939458 | validation: 2.5144913353067815]
	TIME [epoch: 8.37 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6639339519941587		[learning rate: 0.00054665]
		[batch 20/20] avg loss: 1.578152112491404		[learning rate: 0.00054566]
	Learning Rate: 0.000545659
	LOSS [training: 1.6210430322427816 | validation: 2.4986907239997613]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_900.pth
	Model improved!!!
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5655662144653983		[learning rate: 0.00054467]
		[batch 20/20] avg loss: 1.6896895668883505		[learning rate: 0.00054368]
	Learning Rate: 0.000543678
	LOSS [training: 1.6276278906768749 | validation: 2.506387814665481]
	TIME [epoch: 8.37 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4459650991767536		[learning rate: 0.00054269]
		[batch 20/20] avg loss: 1.8000986148370455		[learning rate: 0.00054171]
	Learning Rate: 0.000541705
	LOSS [training: 1.6230318570068991 | validation: 2.5397917576789766]
	TIME [epoch: 8.39 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.737429384697731		[learning rate: 0.00054072]
		[batch 20/20] avg loss: 1.5199933655954716		[learning rate: 0.00053974]
	Learning Rate: 0.00053974
	LOSS [training: 1.6287113751466014 | validation: 2.520454386880775]
	TIME [epoch: 8.38 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.674442200803475		[learning rate: 0.00053876]
		[batch 20/20] avg loss: 1.5878335742221281		[learning rate: 0.00053778]
	Learning Rate: 0.000537781
	LOSS [training: 1.6311378875128013 | validation: 2.4988623193911192]
	TIME [epoch: 8.38 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7044956609887993		[learning rate: 0.0005368]
		[batch 20/20] avg loss: 1.5289365687158087		[learning rate: 0.00053583]
	Learning Rate: 0.000535829
	LOSS [training: 1.616716114852304 | validation: 2.50907275862813]
	TIME [epoch: 8.37 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5996827245654643		[learning rate: 0.00053486]
		[batch 20/20] avg loss: 1.6518275277594139		[learning rate: 0.00053388]
	Learning Rate: 0.000533885
	LOSS [training: 1.625755126162439 | validation: 2.5540806927875077]
	TIME [epoch: 8.37 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7043027683291938		[learning rate: 0.00053291]
		[batch 20/20] avg loss: 1.5779639782631236		[learning rate: 0.00053195]
	Learning Rate: 0.000531947
	LOSS [training: 1.6411333732961588 | validation: 2.5124116858967613]
	TIME [epoch: 8.4 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6552059726382011		[learning rate: 0.00053098]
		[batch 20/20] avg loss: 1.6006988623848213		[learning rate: 0.00053002]
	Learning Rate: 0.000530017
	LOSS [training: 1.6279524175115114 | validation: 2.5081344996296506]
	TIME [epoch: 8.37 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6036541618936042		[learning rate: 0.00052905]
		[batch 20/20] avg loss: 1.6400451575939148		[learning rate: 0.00052809]
	Learning Rate: 0.000528093
	LOSS [training: 1.6218496597437597 | validation: 2.5057712578392906]
	TIME [epoch: 8.38 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6954230191072643		[learning rate: 0.00052713]
		[batch 20/20] avg loss: 1.5618461323694366		[learning rate: 0.00052618]
	Learning Rate: 0.000526177
	LOSS [training: 1.6286345757383505 | validation: 2.5181673230628254]
	TIME [epoch: 8.37 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.518542972125909		[learning rate: 0.00052522]
		[batch 20/20] avg loss: 1.7305910521424852		[learning rate: 0.00052427]
	Learning Rate: 0.000524267
	LOSS [training: 1.624567012134197 | validation: 2.515932020010993]
	TIME [epoch: 8.4 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7593272958800448		[learning rate: 0.00052332]
		[batch 20/20] avg loss: 1.494352328779493		[learning rate: 0.00052236]
	Learning Rate: 0.000522365
	LOSS [training: 1.626839812329769 | validation: 2.5078860491186328]
	TIME [epoch: 8.37 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6461206411551093		[learning rate: 0.00052142]
		[batch 20/20] avg loss: 1.5959959320200352		[learning rate: 0.00052047]
	Learning Rate: 0.000520469
	LOSS [training: 1.6210582865875722 | validation: 2.527061968662357]
	TIME [epoch: 8.38 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.675610533458491		[learning rate: 0.00051952]
		[batch 20/20] avg loss: 1.6026539827841837		[learning rate: 0.00051858]
	Learning Rate: 0.00051858
	LOSS [training: 1.6391322581213372 | validation: 2.5171180882005744]
	TIME [epoch: 8.37 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6383278963649741		[learning rate: 0.00051764]
		[batch 20/20] avg loss: 1.6163265940915381		[learning rate: 0.0005167]
	Learning Rate: 0.000516698
	LOSS [training: 1.6273272452282561 | validation: 2.5104428957764244]
	TIME [epoch: 8.39 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.685773139526806		[learning rate: 0.00051576]
		[batch 20/20] avg loss: 1.553969756534845		[learning rate: 0.00051482]
	Learning Rate: 0.000514823
	LOSS [training: 1.6198714480308254 | validation: 2.5098950985494914]
	TIME [epoch: 8.37 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7257647763521913		[learning rate: 0.00051389]
		[batch 20/20] avg loss: 1.5146011562227149		[learning rate: 0.00051295]
	Learning Rate: 0.000512955
	LOSS [training: 1.6201829662874534 | validation: 2.531557371462679]
	TIME [epoch: 8.37 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7474432603429961		[learning rate: 0.00051202]
		[batch 20/20] avg loss: 1.5091827965166154		[learning rate: 0.00051109]
	Learning Rate: 0.000511093
	LOSS [training: 1.6283130284298053 | validation: 2.519525693520814]
	TIME [epoch: 8.37 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7807777058455845		[learning rate: 0.00051016]
		[batch 20/20] avg loss: 1.475012796747843		[learning rate: 0.00050924]
	Learning Rate: 0.000509238
	LOSS [training: 1.6278952512967138 | validation: 2.543670506114428]
	TIME [epoch: 8.36 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.648569201079234		[learning rate: 0.00050831]
		[batch 20/20] avg loss: 1.610185426022601		[learning rate: 0.00050739]
	Learning Rate: 0.00050739
	LOSS [training: 1.6293773135509173 | validation: 2.506849355445728]
	TIME [epoch: 8.39 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6543798119194428		[learning rate: 0.00050647]
		[batch 20/20] avg loss: 1.5961792986307382		[learning rate: 0.00050555]
	Learning Rate: 0.000505549
	LOSS [training: 1.6252795552750903 | validation: 2.51167367792194]
	TIME [epoch: 8.37 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5411368091286128		[learning rate: 0.00050463]
		[batch 20/20] avg loss: 1.6943214237192994		[learning rate: 0.00050371]
	Learning Rate: 0.000503714
	LOSS [training: 1.617729116423956 | validation: 2.5054898557144782]
	TIME [epoch: 8.37 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6144629257006415		[learning rate: 0.0005028]
		[batch 20/20] avg loss: 1.6525252424926589		[learning rate: 0.00050189]
	Learning Rate: 0.000501886
	LOSS [training: 1.6334940840966499 | validation: 2.508945678355834]
	TIME [epoch: 8.37 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6872972984865857		[learning rate: 0.00050097]
		[batch 20/20] avg loss: 1.5600992176263078		[learning rate: 0.00050006]
	Learning Rate: 0.000500065
	LOSS [training: 1.623698258056447 | validation: 2.500052262939106]
	TIME [epoch: 8.39 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6902716870854957		[learning rate: 0.00049916]
		[batch 20/20] avg loss: 1.573114935913139		[learning rate: 0.00049825]
	Learning Rate: 0.00049825
	LOSS [training: 1.6316933114993168 | validation: 2.5068427432826534]
	TIME [epoch: 8.38 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6770068227656725		[learning rate: 0.00049735]
		[batch 20/20] avg loss: 1.5769137367305546		[learning rate: 0.00049644]
	Learning Rate: 0.000496442
	LOSS [training: 1.6269602797481135 | validation: 2.524737661827185]
	TIME [epoch: 8.38 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5720454264393298		[learning rate: 0.00049554]
		[batch 20/20] avg loss: 1.677235916687237		[learning rate: 0.00049464]
	Learning Rate: 0.00049464
	LOSS [training: 1.6246406715632833 | validation: 2.5001898769116777]
	TIME [epoch: 8.37 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.750128241338026		[learning rate: 0.00049374]
		[batch 20/20] avg loss: 1.5280966503826747		[learning rate: 0.00049285]
	Learning Rate: 0.000492845
	LOSS [training: 1.6391124458603503 | validation: 2.506186075110394]
	TIME [epoch: 8.39 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6877333121248668		[learning rate: 0.00049195]
		[batch 20/20] avg loss: 1.5537457588858161		[learning rate: 0.00049106]
	Learning Rate: 0.000491057
	LOSS [training: 1.6207395355053413 | validation: 2.5092769373674875]
	TIME [epoch: 8.37 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.667681807211962		[learning rate: 0.00049016]
		[batch 20/20] avg loss: 1.5811574484758815		[learning rate: 0.00048927]
	Learning Rate: 0.000489275
	LOSS [training: 1.6244196278439218 | validation: 2.5031030227098157]
	TIME [epoch: 8.37 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5364717234173484		[learning rate: 0.00048839]
		[batch 20/20] avg loss: 1.6952341126622374		[learning rate: 0.0004875]
	Learning Rate: 0.000487499
	LOSS [training: 1.6158529180397927 | validation: 2.5063583469687805]
	TIME [epoch: 8.37 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6061933657954424		[learning rate: 0.00048661]
		[batch 20/20] avg loss: 1.6524321964282174		[learning rate: 0.00048573]
	Learning Rate: 0.00048573
	LOSS [training: 1.6293127811118295 | validation: 2.510771364051841]
	TIME [epoch: 8.37 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.689907856008276		[learning rate: 0.00048485]
		[batch 20/20] avg loss: 1.5658993633111593		[learning rate: 0.00048397]
	Learning Rate: 0.000483967
	LOSS [training: 1.627903609659717 | validation: 2.5166064794840226]
	TIME [epoch: 8.39 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5899613141536597		[learning rate: 0.00048309]
		[batch 20/20] avg loss: 1.658548169799218		[learning rate: 0.00048221]
	Learning Rate: 0.000482211
	LOSS [training: 1.6242547419764386 | validation: 2.5049408853466293]
	TIME [epoch: 8.37 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.623647196186641		[learning rate: 0.00048133]
		[batch 20/20] avg loss: 1.6602829739660794		[learning rate: 0.00048046]
	Learning Rate: 0.000480461
	LOSS [training: 1.6419650850763605 | validation: 2.5031656111208713]
	TIME [epoch: 8.37 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4977827701841888		[learning rate: 0.00047959]
		[batch 20/20] avg loss: 1.7356643079814145		[learning rate: 0.00047872]
	Learning Rate: 0.000478717
	LOSS [training: 1.6167235390828014 | validation: 2.4995998726954847]
	TIME [epoch: 8.37 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7945386749716135		[learning rate: 0.00047785]
		[batch 20/20] avg loss: 1.4438045716399106		[learning rate: 0.00047698]
	Learning Rate: 0.00047698
	LOSS [training: 1.6191716233057623 | validation: 2.5066320126788115]
	TIME [epoch: 8.4 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.541632303342293		[learning rate: 0.00047611]
		[batch 20/20] avg loss: 1.7086243734634294		[learning rate: 0.00047525]
	Learning Rate: 0.000475249
	LOSS [training: 1.6251283384028614 | validation: 2.503665743331237]
	TIME [epoch: 8.38 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.513303460677368		[learning rate: 0.00047439]
		[batch 20/20] avg loss: 1.7362520707315157		[learning rate: 0.00047352]
	Learning Rate: 0.000473524
	LOSS [training: 1.624777765704442 | validation: 2.5065361932670696]
	TIME [epoch: 8.38 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4954123114933666		[learning rate: 0.00047266]
		[batch 20/20] avg loss: 1.755497758457024		[learning rate: 0.00047181]
	Learning Rate: 0.000471806
	LOSS [training: 1.6254550349751953 | validation: 2.5130838199026746]
	TIME [epoch: 8.38 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5957931261061566		[learning rate: 0.00047095]
		[batch 20/20] avg loss: 1.652607170714397		[learning rate: 0.00047009]
	Learning Rate: 0.000470093
	LOSS [training: 1.6242001484102773 | validation: 2.505844156675009]
	TIME [epoch: 8.39 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6173342513851885		[learning rate: 0.00046924]
		[batch 20/20] avg loss: 1.6163051532329327		[learning rate: 0.00046839]
	Learning Rate: 0.000468388
	LOSS [training: 1.6168197023090607 | validation: 2.502432686246164]
	TIME [epoch: 8.37 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5350840938241448		[learning rate: 0.00046754]
		[batch 20/20] avg loss: 1.7141891065635348		[learning rate: 0.00046669]
	Learning Rate: 0.000466688
	LOSS [training: 1.6246366001938397 | validation: 2.504267693979835]
	TIME [epoch: 8.37 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5481749489496057		[learning rate: 0.00046584]
		[batch 20/20] avg loss: 1.684752718855967		[learning rate: 0.00046499]
	Learning Rate: 0.000464994
	LOSS [training: 1.6164638339027864 | validation: 2.5029871612697168]
	TIME [epoch: 8.37 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.549419100592333		[learning rate: 0.00046415]
		[batch 20/20] avg loss: 1.7213529754828507		[learning rate: 0.00046331]
	Learning Rate: 0.000463307
	LOSS [training: 1.6353860380375917 | validation: 2.5099794604820733]
	TIME [epoch: 8.37 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6545243462375825		[learning rate: 0.00046247]
		[batch 20/20] avg loss: 1.6105547361084962		[learning rate: 0.00046163]
	Learning Rate: 0.000461625
	LOSS [training: 1.6325395411730397 | validation: 2.5085189432629633]
	TIME [epoch: 8.38 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7614270479825014		[learning rate: 0.00046079]
		[batch 20/20] avg loss: 1.4967671773797748		[learning rate: 0.00045995]
	Learning Rate: 0.00045995
	LOSS [training: 1.6290971126811378 | validation: 2.521037980762265]
	TIME [epoch: 8.36 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.556525509509726		[learning rate: 0.00045911]
		[batch 20/20] avg loss: 1.689704207045504		[learning rate: 0.00045828]
	Learning Rate: 0.000458281
	LOSS [training: 1.6231148582776147 | validation: 2.5135588316737265]
	TIME [epoch: 8.36 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6819552776933677		[learning rate: 0.00045745]
		[batch 20/20] avg loss: 1.6055585574459106		[learning rate: 0.00045662]
	Learning Rate: 0.000456618
	LOSS [training: 1.6437569175696392 | validation: 2.578282104767438]
	TIME [epoch: 8.37 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.564282683712681		[learning rate: 0.00045579]
		[batch 20/20] avg loss: 1.6682882525735696		[learning rate: 0.00045496]
	Learning Rate: 0.000454961
	LOSS [training: 1.6162854681431251 | validation: 2.518263888676932]
	TIME [epoch: 8.38 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.474421181592867		[learning rate: 0.00045413]
		[batch 20/20] avg loss: 1.7619386710557052		[learning rate: 0.00045331]
	Learning Rate: 0.000453309
	LOSS [training: 1.6181799263242858 | validation: 2.538341286311751]
	TIME [epoch: 8.37 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.604629626305611		[learning rate: 0.00045249]
		[batch 20/20] avg loss: 1.648410265448265		[learning rate: 0.00045166]
	Learning Rate: 0.000451664
	LOSS [training: 1.6265199458769384 | validation: 2.532629391733902]
	TIME [epoch: 8.38 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8405167401069078		[learning rate: 0.00045084]
		[batch 20/20] avg loss: 1.438765700429033		[learning rate: 0.00045003]
	Learning Rate: 0.000450025
	LOSS [training: 1.6396412202679702 | validation: 2.506748046089312]
	TIME [epoch: 8.37 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4754293061204755		[learning rate: 0.00044921]
		[batch 20/20] avg loss: 1.7672321367133876		[learning rate: 0.00044839]
	Learning Rate: 0.000448392
	LOSS [training: 1.6213307214169315 | validation: 2.513925900991286]
	TIME [epoch: 8.38 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4141941307737365		[learning rate: 0.00044758]
		[batch 20/20] avg loss: 1.8320495447145242		[learning rate: 0.00044676]
	Learning Rate: 0.000446765
	LOSS [training: 1.62312183774413 | validation: 2.5324197378762596]
	TIME [epoch: 8.37 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5357626988339412		[learning rate: 0.00044595]
		[batch 20/20] avg loss: 1.7034185298366569		[learning rate: 0.00044514]
	Learning Rate: 0.000445143
	LOSS [training: 1.619590614335299 | validation: 2.5135378979514975]
	TIME [epoch: 8.37 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6340171445918863		[learning rate: 0.00044434]
		[batch 20/20] avg loss: 1.6017159406927406		[learning rate: 0.00044353]
	Learning Rate: 0.000443528
	LOSS [training: 1.6178665426423133 | validation: 2.499552368717592]
	TIME [epoch: 8.37 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4114541843979909		[learning rate: 0.00044272]
		[batch 20/20] avg loss: 1.8295352477125548		[learning rate: 0.00044192]
	Learning Rate: 0.000441918
	LOSS [training: 1.6204947160552732 | validation: 2.5065629465693573]
	TIME [epoch: 8.37 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7212628769247147		[learning rate: 0.00044112]
		[batch 20/20] avg loss: 1.5217074227166107		[learning rate: 0.00044031]
	Learning Rate: 0.000440315
	LOSS [training: 1.6214851498206624 | validation: 2.4996791175434]
	TIME [epoch: 8.38 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.629520203813027		[learning rate: 0.00043952]
		[batch 20/20] avg loss: 1.6189694083115764		[learning rate: 0.00043872]
	Learning Rate: 0.000438717
	LOSS [training: 1.6242448060623018 | validation: 2.5084182527597285]
	TIME [epoch: 8.36 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.773408904251643		[learning rate: 0.00043792]
		[batch 20/20] avg loss: 1.4964260243210408		[learning rate: 0.00043712]
	Learning Rate: 0.000437125
	LOSS [training: 1.634917464286342 | validation: 2.5084826528905526]
	TIME [epoch: 8.36 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6432910239309302		[learning rate: 0.00043633]
		[batch 20/20] avg loss: 1.6114603562881586		[learning rate: 0.00043554]
	Learning Rate: 0.000435538
	LOSS [training: 1.6273756901095449 | validation: 2.534286581194349]
	TIME [epoch: 8.37 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.516469756189676		[learning rate: 0.00043475]
		[batch 20/20] avg loss: 1.755289492381571		[learning rate: 0.00043396]
	Learning Rate: 0.000433958
	LOSS [training: 1.6358796242856237 | validation: 2.5181007448561865]
	TIME [epoch: 8.39 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6350499398272809		[learning rate: 0.00043317]
		[batch 20/20] avg loss: 1.6080958179241047		[learning rate: 0.00043238]
	Learning Rate: 0.000432383
	LOSS [training: 1.6215728788756931 | validation: 2.5123887899949646]
	TIME [epoch: 8.37 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6553933954192046		[learning rate: 0.0004316]
		[batch 20/20] avg loss: 1.583541265314929		[learning rate: 0.00043081]
	Learning Rate: 0.000430814
	LOSS [training: 1.6194673303670668 | validation: 2.507698561792451]
	TIME [epoch: 8.37 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.267839290423083		[learning rate: 0.00043003]
		[batch 20/20] avg loss: 1.9755848655336312		[learning rate: 0.00042925]
	Learning Rate: 0.00042925
	LOSS [training: 1.6217120779783571 | validation: 2.5026587530425712]
	TIME [epoch: 8.37 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5839445546415434		[learning rate: 0.00042847]
		[batch 20/20] avg loss: 1.6626620330964172		[learning rate: 0.00042769]
	Learning Rate: 0.000427692
	LOSS [training: 1.6233032938689802 | validation: 2.5088218876685096]
	TIME [epoch: 8.39 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5278572853927286		[learning rate: 0.00042692]
		[batch 20/20] avg loss: 1.7254687351422269		[learning rate: 0.00042614]
	Learning Rate: 0.00042614
	LOSS [training: 1.6266630102674777 | validation: 2.510091852624314]
	TIME [epoch: 8.38 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5635215160688003		[learning rate: 0.00042537]
		[batch 20/20] avg loss: 1.6751076055336511		[learning rate: 0.00042459]
	Learning Rate: 0.000424594
	LOSS [training: 1.6193145608012258 | validation: 2.5083886016451125]
	TIME [epoch: 8.38 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.653934982053493		[learning rate: 0.00042382]
		[batch 20/20] avg loss: 1.596095128709149		[learning rate: 0.00042305]
	Learning Rate: 0.000423053
	LOSS [training: 1.625015055381321 | validation: 2.527237941755298]
	TIME [epoch: 8.38 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.687935452673631		[learning rate: 0.00042228]
		[batch 20/20] avg loss: 1.5505160750182445		[learning rate: 0.00042152]
	Learning Rate: 0.000421518
	LOSS [training: 1.6192257638459377 | validation: 2.517920104830175]
	TIME [epoch: 8.38 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5634676098315363		[learning rate: 0.00042075]
		[batch 20/20] avg loss: 1.6863193588102536		[learning rate: 0.00041999]
	Learning Rate: 0.000419988
	LOSS [training: 1.624893484320895 | validation: 2.546487152265233]
	TIME [epoch: 8.38 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6530434109360743		[learning rate: 0.00041923]
		[batch 20/20] avg loss: 1.6059527733053691		[learning rate: 0.00041846]
	Learning Rate: 0.000418464
	LOSS [training: 1.629498092120722 | validation: 2.5017993240767367]
	TIME [epoch: 8.37 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6917773899892652		[learning rate: 0.0004177]
		[batch 20/20] avg loss: 1.5387132568189035		[learning rate: 0.00041695]
	Learning Rate: 0.000416945
	LOSS [training: 1.6152453234040842 | validation: 2.517564891392915]
	TIME [epoch: 8.37 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7663938128410517		[learning rate: 0.00041619]
		[batch 20/20] avg loss: 1.476761654915863		[learning rate: 0.00041543]
	Learning Rate: 0.000415432
	LOSS [training: 1.621577733878457 | validation: 2.536859684943785]
	TIME [epoch: 8.37 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.666081576898404		[learning rate: 0.00041468]
		[batch 20/20] avg loss: 1.602277541429586		[learning rate: 0.00041392]
	Learning Rate: 0.000413924
	LOSS [training: 1.6341795591639947 | validation: 2.523468831915528]
	TIME [epoch: 8.39 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5698325168506613		[learning rate: 0.00041317]
		[batch 20/20] avg loss: 1.671045481413794		[learning rate: 0.00041242]
	Learning Rate: 0.000412422
	LOSS [training: 1.6204389991322277 | validation: 2.5058221515231525]
	TIME [epoch: 8.37 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5511523349064131		[learning rate: 0.00041167]
		[batch 20/20] avg loss: 1.6854367366794318		[learning rate: 0.00041093]
	Learning Rate: 0.000410926
	LOSS [training: 1.6182945357929228 | validation: 2.5044724069688327]
	TIME [epoch: 8.37 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6822889113817006		[learning rate: 0.00041018]
		[batch 20/20] avg loss: 1.5723811360109416		[learning rate: 0.00040943]
	Learning Rate: 0.000409434
	LOSS [training: 1.6273350236963213 | validation: 2.535982129178734]
	TIME [epoch: 8.38 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.542671452944751		[learning rate: 0.00040869]
		[batch 20/20] avg loss: 1.6956822600621464		[learning rate: 0.00040795]
	Learning Rate: 0.000407948
	LOSS [training: 1.619176856503449 | validation: 2.5061837656689088]
	TIME [epoch: 8.39 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6040123430673496		[learning rate: 0.00040721]
		[batch 20/20] avg loss: 1.6450207123475535		[learning rate: 0.00040647]
	Learning Rate: 0.000406468
	LOSS [training: 1.6245165277074514 | validation: 2.547830494700542]
	TIME [epoch: 8.38 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5683070303686368		[learning rate: 0.00040573]
		[batch 20/20] avg loss: 1.6765663388562575		[learning rate: 0.00040499]
	Learning Rate: 0.000404993
	LOSS [training: 1.6224366846124472 | validation: 2.5284103883761784]
	TIME [epoch: 8.38 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5428825431199225		[learning rate: 0.00040426]
		[batch 20/20] avg loss: 1.7082758473211825		[learning rate: 0.00040352]
	Learning Rate: 0.000403523
	LOSS [training: 1.625579195220552 | validation: 2.49921893540364]
	TIME [epoch: 8.38 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4337446549095534		[learning rate: 0.00040279]
		[batch 20/20] avg loss: 1.8075325001610671		[learning rate: 0.00040206]
	Learning Rate: 0.000402059
	LOSS [training: 1.6206385775353103 | validation: 2.5045137601263283]
	TIME [epoch: 8.39 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8206676830453496		[learning rate: 0.00040133]
		[batch 20/20] avg loss: 1.4318058657420536		[learning rate: 0.0004006]
	Learning Rate: 0.0004006
	LOSS [training: 1.6262367743937016 | validation: 2.5272907219485696]
	TIME [epoch: 8.39 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7055543815635859		[learning rate: 0.00039987]
		[batch 20/20] avg loss: 1.5317635132426477		[learning rate: 0.00039915]
	Learning Rate: 0.000399146
	LOSS [training: 1.6186589474031166 | validation: 2.514806144580389]
	TIME [epoch: 8.37 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7475167110257488		[learning rate: 0.00039842]
		[batch 20/20] avg loss: 1.4900270144653187		[learning rate: 0.0003977]
	Learning Rate: 0.000397697
	LOSS [training: 1.6187718627455336 | validation: 2.5043194715433703]
	TIME [epoch: 8.37 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.653018567755025		[learning rate: 0.00039698]
		[batch 20/20] avg loss: 1.5822477581581356		[learning rate: 0.00039625]
	Learning Rate: 0.000396254
	LOSS [training: 1.6176331629565806 | validation: 2.5158265109912437]
	TIME [epoch: 8.37 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5046898288758674		[learning rate: 0.00039553]
		[batch 20/20] avg loss: 1.7466392343109765		[learning rate: 0.00039482]
	Learning Rate: 0.000394816
	LOSS [training: 1.6256645315934215 | validation: 2.499976121937121]
	TIME [epoch: 8.39 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7205286313936425		[learning rate: 0.0003941]
		[batch 20/20] avg loss: 1.553209228066132		[learning rate: 0.00039338]
	Learning Rate: 0.000393383
	LOSS [training: 1.636868929729887 | validation: 2.504095917713655]
	TIME [epoch: 8.37 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.671721897999367		[learning rate: 0.00039267]
		[batch 20/20] avg loss: 1.5642946882307336		[learning rate: 0.00039196]
	Learning Rate: 0.000391956
	LOSS [training: 1.6180082931150501 | validation: 2.5080276963946804]
	TIME [epoch: 8.38 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.641440037857245		[learning rate: 0.00039124]
		[batch 20/20] avg loss: 1.6048086711557155		[learning rate: 0.00039053]
	Learning Rate: 0.000390533
	LOSS [training: 1.62312435450648 | validation: 2.5167549540057204]
	TIME [epoch: 8.38 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5371461460519673		[learning rate: 0.00038982]
		[batch 20/20] avg loss: 1.702999957208507		[learning rate: 0.00038912]
	Learning Rate: 0.000389116
	LOSS [training: 1.6200730516302368 | validation: 2.5124093721200333]
	TIME [epoch: 8.4 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.687325456115439		[learning rate: 0.00038841]
		[batch 20/20] avg loss: 1.5488085819087352		[learning rate: 0.0003877]
	Learning Rate: 0.000387704
	LOSS [training: 1.6180670190120872 | validation: 2.5042070841366675]
	TIME [epoch: 8.38 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5011659361403515		[learning rate: 0.000387]
		[batch 20/20] avg loss: 1.7451839318215783		[learning rate: 0.0003863]
	Learning Rate: 0.000386297
	LOSS [training: 1.6231749339809647 | validation: 2.5004635614183592]
	TIME [epoch: 8.38 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.677938119026375		[learning rate: 0.0003856]
		[batch 20/20] avg loss: 1.5622646466679937		[learning rate: 0.00038489]
	Learning Rate: 0.000384895
	LOSS [training: 1.620101382847184 | validation: 2.493435187685504]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_996.pth
	Model improved!!!
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.554193128292416		[learning rate: 0.0003842]
		[batch 20/20] avg loss: 1.696993065203936		[learning rate: 0.0003835]
	Learning Rate: 0.000383498
	LOSS [training: 1.625593096748176 | validation: 2.5193966567303465]
	TIME [epoch: 8.42 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6527608549194501		[learning rate: 0.0003828]
		[batch 20/20] avg loss: 1.596600650356114		[learning rate: 0.00038211]
	Learning Rate: 0.000382106
	LOSS [training: 1.6246807526377822 | validation: 2.501399852412382]
	TIME [epoch: 8.4 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5574432383617518		[learning rate: 0.00038141]
		[batch 20/20] avg loss: 1.693219834665575		[learning rate: 0.00038072]
	Learning Rate: 0.00038072
	LOSS [training: 1.6253315365136636 | validation: 2.499749505143015]
	TIME [epoch: 8.4 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6513729582400498		[learning rate: 0.00038003]
		[batch 20/20] avg loss: 1.5919797582562354		[learning rate: 0.00037934]
	Learning Rate: 0.000379338
	LOSS [training: 1.6216763582481426 | validation: 2.5163295410522686]
	TIME [epoch: 8.4 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5913804386408295		[learning rate: 0.00037865]
		[batch 20/20] avg loss: 1.6594679914099468		[learning rate: 0.00037796]
	Learning Rate: 0.000377961
	LOSS [training: 1.6254242150253884 | validation: 2.5232516879529285]
	TIME [epoch: 8.4 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5419656055154571		[learning rate: 0.00037727]
		[batch 20/20] avg loss: 1.6977031820891029		[learning rate: 0.00037659]
	Learning Rate: 0.00037659
	LOSS [training: 1.61983439380228 | validation: 2.511479739357805]
	TIME [epoch: 8.42 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6995669022077624		[learning rate: 0.00037591]
		[batch 20/20] avg loss: 1.5360301865879207		[learning rate: 0.00037522]
	Learning Rate: 0.000375223
	LOSS [training: 1.6177985443978418 | validation: 2.516094885241941]
	TIME [epoch: 8.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5136837400399572		[learning rate: 0.00037454]
		[batch 20/20] avg loss: 1.7192656872280836		[learning rate: 0.00037386]
	Learning Rate: 0.000373861
	LOSS [training: 1.6164747136340203 | validation: 2.5011243562122356]
	TIME [epoch: 8.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5226593368595767		[learning rate: 0.00037318]
		[batch 20/20] avg loss: 1.727255600163264		[learning rate: 0.0003725]
	Learning Rate: 0.000372505
	LOSS [training: 1.6249574685114208 | validation: 2.5000479762890526]
	TIME [epoch: 8.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6157686272665042		[learning rate: 0.00037183]
		[batch 20/20] avg loss: 1.6197271962487683		[learning rate: 0.00037115]
	Learning Rate: 0.000371153
	LOSS [training: 1.6177479117576365 | validation: 2.49866391448022]
	TIME [epoch: 8.42 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7017843126849836		[learning rate: 0.00037048]
		[batch 20/20] avg loss: 1.5289023609487018		[learning rate: 0.00036981]
	Learning Rate: 0.000369806
	LOSS [training: 1.6153433368168426 | validation: 2.507657691992529]
	TIME [epoch: 8.41 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7944216861555682		[learning rate: 0.00036913]
		[batch 20/20] avg loss: 1.4530741682973327		[learning rate: 0.00036846]
	Learning Rate: 0.000368464
	LOSS [training: 1.6237479272264506 | validation: 2.5063959765320307]
	TIME [epoch: 8.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6129618554593175		[learning rate: 0.00036779]
		[batch 20/20] avg loss: 1.6173354354212504		[learning rate: 0.00036713]
	Learning Rate: 0.000367127
	LOSS [training: 1.6151486454402835 | validation: 2.509138910975625]
	TIME [epoch: 8.4 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6314322690799863		[learning rate: 0.00036646]
		[batch 20/20] avg loss: 1.6068992320052549		[learning rate: 0.00036579]
	Learning Rate: 0.000365794
	LOSS [training: 1.6191657505426202 | validation: 2.5001133190740066]
	TIME [epoch: 8.42 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7511169504166566		[learning rate: 0.00036513]
		[batch 20/20] avg loss: 1.4848719430672572		[learning rate: 0.00036447]
	Learning Rate: 0.000364467
	LOSS [training: 1.6179944467419567 | validation: 2.507701871102673]
	TIME [epoch: 8.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5214861200843004		[learning rate: 0.0003638]
		[batch 20/20] avg loss: 1.7109538048569202		[learning rate: 0.00036314]
	Learning Rate: 0.000363144
	LOSS [training: 1.6162199624706104 | validation: 2.5216690894513314]
	TIME [epoch: 8.4 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.701553373954125		[learning rate: 0.00036248]
		[batch 20/20] avg loss: 1.5415096630860474		[learning rate: 0.00036183]
	Learning Rate: 0.000361826
	LOSS [training: 1.6215315185200865 | validation: 2.501876628285133]
	TIME [epoch: 8.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6703799185106227		[learning rate: 0.00036117]
		[batch 20/20] avg loss: 1.5631203698457208		[learning rate: 0.00036051]
	Learning Rate: 0.000360513
	LOSS [training: 1.6167501441781718 | validation: 2.508056565272215]
	TIME [epoch: 8.4 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6934427743023666		[learning rate: 0.00035986]
		[batch 20/20] avg loss: 1.5311656228699835		[learning rate: 0.0003592]
	Learning Rate: 0.000359205
	LOSS [training: 1.612304198586175 | validation: 2.5039975617500727]
	TIME [epoch: 8.42 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5889135775203393		[learning rate: 0.00035855]
		[batch 20/20] avg loss: 1.6416819110334189		[learning rate: 0.0003579]
	Learning Rate: 0.000357901
	LOSS [training: 1.615297744276879 | validation: 2.5010016781922433]
	TIME [epoch: 8.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5882221399028627		[learning rate: 0.00035725]
		[batch 20/20] avg loss: 1.6421726692617011		[learning rate: 0.0003566]
	Learning Rate: 0.000356602
	LOSS [training: 1.6151974045822814 | validation: 2.501435951852165]
	TIME [epoch: 8.4 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.656771998295002		[learning rate: 0.00035595]
		[batch 20/20] avg loss: 1.5757706823218236		[learning rate: 0.00035531]
	Learning Rate: 0.000355308
	LOSS [training: 1.616271340308413 | validation: 2.5030599905027975]
	TIME [epoch: 8.4 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7060674992654026		[learning rate: 0.00035466]
		[batch 20/20] avg loss: 1.5228057888255928		[learning rate: 0.00035402]
	Learning Rate: 0.000354019
	LOSS [training: 1.6144366440454978 | validation: 2.5086754728842333]
	TIME [epoch: 8.42 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6933058216193817		[learning rate: 0.00035338]
		[batch 20/20] avg loss: 1.5516112557457298		[learning rate: 0.00035273]
	Learning Rate: 0.000352734
	LOSS [training: 1.6224585386825559 | validation: 2.501325822400366]
	TIME [epoch: 8.4 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6746274034403636		[learning rate: 0.00035209]
		[batch 20/20] avg loss: 1.5746219579142573		[learning rate: 0.00035145]
	Learning Rate: 0.000351454
	LOSS [training: 1.6246246806773113 | validation: 2.5042968027530312]
	TIME [epoch: 8.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7080536570229916		[learning rate: 0.00035082]
		[batch 20/20] avg loss: 1.5327713463975248		[learning rate: 0.00035018]
	Learning Rate: 0.000350179
	LOSS [training: 1.6204125017102584 | validation: 2.5197226711729517]
	TIME [epoch: 8.4 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8435068971707131		[learning rate: 0.00034954]
		[batch 20/20] avg loss: 1.4023219533960263		[learning rate: 0.00034891]
	Learning Rate: 0.000348908
	LOSS [training: 1.6229144252833698 | validation: 2.500224366309731]
	TIME [epoch: 8.42 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7544408107371041		[learning rate: 0.00034827]
		[batch 20/20] avg loss: 1.4813370946372912		[learning rate: 0.00034764]
	Learning Rate: 0.000347641
	LOSS [training: 1.6178889526871978 | validation: 2.506424599966566]
	TIME [epoch: 8.4 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7859886122553548		[learning rate: 0.00034701]
		[batch 20/20] avg loss: 1.4641267760540582		[learning rate: 0.00034638]
	Learning Rate: 0.00034638
	LOSS [training: 1.6250576941547064 | validation: 2.5035027702929913]
	TIME [epoch: 8.4 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.665203854392559		[learning rate: 0.00034575]
		[batch 20/20] avg loss: 1.582454562123337		[learning rate: 0.00034512]
	Learning Rate: 0.000345123
	LOSS [training: 1.6238292082579477 | validation: 2.50254497631731]
	TIME [epoch: 8.4 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7963387885527253		[learning rate: 0.0003445]
		[batch 20/20] avg loss: 1.4384370719945108		[learning rate: 0.00034387]
	Learning Rate: 0.00034387
	LOSS [training: 1.6173879302736178 | validation: 2.506542985458325]
	TIME [epoch: 8.41 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5248488996585565		[learning rate: 0.00034325]
		[batch 20/20] avg loss: 1.7099129022554926		[learning rate: 0.00034262]
	Learning Rate: 0.000342622
	LOSS [training: 1.6173809009570248 | validation: 2.507237771549428]
	TIME [epoch: 8.41 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6404724640507353		[learning rate: 0.000342]
		[batch 20/20] avg loss: 1.599411746317924		[learning rate: 0.00034138]
	Learning Rate: 0.000341379
	LOSS [training: 1.61994210518433 | validation: 2.5059873321156783]
	TIME [epoch: 8.4 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7417543674397042		[learning rate: 0.00034076]
		[batch 20/20] avg loss: 1.496529639724049		[learning rate: 0.00034014]
	Learning Rate: 0.00034014
	LOSS [training: 1.6191420035818762 | validation: 2.5163064194673277]
	TIME [epoch: 8.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6351431429512568		[learning rate: 0.00033952]
		[batch 20/20] avg loss: 1.5996350537723647		[learning rate: 0.00033891]
	Learning Rate: 0.000338906
	LOSS [training: 1.6173890983618109 | validation: 2.5380211492036193]
	TIME [epoch: 8.4 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8859008936826704		[learning rate: 0.00033829]
		[batch 20/20] avg loss: 1.3592584497693019		[learning rate: 0.00033768]
	Learning Rate: 0.000337676
	LOSS [training: 1.6225796717259864 | validation: 2.5162472487480128]
	TIME [epoch: 8.43 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6451803644828602		[learning rate: 0.00033706]
		[batch 20/20] avg loss: 1.585893427838697		[learning rate: 0.00033645]
	Learning Rate: 0.00033645
	LOSS [training: 1.6155368961607786 | validation: 2.5292266161694954]
	TIME [epoch: 8.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6857400779979428		[learning rate: 0.00033584]
		[batch 20/20] avg loss: 1.5692179408696265		[learning rate: 0.00033523]
	Learning Rate: 0.000335229
	LOSS [training: 1.6274790094337845 | validation: 2.540801699210415]
	TIME [epoch: 8.4 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5731582579355405		[learning rate: 0.00033462]
		[batch 20/20] avg loss: 1.674052328281316		[learning rate: 0.00033401]
	Learning Rate: 0.000334013
	LOSS [training: 1.6236052931084282 | validation: 2.496387586204063]
	TIME [epoch: 8.4 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6277415845965628		[learning rate: 0.00033341]
		[batch 20/20] avg loss: 1.6071822516518606		[learning rate: 0.0003328]
	Learning Rate: 0.000332801
	LOSS [training: 1.6174619181242114 | validation: 2.508294489651487]
	TIME [epoch: 8.43 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7133897265057896		[learning rate: 0.0003322]
		[batch 20/20] avg loss: 1.52372455288515		[learning rate: 0.00033159]
	Learning Rate: 0.000331593
	LOSS [training: 1.6185571396954697 | validation: 2.5052439503842043]
	TIME [epoch: 8.41 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7721308300238312		[learning rate: 0.00033099]
		[batch 20/20] avg loss: 1.4555177539096413		[learning rate: 0.00033039]
	Learning Rate: 0.00033039
	LOSS [training: 1.6138242919667363 | validation: 2.498632648494259]
	TIME [epoch: 8.4 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.688339659829624		[learning rate: 0.00032979]
		[batch 20/20] avg loss: 1.5428265952056877		[learning rate: 0.00032919]
	Learning Rate: 0.000329191
	LOSS [training: 1.615583127517656 | validation: 2.5234957214974365]
	TIME [epoch: 8.4 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.628988355833195		[learning rate: 0.00032859]
		[batch 20/20] avg loss: 1.6198362248967892		[learning rate: 0.000328]
	Learning Rate: 0.000327996
	LOSS [training: 1.6244122903649925 | validation: 2.4962470906523357]
	TIME [epoch: 8.42 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6793601028831666		[learning rate: 0.0003274]
		[batch 20/20] avg loss: 1.5541367442808398		[learning rate: 0.00032681]
	Learning Rate: 0.000326806
	LOSS [training: 1.6167484235820033 | validation: 2.5041500869059323]
	TIME [epoch: 8.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.654769572019982		[learning rate: 0.00032621]
		[batch 20/20] avg loss: 1.575604728541956		[learning rate: 0.00032562]
	Learning Rate: 0.00032562
	LOSS [training: 1.6151871502809687 | validation: 2.5058902986456624]
	TIME [epoch: 8.4 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7248111309509628		[learning rate: 0.00032503]
		[batch 20/20] avg loss: 1.507902387895935		[learning rate: 0.00032444]
	Learning Rate: 0.000324438
	LOSS [training: 1.616356759423449 | validation: 2.503249531458338]
	TIME [epoch: 8.4 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6647589758244288		[learning rate: 0.00032385]
		[batch 20/20] avg loss: 1.5632996877117802		[learning rate: 0.00032326]
	Learning Rate: 0.00032326
	LOSS [training: 1.6140293317681043 | validation: 2.497002834258025]
	TIME [epoch: 8.4 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6065278218541983		[learning rate: 0.00032267]
		[batch 20/20] avg loss: 1.6250012801760838		[learning rate: 0.00032209]
	Learning Rate: 0.000322087
	LOSS [training: 1.615764551015141 | validation: 2.5030417207832953]
	TIME [epoch: 8.42 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.49571837582069		[learning rate: 0.0003215]
		[batch 20/20] avg loss: 1.7513814635026783		[learning rate: 0.00032092]
	Learning Rate: 0.000320918
	LOSS [training: 1.6235499196616843 | validation: 2.5215532761355455]
	TIME [epoch: 8.41 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8114363610933868		[learning rate: 0.00032034]
		[batch 20/20] avg loss: 1.4345907823066821		[learning rate: 0.00031975]
	Learning Rate: 0.000319754
	LOSS [training: 1.6230135717000345 | validation: 2.504841359662247]
	TIME [epoch: 8.4 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6299710344334617		[learning rate: 0.00031917]
		[batch 20/20] avg loss: 1.6058334962157719		[learning rate: 0.00031859]
	Learning Rate: 0.000318593
	LOSS [training: 1.6179022653246165 | validation: 2.504856541082528]
	TIME [epoch: 8.4 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5081405886595767		[learning rate: 0.00031801]
		[batch 20/20] avg loss: 1.7263263300513383		[learning rate: 0.00031744]
	Learning Rate: 0.000317437
	LOSS [training: 1.6172334593554574 | validation: 2.5085081923716173]
	TIME [epoch: 8.42 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.602006084385636		[learning rate: 0.00031686]
		[batch 20/20] avg loss: 1.6277684381944415		[learning rate: 0.00031629]
	Learning Rate: 0.000316285
	LOSS [training: 1.6148872612900387 | validation: 2.5077746757853188]
	TIME [epoch: 8.4 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5834933150656714		[learning rate: 0.00031571]
		[batch 20/20] avg loss: 1.6524114007733062		[learning rate: 0.00031514]
	Learning Rate: 0.000315137
	LOSS [training: 1.6179523579194885 | validation: 2.506290679767931]
	TIME [epoch: 8.4 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7013992448147348		[learning rate: 0.00031457]
		[batch 20/20] avg loss: 1.5344254235323276		[learning rate: 0.00031399]
	Learning Rate: 0.000313994
	LOSS [training: 1.6179123341735313 | validation: 2.5035146308327487]
	TIME [epoch: 8.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6077068217506276		[learning rate: 0.00031342]
		[batch 20/20] avg loss: 1.6235994489114554		[learning rate: 0.00031285]
	Learning Rate: 0.000312854
	LOSS [training: 1.6156531353310417 | validation: 2.4992788691878847]
	TIME [epoch: 8.42 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.533536892178302		[learning rate: 0.00031229]
		[batch 20/20] avg loss: 1.7046304038568525		[learning rate: 0.00031172]
	Learning Rate: 0.000311719
	LOSS [training: 1.6190836480175772 | validation: 2.5065949912944223]
	TIME [epoch: 8.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6860533586266797		[learning rate: 0.00031115]
		[batch 20/20] avg loss: 1.5409049741951562		[learning rate: 0.00031059]
	Learning Rate: 0.000310588
	LOSS [training: 1.613479166410918 | validation: 2.500913020663497]
	TIME [epoch: 8.4 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5746720430256014		[learning rate: 0.00031002]
		[batch 20/20] avg loss: 1.6589890778394234		[learning rate: 0.00030946]
	Learning Rate: 0.000309461
	LOSS [training: 1.6168305604325126 | validation: 2.5021824095755933]
	TIME [epoch: 8.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6659217190035744		[learning rate: 0.0003089]
		[batch 20/20] avg loss: 1.5774826462018197		[learning rate: 0.00030834]
	Learning Rate: 0.000308338
	LOSS [training: 1.621702182602697 | validation: 2.511419823395224]
	TIME [epoch: 8.4 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5863999028041251		[learning rate: 0.00030778]
		[batch 20/20] avg loss: 1.6376323473065533		[learning rate: 0.00030722]
	Learning Rate: 0.000307219
	LOSS [training: 1.612016125055339 | validation: 2.500148693474259]
	TIME [epoch: 8.42 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6424843021643034		[learning rate: 0.00030666]
		[batch 20/20] avg loss: 1.6069271649488104		[learning rate: 0.0003061]
	Learning Rate: 0.000306104
	LOSS [training: 1.624705733556557 | validation: 2.5083546893105635]
	TIME [epoch: 8.39 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6786546077068696		[learning rate: 0.00030555]
		[batch 20/20] avg loss: 1.5653435840506578		[learning rate: 0.00030499]
	Learning Rate: 0.000304993
	LOSS [training: 1.6219990958787638 | validation: 2.511670297564419]
	TIME [epoch: 8.39 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7344921986982988		[learning rate: 0.00030444]
		[batch 20/20] avg loss: 1.4947384356993738		[learning rate: 0.00030389]
	Learning Rate: 0.000303886
	LOSS [training: 1.6146153171988367 | validation: 2.5115939347667595]
	TIME [epoch: 8.39 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8408361913492706		[learning rate: 0.00030333]
		[batch 20/20] avg loss: 1.3843530477895172		[learning rate: 0.00030278]
	Learning Rate: 0.000302783
	LOSS [training: 1.6125946195693939 | validation: 2.4976630309682224]
	TIME [epoch: 8.42 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.741329165519618		[learning rate: 0.00030223]
		[batch 20/20] avg loss: 1.487199817931709		[learning rate: 0.00030168]
	Learning Rate: 0.000301684
	LOSS [training: 1.6142644917256637 | validation: 2.510353017886511]
	TIME [epoch: 8.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6696333434700041		[learning rate: 0.00030114]
		[batch 20/20] avg loss: 1.5741682821005814		[learning rate: 0.00030059]
	Learning Rate: 0.000300589
	LOSS [training: 1.6219008127852923 | validation: 2.5025407767559185]
	TIME [epoch: 8.39 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.73537211262679		[learning rate: 0.00030004]
		[batch 20/20] avg loss: 1.511713337114607		[learning rate: 0.0002995]
	Learning Rate: 0.000299499
	LOSS [training: 1.6235427248706984 | validation: 2.5011431835524567]
	TIME [epoch: 8.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.553334243567535		[learning rate: 0.00029895]
		[batch 20/20] avg loss: 1.6767814909005747		[learning rate: 0.00029841]
	Learning Rate: 0.000298412
	LOSS [training: 1.6150578672340548 | validation: 2.5200956009499738]
	TIME [epoch: 8.42 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6464051380539577		[learning rate: 0.00029787]
		[batch 20/20] avg loss: 1.5947106649962624		[learning rate: 0.00029733]
	Learning Rate: 0.000297329
	LOSS [training: 1.62055790152511 | validation: 2.5029586599094498]
	TIME [epoch: 8.39 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4837565564659658		[learning rate: 0.00029679]
		[batch 20/20] avg loss: 1.7557052490250886		[learning rate: 0.00029625]
	Learning Rate: 0.00029625
	LOSS [training: 1.6197309027455273 | validation: 2.507639991890492]
	TIME [epoch: 8.4 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6792466651550413		[learning rate: 0.00029571]
		[batch 20/20] avg loss: 1.5482721536325406		[learning rate: 0.00029517]
	Learning Rate: 0.000295175
	LOSS [training: 1.613759409393791 | validation: 2.507386004436575]
	TIME [epoch: 8.39 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5624852349884		[learning rate: 0.00029464]
		[batch 20/20] avg loss: 1.6620355697095957		[learning rate: 0.0002941]
	Learning Rate: 0.000294103
	LOSS [training: 1.6122604023489981 | validation: 2.5006382267951004]
	TIME [epoch: 8.4 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.456284455222558		[learning rate: 0.00029357]
		[batch 20/20] avg loss: 1.7831046301423439		[learning rate: 0.00029304]
	Learning Rate: 0.000293036
	LOSS [training: 1.6196945426824514 | validation: 2.516079491432853]
	TIME [epoch: 8.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5053878314549216		[learning rate: 0.0002925]
		[batch 20/20] avg loss: 1.7327574267478476		[learning rate: 0.00029197]
	Learning Rate: 0.000291973
	LOSS [training: 1.6190726291013842 | validation: 2.5055762730338333]
	TIME [epoch: 8.39 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4880898246623784		[learning rate: 0.00029144]
		[batch 20/20] avg loss: 1.7402271839381576		[learning rate: 0.00029091]
	Learning Rate: 0.000290913
	LOSS [training: 1.6141585043002677 | validation: 2.510624327150114]
	TIME [epoch: 8.39 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6011412071661184		[learning rate: 0.00029038]
		[batch 20/20] avg loss: 1.6308924218524186		[learning rate: 0.00028986]
	Learning Rate: 0.000289857
	LOSS [training: 1.6160168145092684 | validation: 2.5182525462437515]
	TIME [epoch: 8.39 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6927566177282611		[learning rate: 0.00028933]
		[batch 20/20] avg loss: 1.5394161878424089		[learning rate: 0.00028881]
	Learning Rate: 0.000288805
	LOSS [training: 1.616086402785335 | validation: 2.495108028862007]
	TIME [epoch: 8.41 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.610607562738241		[learning rate: 0.00028828]
		[batch 20/20] avg loss: 1.616446604732922		[learning rate: 0.00028776]
	Learning Rate: 0.000287757
	LOSS [training: 1.6135270837355815 | validation: 2.515190004478977]
	TIME [epoch: 8.38 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5864811406483752		[learning rate: 0.00028723]
		[batch 20/20] avg loss: 1.6473832566677544		[learning rate: 0.00028671]
	Learning Rate: 0.000286713
	LOSS [training: 1.616932198658065 | validation: 2.5092355244804687]
	TIME [epoch: 8.39 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5520569537433528		[learning rate: 0.00028619]
		[batch 20/20] avg loss: 1.6695355853323803		[learning rate: 0.00028567]
	Learning Rate: 0.000285672
	LOSS [training: 1.6107962695378668 | validation: 2.50492427711691]
	TIME [epoch: 8.39 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4546514289547339		[learning rate: 0.00028515]
		[batch 20/20] avg loss: 1.7858551505934905		[learning rate: 0.00028464]
	Learning Rate: 0.000284636
	LOSS [training: 1.6202532897741126 | validation: 2.499385271477442]
	TIME [epoch: 8.41 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6796219385463622		[learning rate: 0.00028412]
		[batch 20/20] avg loss: 1.5657189322990308		[learning rate: 0.0002836]
	Learning Rate: 0.000283603
	LOSS [training: 1.6226704354226964 | validation: 2.516178469385042]
	TIME [epoch: 8.39 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5378755886423445		[learning rate: 0.00028309]
		[batch 20/20] avg loss: 1.6881417470368922		[learning rate: 0.00028257]
	Learning Rate: 0.000282574
	LOSS [training: 1.6130086678396183 | validation: 2.4992203118342733]
	TIME [epoch: 8.4 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5492116163703276		[learning rate: 0.00028206]
		[batch 20/20] avg loss: 1.680619050203111		[learning rate: 0.00028155]
	Learning Rate: 0.000281548
	LOSS [training: 1.6149153332867194 | validation: 2.501703081511545]
	TIME [epoch: 8.39 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3844541524560128		[learning rate: 0.00028104]
		[batch 20/20] avg loss: 1.85721107727329		[learning rate: 0.00028053]
	Learning Rate: 0.000280526
	LOSS [training: 1.6208326148646512 | validation: 2.515390587134948]
	TIME [epoch: 8.41 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.483573919709737		[learning rate: 0.00028002]
		[batch 20/20] avg loss: 1.7415880777184487		[learning rate: 0.00027951]
	Learning Rate: 0.000279508
	LOSS [training: 1.6125809987140927 | validation: 2.4965131132177096]
	TIME [epoch: 8.4 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4744109325845898		[learning rate: 0.000279]
		[batch 20/20] avg loss: 1.7710458281262427		[learning rate: 0.00027849]
	Learning Rate: 0.000278494
	LOSS [training: 1.6227283803554164 | validation: 2.5106900863610684]
	TIME [epoch: 8.4 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7476028291501506		[learning rate: 0.00027799]
		[batch 20/20] avg loss: 1.490105262936116		[learning rate: 0.00027748]
	Learning Rate: 0.000277483
	LOSS [training: 1.618854046043133 | validation: 2.500831659119294]
	TIME [epoch: 8.4 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5866965932431774		[learning rate: 0.00027698]
		[batch 20/20] avg loss: 1.6460343997987046		[learning rate: 0.00027648]
	Learning Rate: 0.000276476
	LOSS [training: 1.616365496520941 | validation: 2.5069749070898943]
	TIME [epoch: 8.4 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5957528174202484		[learning rate: 0.00027597]
		[batch 20/20] avg loss: 1.6298108783059089		[learning rate: 0.00027547]
	Learning Rate: 0.000275473
	LOSS [training: 1.6127818478630787 | validation: 2.500889061668752]
	TIME [epoch: 8.42 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4645258337262994		[learning rate: 0.00027497]
		[batch 20/20] avg loss: 1.7609510669009556		[learning rate: 0.00027447]
	Learning Rate: 0.000274473
	LOSS [training: 1.6127384503136277 | validation: 2.5067924077655164]
	TIME [epoch: 8.4 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.618234062141617		[learning rate: 0.00027397]
		[batch 20/20] avg loss: 1.6229082397311612		[learning rate: 0.00027348]
	Learning Rate: 0.000273477
	LOSS [training: 1.6205711509363891 | validation: 2.5036293551230284]
	TIME [epoch: 8.4 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.547346555289555		[learning rate: 0.00027298]
		[batch 20/20] avg loss: 1.6900801858646173		[learning rate: 0.00027248]
	Learning Rate: 0.000272485
	LOSS [training: 1.6187133705770858 | validation: 2.5622365050624145]
	TIME [epoch: 8.4 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6560726987788663		[learning rate: 0.00027199]
		[batch 20/20] avg loss: 1.5826969408462919		[learning rate: 0.0002715]
	Learning Rate: 0.000271496
	LOSS [training: 1.6193848198125789 | validation: 2.5274815205009027]
	TIME [epoch: 8.42 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4993356411814003		[learning rate: 0.000271]
		[batch 20/20] avg loss: 1.725337905710214		[learning rate: 0.00027051]
	Learning Rate: 0.000270511
	LOSS [training: 1.6123367734458072 | validation: 2.5014933660558842]
	TIME [epoch: 8.4 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.582819633603962		[learning rate: 0.00027002]
		[batch 20/20] avg loss: 1.6425948321858528		[learning rate: 0.00026953]
	Learning Rate: 0.000269529
	LOSS [training: 1.6127072328949072 | validation: 2.502845640740908]
	TIME [epoch: 8.4 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5947840527760124		[learning rate: 0.00026904]
		[batch 20/20] avg loss: 1.632748596858184		[learning rate: 0.00026855]
	Learning Rate: 0.000268551
	LOSS [training: 1.6137663248170981 | validation: 2.496945714888623]
	TIME [epoch: 8.4 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6915814338554434		[learning rate: 0.00026806]
		[batch 20/20] avg loss: 1.5372186674619437		[learning rate: 0.00026758]
	Learning Rate: 0.000267576
	LOSS [training: 1.6144000506586937 | validation: 2.496783587392361]
	TIME [epoch: 8.42 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6798584972063062		[learning rate: 0.00026709]
		[batch 20/20] avg loss: 1.5504495105041518		[learning rate: 0.00026661]
	Learning Rate: 0.000266605
	LOSS [training: 1.615154003855229 | validation: 2.5187209393008163]
	TIME [epoch: 8.4 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.625841417976866		[learning rate: 0.00026612]
		[batch 20/20] avg loss: 1.5978032270550024		[learning rate: 0.00026564]
	Learning Rate: 0.000265638
	LOSS [training: 1.6118223225159345 | validation: 2.4987009164153875]
	TIME [epoch: 8.4 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6621138064906464		[learning rate: 0.00026516]
		[batch 20/20] avg loss: 1.5668007366365282		[learning rate: 0.00026467]
	Learning Rate: 0.000264674
	LOSS [training: 1.6144572715635874 | validation: 2.4991445326938844]
	TIME [epoch: 8.4 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.521025253968395		[learning rate: 0.00026419]
		[batch 20/20] avg loss: 1.7123351299440692		[learning rate: 0.00026371]
	Learning Rate: 0.000263713
	LOSS [training: 1.6166801919562324 | validation: 2.5048987546667347]
	TIME [epoch: 8.4 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6080824399197262		[learning rate: 0.00026323]
		[batch 20/20] avg loss: 1.6322083145808222		[learning rate: 0.00026276]
	Learning Rate: 0.000262756
	LOSS [training: 1.6201453772502739 | validation: 2.5153007048788796]
	TIME [epoch: 8.42 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6511856727326735		[learning rate: 0.00026228]
		[batch 20/20] avg loss: 1.5807214323917984		[learning rate: 0.0002618]
	Learning Rate: 0.000261802
	LOSS [training: 1.615953552562236 | validation: 2.498082965252146]
	TIME [epoch: 8.39 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4672848089583044		[learning rate: 0.00026133]
		[batch 20/20] avg loss: 1.759273741178737		[learning rate: 0.00026085]
	Learning Rate: 0.000260852
	LOSS [training: 1.6132792750685205 | validation: 2.4981046596796004]
	TIME [epoch: 8.4 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7352089244898496		[learning rate: 0.00026038]
		[batch 20/20] avg loss: 1.5030659455054456		[learning rate: 0.00025991]
	Learning Rate: 0.000259906
	LOSS [training: 1.6191374349976477 | validation: 2.5115098194874013]
	TIME [epoch: 8.4 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6273764439854628		[learning rate: 0.00025943]
		[batch 20/20] avg loss: 1.600005216379343		[learning rate: 0.00025896]
	Learning Rate: 0.000258962
	LOSS [training: 1.613690830182403 | validation: 2.4999683975073403]
	TIME [epoch: 8.42 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7479696130976472		[learning rate: 0.00025849]
		[batch 20/20] avg loss: 1.477140059143275		[learning rate: 0.00025802]
	Learning Rate: 0.000258023
	LOSS [training: 1.6125548361204607 | validation: 2.5024328458089036]
	TIME [epoch: 8.4 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6168546989617654		[learning rate: 0.00025755]
		[batch 20/20] avg loss: 1.6085317738646474		[learning rate: 0.00025709]
	Learning Rate: 0.000257086
	LOSS [training: 1.6126932364132063 | validation: 2.518071392323616]
	TIME [epoch: 8.4 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6401134104935484		[learning rate: 0.00025662]
		[batch 20/20] avg loss: 1.5881937504246852		[learning rate: 0.00025615]
	Learning Rate: 0.000256153
	LOSS [training: 1.6141535804591167 | validation: 2.495468399334447]
	TIME [epoch: 8.4 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5211383378008048		[learning rate: 0.00025569]
		[batch 20/20] avg loss: 1.7067625066202972		[learning rate: 0.00025522]
	Learning Rate: 0.000255224
	LOSS [training: 1.6139504222105512 | validation: 2.5073701002693523]
	TIME [epoch: 8.42 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7028140265160014		[learning rate: 0.00025476]
		[batch 20/20] avg loss: 1.5242200030040083		[learning rate: 0.0002543]
	Learning Rate: 0.000254298
	LOSS [training: 1.6135170147600046 | validation: 2.4954199491127844]
	TIME [epoch: 8.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5690871470321828		[learning rate: 0.00025384]
		[batch 20/20] avg loss: 1.665262822575673		[learning rate: 0.00025337]
	Learning Rate: 0.000253375
	LOSS [training: 1.6171749848039276 | validation: 2.500523730782621]
	TIME [epoch: 8.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6799100736568584		[learning rate: 0.00025291]
		[batch 20/20] avg loss: 1.5463714029763849		[learning rate: 0.00025246]
	Learning Rate: 0.000252455
	LOSS [training: 1.6131407383166216 | validation: 2.500752956507463]
	TIME [epoch: 8.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6359379203238078		[learning rate: 0.000252]
		[batch 20/20] avg loss: 1.5915550112400645		[learning rate: 0.00025154]
	Learning Rate: 0.000251539
	LOSS [training: 1.613746465781936 | validation: 2.4993482798943822]
	TIME [epoch: 8.4 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.73508089811127		[learning rate: 0.00025108]
		[batch 20/20] avg loss: 1.4998471198325638		[learning rate: 0.00025063]
	Learning Rate: 0.000250626
	LOSS [training: 1.617464008971917 | validation: 2.5117613002047356]
	TIME [epoch: 8.42 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.509221164262899		[learning rate: 0.00025017]
		[batch 20/20] avg loss: 1.719762310162757		[learning rate: 0.00024972]
	Learning Rate: 0.000249717
	LOSS [training: 1.614491737212828 | validation: 2.498244044519798]
	TIME [epoch: 8.4 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6601040095009012		[learning rate: 0.00024926]
		[batch 20/20] avg loss: 1.5712206212245905		[learning rate: 0.00024881]
	Learning Rate: 0.00024881
	LOSS [training: 1.6156623153627456 | validation: 2.5105221158443216]
	TIME [epoch: 8.41 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.650906244872472		[learning rate: 0.00024836]
		[batch 20/20] avg loss: 1.5749894205209871		[learning rate: 0.00024791]
	Learning Rate: 0.000247907
	LOSS [training: 1.6129478326967295 | validation: 2.5077935288804416]
	TIME [epoch: 8.4 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6850942650845373		[learning rate: 0.00024746]
		[batch 20/20] avg loss: 1.546001616029792		[learning rate: 0.00024701]
	Learning Rate: 0.000247008
	LOSS [training: 1.6155479405571647 | validation: 2.4992145433046034]
	TIME [epoch: 8.42 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7203824429379186		[learning rate: 0.00024656]
		[batch 20/20] avg loss: 1.5228966556732348		[learning rate: 0.00024611]
	Learning Rate: 0.000246111
	LOSS [training: 1.6216395493055766 | validation: 2.5417279020201287]
	TIME [epoch: 8.41 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.598996067507155		[learning rate: 0.00024566]
		[batch 20/20] avg loss: 1.6350334829296274		[learning rate: 0.00024522]
	Learning Rate: 0.000245218
	LOSS [training: 1.6170147752183912 | validation: 2.5023777118023536]
	TIME [epoch: 8.4 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6183644024782027		[learning rate: 0.00024477]
		[batch 20/20] avg loss: 1.6013659532537976		[learning rate: 0.00024433]
	Learning Rate: 0.000244328
	LOSS [training: 1.6098651778659998 | validation: 2.505365559776717]
	TIME [epoch: 8.41 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4845146207793438		[learning rate: 0.00024388]
		[batch 20/20] avg loss: 1.7349289297541248		[learning rate: 0.00024344]
	Learning Rate: 0.000243442
	LOSS [training: 1.609721775266734 | validation: 2.497217397227267]
	TIME [epoch: 8.42 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.689450732625895		[learning rate: 0.000243]
		[batch 20/20] avg loss: 1.5441026083860423		[learning rate: 0.00024256]
	Learning Rate: 0.000242558
	LOSS [training: 1.6167766705059687 | validation: 2.5016110308520365]
	TIME [epoch: 8.41 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6578205843504332		[learning rate: 0.00024212]
		[batch 20/20] avg loss: 1.5642181721026363		[learning rate: 0.00024168]
	Learning Rate: 0.000241678
	LOSS [training: 1.6110193782265345 | validation: 2.505389400586531]
	TIME [epoch: 8.4 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5575075191023102		[learning rate: 0.00024124]
		[batch 20/20] avg loss: 1.6778977629779501		[learning rate: 0.0002408]
	Learning Rate: 0.000240801
	LOSS [training: 1.6177026410401303 | validation: 2.507970823920325]
	TIME [epoch: 8.41 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6633084749542444		[learning rate: 0.00024036]
		[batch 20/20] avg loss: 1.571149335047728		[learning rate: 0.00023993]
	Learning Rate: 0.000239927
	LOSS [training: 1.6172289050009863 | validation: 2.4951584494183754]
	TIME [epoch: 8.42 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6418803886674354		[learning rate: 0.00023949]
		[batch 20/20] avg loss: 1.5906686155544334		[learning rate: 0.00023906]
	Learning Rate: 0.000239056
	LOSS [training: 1.6162745021109344 | validation: 2.4974932549861046]
	TIME [epoch: 8.41 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7575197246896255		[learning rate: 0.00023862]
		[batch 20/20] avg loss: 1.4651114579039703		[learning rate: 0.00023819]
	Learning Rate: 0.000238189
	LOSS [training: 1.611315591296798 | validation: 2.5111674766151855]
	TIME [epoch: 8.4 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5094526699470352		[learning rate: 0.00023776]
		[batch 20/20] avg loss: 1.7391121647293173		[learning rate: 0.00023732]
	Learning Rate: 0.000237324
	LOSS [training: 1.6242824173381762 | validation: 2.5030215822846875]
	TIME [epoch: 8.4 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4706369564421469		[learning rate: 0.00023689]
		[batch 20/20] avg loss: 1.7552819065505922		[learning rate: 0.00023646]
	Learning Rate: 0.000236463
	LOSS [training: 1.6129594314963698 | validation: 2.497908943682119]
	TIME [epoch: 8.41 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4719950646333657		[learning rate: 0.00023603]
		[batch 20/20] avg loss: 1.7605910590975942		[learning rate: 0.0002356]
	Learning Rate: 0.000235605
	LOSS [training: 1.61629306186548 | validation: 2.5023738736272767]
	TIME [epoch: 8.43 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.535280603942132		[learning rate: 0.00023518]
		[batch 20/20] avg loss: 1.6861571026520772		[learning rate: 0.00023475]
	Learning Rate: 0.00023475
	LOSS [training: 1.6107188532971048 | validation: 2.495458456039832]
	TIME [epoch: 8.41 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.585865189979035		[learning rate: 0.00023432]
		[batch 20/20] avg loss: 1.6382417606055895		[learning rate: 0.0002339]
	Learning Rate: 0.000233898
	LOSS [training: 1.6120534752923124 | validation: 2.5037855027046714]
	TIME [epoch: 8.41 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.630624687329215		[learning rate: 0.00023347]
		[batch 20/20] avg loss: 1.5961294906231518		[learning rate: 0.00023305]
	Learning Rate: 0.000233049
	LOSS [training: 1.6133770889761838 | validation: 2.5014150370110224]
	TIME [epoch: 8.4 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6312852939520746		[learning rate: 0.00023263]
		[batch 20/20] avg loss: 1.6080107011568021		[learning rate: 0.0002322]
	Learning Rate: 0.000232203
	LOSS [training: 1.6196479975544384 | validation: 2.503798624648904]
	TIME [epoch: 8.42 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7512023382212185		[learning rate: 0.00023178]
		[batch 20/20] avg loss: 1.469265716577373		[learning rate: 0.00023136]
	Learning Rate: 0.000231361
	LOSS [training: 1.6102340273992959 | validation: 2.5052614634950117]
	TIME [epoch: 8.41 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7685433576267797		[learning rate: 0.00023094]
		[batch 20/20] avg loss: 1.4573356344170616		[learning rate: 0.00023052]
	Learning Rate: 0.000230521
	LOSS [training: 1.6129394960219208 | validation: 2.502743275094694]
	TIME [epoch: 8.4 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5792362481011766		[learning rate: 0.0002301]
		[batch 20/20] avg loss: 1.645868648894322		[learning rate: 0.00022968]
	Learning Rate: 0.000229685
	LOSS [training: 1.6125524484977494 | validation: 2.504081804302123]
	TIME [epoch: 8.4 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5831778299620756		[learning rate: 0.00022927]
		[batch 20/20] avg loss: 1.6506966726093335		[learning rate: 0.00022885]
	Learning Rate: 0.000228851
	LOSS [training: 1.616937251285704 | validation: 2.499928728277854]
	TIME [epoch: 8.43 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4398472390261556		[learning rate: 0.00022844]
		[batch 20/20] avg loss: 1.7914191073468093		[learning rate: 0.00022802]
	Learning Rate: 0.00022802
	LOSS [training: 1.6156331731864824 | validation: 2.5014606249982623]
	TIME [epoch: 8.41 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6541118989376549		[learning rate: 0.00022761]
		[batch 20/20] avg loss: 1.570779571725902		[learning rate: 0.00022719]
	Learning Rate: 0.000227193
	LOSS [training: 1.612445735331778 | validation: 2.5006170974578437]
	TIME [epoch: 8.41 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8120866479336997		[learning rate: 0.00022678]
		[batch 20/20] avg loss: 1.4197795918950704		[learning rate: 0.00022637]
	Learning Rate: 0.000226368
	LOSS [training: 1.615933119914385 | validation: 2.50093641592977]
	TIME [epoch: 8.41 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5634406209201592		[learning rate: 0.00022596]
		[batch 20/20] avg loss: 1.6629627924253068		[learning rate: 0.00022555]
	Learning Rate: 0.000225547
	LOSS [training: 1.6132017066727333 | validation: 2.5096821535863953]
	TIME [epoch: 8.4 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7375339889064332		[learning rate: 0.00022514]
		[batch 20/20] avg loss: 1.4849366133638653		[learning rate: 0.00022473]
	Learning Rate: 0.000224728
	LOSS [training: 1.611235301135149 | validation: 2.5297893471822253]
	TIME [epoch: 8.43 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5237060740034623		[learning rate: 0.00022432]
		[batch 20/20] avg loss: 1.7071309560136072		[learning rate: 0.00022391]
	Learning Rate: 0.000223913
	LOSS [training: 1.615418515008535 | validation: 2.4962018202587792]
	TIME [epoch: 8.4 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5790457674524871		[learning rate: 0.00022351]
		[batch 20/20] avg loss: 1.6440634300119483		[learning rate: 0.0002231]
	Learning Rate: 0.0002231
	LOSS [training: 1.611554598732218 | validation: 2.500344524212176]
	TIME [epoch: 8.4 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6556070937693381		[learning rate: 0.0002227]
		[batch 20/20] avg loss: 1.5778169189123799		[learning rate: 0.00022229]
	Learning Rate: 0.000222291
	LOSS [training: 1.6167120063408587 | validation: 2.5193191023090735]
	TIME [epoch: 8.39 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.829701223391917		[learning rate: 0.00022189]
		[batch 20/20] avg loss: 1.3981128995219314		[learning rate: 0.00022148]
	Learning Rate: 0.000221484
	LOSS [training: 1.6139070614569242 | validation: 2.4974413425376563]
	TIME [epoch: 8.42 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4939982930373463		[learning rate: 0.00022108]
		[batch 20/20] avg loss: 1.7289800962273485		[learning rate: 0.00022068]
	Learning Rate: 0.00022068
	LOSS [training: 1.6114891946323475 | validation: 2.4978715319621525]
	TIME [epoch: 8.4 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7563217594886922		[learning rate: 0.00022028]
		[batch 20/20] avg loss: 1.4658429511779505		[learning rate: 0.00021988]
	Learning Rate: 0.000219879
	LOSS [training: 1.6110823553333211 | validation: 2.5522085288460623]
	TIME [epoch: 8.4 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6403985943904629		[learning rate: 0.00021948]
		[batch 20/20] avg loss: 1.5964644451070487		[learning rate: 0.00021908]
	Learning Rate: 0.000219081
	LOSS [training: 1.6184315197487558 | validation: 2.502216122114623]
	TIME [epoch: 8.4 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6696131939840406		[learning rate: 0.00021868]
		[batch 20/20] avg loss: 1.5520024909587036		[learning rate: 0.00021829]
	Learning Rate: 0.000218286
	LOSS [training: 1.610807842471372 | validation: 2.4982457180874142]
	TIME [epoch: 8.42 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7063905611035175		[learning rate: 0.00021789]
		[batch 20/20] avg loss: 1.5194578936045426		[learning rate: 0.00021749]
	Learning Rate: 0.000217494
	LOSS [training: 1.6129242273540298 | validation: 2.52062145712898]
	TIME [epoch: 8.4 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6959498532935346		[learning rate: 0.0002171]
		[batch 20/20] avg loss: 1.5400447552630394		[learning rate: 0.0002167]
	Learning Rate: 0.000216705
	LOSS [training: 1.617997304278287 | validation: 2.5019231921076934]
	TIME [epoch: 8.4 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6057457486587734		[learning rate: 0.00021631]
		[batch 20/20] avg loss: 1.6158767738148825		[learning rate: 0.00021592]
	Learning Rate: 0.000215918
	LOSS [training: 1.6108112612368277 | validation: 2.499892858654478]
	TIME [epoch: 8.4 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.617688851325004		[learning rate: 0.00021553]
		[batch 20/20] avg loss: 1.6019992999290806		[learning rate: 0.00021513]
	Learning Rate: 0.000215135
	LOSS [training: 1.6098440756270425 | validation: 2.497200060431677]
	TIME [epoch: 8.4 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6002147645459153		[learning rate: 0.00021474]
		[batch 20/20] avg loss: 1.6256329739936977		[learning rate: 0.00021435]
	Learning Rate: 0.000214354
	LOSS [training: 1.6129238692698067 | validation: 2.505648833348683]
	TIME [epoch: 8.43 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.438085814493307		[learning rate: 0.00021396]
		[batch 20/20] avg loss: 1.7840738950265334		[learning rate: 0.00021358]
	Learning Rate: 0.000213576
	LOSS [training: 1.6110798547599199 | validation: 2.5243110545190133]
	TIME [epoch: 8.4 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.443576795797576		[learning rate: 0.00021319]
		[batch 20/20] avg loss: 1.7948678600416297		[learning rate: 0.0002128]
	Learning Rate: 0.000212801
	LOSS [training: 1.6192223279196027 | validation: 2.529337389458491]
	TIME [epoch: 8.4 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.677080761752339		[learning rate: 0.00021241]
		[batch 20/20] avg loss: 1.5570164305992382		[learning rate: 0.00021203]
	Learning Rate: 0.000212029
	LOSS [training: 1.6170485961757888 | validation: 2.49442920981564]
	TIME [epoch: 8.4 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6459411873154584		[learning rate: 0.00021164]
		[batch 20/20] avg loss: 1.5863175911682936		[learning rate: 0.00021126]
	Learning Rate: 0.000211259
	LOSS [training: 1.6161293892418762 | validation: 2.513767473445277]
	TIME [epoch: 8.42 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.597321359729301		[learning rate: 0.00021088]
		[batch 20/20] avg loss: 1.6297167357785003		[learning rate: 0.00021049]
	Learning Rate: 0.000210493
	LOSS [training: 1.6135190477539005 | validation: 2.498179115408849]
	TIME [epoch: 8.41 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5571728699123155		[learning rate: 0.00021011]
		[batch 20/20] avg loss: 1.663781484206222		[learning rate: 0.00020973]
	Learning Rate: 0.000209729
	LOSS [training: 1.6104771770592687 | validation: 2.5004303373609877]
	TIME [epoch: 8.4 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5714175574333251		[learning rate: 0.00020935]
		[batch 20/20] avg loss: 1.6677885675904816		[learning rate: 0.00020897]
	Learning Rate: 0.000208968
	LOSS [training: 1.6196030625119036 | validation: 2.502340341773472]
	TIME [epoch: 8.4 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6455604796112522		[learning rate: 0.00020859]
		[batch 20/20] avg loss: 1.5922735340368068		[learning rate: 0.00020821]
	Learning Rate: 0.000208209
	LOSS [training: 1.6189170068240295 | validation: 2.52129368519798]
	TIME [epoch: 8.42 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5320163777442326		[learning rate: 0.00020783]
		[batch 20/20] avg loss: 1.6949002015452639		[learning rate: 0.00020745]
	Learning Rate: 0.000207454
	LOSS [training: 1.6134582896447482 | validation: 2.4998697043906937]
	TIME [epoch: 8.4 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6480686857892386		[learning rate: 0.00020708]
		[batch 20/20] avg loss: 1.5814334270132429		[learning rate: 0.0002067]
	Learning Rate: 0.000206701
	LOSS [training: 1.614751056401241 | validation: 2.5231117765210107]
	TIME [epoch: 8.4 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.628400425796071		[learning rate: 0.00020633]
		[batch 20/20] avg loss: 1.5888886546695906		[learning rate: 0.00020595]
	Learning Rate: 0.000205951
	LOSS [training: 1.6086445402328313 | validation: 2.503479120971574]
	TIME [epoch: 8.4 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7198275749355674		[learning rate: 0.00020558]
		[batch 20/20] avg loss: 1.5090272099010815		[learning rate: 0.0002052]
	Learning Rate: 0.000205203
	LOSS [training: 1.6144273924183241 | validation: 2.504013641162343]
	TIME [epoch: 8.41 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4188836286873117		[learning rate: 0.00020483]
		[batch 20/20] avg loss: 1.8108827854296738		[learning rate: 0.00020446]
	Learning Rate: 0.000204459
	LOSS [training: 1.614883207058493 | validation: 2.501442473517451]
	TIME [epoch: 8.41 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6036446054482412		[learning rate: 0.00020409]
		[batch 20/20] avg loss: 1.618294752838079		[learning rate: 0.00020372]
	Learning Rate: 0.000203717
	LOSS [training: 1.6109696791431598 | validation: 2.496976022473203]
	TIME [epoch: 8.4 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5675763610068667		[learning rate: 0.00020335]
		[batch 20/20] avg loss: 1.6508859576340942		[learning rate: 0.00020298]
	Learning Rate: 0.000202977
	LOSS [training: 1.6092311593204802 | validation: 2.5007877679394266]
	TIME [epoch: 8.4 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.642620084147514		[learning rate: 0.00020261]
		[batch 20/20] avg loss: 1.5859461025260402		[learning rate: 0.00020224]
	Learning Rate: 0.000202241
	LOSS [training: 1.6142830933367773 | validation: 2.501961392433536]
	TIME [epoch: 8.4 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.55358458442709		[learning rate: 0.00020187]
		[batch 20/20] avg loss: 1.6764328233010228		[learning rate: 0.00020151]
	Learning Rate: 0.000201507
	LOSS [training: 1.6150087038640564 | validation: 2.4955756894959356]
	TIME [epoch: 8.42 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6282163834125125		[learning rate: 0.00020114]
		[batch 20/20] avg loss: 1.6161634422302757		[learning rate: 0.00020078]
	Learning Rate: 0.000200775
	LOSS [training: 1.6221899128213937 | validation: 2.5074894624964994]
	TIME [epoch: 8.4 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8292188825463132		[learning rate: 0.00020041]
		[batch 20/20] avg loss: 1.3968269499042894		[learning rate: 0.00020005]
	Learning Rate: 0.000200047
	LOSS [training: 1.6130229162253014 | validation: 2.505241306117321]
	TIME [epoch: 8.4 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.480004289790029		[learning rate: 0.00019968]
		[batch 20/20] avg loss: 1.7426347941679645		[learning rate: 0.00019932]
	Learning Rate: 0.000199321
	LOSS [training: 1.6113195419789963 | validation: 2.5070881251033534]
	TIME [epoch: 8.4 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.423113079062349		[learning rate: 0.00019896]
		[batch 20/20] avg loss: 1.8124439179938563		[learning rate: 0.0001986]
	Learning Rate: 0.000198597
	LOSS [training: 1.6177784985281025 | validation: 2.507899657962824]
	TIME [epoch: 8.43 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.801115502714908		[learning rate: 0.00019824]
		[batch 20/20] avg loss: 1.4221662817620067		[learning rate: 0.00019788]
	Learning Rate: 0.000197877
	LOSS [training: 1.6116408922384573 | validation: 2.4993705545211338]
	TIME [epoch: 8.4 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6652106440041223		[learning rate: 0.00019752]
		[batch 20/20] avg loss: 1.5625694524267963		[learning rate: 0.00019716]
	Learning Rate: 0.000197159
	LOSS [training: 1.6138900482154597 | validation: 2.5057096112677204]
	TIME [epoch: 8.4 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5414347221133498		[learning rate: 0.0001968]
		[batch 20/20] avg loss: 1.6826168238316843		[learning rate: 0.00019644]
	Learning Rate: 0.000196443
	LOSS [training: 1.6120257729725167 | validation: 2.5054776168351816]
	TIME [epoch: 8.39 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4704653395363807		[learning rate: 0.00019609]
		[batch 20/20] avg loss: 1.763937384916701		[learning rate: 0.00019573]
	Learning Rate: 0.00019573
	LOSS [training: 1.617201362226541 | validation: 2.506681576642579]
	TIME [epoch: 8.42 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6022361519679595		[learning rate: 0.00019537]
		[batch 20/20] avg loss: 1.6246187358121715		[learning rate: 0.00019502]
	Learning Rate: 0.00019502
	LOSS [training: 1.6134274438900658 | validation: 2.504194785839821]
	TIME [epoch: 8.41 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6023143522727477		[learning rate: 0.00019467]
		[batch 20/20] avg loss: 1.6150770592568364		[learning rate: 0.00019431]
	Learning Rate: 0.000194312
	LOSS [training: 1.6086957057647921 | validation: 2.503152613031214]
	TIME [epoch: 8.4 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5074168086180033		[learning rate: 0.00019396]
		[batch 20/20] avg loss: 1.7160842680851693		[learning rate: 0.00019361]
	Learning Rate: 0.000193607
	LOSS [training: 1.6117505383515867 | validation: 2.521702072973071]
	TIME [epoch: 8.4 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6667986224771876		[learning rate: 0.00019326]
		[batch 20/20] avg loss: 1.561507413144253		[learning rate: 0.0001929]
	Learning Rate: 0.000192904
	LOSS [training: 1.6141530178107204 | validation: 2.499843887756825]
	TIME [epoch: 8.4 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6232564537165526		[learning rate: 0.00019255]
		[batch 20/20] avg loss: 1.5966107642520189		[learning rate: 0.0001922]
	Learning Rate: 0.000192204
	LOSS [training: 1.6099336089842855 | validation: 2.4981905726960685]
	TIME [epoch: 8.42 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5541250621104008		[learning rate: 0.00019186]
		[batch 20/20] avg loss: 1.6741519173891644		[learning rate: 0.00019151]
	Learning Rate: 0.000191507
	LOSS [training: 1.614138489749783 | validation: 2.4997616446367426]
	TIME [epoch: 8.39 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4326628897162084		[learning rate: 0.00019116]
		[batch 20/20] avg loss: 1.796975739401041		[learning rate: 0.00019081]
	Learning Rate: 0.000190812
	LOSS [training: 1.6148193145586247 | validation: 2.502549712402887]
	TIME [epoch: 8.39 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.467112478455292		[learning rate: 0.00019047]
		[batch 20/20] avg loss: 1.7603154343009755		[learning rate: 0.00019012]
	Learning Rate: 0.000190119
	LOSS [training: 1.613713956378134 | validation: 2.5050273781148205]
	TIME [epoch: 8.4 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6818538337015805		[learning rate: 0.00018977]
		[batch 20/20] avg loss: 1.5509450003630991		[learning rate: 0.00018943]
	Learning Rate: 0.000189429
	LOSS [training: 1.6163994170323401 | validation: 2.500107504045843]
	TIME [epoch: 8.42 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3971486990058855		[learning rate: 0.00018909]
		[batch 20/20] avg loss: 1.8189915418239717		[learning rate: 0.00018874]
	Learning Rate: 0.000188742
	LOSS [training: 1.6080701204149288 | validation: 2.497944595405989]
	TIME [epoch: 8.41 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6472144525335075		[learning rate: 0.0001884]
		[batch 20/20] avg loss: 1.5827361850285393		[learning rate: 0.00018806]
	Learning Rate: 0.000188057
	LOSS [training: 1.6149753187810234 | validation: 2.501785995365614]
	TIME [epoch: 8.4 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6268911364207739		[learning rate: 0.00018772]
		[batch 20/20] avg loss: 1.596509377249172		[learning rate: 0.00018737]
	Learning Rate: 0.000187375
	LOSS [training: 1.6117002568349732 | validation: 2.496379378445989]
	TIME [epoch: 8.4 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6408660763487792		[learning rate: 0.00018703]
		[batch 20/20] avg loss: 1.5860111134884394		[learning rate: 0.00018669]
	Learning Rate: 0.000186695
	LOSS [training: 1.6134385949186094 | validation: 2.5017018708027416]
	TIME [epoch: 8.41 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.756105215710465		[learning rate: 0.00018636]
		[batch 20/20] avg loss: 1.4685440111507002		[learning rate: 0.00018602]
	Learning Rate: 0.000186017
	LOSS [training: 1.6123246134305824 | validation: 2.5008151487799815]
	TIME [epoch: 8.41 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8273069631987187		[learning rate: 0.00018568]
		[batch 20/20] avg loss: 1.3972018059417803		[learning rate: 0.00018534]
	Learning Rate: 0.000185342
	LOSS [training: 1.6122543845702495 | validation: 2.510913544634158]
	TIME [epoch: 8.4 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5623574697380975		[learning rate: 0.00018501]
		[batch 20/20] avg loss: 1.6714509054785673		[learning rate: 0.00018467]
	Learning Rate: 0.000184669
	LOSS [training: 1.6169041876083328 | validation: 2.495473339398403]
	TIME [epoch: 8.4 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7427961635393323		[learning rate: 0.00018433]
		[batch 20/20] avg loss: 1.4822850518372768		[learning rate: 0.000184]
	Learning Rate: 0.000183999
	LOSS [training: 1.6125406076883049 | validation: 2.497192487201332]
	TIME [epoch: 8.4 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5462805063710792		[learning rate: 0.00018367]
		[batch 20/20] avg loss: 1.6777015549352092		[learning rate: 0.00018333]
	Learning Rate: 0.000183331
	LOSS [training: 1.6119910306531442 | validation: 2.50425967811479]
	TIME [epoch: 8.42 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5216823285158592		[learning rate: 0.000183]
		[batch 20/20] avg loss: 1.7113589535656186		[learning rate: 0.00018267]
	Learning Rate: 0.000182666
	LOSS [training: 1.616520641040739 | validation: 2.497917002148904]
	TIME [epoch: 8.4 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7501801271805255		[learning rate: 0.00018233]
		[batch 20/20] avg loss: 1.469801480740313		[learning rate: 0.000182]
	Learning Rate: 0.000182003
	LOSS [training: 1.6099908039604192 | validation: 2.4971008421648238]
	TIME [epoch: 8.4 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6393394447966858		[learning rate: 0.00018167]
		[batch 20/20] avg loss: 1.5837222514522293		[learning rate: 0.00018134]
	Learning Rate: 0.000181343
	LOSS [training: 1.6115308481244575 | validation: 2.496820793356409]
	TIME [epoch: 8.4 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5317868324056974		[learning rate: 0.00018101]
		[batch 20/20] avg loss: 1.691048594612316		[learning rate: 0.00018068]
	Learning Rate: 0.000180685
	LOSS [training: 1.6114177135090066 | validation: 2.4969124942946204]
	TIME [epoch: 8.42 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6848625170584022		[learning rate: 0.00018036]
		[batch 20/20] avg loss: 1.5376816686717563		[learning rate: 0.00018003]
	Learning Rate: 0.000180029
	LOSS [training: 1.6112720928650792 | validation: 2.4987734029155657]
	TIME [epoch: 8.4 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5249379141904285		[learning rate: 0.0001797]
		[batch 20/20] avg loss: 1.69544766176883		[learning rate: 0.00017938]
	Learning Rate: 0.000179376
	LOSS [training: 1.610192787979629 | validation: 2.4984289112613314]
	TIME [epoch: 8.4 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6113255385876992		[learning rate: 0.00017905]
		[batch 20/20] avg loss: 1.615815142401253		[learning rate: 0.00017872]
	Learning Rate: 0.000178725
	LOSS [training: 1.6135703404944763 | validation: 2.501086178066855]
	TIME [epoch: 8.4 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.804240653511132		[learning rate: 0.0001784]
		[batch 20/20] avg loss: 1.4147807360977194		[learning rate: 0.00017808]
	Learning Rate: 0.000178076
	LOSS [training: 1.6095106948044253 | validation: 2.501179451048102]
	TIME [epoch: 8.42 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6394160947339542		[learning rate: 0.00017775]
		[batch 20/20] avg loss: 1.5771675143371364		[learning rate: 0.00017743]
	Learning Rate: 0.00017743
	LOSS [training: 1.6082918045355457 | validation: 2.500683182925404]
	TIME [epoch: 8.4 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5021742478353113		[learning rate: 0.00017711]
		[batch 20/20] avg loss: 1.7329626193216618		[learning rate: 0.00017679]
	Learning Rate: 0.000176786
	LOSS [training: 1.6175684335784866 | validation: 2.4989261003399186]
	TIME [epoch: 8.4 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5841471416557096		[learning rate: 0.00017646]
		[batch 20/20] avg loss: 1.6397152970547215		[learning rate: 0.00017614]
	Learning Rate: 0.000176144
	LOSS [training: 1.6119312193552158 | validation: 2.493991737614979]
	TIME [epoch: 8.4 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.553522452114097		[learning rate: 0.00017582]
		[batch 20/20] avg loss: 1.664529160979209		[learning rate: 0.0001755]
	Learning Rate: 0.000175505
	LOSS [training: 1.6090258065466532 | validation: 2.5109313864039438]
	TIME [epoch: 8.41 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5523293794683823		[learning rate: 0.00017519]
		[batch 20/20] avg loss: 1.6673789077741574		[learning rate: 0.00017487]
	Learning Rate: 0.000174868
	LOSS [training: 1.6098541436212694 | validation: 2.505882897206505]
	TIME [epoch: 8.41 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6640420426341855		[learning rate: 0.00017455]
		[batch 20/20] avg loss: 1.5689214424482656		[learning rate: 0.00017423]
	Learning Rate: 0.000174233
	LOSS [training: 1.6164817425412257 | validation: 2.499077137840974]
	TIME [epoch: 8.39 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6240490626500126		[learning rate: 0.00017392]
		[batch 20/20] avg loss: 1.5958148460076194		[learning rate: 0.0001736]
	Learning Rate: 0.000173601
	LOSS [training: 1.609931954328816 | validation: 2.505779230706718]
	TIME [epoch: 8.4 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6651930886921593		[learning rate: 0.00017329]
		[batch 20/20] avg loss: 1.5600439032479771		[learning rate: 0.00017297]
	Learning Rate: 0.000172971
	LOSS [training: 1.612618495970068 | validation: 2.50312206664215]
	TIME [epoch: 8.39 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.586032514117873		[learning rate: 0.00017266]
		[batch 20/20] avg loss: 1.6384647985549776		[learning rate: 0.00017234]
	Learning Rate: 0.000172343
	LOSS [training: 1.6122486563364251 | validation: 2.5005193459005604]
	TIME [epoch: 8.42 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6185864466794713		[learning rate: 0.00017203]
		[batch 20/20] avg loss: 1.599885369398775		[learning rate: 0.00017172]
	Learning Rate: 0.000171718
	LOSS [training: 1.6092359080391225 | validation: 2.4988254063333506]
	TIME [epoch: 8.39 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5381720922051163		[learning rate: 0.00017141]
		[batch 20/20] avg loss: 1.6846739155042183		[learning rate: 0.00017109]
	Learning Rate: 0.000171095
	LOSS [training: 1.6114230038546673 | validation: 2.5157329111235565]
	TIME [epoch: 8.4 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6021374502380605		[learning rate: 0.00017078]
		[batch 20/20] avg loss: 1.6165507424611376		[learning rate: 0.00017047]
	Learning Rate: 0.000170474
	LOSS [training: 1.609344096349599 | validation: 2.504402489807941]
	TIME [epoch: 8.39 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6897188912433498		[learning rate: 0.00017016]
		[batch 20/20] avg loss: 1.5257054525028966		[learning rate: 0.00016986]
	Learning Rate: 0.000169855
	LOSS [training: 1.6077121718731227 | validation: 2.499137060307651]
	TIME [epoch: 8.41 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5571099162398396		[learning rate: 0.00016955]
		[batch 20/20] avg loss: 1.665667819498717		[learning rate: 0.00016924]
	Learning Rate: 0.000169239
	LOSS [training: 1.6113888678692785 | validation: 2.5044964057129393]
	TIME [epoch: 8.4 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6867866144491248		[learning rate: 0.00016893]
		[batch 20/20] avg loss: 1.5419981482613694		[learning rate: 0.00016862]
	Learning Rate: 0.000168625
	LOSS [training: 1.6143923813552472 | validation: 2.494009843722732]
	TIME [epoch: 8.39 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4484942639718943		[learning rate: 0.00016832]
		[batch 20/20] avg loss: 1.775798068239738		[learning rate: 0.00016801]
	Learning Rate: 0.000168013
	LOSS [training: 1.6121461661058158 | validation: 2.5013504407310956]
	TIME [epoch: 8.39 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6175791352419338		[learning rate: 0.00016771]
		[batch 20/20] avg loss: 1.598128367015432		[learning rate: 0.0001674]
	Learning Rate: 0.000167403
	LOSS [training: 1.6078537511286828 | validation: 2.5026941030303624]
	TIME [epoch: 8.41 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6529581854462023		[learning rate: 0.0001671]
		[batch 20/20] avg loss: 1.5674236486294402		[learning rate: 0.0001668]
	Learning Rate: 0.000166795
	LOSS [training: 1.6101909170378215 | validation: 2.506342212550838]
	TIME [epoch: 8.4 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6617859614963184		[learning rate: 0.00016649]
		[batch 20/20] avg loss: 1.5731084896087058		[learning rate: 0.00016619]
	Learning Rate: 0.00016619
	LOSS [training: 1.617447225552512 | validation: 2.5139270629195685]
	TIME [epoch: 8.39 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5373182619845989		[learning rate: 0.00016589]
		[batch 20/20] avg loss: 1.6799004074052513		[learning rate: 0.00016559]
	Learning Rate: 0.000165587
	LOSS [training: 1.608609334694925 | validation: 2.506201179151109]
	TIME [epoch: 8.4 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5100063832030695		[learning rate: 0.00016529]
		[batch 20/20] avg loss: 1.7121478024781411		[learning rate: 0.00016499]
	Learning Rate: 0.000164986
	LOSS [training: 1.611077092840605 | validation: 2.498016866684792]
	TIME [epoch: 8.4 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.487210600674666		[learning rate: 0.00016469]
		[batch 20/20] avg loss: 1.734730739134699		[learning rate: 0.00016439]
	Learning Rate: 0.000164387
	LOSS [training: 1.6109706699046824 | validation: 2.497562735009569]
	TIME [epoch: 8.42 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5004791150703283		[learning rate: 0.00016409]
		[batch 20/20] avg loss: 1.71603810993523		[learning rate: 0.00016379]
	Learning Rate: 0.000163791
	LOSS [training: 1.6082586125027796 | validation: 2.5005952774227396]
	TIME [epoch: 8.4 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4833658226171433		[learning rate: 0.00016349]
		[batch 20/20] avg loss: 1.738474948796064		[learning rate: 0.0001632]
	Learning Rate: 0.000163196
	LOSS [training: 1.6109203857066035 | validation: 2.5051136579574362]
	TIME [epoch: 8.39 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5879104482600834		[learning rate: 0.0001629]
		[batch 20/20] avg loss: 1.633091107991661		[learning rate: 0.0001626]
	Learning Rate: 0.000162604
	LOSS [training: 1.610500778125872 | validation: 2.4953924678007726]
	TIME [epoch: 8.4 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5201601147183237		[learning rate: 0.00016231]
		[batch 20/20] avg loss: 1.7019947145035352		[learning rate: 0.00016201]
	Learning Rate: 0.000162014
	LOSS [training: 1.6110774146109292 | validation: 2.5030852667642747]
	TIME [epoch: 8.42 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.651403302012064		[learning rate: 0.00016172]
		[batch 20/20] avg loss: 1.5704377876971993		[learning rate: 0.00016143]
	Learning Rate: 0.000161426
	LOSS [training: 1.6109205448546313 | validation: 2.5081741051167143]
	TIME [epoch: 8.41 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5067191065615437		[learning rate: 0.00016113]
		[batch 20/20] avg loss: 1.7144959789993404		[learning rate: 0.00016084]
	Learning Rate: 0.00016084
	LOSS [training: 1.610607542780442 | validation: 2.5045257367105838]
	TIME [epoch: 8.39 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6485416834338333		[learning rate: 0.00016055]
		[batch 20/20] avg loss: 1.5639691294159672		[learning rate: 0.00016026]
	Learning Rate: 0.000160257
	LOSS [training: 1.6062554064249004 | validation: 2.5009341868397463]
	TIME [epoch: 8.4 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5706634136561184		[learning rate: 0.00015997]
		[batch 20/20] avg loss: 1.6449945152884329		[learning rate: 0.00015967]
	Learning Rate: 0.000159675
	LOSS [training: 1.6078289644722754 | validation: 2.505184328043668]
	TIME [epoch: 8.41 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.571292767676383		[learning rate: 0.00015938]
		[batch 20/20] avg loss: 1.645851224103317		[learning rate: 0.0001591]
	Learning Rate: 0.000159096
	LOSS [training: 1.6085719958898501 | validation: 2.5075473538404287]
	TIME [epoch: 8.4 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5859066980995473		[learning rate: 0.00015881]
		[batch 20/20] avg loss: 1.6421614891607679		[learning rate: 0.00015852]
	Learning Rate: 0.000158518
	LOSS [training: 1.6140340936301576 | validation: 2.5022346261129154]
	TIME [epoch: 8.4 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5940336147474297		[learning rate: 0.00015823]
		[batch 20/20] avg loss: 1.6255928353594178		[learning rate: 0.00015794]
	Learning Rate: 0.000157943
	LOSS [training: 1.6098132250534234 | validation: 2.510633470885703]
	TIME [epoch: 8.4 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4564084143851144		[learning rate: 0.00015766]
		[batch 20/20] avg loss: 1.7727836883647925		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 1.6145960513749535 | validation: 2.508338130024341]
	TIME [epoch: 8.4 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5837538762737307		[learning rate: 0.00015708]
		[batch 20/20] avg loss: 1.639266342995488		[learning rate: 0.0001568]
	Learning Rate: 0.000156799
	LOSS [training: 1.6115101096346092 | validation: 2.5027099937183275]
	TIME [epoch: 8.41 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4772076260723714		[learning rate: 0.00015651]
		[batch 20/20] avg loss: 1.745519771766812		[learning rate: 0.00015623]
	Learning Rate: 0.00015623
	LOSS [training: 1.6113636989195914 | validation: 2.5063201325961053]
	TIME [epoch: 8.39 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6313780704797207		[learning rate: 0.00015595]
		[batch 20/20] avg loss: 1.592410863967737		[learning rate: 0.00015566]
	Learning Rate: 0.000155663
	LOSS [training: 1.6118944672237288 | validation: 2.5031633802437203]
	TIME [epoch: 8.39 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6412167152541883		[learning rate: 0.00015538]
		[batch 20/20] avg loss: 1.5809885786783646		[learning rate: 0.0001551]
	Learning Rate: 0.000155098
	LOSS [training: 1.6111026469662764 | validation: 2.502717329970633]
	TIME [epoch: 8.39 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6951894159755554		[learning rate: 0.00015482]
		[batch 20/20] avg loss: 1.5267179167203166		[learning rate: 0.00015453]
	Learning Rate: 0.000154535
	LOSS [training: 1.610953666347936 | validation: 2.494282574819918]
	TIME [epoch: 8.42 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5272166525217314		[learning rate: 0.00015425]
		[batch 20/20] avg loss: 1.6909153263771646		[learning rate: 0.00015397]
	Learning Rate: 0.000153974
	LOSS [training: 1.609065989449448 | validation: 2.501746359138335]
	TIME [epoch: 8.39 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.624334741780886		[learning rate: 0.00015369]
		[batch 20/20] avg loss: 1.5953464626165066		[learning rate: 0.00015342]
	Learning Rate: 0.000153415
	LOSS [training: 1.6098406021986964 | validation: 2.4989628646825817]
	TIME [epoch: 8.39 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6222486038671828		[learning rate: 0.00015314]
		[batch 20/20] avg loss: 1.595922695200891		[learning rate: 0.00015286]
	Learning Rate: 0.000152858
	LOSS [training: 1.6090856495340369 | validation: 2.4962853745058045]
	TIME [epoch: 8.39 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5361792441483573		[learning rate: 0.00015258]
		[batch 20/20] avg loss: 1.6938824450297134		[learning rate: 0.0001523]
	Learning Rate: 0.000152304
	LOSS [training: 1.6150308445890353 | validation: 2.560882215485672]
	TIME [epoch: 8.42 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.760685910692277		[learning rate: 0.00015203]
		[batch 20/20] avg loss: 1.4789694588217501		[learning rate: 0.00015175]
	Learning Rate: 0.000151751
	LOSS [training: 1.6198276847570134 | validation: 2.4971979274801734]
	TIME [epoch: 8.4 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.569440477694499		[learning rate: 0.00015148]
		[batch 20/20] avg loss: 1.6588613850724545		[learning rate: 0.0001512]
	Learning Rate: 0.0001512
	LOSS [training: 1.614150931383477 | validation: 2.4995722125549737]
	TIME [epoch: 8.4 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6968123604065024		[learning rate: 0.00015093]
		[batch 20/20] avg loss: 1.5272811560533892		[learning rate: 0.00015065]
	Learning Rate: 0.000150652
	LOSS [training: 1.6120467582299458 | validation: 2.495500744125221]
	TIME [epoch: 8.39 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7049909193279604		[learning rate: 0.00015038]
		[batch 20/20] avg loss: 1.5108870669514662		[learning rate: 0.0001501]
	Learning Rate: 0.000150105
	LOSS [training: 1.607938993139713 | validation: 2.503068169890807]
	TIME [epoch: 8.4 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5899890951905298		[learning rate: 0.00014983]
		[batch 20/20] avg loss: 1.6438421279522213		[learning rate: 0.00014956]
	Learning Rate: 0.00014956
	LOSS [training: 1.6169156115713754 | validation: 2.5014505965628766]
	TIME [epoch: 8.42 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6929966721554788		[learning rate: 0.00014929]
		[batch 20/20] avg loss: 1.5342200838293185		[learning rate: 0.00014902]
	Learning Rate: 0.000149017
	LOSS [training: 1.6136083779923986 | validation: 2.5051714736472688]
	TIME [epoch: 8.39 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6255998798187608		[learning rate: 0.00014875]
		[batch 20/20] avg loss: 1.5926316208750397		[learning rate: 0.00014848]
	Learning Rate: 0.000148477
	LOSS [training: 1.6091157503469007 | validation: 2.4990177923056756]
	TIME [epoch: 8.4 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5437089674459652		[learning rate: 0.00014821]
		[batch 20/20] avg loss: 1.6753752778180355		[learning rate: 0.00014794]
	Learning Rate: 0.000147938
	LOSS [training: 1.6095421226320004 | validation: 2.49238392461186]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_1259.pth
	Model improved!!!
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5529919648155193		[learning rate: 0.00014767]
		[batch 20/20] avg loss: 1.6679913288754338		[learning rate: 0.0001474]
	Learning Rate: 0.000147401
	LOSS [training: 1.6104916468454769 | validation: 2.4955590438433193]
	TIME [epoch: 8.41 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6008667428671095		[learning rate: 0.00014713]
		[batch 20/20] avg loss: 1.6212100941267553		[learning rate: 0.00014687]
	Learning Rate: 0.000146866
	LOSS [training: 1.6110384184969324 | validation: 2.500000154788369]
	TIME [epoch: 8.39 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5082387776613724		[learning rate: 0.0001466]
		[batch 20/20] avg loss: 1.7113263736212345		[learning rate: 0.00014633]
	Learning Rate: 0.000146333
	LOSS [training: 1.6097825756413038 | validation: 2.5014707705567654]
	TIME [epoch: 8.39 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5568733665978862		[learning rate: 0.00014607]
		[batch 20/20] avg loss: 1.6681926675039658		[learning rate: 0.0001458]
	Learning Rate: 0.000145802
	LOSS [training: 1.6125330170509262 | validation: 2.501253209636999]
	TIME [epoch: 8.39 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6284879577612181		[learning rate: 0.00014554]
		[batch 20/20] avg loss: 1.585004299689842		[learning rate: 0.00014527]
	Learning Rate: 0.000145273
	LOSS [training: 1.6067461287255298 | validation: 2.497758485838146]
	TIME [epoch: 8.4 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5334344990063347		[learning rate: 0.00014501]
		[batch 20/20] avg loss: 1.6860774588432292		[learning rate: 0.00014475]
	Learning Rate: 0.000144746
	LOSS [training: 1.6097559789247817 | validation: 2.497671129264442]
	TIME [epoch: 8.39 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7272369203497528		[learning rate: 0.00014448]
		[batch 20/20] avg loss: 1.479761988150194		[learning rate: 0.00014422]
	Learning Rate: 0.00014422
	LOSS [training: 1.6034994542499732 | validation: 2.4908696960493]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_1266.pth
	Model improved!!!
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5216009438494829		[learning rate: 0.00014396]
		[batch 20/20] avg loss: 1.6954137248766863		[learning rate: 0.0001437]
	Learning Rate: 0.000143697
	LOSS [training: 1.6085073343630847 | validation: 2.4936390727456317]
	TIME [epoch: 8.38 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.638011661710078		[learning rate: 0.00014344]
		[batch 20/20] avg loss: 1.5778823676280627		[learning rate: 0.00014318]
	Learning Rate: 0.000143175
	LOSS [training: 1.6079470146690702 | validation: 2.502591667244383]
	TIME [epoch: 8.4 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7013960519441482		[learning rate: 0.00014292]
		[batch 20/20] avg loss: 1.5219828331892602		[learning rate: 0.00014266]
	Learning Rate: 0.000142656
	LOSS [training: 1.611689442566704 | validation: 2.497683763077396]
	TIME [epoch: 8.38 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7141743045546491		[learning rate: 0.0001424]
		[batch 20/20] avg loss: 1.5115705367622128		[learning rate: 0.00014214]
	Learning Rate: 0.000142138
	LOSS [training: 1.6128724206584306 | validation: 2.511863864729632]
	TIME [epoch: 8.38 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6272658398933335		[learning rate: 0.00014188]
		[batch 20/20] avg loss: 1.5898764503240572		[learning rate: 0.00014162]
	Learning Rate: 0.000141622
	LOSS [training: 1.608571145108695 | validation: 2.4891629399683266]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_1271.pth
	Model improved!!!
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6691786672940228		[learning rate: 0.00014137]
		[batch 20/20] avg loss: 1.5466699745984651		[learning rate: 0.00014111]
	Learning Rate: 0.000141108
	LOSS [training: 1.607924320946244 | validation: 2.4982762026484138]
	TIME [epoch: 8.38 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7979933640048533		[learning rate: 0.00014085]
		[batch 20/20] avg loss: 1.4178993893328535		[learning rate: 0.0001406]
	Learning Rate: 0.000140596
	LOSS [training: 1.6079463766688533 | validation: 2.5038143358169354]
	TIME [epoch: 8.4 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6317752847950266		[learning rate: 0.00014034]
		[batch 20/20] avg loss: 1.5911896350261172		[learning rate: 0.00014009]
	Learning Rate: 0.000140086
	LOSS [training: 1.6114824599105717 | validation: 2.493651213577481]
	TIME [epoch: 8.37 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7898644513640771		[learning rate: 0.00013983]
		[batch 20/20] avg loss: 1.4309581407998286		[learning rate: 0.00013958]
	Learning Rate: 0.000139578
	LOSS [training: 1.6104112960819523 | validation: 2.4962491809656067]
	TIME [epoch: 8.37 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7226746895679743		[learning rate: 0.00013932]
		[batch 20/20] avg loss: 1.4938485550611207		[learning rate: 0.00013907]
	Learning Rate: 0.000139071
	LOSS [training: 1.6082616223145476 | validation: 2.492030862199264]
	TIME [epoch: 8.37 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5286989991010809		[learning rate: 0.00013882]
		[batch 20/20] avg loss: 1.6928850568686624		[learning rate: 0.00013857]
	Learning Rate: 0.000138566
	LOSS [training: 1.6107920279848718 | validation: 2.506352383581206]
	TIME [epoch: 8.39 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5956897035747255		[learning rate: 0.00013831]
		[batch 20/20] avg loss: 1.6303760684237034		[learning rate: 0.00013806]
	Learning Rate: 0.000138064
	LOSS [training: 1.6130328859992145 | validation: 2.503819150424232]
	TIME [epoch: 8.37 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7000426271291418		[learning rate: 0.00013781]
		[batch 20/20] avg loss: 1.5218341988989168		[learning rate: 0.00013756]
	Learning Rate: 0.000137562
	LOSS [training: 1.6109384130140292 | validation: 2.495428920914968]
	TIME [epoch: 8.37 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.92233714716177		[learning rate: 0.00013731]
		[batch 20/20] avg loss: 1.2981294096871765		[learning rate: 0.00013706]
	Learning Rate: 0.000137063
	LOSS [training: 1.6102332784244733 | validation: 2.499290252427579]
	TIME [epoch: 8.37 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.556493882491806		[learning rate: 0.00013681]
		[batch 20/20] avg loss: 1.6607246771401598		[learning rate: 0.00013657]
	Learning Rate: 0.000136566
	LOSS [training: 1.6086092798159828 | validation: 2.501490038937363]
	TIME [epoch: 8.39 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7017838921005044		[learning rate: 0.00013632]
		[batch 20/20] avg loss: 1.5284339353637368		[learning rate: 0.00013607]
	Learning Rate: 0.00013607
	LOSS [training: 1.6151089137321204 | validation: 2.4947634435886825]
	TIME [epoch: 8.37 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6939284720906542		[learning rate: 0.00013582]
		[batch 20/20] avg loss: 1.5302374864894868		[learning rate: 0.00013558]
	Learning Rate: 0.000135576
	LOSS [training: 1.6120829792900704 | validation: 2.5137686773040944]
	TIME [epoch: 8.38 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5535103536157728		[learning rate: 0.00013533]
		[batch 20/20] avg loss: 1.6734284913582613		[learning rate: 0.00013508]
	Learning Rate: 0.000135084
	LOSS [training: 1.613469422487017 | validation: 2.4967993002075297]
	TIME [epoch: 8.38 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8019868191070025		[learning rate: 0.00013484]
		[batch 20/20] avg loss: 1.420916785593866		[learning rate: 0.00013459]
	Learning Rate: 0.000134594
	LOSS [training: 1.6114518023504343 | validation: 2.501221978840772]
	TIME [epoch: 8.37 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.476444739949295		[learning rate: 0.00013435]
		[batch 20/20] avg loss: 1.7477058920228317		[learning rate: 0.00013411]
	Learning Rate: 0.000134106
	LOSS [training: 1.6120753159860635 | validation: 2.495701376196803]
	TIME [epoch: 8.39 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6990264920388458		[learning rate: 0.00013386]
		[batch 20/20] avg loss: 1.513975044807741		[learning rate: 0.00013362]
	Learning Rate: 0.000133619
	LOSS [training: 1.6065007684232935 | validation: 2.4939571779057585]
	TIME [epoch: 8.37 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5310237514453575		[learning rate: 0.00013338]
		[batch 20/20] avg loss: 1.6913337096916332		[learning rate: 0.00013313]
	Learning Rate: 0.000133134
	LOSS [training: 1.6111787305684957 | validation: 2.5064945336582376]
	TIME [epoch: 8.37 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3787704920048243		[learning rate: 0.00013289]
		[batch 20/20] avg loss: 1.8371699396236292		[learning rate: 0.00013265]
	Learning Rate: 0.000132651
	LOSS [training: 1.6079702158142268 | validation: 2.497152704445484]
	TIME [epoch: 8.37 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7900617973533748		[learning rate: 0.00013241]
		[batch 20/20] avg loss: 1.4226292498906399		[learning rate: 0.00013217]
	Learning Rate: 0.00013217
	LOSS [training: 1.6063455236220072 | validation: 2.502246775249672]
	TIME [epoch: 8.39 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5690307314967544		[learning rate: 0.00013193]
		[batch 20/20] avg loss: 1.6551192104950736		[learning rate: 0.00013169]
	Learning Rate: 0.00013169
	LOSS [training: 1.612074970995914 | validation: 2.5045830077658207]
	TIME [epoch: 8.36 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5997346063220266		[learning rate: 0.00013145]
		[batch 20/20] avg loss: 1.6213082640888312		[learning rate: 0.00013121]
	Learning Rate: 0.000131212
	LOSS [training: 1.6105214352054287 | validation: 2.4985737980114977]
	TIME [epoch: 8.38 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7544063586670393		[learning rate: 0.00013097]
		[batch 20/20] avg loss: 1.4649126323291792		[learning rate: 0.00013074]
	Learning Rate: 0.000130736
	LOSS [training: 1.6096594954981094 | validation: 2.495732629316794]
	TIME [epoch: 8.37 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5895328885863427		[learning rate: 0.0001305]
		[batch 20/20] avg loss: 1.6304616990394734		[learning rate: 0.00013026]
	Learning Rate: 0.000130261
	LOSS [training: 1.6099972938129077 | validation: 2.5090678217318727]
	TIME [epoch: 8.39 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.724820329298554		[learning rate: 0.00013002]
		[batch 20/20] avg loss: 1.4945562161695676		[learning rate: 0.00012979]
	Learning Rate: 0.000129789
	LOSS [training: 1.609688272734061 | validation: 2.4977118461990706]
	TIME [epoch: 8.38 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6145471722629978		[learning rate: 0.00012955]
		[batch 20/20] avg loss: 1.6020180076853276		[learning rate: 0.00012932]
	Learning Rate: 0.000129318
	LOSS [training: 1.6082825899741628 | validation: 2.496042999773267]
	TIME [epoch: 8.38 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7097811240230711		[learning rate: 0.00012908]
		[batch 20/20] avg loss: 1.5020753619978453		[learning rate: 0.00012885]
	Learning Rate: 0.000128848
	LOSS [training: 1.605928243010458 | validation: 2.5002212517299807]
	TIME [epoch: 8.37 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4717277375899553		[learning rate: 0.00012861]
		[batch 20/20] avg loss: 1.7424825701513433		[learning rate: 0.00012838]
	Learning Rate: 0.000128381
	LOSS [training: 1.6071051538706491 | validation: 2.4941001320216536]
	TIME [epoch: 8.38 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7547095055967126		[learning rate: 0.00012815]
		[batch 20/20] avg loss: 1.4589871732623068		[learning rate: 0.00012791]
	Learning Rate: 0.000127915
	LOSS [training: 1.6068483394295097 | validation: 2.4942698141071493]
	TIME [epoch: 8.39 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.598220938008684		[learning rate: 0.00012768]
		[batch 20/20] avg loss: 1.61609397302055		[learning rate: 0.00012745]
	Learning Rate: 0.000127451
	LOSS [training: 1.6071574555146166 | validation: 2.500431550371867]
	TIME [epoch: 8.37 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5903549411455586		[learning rate: 0.00012722]
		[batch 20/20] avg loss: 1.6338950967884045		[learning rate: 0.00012699]
	Learning Rate: 0.000126988
	LOSS [training: 1.6121250189669816 | validation: 2.4919234231275356]
	TIME [epoch: 8.37 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5116041658896104		[learning rate: 0.00012676]
		[batch 20/20] avg loss: 1.7125112668673736		[learning rate: 0.00012653]
	Learning Rate: 0.000126527
	LOSS [training: 1.612057716378492 | validation: 2.502121311916789]
	TIME [epoch: 8.37 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6545402527119955		[learning rate: 0.0001263]
		[batch 20/20] avg loss: 1.5653363637304913		[learning rate: 0.00012607]
	Learning Rate: 0.000126068
	LOSS [training: 1.6099383082212433 | validation: 2.4938964228481124]
	TIME [epoch: 8.39 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7384870482714514		[learning rate: 0.00012584]
		[batch 20/20] avg loss: 1.4871764340671751		[learning rate: 0.00012561]
	Learning Rate: 0.000125611
	LOSS [training: 1.6128317411693132 | validation: 2.4914913313494775]
	TIME [epoch: 8.37 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.451533454863824		[learning rate: 0.00012538]
		[batch 20/20] avg loss: 1.7709482310390086		[learning rate: 0.00012515]
	Learning Rate: 0.000125155
	LOSS [training: 1.6112408429514165 | validation: 2.499348862058073]
	TIME [epoch: 8.37 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5663103674364822		[learning rate: 0.00012493]
		[batch 20/20] avg loss: 1.6525304086790211		[learning rate: 0.0001247]
	Learning Rate: 0.000124701
	LOSS [training: 1.6094203880577524 | validation: 2.4985321800520914]
	TIME [epoch: 8.36 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5035666772135603		[learning rate: 0.00012447]
		[batch 20/20] avg loss: 1.7085428951007575		[learning rate: 0.00012425]
	Learning Rate: 0.000124248
	LOSS [training: 1.6060547861571592 | validation: 2.4986618270944208]
	TIME [epoch: 8.39 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7177109235919001		[learning rate: 0.00012402]
		[batch 20/20] avg loss: 1.5064757939305466		[learning rate: 0.0001238]
	Learning Rate: 0.000123797
	LOSS [training: 1.6120933587612232 | validation: 2.496979163170555]
	TIME [epoch: 8.37 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7705481037765096		[learning rate: 0.00012357]
		[batch 20/20] avg loss: 1.4483451729843648		[learning rate: 0.00012335]
	Learning Rate: 0.000123348
	LOSS [training: 1.6094466383804373 | validation: 2.502878735103464]
	TIME [epoch: 8.37 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6207530550971203		[learning rate: 0.00012312]
		[batch 20/20] avg loss: 1.5938397075400499		[learning rate: 0.0001229]
	Learning Rate: 0.0001229
	LOSS [training: 1.607296381318585 | validation: 2.498614381454169]
	TIME [epoch: 8.37 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6203884349883306		[learning rate: 0.00012268]
		[batch 20/20] avg loss: 1.5944158918834537		[learning rate: 0.00012245]
	Learning Rate: 0.000122454
	LOSS [training: 1.6074021634358922 | validation: 2.493127709060116]
	TIME [epoch: 8.38 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5957000206659229		[learning rate: 0.00012223]
		[batch 20/20] avg loss: 1.6217492863896095		[learning rate: 0.00012201]
	Learning Rate: 0.00012201
	LOSS [training: 1.6087246535277662 | validation: 2.494081551983993]
	TIME [epoch: 8.38 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5276370847148502		[learning rate: 0.00012179]
		[batch 20/20] avg loss: 1.6864150056245357		[learning rate: 0.00012157]
	Learning Rate: 0.000121567
	LOSS [training: 1.6070260451696927 | validation: 2.5005144003130084]
	TIME [epoch: 8.37 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4782559956242156		[learning rate: 0.00012135]
		[batch 20/20] avg loss: 1.7444677539177884		[learning rate: 0.00012113]
	Learning Rate: 0.000121126
	LOSS [training: 1.6113618747710017 | validation: 2.50546911707392]
	TIME [epoch: 8.37 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5166848143022311		[learning rate: 0.00012091]
		[batch 20/20] avg loss: 1.7157445050400224		[learning rate: 0.00012069]
	Learning Rate: 0.000120686
	LOSS [training: 1.6162146596711267 | validation: 2.496843511128349]
	TIME [epoch: 8.38 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5709159623890565		[learning rate: 0.00012047]
		[batch 20/20] avg loss: 1.6493624133591402		[learning rate: 0.00012025]
	Learning Rate: 0.000120248
	LOSS [training: 1.610139187874098 | validation: 2.5009037982419855]
	TIME [epoch: 8.39 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6445688195921833		[learning rate: 0.00012003]
		[batch 20/20] avg loss: 1.5704033023222235		[learning rate: 0.00011981]
	Learning Rate: 0.000119812
	LOSS [training: 1.6074860609572028 | validation: 2.498929904836092]
	TIME [epoch: 8.37 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6818604237951593		[learning rate: 0.00011959]
		[batch 20/20] avg loss: 1.5370538346815885		[learning rate: 0.00011938]
	Learning Rate: 0.000119377
	LOSS [training: 1.6094571292383741 | validation: 2.501880713435497]
	TIME [epoch: 8.38 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7335012168325012		[learning rate: 0.00011916]
		[batch 20/20] avg loss: 1.4864909749705562		[learning rate: 0.00011894]
	Learning Rate: 0.000118944
	LOSS [training: 1.6099960959015285 | validation: 2.5008650546117717]
	TIME [epoch: 8.37 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7027545866981957		[learning rate: 0.00011873]
		[batch 20/20] avg loss: 1.5152662411256188		[learning rate: 0.00011851]
	Learning Rate: 0.000118512
	LOSS [training: 1.6090104139119075 | validation: 2.509599230297007]
	TIME [epoch: 8.39 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7288115751721775		[learning rate: 0.0001183]
		[batch 20/20] avg loss: 1.4953129018186229		[learning rate: 0.00011808]
	Learning Rate: 0.000118082
	LOSS [training: 1.6120622384954 | validation: 2.4972537344151884]
	TIME [epoch: 8.37 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6239067041877084		[learning rate: 0.00011787]
		[batch 20/20] avg loss: 1.5962328595720279		[learning rate: 0.00011765]
	Learning Rate: 0.000117654
	LOSS [training: 1.6100697818798682 | validation: 2.506086157589714]
	TIME [epoch: 8.37 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4664019898048906		[learning rate: 0.00011744]
		[batch 20/20] avg loss: 1.7474297730671207		[learning rate: 0.00011723]
	Learning Rate: 0.000117227
	LOSS [training: 1.6069158814360054 | validation: 2.497350279619747]
	TIME [epoch: 8.37 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.537030403799778		[learning rate: 0.00011701]
		[batch 20/20] avg loss: 1.6729698196520448		[learning rate: 0.0001168]
	Learning Rate: 0.000116801
	LOSS [training: 1.6050001117259118 | validation: 2.499762994856739]
	TIME [epoch: 8.39 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.782647512559516		[learning rate: 0.00011659]
		[batch 20/20] avg loss: 1.4370112478287516		[learning rate: 0.00011638]
	Learning Rate: 0.000116377
	LOSS [training: 1.609829380194134 | validation: 2.501900424848244]
	TIME [epoch: 8.38 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5844382162427122		[learning rate: 0.00011617]
		[batch 20/20] avg loss: 1.6353502120037338		[learning rate: 0.00011595]
	Learning Rate: 0.000115955
	LOSS [training: 1.6098942141232233 | validation: 2.5039156905299507]
	TIME [epoch: 8.37 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5859302816038303		[learning rate: 0.00011574]
		[batch 20/20] avg loss: 1.6440546662228979		[learning rate: 0.00011553]
	Learning Rate: 0.000115534
	LOSS [training: 1.614992473913364 | validation: 2.5006872029585008]
	TIME [epoch: 8.38 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5731121205491887		[learning rate: 0.00011532]
		[batch 20/20] avg loss: 1.6397094147348923		[learning rate: 0.00011511]
	Learning Rate: 0.000115115
	LOSS [training: 1.6064107676420403 | validation: 2.498265035553373]
	TIME [epoch: 8.37 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6122569846914643		[learning rate: 0.00011491]
		[batch 20/20] avg loss: 1.6121948877528383		[learning rate: 0.0001147]
	Learning Rate: 0.000114697
	LOSS [training: 1.6122259362221514 | validation: 2.5011115764629155]
	TIME [epoch: 8.39 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6264149040399434		[learning rate: 0.00011449]
		[batch 20/20] avg loss: 1.5959591693981978		[learning rate: 0.00011428]
	Learning Rate: 0.000114281
	LOSS [training: 1.6111870367190704 | validation: 2.49870220023431]
	TIME [epoch: 8.38 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7268912849859717		[learning rate: 0.00011407]
		[batch 20/20] avg loss: 1.4870030558983338		[learning rate: 0.00011387]
	Learning Rate: 0.000113866
	LOSS [training: 1.6069471704421527 | validation: 2.5011087646916206]
	TIME [epoch: 8.37 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4731052642304054		[learning rate: 0.00011366]
		[batch 20/20] avg loss: 1.7451680619432293		[learning rate: 0.00011345]
	Learning Rate: 0.000113453
	LOSS [training: 1.609136663086817 | validation: 2.497807771496966]
	TIME [epoch: 8.37 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6994739451199163		[learning rate: 0.00011325]
		[batch 20/20] avg loss: 1.5142413597207443		[learning rate: 0.00011304]
	Learning Rate: 0.000113041
	LOSS [training: 1.6068576524203304 | validation: 2.4932557962777433]
	TIME [epoch: 8.39 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6531166887319313		[learning rate: 0.00011284]
		[batch 20/20] avg loss: 1.562869083012984		[learning rate: 0.00011263]
	Learning Rate: 0.000112631
	LOSS [training: 1.6079928858724575 | validation: 2.5047652542160943]
	TIME [epoch: 8.37 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7844238311521594		[learning rate: 0.00011243]
		[batch 20/20] avg loss: 1.4370007896133905		[learning rate: 0.00011222]
	Learning Rate: 0.000112222
	LOSS [training: 1.6107123103827745 | validation: 2.492665205734081]
	TIME [epoch: 8.37 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.629745236594205		[learning rate: 0.00011202]
		[batch 20/20] avg loss: 1.5852866907295327		[learning rate: 0.00011181]
	Learning Rate: 0.000111815
	LOSS [training: 1.607515963661869 | validation: 2.4918960672717345]
	TIME [epoch: 8.37 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6069180548828936		[learning rate: 0.00011161]
		[batch 20/20] avg loss: 1.611306511309841		[learning rate: 0.00011141]
	Learning Rate: 0.000111409
	LOSS [training: 1.6091122830963673 | validation: 2.5016602227993956]
	TIME [epoch: 8.39 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5641659190189825		[learning rate: 0.00011121]
		[batch 20/20] avg loss: 1.6614502229464059		[learning rate: 0.000111]
	Learning Rate: 0.000111005
	LOSS [training: 1.612808070982694 | validation: 2.506343418948727]
	TIME [epoch: 8.37 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7500335065427812		[learning rate: 0.0001108]
		[batch 20/20] avg loss: 1.4808717586536306		[learning rate: 0.0001106]
	Learning Rate: 0.000110602
	LOSS [training: 1.6154526325982057 | validation: 2.5000624333988233]
	TIME [epoch: 8.37 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5000514221340473		[learning rate: 0.0001104]
		[batch 20/20] avg loss: 1.7172811300643804		[learning rate: 0.0001102]
	Learning Rate: 0.000110201
	LOSS [training: 1.608666276099214 | validation: 2.494435998412946]
	TIME [epoch: 8.37 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4685923015401119		[learning rate: 0.00011]
		[batch 20/20] avg loss: 1.744813222737792		[learning rate: 0.0001098]
	Learning Rate: 0.000109801
	LOSS [training: 1.606702762138952 | validation: 2.498844620342825]
	TIME [epoch: 8.37 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6396450744565398		[learning rate: 0.0001096]
		[batch 20/20] avg loss: 1.5747692267128193		[learning rate: 0.0001094]
	Learning Rate: 0.000109402
	LOSS [training: 1.6072071505846792 | validation: 2.4943183538670644]
	TIME [epoch: 8.4 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6256550064287094		[learning rate: 0.0001092]
		[batch 20/20] avg loss: 1.596026742077767		[learning rate: 0.00010901]
	Learning Rate: 0.000109005
	LOSS [training: 1.6108408742532376 | validation: 2.5017422355424994]
	TIME [epoch: 8.37 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6967558863226302		[learning rate: 0.00010881]
		[batch 20/20] avg loss: 1.5175660952521866		[learning rate: 0.00010861]
	Learning Rate: 0.00010861
	LOSS [training: 1.6071609907874085 | validation: 2.5028451188789598]
	TIME [epoch: 8.37 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5881251023239318		[learning rate: 0.00010841]
		[batch 20/20] avg loss: 1.6330201031130016		[learning rate: 0.00010822]
	Learning Rate: 0.000108215
	LOSS [training: 1.6105726027184666 | validation: 2.496224658821235]
	TIME [epoch: 8.38 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5538425803100435		[learning rate: 0.00010802]
		[batch 20/20] avg loss: 1.6574123079850172		[learning rate: 0.00010782]
	Learning Rate: 0.000107823
	LOSS [training: 1.6056274441475302 | validation: 2.4950475679457105]
	TIME [epoch: 8.39 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5654552102961798		[learning rate: 0.00010763]
		[batch 20/20] avg loss: 1.6510465193283284		[learning rate: 0.00010743]
	Learning Rate: 0.000107432
	LOSS [training: 1.6082508648122542 | validation: 2.4904883634478074]
	TIME [epoch: 8.38 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6238947191914377		[learning rate: 0.00010724]
		[batch 20/20] avg loss: 1.5953251121310685		[learning rate: 0.00010704]
	Learning Rate: 0.000107042
	LOSS [training: 1.609609915661253 | validation: 2.5029718145938604]
	TIME [epoch: 8.38 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6091956313300848		[learning rate: 0.00010685]
		[batch 20/20] avg loss: 1.6050767502210044		[learning rate: 0.00010665]
	Learning Rate: 0.000106653
	LOSS [training: 1.6071361907755446 | validation: 2.4982516956261915]
	TIME [epoch: 8.37 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5476507603359098		[learning rate: 0.00010646]
		[batch 20/20] avg loss: 1.683382953245518		[learning rate: 0.00010627]
	Learning Rate: 0.000106266
	LOSS [training: 1.6155168567907139 | validation: 2.49832106393969]
	TIME [epoch: 8.39 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7492311700310672		[learning rate: 0.00010607]
		[batch 20/20] avg loss: 1.4672988245963607		[learning rate: 0.00010588]
	Learning Rate: 0.00010588
	LOSS [training: 1.608264997313714 | validation: 2.4978895833645045]
	TIME [epoch: 8.37 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6535445520995755		[learning rate: 0.00010569]
		[batch 20/20] avg loss: 1.574193091730709		[learning rate: 0.0001055]
	Learning Rate: 0.000105496
	LOSS [training: 1.6138688219151418 | validation: 2.5135582813384567]
	TIME [epoch: 8.37 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5518127870962004		[learning rate: 0.0001053]
		[batch 20/20] avg loss: 1.6759182300578435		[learning rate: 0.00010511]
	Learning Rate: 0.000105113
	LOSS [training: 1.6138655085770217 | validation: 2.4966635107017483]
	TIME [epoch: 8.37 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.535907799990702		[learning rate: 0.00010492]
		[batch 20/20] avg loss: 1.6798299959809964		[learning rate: 0.00010473]
	Learning Rate: 0.000104732
	LOSS [training: 1.6078688979858495 | validation: 2.496438642328412]
	TIME [epoch: 8.37 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.543236966138435		[learning rate: 0.00010454]
		[batch 20/20] avg loss: 1.672938738699282		[learning rate: 0.00010435]
	Learning Rate: 0.000104352
	LOSS [training: 1.6080878524188589 | validation: 2.499715067921284]
	TIME [epoch: 8.4 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5497971272239812		[learning rate: 0.00010416]
		[batch 20/20] avg loss: 1.6636916361508216		[learning rate: 0.00010397]
	Learning Rate: 0.000103973
	LOSS [training: 1.6067443816874014 | validation: 2.491709581072247]
	TIME [epoch: 8.37 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6358426488874724		[learning rate: 0.00010378]
		[batch 20/20] avg loss: 1.5821641672101456		[learning rate: 0.0001036]
	Learning Rate: 0.000103596
	LOSS [training: 1.6090034080488091 | validation: 2.4971842186062263]
	TIME [epoch: 8.37 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6069139680932842		[learning rate: 0.00010341]
		[batch 20/20] avg loss: 1.6111794718519046		[learning rate: 0.00010322]
	Learning Rate: 0.00010322
	LOSS [training: 1.6090467199725942 | validation: 2.4957133881137463]
	TIME [epoch: 8.37 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6613958072218		[learning rate: 0.00010303]
		[batch 20/20] avg loss: 1.5575032141657446		[learning rate: 0.00010285]
	Learning Rate: 0.000102845
	LOSS [training: 1.6094495106937725 | validation: 2.498269413581112]
	TIME [epoch: 8.39 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6089695184058848		[learning rate: 0.00010266]
		[batch 20/20] avg loss: 1.608072657782476		[learning rate: 0.00010247]
	Learning Rate: 0.000102472
	LOSS [training: 1.60852108809418 | validation: 2.503627415447292]
	TIME [epoch: 8.37 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4611262727881806		[learning rate: 0.00010229]
		[batch 20/20] avg loss: 1.759531176336139		[learning rate: 0.0001021]
	Learning Rate: 0.0001021
	LOSS [training: 1.6103287245621598 | validation: 2.496208059790271]
	TIME [epoch: 8.37 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7373311196534302		[learning rate: 0.00010191]
		[batch 20/20] avg loss: 1.4793802422498152		[learning rate: 0.00010173]
	Learning Rate: 0.00010173
	LOSS [training: 1.6083556809516224 | validation: 2.4969159742481293]
	TIME [epoch: 8.38 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5191149320037183		[learning rate: 0.00010154]
		[batch 20/20] avg loss: 1.695944003240698		[learning rate: 0.00010136]
	Learning Rate: 0.00010136
	LOSS [training: 1.6075294676222085 | validation: 2.495431750360091]
	TIME [epoch: 8.39 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6281880342711634		[learning rate: 0.00010118]
		[batch 20/20] avg loss: 1.5902895718664534		[learning rate: 0.00010099]
	Learning Rate: 0.000100993
	LOSS [training: 1.6092388030688085 | validation: 2.49822889579202]
	TIME [epoch: 8.37 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6366758095890777		[learning rate: 0.00010081]
		[batch 20/20] avg loss: 1.5816528053558396		[learning rate: 0.00010063]
	Learning Rate: 0.000100626
	LOSS [training: 1.6091643074724586 | validation: 2.4945274835123614]
	TIME [epoch: 8.37 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5653235395145182		[learning rate: 0.00010044]
		[batch 20/20] avg loss: 1.6513481584084961		[learning rate: 0.00010026]
	Learning Rate: 0.000100261
	LOSS [training: 1.6083358489615072 | validation: 2.4981919632254193]
	TIME [epoch: 8.37 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7223689966105766		[learning rate: 0.00010008]
		[batch 20/20] avg loss: 1.494307712380279		[learning rate: 9.9897e-05]
	Learning Rate: 9.98971e-05
	LOSS [training: 1.608338354495428 | validation: 2.4988374408053495]
	TIME [epoch: 8.37 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6533078320879153		[learning rate: 9.9716e-05]
		[batch 20/20] avg loss: 1.5636176132785717		[learning rate: 9.9535e-05]
	Learning Rate: 9.95345e-05
	LOSS [training: 1.6084627226832435 | validation: 2.49306370712947]
	TIME [epoch: 8.39 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6265046531507985		[learning rate: 9.9354e-05]
		[batch 20/20] avg loss: 1.5811024262290663		[learning rate: 9.9173e-05]
	Learning Rate: 9.91733e-05
	LOSS [training: 1.6038035396899322 | validation: 2.496018817987032]
	TIME [epoch: 8.37 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5127977509637245		[learning rate: 9.8993e-05]
		[batch 20/20] avg loss: 1.6958513072829136		[learning rate: 9.8813e-05]
	Learning Rate: 9.88134e-05
	LOSS [training: 1.6043245291233184 | validation: 2.4969384542127298]
	TIME [epoch: 8.38 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6816999169239497		[learning rate: 9.8634e-05]
		[batch 20/20] avg loss: 1.5433339623129325		[learning rate: 9.8455e-05]
	Learning Rate: 9.84548e-05
	LOSS [training: 1.6125169396184411 | validation: 2.4932919388224017]
	TIME [epoch: 8.36 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5220446701361285		[learning rate: 9.8276e-05]
		[batch 20/20] avg loss: 1.6925279694014286		[learning rate: 9.8098e-05]
	Learning Rate: 9.80975e-05
	LOSS [training: 1.6072863197687788 | validation: 2.4949464325614583]
	TIME [epoch: 8.39 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6116553442596657		[learning rate: 9.7919e-05]
		[batch 20/20] avg loss: 1.6066351214293504		[learning rate: 9.7742e-05]
	Learning Rate: 9.77415e-05
	LOSS [training: 1.6091452328445084 | validation: 2.4935247360317976]
	TIME [epoch: 8.37 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.628913972838071		[learning rate: 9.7564e-05]
		[batch 20/20] avg loss: 1.5854579756515512		[learning rate: 9.7387e-05]
	Learning Rate: 9.73868e-05
	LOSS [training: 1.607185974244811 | validation: 2.4999068367288526]
	TIME [epoch: 8.38 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4494615188612654		[learning rate: 9.721e-05]
		[batch 20/20] avg loss: 1.7686641450033505		[learning rate: 9.7033e-05]
	Learning Rate: 9.70334e-05
	LOSS [training: 1.6090628319323081 | validation: 2.4967442452637303]
	TIME [epoch: 8.37 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6589530475797578		[learning rate: 9.6857e-05]
		[batch 20/20] avg loss: 1.5526464243217466		[learning rate: 9.6681e-05]
	Learning Rate: 9.66812e-05
	LOSS [training: 1.6057997359507525 | validation: 2.500711414359803]
	TIME [epoch: 8.4 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7188370320682005		[learning rate: 9.6506e-05]
		[batch 20/20] avg loss: 1.5007685263435795		[learning rate: 9.633e-05]
	Learning Rate: 9.63304e-05
	LOSS [training: 1.60980277920589 | validation: 2.4936489968840743]
	TIME [epoch: 8.38 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.649851691939028		[learning rate: 9.6155e-05]
		[batch 20/20] avg loss: 1.5662976503292827		[learning rate: 9.5981e-05]
	Learning Rate: 9.59808e-05
	LOSS [training: 1.6080746711341554 | validation: 2.498177530041801]
	TIME [epoch: 8.37 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5004503354336503		[learning rate: 9.5806e-05]
		[batch 20/20] avg loss: 1.710989297578757		[learning rate: 9.5632e-05]
	Learning Rate: 9.56324e-05
	LOSS [training: 1.6057198165062037 | validation: 2.503838985199784]
	TIME [epoch: 8.38 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5158340699438642		[learning rate: 9.5459e-05]
		[batch 20/20] avg loss: 1.6938290728398182		[learning rate: 9.5285e-05]
	Learning Rate: 9.52854e-05
	LOSS [training: 1.6048315713918413 | validation: 2.4994554824838517]
	TIME [epoch: 8.39 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.540911453499117		[learning rate: 9.5112e-05]
		[batch 20/20] avg loss: 1.6717191269090794		[learning rate: 9.494e-05]
	Learning Rate: 9.49396e-05
	LOSS [training: 1.606315290204098 | validation: 2.5040022671453204]
	TIME [epoch: 8.39 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5345392411174656		[learning rate: 9.4767e-05]
		[batch 20/20] avg loss: 1.6833745961266355		[learning rate: 9.4595e-05]
	Learning Rate: 9.45951e-05
	LOSS [training: 1.608956918622051 | validation: 2.509030198986389]
	TIME [epoch: 8.38 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4803606041784703		[learning rate: 9.4423e-05]
		[batch 20/20] avg loss: 1.7314807552440605		[learning rate: 9.4252e-05]
	Learning Rate: 9.42518e-05
	LOSS [training: 1.6059206797112655 | validation: 2.4933507831031796]
	TIME [epoch: 8.38 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5665368452768507		[learning rate: 9.4081e-05]
		[batch 20/20] avg loss: 1.6480387016955897		[learning rate: 9.391e-05]
	Learning Rate: 9.39097e-05
	LOSS [training: 1.6072877734862203 | validation: 2.4980621787996236]
	TIME [epoch: 8.37 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7555065973112096		[learning rate: 9.3739e-05]
		[batch 20/20] avg loss: 1.4627627283380489		[learning rate: 9.3569e-05]
	Learning Rate: 9.35689e-05
	LOSS [training: 1.6091346628246292 | validation: 2.4909823411438112]
	TIME [epoch: 8.39 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5541125405469602		[learning rate: 9.3399e-05]
		[batch 20/20] avg loss: 1.6614725945277247		[learning rate: 9.3229e-05]
	Learning Rate: 9.32294e-05
	LOSS [training: 1.6077925675373428 | validation: 2.4956471540204466]
	TIME [epoch: 8.38 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5194206530572598		[learning rate: 9.306e-05]
		[batch 20/20] avg loss: 1.691372524060163		[learning rate: 9.2891e-05]
	Learning Rate: 9.2891e-05
	LOSS [training: 1.6053965885587111 | validation: 2.4963084499451638]
	TIME [epoch: 8.37 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7405604509897734		[learning rate: 9.2722e-05]
		[batch 20/20] avg loss: 1.4713098178478774		[learning rate: 9.2554e-05]
	Learning Rate: 9.25539e-05
	LOSS [training: 1.605935134418825 | validation: 2.5047400105445727]
	TIME [epoch: 8.38 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5022405687934988		[learning rate: 9.2386e-05]
		[batch 20/20] avg loss: 1.7205154024515366		[learning rate: 9.2218e-05]
	Learning Rate: 9.2218e-05
	LOSS [training: 1.6113779856225174 | validation: 2.5078470409385494]
	TIME [epoch: 8.4 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.634881232429849		[learning rate: 9.2051e-05]
		[batch 20/20] avg loss: 1.5829851695455674		[learning rate: 9.1883e-05]
	Learning Rate: 9.18834e-05
	LOSS [training: 1.6089332009877082 | validation: 2.49156573064206]
	TIME [epoch: 8.38 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5391359741531883		[learning rate: 9.1716e-05]
		[batch 20/20] avg loss: 1.6761017569792975		[learning rate: 9.155e-05]
	Learning Rate: 9.15499e-05
	LOSS [training: 1.6076188655662427 | validation: 2.507489517383689]
	TIME [epoch: 8.38 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.702420734301479		[learning rate: 9.1384e-05]
		[batch 20/20] avg loss: 1.5155547443573174		[learning rate: 9.1218e-05]
	Learning Rate: 9.12177e-05
	LOSS [training: 1.608987739329398 | validation: 2.4954359739201544]
	TIME [epoch: 8.38 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.787766291374183		[learning rate: 9.1052e-05]
		[batch 20/20] avg loss: 1.4374024139346921		[learning rate: 9.0887e-05]
	Learning Rate: 9.08866e-05
	LOSS [training: 1.6125843526544377 | validation: 2.5030279177745283]
	TIME [epoch: 8.39 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6288308289635018		[learning rate: 9.0722e-05]
		[batch 20/20] avg loss: 1.5966061041079582		[learning rate: 9.0557e-05]
	Learning Rate: 9.05568e-05
	LOSS [training: 1.6127184665357306 | validation: 2.4941688242383506]
	TIME [epoch: 8.39 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6304016607701262		[learning rate: 9.0392e-05]
		[batch 20/20] avg loss: 1.5936312885196768		[learning rate: 9.0228e-05]
	Learning Rate: 9.02281e-05
	LOSS [training: 1.6120164746449013 | validation: 2.499637132739803]
	TIME [epoch: 8.38 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6904962535137678		[learning rate: 9.0064e-05]
		[batch 20/20] avg loss: 1.5246722392602396		[learning rate: 8.9901e-05]
	Learning Rate: 8.99007e-05
	LOSS [training: 1.6075842463870038 | validation: 2.4971783723771326]
	TIME [epoch: 8.38 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6167478787697582		[learning rate: 8.9737e-05]
		[batch 20/20] avg loss: 1.5880237131729702		[learning rate: 8.9574e-05]
	Learning Rate: 8.95745e-05
	LOSS [training: 1.6023857959713645 | validation: 2.501030223911463]
	TIME [epoch: 8.39 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7290113671821281		[learning rate: 8.9412e-05]
		[batch 20/20] avg loss: 1.4865396584924644		[learning rate: 8.9249e-05]
	Learning Rate: 8.92494e-05
	LOSS [training: 1.6077755128372968 | validation: 2.4992918079995183]
	TIME [epoch: 8.4 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6142516841266477		[learning rate: 8.9087e-05]
		[batch 20/20] avg loss: 1.6063311183428257		[learning rate: 8.8926e-05]
	Learning Rate: 8.89255e-05
	LOSS [training: 1.6102914012347367 | validation: 2.512894415260238]
	TIME [epoch: 8.37 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6626102057571335		[learning rate: 8.8764e-05]
		[batch 20/20] avg loss: 1.5528407681759535		[learning rate: 8.8603e-05]
	Learning Rate: 8.86028e-05
	LOSS [training: 1.6077254869665434 | validation: 2.4927505457661963]
	TIME [epoch: 8.38 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7476666988848055		[learning rate: 8.8442e-05]
		[batch 20/20] avg loss: 1.4614065609091742		[learning rate: 8.8281e-05]
	Learning Rate: 8.82813e-05
	LOSS [training: 1.6045366298969896 | validation: 2.499480287166101]
	TIME [epoch: 8.37 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7198599529530896		[learning rate: 8.8121e-05]
		[batch 20/20] avg loss: 1.5000310513278365		[learning rate: 8.7961e-05]
	Learning Rate: 8.79609e-05
	LOSS [training: 1.609945502140463 | validation: 2.4970346661217424]
	TIME [epoch: 8.4 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6654979782199846		[learning rate: 8.7801e-05]
		[batch 20/20] avg loss: 1.5481815396513754		[learning rate: 8.7642e-05]
	Learning Rate: 8.76417e-05
	LOSS [training: 1.6068397589356807 | validation: 2.502957834406507]
	TIME [epoch: 8.38 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5980968837708063		[learning rate: 8.7482e-05]
		[batch 20/20] avg loss: 1.6210366282163822		[learning rate: 8.7324e-05]
	Learning Rate: 8.73236e-05
	LOSS [training: 1.609566755993594 | validation: 2.4985354886694378]
	TIME [epoch: 8.38 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5918174054616157		[learning rate: 8.7165e-05]
		[batch 20/20] avg loss: 1.6300698349900742		[learning rate: 8.7007e-05]
	Learning Rate: 8.70067e-05
	LOSS [training: 1.6109436202258451 | validation: 2.494048302682364]
	TIME [epoch: 8.37 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5095416750526636		[learning rate: 8.6849e-05]
		[batch 20/20] avg loss: 1.7139166758794935		[learning rate: 8.6691e-05]
	Learning Rate: 8.66909e-05
	LOSS [training: 1.6117291754660783 | validation: 2.496742574443269]
	TIME [epoch: 8.4 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5695472315440115		[learning rate: 8.6533e-05]
		[batch 20/20] avg loss: 1.6429355745722787		[learning rate: 8.6376e-05]
	Learning Rate: 8.63763e-05
	LOSS [training: 1.6062414030581453 | validation: 2.4931712073207457]
	TIME [epoch: 8.38 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.630812327197226		[learning rate: 8.6219e-05]
		[batch 20/20] avg loss: 1.5798936654376967		[learning rate: 8.6063e-05]
	Learning Rate: 8.60629e-05
	LOSS [training: 1.6053529963174615 | validation: 2.494442029747316]
	TIME [epoch: 8.37 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5322250346295563		[learning rate: 8.5907e-05]
		[batch 20/20] avg loss: 1.6768868600412503		[learning rate: 8.5751e-05]
	Learning Rate: 8.57505e-05
	LOSS [training: 1.6045559473354032 | validation: 2.50102768742197]
	TIME [epoch: 8.37 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6636124557101009		[learning rate: 8.5595e-05]
		[batch 20/20] avg loss: 1.5485590901929283		[learning rate: 8.5439e-05]
	Learning Rate: 8.54394e-05
	LOSS [training: 1.6060857729515141 | validation: 2.4961911728143935]
	TIME [epoch: 8.38 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.793809577720792		[learning rate: 8.5284e-05]
		[batch 20/20] avg loss: 1.428668792062996		[learning rate: 8.5129e-05]
	Learning Rate: 8.51293e-05
	LOSS [training: 1.6112391848918939 | validation: 2.5012176302261597]
	TIME [epoch: 8.4 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.764687203108624		[learning rate: 8.4975e-05]
		[batch 20/20] avg loss: 1.4508607087194691		[learning rate: 8.482e-05]
	Learning Rate: 8.48204e-05
	LOSS [training: 1.607773955914047 | validation: 2.5056880666427634]
	TIME [epoch: 8.37 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6413884678914568		[learning rate: 8.4666e-05]
		[batch 20/20] avg loss: 1.5774322250905088		[learning rate: 8.4513e-05]
	Learning Rate: 8.45125e-05
	LOSS [training: 1.6094103464909826 | validation: 2.503101234726671]
	TIME [epoch: 8.38 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.50822242482495		[learning rate: 8.4359e-05]
		[batch 20/20] avg loss: 1.7029697423537882		[learning rate: 8.4206e-05]
	Learning Rate: 8.42058e-05
	LOSS [training: 1.605596083589369 | validation: 2.4987954740043787]
	TIME [epoch: 8.37 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.501294591299827		[learning rate: 8.4053e-05]
		[batch 20/20] avg loss: 1.7139610964993384		[learning rate: 8.39e-05]
	Learning Rate: 8.39002e-05
	LOSS [training: 1.6076278438995828 | validation: 2.499794064275952]
	TIME [epoch: 8.39 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6022329718368415		[learning rate: 8.3748e-05]
		[batch 20/20] avg loss: 1.6068342964189182		[learning rate: 8.3596e-05]
	Learning Rate: 8.35958e-05
	LOSS [training: 1.6045336341278795 | validation: 2.504875769555463]
	TIME [epoch: 8.37 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5277409899554235		[learning rate: 8.3444e-05]
		[batch 20/20] avg loss: 1.6909757312632425		[learning rate: 8.3292e-05]
	Learning Rate: 8.32924e-05
	LOSS [training: 1.609358360609333 | validation: 2.5100571185652294]
	TIME [epoch: 8.38 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4027887572154014		[learning rate: 8.3141e-05]
		[batch 20/20] avg loss: 1.8112931961553849		[learning rate: 8.299e-05]
	Learning Rate: 8.29901e-05
	LOSS [training: 1.6070409766853935 | validation: 2.4979149066576802]
	TIME [epoch: 8.38 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.564268570136043		[learning rate: 8.2839e-05]
		[batch 20/20] avg loss: 1.6468876288311645		[learning rate: 8.2689e-05]
	Learning Rate: 8.26889e-05
	LOSS [training: 1.6055780994836037 | validation: 2.5022689814706487]
	TIME [epoch: 8.39 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6423433030244226		[learning rate: 8.2539e-05]
		[batch 20/20] avg loss: 1.575049451923809		[learning rate: 8.2389e-05]
	Learning Rate: 8.23889e-05
	LOSS [training: 1.6086963774741156 | validation: 2.4969330936905028]
	TIME [epoch: 8.38 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4782364325805712		[learning rate: 8.2239e-05]
		[batch 20/20] avg loss: 1.7374944264023096		[learning rate: 8.209e-05]
	Learning Rate: 8.20899e-05
	LOSS [training: 1.6078654294914405 | validation: 2.496890319428791]
	TIME [epoch: 8.38 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.627004852099421		[learning rate: 8.1941e-05]
		[batch 20/20] avg loss: 1.59069844207097		[learning rate: 8.1792e-05]
	Learning Rate: 8.17919e-05
	LOSS [training: 1.6088516470851957 | validation: 2.5031984347508747]
	TIME [epoch: 8.38 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.482460068553358		[learning rate: 8.1643e-05]
		[batch 20/20] avg loss: 1.7321371033868882		[learning rate: 8.1495e-05]
	Learning Rate: 8.14951e-05
	LOSS [training: 1.607298585970123 | validation: 2.4998925550197395]
	TIME [epoch: 8.37 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6730094835237481		[learning rate: 8.1347e-05]
		[batch 20/20] avg loss: 1.5413454272414155		[learning rate: 8.1199e-05]
	Learning Rate: 8.11994e-05
	LOSS [training: 1.607177455382582 | validation: 2.4957640617597763]
	TIME [epoch: 8.4 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5982672032775622		[learning rate: 8.1052e-05]
		[batch 20/20] avg loss: 1.6234619366549434		[learning rate: 8.0905e-05]
	Learning Rate: 8.09047e-05
	LOSS [training: 1.6108645699662527 | validation: 2.492613783905929]
	TIME [epoch: 8.38 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5865248698923693		[learning rate: 8.0758e-05]
		[batch 20/20] avg loss: 1.6301127238513058		[learning rate: 8.0611e-05]
	Learning Rate: 8.06111e-05
	LOSS [training: 1.6083187968718378 | validation: 2.4990420577684613]
	TIME [epoch: 8.38 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.635386836563967		[learning rate: 8.0465e-05]
		[batch 20/20] avg loss: 1.573402433686128		[learning rate: 8.0319e-05]
	Learning Rate: 8.03186e-05
	LOSS [training: 1.604394635125048 | validation: 2.4939612038683228]
	TIME [epoch: 8.37 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6377352417765554		[learning rate: 8.0173e-05]
		[batch 20/20] avg loss: 1.5788676310131806		[learning rate: 8.0027e-05]
	Learning Rate: 8.00271e-05
	LOSS [training: 1.6083014363948682 | validation: 2.4929493663798756]
	TIME [epoch: 8.39 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7224310700126282		[learning rate: 7.9882e-05]
		[batch 20/20] avg loss: 1.4976241518267057		[learning rate: 7.9737e-05]
	Learning Rate: 7.97367e-05
	LOSS [training: 1.610027610919667 | validation: 2.497577105506703]
	TIME [epoch: 8.38 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5152424183465618		[learning rate: 7.9592e-05]
		[batch 20/20] avg loss: 1.7030591680770477		[learning rate: 7.9447e-05]
	Learning Rate: 7.94473e-05
	LOSS [training: 1.6091507932118048 | validation: 2.492838210451243]
	TIME [epoch: 8.38 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.69048355695137		[learning rate: 7.9303e-05]
		[batch 20/20] avg loss: 1.5245004025261566		[learning rate: 7.9159e-05]
	Learning Rate: 7.91589e-05
	LOSS [training: 1.607491979738763 | validation: 2.501938788654631]
	TIME [epoch: 8.37 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.654117665826707		[learning rate: 7.9015e-05]
		[batch 20/20] avg loss: 1.5652024627178158		[learning rate: 7.8872e-05]
	Learning Rate: 7.88717e-05
	LOSS [training: 1.6096600642722616 | validation: 2.516786810814813]
	TIME [epoch: 8.39 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5957670662716597		[learning rate: 7.8728e-05]
		[batch 20/20] avg loss: 1.6230470375016677		[learning rate: 7.8585e-05]
	Learning Rate: 7.85854e-05
	LOSS [training: 1.6094070518866634 | validation: 2.5018089311346148]
	TIME [epoch: 8.37 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6455203113410568		[learning rate: 7.8443e-05]
		[batch 20/20] avg loss: 1.5706208905317698		[learning rate: 7.83e-05]
	Learning Rate: 7.83003e-05
	LOSS [training: 1.6080706009364132 | validation: 2.4953070636538865]
	TIME [epoch: 8.38 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7443769809394336		[learning rate: 7.8158e-05]
		[batch 20/20] avg loss: 1.4695633075260368		[learning rate: 7.8016e-05]
	Learning Rate: 7.80161e-05
	LOSS [training: 1.606970144232735 | validation: 2.496317007966653]
	TIME [epoch: 8.38 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6004430133764718		[learning rate: 7.7874e-05]
		[batch 20/20] avg loss: 1.6150125226147138		[learning rate: 7.7733e-05]
	Learning Rate: 7.7733e-05
	LOSS [training: 1.607727767995593 | validation: 2.4974860885868786]
	TIME [epoch: 8.38 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.801446985409801		[learning rate: 7.7592e-05]
		[batch 20/20] avg loss: 1.4140288763803153		[learning rate: 7.7451e-05]
	Learning Rate: 7.74509e-05
	LOSS [training: 1.607737930895058 | validation: 2.499446938229168]
	TIME [epoch: 8.39 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.66622065977622		[learning rate: 7.731e-05]
		[batch 20/20] avg loss: 1.55077042932428		[learning rate: 7.717e-05]
	Learning Rate: 7.71698e-05
	LOSS [training: 1.6084955445502502 | validation: 2.4936982096233002]
	TIME [epoch: 8.37 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5735965327936754		[learning rate: 7.703e-05]
		[batch 20/20] avg loss: 1.641861682716136		[learning rate: 7.689e-05]
	Learning Rate: 7.68897e-05
	LOSS [training: 1.6077291077549056 | validation: 2.4913275051462946]
	TIME [epoch: 8.37 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7313355837026094		[learning rate: 7.675e-05]
		[batch 20/20] avg loss: 1.486775872367333		[learning rate: 7.6611e-05]
	Learning Rate: 7.66107e-05
	LOSS [training: 1.609055728034971 | validation: 2.497619665479339]
	TIME [epoch: 8.37 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7012464910152094		[learning rate: 7.6472e-05]
		[batch 20/20] avg loss: 1.524747847637029		[learning rate: 7.6333e-05]
	Learning Rate: 7.63327e-05
	LOSS [training: 1.6129971693261194 | validation: 2.500042627765469]
	TIME [epoch: 8.39 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5175459290177904		[learning rate: 7.6194e-05]
		[batch 20/20] avg loss: 1.7011395222704295		[learning rate: 7.6056e-05]
	Learning Rate: 7.60557e-05
	LOSS [training: 1.6093427256441095 | validation: 2.499784725160788]
	TIME [epoch: 8.38 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5287760114498306		[learning rate: 7.5918e-05]
		[batch 20/20] avg loss: 1.6837075560668022		[learning rate: 7.578e-05]
	Learning Rate: 7.57797e-05
	LOSS [training: 1.6062417837583165 | validation: 2.494885450004496]
	TIME [epoch: 8.38 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6004325220193203		[learning rate: 7.5642e-05]
		[batch 20/20] avg loss: 1.6194790511815698		[learning rate: 7.5505e-05]
	Learning Rate: 7.55047e-05
	LOSS [training: 1.6099557866004452 | validation: 2.4965127697698097]
	TIME [epoch: 8.37 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5617479006512922		[learning rate: 7.5368e-05]
		[batch 20/20] avg loss: 1.648651476111351		[learning rate: 7.5231e-05]
	Learning Rate: 7.52307e-05
	LOSS [training: 1.6051996883813218 | validation: 2.49698164891543]
	TIME [epoch: 8.39 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5429303654546183		[learning rate: 7.5094e-05]
		[batch 20/20] avg loss: 1.6716337453107524		[learning rate: 7.4958e-05]
	Learning Rate: 7.49576e-05
	LOSS [training: 1.6072820553826854 | validation: 2.501590739315634]
	TIME [epoch: 8.38 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6631289741207411		[learning rate: 7.4822e-05]
		[batch 20/20] avg loss: 1.5488144493789486		[learning rate: 7.4686e-05]
	Learning Rate: 7.46856e-05
	LOSS [training: 1.6059717117498447 | validation: 2.496615700977674]
	TIME [epoch: 8.37 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.778302960292266		[learning rate: 7.455e-05]
		[batch 20/20] avg loss: 1.4400789286984392		[learning rate: 7.4415e-05]
	Learning Rate: 7.44146e-05
	LOSS [training: 1.609190944495353 | validation: 2.495266029574145]
	TIME [epoch: 8.37 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7212729554678838		[learning rate: 7.4279e-05]
		[batch 20/20] avg loss: 1.4909173834225453		[learning rate: 7.4144e-05]
	Learning Rate: 7.41445e-05
	LOSS [training: 1.6060951694452146 | validation: 2.4915993814575415]
	TIME [epoch: 8.38 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.584110447363909		[learning rate: 7.401e-05]
		[batch 20/20] avg loss: 1.6230306502641263		[learning rate: 7.3875e-05]
	Learning Rate: 7.38754e-05
	LOSS [training: 1.6035705488140177 | validation: 2.495385461281945]
	TIME [epoch: 8.38 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.56346244433553		[learning rate: 7.3741e-05]
		[batch 20/20] avg loss: 1.648019133094106		[learning rate: 7.3607e-05]
	Learning Rate: 7.36073e-05
	LOSS [training: 1.605740788714818 | validation: 2.502007782301643]
	TIME [epoch: 8.37 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5907426672757516		[learning rate: 7.3474e-05]
		[batch 20/20] avg loss: 1.6327007590489857		[learning rate: 7.334e-05]
	Learning Rate: 7.33402e-05
	LOSS [training: 1.6117217131623687 | validation: 2.5038913306343447]
	TIME [epoch: 8.38 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6578853328685317		[learning rate: 7.3207e-05]
		[batch 20/20] avg loss: 1.559005411158885		[learning rate: 7.3074e-05]
	Learning Rate: 7.30741e-05
	LOSS [training: 1.608445372013708 | validation: 2.501707425690767]
	TIME [epoch: 8.37 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6389540510224179		[learning rate: 7.2941e-05]
		[batch 20/20] avg loss: 1.575042237579621		[learning rate: 7.2809e-05]
	Learning Rate: 7.28089e-05
	LOSS [training: 1.6069981443010195 | validation: 2.494200820380316]
	TIME [epoch: 8.39 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5581768161900542		[learning rate: 7.2677e-05]
		[batch 20/20] avg loss: 1.6549359082188402		[learning rate: 7.2545e-05]
	Learning Rate: 7.25447e-05
	LOSS [training: 1.6065563622044468 | validation: 2.497366190070937]
	TIME [epoch: 8.37 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8111113204241391		[learning rate: 7.2413e-05]
		[batch 20/20] avg loss: 1.4017098819575196		[learning rate: 7.2281e-05]
	Learning Rate: 7.22814e-05
	LOSS [training: 1.6064106011908295 | validation: 2.4953989663018765]
	TIME [epoch: 8.38 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.584534050211608		[learning rate: 7.215e-05]
		[batch 20/20] avg loss: 1.6279898591280326		[learning rate: 7.2019e-05]
	Learning Rate: 7.2019e-05
	LOSS [training: 1.60626195466982 | validation: 2.495407427415939]
	TIME [epoch: 8.37 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.527250346281929		[learning rate: 7.1888e-05]
		[batch 20/20] avg loss: 1.6869889712293848		[learning rate: 7.1758e-05]
	Learning Rate: 7.17577e-05
	LOSS [training: 1.6071196587556567 | validation: 2.4987808833201592]
	TIME [epoch: 8.39 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4509352024014306		[learning rate: 7.1627e-05]
		[batch 20/20] avg loss: 1.765512188637332		[learning rate: 7.1497e-05]
	Learning Rate: 7.14973e-05
	LOSS [training: 1.608223695519381 | validation: 2.4931047833886577]
	TIME [epoch: 8.38 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6028623373591178		[learning rate: 7.1367e-05]
		[batch 20/20] avg loss: 1.6108645651254974		[learning rate: 7.1238e-05]
	Learning Rate: 7.12378e-05
	LOSS [training: 1.6068634512423077 | validation: 2.4943799575848824]
	TIME [epoch: 8.37 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5526781681876347		[learning rate: 7.1108e-05]
		[batch 20/20] avg loss: 1.6574502165307716		[learning rate: 7.0979e-05]
	Learning Rate: 7.09793e-05
	LOSS [training: 1.6050641923592028 | validation: 2.4961029197299096]
	TIME [epoch: 8.37 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6778399774304298		[learning rate: 7.085e-05]
		[batch 20/20] avg loss: 1.5362784049837048		[learning rate: 7.0722e-05]
	Learning Rate: 7.07217e-05
	LOSS [training: 1.607059191207067 | validation: 2.494740898914881]
	TIME [epoch: 8.38 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6716548894237284		[learning rate: 7.0593e-05]
		[batch 20/20] avg loss: 1.536567718558117		[learning rate: 7.0465e-05]
	Learning Rate: 7.0465e-05
	LOSS [training: 1.6041113039909227 | validation: 2.4966186007222544]
	TIME [epoch: 8.39 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.568962442925716		[learning rate: 7.0337e-05]
		[batch 20/20] avg loss: 1.6495301304223502		[learning rate: 7.0209e-05]
	Learning Rate: 7.02093e-05
	LOSS [training: 1.6092462866740334 | validation: 2.499205953462637]
	TIME [epoch: 8.37 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6649185092857368		[learning rate: 7.0082e-05]
		[batch 20/20] avg loss: 1.5472857807185594		[learning rate: 6.9955e-05]
	Learning Rate: 6.99545e-05
	LOSS [training: 1.6061021450021482 | validation: 2.489834565531636]
	TIME [epoch: 8.38 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.588892572912354		[learning rate: 6.9827e-05]
		[batch 20/20] avg loss: 1.6236773682918262		[learning rate: 6.9701e-05]
	Learning Rate: 6.97006e-05
	LOSS [training: 1.6062849706020903 | validation: 2.5010466446656134]
	TIME [epoch: 8.38 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6124107199464834		[learning rate: 6.9574e-05]
		[batch 20/20] avg loss: 1.5997934040494841		[learning rate: 6.9448e-05]
	Learning Rate: 6.94477e-05
	LOSS [training: 1.6061020619979836 | validation: 2.494678690704154]
	TIME [epoch: 8.4 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5822954508882432		[learning rate: 6.9322e-05]
		[batch 20/20] avg loss: 1.6304783846591746		[learning rate: 6.9196e-05]
	Learning Rate: 6.91957e-05
	LOSS [training: 1.606386917773709 | validation: 2.5005597817777123]
	TIME [epoch: 8.37 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6611400381105366		[learning rate: 6.907e-05]
		[batch 20/20] avg loss: 1.5499492905847545		[learning rate: 6.8945e-05]
	Learning Rate: 6.89446e-05
	LOSS [training: 1.6055446643476459 | validation: 2.4975812085301095]
	TIME [epoch: 8.37 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4918647563794136		[learning rate: 6.8819e-05]
		[batch 20/20] avg loss: 1.7198484585598348		[learning rate: 6.8694e-05]
	Learning Rate: 6.86944e-05
	LOSS [training: 1.605856607469624 | validation: 2.497521087169025]
	TIME [epoch: 8.37 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.592202470301633		[learning rate: 6.857e-05]
		[batch 20/20] avg loss: 1.6192293805120337		[learning rate: 6.8445e-05]
	Learning Rate: 6.84451e-05
	LOSS [training: 1.6057159254068334 | validation: 2.500835805862363]
	TIME [epoch: 8.39 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5429742105777318		[learning rate: 6.8321e-05]
		[batch 20/20] avg loss: 1.675061313102189		[learning rate: 6.8197e-05]
	Learning Rate: 6.81967e-05
	LOSS [training: 1.6090177618399604 | validation: 2.4996855901404134]
	TIME [epoch: 8.37 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5599485933461141		[learning rate: 6.8073e-05]
		[batch 20/20] avg loss: 1.6607929394163257		[learning rate: 6.7949e-05]
	Learning Rate: 6.79492e-05
	LOSS [training: 1.6103707663812201 | validation: 2.498374913699641]
	TIME [epoch: 8.37 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8438210461589424		[learning rate: 6.7826e-05]
		[batch 20/20] avg loss: 1.36919596399308		[learning rate: 6.7703e-05]
	Learning Rate: 6.77026e-05
	LOSS [training: 1.606508505076011 | validation: 2.4966577089858992]
	TIME [epoch: 8.37 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5511720873002681		[learning rate: 6.758e-05]
		[batch 20/20] avg loss: 1.6622801146697275		[learning rate: 6.7457e-05]
	Learning Rate: 6.74569e-05
	LOSS [training: 1.6067261009849976 | validation: 2.4950946911795695]
	TIME [epoch: 8.39 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4650895142877836		[learning rate: 6.7334e-05]
		[batch 20/20] avg loss: 1.7458304749805358		[learning rate: 6.7212e-05]
	Learning Rate: 6.72121e-05
	LOSS [training: 1.6054599946341592 | validation: 2.5000036667058634]
	TIME [epoch: 8.37 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.536361415257091		[learning rate: 6.709e-05]
		[batch 20/20] avg loss: 1.6800728290139684		[learning rate: 6.6968e-05]
	Learning Rate: 6.69682e-05
	LOSS [training: 1.6082171221355295 | validation: 2.5024839446898333]
	TIME [epoch: 8.37 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6412611742062253		[learning rate: 6.6847e-05]
		[batch 20/20] avg loss: 1.5691103436355474		[learning rate: 6.6725e-05]
	Learning Rate: 6.67251e-05
	LOSS [training: 1.6051857589208864 | validation: 2.498713459828769]
	TIME [epoch: 8.37 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7085989598164855		[learning rate: 6.6604e-05]
		[batch 20/20] avg loss: 1.511622039869		[learning rate: 6.6483e-05]
	Learning Rate: 6.6483e-05
	LOSS [training: 1.6101104998427431 | validation: 2.5037865882012427]
	TIME [epoch: 8.37 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5583383978093077		[learning rate: 6.6362e-05]
		[batch 20/20] avg loss: 1.6544521868817008		[learning rate: 6.6242e-05]
	Learning Rate: 6.62417e-05
	LOSS [training: 1.606395292345504 | validation: 2.4995157314486653]
	TIME [epoch: 8.39 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5220280831800284		[learning rate: 6.6121e-05]
		[batch 20/20] avg loss: 1.6901132991498957		[learning rate: 6.6001e-05]
	Learning Rate: 6.60013e-05
	LOSS [training: 1.606070691164962 | validation: 2.4909669624268167]
	TIME [epoch: 8.37 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7229398481975085		[learning rate: 6.5881e-05]
		[batch 20/20] avg loss: 1.487746726626535		[learning rate: 6.5762e-05]
	Learning Rate: 6.57618e-05
	LOSS [training: 1.605343287412022 | validation: 2.5024054049320603]
	TIME [epoch: 8.37 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.756622296858141		[learning rate: 6.5642e-05]
		[batch 20/20] avg loss: 1.4566093740140056		[learning rate: 6.5523e-05]
	Learning Rate: 6.55232e-05
	LOSS [training: 1.6066158354360733 | validation: 2.4981098077763333]
	TIME [epoch: 8.37 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5339859962834106		[learning rate: 6.5404e-05]
		[batch 20/20] avg loss: 1.6780982698867781		[learning rate: 6.5285e-05]
	Learning Rate: 6.52854e-05
	LOSS [training: 1.6060421330850942 | validation: 2.495957193498005]
	TIME [epoch: 8.39 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.705429850097035		[learning rate: 6.5167e-05]
		[batch 20/20] avg loss: 1.5088602814027132		[learning rate: 6.5048e-05]
	Learning Rate: 6.50484e-05
	LOSS [training: 1.607145065749874 | validation: 2.5021373851711393]
	TIME [epoch: 8.37 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6848384806792882		[learning rate: 6.493e-05]
		[batch 20/20] avg loss: 1.5265322457891193		[learning rate: 6.4812e-05]
	Learning Rate: 6.48124e-05
	LOSS [training: 1.6056853632342036 | validation: 2.4969500232192474]
	TIME [epoch: 8.37 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6087612048227569		[learning rate: 6.4695e-05]
		[batch 20/20] avg loss: 1.602835664658275		[learning rate: 6.4577e-05]
	Learning Rate: 6.45772e-05
	LOSS [training: 1.6057984347405163 | validation: 2.491312338814582]
	TIME [epoch: 8.37 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5116687909100637		[learning rate: 6.446e-05]
		[batch 20/20] avg loss: 1.6986881244318828		[learning rate: 6.4343e-05]
	Learning Rate: 6.43428e-05
	LOSS [training: 1.6051784576709731 | validation: 2.4998205560706888]
	TIME [epoch: 8.39 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.466617306032091		[learning rate: 6.4226e-05]
		[batch 20/20] avg loss: 1.7408095774203285		[learning rate: 6.4109e-05]
	Learning Rate: 6.41093e-05
	LOSS [training: 1.6037134417262098 | validation: 2.4877269382310545]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_1489.pth
	Model improved!!!
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4714710164721967		[learning rate: 6.3993e-05]
		[batch 20/20] avg loss: 1.7420835938738315		[learning rate: 6.3877e-05]
	Learning Rate: 6.38767e-05
	LOSS [training: 1.6067773051730143 | validation: 2.4925476846944776]
	TIME [epoch: 8.37 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5428697136985225		[learning rate: 6.3761e-05]
		[batch 20/20] avg loss: 1.6714126340807216		[learning rate: 6.3645e-05]
	Learning Rate: 6.36449e-05
	LOSS [training: 1.607141173889622 | validation: 2.4957390216103494]
	TIME [epoch: 8.36 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.551651635485191		[learning rate: 6.3529e-05]
		[batch 20/20] avg loss: 1.6689054827983287		[learning rate: 6.3414e-05]
	Learning Rate: 6.34139e-05
	LOSS [training: 1.6102785591417597 | validation: 2.504283783958554]
	TIME [epoch: 8.36 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5900562945164032		[learning rate: 6.3299e-05]
		[batch 20/20] avg loss: 1.6158621424183632		[learning rate: 6.3184e-05]
	Learning Rate: 6.31837e-05
	LOSS [training: 1.602959218467383 | validation: 2.5051472542186004]
	TIME [epoch: 8.38 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5742336709699778		[learning rate: 6.3069e-05]
		[batch 20/20] avg loss: 1.6356262152668901		[learning rate: 6.2954e-05]
	Learning Rate: 6.29544e-05
	LOSS [training: 1.604929943118434 | validation: 2.4936053529265183]
	TIME [epoch: 8.36 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6298528453748502		[learning rate: 6.284e-05]
		[batch 20/20] avg loss: 1.5810904752954331		[learning rate: 6.2726e-05]
	Learning Rate: 6.2726e-05
	LOSS [training: 1.6054716603351415 | validation: 2.4953491574638154]
	TIME [epoch: 8.37 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.555335237862136		[learning rate: 6.2612e-05]
		[batch 20/20] avg loss: 1.6575092640746028		[learning rate: 6.2498e-05]
	Learning Rate: 6.24983e-05
	LOSS [training: 1.6064222509683692 | validation: 2.495887350670495]
	TIME [epoch: 8.36 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.43319059427689		[learning rate: 6.2385e-05]
		[batch 20/20] avg loss: 1.7787756883356398		[learning rate: 6.2272e-05]
	Learning Rate: 6.22715e-05
	LOSS [training: 1.605983141306265 | validation: 2.4983358194331724]
	TIME [epoch: 8.39 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5504903164220383		[learning rate: 6.2158e-05]
		[batch 20/20] avg loss: 1.6628172616143957		[learning rate: 6.2046e-05]
	Learning Rate: 6.20455e-05
	LOSS [training: 1.606653789018217 | validation: 2.5064816313001077]
	TIME [epoch: 8.37 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6748761625158917		[learning rate: 6.1933e-05]
		[batch 20/20] avg loss: 1.5332105853971367		[learning rate: 6.182e-05]
	Learning Rate: 6.18204e-05
	LOSS [training: 1.604043373956514 | validation: 2.4953504722995015]
	TIME [epoch: 8.36 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6380631152395293		[learning rate: 6.1708e-05]
		[batch 20/20] avg loss: 1.5772241999756784		[learning rate: 6.1596e-05]
	Learning Rate: 6.1596e-05
	LOSS [training: 1.6076436576076039 | validation: 2.4943121395364205]
	TIME [epoch: 8.36 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4645665677685915		[learning rate: 6.1484e-05]
		[batch 20/20] avg loss: 1.7495743195892115		[learning rate: 6.1372e-05]
	Learning Rate: 6.13725e-05
	LOSS [training: 1.6070704436789014 | validation: 2.49641973656775]
	TIME [epoch: 8.38 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5332580725835456		[learning rate: 6.1261e-05]
		[batch 20/20] avg loss: 1.679912311162712		[learning rate: 6.115e-05]
	Learning Rate: 6.11498e-05
	LOSS [training: 1.6065851918731286 | validation: 2.498516071041278]
	TIME [epoch: 8.37 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.642528577225478		[learning rate: 6.1039e-05]
		[batch 20/20] avg loss: 1.5724666574402568		[learning rate: 6.0928e-05]
	Learning Rate: 6.09278e-05
	LOSS [training: 1.6074976173328674 | validation: 2.50223006218369]
	TIME [epoch: 8.37 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5397200192085365		[learning rate: 6.0817e-05]
		[batch 20/20] avg loss: 1.678173122650401		[learning rate: 6.0707e-05]
	Learning Rate: 6.07067e-05
	LOSS [training: 1.6089465709294686 | validation: 2.4981575440079213]
	TIME [epoch: 8.36 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5437642327917944		[learning rate: 6.0596e-05]
		[batch 20/20] avg loss: 1.6694243426729762		[learning rate: 6.0486e-05]
	Learning Rate: 6.04864e-05
	LOSS [training: 1.6065942877323853 | validation: 2.4949262730426573]
	TIME [epoch: 8.37 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6010069282356085		[learning rate: 6.0377e-05]
		[batch 20/20] avg loss: 1.613055339552917		[learning rate: 6.0267e-05]
	Learning Rate: 6.02669e-05
	LOSS [training: 1.6070311338942627 | validation: 2.497393463296452]
	TIME [epoch: 8.39 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6042488266884107		[learning rate: 6.0157e-05]
		[batch 20/20] avg loss: 1.6062960988479347		[learning rate: 6.0048e-05]
	Learning Rate: 6.00482e-05
	LOSS [training: 1.605272462768173 | validation: 2.4972047454001074]
	TIME [epoch: 8.37 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.762194575231589		[learning rate: 5.9939e-05]
		[batch 20/20] avg loss: 1.4488658122467122		[learning rate: 5.983e-05]
	Learning Rate: 5.98303e-05
	LOSS [training: 1.6055301937391508 | validation: 2.49947660161662]
	TIME [epoch: 8.37 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.549035805644388		[learning rate: 5.9722e-05]
		[batch 20/20] avg loss: 1.6622436563363163		[learning rate: 5.9613e-05]
	Learning Rate: 5.96132e-05
	LOSS [training: 1.605639730990352 | validation: 2.4960946801345525]
	TIME [epoch: 8.37 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4676628176371957		[learning rate: 5.9505e-05]
		[batch 20/20] avg loss: 1.7442074355562016		[learning rate: 5.9397e-05]
	Learning Rate: 5.93968e-05
	LOSS [training: 1.6059351265966988 | validation: 2.4998873519128653]
	TIME [epoch: 8.39 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5997442157789992		[learning rate: 5.9289e-05]
		[batch 20/20] avg loss: 1.6109427998055685		[learning rate: 5.9181e-05]
	Learning Rate: 5.91813e-05
	LOSS [training: 1.6053435077922837 | validation: 2.4926841426913846]
	TIME [epoch: 8.37 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8208091190332554		[learning rate: 5.9074e-05]
		[batch 20/20] avg loss: 1.3900984580815865		[learning rate: 5.8966e-05]
	Learning Rate: 5.89665e-05
	LOSS [training: 1.605453788557421 | validation: 2.4997200460810016]
	TIME [epoch: 8.36 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4778021421514185		[learning rate: 5.8859e-05]
		[batch 20/20] avg loss: 1.7316689671153624		[learning rate: 5.8752e-05]
	Learning Rate: 5.87525e-05
	LOSS [training: 1.6047355546333901 | validation: 2.50450897662245]
	TIME [epoch: 8.36 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5814231221739132		[learning rate: 5.8646e-05]
		[batch 20/20] avg loss: 1.6322346398589325		[learning rate: 5.8539e-05]
	Learning Rate: 5.85393e-05
	LOSS [training: 1.6068288810164226 | validation: 2.496892206003669]
	TIME [epoch: 8.38 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5290124055394592		[learning rate: 5.8433e-05]
		[batch 20/20] avg loss: 1.679925599040453		[learning rate: 5.8327e-05]
	Learning Rate: 5.83268e-05
	LOSS [training: 1.6044690022899561 | validation: 2.493206050610086]
	TIME [epoch: 8.37 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5829231277568026		[learning rate: 5.8221e-05]
		[batch 20/20] avg loss: 1.631583765585209		[learning rate: 5.8115e-05]
	Learning Rate: 5.81152e-05
	LOSS [training: 1.607253446671006 | validation: 2.5046391759755835]
	TIME [epoch: 8.36 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6935305232106088		[learning rate: 5.801e-05]
		[batch 20/20] avg loss: 1.5228709374577236		[learning rate: 5.7904e-05]
	Learning Rate: 5.79043e-05
	LOSS [training: 1.6082007303341666 | validation: 2.4963725607943674]
	TIME [epoch: 8.36 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5334041836323653		[learning rate: 5.7799e-05]
		[batch 20/20] avg loss: 1.6783209101719272		[learning rate: 5.7694e-05]
	Learning Rate: 5.76941e-05
	LOSS [training: 1.6058625469021464 | validation: 2.498048949743943]
	TIME [epoch: 8.38 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4918117325479774		[learning rate: 5.7589e-05]
		[batch 20/20] avg loss: 1.7228038618325683		[learning rate: 5.7485e-05]
	Learning Rate: 5.74847e-05
	LOSS [training: 1.6073077971902727 | validation: 2.4910322829204836]
	TIME [epoch: 8.38 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5032145543686926		[learning rate: 5.738e-05]
		[batch 20/20] avg loss: 1.706927865193308		[learning rate: 5.7276e-05]
	Learning Rate: 5.72761e-05
	LOSS [training: 1.6050712097809998 | validation: 2.495569449333818]
	TIME [epoch: 8.37 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5182933969822563		[learning rate: 5.7172e-05]
		[batch 20/20] avg loss: 1.6953607885810864		[learning rate: 5.7068e-05]
	Learning Rate: 5.70683e-05
	LOSS [training: 1.6068270927816717 | validation: 2.4950906264655006]
	TIME [epoch: 8.36 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6442277254449948		[learning rate: 5.6965e-05]
		[batch 20/20] avg loss: 1.5767005209284715		[learning rate: 5.6861e-05]
	Learning Rate: 5.68612e-05
	LOSS [training: 1.6104641231867334 | validation: 2.4979301273471757]
	TIME [epoch: 8.36 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.662595382088469		[learning rate: 5.6758e-05]
		[batch 20/20] avg loss: 1.5488974768340806		[learning rate: 5.6655e-05]
	Learning Rate: 5.66548e-05
	LOSS [training: 1.6057464294612749 | validation: 2.5020838542184687]
	TIME [epoch: 8.38 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.471179738514708		[learning rate: 5.6552e-05]
		[batch 20/20] avg loss: 1.7382869119302904		[learning rate: 5.6449e-05]
	Learning Rate: 5.64492e-05
	LOSS [training: 1.6047333252224991 | validation: 2.50014413644962]
	TIME [epoch: 8.37 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5502766629463718		[learning rate: 5.6347e-05]
		[batch 20/20] avg loss: 1.6585141839427222		[learning rate: 5.6244e-05]
	Learning Rate: 5.62444e-05
	LOSS [training: 1.6043954234445468 | validation: 2.49632070189731]
	TIME [epoch: 8.36 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7068707047849354		[learning rate: 5.6142e-05]
		[batch 20/20] avg loss: 1.5036197065768235		[learning rate: 5.604e-05]
	Learning Rate: 5.60403e-05
	LOSS [training: 1.6052452056808793 | validation: 2.498522465945273]
	TIME [epoch: 8.36 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.451660238557747		[learning rate: 5.5938e-05]
		[batch 20/20] avg loss: 1.761913868006636		[learning rate: 5.5837e-05]
	Learning Rate: 5.58369e-05
	LOSS [training: 1.6067870532821913 | validation: 2.5041615135167454]
	TIME [epoch: 8.38 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6308785101695384		[learning rate: 5.5735e-05]
		[batch 20/20] avg loss: 1.5861058832777073		[learning rate: 5.5634e-05]
	Learning Rate: 5.56342e-05
	LOSS [training: 1.6084921967236228 | validation: 2.4963965024082864]
	TIME [epoch: 8.36 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.412557735899346		[learning rate: 5.5533e-05]
		[batch 20/20] avg loss: 1.8031812406206567		[learning rate: 5.5432e-05]
	Learning Rate: 5.54323e-05
	LOSS [training: 1.6078694882600015 | validation: 2.4902489004868182]
	TIME [epoch: 8.36 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7421414750423334		[learning rate: 5.5332e-05]
		[batch 20/20] avg loss: 1.467885752429192		[learning rate: 5.5231e-05]
	Learning Rate: 5.52312e-05
	LOSS [training: 1.6050136137357627 | validation: 2.4951581061677786]
	TIME [epoch: 8.36 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5178826516516961		[learning rate: 5.5131e-05]
		[batch 20/20] avg loss: 1.694263200492129		[learning rate: 5.5031e-05]
	Learning Rate: 5.50307e-05
	LOSS [training: 1.6060729260719124 | validation: 2.4930374614834236]
	TIME [epoch: 8.37 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4395882994806422		[learning rate: 5.4931e-05]
		[batch 20/20] avg loss: 1.7741838082011838		[learning rate: 5.4831e-05]
	Learning Rate: 5.4831e-05
	LOSS [training: 1.6068860538409129 | validation: 2.502169470147324]
	TIME [epoch: 8.37 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.706650248234153		[learning rate: 5.4731e-05]
		[batch 20/20] avg loss: 1.5041269171779033		[learning rate: 5.4632e-05]
	Learning Rate: 5.4632e-05
	LOSS [training: 1.6053885827060281 | validation: 2.496026394081088]
	TIME [epoch: 8.36 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7329024034565166		[learning rate: 5.4533e-05]
		[batch 20/20] avg loss: 1.4842099736891343		[learning rate: 5.4434e-05]
	Learning Rate: 5.44338e-05
	LOSS [training: 1.6085561885728257 | validation: 2.4997195453393184]
	TIME [epoch: 8.36 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.582439325191752		[learning rate: 5.4335e-05]
		[batch 20/20] avg loss: 1.6337737401498789		[learning rate: 5.4236e-05]
	Learning Rate: 5.42362e-05
	LOSS [training: 1.6081065326708157 | validation: 2.4953022517048984]
	TIME [epoch: 8.36 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8153166053122611		[learning rate: 5.4138e-05]
		[batch 20/20] avg loss: 1.3927393510948085		[learning rate: 5.4039e-05]
	Learning Rate: 5.40394e-05
	LOSS [training: 1.6040279782035345 | validation: 2.495679536146343]
	TIME [epoch: 8.39 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4580916363556558		[learning rate: 5.3941e-05]
		[batch 20/20] avg loss: 1.754700145818563		[learning rate: 5.3843e-05]
	Learning Rate: 5.38433e-05
	LOSS [training: 1.6063958910871095 | validation: 2.4984315078390993]
	TIME [epoch: 8.36 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5729528516921214		[learning rate: 5.3746e-05]
		[batch 20/20] avg loss: 1.641248118981783		[learning rate: 5.3648e-05]
	Learning Rate: 5.36479e-05
	LOSS [training: 1.607100485336952 | validation: 2.499862850840614]
	TIME [epoch: 8.36 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6281126943565762		[learning rate: 5.355e-05]
		[batch 20/20] avg loss: 1.5874812537656893		[learning rate: 5.3453e-05]
	Learning Rate: 5.34532e-05
	LOSS [training: 1.6077969740611326 | validation: 2.496288937722518]
	TIME [epoch: 8.36 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5725198944298648		[learning rate: 5.3356e-05]
		[batch 20/20] avg loss: 1.6357139371557157		[learning rate: 5.3259e-05]
	Learning Rate: 5.32592e-05
	LOSS [training: 1.6041169157927904 | validation: 2.4950362327710915]
	TIME [epoch: 8.39 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6195522510264788		[learning rate: 5.3162e-05]
		[batch 20/20] avg loss: 1.5987636022286686		[learning rate: 5.3066e-05]
	Learning Rate: 5.30659e-05
	LOSS [training: 1.6091579266275733 | validation: 2.501562004260805]
	TIME [epoch: 8.37 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5552263811101876		[learning rate: 5.297e-05]
		[batch 20/20] avg loss: 1.6600479919259257		[learning rate: 5.2873e-05]
	Learning Rate: 5.28734e-05
	LOSS [training: 1.6076371865180565 | validation: 2.4898103224425956]
	TIME [epoch: 8.36 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7035449086014718		[learning rate: 5.2777e-05]
		[batch 20/20] avg loss: 1.5144851805054949		[learning rate: 5.2681e-05]
	Learning Rate: 5.26815e-05
	LOSS [training: 1.6090150445534834 | validation: 2.4962440831018498]
	TIME [epoch: 8.36 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7233298536172987		[learning rate: 5.2586e-05]
		[batch 20/20] avg loss: 1.485833283589185		[learning rate: 5.249e-05]
	Learning Rate: 5.24903e-05
	LOSS [training: 1.6045815686032419 | validation: 2.4958852364812865]
	TIME [epoch: 8.38 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5229306512298737		[learning rate: 5.2395e-05]
		[batch 20/20] avg loss: 1.6902763628129314		[learning rate: 5.23e-05]
	Learning Rate: 5.22998e-05
	LOSS [training: 1.6066035070214024 | validation: 2.497945017969477]
	TIME [epoch: 8.37 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6440923431873873		[learning rate: 5.2205e-05]
		[batch 20/20] avg loss: 1.561703805775809		[learning rate: 5.211e-05]
	Learning Rate: 5.211e-05
	LOSS [training: 1.6028980744815982 | validation: 2.4966345574165056]
	TIME [epoch: 8.36 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6832075527011892		[learning rate: 5.2015e-05]
		[batch 20/20] avg loss: 1.5290718820522093		[learning rate: 5.1921e-05]
	Learning Rate: 5.19209e-05
	LOSS [training: 1.6061397173766991 | validation: 2.4851062723230184]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study201/model_tr_study201_r0_20240219_190908/states/model_tr_study201_1547.pth
	Model improved!!!
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5417839057320502		[learning rate: 5.1827e-05]
		[batch 20/20] avg loss: 1.6726220970090417		[learning rate: 5.1732e-05]
	Learning Rate: 5.17325e-05
	LOSS [training: 1.6072030013705458 | validation: 2.496992757380414]
	TIME [epoch: 8.37 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.646151514180957		[learning rate: 5.1639e-05]
		[batch 20/20] avg loss: 1.5667376760172669		[learning rate: 5.1545e-05]
	Learning Rate: 5.15447e-05
	LOSS [training: 1.6064445950991117 | validation: 2.4881776877706034]
	TIME [epoch: 8.39 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5218616592501593		[learning rate: 5.1451e-05]
		[batch 20/20] avg loss: 1.6922159941779402		[learning rate: 5.1358e-05]
	Learning Rate: 5.13577e-05
	LOSS [training: 1.6070388267140499 | validation: 2.4909379579315853]
	TIME [epoch: 8.37 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6400310142050507		[learning rate: 5.1264e-05]
		[batch 20/20] avg loss: 1.5717382837469394		[learning rate: 5.1171e-05]
	Learning Rate: 5.11713e-05
	LOSS [training: 1.6058846489759948 | validation: 2.4986304665498054]
	TIME [epoch: 8.37 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5498763317252742		[learning rate: 5.1078e-05]
		[batch 20/20] avg loss: 1.65863960925207		[learning rate: 5.0986e-05]
	Learning Rate: 5.09856e-05
	LOSS [training: 1.604257970488672 | validation: 2.493252016123219]
	TIME [epoch: 8.36 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5867069819826787		[learning rate: 5.0893e-05]
		[batch 20/20] avg loss: 1.6228542874322631		[learning rate: 5.0801e-05]
	Learning Rate: 5.08006e-05
	LOSS [training: 1.604780634707471 | validation: 2.4992568414732372]
	TIME [epoch: 8.38 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6116509761451474		[learning rate: 5.0708e-05]
		[batch 20/20] avg loss: 1.6003377546296065		[learning rate: 5.0616e-05]
	Learning Rate: 5.06162e-05
	LOSS [training: 1.605994365387377 | validation: 2.4982810554385906]
	TIME [epoch: 8.37 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5563644381736332		[learning rate: 5.0524e-05]
		[batch 20/20] avg loss: 1.6560530409628025		[learning rate: 5.0433e-05]
	Learning Rate: 5.04325e-05
	LOSS [training: 1.6062087395682176 | validation: 2.496136921578713]
	TIME [epoch: 8.37 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5484563981475208		[learning rate: 5.0341e-05]
		[batch 20/20] avg loss: 1.6635154324090529		[learning rate: 5.0249e-05]
	Learning Rate: 5.02495e-05
	LOSS [training: 1.605985915278287 | validation: 2.4928835683039554]
	TIME [epoch: 8.36 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4414639558438147		[learning rate: 5.0158e-05]
		[batch 20/20] avg loss: 1.7680996757973038		[learning rate: 5.0067e-05]
	Learning Rate: 5.00671e-05
	LOSS [training: 1.6047818158205593 | validation: 2.498418259041043]
	TIME [epoch: 8.38 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.46027778686517		[learning rate: 4.9976e-05]
		[batch 20/20] avg loss: 1.7562361810315088		[learning rate: 4.9885e-05]
	Learning Rate: 4.98854e-05
	LOSS [training: 1.6082569839483398 | validation: 2.4927504087578383]
	TIME [epoch: 8.37 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5891438387839485		[learning rate: 4.9795e-05]
		[batch 20/20] avg loss: 1.6183167666460374		[learning rate: 4.9704e-05]
	Learning Rate: 4.97044e-05
	LOSS [training: 1.6037303027149927 | validation: 2.4871397978689425]
	TIME [epoch: 8.37 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6125446953619664		[learning rate: 4.9614e-05]
		[batch 20/20] avg loss: 1.5997107045537808		[learning rate: 4.9524e-05]
	Learning Rate: 4.9524e-05
	LOSS [training: 1.6061276999578737 | validation: 2.4988542412241483]
	TIME [epoch: 8.36 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6558400562244138		[learning rate: 4.9434e-05]
		[batch 20/20] avg loss: 1.5564789514881128		[learning rate: 4.9344e-05]
	Learning Rate: 4.93443e-05
	LOSS [training: 1.6061595038562637 | validation: 2.492330625478227]
	TIME [epoch: 8.37 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6597661733136744		[learning rate: 4.9255e-05]
		[batch 20/20] avg loss: 1.5538660704178566		[learning rate: 4.9165e-05]
	Learning Rate: 4.91652e-05
	LOSS [training: 1.606816121865765 | validation: 2.4976787574806734]
	TIME [epoch: 8.39 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6928672781048		[learning rate: 4.9076e-05]
		[batch 20/20] avg loss: 1.525224408280355		[learning rate: 4.8987e-05]
	Learning Rate: 4.89868e-05
	LOSS [training: 1.6090458431925776 | validation: 2.5026427220188263]
	TIME [epoch: 8.37 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.579559729599979		[learning rate: 4.8898e-05]
		[batch 20/20] avg loss: 1.6307272792352094		[learning rate: 4.8809e-05]
	Learning Rate: 4.8809e-05
	LOSS [training: 1.6051435044175943 | validation: 2.4961895034448682]
	TIME [epoch: 8.37 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5372273109889962		[learning rate: 4.872e-05]
		[batch 20/20] avg loss: 1.6740346434411049		[learning rate: 4.8632e-05]
	Learning Rate: 4.86319e-05
	LOSS [training: 1.6056309772150505 | validation: 2.49462816411579]
	TIME [epoch: 8.36 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7624616804258353		[learning rate: 4.8544e-05]
		[batch 20/20] avg loss: 1.448597168283849		[learning rate: 4.8455e-05]
	Learning Rate: 4.84554e-05
	LOSS [training: 1.6055294243548421 | validation: 2.490014084802933]
	TIME [epoch: 8.39 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.555206423697966		[learning rate: 4.8367e-05]
		[batch 20/20] avg loss: 1.6539564727281983		[learning rate: 4.828e-05]
	Learning Rate: 4.82795e-05
	LOSS [training: 1.6045814482130818 | validation: 2.4944231612853147]
	TIME [epoch: 8.37 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4853865731975797		[learning rate: 4.8192e-05]
		[batch 20/20] avg loss: 1.7248176955603036		[learning rate: 4.8104e-05]
	Learning Rate: 4.81043e-05
	LOSS [training: 1.6051021343789418 | validation: 2.496106216901399]
	TIME [epoch: 8.37 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5312700130045862		[learning rate: 4.8017e-05]
		[batch 20/20] avg loss: 1.6875477955776763		[learning rate: 4.793e-05]
	Learning Rate: 4.79298e-05
	LOSS [training: 1.6094089042911315 | validation: 2.493537506908023]
	TIME [epoch: 8.37 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6649119131372387		[learning rate: 4.7843e-05]
		[batch 20/20] avg loss: 1.5502414844506283		[learning rate: 4.7756e-05]
	Learning Rate: 4.77558e-05
	LOSS [training: 1.6075766987939333 | validation: 2.4983985007238947]
	TIME [epoch: 8.39 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5508767799275085		[learning rate: 4.7669e-05]
		[batch 20/20] avg loss: 1.6542045837327062		[learning rate: 4.7583e-05]
	Learning Rate: 4.75825e-05
	LOSS [training: 1.6025406818301076 | validation: 2.492966638593974]
	TIME [epoch: 8.37 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5544620837612833		[learning rate: 4.7496e-05]
		[batch 20/20] avg loss: 1.6600267345417496		[learning rate: 4.741e-05]
	Learning Rate: 4.74098e-05
	LOSS [training: 1.6072444091515166 | validation: 2.4950710841885266]
	TIME [epoch: 8.37 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5593792490443028		[learning rate: 4.7324e-05]
		[batch 20/20] avg loss: 1.6575178276025486		[learning rate: 4.7238e-05]
	Learning Rate: 4.72378e-05
	LOSS [training: 1.6084485383234255 | validation: 2.497732052914597]
	TIME [epoch: 8.36 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6040708655302836		[learning rate: 4.7152e-05]
		[batch 20/20] avg loss: 1.6155981956919774		[learning rate: 4.7066e-05]
	Learning Rate: 4.70664e-05
	LOSS [training: 1.6098345306111306 | validation: 2.498095468899287]
	TIME [epoch: 8.37 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6721521771864918		[learning rate: 4.6981e-05]
		[batch 20/20] avg loss: 1.533184528685511		[learning rate: 4.6896e-05]
	Learning Rate: 4.68955e-05
	LOSS [training: 1.6026683529360013 | validation: 2.4963036964605188]
	TIME [epoch: 8.39 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5797469066268603		[learning rate: 4.681e-05]
		[batch 20/20] avg loss: 1.6295530170473458		[learning rate: 4.6725e-05]
	Learning Rate: 4.67254e-05
	LOSS [training: 1.604649961837103 | validation: 2.49948278799776]
	TIME [epoch: 8.36 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4546597132810988		[learning rate: 4.6641e-05]
		[batch 20/20] avg loss: 1.756541096640341		[learning rate: 4.6556e-05]
	Learning Rate: 4.65558e-05
	LOSS [training: 1.6056004049607195 | validation: 2.500901513600659]
	TIME [epoch: 8.37 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4970101806997305		[learning rate: 4.6471e-05]
		[batch 20/20] avg loss: 1.7144286718006227		[learning rate: 4.6387e-05]
	Learning Rate: 4.63868e-05
	LOSS [training: 1.6057194262501766 | validation: 2.492434864482575]
	TIME [epoch: 8.36 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7044243647745603		[learning rate: 4.6303e-05]
		[batch 20/20] avg loss: 1.5073624495873557		[learning rate: 4.6219e-05]
	Learning Rate: 4.62185e-05
	LOSS [training: 1.605893407180958 | validation: 2.4972389871906913]
	TIME [epoch: 8.39 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6244775522387787		[learning rate: 4.6135e-05]
		[batch 20/20] avg loss: 1.5896044742572866		[learning rate: 4.6051e-05]
	Learning Rate: 4.60508e-05
	LOSS [training: 1.6070410132480326 | validation: 2.4945011933573915]
	TIME [epoch: 8.37 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6081612000524537		[learning rate: 4.5967e-05]
		[batch 20/20] avg loss: 1.6012931123641376		[learning rate: 4.5884e-05]
	Learning Rate: 4.58836e-05
	LOSS [training: 1.6047271562082954 | validation: 2.495435239020922]
	TIME [epoch: 8.36 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6946683262089084		[learning rate: 4.58e-05]
		[batch 20/20] avg loss: 1.5159809911918043		[learning rate: 4.5717e-05]
	Learning Rate: 4.57171e-05
	LOSS [training: 1.605324658700356 | validation: 2.5003571197839096]
	TIME [epoch: 8.36 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5805320942917913		[learning rate: 4.5634e-05]
		[batch 20/20] avg loss: 1.63507595493599		[learning rate: 4.5551e-05]
	Learning Rate: 4.55512e-05
	LOSS [training: 1.60780402461389 | validation: 2.502580927620947]
	TIME [epoch: 8.38 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6541737879249734		[learning rate: 4.5468e-05]
		[batch 20/20] avg loss: 1.5570960129276559		[learning rate: 4.5386e-05]
	Learning Rate: 4.53859e-05
	LOSS [training: 1.605634900426315 | validation: 2.4962152504144095]
	TIME [epoch: 8.37 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4488484339787977		[learning rate: 4.5303e-05]
		[batch 20/20] avg loss: 1.7674479114595472		[learning rate: 4.5221e-05]
	Learning Rate: 4.52212e-05
	LOSS [training: 1.6081481727191727 | validation: 2.5001854849676475]
	TIME [epoch: 8.37 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6884449842678317		[learning rate: 4.5139e-05]
		[batch 20/20] avg loss: 1.5253554651064767		[learning rate: 4.5057e-05]
	Learning Rate: 4.50571e-05
	LOSS [training: 1.6069002246871544 | validation: 2.506304222845489]
	TIME [epoch: 8.37 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7020961366936818		[learning rate: 4.4975e-05]
		[batch 20/20] avg loss: 1.5147900882713796		[learning rate: 4.4894e-05]
	Learning Rate: 4.48936e-05
	LOSS [training: 1.6084431124825307 | validation: 2.4996071290090205]
	TIME [epoch: 8.37 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5851900189330606		[learning rate: 4.4812e-05]
		[batch 20/20] avg loss: 1.626372749593921		[learning rate: 4.4731e-05]
	Learning Rate: 4.47307e-05
	LOSS [training: 1.6057813842634907 | validation: 2.500533232643704]
	TIME [epoch: 8.39 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5974844789070963		[learning rate: 4.4649e-05]
		[batch 20/20] avg loss: 1.6147499355243256		[learning rate: 4.4568e-05]
	Learning Rate: 4.45683e-05
	LOSS [training: 1.6061172072157113 | validation: 2.493158075790388]
	TIME [epoch: 8.37 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6389121854641608		[learning rate: 4.4487e-05]
		[batch 20/20] avg loss: 1.5740878634803077		[learning rate: 4.4407e-05]
	Learning Rate: 4.44066e-05
	LOSS [training: 1.606500024472234 | validation: 2.49389513053658]
	TIME [epoch: 8.37 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.644614843881903		[learning rate: 4.4326e-05]
		[batch 20/20] avg loss: 1.5704675623393334		[learning rate: 4.4245e-05]
	Learning Rate: 4.42454e-05
	LOSS [training: 1.6075412031106182 | validation: 2.5023119575496393]
	TIME [epoch: 8.37 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7184023706276512		[learning rate: 4.4165e-05]
		[batch 20/20] avg loss: 1.4938386058263138		[learning rate: 4.4085e-05]
	Learning Rate: 4.40849e-05
	LOSS [training: 1.6061204882269824 | validation: 2.4942288330562534]
	TIME [epoch: 8.38 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3854042364186467		[learning rate: 4.4005e-05]
		[batch 20/20] avg loss: 1.8252991195399315		[learning rate: 4.3925e-05]
	Learning Rate: 4.39249e-05
	LOSS [training: 1.605351677979289 | validation: 2.4943211550287243]
	TIME [epoch: 8.37 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.446867241591859		[learning rate: 4.3845e-05]
		[batch 20/20] avg loss: 1.7641731358725636		[learning rate: 4.3765e-05]
	Learning Rate: 4.37655e-05
	LOSS [training: 1.6055201887322113 | validation: 2.4964275317062867]
	TIME [epoch: 8.36 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.681112690710481		[learning rate: 4.3686e-05]
		[batch 20/20] avg loss: 1.5265580238317695		[learning rate: 4.3607e-05]
	Learning Rate: 4.36067e-05
	LOSS [training: 1.6038353572711255 | validation: 2.4980579275111134]
	TIME [epoch: 8.37 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5158326832211426		[learning rate: 4.3527e-05]
		[batch 20/20] avg loss: 1.6989109736452666		[learning rate: 4.3448e-05]
	Learning Rate: 4.34484e-05
	LOSS [training: 1.6073718284332046 | validation: 2.496282818450869]
	TIME [epoch: 8.38 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5391817874991465		[learning rate: 4.3369e-05]
		[batch 20/20] avg loss: 1.6692628936244156		[learning rate: 4.3291e-05]
	Learning Rate: 4.32907e-05
	LOSS [training: 1.604222340561781 | validation: 2.48816808663258]
	TIME [epoch: 8.37 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.770164369269914		[learning rate: 4.3212e-05]
		[batch 20/20] avg loss: 1.4381253748574827		[learning rate: 4.3134e-05]
	Learning Rate: 4.31336e-05
	LOSS [training: 1.6041448720636986 | validation: 2.4890696208469407]
	TIME [epoch: 8.37 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6614720117041089		[learning rate: 4.3055e-05]
		[batch 20/20] avg loss: 1.5509279017390558		[learning rate: 4.2977e-05]
	Learning Rate: 4.29771e-05
	LOSS [training: 1.6061999567215826 | validation: 2.502625693503481]
	TIME [epoch: 8.36 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.741548844698422		[learning rate: 4.2899e-05]
		[batch 20/20] avg loss: 1.4665328276255327		[learning rate: 4.2821e-05]
	Learning Rate: 4.28211e-05
	LOSS [training: 1.6040408361619771 | validation: 2.5015340910343964]
	TIME [epoch: 8.38 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6823318932852949		[learning rate: 4.2743e-05]
		[batch 20/20] avg loss: 1.52293977035491		[learning rate: 4.2666e-05]
	Learning Rate: 4.26657e-05
	LOSS [training: 1.6026358318201024 | validation: 2.498511246568375]
	TIME [epoch: 8.37 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.51787760817639		[learning rate: 4.2588e-05]
		[batch 20/20] avg loss: 1.691419544522486		[learning rate: 4.2511e-05]
	Learning Rate: 4.25109e-05
	LOSS [training: 1.604648576349438 | validation: 2.4950489894939745]
	TIME [epoch: 8.36 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5846609363919097		[learning rate: 4.2434e-05]
		[batch 20/20] avg loss: 1.6317404836045462		[learning rate: 4.2357e-05]
	Learning Rate: 4.23566e-05
	LOSS [training: 1.608200709998228 | validation: 2.4969913846421097]
	TIME [epoch: 8.36 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.527813364418592		[learning rate: 4.228e-05]
		[batch 20/20] avg loss: 1.6854590525846156		[learning rate: 4.2203e-05]
	Learning Rate: 4.22029e-05
	LOSS [training: 1.606636208501604 | validation: 2.5006495432018685]
	TIME [epoch: 8.36 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7165922460478509		[learning rate: 4.2126e-05]
		[batch 20/20] avg loss: 1.504021187772181		[learning rate: 4.205e-05]
	Learning Rate: 4.20497e-05
	LOSS [training: 1.6103067169100158 | validation: 2.498631019246006]
	TIME [epoch: 8.38 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6193562731323112		[learning rate: 4.1973e-05]
		[batch 20/20] avg loss: 1.594108650771125		[learning rate: 4.1897e-05]
	Learning Rate: 4.18971e-05
	LOSS [training: 1.606732461951718 | validation: 2.4970183336111464]
	TIME [epoch: 8.37 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7899623435601597		[learning rate: 4.1821e-05]
		[batch 20/20] avg loss: 1.4228299044473334		[learning rate: 4.1745e-05]
	Learning Rate: 4.17451e-05
	LOSS [training: 1.6063961240037468 | validation: 2.4976951471709103]
	TIME [epoch: 8.37 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6980506967484121		[learning rate: 4.1669e-05]
		[batch 20/20] avg loss: 1.5125315223355709		[learning rate: 4.1594e-05]
	Learning Rate: 4.15936e-05
	LOSS [training: 1.6052911095419915 | validation: 2.493250029639879]
	TIME [epoch: 8.36 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6358359213229057		[learning rate: 4.1518e-05]
		[batch 20/20] avg loss: 1.572393422297775		[learning rate: 4.1443e-05]
	Learning Rate: 4.14426e-05
	LOSS [training: 1.6041146718103403 | validation: 2.4977549004722284]
	TIME [epoch: 8.38 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5536252946861322		[learning rate: 4.1367e-05]
		[batch 20/20] avg loss: 1.6575305563797174		[learning rate: 4.1292e-05]
	Learning Rate: 4.12922e-05
	LOSS [training: 1.6055779255329246 | validation: 2.494601587176236]
	TIME [epoch: 8.36 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6621323885838968		[learning rate: 4.1217e-05]
		[batch 20/20] avg loss: 1.5471160456947763		[learning rate: 4.1142e-05]
	Learning Rate: 4.11424e-05
	LOSS [training: 1.6046242171393366 | validation: 2.4939115359245143]
	TIME [epoch: 8.36 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4929250897730744		[learning rate: 4.1068e-05]
		[batch 20/20] avg loss: 1.716028537468827		[learning rate: 4.0993e-05]
	Learning Rate: 4.09931e-05
	LOSS [training: 1.6044768136209506 | validation: 2.4902203300396413]
	TIME [epoch: 8.36 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.681906224670786		[learning rate: 4.0919e-05]
		[batch 20/20] avg loss: 1.5289345040553897		[learning rate: 4.0844e-05]
	Learning Rate: 4.08443e-05
	LOSS [training: 1.6054203643630878 | validation: 2.50257272939199]
	TIME [epoch: 8.37 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5804179825858726		[learning rate: 4.077e-05]
		[batch 20/20] avg loss: 1.6307284449257082		[learning rate: 4.0696e-05]
	Learning Rate: 4.06961e-05
	LOSS [training: 1.6055732137557903 | validation: 2.498645444308979]
	TIME [epoch: 8.37 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5809879647695595		[learning rate: 4.0622e-05]
		[batch 20/20] avg loss: 1.6297027868224947		[learning rate: 4.0548e-05]
	Learning Rate: 4.05484e-05
	LOSS [training: 1.605345375796027 | validation: 2.498537401727619]
	TIME [epoch: 8.36 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.463398561518908		[learning rate: 4.0475e-05]
		[batch 20/20] avg loss: 1.7481538111305905		[learning rate: 4.0401e-05]
	Learning Rate: 4.04012e-05
	LOSS [training: 1.6057761863247493 | validation: 2.488683919136071]
	TIME [epoch: 8.37 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6040709083549167		[learning rate: 4.0328e-05]
		[batch 20/20] avg loss: 1.6080939684199298		[learning rate: 4.0255e-05]
	Learning Rate: 4.02546e-05
	LOSS [training: 1.606082438387423 | validation: 2.4957777714771443]
	TIME [epoch: 8.36 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.590116978153786		[learning rate: 4.0182e-05]
		[batch 20/20] avg loss: 1.6233794369387433		[learning rate: 4.0109e-05]
	Learning Rate: 4.01085e-05
	LOSS [training: 1.6067482075462642 | validation: 2.493786314431099]
	TIME [epoch: 8.38 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5639179542633088		[learning rate: 4.0036e-05]
		[batch 20/20] avg loss: 1.641597053975937		[learning rate: 3.9963e-05]
	Learning Rate: 3.9963e-05
	LOSS [training: 1.602757504119623 | validation: 2.4956134395430793]
	TIME [epoch: 8.36 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7851288468437496		[learning rate: 3.989e-05]
		[batch 20/20] avg loss: 1.4235769691605815		[learning rate: 3.9818e-05]
	Learning Rate: 3.9818e-05
	LOSS [training: 1.6043529080021657 | validation: 2.4962903469312723]
	TIME [epoch: 8.37 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7443708598625958		[learning rate: 3.9746e-05]
		[batch 20/20] avg loss: 1.4646600708563808		[learning rate: 3.9673e-05]
	Learning Rate: 3.96735e-05
	LOSS [training: 1.6045154653594877 | validation: 2.4990151960756837]
	TIME [epoch: 8.36 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.411121145429925		[learning rate: 3.9601e-05]
		[batch 20/20] avg loss: 1.8071311210477123		[learning rate: 3.9529e-05]
	Learning Rate: 3.95295e-05
	LOSS [training: 1.6091261332388185 | validation: 2.501287697457416]
	TIME [epoch: 8.39 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6346365372573448		[learning rate: 3.9458e-05]
		[batch 20/20] avg loss: 1.577085388153274		[learning rate: 3.9386e-05]
	Learning Rate: 3.9386e-05
	LOSS [training: 1.605860962705309 | validation: 2.500206287429223]
	TIME [epoch: 8.37 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.689479026887852		[learning rate: 3.9315e-05]
		[batch 20/20] avg loss: 1.5259241924775382		[learning rate: 3.9243e-05]
	Learning Rate: 3.92431e-05
	LOSS [training: 1.6077016096826953 | validation: 2.501993350677385]
	TIME [epoch: 8.36 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5787863437241474		[learning rate: 3.9172e-05]
		[batch 20/20] avg loss: 1.6441036503503799		[learning rate: 3.9101e-05]
	Learning Rate: 3.91007e-05
	LOSS [training: 1.6114449970372635 | validation: 2.501757052688179]
	TIME [epoch: 8.36 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4483478767292253		[learning rate: 3.903e-05]
		[batch 20/20] avg loss: 1.7581914725462955		[learning rate: 3.8959e-05]
	Learning Rate: 3.89588e-05
	LOSS [training: 1.6032696746377604 | validation: 2.4981957194260307]
	TIME [epoch: 8.39 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.644287198840269		[learning rate: 3.8888e-05]
		[batch 20/20] avg loss: 1.5688687402923338		[learning rate: 3.8817e-05]
	Learning Rate: 3.88174e-05
	LOSS [training: 1.6065779695663014 | validation: 2.4944115031337946]
	TIME [epoch: 8.37 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.568425767292363		[learning rate: 3.8747e-05]
		[batch 20/20] avg loss: 1.637005892865997		[learning rate: 3.8677e-05]
	Learning Rate: 3.86765e-05
	LOSS [training: 1.6027158300791797 | validation: 2.4967895880308824]
	TIME [epoch: 8.36 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.517490694519275		[learning rate: 3.8606e-05]
		[batch 20/20] avg loss: 1.6905415858576671		[learning rate: 3.8536e-05]
	Learning Rate: 3.85362e-05
	LOSS [training: 1.604016140188471 | validation: 2.490150678399215]
	TIME [epoch: 8.37 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6363740959012394		[learning rate: 3.8466e-05]
		[batch 20/20] avg loss: 1.570597187070446		[learning rate: 3.8396e-05]
	Learning Rate: 3.83963e-05
	LOSS [training: 1.6034856414858427 | validation: 2.495985307337416]
	TIME [epoch: 8.36 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.601578116186427		[learning rate: 3.8327e-05]
		[batch 20/20] avg loss: 1.6055538981384678		[learning rate: 3.8257e-05]
	Learning Rate: 3.8257e-05
	LOSS [training: 1.6035660071624473 | validation: 2.4922019838699434]
	TIME [epoch: 8.39 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5693801518435282		[learning rate: 3.8187e-05]
		[batch 20/20] avg loss: 1.6389418598728998		[learning rate: 3.8118e-05]
	Learning Rate: 3.81181e-05
	LOSS [training: 1.604161005858214 | validation: 2.4958166914645523]
	TIME [epoch: 8.36 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6671325890572586		[learning rate: 3.8049e-05]
		[batch 20/20] avg loss: 1.5407902501459063		[learning rate: 3.798e-05]
	Learning Rate: 3.79798e-05
	LOSS [training: 1.6039614196015826 | validation: 2.498481026628655]
	TIME [epoch: 8.37 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6893384627623405		[learning rate: 3.7911e-05]
		[batch 20/20] avg loss: 1.5257261543829543		[learning rate: 3.7842e-05]
	Learning Rate: 3.7842e-05
	LOSS [training: 1.607532308572647 | validation: 2.4971731836040716]
	TIME [epoch: 8.37 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4272720519580906		[learning rate: 3.7773e-05]
		[batch 20/20] avg loss: 1.7865344928894626		[learning rate: 3.7705e-05]
	Learning Rate: 3.77046e-05
	LOSS [training: 1.6069032724237768 | validation: 2.5042128989732166]
	TIME [epoch: 8.38 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6678497913137904		[learning rate: 3.7636e-05]
		[batch 20/20] avg loss: 1.5425267710546886		[learning rate: 3.7568e-05]
	Learning Rate: 3.75678e-05
	LOSS [training: 1.605188281184239 | validation: 2.496391235287477]
	TIME [epoch: 8.37 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7015627346610747		[learning rate: 3.75e-05]
		[batch 20/20] avg loss: 1.5047096423191064		[learning rate: 3.7431e-05]
	Learning Rate: 3.74315e-05
	LOSS [training: 1.6031361884900908 | validation: 2.502851881975777]
	TIME [epoch: 8.36 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6419285725173733		[learning rate: 3.7363e-05]
		[batch 20/20] avg loss: 1.571567664642717		[learning rate: 3.7296e-05]
	Learning Rate: 3.72956e-05
	LOSS [training: 1.606748118580045 | validation: 2.5027763081031136]
	TIME [epoch: 8.36 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.548002952978503		[learning rate: 3.7228e-05]
		[batch 20/20] avg loss: 1.6638012458769484		[learning rate: 3.716e-05]
	Learning Rate: 3.71603e-05
	LOSS [training: 1.6059020994277258 | validation: 2.491745742039842]
	TIME [epoch: 8.38 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6093372583241847		[learning rate: 3.7093e-05]
		[batch 20/20] avg loss: 1.6051969702156188		[learning rate: 3.7025e-05]
	Learning Rate: 3.70254e-05
	LOSS [training: 1.6072671142699015 | validation: 2.4993722655761044]
	TIME [epoch: 8.37 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4802168997864145		[learning rate: 3.6958e-05]
		[batch 20/20] avg loss: 1.7295726620429712		[learning rate: 3.6891e-05]
	Learning Rate: 3.68911e-05
	LOSS [training: 1.6048947809146927 | validation: 2.492686978777231]
	TIME [epoch: 8.36 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5090187473568701		[learning rate: 3.6824e-05]
		[batch 20/20] avg loss: 1.7006289648141237		[learning rate: 3.6757e-05]
	Learning Rate: 3.67572e-05
	LOSS [training: 1.6048238560854968 | validation: 2.496366394381042]
	TIME [epoch: 8.36 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4792418960847744		[learning rate: 3.669e-05]
		[batch 20/20] avg loss: 1.7297924718446427		[learning rate: 3.6624e-05]
	Learning Rate: 3.66238e-05
	LOSS [training: 1.604517183964709 | validation: 2.4944465906940145]
	TIME [epoch: 8.36 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.682389271613366		[learning rate: 3.6557e-05]
		[batch 20/20] avg loss: 1.5317152202563362		[learning rate: 3.6491e-05]
	Learning Rate: 3.64909e-05
	LOSS [training: 1.607052245934851 | validation: 2.5021779890535094]
	TIME [epoch: 8.38 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.683511500667708		[learning rate: 3.6425e-05]
		[batch 20/20] avg loss: 1.5293237406035802		[learning rate: 3.6358e-05]
	Learning Rate: 3.63584e-05
	LOSS [training: 1.606417620635644 | validation: 2.498107373649063]
	TIME [epoch: 8.36 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5601046436648076		[learning rate: 3.6292e-05]
		[batch 20/20] avg loss: 1.6475504365903302		[learning rate: 3.6226e-05]
	Learning Rate: 3.62265e-05
	LOSS [training: 1.603827540127569 | validation: 2.4990358438159728]
	TIME [epoch: 8.36 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.628356614045441		[learning rate: 3.6161e-05]
		[batch 20/20] avg loss: 1.588860585119356		[learning rate: 3.6095e-05]
	Learning Rate: 3.6095e-05
	LOSS [training: 1.6086085995823987 | validation: 2.4998540415472656]
	TIME [epoch: 8.36 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.646071973188984		[learning rate: 3.6029e-05]
		[batch 20/20] avg loss: 1.5642384294675586		[learning rate: 3.5964e-05]
	Learning Rate: 3.5964e-05
	LOSS [training: 1.6051552013282717 | validation: 2.499412181161806]
	TIME [epoch: 8.38 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.60750821083063		[learning rate: 3.5899e-05]
		[batch 20/20] avg loss: 1.599721472537988		[learning rate: 3.5834e-05]
	Learning Rate: 3.58335e-05
	LOSS [training: 1.603614841684309 | validation: 2.498415632796236]
	TIME [epoch: 8.37 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6710286614424432		[learning rate: 3.5768e-05]
		[batch 20/20] avg loss: 1.539123662034862		[learning rate: 3.5703e-05]
	Learning Rate: 3.57035e-05
	LOSS [training: 1.6050761617386526 | validation: 2.494841334452037]
	TIME [epoch: 8.36 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6176991995032548		[learning rate: 3.5639e-05]
		[batch 20/20] avg loss: 1.5952599361878292		[learning rate: 3.5574e-05]
	Learning Rate: 3.55739e-05
	LOSS [training: 1.606479567845542 | validation: 2.4953179354072352]
	TIME [epoch: 8.36 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6008705401054553		[learning rate: 3.5509e-05]
		[batch 20/20] avg loss: 1.6153558249130378		[learning rate: 3.5445e-05]
	Learning Rate: 3.54448e-05
	LOSS [training: 1.6081131825092463 | validation: 2.498390203657642]
	TIME [epoch: 8.38 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4827026630429692		[learning rate: 3.538e-05]
		[batch 20/20] avg loss: 1.7378201644472948		[learning rate: 3.5316e-05]
	Learning Rate: 3.53162e-05
	LOSS [training: 1.6102614137451319 | validation: 2.4972439155505026]
	TIME [epoch: 8.37 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7323571788965648		[learning rate: 3.5252e-05]
		[batch 20/20] avg loss: 1.4778269157038322		[learning rate: 3.5188e-05]
	Learning Rate: 3.5188e-05
	LOSS [training: 1.6050920473001984 | validation: 2.498248937670996]
	TIME [epoch: 8.37 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4638298998757322		[learning rate: 3.5124e-05]
		[batch 20/20] avg loss: 1.748818407678734		[learning rate: 3.506e-05]
	Learning Rate: 3.50603e-05
	LOSS [training: 1.606324153777233 | validation: 2.498918116312185]
	TIME [epoch: 8.36 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.643540313919272		[learning rate: 3.4997e-05]
		[batch 20/20] avg loss: 1.571629404871826		[learning rate: 3.4933e-05]
	Learning Rate: 3.49331e-05
	LOSS [training: 1.6075848593955484 | validation: 2.4942442027276703]
	TIME [epoch: 8.37 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4966497806646126		[learning rate: 3.487e-05]
		[batch 20/20] avg loss: 1.7153010520501977		[learning rate: 3.4806e-05]
	Learning Rate: 3.48063e-05
	LOSS [training: 1.605975416357405 | validation: 2.493070924432998]
	TIME [epoch: 8.39 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.59975666319137		[learning rate: 3.4743e-05]
		[batch 20/20] avg loss: 1.6134397269261798		[learning rate: 3.468e-05]
	Learning Rate: 3.468e-05
	LOSS [training: 1.6065981950587744 | validation: 2.4940224238660034]
	TIME [epoch: 8.36 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5752508886501295		[learning rate: 3.4617e-05]
		[batch 20/20] avg loss: 1.6350937234432468		[learning rate: 3.4554e-05]
	Learning Rate: 3.45541e-05
	LOSS [training: 1.6051723060466885 | validation: 2.490790226771207]
	TIME [epoch: 8.36 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7222336421223905		[learning rate: 3.4491e-05]
		[batch 20/20] avg loss: 1.4875972285544168		[learning rate: 3.4429e-05]
	Learning Rate: 3.44287e-05
	LOSS [training: 1.6049154353384036 | validation: 2.4933670791170357]
	TIME [epoch: 8.36 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6486939635664606		[learning rate: 3.4366e-05]
		[batch 20/20] avg loss: 1.5579090041379016		[learning rate: 3.4304e-05]
	Learning Rate: 3.43038e-05
	LOSS [training: 1.603301483852181 | validation: 2.4951055146161942]
	TIME [epoch: 8.39 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.482404895144033		[learning rate: 3.4241e-05]
		[batch 20/20] avg loss: 1.726373147420814		[learning rate: 3.4179e-05]
	Learning Rate: 3.41793e-05
	LOSS [training: 1.6043890212824234 | validation: 2.497723442375628]
	TIME [epoch: 8.37 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6217855688319944		[learning rate: 3.4117e-05]
		[batch 20/20] avg loss: 1.5878844204269162		[learning rate: 3.4055e-05]
	Learning Rate: 3.40553e-05
	LOSS [training: 1.6048349946294551 | validation: 2.4980398134572073]
	TIME [epoch: 8.36 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6392307373123867		[learning rate: 3.3993e-05]
		[batch 20/20] avg loss: 1.5702854959090846		[learning rate: 3.3932e-05]
	Learning Rate: 3.39317e-05
	LOSS [training: 1.6047581166107356 | validation: 2.497954386727118]
	TIME [epoch: 8.36 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5697064689363525		[learning rate: 3.387e-05]
		[batch 20/20] avg loss: 1.6446218916252846		[learning rate: 3.3809e-05]
	Learning Rate: 3.38085e-05
	LOSS [training: 1.6071641802808183 | validation: 2.4945536135018376]
	TIME [epoch: 8.38 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5192524910651604		[learning rate: 3.3747e-05]
		[batch 20/20] avg loss: 1.6946329224426884		[learning rate: 3.3686e-05]
	Learning Rate: 3.36858e-05
	LOSS [training: 1.6069427067539241 | validation: 2.4901493093372205]
	TIME [epoch: 8.37 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5015932681483168		[learning rate: 3.3625e-05]
		[batch 20/20] avg loss: 1.7155821172381898		[learning rate: 3.3564e-05]
	Learning Rate: 3.35636e-05
	LOSS [training: 1.6085876926932534 | validation: 2.4964176514857694]
	TIME [epoch: 8.36 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6171284080720316		[learning rate: 3.3503e-05]
		[batch 20/20] avg loss: 1.5942321061172562		[learning rate: 3.3442e-05]
	Learning Rate: 3.34418e-05
	LOSS [training: 1.6056802570946442 | validation: 2.494496651422528]
	TIME [epoch: 8.37 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.538159031794767		[learning rate: 3.3381e-05]
		[batch 20/20] avg loss: 1.67700979440457		[learning rate: 3.332e-05]
	Learning Rate: 3.33204e-05
	LOSS [training: 1.6075844130996686 | validation: 2.4942006059066038]
	TIME [epoch: 8.37 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.67604728281583		[learning rate: 3.326e-05]
		[batch 20/20] avg loss: 1.5350961025473133		[learning rate: 3.32e-05]
	Learning Rate: 3.31995e-05
	LOSS [training: 1.6055716926815715 | validation: 2.4909406044529234]
	TIME [epoch: 8.39 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8440365973232378		[learning rate: 3.3139e-05]
		[batch 20/20] avg loss: 1.3638980333870152		[learning rate: 3.3079e-05]
	Learning Rate: 3.3079e-05
	LOSS [training: 1.6039673153551266 | validation: 2.4963019191868874]
	TIME [epoch: 8.37 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6763703431375503		[learning rate: 3.3019e-05]
		[batch 20/20] avg loss: 1.5330398533158731		[learning rate: 3.2959e-05]
	Learning Rate: 3.2959e-05
	LOSS [training: 1.6047050982267117 | validation: 2.4963606425003286]
	TIME [epoch: 8.37 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7014553588552974		[learning rate: 3.2899e-05]
		[batch 20/20] avg loss: 1.507312278799293		[learning rate: 3.2839e-05]
	Learning Rate: 3.28394e-05
	LOSS [training: 1.604383818827295 | validation: 2.4928048622218455]
	TIME [epoch: 8.37 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5107529545315597		[learning rate: 3.278e-05]
		[batch 20/20] avg loss: 1.7041293104668989		[learning rate: 3.272e-05]
	Learning Rate: 3.27202e-05
	LOSS [training: 1.607441132499229 | validation: 2.4942731031971466]
	TIME [epoch: 8.39 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6096022924847546		[learning rate: 3.2661e-05]
		[batch 20/20] avg loss: 1.5984409772042574		[learning rate: 3.2601e-05]
	Learning Rate: 3.26014e-05
	LOSS [training: 1.6040216348445058 | validation: 2.4938435062611246]
	TIME [epoch: 8.37 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.663101722474201		[learning rate: 3.2542e-05]
		[batch 20/20] avg loss: 1.546055251301364		[learning rate: 3.2483e-05]
	Learning Rate: 3.24831e-05
	LOSS [training: 1.6045784868877824 | validation: 2.495335555982523]
	TIME [epoch: 8.36 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5296226969522708		[learning rate: 3.2424e-05]
		[batch 20/20] avg loss: 1.6882249894678547		[learning rate: 3.2365e-05]
	Learning Rate: 3.23652e-05
	LOSS [training: 1.6089238432100632 | validation: 2.5063720868687103]
	TIME [epoch: 8.37 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.626062474171387		[learning rate: 3.2306e-05]
		[batch 20/20] avg loss: 1.5909006088066777		[learning rate: 3.2248e-05]
	Learning Rate: 3.22478e-05
	LOSS [training: 1.6084815414890326 | validation: 2.497153588245416]
	TIME [epoch: 8.38 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.565421118856537		[learning rate: 3.2189e-05]
		[batch 20/20] avg loss: 1.6402912417524256		[learning rate: 3.2131e-05]
	Learning Rate: 3.21308e-05
	LOSS [training: 1.602856180304481 | validation: 2.4961255855446067]
	TIME [epoch: 8.37 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7719767089324532		[learning rate: 3.2072e-05]
		[batch 20/20] avg loss: 1.4411075893497147		[learning rate: 3.2014e-05]
	Learning Rate: 3.20142e-05
	LOSS [training: 1.6065421491410834 | validation: 2.498195253406079]
	TIME [epoch: 8.37 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5947245863538053		[learning rate: 3.1956e-05]
		[batch 20/20] avg loss: 1.618897622899953		[learning rate: 3.1898e-05]
	Learning Rate: 3.1898e-05
	LOSS [training: 1.6068111046268794 | validation: 2.4976947921689026]
	TIME [epoch: 8.37 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6164179385164783		[learning rate: 3.184e-05]
		[batch 20/20] avg loss: 1.596411736454574		[learning rate: 3.1782e-05]
	Learning Rate: 3.17822e-05
	LOSS [training: 1.6064148374855265 | validation: 2.4996731432023243]
	TIME [epoch: 8.38 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6126972037470064		[learning rate: 3.1725e-05]
		[batch 20/20] avg loss: 1.5999551535763838		[learning rate: 3.1667e-05]
	Learning Rate: 3.16669e-05
	LOSS [training: 1.6063261786616951 | validation: 2.4925750587921707]
	TIME [epoch: 8.37 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.65861793445875		[learning rate: 3.1609e-05]
		[batch 20/20] avg loss: 1.549678884062752		[learning rate: 3.1552e-05]
	Learning Rate: 3.1552e-05
	LOSS [training: 1.6041484092607512 | validation: 2.499671952076878]
	TIME [epoch: 8.36 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6340109335260593		[learning rate: 3.1495e-05]
		[batch 20/20] avg loss: 1.5736829425219006		[learning rate: 3.1437e-05]
	Learning Rate: 3.14375e-05
	LOSS [training: 1.60384693802398 | validation: 2.4949723797488836]
	TIME [epoch: 8.37 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.51171758407339		[learning rate: 3.138e-05]
		[batch 20/20] avg loss: 1.6984261030781724		[learning rate: 3.1323e-05]
	Learning Rate: 3.13234e-05
	LOSS [training: 1.605071843575781 | validation: 2.4922034258548154]
	TIME [epoch: 8.37 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7311282320677663		[learning rate: 3.1266e-05]
		[batch 20/20] avg loss: 1.4786146089514993		[learning rate: 3.121e-05]
	Learning Rate: 3.12097e-05
	LOSS [training: 1.604871420509633 | validation: 2.4971254633510775]
	TIME [epoch: 8.38 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5761856251461706		[learning rate: 3.1153e-05]
		[batch 20/20] avg loss: 1.63571414475959		[learning rate: 3.1096e-05]
	Learning Rate: 3.10964e-05
	LOSS [training: 1.6059498849528804 | validation: 2.496816646965246]
	TIME [epoch: 8.37 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7128221846573577		[learning rate: 3.104e-05]
		[batch 20/20] avg loss: 1.4945688541615778		[learning rate: 3.0984e-05]
	Learning Rate: 3.09836e-05
	LOSS [training: 1.6036955194094678 | validation: 2.4945619399586993]
	TIME [epoch: 8.36 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.766920658916972		[learning rate: 3.0927e-05]
		[batch 20/20] avg loss: 1.441859864547106		[learning rate: 3.0871e-05]
	Learning Rate: 3.08711e-05
	LOSS [training: 1.6043902617320387 | validation: 2.4934027412554975]
	TIME [epoch: 8.37 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7306374517961305		[learning rate: 3.0815e-05]
		[batch 20/20] avg loss: 1.4780286910443683		[learning rate: 3.0759e-05]
	Learning Rate: 3.07591e-05
	LOSS [training: 1.6043330714202493 | validation: 2.4968925724492186]
	TIME [epoch: 8.39 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4896726015040056		[learning rate: 3.0703e-05]
		[batch 20/20] avg loss: 1.7199664991248889		[learning rate: 3.0647e-05]
	Learning Rate: 3.06475e-05
	LOSS [training: 1.604819550314447 | validation: 2.4976462148978653]
	TIME [epoch: 8.37 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5866746679970274		[learning rate: 3.0592e-05]
		[batch 20/20] avg loss: 1.627654486845343		[learning rate: 3.0536e-05]
	Learning Rate: 3.05363e-05
	LOSS [training: 1.6071645774211851 | validation: 2.501556719521369]
	TIME [epoch: 8.37 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5998494121201052		[learning rate: 3.0481e-05]
		[batch 20/20] avg loss: 1.6143507307154603		[learning rate: 3.0425e-05]
	Learning Rate: 3.04254e-05
	LOSS [training: 1.6071000714177832 | validation: 2.4934934506456328]
	TIME [epoch: 8.37 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6940744380243262		[learning rate: 3.037e-05]
		[batch 20/20] avg loss: 1.5132652587142295		[learning rate: 3.0315e-05]
	Learning Rate: 3.0315e-05
	LOSS [training: 1.6036698483692777 | validation: 2.496566740357134]
	TIME [epoch: 8.38 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6291389105245166		[learning rate: 3.026e-05]
		[batch 20/20] avg loss: 1.57976272664909		[learning rate: 3.0205e-05]
	Learning Rate: 3.0205e-05
	LOSS [training: 1.6044508185868036 | validation: 2.4906112756160828]
	TIME [epoch: 8.37 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.563038510652707		[learning rate: 3.015e-05]
		[batch 20/20] avg loss: 1.6419070484477623		[learning rate: 3.0095e-05]
	Learning Rate: 3.00954e-05
	LOSS [training: 1.6024727795502351 | validation: 2.492182866125634]
	TIME [epoch: 8.37 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.665230027018603		[learning rate: 3.0041e-05]
		[batch 20/20] avg loss: 1.549460112257711		[learning rate: 2.9986e-05]
	Learning Rate: 2.99862e-05
	LOSS [training: 1.607345069638157 | validation: 2.4977560069235185]
	TIME [epoch: 8.37 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6912734484760006		[learning rate: 2.9932e-05]
		[batch 20/20] avg loss: 1.5145941373796101		[learning rate: 2.9877e-05]
	Learning Rate: 2.98774e-05
	LOSS [training: 1.602933792927805 | validation: 2.4936351120764177]
	TIME [epoch: 8.37 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5810467774912573		[learning rate: 2.9823e-05]
		[batch 20/20] avg loss: 1.6324783807373844		[learning rate: 2.9769e-05]
	Learning Rate: 2.97689e-05
	LOSS [training: 1.6067625791143207 | validation: 2.501446033195866]
	TIME [epoch: 8.39 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7335986140583952		[learning rate: 2.9715e-05]
		[batch 20/20] avg loss: 1.4778411317561264		[learning rate: 2.9661e-05]
	Learning Rate: 2.96609e-05
	LOSS [training: 1.605719872907261 | validation: 2.4988795770818015]
	TIME [epoch: 8.36 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.615470030910248		[learning rate: 2.9607e-05]
		[batch 20/20] avg loss: 1.598979527252308		[learning rate: 2.9553e-05]
	Learning Rate: 2.95533e-05
	LOSS [training: 1.6072247790812781 | validation: 2.495852846894829]
	TIME [epoch: 8.36 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5895971254395103		[learning rate: 2.95e-05]
		[batch 20/20] avg loss: 1.61761840897216		[learning rate: 2.9446e-05]
	Learning Rate: 2.9446e-05
	LOSS [training: 1.6036077672058346 | validation: 2.491418886008602]
	TIME [epoch: 8.37 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.723101977956469		[learning rate: 2.9393e-05]
		[batch 20/20] avg loss: 1.4877160325793655		[learning rate: 2.9339e-05]
	Learning Rate: 2.93391e-05
	LOSS [training: 1.605409005267917 | validation: 2.490052153393145]
	TIME [epoch: 8.39 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5494617638560977		[learning rate: 2.9286e-05]
		[batch 20/20] avg loss: 1.6637716919858487		[learning rate: 2.9233e-05]
	Learning Rate: 2.92327e-05
	LOSS [training: 1.6066167279209729 | validation: 2.495560561979488]
	TIME [epoch: 8.36 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.652702298300395		[learning rate: 2.918e-05]
		[batch 20/20] avg loss: 1.5545175144807075		[learning rate: 2.9127e-05]
	Learning Rate: 2.91266e-05
	LOSS [training: 1.6036099063905513 | validation: 2.4968171476118557]
	TIME [epoch: 8.37 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6482999568372545		[learning rate: 2.9074e-05]
		[batch 20/20] avg loss: 1.5630601833796782		[learning rate: 2.9021e-05]
	Learning Rate: 2.90209e-05
	LOSS [training: 1.6056800701084666 | validation: 2.4951134744747607]
	TIME [epoch: 8.37 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6404919462525722		[learning rate: 2.8968e-05]
		[batch 20/20] avg loss: 1.5694216057324548		[learning rate: 2.8916e-05]
	Learning Rate: 2.89156e-05
	LOSS [training: 1.6049567759925136 | validation: 2.495003411494737]
	TIME [epoch: 8.38 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7920772213791754		[learning rate: 2.8863e-05]
		[batch 20/20] avg loss: 1.4179310797839904		[learning rate: 2.8811e-05]
	Learning Rate: 2.88106e-05
	LOSS [training: 1.605004150581583 | validation: 2.4952673591878063]
	TIME [epoch: 8.37 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4840381207169506		[learning rate: 2.8758e-05]
		[batch 20/20] avg loss: 1.7277998144035807		[learning rate: 2.8706e-05]
	Learning Rate: 2.87061e-05
	LOSS [training: 1.6059189675602656 | validation: 2.4974114174962145]
	TIME [epoch: 8.36 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5809571775791782		[learning rate: 2.8654e-05]
		[batch 20/20] avg loss: 1.6408510349443937		[learning rate: 2.8602e-05]
	Learning Rate: 2.86019e-05
	LOSS [training: 1.6109041062617862 | validation: 2.5039023872079538]
	TIME [epoch: 8.36 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6996730233325412		[learning rate: 2.855e-05]
		[batch 20/20] avg loss: 1.518668012414643		[learning rate: 2.8498e-05]
	Learning Rate: 2.84981e-05
	LOSS [training: 1.6091705178735918 | validation: 2.497869169673865]
	TIME [epoch: 8.36 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6648712124882337		[learning rate: 2.8446e-05]
		[batch 20/20] avg loss: 1.5421969523572687		[learning rate: 2.8395e-05]
	Learning Rate: 2.83947e-05
	LOSS [training: 1.6035340824227515 | validation: 2.491427917387664]
	TIME [epoch: 8.39 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7232487878813973		[learning rate: 2.8343e-05]
		[batch 20/20] avg loss: 1.4854484248992534		[learning rate: 2.8292e-05]
	Learning Rate: 2.82916e-05
	LOSS [training: 1.6043486063903256 | validation: 2.497895041756231]
	TIME [epoch: 8.37 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7404273457890664		[learning rate: 2.824e-05]
		[batch 20/20] avg loss: 1.4660412668702347		[learning rate: 2.8189e-05]
	Learning Rate: 2.8189e-05
	LOSS [training: 1.6032343063296501 | validation: 2.4990248948238647]
	TIME [epoch: 8.37 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5854484119332592		[learning rate: 2.8138e-05]
		[batch 20/20] avg loss: 1.6297356777388476		[learning rate: 2.8087e-05]
	Learning Rate: 2.80867e-05
	LOSS [training: 1.607592044836054 | validation: 2.4922131397925495]
	TIME [epoch: 8.37 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6887479812822488		[learning rate: 2.8036e-05]
		[batch 20/20] avg loss: 1.5199696513992054		[learning rate: 2.7985e-05]
	Learning Rate: 2.79847e-05
	LOSS [training: 1.6043588163407267 | validation: 2.4944454373654876]
	TIME [epoch: 8.39 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6772786212114486		[learning rate: 2.7934e-05]
		[batch 20/20] avg loss: 1.5340936414777873		[learning rate: 2.7883e-05]
	Learning Rate: 2.78832e-05
	LOSS [training: 1.6056861313446178 | validation: 2.4922744632260496]
	TIME [epoch: 8.37 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8190693645590148		[learning rate: 2.7833e-05]
		[batch 20/20] avg loss: 1.3898861589001286		[learning rate: 2.7782e-05]
	Learning Rate: 2.7782e-05
	LOSS [training: 1.6044777617295718 | validation: 2.501903186425875]
	TIME [epoch: 8.36 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6708460221273918		[learning rate: 2.7732e-05]
		[batch 20/20] avg loss: 1.534922580457442		[learning rate: 2.7681e-05]
	Learning Rate: 2.76812e-05
	LOSS [training: 1.6028843012924168 | validation: 2.4989935669047307]
	TIME [epoch: 8.36 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5143923647257993		[learning rate: 2.7631e-05]
		[batch 20/20] avg loss: 1.6946836678869555		[learning rate: 2.7581e-05]
	Learning Rate: 2.75807e-05
	LOSS [training: 1.6045380163063772 | validation: 2.501207120752717]
	TIME [epoch: 8.39 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.705426743107686		[learning rate: 2.7531e-05]
		[batch 20/20] avg loss: 1.5071923278788544		[learning rate: 2.7481e-05]
	Learning Rate: 2.74806e-05
	LOSS [training: 1.6063095354932702 | validation: 2.5045359089590904]
	TIME [epoch: 8.37 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6736389889114256		[learning rate: 2.7431e-05]
		[batch 20/20] avg loss: 1.5385795328239091		[learning rate: 2.7381e-05]
	Learning Rate: 2.73809e-05
	LOSS [training: 1.6061092608676675 | validation: 2.49728863135165]
	TIME [epoch: 8.37 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4607628507628383		[learning rate: 2.7331e-05]
		[batch 20/20] avg loss: 1.7447377215257398		[learning rate: 2.7282e-05]
	Learning Rate: 2.72815e-05
	LOSS [training: 1.602750286144289 | validation: 2.497128845848256]
	TIME [epoch: 8.36 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6900313542019514		[learning rate: 2.7232e-05]
		[batch 20/20] avg loss: 1.5211357519729252		[learning rate: 2.7183e-05]
	Learning Rate: 2.71825e-05
	LOSS [training: 1.6055835530874383 | validation: 2.5016895804347734]
	TIME [epoch: 8.37 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5577612483489536		[learning rate: 2.7133e-05]
		[batch 20/20] avg loss: 1.6505776426897012		[learning rate: 2.7084e-05]
	Learning Rate: 2.70839e-05
	LOSS [training: 1.604169445519327 | validation: 2.4971570474611284]
	TIME [epoch: 8.39 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6452628449794946		[learning rate: 2.7035e-05]
		[batch 20/20] avg loss: 1.569667688274031		[learning rate: 2.6986e-05]
	Learning Rate: 2.69856e-05
	LOSS [training: 1.6074652666267633 | validation: 2.4961527073461336]
	TIME [epoch: 8.37 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.54355801670066		[learning rate: 2.6937e-05]
		[batch 20/20] avg loss: 1.6705249318742301		[learning rate: 2.6888e-05]
	Learning Rate: 2.68876e-05
	LOSS [training: 1.6070414742874448 | validation: 2.507185171943911]
	TIME [epoch: 8.36 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7725901913484712		[learning rate: 2.6839e-05]
		[batch 20/20] avg loss: 1.439299110258415		[learning rate: 2.679e-05]
	Learning Rate: 2.67901e-05
	LOSS [training: 1.605944650803443 | validation: 2.4944466267148053]
	TIME [epoch: 8.36 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.45213966031926		[learning rate: 2.6741e-05]
		[batch 20/20] avg loss: 1.7531923416106046		[learning rate: 2.6693e-05]
	Learning Rate: 2.66928e-05
	LOSS [training: 1.6026660009649323 | validation: 2.494938989768608]
	TIME [epoch: 8.38 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4889941781449143		[learning rate: 2.6644e-05]
		[batch 20/20] avg loss: 1.720673556746108		[learning rate: 2.6596e-05]
	Learning Rate: 2.6596e-05
	LOSS [training: 1.6048338674455116 | validation: 2.4951941140294176]
	TIME [epoch: 8.37 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5202969867523477		[learning rate: 2.6548e-05]
		[batch 20/20] avg loss: 1.686545979750091		[learning rate: 2.6499e-05]
	Learning Rate: 2.64994e-05
	LOSS [training: 1.6034214832512195 | validation: 2.4893350700720944]
	TIME [epoch: 8.36 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6307187098269391		[learning rate: 2.6451e-05]
		[batch 20/20] avg loss: 1.5790521308093926		[learning rate: 2.6403e-05]
	Learning Rate: 2.64033e-05
	LOSS [training: 1.604885420318166 | validation: 2.501711458051587]
	TIME [epoch: 8.36 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6004592365703387		[learning rate: 2.6355e-05]
		[batch 20/20] avg loss: 1.612795931692262		[learning rate: 2.6307e-05]
	Learning Rate: 2.63075e-05
	LOSS [training: 1.6066275841313005 | validation: 2.4983154527557843]
	TIME [epoch: 8.38 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.533145542577455		[learning rate: 2.626e-05]
		[batch 20/20] avg loss: 1.6749833629568793		[learning rate: 2.6212e-05]
	Learning Rate: 2.6212e-05
	LOSS [training: 1.604064452767167 | validation: 2.4971978483811332]
	TIME [epoch: 8.37 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5382681100545408		[learning rate: 2.6164e-05]
		[batch 20/20] avg loss: 1.6792954071753505		[learning rate: 2.6117e-05]
	Learning Rate: 2.61169e-05
	LOSS [training: 1.6087817586149455 | validation: 2.4939390993026596]
	TIME [epoch: 8.36 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7316072699214018		[learning rate: 2.6069e-05]
		[batch 20/20] avg loss: 1.4791843182086017		[learning rate: 2.6022e-05]
	Learning Rate: 2.60221e-05
	LOSS [training: 1.6053957940650014 | validation: 2.4945992423090577]
	TIME [epoch: 8.36 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5385872347075962		[learning rate: 2.5975e-05]
		[batch 20/20] avg loss: 1.6669927440249546		[learning rate: 2.5928e-05]
	Learning Rate: 2.59277e-05
	LOSS [training: 1.6027899893662751 | validation: 2.4934451278221057]
	TIME [epoch: 8.37 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6480224549776534		[learning rate: 2.5881e-05]
		[batch 20/20] avg loss: 1.5613331112108253		[learning rate: 2.5834e-05]
	Learning Rate: 2.58336e-05
	LOSS [training: 1.6046777830942394 | validation: 2.4969162589125213]
	TIME [epoch: 8.39 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.553121985167702		[learning rate: 2.5787e-05]
		[batch 20/20] avg loss: 1.6584912838932016		[learning rate: 2.574e-05]
	Learning Rate: 2.57398e-05
	LOSS [training: 1.6058066345304518 | validation: 2.500403642202085]
	TIME [epoch: 8.36 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6233444174212504		[learning rate: 2.5693e-05]
		[batch 20/20] avg loss: 1.5949125008927436		[learning rate: 2.5646e-05]
	Learning Rate: 2.56464e-05
	LOSS [training: 1.609128459156997 | validation: 2.499373440638736]
	TIME [epoch: 8.37 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6061108270635933		[learning rate: 2.56e-05]
		[batch 20/20] avg loss: 1.608365458874189		[learning rate: 2.5553e-05]
	Learning Rate: 2.55533e-05
	LOSS [training: 1.6072381429688911 | validation: 2.4891786950099237]
	TIME [epoch: 8.36 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4677367769691083		[learning rate: 2.5507e-05]
		[batch 20/20] avg loss: 1.7396287792774132		[learning rate: 2.5461e-05]
	Learning Rate: 2.54606e-05
	LOSS [training: 1.603682778123261 | validation: 2.4919246774387074]
	TIME [epoch: 8.39 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5873945584627642		[learning rate: 2.5414e-05]
		[batch 20/20] avg loss: 1.6214295472049822		[learning rate: 2.5368e-05]
	Learning Rate: 2.53682e-05
	LOSS [training: 1.604412052833873 | validation: 2.4938396730929195]
	TIME [epoch: 8.37 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6226706067431997		[learning rate: 2.5322e-05]
		[batch 20/20] avg loss: 1.5875012680845944		[learning rate: 2.5276e-05]
	Learning Rate: 2.52761e-05
	LOSS [training: 1.605085937413897 | validation: 2.4943604753876083]
	TIME [epoch: 8.37 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6171563707703807		[learning rate: 2.523e-05]
		[batch 20/20] avg loss: 1.5949640718851528		[learning rate: 2.5184e-05]
	Learning Rate: 2.51844e-05
	LOSS [training: 1.6060602213277668 | validation: 2.4910714285121474]
	TIME [epoch: 8.36 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6734174690061792		[learning rate: 2.5139e-05]
		[batch 20/20] avg loss: 1.5369003301429163		[learning rate: 2.5093e-05]
	Learning Rate: 2.5093e-05
	LOSS [training: 1.6051588995745476 | validation: 2.4933136635322297]
	TIME [epoch: 8.38 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6504245661743215		[learning rate: 2.5047e-05]
		[batch 20/20] avg loss: 1.559165438919217		[learning rate: 2.5002e-05]
	Learning Rate: 2.50019e-05
	LOSS [training: 1.6047950025467692 | validation: 2.4932471059926913]
	TIME [epoch: 8.37 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7073300049066433		[learning rate: 2.4957e-05]
		[batch 20/20] avg loss: 1.502261295615716		[learning rate: 2.4911e-05]
	Learning Rate: 2.49112e-05
	LOSS [training: 1.6047956502611798 | validation: 2.4878960803642896]
	TIME [epoch: 8.36 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6152652711142856		[learning rate: 2.4866e-05]
		[batch 20/20] avg loss: 1.58996150629523		[learning rate: 2.4821e-05]
	Learning Rate: 2.48208e-05
	LOSS [training: 1.6026133887047578 | validation: 2.491282621269024]
	TIME [epoch: 8.36 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5819074421357149		[learning rate: 2.4776e-05]
		[batch 20/20] avg loss: 1.630233377466611		[learning rate: 2.4731e-05]
	Learning Rate: 2.47307e-05
	LOSS [training: 1.6060704098011627 | validation: 2.4955322462344154]
	TIME [epoch: 8.37 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.601882750667346		[learning rate: 2.4686e-05]
		[batch 20/20] avg loss: 1.6132075034452178		[learning rate: 2.4641e-05]
	Learning Rate: 2.4641e-05
	LOSS [training: 1.607545127056282 | validation: 2.4954564947453686]
	TIME [epoch: 8.39 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5088542716587452		[learning rate: 2.4596e-05]
		[batch 20/20] avg loss: 1.7021129919248736		[learning rate: 2.4552e-05]
	Learning Rate: 2.45516e-05
	LOSS [training: 1.6054836317918095 | validation: 2.5031474668897986]
	TIME [epoch: 8.37 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7106204538749359		[learning rate: 2.4507e-05]
		[batch 20/20] avg loss: 1.4995621459164408		[learning rate: 2.4462e-05]
	Learning Rate: 2.44625e-05
	LOSS [training: 1.605091299895688 | validation: 2.4898755948889586]
	TIME [epoch: 8.37 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6626137316348228		[learning rate: 2.4418e-05]
		[batch 20/20] avg loss: 1.5483881342114807		[learning rate: 2.4374e-05]
	Learning Rate: 2.43737e-05
	LOSS [training: 1.6055009329231513 | validation: 2.4962382048017915]
	TIME [epoch: 8.36 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6046015403070766		[learning rate: 2.4329e-05]
		[batch 20/20] avg loss: 1.6058385521524037		[learning rate: 2.4285e-05]
	Learning Rate: 2.42852e-05
	LOSS [training: 1.6052200462297403 | validation: 2.4986176383448364]
	TIME [epoch: 8.38 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5236507959621688		[learning rate: 2.4241e-05]
		[batch 20/20] avg loss: 1.6863727312580163		[learning rate: 2.4197e-05]
	Learning Rate: 2.41971e-05
	LOSS [training: 1.6050117636100925 | validation: 2.4943016727705705]
	TIME [epoch: 8.37 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6691255647946779		[learning rate: 2.4153e-05]
		[batch 20/20] avg loss: 1.5437858148973536		[learning rate: 2.4109e-05]
	Learning Rate: 2.41093e-05
	LOSS [training: 1.6064556898460158 | validation: 2.497169845211492]
	TIME [epoch: 8.36 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.523133685109044		[learning rate: 2.4065e-05]
		[batch 20/20] avg loss: 1.6865616932330567		[learning rate: 2.4022e-05]
	Learning Rate: 2.40218e-05
	LOSS [training: 1.6048476891710504 | validation: 2.497170162862089]
	TIME [epoch: 8.37 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5387334524153125		[learning rate: 2.3978e-05]
		[batch 20/20] avg loss: 1.6783324858355648		[learning rate: 2.3935e-05]
	Learning Rate: 2.39346e-05
	LOSS [training: 1.6085329691254386 | validation: 2.497651845897843]
	TIME [epoch: 8.39 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6769305183195578		[learning rate: 2.3891e-05]
		[batch 20/20] avg loss: 1.5354937670617035		[learning rate: 2.3848e-05]
	Learning Rate: 2.38477e-05
	LOSS [training: 1.6062121426906308 | validation: 2.49073898101883]
	TIME [epoch: 8.37 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5733985088887001		[learning rate: 2.3804e-05]
		[batch 20/20] avg loss: 1.6399236089591913		[learning rate: 2.3761e-05]
	Learning Rate: 2.37612e-05
	LOSS [training: 1.6066610589239456 | validation: 2.496473113826167]
	TIME [epoch: 8.37 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5997829118938625		[learning rate: 2.3718e-05]
		[batch 20/20] avg loss: 1.6123212073795667		[learning rate: 2.3675e-05]
	Learning Rate: 2.3675e-05
	LOSS [training: 1.6060520596367145 | validation: 2.4985047665842988]
	TIME [epoch: 8.37 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6373593846633245		[learning rate: 2.3632e-05]
		[batch 20/20] avg loss: 1.5704631365796486		[learning rate: 2.3589e-05]
	Learning Rate: 2.35891e-05
	LOSS [training: 1.6039112606214867 | validation: 2.5020187609951394]
	TIME [epoch: 8.37 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4967509087857942		[learning rate: 2.3546e-05]
		[batch 20/20] avg loss: 1.7155469706595583		[learning rate: 2.3503e-05]
	Learning Rate: 2.35034e-05
	LOSS [training: 1.6061489397226762 | validation: 2.4970704316678134]
	TIME [epoch: 8.38 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5865073792375508		[learning rate: 2.3461e-05]
		[batch 20/20] avg loss: 1.6224396735869966		[learning rate: 2.3418e-05]
	Learning Rate: 2.34182e-05
	LOSS [training: 1.6044735264122738 | validation: 2.496942192681874]
	TIME [epoch: 8.36 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.556341929466838		[learning rate: 2.3376e-05]
		[batch 20/20] avg loss: 1.654476965634045		[learning rate: 2.3333e-05]
	Learning Rate: 2.33332e-05
	LOSS [training: 1.6054094475504417 | validation: 2.495822382487929]
	TIME [epoch: 8.36 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6843615371374743		[learning rate: 2.3291e-05]
		[batch 20/20] avg loss: 1.527187544506138		[learning rate: 2.3248e-05]
	Learning Rate: 2.32485e-05
	LOSS [training: 1.605774540821806 | validation: 2.4922128095713285]
	TIME [epoch: 8.37 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5904010373933206		[learning rate: 2.3206e-05]
		[batch 20/20] avg loss: 1.6179213988513865		[learning rate: 2.3164e-05]
	Learning Rate: 2.31641e-05
	LOSS [training: 1.6041612181223535 | validation: 2.4974086361321306]
	TIME [epoch: 8.38 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6756258028730808		[learning rate: 2.3122e-05]
		[batch 20/20] avg loss: 1.5341532823242507		[learning rate: 2.308e-05]
	Learning Rate: 2.30801e-05
	LOSS [training: 1.6048895425986658 | validation: 2.4974814223531703]
	TIME [epoch: 8.37 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6189456452695776		[learning rate: 2.3038e-05]
		[batch 20/20] avg loss: 1.588655871754758		[learning rate: 2.2996e-05]
	Learning Rate: 2.29963e-05
	LOSS [training: 1.6038007585121679 | validation: 2.4969388258245973]
	TIME [epoch: 8.37 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5687559358173793		[learning rate: 2.2955e-05]
		[batch 20/20] avg loss: 1.6427381915135466		[learning rate: 2.2913e-05]
	Learning Rate: 2.29128e-05
	LOSS [training: 1.6057470636654632 | validation: 2.495937934964095]
	TIME [epoch: 8.36 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5407850799031957		[learning rate: 2.2871e-05]
		[batch 20/20] avg loss: 1.6665920406570038		[learning rate: 2.283e-05]
	Learning Rate: 2.28297e-05
	LOSS [training: 1.6036885602800997 | validation: 2.4921136277492564]
	TIME [epoch: 8.39 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5921300252880752		[learning rate: 2.2788e-05]
		[batch 20/20] avg loss: 1.6187526190945356		[learning rate: 2.2747e-05]
	Learning Rate: 2.27468e-05
	LOSS [training: 1.605441322191305 | validation: 2.4927251109407997]
	TIME [epoch: 8.36 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.590738433726786		[learning rate: 2.2706e-05]
		[batch 20/20] avg loss: 1.6149234456455208		[learning rate: 2.2664e-05]
	Learning Rate: 2.26643e-05
	LOSS [training: 1.6028309396861538 | validation: 2.4902696123225234]
	TIME [epoch: 8.37 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.696168370073201		[learning rate: 2.2623e-05]
		[batch 20/20] avg loss: 1.5144421214329524		[learning rate: 2.2582e-05]
	Learning Rate: 2.2582e-05
	LOSS [training: 1.6053052457530765 | validation: 2.497140984744188]
	TIME [epoch: 8.36 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6641742188656654		[learning rate: 2.2541e-05]
		[batch 20/20] avg loss: 1.5491137856212787		[learning rate: 2.25e-05]
	Learning Rate: 2.25001e-05
	LOSS [training: 1.6066440022434718 | validation: 2.4940645678090765]
	TIME [epoch: 8.38 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6390504071080947		[learning rate: 2.2459e-05]
		[batch 20/20] avg loss: 1.5737783173410531		[learning rate: 2.2418e-05]
	Learning Rate: 2.24184e-05
	LOSS [training: 1.6064143622245741 | validation: 2.4966118051265753]
	TIME [epoch: 8.38 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.446788833502221		[learning rate: 2.2378e-05]
		[batch 20/20] avg loss: 1.7597899831512904		[learning rate: 2.2337e-05]
	Learning Rate: 2.23371e-05
	LOSS [training: 1.6032894083267557 | validation: 2.4985288998973405]
	TIME [epoch: 8.37 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5546258586186852		[learning rate: 2.2297e-05]
		[batch 20/20] avg loss: 1.6535836088593645		[learning rate: 2.2256e-05]
	Learning Rate: 2.2256e-05
	LOSS [training: 1.6041047337390246 | validation: 2.4943361647588036]
	TIME [epoch: 8.37 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5555849446031207		[learning rate: 2.2216e-05]
		[batch 20/20] avg loss: 1.65764422540142		[learning rate: 2.2175e-05]
	Learning Rate: 2.21753e-05
	LOSS [training: 1.6066145850022704 | validation: 2.495357838953924]
	TIME [epoch: 8.37 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6740303098450178		[learning rate: 2.2135e-05]
		[batch 20/20] avg loss: 1.5354504372517743		[learning rate: 2.2095e-05]
	Learning Rate: 2.20948e-05
	LOSS [training: 1.6047403735483958 | validation: 2.503599827607638]
	TIME [epoch: 8.39 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6460030720510084		[learning rate: 2.2055e-05]
		[batch 20/20] avg loss: 1.5621601343647966		[learning rate: 2.2015e-05]
	Learning Rate: 2.20146e-05
	LOSS [training: 1.6040816032079026 | validation: 2.500349799959968]
	TIME [epoch: 8.37 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4902666431890768		[learning rate: 2.1975e-05]
		[batch 20/20] avg loss: 1.7196532774920499		[learning rate: 2.1935e-05]
	Learning Rate: 2.19347e-05
	LOSS [training: 1.6049599603405635 | validation: 2.4963394014228637]
	TIME [epoch: 8.37 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5779221037488105		[learning rate: 2.1895e-05]
		[batch 20/20] avg loss: 1.635772806938056		[learning rate: 2.1855e-05]
	Learning Rate: 2.18551e-05
	LOSS [training: 1.606847455343433 | validation: 2.497009896148356]
	TIME [epoch: 8.37 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.71663550012159		[learning rate: 2.1815e-05]
		[batch 20/20] avg loss: 1.494372433987923		[learning rate: 2.1776e-05]
	Learning Rate: 2.17758e-05
	LOSS [training: 1.6055039670547564 | validation: 2.4943116866905]
	TIME [epoch: 8.39 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5085123108157963		[learning rate: 2.1736e-05]
		[batch 20/20] avg loss: 1.7074880680605649		[learning rate: 2.1697e-05]
	Learning Rate: 2.16968e-05
	LOSS [training: 1.6080001894381806 | validation: 2.491162442707132]
	TIME [epoch: 8.37 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5436478189783576		[learning rate: 2.1657e-05]
		[batch 20/20] avg loss: 1.6675731686826176		[learning rate: 2.1618e-05]
	Learning Rate: 2.1618e-05
	LOSS [training: 1.6056104938304876 | validation: 2.4978866949779035]
	TIME [epoch: 8.37 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5889097072949239		[learning rate: 2.1579e-05]
		[batch 20/20] avg loss: 1.6227004810228003		[learning rate: 2.154e-05]
	Learning Rate: 2.15396e-05
	LOSS [training: 1.6058050941588622 | validation: 2.4920051419159357]
	TIME [epoch: 8.36 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5671392970261788		[learning rate: 2.15e-05]
		[batch 20/20] avg loss: 1.6418970435505003		[learning rate: 2.1461e-05]
	Learning Rate: 2.14614e-05
	LOSS [training: 1.6045181702883398 | validation: 2.493480308842406]
	TIME [epoch: 8.39 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.665442921931176		[learning rate: 2.1422e-05]
		[batch 20/20] avg loss: 1.5429344103204277		[learning rate: 2.1384e-05]
	Learning Rate: 2.13835e-05
	LOSS [training: 1.6041886661258016 | validation: 2.4967230054925675]
	TIME [epoch: 8.37 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7425541901476915		[learning rate: 2.1345e-05]
		[batch 20/20] avg loss: 1.4619581175263907		[learning rate: 2.1306e-05]
	Learning Rate: 2.13059e-05
	LOSS [training: 1.602256153837041 | validation: 2.494064993774461]
	TIME [epoch: 8.37 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5389909363808838		[learning rate: 2.1267e-05]
		[batch 20/20] avg loss: 1.6659766844946613		[learning rate: 2.1229e-05]
	Learning Rate: 2.12286e-05
	LOSS [training: 1.602483810437772 | validation: 2.4990658682625035]
	TIME [epoch: 8.37 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.570404620750254		[learning rate: 2.119e-05]
		[batch 20/20] avg loss: 1.6390550437167786		[learning rate: 2.1152e-05]
	Learning Rate: 2.11515e-05
	LOSS [training: 1.6047298322335162 | validation: 2.498482946652866]
	TIME [epoch: 8.37 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.470065052450667		[learning rate: 2.1113e-05]
		[batch 20/20] avg loss: 1.7402994572578308		[learning rate: 2.1075e-05]
	Learning Rate: 2.10748e-05
	LOSS [training: 1.605182254854249 | validation: 2.4962001313313626]
	TIME [epoch: 8.39 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5644985262596225		[learning rate: 2.1037e-05]
		[batch 20/20] avg loss: 1.6367218298054673		[learning rate: 2.0998e-05]
	Learning Rate: 2.09983e-05
	LOSS [training: 1.6006101780325452 | validation: 2.4929700525049774]
	TIME [epoch: 8.37 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7229889662899864		[learning rate: 2.096e-05]
		[batch 20/20] avg loss: 1.4878216282019925		[learning rate: 2.0922e-05]
	Learning Rate: 2.09221e-05
	LOSS [training: 1.6054052972459893 | validation: 2.4875437150998203]
	TIME [epoch: 8.37 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5703513353655911		[learning rate: 2.0884e-05]
		[batch 20/20] avg loss: 1.633979948208772		[learning rate: 2.0846e-05]
	Learning Rate: 2.08462e-05
	LOSS [training: 1.6021656417871817 | validation: 2.493320835074524]
	TIME [epoch: 8.36 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5763930889443776		[learning rate: 2.0808e-05]
		[batch 20/20] avg loss: 1.6301732829196403		[learning rate: 2.0771e-05]
	Learning Rate: 2.07705e-05
	LOSS [training: 1.6032831859320091 | validation: 2.4968675329552643]
	TIME [epoch: 8.38 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5829447222231425		[learning rate: 2.0733e-05]
		[batch 20/20] avg loss: 1.6321920960897771		[learning rate: 2.0695e-05]
	Learning Rate: 2.06951e-05
	LOSS [training: 1.60756840915646 | validation: 2.4966440952797457]
	TIME [epoch: 8.37 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6693475765562116		[learning rate: 2.0658e-05]
		[batch 20/20] avg loss: 1.5382832102418849		[learning rate: 2.062e-05]
	Learning Rate: 2.062e-05
	LOSS [training: 1.6038153933990482 | validation: 2.496772630830945]
	TIME [epoch: 8.36 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4861605387169454		[learning rate: 2.0583e-05]
		[batch 20/20] avg loss: 1.722593911028025		[learning rate: 2.0545e-05]
	Learning Rate: 2.05452e-05
	LOSS [training: 1.6043772248724852 | validation: 2.496478527687587]
	TIME [epoch: 8.37 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6075749376534116		[learning rate: 2.0508e-05]
		[batch 20/20] avg loss: 1.6037551062621602		[learning rate: 2.0471e-05]
	Learning Rate: 2.04706e-05
	LOSS [training: 1.6056650219577862 | validation: 2.496681284399598]
	TIME [epoch: 8.39 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6970682006639362		[learning rate: 2.0433e-05]
		[batch 20/20] avg loss: 1.511619875844459		[learning rate: 2.0396e-05]
	Learning Rate: 2.03964e-05
	LOSS [training: 1.6043440382541978 | validation: 2.4912007388477253]
	TIME [epoch: 8.37 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.65075186589543		[learning rate: 2.0359e-05]
		[batch 20/20] avg loss: 1.558187426776989		[learning rate: 2.0322e-05]
	Learning Rate: 2.03223e-05
	LOSS [training: 1.6044696463362094 | validation: 2.496444789698384]
	TIME [epoch: 8.37 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7038168438510812		[learning rate: 2.0285e-05]
		[batch 20/20] avg loss: 1.512829753155691		[learning rate: 2.0249e-05]
	Learning Rate: 2.02486e-05
	LOSS [training: 1.6083232985033855 | validation: 2.492949299046829]
	TIME [epoch: 8.36 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6796817190984235		[learning rate: 2.0212e-05]
		[batch 20/20] avg loss: 1.5251910404567937		[learning rate: 2.0175e-05]
	Learning Rate: 2.01751e-05
	LOSS [training: 1.6024363797776087 | validation: 2.4971559398257295]
	TIME [epoch: 8.37 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6372353672972786		[learning rate: 2.0138e-05]
		[batch 20/20] avg loss: 1.5709597026623765		[learning rate: 2.0102e-05]
	Learning Rate: 2.01019e-05
	LOSS [training: 1.6040975349798277 | validation: 2.493332642655462]
	TIME [epoch: 8.39 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7421394240086194		[learning rate: 2.0065e-05]
		[batch 20/20] avg loss: 1.4680308270313223		[learning rate: 2.0029e-05]
	Learning Rate: 2.00289e-05
	LOSS [training: 1.605085125519971 | validation: 2.499064523922729]
	TIME [epoch: 8.37 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5018369458549017		[learning rate: 1.9993e-05]
		[batch 20/20] avg loss: 1.707132307343312		[learning rate: 1.9956e-05]
	Learning Rate: 1.99563e-05
	LOSS [training: 1.6044846265991073 | validation: 2.4995805273775344]
	TIME [epoch: 8.37 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.723240428432785		[learning rate: 1.992e-05]
		[batch 20/20] avg loss: 1.4834357088442194		[learning rate: 1.9884e-05]
	Learning Rate: 1.98838e-05
	LOSS [training: 1.603338068638502 | validation: 2.4904047260512057]
	TIME [epoch: 8.37 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5882643845515785		[learning rate: 1.9848e-05]
		[batch 20/20] avg loss: 1.6172733908658614		[learning rate: 1.9812e-05]
	Learning Rate: 1.98117e-05
	LOSS [training: 1.6027688877087196 | validation: 2.4948574287842584]
	TIME [epoch: 8.39 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.462969649090725		[learning rate: 1.9776e-05]
		[batch 20/20] avg loss: 1.7483449041504542		[learning rate: 1.974e-05]
	Learning Rate: 1.97398e-05
	LOSS [training: 1.6056572766205892 | validation: 2.4991025663267035]
	TIME [epoch: 8.37 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5899290833793267		[learning rate: 1.9704e-05]
		[batch 20/20] avg loss: 1.6212571492436414		[learning rate: 1.9668e-05]
	Learning Rate: 1.96681e-05
	LOSS [training: 1.6055931163114843 | validation: 2.495845301112824]
	TIME [epoch: 8.36 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5022885135548678		[learning rate: 1.9632e-05]
		[batch 20/20] avg loss: 1.7083371982431785		[learning rate: 1.9597e-05]
	Learning Rate: 1.95968e-05
	LOSS [training: 1.605312855899023 | validation: 2.499772562735282]
	TIME [epoch: 8.37 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5522831747903605		[learning rate: 1.9561e-05]
		[batch 20/20] avg loss: 1.6589752664401325		[learning rate: 1.9526e-05]
	Learning Rate: 1.95256e-05
	LOSS [training: 1.6056292206152467 | validation: 2.5020671881567838]
	TIME [epoch: 8.39 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5918405748646254		[learning rate: 1.949e-05]
		[batch 20/20] avg loss: 1.620372343725326		[learning rate: 1.9455e-05]
	Learning Rate: 1.94548e-05
	LOSS [training: 1.6061064592949759 | validation: 2.4938700876229367]
	TIME [epoch: 8.37 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6860520098801413		[learning rate: 1.9419e-05]
		[batch 20/20] avg loss: 1.5218725209079644		[learning rate: 1.9384e-05]
	Learning Rate: 1.93842e-05
	LOSS [training: 1.603962265394053 | validation: 2.4972680794091455]
	TIME [epoch: 8.37 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.587214348317088		[learning rate: 1.9349e-05]
		[batch 20/20] avg loss: 1.6177017303413979		[learning rate: 1.9314e-05]
	Learning Rate: 1.93138e-05
	LOSS [training: 1.602458039329243 | validation: 2.493423824241]
	TIME [epoch: 8.36 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.653548378443227		[learning rate: 1.9279e-05]
		[batch 20/20] avg loss: 1.5619279405909503		[learning rate: 1.9244e-05]
	Learning Rate: 1.92437e-05
	LOSS [training: 1.6077381595170883 | validation: 2.495387125887939]
	TIME [epoch: 8.37 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4389774116517735		[learning rate: 1.9209e-05]
		[batch 20/20] avg loss: 1.7690909099126333		[learning rate: 1.9174e-05]
	Learning Rate: 1.91739e-05
	LOSS [training: 1.6040341607822033 | validation: 2.493433233256631]
	TIME [epoch: 8.38 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6066304566595384		[learning rate: 1.9139e-05]
		[batch 20/20] avg loss: 1.5993671689304487		[learning rate: 1.9104e-05]
	Learning Rate: 1.91043e-05
	LOSS [training: 1.6029988127949935 | validation: 2.4981516688971848]
	TIME [epoch: 8.36 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5843938729394178		[learning rate: 1.907e-05]
		[batch 20/20] avg loss: 1.6245026271541305		[learning rate: 1.9035e-05]
	Learning Rate: 1.9035e-05
	LOSS [training: 1.6044482500467745 | validation: 2.4962666322697924]
	TIME [epoch: 8.37 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6158150555134607		[learning rate: 1.9e-05]
		[batch 20/20] avg loss: 1.5920233834263242		[learning rate: 1.8966e-05]
	Learning Rate: 1.89659e-05
	LOSS [training: 1.6039192194698928 | validation: 2.4914746713385556]
	TIME [epoch: 8.36 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5077765086772783		[learning rate: 1.8931e-05]
		[batch 20/20] avg loss: 1.6974663076522674		[learning rate: 1.8897e-05]
	Learning Rate: 1.88971e-05
	LOSS [training: 1.6026214081647727 | validation: 2.4921666160843547]
	TIME [epoch: 8.39 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5805167237085462		[learning rate: 1.8863e-05]
		[batch 20/20] avg loss: 1.6262478377648648		[learning rate: 1.8829e-05]
	Learning Rate: 1.88285e-05
	LOSS [training: 1.6033822807367055 | validation: 2.497815869177058]
	TIME [epoch: 8.37 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5877592385861048		[learning rate: 1.8794e-05]
		[batch 20/20] avg loss: 1.6201055698412525		[learning rate: 1.876e-05]
	Learning Rate: 1.87602e-05
	LOSS [training: 1.6039324042136784 | validation: 2.4907028799806348]
	TIME [epoch: 8.36 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6566794099251159		[learning rate: 1.8726e-05]
		[batch 20/20] avg loss: 1.5520542705559024		[learning rate: 1.8692e-05]
	Learning Rate: 1.86921e-05
	LOSS [training: 1.6043668402405085 | validation: 2.4983003561302186]
	TIME [epoch: 8.36 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5588820555605813		[learning rate: 1.8658e-05]
		[batch 20/20] avg loss: 1.648256197568291		[learning rate: 1.8624e-05]
	Learning Rate: 1.86243e-05
	LOSS [training: 1.603569126564436 | validation: 2.4941802902630075]
	TIME [epoch: 8.38 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.476662799202526		[learning rate: 1.859e-05]
		[batch 20/20] avg loss: 1.7355450036439002		[learning rate: 1.8557e-05]
	Learning Rate: 1.85567e-05
	LOSS [training: 1.606103901423213 | validation: 2.4957229894937454]
	TIME [epoch: 8.37 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5423908682714782		[learning rate: 1.8523e-05]
		[batch 20/20] avg loss: 1.6686695591666407		[learning rate: 1.8489e-05]
	Learning Rate: 1.84893e-05
	LOSS [training: 1.6055302137190595 | validation: 2.502030937934751]
	TIME [epoch: 8.37 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6687667110936277		[learning rate: 1.8456e-05]
		[batch 20/20] avg loss: 1.541947839480673		[learning rate: 1.8422e-05]
	Learning Rate: 1.84222e-05
	LOSS [training: 1.6053572752871503 | validation: 2.494465181694543]
	TIME [epoch: 8.37 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5850473228883348		[learning rate: 1.8389e-05]
		[batch 20/20] avg loss: 1.6269026717710016		[learning rate: 1.8355e-05]
	Learning Rate: 1.83554e-05
	LOSS [training: 1.605974997329668 | validation: 2.49546680081448]
	TIME [epoch: 8.36 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5179366311154108		[learning rate: 1.8322e-05]
		[batch 20/20] avg loss: 1.6915481167608637		[learning rate: 1.8289e-05]
	Learning Rate: 1.82888e-05
	LOSS [training: 1.6047423739381377 | validation: 2.4955312960348284]
	TIME [epoch: 8.39 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.598651977915824		[learning rate: 1.8256e-05]
		[batch 20/20] avg loss: 1.6101703189294105		[learning rate: 1.8222e-05]
	Learning Rate: 1.82224e-05
	LOSS [training: 1.6044111484226171 | validation: 2.4953659873227974]
	TIME [epoch: 8.37 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8509695455684496		[learning rate: 1.8189e-05]
		[batch 20/20] avg loss: 1.3609285226705417		[learning rate: 1.8156e-05]
	Learning Rate: 1.81563e-05
	LOSS [training: 1.6059490341194955 | validation: 2.4953384465986255]
	TIME [epoch: 8.37 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3912213812162957		[learning rate: 1.8123e-05]
		[batch 20/20] avg loss: 1.8191466486413286		[learning rate: 1.809e-05]
	Learning Rate: 1.80904e-05
	LOSS [training: 1.6051840149288121 | validation: 2.495533912679643]
	TIME [epoch: 8.37 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5147634042353102		[learning rate: 1.8058e-05]
		[batch 20/20] avg loss: 1.6998877241767971		[learning rate: 1.8025e-05]
	Learning Rate: 1.80247e-05
	LOSS [training: 1.6073255642060538 | validation: 2.49284117786443]
	TIME [epoch: 8.39 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6486778026108773		[learning rate: 1.7992e-05]
		[batch 20/20] avg loss: 1.5601666247878607		[learning rate: 1.7959e-05]
	Learning Rate: 1.79593e-05
	LOSS [training: 1.6044222136993689 | validation: 2.4896135018693712]
	TIME [epoch: 8.37 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6210138957708673		[learning rate: 1.7927e-05]
		[batch 20/20] avg loss: 1.5899375798965072		[learning rate: 1.7894e-05]
	Learning Rate: 1.78941e-05
	LOSS [training: 1.6054757378336872 | validation: 2.4956715756724135]
	TIME [epoch: 8.37 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6152088084075373		[learning rate: 1.7862e-05]
		[batch 20/20] avg loss: 1.5920373721759993		[learning rate: 1.7829e-05]
	Learning Rate: 1.78292e-05
	LOSS [training: 1.603623090291768 | validation: 2.489231176762669]
	TIME [epoch: 8.36 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7376092485096195		[learning rate: 1.7797e-05]
		[batch 20/20] avg loss: 1.4709329324179308		[learning rate: 1.7764e-05]
	Learning Rate: 1.77645e-05
	LOSS [training: 1.6042710904637751 | validation: 2.495470800811092]
	TIME [epoch: 8.38 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.49127775693439		[learning rate: 1.7732e-05]
		[batch 20/20] avg loss: 1.7153114424942584		[learning rate: 1.77e-05]
	Learning Rate: 1.77e-05
	LOSS [training: 1.6032945997143244 | validation: 2.496454888339261]
	TIME [epoch: 8.37 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6422160585008743		[learning rate: 1.7668e-05]
		[batch 20/20] avg loss: 1.5672944797296249		[learning rate: 1.7636e-05]
	Learning Rate: 1.76358e-05
	LOSS [training: 1.6047552691152496 | validation: 2.490993981464989]
	TIME [epoch: 8.36 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5616541953962226		[learning rate: 1.7604e-05]
		[batch 20/20] avg loss: 1.6478501067470739		[learning rate: 1.7572e-05]
	Learning Rate: 1.75718e-05
	LOSS [training: 1.6047521510716478 | validation: 2.495739198088174]
	TIME [epoch: 8.37 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5928442863834575		[learning rate: 1.754e-05]
		[batch 20/20] avg loss: 1.6206747143562155		[learning rate: 1.7508e-05]
	Learning Rate: 1.7508e-05
	LOSS [training: 1.6067595003698365 | validation: 2.497072732260979]
	TIME [epoch: 8.36 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6882940064314471		[learning rate: 1.7476e-05]
		[batch 20/20] avg loss: 1.5194398595587935		[learning rate: 1.7444e-05]
	Learning Rate: 1.74445e-05
	LOSS [training: 1.60386693299512 | validation: 2.493646985023012]
	TIME [epoch: 8.36 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4629039898263936		[learning rate: 1.7413e-05]
		[batch 20/20] avg loss: 1.7507391068946414		[learning rate: 1.7381e-05]
	Learning Rate: 1.73812e-05
	LOSS [training: 1.6068215483605175 | validation: 2.4963632785409775]
	TIME [epoch: 8.39 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5458000668779532		[learning rate: 1.735e-05]
		[batch 20/20] avg loss: 1.660080691303401		[learning rate: 1.7318e-05]
	Learning Rate: 1.73181e-05
	LOSS [training: 1.6029403790906769 | validation: 2.4990276368345787]
	TIME [epoch: 8.39 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6365599908903157		[learning rate: 1.7287e-05]
		[batch 20/20] avg loss: 1.576669745231778		[learning rate: 1.7255e-05]
	Learning Rate: 1.72552e-05
	LOSS [training: 1.6066148680610468 | validation: 2.4925799336105823]
	TIME [epoch: 8.4 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6089372781377822		[learning rate: 1.7224e-05]
		[batch 20/20] avg loss: 1.6020297023299737		[learning rate: 1.7193e-05]
	Learning Rate: 1.71926e-05
	LOSS [training: 1.605483490233878 | validation: 2.491853974964933]
	TIME [epoch: 8.42 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5637451779903924		[learning rate: 1.7161e-05]
		[batch 20/20] avg loss: 1.6428302610433936		[learning rate: 1.713e-05]
	Learning Rate: 1.71302e-05
	LOSS [training: 1.6032877195168929 | validation: 2.4968909475329477]
	TIME [epoch: 8.4 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4631665183714624		[learning rate: 1.7099e-05]
		[batch 20/20] avg loss: 1.7405768125582974		[learning rate: 1.7068e-05]
	Learning Rate: 1.70681e-05
	LOSS [training: 1.60187166546488 | validation: 2.489147931168544]
	TIME [epoch: 8.4 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6024571716779725		[learning rate: 1.7037e-05]
		[batch 20/20] avg loss: 1.607517962120087		[learning rate: 1.7006e-05]
	Learning Rate: 1.70061e-05
	LOSS [training: 1.6049875668990297 | validation: 2.4899779225439174]
	TIME [epoch: 8.4 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3896798730632731		[learning rate: 1.6975e-05]
		[batch 20/20] avg loss: 1.818900952310813		[learning rate: 1.6944e-05]
	Learning Rate: 1.69444e-05
	LOSS [training: 1.6042904126870432 | validation: 2.494078802761256]
	TIME [epoch: 8.43 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5147168608903558		[learning rate: 1.6914e-05]
		[batch 20/20] avg loss: 1.6938258411305278		[learning rate: 1.6883e-05]
	Learning Rate: 1.68829e-05
	LOSS [training: 1.6042713510104416 | validation: 2.490406735552811]
	TIME [epoch: 8.41 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4449093781088478		[learning rate: 1.6852e-05]
		[batch 20/20] avg loss: 1.761697778731887		[learning rate: 1.6822e-05]
	Learning Rate: 1.68216e-05
	LOSS [training: 1.6033035784203673 | validation: 2.496328820886023]
	TIME [epoch: 8.4 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5533088168682356		[learning rate: 1.6791e-05]
		[batch 20/20] avg loss: 1.6522030776843082		[learning rate: 1.6761e-05]
	Learning Rate: 1.67606e-05
	LOSS [training: 1.6027559472762718 | validation: 2.49628966512953]
	TIME [epoch: 8.41 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.612935799041623		[learning rate: 1.673e-05]
		[batch 20/20] avg loss: 1.589937000594349		[learning rate: 1.67e-05]
	Learning Rate: 1.66998e-05
	LOSS [training: 1.6014363998179857 | validation: 2.4941969689569747]
	TIME [epoch: 8.43 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.617111719674849		[learning rate: 1.6669e-05]
		[batch 20/20] avg loss: 1.5909698544433155		[learning rate: 1.6639e-05]
	Learning Rate: 1.66392e-05
	LOSS [training: 1.6040407870590823 | validation: 2.4985282670113085]
	TIME [epoch: 8.41 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.647050501243245		[learning rate: 1.6609e-05]
		[batch 20/20] avg loss: 1.5571835225653947		[learning rate: 1.6579e-05]
	Learning Rate: 1.65788e-05
	LOSS [training: 1.6021170119043198 | validation: 2.497561884902506]
	TIME [epoch: 8.4 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3921108353040457		[learning rate: 1.6549e-05]
		[batch 20/20] avg loss: 1.8200305472494425		[learning rate: 1.6519e-05]
	Learning Rate: 1.65186e-05
	LOSS [training: 1.6060706912767444 | validation: 2.497487813638184]
	TIME [epoch: 8.41 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5306416519553816		[learning rate: 1.6489e-05]
		[batch 20/20] avg loss: 1.6811080568520507		[learning rate: 1.6459e-05]
	Learning Rate: 1.64587e-05
	LOSS [training: 1.6058748544037158 | validation: 2.4904088108457185]
	TIME [epoch: 8.41 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6587208757166931		[learning rate: 1.6429e-05]
		[batch 20/20] avg loss: 1.5499131820211844		[learning rate: 1.6399e-05]
	Learning Rate: 1.63989e-05
	LOSS [training: 1.6043170288689392 | validation: 2.4945036978950452]
	TIME [epoch: 8.43 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.552696405172246		[learning rate: 1.6369e-05]
		[batch 20/20] avg loss: 1.6511056506973165		[learning rate: 1.6339e-05]
	Learning Rate: 1.63394e-05
	LOSS [training: 1.6019010279347818 | validation: 2.4914715690368223]
	TIME [epoch: 8.39 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6609046514170174		[learning rate: 1.631e-05]
		[batch 20/20] avg loss: 1.5485105652003126		[learning rate: 1.628e-05]
	Learning Rate: 1.62801e-05
	LOSS [training: 1.6047076083086647 | validation: 2.4944501739624316]
	TIME [epoch: 8.37 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6489786005975895		[learning rate: 1.6251e-05]
		[batch 20/20] avg loss: 1.55920507785939		[learning rate: 1.6221e-05]
	Learning Rate: 1.62211e-05
	LOSS [training: 1.6040918392284897 | validation: 2.4995924124957645]
	TIME [epoch: 8.4 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6968037000265723		[learning rate: 1.6192e-05]
		[batch 20/20] avg loss: 1.5122040703493158		[learning rate: 1.6162e-05]
	Learning Rate: 1.61622e-05
	LOSS [training: 1.6045038851879443 | validation: 2.4966799004956406]
	TIME [epoch: 8.43 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4967695585369654		[learning rate: 1.6133e-05]
		[batch 20/20] avg loss: 1.7129021123823756		[learning rate: 1.6104e-05]
	Learning Rate: 1.61035e-05
	LOSS [training: 1.6048358354596703 | validation: 2.495668178054494]
	TIME [epoch: 8.41 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4288496151704901		[learning rate: 1.6074e-05]
		[batch 20/20] avg loss: 1.776126561174652		[learning rate: 1.6045e-05]
	Learning Rate: 1.60451e-05
	LOSS [training: 1.6024880881725712 | validation: 2.4957996695069804]
	TIME [epoch: 8.41 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6905510581512082		[learning rate: 1.6016e-05]
		[batch 20/20] avg loss: 1.5180445783794911		[learning rate: 1.5987e-05]
	Learning Rate: 1.59869e-05
	LOSS [training: 1.6042978182653496 | validation: 2.4959294556448994]
	TIME [epoch: 8.39 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7735164781450674		[learning rate: 1.5958e-05]
		[batch 20/20] avg loss: 1.4357705725353667		[learning rate: 1.5929e-05]
	Learning Rate: 1.59288e-05
	LOSS [training: 1.6046435253402165 | validation: 2.494592827775877]
	TIME [epoch: 8.42 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.681284628766163		[learning rate: 1.59e-05]
		[batch 20/20] avg loss: 1.5268801576633486		[learning rate: 1.5871e-05]
	Learning Rate: 1.5871e-05
	LOSS [training: 1.6040823932147557 | validation: 2.495529524053249]
	TIME [epoch: 8.4 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6629630958438324		[learning rate: 1.5842e-05]
		[batch 20/20] avg loss: 1.5460242243593025		[learning rate: 1.5813e-05]
	Learning Rate: 1.58134e-05
	LOSS [training: 1.6044936601015674 | validation: 2.4953526847922216]
	TIME [epoch: 8.4 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.531204290579893		[learning rate: 1.5785e-05]
		[batch 20/20] avg loss: 1.6816683583924532		[learning rate: 1.5756e-05]
	Learning Rate: 1.57561e-05
	LOSS [training: 1.6064363244861735 | validation: 2.492087610141887]
	TIME [epoch: 8.4 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5534828405786376		[learning rate: 1.5727e-05]
		[batch 20/20] avg loss: 1.6587246870463854		[learning rate: 1.5699e-05]
	Learning Rate: 1.56989e-05
	LOSS [training: 1.6061037638125115 | validation: 2.4965531159899483]
	TIME [epoch: 8.4 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5809111288103193		[learning rate: 1.567e-05]
		[batch 20/20] avg loss: 1.6258460317431294		[learning rate: 1.5642e-05]
	Learning Rate: 1.56419e-05
	LOSS [training: 1.6033785802767244 | validation: 2.49562172263606]
	TIME [epoch: 8.43 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5950344425334748		[learning rate: 1.5613e-05]
		[batch 20/20] avg loss: 1.611767218974358		[learning rate: 1.5585e-05]
	Learning Rate: 1.55851e-05
	LOSS [training: 1.6034008307539165 | validation: 2.4929934982141964]
	TIME [epoch: 8.41 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6495192746834852		[learning rate: 1.5557e-05]
		[batch 20/20] avg loss: 1.5654023820191298		[learning rate: 1.5529e-05]
	Learning Rate: 1.55286e-05
	LOSS [training: 1.6074608283513077 | validation: 2.498587783729795]
	TIME [epoch: 8.46 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6366718426878126		[learning rate: 1.55e-05]
		[batch 20/20] avg loss: 1.5726010102180656		[learning rate: 1.5472e-05]
	Learning Rate: 1.54722e-05
	LOSS [training: 1.6046364264529391 | validation: 2.4944756672937873]
	TIME [epoch: 8.41 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5823318822142554		[learning rate: 1.5444e-05]
		[batch 20/20] avg loss: 1.6311762489000798		[learning rate: 1.5416e-05]
	Learning Rate: 1.54161e-05
	LOSS [training: 1.6067540655571673 | validation: 2.4945576149298834]
	TIME [epoch: 8.43 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5807318743783259		[learning rate: 1.5388e-05]
		[batch 20/20] avg loss: 1.6302447604435653		[learning rate: 1.536e-05]
	Learning Rate: 1.53601e-05
	LOSS [training: 1.6054883174109456 | validation: 2.4947422198202367]
	TIME [epoch: 8.41 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6909205229502213		[learning rate: 1.5332e-05]
		[batch 20/20] avg loss: 1.520083202096559		[learning rate: 1.5304e-05]
	Learning Rate: 1.53044e-05
	LOSS [training: 1.6055018625233903 | validation: 2.495702006412287]
	TIME [epoch: 8.41 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6558709906133213		[learning rate: 1.5277e-05]
		[batch 20/20] avg loss: 1.5498054126284972		[learning rate: 1.5249e-05]
	Learning Rate: 1.52488e-05
	LOSS [training: 1.602838201620909 | validation: 2.48956243696931]
	TIME [epoch: 8.41 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5430419358728114		[learning rate: 1.5221e-05]
		[batch 20/20] avg loss: 1.6671276109204398		[learning rate: 1.5194e-05]
	Learning Rate: 1.51935e-05
	LOSS [training: 1.6050847733966254 | validation: 2.4938704684127124]
	TIME [epoch: 8.42 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5968763022631256		[learning rate: 1.5166e-05]
		[batch 20/20] avg loss: 1.6175310853224647		[learning rate: 1.5138e-05]
	Learning Rate: 1.51384e-05
	LOSS [training: 1.6072036937927952 | validation: 2.5007045212605306]
	TIME [epoch: 8.41 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5617386397693505		[learning rate: 1.5111e-05]
		[batch 20/20] avg loss: 1.6447306018274321		[learning rate: 1.5083e-05]
	Learning Rate: 1.50834e-05
	LOSS [training: 1.603234620798391 | validation: 2.4959143452732224]
	TIME [epoch: 8.41 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.613942352303599		[learning rate: 1.5056e-05]
		[batch 20/20] avg loss: 1.5976149935934898		[learning rate: 1.5029e-05]
	Learning Rate: 1.50287e-05
	LOSS [training: 1.6057786729485444 | validation: 2.497208353586025]
	TIME [epoch: 8.4 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5788782085188495		[learning rate: 1.5001e-05]
		[batch 20/20] avg loss: 1.6311438331256742		[learning rate: 1.4974e-05]
	Learning Rate: 1.49741e-05
	LOSS [training: 1.6050110208222619 | validation: 2.5001952866408623]
	TIME [epoch: 8.42 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5097939216280873		[learning rate: 1.4947e-05]
		[batch 20/20] avg loss: 1.6993764649891		[learning rate: 1.492e-05]
	Learning Rate: 1.49198e-05
	LOSS [training: 1.6045851933085942 | validation: 2.493036872890151]
	TIME [epoch: 8.4 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5952086498323896		[learning rate: 1.4893e-05]
		[batch 20/20] avg loss: 1.6174303124423068		[learning rate: 1.4866e-05]
	Learning Rate: 1.48657e-05
	LOSS [training: 1.6063194811373485 | validation: 2.4892028472751995]
	TIME [epoch: 8.4 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4828320902983703		[learning rate: 1.4839e-05]
		[batch 20/20] avg loss: 1.7288751447085722		[learning rate: 1.4812e-05]
	Learning Rate: 1.48117e-05
	LOSS [training: 1.6058536175034714 | validation: 2.4963939020522274]
	TIME [epoch: 8.4 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7010583546743976		[learning rate: 1.4785e-05]
		[batch 20/20] avg loss: 1.5092870286107753		[learning rate: 1.4758e-05]
	Learning Rate: 1.4758e-05
	LOSS [training: 1.6051726916425864 | validation: 2.4907513749217682]
	TIME [epoch: 8.4 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6376255602085414		[learning rate: 1.4731e-05]
		[batch 20/20] avg loss: 1.5694118293765693		[learning rate: 1.4704e-05]
	Learning Rate: 1.47044e-05
	LOSS [training: 1.6035186947925553 | validation: 2.495043603588531]
	TIME [epoch: 8.43 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6849027730596684		[learning rate: 1.4678e-05]
		[batch 20/20] avg loss: 1.5260224731456873		[learning rate: 1.4651e-05]
	Learning Rate: 1.4651e-05
	LOSS [training: 1.605462623102678 | validation: 2.497186064935846]
	TIME [epoch: 8.4 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.457538651377566		[learning rate: 1.4624e-05]
		[batch 20/20] avg loss: 1.7446844015805971		[learning rate: 1.4598e-05]
	Learning Rate: 1.45979e-05
	LOSS [training: 1.6011115264790816 | validation: 2.4929614837880223]
	TIME [epoch: 8.4 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5345819244439347		[learning rate: 1.4571e-05]
		[batch 20/20] avg loss: 1.6745212233953786		[learning rate: 1.4545e-05]
	Learning Rate: 1.45449e-05
	LOSS [training: 1.6045515739196567 | validation: 2.4945241293842946]
	TIME [epoch: 8.36 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5711830033489085		[learning rate: 1.4518e-05]
		[batch 20/20] avg loss: 1.636130693550792		[learning rate: 1.4492e-05]
	Learning Rate: 1.44921e-05
	LOSS [training: 1.6036568484498503 | validation: 2.4946892542243666]
	TIME [epoch: 8.37 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6396173157202845		[learning rate: 1.4466e-05]
		[batch 20/20] avg loss: 1.5710187792307173		[learning rate: 1.444e-05]
	Learning Rate: 1.44395e-05
	LOSS [training: 1.6053180474755009 | validation: 2.496646471907546]
	TIME [epoch: 8.38 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.425319982870567		[learning rate: 1.4413e-05]
		[batch 20/20] avg loss: 1.7843147125805126		[learning rate: 1.4387e-05]
	Learning Rate: 1.43871e-05
	LOSS [training: 1.6048173477255396 | validation: 2.4951766376417464]
	TIME [epoch: 8.39 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5081023732857515		[learning rate: 1.4361e-05]
		[batch 20/20] avg loss: 1.6986542934791455		[learning rate: 1.4335e-05]
	Learning Rate: 1.43349e-05
	LOSS [training: 1.6033783333824487 | validation: 2.4906439995226366]
	TIME [epoch: 8.4 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5655871152510572		[learning rate: 1.4309e-05]
		[batch 20/20] avg loss: 1.638446955756826		[learning rate: 1.4283e-05]
	Learning Rate: 1.42829e-05
	LOSS [training: 1.6020170355039416 | validation: 2.4963535043505862]
	TIME [epoch: 8.4 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.684589820848187		[learning rate: 1.4257e-05]
		[batch 20/20] avg loss: 1.5237276735867984		[learning rate: 1.4231e-05]
	Learning Rate: 1.4231e-05
	LOSS [training: 1.6041587472174927 | validation: 2.4966671405107173]
	TIME [epoch: 8.39 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.62192508076153		[learning rate: 1.4205e-05]
		[batch 20/20] avg loss: 1.5842536875404511		[learning rate: 1.4179e-05]
	Learning Rate: 1.41794e-05
	LOSS [training: 1.6030893841509903 | validation: 2.494085015791693]
	TIME [epoch: 8.38 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3466566224194103		[learning rate: 1.4154e-05]
		[batch 20/20] avg loss: 1.861426886788027		[learning rate: 1.4128e-05]
	Learning Rate: 1.41279e-05
	LOSS [training: 1.6040417546037187 | validation: 2.496439156064438]
	TIME [epoch: 8.36 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6489102693612616		[learning rate: 1.4102e-05]
		[batch 20/20] avg loss: 1.5628632310688149		[learning rate: 1.4077e-05]
	Learning Rate: 1.40767e-05
	LOSS [training: 1.605886750215038 | validation: 2.492789149436383]
	TIME [epoch: 8.39 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.800176432452623		[learning rate: 1.4051e-05]
		[batch 20/20] avg loss: 1.414667865732585		[learning rate: 1.4026e-05]
	Learning Rate: 1.40256e-05
	LOSS [training: 1.607422149092604 | validation: 2.4954433211001694]
	TIME [epoch: 8.4 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5754623449489085		[learning rate: 1.4e-05]
		[batch 20/20] avg loss: 1.629629359505517		[learning rate: 1.3975e-05]
	Learning Rate: 1.39747e-05
	LOSS [training: 1.602545852227213 | validation: 2.491575151091965]
	TIME [epoch: 8.36 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.46366586949219		[learning rate: 1.3949e-05]
		[batch 20/20] avg loss: 1.7411768109948287		[learning rate: 1.3924e-05]
	Learning Rate: 1.3924e-05
	LOSS [training: 1.6024213402435095 | validation: 2.4901221940919083]
	TIME [epoch: 8.36 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4883764125699535		[learning rate: 1.3899e-05]
		[batch 20/20] avg loss: 1.7211851402567446		[learning rate: 1.3873e-05]
	Learning Rate: 1.38734e-05
	LOSS [training: 1.604780776413349 | validation: 2.493962634291833]
	TIME [epoch: 8.36 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5630715890637434		[learning rate: 1.3848e-05]
		[batch 20/20] avg loss: 1.6451418403061726		[learning rate: 1.3823e-05]
	Learning Rate: 1.38231e-05
	LOSS [training: 1.6041067146849581 | validation: 2.4975645292672333]
	TIME [epoch: 8.38 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.542636737625462		[learning rate: 1.3798e-05]
		[batch 20/20] avg loss: 1.6671492196965718		[learning rate: 1.3773e-05]
	Learning Rate: 1.37729e-05
	LOSS [training: 1.6048929786610173 | validation: 2.4970028475372543]
	TIME [epoch: 8.36 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6127702168015439		[learning rate: 1.3748e-05]
		[batch 20/20] avg loss: 1.5997865817847388		[learning rate: 1.3723e-05]
	Learning Rate: 1.37229e-05
	LOSS [training: 1.6062783992931415 | validation: 2.492857892520032]
	TIME [epoch: 8.36 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5267174684762659		[learning rate: 1.3698e-05]
		[batch 20/20] avg loss: 1.6812184849608223		[learning rate: 1.3673e-05]
	Learning Rate: 1.36731e-05
	LOSS [training: 1.603967976718544 | validation: 2.495195694898648]
	TIME [epoch: 8.36 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4388131208457837		[learning rate: 1.3648e-05]
		[batch 20/20] avg loss: 1.7677980618258835		[learning rate: 1.3624e-05]
	Learning Rate: 1.36235e-05
	LOSS [training: 1.6033055913358332 | validation: 2.49491378483319]
	TIME [epoch: 8.37 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5782594378696237		[learning rate: 1.3599e-05]
		[batch 20/20] avg loss: 1.6316104799132654		[learning rate: 1.3574e-05]
	Learning Rate: 1.35741e-05
	LOSS [training: 1.6049349588914446 | validation: 2.4916995791278196]
	TIME [epoch: 8.37 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.687102905743736		[learning rate: 1.3549e-05]
		[batch 20/20] avg loss: 1.5273114219155803		[learning rate: 1.3525e-05]
	Learning Rate: 1.35248e-05
	LOSS [training: 1.6072071638296577 | validation: 2.4945182125683876]
	TIME [epoch: 8.36 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6028005001088268		[learning rate: 1.35e-05]
		[batch 20/20] avg loss: 1.6030318103862418		[learning rate: 1.3476e-05]
	Learning Rate: 1.34757e-05
	LOSS [training: 1.6029161552475344 | validation: 2.4939783771661426]
	TIME [epoch: 8.37 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5759757576495466		[learning rate: 1.3451e-05]
		[batch 20/20] avg loss: 1.6360919298387686		[learning rate: 1.3427e-05]
	Learning Rate: 1.34268e-05
	LOSS [training: 1.6060338437441577 | validation: 2.4988857487358183]
	TIME [epoch: 8.36 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5142827422120373		[learning rate: 1.3402e-05]
		[batch 20/20] avg loss: 1.693576878192458		[learning rate: 1.3378e-05]
	Learning Rate: 1.33781e-05
	LOSS [training: 1.6039298102022475 | validation: 2.48887468309404]
	TIME [epoch: 8.38 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.587742685681104		[learning rate: 1.3354e-05]
		[batch 20/20] avg loss: 1.6267124242454163		[learning rate: 1.333e-05]
	Learning Rate: 1.33296e-05
	LOSS [training: 1.6072275549632604 | validation: 2.4938828767919405]
	TIME [epoch: 8.36 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4980653538926867		[learning rate: 1.3305e-05]
		[batch 20/20] avg loss: 1.7080831089053305		[learning rate: 1.3281e-05]
	Learning Rate: 1.32812e-05
	LOSS [training: 1.6030742313990085 | validation: 2.4933416516493505]
	TIME [epoch: 8.36 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7348826753985098		[learning rate: 1.3257e-05]
		[batch 20/20] avg loss: 1.4712972197774636		[learning rate: 1.3233e-05]
	Learning Rate: 1.3233e-05
	LOSS [training: 1.6030899475879867 | validation: 2.4973194907321323]
	TIME [epoch: 8.36 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.651137345241913		[learning rate: 1.3209e-05]
		[batch 20/20] avg loss: 1.5591575902559351		[learning rate: 1.3185e-05]
	Learning Rate: 1.3185e-05
	LOSS [training: 1.6051474677489241 | validation: 2.4948753531794754]
	TIME [epoch: 8.38 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5649307829300745		[learning rate: 1.3161e-05]
		[batch 20/20] avg loss: 1.6451467255507422		[learning rate: 1.3137e-05]
	Learning Rate: 1.31371e-05
	LOSS [training: 1.6050387542404085 | validation: 2.498104831728504]
	TIME [epoch: 8.37 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6640005804309326		[learning rate: 1.3113e-05]
		[batch 20/20] avg loss: 1.544419825620277		[learning rate: 1.3089e-05]
	Learning Rate: 1.30894e-05
	LOSS [training: 1.6042102030256047 | validation: 2.4993847364678783]
	TIME [epoch: 8.36 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.40474281774148		[learning rate: 1.3066e-05]
		[batch 20/20] avg loss: 1.8015506840570794		[learning rate: 1.3042e-05]
	Learning Rate: 1.30419e-05
	LOSS [training: 1.6031467508992798 | validation: 2.4928623158189933]
	TIME [epoch: 8.36 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.681615274059726		[learning rate: 1.3018e-05]
		[batch 20/20] avg loss: 1.5270945162663747		[learning rate: 1.2995e-05]
	Learning Rate: 1.29946e-05
	LOSS [training: 1.6043548951630506 | validation: 2.501939099128624]
	TIME [epoch: 8.38 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6796208358267464		[learning rate: 1.2971e-05]
		[batch 20/20] avg loss: 1.5263470492685995		[learning rate: 1.2947e-05]
	Learning Rate: 1.29475e-05
	LOSS [training: 1.602983942547673 | validation: 2.494558020989812]
	TIME [epoch: 8.36 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4177856853108484		[learning rate: 1.2924e-05]
		[batch 20/20] avg loss: 1.7902209861534701		[learning rate: 1.29e-05]
	Learning Rate: 1.29005e-05
	LOSS [training: 1.6040033357321595 | validation: 2.4931465725445427]
	TIME [epoch: 8.36 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6236082937782192		[learning rate: 1.2877e-05]
		[batch 20/20] avg loss: 1.5839850585669606		[learning rate: 1.2854e-05]
	Learning Rate: 1.28536e-05
	LOSS [training: 1.6037966761725897 | validation: 2.4961610663686677]
	TIME [epoch: 8.42 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5369443249073178		[learning rate: 1.283e-05]
		[batch 20/20] avg loss: 1.6699864422137427		[learning rate: 1.2807e-05]
	Learning Rate: 1.2807e-05
	LOSS [training: 1.6034653835605301 | validation: 2.4923902435167835]
	TIME [epoch: 8.36 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6425061098412523		[learning rate: 1.2784e-05]
		[batch 20/20] avg loss: 1.5652517181336298		[learning rate: 1.2761e-05]
	Learning Rate: 1.27605e-05
	LOSS [training: 1.603878913987441 | validation: 2.4969675615934968]
	TIME [epoch: 8.38 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6062154029797724		[learning rate: 1.2737e-05]
		[batch 20/20] avg loss: 1.6055823780441443		[learning rate: 1.2714e-05]
	Learning Rate: 1.27142e-05
	LOSS [training: 1.6058988905119584 | validation: 2.4969262102214715]
	TIME [epoch: 8.36 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5710026985726069		[learning rate: 1.2691e-05]
		[batch 20/20] avg loss: 1.6392666978185197		[learning rate: 1.2668e-05]
	Learning Rate: 1.26681e-05
	LOSS [training: 1.6051346981955636 | validation: 2.501283744804828]
	TIME [epoch: 8.36 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6927562723850311		[learning rate: 1.2645e-05]
		[batch 20/20] avg loss: 1.5148342608133576		[learning rate: 1.2622e-05]
	Learning Rate: 1.26221e-05
	LOSS [training: 1.6037952665991941 | validation: 2.4936409228499574]
	TIME [epoch: 8.36 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.590966318935783		[learning rate: 1.2599e-05]
		[batch 20/20] avg loss: 1.6185471244204321		[learning rate: 1.2576e-05]
	Learning Rate: 1.25763e-05
	LOSS [training: 1.6047567216781076 | validation: 2.5006966659712155]
	TIME [epoch: 8.38 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.508278583569362		[learning rate: 1.2553e-05]
		[batch 20/20] avg loss: 1.70257521716203		[learning rate: 1.2531e-05]
	Learning Rate: 1.25307e-05
	LOSS [training: 1.6054269003656962 | validation: 2.4936547792672923]
	TIME [epoch: 8.36 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5304028299477805		[learning rate: 1.2508e-05]
		[batch 20/20] avg loss: 1.676967559213838		[learning rate: 1.2485e-05]
	Learning Rate: 1.24852e-05
	LOSS [training: 1.6036851945808095 | validation: 2.496439401902408]
	TIME [epoch: 8.36 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.579672788154502		[learning rate: 1.2463e-05]
		[batch 20/20] avg loss: 1.6307238741469887		[learning rate: 1.244e-05]
	Learning Rate: 1.24399e-05
	LOSS [training: 1.6051983311507456 | validation: 2.496648221956829]
	TIME [epoch: 8.36 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5382501881782717		[learning rate: 1.2417e-05]
		[batch 20/20] avg loss: 1.6716317809987025		[learning rate: 1.2395e-05]
	Learning Rate: 1.23947e-05
	LOSS [training: 1.6049409845884874 | validation: 2.4959251612502005]
	TIME [epoch: 8.38 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5742691698173856		[learning rate: 1.2372e-05]
		[batch 20/20] avg loss: 1.6317342579005996		[learning rate: 1.235e-05]
	Learning Rate: 1.23497e-05
	LOSS [training: 1.6030017138589927 | validation: 2.4931082200360217]
	TIME [epoch: 8.36 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6777162919724475		[learning rate: 1.2327e-05]
		[batch 20/20] avg loss: 1.5362068859465052		[learning rate: 1.2305e-05]
	Learning Rate: 1.23049e-05
	LOSS [training: 1.6069615889594762 | validation: 2.490790615272983]
	TIME [epoch: 8.36 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6886710831749536		[learning rate: 1.2283e-05]
		[batch 20/20] avg loss: 1.5233510880307903		[learning rate: 1.226e-05]
	Learning Rate: 1.22603e-05
	LOSS [training: 1.6060110856028718 | validation: 2.4927219361327824]
	TIME [epoch: 8.36 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5437312888924104		[learning rate: 1.2238e-05]
		[batch 20/20] avg loss: 1.662980163553851		[learning rate: 1.2216e-05]
	Learning Rate: 1.22158e-05
	LOSS [training: 1.6033557262231306 | validation: 2.4924089683736264]
	TIME [epoch: 8.37 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7871432267280412		[learning rate: 1.2194e-05]
		[batch 20/20] avg loss: 1.4199247957897887		[learning rate: 1.2171e-05]
	Learning Rate: 1.21714e-05
	LOSS [training: 1.603534011258915 | validation: 2.4980322871907417]
	TIME [epoch: 8.38 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.686172862151194		[learning rate: 1.2149e-05]
		[batch 20/20] avg loss: 1.5242784951687363		[learning rate: 1.2127e-05]
	Learning Rate: 1.21273e-05
	LOSS [training: 1.6052256786599646 | validation: 2.495159441905807]
	TIME [epoch: 8.36 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4759393342381342		[learning rate: 1.2105e-05]
		[batch 20/20] avg loss: 1.7330235838405503		[learning rate: 1.2083e-05]
	Learning Rate: 1.20833e-05
	LOSS [training: 1.6044814590393421 | validation: 2.494849265481186]
	TIME [epoch: 8.36 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.538057027346045		[learning rate: 1.2061e-05]
		[batch 20/20] avg loss: 1.6719264720047267		[learning rate: 1.2039e-05]
	Learning Rate: 1.20394e-05
	LOSS [training: 1.6049917496753856 | validation: 2.492040355734947]
	TIME [epoch: 8.36 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.488880996508465		[learning rate: 1.2018e-05]
		[batch 20/20] avg loss: 1.7180038779780826		[learning rate: 1.1996e-05]
	Learning Rate: 1.19957e-05
	LOSS [training: 1.6034424372432738 | validation: 2.4980880300977764]
	TIME [epoch: 8.38 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4449884196468576		[learning rate: 1.1974e-05]
		[batch 20/20] avg loss: 1.765977664603683		[learning rate: 1.1952e-05]
	Learning Rate: 1.19522e-05
	LOSS [training: 1.6054830421252706 | validation: 2.4984322174754676]
	TIME [epoch: 8.37 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6261969956618745		[learning rate: 1.193e-05]
		[batch 20/20] avg loss: 1.5848470922012456		[learning rate: 1.1909e-05]
	Learning Rate: 1.19088e-05
	LOSS [training: 1.60552204393156 | validation: 2.4904103816485854]
	TIME [epoch: 8.36 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.601800653803128		[learning rate: 1.1887e-05]
		[batch 20/20] avg loss: 1.606078737887809		[learning rate: 1.1866e-05]
	Learning Rate: 1.18656e-05
	LOSS [training: 1.6039396958454684 | validation: 2.4976893345105204]
	TIME [epoch: 8.37 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4695772748793359		[learning rate: 1.1844e-05]
		[batch 20/20] avg loss: 1.7401086348128192		[learning rate: 1.1823e-05]
	Learning Rate: 1.18225e-05
	LOSS [training: 1.6048429548460774 | validation: 2.4983957858990435]
	TIME [epoch: 8.37 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4185396612802823		[learning rate: 1.1801e-05]
		[batch 20/20] avg loss: 1.7842289509103932		[learning rate: 1.178e-05]
	Learning Rate: 1.17796e-05
	LOSS [training: 1.6013843060953377 | validation: 2.4944450616308615]
	TIME [epoch: 8.36 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6586170900682808		[learning rate: 1.1758e-05]
		[batch 20/20] avg loss: 1.5533506942576891		[learning rate: 1.1737e-05]
	Learning Rate: 1.17369e-05
	LOSS [training: 1.6059838921629848 | validation: 2.497080998104814]
	TIME [epoch: 8.36 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6865027344058494		[learning rate: 1.1716e-05]
		[batch 20/20] avg loss: 1.51934025034106		[learning rate: 1.1694e-05]
	Learning Rate: 1.16943e-05
	LOSS [training: 1.6029214923734547 | validation: 2.492740982662538]
	TIME [epoch: 8.36 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5061150992028105		[learning rate: 1.1673e-05]
		[batch 20/20] avg loss: 1.705240167684679		[learning rate: 1.1652e-05]
	Learning Rate: 1.16518e-05
	LOSS [training: 1.6056776334437448 | validation: 2.499591055305363]
	TIME [epoch: 8.37 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.555554920401966		[learning rate: 1.1631e-05]
		[batch 20/20] avg loss: 1.6521762773662996		[learning rate: 1.161e-05]
	Learning Rate: 1.16096e-05
	LOSS [training: 1.6038655988841328 | validation: 2.4970949831185307]
	TIME [epoch: 8.36 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5220314747707968		[learning rate: 1.1588e-05]
		[batch 20/20] avg loss: 1.69016747832906		[learning rate: 1.1567e-05]
	Learning Rate: 1.15674e-05
	LOSS [training: 1.6060994765499281 | validation: 2.492355919865327]
	TIME [epoch: 8.36 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5787391203565162		[learning rate: 1.1546e-05]
		[batch 20/20] avg loss: 1.6318441616303652		[learning rate: 1.1525e-05]
	Learning Rate: 1.15255e-05
	LOSS [training: 1.6052916409934408 | validation: 2.490564766571831]
	TIME [epoch: 8.36 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7585176205014907		[learning rate: 1.1505e-05]
		[batch 20/20] avg loss: 1.448313456669532		[learning rate: 1.1484e-05]
	Learning Rate: 1.14836e-05
	LOSS [training: 1.6034155385855111 | validation: 2.4981008457554164]
	TIME [epoch: 8.36 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5604557736799385		[learning rate: 1.1463e-05]
		[batch 20/20] avg loss: 1.651866244449043		[learning rate: 1.1442e-05]
	Learning Rate: 1.1442e-05
	LOSS [training: 1.6061610090644904 | validation: 2.4967406641820364]
	TIME [epoch: 8.38 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5390960401002964		[learning rate: 1.1421e-05]
		[batch 20/20] avg loss: 1.669346784697252		[learning rate: 1.14e-05]
	Learning Rate: 1.14004e-05
	LOSS [training: 1.6042214123987741 | validation: 2.4947425111304002]
	TIME [epoch: 8.36 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6067514731739831		[learning rate: 1.138e-05]
		[batch 20/20] avg loss: 1.6032526021026925		[learning rate: 1.1359e-05]
	Learning Rate: 1.13591e-05
	LOSS [training: 1.6050020376383376 | validation: 2.494164744833414]
	TIME [epoch: 8.36 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.621706257525315		[learning rate: 1.1338e-05]
		[batch 20/20] avg loss: 1.583316390529013		[learning rate: 1.1318e-05]
	Learning Rate: 1.13178e-05
	LOSS [training: 1.602511324027164 | validation: 2.496656349609383]
	TIME [epoch: 8.36 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6120556107737098		[learning rate: 1.1297e-05]
		[batch 20/20] avg loss: 1.603339920037514		[learning rate: 1.1277e-05]
	Learning Rate: 1.12768e-05
	LOSS [training: 1.607697765405612 | validation: 2.4896737572467798]
	TIME [epoch: 8.38 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6354670048989497		[learning rate: 1.1256e-05]
		[batch 20/20] avg loss: 1.5696124080664529		[learning rate: 1.1236e-05]
	Learning Rate: 1.12358e-05
	LOSS [training: 1.6025397064827012 | validation: 2.492614450059593]
	TIME [epoch: 8.37 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5030399869056654		[learning rate: 1.1215e-05]
		[batch 20/20] avg loss: 1.7024402933862386		[learning rate: 1.1195e-05]
	Learning Rate: 1.11951e-05
	LOSS [training: 1.602740140145952 | validation: 2.495414437415717]
	TIME [epoch: 8.36 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5714079363136113		[learning rate: 1.1175e-05]
		[batch 20/20] avg loss: 1.639667988986283		[learning rate: 1.1154e-05]
	Learning Rate: 1.11544e-05
	LOSS [training: 1.6055379626499473 | validation: 2.4980334003266034]
	TIME [epoch: 8.36 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6336497485186503		[learning rate: 1.1134e-05]
		[batch 20/20] avg loss: 1.5765919484149182		[learning rate: 1.1114e-05]
	Learning Rate: 1.1114e-05
	LOSS [training: 1.6051208484667843 | validation: 2.495405721583976]
	TIME [epoch: 8.38 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5961529575513835		[learning rate: 1.1094e-05]
		[batch 20/20] avg loss: 1.6102503999666156		[learning rate: 1.1074e-05]
	Learning Rate: 1.10736e-05
	LOSS [training: 1.6032016787589995 | validation: 2.497161662527324]
	TIME [epoch: 8.36 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.646220668594297		[learning rate: 1.1054e-05]
		[batch 20/20] avg loss: 1.5603611758198062		[learning rate: 1.1033e-05]
	Learning Rate: 1.10334e-05
	LOSS [training: 1.603290922207052 | validation: 2.4937370517176545]
	TIME [epoch: 8.36 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.546230766257052		[learning rate: 1.1013e-05]
		[batch 20/20] avg loss: 1.6641072127109815		[learning rate: 1.0993e-05]
	Learning Rate: 1.09934e-05
	LOSS [training: 1.6051689894840169 | validation: 2.4989311955991584]
	TIME [epoch: 8.36 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6533394340577132		[learning rate: 1.0973e-05]
		[batch 20/20] avg loss: 1.5552485909656812		[learning rate: 1.0953e-05]
	Learning Rate: 1.09535e-05
	LOSS [training: 1.6042940125116971 | validation: 2.5014244278283524]
	TIME [epoch: 8.37 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.606205671534548		[learning rate: 1.0934e-05]
		[batch 20/20] avg loss: 1.6049816695275652		[learning rate: 1.0914e-05]
	Learning Rate: 1.09137e-05
	LOSS [training: 1.6055936705310565 | validation: 2.4913650844117208]
	TIME [epoch: 8.38 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6155844591139945		[learning rate: 1.0894e-05]
		[batch 20/20] avg loss: 1.5904647158385232		[learning rate: 1.0874e-05]
	Learning Rate: 1.08741e-05
	LOSS [training: 1.6030245874762588 | validation: 2.496765361071982]
	TIME [epoch: 8.36 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6327324369213365		[learning rate: 1.0854e-05]
		[batch 20/20] avg loss: 1.5767860451559874		[learning rate: 1.0835e-05]
	Learning Rate: 1.08347e-05
	LOSS [training: 1.604759241038662 | validation: 2.4951508381905714]
	TIME [epoch: 8.36 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5580872032302615		[learning rate: 1.0815e-05]
		[batch 20/20] avg loss: 1.6525393504320003		[learning rate: 1.0795e-05]
	Learning Rate: 1.07954e-05
	LOSS [training: 1.6053132768311307 | validation: 2.4984723159786335]
	TIME [epoch: 8.36 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4143884500132375		[learning rate: 1.0776e-05]
		[batch 20/20] avg loss: 1.7943758752364374		[learning rate: 1.0756e-05]
	Learning Rate: 1.07562e-05
	LOSS [training: 1.6043821626248373 | validation: 2.4921339150461685]
	TIME [epoch: 8.38 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5709624073647637		[learning rate: 1.0737e-05]
		[batch 20/20] avg loss: 1.640843073800242		[learning rate: 1.0717e-05]
	Learning Rate: 1.07171e-05
	LOSS [training: 1.6059027405825028 | validation: 2.491033983849117]
	TIME [epoch: 8.36 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5879064211579454		[learning rate: 1.0698e-05]
		[batch 20/20] avg loss: 1.6228905735078183		[learning rate: 1.0678e-05]
	Learning Rate: 1.06782e-05
	LOSS [training: 1.605398497332882 | validation: 2.493914918745892]
	TIME [epoch: 8.36 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6594551410705098		[learning rate: 1.0659e-05]
		[batch 20/20] avg loss: 1.54554275462768		[learning rate: 1.0639e-05]
	Learning Rate: 1.06395e-05
	LOSS [training: 1.6024989478490952 | validation: 2.499188866522839]
	TIME [epoch: 8.36 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.599720930445147		[learning rate: 1.062e-05]
		[batch 20/20] avg loss: 1.6110028685222695		[learning rate: 1.0601e-05]
	Learning Rate: 1.06009e-05
	LOSS [training: 1.6053618994837084 | validation: 2.4941171217313185]
	TIME [epoch: 8.38 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4615192329610278		[learning rate: 1.0582e-05]
		[batch 20/20] avg loss: 1.7540578741821247		[learning rate: 1.0562e-05]
	Learning Rate: 1.05624e-05
	LOSS [training: 1.6077885535715761 | validation: 2.4925737745735375]
	TIME [epoch: 8.36 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6240009233529034		[learning rate: 1.0543e-05]
		[batch 20/20] avg loss: 1.5811809782861936		[learning rate: 1.0524e-05]
	Learning Rate: 1.05241e-05
	LOSS [training: 1.6025909508195486 | validation: 2.4970032471794754]
	TIME [epoch: 8.36 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5647804945455825		[learning rate: 1.0505e-05]
		[batch 20/20] avg loss: 1.6427816525849512		[learning rate: 1.0486e-05]
	Learning Rate: 1.04859e-05
	LOSS [training: 1.6037810735652667 | validation: 2.4943298463910297]
	TIME [epoch: 8.36 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7493218947084368		[learning rate: 1.0467e-05]
		[batch 20/20] avg loss: 1.462195938698344		[learning rate: 1.0448e-05]
	Learning Rate: 1.04478e-05
	LOSS [training: 1.6057589167033908 | validation: 2.4985807287243755]
	TIME [epoch: 8.36 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.716053951023568		[learning rate: 1.0429e-05]
		[batch 20/20] avg loss: 1.493741655420648		[learning rate: 1.041e-05]
	Learning Rate: 1.04099e-05
	LOSS [training: 1.6048978032221082 | validation: 2.4946733084740913]
	TIME [epoch: 8.38 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.528066471551372		[learning rate: 1.0391e-05]
		[batch 20/20] avg loss: 1.6833918701069959		[learning rate: 1.0372e-05]
	Learning Rate: 1.03721e-05
	LOSS [training: 1.6057291708291839 | validation: 2.4964423893569627]
	TIME [epoch: 8.36 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5231360489790648		[learning rate: 1.0353e-05]
		[batch 20/20] avg loss: 1.6879253580828517		[learning rate: 1.0335e-05]
	Learning Rate: 1.03345e-05
	LOSS [training: 1.6055307035309583 | validation: 2.5008032680286085]
	TIME [epoch: 8.36 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5915866280174442		[learning rate: 1.0316e-05]
		[batch 20/20] avg loss: 1.627268532596761		[learning rate: 1.0297e-05]
	Learning Rate: 1.0297e-05
	LOSS [training: 1.6094275803071028 | validation: 2.502522895838893]
	TIME [epoch: 8.36 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.472230092539342		[learning rate: 1.0278e-05]
		[batch 20/20] avg loss: 1.7372584776038813		[learning rate: 1.026e-05]
	Learning Rate: 1.02596e-05
	LOSS [training: 1.6047442850716116 | validation: 2.4997212363296586]
	TIME [epoch: 8.38 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4883296291019457		[learning rate: 1.0241e-05]
		[batch 20/20] avg loss: 1.722990045076417		[learning rate: 1.0222e-05]
	Learning Rate: 1.02224e-05
	LOSS [training: 1.6056598370891817 | validation: 2.493294474402248]
	TIME [epoch: 8.37 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6941127498157418		[learning rate: 1.0204e-05]
		[batch 20/20] avg loss: 1.5093679686535433		[learning rate: 1.0185e-05]
	Learning Rate: 1.01853e-05
	LOSS [training: 1.6017403592346426 | validation: 2.4958686782686166]
	TIME [epoch: 8.36 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5408717492632138		[learning rate: 1.0167e-05]
		[batch 20/20] avg loss: 1.6637184399304108		[learning rate: 1.0148e-05]
	Learning Rate: 1.01483e-05
	LOSS [training: 1.6022950945968124 | validation: 2.4961514144874704]
	TIME [epoch: 8.37 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.541169840273195		[learning rate: 1.013e-05]
		[batch 20/20] avg loss: 1.672426569030306		[learning rate: 1.0112e-05]
	Learning Rate: 1.01115e-05
	LOSS [training: 1.6067982046517506 | validation: 2.49505960858496]
	TIME [epoch: 8.38 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.655678643482857		[learning rate: 1.0093e-05]
		[batch 20/20] avg loss: 1.5582279688084428		[learning rate: 1.0075e-05]
	Learning Rate: 1.00748e-05
	LOSS [training: 1.6069533061456496 | validation: 2.495208034456433]
	TIME [epoch: 8.37 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.765811275677081		[learning rate: 1.0057e-05]
		[batch 20/20] avg loss: 1.44222670878541		[learning rate: 1.0038e-05]
	Learning Rate: 1.00382e-05
	LOSS [training: 1.6040189922312453 | validation: 2.495304884984031]
	TIME [epoch: 8.36 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.716143717623583		[learning rate: 1.002e-05]
		[batch 20/20] avg loss: 1.491370677393076		[learning rate: 1.0002e-05]
	Learning Rate: 1.00018e-05
	LOSS [training: 1.6037571975083291 | validation: 2.495097212713381]
	TIME [epoch: 8.36 sec]
Finished training in 16897.133 seconds.
