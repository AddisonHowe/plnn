Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r0', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 380105671

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.641886355184202		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.850715048476971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.246300701830588 | validation: 8.808832831927553]
	TIME [epoch: 45.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.601402318474694		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.396119183693389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.998760751084044 | validation: 6.972496329020076]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.4909663819625845		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.622745288683303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.056855835322945 | validation: 6.319019717547709]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.274852075405006		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.107524994897557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.191188535151281 | validation: 5.894623971402112]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.6846634528234095		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.536840746089391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6107520994564 | validation: 5.387480072732908]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.451072386961902		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.338529391042373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.394800889002137 | validation: 5.180055175249655]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.211504642343959		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.236082746603045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.223793694473502 | validation: 5.063668652520711]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.0994520668377845		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.761181931466167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.930316999151977 | validation: 4.846829350028768]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.657010543188799		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.276033395186152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.466521969187476 | validation: 4.044622158842532]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.016493280848811		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7089281789824993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.862710729915656 | validation: 3.377486046916362]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.467240574485822		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1017589935701406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.284499784027981 | validation: 2.835982714034027]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.825062236205237		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.817759646228776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8214109412170063 | validation: 2.465451218968347]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3748146779906265		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4270018253081656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4009082516493963 | validation: 2.071257446841118]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.123135494411822		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.179590469624916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.151362982018369 | validation: 2.0473094843759347]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9118929287347406		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8273495891368312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8696212589357857 | validation: 1.5075412703536055]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.541055026035556		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6200797684738621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5805673972547087 | validation: 1.6166300760136187]
	TIME [epoch: 8.78 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.563930390391836		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6576151784458315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6107727844188335 | validation: 1.965600292310902]
	TIME [epoch: 8.79 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4728692812885014		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6130407168250116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5429549990567561 | validation: 1.4266050302615803]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3504989751237368		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4571700933785106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4038345342511236 | validation: 1.676653629667285]
	TIME [epoch: 9.07 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5705780151781825		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4110959395851204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4908369773816514 | validation: 1.1108630434911437]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4138582742835983		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2807738659117498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3473160700976738 | validation: 1.36042699337823]
	TIME [epoch: 8.81 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0925112180668948		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3026519618680577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1975815899674764 | validation: 0.9968412357591653]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1997072332997258		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3950406171900374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2973739252448813 | validation: 1.3810330537864157]
	TIME [epoch: 8.8 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2804454083720775		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2945100745332634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2874777414526708 | validation: 0.9485582108385833]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.096856929941407		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3993959025403753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2481264162408912 | validation: 0.9251833638291665]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3018520436049477		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1833792239782812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2426156337916143 | validation: 1.0239475337037685]
	TIME [epoch: 8.81 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.157801066759213		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.138716877090757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1482589719249847 | validation: 1.3166973055363733]
	TIME [epoch: 8.79 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1934729993773052		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2576238452076047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2255484222924549 | validation: 1.4223316437785591]
	TIME [epoch: 8.8 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0878055637714108		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1929490259916287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1403772948815198 | validation: 1.095666064960918]
	TIME [epoch: 8.81 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0960266294130636		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.284553727060209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1902901782366362 | validation: 0.8975643538070422]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9470523730225683		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1890732718254042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.068062822423986 | validation: 0.9099209243404869]
	TIME [epoch: 8.8 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5460289087912131		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0650253828278742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3055271458095432 | validation: 0.8042203803476264]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9937818203687436		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1168174547870393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0552996375778914 | validation: 0.9749599306752941]
	TIME [epoch: 8.81 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0131518957023382		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9579098195383832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.985530857620361 | validation: 1.1044593871951152]
	TIME [epoch: 8.81 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0315995547895114		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1081637305745775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0698816426820446 | validation: 0.947062413915717]
	TIME [epoch: 8.8 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0296249793609664		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0281379928337209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0288814860973434 | validation: 0.9168023478985614]
	TIME [epoch: 8.8 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0732059935395024		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0493892104383207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0612976019889113 | validation: 0.8596533903429071]
	TIME [epoch: 8.8 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0230603371298908		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0879277791984836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0554940581641872 | validation: 0.8335410474468467]
	TIME [epoch: 8.83 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2135368480641975		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0243400441010748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1189384460826362 | validation: 0.995751496159172]
	TIME [epoch: 8.8 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0340714747448532		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2380330362467107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1360522554957817 | validation: 1.1755895186444607]
	TIME [epoch: 8.81 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0146957093598667		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.962809726535248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9887527179475573 | validation: 1.2546166023756833]
	TIME [epoch: 8.79 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.023198761468962		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0106409585018645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0169198599854135 | validation: 1.3897732483580003]
	TIME [epoch: 8.81 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.328852027912411		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9182086480312064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.123530337971809 | validation: 1.8053037945934913]
	TIME [epoch: 8.81 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.023255954790232		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9954977163181653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0093768355541988 | validation: 0.9486275715515902]
	TIME [epoch: 8.79 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.277485069551353		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9683061968428961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1228956331971247 | validation: 0.7110506790025555]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9764436174988245		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8770285432055849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9267360803522046 | validation: 0.7725841316890767]
	TIME [epoch: 8.78 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0311817964675227		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.940670282216096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9859260393418093 | validation: 1.3700536897296773]
	TIME [epoch: 8.81 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0290698466147041		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.893062203150621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9610660248826626 | validation: 0.8092672132621672]
	TIME [epoch: 8.79 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8923815229818451		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9081930860929888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9002873045374171 | validation: 1.2687618769481621]
	TIME [epoch: 8.78 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0100856489685788		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1208390888196695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0654623688941243 | validation: 1.1597445368972208]
	TIME [epoch: 8.79 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9505031281306187		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8355503408232925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8930267344769556 | validation: 0.7774443139511228]
	TIME [epoch: 8.8 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9066863467756393		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9424118125980299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9245490796868345 | validation: 0.7646028351711036]
	TIME [epoch: 8.82 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9286671273025116		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8679378413669028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.898302484334707 | validation: 0.8635223282832108]
	TIME [epoch: 8.79 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8966857419001568		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0341301275274266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9654079347137916 | validation: 0.9593750685898059]
	TIME [epoch: 8.8 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9519247657216805		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2318274522858637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0918761090037719 | validation: 1.0413657535409482]
	TIME [epoch: 8.8 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9544379773629871		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.918940238728078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9366891080455325 | validation: 1.0957213075874892]
	TIME [epoch: 8.82 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9372525799570077		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9147785426430499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9260155613000286 | validation: 0.6629039534630365]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9622485017380139		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0183505474863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.990299524612157 | validation: 0.6975785917269544]
	TIME [epoch: 8.81 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.864477019250025		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9568689305641993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9106729749071117 | validation: 0.8535643432915666]
	TIME [epoch: 8.8 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9536799602416401		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8207060719910173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8871930161163288 | validation: 0.6957817979940984]
	TIME [epoch: 8.82 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8671640938948336		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0573253307640436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9622447123294385 | validation: 0.7759731092703642]
	TIME [epoch: 8.8 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8752379609256483		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9788379899668016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9270379754462251 | validation: 0.7866369688615298]
	TIME [epoch: 8.8 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9025168843736375		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8797604234026503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8911386538881441 | validation: 0.6801541471860432]
	TIME [epoch: 8.8 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1197544766861105		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7945495366705126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9571520066783116 | validation: 0.7512598535337025]
	TIME [epoch: 8.8 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8576423213834012		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7419753548414961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7998088381124486 | validation: 0.7819172496721557]
	TIME [epoch: 8.83 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7889320114869477		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8899332088866589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8394326101868034 | validation: 1.3594330849684795]
	TIME [epoch: 8.8 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8938383743810168		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8881213048895938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8909798396353052 | validation: 1.1998613798155864]
	TIME [epoch: 8.81 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8608962317349447		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9028311033968857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8818636675659152 | validation: 0.807049918482284]
	TIME [epoch: 8.8 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.711741327211749		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7583889076099719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7350651174108604 | validation: 0.6171671668413944]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7829977098303107		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8282478176825178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8056227637564142 | validation: 0.8595277572772324]
	TIME [epoch: 8.8 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8723207987756141		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7930226654189501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8326717320972821 | validation: 0.7891725756676523]
	TIME [epoch: 8.79 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6963835609970395		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8240111770227383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7601973690098888 | validation: 0.667568477421339]
	TIME [epoch: 8.79 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6837997877574781		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7495600653936857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7166799265755819 | validation: 0.6924265405274902]
	TIME [epoch: 8.78 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7120399639990713		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7287236102920114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7203817871455414 | validation: 1.1226319138264629]
	TIME [epoch: 8.82 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.865878743896879		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6731071892841716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7694929665905254 | validation: 0.7360141152485952]
	TIME [epoch: 8.78 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7855673199784808		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6689775905640154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7272724552712481 | validation: 0.8962154883593692]
	TIME [epoch: 8.8 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9086767288032407		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7286952241314062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8186859764673236 | validation: 0.6370545763769476]
	TIME [epoch: 8.79 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7447179397985938		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7326113275204909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7386646336595424 | validation: 0.5247241724847748]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6328365607686696		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7306701927083187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.681753376738494 | validation: 0.48826157846650164]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6296675059811504		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6869533436858208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6583104248334857 | validation: 0.7258231608362657]
	TIME [epoch: 8.82 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6448066423409025		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6598777738667424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6523422081038224 | validation: 0.7351677964161561]
	TIME [epoch: 8.81 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7264203702982248		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5726633990903941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6495418846943094 | validation: 0.5559787701338808]
	TIME [epoch: 8.82 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5917061344557142		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7396582814121488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6656822079339315 | validation: 0.7145748735873944]
	TIME [epoch: 8.82 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7557833539786093		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6351737869391959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6954785704589026 | validation: 0.774883270931619]
	TIME [epoch: 8.8 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5918837943327852		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6769561410805466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.634419967706666 | validation: 0.8769547823883562]
	TIME [epoch: 8.81 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5960877531425999		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5322049379114739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5641463455270369 | validation: 0.5600740464612952]
	TIME [epoch: 8.81 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5066182346651452		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6243957549550692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5655069948101072 | validation: 0.5626308481463749]
	TIME [epoch: 8.81 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5879201693456928		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5658512071251217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5768856882354072 | validation: 0.6040947209122713]
	TIME [epoch: 8.82 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6814030802254443		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.538149719766491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6097763999959676 | validation: 1.3745647434000996]
	TIME [epoch: 8.81 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6070508694095426		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5121150141598366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5595829417846895 | validation: 0.47514861389868157]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5374553890369611		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5588156646417869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.548135526839374 | validation: 0.9315396514135982]
	TIME [epoch: 8.81 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6494831620713435		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5300059593494144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5897445607103788 | validation: 0.6568287861954745]
	TIME [epoch: 8.82 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5597373631083915		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5668226958523812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5632800294803862 | validation: 0.4592668523152744]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4555200116060843		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6233179974407503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5394190045234172 | validation: 0.49131379609253933]
	TIME [epoch: 8.81 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5874436412245407		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6776834787498425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6325635599871916 | validation: 0.46642415917356983]
	TIME [epoch: 8.78 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5811966582836823		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48758267283529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5343896655594862 | validation: 0.2948842266752444]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7249242894784085		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5087778767043313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6168510830913698 | validation: 0.5164952069124051]
	TIME [epoch: 8.82 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6439067502006305		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6648998021917122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6544032761961713 | validation: 0.5482071639889947]
	TIME [epoch: 8.8 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4502550613901131		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.514947622653252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48260134202168253 | validation: 0.6068666945482974]
	TIME [epoch: 8.79 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5667278250569594		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.531413044842584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5490704349497717 | validation: 0.6169167958969869]
	TIME [epoch: 8.79 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.608249357901378		[learning rate: 0.0099891]
		[batch 20/20] avg loss: 0.734611146761751		[learning rate: 0.009977]
	Learning Rate: 0.009977
	LOSS [training: 0.6714302523315643 | validation: 0.34661353381165527]
	TIME [epoch: 8.82 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6263590736067027		[learning rate: 0.0099649]
		[batch 20/20] avg loss: 0.46929657054573415		[learning rate: 0.0099528]
	Learning Rate: 0.00995285
	LOSS [training: 0.5478278220762185 | validation: 0.5436507451165938]
	TIME [epoch: 8.79 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.503214591675601		[learning rate: 0.0099408]
		[batch 20/20] avg loss: 1.024455147737775		[learning rate: 0.0099288]
	Learning Rate: 0.00992875
	LOSS [training: 0.7638348697066881 | validation: 1.148196063015051]
	TIME [epoch: 8.8 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6591822909455818		[learning rate: 0.0099167]
		[batch 20/20] avg loss: 0.5599632250544718		[learning rate: 0.0099047]
	Learning Rate: 0.00990472
	LOSS [training: 0.609572758000027 | validation: 0.6441253147708101]
	TIME [epoch: 8.79 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6114398096661268		[learning rate: 0.0098927]
		[batch 20/20] avg loss: 0.4881034026287347		[learning rate: 0.0098807]
	Learning Rate: 0.00988074
	LOSS [training: 0.5497716061474308 | validation: 0.8601982778920855]
	TIME [epoch: 8.81 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5090185813100432		[learning rate: 0.0098688]
		[batch 20/20] avg loss: 0.5862695627040828		[learning rate: 0.0098568]
	Learning Rate: 0.00985682
	LOSS [training: 0.5476440720070631 | validation: 0.39632244939968486]
	TIME [epoch: 8.83 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4984162750385771		[learning rate: 0.0098449]
		[batch 20/20] avg loss: 0.45063818293414204		[learning rate: 0.009833]
	Learning Rate: 0.00983296
	LOSS [training: 0.47452722898635963 | validation: 0.8426094883883295]
	TIME [epoch: 8.8 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.458924960051404		[learning rate: 0.009821]
		[batch 20/20] avg loss: 0.5664874902241696		[learning rate: 0.0098092]
	Learning Rate: 0.00980915
	LOSS [training: 0.5127062251377867 | validation: 0.8666199584932965]
	TIME [epoch: 8.8 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4934929094209773		[learning rate: 0.0097973]
		[batch 20/20] avg loss: 0.4234900159418883		[learning rate: 0.0097854]
	Learning Rate: 0.00978541
	LOSS [training: 0.4584914626814328 | validation: 0.4188566796636046]
	TIME [epoch: 8.81 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46431036656647595		[learning rate: 0.0097736]
		[batch 20/20] avg loss: 0.5685723101150719		[learning rate: 0.0097617]
	Learning Rate: 0.00976172
	LOSS [training: 0.5164413383407739 | validation: 0.3878049240349324]
	TIME [epoch: 8.8 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4755283540046774		[learning rate: 0.0097499]
		[batch 20/20] avg loss: 0.5177215325513054		[learning rate: 0.0097381]
	Learning Rate: 0.00973809
	LOSS [training: 0.4966249432779913 | validation: 0.7820573163302396]
	TIME [epoch: 8.79 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40793511112076286		[learning rate: 0.0097263]
		[batch 20/20] avg loss: 0.43858369565771155		[learning rate: 0.0097145]
	Learning Rate: 0.00971451
	LOSS [training: 0.42325940338923723 | validation: 0.37230339295713544]
	TIME [epoch: 8.78 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4679266419084299		[learning rate: 0.0097027]
		[batch 20/20] avg loss: 0.431687152764552		[learning rate: 0.009691]
	Learning Rate: 0.009691
	LOSS [training: 0.44980689733649093 | validation: 0.3437709833974665]
	TIME [epoch: 8.8 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40963013108274965		[learning rate: 0.0096793]
		[batch 20/20] avg loss: 0.4653825648948911		[learning rate: 0.0096675]
	Learning Rate: 0.00966754
	LOSS [training: 0.4375063479888204 | validation: 0.26646709186172624]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4470806311268162		[learning rate: 0.0096558]
		[batch 20/20] avg loss: 0.4165846996347162		[learning rate: 0.0096441]
	Learning Rate: 0.00964413
	LOSS [training: 0.4318326653807662 | validation: 0.2839266858610683]
	TIME [epoch: 8.79 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4822801496332455		[learning rate: 0.0096325]
		[batch 20/20] avg loss: 0.43490505845841876		[learning rate: 0.0096208]
	Learning Rate: 0.00962078
	LOSS [training: 0.4585926040458322 | validation: 0.33648659711361584]
	TIME [epoch: 8.8 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34486119249414793		[learning rate: 0.0096091]
		[batch 20/20] avg loss: 0.3441724592845025		[learning rate: 0.0095975]
	Learning Rate: 0.00959749
	LOSS [training: 0.34451682588932514 | validation: 0.7215916155046037]
	TIME [epoch: 8.78 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4348653527150943		[learning rate: 0.0095859]
		[batch 20/20] avg loss: 0.39236680791693185		[learning rate: 0.0095743]
	Learning Rate: 0.00957426
	LOSS [training: 0.41361608031601305 | validation: 0.2805539480623394]
	TIME [epoch: 8.8 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.589980556130117		[learning rate: 0.0095627]
		[batch 20/20] avg loss: 0.4940407167178121		[learning rate: 0.0095511]
	Learning Rate: 0.00955108
	LOSS [training: 0.5420106364239644 | validation: 0.7264211109160189]
	TIME [epoch: 8.82 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45664660299066406		[learning rate: 0.0095395]
		[batch 20/20] avg loss: 0.5004615882949237		[learning rate: 0.009528]
	Learning Rate: 0.00952796
	LOSS [training: 0.47855409564279394 | validation: 0.3331291671067468]
	TIME [epoch: 8.8 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41548502726609965		[learning rate: 0.0095164]
		[batch 20/20] avg loss: 0.33535632623122835		[learning rate: 0.0095049]
	Learning Rate: 0.0095049
	LOSS [training: 0.37542067674866403 | validation: 0.4334321124176337]
	TIME [epoch: 8.8 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48957782838532626		[learning rate: 0.0094934]
		[batch 20/20] avg loss: 0.8969373532264348		[learning rate: 0.0094819]
	Learning Rate: 0.00948189
	LOSS [training: 0.6932575908058807 | validation: 0.3491836770904619]
	TIME [epoch: 8.79 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5324907461361053		[learning rate: 0.0094704]
		[batch 20/20] avg loss: 0.4152252889423839		[learning rate: 0.0094589]
	Learning Rate: 0.00945893
	LOSS [training: 0.4738580175392446 | validation: 0.3920029704159994]
	TIME [epoch: 8.81 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4482691046261169		[learning rate: 0.0094475]
		[batch 20/20] avg loss: 0.4095085832019172		[learning rate: 0.009436]
	Learning Rate: 0.00943603
	LOSS [training: 0.42888884391401705 | validation: 0.6368042727830321]
	TIME [epoch: 8.81 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4399026276773467		[learning rate: 0.0094246]
		[batch 20/20] avg loss: 0.5575109270191004		[learning rate: 0.0094132]
	Learning Rate: 0.00941319
	LOSS [training: 0.4987067773482236 | validation: 0.4836542609086417]
	TIME [epoch: 8.79 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40675075431100316		[learning rate: 0.0094018]
		[batch 20/20] avg loss: 0.5007373733340741		[learning rate: 0.0093904]
	Learning Rate: 0.0093904
	LOSS [training: 0.4537440638225386 | validation: 0.4510441326547676]
	TIME [epoch: 8.8 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44321091272685437		[learning rate: 0.009379]
		[batch 20/20] avg loss: 0.33862573204097324		[learning rate: 0.0093677]
	Learning Rate: 0.00936767
	LOSS [training: 0.3909183223839138 | validation: 0.4223909395733936]
	TIME [epoch: 8.79 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4809340085335744		[learning rate: 0.0093563]
		[batch 20/20] avg loss: 0.38836055284589965		[learning rate: 0.009345]
	Learning Rate: 0.00934499
	LOSS [training: 0.43464728068973696 | validation: 0.5991564406869448]
	TIME [epoch: 8.82 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4606763663441794		[learning rate: 0.0093337]
		[batch 20/20] avg loss: 0.3973941121745598		[learning rate: 0.0093224]
	Learning Rate: 0.00932237
	LOSS [training: 0.42903523925936965 | validation: 0.3675914734675397]
	TIME [epoch: 8.79 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3801165132874957		[learning rate: 0.0093111]
		[batch 20/20] avg loss: 0.47301933342130253		[learning rate: 0.0092998]
	Learning Rate: 0.0092998
	LOSS [training: 0.4265679233543992 | validation: 0.35261315713066305]
	TIME [epoch: 8.8 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3432896648161505		[learning rate: 0.0092885]
		[batch 20/20] avg loss: 0.42868826123272863		[learning rate: 0.0092773]
	Learning Rate: 0.00927729
	LOSS [training: 0.38598896302443947 | validation: 0.803426650859504]
	TIME [epoch: 8.79 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44961739859244804		[learning rate: 0.0092661]
		[batch 20/20] avg loss: 0.33680003218574833		[learning rate: 0.0092548]
	Learning Rate: 0.00925483
	LOSS [training: 0.39320871538909813 | validation: 0.6616227179134118]
	TIME [epoch: 8.82 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5159863656940656		[learning rate: 0.0092436]
		[batch 20/20] avg loss: 0.3976342990030216		[learning rate: 0.0092324]
	Learning Rate: 0.00923242
	LOSS [training: 0.4568103323485436 | validation: 0.6181939515083507]
	TIME [epoch: 8.8 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4074728068178891		[learning rate: 0.0092212]
		[batch 20/20] avg loss: 0.4003368648225491		[learning rate: 0.0092101]
	Learning Rate: 0.00921007
	LOSS [training: 0.4039048358202191 | validation: 0.5226511166420087]
	TIME [epoch: 8.79 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3669809789172552		[learning rate: 0.0091989]
		[batch 20/20] avg loss: 0.234099698239024		[learning rate: 0.0091878]
	Learning Rate: 0.00918778
	LOSS [training: 0.3005403385781395 | validation: 0.22603117812385556]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3292301591423955		[learning rate: 0.0091767]
		[batch 20/20] avg loss: 0.3091481823176519		[learning rate: 0.0091655]
	Learning Rate: 0.00916554
	LOSS [training: 0.3191891707300236 | validation: 0.32903441295098407]
	TIME [epoch: 8.81 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44630061896466255		[learning rate: 0.0091544]
		[batch 20/20] avg loss: 0.32729903995750254		[learning rate: 0.0091433]
	Learning Rate: 0.00914335
	LOSS [training: 0.3867998294610825 | validation: 0.3692847312169464]
	TIME [epoch: 8.8 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47145023578479917		[learning rate: 0.0091323]
		[batch 20/20] avg loss: 0.40653444360187807		[learning rate: 0.0091212]
	Learning Rate: 0.00912121
	LOSS [training: 0.43899233969333873 | validation: 0.7345883403792449]
	TIME [epoch: 8.79 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5362429790036817		[learning rate: 0.0091102]
		[batch 20/20] avg loss: 0.29045332779493654		[learning rate: 0.0090991]
	Learning Rate: 0.00909913
	LOSS [training: 0.4133481533993092 | validation: 0.6190921079739108]
	TIME [epoch: 8.79 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44817467728059307		[learning rate: 0.0090881]
		[batch 20/20] avg loss: 0.38001063448211225		[learning rate: 0.0090771]
	Learning Rate: 0.0090771
	LOSS [training: 0.41409265588135263 | validation: 0.5468775830009879]
	TIME [epoch: 8.79 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5074988635801377		[learning rate: 0.0090661]
		[batch 20/20] avg loss: 0.3720455097163165		[learning rate: 0.0090551]
	Learning Rate: 0.00905513
	LOSS [training: 0.43977218664822715 | validation: 0.24695696060906355]
	TIME [epoch: 8.81 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3664476801180597		[learning rate: 0.0090442]
		[batch 20/20] avg loss: 0.3812519564860303		[learning rate: 0.0090332]
	Learning Rate: 0.00903321
	LOSS [training: 0.373849818302045 | validation: 0.4272304906526637]
	TIME [epoch: 8.8 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5800375865506482		[learning rate: 0.0090223]
		[batch 20/20] avg loss: 0.39704493570223354		[learning rate: 0.0090113]
	Learning Rate: 0.00901134
	LOSS [training: 0.48854126112644086 | validation: 0.2693371847740763]
	TIME [epoch: 8.79 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4266338064801646		[learning rate: 0.0090004]
		[batch 20/20] avg loss: 0.44226601760120865		[learning rate: 0.0089895]
	Learning Rate: 0.00898953
	LOSS [training: 0.43444991204068667 | validation: 0.2914216968046035]
	TIME [epoch: 8.79 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39731004162405464		[learning rate: 0.0089786]
		[batch 20/20] avg loss: 0.43968280680593885		[learning rate: 0.0089678]
	Learning Rate: 0.00896776
	LOSS [training: 0.4184964242149968 | validation: 0.3679849542150119]
	TIME [epoch: 8.81 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3849308482521673		[learning rate: 0.0089569]
		[batch 20/20] avg loss: 0.41648306135639707		[learning rate: 0.0089461]
	Learning Rate: 0.00894605
	LOSS [training: 0.40070695480428203 | validation: 0.4895213004266196]
	TIME [epoch: 8.81 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38468439722212866		[learning rate: 0.0089352]
		[batch 20/20] avg loss: 0.3926789065528461		[learning rate: 0.0089244]
	Learning Rate: 0.0089244
	LOSS [training: 0.38868165188748743 | validation: 0.2551649133020068]
	TIME [epoch: 8.81 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2976591371971943		[learning rate: 0.0089136]
		[batch 20/20] avg loss: 0.4518453102524428		[learning rate: 0.0089028]
	Learning Rate: 0.00890279
	LOSS [training: 0.3747522237248186 | validation: 0.4857624631247934]
	TIME [epoch: 8.81 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42648407921180753		[learning rate: 0.008892]
		[batch 20/20] avg loss: 0.3733906936069359		[learning rate: 0.0088812]
	Learning Rate: 0.00888124
	LOSS [training: 0.39993738640937176 | validation: 0.2737930396672059]
	TIME [epoch: 8.81 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3902416341388221		[learning rate: 0.0088705]
		[batch 20/20] avg loss: 0.34042380611540896		[learning rate: 0.0088597]
	Learning Rate: 0.00885974
	LOSS [training: 0.36533272012711554 | validation: 0.36480693768981615]
	TIME [epoch: 8.81 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4098237050632729		[learning rate: 0.008849]
		[batch 20/20] avg loss: 0.49526610613665795		[learning rate: 0.0088383]
	Learning Rate: 0.00883829
	LOSS [training: 0.45254490559996546 | validation: 0.385068256968376]
	TIME [epoch: 8.79 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42416451292340457		[learning rate: 0.0088276]
		[batch 20/20] avg loss: 0.5227465134552164		[learning rate: 0.0088169]
	Learning Rate: 0.0088169
	LOSS [training: 0.47345551318931045 | validation: 0.2909648144645924]
	TIME [epoch: 8.79 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9950140078706895		[learning rate: 0.0088062]
		[batch 20/20] avg loss: 0.6894758418724891		[learning rate: 0.0087956]
	Learning Rate: 0.00879555
	LOSS [training: 0.8422449248715893 | validation: 0.3986893487916853]
	TIME [epoch: 8.8 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42431715034731193		[learning rate: 0.0087849]
		[batch 20/20] avg loss: 0.5267205958649502		[learning rate: 0.0087743]
	Learning Rate: 0.00877426
	LOSS [training: 0.4755188731061312 | validation: 0.29257315507546666]
	TIME [epoch: 8.82 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3284036119691244		[learning rate: 0.0087636]
		[batch 20/20] avg loss: 0.3522379824089718		[learning rate: 0.008753]
	Learning Rate: 0.00875302
	LOSS [training: 0.3403207971890482 | validation: 0.47521718851922556]
	TIME [epoch: 8.8 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.412742662578992		[learning rate: 0.0087424]
		[batch 20/20] avg loss: 0.3852742176456387		[learning rate: 0.0087318]
	Learning Rate: 0.00873183
	LOSS [training: 0.39900844011231534 | validation: 0.2669197010091526]
	TIME [epoch: 8.79 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4007590446575685		[learning rate: 0.0087213]
		[batch 20/20] avg loss: 0.4658872140550601		[learning rate: 0.0087107]
	Learning Rate: 0.00871069
	LOSS [training: 0.43332312935631434 | validation: 0.25047367662665726]
	TIME [epoch: 8.79 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30705520573654554		[learning rate: 0.0087001]
		[batch 20/20] avg loss: 0.3721561428478738		[learning rate: 0.0086896]
	Learning Rate: 0.0086896
	LOSS [training: 0.3396056742922096 | validation: 0.7557815308144273]
	TIME [epoch: 8.81 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5218180253785543		[learning rate: 0.0086791]
		[batch 20/20] avg loss: 0.321863001253958		[learning rate: 0.0086686]
	Learning Rate: 0.00866857
	LOSS [training: 0.42184051331625616 | validation: 0.3488491505414202]
	TIME [epoch: 8.8 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3746923099099734		[learning rate: 0.0086581]
		[batch 20/20] avg loss: 0.4246237563213066		[learning rate: 0.0086476]
	Learning Rate: 0.00864758
	LOSS [training: 0.39965803311563997 | validation: 0.18849770741800917]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4172072169639275		[learning rate: 0.0086371]
		[batch 20/20] avg loss: 0.33113578528557813		[learning rate: 0.0086266]
	Learning Rate: 0.00862665
	LOSS [training: 0.37417150112475284 | validation: 0.2544472001973764]
	TIME [epoch: 8.78 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29583027873279766		[learning rate: 0.0086162]
		[batch 20/20] avg loss: 0.5700929909912862		[learning rate: 0.0086058]
	Learning Rate: 0.00860576
	LOSS [training: 0.4329616348620419 | validation: 0.604636713272558]
	TIME [epoch: 8.79 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39358086145656757		[learning rate: 0.0085953]
		[batch 20/20] avg loss: 0.28079749887254646		[learning rate: 0.0085849]
	Learning Rate: 0.00858493
	LOSS [training: 0.33718918016455696 | validation: 0.35667413326907316]
	TIME [epoch: 8.79 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34497893862454554		[learning rate: 0.0085745]
		[batch 20/20] avg loss: 0.5925183955107433		[learning rate: 0.0085641]
	Learning Rate: 0.00856415
	LOSS [training: 0.4687486670676444 | validation: 0.7613875186993305]
	TIME [epoch: 8.79 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4218992450253459		[learning rate: 0.0085538]
		[batch 20/20] avg loss: 0.3737188367737045		[learning rate: 0.0085434]
	Learning Rate: 0.00854342
	LOSS [training: 0.3978090408995253 | validation: 0.32002804710668914]
	TIME [epoch: 8.78 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.536416876008466		[learning rate: 0.0085331]
		[batch 20/20] avg loss: 0.45295939376580574		[learning rate: 0.0085227]
	Learning Rate: 0.00852273
	LOSS [training: 0.49468813488713587 | validation: 0.3278928141129009]
	TIME [epoch: 8.79 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3838187669015841		[learning rate: 0.0085124]
		[batch 20/20] avg loss: 0.3894648059985578		[learning rate: 0.0085021]
	Learning Rate: 0.0085021
	LOSS [training: 0.386641786450071 | validation: 0.24457345770756078]
	TIME [epoch: 8.82 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4387429347342464		[learning rate: 0.0084918]
		[batch 20/20] avg loss: 0.35145667094028477		[learning rate: 0.0084815]
	Learning Rate: 0.00848152
	LOSS [training: 0.39509980283726553 | validation: 0.46879601257441605]
	TIME [epoch: 8.8 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3353530608673949		[learning rate: 0.0084712]
		[batch 20/20] avg loss: 0.3818118017450247		[learning rate: 0.008461]
	Learning Rate: 0.00846099
	LOSS [training: 0.3585824313062098 | validation: 0.3397682893831787]
	TIME [epoch: 8.79 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33119782973640927		[learning rate: 0.0084507]
		[batch 20/20] avg loss: 0.31896708423776354		[learning rate: 0.0084405]
	Learning Rate: 0.0084405
	LOSS [training: 0.3250824569870864 | validation: 0.36434854651297033]
	TIME [epoch: 8.78 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28972215988273303		[learning rate: 0.0084303]
		[batch 20/20] avg loss: 0.5414876056017477		[learning rate: 0.0084201]
	Learning Rate: 0.00842007
	LOSS [training: 0.4156048827422404 | validation: 0.46092585104438855]
	TIME [epoch: 8.8 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4083074796449056		[learning rate: 0.0084099]
		[batch 20/20] avg loss: 0.2643387109291174		[learning rate: 0.0083997]
	Learning Rate: 0.00839969
	LOSS [training: 0.3363230952870115 | validation: 0.4365858273121176]
	TIME [epoch: 8.81 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38591058831665864		[learning rate: 0.0083895]
		[batch 20/20] avg loss: 0.4344439041422767		[learning rate: 0.0083794]
	Learning Rate: 0.00837935
	LOSS [training: 0.41017724622946766 | validation: 0.2169484504809967]
	TIME [epoch: 8.79 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.408931833011002		[learning rate: 0.0083692]
		[batch 20/20] avg loss: 0.2990878925257119		[learning rate: 0.0083591]
	Learning Rate: 0.00835907
	LOSS [training: 0.3540098627683569 | validation: 0.3198590703464627]
	TIME [epoch: 8.81 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33109926379123167		[learning rate: 0.0083489]
		[batch 20/20] avg loss: 0.22573594495680632		[learning rate: 0.0083388]
	Learning Rate: 0.00833883
	LOSS [training: 0.278417604374019 | validation: 0.8650062368398328]
	TIME [epoch: 8.8 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38233248089985833		[learning rate: 0.0083287]
		[batch 20/20] avg loss: 0.3506532447800693		[learning rate: 0.0083186]
	Learning Rate: 0.00831864
	LOSS [training: 0.3664928628399638 | validation: 0.3290692522382451]
	TIME [epoch: 8.82 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.405024939675889		[learning rate: 0.0083086]
		[batch 20/20] avg loss: 0.4346603804107666		[learning rate: 0.0082985]
	Learning Rate: 0.00829851
	LOSS [training: 0.41984266004332776 | validation: 0.2702323709955837]
	TIME [epoch: 8.79 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3350498116075511		[learning rate: 0.0082885]
		[batch 20/20] avg loss: 0.37601825873090383		[learning rate: 0.0082784]
	Learning Rate: 0.00827842
	LOSS [training: 0.3555340351692275 | validation: 0.3305605773963048]
	TIME [epoch: 8.79 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3360469014939505		[learning rate: 0.0082684]
		[batch 20/20] avg loss: 0.29885497195293736		[learning rate: 0.0082584]
	Learning Rate: 0.00825838
	LOSS [training: 0.317450936723444 | validation: 0.791210537372619]
	TIME [epoch: 8.79 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39737020280039875		[learning rate: 0.0082484]
		[batch 20/20] avg loss: 0.2852911669755464		[learning rate: 0.0082384]
	Learning Rate: 0.00823839
	LOSS [training: 0.3413306848879726 | validation: 0.49437485426777195]
	TIME [epoch: 8.81 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36257556565928917		[learning rate: 0.0082284]
		[batch 20/20] avg loss: 0.3099153327294422		[learning rate: 0.0082184]
	Learning Rate: 0.00821844
	LOSS [training: 0.3362454491943657 | validation: 0.3249655345645117]
	TIME [epoch: 8.8 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33965625544726724		[learning rate: 0.0082085]
		[batch 20/20] avg loss: 0.2762121359735508		[learning rate: 0.0081985]
	Learning Rate: 0.00819855
	LOSS [training: 0.30793419571040903 | validation: 0.22339650853422993]
	TIME [epoch: 8.79 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2889854325466471		[learning rate: 0.0081886]
		[batch 20/20] avg loss: 0.37721359751659633		[learning rate: 0.0081787]
	Learning Rate: 0.0081787
	LOSS [training: 0.3330995150316217 | validation: 0.263980879102119]
	TIME [epoch: 8.78 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3643347656198854		[learning rate: 0.0081688]
		[batch 20/20] avg loss: 0.3428184435292866		[learning rate: 0.0081589]
	Learning Rate: 0.0081589
	LOSS [training: 0.35357660457458595 | validation: 0.2699058201857867]
	TIME [epoch: 8.8 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3607068143021124		[learning rate: 0.008149]
		[batch 20/20] avg loss: 0.5442507318802645		[learning rate: 0.0081391]
	Learning Rate: 0.00813915
	LOSS [training: 0.45247877309118856 | validation: 0.34993287864587724]
	TIME [epoch: 8.8 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3040553280710661		[learning rate: 0.0081293]
		[batch 20/20] avg loss: 0.4528990246237317		[learning rate: 0.0081194]
	Learning Rate: 0.00811944
	LOSS [training: 0.3784771763473989 | validation: 0.2834242611862021]
	TIME [epoch: 8.8 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29207045219912386		[learning rate: 0.0081096]
		[batch 20/20] avg loss: 0.35373656000465914		[learning rate: 0.0080998]
	Learning Rate: 0.00809979
	LOSS [training: 0.32290350610189156 | validation: 0.21610056807200934]
	TIME [epoch: 8.79 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29041040784346994		[learning rate: 0.00809]
		[batch 20/20] avg loss: 0.3497297629087907		[learning rate: 0.0080802]
	Learning Rate: 0.00808018
	LOSS [training: 0.32007008537613035 | validation: 0.26425938370115554]
	TIME [epoch: 8.8 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3153038531752627		[learning rate: 0.0080704]
		[batch 20/20] avg loss: 0.39296715781463154		[learning rate: 0.0080606]
	Learning Rate: 0.00806062
	LOSS [training: 0.3541355054949471 | validation: 0.40321261833727945]
	TIME [epoch: 8.82 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32151769011953063		[learning rate: 0.0080509]
		[batch 20/20] avg loss: 0.5798573393474821		[learning rate: 0.0080411]
	Learning Rate: 0.00804111
	LOSS [training: 0.4506875147335063 | validation: 0.39701255090950555]
	TIME [epoch: 8.8 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3935439204337346		[learning rate: 0.0080314]
		[batch 20/20] avg loss: 0.3622265726762454		[learning rate: 0.0080216]
	Learning Rate: 0.00802164
	LOSS [training: 0.37788524655499006 | validation: 0.3349620598209147]
	TIME [epoch: 8.79 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3341971903339582		[learning rate: 0.0080119]
		[batch 20/20] avg loss: 0.31344768780447707		[learning rate: 0.0080022]
	Learning Rate: 0.00800222
	LOSS [training: 0.3238224390692176 | validation: 0.21482593382749346]
	TIME [epoch: 8.79 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31047745948452066		[learning rate: 0.0079925]
		[batch 20/20] avg loss: 0.37260055188799673		[learning rate: 0.0079828]
	Learning Rate: 0.00798285
	LOSS [training: 0.3415390056862587 | validation: 0.3229232241363405]
	TIME [epoch: 8.82 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39016178542366553		[learning rate: 0.0079732]
		[batch 20/20] avg loss: 0.28682638256110493		[learning rate: 0.0079635]
	Learning Rate: 0.00796352
	LOSS [training: 0.33849408399238523 | validation: 0.28968206976696836]
	TIME [epoch: 8.8 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3005769293486086		[learning rate: 0.0079539]
		[batch 20/20] avg loss: 0.29722555462541117		[learning rate: 0.0079442]
	Learning Rate: 0.00794424
	LOSS [training: 0.29890124198700996 | validation: 0.2939553198967461]
	TIME [epoch: 8.79 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3833203531302031		[learning rate: 0.0079346]
		[batch 20/20] avg loss: 0.5340114227466368		[learning rate: 0.007925]
	Learning Rate: 0.00792501
	LOSS [training: 0.45866588793842 | validation: 0.24651528214652763]
	TIME [epoch: 8.8 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38474667430847564		[learning rate: 0.0079154]
		[batch 20/20] avg loss: 0.31160698076037496		[learning rate: 0.0079058]
	Learning Rate: 0.00790583
	LOSS [training: 0.34817682753442536 | validation: 0.3130595671935767]
	TIME [epoch: 8.8 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3361853337498567		[learning rate: 0.0078963]
		[batch 20/20] avg loss: 0.29475071169859995		[learning rate: 0.0078867]
	Learning Rate: 0.00788669
	LOSS [training: 0.3154680227242283 | validation: 0.2537164264977356]
	TIME [epoch: 8.81 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.326738398306766		[learning rate: 0.0078771]
		[batch 20/20] avg loss: 0.33645023948739855		[learning rate: 0.0078676]
	Learning Rate: 0.0078676
	LOSS [training: 0.3315943188970822 | validation: 0.4338649669271578]
	TIME [epoch: 8.79 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24770886348605448		[learning rate: 0.0078581]
		[batch 20/20] avg loss: 0.22845637190000492		[learning rate: 0.0078486]
	Learning Rate: 0.00784855
	LOSS [training: 0.2380826176930298 | validation: 0.4542661623469705]
	TIME [epoch: 8.8 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3590236575151614		[learning rate: 0.007839]
		[batch 20/20] avg loss: 0.45397287888260845		[learning rate: 0.0078296]
	Learning Rate: 0.00782955
	LOSS [training: 0.4064982681988849 | validation: 0.2216469431257337]
	TIME [epoch: 8.82 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.429291781976732		[learning rate: 0.0078201]
		[batch 20/20] avg loss: 0.3746731464553378		[learning rate: 0.0078106]
	Learning Rate: 0.0078106
	LOSS [training: 0.4019824642160349 | validation: 0.4339695226754299]
	TIME [epoch: 8.81 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3877622886861631		[learning rate: 0.0078011]
		[batch 20/20] avg loss: 0.283349267622848		[learning rate: 0.0077917]
	Learning Rate: 0.00779169
	LOSS [training: 0.33555577815450544 | validation: 0.369642421849009]
	TIME [epoch: 8.79 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31789765605365405		[learning rate: 0.0077823]
		[batch 20/20] avg loss: 0.4065513650583942		[learning rate: 0.0077728]
	Learning Rate: 0.00777283
	LOSS [training: 0.3622245105560241 | validation: 0.2612057943516024]
	TIME [epoch: 8.79 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20403018110012017		[learning rate: 0.0077634]
		[batch 20/20] avg loss: 0.31001382563167934		[learning rate: 0.007754]
	Learning Rate: 0.00775401
	LOSS [training: 0.25702200336589975 | validation: 0.7828142266805]
	TIME [epoch: 8.79 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39766307348914115		[learning rate: 0.0077446]
		[batch 20/20] avg loss: 0.2719969726798361		[learning rate: 0.0077352]
	Learning Rate: 0.00773524
	LOSS [training: 0.33483002308448856 | validation: 0.40483061520507574]
	TIME [epoch: 8.81 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36617315024328845		[learning rate: 0.0077259]
		[batch 20/20] avg loss: 0.3615027188806909		[learning rate: 0.0077165]
	Learning Rate: 0.00771651
	LOSS [training: 0.36383793456198965 | validation: 0.31223979452246975]
	TIME [epoch: 8.8 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.254349901075568		[learning rate: 0.0077072]
		[batch 20/20] avg loss: 0.3227880312842056		[learning rate: 0.0076978]
	Learning Rate: 0.00769783
	LOSS [training: 0.2885689661798868 | validation: 0.31463359896467796]
	TIME [epoch: 8.79 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4358621821122076		[learning rate: 0.0076885]
		[batch 20/20] avg loss: 0.32086518107071454		[learning rate: 0.0076792]
	Learning Rate: 0.0076792
	LOSS [training: 0.3783636815914611 | validation: 0.259841095033192]
	TIME [epoch: 8.79 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39648385694668214		[learning rate: 0.0076699]
		[batch 20/20] avg loss: 0.3135621907839625		[learning rate: 0.0076606]
	Learning Rate: 0.00766061
	LOSS [training: 0.35502302386532236 | validation: 0.27039058586440123]
	TIME [epoch: 8.82 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3038921782462737		[learning rate: 0.0076513]
		[batch 20/20] avg loss: 0.33252680999143636		[learning rate: 0.0076421]
	Learning Rate: 0.00764206
	LOSS [training: 0.31820949411885496 | validation: 0.19478767796721283]
	TIME [epoch: 8.8 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35980781830117947		[learning rate: 0.0076328]
		[batch 20/20] avg loss: 0.30536622825948356		[learning rate: 0.0076236]
	Learning Rate: 0.00762356
	LOSS [training: 0.3325870232803315 | validation: 0.2111421915899145]
	TIME [epoch: 8.79 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3786770904373161		[learning rate: 0.0076143]
		[batch 20/20] avg loss: 0.26015663894868746		[learning rate: 0.0076051]
	Learning Rate: 0.00760511
	LOSS [training: 0.3194168646930017 | validation: 0.41041288617001576]
	TIME [epoch: 8.8 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4189153359732959		[learning rate: 0.0075959]
		[batch 20/20] avg loss: 0.3087944434976391		[learning rate: 0.0075867]
	Learning Rate: 0.00758669
	LOSS [training: 0.36385488973546753 | validation: 0.5443976315206572]
	TIME [epoch: 8.82 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3477448317915131		[learning rate: 0.0075775]
		[batch 20/20] avg loss: 0.3148845941341093		[learning rate: 0.0075683]
	Learning Rate: 0.00756833
	LOSS [training: 0.3313147129628111 | validation: 0.1923745485550881]
	TIME [epoch: 8.83 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3260836167119176		[learning rate: 0.0075592]
		[batch 20/20] avg loss: 0.33719956718273575		[learning rate: 0.00755]
	Learning Rate: 0.00755001
	LOSS [training: 0.3316415919473267 | validation: 0.21354666887767665]
	TIME [epoch: 8.81 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26210662630125936		[learning rate: 0.0075409]
		[batch 20/20] avg loss: 0.3474727347771599		[learning rate: 0.0075317]
	Learning Rate: 0.00753173
	LOSS [training: 0.3047896805392095 | validation: 0.34443871174547497]
	TIME [epoch: 8.81 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.449596168897039		[learning rate: 0.0075226]
		[batch 20/20] avg loss: 0.22566096919947848		[learning rate: 0.0075135]
	Learning Rate: 0.0075135
	LOSS [training: 0.33762856904825883 | validation: 0.17562974285746838]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3011075241319983		[learning rate: 0.0075044]
		[batch 20/20] avg loss: 0.22493956250078662		[learning rate: 0.0074953]
	Learning Rate: 0.00749531
	LOSS [training: 0.26302354331639244 | validation: 0.22539367103313349]
	TIME [epoch: 8.81 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32099326610115775		[learning rate: 0.0074862]
		[batch 20/20] avg loss: 0.27782541749706163		[learning rate: 0.0074772]
	Learning Rate: 0.00747716
	LOSS [training: 0.2994093417991097 | validation: 0.21950958764621978]
	TIME [epoch: 8.79 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33208663329911586		[learning rate: 0.0074681]
		[batch 20/20] avg loss: 0.25928500687771666		[learning rate: 0.0074591]
	Learning Rate: 0.00745906
	LOSS [training: 0.29568582008841626 | validation: 0.31941374788158694]
	TIME [epoch: 8.79 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4236174713382776		[learning rate: 0.00745]
		[batch 20/20] avg loss: 0.37392455477430336		[learning rate: 0.007441]
	Learning Rate: 0.007441
	LOSS [training: 0.39877101305629037 | validation: 0.37859640810286005]
	TIME [epoch: 8.79 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3447105204007539		[learning rate: 0.007432]
		[batch 20/20] avg loss: 0.26631175554781317		[learning rate: 0.007423]
	Learning Rate: 0.00742299
	LOSS [training: 0.30551113797428353 | validation: 0.2020555188388714]
	TIME [epoch: 8.8 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3116882260133532		[learning rate: 0.007414]
		[batch 20/20] avg loss: 0.305277463143348		[learning rate: 0.007405]
	Learning Rate: 0.00740502
	LOSS [training: 0.3084828445783506 | validation: 0.17459637711809442]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43356559937939104		[learning rate: 0.0073961]
		[batch 20/20] avg loss: 0.36077550437541644		[learning rate: 0.0073871]
	Learning Rate: 0.0073871
	LOSS [training: 0.3971705518774037 | validation: 0.2591448508592018]
	TIME [epoch: 8.81 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2726332791661231		[learning rate: 0.0073781]
		[batch 20/20] avg loss: 0.2908818460681569		[learning rate: 0.0073692]
	Learning Rate: 0.00736921
	LOSS [training: 0.28175756261713997 | validation: 0.46234466765797283]
	TIME [epoch: 8.8 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34569677987104797		[learning rate: 0.0073603]
		[batch 20/20] avg loss: 0.25397625897923415		[learning rate: 0.0073514]
	Learning Rate: 0.00735137
	LOSS [training: 0.299836519425141 | validation: 0.4599256706965123]
	TIME [epoch: 8.81 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23614326395316737		[learning rate: 0.0073425]
		[batch 20/20] avg loss: 0.3328276756520368		[learning rate: 0.0073336]
	Learning Rate: 0.00733358
	LOSS [training: 0.28448546980260214 | validation: 0.5581706761738302]
	TIME [epoch: 8.85 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5174156233413243		[learning rate: 0.0073247]
		[batch 20/20] avg loss: 0.2421373078509343		[learning rate: 0.0073158]
	Learning Rate: 0.00731582
	LOSS [training: 0.3797764655961294 | validation: 0.22691005864520708]
	TIME [epoch: 8.81 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.342918163495251		[learning rate: 0.007307]
		[batch 20/20] avg loss: 0.3358807769862028		[learning rate: 0.0072981]
	Learning Rate: 0.00729811
	LOSS [training: 0.3393994702407269 | validation: 0.22479989671869113]
	TIME [epoch: 8.81 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25413082331400605		[learning rate: 0.0072893]
		[batch 20/20] avg loss: 0.22023850087011362		[learning rate: 0.0072804]
	Learning Rate: 0.00728044
	LOSS [training: 0.23718466209205982 | validation: 0.2348311414520719]
	TIME [epoch: 8.8 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2538316078567665		[learning rate: 0.0072716]
		[batch 20/20] avg loss: 0.389421751479327		[learning rate: 0.0072628]
	Learning Rate: 0.00726282
	LOSS [training: 0.3216266796680468 | validation: 0.45428775806443766]
	TIME [epoch: 8.83 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31763938275203923		[learning rate: 0.007254]
		[batch 20/20] avg loss: 0.30555160406173554		[learning rate: 0.0072452]
	Learning Rate: 0.00724524
	LOSS [training: 0.31159549340688736 | validation: 0.21319658250777135]
	TIME [epoch: 8.82 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21331885532816974		[learning rate: 0.0072365]
		[batch 20/20] avg loss: 0.24426735103272734		[learning rate: 0.0072277]
	Learning Rate: 0.0072277
	LOSS [training: 0.22879310318044851 | validation: 0.1732033607034642]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26369443349739496		[learning rate: 0.0072189]
		[batch 20/20] avg loss: 0.2360279764609076		[learning rate: 0.0072102]
	Learning Rate: 0.0072102
	LOSS [training: 0.24986120497915126 | validation: 0.24126283368053233]
	TIME [epoch: 8.81 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44040711714580744		[learning rate: 0.0072015]
		[batch 20/20] avg loss: 0.2621990656694785		[learning rate: 0.0071927]
	Learning Rate: 0.00719275
	LOSS [training: 0.3513030914076428 | validation: 0.3845251732837504]
	TIME [epoch: 8.81 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4037163391114971		[learning rate: 0.007184]
		[batch 20/20] avg loss: 0.3711336897783242		[learning rate: 0.0071753]
	Learning Rate: 0.00717533
	LOSS [training: 0.3874250144449106 | validation: 0.22060893666722414]
	TIME [epoch: 8.83 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28765294146642717		[learning rate: 0.0071666]
		[batch 20/20] avg loss: 0.3878183082018798		[learning rate: 0.007158]
	Learning Rate: 0.00715796
	LOSS [training: 0.33773562483415354 | validation: 0.16639410584004355]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3544239732239246		[learning rate: 0.0071493]
		[batch 20/20] avg loss: 0.2779836482817152		[learning rate: 0.0071406]
	Learning Rate: 0.00714064
	LOSS [training: 0.31620381075281995 | validation: 0.3259153010774219]
	TIME [epoch: 8.81 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.341935354486899		[learning rate: 0.007132]
		[batch 20/20] avg loss: 0.27583990493953536		[learning rate: 0.0071233]
	Learning Rate: 0.00712335
	LOSS [training: 0.30888762971321715 | validation: 0.16335954042867093]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25405546105930954		[learning rate: 0.0071147]
		[batch 20/20] avg loss: 0.27769541032264355		[learning rate: 0.0071061]
	Learning Rate: 0.0071061
	LOSS [training: 0.26587543569097644 | validation: 0.26609405698971206]
	TIME [epoch: 8.82 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.300994256354342		[learning rate: 0.0070975]
		[batch 20/20] avg loss: 0.23727517321373023		[learning rate: 0.0070889]
	Learning Rate: 0.0070889
	LOSS [training: 0.26913471478403606 | validation: 0.200030378137831]
	TIME [epoch: 8.83 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27645800977304946		[learning rate: 0.0070803]
		[batch 20/20] avg loss: 0.2905367106269131		[learning rate: 0.0070717]
	Learning Rate: 0.00707174
	LOSS [training: 0.28349736019998123 | validation: 0.24617982377312417]
	TIME [epoch: 8.81 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2920547929621952		[learning rate: 0.0070632]
		[batch 20/20] avg loss: 0.33148183791059227		[learning rate: 0.0070546]
	Learning Rate: 0.00705462
	LOSS [training: 0.31176831543639383 | validation: 0.21721643573510016]
	TIME [epoch: 8.81 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2563170737097182		[learning rate: 0.0070461]
		[batch 20/20] avg loss: 0.299751951866462		[learning rate: 0.0070375]
	Learning Rate: 0.00703754
	LOSS [training: 0.2780345127880901 | validation: 0.38143344475240537]
	TIME [epoch: 8.81 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23738419181081283		[learning rate: 0.007029]
		[batch 20/20] avg loss: 0.21949422316938993		[learning rate: 0.0070205]
	Learning Rate: 0.00702051
	LOSS [training: 0.22843920749010144 | validation: 0.20386059641697912]
	TIME [epoch: 8.81 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25277333679279657		[learning rate: 0.007012]
		[batch 20/20] avg loss: 0.19822102188338314		[learning rate: 0.0070035]
	Learning Rate: 0.00700351
	LOSS [training: 0.22549717933808985 | validation: 0.43160406401309453]
	TIME [epoch: 8.8 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2410484128548867		[learning rate: 0.006995]
		[batch 20/20] avg loss: 0.2987430551964948		[learning rate: 0.0069866]
	Learning Rate: 0.00698656
	LOSS [training: 0.26989573402569084 | validation: 0.3495892890534924]
	TIME [epoch: 8.81 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2863993510244097		[learning rate: 0.0069781]
		[batch 20/20] avg loss: 0.2648450520326973		[learning rate: 0.0069696]
	Learning Rate: 0.00696964
	LOSS [training: 0.2756222015285535 | validation: 0.27737932600833254]
	TIME [epoch: 8.81 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26900746227625255		[learning rate: 0.0069612]
		[batch 20/20] avg loss: 0.26413138703187855		[learning rate: 0.0069528]
	Learning Rate: 0.00695277
	LOSS [training: 0.2665694246540655 | validation: 0.24680737608864362]
	TIME [epoch: 8.81 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32345313147709276		[learning rate: 0.0069443]
		[batch 20/20] avg loss: 0.3235592321905314		[learning rate: 0.0069359]
	Learning Rate: 0.00693594
	LOSS [training: 0.3235061818338121 | validation: 0.41143337781287304]
	TIME [epoch: 8.82 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24636323746721106		[learning rate: 0.0069275]
		[batch 20/20] avg loss: 0.28332149205221824		[learning rate: 0.0069191]
	Learning Rate: 0.00691915
	LOSS [training: 0.2648423647597147 | validation: 0.36632232248090174]
	TIME [epoch: 8.8 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2567767560690547		[learning rate: 0.0069108]
		[batch 20/20] avg loss: 0.2310023424174577		[learning rate: 0.0069024]
	Learning Rate: 0.0069024
	LOSS [training: 0.24388954924325623 | validation: 0.121842378358961]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24052447672743757		[learning rate: 0.006894]
		[batch 20/20] avg loss: 0.2282879834194827		[learning rate: 0.0068857]
	Learning Rate: 0.00688569
	LOSS [training: 0.2344062300734601 | validation: 0.10796839959035856]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24854210897694734		[learning rate: 0.0068773]
		[batch 20/20] avg loss: 0.23212963743646475		[learning rate: 0.006869]
	Learning Rate: 0.00686902
	LOSS [training: 0.24033587320670596 | validation: 0.44921634259212073]
	TIME [epoch: 8.83 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4263160171890239		[learning rate: 0.0068607]
		[batch 20/20] avg loss: 0.2125395920651061		[learning rate: 0.0068524]
	Learning Rate: 0.00685239
	LOSS [training: 0.31942780462706494 | validation: 0.11594468355089133]
	TIME [epoch: 8.8 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23148941333681697		[learning rate: 0.0068441]
		[batch 20/20] avg loss: 0.25623668647556563		[learning rate: 0.0068358]
	Learning Rate: 0.0068358
	LOSS [training: 0.24386304990619134 | validation: 0.15483859299569236]
	TIME [epoch: 8.8 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26083541557446815		[learning rate: 0.0068275]
		[batch 20/20] avg loss: 0.1991821797352712		[learning rate: 0.0068193]
	Learning Rate: 0.00681925
	LOSS [training: 0.2300087976548697 | validation: 0.26922859120885395]
	TIME [epoch: 8.79 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2811725571104945		[learning rate: 0.006811]
		[batch 20/20] avg loss: 0.29389558111600705		[learning rate: 0.0068027]
	Learning Rate: 0.00680275
	LOSS [training: 0.28753406911325075 | validation: 0.4832941111083309]
	TIME [epoch: 8.81 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28171182653626914		[learning rate: 0.0067945]
		[batch 20/20] avg loss: 0.27622742345208556		[learning rate: 0.0067863]
	Learning Rate: 0.00678628
	LOSS [training: 0.2789696249941773 | validation: 0.13865558377683024]
	TIME [epoch: 8.81 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2183807140013582		[learning rate: 0.0067781]
		[batch 20/20] avg loss: 0.23779280691868684		[learning rate: 0.0067698]
	Learning Rate: 0.00676985
	LOSS [training: 0.22808676046002252 | validation: 0.13586460111008777]
	TIME [epoch: 8.8 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20700908839740592		[learning rate: 0.0067616]
		[batch 20/20] avg loss: 0.2806800612880592		[learning rate: 0.0067535]
	Learning Rate: 0.00675346
	LOSS [training: 0.24384457484273256 | validation: 0.16627647661519512]
	TIME [epoch: 8.8 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22994484480333957		[learning rate: 0.0067453]
		[batch 20/20] avg loss: 0.3771069959276504		[learning rate: 0.0067371]
	Learning Rate: 0.00673711
	LOSS [training: 0.30352592036549497 | validation: 0.20248086172537078]
	TIME [epoch: 8.79 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19104092173972506		[learning rate: 0.006729]
		[batch 20/20] avg loss: 0.3236615849400184		[learning rate: 0.0067208]
	Learning Rate: 0.0067208
	LOSS [training: 0.25735125333987174 | validation: 0.2712958058824829]
	TIME [epoch: 8.82 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3046620502729618		[learning rate: 0.0067127]
		[batch 20/20] avg loss: 0.20930180560074416		[learning rate: 0.0067045]
	Learning Rate: 0.00670453
	LOSS [training: 0.2569819279368529 | validation: 0.18825435557214046]
	TIME [epoch: 8.79 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28965492866822706		[learning rate: 0.0066964]
		[batch 20/20] avg loss: 0.2714737272099729		[learning rate: 0.0066883]
	Learning Rate: 0.0066883
	LOSS [training: 0.2805643279391 | validation: 0.14793779332627843]
	TIME [epoch: 8.79 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2504234907345377		[learning rate: 0.0066802]
		[batch 20/20] avg loss: 0.25933081454759954		[learning rate: 0.0066721]
	Learning Rate: 0.00667211
	LOSS [training: 0.2548771526410686 | validation: 0.25187430442325665]
	TIME [epoch: 8.79 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35882266945392893		[learning rate: 0.006664]
		[batch 20/20] avg loss: 0.3153309481542879		[learning rate: 0.006656]
	Learning Rate: 0.00665596
	LOSS [training: 0.33707680880410845 | validation: 0.2695598191017466]
	TIME [epoch: 8.8 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2271628122051675		[learning rate: 0.0066479]
		[batch 20/20] avg loss: 0.21768158434357873		[learning rate: 0.0066398]
	Learning Rate: 0.00663984
	LOSS [training: 0.22242219827437318 | validation: 0.31616947829117825]
	TIME [epoch: 8.83 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27115238482158727		[learning rate: 0.0066318]
		[batch 20/20] avg loss: 0.22454356584812532		[learning rate: 0.0066238]
	Learning Rate: 0.00662377
	LOSS [training: 0.24784797533485628 | validation: 0.17637859859221056]
	TIME [epoch: 8.8 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22762644195059697		[learning rate: 0.0066157]
		[batch 20/20] avg loss: 0.25116622177982206		[learning rate: 0.0066077]
	Learning Rate: 0.00660774
	LOSS [training: 0.23939633186520956 | validation: 0.25129367550185516]
	TIME [epoch: 8.8 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21491867637212275		[learning rate: 0.0065997]
		[batch 20/20] avg loss: 0.23289432017147757		[learning rate: 0.0065917]
	Learning Rate: 0.00659174
	LOSS [training: 0.2239064982718002 | validation: 0.1848035111371708]
	TIME [epoch: 8.8 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1857300195978724		[learning rate: 0.0065838]
		[batch 20/20] avg loss: 0.22401236707180985		[learning rate: 0.0065758]
	Learning Rate: 0.00657578
	LOSS [training: 0.20487119333484105 | validation: 0.34069143959754145]
	TIME [epoch: 8.81 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2895439145052683		[learning rate: 0.0065678]
		[batch 20/20] avg loss: 0.2542455735134489		[learning rate: 0.0065599]
	Learning Rate: 0.00655986
	LOSS [training: 0.2718947440093586 | validation: 0.22852267079500638]
	TIME [epoch: 8.8 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17956746882557903		[learning rate: 0.0065519]
		[batch 20/20] avg loss: 0.2302475877750585		[learning rate: 0.006544]
	Learning Rate: 0.00654398
	LOSS [training: 0.20490752830031872 | validation: 0.1632529484323209]
	TIME [epoch: 8.79 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2559426938544433		[learning rate: 0.0065361]
		[batch 20/20] avg loss: 0.20565575251228277		[learning rate: 0.0065281]
	Learning Rate: 0.00652814
	LOSS [training: 0.23079922318336304 | validation: 0.2443069646355802]
	TIME [epoch: 8.8 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22578189773160068		[learning rate: 0.0065202]
		[batch 20/20] avg loss: 0.2286057443165904		[learning rate: 0.0065123]
	Learning Rate: 0.00651234
	LOSS [training: 0.2271938210240955 | validation: 0.1746452081543241]
	TIME [epoch: 8.81 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21204261004542962		[learning rate: 0.0065044]
		[batch 20/20] avg loss: 0.2164292767115649		[learning rate: 0.0064966]
	Learning Rate: 0.00649657
	LOSS [training: 0.21423594337849722 | validation: 0.18900667955919692]
	TIME [epoch: 8.81 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22890552885284426		[learning rate: 0.0064887]
		[batch 20/20] avg loss: 0.2244238703651012		[learning rate: 0.0064808]
	Learning Rate: 0.00648084
	LOSS [training: 0.22666469960897273 | validation: 0.08179698537781285]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27195957512954133		[learning rate: 0.006473]
		[batch 20/20] avg loss: 0.2417003225166956		[learning rate: 0.0064652]
	Learning Rate: 0.00646516
	LOSS [training: 0.2568299488231185 | validation: 0.25288669283589504]
	TIME [epoch: 8.79 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2355049856453392		[learning rate: 0.0064573]
		[batch 20/20] avg loss: 0.2068636826820079		[learning rate: 0.0064495]
	Learning Rate: 0.0064495
	LOSS [training: 0.22118433416367358 | validation: 0.33613337741037624]
	TIME [epoch: 8.8 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2664380517256792		[learning rate: 0.0064417]
		[batch 20/20] avg loss: 0.18284168698300624		[learning rate: 0.0064339]
	Learning Rate: 0.00643389
	LOSS [training: 0.22463986935434277 | validation: 0.1304182844430786]
	TIME [epoch: 8.83 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18817269260633154		[learning rate: 0.0064261]
		[batch 20/20] avg loss: 0.20116703193474916		[learning rate: 0.0064183]
	Learning Rate: 0.00641832
	LOSS [training: 0.19466986227054034 | validation: 0.19803633488570765]
	TIME [epoch: 8.82 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18715589178436842		[learning rate: 0.0064105]
		[batch 20/20] avg loss: 0.1891425903408693		[learning rate: 0.0064028]
	Learning Rate: 0.00640278
	LOSS [training: 0.18814924106261885 | validation: 0.1918829451778968]
	TIME [epoch: 8.81 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3926721453437049		[learning rate: 0.006395]
		[batch 20/20] avg loss: 0.2510175358324198		[learning rate: 0.0063873]
	Learning Rate: 0.00638728
	LOSS [training: 0.3218448405880624 | validation: 0.22230807014691928]
	TIME [epoch: 8.8 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.261759976629713		[learning rate: 0.0063795]
		[batch 20/20] avg loss: 0.1980604639379357		[learning rate: 0.0063718]
	Learning Rate: 0.00637182
	LOSS [training: 0.22991022028382435 | validation: 0.17121716925027017]
	TIME [epoch: 8.8 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18932930436374945		[learning rate: 0.0063641]
		[batch 20/20] avg loss: 0.2684004862938113		[learning rate: 0.0063564]
	Learning Rate: 0.00635639
	LOSS [training: 0.22886489532878035 | validation: 0.11573055952180776]
	TIME [epoch: 8.8 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19909594246902368		[learning rate: 0.0063487]
		[batch 20/20] avg loss: 0.2232391063627112		[learning rate: 0.006341]
	Learning Rate: 0.006341
	LOSS [training: 0.21116752441586745 | validation: 0.2667386190164523]
	TIME [epoch: 8.79 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27895633681111576		[learning rate: 0.0063333]
		[batch 20/20] avg loss: 0.26133798825532245		[learning rate: 0.0063257]
	Learning Rate: 0.00632565
	LOSS [training: 0.27014716253321913 | validation: 0.10052977601110862]
	TIME [epoch: 8.8 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1896091270256954		[learning rate: 0.006318]
		[batch 20/20] avg loss: 0.26331195607789526		[learning rate: 0.0063103]
	Learning Rate: 0.00631034
	LOSS [training: 0.2264605415517953 | validation: 0.3621659888008291]
	TIME [epoch: 8.79 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2274487791594917		[learning rate: 0.0063027]
		[batch 20/20] avg loss: 0.21406111007166778		[learning rate: 0.0062951]
	Learning Rate: 0.00629506
	LOSS [training: 0.22075494461557973 | validation: 0.34326925389883]
	TIME [epoch: 8.82 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21333827176469708		[learning rate: 0.0062874]
		[batch 20/20] avg loss: 0.17839223060021142		[learning rate: 0.0062798]
	Learning Rate: 0.00627982
	LOSS [training: 0.19586525118245424 | validation: 0.1449083043809462]
	TIME [epoch: 8.79 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19031855625407207		[learning rate: 0.0062722]
		[batch 20/20] avg loss: 0.22853848952690337		[learning rate: 0.0062646]
	Learning Rate: 0.00626462
	LOSS [training: 0.20942852289048774 | validation: 0.32454923055111173]
	TIME [epoch: 8.79 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22387792728973288		[learning rate: 0.006257]
		[batch 20/20] avg loss: 0.19296590706288216		[learning rate: 0.0062495]
	Learning Rate: 0.00624945
	LOSS [training: 0.20842191717630748 | validation: 0.15815136988897546]
	TIME [epoch: 8.79 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23122480725408043		[learning rate: 0.0062419]
		[batch 20/20] avg loss: 0.23372183634119387		[learning rate: 0.0062343]
	Learning Rate: 0.00623433
	LOSS [training: 0.23247332179763713 | validation: 0.20545870682748463]
	TIME [epoch: 8.83 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20194468412960798		[learning rate: 0.0062268]
		[batch 20/20] avg loss: 0.18852287757554928		[learning rate: 0.0062192]
	Learning Rate: 0.00621923
	LOSS [training: 0.19523378085257864 | validation: 0.3096893395606415]
	TIME [epoch: 8.82 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24823143318134094		[learning rate: 0.0062117]
		[batch 20/20] avg loss: 0.1918141854296985		[learning rate: 0.0062042]
	Learning Rate: 0.00620418
	LOSS [training: 0.22002280930551973 | validation: 0.2163283285720563]
	TIME [epoch: 8.8 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2694177083870633		[learning rate: 0.0061967]
		[batch 20/20] avg loss: 0.23586782942949566		[learning rate: 0.0061892]
	Learning Rate: 0.00618916
	LOSS [training: 0.2526427689082795 | validation: 0.32198214543687353]
	TIME [epoch: 8.8 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18791436287808733		[learning rate: 0.0061817]
		[batch 20/20] avg loss: 0.1727037268585622		[learning rate: 0.0061742]
	Learning Rate: 0.00617417
	LOSS [training: 0.18030904486832477 | validation: 0.2042483330948731]
	TIME [epoch: 8.81 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25819305186146096		[learning rate: 0.0061667]
		[batch 20/20] avg loss: 0.23828256081554824		[learning rate: 0.0061592]
	Learning Rate: 0.00615923
	LOSS [training: 0.2482378063385046 | validation: 0.5000463585025419]
	TIME [epoch: 8.81 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2358336620142747		[learning rate: 0.0061518]
		[batch 20/20] avg loss: 0.2099231733295357		[learning rate: 0.0061443]
	Learning Rate: 0.00614432
	LOSS [training: 0.2228784176719052 | validation: 0.10278729171550714]
	TIME [epoch: 8.8 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3213312655764753		[learning rate: 0.0061369]
		[batch 20/20] avg loss: 0.2655046780222348		[learning rate: 0.0061294]
	Learning Rate: 0.00612944
	LOSS [training: 0.29341797179935514 | validation: 0.33305974746069295]
	TIME [epoch: 8.79 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22304334226484376		[learning rate: 0.006122]
		[batch 20/20] avg loss: 0.26117714105006723		[learning rate: 0.0061146]
	Learning Rate: 0.0061146
	LOSS [training: 0.2421102416574555 | validation: 0.24795119530097823]
	TIME [epoch: 8.8 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28772828371733866		[learning rate: 0.0061072]
		[batch 20/20] avg loss: 0.30156999558716546		[learning rate: 0.0060998]
	Learning Rate: 0.0060998
	LOSS [training: 0.29464913965225203 | validation: 0.18707123354309044]
	TIME [epoch: 8.81 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21355512914516472		[learning rate: 0.0060924]
		[batch 20/20] avg loss: 0.19602757291091996		[learning rate: 0.006085]
	Learning Rate: 0.00608504
	LOSS [training: 0.20479135102804236 | validation: 0.2172440286556889]
	TIME [epoch: 8.79 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24425113701600926		[learning rate: 0.0060777]
		[batch 20/20] avg loss: 0.23078854293956721		[learning rate: 0.0060703]
	Learning Rate: 0.00607031
	LOSS [training: 0.2375198399777882 | validation: 0.10316892540536515]
	TIME [epoch: 8.8 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17441480845836824		[learning rate: 0.006063]
		[batch 20/20] avg loss: 0.23419700030251384		[learning rate: 0.0060556]
	Learning Rate: 0.00605561
	LOSS [training: 0.20430590438044108 | validation: 0.19987419890809796]
	TIME [epoch: 8.79 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19042423785246823		[learning rate: 0.0060483]
		[batch 20/20] avg loss: 0.17953731228879335		[learning rate: 0.006041]
	Learning Rate: 0.00604095
	LOSS [training: 0.1849807750706308 | validation: 0.17736881942550173]
	TIME [epoch: 8.8 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21046919149590032		[learning rate: 0.0060336]
		[batch 20/20] avg loss: 0.23854882617170453		[learning rate: 0.0060263]
	Learning Rate: 0.00602633
	LOSS [training: 0.22450900883380243 | validation: 0.20084034428914804]
	TIME [epoch: 8.82 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29017080757628044		[learning rate: 0.006019]
		[batch 20/20] avg loss: 0.15917671718423065		[learning rate: 0.0060117]
	Learning Rate: 0.00601174
	LOSS [training: 0.2246737623802555 | validation: 0.16956130289350407]
	TIME [epoch: 8.8 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3317486311203016		[learning rate: 0.0060045]
		[batch 20/20] avg loss: 0.1866066648259948		[learning rate: 0.0059972]
	Learning Rate: 0.00599718
	LOSS [training: 0.2591776479731482 | validation: 0.15163459958985526]
	TIME [epoch: 8.8 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24670276071672143		[learning rate: 0.0059899]
		[batch 20/20] avg loss: 0.36057664047497606		[learning rate: 0.0059827]
	Learning Rate: 0.00598267
	LOSS [training: 0.3036397005958487 | validation: 0.18277284241794473]
	TIME [epoch: 8.8 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18126056487456138		[learning rate: 0.0059754]
		[batch 20/20] avg loss: 0.31635485576638933		[learning rate: 0.0059682]
	Learning Rate: 0.00596818
	LOSS [training: 0.24880771032047538 | validation: 0.24553002033709548]
	TIME [epoch: 8.81 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19899457658564096		[learning rate: 0.005961]
		[batch 20/20] avg loss: 0.24338111146328195		[learning rate: 0.0059537]
	Learning Rate: 0.00595374
	LOSS [training: 0.22118784402446146 | validation: 0.26416873681256375]
	TIME [epoch: 8.78 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26550570842493804		[learning rate: 0.0059465]
		[batch 20/20] avg loss: 0.21463629568375495		[learning rate: 0.0059393]
	Learning Rate: 0.00593932
	LOSS [training: 0.24007100205434653 | validation: 0.3021174187594772]
	TIME [epoch: 8.79 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24947922603036843		[learning rate: 0.0059321]
		[batch 20/20] avg loss: 0.22473284686392914		[learning rate: 0.0059249]
	Learning Rate: 0.00592494
	LOSS [training: 0.23710603644714875 | validation: 0.17810245867151445]
	TIME [epoch: 8.79 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2712259349637782		[learning rate: 0.0059178]
		[batch 20/20] avg loss: 0.1770876844276174		[learning rate: 0.0059106]
	Learning Rate: 0.0059106
	LOSS [training: 0.22415680969569776 | validation: 0.23540327867858712]
	TIME [epoch: 8.81 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22429229591375907		[learning rate: 0.0059034]
		[batch 20/20] avg loss: 0.22079918373041343		[learning rate: 0.0058963]
	Learning Rate: 0.00589629
	LOSS [training: 0.2225457398220863 | validation: 0.18485239949714838]
	TIME [epoch: 8.8 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23211158645874655		[learning rate: 0.0058892]
		[batch 20/20] avg loss: 0.24147867716294433		[learning rate: 0.005882]
	Learning Rate: 0.00588202
	LOSS [training: 0.23679513181084544 | validation: 0.12042905982921229]
	TIME [epoch: 8.79 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16045285735266526		[learning rate: 0.0058749]
		[batch 20/20] avg loss: 0.2564419839493049		[learning rate: 0.0058678]
	Learning Rate: 0.00586778
	LOSS [training: 0.20844742065098507 | validation: 0.14145270332499751]
	TIME [epoch: 8.79 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16105004356578506		[learning rate: 0.0058607]
		[batch 20/20] avg loss: 0.19545860324701975		[learning rate: 0.0058536]
	Learning Rate: 0.00585357
	LOSS [training: 0.17825432340640243 | validation: 0.22636369962808958]
	TIME [epoch: 8.8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2134516963382933		[learning rate: 0.0058465]
		[batch 20/20] avg loss: 0.23540667247141775		[learning rate: 0.0058394]
	Learning Rate: 0.0058394
	LOSS [training: 0.22442918440485551 | validation: 0.24311239946739344]
	TIME [epoch: 8.81 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24333466448871416		[learning rate: 0.0058323]
		[batch 20/20] avg loss: 0.21146344042097173		[learning rate: 0.0058253]
	Learning Rate: 0.00582527
	LOSS [training: 0.22739905245484296 | validation: 0.15777349385369344]
	TIME [epoch: 8.8 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21273062365239478		[learning rate: 0.0058182]
		[batch 20/20] avg loss: 0.18909626628650417		[learning rate: 0.0058112]
	Learning Rate: 0.00581116
	LOSS [training: 0.20091344496944946 | validation: 0.19793948625514188]
	TIME [epoch: 8.8 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21205931820774468		[learning rate: 0.0058041]
		[batch 20/20] avg loss: 0.15937011518807512		[learning rate: 0.0057971]
	Learning Rate: 0.0057971
	LOSS [training: 0.1857147166979099 | validation: 0.12599428132441104]
	TIME [epoch: 8.81 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18992418835875052		[learning rate: 0.0057901]
		[batch 20/20] avg loss: 0.23710932684277522		[learning rate: 0.0057831]
	Learning Rate: 0.00578306
	LOSS [training: 0.2135167576007629 | validation: 0.13168479462592048]
	TIME [epoch: 8.8 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13041113748101876		[learning rate: 0.0057761]
		[batch 20/20] avg loss: 0.2847784566829216		[learning rate: 0.0057691]
	Learning Rate: 0.00576906
	LOSS [training: 0.2075947970819702 | validation: 0.1820666578104194]
	TIME [epoch: 8.79 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28947177436734783		[learning rate: 0.0057621]
		[batch 20/20] avg loss: 0.20010925718479533		[learning rate: 0.0057551]
	Learning Rate: 0.0057551
	LOSS [training: 0.24479051577607164 | validation: 0.15152874568820743]
	TIME [epoch: 8.79 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2081775416866687		[learning rate: 0.0057481]
		[batch 20/20] avg loss: 0.22465539170866236		[learning rate: 0.0057412]
	Learning Rate: 0.00574116
	LOSS [training: 0.2164164666976655 | validation: 0.1330202616854003]
	TIME [epoch: 8.79 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15214987662560203		[learning rate: 0.0057342]
		[batch 20/20] avg loss: 0.30328258512154493		[learning rate: 0.0057273]
	Learning Rate: 0.00572727
	LOSS [training: 0.22771623087357348 | validation: 0.1877118357449669]
	TIME [epoch: 8.82 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18738224987437196		[learning rate: 0.0057203]
		[batch 20/20] avg loss: 0.2186225696317785		[learning rate: 0.0057134]
	Learning Rate: 0.0057134
	LOSS [training: 0.20300240975307524 | validation: 0.2010992323035866]
	TIME [epoch: 8.79 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.202555462217107		[learning rate: 0.0057065]
		[batch 20/20] avg loss: 0.2095312624453241		[learning rate: 0.0056996]
	Learning Rate: 0.00569957
	LOSS [training: 0.20604336233121553 | validation: 0.17726418701805285]
	TIME [epoch: 8.79 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25385298167717		[learning rate: 0.0056927]
		[batch 20/20] avg loss: 0.1720026139102724		[learning rate: 0.0056858]
	Learning Rate: 0.00568577
	LOSS [training: 0.21292779779372117 | validation: 0.11533936321256058]
	TIME [epoch: 8.78 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17701598850829325		[learning rate: 0.0056789]
		[batch 20/20] avg loss: 0.14918135327152832		[learning rate: 0.005672]
	Learning Rate: 0.00567201
	LOSS [training: 0.1630986708899108 | validation: 0.08750003484874318]
	TIME [epoch: 8.81 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19058717095823807		[learning rate: 0.0056651]
		[batch 20/20] avg loss: 0.17431946678966342		[learning rate: 0.0056583]
	Learning Rate: 0.00565828
	LOSS [training: 0.18245331887395072 | validation: 0.1295813840637244]
	TIME [epoch: 8.79 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1757888280332545		[learning rate: 0.0056514]
		[batch 20/20] avg loss: 0.34133427718367243		[learning rate: 0.0056446]
	Learning Rate: 0.00564458
	LOSS [training: 0.2585615526084635 | validation: 0.26327604452590864]
	TIME [epoch: 8.79 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1881624852414449		[learning rate: 0.0056377]
		[batch 20/20] avg loss: 0.2378614104484022		[learning rate: 0.0056309]
	Learning Rate: 0.00563092
	LOSS [training: 0.21301194784492355 | validation: 0.3022006791933082]
	TIME [epoch: 8.8 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2159345831106671		[learning rate: 0.0056241]
		[batch 20/20] avg loss: 0.21864201898107521		[learning rate: 0.0056173]
	Learning Rate: 0.00561728
	LOSS [training: 0.2172883010458711 | validation: 0.11062811854968616]
	TIME [epoch: 8.8 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18134882564675187		[learning rate: 0.0056105]
		[batch 20/20] avg loss: 0.1719125089605073		[learning rate: 0.0056037]
	Learning Rate: 0.00560368
	LOSS [training: 0.17663066730362956 | validation: 0.21323393260741708]
	TIME [epoch: 8.81 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17790348971828646		[learning rate: 0.0055969]
		[batch 20/20] avg loss: 0.14100010020944462		[learning rate: 0.0055901]
	Learning Rate: 0.00559012
	LOSS [training: 0.15945179496386555 | validation: 0.11563825469381989]
	TIME [epoch: 8.79 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23541658980265828		[learning rate: 0.0055833]
		[batch 20/20] avg loss: 0.1880255405766379		[learning rate: 0.0055766]
	Learning Rate: 0.00557659
	LOSS [training: 0.2117210651896481 | validation: 0.16109091620714158]
	TIME [epoch: 8.79 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2409968111152309		[learning rate: 0.0055698]
		[batch 20/20] avg loss: 0.18402370391194417		[learning rate: 0.0055631]
	Learning Rate: 0.00556309
	LOSS [training: 0.2125102575135876 | validation: 0.16410633657255158]
	TIME [epoch: 8.8 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17374638544763424		[learning rate: 0.0055563]
		[batch 20/20] avg loss: 0.17440585445940818		[learning rate: 0.0055496]
	Learning Rate: 0.00554962
	LOSS [training: 0.17407611995352118 | validation: 0.22039689604631235]
	TIME [epoch: 8.81 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17561387036488285		[learning rate: 0.0055429]
		[batch 20/20] avg loss: 0.18740360441299145		[learning rate: 0.0055362]
	Learning Rate: 0.00553618
	LOSS [training: 0.18150873738893716 | validation: 0.2885053973820671]
	TIME [epoch: 8.79 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2079114552411201		[learning rate: 0.0055295]
		[batch 20/20] avg loss: 0.1806003123595627		[learning rate: 0.0055228]
	Learning Rate: 0.00552278
	LOSS [training: 0.19425588380034142 | validation: 0.11741219242346095]
	TIME [epoch: 8.78 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17058650486588983		[learning rate: 0.0055161]
		[batch 20/20] avg loss: 0.18928403604120472		[learning rate: 0.0055094]
	Learning Rate: 0.00550941
	LOSS [training: 0.17993527045354724 | validation: 0.22962174806631172]
	TIME [epoch: 8.79 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.175721863711464		[learning rate: 0.0055027]
		[batch 20/20] avg loss: 0.25854020951623174		[learning rate: 0.0054961]
	Learning Rate: 0.00549607
	LOSS [training: 0.21713103661384792 | validation: 0.19847838606151746]
	TIME [epoch: 8.81 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15849222692418224		[learning rate: 0.0054894]
		[batch 20/20] avg loss: 0.16116689470307244		[learning rate: 0.0054828]
	Learning Rate: 0.00548277
	LOSS [training: 0.15982956081362737 | validation: 0.13702827735679599]
	TIME [epoch: 8.79 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24915258496377612		[learning rate: 0.0054761]
		[batch 20/20] avg loss: 0.26450225686334417		[learning rate: 0.0054695]
	Learning Rate: 0.0054695
	LOSS [training: 0.2568274209135602 | validation: 0.29400196435494935]
	TIME [epoch: 8.79 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2224504792357985		[learning rate: 0.0054629]
		[batch 20/20] avg loss: 0.16293912172033767		[learning rate: 0.0054563]
	Learning Rate: 0.00545626
	LOSS [training: 0.19269480047806814 | validation: 0.1345532626670629]
	TIME [epoch: 8.79 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14475847434654301		[learning rate: 0.0054496]
		[batch 20/20] avg loss: 0.1365339639158903		[learning rate: 0.005443]
	Learning Rate: 0.00544305
	LOSS [training: 0.14064621913121667 | validation: 0.08882778501153837]
	TIME [epoch: 8.82 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15988392656213174		[learning rate: 0.0054365]
		[batch 20/20] avg loss: 0.15307606527693593		[learning rate: 0.0054299]
	Learning Rate: 0.00542987
	LOSS [training: 0.15647999591953385 | validation: 0.1590340209132002]
	TIME [epoch: 8.82 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19260935434125992		[learning rate: 0.0054233]
		[batch 20/20] avg loss: 0.19265857029528813		[learning rate: 0.0054167]
	Learning Rate: 0.00541673
	LOSS [training: 0.192633962318274 | validation: 0.17382373967501039]
	TIME [epoch: 8.79 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17801035374655202		[learning rate: 0.0054102]
		[batch 20/20] avg loss: 0.17062410609496464		[learning rate: 0.0054036]
	Learning Rate: 0.00540361
	LOSS [training: 0.17431722992075835 | validation: 0.37444362861788005]
	TIME [epoch: 8.79 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22046473611564782		[learning rate: 0.0053971]
		[batch 20/20] avg loss: 0.18884547657054057		[learning rate: 0.0053905]
	Learning Rate: 0.00539053
	LOSS [training: 0.2046551063430942 | validation: 0.15541722232247668]
	TIME [epoch: 8.79 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15893998079849042		[learning rate: 0.005384]
		[batch 20/20] avg loss: 0.17803540452353933		[learning rate: 0.0053775]
	Learning Rate: 0.00537748
	LOSS [training: 0.16848769266101485 | validation: 0.32704261631478465]
	TIME [epoch: 8.82 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23802191571565343		[learning rate: 0.005371]
		[batch 20/20] avg loss: 0.2649157468397152		[learning rate: 0.0053645]
	Learning Rate: 0.00536446
	LOSS [training: 0.25146883127768427 | validation: 0.23369424973355213]
	TIME [epoch: 8.79 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19829817085956206		[learning rate: 0.005358]
		[batch 20/20] avg loss: 0.19954000491753907		[learning rate: 0.0053515]
	Learning Rate: 0.00535148
	LOSS [training: 0.19891908788855056 | validation: 0.1376331688720394]
	TIME [epoch: 8.79 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18641409603716672		[learning rate: 0.005345]
		[batch 20/20] avg loss: 0.2557599242585258		[learning rate: 0.0053385]
	Learning Rate: 0.00533852
	LOSS [training: 0.22108701014784624 | validation: 0.19106487036080858]
	TIME [epoch: 8.79 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15087339743809236		[learning rate: 0.0053321]
		[batch 20/20] avg loss: 0.19804706170233133		[learning rate: 0.0053256]
	Learning Rate: 0.0053256
	LOSS [training: 0.17446022957021184 | validation: 0.19580712158859792]
	TIME [epoch: 8.81 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19265066030659353		[learning rate: 0.0053191]
		[batch 20/20] avg loss: 0.16031135955307105		[learning rate: 0.0053127]
	Learning Rate: 0.00531271
	LOSS [training: 0.1764810099298323 | validation: 0.16684359641145613]
	TIME [epoch: 8.79 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13506081183580143		[learning rate: 0.0053063]
		[batch 20/20] avg loss: 0.1692383833714939		[learning rate: 0.0052998]
	Learning Rate: 0.00529984
	LOSS [training: 0.15214959760364766 | validation: 0.20186256266919722]
	TIME [epoch: 8.78 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1303779455349356		[learning rate: 0.0052934]
		[batch 20/20] avg loss: 0.1946265051077863		[learning rate: 0.005287]
	Learning Rate: 0.00528701
	LOSS [training: 0.162502225321361 | validation: 0.1790339054570015]
	TIME [epoch: 8.79 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23804445189503967		[learning rate: 0.0052806]
		[batch 20/20] avg loss: 0.25740367303924355		[learning rate: 0.0052742]
	Learning Rate: 0.00527422
	LOSS [training: 0.24772406246714157 | validation: 0.11473576595982762]
	TIME [epoch: 8.8 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15972232782522747		[learning rate: 0.0052678]
		[batch 20/20] avg loss: 0.30648119965482395		[learning rate: 0.0052614]
	Learning Rate: 0.00526145
	LOSS [training: 0.23310176374002567 | validation: 0.23464388075237314]
	TIME [epoch: 8.8 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22268256517483187		[learning rate: 0.0052551]
		[batch 20/20] avg loss: 0.14689563725100113		[learning rate: 0.0052487]
	Learning Rate: 0.00524871
	LOSS [training: 0.18478910121291647 | validation: 0.14494411437366514]
	TIME [epoch: 8.79 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1452103777564022		[learning rate: 0.0052424]
		[batch 20/20] avg loss: 0.15223574703892642		[learning rate: 0.005236]
	Learning Rate: 0.005236
	LOSS [training: 0.14872306239766434 | validation: 0.12244432512292494]
	TIME [epoch: 8.79 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1824905296931252		[learning rate: 0.0052297]
		[batch 20/20] avg loss: 0.16474213977645247		[learning rate: 0.0052233]
	Learning Rate: 0.00522333
	LOSS [training: 0.17361633473478885 | validation: 0.27532594415863365]
	TIME [epoch: 8.8 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.214680201759447		[learning rate: 0.005217]
		[batch 20/20] avg loss: 0.15552455768937962		[learning rate: 0.0052107]
	Learning Rate: 0.00521068
	LOSS [training: 0.18510237972441335 | validation: 0.16764710369610428]
	TIME [epoch: 8.81 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15516301492273304		[learning rate: 0.0052044]
		[batch 20/20] avg loss: 0.2385355650764862		[learning rate: 0.0051981]
	Learning Rate: 0.00519807
	LOSS [training: 0.19684928999960966 | validation: 0.20611174270425253]
	TIME [epoch: 8.79 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1807354005815825		[learning rate: 0.0051918]
		[batch 20/20] avg loss: 0.16286480234131637		[learning rate: 0.0051855]
	Learning Rate: 0.00518549
	LOSS [training: 0.17180010146144942 | validation: 0.3572361343792916]
	TIME [epoch: 8.81 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22001469209441052		[learning rate: 0.0051792]
		[batch 20/20] avg loss: 0.1696263891048409		[learning rate: 0.0051729]
	Learning Rate: 0.00517293
	LOSS [training: 0.19482054059962567 | validation: 0.42162152559241556]
	TIME [epoch: 8.79 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2643441436905561		[learning rate: 0.0051667]
		[batch 20/20] avg loss: 0.3459131589723857		[learning rate: 0.0051604]
	Learning Rate: 0.00516041
	LOSS [training: 0.30512865133147093 | validation: 0.23633191242260254]
	TIME [epoch: 8.82 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17923285031437125		[learning rate: 0.0051542]
		[batch 20/20] avg loss: 0.19007921704386946		[learning rate: 0.0051479]
	Learning Rate: 0.00514792
	LOSS [training: 0.18465603367912034 | validation: 0.12676687261016042]
	TIME [epoch: 8.8 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14855879571867195		[learning rate: 0.0051417]
		[batch 20/20] avg loss: 0.1750123638792455		[learning rate: 0.0051355]
	Learning Rate: 0.00513546
	LOSS [training: 0.16178557979895877 | validation: 0.17287718959990403]
	TIME [epoch: 8.8 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2527847706534906		[learning rate: 0.0051292]
		[batch 20/20] avg loss: 0.21058173582900572		[learning rate: 0.005123]
	Learning Rate: 0.00512302
	LOSS [training: 0.2316832532412481 | validation: 0.22432515838100897]
	TIME [epoch: 8.8 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16959038080832842		[learning rate: 0.0051168]
		[batch 20/20] avg loss: 0.1414341712851958		[learning rate: 0.0051106]
	Learning Rate: 0.00511062
	LOSS [training: 0.15551227604676213 | validation: 0.4006553016221521]
	TIME [epoch: 8.81 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16528207170845083		[learning rate: 0.0051044]
		[batch 20/20] avg loss: 0.1765696002593477		[learning rate: 0.0050982]
	Learning Rate: 0.00509825
	LOSS [training: 0.17092583598389927 | validation: 0.13302317079100973]
	TIME [epoch: 8.81 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18492788238254992		[learning rate: 0.0050921]
		[batch 20/20] avg loss: 0.23738552504405744		[learning rate: 0.0050859]
	Learning Rate: 0.00508591
	LOSS [training: 0.21115670371330367 | validation: 0.12507719363480466]
	TIME [epoch: 8.81 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1937766244880687		[learning rate: 0.0050797]
		[batch 20/20] avg loss: 0.1890894891624136		[learning rate: 0.0050736]
	Learning Rate: 0.00507359
	LOSS [training: 0.1914330568252412 | validation: 0.2497453385307379]
	TIME [epoch: 8.88 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18205449093969392		[learning rate: 0.0050674]
		[batch 20/20] avg loss: 0.2399004320424063		[learning rate: 0.0050613]
	Learning Rate: 0.00506131
	LOSS [training: 0.2109774614910501 | validation: 0.38414216065780665]
	TIME [epoch: 8.91 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1816654939195914		[learning rate: 0.0050552]
		[batch 20/20] avg loss: 0.1775158357288838		[learning rate: 0.0050491]
	Learning Rate: 0.00504906
	LOSS [training: 0.17959066482423755 | validation: 0.16469543485412685]
	TIME [epoch: 8.92 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16384595011129552		[learning rate: 0.0050429]
		[batch 20/20] avg loss: 0.17517724457544653		[learning rate: 0.0050368]
	Learning Rate: 0.00503684
	LOSS [training: 0.169511597343371 | validation: 0.10489468745777195]
	TIME [epoch: 8.93 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10289641476689708		[learning rate: 0.0050307]
		[batch 20/20] avg loss: 0.15484807774130296		[learning rate: 0.0050246]
	Learning Rate: 0.00502464
	LOSS [training: 0.12887224625410001 | validation: 0.13600740070298273]
	TIME [epoch: 8.81 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1735716478858927		[learning rate: 0.0050186]
		[batch 20/20] avg loss: 0.15460400074231373		[learning rate: 0.0050125]
	Learning Rate: 0.00501248
	LOSS [training: 0.16408782431410318 | validation: 0.1345227318287134]
	TIME [epoch: 8.81 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2027112091063569		[learning rate: 0.0050064]
		[batch 20/20] avg loss: 0.1928154577986759		[learning rate: 0.0050003]
	Learning Rate: 0.00500034
	LOSS [training: 0.19776333345251637 | validation: 0.3240449929191911]
	TIME [epoch: 8.82 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18332345485980514		[learning rate: 0.0049943]
		[batch 20/20] avg loss: 0.19205951166863802		[learning rate: 0.0049882]
	Learning Rate: 0.00498824
	LOSS [training: 0.1876914832642216 | validation: 0.3123689312128233]
	TIME [epoch: 8.8 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15279741877861092		[learning rate: 0.0049822]
		[batch 20/20] avg loss: 0.24319941614904997		[learning rate: 0.0049762]
	Learning Rate: 0.00497616
	LOSS [training: 0.19799841746383043 | validation: 0.21211079288276877]
	TIME [epoch: 8.8 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13828511822087025		[learning rate: 0.0049701]
		[batch 20/20] avg loss: 0.23032633106371678		[learning rate: 0.0049641]
	Learning Rate: 0.00496412
	LOSS [training: 0.18430572464229353 | validation: 0.17434922835822117]
	TIME [epoch: 8.8 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19588648534135783		[learning rate: 0.0049581]
		[batch 20/20] avg loss: 0.15052467406648687		[learning rate: 0.0049521]
	Learning Rate: 0.0049521
	LOSS [training: 0.1732055797039223 | validation: 0.7759462201746103]
	TIME [epoch: 8.82 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3139629761988404		[learning rate: 0.0049461]
		[batch 20/20] avg loss: 0.1567301277501017		[learning rate: 0.0049401]
	Learning Rate: 0.00494011
	LOSS [training: 0.23534655197447102 | validation: 0.10674539577960834]
	TIME [epoch: 8.81 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15519977935831672		[learning rate: 0.0049341]
		[batch 20/20] avg loss: 0.16259081885441143		[learning rate: 0.0049282]
	Learning Rate: 0.00492815
	LOSS [training: 0.15889529910636407 | validation: 0.14781799485266062]
	TIME [epoch: 8.81 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20179491598354676		[learning rate: 0.0049222]
		[batch 20/20] avg loss: 0.14223577874757481		[learning rate: 0.0049162]
	Learning Rate: 0.00491622
	LOSS [training: 0.17201534736556082 | validation: 0.10151216420086742]
	TIME [epoch: 8.8 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20263608758316023		[learning rate: 0.0049103]
		[batch 20/20] avg loss: 0.1918931633912235		[learning rate: 0.0049043]
	Learning Rate: 0.00490432
	LOSS [training: 0.19726462548719184 | validation: 0.22466650341519806]
	TIME [epoch: 8.79 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18618275283871538		[learning rate: 0.0048984]
		[batch 20/20] avg loss: 0.17922631586371446		[learning rate: 0.0048924]
	Learning Rate: 0.00489245
	LOSS [training: 0.18270453435121495 | validation: 0.18280061764429895]
	TIME [epoch: 8.8 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17550268258454754		[learning rate: 0.0048865]
		[batch 20/20] avg loss: 0.16516005789712723		[learning rate: 0.0048806]
	Learning Rate: 0.00488061
	LOSS [training: 0.1703313702408374 | validation: 0.20429774489826327]
	TIME [epoch: 8.79 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23238251182231356		[learning rate: 0.0048747]
		[batch 20/20] avg loss: 0.15526711399235202		[learning rate: 0.0048688]
	Learning Rate: 0.00486879
	LOSS [training: 0.1938248129073328 | validation: 0.23479948716905533]
	TIME [epoch: 8.79 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2203290864026249		[learning rate: 0.0048629]
		[batch 20/20] avg loss: 0.1569160556854114		[learning rate: 0.004857]
	Learning Rate: 0.004857
	LOSS [training: 0.1886225710440182 | validation: 0.07605572825078702]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1247553496357369		[learning rate: 0.0048511]
		[batch 20/20] avg loss: 0.15096772973276884		[learning rate: 0.0048452]
	Learning Rate: 0.00484525
	LOSS [training: 0.13786153968425285 | validation: 0.18556591128089764]
	TIME [epoch: 8.81 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13844248199604373		[learning rate: 0.0048394]
		[batch 20/20] avg loss: 0.1419202431047188		[learning rate: 0.0048335]
	Learning Rate: 0.00483352
	LOSS [training: 0.14018136255038127 | validation: 0.20508415396001858]
	TIME [epoch: 8.79 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.155317483191428		[learning rate: 0.0048277]
		[batch 20/20] avg loss: 0.13753842063570043		[learning rate: 0.0048218]
	Learning Rate: 0.00482181
	LOSS [training: 0.14642795191356425 | validation: 0.17252608957564053]
	TIME [epoch: 8.78 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22449520676065698		[learning rate: 0.004816]
		[batch 20/20] avg loss: 0.14387331696673591		[learning rate: 0.0048101]
	Learning Rate: 0.00481014
	LOSS [training: 0.18418426186369644 | validation: 0.12635283515388465]
	TIME [epoch: 8.79 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16039538035956183		[learning rate: 0.0048043]
		[batch 20/20] avg loss: 0.21359091339038053		[learning rate: 0.0047985]
	Learning Rate: 0.0047985
	LOSS [training: 0.1869931468749712 | validation: 0.17100948604179997]
	TIME [epoch: 8.79 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1274216648012461		[learning rate: 0.0047927]
		[batch 20/20] avg loss: 0.1451968248248829		[learning rate: 0.0047869]
	Learning Rate: 0.00478688
	LOSS [training: 0.13630924481306447 | validation: 0.10185266427784498]
	TIME [epoch: 8.8 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1542233801138749		[learning rate: 0.0047811]
		[batch 20/20] avg loss: 0.17675727836090682		[learning rate: 0.0047753]
	Learning Rate: 0.00477529
	LOSS [training: 0.16549032923739088 | validation: 0.1599918993142379]
	TIME [epoch: 8.8 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15931030910452834		[learning rate: 0.0047695]
		[batch 20/20] avg loss: 0.12398393140774136		[learning rate: 0.0047637]
	Learning Rate: 0.00476373
	LOSS [training: 0.14164712025613485 | validation: 0.29431629403750736]
	TIME [epoch: 8.79 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18040026354760852		[learning rate: 0.004758]
		[batch 20/20] avg loss: 0.18852261597018397		[learning rate: 0.0047522]
	Learning Rate: 0.0047522
	LOSS [training: 0.18446143975889623 | validation: 0.21822472103795992]
	TIME [epoch: 8.79 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.155098506821992		[learning rate: 0.0047464]
		[batch 20/20] avg loss: 0.24130201628714182		[learning rate: 0.0047407]
	Learning Rate: 0.0047407
	LOSS [training: 0.1982002615545669 | validation: 0.3156515756146451]
	TIME [epoch: 8.81 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1712604088404463		[learning rate: 0.004735]
		[batch 20/20] avg loss: 0.13538614680354932		[learning rate: 0.0047292]
	Learning Rate: 0.00472922
	LOSS [training: 0.1533232778219978 | validation: 0.14893003246368103]
	TIME [epoch: 8.79 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1570236470943706		[learning rate: 0.0047235]
		[batch 20/20] avg loss: 0.18447412404507543		[learning rate: 0.0047178]
	Learning Rate: 0.00471777
	LOSS [training: 0.17074888556972306 | validation: 0.12895185205131263]
	TIME [epoch: 8.78 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16797450261260755		[learning rate: 0.0047121]
		[batch 20/20] avg loss: 0.1234979469235398		[learning rate: 0.0047064]
	Learning Rate: 0.00470635
	LOSS [training: 0.14573622476807366 | validation: 0.2927351604253662]
	TIME [epoch: 8.79 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2190056390836194		[learning rate: 0.0047006]
		[batch 20/20] avg loss: 0.2219951621295281		[learning rate: 0.004695]
	Learning Rate: 0.00469496
	LOSS [training: 0.22050040060657375 | validation: 0.26576414915727525]
	TIME [epoch: 8.8 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14993580455162797		[learning rate: 0.0046893]
		[batch 20/20] avg loss: 0.13925988313624393		[learning rate: 0.0046836]
	Learning Rate: 0.00468359
	LOSS [training: 0.14459784384393598 | validation: 0.14493106080855223]
	TIME [epoch: 8.8 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18499096080571803		[learning rate: 0.0046779]
		[batch 20/20] avg loss: 0.1874875350595791		[learning rate: 0.0046723]
	Learning Rate: 0.00467225
	LOSS [training: 0.1862392479326485 | validation: 0.3099229818597472]
	TIME [epoch: 8.79 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15520690163992207		[learning rate: 0.0046666]
		[batch 20/20] avg loss: 0.17059460509423585		[learning rate: 0.0046609]
	Learning Rate: 0.00466094
	LOSS [training: 0.16290075336707893 | validation: 0.13112304867444563]
	TIME [epoch: 8.79 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2222513302423526		[learning rate: 0.0046553]
		[batch 20/20] avg loss: 0.19056384132782078		[learning rate: 0.0046497]
	Learning Rate: 0.00464966
	LOSS [training: 0.20640758578508667 | validation: 0.4757103346617029]
	TIME [epoch: 8.78 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25474667823878483		[learning rate: 0.004644]
		[batch 20/20] avg loss: 0.15538696145169512		[learning rate: 0.0046384]
	Learning Rate: 0.0046384
	LOSS [training: 0.20506681984523994 | validation: 0.20737948672525963]
	TIME [epoch: 8.81 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16813405860007158		[learning rate: 0.0046328]
		[batch 20/20] avg loss: 0.14964158944095562		[learning rate: 0.0046272]
	Learning Rate: 0.00462717
	LOSS [training: 0.15888782402051357 | validation: 0.10354069326603173]
	TIME [epoch: 8.8 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1536492797312394		[learning rate: 0.0046216]
		[batch 20/20] avg loss: 0.17389658312029177		[learning rate: 0.004616]
	Learning Rate: 0.00461597
	LOSS [training: 0.16377293142576563 | validation: 0.13246460478508246]
	TIME [epoch: 8.8 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23598725347512883		[learning rate: 0.0046104]
		[batch 20/20] avg loss: 0.167756314623275		[learning rate: 0.0046048]
	Learning Rate: 0.0046048
	LOSS [training: 0.20187178404920192 | validation: 0.08056120407206005]
	TIME [epoch: 8.8 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16630271856762083		[learning rate: 0.0045992]
		[batch 20/20] avg loss: 0.14589185892365544		[learning rate: 0.0045936]
	Learning Rate: 0.00459365
	LOSS [training: 0.15609728874563816 | validation: 0.1515701259898368]
	TIME [epoch: 8.81 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19385876667840493		[learning rate: 0.0045881]
		[batch 20/20] avg loss: 0.2881551802602972		[learning rate: 0.0045825]
	Learning Rate: 0.00458253
	LOSS [training: 0.24100697346935102 | validation: 0.12108418351908978]
	TIME [epoch: 8.81 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22445545140890744		[learning rate: 0.004577]
		[batch 20/20] avg loss: 0.16051018951591373		[learning rate: 0.0045714]
	Learning Rate: 0.00457144
	LOSS [training: 0.19248282046241055 | validation: 0.15453006096198102]
	TIME [epoch: 8.78 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14691884914248535		[learning rate: 0.0045659]
		[batch 20/20] avg loss: 0.20109606944044406		[learning rate: 0.0045604]
	Learning Rate: 0.00456037
	LOSS [training: 0.1740074592914647 | validation: 0.1452788406177758]
	TIME [epoch: 8.79 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15716361251705963		[learning rate: 0.0045548]
		[batch 20/20] avg loss: 0.29685453804169387		[learning rate: 0.0045493]
	Learning Rate: 0.00454933
	LOSS [training: 0.22700907527937675 | validation: 0.42868062076800584]
	TIME [epoch: 8.8 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17833782479294477		[learning rate: 0.0045438]
		[batch 20/20] avg loss: 0.18036813214103412		[learning rate: 0.0045383]
	Learning Rate: 0.00453832
	LOSS [training: 0.17935297846698944 | validation: 0.08332806874182572]
	TIME [epoch: 8.81 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15382335318295173		[learning rate: 0.0045328]
		[batch 20/20] avg loss: 0.19145320245328645		[learning rate: 0.0045273]
	Learning Rate: 0.00452733
	LOSS [training: 0.17263827781811908 | validation: 0.23257069867646357]
	TIME [epoch: 8.79 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16684411943951605		[learning rate: 0.0045218]
		[batch 20/20] avg loss: 0.12366706121413143		[learning rate: 0.0045164]
	Learning Rate: 0.00451637
	LOSS [training: 0.14525559032682372 | validation: 0.10463641032514406]
	TIME [epoch: 8.79 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12842007402225264		[learning rate: 0.0045109]
		[batch 20/20] avg loss: 0.1404918461281086		[learning rate: 0.0045054]
	Learning Rate: 0.00450544
	LOSS [training: 0.13445596007518062 | validation: 0.11911769029347596]
	TIME [epoch: 8.78 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17001385512061679		[learning rate: 0.0045]
		[batch 20/20] avg loss: 0.10844517500046588		[learning rate: 0.0044945]
	Learning Rate: 0.00449453
	LOSS [training: 0.13922951506054132 | validation: 0.0979785494120172]
	TIME [epoch: 8.79 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20609472671670273		[learning rate: 0.0044891]
		[batch 20/20] avg loss: 0.13923105145610384		[learning rate: 0.0044836]
	Learning Rate: 0.00448365
	LOSS [training: 0.17266288908640332 | validation: 0.08300145249696791]
	TIME [epoch: 8.8 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.154366990819614		[learning rate: 0.0044782]
		[batch 20/20] avg loss: 0.16199659803316718		[learning rate: 0.0044728]
	Learning Rate: 0.00447279
	LOSS [training: 0.1581817944263906 | validation: 0.07790220868727875]
	TIME [epoch: 8.79 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1052751261572266		[learning rate: 0.0044674]
		[batch 20/20] avg loss: 0.17073421606417505		[learning rate: 0.004462]
	Learning Rate: 0.00446197
	LOSS [training: 0.1380046711107008 | validation: 0.21575621105245846]
	TIME [epoch: 8.79 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22381149958335228		[learning rate: 0.0044566]
		[batch 20/20] avg loss: 0.17093762405369656		[learning rate: 0.0044512]
	Learning Rate: 0.00445116
	LOSS [training: 0.19737456181852445 | validation: 0.20521033928415444]
	TIME [epoch: 8.8 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2693895546060029		[learning rate: 0.0044458]
		[batch 20/20] avg loss: 0.14247897741714502		[learning rate: 0.0044404]
	Learning Rate: 0.00444039
	LOSS [training: 0.20593426601157394 | validation: 0.13414602777060675]
	TIME [epoch: 8.8 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1183093341572119		[learning rate: 0.004435]
		[batch 20/20] avg loss: 0.11278152140767392		[learning rate: 0.0044296]
	Learning Rate: 0.00442964
	LOSS [training: 0.11554542778244292 | validation: 0.17274410369088006]
	TIME [epoch: 8.78 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15777179897258606		[learning rate: 0.0044243]
		[batch 20/20] avg loss: 0.17023468464612213		[learning rate: 0.0044189]
	Learning Rate: 0.00441892
	LOSS [training: 0.16400324180935405 | validation: 0.08425622037091957]
	TIME [epoch: 8.78 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19499136313116822		[learning rate: 0.0044136]
		[batch 20/20] avg loss: 0.16046234935601986		[learning rate: 0.0044082]
	Learning Rate: 0.00440822
	LOSS [training: 0.17772685624359402 | validation: 0.1273478463313977]
	TIME [epoch: 8.78 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14139383626631932		[learning rate: 0.0044029]
		[batch 20/20] avg loss: 0.12385306425887625		[learning rate: 0.0043975]
	Learning Rate: 0.00439755
	LOSS [training: 0.13262345026259775 | validation: 0.12340457816131273]
	TIME [epoch: 8.81 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1417893023593474		[learning rate: 0.0043922]
		[batch 20/20] avg loss: 0.17705007557985977		[learning rate: 0.0043869]
	Learning Rate: 0.0043869
	LOSS [training: 0.1594196889696036 | validation: 0.06399668079392785]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_440.pth
	Model improved!!!
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17807335864953794		[learning rate: 0.0043816]
		[batch 20/20] avg loss: 0.13285526658344612		[learning rate: 0.0043763]
	Learning Rate: 0.00437628
	LOSS [training: 0.15546431261649205 | validation: 0.12189549933708826]
	TIME [epoch: 8.8 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1434172814513647		[learning rate: 0.004371]
		[batch 20/20] avg loss: 0.12872091881482453		[learning rate: 0.0043657]
	Learning Rate: 0.00436569
	LOSS [training: 0.13606910013309464 | validation: 0.1087790800690812]
	TIME [epoch: 8.79 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22889607970905884		[learning rate: 0.0043604]
		[batch 20/20] avg loss: 0.14656830245990665		[learning rate: 0.0043551]
	Learning Rate: 0.00435512
	LOSS [training: 0.18773219108448277 | validation: 0.13757848005707768]
	TIME [epoch: 8.81 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16249910362819792		[learning rate: 0.0043498]
		[batch 20/20] avg loss: 0.11252349636949806		[learning rate: 0.0043446]
	Learning Rate: 0.00434458
	LOSS [training: 0.13751129999884804 | validation: 0.22366835688801914]
	TIME [epoch: 8.79 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15422451539525117		[learning rate: 0.0043393]
		[batch 20/20] avg loss: 0.22766619334633287		[learning rate: 0.0043341]
	Learning Rate: 0.00433406
	LOSS [training: 0.19094535437079205 | validation: 0.24669883057527403]
	TIME [epoch: 8.79 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.116071485825966		[learning rate: 0.0043288]
		[batch 20/20] avg loss: 0.1934231729012985		[learning rate: 0.0043236]
	Learning Rate: 0.00432357
	LOSS [training: 0.15474732936363225 | validation: 0.15710886334516794]
	TIME [epoch: 8.8 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14326008182961308		[learning rate: 0.0043183]
		[batch 20/20] avg loss: 0.17362727306070155		[learning rate: 0.0043131]
	Learning Rate: 0.0043131
	LOSS [training: 0.1584436774451573 | validation: 0.10419635904573965]
	TIME [epoch: 8.8 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18466927003471892		[learning rate: 0.0043079]
		[batch 20/20] avg loss: 0.20593688247461372		[learning rate: 0.0043027]
	Learning Rate: 0.00430266
	LOSS [training: 0.1953030762546663 | validation: 0.14591210868104945]
	TIME [epoch: 8.81 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1277781958801501		[learning rate: 0.0042974]
		[batch 20/20] avg loss: 0.09591369451239509		[learning rate: 0.0042922]
	Learning Rate: 0.00429224
	LOSS [training: 0.1118459451962726 | validation: 0.10328125688362762]
	TIME [epoch: 8.79 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16404905051808752		[learning rate: 0.004287]
		[batch 20/20] avg loss: 0.15852232748232886		[learning rate: 0.0042819]
	Learning Rate: 0.00428185
	LOSS [training: 0.1612856890002082 | validation: 0.09858403278658522]
	TIME [epoch: 8.78 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1386841479685501		[learning rate: 0.0042767]
		[batch 20/20] avg loss: 0.1611946966425094		[learning rate: 0.0042715]
	Learning Rate: 0.00427149
	LOSS [training: 0.14993942230552976 | validation: 0.3007252496093057]
	TIME [epoch: 8.79 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1770335528207593		[learning rate: 0.0042663]
		[batch 20/20] avg loss: 0.11353101845355891		[learning rate: 0.0042611]
	Learning Rate: 0.00426114
	LOSS [training: 0.14528228563715911 | validation: 0.1314507983475646]
	TIME [epoch: 8.8 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14575974112105386		[learning rate: 0.004256]
		[batch 20/20] avg loss: 0.15371954349647976		[learning rate: 0.0042508]
	Learning Rate: 0.00425083
	LOSS [training: 0.1497396423087668 | validation: 0.1599648087826998]
	TIME [epoch: 8.8 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1222523042664124		[learning rate: 0.0042457]
		[batch 20/20] avg loss: 0.12935103028044917		[learning rate: 0.0042405]
	Learning Rate: 0.00424054
	LOSS [training: 0.12580166727343076 | validation: 0.07710338535100437]
	TIME [epoch: 8.79 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14869147157853557		[learning rate: 0.0042354]
		[batch 20/20] avg loss: 0.15053884420176278		[learning rate: 0.0042303]
	Learning Rate: 0.00423027
	LOSS [training: 0.14961515789014918 | validation: 0.07791391257722552]
	TIME [epoch: 8.78 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18636602059881216		[learning rate: 0.0042251]
		[batch 20/20] avg loss: 0.10742709825696446		[learning rate: 0.00422]
	Learning Rate: 0.00422003
	LOSS [training: 0.14689655942788832 | validation: 0.09559464004408763]
	TIME [epoch: 8.8 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12835450813074237		[learning rate: 0.0042149]
		[batch 20/20] avg loss: 0.1548488124013367		[learning rate: 0.0042098]
	Learning Rate: 0.00420982
	LOSS [training: 0.14160166026603954 | validation: 0.14174315256033304]
	TIME [epoch: 8.79 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13859365324857845		[learning rate: 0.0042047]
		[batch 20/20] avg loss: 0.14014330854731555		[learning rate: 0.0041996]
	Learning Rate: 0.00419962
	LOSS [training: 0.139368480897947 | validation: 0.2979862001060029]
	TIME [epoch: 8.79 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16499667476922766		[learning rate: 0.0041945]
		[batch 20/20] avg loss: 0.1439536463195781		[learning rate: 0.0041895]
	Learning Rate: 0.00418946
	LOSS [training: 0.15447516054440286 | validation: 0.1371305859124695]
	TIME [epoch: 8.8 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13182426462013677		[learning rate: 0.0041844]
		[batch 20/20] avg loss: 0.11903577404016508		[learning rate: 0.0041793]
	Learning Rate: 0.00417932
	LOSS [training: 0.12543001933015094 | validation: 0.07117064577968221]
	TIME [epoch: 8.81 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12993833071637165		[learning rate: 0.0041743]
		[batch 20/20] avg loss: 0.16679599479145663		[learning rate: 0.0041692]
	Learning Rate: 0.0041692
	LOSS [training: 0.14836716275391412 | validation: 0.19376639881146968]
	TIME [epoch: 8.81 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17534464967074942		[learning rate: 0.0041641]
		[batch 20/20] avg loss: 0.1282486234955022		[learning rate: 0.0041591]
	Learning Rate: 0.00415911
	LOSS [training: 0.1517966365831258 | validation: 0.1276392636081316]
	TIME [epoch: 8.79 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09456951578631886		[learning rate: 0.0041541]
		[batch 20/20] avg loss: 0.13596551574415594		[learning rate: 0.004149]
	Learning Rate: 0.00414904
	LOSS [training: 0.11526751576523739 | validation: 0.133434456391774]
	TIME [epoch: 8.79 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12478653618801516		[learning rate: 0.004144]
		[batch 20/20] avg loss: 0.1109370638224356		[learning rate: 0.004139]
	Learning Rate: 0.00413899
	LOSS [training: 0.11786180000522539 | validation: 0.29216745837772484]
	TIME [epoch: 8.78 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12556748696132694		[learning rate: 0.004134]
		[batch 20/20] avg loss: 0.12727370119497083		[learning rate: 0.004129]
	Learning Rate: 0.00412897
	LOSS [training: 0.12642059407814893 | validation: 0.10008997695406933]
	TIME [epoch: 8.81 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11367823505445665		[learning rate: 0.004124]
		[batch 20/20] avg loss: 0.10839031124131893		[learning rate: 0.004119]
	Learning Rate: 0.00411898
	LOSS [training: 0.11103427314788779 | validation: 0.2733558641048638]
	TIME [epoch: 8.79 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1521177657620409		[learning rate: 0.004114]
		[batch 20/20] avg loss: 0.11553296781589242		[learning rate: 0.004109]
	Learning Rate: 0.00410901
	LOSS [training: 0.13382536678896664 | validation: 0.08011397107605368]
	TIME [epoch: 8.78 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10050370029513662		[learning rate: 0.004104]
		[batch 20/20] avg loss: 0.11677380085410842		[learning rate: 0.0040991]
	Learning Rate: 0.00409906
	LOSS [training: 0.10863875057462254 | validation: 0.23909684473240683]
	TIME [epoch: 8.79 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12592647822676908		[learning rate: 0.0040941]
		[batch 20/20] avg loss: 0.15794396535822822		[learning rate: 0.0040891]
	Learning Rate: 0.00408914
	LOSS [training: 0.14193522179249868 | validation: 0.23596605625693712]
	TIME [epoch: 8.8 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14892921355258587		[learning rate: 0.0040842]
		[batch 20/20] avg loss: 0.15189475997072596		[learning rate: 0.0040792]
	Learning Rate: 0.00407924
	LOSS [training: 0.15041198676165593 | validation: 0.1116958551368582]
	TIME [epoch: 8.8 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11545894466045863		[learning rate: 0.0040743]
		[batch 20/20] avg loss: 0.12001321909046639		[learning rate: 0.0040694]
	Learning Rate: 0.00406936
	LOSS [training: 0.1177360818754625 | validation: 0.10140194052015397]
	TIME [epoch: 8.78 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13330226205617277		[learning rate: 0.0040644]
		[batch 20/20] avg loss: 0.16731663057489093		[learning rate: 0.0040595]
	Learning Rate: 0.00405951
	LOSS [training: 0.15030944631553184 | validation: 0.12641493377145124]
	TIME [epoch: 8.79 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17005350695217322		[learning rate: 0.0040546]
		[batch 20/20] avg loss: 0.13706306625604525		[learning rate: 0.0040497]
	Learning Rate: 0.00404968
	LOSS [training: 0.15355828660410928 | validation: 0.10725179982136301]
	TIME [epoch: 8.81 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10062441180904531		[learning rate: 0.0040448]
		[batch 20/20] avg loss: 0.2196359016127189		[learning rate: 0.0040399]
	Learning Rate: 0.00403988
	LOSS [training: 0.16013015671088207 | validation: 0.10803431989775208]
	TIME [epoch: 8.8 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15416542342130524		[learning rate: 0.004035]
		[batch 20/20] avg loss: 0.15117968648180824		[learning rate: 0.0040301]
	Learning Rate: 0.0040301
	LOSS [training: 0.1526725549515567 | validation: 0.13392628231309586]
	TIME [epoch: 8.8 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15317341373862337		[learning rate: 0.0040252]
		[batch 20/20] avg loss: 0.07641860736425275		[learning rate: 0.0040203]
	Learning Rate: 0.00402034
	LOSS [training: 0.11479601055143804 | validation: 0.10960019839321972]
	TIME [epoch: 8.78 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1598469185482037		[learning rate: 0.0040155]
		[batch 20/20] avg loss: 0.21314330922034994		[learning rate: 0.0040106]
	Learning Rate: 0.00401061
	LOSS [training: 0.1864951138842768 | validation: 0.1886600946446188]
	TIME [epoch: 8.79 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13239176964158225		[learning rate: 0.0040058]
		[batch 20/20] avg loss: 0.1259183242770801		[learning rate: 0.0040009]
	Learning Rate: 0.0040009
	LOSS [training: 0.12915504695933117 | validation: 0.11245668974932213]
	TIME [epoch: 8.81 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09693414765944178		[learning rate: 0.0039961]
		[batch 20/20] avg loss: 0.10173280505676066		[learning rate: 0.0039912]
	Learning Rate: 0.00399122
	LOSS [training: 0.09933347635810122 | validation: 0.15454369959465036]
	TIME [epoch: 8.78 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1293473201498811		[learning rate: 0.0039864]
		[batch 20/20] avg loss: 0.09969268566930116		[learning rate: 0.0039816]
	Learning Rate: 0.00398155
	LOSS [training: 0.11452000290959115 | validation: 0.18975490870789002]
	TIME [epoch: 8.79 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14554018450280798		[learning rate: 0.0039767]
		[batch 20/20] avg loss: 0.2750756340654295		[learning rate: 0.0039719]
	Learning Rate: 0.00397192
	LOSS [training: 0.21030790928411874 | validation: 0.13464134654185686]
	TIME [epoch: 8.78 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1346099211303621		[learning rate: 0.0039671]
		[batch 20/20] avg loss: 0.18813421519535384		[learning rate: 0.0039623]
	Learning Rate: 0.0039623
	LOSS [training: 0.16137206816285796 | validation: 0.4586313736393049]
	TIME [epoch: 8.81 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1335760113008952		[learning rate: 0.0039575]
		[batch 20/20] avg loss: 0.2589356042421775		[learning rate: 0.0039527]
	Learning Rate: 0.00395271
	LOSS [training: 0.19625580777153634 | validation: 0.3056191558971254]
	TIME [epoch: 8.79 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19717537551180445		[learning rate: 0.0039479]
		[batch 20/20] avg loss: 0.11804315437969568		[learning rate: 0.0039431]
	Learning Rate: 0.00394314
	LOSS [training: 0.15760926494575006 | validation: 0.11166300078629963]
	TIME [epoch: 8.78 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14828262070483522		[learning rate: 0.0039384]
		[batch 20/20] avg loss: 0.13858909652759405		[learning rate: 0.0039336]
	Learning Rate: 0.00393359
	LOSS [training: 0.14343585861621466 | validation: 0.1169719058922949]
	TIME [epoch: 8.78 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10305606386248836		[learning rate: 0.0039288]
		[batch 20/20] avg loss: 0.10327986802341063		[learning rate: 0.0039241]
	Learning Rate: 0.00392407
	LOSS [training: 0.10316796594294948 | validation: 0.14907994627179824]
	TIME [epoch: 8.81 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10561215354851597		[learning rate: 0.0039193]
		[batch 20/20] avg loss: 0.13852745517419524		[learning rate: 0.0039146]
	Learning Rate: 0.00391457
	LOSS [training: 0.12206980436135562 | validation: 0.1364340838024697]
	TIME [epoch: 8.83 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12465191157595552		[learning rate: 0.0039098]
		[batch 20/20] avg loss: 0.10484113414112214		[learning rate: 0.0039051]
	Learning Rate: 0.00390509
	LOSS [training: 0.11474652285853884 | validation: 0.052358154211562445]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12443617814667411		[learning rate: 0.0039004]
		[batch 20/20] avg loss: 0.13767151657400603		[learning rate: 0.0038956]
	Learning Rate: 0.00389564
	LOSS [training: 0.13105384736034004 | validation: 0.1492197068702455]
	TIME [epoch: 8.82 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13194503782353836		[learning rate: 0.0038909]
		[batch 20/20] avg loss: 0.10291851280233347		[learning rate: 0.0038862]
	Learning Rate: 0.00388621
	LOSS [training: 0.11743177531293592 | validation: 0.2474665687539362]
	TIME [epoch: 8.81 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16197028054509593		[learning rate: 0.0038815]
		[batch 20/20] avg loss: 0.21362545542259528		[learning rate: 0.0038768]
	Learning Rate: 0.0038768
	LOSS [training: 0.18779786798384562 | validation: 0.1648689039106919]
	TIME [epoch: 8.82 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16573912816247804		[learning rate: 0.0038721]
		[batch 20/20] avg loss: 0.11545148008441905		[learning rate: 0.0038674]
	Learning Rate: 0.00386742
	LOSS [training: 0.14059530412344862 | validation: 0.15914931418589104]
	TIME [epoch: 8.81 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14199025168494897		[learning rate: 0.0038627]
		[batch 20/20] avg loss: 0.1391807873578659		[learning rate: 0.0038581]
	Learning Rate: 0.00385805
	LOSS [training: 0.14058551952140744 | validation: 0.08681342014226087]
	TIME [epoch: 8.81 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11639582521185968		[learning rate: 0.0038534]
		[batch 20/20] avg loss: 0.10368542260752958		[learning rate: 0.0038487]
	Learning Rate: 0.00384871
	LOSS [training: 0.11004062390969464 | validation: 0.05458783651619452]
	TIME [epoch: 8.81 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10492855267293819		[learning rate: 0.0038441]
		[batch 20/20] avg loss: 0.10093072189700321		[learning rate: 0.0038394]
	Learning Rate: 0.0038394
	LOSS [training: 0.10292963728497069 | validation: 0.06024325115911909]
	TIME [epoch: 8.82 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0839658343142239		[learning rate: 0.0038347]
		[batch 20/20] avg loss: 0.10893812504767045		[learning rate: 0.0038301]
	Learning Rate: 0.0038301
	LOSS [training: 0.09645197968094717 | validation: 0.252494231753696]
	TIME [epoch: 8.82 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1441094430667081		[learning rate: 0.0038255]
		[batch 20/20] avg loss: 0.10189014322368177		[learning rate: 0.0038208]
	Learning Rate: 0.00382083
	LOSS [training: 0.12299979314519496 | validation: 0.11378994052555581]
	TIME [epoch: 8.81 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10852080711035202		[learning rate: 0.0038162]
		[batch 20/20] avg loss: 0.1559236811737412		[learning rate: 0.0038116]
	Learning Rate: 0.00381158
	LOSS [training: 0.1322222441420466 | validation: 0.0856909601863973]
	TIME [epoch: 8.8 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1165751129478784		[learning rate: 0.003807]
		[batch 20/20] avg loss: 0.1434344672323703		[learning rate: 0.0038024]
	Learning Rate: 0.00380235
	LOSS [training: 0.13000479009012436 | validation: 0.10635559389214382]
	TIME [epoch: 8.81 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12362584606574249		[learning rate: 0.0037977]
		[batch 20/20] avg loss: 0.11984491447962187		[learning rate: 0.0037931]
	Learning Rate: 0.00379315
	LOSS [training: 0.1217353802726822 | validation: 0.23331589305414246]
	TIME [epoch: 8.85 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1355485138566675		[learning rate: 0.0037886]
		[batch 20/20] avg loss: 0.14659834572599936		[learning rate: 0.003784]
	Learning Rate: 0.00378397
	LOSS [training: 0.14107342979133342 | validation: 0.14037423233142965]
	TIME [epoch: 8.82 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13149271715081964		[learning rate: 0.0037794]
		[batch 20/20] avg loss: 0.11536210179297046		[learning rate: 0.0037748]
	Learning Rate: 0.00377481
	LOSS [training: 0.12342740947189505 | validation: 0.184885079601494]
	TIME [epoch: 8.81 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13180384191480227		[learning rate: 0.0037702]
		[batch 20/20] avg loss: 0.10546588229998721		[learning rate: 0.0037657]
	Learning Rate: 0.00376567
	LOSS [training: 0.11863486210739475 | validation: 0.13892727590060788]
	TIME [epoch: 8.8 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12797528705198974		[learning rate: 0.0037611]
		[batch 20/20] avg loss: 0.13902175328924762		[learning rate: 0.0037566]
	Learning Rate: 0.00375655
	LOSS [training: 0.13349852017061867 | validation: 0.1104751526791934]
	TIME [epoch: 8.83 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1367327388512538		[learning rate: 0.003752]
		[batch 20/20] avg loss: 0.09837307325119594		[learning rate: 0.0037475]
	Learning Rate: 0.00374746
	LOSS [training: 0.11755290605122486 | validation: 0.09051860715611589]
	TIME [epoch: 8.8 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14931782703030727		[learning rate: 0.0037429]
		[batch 20/20] avg loss: 0.11602252070523851		[learning rate: 0.0037384]
	Learning Rate: 0.00373839
	LOSS [training: 0.1326701738677729 | validation: 0.17582873957434592]
	TIME [epoch: 8.81 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19718781024368032		[learning rate: 0.0037339]
		[batch 20/20] avg loss: 0.12272873968593814		[learning rate: 0.0037293]
	Learning Rate: 0.00372934
	LOSS [training: 0.15995827496480924 | validation: 0.10630793319793554]
	TIME [epoch: 8.81 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09719034794825715		[learning rate: 0.0037248]
		[batch 20/20] avg loss: 0.1310086537200786		[learning rate: 0.0037203]
	Learning Rate: 0.00372031
	LOSS [training: 0.11409950083416787 | validation: 0.2295724821711517]
	TIME [epoch: 8.82 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1816383518848475		[learning rate: 0.0037158]
		[batch 20/20] avg loss: 0.14112607640170952		[learning rate: 0.0037113]
	Learning Rate: 0.0037113
	LOSS [training: 0.16138221414327852 | validation: 0.109729075997099]
	TIME [epoch: 8.81 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1325957134467866		[learning rate: 0.0037068]
		[batch 20/20] avg loss: 0.3307577080963875		[learning rate: 0.0037023]
	Learning Rate: 0.00370232
	LOSS [training: 0.23167671077158705 | validation: 0.1035744724602643]
	TIME [epoch: 8.8 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09926997954415244		[learning rate: 0.0036978]
		[batch 20/20] avg loss: 0.10589406855864762		[learning rate: 0.0036934]
	Learning Rate: 0.00369336
	LOSS [training: 0.10258202405140004 | validation: 0.08272630545523986]
	TIME [epoch: 8.79 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11962611008883069		[learning rate: 0.0036889]
		[batch 20/20] avg loss: 0.12262789546184541		[learning rate: 0.0036844]
	Learning Rate: 0.00368441
	LOSS [training: 0.12112700277533803 | validation: 0.1091305908723465]
	TIME [epoch: 8.82 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15966421236765843		[learning rate: 0.00368]
		[batch 20/20] avg loss: 0.11884725813467578		[learning rate: 0.0036755]
	Learning Rate: 0.00367549
	LOSS [training: 0.13925573525116713 | validation: 0.11487917163240563]
	TIME [epoch: 8.82 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1542222456245199		[learning rate: 0.003671]
		[batch 20/20] avg loss: 0.1301900974087717		[learning rate: 0.0036666]
	Learning Rate: 0.0036666
	LOSS [training: 0.14220617151664577 | validation: 0.09172104714039842]
	TIME [epoch: 8.82 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08839734938164259		[learning rate: 0.0036622]
		[batch 20/20] avg loss: 0.1348974954239064		[learning rate: 0.0036577]
	Learning Rate: 0.00365772
	LOSS [training: 0.11164742240277448 | validation: 0.06268463165511742]
	TIME [epoch: 8.81 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10184784519390626		[learning rate: 0.0036533]
		[batch 20/20] avg loss: 0.16543116856495446		[learning rate: 0.0036489]
	Learning Rate: 0.00364887
	LOSS [training: 0.13363950687943038 | validation: 0.13752046734953757]
	TIME [epoch: 8.81 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08552565387996308		[learning rate: 0.0036444]
		[batch 20/20] avg loss: 0.16498842786362344		[learning rate: 0.00364]
	Learning Rate: 0.00364003
	LOSS [training: 0.12525704087179326 | validation: 0.0653321916932168]
	TIME [epoch: 8.83 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1314661724491732		[learning rate: 0.0036356]
		[batch 20/20] avg loss: 0.10735463282522231		[learning rate: 0.0036312]
	Learning Rate: 0.00363122
	LOSS [training: 0.11941040263719778 | validation: 0.13220029578631717]
	TIME [epoch: 8.8 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11257723425173716		[learning rate: 0.0036268]
		[batch 20/20] avg loss: 0.09980653764855718		[learning rate: 0.0036224]
	Learning Rate: 0.00362243
	LOSS [training: 0.10619188595014717 | validation: 0.30709005932015065]
	TIME [epoch: 8.8 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13980302403225556		[learning rate: 0.003618]
		[batch 20/20] avg loss: 0.13898361822255828		[learning rate: 0.0036137]
	Learning Rate: 0.00361366
	LOSS [training: 0.1393933211274069 | validation: 0.21398132073370663]
	TIME [epoch: 8.8 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1093958194343508		[learning rate: 0.0036093]
		[batch 20/20] avg loss: 0.09132006791218547		[learning rate: 0.0036049]
	Learning Rate: 0.00360491
	LOSS [training: 0.10035794367326815 | validation: 0.10063597282502716]
	TIME [epoch: 8.82 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12840054621750424		[learning rate: 0.0036005]
		[batch 20/20] avg loss: 0.11794530779957961		[learning rate: 0.0035962]
	Learning Rate: 0.00359619
	LOSS [training: 0.12317292700854192 | validation: 0.1213135083041458]
	TIME [epoch: 8.81 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16100212067735037		[learning rate: 0.0035918]
		[batch 20/20] avg loss: 0.09125767296416541		[learning rate: 0.0035875]
	Learning Rate: 0.00358748
	LOSS [training: 0.1261298968207579 | validation: 0.21461513216425193]
	TIME [epoch: 8.8 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17186343132399373		[learning rate: 0.0035831]
		[batch 20/20] avg loss: 0.10580354053062117		[learning rate: 0.0035788]
	Learning Rate: 0.0035788
	LOSS [training: 0.13883348592730743 | validation: 0.17019492472147754]
	TIME [epoch: 8.8 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11015540163319308		[learning rate: 0.0035745]
		[batch 20/20] avg loss: 0.11943986522676664		[learning rate: 0.0035701]
	Learning Rate: 0.00357013
	LOSS [training: 0.11479763342997987 | validation: 0.14618632835153506]
	TIME [epoch: 8.81 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08549971006745734		[learning rate: 0.0035658]
		[batch 20/20] avg loss: 0.13030827828061806		[learning rate: 0.0035615]
	Learning Rate: 0.00356149
	LOSS [training: 0.1079039941740377 | validation: 0.11807554813507232]
	TIME [epoch: 8.82 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11547842725240594		[learning rate: 0.0035572]
		[batch 20/20] avg loss: 0.09838451409989742		[learning rate: 0.0035529]
	Learning Rate: 0.00355287
	LOSS [training: 0.10693147067615165 | validation: 0.10016834522438593]
	TIME [epoch: 8.81 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08391086939101336		[learning rate: 0.0035486]
		[batch 20/20] avg loss: 0.09012145216383513		[learning rate: 0.0035443]
	Learning Rate: 0.00354427
	LOSS [training: 0.08701616077742427 | validation: 0.12721067309651812]
	TIME [epoch: 8.8 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14211561692874136		[learning rate: 0.00354]
		[batch 20/20] avg loss: 0.09538884190697279		[learning rate: 0.0035357]
	Learning Rate: 0.00353569
	LOSS [training: 0.11875222941785706 | validation: 0.09520744845166315]
	TIME [epoch: 8.81 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16059548228171122		[learning rate: 0.0035314]
		[batch 20/20] avg loss: 0.1174315855423127		[learning rate: 0.0035271]
	Learning Rate: 0.00352713
	LOSS [training: 0.13901353391201193 | validation: 0.09098533220675956]
	TIME [epoch: 8.81 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11252434203395674		[learning rate: 0.0035229]
		[batch 20/20] avg loss: 0.10794656596062624		[learning rate: 0.0035186]
	Learning Rate: 0.00351859
	LOSS [training: 0.11023545399729148 | validation: 0.06519273259734952]
	TIME [epoch: 8.81 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11536430547600833		[learning rate: 0.0035143]
		[batch 20/20] avg loss: 0.1406524325228786		[learning rate: 0.0035101]
	Learning Rate: 0.00351007
	LOSS [training: 0.12800836899944346 | validation: 0.07167590720445863]
	TIME [epoch: 8.79 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0996298070532959		[learning rate: 0.0035058]
		[batch 20/20] avg loss: 0.08973821561890528		[learning rate: 0.0035016]
	Learning Rate: 0.00350157
	LOSS [training: 0.09468401133610058 | validation: 0.2000000406837826]
	TIME [epoch: 8.81 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1252947416214651		[learning rate: 0.0034973]
		[batch 20/20] avg loss: 0.09884818065937367		[learning rate: 0.0034931]
	Learning Rate: 0.0034931
	LOSS [training: 0.11207146114041937 | validation: 0.20650002119707078]
	TIME [epoch: 8.82 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1220339947582765		[learning rate: 0.0034889]
		[batch 20/20] avg loss: 0.09425190346926288		[learning rate: 0.0034846]
	Learning Rate: 0.00348464
	LOSS [training: 0.10814294911376969 | validation: 0.08336903545685768]
	TIME [epoch: 8.8 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13141281569707247		[learning rate: 0.0034804]
		[batch 20/20] avg loss: 0.128348280977113		[learning rate: 0.0034762]
	Learning Rate: 0.0034762
	LOSS [training: 0.12988054833709275 | validation: 0.09469922122574628]
	TIME [epoch: 8.8 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12139825434297372		[learning rate: 0.003472]
		[batch 20/20] avg loss: 0.09432755220994456		[learning rate: 0.0034678]
	Learning Rate: 0.00346779
	LOSS [training: 0.10786290327645916 | validation: 0.0990383926848289]
	TIME [epoch: 8.8 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10375680719881852		[learning rate: 0.0034636]
		[batch 20/20] avg loss: 0.1822522401715964		[learning rate: 0.0034594]
	Learning Rate: 0.00345939
	LOSS [training: 0.14300452368520747 | validation: 0.17570836950705787]
	TIME [epoch: 8.81 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11835464347384106		[learning rate: 0.0034552]
		[batch 20/20] avg loss: 0.17931171817172215		[learning rate: 0.003451]
	Learning Rate: 0.00345102
	LOSS [training: 0.14883318082278157 | validation: 0.08894354220310298]
	TIME [epoch: 8.82 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20352804948646747		[learning rate: 0.0034468]
		[batch 20/20] avg loss: 0.1001867508437658		[learning rate: 0.0034427]
	Learning Rate: 0.00344266
	LOSS [training: 0.15185740016511665 | validation: 0.25116233786502445]
	TIME [epoch: 8.8 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1460524827268527		[learning rate: 0.0034385]
		[batch 20/20] avg loss: 0.11605900872562873		[learning rate: 0.0034343]
	Learning Rate: 0.00343433
	LOSS [training: 0.13105574572624074 | validation: 0.1586300688449414]
	TIME [epoch: 8.8 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10393443152073782		[learning rate: 0.0034302]
		[batch 20/20] avg loss: 0.10564261602185923		[learning rate: 0.003426]
	Learning Rate: 0.00342602
	LOSS [training: 0.10478852377129852 | validation: 0.15954157231123367]
	TIME [epoch: 8.81 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09524280919535927		[learning rate: 0.0034219]
		[batch 20/20] avg loss: 0.11291102359363825		[learning rate: 0.0034177]
	Learning Rate: 0.00341772
	LOSS [training: 0.10407691639449876 | validation: 0.05840036784286837]
	TIME [epoch: 8.82 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1144411234502907		[learning rate: 0.0034136]
		[batch 20/20] avg loss: 0.09448862706049246		[learning rate: 0.0034094]
	Learning Rate: 0.00340945
	LOSS [training: 0.10446487525539158 | validation: 0.2695659214053334]
	TIME [epoch: 8.8 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15077270568515444		[learning rate: 0.0034053]
		[batch 20/20] avg loss: 0.14529999598261298		[learning rate: 0.0034012]
	Learning Rate: 0.0034012
	LOSS [training: 0.14803635083388372 | validation: 0.06920167435419858]
	TIME [epoch: 8.8 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11643473600864665		[learning rate: 0.0033971]
		[batch 20/20] avg loss: 0.10160395351877213		[learning rate: 0.003393]
	Learning Rate: 0.00339296
	LOSS [training: 0.10901934476370942 | validation: 0.07955566177076284]
	TIME [epoch: 8.8 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10279416617858232		[learning rate: 0.0033889]
		[batch 20/20] avg loss: 0.12273154235740721		[learning rate: 0.0033847]
	Learning Rate: 0.00338475
	LOSS [training: 0.11276285426799479 | validation: 0.0810365238597607]
	TIME [epoch: 8.82 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07914854436707103		[learning rate: 0.0033806]
		[batch 20/20] avg loss: 0.11076216395620284		[learning rate: 0.0033766]
	Learning Rate: 0.00337655
	LOSS [training: 0.09495535416163693 | validation: 0.12259887512050745]
	TIME [epoch: 8.81 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10762411945450182		[learning rate: 0.0033725]
		[batch 20/20] avg loss: 0.13342120888820322		[learning rate: 0.0033684]
	Learning Rate: 0.00336838
	LOSS [training: 0.12052266417135252 | validation: 0.09663829929492983]
	TIME [epoch: 8.8 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1072057726186286		[learning rate: 0.0033643]
		[batch 20/20] avg loss: 0.1096190090001817		[learning rate: 0.0033602]
	Learning Rate: 0.00336023
	LOSS [training: 0.10841239080940515 | validation: 0.11992953851918046]
	TIME [epoch: 8.8 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10293340735676197		[learning rate: 0.0033562]
		[batch 20/20] avg loss: 0.10836477336779589		[learning rate: 0.0033521]
	Learning Rate: 0.00335209
	LOSS [training: 0.10564909036227894 | validation: 0.08757953732605268]
	TIME [epoch: 8.82 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07037996526617084		[learning rate: 0.003348]
		[batch 20/20] avg loss: 0.08325987280500305		[learning rate: 0.003344]
	Learning Rate: 0.00334398
	LOSS [training: 0.07681991903558694 | validation: 0.1244445591324909]
	TIME [epoch: 8.8 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14722402721157765		[learning rate: 0.0033399]
		[batch 20/20] avg loss: 0.09542624193181465		[learning rate: 0.0033359]
	Learning Rate: 0.00333588
	LOSS [training: 0.12132513457169614 | validation: 0.142986547302428]
	TIME [epoch: 8.81 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15671632236487545		[learning rate: 0.0033318]
		[batch 20/20] avg loss: 0.11283209282356485		[learning rate: 0.0033278]
	Learning Rate: 0.00332781
	LOSS [training: 0.13477420759422018 | validation: 0.1708271429933773]
	TIME [epoch: 8.81 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08140666537676566		[learning rate: 0.0033238]
		[batch 20/20] avg loss: 0.13837172841289008		[learning rate: 0.0033197]
	Learning Rate: 0.00331975
	LOSS [training: 0.1098891968948279 | validation: 0.19853465403099585]
	TIME [epoch: 8.83 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15242236002401202		[learning rate: 0.0033157]
		[batch 20/20] avg loss: 0.13575860142995028		[learning rate: 0.0033117]
	Learning Rate: 0.00331171
	LOSS [training: 0.14409048072698116 | validation: 0.1274908111858941]
	TIME [epoch: 8.83 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12980414107745966		[learning rate: 0.0033077]
		[batch 20/20] avg loss: 0.11617562105941259		[learning rate: 0.0033037]
	Learning Rate: 0.0033037
	LOSS [training: 0.12298988106843615 | validation: 0.17127326731793588]
	TIME [epoch: 8.8 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09789566395256687		[learning rate: 0.0032997]
		[batch 20/20] avg loss: 0.07278755206271928		[learning rate: 0.0032957]
	Learning Rate: 0.0032957
	LOSS [training: 0.08534160800764307 | validation: 0.09393444311828661]
	TIME [epoch: 8.8 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09274234951373309		[learning rate: 0.0032917]
		[batch 20/20] avg loss: 0.15857231945134206		[learning rate: 0.0032877]
	Learning Rate: 0.00328772
	LOSS [training: 0.12565733448253757 | validation: 0.12159722202943762]
	TIME [epoch: 8.79 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13523926465985875		[learning rate: 0.0032837]
		[batch 20/20] avg loss: 0.10042661888616741		[learning rate: 0.0032798]
	Learning Rate: 0.00327976
	LOSS [training: 0.11783294177301307 | validation: 0.09852724481048251]
	TIME [epoch: 8.83 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0977630208790829		[learning rate: 0.0032758]
		[batch 20/20] avg loss: 0.10191369075148393		[learning rate: 0.0032718]
	Learning Rate: 0.00327182
	LOSS [training: 0.09983835581528341 | validation: 0.09521703382140093]
	TIME [epoch: 8.81 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09194884070309063		[learning rate: 0.0032679]
		[batch 20/20] avg loss: 0.08182614991196166		[learning rate: 0.0032639]
	Learning Rate: 0.0032639
	LOSS [training: 0.08688749530752614 | validation: 0.10874019562873094]
	TIME [epoch: 8.81 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0882093489033284		[learning rate: 0.0032599]
		[batch 20/20] avg loss: 0.12839828503193387		[learning rate: 0.003256]
	Learning Rate: 0.003256
	LOSS [training: 0.10830381696763117 | validation: 0.15871940469568016]
	TIME [epoch: 8.82 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1105760921049371		[learning rate: 0.0032521]
		[batch 20/20] avg loss: 0.1422733671410173		[learning rate: 0.0032481]
	Learning Rate: 0.00324812
	LOSS [training: 0.12642472962297718 | validation: 0.10554351164252412]
	TIME [epoch: 8.83 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10393942148162365		[learning rate: 0.0032442]
		[batch 20/20] avg loss: 0.08484728103517496		[learning rate: 0.0032403]
	Learning Rate: 0.00324025
	LOSS [training: 0.0943933512583993 | validation: 0.05704880177507002]
	TIME [epoch: 8.81 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10016953959283119		[learning rate: 0.0032363]
		[batch 20/20] avg loss: 0.1321970065196912		[learning rate: 0.0032324]
	Learning Rate: 0.00323241
	LOSS [training: 0.11618327305626122 | validation: 0.0550040010554489]
	TIME [epoch: 8.8 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1290216692359527		[learning rate: 0.0032285]
		[batch 20/20] avg loss: 0.07686853673569746		[learning rate: 0.0032246]
	Learning Rate: 0.00322458
	LOSS [training: 0.10294510298582507 | validation: 0.0628704299755518]
	TIME [epoch: 8.82 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0962247903491971		[learning rate: 0.0032207]
		[batch 20/20] avg loss: 0.09841921325080671		[learning rate: 0.0032168]
	Learning Rate: 0.00321678
	LOSS [training: 0.09732200180000192 | validation: 0.047328786460471894]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0741574135868999		[learning rate: 0.0032129]
		[batch 20/20] avg loss: 0.07541017430350842		[learning rate: 0.003209]
	Learning Rate: 0.00320899
	LOSS [training: 0.07478379394520415 | validation: 0.11620001991471161]
	TIME [epoch: 8.82 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09811250216082322		[learning rate: 0.0032051]
		[batch 20/20] avg loss: 0.09523033609207651		[learning rate: 0.0032012]
	Learning Rate: 0.00320122
	LOSS [training: 0.09667141912644986 | validation: 0.03897019979587991]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1021029364343415		[learning rate: 0.0031973]
		[batch 20/20] avg loss: 0.1217551457362418		[learning rate: 0.0031935]
	Learning Rate: 0.00319347
	LOSS [training: 0.11192904108529164 | validation: 0.1385571752741915]
	TIME [epoch: 8.8 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10534960599968972		[learning rate: 0.0031896]
		[batch 20/20] avg loss: 0.09984474145428512		[learning rate: 0.0031857]
	Learning Rate: 0.00318574
	LOSS [training: 0.10259717372698743 | validation: 0.09188058638949176]
	TIME [epoch: 8.8 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07090281064886737		[learning rate: 0.0031819]
		[batch 20/20] avg loss: 0.07618597390496108		[learning rate: 0.003178]
	Learning Rate: 0.00317803
	LOSS [training: 0.07354439227691423 | validation: 0.06066734106212645]
	TIME [epoch: 8.81 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0995657800613762		[learning rate: 0.0031742]
		[batch 20/20] avg loss: 0.13927399444349856		[learning rate: 0.0031703]
	Learning Rate: 0.00317034
	LOSS [training: 0.11941988725243738 | validation: 0.09188559850213954]
	TIME [epoch: 8.8 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1266461666712509		[learning rate: 0.0031665]
		[batch 20/20] avg loss: 0.10725825006621756		[learning rate: 0.0031627]
	Learning Rate: 0.00316266
	LOSS [training: 0.11695220836873421 | validation: 0.08773888193611266]
	TIME [epoch: 8.8 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08255140268194376		[learning rate: 0.0031588]
		[batch 20/20] avg loss: 0.1070654599856861		[learning rate: 0.003155]
	Learning Rate: 0.003155
	LOSS [training: 0.09480843133381492 | validation: 0.11819508802734434]
	TIME [epoch: 8.79 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09039392385045138		[learning rate: 0.0031512]
		[batch 20/20] avg loss: 0.0819220010498414		[learning rate: 0.0031474]
	Learning Rate: 0.00314737
	LOSS [training: 0.08615796245014638 | validation: 0.08623149335474986]
	TIME [epoch: 8.81 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11252209616836732		[learning rate: 0.0031436]
		[batch 20/20] avg loss: 0.06097204414253306		[learning rate: 0.0031397]
	Learning Rate: 0.00313975
	LOSS [training: 0.08674707015545018 | validation: 0.06150684737266291]
	TIME [epoch: 8.81 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13741993017258164		[learning rate: 0.0031359]
		[batch 20/20] avg loss: 0.11933208907826809		[learning rate: 0.0031321]
	Learning Rate: 0.00313215
	LOSS [training: 0.1283760096254249 | validation: 0.07392566057045057]
	TIME [epoch: 8.79 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07016307672479971		[learning rate: 0.0031284]
		[batch 20/20] avg loss: 0.08631375796328643		[learning rate: 0.0031246]
	Learning Rate: 0.00312456
	LOSS [training: 0.07823841734404306 | validation: 0.15592499321548234]
	TIME [epoch: 8.8 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07275064360856223		[learning rate: 0.0031208]
		[batch 20/20] avg loss: 0.17684092436260807		[learning rate: 0.003117]
	Learning Rate: 0.003117
	LOSS [training: 0.12479578398558513 | validation: 0.13380265440425276]
	TIME [epoch: 8.8 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13165164935409304		[learning rate: 0.0031132]
		[batch 20/20] avg loss: 0.10425186047393349		[learning rate: 0.0031095]
	Learning Rate: 0.00310945
	LOSS [training: 0.11795175491401325 | validation: 0.09539853777190539]
	TIME [epoch: 8.83 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1436919191375089		[learning rate: 0.0031057]
		[batch 20/20] avg loss: 0.13297390343620274		[learning rate: 0.0031019]
	Learning Rate: 0.00310193
	LOSS [training: 0.13833291128685582 | validation: 0.0836630743964684]
	TIME [epoch: 8.8 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06774264512799004		[learning rate: 0.0030982]
		[batch 20/20] avg loss: 0.07820547338194248		[learning rate: 0.0030944]
	Learning Rate: 0.00309442
	LOSS [training: 0.07297405925496625 | validation: 0.06598156392607188]
	TIME [epoch: 8.8 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09200219520578401		[learning rate: 0.0030907]
		[batch 20/20] avg loss: 0.0856186795513437		[learning rate: 0.0030869]
	Learning Rate: 0.00308693
	LOSS [training: 0.08881043737856385 | validation: 0.04986473971298686]
	TIME [epoch: 8.8 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06433691376235114		[learning rate: 0.0030832]
		[batch 20/20] avg loss: 0.13330564086980307		[learning rate: 0.0030795]
	Learning Rate: 0.00307945
	LOSS [training: 0.0988212773160771 | validation: 0.18666234735241916]
	TIME [epoch: 8.79 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12294742813926154		[learning rate: 0.0030757]
		[batch 20/20] avg loss: 0.09926647858184827		[learning rate: 0.003072]
	Learning Rate: 0.003072
	LOSS [training: 0.11110695336055491 | validation: 0.07423105919626875]
	TIME [epoch: 8.82 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11759981546303015		[learning rate: 0.0030683]
		[batch 20/20] avg loss: 0.09885822998965407		[learning rate: 0.0030646]
	Learning Rate: 0.00306456
	LOSS [training: 0.10822902272634212 | validation: 0.11705420639536888]
	TIME [epoch: 8.79 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09762572580893628		[learning rate: 0.0030609]
		[batch 20/20] avg loss: 0.13149632829253824		[learning rate: 0.0030571]
	Learning Rate: 0.00305714
	LOSS [training: 0.11456102705073727 | validation: 0.18933214976922455]
	TIME [epoch: 8.8 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15695197057924493		[learning rate: 0.0030534]
		[batch 20/20] avg loss: 0.06728451238225075		[learning rate: 0.0030497]
	Learning Rate: 0.00304974
	LOSS [training: 0.11211824148074785 | validation: 0.11863101454911862]
	TIME [epoch: 8.79 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09442398964763507		[learning rate: 0.003046]
		[batch 20/20] avg loss: 0.08222064852007924		[learning rate: 0.0030424]
	Learning Rate: 0.00304236
	LOSS [training: 0.08832231908385714 | validation: 0.22807022763800658]
	TIME [epoch: 8.81 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10663193966771387		[learning rate: 0.0030387]
		[batch 20/20] avg loss: 0.1433134070737332		[learning rate: 0.003035]
	Learning Rate: 0.00303499
	LOSS [training: 0.12497267337072351 | validation: 0.12281844281096964]
	TIME [epoch: 8.8 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07556304554091375		[learning rate: 0.0030313]
		[batch 20/20] avg loss: 0.09584253241515141		[learning rate: 0.0030276]
	Learning Rate: 0.00302765
	LOSS [training: 0.08570278897803259 | validation: 0.07864204374250222]
	TIME [epoch: 8.79 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07895221620491497		[learning rate: 0.003024]
		[batch 20/20] avg loss: 0.12450239412774848		[learning rate: 0.0030203]
	Learning Rate: 0.00302032
	LOSS [training: 0.10172730516633173 | validation: 0.08826156982647598]
	TIME [epoch: 8.78 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10195014787322107		[learning rate: 0.0030167]
		[batch 20/20] avg loss: 0.16385170954210018		[learning rate: 0.003013]
	Learning Rate: 0.00301301
	LOSS [training: 0.13290092870766063 | validation: 0.2506849452020664]
	TIME [epoch: 8.79 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18088717915437075		[learning rate: 0.0030094]
		[batch 20/20] avg loss: 0.13213665670001312		[learning rate: 0.0030057]
	Learning Rate: 0.00300571
	LOSS [training: 0.15651191792719193 | validation: 0.06543928061956707]
	TIME [epoch: 8.78 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0838156478992834		[learning rate: 0.0030021]
		[batch 20/20] avg loss: 0.09579303814042521		[learning rate: 0.0029984]
	Learning Rate: 0.00299844
	LOSS [training: 0.08980434301985431 | validation: 0.0589214100784939]
	TIME [epoch: 8.77 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09296855993821516		[learning rate: 0.0029948]
		[batch 20/20] avg loss: 0.0846766303384289		[learning rate: 0.0029912]
	Learning Rate: 0.00299118
	LOSS [training: 0.08882259513832207 | validation: 0.11285732867558936]
	TIME [epoch: 8.77 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1230288254198731		[learning rate: 0.0029876]
		[batch 20/20] avg loss: 0.08981878912931433		[learning rate: 0.0029839]
	Learning Rate: 0.00298394
	LOSS [training: 0.1064238072745937 | validation: 0.040727209471851655]
	TIME [epoch: 8.79 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07936458854243325		[learning rate: 0.0029803]
		[batch 20/20] avg loss: 0.07711620709295096		[learning rate: 0.0029767]
	Learning Rate: 0.00297671
	LOSS [training: 0.07824039781769211 | validation: 0.18171123054922123]
	TIME [epoch: 8.82 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13737527318670048		[learning rate: 0.0029731]
		[batch 20/20] avg loss: 0.13127018964515003		[learning rate: 0.0029695]
	Learning Rate: 0.00296951
	LOSS [training: 0.13432273141592527 | validation: 0.060045879252834694]
	TIME [epoch: 8.79 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12508632201412279		[learning rate: 0.0029659]
		[batch 20/20] avg loss: 0.1257719602972744		[learning rate: 0.0029623]
	Learning Rate: 0.00296232
	LOSS [training: 0.1254291411556986 | validation: 0.08531259453742124]
	TIME [epoch: 8.8 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09925576875739778		[learning rate: 0.0029587]
		[batch 20/20] avg loss: 0.11000690302246277		[learning rate: 0.0029551]
	Learning Rate: 0.00295515
	LOSS [training: 0.10463133588993027 | validation: 0.0622150755022985]
	TIME [epoch: 8.79 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09088801651415021		[learning rate: 0.0029516]
		[batch 20/20] avg loss: 0.08595578696263477		[learning rate: 0.002948]
	Learning Rate: 0.00294799
	LOSS [training: 0.08842190173839248 | validation: 0.09003280538407481]
	TIME [epoch: 8.82 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1384636845424198		[learning rate: 0.0029444]
		[batch 20/20] avg loss: 0.07232775903460456		[learning rate: 0.0029409]
	Learning Rate: 0.00294086
	LOSS [training: 0.1053957217885122 | validation: 0.08419107958795052]
	TIME [epoch: 8.8 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09596346546011246		[learning rate: 0.0029373]
		[batch 20/20] avg loss: 0.053659267580193105		[learning rate: 0.0029337]
	Learning Rate: 0.00293374
	LOSS [training: 0.07481136652015277 | validation: 0.07200962918152408]
	TIME [epoch: 8.81 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13280121076254658		[learning rate: 0.0029302]
		[batch 20/20] avg loss: 0.10207755491319934		[learning rate: 0.0029266]
	Learning Rate: 0.00292663
	LOSS [training: 0.11743938283787296 | validation: 0.10996709238849306]
	TIME [epoch: 8.79 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10299182690399733		[learning rate: 0.0029231]
		[batch 20/20] avg loss: 0.07996383029639213		[learning rate: 0.0029195]
	Learning Rate: 0.00291955
	LOSS [training: 0.09147782860019472 | validation: 0.06365432229440046]
	TIME [epoch: 8.82 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07091530194673953		[learning rate: 0.002916]
		[batch 20/20] avg loss: 0.061525885374961155		[learning rate: 0.0029125]
	Learning Rate: 0.00291248
	LOSS [training: 0.06622059366085034 | validation: 0.07368687687564696]
	TIME [epoch: 8.81 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06342370377184206		[learning rate: 0.002909]
		[batch 20/20] avg loss: 0.08911926743395937		[learning rate: 0.0029054]
	Learning Rate: 0.00290543
	LOSS [training: 0.0762714856029007 | validation: 0.06279852827796312]
	TIME [epoch: 8.8 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08739545872976538		[learning rate: 0.0029019]
		[batch 20/20] avg loss: 0.098648132742882		[learning rate: 0.0028984]
	Learning Rate: 0.0028984
	LOSS [training: 0.09302179573632369 | validation: 0.10363826915396177]
	TIME [epoch: 8.8 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11830366870652016		[learning rate: 0.0028949]
		[batch 20/20] avg loss: 0.08737773503037863		[learning rate: 0.0028914]
	Learning Rate: 0.00289138
	LOSS [training: 0.10284070186844937 | validation: 0.12045349653563461]
	TIME [epoch: 8.81 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09132815022811883		[learning rate: 0.0028879]
		[batch 20/20] avg loss: 0.11071795108374158		[learning rate: 0.0028844]
	Learning Rate: 0.00288438
	LOSS [training: 0.10102305065593023 | validation: 0.1583237144587067]
	TIME [epoch: 8.81 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08634875343842471		[learning rate: 0.0028809]
		[batch 20/20] avg loss: 0.11788820861457172		[learning rate: 0.0028774]
	Learning Rate: 0.0028774
	LOSS [training: 0.1021184810264982 | validation: 0.07518325020192673]
	TIME [epoch: 8.8 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06762237137949877		[learning rate: 0.0028739]
		[batch 20/20] avg loss: 0.07575421103159755		[learning rate: 0.0028704]
	Learning Rate: 0.00287043
	LOSS [training: 0.07168829120554815 | validation: 0.15395189469828127]
	TIME [epoch: 8.79 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10392323442811657		[learning rate: 0.002867]
		[batch 20/20] avg loss: 0.1319824670289247		[learning rate: 0.0028635]
	Learning Rate: 0.00286348
	LOSS [training: 0.11795285072852062 | validation: 0.06572285010625395]
	TIME [epoch: 8.8 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06980231387100827		[learning rate: 0.00286]
		[batch 20/20] avg loss: 0.07304747738937725		[learning rate: 0.0028566]
	Learning Rate: 0.00285655
	LOSS [training: 0.07142489563019275 | validation: 0.11766736823322395]
	TIME [epoch: 8.81 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13506707458512923		[learning rate: 0.0028531]
		[batch 20/20] avg loss: 0.17261016625704304		[learning rate: 0.0028496]
	Learning Rate: 0.00284964
	LOSS [training: 0.15383862042108612 | validation: 0.0962378949541057]
	TIME [epoch: 8.79 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07998449562645896		[learning rate: 0.0028462]
		[batch 20/20] avg loss: 0.09710945496091214		[learning rate: 0.0028427]
	Learning Rate: 0.00284274
	LOSS [training: 0.08854697529368553 | validation: 0.18288375072075747]
	TIME [epoch: 8.79 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09037347591028834		[learning rate: 0.0028393]
		[batch 20/20] avg loss: 0.06626521166292326		[learning rate: 0.0028359]
	Learning Rate: 0.00283586
	LOSS [training: 0.07831934378660581 | validation: 0.1629908815171335]
	TIME [epoch: 8.79 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10865389986581282		[learning rate: 0.0028324]
		[batch 20/20] avg loss: 0.07269554959833549		[learning rate: 0.002829]
	Learning Rate: 0.00282899
	LOSS [training: 0.09067472473207414 | validation: 0.1468633067064409]
	TIME [epoch: 8.82 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13164308524240043		[learning rate: 0.0028256]
		[batch 20/20] avg loss: 0.06521124321664315		[learning rate: 0.0028221]
	Learning Rate: 0.00282214
	LOSS [training: 0.09842716422952179 | validation: 0.04562322289898433]
	TIME [epoch: 8.81 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07074597761562676		[learning rate: 0.0028187]
		[batch 20/20] avg loss: 0.12367133842069163		[learning rate: 0.0028153]
	Learning Rate: 0.00281531
	LOSS [training: 0.0972086580181592 | validation: 0.12403845856043588]
	TIME [epoch: 8.81 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.140769131215343		[learning rate: 0.0028119]
		[batch 20/20] avg loss: 0.10515871611177674		[learning rate: 0.0028085]
	Learning Rate: 0.00280849
	LOSS [training: 0.12296392366355986 | validation: 0.1171474363414037]
	TIME [epoch: 8.81 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0704259328787151		[learning rate: 0.0028051]
		[batch 20/20] avg loss: 0.13009003275609862		[learning rate: 0.0028017]
	Learning Rate: 0.0028017
	LOSS [training: 0.10025798281740687 | validation: 0.10831066543969496]
	TIME [epoch: 8.8 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14298050181653343		[learning rate: 0.0027983]
		[batch 20/20] avg loss: 0.07802092487613974		[learning rate: 0.0027949]
	Learning Rate: 0.00279491
	LOSS [training: 0.1105007133463366 | validation: 0.048725632614605274]
	TIME [epoch: 8.81 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08273821773844034		[learning rate: 0.0027915]
		[batch 20/20] avg loss: 0.10008393981291222		[learning rate: 0.0027881]
	Learning Rate: 0.00278815
	LOSS [training: 0.09141107877567628 | validation: 0.14289909837877973]
	TIME [epoch: 8.79 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07425672164442078		[learning rate: 0.0027848]
		[batch 20/20] avg loss: 0.059645121209876154		[learning rate: 0.0027814]
	Learning Rate: 0.0027814
	LOSS [training: 0.06695092142714847 | validation: 0.16083595218706548]
	TIME [epoch: 8.81 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08317549644703223		[learning rate: 0.002778]
		[batch 20/20] avg loss: 0.10860937844302303		[learning rate: 0.0027747]
	Learning Rate: 0.00277466
	LOSS [training: 0.09589243744502762 | validation: 0.08850702587196702]
	TIME [epoch: 8.8 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08618155202288141		[learning rate: 0.0027713]
		[batch 20/20] avg loss: 0.11515129099668128		[learning rate: 0.0027679]
	Learning Rate: 0.00276795
	LOSS [training: 0.10066642150978136 | validation: 0.08631487264012223]
	TIME [epoch: 8.82 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09322452451981424		[learning rate: 0.0027646]
		[batch 20/20] avg loss: 0.08368146332110525		[learning rate: 0.0027612]
	Learning Rate: 0.00276125
	LOSS [training: 0.08845299392045976 | validation: 0.07377602416393075]
	TIME [epoch: 8.8 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11365947456010309		[learning rate: 0.0027579]
		[batch 20/20] avg loss: 0.06216119410312335		[learning rate: 0.0027546]
	Learning Rate: 0.00275456
	LOSS [training: 0.08791033433161324 | validation: 0.05750122180683839]
	TIME [epoch: 8.79 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09551267872454983		[learning rate: 0.0027512]
		[batch 20/20] avg loss: 0.0944469652689416		[learning rate: 0.0027479]
	Learning Rate: 0.00274789
	LOSS [training: 0.09497982199674569 | validation: 0.04777073099428435]
	TIME [epoch: 8.8 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25054943022216325		[learning rate: 0.0027446]
		[batch 20/20] avg loss: 0.05725039700970269		[learning rate: 0.0027412]
	Learning Rate: 0.00274124
	LOSS [training: 0.15389991361593297 | validation: 0.06390757103165959]
	TIME [epoch: 8.81 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08301190737216206		[learning rate: 0.0027379]
		[batch 20/20] avg loss: 0.05914854249859318		[learning rate: 0.0027346]
	Learning Rate: 0.00273461
	LOSS [training: 0.07108022493537763 | validation: 0.04499762919107821]
	TIME [epoch: 8.8 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0967916755350153		[learning rate: 0.0027313]
		[batch 20/20] avg loss: 0.0750150693855569		[learning rate: 0.002728]
	Learning Rate: 0.00272799
	LOSS [training: 0.0859033724602861 | validation: 0.13111599389997314]
	TIME [epoch: 8.81 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0668838099688092		[learning rate: 0.0027247]
		[batch 20/20] avg loss: 0.08943231587546618		[learning rate: 0.0027214]
	Learning Rate: 0.00272138
	LOSS [training: 0.0781580629221377 | validation: 0.11953963668847828]
	TIME [epoch: 8.81 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08908741566094065		[learning rate: 0.0027181]
		[batch 20/20] avg loss: 0.09053958201636356		[learning rate: 0.0027148]
	Learning Rate: 0.00271479
	LOSS [training: 0.08981349883865213 | validation: 0.11034229755387664]
	TIME [epoch: 8.8 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10108667672331066		[learning rate: 0.0027115]
		[batch 20/20] avg loss: 0.048218171613154275		[learning rate: 0.0027082]
	Learning Rate: 0.00270822
	LOSS [training: 0.07465242416823245 | validation: 0.067976036806494]
	TIME [epoch: 8.82 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13739437500568236		[learning rate: 0.0027049]
		[batch 20/20] avg loss: 0.10784641031373599		[learning rate: 0.0027017]
	Learning Rate: 0.00270167
	LOSS [training: 0.12262039265970914 | validation: 0.1116209724218014]
	TIME [epoch: 8.79 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07939897342290461		[learning rate: 0.0026984]
		[batch 20/20] avg loss: 0.06356056999315626		[learning rate: 0.0026951]
	Learning Rate: 0.00269513
	LOSS [training: 0.07147977170803044 | validation: 0.05858592480887108]
	TIME [epoch: 8.8 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1468955055872579		[learning rate: 0.0026919]
		[batch 20/20] avg loss: 0.09996080397414399		[learning rate: 0.0026886]
	Learning Rate: 0.0026886
	LOSS [training: 0.12342815478070095 | validation: 0.07274771159380694]
	TIME [epoch: 8.8 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09754060578029525		[learning rate: 0.0026853]
		[batch 20/20] avg loss: 0.07758645961128746		[learning rate: 0.0026821]
	Learning Rate: 0.00268209
	LOSS [training: 0.08756353269579134 | validation: 0.069620777377966]
	TIME [epoch: 8.82 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11010996174919452		[learning rate: 0.0026788]
		[batch 20/20] avg loss: 0.0889098365774407		[learning rate: 0.0026756]
	Learning Rate: 0.0026756
	LOSS [training: 0.09950989916331762 | validation: 0.06638044735467527]
	TIME [epoch: 8.81 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0590947746450198		[learning rate: 0.0026724]
		[batch 20/20] avg loss: 0.08614678740672518		[learning rate: 0.0026691]
	Learning Rate: 0.00266912
	LOSS [training: 0.07262078102587248 | validation: 0.09719759248463623]
	TIME [epoch: 8.8 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06877457324498003		[learning rate: 0.0026659]
		[batch 20/20] avg loss: 0.09276913566977478		[learning rate: 0.0026627]
	Learning Rate: 0.00266266
	LOSS [training: 0.0807718544573774 | validation: 0.10980353270417678]
	TIME [epoch: 8.79 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08204219318653942		[learning rate: 0.0026594]
		[batch 20/20] avg loss: 0.09217521046120945		[learning rate: 0.0026562]
	Learning Rate: 0.00265621
	LOSS [training: 0.08710870182387442 | validation: 0.08971767925162352]
	TIME [epoch: 8.81 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10444700283854172		[learning rate: 0.002653]
		[batch 20/20] avg loss: 0.13262429423641434		[learning rate: 0.0026498]
	Learning Rate: 0.00264978
	LOSS [training: 0.11853564853747803 | validation: 0.17189780159497225]
	TIME [epoch: 8.8 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10909609732141627		[learning rate: 0.0026466]
		[batch 20/20] avg loss: 0.10673068328220092		[learning rate: 0.0026434]
	Learning Rate: 0.00264337
	LOSS [training: 0.10791339030180858 | validation: 0.08636297321044323]
	TIME [epoch: 8.79 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12714741785727376		[learning rate: 0.0026402]
		[batch 20/20] avg loss: 0.13298651512732113		[learning rate: 0.002637]
	Learning Rate: 0.00263697
	LOSS [training: 0.13006696649229746 | validation: 0.09790115857099795]
	TIME [epoch: 8.81 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11447100042802202		[learning rate: 0.0026338]
		[batch 20/20] avg loss: 0.10947864695550388		[learning rate: 0.0026306]
	Learning Rate: 0.00263059
	LOSS [training: 0.11197482369176295 | validation: 0.20031004372536537]
	TIME [epoch: 8.82 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10385185003378797		[learning rate: 0.0026274]
		[batch 20/20] avg loss: 0.0890465765958443		[learning rate: 0.0026242]
	Learning Rate: 0.00262422
	LOSS [training: 0.09644921331481612 | validation: 0.08067074335899321]
	TIME [epoch: 8.8 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0710104151936993		[learning rate: 0.002621]
		[batch 20/20] avg loss: 0.0947206354113398		[learning rate: 0.0026179]
	Learning Rate: 0.00261787
	LOSS [training: 0.08286552530251957 | validation: 0.0931316589907876]
	TIME [epoch: 8.79 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08782424029646865		[learning rate: 0.0026147]
		[batch 20/20] avg loss: 0.0987116112465816		[learning rate: 0.0026115]
	Learning Rate: 0.00261153
	LOSS [training: 0.09326792577152512 | validation: 0.06980551961754801]
	TIME [epoch: 8.79 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07194680916124355		[learning rate: 0.0026084]
		[batch 20/20] avg loss: 0.10440729158621054		[learning rate: 0.0026052]
	Learning Rate: 0.00260521
	LOSS [training: 0.08817705037372704 | validation: 0.154238955182312]
	TIME [epoch: 8.81 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09263048984653524		[learning rate: 0.0026021]
		[batch 20/20] avg loss: 0.09854739146261762		[learning rate: 0.0025989]
	Learning Rate: 0.0025989
	LOSS [training: 0.09558894065457643 | validation: 0.10609366818929084]
	TIME [epoch: 8.8 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1227325119060847		[learning rate: 0.0025958]
		[batch 20/20] avg loss: 0.08401901804963792		[learning rate: 0.0025926]
	Learning Rate: 0.00259261
	LOSS [training: 0.10337576497786131 | validation: 0.07189266571345075]
	TIME [epoch: 8.8 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07474972866793879		[learning rate: 0.0025895]
		[batch 20/20] avg loss: 0.12013726429701177		[learning rate: 0.0025863]
	Learning Rate: 0.00258633
	LOSS [training: 0.09744349648247527 | validation: 0.17753526217559915]
	TIME [epoch: 8.79 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07740135987644751		[learning rate: 0.0025832]
		[batch 20/20] avg loss: 0.1028129617672369		[learning rate: 0.0025801]
	Learning Rate: 0.00258007
	LOSS [training: 0.0901071608218422 | validation: 0.1539797511515801]
	TIME [epoch: 8.79 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08193447152596468		[learning rate: 0.0025769]
		[batch 20/20] avg loss: 0.0726484018671564		[learning rate: 0.0025738]
	Learning Rate: 0.00257382
	LOSS [training: 0.07729143669656055 | validation: 0.07920700053604961]
	TIME [epoch: 8.82 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06707499703942378		[learning rate: 0.0025707]
		[batch 20/20] avg loss: 0.09520345266634396		[learning rate: 0.0025676]
	Learning Rate: 0.00256759
	LOSS [training: 0.08113922485288387 | validation: 0.15161595647468568]
	TIME [epoch: 8.8 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09294684522888674		[learning rate: 0.0025645]
		[batch 20/20] avg loss: 0.07203826866094566		[learning rate: 0.0025614]
	Learning Rate: 0.00256138
	LOSS [training: 0.08249255694491622 | validation: 0.05836123124263646]
	TIME [epoch: 8.81 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07552513624788676		[learning rate: 0.0025583]
		[batch 20/20] avg loss: 0.05808315049808108		[learning rate: 0.0025552]
	Learning Rate: 0.00255518
	LOSS [training: 0.06680414337298393 | validation: 0.09581609797189038]
	TIME [epoch: 8.81 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1181511336965311		[learning rate: 0.0025521]
		[batch 20/20] avg loss: 0.038788710168086354		[learning rate: 0.002549]
	Learning Rate: 0.00254899
	LOSS [training: 0.07846992193230873 | validation: 0.060467451124731605]
	TIME [epoch: 8.82 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061716080055690924		[learning rate: 0.0025459]
		[batch 20/20] avg loss: 0.07726030493266427		[learning rate: 0.0025428]
	Learning Rate: 0.00254282
	LOSS [training: 0.06948819249417759 | validation: 0.049191965072119564]
	TIME [epoch: 8.82 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11138184557780537		[learning rate: 0.0025397]
		[batch 20/20] avg loss: 0.14541513140459447		[learning rate: 0.0025367]
	Learning Rate: 0.00253667
	LOSS [training: 0.12839848849119992 | validation: 0.1380138951400724]
	TIME [epoch: 8.79 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11429846594338762		[learning rate: 0.0025336]
		[batch 20/20] avg loss: 0.07117640993229028		[learning rate: 0.0025305]
	Learning Rate: 0.00253052
	LOSS [training: 0.09273743793783897 | validation: 0.03649004936918784]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_667.pth
	Model improved!!!
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056216647729749415		[learning rate: 0.0025275]
		[batch 20/20] avg loss: 0.07669437780832082		[learning rate: 0.0025244]
	Learning Rate: 0.0025244
	LOSS [training: 0.06645551276903514 | validation: 0.09392975605260973]
	TIME [epoch: 8.81 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08907225417360082		[learning rate: 0.0025213]
		[batch 20/20] avg loss: 0.09238334355098109		[learning rate: 0.0025183]
	Learning Rate: 0.00251829
	LOSS [training: 0.09072779886229096 | validation: 0.05142329824671141]
	TIME [epoch: 8.8 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07794446310920201		[learning rate: 0.0025152]
		[batch 20/20] avg loss: 0.08467081904863273		[learning rate: 0.0025122]
	Learning Rate: 0.00251219
	LOSS [training: 0.08130764107891739 | validation: 0.07679961517244611]
	TIME [epoch: 8.8 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06731276208038359		[learning rate: 0.0025091]
		[batch 20/20] avg loss: 0.07691910347622097		[learning rate: 0.0025061]
	Learning Rate: 0.00250611
	LOSS [training: 0.07211593277830228 | validation: 0.10647959722122603]
	TIME [epoch: 8.79 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06815703147181502		[learning rate: 0.0025031]
		[batch 20/20] avg loss: 0.10289749717984006		[learning rate: 0.0025]
	Learning Rate: 0.00250004
	LOSS [training: 0.08552726432582755 | validation: 0.08406274064123494]
	TIME [epoch: 8.81 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08730323116631349		[learning rate: 0.002497]
		[batch 20/20] avg loss: 0.09111790183866912		[learning rate: 0.002494]
	Learning Rate: 0.00249399
	LOSS [training: 0.08921056650249132 | validation: 0.11493397754939647]
	TIME [epoch: 8.81 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0772163629155024		[learning rate: 0.002491]
		[batch 20/20] avg loss: 0.07296156415602148		[learning rate: 0.002488]
	Learning Rate: 0.00248795
	LOSS [training: 0.07508896353576194 | validation: 0.11988809672980272]
	TIME [epoch: 8.79 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052528260720340246		[learning rate: 0.0024849]
		[batch 20/20] avg loss: 0.08329215213451754		[learning rate: 0.0024819]
	Learning Rate: 0.00248193
	LOSS [training: 0.06791020642742888 | validation: 0.12549991559833443]
	TIME [epoch: 8.79 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09289636361085205		[learning rate: 0.0024789]
		[batch 20/20] avg loss: 0.07306426676471607		[learning rate: 0.0024759]
	Learning Rate: 0.00247592
	LOSS [training: 0.08298031518778405 | validation: 0.07016892481732871]
	TIME [epoch: 8.79 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13721243313816772		[learning rate: 0.0024729]
		[batch 20/20] avg loss: 0.11618075122056162		[learning rate: 0.0024699]
	Learning Rate: 0.00246993
	LOSS [training: 0.12669659217936466 | validation: 0.11068368427937084]
	TIME [epoch: 8.81 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09594013659905667		[learning rate: 0.0024669]
		[batch 20/20] avg loss: 0.07758407659402598		[learning rate: 0.0024639]
	Learning Rate: 0.00246395
	LOSS [training: 0.08676210659654134 | validation: 0.06634398506850175]
	TIME [epoch: 8.82 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08970483501030359		[learning rate: 0.002461]
		[batch 20/20] avg loss: 0.10563633218205745		[learning rate: 0.002458]
	Learning Rate: 0.00245798
	LOSS [training: 0.09767058359618053 | validation: 0.12441487225785758]
	TIME [epoch: 8.79 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0676261709346703		[learning rate: 0.002455]
		[batch 20/20] avg loss: 0.08547378444960649		[learning rate: 0.002452]
	Learning Rate: 0.00245203
	LOSS [training: 0.07654997769213838 | validation: 0.23248941834600892]
	TIME [epoch: 8.79 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12485932503956994		[learning rate: 0.0024491]
		[batch 20/20] avg loss: 0.07666043842591816		[learning rate: 0.0024461]
	Learning Rate: 0.0024461
	LOSS [training: 0.10075988173274404 | validation: 0.06558374298405313]
	TIME [epoch: 8.78 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0820794448650687		[learning rate: 0.0024431]
		[batch 20/20] avg loss: 0.08685492497572742		[learning rate: 0.0024402]
	Learning Rate: 0.00244018
	LOSS [training: 0.08446718492039805 | validation: 0.07995006919209541]
	TIME [epoch: 8.81 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08651467936272875		[learning rate: 0.0024372]
		[batch 20/20] avg loss: 0.09665696795603505		[learning rate: 0.0024343]
	Learning Rate: 0.00243427
	LOSS [training: 0.09158582365938191 | validation: 0.0564500377600697]
	TIME [epoch: 8.78 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06166454776438698		[learning rate: 0.0024313]
		[batch 20/20] avg loss: 0.0910805472981489		[learning rate: 0.0024284]
	Learning Rate: 0.00242837
	LOSS [training: 0.07637254753126795 | validation: 0.05950996460555312]
	TIME [epoch: 8.78 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059321024890887544		[learning rate: 0.0024254]
		[batch 20/20] avg loss: 0.067427162426489		[learning rate: 0.0024225]
	Learning Rate: 0.0024225
	LOSS [training: 0.06337409365868826 | validation: 0.08416703992713256]
	TIME [epoch: 8.79 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06658001076907148		[learning rate: 0.0024196]
		[batch 20/20] avg loss: 0.09231856673456632		[learning rate: 0.0024166]
	Learning Rate: 0.00241663
	LOSS [training: 0.07944928875181888 | validation: 0.06011397860275948]
	TIME [epoch: 8.8 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08264100684445982		[learning rate: 0.0024137]
		[batch 20/20] avg loss: 0.085920829935592		[learning rate: 0.0024108]
	Learning Rate: 0.00241078
	LOSS [training: 0.08428091839002591 | validation: 0.06442498874688347]
	TIME [epoch: 8.79 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08368611546734951		[learning rate: 0.0024079]
		[batch 20/20] avg loss: 0.06552273040784376		[learning rate: 0.0024049]
	Learning Rate: 0.00240495
	LOSS [training: 0.07460442293759664 | validation: 0.051446990161828014]
	TIME [epoch: 8.78 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056006787897924695		[learning rate: 0.002402]
		[batch 20/20] avg loss: 0.08959089701922986		[learning rate: 0.0023991]
	Learning Rate: 0.00239912
	LOSS [training: 0.07279884245857728 | validation: 0.06726311610245339]
	TIME [epoch: 8.8 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10883850679613039		[learning rate: 0.0023962]
		[batch 20/20] avg loss: 0.09233574503756894		[learning rate: 0.0023933]
	Learning Rate: 0.00239332
	LOSS [training: 0.10058712591684967 | validation: 0.04629604447616134]
	TIME [epoch: 8.81 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05768820278469078		[learning rate: 0.0023904]
		[batch 20/20] avg loss: 0.06068885313255257		[learning rate: 0.0023875]
	Learning Rate: 0.00238752
	LOSS [training: 0.05918852795862167 | validation: 0.03087172071487616]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_691.pth
	Model improved!!!
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07149662391411737		[learning rate: 0.0023846]
		[batch 20/20] avg loss: 0.09036254270457365		[learning rate: 0.0023817]
	Learning Rate: 0.00238174
	LOSS [training: 0.08092958330934553 | validation: 0.07317404962433548]
	TIME [epoch: 8.81 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09539434850214855		[learning rate: 0.0023789]
		[batch 20/20] avg loss: 0.07812847121296457		[learning rate: 0.002376]
	Learning Rate: 0.00237598
	LOSS [training: 0.08676140985755655 | validation: 0.06715942368881153]
	TIME [epoch: 8.78 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0720875423040441		[learning rate: 0.0023731]
		[batch 20/20] avg loss: 0.08032955579628823		[learning rate: 0.0023702]
	Learning Rate: 0.00237022
	LOSS [training: 0.07620854905016615 | validation: 0.06262973196546302]
	TIME [epoch: 8.8 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053664848297300574		[learning rate: 0.0023674]
		[batch 20/20] avg loss: 0.05997665118089281		[learning rate: 0.0023645]
	Learning Rate: 0.00236449
	LOSS [training: 0.05682074973909669 | validation: 0.09663315043086886]
	TIME [epoch: 8.81 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059353455879678496		[learning rate: 0.0023616]
		[batch 20/20] avg loss: 0.09427955781006969		[learning rate: 0.0023588]
	Learning Rate: 0.00235876
	LOSS [training: 0.07681650684487411 | validation: 0.06529719480924892]
	TIME [epoch: 8.79 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08966726599162853		[learning rate: 0.0023559]
		[batch 20/20] avg loss: 0.09376029879796155		[learning rate: 0.0023531]
	Learning Rate: 0.00235305
	LOSS [training: 0.09171378239479501 | validation: 0.07016276823016523]
	TIME [epoch: 8.8 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10124224234339593		[learning rate: 0.0023502]
		[batch 20/20] avg loss: 0.06415311929989151		[learning rate: 0.0023474]
	Learning Rate: 0.00234736
	LOSS [training: 0.08269768082164372 | validation: 0.07744568745093176]
	TIME [epoch: 8.79 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1052184111681584		[learning rate: 0.0023445]
		[batch 20/20] avg loss: 0.08549297276659612		[learning rate: 0.0023417]
	Learning Rate: 0.00234167
	LOSS [training: 0.09535569196737727 | validation: 0.08435635837306782]
	TIME [epoch: 8.81 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08474792081243746		[learning rate: 0.0023388]
		[batch 20/20] avg loss: 0.0568907942431832		[learning rate: 0.002336]
	Learning Rate: 0.002336
	LOSS [training: 0.07081935752781032 | validation: 0.039591641628321614]
	TIME [epoch: 8.8 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07562855112827374		[learning rate: 0.0023332]
		[batch 20/20] avg loss: 0.06843662567796346		[learning rate: 0.0023303]
	Learning Rate: 0.00233035
	LOSS [training: 0.07203258840311863 | validation: 0.0626434742230118]
	TIME [epoch: 8.79 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07694346055664475		[learning rate: 0.0023275]
		[batch 20/20] avg loss: 0.06426928718158101		[learning rate: 0.0023247]
	Learning Rate: 0.00232471
	LOSS [training: 0.07060637386911287 | validation: 0.12390520553679751]
	TIME [epoch: 8.79 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07535068880438142		[learning rate: 0.0023219]
		[batch 20/20] avg loss: 0.09050988482670615		[learning rate: 0.0023191]
	Learning Rate: 0.00231908
	LOSS [training: 0.08293028681554378 | validation: 0.036642360163358284]
	TIME [epoch: 8.81 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06734016017537202		[learning rate: 0.0023163]
		[batch 20/20] avg loss: 0.07926067494572839		[learning rate: 0.0023135]
	Learning Rate: 0.00231347
	LOSS [training: 0.0733004175605502 | validation: 0.10102434146075794]
	TIME [epoch: 8.82 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054953764123295376		[learning rate: 0.0023107]
		[batch 20/20] avg loss: 0.06383711365166868		[learning rate: 0.0023079]
	Learning Rate: 0.00230787
	LOSS [training: 0.05939543888748202 | validation: 0.0974957227164563]
	TIME [epoch: 8.8 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0893192170326174		[learning rate: 0.0023051]
		[batch 20/20] avg loss: 0.09992158613502158		[learning rate: 0.0023023]
	Learning Rate: 0.00230228
	LOSS [training: 0.09462040158381949 | validation: 0.05847037365616721]
	TIME [epoch: 8.79 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07492822232962354		[learning rate: 0.0022995]
		[batch 20/20] avg loss: 0.0670653646094908		[learning rate: 0.0022967]
	Learning Rate: 0.00229671
	LOSS [training: 0.07099679346955716 | validation: 0.1071885308416945]
	TIME [epoch: 8.79 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10806712005175703		[learning rate: 0.0022939]
		[batch 20/20] avg loss: 0.07955025519822861		[learning rate: 0.0022911]
	Learning Rate: 0.00229115
	LOSS [training: 0.09380868762499281 | validation: 0.10749914368967309]
	TIME [epoch: 8.8 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06692740754866992		[learning rate: 0.0022884]
		[batch 20/20] avg loss: 0.06923390260345633		[learning rate: 0.0022856]
	Learning Rate: 0.0022856
	LOSS [training: 0.06808065507606312 | validation: 0.06035895292533421]
	TIME [epoch: 8.8 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06531103102568771		[learning rate: 0.0022828]
		[batch 20/20] avg loss: 0.05460070208246646		[learning rate: 0.0022801]
	Learning Rate: 0.00228007
	LOSS [training: 0.05995586655407707 | validation: 0.12255961174809979]
	TIME [epoch: 8.79 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05822284120879264		[learning rate: 0.0022773]
		[batch 20/20] avg loss: 0.09840400393494024		[learning rate: 0.0022745]
	Learning Rate: 0.00227455
	LOSS [training: 0.07831342257186644 | validation: 0.08249464605266157]
	TIME [epoch: 8.79 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08259973265709171		[learning rate: 0.0022718]
		[batch 20/20] avg loss: 0.08614058394630544		[learning rate: 0.002269]
	Learning Rate: 0.00226904
	LOSS [training: 0.08437015830169858 | validation: 0.07810170199766596]
	TIME [epoch: 8.82 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07272230799147397		[learning rate: 0.0022663]
		[batch 20/20] avg loss: 0.05508812671067599		[learning rate: 0.0022635]
	Learning Rate: 0.00226355
	LOSS [training: 0.06390521735107499 | validation: 0.04065464649391407]
	TIME [epoch: 8.79 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07943908823652811		[learning rate: 0.0022608]
		[batch 20/20] avg loss: 0.08405340819366511		[learning rate: 0.0022581]
	Learning Rate: 0.00225807
	LOSS [training: 0.08174624821509661 | validation: 0.08123712755546063]
	TIME [epoch: 8.8 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0976214180795862		[learning rate: 0.0022553]
		[batch 20/20] avg loss: 0.05379288699288467		[learning rate: 0.0022526]
	Learning Rate: 0.0022526
	LOSS [training: 0.07570715253623546 | validation: 0.08795365500010421]
	TIME [epoch: 8.79 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06560143314249076		[learning rate: 0.0022499]
		[batch 20/20] avg loss: 0.09413698376461684		[learning rate: 0.0022471]
	Learning Rate: 0.00224715
	LOSS [training: 0.0798692084535538 | validation: 0.0918794018111031]
	TIME [epoch: 8.81 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06230253452149019		[learning rate: 0.0022444]
		[batch 20/20] avg loss: 0.07922916015219962		[learning rate: 0.0022417]
	Learning Rate: 0.00224171
	LOSS [training: 0.0707658473368449 | validation: 0.04871013929735182]
	TIME [epoch: 8.8 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07092432266959603		[learning rate: 0.002239]
		[batch 20/20] avg loss: 0.05179060200810139		[learning rate: 0.0022363]
	Learning Rate: 0.00223628
	LOSS [training: 0.061357462338848714 | validation: 0.06859942528641541]
	TIME [epoch: 8.8 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04551159319435735		[learning rate: 0.0022336]
		[batch 20/20] avg loss: 0.06948402699586109		[learning rate: 0.0022309]
	Learning Rate: 0.00223087
	LOSS [training: 0.057497810095109236 | validation: 0.09910780297679127]
	TIME [epoch: 8.8 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07473372741055098		[learning rate: 0.0022282]
		[batch 20/20] avg loss: 0.05293393266443769		[learning rate: 0.0022255]
	Learning Rate: 0.00222547
	LOSS [training: 0.06383383003749435 | validation: 0.03893059936629076]
	TIME [epoch: 8.79 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08045670771875528		[learning rate: 0.0022228]
		[batch 20/20] avg loss: 0.09297199538641329		[learning rate: 0.0022201]
	Learning Rate: 0.00222008
	LOSS [training: 0.08671435155258429 | validation: 0.16315244139711269]
	TIME [epoch: 8.82 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08083971818233983		[learning rate: 0.0022174]
		[batch 20/20] avg loss: 0.09236498101606154		[learning rate: 0.0022147]
	Learning Rate: 0.0022147
	LOSS [training: 0.08660234959920068 | validation: 0.07530978100431726]
	TIME [epoch: 8.79 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0695052246307178		[learning rate: 0.002212]
		[batch 20/20] avg loss: 0.12161581886495298		[learning rate: 0.0022093]
	Learning Rate: 0.00220934
	LOSS [training: 0.09556052174783539 | validation: 0.08285446377689876]
	TIME [epoch: 8.79 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0915094851334469		[learning rate: 0.0022067]
		[batch 20/20] avg loss: 0.06889381291565572		[learning rate: 0.002204]
	Learning Rate: 0.00220399
	LOSS [training: 0.08020164902455132 | validation: 0.06420263790645143]
	TIME [epoch: 8.79 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07348736894574467		[learning rate: 0.0022013]
		[batch 20/20] avg loss: 0.10281999615509729		[learning rate: 0.0021987]
	Learning Rate: 0.00219866
	LOSS [training: 0.08815368255042097 | validation: 0.04426286964919802]
	TIME [epoch: 8.81 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0872133723477134		[learning rate: 0.002196]
		[batch 20/20] avg loss: 0.07391696952506935		[learning rate: 0.0021933]
	Learning Rate: 0.00219334
	LOSS [training: 0.08056517093639137 | validation: 0.1763793737612873]
	TIME [epoch: 8.8 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0854265268298063		[learning rate: 0.0021907]
		[batch 20/20] avg loss: 0.08286851585043134		[learning rate: 0.002188]
	Learning Rate: 0.00218803
	LOSS [training: 0.0841475213401188 | validation: 0.11009545244590783]
	TIME [epoch: 8.8 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07174536989333312		[learning rate: 0.0021854]
		[batch 20/20] avg loss: 0.10018837870415546		[learning rate: 0.0021827]
	Learning Rate: 0.00218273
	LOSS [training: 0.08596687429874429 | validation: 0.07263134093884703]
	TIME [epoch: 8.79 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06928132772756317		[learning rate: 0.0021801]
		[batch 20/20] avg loss: 0.052814810335808635		[learning rate: 0.0021774]
	Learning Rate: 0.00217745
	LOSS [training: 0.0610480690316859 | validation: 0.08390054889493528]
	TIME [epoch: 8.8 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0673758370092039		[learning rate: 0.0021748]
		[batch 20/20] avg loss: 0.05824721036157281		[learning rate: 0.0021722]
	Learning Rate: 0.00217217
	LOSS [training: 0.06281152368538837 | validation: 0.04403330954425964]
	TIME [epoch: 8.82 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11434050295403916		[learning rate: 0.0021695]
		[batch 20/20] avg loss: 0.09588133553669502		[learning rate: 0.0021669]
	Learning Rate: 0.00216692
	LOSS [training: 0.10511091924536708 | validation: 0.1682846201119001]
	TIME [epoch: 8.82 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12351211082466818		[learning rate: 0.0021643]
		[batch 20/20] avg loss: 0.08935162941301816		[learning rate: 0.0021617]
	Learning Rate: 0.00216167
	LOSS [training: 0.10643187011884317 | validation: 0.07993816499354311]
	TIME [epoch: 8.81 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06758983726722305		[learning rate: 0.0021591]
		[batch 20/20] avg loss: 0.10397424843966788		[learning rate: 0.0021564]
	Learning Rate: 0.00215644
	LOSS [training: 0.08578204285344546 | validation: 0.06893208574479462]
	TIME [epoch: 8.8 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06496794210232645		[learning rate: 0.0021538]
		[batch 20/20] avg loss: 0.05517171558394228		[learning rate: 0.0021512]
	Learning Rate: 0.00215122
	LOSS [training: 0.06006982884313437 | validation: 0.1419905038190994]
	TIME [epoch: 8.82 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08253049486239075		[learning rate: 0.0021486]
		[batch 20/20] avg loss: 0.04808086959277622		[learning rate: 0.002146]
	Learning Rate: 0.00214601
	LOSS [training: 0.0653056822275835 | validation: 0.058727370487285546]
	TIME [epoch: 8.8 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08234208271252653		[learning rate: 0.0021434]
		[batch 20/20] avg loss: 0.061340412571437554		[learning rate: 0.0021408]
	Learning Rate: 0.00214081
	LOSS [training: 0.07184124764198205 | validation: 0.05238543183254727]
	TIME [epoch: 8.8 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04687326796931078		[learning rate: 0.0021382]
		[batch 20/20] avg loss: 0.060381203174950104		[learning rate: 0.0021356]
	Learning Rate: 0.00213563
	LOSS [training: 0.05362723557213045 | validation: 0.06607413793834405]
	TIME [epoch: 8.79 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1260780047170221		[learning rate: 0.002133]
		[batch 20/20] avg loss: 0.11695659893176495		[learning rate: 0.0021305]
	Learning Rate: 0.00213046
	LOSS [training: 0.12151730182439353 | validation: 0.10280958177791236]
	TIME [epoch: 8.82 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08750674386175335		[learning rate: 0.0021279]
		[batch 20/20] avg loss: 0.1027189149564444		[learning rate: 0.0021253]
	Learning Rate: 0.0021253
	LOSS [training: 0.0951128294090989 | validation: 0.09129451344211309]
	TIME [epoch: 8.8 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07252417037496392		[learning rate: 0.0021227]
		[batch 20/20] avg loss: 0.06615674841601897		[learning rate: 0.0021202]
	Learning Rate: 0.00212016
	LOSS [training: 0.06934045939549145 | validation: 0.05182850108185294]
	TIME [epoch: 8.8 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05668902755213636		[learning rate: 0.0021176]
		[batch 20/20] avg loss: 0.07682507646891273		[learning rate: 0.002115]
	Learning Rate: 0.00211503
	LOSS [training: 0.06675705201052455 | validation: 0.09307539151812777]
	TIME [epoch: 8.8 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0896235970301174		[learning rate: 0.0021125]
		[batch 20/20] avg loss: 0.07177400401407398		[learning rate: 0.0021099]
	Learning Rate: 0.00210991
	LOSS [training: 0.0806988005220957 | validation: 0.11797531303158813]
	TIME [epoch: 8.81 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05538341931020848		[learning rate: 0.0021074]
		[batch 20/20] avg loss: 0.09400481885769944		[learning rate: 0.0021048]
	Learning Rate: 0.0021048
	LOSS [training: 0.07469411908395397 | validation: 0.15049035801345378]
	TIME [epoch: 8.81 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06288591545337632		[learning rate: 0.0021022]
		[batch 20/20] avg loss: 0.06276833959437465		[learning rate: 0.0020997]
	Learning Rate: 0.0020997
	LOSS [training: 0.06282712752387548 | validation: 0.06249361181408313]
	TIME [epoch: 8.8 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08694644976289674		[learning rate: 0.0020972]
		[batch 20/20] avg loss: 0.07573846103007739		[learning rate: 0.0020946]
	Learning Rate: 0.00209462
	LOSS [training: 0.08134245539648705 | validation: 0.07906136669415849]
	TIME [epoch: 8.8 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05243053765854616		[learning rate: 0.0020921]
		[batch 20/20] avg loss: 0.12021122974451307		[learning rate: 0.0020895]
	Learning Rate: 0.00208955
	LOSS [training: 0.08632088370152963 | validation: 0.15693568732886842]
	TIME [epoch: 8.81 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0988635259357585		[learning rate: 0.002087]
		[batch 20/20] avg loss: 0.07629618475642871		[learning rate: 0.0020845]
	Learning Rate: 0.00208449
	LOSS [training: 0.08757985534609361 | validation: 0.13391381159912735]
	TIME [epoch: 8.8 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08831480424525959		[learning rate: 0.002082]
		[batch 20/20] avg loss: 0.05760359084867299		[learning rate: 0.0020794]
	Learning Rate: 0.00207944
	LOSS [training: 0.07295919754696631 | validation: 0.07085260079928803]
	TIME [epoch: 8.79 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05103473654180914		[learning rate: 0.0020769]
		[batch 20/20] avg loss: 0.0560995855175858		[learning rate: 0.0020744]
	Learning Rate: 0.00207441
	LOSS [training: 0.053567161029697476 | validation: 0.06111825970639123]
	TIME [epoch: 8.79 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06517340422034124		[learning rate: 0.0020719]
		[batch 20/20] avg loss: 0.09869668865696095		[learning rate: 0.0020694]
	Learning Rate: 0.00206939
	LOSS [training: 0.0819350464386511 | validation: 0.09026176188658652]
	TIME [epoch: 8.79 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05926965607464472		[learning rate: 0.0020669]
		[batch 20/20] avg loss: 0.05620130752940996		[learning rate: 0.0020644]
	Learning Rate: 0.00206438
	LOSS [training: 0.057735481802027346 | validation: 0.11456300091239754]
	TIME [epoch: 8.81 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06796287097477183		[learning rate: 0.0020619]
		[batch 20/20] avg loss: 0.08469548866520075		[learning rate: 0.0020594]
	Learning Rate: 0.00205938
	LOSS [training: 0.0763291798199863 | validation: 0.09315363551113805]
	TIME [epoch: 8.8 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08897457825104635		[learning rate: 0.0020569]
		[batch 20/20] avg loss: 0.08665337294092369		[learning rate: 0.0020544]
	Learning Rate: 0.0020544
	LOSS [training: 0.08781397559598503 | validation: 0.06480039716339434]
	TIME [epoch: 8.81 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05859386233923854		[learning rate: 0.0020519]
		[batch 20/20] avg loss: 0.06478664278486714		[learning rate: 0.0020494]
	Learning Rate: 0.00204942
	LOSS [training: 0.06169025256205283 | validation: 0.026822950811270157]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_754.pth
	Model improved!!!
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041849588139481655		[learning rate: 0.0020469]
		[batch 20/20] avg loss: 0.07899651804887241		[learning rate: 0.0020445]
	Learning Rate: 0.00204446
	LOSS [training: 0.060423053094177025 | validation: 0.07486304687989886]
	TIME [epoch: 8.82 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08274498297728627		[learning rate: 0.002042]
		[batch 20/20] avg loss: 0.05660322718830483		[learning rate: 0.0020395]
	Learning Rate: 0.00203951
	LOSS [training: 0.06967410508279556 | validation: 0.05391799871866011]
	TIME [epoch: 8.78 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06833627667922888		[learning rate: 0.002037]
		[batch 20/20] avg loss: 0.07622661579630682		[learning rate: 0.0020346]
	Learning Rate: 0.00203457
	LOSS [training: 0.07228144623776785 | validation: 0.0796668920244292]
	TIME [epoch: 8.8 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044391223277168566		[learning rate: 0.0020321]
		[batch 20/20] avg loss: 0.0653002695624155		[learning rate: 0.0020296]
	Learning Rate: 0.00202965
	LOSS [training: 0.05484574641979204 | validation: 0.038938040596993684]
	TIME [epoch: 8.8 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05624814645263765		[learning rate: 0.0020272]
		[batch 20/20] avg loss: 0.05134863706394284		[learning rate: 0.0020247]
	Learning Rate: 0.00202474
	LOSS [training: 0.053798391758290244 | validation: 0.09118568795242249]
	TIME [epoch: 8.8 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07648667311095161		[learning rate: 0.0020223]
		[batch 20/20] avg loss: 0.05311115115722055		[learning rate: 0.0020198]
	Learning Rate: 0.00201983
	LOSS [training: 0.06479891213408609 | validation: 0.0640303159854265]
	TIME [epoch: 8.82 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08252943447497199		[learning rate: 0.0020174]
		[batch 20/20] avg loss: 0.0681394289940416		[learning rate: 0.0020149]
	Learning Rate: 0.00201494
	LOSS [training: 0.0753344317345068 | validation: 0.04722429157793162]
	TIME [epoch: 8.79 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0662048293460034		[learning rate: 0.0020125]
		[batch 20/20] avg loss: 0.07702356178708411		[learning rate: 0.0020101]
	Learning Rate: 0.00201007
	LOSS [training: 0.07161419556654376 | validation: 0.01860507266207674]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_762.pth
	Model improved!!!
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05956342413249912		[learning rate: 0.0020076]
		[batch 20/20] avg loss: 0.07652891134366255		[learning rate: 0.0020052]
	Learning Rate: 0.0020052
	LOSS [training: 0.06804616773808084 | validation: 0.0365818486636665]
	TIME [epoch: 8.79 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045846135469334366		[learning rate: 0.0020028]
		[batch 20/20] avg loss: 0.042485687907906966		[learning rate: 0.0020003]
	Learning Rate: 0.00200035
	LOSS [training: 0.044165911688620656 | validation: 0.031252004908591674]
	TIME [epoch: 8.8 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04748096541154967		[learning rate: 0.0019979]
		[batch 20/20] avg loss: 0.09989879597180736		[learning rate: 0.0019955]
	Learning Rate: 0.0019955
	LOSS [training: 0.0736898806916785 | validation: 0.05648511803029931]
	TIME [epoch: 8.8 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06381570700980527		[learning rate: 0.0019931]
		[batch 20/20] avg loss: 0.0820371132831396		[learning rate: 0.0019907]
	Learning Rate: 0.00199067
	LOSS [training: 0.07292641014647244 | validation: 0.06137413828653348]
	TIME [epoch: 8.78 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08705082130924369		[learning rate: 0.0019883]
		[batch 20/20] avg loss: 0.07035286220896227		[learning rate: 0.0019859]
	Learning Rate: 0.00198585
	LOSS [training: 0.078701841759103 | validation: 0.04894575849607812]
	TIME [epoch: 8.79 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053031000889044035		[learning rate: 0.0019834]
		[batch 20/20] avg loss: 0.06881955043068094		[learning rate: 0.001981]
	Learning Rate: 0.00198105
	LOSS [training: 0.060925275659862485 | validation: 0.08031603444455358]
	TIME [epoch: 8.78 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06908680706307821		[learning rate: 0.0019786]
		[batch 20/20] avg loss: 0.050269773043039034		[learning rate: 0.0019763]
	Learning Rate: 0.00197625
	LOSS [training: 0.05967829005305862 | validation: 0.02816598321763579]
	TIME [epoch: 8.82 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046583928931480466		[learning rate: 0.0019739]
		[batch 20/20] avg loss: 0.04515240211281933		[learning rate: 0.0019715]
	Learning Rate: 0.00197147
	LOSS [training: 0.0458681655221499 | validation: 0.038006629631180316]
	TIME [epoch: 8.79 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0849561091159665		[learning rate: 0.0019691]
		[batch 20/20] avg loss: 0.10870138253679998		[learning rate: 0.0019667]
	Learning Rate: 0.00196669
	LOSS [training: 0.09682874582638323 | validation: 0.07164755363427344]
	TIME [epoch: 8.8 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07693639958103758		[learning rate: 0.0019643]
		[batch 20/20] avg loss: 0.07124579887241361		[learning rate: 0.0019619]
	Learning Rate: 0.00196193
	LOSS [training: 0.0740910992267256 | validation: 0.06379171737880074]
	TIME [epoch: 8.8 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07603423273328098		[learning rate: 0.0019596]
		[batch 20/20] avg loss: 0.06887521850103433		[learning rate: 0.0019572]
	Learning Rate: 0.00195718
	LOSS [training: 0.07245472561715766 | validation: 0.03143986195832357]
	TIME [epoch: 8.82 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07006799362644185		[learning rate: 0.0019548]
		[batch 20/20] avg loss: 0.06172056693067709		[learning rate: 0.0019524]
	Learning Rate: 0.00195245
	LOSS [training: 0.06589428027855947 | validation: 0.02988005364949217]
	TIME [epoch: 8.8 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04772093417876412		[learning rate: 0.0019501]
		[batch 20/20] avg loss: 0.06033518254479138		[learning rate: 0.0019477]
	Learning Rate: 0.00194772
	LOSS [training: 0.05402805836177775 | validation: 0.047670068740650745]
	TIME [epoch: 8.79 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11666094008541694		[learning rate: 0.0019454]
		[batch 20/20] avg loss: 0.09649201939666986		[learning rate: 0.001943]
	Learning Rate: 0.001943
	LOSS [training: 0.10657647974104338 | validation: 0.0875377745480673]
	TIME [epoch: 8.78 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07746420367862702		[learning rate: 0.0019407]
		[batch 20/20] avg loss: 0.06837182664293298		[learning rate: 0.0019383]
	Learning Rate: 0.0019383
	LOSS [training: 0.07291801516078002 | validation: 0.07196234117994268]
	TIME [epoch: 8.8 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06900576268318685		[learning rate: 0.001936]
		[batch 20/20] avg loss: 0.06548630149523271		[learning rate: 0.0019336]
	Learning Rate: 0.00193361
	LOSS [training: 0.06724603208920978 | validation: 0.08366415399467536]
	TIME [epoch: 8.8 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0941641910781106		[learning rate: 0.0019313]
		[batch 20/20] avg loss: 0.06673359237325305		[learning rate: 0.0019289]
	Learning Rate: 0.00192893
	LOSS [training: 0.08044889172568181 | validation: 0.06866997322572661]
	TIME [epoch: 8.79 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05048176557414004		[learning rate: 0.0019266]
		[batch 20/20] avg loss: 0.06689208426898227		[learning rate: 0.0019243]
	Learning Rate: 0.00192426
	LOSS [training: 0.05868692492156115 | validation: 0.0397917300503662]
	TIME [epoch: 8.78 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04381612402965665		[learning rate: 0.0019219]
		[batch 20/20] avg loss: 0.04269223462514249		[learning rate: 0.0019196]
	Learning Rate: 0.0019196
	LOSS [training: 0.04325417932739957 | validation: 0.05437318952612355]
	TIME [epoch: 8.78 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0510394430576082		[learning rate: 0.0019173]
		[batch 20/20] avg loss: 0.055916947855854736		[learning rate: 0.001915]
	Learning Rate: 0.00191495
	LOSS [training: 0.05347819545673147 | validation: 0.06380912232284347]
	TIME [epoch: 8.82 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05531616769277692		[learning rate: 0.0019126]
		[batch 20/20] avg loss: 0.050899263618893145		[learning rate: 0.0019103]
	Learning Rate: 0.00191032
	LOSS [training: 0.05310771565583504 | validation: 0.06988985186856013]
	TIME [epoch: 8.79 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10739277544513952		[learning rate: 0.001908]
		[batch 20/20] avg loss: 0.06207806137910001		[learning rate: 0.0019057]
	Learning Rate: 0.00190569
	LOSS [training: 0.08473541841211976 | validation: 0.036597629213854926]
	TIME [epoch: 8.8 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03517887580137918		[learning rate: 0.0019034]
		[batch 20/20] avg loss: 0.046067418978124046		[learning rate: 0.0019011]
	Learning Rate: 0.00190108
	LOSS [training: 0.04062314738975162 | validation: 0.0863313001605235]
	TIME [epoch: 8.8 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07596848971474247		[learning rate: 0.0018988]
		[batch 20/20] avg loss: 0.05946940785318672		[learning rate: 0.0018965]
	Learning Rate: 0.00189648
	LOSS [training: 0.06771894878396459 | validation: 0.06771906867161052]
	TIME [epoch: 8.82 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05180671981040315		[learning rate: 0.0018942]
		[batch 20/20] avg loss: 0.06315428546288847		[learning rate: 0.0018919]
	Learning Rate: 0.00189188
	LOSS [training: 0.05748050263664582 | validation: 0.05049406651857683]
	TIME [epoch: 8.81 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05160754957790359		[learning rate: 0.0018896]
		[batch 20/20] avg loss: 0.07628418723026215		[learning rate: 0.0018873]
	Learning Rate: 0.00188731
	LOSS [training: 0.06394586840408287 | validation: 0.038801158162884236]
	TIME [epoch: 8.78 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09153623592830039		[learning rate: 0.001885]
		[batch 20/20] avg loss: 0.08369437568488367		[learning rate: 0.0018827]
	Learning Rate: 0.00188274
	LOSS [training: 0.08761530580659205 | validation: 0.09999983329626097]
	TIME [epoch: 8.79 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07396902401090909		[learning rate: 0.0018805]
		[batch 20/20] avg loss: 0.07025518072302747		[learning rate: 0.0018782]
	Learning Rate: 0.00187818
	LOSS [training: 0.07211210236696829 | validation: 0.05095367843991081]
	TIME [epoch: 8.79 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05843597358900552		[learning rate: 0.0018759]
		[batch 20/20] avg loss: 0.05258212193671245		[learning rate: 0.0018736]
	Learning Rate: 0.00187363
	LOSS [training: 0.05550904776285899 | validation: 0.06681495629936951]
	TIME [epoch: 8.8 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09573742902663386		[learning rate: 0.0018714]
		[batch 20/20] avg loss: 0.054696080072905495		[learning rate: 0.0018691]
	Learning Rate: 0.0018691
	LOSS [training: 0.0752167545497697 | validation: 0.06591233599315334]
	TIME [epoch: 8.79 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07226413875542545		[learning rate: 0.0018668]
		[batch 20/20] avg loss: 0.04851338291168776		[learning rate: 0.0018646]
	Learning Rate: 0.00186457
	LOSS [training: 0.06038876083355661 | validation: 0.045246483753925086]
	TIME [epoch: 8.79 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0510153218745113		[learning rate: 0.0018623]
		[batch 20/20] avg loss: 0.07418328300884991		[learning rate: 0.0018601]
	Learning Rate: 0.00186006
	LOSS [training: 0.06259930244168059 | validation: 0.1005797963730008]
	TIME [epoch: 8.78 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06765959907255867		[learning rate: 0.0018578]
		[batch 20/20] avg loss: 0.062372624616635466		[learning rate: 0.0018556]
	Learning Rate: 0.00185555
	LOSS [training: 0.06501611184459707 | validation: 0.06679122132691967]
	TIME [epoch: 8.81 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07539875559592032		[learning rate: 0.0018533]
		[batch 20/20] avg loss: 0.05020277288063637		[learning rate: 0.0018511]
	Learning Rate: 0.00185106
	LOSS [training: 0.06280076423827835 | validation: 0.05030810269341947]
	TIME [epoch: 8.8 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04772807761717433		[learning rate: 0.0018488]
		[batch 20/20] avg loss: 0.06404838745865259		[learning rate: 0.0018466]
	Learning Rate: 0.00184658
	LOSS [training: 0.055888232537913474 | validation: 0.06471870683579704]
	TIME [epoch: 8.79 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04660933417347251		[learning rate: 0.0018443]
		[batch 20/20] avg loss: 0.08667345211309911		[learning rate: 0.0018421]
	Learning Rate: 0.00184211
	LOSS [training: 0.06664139314328581 | validation: 0.04158330111212939]
	TIME [epoch: 8.79 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07551058675687006		[learning rate: 0.0018399]
		[batch 20/20] avg loss: 0.06761555300786899		[learning rate: 0.0018377]
	Learning Rate: 0.00183765
	LOSS [training: 0.07156306988236952 | validation: 0.054351913095209495]
	TIME [epoch: 8.82 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06820485190118811		[learning rate: 0.0018354]
		[batch 20/20] avg loss: 0.0934756591393471		[learning rate: 0.0018332]
	Learning Rate: 0.0018332
	LOSS [training: 0.08084025552026759 | validation: 0.06381876893713623]
	TIME [epoch: 8.79 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06538959200844326		[learning rate: 0.001831]
		[batch 20/20] avg loss: 0.0465757492102946		[learning rate: 0.0018288]
	Learning Rate: 0.00182876
	LOSS [training: 0.055982670609368936 | validation: 0.06637436921711781]
	TIME [epoch: 8.78 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04822984098543488		[learning rate: 0.0018265]
		[batch 20/20] avg loss: 0.07344497027731464		[learning rate: 0.0018243]
	Learning Rate: 0.00182434
	LOSS [training: 0.060837405631374755 | validation: 0.029879040249014912]
	TIME [epoch: 8.79 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04979176544090759		[learning rate: 0.0018221]
		[batch 20/20] avg loss: 0.0881138851201941		[learning rate: 0.0018199]
	Learning Rate: 0.00181992
	LOSS [training: 0.06895282528055083 | validation: 0.10769529622598853]
	TIME [epoch: 8.8 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04817398536169015		[learning rate: 0.0018177]
		[batch 20/20] avg loss: 0.049601421180567054		[learning rate: 0.0018155]
	Learning Rate: 0.00181552
	LOSS [training: 0.0488877032711286 | validation: 0.07883585182478942]
	TIME [epoch: 8.79 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09359427817019976		[learning rate: 0.0018133]
		[batch 20/20] avg loss: 0.07625460967437653		[learning rate: 0.0018111]
	Learning Rate: 0.00181112
	LOSS [training: 0.08492444392228815 | validation: 0.03699903085716769]
	TIME [epoch: 8.78 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03651568375620866		[learning rate: 0.0018089]
		[batch 20/20] avg loss: 0.06997206070897002		[learning rate: 0.0018067]
	Learning Rate: 0.00180674
	LOSS [training: 0.05324387223258933 | validation: 0.05364500384671389]
	TIME [epoch: 8.78 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04088236110320863		[learning rate: 0.0018045]
		[batch 20/20] avg loss: 0.09961078732987132		[learning rate: 0.0018024]
	Learning Rate: 0.00180236
	LOSS [training: 0.07024657421653997 | validation: 0.0704348945970398]
	TIME [epoch: 8.8 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06261371977551664		[learning rate: 0.0018002]
		[batch 20/20] avg loss: 0.05485386396483505		[learning rate: 0.001798]
	Learning Rate: 0.001798
	LOSS [training: 0.05873379187017584 | validation: 0.053879743607390035]
	TIME [epoch: 8.8 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07635189175192403		[learning rate: 0.0017958]
		[batch 20/20] avg loss: 0.045693652971993716		[learning rate: 0.0017936]
	Learning Rate: 0.00179365
	LOSS [training: 0.06102277236195887 | validation: 0.07710539654900253]
	TIME [epoch: 8.79 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049844786199749434		[learning rate: 0.0017915]
		[batch 20/20] avg loss: 0.11025740944981392		[learning rate: 0.0017893]
	Learning Rate: 0.0017893
	LOSS [training: 0.08005109782478168 | validation: 0.10265907605818442]
	TIME [epoch: 8.78 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05387195684436824		[learning rate: 0.0017871]
		[batch 20/20] avg loss: 0.06917588731529792		[learning rate: 0.001785]
	Learning Rate: 0.00178497
	LOSS [training: 0.06152392207983307 | validation: 0.10605183870053278]
	TIME [epoch: 8.8 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05228819104229911		[learning rate: 0.0017828]
		[batch 20/20] avg loss: 0.062155481158149214		[learning rate: 0.0017807]
	Learning Rate: 0.00178065
	LOSS [training: 0.057221836100224174 | validation: 0.06740309842771323]
	TIME [epoch: 8.82 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0491982404314331		[learning rate: 0.0017785]
		[batch 20/20] avg loss: 0.05081874128942736		[learning rate: 0.0017763]
	Learning Rate: 0.00177634
	LOSS [training: 0.05000849086043022 | validation: 0.044613345927458484]
	TIME [epoch: 8.81 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06270825332808055		[learning rate: 0.0017742]
		[batch 20/20] avg loss: 0.06953202341076054		[learning rate: 0.001772]
	Learning Rate: 0.00177204
	LOSS [training: 0.06612013836942052 | validation: 0.07957029907792439]
	TIME [epoch: 8.8 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058487367658414		[learning rate: 0.0017699]
		[batch 20/20] avg loss: 0.06107714941332122		[learning rate: 0.0017678]
	Learning Rate: 0.00176775
	LOSS [training: 0.05978225853586762 | validation: 0.03534722718378802]
	TIME [epoch: 8.79 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04505212856743477		[learning rate: 0.0017656]
		[batch 20/20] avg loss: 0.05353567759784185		[learning rate: 0.0017635]
	Learning Rate: 0.00176347
	LOSS [training: 0.049293903082638306 | validation: 0.05154443112290122]
	TIME [epoch: 8.82 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09209602624707339		[learning rate: 0.0017613]
		[batch 20/20] avg loss: 0.057866983108207204		[learning rate: 0.0017592]
	Learning Rate: 0.0017592
	LOSS [training: 0.07498150467764028 | validation: 0.061716485016103086]
	TIME [epoch: 8.79 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04715755949939126		[learning rate: 0.0017571]
		[batch 20/20] avg loss: 0.06179362661447474		[learning rate: 0.0017549]
	Learning Rate: 0.00175494
	LOSS [training: 0.054475593056933 | validation: 0.03561448879621585]
	TIME [epoch: 8.79 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04559480766812467		[learning rate: 0.0017528]
		[batch 20/20] avg loss: 0.06311641049618742		[learning rate: 0.0017507]
	Learning Rate: 0.0017507
	LOSS [training: 0.05435560908215605 | validation: 0.01821889448191333]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_819.pth
	Model improved!!!
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06408169110306322		[learning rate: 0.0017486]
		[batch 20/20] avg loss: 0.07481434890746205		[learning rate: 0.0017465]
	Learning Rate: 0.00174646
	LOSS [training: 0.06944802000526264 | validation: 0.0674635537338745]
	TIME [epoch: 8.81 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039199923653293736		[learning rate: 0.0017443]
		[batch 20/20] avg loss: 0.0796344964655398		[learning rate: 0.0017422]
	Learning Rate: 0.00174223
	LOSS [training: 0.05941721005941676 | validation: 0.05675931365344216]
	TIME [epoch: 8.81 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06003685317154739		[learning rate: 0.0017401]
		[batch 20/20] avg loss: 0.062276203012148666		[learning rate: 0.001738]
	Learning Rate: 0.00173801
	LOSS [training: 0.06115652809184803 | validation: 0.09007708416581223]
	TIME [epoch: 8.78 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08075910751630794		[learning rate: 0.0017359]
		[batch 20/20] avg loss: 0.06385562367983375		[learning rate: 0.0017338]
	Learning Rate: 0.0017338
	LOSS [training: 0.07230736559807086 | validation: 0.07385700831499872]
	TIME [epoch: 8.79 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04531206964881718		[learning rate: 0.0017317]
		[batch 20/20] avg loss: 0.04486912596948382		[learning rate: 0.0017296]
	Learning Rate: 0.00172961
	LOSS [training: 0.04509059780915049 | validation: 0.042550395500126556]
	TIME [epoch: 8.78 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07357703121131597		[learning rate: 0.0017275]
		[batch 20/20] avg loss: 0.05306322596905586		[learning rate: 0.0017254]
	Learning Rate: 0.00172542
	LOSS [training: 0.0633201285901859 | validation: 0.05064553963267859]
	TIME [epoch: 8.82 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07478516230685402		[learning rate: 0.0017233]
		[batch 20/20] avg loss: 0.04339230121982418		[learning rate: 0.0017212]
	Learning Rate: 0.00172124
	LOSS [training: 0.0590887317633391 | validation: 0.05389422532695211]
	TIME [epoch: 8.81 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0796278722279651		[learning rate: 0.0017192]
		[batch 20/20] avg loss: 0.06103772025383447		[learning rate: 0.0017171]
	Learning Rate: 0.00171708
	LOSS [training: 0.07033279624089978 | validation: 0.04580300730223921]
	TIME [epoch: 8.79 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060331416771405834		[learning rate: 0.001715]
		[batch 20/20] avg loss: 0.0486358309783997		[learning rate: 0.0017129]
	Learning Rate: 0.00171292
	LOSS [training: 0.05448362387490278 | validation: 0.04405095396553917]
	TIME [epoch: 8.8 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06456055810018958		[learning rate: 0.0017108]
		[batch 20/20] avg loss: 0.06921728516232692		[learning rate: 0.0017088]
	Learning Rate: 0.00170877
	LOSS [training: 0.06688892163125826 | validation: 0.02955338495634635]
	TIME [epoch: 8.8 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055944916133728796		[learning rate: 0.0017067]
		[batch 20/20] avg loss: 0.04350598682399418		[learning rate: 0.0017046]
	Learning Rate: 0.00170464
	LOSS [training: 0.04972545147886149 | validation: 0.04089442749377764]
	TIME [epoch: 8.79 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07173357442997388		[learning rate: 0.0017026]
		[batch 20/20] avg loss: 0.0976059720086221		[learning rate: 0.0017005]
	Learning Rate: 0.00170051
	LOSS [training: 0.08466977321929799 | validation: 0.04247459592483286]
	TIME [epoch: 8.78 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046674006339705004		[learning rate: 0.0016984]
		[batch 20/20] avg loss: 0.059309688007259144		[learning rate: 0.0016964]
	Learning Rate: 0.00169639
	LOSS [training: 0.05299184717348208 | validation: 0.1379620251267748]
	TIME [epoch: 8.79 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08053937286499961		[learning rate: 0.0016943]
		[batch 20/20] avg loss: 0.04672269448435774		[learning rate: 0.0016923]
	Learning Rate: 0.00169229
	LOSS [training: 0.06363103367467868 | validation: 0.06518942488294743]
	TIME [epoch: 8.79 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05705204021083941		[learning rate: 0.0016902]
		[batch 20/20] avg loss: 0.05715126816622955		[learning rate: 0.0016882]
	Learning Rate: 0.00168819
	LOSS [training: 0.05710165418853448 | validation: 0.04434244108096912]
	TIME [epoch: 8.81 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04857270583547239		[learning rate: 0.0016861]
		[batch 20/20] avg loss: 0.0436504739159911		[learning rate: 0.0016841]
	Learning Rate: 0.0016841
	LOSS [training: 0.046111589875731746 | validation: 0.05016566950756669]
	TIME [epoch: 8.78 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043795122743437186		[learning rate: 0.0016821]
		[batch 20/20] avg loss: 0.044805737330710846		[learning rate: 0.00168]
	Learning Rate: 0.00168003
	LOSS [training: 0.044300430037074026 | validation: 0.03196728644335333]
	TIME [epoch: 8.79 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06544742494376764		[learning rate: 0.001678]
		[batch 20/20] avg loss: 0.06655533577287936		[learning rate: 0.001676]
	Learning Rate: 0.00167596
	LOSS [training: 0.06600138035832351 | validation: 0.06630162344459913]
	TIME [epoch: 8.78 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08924637448434795		[learning rate: 0.0016739]
		[batch 20/20] avg loss: 0.055460316686193147		[learning rate: 0.0016719]
	Learning Rate: 0.0016719
	LOSS [training: 0.07235334558527055 | validation: 0.08604943910928553]
	TIME [epoch: 8.79 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058860960810785776		[learning rate: 0.0016699]
		[batch 20/20] avg loss: 0.06828529486717694		[learning rate: 0.0016679]
	Learning Rate: 0.00166785
	LOSS [training: 0.06357312783898136 | validation: 0.12767208233606]
	TIME [epoch: 8.81 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05706324892296048		[learning rate: 0.0016658]
		[batch 20/20] avg loss: 0.09516040390527487		[learning rate: 0.0016638]
	Learning Rate: 0.00166382
	LOSS [training: 0.07611182641411768 | validation: 0.11769970813551409]
	TIME [epoch: 8.81 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07269149716653925		[learning rate: 0.0016618]
		[batch 20/20] avg loss: 0.052277157988553126		[learning rate: 0.0016598]
	Learning Rate: 0.00165979
	LOSS [training: 0.06248432757754617 | validation: 0.07345207506401759]
	TIME [epoch: 8.8 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07934297895869218		[learning rate: 0.0016578]
		[batch 20/20] avg loss: 0.04443160176537181		[learning rate: 0.0016558]
	Learning Rate: 0.00165577
	LOSS [training: 0.061887290362031985 | validation: 0.03576292169021956]
	TIME [epoch: 8.78 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04789633328408492		[learning rate: 0.0016538]
		[batch 20/20] avg loss: 0.06483539795140494		[learning rate: 0.0016518]
	Learning Rate: 0.00165176
	LOSS [training: 0.05636586561774495 | validation: 0.10410193257297039]
	TIME [epoch: 8.82 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047237201946612246		[learning rate: 0.0016498]
		[batch 20/20] avg loss: 0.0511523518296831		[learning rate: 0.0016478]
	Learning Rate: 0.00164776
	LOSS [training: 0.04919477688814767 | validation: 0.033385831115983186]
	TIME [epoch: 8.78 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04922834318493768		[learning rate: 0.0016458]
		[batch 20/20] avg loss: 0.03646099050910663		[learning rate: 0.0016438]
	Learning Rate: 0.00164377
	LOSS [training: 0.04284466684702215 | validation: 0.0464871961946647]
	TIME [epoch: 8.79 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05654590615473891		[learning rate: 0.0016418]
		[batch 20/20] avg loss: 0.06929322651502845		[learning rate: 0.0016398]
	Learning Rate: 0.00163979
	LOSS [training: 0.06291956633488367 | validation: 0.07888317978143397]
	TIME [epoch: 8.78 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04756746727371212		[learning rate: 0.0016378]
		[batch 20/20] avg loss: 0.043815009639555313		[learning rate: 0.0016358]
	Learning Rate: 0.00163583
	LOSS [training: 0.04569123845663372 | validation: 0.07436370397512142]
	TIME [epoch: 8.81 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07059009167926637		[learning rate: 0.0016338]
		[batch 20/20] avg loss: 0.03992096405422223		[learning rate: 0.0016319]
	Learning Rate: 0.00163186
	LOSS [training: 0.0552555278667443 | validation: 0.051455407688564]
	TIME [epoch: 8.79 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05790512920706453		[learning rate: 0.0016299]
		[batch 20/20] avg loss: 0.07619551622922108		[learning rate: 0.0016279]
	Learning Rate: 0.00162791
	LOSS [training: 0.0670503227181428 | validation: 0.07831455401626662]
	TIME [epoch: 8.78 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04571484413891375		[learning rate: 0.0016259]
		[batch 20/20] avg loss: 0.050125056339844286		[learning rate: 0.001624]
	Learning Rate: 0.00162397
	LOSS [training: 0.04791995023937902 | validation: 0.0441634671509294]
	TIME [epoch: 8.79 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043326908428778		[learning rate: 0.001622]
		[batch 20/20] avg loss: 0.05714831670945608		[learning rate: 0.00162]
	Learning Rate: 0.00162004
	LOSS [training: 0.05023761256911704 | validation: 0.04874787053857341]
	TIME [epoch: 8.78 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043048265915720314		[learning rate: 0.0016181]
		[batch 20/20] avg loss: 0.050743263394945816		[learning rate: 0.0016161]
	Learning Rate: 0.00161612
	LOSS [training: 0.04689576465533306 | validation: 0.04751364744524645]
	TIME [epoch: 8.8 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029695870789698004		[learning rate: 0.0016142]
		[batch 20/20] avg loss: 0.04162304060060536		[learning rate: 0.0016122]
	Learning Rate: 0.00161221
	LOSS [training: 0.035659455695151686 | validation: 0.07395048541663829]
	TIME [epoch: 8.79 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05191657449643626		[learning rate: 0.0016103]
		[batch 20/20] avg loss: 0.03503567807849835		[learning rate: 0.0016083]
	Learning Rate: 0.0016083
	LOSS [training: 0.043476126287467304 | validation: 0.04210420684681531]
	TIME [epoch: 8.8 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053301483148070226		[learning rate: 0.0016064]
		[batch 20/20] avg loss: 0.0506474016658867		[learning rate: 0.0016044]
	Learning Rate: 0.00160441
	LOSS [training: 0.051974442406978474 | validation: 0.12656990903961904]
	TIME [epoch: 8.8 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09562480411371994		[learning rate: 0.0016025]
		[batch 20/20] avg loss: 0.08858974048413304		[learning rate: 0.0016005]
	Learning Rate: 0.00160053
	LOSS [training: 0.09210727229892647 | validation: 0.06420137293586095]
	TIME [epoch: 8.8 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054694936888406064		[learning rate: 0.0015986]
		[batch 20/20] avg loss: 0.07689526252877076		[learning rate: 0.0015967]
	Learning Rate: 0.00159665
	LOSS [training: 0.0657950997085884 | validation: 0.059988478902003595]
	TIME [epoch: 8.79 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05335978095753384		[learning rate: 0.0015947]
		[batch 20/20] avg loss: 0.044233029556288064		[learning rate: 0.0015928]
	Learning Rate: 0.00159279
	LOSS [training: 0.04879640525691095 | validation: 0.03858305203818534]
	TIME [epoch: 8.79 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04318191517886598		[learning rate: 0.0015909]
		[batch 20/20] avg loss: 0.03719214238055211		[learning rate: 0.0015889]
	Learning Rate: 0.00158893
	LOSS [training: 0.04018702877970905 | validation: 0.04017031652014389]
	TIME [epoch: 8.79 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04493028668018438		[learning rate: 0.001587]
		[batch 20/20] avg loss: 0.049280261469218614		[learning rate: 0.0015851]
	Learning Rate: 0.00158509
	LOSS [training: 0.04710527407470149 | validation: 0.06711664588774247]
	TIME [epoch: 8.81 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057364810986964485		[learning rate: 0.0015832]
		[batch 20/20] avg loss: 0.06163472634498206		[learning rate: 0.0015812]
	Learning Rate: 0.00158125
	LOSS [training: 0.05949976866597327 | validation: 0.05058124445901705]
	TIME [epoch: 8.79 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04961229551423127		[learning rate: 0.0015793]
		[batch 20/20] avg loss: 0.056286482101945576		[learning rate: 0.0015774]
	Learning Rate: 0.00157742
	LOSS [training: 0.05294938880808843 | validation: 0.03521031733589999]
	TIME [epoch: 8.79 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03870027358839061		[learning rate: 0.0015755]
		[batch 20/20] avg loss: 0.04629315930960952		[learning rate: 0.0015736]
	Learning Rate: 0.0015736
	LOSS [training: 0.04249671644900006 | validation: 0.04205075302693097]
	TIME [epoch: 8.79 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04168817345441482		[learning rate: 0.0015717]
		[batch 20/20] avg loss: 0.07490069283768014		[learning rate: 0.0015698]
	Learning Rate: 0.00156979
	LOSS [training: 0.058294433146047465 | validation: 0.040204898512990805]
	TIME [epoch: 8.79 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04816854510166936		[learning rate: 0.0015679]
		[batch 20/20] avg loss: 0.05182987593466666		[learning rate: 0.001566]
	Learning Rate: 0.00156599
	LOSS [training: 0.049999210518168004 | validation: 0.11143884937300681]
	TIME [epoch: 8.79 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07588713753005742		[learning rate: 0.0015641]
		[batch 20/20] avg loss: 0.08314839376007092		[learning rate: 0.0015622]
	Learning Rate: 0.0015622
	LOSS [training: 0.07951776564506417 | validation: 0.07246097534301163]
	TIME [epoch: 8.78 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04211861764174586		[learning rate: 0.0015603]
		[batch 20/20] avg loss: 0.032170697518830074		[learning rate: 0.0015584]
	Learning Rate: 0.00155842
	LOSS [training: 0.03714465758028797 | validation: 0.0336788567312253]
	TIME [epoch: 8.79 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06670872406592941		[learning rate: 0.0015565]
		[batch 20/20] avg loss: 0.03811157421010582		[learning rate: 0.0015546]
	Learning Rate: 0.00155465
	LOSS [training: 0.052410149138017614 | validation: 0.03270305709832342]
	TIME [epoch: 8.79 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041148506671570134		[learning rate: 0.0015528]
		[batch 20/20] avg loss: 0.04338278036311196		[learning rate: 0.0015509]
	Learning Rate: 0.00155088
	LOSS [training: 0.04226564351734104 | validation: 0.05357198500786471]
	TIME [epoch: 8.8 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04410142798209611		[learning rate: 0.001549]
		[batch 20/20] avg loss: 0.07554500348918132		[learning rate: 0.0015471]
	Learning Rate: 0.00154713
	LOSS [training: 0.059823215735638716 | validation: 0.05090558484480519]
	TIME [epoch: 8.77 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06274057044342476		[learning rate: 0.0015453]
		[batch 20/20] avg loss: 0.030960378326781578		[learning rate: 0.0015434]
	Learning Rate: 0.00154338
	LOSS [training: 0.04685047438510317 | validation: 0.04239197569369568]
	TIME [epoch: 8.77 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06181045821776885		[learning rate: 0.0015415]
		[batch 20/20] avg loss: 0.048310670941932775		[learning rate: 0.0015396]
	Learning Rate: 0.00153965
	LOSS [training: 0.0550605645798508 | validation: 0.042647404750215126]
	TIME [epoch: 8.78 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05558381133148767		[learning rate: 0.0015378]
		[batch 20/20] avg loss: 0.044736167305389485		[learning rate: 0.0015359]
	Learning Rate: 0.00153592
	LOSS [training: 0.05015998931843858 | validation: 0.0606325288783228]
	TIME [epoch: 8.81 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05333171529544599		[learning rate: 0.0015341]
		[batch 20/20] avg loss: 0.054228202535269034		[learning rate: 0.0015322]
	Learning Rate: 0.0015322
	LOSS [training: 0.053779958915357515 | validation: 0.027475013946360437]
	TIME [epoch: 8.8 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04857177149550922		[learning rate: 0.0015303]
		[batch 20/20] avg loss: 0.05641494409250901		[learning rate: 0.0015285]
	Learning Rate: 0.00152849
	LOSS [training: 0.05249335779400911 | validation: 0.06616863374014831]
	TIME [epoch: 8.78 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07136266114738025		[learning rate: 0.0015266]
		[batch 20/20] avg loss: 0.04539351200723017		[learning rate: 0.0015248]
	Learning Rate: 0.00152479
	LOSS [training: 0.05837808657730522 | validation: 0.05002924392181941]
	TIME [epoch: 8.88 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045230316812293246		[learning rate: 0.0015229]
		[batch 20/20] avg loss: 0.03355579196073631		[learning rate: 0.0015211]
	Learning Rate: 0.0015211
	LOSS [training: 0.03939305438651478 | validation: 0.05639824300886714]
	TIME [epoch: 8.82 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07789921349335541		[learning rate: 0.0015193]
		[batch 20/20] avg loss: 0.041034156729089485		[learning rate: 0.0015174]
	Learning Rate: 0.00151742
	LOSS [training: 0.05946668511122244 | validation: 0.038416130261023036]
	TIME [epoch: 8.79 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059996843422362424		[learning rate: 0.0015156]
		[batch 20/20] avg loss: 0.046211895501867606		[learning rate: 0.0015137]
	Learning Rate: 0.00151374
	LOSS [training: 0.05310436946211501 | validation: 0.028420957193265654]
	TIME [epoch: 8.79 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04268557848769569		[learning rate: 0.0015119]
		[batch 20/20] avg loss: 0.05076653936768707		[learning rate: 0.0015101]
	Learning Rate: 0.00151008
	LOSS [training: 0.04672605892769138 | validation: 0.10928354423734668]
	TIME [epoch: 8.79 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05871462012935772		[learning rate: 0.0015083]
		[batch 20/20] avg loss: 0.04553383932882517		[learning rate: 0.0015064]
	Learning Rate: 0.00150642
	LOSS [training: 0.05212422972909144 | validation: 0.04384641841614976]
	TIME [epoch: 8.81 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043114714877988444		[learning rate: 0.0015046]
		[batch 20/20] avg loss: 0.051598877172586224		[learning rate: 0.0015028]
	Learning Rate: 0.00150278
	LOSS [training: 0.04735679602528733 | validation: 0.05235081656079219]
	TIME [epoch: 8.81 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054398690413544705		[learning rate: 0.001501]
		[batch 20/20] avg loss: 0.05179543019685008		[learning rate: 0.0014991]
	Learning Rate: 0.00149914
	LOSS [training: 0.05309706030519738 | validation: 0.04928796495622371]
	TIME [epoch: 8.78 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036769050970330695		[learning rate: 0.0014973]
		[batch 20/20] avg loss: 0.05189462841030744		[learning rate: 0.0014955]
	Learning Rate: 0.00149551
	LOSS [training: 0.04433183969031908 | validation: 0.06764408526629179]
	TIME [epoch: 8.79 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04725309306667408		[learning rate: 0.0014937]
		[batch 20/20] avg loss: 0.04765189321267682		[learning rate: 0.0014919]
	Learning Rate: 0.00149189
	LOSS [training: 0.04745249313967545 | validation: 0.02874383686554104]
	TIME [epoch: 8.78 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034231085445149315		[learning rate: 0.0014901]
		[batch 20/20] avg loss: 0.03774379660160692		[learning rate: 0.0014883]
	Learning Rate: 0.00148828
	LOSS [training: 0.035987441023378114 | validation: 0.06294336543999855]
	TIME [epoch: 8.82 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051908948047933424		[learning rate: 0.0014865]
		[batch 20/20] avg loss: 0.050813457439952525		[learning rate: 0.0014847]
	Learning Rate: 0.00148468
	LOSS [training: 0.05136120274394298 | validation: 0.03524145712028329]
	TIME [epoch: 8.8 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05368070028502527		[learning rate: 0.0014829]
		[batch 20/20] avg loss: 0.027033353391384733		[learning rate: 0.0014811]
	Learning Rate: 0.00148108
	LOSS [training: 0.04035702683820501 | validation: 0.037957627055099794]
	TIME [epoch: 8.8 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038217979594687826		[learning rate: 0.0014793]
		[batch 20/20] avg loss: 0.06708031209177254		[learning rate: 0.0014775]
	Learning Rate: 0.0014775
	LOSS [training: 0.052649145843230175 | validation: 0.08957906217594604]
	TIME [epoch: 8.8 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05966804260031464		[learning rate: 0.0014757]
		[batch 20/20] avg loss: 0.0652039746537476		[learning rate: 0.0014739]
	Learning Rate: 0.00147392
	LOSS [training: 0.06243600862703115 | validation: 0.05126155188219271]
	TIME [epoch: 8.81 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05348443331994242		[learning rate: 0.0014721]
		[batch 20/20] avg loss: 0.06405201221626343		[learning rate: 0.0014704]
	Learning Rate: 0.00147035
	LOSS [training: 0.058768222768102917 | validation: 0.05490796578622622]
	TIME [epoch: 8.8 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07111187025692496		[learning rate: 0.0014686]
		[batch 20/20] avg loss: 0.03532698247475356		[learning rate: 0.0014668]
	Learning Rate: 0.00146679
	LOSS [training: 0.05321942636583925 | validation: 0.04181211011255519]
	TIME [epoch: 8.79 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02977302782610407		[learning rate: 0.001465]
		[batch 20/20] avg loss: 0.07126226497084402		[learning rate: 0.0014632]
	Learning Rate: 0.00146324
	LOSS [training: 0.05051764639847404 | validation: 0.10807422106635661]
	TIME [epoch: 8.81 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06709404344363104		[learning rate: 0.0014615]
		[batch 20/20] avg loss: 0.0704796542059365		[learning rate: 0.0014597]
	Learning Rate: 0.0014597
	LOSS [training: 0.06878684882478377 | validation: 0.07697109828443124]
	TIME [epoch: 8.82 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04451600687013073		[learning rate: 0.0014579]
		[batch 20/20] avg loss: 0.07847186224809952		[learning rate: 0.0014562]
	Learning Rate: 0.00145616
	LOSS [training: 0.061493934559115115 | validation: 0.09295711180370692]
	TIME [epoch: 8.83 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11645912215930534		[learning rate: 0.0014544]
		[batch 20/20] avg loss: 0.0678463739801103		[learning rate: 0.0014526]
	Learning Rate: 0.00145264
	LOSS [training: 0.09215274806970783 | validation: 0.0408332913554279]
	TIME [epoch: 8.81 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05019947397973675		[learning rate: 0.0014509]
		[batch 20/20] avg loss: 0.05015973309392694		[learning rate: 0.0014491]
	Learning Rate: 0.00144912
	LOSS [training: 0.05017960353683184 | validation: 0.033460915911850846]
	TIME [epoch: 8.79 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04201474773671201		[learning rate: 0.0014474]
		[batch 20/20] avg loss: 0.03202850761333276		[learning rate: 0.0014456]
	Learning Rate: 0.00144562
	LOSS [training: 0.03702162767502239 | validation: 0.04429756464118934]
	TIME [epoch: 8.8 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02950190070255685		[learning rate: 0.0014439]
		[batch 20/20] avg loss: 0.046129915185192626		[learning rate: 0.0014421]
	Learning Rate: 0.00144212
	LOSS [training: 0.037815907943874745 | validation: 0.11559888606425216]
	TIME [epoch: 8.81 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08065667891353738		[learning rate: 0.0014404]
		[batch 20/20] avg loss: 0.056189840877747865		[learning rate: 0.0014386]
	Learning Rate: 0.00143862
	LOSS [training: 0.06842325989564263 | validation: 0.039663143396599335]
	TIME [epoch: 8.8 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036691442301594296		[learning rate: 0.0014369]
		[batch 20/20] avg loss: 0.06420130690698456		[learning rate: 0.0014351]
	Learning Rate: 0.00143514
	LOSS [training: 0.050446374604289426 | validation: 0.07386535556334664]
	TIME [epoch: 8.8 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061699396622219724		[learning rate: 0.0014334]
		[batch 20/20] avg loss: 0.047256717067644546		[learning rate: 0.0014317]
	Learning Rate: 0.00143167
	LOSS [training: 0.054478056844932135 | validation: 0.041895616919763394]
	TIME [epoch: 8.78 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04304153888226524		[learning rate: 0.0014299]
		[batch 20/20] avg loss: 0.06202198741028443		[learning rate: 0.0014282]
	Learning Rate: 0.0014282
	LOSS [training: 0.052531763146274835 | validation: 0.053452718028836346]
	TIME [epoch: 8.82 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050203764113356183		[learning rate: 0.0014265]
		[batch 20/20] avg loss: 0.036127141810492217		[learning rate: 0.0014247]
	Learning Rate: 0.00142474
	LOSS [training: 0.043165452961924214 | validation: 0.04197347236545077]
	TIME [epoch: 8.79 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04277750243933269		[learning rate: 0.001423]
		[batch 20/20] avg loss: 0.04859824175535684		[learning rate: 0.0014213]
	Learning Rate: 0.00142129
	LOSS [training: 0.04568787209734476 | validation: 0.05198439662648022]
	TIME [epoch: 8.79 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06174582078342591		[learning rate: 0.0014196]
		[batch 20/20] avg loss: 0.06274272238259601		[learning rate: 0.0014179]
	Learning Rate: 0.00141785
	LOSS [training: 0.062244271583010956 | validation: 0.08332654886718119]
	TIME [epoch: 8.79 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07162658620609294		[learning rate: 0.0014161]
		[batch 20/20] avg loss: 0.0403807782296955		[learning rate: 0.0014144]
	Learning Rate: 0.00141442
	LOSS [training: 0.05600368221789422 | validation: 0.04576032780938847]
	TIME [epoch: 8.81 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0720630919542371		[learning rate: 0.0014127]
		[batch 20/20] avg loss: 0.03849224358446398		[learning rate: 0.001411]
	Learning Rate: 0.001411
	LOSS [training: 0.05527766776935054 | validation: 0.04084310859909338]
	TIME [epoch: 8.8 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04805868819187572		[learning rate: 0.0014093]
		[batch 20/20] avg loss: 0.06009139247458213		[learning rate: 0.0014076]
	Learning Rate: 0.00140758
	LOSS [training: 0.05407504033322892 | validation: 0.05383432885501178]
	TIME [epoch: 8.79 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05294382289084565		[learning rate: 0.0014059]
		[batch 20/20] avg loss: 0.06519949411031206		[learning rate: 0.0014042]
	Learning Rate: 0.00140417
	LOSS [training: 0.05907165850057886 | validation: 0.03997806782154874]
	TIME [epoch: 8.8 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0567059804964425		[learning rate: 0.0014025]
		[batch 20/20] avg loss: 0.05903403979033002		[learning rate: 0.0014008]
	Learning Rate: 0.00140078
	LOSS [training: 0.05787001014338624 | validation: 0.06041154173954245]
	TIME [epoch: 8.8 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058305801594749095		[learning rate: 0.0013991]
		[batch 20/20] avg loss: 0.03910142876321336		[learning rate: 0.0013974]
	Learning Rate: 0.00139738
	LOSS [training: 0.04870361517898124 | validation: 0.059682414753627046]
	TIME [epoch: 8.8 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0386552094744395		[learning rate: 0.0013957]
		[batch 20/20] avg loss: 0.06413320426067062		[learning rate: 0.001394]
	Learning Rate: 0.001394
	LOSS [training: 0.05139420686755507 | validation: 0.09232977976016127]
	TIME [epoch: 8.79 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07278858908279279		[learning rate: 0.0013923]
		[batch 20/20] avg loss: 0.0651540998586814		[learning rate: 0.0013906]
	Learning Rate: 0.00139063
	LOSS [training: 0.06897134447073708 | validation: 0.035739225363309424]
	TIME [epoch: 8.79 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05240212106377211		[learning rate: 0.0013889]
		[batch 20/20] avg loss: 0.04371958617576017		[learning rate: 0.0013873]
	Learning Rate: 0.00138726
	LOSS [training: 0.048060853619766135 | validation: 0.03278018485952958]
	TIME [epoch: 8.79 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0392201073035154		[learning rate: 0.0013856]
		[batch 20/20] avg loss: 0.04094272473114171		[learning rate: 0.0013839]
	Learning Rate: 0.0013839
	LOSS [training: 0.04008141601732855 | validation: 0.031637448536499795]
	TIME [epoch: 8.81 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056859601327265676		[learning rate: 0.0013822]
		[batch 20/20] avg loss: 0.047529768227210854		[learning rate: 0.0013806]
	Learning Rate: 0.00138055
	LOSS [training: 0.052194684777238265 | validation: 0.07484203161047659]
	TIME [epoch: 8.79 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05476510003417316		[learning rate: 0.0013789]
		[batch 20/20] avg loss: 0.049652262844108994		[learning rate: 0.0013772]
	Learning Rate: 0.00137721
	LOSS [training: 0.05220868143914108 | validation: 0.028452932554355576]
	TIME [epoch: 8.79 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04775820390672712		[learning rate: 0.0013755]
		[batch 20/20] avg loss: 0.05144504907472813		[learning rate: 0.0013739]
	Learning Rate: 0.00137388
	LOSS [training: 0.04960162649072763 | validation: 0.0737305150548282]
	TIME [epoch: 8.78 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03789504819124563		[learning rate: 0.0013722]
		[batch 20/20] avg loss: 0.0614458487518428		[learning rate: 0.0013705]
	Learning Rate: 0.00137055
	LOSS [training: 0.049670448471544226 | validation: 0.08423497044321371]
	TIME [epoch: 8.81 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09499942759629913		[learning rate: 0.0013689]
		[batch 20/20] avg loss: 0.03383496427228751		[learning rate: 0.0013672]
	Learning Rate: 0.00136723
	LOSS [training: 0.06441719593429332 | validation: 0.03334287962986933]
	TIME [epoch: 8.8 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039434069691696806		[learning rate: 0.0013656]
		[batch 20/20] avg loss: 0.04566861138406823		[learning rate: 0.0013639]
	Learning Rate: 0.00136392
	LOSS [training: 0.042551340537882516 | validation: 0.0329680591338235]
	TIME [epoch: 8.79 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06113506594776302		[learning rate: 0.0013623]
		[batch 20/20] avg loss: 0.04670458867774382		[learning rate: 0.0013606]
	Learning Rate: 0.00136062
	LOSS [training: 0.05391982731275341 | validation: 0.028060675910895356]
	TIME [epoch: 8.79 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03943527970655629		[learning rate: 0.001359]
		[batch 20/20] avg loss: 0.05214360968105808		[learning rate: 0.0013573]
	Learning Rate: 0.00135733
	LOSS [training: 0.04578944469380718 | validation: 0.09279475666324942]
	TIME [epoch: 8.79 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05529493520268458		[learning rate: 0.0013557]
		[batch 20/20] avg loss: 0.04956545017573008		[learning rate: 0.001354]
	Learning Rate: 0.00135404
	LOSS [training: 0.05243019268920733 | validation: 0.05634275642179923]
	TIME [epoch: 8.8 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04560273076590055		[learning rate: 0.0013524]
		[batch 20/20] avg loss: 0.04765012075422946		[learning rate: 0.0013508]
	Learning Rate: 0.00135076
	LOSS [training: 0.04662642576006501 | validation: 0.05076051950855092]
	TIME [epoch: 8.78 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05013009623946585		[learning rate: 0.0013491]
		[batch 20/20] avg loss: 0.034449827341161636		[learning rate: 0.0013475]
	Learning Rate: 0.00134749
	LOSS [training: 0.04228996179031374 | validation: 0.04144348944186968]
	TIME [epoch: 8.79 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03831637890246621		[learning rate: 0.0013459]
		[batch 20/20] avg loss: 0.04143553462257621		[learning rate: 0.0013442]
	Learning Rate: 0.00134423
	LOSS [training: 0.0398759567625212 | validation: 0.044687260428611306]
	TIME [epoch: 8.79 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05400622315711819		[learning rate: 0.0013426]
		[batch 20/20] avg loss: 0.04957411554243492		[learning rate: 0.001341]
	Learning Rate: 0.00134098
	LOSS [training: 0.05179016934977656 | validation: 0.028206940257014344]
	TIME [epoch: 8.8 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05968768048418971		[learning rate: 0.0013394]
		[batch 20/20] avg loss: 0.04192310583465712		[learning rate: 0.0013377]
	Learning Rate: 0.00133773
	LOSS [training: 0.05080539315942341 | validation: 0.040594508158772184]
	TIME [epoch: 8.79 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04959288451142654		[learning rate: 0.0013361]
		[batch 20/20] avg loss: 0.050058908165282634		[learning rate: 0.0013345]
	Learning Rate: 0.00133449
	LOSS [training: 0.04982589633835459 | validation: 0.03822116898382466]
	TIME [epoch: 8.78 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048229242631389384		[learning rate: 0.0013329]
		[batch 20/20] avg loss: 0.05260909439027689		[learning rate: 0.0013313]
	Learning Rate: 0.00133126
	LOSS [training: 0.05041916851083313 | validation: 0.03011284743581447]
	TIME [epoch: 8.79 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04841999325504459		[learning rate: 0.0013296]
		[batch 20/20] avg loss: 0.052174658934237096		[learning rate: 0.001328]
	Learning Rate: 0.00132804
	LOSS [training: 0.05029732609464084 | validation: 0.05237410131629194]
	TIME [epoch: 8.8 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058806392277863676		[learning rate: 0.0013264]
		[batch 20/20] avg loss: 0.048228860115880214		[learning rate: 0.0013248]
	Learning Rate: 0.00132482
	LOSS [training: 0.05351762619687195 | validation: 0.02886005438164546]
	TIME [epoch: 8.8 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03934430945634175		[learning rate: 0.0013232]
		[batch 20/20] avg loss: 0.03289427054948113		[learning rate: 0.0013216]
	Learning Rate: 0.00132162
	LOSS [training: 0.03611929000291143 | validation: 0.016493573236775697]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_935.pth
	Model improved!!!
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051845139363238035		[learning rate: 0.00132]
		[batch 20/20] avg loss: 0.053752803316458056		[learning rate: 0.0013184]
	Learning Rate: 0.00131842
	LOSS [training: 0.052798971339848046 | validation: 0.06615277181170803]
	TIME [epoch: 8.81 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05054325367358416		[learning rate: 0.0013168]
		[batch 20/20] avg loss: 0.04075162318183586		[learning rate: 0.0013152]
	Learning Rate: 0.00131522
	LOSS [training: 0.04564743842771 | validation: 0.0785551607045791]
	TIME [epoch: 8.82 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048707566871880735		[learning rate: 0.0013136]
		[batch 20/20] avg loss: 0.03501388771030958		[learning rate: 0.001312]
	Learning Rate: 0.00131204
	LOSS [training: 0.041860727291095155 | validation: 0.026029767831872646]
	TIME [epoch: 8.81 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04262244541789474		[learning rate: 0.0013105]
		[batch 20/20] avg loss: 0.05966745608520928		[learning rate: 0.0013089]
	Learning Rate: 0.00130886
	LOSS [training: 0.05114495075155201 | validation: 0.030324509889084517]
	TIME [epoch: 8.8 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03859752188936919		[learning rate: 0.0013073]
		[batch 20/20] avg loss: 0.03369843811025054		[learning rate: 0.0013057]
	Learning Rate: 0.0013057
	LOSS [training: 0.036147979999809864 | validation: 0.04168264755449175]
	TIME [epoch: 8.81 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027331694532963385		[learning rate: 0.0013041]
		[batch 20/20] avg loss: 0.05330567587659316		[learning rate: 0.0013025]
	Learning Rate: 0.00130254
	LOSS [training: 0.04031868520477828 | validation: 0.10076007442323776]
	TIME [epoch: 8.79 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043591157697367766		[learning rate: 0.001301]
		[batch 20/20] avg loss: 0.04806836224892541		[learning rate: 0.0012994]
	Learning Rate: 0.00129938
	LOSS [training: 0.04582975997314658 | validation: 0.09418776224457073]
	TIME [epoch: 8.83 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03981424196628837		[learning rate: 0.0012978]
		[batch 20/20] avg loss: 0.029765873864148612		[learning rate: 0.0012962]
	Learning Rate: 0.00129624
	LOSS [training: 0.03479005791521849 | validation: 0.07198404918771886]
	TIME [epoch: 8.8 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051378874700430524		[learning rate: 0.0012947]
		[batch 20/20] avg loss: 0.04602622843645035		[learning rate: 0.0012931]
	Learning Rate: 0.0012931
	LOSS [training: 0.048702551568440444 | validation: 0.043912966701473904]
	TIME [epoch: 8.8 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05157355593116669		[learning rate: 0.0012915]
		[batch 20/20] avg loss: 0.056487842818823394		[learning rate: 0.00129]
	Learning Rate: 0.00128997
	LOSS [training: 0.054030699374995036 | validation: 0.023072759953672705]
	TIME [epoch: 8.8 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032834276551491376		[learning rate: 0.0012884]
		[batch 20/20] avg loss: 0.03853382686444164		[learning rate: 0.0012868]
	Learning Rate: 0.00128685
	LOSS [training: 0.0356840517079665 | validation: 0.04568177516994521]
	TIME [epoch: 8.8 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043039380783897144		[learning rate: 0.0012853]
		[batch 20/20] avg loss: 0.03876895568920828		[learning rate: 0.0012837]
	Learning Rate: 0.00128373
	LOSS [training: 0.04090416823655271 | validation: 0.08126873533628151]
	TIME [epoch: 8.83 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06794500898093603		[learning rate: 0.0012822]
		[batch 20/20] avg loss: 0.055919497403808735		[learning rate: 0.0012806]
	Learning Rate: 0.00128062
	LOSS [training: 0.06193225319237239 | validation: 0.03820294021305727]
	TIME [epoch: 8.81 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03557191758769579		[learning rate: 0.0012791]
		[batch 20/20] avg loss: 0.07143770032079029		[learning rate: 0.0012775]
	Learning Rate: 0.00127752
	LOSS [training: 0.05350480895424303 | validation: 0.0520876165555913]
	TIME [epoch: 8.81 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04064287267268022		[learning rate: 0.001276]
		[batch 20/20] avg loss: 0.05544131455948568		[learning rate: 0.0012744]
	Learning Rate: 0.00127443
	LOSS [training: 0.04804209361608294 | validation: 0.03576713229987512]
	TIME [epoch: 8.81 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0447675222570204		[learning rate: 0.0012729]
		[batch 20/20] avg loss: 0.04311531468305472		[learning rate: 0.0012713]
	Learning Rate: 0.00127134
	LOSS [training: 0.04394141847003756 | validation: 0.0318549148806912]
	TIME [epoch: 8.8 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04728408096427645		[learning rate: 0.0012698]
		[batch 20/20] avg loss: 0.047773598642869244		[learning rate: 0.0012683]
	Learning Rate: 0.00126827
	LOSS [training: 0.04752883980357284 | validation: 0.04176877597308256]
	TIME [epoch: 8.81 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06626411175752607		[learning rate: 0.0012667]
		[batch 20/20] avg loss: 0.05626205970437267		[learning rate: 0.0012652]
	Learning Rate: 0.0012652
	LOSS [training: 0.06126308573094937 | validation: 0.03568846878616723]
	TIME [epoch: 8.8 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03536979495447183		[learning rate: 0.0012637]
		[batch 20/20] avg loss: 0.04984171795221116		[learning rate: 0.0012621]
	Learning Rate: 0.00126213
	LOSS [training: 0.042605756453341505 | validation: 0.019508536164993723]
	TIME [epoch: 8.81 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026856265916021006		[learning rate: 0.0012606]
		[batch 20/20] avg loss: 0.0422177351399123		[learning rate: 0.0012591]
	Learning Rate: 0.00125908
	LOSS [training: 0.03453700052796665 | validation: 0.06633642025059827]
	TIME [epoch: 8.8 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04036731533960942		[learning rate: 0.0012576]
		[batch 20/20] avg loss: 0.058804721664479864		[learning rate: 0.001256]
	Learning Rate: 0.00125603
	LOSS [training: 0.049586018502044645 | validation: 0.03501671270560344]
	TIME [epoch: 8.82 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04111931731179537		[learning rate: 0.0012545]
		[batch 20/20] avg loss: 0.03997414929307281		[learning rate: 0.001253]
	Learning Rate: 0.00125299
	LOSS [training: 0.04054673330243409 | validation: 0.055539631073897505]
	TIME [epoch: 8.81 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060451071322063774		[learning rate: 0.0012515]
		[batch 20/20] avg loss: 0.05848979488392586		[learning rate: 0.00125]
	Learning Rate: 0.00124996
	LOSS [training: 0.059470433102994824 | validation: 0.056841180927800404]
	TIME [epoch: 8.79 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03715793419538652		[learning rate: 0.0012484]
		[batch 20/20] avg loss: 0.032521570385566446		[learning rate: 0.0012469]
	Learning Rate: 0.00124693
	LOSS [training: 0.0348397522904765 | validation: 0.033194021986676724]
	TIME [epoch: 8.8 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034348764491605		[learning rate: 0.0012454]
		[batch 20/20] avg loss: 0.02676734719921666		[learning rate: 0.0012439]
	Learning Rate: 0.00124391
	LOSS [training: 0.030558055845410832 | validation: 0.03580391556667707]
	TIME [epoch: 8.79 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03034798597606485		[learning rate: 0.0012424]
		[batch 20/20] avg loss: 0.024832438247264697		[learning rate: 0.0012409]
	Learning Rate: 0.0012409
	LOSS [training: 0.02759021211166477 | validation: 0.03719779318398099]
	TIME [epoch: 8.83 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05494886377817835		[learning rate: 0.0012394]
		[batch 20/20] avg loss: 0.05424388904551852		[learning rate: 0.0012379]
	Learning Rate: 0.0012379
	LOSS [training: 0.05459637641184843 | validation: 0.045374117752859225]
	TIME [epoch: 8.81 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036187463904872305		[learning rate: 0.0012364]
		[batch 20/20] avg loss: 0.036981462655559155		[learning rate: 0.0012349]
	Learning Rate: 0.0012349
	LOSS [training: 0.03658446328021574 | validation: 0.06435408303321932]
	TIME [epoch: 8.81 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04439682843400209		[learning rate: 0.0012334]
		[batch 20/20] avg loss: 0.058992858649989044		[learning rate: 0.0012319]
	Learning Rate: 0.00123191
	LOSS [training: 0.051694843541995585 | validation: 0.04160705648793578]
	TIME [epoch: 8.8 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04035040495573779		[learning rate: 0.0012304]
		[batch 20/20] avg loss: 0.045311483354302795		[learning rate: 0.0012289]
	Learning Rate: 0.00122893
	LOSS [training: 0.0428309441550203 | validation: 0.037924168920976824]
	TIME [epoch: 8.81 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03558446963816088		[learning rate: 0.0012274]
		[batch 20/20] avg loss: 0.035859482688517144		[learning rate: 0.001226]
	Learning Rate: 0.00122595
	LOSS [training: 0.03572197616333901 | validation: 0.04436061400287693]
	TIME [epoch: 8.81 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0518431925225832		[learning rate: 0.0012245]
		[batch 20/20] avg loss: 0.04735507603933072		[learning rate: 0.001223]
	Learning Rate: 0.00122298
	LOSS [training: 0.04959913428095696 | validation: 0.09406035392825256]
	TIME [epoch: 8.8 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059332398818203826		[learning rate: 0.0012215]
		[batch 20/20] avg loss: 0.056857506873858786		[learning rate: 0.00122]
	Learning Rate: 0.00122002
	LOSS [training: 0.05809495284603131 | validation: 0.03046457629867725]
	TIME [epoch: 8.79 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055728448356163876		[learning rate: 0.0012185]
		[batch 20/20] avg loss: 0.03349265111505784		[learning rate: 0.0012171]
	Learning Rate: 0.00121707
	LOSS [training: 0.044610549735610856 | validation: 0.0227643159263647]
	TIME [epoch: 8.8 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03360663678327351		[learning rate: 0.0012156]
		[batch 20/20] avg loss: 0.0307749121959999		[learning rate: 0.0012141]
	Learning Rate: 0.00121412
	LOSS [training: 0.03219077448963671 | validation: 0.07943710828996231]
	TIME [epoch: 8.8 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04528916675821659		[learning rate: 0.0012127]
		[batch 20/20] avg loss: 0.03074809029765791		[learning rate: 0.0012112]
	Learning Rate: 0.00121119
	LOSS [training: 0.03801862852793725 | validation: 0.025584625763942805]
	TIME [epoch: 8.82 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04475857689226125		[learning rate: 0.0012097]
		[batch 20/20] avg loss: 0.03644233468296685		[learning rate: 0.0012083]
	Learning Rate: 0.00120825
	LOSS [training: 0.04060045578761405 | validation: 0.02369245690420175]
	TIME [epoch: 8.79 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0370715338467601		[learning rate: 0.0012068]
		[batch 20/20] avg loss: 0.05104327579095419		[learning rate: 0.0012053]
	Learning Rate: 0.00120533
	LOSS [training: 0.04405740481885714 | validation: 0.016969205402080977]
	TIME [epoch: 8.79 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03740771849432272		[learning rate: 0.0012039]
		[batch 20/20] avg loss: 0.027534069810089417		[learning rate: 0.0012024]
	Learning Rate: 0.00120241
	LOSS [training: 0.03247089415220607 | validation: 0.013368126111063142]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_974.pth
	Model improved!!!
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029646903419707514		[learning rate: 0.001201]
		[batch 20/20] avg loss: 0.05161470651331562		[learning rate: 0.0011995]
	Learning Rate: 0.0011995
	LOSS [training: 0.04063080496651157 | validation: 0.04168122007989136]
	TIME [epoch: 8.83 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045053628195960954		[learning rate: 0.001198]
		[batch 20/20] avg loss: 0.02943731772875986		[learning rate: 0.0011966]
	Learning Rate: 0.0011966
	LOSS [training: 0.03724547296236041 | validation: 0.03138787511245489]
	TIME [epoch: 8.8 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03753782287735225		[learning rate: 0.0011951]
		[batch 20/20] avg loss: 0.03522478368825009		[learning rate: 0.0011937]
	Learning Rate: 0.0011937
	LOSS [training: 0.036381303282801175 | validation: 0.030405242016830432]
	TIME [epoch: 8.81 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07373658859518013		[learning rate: 0.0011923]
		[batch 20/20] avg loss: 0.08963515603974889		[learning rate: 0.0011908]
	Learning Rate: 0.00119081
	LOSS [training: 0.0816858723174645 | validation: 0.04052908690348282]
	TIME [epoch: 8.79 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03213012817435003		[learning rate: 0.0011894]
		[batch 20/20] avg loss: 0.04125533666959879		[learning rate: 0.0011879]
	Learning Rate: 0.00118793
	LOSS [training: 0.036692732421974406 | validation: 0.04538285837031107]
	TIME [epoch: 8.79 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035110479943451595		[learning rate: 0.0011865]
		[batch 20/20] avg loss: 0.0347889494367131		[learning rate: 0.0011851]
	Learning Rate: 0.00118505
	LOSS [training: 0.034949714690082345 | validation: 0.04003909984003638]
	TIME [epoch: 8.81 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06152565391774163		[learning rate: 0.0011836]
		[batch 20/20] avg loss: 0.03775361413182392		[learning rate: 0.0011822]
	Learning Rate: 0.00118218
	LOSS [training: 0.04963963402478276 | validation: 0.033455905266442515]
	TIME [epoch: 8.79 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04477492494321275		[learning rate: 0.0011807]
		[batch 20/20] avg loss: 0.04736607804099308		[learning rate: 0.0011793]
	Learning Rate: 0.00117932
	LOSS [training: 0.04607050149210291 | validation: 0.03708891396052367]
	TIME [epoch: 8.79 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03820011999558519		[learning rate: 0.0011779]
		[batch 20/20] avg loss: 0.051636538232690446		[learning rate: 0.0011765]
	Learning Rate: 0.00117646
	LOSS [training: 0.044918329114137824 | validation: 0.08374191466707995]
	TIME [epoch: 8.79 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03594658101966466		[learning rate: 0.001175]
		[batch 20/20] avg loss: 0.04958843798809488		[learning rate: 0.0011736]
	Learning Rate: 0.00117362
	LOSS [training: 0.042767509503879766 | validation: 0.06721887966024602]
	TIME [epoch: 8.8 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0482603089337349		[learning rate: 0.0011722]
		[batch 20/20] avg loss: 0.04857365277258707		[learning rate: 0.0011708]
	Learning Rate: 0.00117078
	LOSS [training: 0.048416980853161 | validation: 0.02286698324256016]
	TIME [epoch: 8.79 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0545590287705738		[learning rate: 0.0011694]
		[batch 20/20] avg loss: 0.0552187685420768		[learning rate: 0.0011679]
	Learning Rate: 0.00116794
	LOSS [training: 0.05488889865632531 | validation: 0.07355108383100041]
	TIME [epoch: 8.8 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04471537572526339		[learning rate: 0.0011665]
		[batch 20/20] avg loss: 0.05346098915084391		[learning rate: 0.0011651]
	Learning Rate: 0.00116511
	LOSS [training: 0.04908818243805366 | validation: 0.032843283706808384]
	TIME [epoch: 8.78 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030368373492853575		[learning rate: 0.0011637]
		[batch 20/20] avg loss: 0.04693632640864105		[learning rate: 0.0011623]
	Learning Rate: 0.00116229
	LOSS [training: 0.0386523499507473 | validation: 0.027593230324870502]
	TIME [epoch: 8.79 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03676829911949623		[learning rate: 0.0011609]
		[batch 20/20] avg loss: 0.03131579082157712		[learning rate: 0.0011595]
	Learning Rate: 0.00115948
	LOSS [training: 0.03404204497053667 | validation: 0.014768173886618293]
	TIME [epoch: 8.82 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02770663431159696		[learning rate: 0.0011581]
		[batch 20/20] avg loss: 0.035852636532712256		[learning rate: 0.0011567]
	Learning Rate: 0.00115667
	LOSS [training: 0.0317796354221546 | validation: 0.046349084564821535]
	TIME [epoch: 8.8 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06617579859399486		[learning rate: 0.0011553]
		[batch 20/20] avg loss: 0.03678890116650925		[learning rate: 0.0011539]
	Learning Rate: 0.00115387
	LOSS [training: 0.05148234988025204 | validation: 0.02153847848575369]
	TIME [epoch: 8.8 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0317339986202168		[learning rate: 0.0011525]
		[batch 20/20] avg loss: 0.04004289536272757		[learning rate: 0.0011511]
	Learning Rate: 0.00115108
	LOSS [training: 0.03588844699147219 | validation: 0.054849184092958234]
	TIME [epoch: 8.79 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06670837067359904		[learning rate: 0.0011497]
		[batch 20/20] avg loss: 0.03251417649673403		[learning rate: 0.0011483]
	Learning Rate: 0.00114829
	LOSS [training: 0.04961127358516654 | validation: 0.03728770526649317]
	TIME [epoch: 8.79 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04032139478544244		[learning rate: 0.0011469]
		[batch 20/20] avg loss: 0.03212010644121063		[learning rate: 0.0011455]
	Learning Rate: 0.00114551
	LOSS [training: 0.036220750613326524 | validation: 0.015635173280476283]
	TIME [epoch: 8.81 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035960367403346336		[learning rate: 0.0011441]
		[batch 20/20] avg loss: 0.04428372676672643		[learning rate: 0.0011427]
	Learning Rate: 0.00114274
	LOSS [training: 0.04012204708503638 | validation: 0.028607013680795733]
	TIME [epoch: 8.79 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03355589247587909		[learning rate: 0.0011414]
		[batch 20/20] avg loss: 0.06708251840435973		[learning rate: 0.00114]
	Learning Rate: 0.00113997
	LOSS [training: 0.050319205440119405 | validation: 0.07872087759330137]
	TIME [epoch: 8.79 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059295589889766874		[learning rate: 0.0011386]
		[batch 20/20] avg loss: 0.038511972771159704		[learning rate: 0.0011372]
	Learning Rate: 0.00113721
	LOSS [training: 0.04890378133046328 | validation: 0.030895781517986706]
	TIME [epoch: 8.78 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035845453056825155		[learning rate: 0.0011358]
		[batch 20/20] avg loss: 0.042815659586125446		[learning rate: 0.0011345]
	Learning Rate: 0.00113446
	LOSS [training: 0.0393305563214753 | validation: 0.058893207541296476]
	TIME [epoch: 8.8 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0485442249584144		[learning rate: 0.0011331]
		[batch 20/20] avg loss: 0.03371041085664742		[learning rate: 0.0011317]
	Learning Rate: 0.00113171
	LOSS [training: 0.04112731790753092 | validation: 0.042285150807085864]
	TIME [epoch: 8.81 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03043520863730339		[learning rate: 0.0011303]
		[batch 20/20] avg loss: 0.046031789127592884		[learning rate: 0.001129]
	Learning Rate: 0.00112897
	LOSS [training: 0.03823349888244813 | validation: 0.02777919814347914]
	TIME [epoch: 8.78 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034055937777907115		[learning rate: 0.0011276]
		[batch 20/20] avg loss: 0.04910916976290223		[learning rate: 0.0011262]
	Learning Rate: 0.00112624
	LOSS [training: 0.041582553770404665 | validation: 0.02813127384128302]
	TIME [epoch: 8.8 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048734830250757935		[learning rate: 0.0011249]
		[batch 20/20] avg loss: 0.05592371488732164		[learning rate: 0.0011235]
	Learning Rate: 0.00112352
	LOSS [training: 0.052329272569039784 | validation: 0.04356944962745775]
	TIME [epoch: 8.79 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034862179537655844		[learning rate: 0.0011222]
		[batch 20/20] avg loss: 0.05595552861161035		[learning rate: 0.0011208]
	Learning Rate: 0.0011208
	LOSS [training: 0.04540885407463309 | validation: 0.06237625538505778]
	TIME [epoch: 8.8 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04954787349786104		[learning rate: 0.0011194]
		[batch 20/20] avg loss: 0.044145349896160105		[learning rate: 0.0011181]
	Learning Rate: 0.00111808
	LOSS [training: 0.046846611697010584 | validation: 0.05519741720930693]
	TIME [epoch: 8.8 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05158366201815416		[learning rate: 0.0011167]
		[batch 20/20] avg loss: 0.03628647619342719		[learning rate: 0.0011154]
	Learning Rate: 0.00111538
	LOSS [training: 0.04393506910579068 | validation: 0.03917149894308046]
	TIME [epoch: 8.79 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0478280947653273		[learning rate: 0.001114]
		[batch 20/20] avg loss: 0.04965189443126379		[learning rate: 0.0011127]
	Learning Rate: 0.00111268
	LOSS [training: 0.048739994598295554 | validation: 0.058528020527491134]
	TIME [epoch: 8.8 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03377755745126965		[learning rate: 0.0011113]
		[batch 20/20] avg loss: 0.03317817675874384		[learning rate: 0.00111]
	Learning Rate: 0.00110998
	LOSS [training: 0.03347786710500674 | validation: 0.03330842544729043]
	TIME [epoch: 8.78 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04284929652064999		[learning rate: 0.0011086]
		[batch 20/20] avg loss: 0.03606256100341577		[learning rate: 0.0011073]
	Learning Rate: 0.00110729
	LOSS [training: 0.03945592876203287 | validation: 0.025309030764902887]
	TIME [epoch: 8.81 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0386129675741685		[learning rate: 0.001106]
		[batch 20/20] avg loss: 0.03073375823244428		[learning rate: 0.0011046]
	Learning Rate: 0.00110461
	LOSS [training: 0.034673362903306394 | validation: 0.051355943663050005]
	TIME [epoch: 8.79 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03876405869050608		[learning rate: 0.0011033]
		[batch 20/20] avg loss: 0.06274072627394583		[learning rate: 0.0011019]
	Learning Rate: 0.00110194
	LOSS [training: 0.05075239248222596 | validation: 0.04070639036576236]
	TIME [epoch: 8.8 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0349518687202771		[learning rate: 0.0011006]
		[batch 20/20] avg loss: 0.03478445675880081		[learning rate: 0.0010993]
	Learning Rate: 0.00109927
	LOSS [training: 0.034868162739538966 | validation: 0.019856221739686982]
	TIME [epoch: 8.79 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056034211050384385		[learning rate: 0.0010979]
		[batch 20/20] avg loss: 0.05102208804304672		[learning rate: 0.0010966]
	Learning Rate: 0.00109661
	LOSS [training: 0.05352814954671555 | validation: 0.03604755014014387]
	TIME [epoch: 8.79 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048347884116408106		[learning rate: 0.0010953]
		[batch 20/20] avg loss: 0.033713226176649215		[learning rate: 0.001094]
	Learning Rate: 0.00109396
	LOSS [training: 0.04103055514652867 | validation: 0.016500453083077307]
	TIME [epoch: 8.81 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017388740172975963		[learning rate: 0.0010926]
		[batch 20/20] avg loss: 0.035948055022857875		[learning rate: 0.0010913]
	Learning Rate: 0.00109131
	LOSS [training: 0.026668397597916914 | validation: 0.01595242964143726]
	TIME [epoch: 8.78 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03526486201421726		[learning rate: 0.00109]
		[batch 20/20] avg loss: 0.03586956795395787		[learning rate: 0.0010887]
	Learning Rate: 0.00108867
	LOSS [training: 0.03556721498408756 | validation: 0.03526276696067241]
	TIME [epoch: 8.79 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042753750913489263		[learning rate: 0.0010873]
		[batch 20/20] avg loss: 0.045222301451184825		[learning rate: 0.001086]
	Learning Rate: 0.00108603
	LOSS [training: 0.043988026182337055 | validation: 0.03559269972447066]
	TIME [epoch: 8.8 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024843724773049653		[learning rate: 0.0010847]
		[batch 20/20] avg loss: 0.037935956445125416		[learning rate: 0.0010834]
	Learning Rate: 0.0010834
	LOSS [training: 0.03138984060908754 | validation: 0.04652590938570264]
	TIME [epoch: 8.8 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029061816794605188		[learning rate: 0.0010821]
		[batch 20/20] avg loss: 0.03772565119095741		[learning rate: 0.0010808]
	Learning Rate: 0.00108078
	LOSS [training: 0.033393733992781297 | validation: 0.038819123809850614]
	TIME [epoch: 8.81 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027127101013033443		[learning rate: 0.0010795]
		[batch 20/20] avg loss: 0.06002650250694643		[learning rate: 0.0010782]
	Learning Rate: 0.00107816
	LOSS [training: 0.043576801759989944 | validation: 0.042384206136825794]
	TIME [epoch: 8.78 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03304127859578907		[learning rate: 0.0010769]
		[batch 20/20] avg loss: 0.03457466680244385		[learning rate: 0.0010756]
	Learning Rate: 0.00107555
	LOSS [training: 0.03380797269911645 | validation: 0.027364514054389985]
	TIME [epoch: 8.8 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03394624540556431		[learning rate: 0.0010742]
		[batch 20/20] avg loss: 0.02697391640860946		[learning rate: 0.0010729]
	Learning Rate: 0.00107295
	LOSS [training: 0.030460080907086885 | validation: 0.03500488829882553]
	TIME [epoch: 8.78 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04360635456083511		[learning rate: 0.0010716]
		[batch 20/20] avg loss: 0.029947435033699947		[learning rate: 0.0010704]
	Learning Rate: 0.00107035
	LOSS [training: 0.03677689479726752 | validation: 0.056501838967173104]
	TIME [epoch: 8.8 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024442592358441883		[learning rate: 0.0010691]
		[batch 20/20] avg loss: 0.036732310753570016		[learning rate: 0.0010678]
	Learning Rate: 0.00106776
	LOSS [training: 0.030587451556005944 | validation: 0.07033833051245507]
	TIME [epoch: 8.79 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047868157853014334		[learning rate: 0.0010665]
		[batch 20/20] avg loss: 0.04524334307540681		[learning rate: 0.0010652]
	Learning Rate: 0.00106518
	LOSS [training: 0.04655575046421057 | validation: 0.04515628753923299]
	TIME [epoch: 8.79 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0394483177919306		[learning rate: 0.0010639]
		[batch 20/20] avg loss: 0.03752665835593159		[learning rate: 0.0010626]
	Learning Rate: 0.0010626
	LOSS [training: 0.03848748807393109 | validation: 0.016172711717550855]
	TIME [epoch: 8.79 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027442765404160314		[learning rate: 0.0010613]
		[batch 20/20] avg loss: 0.04050327736656913		[learning rate: 0.00106]
	Learning Rate: 0.00106002
	LOSS [training: 0.03397302138536472 | validation: 0.03665857757048059]
	TIME [epoch: 8.78 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024111515360700036		[learning rate: 0.0010587]
		[batch 20/20] avg loss: 0.03396960599827281		[learning rate: 0.0010575]
	Learning Rate: 0.00105746
	LOSS [training: 0.029040560679486425 | validation: 0.08176963129936315]
	TIME [epoch: 8.8 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04309018115495626		[learning rate: 0.0010562]
		[batch 20/20] avg loss: 0.04716932644520269		[learning rate: 0.0010549]
	Learning Rate: 0.0010549
	LOSS [training: 0.04512975380007947 | validation: 0.08198518603747111]
	TIME [epoch: 8.79 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05965151461567904		[learning rate: 0.0010536]
		[batch 20/20] avg loss: 0.03891960498244889		[learning rate: 0.0010523]
	Learning Rate: 0.00105234
	LOSS [training: 0.049285559799063974 | validation: 0.05871393800952181]
	TIME [epoch: 8.8 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047725449545425354		[learning rate: 0.0010511]
		[batch 20/20] avg loss: 0.03156444166086893		[learning rate: 0.0010498]
	Learning Rate: 0.0010498
	LOSS [training: 0.039644945603147144 | validation: 0.06948988704612048]
	TIME [epoch: 8.81 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04704123736376929		[learning rate: 0.0010485]
		[batch 20/20] avg loss: 0.03515978449817638		[learning rate: 0.0010473]
	Learning Rate: 0.00104726
	LOSS [training: 0.04110051093097284 | validation: 0.038247133418657875]
	TIME [epoch: 8.8 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02452873744010453		[learning rate: 0.001046]
		[batch 20/20] avg loss: 0.044921501023538694		[learning rate: 0.0010447]
	Learning Rate: 0.00104472
	LOSS [training: 0.03472511923182161 | validation: 0.03660139746859978]
	TIME [epoch: 8.82 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04965040397399323		[learning rate: 0.0010435]
		[batch 20/20] avg loss: 0.03078255688900543		[learning rate: 0.0010422]
	Learning Rate: 0.00104219
	LOSS [training: 0.040216480431499335 | validation: 0.03704358496695915]
	TIME [epoch: 8.79 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044027308209575435		[learning rate: 0.0010409]
		[batch 20/20] avg loss: 0.0305908170189583		[learning rate: 0.0010397]
	Learning Rate: 0.00103967
	LOSS [training: 0.03730906261426687 | validation: 0.0381864047840255]
	TIME [epoch: 8.78 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04076713379431235		[learning rate: 0.0010384]
		[batch 20/20] avg loss: 0.02921278913167872		[learning rate: 0.0010372]
	Learning Rate: 0.00103715
	LOSS [training: 0.03498996146299553 | validation: 0.02615903135286553]
	TIME [epoch: 8.79 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039676856300195996		[learning rate: 0.0010359]
		[batch 20/20] avg loss: 0.029080626933466868		[learning rate: 0.0010346]
	Learning Rate: 0.00103464
	LOSS [training: 0.034378741616831424 | validation: 0.040835192723645074]
	TIME [epoch: 8.79 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046341646814298365		[learning rate: 0.0010334]
		[batch 20/20] avg loss: 0.046657307451319754		[learning rate: 0.0010321]
	Learning Rate: 0.00103214
	LOSS [training: 0.04649947713280904 | validation: 0.04143109705705434]
	TIME [epoch: 8.81 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04032679296612378		[learning rate: 0.0010309]
		[batch 20/20] avg loss: 0.03490484300062954		[learning rate: 0.0010296]
	Learning Rate: 0.00102964
	LOSS [training: 0.03761581798337667 | validation: 0.08921467899225896]
	TIME [epoch: 8.79 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0348855591379381		[learning rate: 0.0010284]
		[batch 20/20] avg loss: 0.03736528506464319		[learning rate: 0.0010271]
	Learning Rate: 0.00102714
	LOSS [training: 0.03612542210129065 | validation: 0.04035451991995201]
	TIME [epoch: 8.8 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029372212725357588		[learning rate: 0.0010259]
		[batch 20/20] avg loss: 0.038173920011604356		[learning rate: 0.0010247]
	Learning Rate: 0.00102466
	LOSS [training: 0.03377306636848097 | validation: 0.02537313040878498]
	TIME [epoch: 8.78 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02749024723907308		[learning rate: 0.0010234]
		[batch 20/20] avg loss: 0.03461359568855996		[learning rate: 0.0010222]
	Learning Rate: 0.00102218
	LOSS [training: 0.031051921463816513 | validation: 0.03801989185002537]
	TIME [epoch: 8.81 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022010961377531155		[learning rate: 0.0010209]
		[batch 20/20] avg loss: 0.029903075732097596		[learning rate: 0.0010197]
	Learning Rate: 0.0010197
	LOSS [training: 0.02595701855481437 | validation: 0.03170790817045511]
	TIME [epoch: 8.79 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028597088112504256		[learning rate: 0.0010185]
		[batch 20/20] avg loss: 0.03506382597152848		[learning rate: 0.0010172]
	Learning Rate: 0.00101723
	LOSS [training: 0.03183045704201637 | validation: 0.08205960427342564]
	TIME [epoch: 8.8 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056401465098347904		[learning rate: 0.001016]
		[batch 20/20] avg loss: 0.048103644267820184		[learning rate: 0.0010148]
	Learning Rate: 0.00101477
	LOSS [training: 0.052252554683084054 | validation: 0.02960843156433144]
	TIME [epoch: 8.81 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0216492887088605		[learning rate: 0.0010135]
		[batch 20/20] avg loss: 0.023559231640787114		[learning rate: 0.0010123]
	Learning Rate: 0.00101232
	LOSS [training: 0.022604260174823806 | validation: 0.02705612890677052]
	TIME [epoch: 8.81 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02308221281683113		[learning rate: 0.0010111]
		[batch 20/20] avg loss: 0.034749908444251625		[learning rate: 0.0010099]
	Learning Rate: 0.00100986
	LOSS [training: 0.02891606063054138 | validation: 0.03748039992759021]
	TIME [epoch: 8.8 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027110828345179028		[learning rate: 0.0010086]
		[batch 20/20] avg loss: 0.02818720671420947		[learning rate: 0.0010074]
	Learning Rate: 0.00100742
	LOSS [training: 0.027649017529694248 | validation: 0.024349487887758554]
	TIME [epoch: 8.8 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04798463925728956		[learning rate: 0.0010062]
		[batch 20/20] avg loss: 0.03371320574834104		[learning rate: 0.001005]
	Learning Rate: 0.00100498
	LOSS [training: 0.040848922502815285 | validation: 0.03954088761219006]
	TIME [epoch: 8.78 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03250157300838221		[learning rate: 0.0010038]
		[batch 20/20] avg loss: 0.03968581868834613		[learning rate: 0.0010025]
	Learning Rate: 0.00100255
	LOSS [training: 0.03609369584836417 | validation: 0.02240322366068216]
	TIME [epoch: 8.79 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02330571782689945		[learning rate: 0.0010013]
		[batch 20/20] avg loss: 0.03401642999179939		[learning rate: 0.0010001]
	Learning Rate: 0.00100012
	LOSS [training: 0.02866107390934942 | validation: 0.02341864533451364]
	TIME [epoch: 8.79 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03385199509030742		[learning rate: 0.00099891]
		[batch 20/20] avg loss: 0.042140366142969815		[learning rate: 0.0009977]
	Learning Rate: 0.0009977
	LOSS [training: 0.03799618061663862 | validation: 0.034856555691956934]
	TIME [epoch: 8.82 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03941348211565261		[learning rate: 0.00099649]
		[batch 20/20] avg loss: 0.035866915803715135		[learning rate: 0.00099528]
	Learning Rate: 0.000995285
	LOSS [training: 0.037640198959683874 | validation: 0.03391888088674857]
	TIME [epoch: 8.79 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03133439933942389		[learning rate: 0.00099408]
		[batch 20/20] avg loss: 0.045170677478866286		[learning rate: 0.00099288]
	Learning Rate: 0.000992875
	LOSS [training: 0.03825253840914509 | validation: 0.04232072879618031]
	TIME [epoch: 8.79 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025565795615061164		[learning rate: 0.00099167]
		[batch 20/20] avg loss: 0.02905872076799884		[learning rate: 0.00099047]
	Learning Rate: 0.000990472
	LOSS [training: 0.027312258191530003 | validation: 0.016289166512128528]
	TIME [epoch: 8.78 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03784000677867326		[learning rate: 0.00098927]
		[batch 20/20] avg loss: 0.03714740445409116		[learning rate: 0.00098807]
	Learning Rate: 0.000988074
	LOSS [training: 0.03749370561638222 | validation: 0.03167964280682542]
	TIME [epoch: 8.8 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028818792957580063		[learning rate: 0.00098688]
		[batch 20/20] avg loss: 0.028924317264510724		[learning rate: 0.00098568]
	Learning Rate: 0.000985682
	LOSS [training: 0.02887155511104539 | validation: 0.051835118827231685]
	TIME [epoch: 8.81 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04470164484180239		[learning rate: 0.00098449]
		[batch 20/20] avg loss: 0.03023722551962288		[learning rate: 0.0009833]
	Learning Rate: 0.000983296
	LOSS [training: 0.03746943518071264 | validation: 0.02519669587004749]
	TIME [epoch: 8.81 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034063375474620405		[learning rate: 0.0009821]
		[batch 20/20] avg loss: 0.02567770567590318		[learning rate: 0.00098092]
	Learning Rate: 0.000980916
	LOSS [training: 0.02987054057526179 | validation: 0.028476452733072748]
	TIME [epoch: 8.8 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020943897139707412		[learning rate: 0.00097973]
		[batch 20/20] avg loss: 0.03109184737827509		[learning rate: 0.00097854]
	Learning Rate: 0.000978541
	LOSS [training: 0.026017872258991253 | validation: 0.04328509699466759]
	TIME [epoch: 8.8 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03109042917584608		[learning rate: 0.00097736]
		[batch 20/20] avg loss: 0.05296697684879981		[learning rate: 0.00097617]
	Learning Rate: 0.000976172
	LOSS [training: 0.042028703012322935 | validation: 0.04135529631338901]
	TIME [epoch: 8.81 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03263533992990149		[learning rate: 0.00097499]
		[batch 20/20] avg loss: 0.031478511803799694		[learning rate: 0.00097381]
	Learning Rate: 0.000973809
	LOSS [training: 0.032056925866850584 | validation: 0.01685178333023846]
	TIME [epoch: 8.79 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030109859470453516		[learning rate: 0.00097263]
		[batch 20/20] avg loss: 0.04106060026263609		[learning rate: 0.00097145]
	Learning Rate: 0.000971451
	LOSS [training: 0.03558522986654481 | validation: 0.036065101766181176]
	TIME [epoch: 8.79 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02459397532025651		[learning rate: 0.00097027]
		[batch 20/20] avg loss: 0.05250184622095709		[learning rate: 0.0009691]
	Learning Rate: 0.0009691
	LOSS [training: 0.038547910770606805 | validation: 0.08083670984060934]
	TIME [epoch: 8.79 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03379777369713288		[learning rate: 0.00096793]
		[batch 20/20] avg loss: 0.03911523049802857		[learning rate: 0.00096675]
	Learning Rate: 0.000966754
	LOSS [training: 0.03645650209758072 | validation: 0.03112127306508266]
	TIME [epoch: 8.79 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026655884912899868		[learning rate: 0.00096558]
		[batch 20/20] avg loss: 0.03483366632426703		[learning rate: 0.00096441]
	Learning Rate: 0.000964413
	LOSS [training: 0.030744775618583443 | validation: 0.026247310744312403]
	TIME [epoch: 8.82 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036088002262501306		[learning rate: 0.00096325]
		[batch 20/20] avg loss: 0.04512156766709803		[learning rate: 0.00096208]
	Learning Rate: 0.000962079
	LOSS [training: 0.040604784964799676 | validation: 0.02864081795288237]
	TIME [epoch: 8.79 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044702968265440055		[learning rate: 0.00096091]
		[batch 20/20] avg loss: 0.040492515772602364		[learning rate: 0.00095975]
	Learning Rate: 0.00095975
	LOSS [training: 0.04259774201902121 | validation: 0.043592696702897754]
	TIME [epoch: 8.79 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03249622658608366		[learning rate: 0.00095859]
		[batch 20/20] avg loss: 0.04471511336643385		[learning rate: 0.00095743]
	Learning Rate: 0.000957426
	LOSS [training: 0.03860566997625876 | validation: 0.029354311991460845]
	TIME [epoch: 8.79 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033877117143851755		[learning rate: 0.00095627]
		[batch 20/20] avg loss: 0.03057485921267237		[learning rate: 0.00095511]
	Learning Rate: 0.000955108
	LOSS [training: 0.03222598817826206 | validation: 0.03929782665333397]
	TIME [epoch: 8.8 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03253347340666619		[learning rate: 0.00095395]
		[batch 20/20] avg loss: 0.039309744400581784		[learning rate: 0.0009528]
	Learning Rate: 0.000952796
	LOSS [training: 0.03592160890362399 | validation: 0.03466220822363837]
	TIME [epoch: 8.81 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031873240505569264		[learning rate: 0.00095164]
		[batch 20/20] avg loss: 0.019943028982724938		[learning rate: 0.00095049]
	Learning Rate: 0.00095049
	LOSS [training: 0.025908134744147104 | validation: 0.0377918932192082]
	TIME [epoch: 8.81 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02720163181195979		[learning rate: 0.00094934]
		[batch 20/20] avg loss: 0.03756667699908019		[learning rate: 0.00094819]
	Learning Rate: 0.000948189
	LOSS [training: 0.03238415440551999 | validation: 0.031076635547138035]
	TIME [epoch: 8.79 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04399373679433286		[learning rate: 0.00094704]
		[batch 20/20] avg loss: 0.024223298556997938		[learning rate: 0.00094589]
	Learning Rate: 0.000945893
	LOSS [training: 0.034108517675665403 | validation: 0.0326531302780635]
	TIME [epoch: 8.79 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03841254783666016		[learning rate: 0.00094475]
		[batch 20/20] avg loss: 0.039494359170777746		[learning rate: 0.0009436]
	Learning Rate: 0.000943603
	LOSS [training: 0.038953453503718954 | validation: 0.05178162625354109]
	TIME [epoch: 8.8 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04309043844705394		[learning rate: 0.00094246]
		[batch 20/20] avg loss: 0.05222131962579145		[learning rate: 0.00094132]
	Learning Rate: 0.000941319
	LOSS [training: 0.047655879036422696 | validation: 0.039405606778111925]
	TIME [epoch: 8.79 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042855139096697845		[learning rate: 0.00094018]
		[batch 20/20] avg loss: 0.04672276403989368		[learning rate: 0.00093904]
	Learning Rate: 0.00093904
	LOSS [training: 0.04478895156829577 | validation: 0.0522370454107495]
	TIME [epoch: 8.78 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038361043918261326		[learning rate: 0.0009379]
		[batch 20/20] avg loss: 0.03998185159510803		[learning rate: 0.00093677]
	Learning Rate: 0.000936767
	LOSS [training: 0.03917144775668468 | validation: 0.027074959001874285]
	TIME [epoch: 8.79 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05450760179581006		[learning rate: 0.00093563]
		[batch 20/20] avg loss: 0.04022003688242993		[learning rate: 0.0009345]
	Learning Rate: 0.000934499
	LOSS [training: 0.04736381933912 | validation: 0.022391305758860468]
	TIME [epoch: 8.79 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03630529610376253		[learning rate: 0.00093337]
		[batch 20/20] avg loss: 0.0438553760331578		[learning rate: 0.00093224]
	Learning Rate: 0.000932237
	LOSS [training: 0.04008033606846016 | validation: 0.04085510259249139]
	TIME [epoch: 8.81 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027512848925089296		[learning rate: 0.00093111]
		[batch 20/20] avg loss: 0.039222320192634816		[learning rate: 0.00092998]
	Learning Rate: 0.00092998
	LOSS [training: 0.03336758455886206 | validation: 0.02639914463380447]
	TIME [epoch: 8.78 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031840114302450984		[learning rate: 0.00092885]
		[batch 20/20] avg loss: 0.04556559820537971		[learning rate: 0.00092773]
	Learning Rate: 0.000927729
	LOSS [training: 0.03870285625391535 | validation: 0.02282727434435811]
	TIME [epoch: 8.79 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038172819004656175		[learning rate: 0.00092661]
		[batch 20/20] avg loss: 0.03542396467114127		[learning rate: 0.00092548]
	Learning Rate: 0.000925483
	LOSS [training: 0.03679839183789872 | validation: 0.02622494373891551]
	TIME [epoch: 8.78 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047275889262836976		[learning rate: 0.00092436]
		[batch 20/20] avg loss: 0.024843542021086417		[learning rate: 0.00092324]
	Learning Rate: 0.000923243
	LOSS [training: 0.0360597156419617 | validation: 0.02874904323060428]
	TIME [epoch: 8.79 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042848913727049076		[learning rate: 0.00092212]
		[batch 20/20] avg loss: 0.03705751824849729		[learning rate: 0.00092101]
	Learning Rate: 0.000921008
	LOSS [training: 0.0399532159877732 | validation: 0.028675215340439333]
	TIME [epoch: 8.81 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034582824909008325		[learning rate: 0.00091989]
		[batch 20/20] avg loss: 0.024641094773729187		[learning rate: 0.00091878]
	Learning Rate: 0.000918778
	LOSS [training: 0.029611959841368763 | validation: 0.030519307960311395]
	TIME [epoch: 8.79 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0484687840118668		[learning rate: 0.00091767]
		[batch 20/20] avg loss: 0.03610730234172283		[learning rate: 0.00091655]
	Learning Rate: 0.000916554
	LOSS [training: 0.042288043176794805 | validation: 0.029255684577312133]
	TIME [epoch: 8.79 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03974613763008157		[learning rate: 0.00091544]
		[batch 20/20] avg loss: 0.06211435930012236		[learning rate: 0.00091433]
	Learning Rate: 0.000914335
	LOSS [training: 0.050930248465101954 | validation: 0.04887122239429256]
	TIME [epoch: 8.78 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0368749183119426		[learning rate: 0.00091323]
		[batch 20/20] avg loss: 0.050585316827194506		[learning rate: 0.00091212]
	Learning Rate: 0.000912121
	LOSS [training: 0.043730117569568565 | validation: 0.03318752415370835]
	TIME [epoch: 8.8 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03820484885706255		[learning rate: 0.00091102]
		[batch 20/20] avg loss: 0.03836966684194339		[learning rate: 0.00090991]
	Learning Rate: 0.000909913
	LOSS [training: 0.038287257849502966 | validation: 0.02586071470896427]
	TIME [epoch: 8.79 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0289040233262484		[learning rate: 0.00090881]
		[batch 20/20] avg loss: 0.02914231567443142		[learning rate: 0.00090771]
	Learning Rate: 0.00090771
	LOSS [training: 0.029023169500339913 | validation: 0.04561228100000545]
	TIME [epoch: 8.78 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03598530613947416		[learning rate: 0.00090661]
		[batch 20/20] avg loss: 0.03534798363694961		[learning rate: 0.00090551]
	Learning Rate: 0.000905513
	LOSS [training: 0.035666644888211876 | validation: 0.045537246302871585]
	TIME [epoch: 8.78 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0398138257396209		[learning rate: 0.00090442]
		[batch 20/20] avg loss: 0.02878326957858699		[learning rate: 0.00090332]
	Learning Rate: 0.000903321
	LOSS [training: 0.03429854765910395 | validation: 0.016190979851382594]
	TIME [epoch: 8.77 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0280323584609957		[learning rate: 0.00090223]
		[batch 20/20] avg loss: 0.037454451989583235		[learning rate: 0.00090113]
	Learning Rate: 0.000901134
	LOSS [training: 0.032743405225289464 | validation: 0.022194439774292446]
	TIME [epoch: 8.81 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04498761072263259		[learning rate: 0.00090004]
		[batch 20/20] avg loss: 0.03413265029922644		[learning rate: 0.00089895]
	Learning Rate: 0.000898953
	LOSS [training: 0.03956013051092951 | validation: 0.034493401066373046]
	TIME [epoch: 8.78 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02163618781572625		[learning rate: 0.00089786]
		[batch 20/20] avg loss: 0.03061155324961685		[learning rate: 0.00089678]
	Learning Rate: 0.000896777
	LOSS [training: 0.026123870532671557 | validation: 0.035980686271922246]
	TIME [epoch: 8.79 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025999341776620517		[learning rate: 0.00089569]
		[batch 20/20] avg loss: 0.02760770745809004		[learning rate: 0.00089461]
	Learning Rate: 0.000894605
	LOSS [training: 0.026803524617355284 | validation: 0.02649669887406809]
	TIME [epoch: 8.79 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03374930877806917		[learning rate: 0.00089352]
		[batch 20/20] avg loss: 0.029085333312874585		[learning rate: 0.00089244]
	Learning Rate: 0.00089244
	LOSS [training: 0.031417321045471874 | validation: 0.046515018586209926]
	TIME [epoch: 8.79 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03682451284313273		[learning rate: 0.00089136]
		[batch 20/20] avg loss: 0.020196735035842083		[learning rate: 0.00089028]
	Learning Rate: 0.000890279
	LOSS [training: 0.0285106239394874 | validation: 0.004915040833999528]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_1098.pth
	Model improved!!!
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04017262755163841		[learning rate: 0.0008892]
		[batch 20/20] avg loss: 0.023308344276589444		[learning rate: 0.00088812]
	Learning Rate: 0.000888124
	LOSS [training: 0.03174048591411392 | validation: 0.050791141117194766]
	TIME [epoch: 8.8 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03723251011788889		[learning rate: 0.00088705]
		[batch 20/20] avg loss: 0.028860343569219026		[learning rate: 0.00088597]
	Learning Rate: 0.000885974
	LOSS [training: 0.03304642684355395 | validation: 0.040842091395890505]
	TIME [epoch: 8.79 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03828524650522105		[learning rate: 0.0008849]
		[batch 20/20] avg loss: 0.035832625826906384		[learning rate: 0.00088383]
	Learning Rate: 0.000883829
	LOSS [training: 0.03705893616606372 | validation: 0.037489254445634515]
	TIME [epoch: 8.78 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04311961748460559		[learning rate: 0.00088276]
		[batch 20/20] avg loss: 0.025667991614742813		[learning rate: 0.00088169]
	Learning Rate: 0.00088169
	LOSS [training: 0.0343938045496742 | validation: 0.03807080581942049]
	TIME [epoch: 8.78 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03164420636984972		[learning rate: 0.00088062]
		[batch 20/20] avg loss: 0.03695903663663902		[learning rate: 0.00087956]
	Learning Rate: 0.000879555
	LOSS [training: 0.034301621503244374 | validation: 0.01882699836511553]
	TIME [epoch: 8.8 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028882699984170968		[learning rate: 0.00087849]
		[batch 20/20] avg loss: 0.033194411019592404		[learning rate: 0.00087743]
	Learning Rate: 0.000877426
	LOSS [training: 0.03103855550188168 | validation: 0.035396912968267934]
	TIME [epoch: 8.78 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025005028548488017		[learning rate: 0.00087636]
		[batch 20/20] avg loss: 0.023115018428823932		[learning rate: 0.0008753]
	Learning Rate: 0.000875302
	LOSS [training: 0.02406002348865597 | validation: 0.026520490373470682]
	TIME [epoch: 8.78 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03179175864240215		[learning rate: 0.00087424]
		[batch 20/20] avg loss: 0.030458774598278192		[learning rate: 0.00087318]
	Learning Rate: 0.000873183
	LOSS [training: 0.031125266620340174 | validation: 0.02040516395124913]
	TIME [epoch: 8.78 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020054234630154856		[learning rate: 0.00087213]
		[batch 20/20] avg loss: 0.027193149616815727		[learning rate: 0.00087107]
	Learning Rate: 0.000871069
	LOSS [training: 0.023623692123485286 | validation: 0.029027257395178453]
	TIME [epoch: 8.81 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026941414838853282		[learning rate: 0.00087001]
		[batch 20/20] avg loss: 0.03527330191881657		[learning rate: 0.00086896]
	Learning Rate: 0.00086896
	LOSS [training: 0.031107358378834927 | validation: 0.01817517882694893]
	TIME [epoch: 8.8 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023517962600595025		[learning rate: 0.00086791]
		[batch 20/20] avg loss: 0.032065427067160174		[learning rate: 0.00086686]
	Learning Rate: 0.000866857
	LOSS [training: 0.027791694833877594 | validation: 0.0323948420884844]
	TIME [epoch: 8.77 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03907405654299772		[learning rate: 0.00086581]
		[batch 20/20] avg loss: 0.041925697905421784		[learning rate: 0.00086476]
	Learning Rate: 0.000864758
	LOSS [training: 0.04049987722420975 | validation: 0.03303415725498608]
	TIME [epoch: 8.78 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02193529402169674		[learning rate: 0.00086371]
		[batch 20/20] avg loss: 0.036046736497813		[learning rate: 0.00086266]
	Learning Rate: 0.000862665
	LOSS [training: 0.028991015259754872 | validation: 0.019016570072586322]
	TIME [epoch: 8.79 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030679758857766876		[learning rate: 0.00086162]
		[batch 20/20] avg loss: 0.03819536216810146		[learning rate: 0.00086058]
	Learning Rate: 0.000860577
	LOSS [training: 0.03443756051293416 | validation: 0.027514712199910623]
	TIME [epoch: 8.81 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027129109935076806		[learning rate: 0.00085953]
		[batch 20/20] avg loss: 0.02299712371509207		[learning rate: 0.00085849]
	Learning Rate: 0.000858493
	LOSS [training: 0.025063116825084435 | validation: 0.024827096895480974]
	TIME [epoch: 8.79 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04298826110984057		[learning rate: 0.00085745]
		[batch 20/20] avg loss: 0.01669173974839141		[learning rate: 0.00085641]
	Learning Rate: 0.000856415
	LOSS [training: 0.029840000429115988 | validation: 0.02200067774231748]
	TIME [epoch: 8.78 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024293377205509593		[learning rate: 0.00085538]
		[batch 20/20] avg loss: 0.03658219612765755		[learning rate: 0.00085434]
	Learning Rate: 0.000854342
	LOSS [training: 0.030437786666583568 | validation: 0.015263070255436658]
	TIME [epoch: 8.79 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02753365078110756		[learning rate: 0.00085331]
		[batch 20/20] avg loss: 0.03971168488858339		[learning rate: 0.00085227]
	Learning Rate: 0.000852273
	LOSS [training: 0.03362266783484547 | validation: 0.07766698515880903]
	TIME [epoch: 8.77 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0437741316453849		[learning rate: 0.00085124]
		[batch 20/20] avg loss: 0.0475953566118697		[learning rate: 0.00085021]
	Learning Rate: 0.00085021
	LOSS [training: 0.045684744128627296 | validation: 0.021654256800567567]
	TIME [epoch: 8.8 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03506403363567886		[learning rate: 0.00084918]
		[batch 20/20] avg loss: 0.02691415165818615		[learning rate: 0.00084815]
	Learning Rate: 0.000848152
	LOSS [training: 0.030989092646932496 | validation: 0.041391162196627085]
	TIME [epoch: 8.78 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034585463890785116		[learning rate: 0.00084712]
		[batch 20/20] avg loss: 0.02757880363786176		[learning rate: 0.0008461]
	Learning Rate: 0.000846099
	LOSS [training: 0.03108213376432344 | validation: 0.023090890003053754]
	TIME [epoch: 8.78 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022758354036022783		[learning rate: 0.00084507]
		[batch 20/20] avg loss: 0.03334274989738372		[learning rate: 0.00084405]
	Learning Rate: 0.000844051
	LOSS [training: 0.02805055196670326 | validation: 0.026985896289413537]
	TIME [epoch: 8.78 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03003830286225185		[learning rate: 0.00084303]
		[batch 20/20] avg loss: 0.02706572000554181		[learning rate: 0.00084201]
	Learning Rate: 0.000842007
	LOSS [training: 0.028552011433896834 | validation: 0.02016721717496532]
	TIME [epoch: 8.78 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021702157753128393		[learning rate: 0.00084099]
		[batch 20/20] avg loss: 0.034116942993552904		[learning rate: 0.00083997]
	Learning Rate: 0.000839969
	LOSS [training: 0.02790955037334065 | validation: 0.030896353160169688]
	TIME [epoch: 8.79 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045520000489033416		[learning rate: 0.00083895]
		[batch 20/20] avg loss: 0.03560834742981543		[learning rate: 0.00083794]
	Learning Rate: 0.000837935
	LOSS [training: 0.040564173959424424 | validation: 0.02727997648905899]
	TIME [epoch: 8.77 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025377611198630624		[learning rate: 0.00083692]
		[batch 20/20] avg loss: 0.035437490906828684		[learning rate: 0.00083591]
	Learning Rate: 0.000835907
	LOSS [training: 0.03040755105272966 | validation: 0.01567361565753303]
	TIME [epoch: 8.78 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025461965799816764		[learning rate: 0.00083489]
		[batch 20/20] avg loss: 0.029614777316338037		[learning rate: 0.00083388]
	Learning Rate: 0.000833883
	LOSS [training: 0.027538371558077406 | validation: 0.02490539668166603]
	TIME [epoch: 8.78 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024060405002729733		[learning rate: 0.00083287]
		[batch 20/20] avg loss: 0.052715797829493385		[learning rate: 0.00083186]
	Learning Rate: 0.000831865
	LOSS [training: 0.038388101416111556 | validation: 0.04954644909485953]
	TIME [epoch: 8.79 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037151302874329155		[learning rate: 0.00083086]
		[batch 20/20] avg loss: 0.022074198017078807		[learning rate: 0.00082985]
	Learning Rate: 0.000829851
	LOSS [training: 0.029612750445703984 | validation: 0.022296095674565662]
	TIME [epoch: 8.79 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02684868648099059		[learning rate: 0.00082885]
		[batch 20/20] avg loss: 0.03557697843726877		[learning rate: 0.00082784]
	Learning Rate: 0.000827842
	LOSS [training: 0.03121283245912968 | validation: 0.03306169910832514]
	TIME [epoch: 8.78 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03394121267927111		[learning rate: 0.00082684]
		[batch 20/20] avg loss: 0.05123784223894888		[learning rate: 0.00082584]
	Learning Rate: 0.000825838
	LOSS [training: 0.04258952745911 | validation: 0.03457019575244241]
	TIME [epoch: 8.77 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03893567215247557		[learning rate: 0.00082484]
		[batch 20/20] avg loss: 0.03018176298112757		[learning rate: 0.00082384]
	Learning Rate: 0.000823839
	LOSS [training: 0.034558717566801574 | validation: 0.02725903891331096]
	TIME [epoch: 8.78 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024442916341282737		[learning rate: 0.00082284]
		[batch 20/20] avg loss: 0.03244153531193063		[learning rate: 0.00082184]
	Learning Rate: 0.000821844
	LOSS [training: 0.02844222582660668 | validation: 0.03123065895967341]
	TIME [epoch: 8.79 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02841883697758864		[learning rate: 0.00082085]
		[batch 20/20] avg loss: 0.02758368846644812		[learning rate: 0.00081985]
	Learning Rate: 0.000819855
	LOSS [training: 0.02800126272201838 | validation: 0.019657588186966175]
	TIME [epoch: 8.79 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02125814784159672		[learning rate: 0.00081886]
		[batch 20/20] avg loss: 0.0375723018056542		[learning rate: 0.00081787]
	Learning Rate: 0.00081787
	LOSS [training: 0.029415224823625456 | validation: 0.0434661352861994]
	TIME [epoch: 8.78 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040482430394549025		[learning rate: 0.00081688]
		[batch 20/20] avg loss: 0.02603541965728859		[learning rate: 0.00081589]
	Learning Rate: 0.00081589
	LOSS [training: 0.03325892502591882 | validation: 0.011014370091911403]
	TIME [epoch: 8.78 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025873423352127334		[learning rate: 0.0008149]
		[batch 20/20] avg loss: 0.02063015070805883		[learning rate: 0.00081391]
	Learning Rate: 0.000813915
	LOSS [training: 0.02325178703009308 | validation: 0.017100098103422348]
	TIME [epoch: 8.78 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020769215442969936		[learning rate: 0.00081293]
		[batch 20/20] avg loss: 0.033982490126011886		[learning rate: 0.00081194]
	Learning Rate: 0.000811944
	LOSS [training: 0.027375852784490916 | validation: 0.026407788720957955]
	TIME [epoch: 8.8 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025460533133824348		[learning rate: 0.00081096]
		[batch 20/20] avg loss: 0.0303894221184923		[learning rate: 0.00080998]
	Learning Rate: 0.000809979
	LOSS [training: 0.02792497762615832 | validation: 0.022063571023815363]
	TIME [epoch: 8.78 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027852571519294565		[learning rate: 0.000809]
		[batch 20/20] avg loss: 0.03626279243530256		[learning rate: 0.00080802]
	Learning Rate: 0.000808018
	LOSS [training: 0.032057681977298566 | validation: 0.06270556954971498]
	TIME [epoch: 8.78 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04685958943755823		[learning rate: 0.00080704]
		[batch 20/20] avg loss: 0.02808318912226141		[learning rate: 0.00080606]
	Learning Rate: 0.000806062
	LOSS [training: 0.03747138927990982 | validation: 0.029058229298618735]
	TIME [epoch: 8.78 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029900145459572775		[learning rate: 0.00080509]
		[batch 20/20] avg loss: 0.03085595481502675		[learning rate: 0.00080411]
	Learning Rate: 0.000804111
	LOSS [training: 0.030378050137299762 | validation: 0.03214466775996132]
	TIME [epoch: 8.79 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043017732923715184		[learning rate: 0.00080314]
		[batch 20/20] avg loss: 0.03900731627433721		[learning rate: 0.00080216]
	Learning Rate: 0.000802164
	LOSS [training: 0.041012524599026205 | validation: 0.034883477480661895]
	TIME [epoch: 8.8 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033816316421077706		[learning rate: 0.00080119]
		[batch 20/20] avg loss: 0.02450415801873932		[learning rate: 0.00080022]
	Learning Rate: 0.000800222
	LOSS [training: 0.029160237219908514 | validation: 0.024705622677596552]
	TIME [epoch: 8.77 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023057799166405896		[learning rate: 0.00079925]
		[batch 20/20] avg loss: 0.02663978796871026		[learning rate: 0.00079828]
	Learning Rate: 0.000798285
	LOSS [training: 0.024848793567558078 | validation: 0.03763656001279602]
	TIME [epoch: 8.78 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03567229886473228		[learning rate: 0.00079732]
		[batch 20/20] avg loss: 0.039473615016906105		[learning rate: 0.00079635]
	Learning Rate: 0.000796352
	LOSS [training: 0.03757295694081919 | validation: 0.049615976475720726]
	TIME [epoch: 8.78 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03141565609203641		[learning rate: 0.00079539]
		[batch 20/20] avg loss: 0.025898845563749207		[learning rate: 0.00079442]
	Learning Rate: 0.000794424
	LOSS [training: 0.028657250827892812 | validation: 0.03385404486087511]
	TIME [epoch: 8.8 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03739929072024136		[learning rate: 0.00079346]
		[batch 20/20] avg loss: 0.034319268523800475		[learning rate: 0.0007925]
	Learning Rate: 0.000792501
	LOSS [training: 0.03585927962202092 | validation: 0.03888847351799933]
	TIME [epoch: 8.78 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03018334209619279		[learning rate: 0.00079154]
		[batch 20/20] avg loss: 0.04114039672064709		[learning rate: 0.00079058]
	Learning Rate: 0.000790583
	LOSS [training: 0.03566186940841994 | validation: 0.023713519468566483]
	TIME [epoch: 8.78 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025975759249895154		[learning rate: 0.00078963]
		[batch 20/20] avg loss: 0.032430235059857385		[learning rate: 0.00078867]
	Learning Rate: 0.000788669
	LOSS [training: 0.029202997154876266 | validation: 0.02842533646926597]
	TIME [epoch: 8.77 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0287498826560863		[learning rate: 0.00078771]
		[batch 20/20] avg loss: 0.02098469959393901		[learning rate: 0.00078676]
	Learning Rate: 0.00078676
	LOSS [training: 0.024867291125012653 | validation: 0.023180302056365776]
	TIME [epoch: 8.79 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027717096395900243		[learning rate: 0.00078581]
		[batch 20/20] avg loss: 0.01933688204164464		[learning rate: 0.00078486]
	Learning Rate: 0.000784855
	LOSS [training: 0.023526989218772436 | validation: 0.01532962780173613]
	TIME [epoch: 8.79 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02981704738228997		[learning rate: 0.0007839]
		[batch 20/20] avg loss: 0.029055752023280663		[learning rate: 0.00078296]
	Learning Rate: 0.000782955
	LOSS [training: 0.029436399702785315 | validation: 0.018477845531968837]
	TIME [epoch: 8.79 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039175227455593785		[learning rate: 0.00078201]
		[batch 20/20] avg loss: 0.01901257984760754		[learning rate: 0.00078106]
	Learning Rate: 0.00078106
	LOSS [training: 0.029093903651600674 | validation: 0.01592670027893129]
	TIME [epoch: 8.78 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017090430717729845		[learning rate: 0.00078011]
		[batch 20/20] avg loss: 0.023205113077357866		[learning rate: 0.00077917]
	Learning Rate: 0.000779169
	LOSS [training: 0.02014777189754386 | validation: 0.026118217757475096]
	TIME [epoch: 8.79 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03661258486410314		[learning rate: 0.00077823]
		[batch 20/20] avg loss: 0.032929638968968895		[learning rate: 0.00077728]
	Learning Rate: 0.000777283
	LOSS [training: 0.03477111191653602 | validation: 0.032725420131068994]
	TIME [epoch: 8.78 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0392044521158068		[learning rate: 0.00077634]
		[batch 20/20] avg loss: 0.037891663301908685		[learning rate: 0.0007754]
	Learning Rate: 0.000775401
	LOSS [training: 0.03854805770885775 | validation: 0.023991303705414985]
	TIME [epoch: 8.8 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034032031196245566		[learning rate: 0.00077446]
		[batch 20/20] avg loss: 0.024435611993260324		[learning rate: 0.00077352]
	Learning Rate: 0.000773524
	LOSS [training: 0.02923382159475294 | validation: 0.026028194536436903]
	TIME [epoch: 8.78 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020673520251400065		[learning rate: 0.00077259]
		[batch 20/20] avg loss: 0.03642878120510595		[learning rate: 0.00077165]
	Learning Rate: 0.000771651
	LOSS [training: 0.028551150728253004 | validation: 0.048940981054454775]
	TIME [epoch: 8.77 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039384549410043074		[learning rate: 0.00077072]
		[batch 20/20] avg loss: 0.028348265270201067		[learning rate: 0.00076978]
	Learning Rate: 0.000769783
	LOSS [training: 0.03386640734012207 | validation: 0.045216032117638444]
	TIME [epoch: 8.78 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02385817702487612		[learning rate: 0.00076885]
		[batch 20/20] avg loss: 0.01904877983752269		[learning rate: 0.00076792]
	Learning Rate: 0.00076792
	LOSS [training: 0.021453478431199408 | validation: 0.024123353961259742]
	TIME [epoch: 8.79 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02651774387373778		[learning rate: 0.00076699]
		[batch 20/20] avg loss: 0.04444326072984691		[learning rate: 0.00076606]
	Learning Rate: 0.000766061
	LOSS [training: 0.03548050230179235 | validation: 0.026060087450123483]
	TIME [epoch: 8.79 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034130422447322326		[learning rate: 0.00076513]
		[batch 20/20] avg loss: 0.02539705075992446		[learning rate: 0.00076421]
	Learning Rate: 0.000764206
	LOSS [training: 0.029763736603623393 | validation: 0.024578852115203005]
	TIME [epoch: 8.77 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028184799611076096		[learning rate: 0.00076328]
		[batch 20/20] avg loss: 0.029341568183772686		[learning rate: 0.00076236]
	Learning Rate: 0.000762356
	LOSS [training: 0.02876318389742439 | validation: 0.035086902039478104]
	TIME [epoch: 8.77 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03067411960739811		[learning rate: 0.00076143]
		[batch 20/20] avg loss: 0.017952928600749152		[learning rate: 0.00076051]
	Learning Rate: 0.000760511
	LOSS [training: 0.024313524104073635 | validation: 0.021498820670100702]
	TIME [epoch: 8.78 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026168350430606613		[learning rate: 0.00075959]
		[batch 20/20] avg loss: 0.02970865648064016		[learning rate: 0.00075867]
	Learning Rate: 0.00075867
	LOSS [training: 0.027938503455623386 | validation: 0.05035230087051526]
	TIME [epoch: 8.79 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027069063304924857		[learning rate: 0.00075775]
		[batch 20/20] avg loss: 0.02625966842721745		[learning rate: 0.00075683]
	Learning Rate: 0.000756833
	LOSS [training: 0.026664365866071148 | validation: 0.02548908432178123]
	TIME [epoch: 8.79 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031211264513386027		[learning rate: 0.00075592]
		[batch 20/20] avg loss: 0.03066046573945101		[learning rate: 0.000755]
	Learning Rate: 0.000755001
	LOSS [training: 0.030935865126418514 | validation: 0.0338254603244712]
	TIME [epoch: 8.8 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040407509099466124		[learning rate: 0.00075409]
		[batch 20/20] avg loss: 0.018749050033292657		[learning rate: 0.00075317]
	Learning Rate: 0.000753173
	LOSS [training: 0.02957827956637939 | validation: 0.021275348219940628]
	TIME [epoch: 8.79 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033885393906820244		[learning rate: 0.00075226]
		[batch 20/20] avg loss: 0.028860986291710578		[learning rate: 0.00075135]
	Learning Rate: 0.00075135
	LOSS [training: 0.03137319009926541 | validation: 0.02717120371533679]
	TIME [epoch: 8.79 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03513617008576783		[learning rate: 0.00075044]
		[batch 20/20] avg loss: 0.04299474679933636		[learning rate: 0.00074953]
	Learning Rate: 0.000749531
	LOSS [training: 0.03906545844255209 | validation: 0.030870543782809366]
	TIME [epoch: 8.79 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026096478941107566		[learning rate: 0.00074862]
		[batch 20/20] avg loss: 0.027416117454186623		[learning rate: 0.00074772]
	Learning Rate: 0.000747716
	LOSS [training: 0.026756298197647094 | validation: 0.017210125338564468]
	TIME [epoch: 8.78 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02991507607576725		[learning rate: 0.00074681]
		[batch 20/20] avg loss: 0.029934888595995614		[learning rate: 0.00074591]
	Learning Rate: 0.000745906
	LOSS [training: 0.02992498233588143 | validation: 0.042007321098119596]
	TIME [epoch: 8.77 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04134307765658597		[learning rate: 0.000745]
		[batch 20/20] avg loss: 0.03530381118690975		[learning rate: 0.0007441]
	Learning Rate: 0.0007441
	LOSS [training: 0.03832344442174786 | validation: 0.038823333978599994]
	TIME [epoch: 8.79 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030298464668487325		[learning rate: 0.0007432]
		[batch 20/20] avg loss: 0.017367464278985334		[learning rate: 0.0007423]
	Learning Rate: 0.000742299
	LOSS [training: 0.02383296447373633 | validation: 0.0070587466166678]
	TIME [epoch: 8.8 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03258136955988829		[learning rate: 0.0007414]
		[batch 20/20] avg loss: 0.038469620301064614		[learning rate: 0.0007405]
	Learning Rate: 0.000740502
	LOSS [training: 0.03552549493047645 | validation: 0.03873350124300065]
	TIME [epoch: 8.79 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02618122651126791		[learning rate: 0.00073961]
		[batch 20/20] avg loss: 0.028999695952103145		[learning rate: 0.00073871]
	Learning Rate: 0.000738709
	LOSS [training: 0.027590461231685525 | validation: 0.02768857559984679]
	TIME [epoch: 8.78 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023361315295608854		[learning rate: 0.00073781]
		[batch 20/20] avg loss: 0.047068877194085604		[learning rate: 0.00073692]
	Learning Rate: 0.000736921
	LOSS [training: 0.035215096244847234 | validation: 0.024394003747376925]
	TIME [epoch: 8.78 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02812563213473957		[learning rate: 0.00073603]
		[batch 20/20] avg loss: 0.023454588707082162		[learning rate: 0.00073514]
	Learning Rate: 0.000735137
	LOSS [training: 0.02579011042091086 | validation: 0.04652184326804312]
	TIME [epoch: 8.77 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028060428330035414		[learning rate: 0.00073425]
		[batch 20/20] avg loss: 0.020793179296620147		[learning rate: 0.00073336]
	Learning Rate: 0.000733358
	LOSS [training: 0.02442680381332778 | validation: 0.0410607525478292]
	TIME [epoch: 8.79 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044716969946478315		[learning rate: 0.00073247]
		[batch 20/20] avg loss: 0.03150471769513613		[learning rate: 0.00073158]
	Learning Rate: 0.000731582
	LOSS [training: 0.038110843820807226 | validation: 0.05589617557178084]
	TIME [epoch: 8.81 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025521957369921466		[learning rate: 0.0007307]
		[batch 20/20] avg loss: 0.02493724864806594		[learning rate: 0.00072981]
	Learning Rate: 0.000729811
	LOSS [training: 0.025229603008993705 | validation: 0.03892356650884704]
	TIME [epoch: 8.8 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0414339459386187		[learning rate: 0.00072893]
		[batch 20/20] avg loss: 0.030328514354306568		[learning rate: 0.00072804]
	Learning Rate: 0.000728044
	LOSS [training: 0.03588123014646264 | validation: 0.05228794713813249]
	TIME [epoch: 8.8 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029506948748513517		[learning rate: 0.00072716]
		[batch 20/20] avg loss: 0.021217198006316548		[learning rate: 0.00072628]
	Learning Rate: 0.000726282
	LOSS [training: 0.02536207337741503 | validation: 0.016837737620199626]
	TIME [epoch: 8.78 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027190765072145		[learning rate: 0.0007254]
		[batch 20/20] avg loss: 0.036950045211647665		[learning rate: 0.00072452]
	Learning Rate: 0.000724524
	LOSS [training: 0.03207040514189633 | validation: 0.02150644534055325]
	TIME [epoch: 8.8 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020216143229413085		[learning rate: 0.00072365]
		[batch 20/20] avg loss: 0.024082341338753626		[learning rate: 0.00072277]
	Learning Rate: 0.00072277
	LOSS [training: 0.022149242284083357 | validation: 0.026080744639555044]
	TIME [epoch: 8.78 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026196310839287983		[learning rate: 0.00072189]
		[batch 20/20] avg loss: 0.026148536008811958		[learning rate: 0.00072102]
	Learning Rate: 0.00072102
	LOSS [training: 0.026172423424049972 | validation: 0.02024902641868452]
	TIME [epoch: 8.82 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03351462616093581		[learning rate: 0.00072015]
		[batch 20/20] avg loss: 0.02167280247556895		[learning rate: 0.00071927]
	Learning Rate: 0.000719275
	LOSS [training: 0.02759371431825238 | validation: 0.033380349410250386]
	TIME [epoch: 8.87 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02545469723811477		[learning rate: 0.0007184]
		[batch 20/20] avg loss: 0.028905538823963302		[learning rate: 0.00071753]
	Learning Rate: 0.000717533
	LOSS [training: 0.027180118031039036 | validation: 0.02901551597692868]
	TIME [epoch: 8.91 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03385995026317701		[learning rate: 0.00071666]
		[batch 20/20] avg loss: 0.028312136364621043		[learning rate: 0.0007158]
	Learning Rate: 0.000715796
	LOSS [training: 0.031086043313899032 | validation: 0.025088012841104013]
	TIME [epoch: 8.94 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02352475144016334		[learning rate: 0.00071493]
		[batch 20/20] avg loss: 0.028777469868213685		[learning rate: 0.00071406]
	Learning Rate: 0.000714064
	LOSS [training: 0.026151110654188507 | validation: 0.02573010408118608]
	TIME [epoch: 8.89 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027865003275826992		[learning rate: 0.0007132]
		[batch 20/20] avg loss: 0.03268126202300184		[learning rate: 0.00071233]
	Learning Rate: 0.000712335
	LOSS [training: 0.030273132649414412 | validation: 0.023121795282678725]
	TIME [epoch: 8.79 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0269385507265835		[learning rate: 0.00071147]
		[batch 20/20] avg loss: 0.02979374252601319		[learning rate: 0.00071061]
	Learning Rate: 0.00071061
	LOSS [training: 0.02836614662629835 | validation: 0.034486205441386614]
	TIME [epoch: 8.77 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02697534588008072		[learning rate: 0.00070975]
		[batch 20/20] avg loss: 0.039962599706483845		[learning rate: 0.00070889]
	Learning Rate: 0.00070889
	LOSS [training: 0.033468972793282294 | validation: 0.04807103963520492]
	TIME [epoch: 8.8 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033753833288319865		[learning rate: 0.00070803]
		[batch 20/20] avg loss: 0.025009910550283804		[learning rate: 0.00070717]
	Learning Rate: 0.000707174
	LOSS [training: 0.029381871919301833 | validation: 0.019996165188188236]
	TIME [epoch: 8.8 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01676437244114123		[learning rate: 0.00070632]
		[batch 20/20] avg loss: 0.020292316395747006		[learning rate: 0.00070546]
	Learning Rate: 0.000705462
	LOSS [training: 0.018528344418444122 | validation: 0.023869902453811834]
	TIME [epoch: 8.79 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02200165672767605		[learning rate: 0.00070461]
		[batch 20/20] avg loss: 0.03183665393646892		[learning rate: 0.00070375]
	Learning Rate: 0.000703754
	LOSS [training: 0.026919155332072488 | validation: 0.027634878885636444]
	TIME [epoch: 8.8 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026034401678279723		[learning rate: 0.0007029]
		[batch 20/20] avg loss: 0.021595553343817256		[learning rate: 0.00070205]
	Learning Rate: 0.000702051
	LOSS [training: 0.02381497751104849 | validation: 0.026388512308142185]
	TIME [epoch: 8.78 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018359055366958844		[learning rate: 0.0007012]
		[batch 20/20] avg loss: 0.018301579990893012		[learning rate: 0.00070035]
	Learning Rate: 0.000700351
	LOSS [training: 0.018330317678925925 | validation: 0.034325583649142954]
	TIME [epoch: 8.8 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02069098691185667		[learning rate: 0.0006995]
		[batch 20/20] avg loss: 0.029031512157589097		[learning rate: 0.00069866]
	Learning Rate: 0.000698656
	LOSS [training: 0.024861249534722882 | validation: 0.021233857149242707]
	TIME [epoch: 8.79 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03270137587702277		[learning rate: 0.00069781]
		[batch 20/20] avg loss: 0.020432870647784956		[learning rate: 0.00069696]
	Learning Rate: 0.000696964
	LOSS [training: 0.026567123262403864 | validation: 0.04057367919643697]
	TIME [epoch: 8.78 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019829190123193736		[learning rate: 0.00069612]
		[batch 20/20] avg loss: 0.02005264658982177		[learning rate: 0.00069528]
	Learning Rate: 0.000695277
	LOSS [training: 0.019940918356507752 | validation: 0.02523577461772482]
	TIME [epoch: 8.79 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030806810823708647		[learning rate: 0.00069444]
		[batch 20/20] avg loss: 0.015390271858457635		[learning rate: 0.00069359]
	Learning Rate: 0.000693594
	LOSS [training: 0.02309854134108314 | validation: 0.035451182887934946]
	TIME [epoch: 8.77 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024625118884619965		[learning rate: 0.00069275]
		[batch 20/20] avg loss: 0.03267912439181922		[learning rate: 0.00069191]
	Learning Rate: 0.000691915
	LOSS [training: 0.02865212163821959 | validation: 0.035102978375625316]
	TIME [epoch: 8.81 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0267816056277679		[learning rate: 0.00069108]
		[batch 20/20] avg loss: 0.060645396849159906		[learning rate: 0.00069024]
	Learning Rate: 0.00069024
	LOSS [training: 0.0437135012384639 | validation: 0.024857878696466718]
	TIME [epoch: 8.79 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025055359090508433		[learning rate: 0.0006894]
		[batch 20/20] avg loss: 0.039626098148525996		[learning rate: 0.00068857]
	Learning Rate: 0.000688569
	LOSS [training: 0.032340728619517214 | validation: 0.01188664538570274]
	TIME [epoch: 8.78 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026148777052610317		[learning rate: 0.00068773]
		[batch 20/20] avg loss: 0.03188077109709458		[learning rate: 0.0006869]
	Learning Rate: 0.000686902
	LOSS [training: 0.02901477407485245 | validation: 0.02573520843459439]
	TIME [epoch: 8.78 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026592361597725704		[learning rate: 0.00068607]
		[batch 20/20] avg loss: 0.022036216710576047		[learning rate: 0.00068524]
	Learning Rate: 0.000685239
	LOSS [training: 0.024314289154150875 | validation: 0.03814846154865446]
	TIME [epoch: 8.79 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02352586962025352		[learning rate: 0.00068441]
		[batch 20/20] avg loss: 0.040057755619008976		[learning rate: 0.00068358]
	Learning Rate: 0.00068358
	LOSS [training: 0.03179181261963125 | validation: 0.019316048637401936]
	TIME [epoch: 8.81 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025704248321450414		[learning rate: 0.00068275]
		[batch 20/20] avg loss: 0.0318821305903776		[learning rate: 0.00068193]
	Learning Rate: 0.000681925
	LOSS [training: 0.028793189455914015 | validation: 0.017134762168843076]
	TIME [epoch: 8.79 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030797187957388162		[learning rate: 0.0006811]
		[batch 20/20] avg loss: 0.030245385682734167		[learning rate: 0.00068027]
	Learning Rate: 0.000680275
	LOSS [training: 0.03052128682006116 | validation: 0.03270990691132441]
	TIME [epoch: 8.78 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03785991687314332		[learning rate: 0.00067945]
		[batch 20/20] avg loss: 0.026621970742912494		[learning rate: 0.00067863]
	Learning Rate: 0.000678628
	LOSS [training: 0.032240943808027916 | validation: 0.049104253272349546]
	TIME [epoch: 8.79 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044169767108363844		[learning rate: 0.00067781]
		[batch 20/20] avg loss: 0.025131856852885152		[learning rate: 0.00067698]
	Learning Rate: 0.000676985
	LOSS [training: 0.0346508119806245 | validation: 0.01458250292003528]
	TIME [epoch: 8.78 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016765451766122055		[learning rate: 0.00067616]
		[batch 20/20] avg loss: 0.027719126561951014		[learning rate: 0.00067535]
	Learning Rate: 0.000675346
	LOSS [training: 0.02224228916403653 | validation: 0.022596449789769397]
	TIME [epoch: 8.8 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023274434016791086		[learning rate: 0.00067453]
		[batch 20/20] avg loss: 0.04436352209268109		[learning rate: 0.00067371]
	Learning Rate: 0.000673711
	LOSS [training: 0.03381897805473609 | validation: 0.021311085210402803]
	TIME [epoch: 8.78 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03377427023537531		[learning rate: 0.0006729]
		[batch 20/20] avg loss: 0.026623601906761385		[learning rate: 0.00067208]
	Learning Rate: 0.00067208
	LOSS [training: 0.030198936071068348 | validation: 0.016033487246412514]
	TIME [epoch: 8.78 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04456488692818161		[learning rate: 0.00067127]
		[batch 20/20] avg loss: 0.038099619821907384		[learning rate: 0.00067045]
	Learning Rate: 0.000670453
	LOSS [training: 0.041332253375044495 | validation: 0.039529434632434475]
	TIME [epoch: 8.78 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0371466448213928		[learning rate: 0.00066964]
		[batch 20/20] avg loss: 0.028880153911287815		[learning rate: 0.00066883]
	Learning Rate: 0.00066883
	LOSS [training: 0.033013399366340304 | validation: 0.029480568296438005]
	TIME [epoch: 8.8 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026641298287272804		[learning rate: 0.00066802]
		[batch 20/20] avg loss: 0.027122131034066476		[learning rate: 0.00066721]
	Learning Rate: 0.000667211
	LOSS [training: 0.026881714660669638 | validation: 0.02214759610755781]
	TIME [epoch: 8.78 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01967872936939704		[learning rate: 0.0006664]
		[batch 20/20] avg loss: 0.033565854928548286		[learning rate: 0.0006656]
	Learning Rate: 0.000665596
	LOSS [training: 0.02662229214897266 | validation: 0.06546910210115661]
	TIME [epoch: 8.78 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03811624685455015		[learning rate: 0.00066479]
		[batch 20/20] avg loss: 0.024304712640423894		[learning rate: 0.00066398]
	Learning Rate: 0.000663984
	LOSS [training: 0.03121047974748702 | validation: 0.01643858017059306]
	TIME [epoch: 8.78 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026054988638505035		[learning rate: 0.00066318]
		[batch 20/20] avg loss: 0.03307548199667276		[learning rate: 0.00066238]
	Learning Rate: 0.000662377
	LOSS [training: 0.029565235317588895 | validation: 0.03937070940550538]
	TIME [epoch: 8.78 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02925589839605829		[learning rate: 0.00066157]
		[batch 20/20] avg loss: 0.026336690259559814		[learning rate: 0.00066077]
	Learning Rate: 0.000660773
	LOSS [training: 0.02779629432780905 | validation: 0.020461105754134616]
	TIME [epoch: 8.81 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019894129945223195		[learning rate: 0.00065997]
		[batch 20/20] avg loss: 0.022766808324822148		[learning rate: 0.00065917]
	Learning Rate: 0.000659174
	LOSS [training: 0.02133046913502267 | validation: 0.03255989688750643]
	TIME [epoch: 8.79 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028533479111051974		[learning rate: 0.00065838]
		[batch 20/20] avg loss: 0.03512808133747902		[learning rate: 0.00065758]
	Learning Rate: 0.000657578
	LOSS [training: 0.031830780224265505 | validation: 0.03695023690978107]
	TIME [epoch: 8.78 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022511536854809406		[learning rate: 0.00065678]
		[batch 20/20] avg loss: 0.03371860337130676		[learning rate: 0.00065599]
	Learning Rate: 0.000655986
	LOSS [training: 0.028115070113058083 | validation: 0.01282837072469392]
	TIME [epoch: 8.78 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013371955294963442		[learning rate: 0.00065519]
		[batch 20/20] avg loss: 0.024928718214499417		[learning rate: 0.0006544]
	Learning Rate: 0.000654398
	LOSS [training: 0.019150336754731427 | validation: 0.02169183789103145]
	TIME [epoch: 8.78 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019405627867562943		[learning rate: 0.00065361]
		[batch 20/20] avg loss: 0.03486448555822753		[learning rate: 0.00065281]
	Learning Rate: 0.000652814
	LOSS [training: 0.027135056712895234 | validation: 0.013725805295559747]
	TIME [epoch: 8.79 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028473570319806452		[learning rate: 0.00065202]
		[batch 20/20] avg loss: 0.01953365127702304		[learning rate: 0.00065123]
	Learning Rate: 0.000651234
	LOSS [training: 0.024003610798414742 | validation: 0.008906991627326208]
	TIME [epoch: 8.78 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025536650385118754		[learning rate: 0.00065045]
		[batch 20/20] avg loss: 0.02286231368427308		[learning rate: 0.00064966]
	Learning Rate: 0.000649657
	LOSS [training: 0.024199482034695914 | validation: 0.01353244524403382]
	TIME [epoch: 8.78 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013540340078430257		[learning rate: 0.00064887]
		[batch 20/20] avg loss: 0.03369910945467779		[learning rate: 0.00064808]
	Learning Rate: 0.000648084
	LOSS [training: 0.02361972476655402 | validation: 0.024449319587320967]
	TIME [epoch: 8.79 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025809448935182666		[learning rate: 0.0006473]
		[batch 20/20] avg loss: 0.03438262295220114		[learning rate: 0.00064652]
	Learning Rate: 0.000646515
	LOSS [training: 0.030096035943691906 | validation: 0.0180914622430423]
	TIME [epoch: 8.8 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02805067675314842		[learning rate: 0.00064573]
		[batch 20/20] avg loss: 0.022865033993799543		[learning rate: 0.00064495]
	Learning Rate: 0.00064495
	LOSS [training: 0.025457855373473975 | validation: 0.015706367937478868]
	TIME [epoch: 8.79 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02002304036788543		[learning rate: 0.00064417]
		[batch 20/20] avg loss: 0.012563619109047635		[learning rate: 0.00064339]
	Learning Rate: 0.000643389
	LOSS [training: 0.01629332973846653 | validation: 0.019214015971004406]
	TIME [epoch: 8.78 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009585633369692096		[learning rate: 0.00064261]
		[batch 20/20] avg loss: 0.023945351908839663		[learning rate: 0.00064183]
	Learning Rate: 0.000641832
	LOSS [training: 0.016765492639265876 | validation: 0.026830527391298365]
	TIME [epoch: 8.79 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03141516126147794		[learning rate: 0.00064105]
		[batch 20/20] avg loss: 0.017492382446144254		[learning rate: 0.00064028]
	Learning Rate: 0.000640278
	LOSS [training: 0.024453771853811097 | validation: 0.025275955927856558]
	TIME [epoch: 8.81 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011758126002594033		[learning rate: 0.0006395]
		[batch 20/20] avg loss: 0.02717203389876475		[learning rate: 0.00063873]
	Learning Rate: 0.000638728
	LOSS [training: 0.019465079950679393 | validation: 0.014806399217543904]
	TIME [epoch: 8.82 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029332896794775666		[learning rate: 0.00063795]
		[batch 20/20] avg loss: 0.028819246799724446		[learning rate: 0.00063718]
	Learning Rate: 0.000637182
	LOSS [training: 0.02907607179725006 | validation: 0.049958322332306696]
	TIME [epoch: 8.79 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019186488766712823		[learning rate: 0.00063641]
		[batch 20/20] avg loss: 0.023160514721085695		[learning rate: 0.00063564]
	Learning Rate: 0.000635639
	LOSS [training: 0.02117350174389926 | validation: 0.013371816269660271]
	TIME [epoch: 8.78 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024781661334914565		[learning rate: 0.00063487]
		[batch 20/20] avg loss: 0.029465472742303746		[learning rate: 0.0006341]
	Learning Rate: 0.0006341
	LOSS [training: 0.027123567038609152 | validation: 0.024964051343154714]
	TIME [epoch: 8.78 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023445693484011522		[learning rate: 0.00063333]
		[batch 20/20] avg loss: 0.02621475255926701		[learning rate: 0.00063257]
	Learning Rate: 0.000632565
	LOSS [training: 0.024830223021639265 | validation: 0.01591216373459478]
	TIME [epoch: 8.78 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017252334478601507		[learning rate: 0.0006318]
		[batch 20/20] avg loss: 0.018787019706463848		[learning rate: 0.00063103]
	Learning Rate: 0.000631034
	LOSS [training: 0.01801967709253268 | validation: 0.031247754386810704]
	TIME [epoch: 8.81 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032051490693012746		[learning rate: 0.00063027]
		[batch 20/20] avg loss: 0.021589502145072447		[learning rate: 0.00062951]
	Learning Rate: 0.000629506
	LOSS [training: 0.026820496419042595 | validation: 0.026839952126356047]
	TIME [epoch: 8.78 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01910144910209738		[learning rate: 0.00062874]
		[batch 20/20] avg loss: 0.04056003711714335		[learning rate: 0.00062798]
	Learning Rate: 0.000627982
	LOSS [training: 0.02983074310962036 | validation: 0.02138554824350886]
	TIME [epoch: 8.78 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01864109598150645		[learning rate: 0.00062722]
		[batch 20/20] avg loss: 0.017038944960099365		[learning rate: 0.00062646]
	Learning Rate: 0.000626462
	LOSS [training: 0.017840020470802905 | validation: 0.02363437234088724]
	TIME [epoch: 8.78 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02306966092049036		[learning rate: 0.0006257]
		[batch 20/20] avg loss: 0.022890703209522693		[learning rate: 0.00062495]
	Learning Rate: 0.000624946
	LOSS [training: 0.022980182065006523 | validation: 0.027576877146015175]
	TIME [epoch: 8.79 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03127763004577743		[learning rate: 0.00062419]
		[batch 20/20] avg loss: 0.02146051695336717		[learning rate: 0.00062343]
	Learning Rate: 0.000623433
	LOSS [training: 0.026369073499572303 | validation: 0.011007612939297338]
	TIME [epoch: 8.78 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02611463058484511		[learning rate: 0.00062268]
		[batch 20/20] avg loss: 0.019126846337983904		[learning rate: 0.00062192]
	Learning Rate: 0.000621923
	LOSS [training: 0.02262073846141451 | validation: 0.0068504861582633775]
	TIME [epoch: 8.78 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01670087522881827		[learning rate: 0.00062117]
		[batch 20/20] avg loss: 0.02455722998075461		[learning rate: 0.00062042]
	Learning Rate: 0.000620418
	LOSS [training: 0.02062905260478644 | validation: 0.03504492087388803]
	TIME [epoch: 8.79 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03525123695816523		[learning rate: 0.00061967]
		[batch 20/20] avg loss: 0.031215839491257103		[learning rate: 0.00061892]
	Learning Rate: 0.000618916
	LOSS [training: 0.033233538224711166 | validation: 0.042074366831435624]
	TIME [epoch: 8.79 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03400216449922259		[learning rate: 0.00061817]
		[batch 20/20] avg loss: 0.02728341171704924		[learning rate: 0.00061742]
	Learning Rate: 0.000617418
	LOSS [training: 0.03064278810813591 | validation: 0.020966863205855005]
	TIME [epoch: 8.82 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028586479087141976		[learning rate: 0.00061667]
		[batch 20/20] avg loss: 0.022727896015003926		[learning rate: 0.00061592]
	Learning Rate: 0.000615923
	LOSS [training: 0.025657187551072946 | validation: 0.03579542064528752]
	TIME [epoch: 8.78 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029302600475127318		[learning rate: 0.00061518]
		[batch 20/20] avg loss: 0.018758245895403165		[learning rate: 0.00061443]
	Learning Rate: 0.000614432
	LOSS [training: 0.02403042318526524 | validation: 0.025138380029446992]
	TIME [epoch: 8.79 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02167780461809958		[learning rate: 0.00061369]
		[batch 20/20] avg loss: 0.02614989907380058		[learning rate: 0.00061294]
	Learning Rate: 0.000612944
	LOSS [training: 0.023913851845950088 | validation: 0.0261412011887112]
	TIME [epoch: 8.77 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029117700902707756		[learning rate: 0.0006122]
		[batch 20/20] avg loss: 0.031712231123261554		[learning rate: 0.00061146]
	Learning Rate: 0.000611461
	LOSS [training: 0.03041496601298465 | validation: 0.01618861396625366]
	TIME [epoch: 8.78 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017830198335871943		[learning rate: 0.00061072]
		[batch 20/20] avg loss: 0.021196003947625837		[learning rate: 0.00060998]
	Learning Rate: 0.00060998
	LOSS [training: 0.01951310114174889 | validation: 0.019762838952281633]
	TIME [epoch: 8.8 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02678078693165517		[learning rate: 0.00060924]
		[batch 20/20] avg loss: 0.03488393124075971		[learning rate: 0.0006085]
	Learning Rate: 0.000608504
	LOSS [training: 0.03083235908620744 | validation: 0.04864745000075352]
	TIME [epoch: 8.79 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040108520106305966		[learning rate: 0.00060777]
		[batch 20/20] avg loss: 0.03765930016981732		[learning rate: 0.00060703]
	Learning Rate: 0.00060703
	LOSS [training: 0.038883910138061646 | validation: 0.03586521113898121]
	TIME [epoch: 8.8 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03348766051659446		[learning rate: 0.0006063]
		[batch 20/20] avg loss: 0.02717387306155581		[learning rate: 0.00060556]
	Learning Rate: 0.000605561
	LOSS [training: 0.03033076678907513 | validation: 0.009795812302037002]
	TIME [epoch: 8.78 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021836327363474815		[learning rate: 0.00060483]
		[batch 20/20] avg loss: 0.02028158318261252		[learning rate: 0.00060409]
	Learning Rate: 0.000604095
	LOSS [training: 0.02105895527304367 | validation: 0.00985181180226942]
	TIME [epoch: 8.79 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01654307922799177		[learning rate: 0.00060336]
		[batch 20/20] avg loss: 0.02171572173116516		[learning rate: 0.00060263]
	Learning Rate: 0.000602633
	LOSS [training: 0.01912940047957846 | validation: 0.014057322855731812]
	TIME [epoch: 8.79 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02036107197535495		[learning rate: 0.0006019]
		[batch 20/20] avg loss: 0.014435398219448017		[learning rate: 0.00060117]
	Learning Rate: 0.000601174
	LOSS [training: 0.017398235097401488 | validation: 0.011107901056385238]
	TIME [epoch: 8.79 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017955432582998927		[learning rate: 0.00060045]
		[batch 20/20] avg loss: 0.02500553562546278		[learning rate: 0.00059972]
	Learning Rate: 0.000599718
	LOSS [training: 0.021480484104230858 | validation: 0.028857552578729156]
	TIME [epoch: 8.79 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017872423108747208		[learning rate: 0.00059899]
		[batch 20/20] avg loss: 0.013734335553810512		[learning rate: 0.00059827]
	Learning Rate: 0.000598267
	LOSS [training: 0.01580337933127886 | validation: 0.03026412875508925]
	TIME [epoch: 8.79 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033742177604489776		[learning rate: 0.00059754]
		[batch 20/20] avg loss: 0.021172571669723694		[learning rate: 0.00059682]
	Learning Rate: 0.000596818
	LOSS [training: 0.027457374637106735 | validation: 0.0182643193152259]
	TIME [epoch: 8.8 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022390638836079144		[learning rate: 0.0005961]
		[batch 20/20] avg loss: 0.02114053769084405		[learning rate: 0.00059537]
	Learning Rate: 0.000595373
	LOSS [training: 0.021765588263461595 | validation: 0.015315715881478934]
	TIME [epoch: 8.8 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027024340022104566		[learning rate: 0.00059465]
		[batch 20/20] avg loss: 0.020960793183416346		[learning rate: 0.00059393]
	Learning Rate: 0.000593932
	LOSS [training: 0.023992566602760456 | validation: 0.019110159980324237]
	TIME [epoch: 8.79 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027866403756775314		[learning rate: 0.00059321]
		[batch 20/20] avg loss: 0.02060851224883649		[learning rate: 0.00059249]
	Learning Rate: 0.000592494
	LOSS [training: 0.024237458002805905 | validation: 0.011754244764681083]
	TIME [epoch: 8.79 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029918893865263196		[learning rate: 0.00059178]
		[batch 20/20] avg loss: 0.018375829277912276		[learning rate: 0.00059106]
	Learning Rate: 0.00059106
	LOSS [training: 0.02414736157158773 | validation: 0.015774076431398935]
	TIME [epoch: 8.78 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013999219236472033		[learning rate: 0.00059034]
		[batch 20/20] avg loss: 0.026839170311759496		[learning rate: 0.00058963]
	Learning Rate: 0.000589629
	LOSS [training: 0.020419194774115766 | validation: 0.01679227427948391]
	TIME [epoch: 8.81 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019562684792936966		[learning rate: 0.00058892]
		[batch 20/20] avg loss: 0.025986227840336617		[learning rate: 0.0005882]
	Learning Rate: 0.000588202
	LOSS [training: 0.022774456316636788 | validation: 0.018841186214817097]
	TIME [epoch: 8.78 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027733015244519228		[learning rate: 0.00058749]
		[batch 20/20] avg loss: 0.025569062162799406		[learning rate: 0.00058678]
	Learning Rate: 0.000586778
	LOSS [training: 0.026651038703659313 | validation: 0.016916462587089776]
	TIME [epoch: 8.79 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021601098137808545		[learning rate: 0.00058607]
		[batch 20/20] avg loss: 0.012285006633340948		[learning rate: 0.00058536]
	Learning Rate: 0.000585357
	LOSS [training: 0.016943052385574746 | validation: 0.020938938480942727]
	TIME [epoch: 8.77 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01648259418523345		[learning rate: 0.00058465]
		[batch 20/20] avg loss: 0.024909574975521962		[learning rate: 0.00058394]
	Learning Rate: 0.00058394
	LOSS [training: 0.02069608458037771 | validation: 0.026608525379966483]
	TIME [epoch: 8.79 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016883973990020704		[learning rate: 0.00058323]
		[batch 20/20] avg loss: 0.03241645323789257		[learning rate: 0.00058253]
	Learning Rate: 0.000582527
	LOSS [training: 0.024650213613956643 | validation: 0.04898741641370924]
	TIME [epoch: 8.8 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033587129071361986		[learning rate: 0.00058182]
		[batch 20/20] avg loss: 0.029192589561969982		[learning rate: 0.00058112]
	Learning Rate: 0.000581116
	LOSS [training: 0.031389859316665984 | validation: 0.03445220551615998]
	TIME [epoch: 8.8 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030128993081408435		[learning rate: 0.00058041]
		[batch 20/20] avg loss: 0.021537325605962367		[learning rate: 0.00057971]
	Learning Rate: 0.00057971
	LOSS [training: 0.0258331593436854 | validation: 0.026176822193841]
	TIME [epoch: 8.79 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02320218524984708		[learning rate: 0.00057901]
		[batch 20/20] avg loss: 0.03776021786938763		[learning rate: 0.00057831]
	Learning Rate: 0.000578306
	LOSS [training: 0.03048120155961735 | validation: 0.021381126172364054]
	TIME [epoch: 8.79 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02434836276132389		[learning rate: 0.00057761]
		[batch 20/20] avg loss: 0.032157676141951044		[learning rate: 0.00057691]
	Learning Rate: 0.000576906
	LOSS [training: 0.028253019451637468 | validation: 0.026707253971855482]
	TIME [epoch: 8.8 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017867693726265985		[learning rate: 0.00057621]
		[batch 20/20] avg loss: 0.020896383774376838		[learning rate: 0.00057551]
	Learning Rate: 0.00057551
	LOSS [training: 0.01938203875032141 | validation: 0.02376864321459476]
	TIME [epoch: 8.8 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021570052444862195		[learning rate: 0.00057481]
		[batch 20/20] avg loss: 0.035518807701220324		[learning rate: 0.00057412]
	Learning Rate: 0.000574117
	LOSS [training: 0.02854443007304126 | validation: 0.03188042632818439]
	TIME [epoch: 8.78 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03721783924192134		[learning rate: 0.00057342]
		[batch 20/20] avg loss: 0.018690857943577686		[learning rate: 0.00057273]
	Learning Rate: 0.000572727
	LOSS [training: 0.027954348592749517 | validation: 0.03672384713542256]
	TIME [epoch: 8.78 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03067227930842506		[learning rate: 0.00057203]
		[batch 20/20] avg loss: 0.04163904825801082		[learning rate: 0.00057134]
	Learning Rate: 0.00057134
	LOSS [training: 0.03615566378321793 | validation: 0.023929540016602087]
	TIME [epoch: 8.79 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03254735426872382		[learning rate: 0.00057065]
		[batch 20/20] avg loss: 0.016393925509172586		[learning rate: 0.00056996]
	Learning Rate: 0.000569957
	LOSS [training: 0.024470639888948202 | validation: 0.025665600855880606]
	TIME [epoch: 8.8 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03233934199973189		[learning rate: 0.00056927]
		[batch 20/20] avg loss: 0.02393909329469288		[learning rate: 0.00056858]
	Learning Rate: 0.000568577
	LOSS [training: 0.02813921764721238 | validation: 0.040821890799159974]
	TIME [epoch: 8.8 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03319889936168056		[learning rate: 0.00056789]
		[batch 20/20] avg loss: 0.02983894404985492		[learning rate: 0.0005672]
	Learning Rate: 0.000567201
	LOSS [training: 0.03151892170576774 | validation: 0.037234710880755695]
	TIME [epoch: 8.78 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03527906625718708		[learning rate: 0.00056651]
		[batch 20/20] avg loss: 0.03396380047014971		[learning rate: 0.00056583]
	Learning Rate: 0.000565828
	LOSS [training: 0.0346214333636684 | validation: 0.025389521747733505]
	TIME [epoch: 8.78 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02115694095186526		[learning rate: 0.00056514]
		[batch 20/20] avg loss: 0.017669784837511332		[learning rate: 0.00056446]
	Learning Rate: 0.000564458
	LOSS [training: 0.019413362894688292 | validation: 0.02314007789768493]
	TIME [epoch: 8.78 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030088809070980993		[learning rate: 0.00056377]
		[batch 20/20] avg loss: 0.034898230151703756		[learning rate: 0.00056309]
	Learning Rate: 0.000563092
	LOSS [training: 0.03249351961134238 | validation: 0.030243158300896304]
	TIME [epoch: 8.81 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0189977999102196		[learning rate: 0.00056241]
		[batch 20/20] avg loss: 0.02341661269022716		[learning rate: 0.00056173]
	Learning Rate: 0.000561728
	LOSS [training: 0.021207206300223373 | validation: 0.033757200144573264]
	TIME [epoch: 8.79 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02643819985084624		[learning rate: 0.00056105]
		[batch 20/20] avg loss: 0.01998913586726282		[learning rate: 0.00056037]
	Learning Rate: 0.000560369
	LOSS [training: 0.023213667859054526 | validation: 0.014869532503214584]
	TIME [epoch: 8.8 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01334189595092607		[learning rate: 0.00055969]
		[batch 20/20] avg loss: 0.028341545968865532		[learning rate: 0.00055901]
	Learning Rate: 0.000559012
	LOSS [training: 0.020841720959895803 | validation: 0.03828360790761693]
	TIME [epoch: 8.8 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03423406108889201		[learning rate: 0.00055833]
		[batch 20/20] avg loss: 0.03378161988576063		[learning rate: 0.00055766]
	Learning Rate: 0.000557659
	LOSS [training: 0.03400784048732632 | validation: 0.02583967255685132]
	TIME [epoch: 8.78 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028904377111085188		[learning rate: 0.00055698]
		[batch 20/20] avg loss: 0.009496876713555842		[learning rate: 0.00055631]
	Learning Rate: 0.000556309
	LOSS [training: 0.019200626912320516 | validation: 0.01834897513858491]
	TIME [epoch: 8.81 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02544829054968446		[learning rate: 0.00055563]
		[batch 20/20] avg loss: 0.028534356558885126		[learning rate: 0.00055496]
	Learning Rate: 0.000554962
	LOSS [training: 0.026991323554284796 | validation: 0.021954214919555345]
	TIME [epoch: 8.77 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038861947733911655		[learning rate: 0.00055429]
		[batch 20/20] avg loss: 0.03612085371980116		[learning rate: 0.00055362]
	Learning Rate: 0.000553618
	LOSS [training: 0.0374914007268564 | validation: 0.03294916482227665]
	TIME [epoch: 8.79 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029522130786435417		[learning rate: 0.00055295]
		[batch 20/20] avg loss: 0.02692691805203302		[learning rate: 0.00055228]
	Learning Rate: 0.000552278
	LOSS [training: 0.028224524419234214 | validation: 0.030104283537725096]
	TIME [epoch: 8.79 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028635087814462952		[learning rate: 0.00055161]
		[batch 20/20] avg loss: 0.04172601335166386		[learning rate: 0.00055094]
	Learning Rate: 0.000550941
	LOSS [training: 0.03518055058306341 | validation: 0.029009854474632887]
	TIME [epoch: 8.79 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02674746894968526		[learning rate: 0.00055027]
		[batch 20/20] avg loss: 0.03288248715000015		[learning rate: 0.00054961]
	Learning Rate: 0.000549608
	LOSS [training: 0.0298149780498427 | validation: 0.028585039388057103]
	TIME [epoch: 8.8 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026361491889973214		[learning rate: 0.00054894]
		[batch 20/20] avg loss: 0.018411324114869874		[learning rate: 0.00054828]
	Learning Rate: 0.000548277
	LOSS [training: 0.022386408002421546 | validation: 0.030067402451234324]
	TIME [epoch: 8.78 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03208869357046633		[learning rate: 0.00054761]
		[batch 20/20] avg loss: 0.03305657134799127		[learning rate: 0.00054695]
	Learning Rate: 0.00054695
	LOSS [training: 0.0325726324592288 | validation: 0.03172372939310357]
	TIME [epoch: 8.79 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03126720507005431		[learning rate: 0.00054629]
		[batch 20/20] avg loss: 0.01302904479780416		[learning rate: 0.00054563]
	Learning Rate: 0.000545626
	LOSS [training: 0.022148124933929234 | validation: 0.039533109559165926]
	TIME [epoch: 8.78 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020018015593387874		[learning rate: 0.00054496]
		[batch 20/20] avg loss: 0.03130127201313393		[learning rate: 0.0005443]
	Learning Rate: 0.000544305
	LOSS [training: 0.025659643803260905 | validation: 0.033316975106798624]
	TIME [epoch: 8.81 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024361632237594234		[learning rate: 0.00054365]
		[batch 20/20] avg loss: 0.01647587488477365		[learning rate: 0.00054299]
	Learning Rate: 0.000542987
	LOSS [training: 0.02041875356118394 | validation: 0.01872409481585434]
	TIME [epoch: 8.8 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022152503592644066		[learning rate: 0.00054233]
		[batch 20/20] avg loss: 0.02561840729754049		[learning rate: 0.00054167]
	Learning Rate: 0.000541673
	LOSS [training: 0.023885455445092275 | validation: 0.01970262484894552]
	TIME [epoch: 8.79 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0191550146356536		[learning rate: 0.00054102]
		[batch 20/20] avg loss: 0.02338728629799818		[learning rate: 0.00054036]
	Learning Rate: 0.000540361
	LOSS [training: 0.02127115046682589 | validation: 0.022831312366628418]
	TIME [epoch: 8.79 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02207933852248205		[learning rate: 0.00053971]
		[batch 20/20] avg loss: 0.025046033253746196		[learning rate: 0.00053905]
	Learning Rate: 0.000539053
	LOSS [training: 0.023562685888114124 | validation: 0.0279118138860549]
	TIME [epoch: 8.77 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04329060132255803		[learning rate: 0.0005384]
		[batch 20/20] avg loss: 0.029393699449667583		[learning rate: 0.00053775]
	Learning Rate: 0.000537748
	LOSS [training: 0.03634215038611281 | validation: 0.020869564875074416]
	TIME [epoch: 8.81 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034603919610676694		[learning rate: 0.0005371]
		[batch 20/20] avg loss: 0.031116445554779926		[learning rate: 0.00053645]
	Learning Rate: 0.000536446
	LOSS [training: 0.03286018258272831 | validation: 0.041156653467980246]
	TIME [epoch: 8.78 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03851413497142723		[learning rate: 0.0005358]
		[batch 20/20] avg loss: 0.02574877950277952		[learning rate: 0.00053515]
	Learning Rate: 0.000535148
	LOSS [training: 0.032131457237103374 | validation: 0.014075589463211092]
	TIME [epoch: 8.79 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013458454359196487		[learning rate: 0.0005345]
		[batch 20/20] avg loss: 0.027424887162787202		[learning rate: 0.00053385]
	Learning Rate: 0.000533852
	LOSS [training: 0.020441670760991845 | validation: 0.02921396160282002]
	TIME [epoch: 8.79 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030602274685929527		[learning rate: 0.00053321]
		[batch 20/20] avg loss: 0.03748066958603557		[learning rate: 0.00053256]
	Learning Rate: 0.00053256
	LOSS [training: 0.034041472135982555 | validation: 0.048461977743069]
	TIME [epoch: 8.78 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032301308072914585		[learning rate: 0.00053191]
		[batch 20/20] avg loss: 0.03360691940139911		[learning rate: 0.00053127]
	Learning Rate: 0.000531271
	LOSS [training: 0.03295411373715685 | validation: 0.0308138770211902]
	TIME [epoch: 8.8 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023468373191691993		[learning rate: 0.00053063]
		[batch 20/20] avg loss: 0.030477106105417328		[learning rate: 0.00052998]
	Learning Rate: 0.000529985
	LOSS [training: 0.02697273964855466 | validation: 0.015922848735398405]
	TIME [epoch: 8.78 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02615847006180851		[learning rate: 0.00052934]
		[batch 20/20] avg loss: 0.025541102434960872		[learning rate: 0.0005287]
	Learning Rate: 0.000528702
	LOSS [training: 0.025849786248384687 | validation: 0.03717578216912197]
	TIME [epoch: 8.78 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03029750318842896		[learning rate: 0.00052806]
		[batch 20/20] avg loss: 0.0256709326085451		[learning rate: 0.00052742]
	Learning Rate: 0.000527422
	LOSS [training: 0.027984217898487034 | validation: 0.016039071738058904]
	TIME [epoch: 8.78 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03413449567056254		[learning rate: 0.00052678]
		[batch 20/20] avg loss: 0.02860347087234564		[learning rate: 0.00052614]
	Learning Rate: 0.000526145
	LOSS [training: 0.031368983271454084 | validation: 0.03650127298239532]
	TIME [epoch: 8.8 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023064731742761805		[learning rate: 0.00052551]
		[batch 20/20] avg loss: 0.015848959074724164		[learning rate: 0.00052487]
	Learning Rate: 0.000524871
	LOSS [training: 0.019456845408742984 | validation: 0.016389521718968108]
	TIME [epoch: 8.81 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015134208136578332		[learning rate: 0.00052424]
		[batch 20/20] avg loss: 0.016627423516424128		[learning rate: 0.0005236]
	Learning Rate: 0.0005236
	LOSS [training: 0.01588081582650123 | validation: 0.01906932455669072]
	TIME [epoch: 8.79 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01856514026636854		[learning rate: 0.00052297]
		[batch 20/20] avg loss: 0.01610779848208787		[learning rate: 0.00052233]
	Learning Rate: 0.000522333
	LOSS [training: 0.017336469374228197 | validation: 0.015681171749384798]
	TIME [epoch: 8.79 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009299083866280445		[learning rate: 0.0005217]
		[batch 20/20] avg loss: 0.019724561491218047		[learning rate: 0.00052107]
	Learning Rate: 0.000521068
	LOSS [training: 0.014511822678749245 | validation: 0.011888968850652243]
	TIME [epoch: 8.78 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016135878862634023		[learning rate: 0.00052044]
		[batch 20/20] avg loss: 0.025400300527511993		[learning rate: 0.00051981]
	Learning Rate: 0.000519807
	LOSS [training: 0.02076808969507301 | validation: 0.024276692039627602]
	TIME [epoch: 8.8 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04240336053379947		[learning rate: 0.00051918]
		[batch 20/20] avg loss: 0.032324659059667374		[learning rate: 0.00051855]
	Learning Rate: 0.000518549
	LOSS [training: 0.03736400979673342 | validation: 0.028013674798704907]
	TIME [epoch: 8.78 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04717873582510848		[learning rate: 0.00051792]
		[batch 20/20] avg loss: 0.034028181614608535		[learning rate: 0.00051729]
	Learning Rate: 0.000517293
	LOSS [training: 0.0406034587198585 | validation: 0.03419225174866827]
	TIME [epoch: 8.78 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02189563385542672		[learning rate: 0.00051667]
		[batch 20/20] avg loss: 0.0360348079870937		[learning rate: 0.00051604]
	Learning Rate: 0.000516041
	LOSS [training: 0.028965220921260214 | validation: 0.040322474279845026]
	TIME [epoch: 8.79 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018267507498807525		[learning rate: 0.00051542]
		[batch 20/20] avg loss: 0.025785719676406877		[learning rate: 0.00051479]
	Learning Rate: 0.000514792
	LOSS [training: 0.0220266135876072 | validation: 0.026493471034090325]
	TIME [epoch: 8.79 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019498244941944937		[learning rate: 0.00051417]
		[batch 20/20] avg loss: 0.023349514877021044		[learning rate: 0.00051355]
	Learning Rate: 0.000513545
	LOSS [training: 0.021423879909482992 | validation: 0.04164573762831458]
	TIME [epoch: 8.79 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030225157065813097		[learning rate: 0.00051292]
		[batch 20/20] avg loss: 0.01669670646223768		[learning rate: 0.0005123]
	Learning Rate: 0.000512302
	LOSS [training: 0.02346093176402539 | validation: 0.028135569266546402]
	TIME [epoch: 8.79 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020557755319112574		[learning rate: 0.00051168]
		[batch 20/20] avg loss: 0.028626314428557655		[learning rate: 0.00051106]
	Learning Rate: 0.000511062
	LOSS [training: 0.024592034873835114 | validation: 0.030611820408698476]
	TIME [epoch: 8.77 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02672448446447686		[learning rate: 0.00051044]
		[batch 20/20] avg loss: 0.02122152995009268		[learning rate: 0.00050982]
	Learning Rate: 0.000509825
	LOSS [training: 0.02397300720728477 | validation: 0.026486937842735547]
	TIME [epoch: 8.81 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018511103447525403		[learning rate: 0.00050921]
		[batch 20/20] avg loss: 0.017436441970139		[learning rate: 0.00050859]
	Learning Rate: 0.000508591
	LOSS [training: 0.017973772708832204 | validation: 0.01615788242374709]
	TIME [epoch: 8.8 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01579574870749979		[learning rate: 0.00050797]
		[batch 20/20] avg loss: 0.02952412774792912		[learning rate: 0.00050736]
	Learning Rate: 0.00050736
	LOSS [training: 0.022659938227714456 | validation: 0.029156389424109568]
	TIME [epoch: 8.82 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029511173743311192		[learning rate: 0.00050675]
		[batch 20/20] avg loss: 0.01898854228569		[learning rate: 0.00050613]
	Learning Rate: 0.000506131
	LOSS [training: 0.024249858014500598 | validation: 0.018048209342915213]
	TIME [epoch: 8.79 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01694433089695587		[learning rate: 0.00050552]
		[batch 20/20] avg loss: 0.028426863759717362		[learning rate: 0.00050491]
	Learning Rate: 0.000504906
	LOSS [training: 0.022685597328336617 | validation: 0.03838507966994043]
	TIME [epoch: 8.78 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02456211342685848		[learning rate: 0.00050429]
		[batch 20/20] avg loss: 0.01726707069257128		[learning rate: 0.00050368]
	Learning Rate: 0.000503684
	LOSS [training: 0.020914592059714875 | validation: 0.014553136035406119]
	TIME [epoch: 8.78 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01148614949306078		[learning rate: 0.00050307]
		[batch 20/20] avg loss: 0.028630792167686464		[learning rate: 0.00050246]
	Learning Rate: 0.000502464
	LOSS [training: 0.02005847083037362 | validation: 0.022829940932690276]
	TIME [epoch: 8.79 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023790527997241195		[learning rate: 0.00050186]
		[batch 20/20] avg loss: 0.020074528121100377		[learning rate: 0.00050125]
	Learning Rate: 0.000501248
	LOSS [training: 0.02193252805917079 | validation: 0.01758428773964182]
	TIME [epoch: 8.8 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022413258504865582		[learning rate: 0.00050064]
		[batch 20/20] avg loss: 0.012189229814400213		[learning rate: 0.00050003]
	Learning Rate: 0.000500034
	LOSS [training: 0.0173012441596329 | validation: 0.014158282813939717]
	TIME [epoch: 8.79 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020116424956361584		[learning rate: 0.00049943]
		[batch 20/20] avg loss: 0.03387085302052546		[learning rate: 0.00049882]
	Learning Rate: 0.000498824
	LOSS [training: 0.02699363898844352 | validation: 0.021033164792656062]
	TIME [epoch: 8.79 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017272505895987146		[learning rate: 0.00049822]
		[batch 20/20] avg loss: 0.0260174720434109		[learning rate: 0.00049762]
	Learning Rate: 0.000497616
	LOSS [training: 0.021644988969699023 | validation: 0.02677702712568464]
	TIME [epoch: 8.79 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02252807938436994		[learning rate: 0.00049701]
		[batch 20/20] avg loss: 0.02539117927349996		[learning rate: 0.00049641]
	Learning Rate: 0.000496412
	LOSS [training: 0.023959629328934945 | validation: 0.016404297961914806]
	TIME [epoch: 8.79 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014034009639260206		[learning rate: 0.00049581]
		[batch 20/20] avg loss: 0.020572655337355753		[learning rate: 0.00049521]
	Learning Rate: 0.00049521
	LOSS [training: 0.017303332488307982 | validation: 0.019254907703160597]
	TIME [epoch: 8.79 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025623332675435977		[learning rate: 0.00049461]
		[batch 20/20] avg loss: 0.025763487048600375		[learning rate: 0.00049401]
	Learning Rate: 0.000494011
	LOSS [training: 0.02569340986201818 | validation: 0.03970628462724029]
	TIME [epoch: 8.78 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030166940180617895		[learning rate: 0.00049341]
		[batch 20/20] avg loss: 0.0250667599116694		[learning rate: 0.00049282]
	Learning Rate: 0.000492815
	LOSS [training: 0.02761685004614365 | validation: 0.015309558770795114]
	TIME [epoch: 8.8 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028049196207395478		[learning rate: 0.00049222]
		[batch 20/20] avg loss: 0.0223096560773691		[learning rate: 0.00049162]
	Learning Rate: 0.000491622
	LOSS [training: 0.025179426142382284 | validation: 0.029887340529548487]
	TIME [epoch: 8.8 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02776215511471248		[learning rate: 0.00049103]
		[batch 20/20] avg loss: 0.021874073397930842		[learning rate: 0.00049043]
	Learning Rate: 0.000490432
	LOSS [training: 0.02481811425632166 | validation: 0.024885315221241464]
	TIME [epoch: 8.82 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022764128266840742		[learning rate: 0.00048984]
		[batch 20/20] avg loss: 0.022896163852990755		[learning rate: 0.00048924]
	Learning Rate: 0.000489245
	LOSS [training: 0.022830146059915745 | validation: 0.03169601512724601]
	TIME [epoch: 8.78 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020217830408916424		[learning rate: 0.00048865]
		[batch 20/20] avg loss: 0.02350171191337972		[learning rate: 0.00048806]
	Learning Rate: 0.000488061
	LOSS [training: 0.021859771161148074 | validation: 0.01709515717783336]
	TIME [epoch: 8.78 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024957105359995894		[learning rate: 0.00048747]
		[batch 20/20] avg loss: 0.018893674986157582		[learning rate: 0.00048688]
	Learning Rate: 0.000486879
	LOSS [training: 0.021925390173076733 | validation: 0.022545834620387742]
	TIME [epoch: 8.78 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01963584481872044		[learning rate: 0.00048629]
		[batch 20/20] avg loss: 0.01334449100699726		[learning rate: 0.0004857]
	Learning Rate: 0.0004857
	LOSS [training: 0.01649016791285885 | validation: 0.02143893932083611]
	TIME [epoch: 8.79 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02502044632862884		[learning rate: 0.00048511]
		[batch 20/20] avg loss: 0.020864996576588902		[learning rate: 0.00048452]
	Learning Rate: 0.000484525
	LOSS [training: 0.022942721452608873 | validation: 0.03315720899980884]
	TIME [epoch: 8.79 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02355763687216119		[learning rate: 0.00048394]
		[batch 20/20] avg loss: 0.01769814722577136		[learning rate: 0.00048335]
	Learning Rate: 0.000483352
	LOSS [training: 0.020627892048966274 | validation: 0.03356236823897675]
	TIME [epoch: 8.79 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021988032404333703		[learning rate: 0.00048277]
		[batch 20/20] avg loss: 0.018672862016128673		[learning rate: 0.00048218]
	Learning Rate: 0.000482181
	LOSS [training: 0.020330447210231188 | validation: 0.027609129670706015]
	TIME [epoch: 8.79 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02555008063326253		[learning rate: 0.0004816]
		[batch 20/20] avg loss: 0.026102000341727515		[learning rate: 0.00048101]
	Learning Rate: 0.000481014
	LOSS [training: 0.02582604048749502 | validation: 0.03420921752518806]
	TIME [epoch: 8.79 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022157720692566763		[learning rate: 0.00048043]
		[batch 20/20] avg loss: 0.017564148265196335		[learning rate: 0.00047985]
	Learning Rate: 0.00047985
	LOSS [training: 0.01986093447888155 | validation: 0.012689374071594407]
	TIME [epoch: 8.8 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018555352428759237		[learning rate: 0.00047927]
		[batch 20/20] avg loss: 0.015698277346605765		[learning rate: 0.00047869]
	Learning Rate: 0.000478688
	LOSS [training: 0.017126814887682494 | validation: 0.02342274691016577]
	TIME [epoch: 8.79 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02885113779972035		[learning rate: 0.00047811]
		[batch 20/20] avg loss: 0.016563361549883016		[learning rate: 0.00047753]
	Learning Rate: 0.000477529
	LOSS [training: 0.02270724967480169 | validation: 0.02975412861017273]
	TIME [epoch: 8.79 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01927145867122855		[learning rate: 0.00047695]
		[batch 20/20] avg loss: 0.02454209523897832		[learning rate: 0.00047637]
	Learning Rate: 0.000476373
	LOSS [training: 0.021906776955103437 | validation: 0.024164721308588797]
	TIME [epoch: 8.79 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025464478824169308		[learning rate: 0.0004758]
		[batch 20/20] avg loss: 0.017731178565353993		[learning rate: 0.00047522]
	Learning Rate: 0.00047522
	LOSS [training: 0.021597828694761654 | validation: 0.03526401622656901]
	TIME [epoch: 8.79 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022541015700998382		[learning rate: 0.00047464]
		[batch 20/20] avg loss: 0.023266318149179697		[learning rate: 0.00047407]
	Learning Rate: 0.00047407
	LOSS [training: 0.022903666925089043 | validation: 0.035276434085909164]
	TIME [epoch: 8.82 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02744934713170342		[learning rate: 0.0004735]
		[batch 20/20] avg loss: 0.0229491618116558		[learning rate: 0.00047292]
	Learning Rate: 0.000472922
	LOSS [training: 0.02519925447167961 | validation: 0.01857065756930429]
	TIME [epoch: 8.78 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018550457442391772		[learning rate: 0.00047235]
		[batch 20/20] avg loss: 0.03017490372282066		[learning rate: 0.00047178]
	Learning Rate: 0.000471777
	LOSS [training: 0.02436268058260621 | validation: 0.017800658764133174]
	TIME [epoch: 8.79 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019944041856779025		[learning rate: 0.00047121]
		[batch 20/20] avg loss: 0.014987858698055731		[learning rate: 0.00047063]
	Learning Rate: 0.000470635
	LOSS [training: 0.017465950277417378 | validation: 0.021338795935444365]
	TIME [epoch: 8.77 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025088519413878775		[learning rate: 0.00047006]
		[batch 20/20] avg loss: 0.02094867407062505		[learning rate: 0.0004695]
	Learning Rate: 0.000469496
	LOSS [training: 0.023018596742251908 | validation: 0.024169776358060526]
	TIME [epoch: 8.79 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013208358280449842		[learning rate: 0.00046893]
		[batch 20/20] avg loss: 0.016015031396977297		[learning rate: 0.00046836]
	Learning Rate: 0.000468359
	LOSS [training: 0.014611694838713569 | validation: 0.020804590770220028]
	TIME [epoch: 8.81 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026955339048772908		[learning rate: 0.00046779]
		[batch 20/20] avg loss: 0.022912854248932857		[learning rate: 0.00046723]
	Learning Rate: 0.000467225
	LOSS [training: 0.024934096648852884 | validation: 0.02037399078506628]
	TIME [epoch: 8.78 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03376002952499041		[learning rate: 0.00046666]
		[batch 20/20] avg loss: 0.035185004613927345		[learning rate: 0.00046609]
	Learning Rate: 0.000466094
	LOSS [training: 0.03447251706945888 | validation: 0.038838467537063916]
	TIME [epoch: 8.78 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038286278475937026		[learning rate: 0.00046553]
		[batch 20/20] avg loss: 0.03354841562642254		[learning rate: 0.00046497]
	Learning Rate: 0.000464966
	LOSS [training: 0.03591734705117979 | validation: 0.02151232777724278]
	TIME [epoch: 8.78 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021463078624530542		[learning rate: 0.0004644]
		[batch 20/20] avg loss: 0.02557417057165167		[learning rate: 0.00046384]
	Learning Rate: 0.00046384
	LOSS [training: 0.023518624598091103 | validation: 0.028883210960438974]
	TIME [epoch: 8.79 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02620034174042621		[learning rate: 0.00046328]
		[batch 20/20] avg loss: 0.03466758063967908		[learning rate: 0.00046272]
	Learning Rate: 0.000462717
	LOSS [training: 0.030433961190052643 | validation: 0.017448415673836183]
	TIME [epoch: 8.8 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019225830304232583		[learning rate: 0.00046216]
		[batch 20/20] avg loss: 0.019502419534026767		[learning rate: 0.0004616]
	Learning Rate: 0.000461597
	LOSS [training: 0.019364124919129675 | validation: 0.02307674961591552]
	TIME [epoch: 8.79 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013313594049382909		[learning rate: 0.00046104]
		[batch 20/20] avg loss: 0.016879219790471127		[learning rate: 0.00046048]
	Learning Rate: 0.00046048
	LOSS [training: 0.01509640691992702 | validation: 0.02989671948547465]
	TIME [epoch: 8.8 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03299695164817526		[learning rate: 0.00045992]
		[batch 20/20] avg loss: 0.027087333955391108		[learning rate: 0.00045937]
	Learning Rate: 0.000459365
	LOSS [training: 0.030042142801783182 | validation: 0.020875640408254202]
	TIME [epoch: 8.79 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02274434279542563		[learning rate: 0.00045881]
		[batch 20/20] avg loss: 0.020129155396475213		[learning rate: 0.00045825]
	Learning Rate: 0.000458253
	LOSS [training: 0.021436749095950426 | validation: 0.020982355308384264]
	TIME [epoch: 8.82 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018149270127998954		[learning rate: 0.0004577]
		[batch 20/20] avg loss: 0.016718750259548235		[learning rate: 0.00045714]
	Learning Rate: 0.000457144
	LOSS [training: 0.017434010193773594 | validation: 0.013841904785726968]
	TIME [epoch: 8.78 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014900670772941483		[learning rate: 0.00045659]
		[batch 20/20] avg loss: 0.026192085243374012		[learning rate: 0.00045604]
	Learning Rate: 0.000456037
	LOSS [training: 0.02054637800815775 | validation: 0.022301806313311204]
	TIME [epoch: 8.79 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023400983171894603		[learning rate: 0.00045548]
		[batch 20/20] avg loss: 0.017357202168558752		[learning rate: 0.00045493]
	Learning Rate: 0.000454933
	LOSS [training: 0.020379092670226678 | validation: 0.019577398365206788]
	TIME [epoch: 8.78 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022586166802295093		[learning rate: 0.00045438]
		[batch 20/20] avg loss: 0.030324944370470813		[learning rate: 0.00045383]
	Learning Rate: 0.000453832
	LOSS [training: 0.02645555558638295 | validation: 0.014725098988684087]
	TIME [epoch: 8.79 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01776127273181554		[learning rate: 0.00045328]
		[batch 20/20] avg loss: 0.01452846898518372		[learning rate: 0.00045273]
	Learning Rate: 0.000452733
	LOSS [training: 0.016144870858499628 | validation: 0.012040421085768888]
	TIME [epoch: 8.81 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01947413943406982		[learning rate: 0.00045218]
		[batch 20/20] avg loss: 0.024146545870560802		[learning rate: 0.00045164]
	Learning Rate: 0.000451637
	LOSS [training: 0.021810342652315313 | validation: 0.024149477417953827]
	TIME [epoch: 8.78 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02577132950067689		[learning rate: 0.00045109]
		[batch 20/20] avg loss: 0.01854108928449223		[learning rate: 0.00045054]
	Learning Rate: 0.000450544
	LOSS [training: 0.022156209392584555 | validation: 0.033786399324512384]
	TIME [epoch: 8.79 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017073402657148247		[learning rate: 0.00045]
		[batch 20/20] avg loss: 0.016059353662860043		[learning rate: 0.00044945]
	Learning Rate: 0.000449453
	LOSS [training: 0.01656637816000414 | validation: 0.014983802821309893]
	TIME [epoch: 8.78 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01919747918294575		[learning rate: 0.00044891]
		[batch 20/20] avg loss: 0.03384049881330139		[learning rate: 0.00044836]
	Learning Rate: 0.000448365
	LOSS [training: 0.02651898899812357 | validation: 0.03230149759292016]
	TIME [epoch: 8.79 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021002302627391906		[learning rate: 0.00044782]
		[batch 20/20] avg loss: 0.014826257587303285		[learning rate: 0.00044728]
	Learning Rate: 0.000447279
	LOSS [training: 0.017914280107347594 | validation: 0.015552295242341632]
	TIME [epoch: 8.81 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017846587269612846		[learning rate: 0.00044674]
		[batch 20/20] avg loss: 0.019547786901274188		[learning rate: 0.0004462]
	Learning Rate: 0.000446197
	LOSS [training: 0.01869718708544352 | validation: 0.026343806705896818]
	TIME [epoch: 8.78 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02589688758796731		[learning rate: 0.00044566]
		[batch 20/20] avg loss: 0.020774614622890007		[learning rate: 0.00044512]
	Learning Rate: 0.000445117
	LOSS [training: 0.02333575110542866 | validation: 0.019454223439281562]
	TIME [epoch: 8.8 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022511030433809275		[learning rate: 0.00044458]
		[batch 20/20] avg loss: 0.016138714854413948		[learning rate: 0.00044404]
	Learning Rate: 0.000444039
	LOSS [training: 0.01932487264411161 | validation: 0.01435095937449733]
	TIME [epoch: 8.78 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019253110657589455		[learning rate: 0.0004435]
		[batch 20/20] avg loss: 0.022307458745898477		[learning rate: 0.00044296]
	Learning Rate: 0.000442964
	LOSS [training: 0.020780284701743965 | validation: 0.00551662933263764]
	TIME [epoch: 8.8 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013358085391084832		[learning rate: 0.00044243]
		[batch 20/20] avg loss: 0.017922375372805933		[learning rate: 0.00044189]
	Learning Rate: 0.000441892
	LOSS [training: 0.015640230381945383 | validation: 0.016833982268591712]
	TIME [epoch: 8.8 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014464620466087666		[learning rate: 0.00044136]
		[batch 20/20] avg loss: 0.021320306316630528		[learning rate: 0.00044082]
	Learning Rate: 0.000440822
	LOSS [training: 0.017892463391359092 | validation: 0.020951593145934002]
	TIME [epoch: 8.78 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025008565938210936		[learning rate: 0.00044029]
		[batch 20/20] avg loss: 0.02364614931270028		[learning rate: 0.00043975]
	Learning Rate: 0.000439755
	LOSS [training: 0.02432735762545561 | validation: 0.016356354739331738]
	TIME [epoch: 8.78 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019506253956682717		[learning rate: 0.00043922]
		[batch 20/20] avg loss: 0.021361741144278586		[learning rate: 0.00043869]
	Learning Rate: 0.00043869
	LOSS [training: 0.020433997550480653 | validation: 0.011883240390226882]
	TIME [epoch: 8.78 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016988911560238566		[learning rate: 0.00043816]
		[batch 20/20] avg loss: 0.019456893632206967		[learning rate: 0.00043763]
	Learning Rate: 0.000437628
	LOSS [training: 0.01822290259622277 | validation: 0.024309076214105715]
	TIME [epoch: 8.8 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016569536169746955		[learning rate: 0.0004371]
		[batch 20/20] avg loss: 0.029679298923882914		[learning rate: 0.00043657]
	Learning Rate: 0.000436569
	LOSS [training: 0.023124417546814928 | validation: 0.009271085626434644]
	TIME [epoch: 8.8 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019052591629048873		[learning rate: 0.00043604]
		[batch 20/20] avg loss: 0.019242269250635255		[learning rate: 0.00043551]
	Learning Rate: 0.000435512
	LOSS [training: 0.019147430439842066 | validation: 0.02509584722013403]
	TIME [epoch: 8.78 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023377714590948402		[learning rate: 0.00043498]
		[batch 20/20] avg loss: 0.023736069801782028		[learning rate: 0.00043446]
	Learning Rate: 0.000434458
	LOSS [training: 0.02355689219636522 | validation: 0.03187037920444113]
	TIME [epoch: 8.79 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04925446144779387		[learning rate: 0.00043393]
		[batch 20/20] avg loss: 0.012958145425875974		[learning rate: 0.00043341]
	Learning Rate: 0.000433406
	LOSS [training: 0.031106303436834924 | validation: 0.02354744881364236]
	TIME [epoch: 8.77 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019183064437665286		[learning rate: 0.00043288]
		[batch 20/20] avg loss: 0.018841657322504236		[learning rate: 0.00043236]
	Learning Rate: 0.000432357
	LOSS [training: 0.019012360880084758 | validation: 0.018821805554248575]
	TIME [epoch: 8.82 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01795119427356825		[learning rate: 0.00043183]
		[batch 20/20] avg loss: 0.01104176614751291		[learning rate: 0.00043131]
	Learning Rate: 0.00043131
	LOSS [training: 0.014496480210540582 | validation: 0.019781538646367513]
	TIME [epoch: 8.8 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02068164508123886		[learning rate: 0.00043079]
		[batch 20/20] avg loss: 0.023575576390961462		[learning rate: 0.00043027]
	Learning Rate: 0.000430266
	LOSS [training: 0.022128610736100163 | validation: 0.020845689253677687]
	TIME [epoch: 8.79 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021131761889394078		[learning rate: 0.00042974]
		[batch 20/20] avg loss: 0.022093820303825326		[learning rate: 0.00042922]
	Learning Rate: 0.000429224
	LOSS [training: 0.021612791096609702 | validation: 0.01452300494772003]
	TIME [epoch: 8.79 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023011641574248568		[learning rate: 0.0004287]
		[batch 20/20] avg loss: 0.014818988361352517		[learning rate: 0.00042819]
	Learning Rate: 0.000428185
	LOSS [training: 0.018915314967800544 | validation: 0.025412562329281897]
	TIME [epoch: 8.79 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01868815196415944		[learning rate: 0.00042767]
		[batch 20/20] avg loss: 0.009108556681264963		[learning rate: 0.00042715]
	Learning Rate: 0.000427149
	LOSS [training: 0.013898354322712202 | validation: 0.020885286474173793]
	TIME [epoch: 8.79 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024214537050060297		[learning rate: 0.00042663]
		[batch 20/20] avg loss: 0.023601185827725055		[learning rate: 0.00042611]
	Learning Rate: 0.000426115
	LOSS [training: 0.02390786143889268 | validation: 0.021803988703825834]
	TIME [epoch: 8.78 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029629385687090733		[learning rate: 0.0004256]
		[batch 20/20] avg loss: 0.016663549178349155		[learning rate: 0.00042508]
	Learning Rate: 0.000425083
	LOSS [training: 0.02314646743271994 | validation: 0.018392703843311046]
	TIME [epoch: 8.78 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025428923486584674		[learning rate: 0.00042457]
		[batch 20/20] avg loss: 0.021079659902731314		[learning rate: 0.00042405]
	Learning Rate: 0.000424054
	LOSS [training: 0.023254291694657998 | validation: 0.012325690225823938]
	TIME [epoch: 8.78 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01622552898344019		[learning rate: 0.00042354]
		[batch 20/20] avg loss: 0.01685654738203058		[learning rate: 0.00042303]
	Learning Rate: 0.000423027
	LOSS [training: 0.016541038182735383 | validation: 0.02008883182853716]
	TIME [epoch: 8.8 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01471651169597181		[learning rate: 0.00042251]
		[batch 20/20] avg loss: 0.016374968635645352		[learning rate: 0.000422]
	Learning Rate: 0.000422003
	LOSS [training: 0.015545740165808581 | validation: 0.013452093357936864]
	TIME [epoch: 8.8 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016232217022802097		[learning rate: 0.00042149]
		[batch 20/20] avg loss: 0.01360584734343826		[learning rate: 0.00042098]
	Learning Rate: 0.000420982
	LOSS [training: 0.01491903218312018 | validation: 0.017637818632182403]
	TIME [epoch: 8.78 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013771954099417894		[learning rate: 0.00042047]
		[batch 20/20] avg loss: 0.021681886691212744		[learning rate: 0.00041996]
	Learning Rate: 0.000419963
	LOSS [training: 0.017726920395315315 | validation: 0.030014524576007652]
	TIME [epoch: 8.78 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016618104030318985		[learning rate: 0.00041945]
		[batch 20/20] avg loss: 0.020447853297019668		[learning rate: 0.00041895]
	Learning Rate: 0.000418946
	LOSS [training: 0.018532978663669326 | validation: 0.022862468471342934]
	TIME [epoch: 8.78 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025213586337225813		[learning rate: 0.00041844]
		[batch 20/20] avg loss: 0.03094387097784415		[learning rate: 0.00041793]
	Learning Rate: 0.000417932
	LOSS [training: 0.02807872865753498 | validation: 0.024952919374623413]
	TIME [epoch: 8.81 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017308423275662642		[learning rate: 0.00041743]
		[batch 20/20] avg loss: 0.0157078598652315		[learning rate: 0.00041692]
	Learning Rate: 0.00041692
	LOSS [training: 0.016508141570447072 | validation: 0.011910553023043835]
	TIME [epoch: 8.81 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017151789808220023		[learning rate: 0.00041641]
		[batch 20/20] avg loss: 0.022308898209963457		[learning rate: 0.00041591]
	Learning Rate: 0.000415911
	LOSS [training: 0.019730344009091738 | validation: 0.010435333668531025]
	TIME [epoch: 8.79 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015757220390474492		[learning rate: 0.00041541]
		[batch 20/20] avg loss: 0.016561675836870418		[learning rate: 0.0004149]
	Learning Rate: 0.000414904
	LOSS [training: 0.016159448113672453 | validation: 0.011324830149803724]
	TIME [epoch: 8.8 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02029363959088779		[learning rate: 0.0004144]
		[batch 20/20] avg loss: 0.019112087748227816		[learning rate: 0.0004139]
	Learning Rate: 0.000413899
	LOSS [training: 0.0197028636695578 | validation: 0.0215719021466137]
	TIME [epoch: 8.78 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017733139431105133		[learning rate: 0.0004134]
		[batch 20/20] avg loss: 0.020244601191824512		[learning rate: 0.0004129]
	Learning Rate: 0.000412897
	LOSS [training: 0.018988870311464824 | validation: 0.026257207568094316]
	TIME [epoch: 8.8 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013689625588792406		[learning rate: 0.0004124]
		[batch 20/20] avg loss: 0.018945691012497153		[learning rate: 0.0004119]
	Learning Rate: 0.000411898
	LOSS [training: 0.01631765830064478 | validation: 0.020349925635882227]
	TIME [epoch: 8.79 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014627808683215981		[learning rate: 0.0004114]
		[batch 20/20] avg loss: 0.020589000656301683		[learning rate: 0.0004109]
	Learning Rate: 0.000410901
	LOSS [training: 0.017608404669758833 | validation: 0.015589556249288728]
	TIME [epoch: 8.78 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019272479732186102		[learning rate: 0.0004104]
		[batch 20/20] avg loss: 0.018518250380074524		[learning rate: 0.00040991]
	Learning Rate: 0.000409906
	LOSS [training: 0.018895365056130313 | validation: 0.02147582726450686]
	TIME [epoch: 8.79 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018722337345975178		[learning rate: 0.00040941]
		[batch 20/20] avg loss: 0.02332911601584736		[learning rate: 0.00040891]
	Learning Rate: 0.000408914
	LOSS [training: 0.021025726680911273 | validation: 0.019940704018285488]
	TIME [epoch: 8.79 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021223763055218378		[learning rate: 0.00040842]
		[batch 20/20] avg loss: 0.02857363967825228		[learning rate: 0.00040792]
	Learning Rate: 0.000407924
	LOSS [training: 0.02489870136673533 | validation: 0.016063132362004576]
	TIME [epoch: 8.8 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017468170700719286		[learning rate: 0.00040743]
		[batch 20/20] avg loss: 0.015021102422203846		[learning rate: 0.00040694]
	Learning Rate: 0.000406936
	LOSS [training: 0.016244636561461565 | validation: 0.019239053420369724]
	TIME [epoch: 8.79 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024011160572072792		[learning rate: 0.00040644]
		[batch 20/20] avg loss: 0.02878008003292657		[learning rate: 0.00040595]
	Learning Rate: 0.000405951
	LOSS [training: 0.02639562030249968 | validation: 0.018273382052646545]
	TIME [epoch: 8.78 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015590504257361418		[learning rate: 0.00040546]
		[batch 20/20] avg loss: 0.013474430799622617		[learning rate: 0.00040497]
	Learning Rate: 0.000404968
	LOSS [training: 0.014532467528492013 | validation: 0.031182015358933324]
	TIME [epoch: 8.79 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029778565633280546		[learning rate: 0.00040448]
		[batch 20/20] avg loss: 0.02343318143592834		[learning rate: 0.00040399]
	Learning Rate: 0.000403988
	LOSS [training: 0.02660587353460444 | validation: 0.028611224382188556]
	TIME [epoch: 8.81 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020825734782564394		[learning rate: 0.0004035]
		[batch 20/20] avg loss: 0.029410678474477182		[learning rate: 0.00040301]
	Learning Rate: 0.00040301
	LOSS [training: 0.025118206628520785 | validation: 0.043928805357396294]
	TIME [epoch: 8.79 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024741570841906705		[learning rate: 0.00040252]
		[batch 20/20] avg loss: 0.019223389118616464		[learning rate: 0.00040203]
	Learning Rate: 0.000402034
	LOSS [training: 0.021982479980261583 | validation: 0.016086010984148625]
	TIME [epoch: 8.79 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01598345574575997		[learning rate: 0.00040155]
		[batch 20/20] avg loss: 0.021964329376984587		[learning rate: 0.00040106]
	Learning Rate: 0.000401061
	LOSS [training: 0.01897389256137228 | validation: 0.024749776095571434]
	TIME [epoch: 8.77 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015286386217480626		[learning rate: 0.00040058]
		[batch 20/20] avg loss: 0.024846730368738298		[learning rate: 0.00040009]
	Learning Rate: 0.00040009
	LOSS [training: 0.020066558293109465 | validation: 0.025103023167112406]
	TIME [epoch: 8.79 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02283302862271949		[learning rate: 0.00039961]
		[batch 20/20] avg loss: 0.02258908088566293		[learning rate: 0.00039912]
	Learning Rate: 0.000399122
	LOSS [training: 0.02271105475419121 | validation: 0.018441509101305122]
	TIME [epoch: 8.79 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019015128694383633		[learning rate: 0.00039864]
		[batch 20/20] avg loss: 0.025878834038339925		[learning rate: 0.00039816]
	Learning Rate: 0.000398155
	LOSS [training: 0.02244698136636178 | validation: 0.01661051512063476]
	TIME [epoch: 8.79 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019855822936861733		[learning rate: 0.00039767]
		[batch 20/20] avg loss: 0.030245637213591536		[learning rate: 0.00039719]
	Learning Rate: 0.000397192
	LOSS [training: 0.025050730075226634 | validation: 0.022620886709223784]
	TIME [epoch: 8.78 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026406401498722053		[learning rate: 0.00039671]
		[batch 20/20] avg loss: 0.03138586040597251		[learning rate: 0.00039623]
	Learning Rate: 0.00039623
	LOSS [training: 0.028896130952347276 | validation: 0.023380587961896522]
	TIME [epoch: 8.78 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014139963625549682		[learning rate: 0.00039575]
		[batch 20/20] avg loss: 0.02794966660437297		[learning rate: 0.00039527]
	Learning Rate: 0.000395271
	LOSS [training: 0.021044815114961323 | validation: 0.028711765896397654]
	TIME [epoch: 8.78 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027942728080611667		[learning rate: 0.00039479]
		[batch 20/20] avg loss: 0.01307222826768931		[learning rate: 0.00039431]
	Learning Rate: 0.000394314
	LOSS [training: 0.020507478174150495 | validation: 0.01719054275711172]
	TIME [epoch: 8.8 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020754292129991213		[learning rate: 0.00039384]
		[batch 20/20] avg loss: 0.022243596072714568		[learning rate: 0.00039336]
	Learning Rate: 0.000393359
	LOSS [training: 0.02149894410135289 | validation: 0.024631245374174826]
	TIME [epoch: 8.78 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01651207765779234		[learning rate: 0.00039288]
		[batch 20/20] avg loss: 0.016999423966135165		[learning rate: 0.00039241]
	Learning Rate: 0.000392407
	LOSS [training: 0.016755750811963753 | validation: 0.022309111446736363]
	TIME [epoch: 8.78 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013980896395616694		[learning rate: 0.00039193]
		[batch 20/20] avg loss: 0.016622070863226514		[learning rate: 0.00039146]
	Learning Rate: 0.000391457
	LOSS [training: 0.015301483629421603 | validation: 0.017259007226128505]
	TIME [epoch: 8.8 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019461156908315024		[learning rate: 0.00039098]
		[batch 20/20] avg loss: 0.016580986224563395		[learning rate: 0.00039051]
	Learning Rate: 0.00039051
	LOSS [training: 0.018021071566439208 | validation: 0.01807309208778686]
	TIME [epoch: 8.81 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018587668814605017		[learning rate: 0.00039004]
		[batch 20/20] avg loss: 0.025715074650713793		[learning rate: 0.00038956]
	Learning Rate: 0.000389564
	LOSS [training: 0.022151371732659405 | validation: 0.014368281880009853]
	TIME [epoch: 8.81 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011082075056706709		[learning rate: 0.00038909]
		[batch 20/20] avg loss: 0.02569216705533984		[learning rate: 0.00038862]
	Learning Rate: 0.000388621
	LOSS [training: 0.018387121056023272 | validation: 0.019853880668387305]
	TIME [epoch: 8.79 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009967459179729266		[learning rate: 0.00038815]
		[batch 20/20] avg loss: 0.02075751828718868		[learning rate: 0.00038768]
	Learning Rate: 0.00038768
	LOSS [training: 0.015362488733458971 | validation: 0.024655851521484456]
	TIME [epoch: 8.77 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01653687813605419		[learning rate: 0.00038721]
		[batch 20/20] avg loss: 0.016211668032980873		[learning rate: 0.00038674]
	Learning Rate: 0.000386742
	LOSS [training: 0.016374273084517534 | validation: 0.021373317511093372]
	TIME [epoch: 8.78 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018631936185984417		[learning rate: 0.00038627]
		[batch 20/20] avg loss: 0.014937497039243658		[learning rate: 0.00038581]
	Learning Rate: 0.000385805
	LOSS [training: 0.016784716612614037 | validation: 0.024249413743008427]
	TIME [epoch: 8.8 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018322124387531693		[learning rate: 0.00038534]
		[batch 20/20] avg loss: 0.017450149328931575		[learning rate: 0.00038487]
	Learning Rate: 0.000384872
	LOSS [training: 0.01788613685823163 | validation: 0.020043714688779322]
	TIME [epoch: 8.78 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018064201152466798		[learning rate: 0.00038441]
		[batch 20/20] avg loss: 0.011819643135583726		[learning rate: 0.00038394]
	Learning Rate: 0.00038394
	LOSS [training: 0.014941922144025257 | validation: 0.019162873381058917]
	TIME [epoch: 8.79 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02293093327278221		[learning rate: 0.00038347]
		[batch 20/20] avg loss: 0.025482257365773253		[learning rate: 0.00038301]
	Learning Rate: 0.00038301
	LOSS [training: 0.024206595319277734 | validation: 0.01828229689530429]
	TIME [epoch: 8.77 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015267848883074118		[learning rate: 0.00038255]
		[batch 20/20] avg loss: 0.014596883856172305		[learning rate: 0.00038208]
	Learning Rate: 0.000382083
	LOSS [training: 0.014932366369623215 | validation: 0.01624146968536702]
	TIME [epoch: 8.78 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014308206182228089		[learning rate: 0.00038162]
		[batch 20/20] avg loss: 0.016513563432294587		[learning rate: 0.00038116]
	Learning Rate: 0.000381158
	LOSS [training: 0.01541088480726134 | validation: 0.02035124972350573]
	TIME [epoch: 8.81 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020343567386872687		[learning rate: 0.0003807]
		[batch 20/20] avg loss: 0.008984649149236822		[learning rate: 0.00038024]
	Learning Rate: 0.000380235
	LOSS [training: 0.01466410826805475 | validation: 0.009585292535883682]
	TIME [epoch: 8.77 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01799461446969115		[learning rate: 0.00037977]
		[batch 20/20] avg loss: 0.019267934450982852		[learning rate: 0.00037931]
	Learning Rate: 0.000379315
	LOSS [training: 0.018631274460337 | validation: 0.016596380401345923]
	TIME [epoch: 8.77 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017492669365704008		[learning rate: 0.00037886]
		[batch 20/20] avg loss: 0.01417619576143275		[learning rate: 0.0003784]
	Learning Rate: 0.000378397
	LOSS [training: 0.01583443256356838 | validation: 0.014186955826483457]
	TIME [epoch: 8.79 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0201455047135244		[learning rate: 0.00037794]
		[batch 20/20] avg loss: 0.008443934762459412		[learning rate: 0.00037748]
	Learning Rate: 0.000377481
	LOSS [training: 0.014294719737991906 | validation: 0.01031792734748101]
	TIME [epoch: 8.8 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019157063002358744		[learning rate: 0.00037702]
		[batch 20/20] avg loss: 0.013786652625392744		[learning rate: 0.00037657]
	Learning Rate: 0.000376567
	LOSS [training: 0.016471857813875744 | validation: 0.01769984470947528]
	TIME [epoch: 8.8 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01912160077974942		[learning rate: 0.00037611]
		[batch 20/20] avg loss: 0.011971800672132566		[learning rate: 0.00037566]
	Learning Rate: 0.000375655
	LOSS [training: 0.015546700725940993 | validation: 0.013846116609788123]
	TIME [epoch: 8.79 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022382078966915228		[learning rate: 0.0003752]
		[batch 20/20] avg loss: 0.019178912691999336		[learning rate: 0.00037475]
	Learning Rate: 0.000374746
	LOSS [training: 0.020780495829457284 | validation: 0.013603033205351129]
	TIME [epoch: 8.79 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02419011879063271		[learning rate: 0.00037429]
		[batch 20/20] avg loss: 0.034123415922591833		[learning rate: 0.00037384]
	Learning Rate: 0.000373839
	LOSS [training: 0.029156767356612275 | validation: 0.01660956029878513]
	TIME [epoch: 8.78 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020021381205591392		[learning rate: 0.00037339]
		[batch 20/20] avg loss: 0.031055534567092008		[learning rate: 0.00037293]
	Learning Rate: 0.000372934
	LOSS [training: 0.0255384578863417 | validation: 0.019398833037029797]
	TIME [epoch: 8.8 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01957364840617453		[learning rate: 0.00037248]
		[batch 20/20] avg loss: 0.020461418890373572		[learning rate: 0.00037203]
	Learning Rate: 0.000372031
	LOSS [training: 0.02001753364827405 | validation: 0.03488089653936101]
	TIME [epoch: 8.79 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026565309581430708		[learning rate: 0.00037158]
		[batch 20/20] avg loss: 0.029817141385072403		[learning rate: 0.00037113]
	Learning Rate: 0.00037113
	LOSS [training: 0.028191225483251557 | validation: 0.023210082957782912]
	TIME [epoch: 8.79 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025993158027670604		[learning rate: 0.00037068]
		[batch 20/20] avg loss: 0.025398748326001912		[learning rate: 0.00037023]
	Learning Rate: 0.000370232
	LOSS [training: 0.02569595317683626 | validation: 0.035120494437234515]
	TIME [epoch: 8.78 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025457030609025183		[learning rate: 0.00036978]
		[batch 20/20] avg loss: 0.013880106253440971		[learning rate: 0.00036934]
	Learning Rate: 0.000369336
	LOSS [training: 0.019668568431233074 | validation: 0.01666684243882882]
	TIME [epoch: 8.77 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009828843811778959		[learning rate: 0.00036889]
		[batch 20/20] avg loss: 0.01761772395459128		[learning rate: 0.00036844]
	Learning Rate: 0.000368441
	LOSS [training: 0.01372328388318512 | validation: 0.009787627022501404]
	TIME [epoch: 8.81 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015534084456154404		[learning rate: 0.000368]
		[batch 20/20] avg loss: 0.012742854943093851		[learning rate: 0.00036755]
	Learning Rate: 0.00036755
	LOSS [training: 0.014138469699624127 | validation: 0.014609325701991527]
	TIME [epoch: 8.77 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016448873734945857		[learning rate: 0.0003671]
		[batch 20/20] avg loss: 0.019529078214238806		[learning rate: 0.00036666]
	Learning Rate: 0.00036666
	LOSS [training: 0.017988975974592335 | validation: 0.015413743467195108]
	TIME [epoch: 8.79 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013689851711840317		[learning rate: 0.00036622]
		[batch 20/20] avg loss: 0.015157802697629901		[learning rate: 0.00036577]
	Learning Rate: 0.000365772
	LOSS [training: 0.014423827204735108 | validation: 0.024654848142499644]
	TIME [epoch: 8.79 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020791987519769052		[learning rate: 0.00036533]
		[batch 20/20] avg loss: 0.019706735686001817		[learning rate: 0.00036489]
	Learning Rate: 0.000364887
	LOSS [training: 0.020249361602885436 | validation: 0.020947328491526734]
	TIME [epoch: 8.79 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02645144477794596		[learning rate: 0.00036444]
		[batch 20/20] avg loss: 0.024374016518519684		[learning rate: 0.000364]
	Learning Rate: 0.000364003
	LOSS [training: 0.025412730648232823 | validation: 0.01314078827930281]
	TIME [epoch: 8.82 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015170219018671377		[learning rate: 0.00036356]
		[batch 20/20] avg loss: 0.02031236984406764		[learning rate: 0.00036312]
	Learning Rate: 0.000363122
	LOSS [training: 0.017741294431369507 | validation: 0.032895870173688786]
	TIME [epoch: 8.78 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015655737704489917		[learning rate: 0.00036268]
		[batch 20/20] avg loss: 0.008794627146709628		[learning rate: 0.00036224]
	Learning Rate: 0.000362243
	LOSS [training: 0.01222518242559977 | validation: 0.016926104131861262]
	TIME [epoch: 8.79 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013365098212030135		[learning rate: 0.0003618]
		[batch 20/20] avg loss: 0.01851819446233701		[learning rate: 0.00036137]
	Learning Rate: 0.000361366
	LOSS [training: 0.015941646337183573 | validation: 0.018503445827655684]
	TIME [epoch: 8.77 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011678150853229905		[learning rate: 0.00036093]
		[batch 20/20] avg loss: 0.012266023897181556		[learning rate: 0.00036049]
	Learning Rate: 0.000360491
	LOSS [training: 0.01197208737520573 | validation: 0.02016028433069759]
	TIME [epoch: 8.8 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01376236196912324		[learning rate: 0.00036005]
		[batch 20/20] avg loss: 0.012793606483425446		[learning rate: 0.00035962]
	Learning Rate: 0.000359619
	LOSS [training: 0.013277984226274344 | validation: 0.021257117955725623]
	TIME [epoch: 8.79 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01629598388476671		[learning rate: 0.00035918]
		[batch 20/20] avg loss: 0.005762937375486584		[learning rate: 0.00035875]
	Learning Rate: 0.000358748
	LOSS [training: 0.011029460630126648 | validation: 0.015219573458195922]
	TIME [epoch: 8.78 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009875872982857291		[learning rate: 0.00035831]
		[batch 20/20] avg loss: 0.016173605284860833		[learning rate: 0.00035788]
	Learning Rate: 0.00035788
	LOSS [training: 0.013024739133859061 | validation: 0.023173945346257854]
	TIME [epoch: 8.78 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027644657021970147		[learning rate: 0.00035745]
		[batch 20/20] avg loss: 0.02147002454725742		[learning rate: 0.00035701]
	Learning Rate: 0.000357013
	LOSS [training: 0.024557340784613784 | validation: 0.01927059568873579]
	TIME [epoch: 8.78 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014988375223454701		[learning rate: 0.00035658]
		[batch 20/20] avg loss: 0.00892694700011554		[learning rate: 0.00035615]
	Learning Rate: 0.000356149
	LOSS [training: 0.01195766111178512 | validation: 0.011654016757445457]
	TIME [epoch: 8.8 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014820104346759445		[learning rate: 0.00035572]
		[batch 20/20] avg loss: 0.022427812775707035		[learning rate: 0.00035529]
	Learning Rate: 0.000355287
	LOSS [training: 0.01862395856123324 | validation: 0.020910221366486594]
	TIME [epoch: 8.78 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017374990092879967		[learning rate: 0.00035486]
		[batch 20/20] avg loss: 0.024013100461708806		[learning rate: 0.00035443]
	Learning Rate: 0.000354427
	LOSS [training: 0.020694045277294383 | validation: 0.02459611331623803]
	TIME [epoch: 8.78 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022544739561246342		[learning rate: 0.000354]
		[batch 20/20] avg loss: 0.023346991465623794		[learning rate: 0.00035357]
	Learning Rate: 0.000353569
	LOSS [training: 0.022945865513435065 | validation: 0.027429888041584846]
	TIME [epoch: 8.79 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014809641473946367		[learning rate: 0.00035314]
		[batch 20/20] avg loss: 0.02337124733249287		[learning rate: 0.00035271]
	Learning Rate: 0.000352713
	LOSS [training: 0.01909044440321962 | validation: 0.018938747424922665]
	TIME [epoch: 8.78 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02084390637863926		[learning rate: 0.00035229]
		[batch 20/20] avg loss: 0.013220610364489424		[learning rate: 0.00035186]
	Learning Rate: 0.000351859
	LOSS [training: 0.017032258371564343 | validation: 0.021620947218390068]
	TIME [epoch: 8.8 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023986617105928566		[learning rate: 0.00035143]
		[batch 20/20] avg loss: 0.01578542298135084		[learning rate: 0.00035101]
	Learning Rate: 0.000351007
	LOSS [training: 0.019886020043639704 | validation: 0.025667328559064742]
	TIME [epoch: 8.78 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014593480403450468		[learning rate: 0.00035058]
		[batch 20/20] avg loss: 0.009768407692890193		[learning rate: 0.00035016]
	Learning Rate: 0.000350157
	LOSS [training: 0.01218094404817033 | validation: 0.022038308261035405]
	TIME [epoch: 8.78 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010534155409385789		[learning rate: 0.00034973]
		[batch 20/20] avg loss: 0.018007037587286612		[learning rate: 0.00034931]
	Learning Rate: 0.00034931
	LOSS [training: 0.014270596498336199 | validation: 0.009334950671588616]
	TIME [epoch: 8.78 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018126946784561063		[learning rate: 0.00034889]
		[batch 20/20] avg loss: 0.019156112954831463		[learning rate: 0.00034846]
	Learning Rate: 0.000348464
	LOSS [training: 0.01864152986969626 | validation: 0.03508850458194601]
	TIME [epoch: 8.77 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01987932616144454		[learning rate: 0.00034804]
		[batch 20/20] avg loss: 0.014674229092977003		[learning rate: 0.00034762]
	Learning Rate: 0.00034762
	LOSS [training: 0.017276777627210773 | validation: 0.0160589358812538]
	TIME [epoch: 8.81 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01725328782633149		[learning rate: 0.0003472]
		[batch 20/20] avg loss: 0.022436551762570364		[learning rate: 0.00034678]
	Learning Rate: 0.000346779
	LOSS [training: 0.019844919794450924 | validation: 0.014067984496361152]
	TIME [epoch: 8.77 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020298537512417725		[learning rate: 0.00034636]
		[batch 20/20] avg loss: 0.009839757558642418		[learning rate: 0.00034594]
	Learning Rate: 0.000345939
	LOSS [training: 0.015069147535530073 | validation: 0.01957568297041451]
	TIME [epoch: 8.79 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03021735826108431		[learning rate: 0.00034552]
		[batch 20/20] avg loss: 0.01671250578963382		[learning rate: 0.0003451]
	Learning Rate: 0.000345102
	LOSS [training: 0.023464932025359066 | validation: 0.021707955303067204]
	TIME [epoch: 8.78 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015173210397430225		[learning rate: 0.00034468]
		[batch 20/20] avg loss: 0.010777389817214766		[learning rate: 0.00034427]
	Learning Rate: 0.000344267
	LOSS [training: 0.012975300107322497 | validation: 0.013582368147549567]
	TIME [epoch: 8.79 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013073799871860987		[learning rate: 0.00034385]
		[batch 20/20] avg loss: 0.015114313810433983		[learning rate: 0.00034343]
	Learning Rate: 0.000343433
	LOSS [training: 0.014094056841147484 | validation: 0.020787421748876905]
	TIME [epoch: 8.79 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02009478517089028		[learning rate: 0.00034302]
		[batch 20/20] avg loss: 0.007528689521518037		[learning rate: 0.0003426]
	Learning Rate: 0.000342602
	LOSS [training: 0.013811737346204158 | validation: 0.014778565079283032]
	TIME [epoch: 8.78 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014259814964555046		[learning rate: 0.00034219]
		[batch 20/20] avg loss: 0.02164915758788221		[learning rate: 0.00034177]
	Learning Rate: 0.000341772
	LOSS [training: 0.01795448627621863 | validation: 0.021149405629960826]
	TIME [epoch: 8.79 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02345772303867509		[learning rate: 0.00034136]
		[batch 20/20] avg loss: 0.016332241252780037		[learning rate: 0.00034094]
	Learning Rate: 0.000340945
	LOSS [training: 0.019894982145727567 | validation: 0.034605395468932036]
	TIME [epoch: 8.79 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016593351610529973		[learning rate: 0.00034053]
		[batch 20/20] avg loss: 0.015429547381862621		[learning rate: 0.00034012]
	Learning Rate: 0.00034012
	LOSS [training: 0.016011449496196296 | validation: 0.015994293222979437]
	TIME [epoch: 8.79 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016546321576551566		[learning rate: 0.00033971]
		[batch 20/20] avg loss: 0.017788742768394806		[learning rate: 0.0003393]
	Learning Rate: 0.000339296
	LOSS [training: 0.017167532172473184 | validation: 0.014127213392220993]
	TIME [epoch: 8.79 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01269893838894407		[learning rate: 0.00033889]
		[batch 20/20] avg loss: 0.015155407102186663		[learning rate: 0.00033847]
	Learning Rate: 0.000338475
	LOSS [training: 0.013927172745565366 | validation: 0.018099599419304863]
	TIME [epoch: 8.77 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012604382573022687		[learning rate: 0.00033806]
		[batch 20/20] avg loss: 0.01858389709283996		[learning rate: 0.00033766]
	Learning Rate: 0.000337655
	LOSS [training: 0.015594139832931325 | validation: 0.016648098817545233]
	TIME [epoch: 8.79 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015897569926751447		[learning rate: 0.00033725]
		[batch 20/20] avg loss: 0.02367736307414098		[learning rate: 0.00033684]
	Learning Rate: 0.000336838
	LOSS [training: 0.019787466500446212 | validation: 0.0165865930067352]
	TIME [epoch: 8.78 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01829677987679517		[learning rate: 0.00033643]
		[batch 20/20] avg loss: 0.016580993833729737		[learning rate: 0.00033602]
	Learning Rate: 0.000336023
	LOSS [training: 0.01743888685526246 | validation: 0.01578349345046638]
	TIME [epoch: 8.82 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01121869948683583		[learning rate: 0.00033562]
		[batch 20/20] avg loss: 0.0103513268100476		[learning rate: 0.00033521]
	Learning Rate: 0.000335209
	LOSS [training: 0.010785013148441713 | validation: 0.017715483315105845]
	TIME [epoch: 8.78 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0201478356625009		[learning rate: 0.0003348]
		[batch 20/20] avg loss: 0.012649272121872095		[learning rate: 0.0003344]
	Learning Rate: 0.000334398
	LOSS [training: 0.016398553892186495 | validation: 0.020671021912260595]
	TIME [epoch: 8.78 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013097780808296346		[learning rate: 0.00033399]
		[batch 20/20] avg loss: 0.015923204469397705		[learning rate: 0.00033359]
	Learning Rate: 0.000333588
	LOSS [training: 0.014510492638847027 | validation: 0.018084757538799085]
	TIME [epoch: 8.78 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018621467045422453		[learning rate: 0.00033318]
		[batch 20/20] avg loss: 0.015055616967887245		[learning rate: 0.00033278]
	Learning Rate: 0.000332781
	LOSS [training: 0.016838542006654852 | validation: 0.026291929779017058]
	TIME [epoch: 8.78 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020961817816000306		[learning rate: 0.00033238]
		[batch 20/20] avg loss: 0.014967552462436104		[learning rate: 0.00033197]
	Learning Rate: 0.000331975
	LOSS [training: 0.017964685139218204 | validation: 0.017716400276999205]
	TIME [epoch: 8.8 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013936769868814727		[learning rate: 0.00033157]
		[batch 20/20] avg loss: 0.021889396015386177		[learning rate: 0.00033117]
	Learning Rate: 0.000331171
	LOSS [training: 0.01791308294210045 | validation: 0.015937421674811783]
	TIME [epoch: 8.79 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012962645018651806		[learning rate: 0.00033077]
		[batch 20/20] avg loss: 0.017155064145369256		[learning rate: 0.00033037]
	Learning Rate: 0.00033037
	LOSS [training: 0.01505885458201053 | validation: 0.011446255195280138]
	TIME [epoch: 8.78 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019530735859511116		[learning rate: 0.00032997]
		[batch 20/20] avg loss: 0.013793470590868004		[learning rate: 0.00032957]
	Learning Rate: 0.00032957
	LOSS [training: 0.01666210322518956 | validation: 0.015423168225618665]
	TIME [epoch: 8.79 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014417147170988565		[learning rate: 0.00032917]
		[batch 20/20] avg loss: 0.01460926931899199		[learning rate: 0.00032877]
	Learning Rate: 0.000328772
	LOSS [training: 0.014513208244990281 | validation: 0.028074126288905342]
	TIME [epoch: 8.77 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01601213226153055		[learning rate: 0.00032837]
		[batch 20/20] avg loss: 0.023985522714056264		[learning rate: 0.00032798]
	Learning Rate: 0.000327976
	LOSS [training: 0.019998827487793407 | validation: 0.018520373878897343]
	TIME [epoch: 8.79 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016268186462169713		[learning rate: 0.00032758]
		[batch 20/20] avg loss: 0.015055239969394101		[learning rate: 0.00032718]
	Learning Rate: 0.000327182
	LOSS [training: 0.015661713215781907 | validation: 0.03599094521827025]
	TIME [epoch: 8.77 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02036972988268914		[learning rate: 0.00032679]
		[batch 20/20] avg loss: 0.014165939672397746		[learning rate: 0.00032639]
	Learning Rate: 0.00032639
	LOSS [training: 0.017267834777543442 | validation: 0.01903263737334607]
	TIME [epoch: 8.77 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016587380619606247		[learning rate: 0.00032599]
		[batch 20/20] avg loss: 0.022396246438333777		[learning rate: 0.0003256]
	Learning Rate: 0.0003256
	LOSS [training: 0.01949181352897001 | validation: 0.030567451900263644]
	TIME [epoch: 8.78 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025370008694214147		[learning rate: 0.00032521]
		[batch 20/20] avg loss: 0.012704008305350642		[learning rate: 0.00032481]
	Learning Rate: 0.000324812
	LOSS [training: 0.01903700849978239 | validation: 0.008036392973864414]
	TIME [epoch: 8.79 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01402906978339774		[learning rate: 0.00032442]
		[batch 20/20] avg loss: 0.029594922548887192		[learning rate: 0.00032403]
	Learning Rate: 0.000324025
	LOSS [training: 0.021811996166142468 | validation: 0.014646849883236985]
	TIME [epoch: 8.78 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013994549818460869		[learning rate: 0.00032363]
		[batch 20/20] avg loss: 0.014682733273152764		[learning rate: 0.00032324]
	Learning Rate: 0.000323241
	LOSS [training: 0.014338641545806819 | validation: 0.019171228889836214]
	TIME [epoch: 8.78 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013220483564287091		[learning rate: 0.00032285]
		[batch 20/20] avg loss: 0.014786329162941928		[learning rate: 0.00032246]
	Learning Rate: 0.000322458
	LOSS [training: 0.014003406363614513 | validation: 0.015512761826525731]
	TIME [epoch: 8.78 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010092627096882868		[learning rate: 0.00032207]
		[batch 20/20] avg loss: 0.012586561508778713		[learning rate: 0.00032168]
	Learning Rate: 0.000321678
	LOSS [training: 0.01133959430283079 | validation: 0.010841263742606995]
	TIME [epoch: 8.78 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014171823121338384		[learning rate: 0.00032129]
		[batch 20/20] avg loss: 0.01746484472524614		[learning rate: 0.0003209]
	Learning Rate: 0.000320899
	LOSS [training: 0.015818333923292262 | validation: 0.016929961647762694]
	TIME [epoch: 8.81 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01562168384285236		[learning rate: 0.00032051]
		[batch 20/20] avg loss: 0.01760677795231293		[learning rate: 0.00032012]
	Learning Rate: 0.000320122
	LOSS [training: 0.016614230897582644 | validation: 0.015571052811788864]
	TIME [epoch: 8.8 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017838709352120542		[learning rate: 0.00031973]
		[batch 20/20] avg loss: 0.01539772672196123		[learning rate: 0.00031935]
	Learning Rate: 0.000319347
	LOSS [training: 0.01661821803704088 | validation: 0.024384294030705987]
	TIME [epoch: 8.79 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016055747482328502		[learning rate: 0.00031896]
		[batch 20/20] avg loss: 0.01474596527727071		[learning rate: 0.00031857]
	Learning Rate: 0.000318574
	LOSS [training: 0.015400856379799604 | validation: 0.009437210927754687]
	TIME [epoch: 8.78 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018832462672840066		[learning rate: 0.00031819]
		[batch 20/20] avg loss: 0.011434727237580915		[learning rate: 0.0003178]
	Learning Rate: 0.000317803
	LOSS [training: 0.015133594955210494 | validation: 0.02069012601907588]
	TIME [epoch: 8.8 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013106334617510262		[learning rate: 0.00031742]
		[batch 20/20] avg loss: 0.019169521130421106		[learning rate: 0.00031703]
	Learning Rate: 0.000317034
	LOSS [training: 0.016137927873965688 | validation: 0.009657227668097646]
	TIME [epoch: 8.79 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00759073610909647		[learning rate: 0.00031665]
		[batch 20/20] avg loss: 0.021874353523239076		[learning rate: 0.00031627]
	Learning Rate: 0.000316266
	LOSS [training: 0.014732544816167772 | validation: 0.020511552488814266]
	TIME [epoch: 8.8 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017656592985683485		[learning rate: 0.00031588]
		[batch 20/20] avg loss: 0.026353128562878313		[learning rate: 0.0003155]
	Learning Rate: 0.0003155
	LOSS [training: 0.022004860774280897 | validation: 0.021805427941529284]
	TIME [epoch: 8.78 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018801525666583652		[learning rate: 0.00031512]
		[batch 20/20] avg loss: 0.02190427841888976		[learning rate: 0.00031474]
	Learning Rate: 0.000314737
	LOSS [training: 0.020352902042736702 | validation: 0.015400694996504845]
	TIME [epoch: 8.78 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009786062463407949		[learning rate: 0.00031436]
		[batch 20/20] avg loss: 0.015799978523293786		[learning rate: 0.00031397]
	Learning Rate: 0.000313975
	LOSS [training: 0.012793020493350868 | validation: 0.013298340724638385]
	TIME [epoch: 8.81 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010439653006058952		[learning rate: 0.00031359]
		[batch 20/20] avg loss: 0.014663061931627147		[learning rate: 0.00031321]
	Learning Rate: 0.000313215
	LOSS [training: 0.01255135746884305 | validation: 0.030950332558824116]
	TIME [epoch: 8.78 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016945239234703292		[learning rate: 0.00031284]
		[batch 20/20] avg loss: 0.017533824991597564		[learning rate: 0.00031246]
	Learning Rate: 0.000312456
	LOSS [training: 0.017239532113150425 | validation: 0.02364326880651325]
	TIME [epoch: 8.79 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01994818681626956		[learning rate: 0.00031208]
		[batch 20/20] avg loss: 0.014476794847664506		[learning rate: 0.0003117]
	Learning Rate: 0.0003117
	LOSS [training: 0.01721249083196703 | validation: 0.020064665370884673]
	TIME [epoch: 8.77 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014801304691108647		[learning rate: 0.00031132]
		[batch 20/20] avg loss: 0.02111272584416875		[learning rate: 0.00031095]
	Learning Rate: 0.000310945
	LOSS [training: 0.0179570152676387 | validation: 0.011265998971617513]
	TIME [epoch: 8.79 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021907535068223925		[learning rate: 0.00031057]
		[batch 20/20] avg loss: 0.021627770315679338		[learning rate: 0.00031019]
	Learning Rate: 0.000310193
	LOSS [training: 0.021767652691951633 | validation: 0.018709216737074583]
	TIME [epoch: 8.81 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018280109421851883		[learning rate: 0.00030982]
		[batch 20/20] avg loss: 0.014403166824813332		[learning rate: 0.00030944]
	Learning Rate: 0.000309442
	LOSS [training: 0.016341638123332607 | validation: 0.023876727712152518]
	TIME [epoch: 8.8 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013136694179701577		[learning rate: 0.00030907]
		[batch 20/20] avg loss: 0.015667036102119756		[learning rate: 0.00030869]
	Learning Rate: 0.000308693
	LOSS [training: 0.014401865140910663 | validation: 0.015123939596397531]
	TIME [epoch: 8.79 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014716969583438978		[learning rate: 0.00030832]
		[batch 20/20] avg loss: 0.014164405486917667		[learning rate: 0.00030795]
	Learning Rate: 0.000307945
	LOSS [training: 0.014440687535178322 | validation: 0.015252688192173642]
	TIME [epoch: 8.78 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0149917700227861		[learning rate: 0.00030757]
		[batch 20/20] avg loss: 0.015165458584854343		[learning rate: 0.0003072]
	Learning Rate: 0.0003072
	LOSS [training: 0.015078614303820223 | validation: 0.007161504104927794]
	TIME [epoch: 8.78 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0042971787432566425		[learning rate: 0.00030683]
		[batch 20/20] avg loss: 0.020115115712967792		[learning rate: 0.00030646]
	Learning Rate: 0.000306456
	LOSS [training: 0.012206147228112217 | validation: 0.0172344521268158]
	TIME [epoch: 8.8 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019825651054572593		[learning rate: 0.00030609]
		[batch 20/20] avg loss: 0.03144844106805419		[learning rate: 0.00030571]
	Learning Rate: 0.000305714
	LOSS [training: 0.025637046061313395 | validation: 0.021824041851737683]
	TIME [epoch: 8.78 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023409916008235082		[learning rate: 0.00030534]
		[batch 20/20] avg loss: 0.018282454059008545		[learning rate: 0.00030497]
	Learning Rate: 0.000304974
	LOSS [training: 0.02084618503362181 | validation: 0.01800650530887547]
	TIME [epoch: 8.8 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011709509102410394		[learning rate: 0.0003046]
		[batch 20/20] avg loss: 0.014742063909948071		[learning rate: 0.00030424]
	Learning Rate: 0.000304236
	LOSS [training: 0.013225786506179232 | validation: 0.018183695324159466]
	TIME [epoch: 8.78 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013687401712118704		[learning rate: 0.00030387]
		[batch 20/20] avg loss: 0.01533926982203716		[learning rate: 0.0003035]
	Learning Rate: 0.000303499
	LOSS [training: 0.014513335767077931 | validation: 0.01037148689056612]
	TIME [epoch: 8.8 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012224847358642928		[learning rate: 0.00030313]
		[batch 20/20] avg loss: 0.014902574497610902		[learning rate: 0.00030276]
	Learning Rate: 0.000302765
	LOSS [training: 0.013563710928126915 | validation: 0.01633431131597016]
	TIME [epoch: 8.78 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007929910558526541		[learning rate: 0.0003024]
		[batch 20/20] avg loss: 0.008507371429028698		[learning rate: 0.00030203]
	Learning Rate: 0.000302032
	LOSS [training: 0.008218640993777619 | validation: 0.015008757877873518]
	TIME [epoch: 8.8 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014536664717444775		[learning rate: 0.00030167]
		[batch 20/20] avg loss: 0.016281210806642337		[learning rate: 0.0003013]
	Learning Rate: 0.000301301
	LOSS [training: 0.015408937762043556 | validation: 0.009349271374724799]
	TIME [epoch: 8.78 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01462890184466668		[learning rate: 0.00030094]
		[batch 20/20] avg loss: 0.010575609731314068		[learning rate: 0.00030057]
	Learning Rate: 0.000300571
	LOSS [training: 0.012602255787990374 | validation: 0.016490472791115898]
	TIME [epoch: 8.79 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014271942990950252		[learning rate: 0.00030021]
		[batch 20/20] avg loss: 0.019361116269334944		[learning rate: 0.00029984]
	Learning Rate: 0.000299844
	LOSS [training: 0.016816529630142598 | validation: 0.011584233360354107]
	TIME [epoch: 8.81 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013911190176913913		[learning rate: 0.00029948]
		[batch 20/20] avg loss: 0.020926514464172246		[learning rate: 0.00029912]
	Learning Rate: 0.000299118
	LOSS [training: 0.017418852320543084 | validation: 0.021372128747432002]
	TIME [epoch: 8.79 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020446003493119376		[learning rate: 0.00029876]
		[batch 20/20] avg loss: 0.005491624003031339		[learning rate: 0.00029839]
	Learning Rate: 0.000298394
	LOSS [training: 0.012968813748075356 | validation: 0.013213252647097353]
	TIME [epoch: 8.79 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010602683521029704		[learning rate: 0.00029803]
		[batch 20/20] avg loss: 0.018384686952131114		[learning rate: 0.00029767]
	Learning Rate: 0.000297671
	LOSS [training: 0.01449368523658041 | validation: 0.01611098866857704]
	TIME [epoch: 8.78 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020465217820103666		[learning rate: 0.00029731]
		[batch 20/20] avg loss: 0.013995880904029718		[learning rate: 0.00029695]
	Learning Rate: 0.000296951
	LOSS [training: 0.017230549362066687 | validation: 0.011705418119621776]
	TIME [epoch: 8.78 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010868791160872896		[learning rate: 0.00029659]
		[batch 20/20] avg loss: 0.0182575818533258		[learning rate: 0.00029623]
	Learning Rate: 0.000296232
	LOSS [training: 0.01456318650709935 | validation: 0.02820309772332559]
	TIME [epoch: 8.8 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020854067627701377		[learning rate: 0.00029587]
		[batch 20/20] avg loss: 0.014038739921391496		[learning rate: 0.00029551]
	Learning Rate: 0.000295515
	LOSS [training: 0.017446403774546437 | validation: 0.020880653755164118]
	TIME [epoch: 8.78 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017703941981154162		[learning rate: 0.00029516]
		[batch 20/20] avg loss: 0.014637598633567303		[learning rate: 0.0002948]
	Learning Rate: 0.000294799
	LOSS [training: 0.01617077030736073 | validation: 0.027762044442454704]
	TIME [epoch: 8.78 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022905324400870723		[learning rate: 0.00029444]
		[batch 20/20] avg loss: 0.012576893195471967		[learning rate: 0.00029409]
	Learning Rate: 0.000294086
	LOSS [training: 0.017741108798171347 | validation: 0.02864302689271992]
	TIME [epoch: 8.78 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024025078655702428		[learning rate: 0.00029373]
		[batch 20/20] avg loss: 0.01738116135903082		[learning rate: 0.00029337]
	Learning Rate: 0.000293374
	LOSS [training: 0.020703120007366623 | validation: 0.02446291570938132]
	TIME [epoch: 8.79 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018970647420482233		[learning rate: 0.00029302]
		[batch 20/20] avg loss: 0.026390064253352497		[learning rate: 0.00029266]
	Learning Rate: 0.000292663
	LOSS [training: 0.022680355836917365 | validation: 0.020956834365433323]
	TIME [epoch: 8.8 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02466500464968178		[learning rate: 0.00029231]
		[batch 20/20] avg loss: 0.010620598891970489		[learning rate: 0.00029195]
	Learning Rate: 0.000291955
	LOSS [training: 0.01764280177082613 | validation: 0.015383779675908904]
	TIME [epoch: 8.77 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0170868599472608		[learning rate: 0.0002916]
		[batch 20/20] avg loss: 0.0156079972102006		[learning rate: 0.00029125]
	Learning Rate: 0.000291248
	LOSS [training: 0.0163474285787307 | validation: 0.012040431662786632]
	TIME [epoch: 8.79 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012471953855357699		[learning rate: 0.0002909]
		[batch 20/20] avg loss: 0.0216602960229198		[learning rate: 0.00029054]
	Learning Rate: 0.000290543
	LOSS [training: 0.017066124939138748 | validation: 0.022871724908786494]
	TIME [epoch: 8.77 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009121213388183886		[learning rate: 0.00029019]
		[batch 20/20] avg loss: 0.015238527241219945		[learning rate: 0.00028984]
	Learning Rate: 0.00028984
	LOSS [training: 0.012179870314701914 | validation: 0.014312968206184487]
	TIME [epoch: 8.8 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0155854285437492		[learning rate: 0.00028949]
		[batch 20/20] avg loss: 0.012982244832954853		[learning rate: 0.00028914]
	Learning Rate: 0.000289138
	LOSS [training: 0.014283836688352026 | validation: 0.018995730800384206]
	TIME [epoch: 8.79 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018901025506430007		[learning rate: 0.00028879]
		[batch 20/20] avg loss: 0.015867113229497732		[learning rate: 0.00028844]
	Learning Rate: 0.000288438
	LOSS [training: 0.017384069367963873 | validation: 0.005888373385016729]
	TIME [epoch: 8.77 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014512204808172769		[learning rate: 0.00028809]
		[batch 20/20] avg loss: 0.017803182592006292		[learning rate: 0.00028774]
	Learning Rate: 0.00028774
	LOSS [training: 0.016157693700089532 | validation: 0.012665329284768803]
	TIME [epoch: 8.78 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011376545812908886		[learning rate: 0.00028739]
		[batch 20/20] avg loss: 0.014591777362508971		[learning rate: 0.00028704]
	Learning Rate: 0.000287043
	LOSS [training: 0.012984161587708926 | validation: 0.015597464048444351]
	TIME [epoch: 8.77 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012604272171972242		[learning rate: 0.0002867]
		[batch 20/20] avg loss: 0.018742668007936152		[learning rate: 0.00028635]
	Learning Rate: 0.000286348
	LOSS [training: 0.015673470089954195 | validation: 0.02142424040403261]
	TIME [epoch: 8.81 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01462395947784284		[learning rate: 0.000286]
		[batch 20/20] avg loss: 0.013538941029668886		[learning rate: 0.00028566]
	Learning Rate: 0.000285655
	LOSS [training: 0.014081450253755862 | validation: 0.013352706906701622]
	TIME [epoch: 8.78 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021313177287138947		[learning rate: 0.00028531]
		[batch 20/20] avg loss: 0.018224414763202655		[learning rate: 0.00028496]
	Learning Rate: 0.000284964
	LOSS [training: 0.0197687960251708 | validation: 0.01630675577097091]
	TIME [epoch: 8.78 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010212435973048491		[learning rate: 0.00028462]
		[batch 20/20] avg loss: 0.016520878513115442		[learning rate: 0.00028427]
	Learning Rate: 0.000284274
	LOSS [training: 0.013366657243081967 | validation: 0.009398372545987002]
	TIME [epoch: 8.78 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011583160190694074		[learning rate: 0.00028393]
		[batch 20/20] avg loss: 0.01156475946555357		[learning rate: 0.00028359]
	Learning Rate: 0.000283586
	LOSS [training: 0.011573959828123823 | validation: 0.013434970498618823]
	TIME [epoch: 8.78 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016462706900314896		[learning rate: 0.00028324]
		[batch 20/20] avg loss: 0.015883460340958916		[learning rate: 0.0002829]
	Learning Rate: 0.000282899
	LOSS [training: 0.016173083620636908 | validation: 0.01583651602850142]
	TIME [epoch: 8.81 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009939842302317713		[learning rate: 0.00028256]
		[batch 20/20] avg loss: 0.010398269455194896		[learning rate: 0.00028221]
	Learning Rate: 0.000282214
	LOSS [training: 0.010169055878756305 | validation: 0.012358764822027956]
	TIME [epoch: 8.77 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01534104044340127		[learning rate: 0.00028187]
		[batch 20/20] avg loss: 0.009212031842017906		[learning rate: 0.00028153]
	Learning Rate: 0.000281531
	LOSS [training: 0.012276536142709588 | validation: 0.014277254774545362]
	TIME [epoch: 8.78 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010378377183394053		[learning rate: 0.00028119]
		[batch 20/20] avg loss: 0.01594472150400618		[learning rate: 0.00028085]
	Learning Rate: 0.000280849
	LOSS [training: 0.013161549343700115 | validation: 0.02222850454964474]
	TIME [epoch: 8.8 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012335814033452127		[learning rate: 0.00028051]
		[batch 20/20] avg loss: 0.016330935933673792		[learning rate: 0.00028017]
	Learning Rate: 0.00028017
	LOSS [training: 0.01433337498356296 | validation: 0.012221656288752249]
	TIME [epoch: 8.8 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015627120775709453		[learning rate: 0.00027983]
		[batch 20/20] avg loss: 0.015932958085259137		[learning rate: 0.00027949]
	Learning Rate: 0.000279491
	LOSS [training: 0.015780039430484297 | validation: 0.019102075875764787]
	TIME [epoch: 8.81 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011827640221537043		[learning rate: 0.00027915]
		[batch 20/20] avg loss: 0.016310105830106985		[learning rate: 0.00027881]
	Learning Rate: 0.000278815
	LOSS [training: 0.014068873025822012 | validation: 0.01125833793320364]
	TIME [epoch: 8.77 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01136333166494794		[learning rate: 0.00027848]
		[batch 20/20] avg loss: 0.013131602690965801		[learning rate: 0.00027814]
	Learning Rate: 0.00027814
	LOSS [training: 0.01224746717795687 | validation: 0.010879080786875625]
	TIME [epoch: 8.78 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012550921820375784		[learning rate: 0.0002778]
		[batch 20/20] avg loss: 0.019025863790021164		[learning rate: 0.00027747]
	Learning Rate: 0.000277467
	LOSS [training: 0.015788392805198476 | validation: 0.014882961915831357]
	TIME [epoch: 8.77 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018636775734472543		[learning rate: 0.00027713]
		[batch 20/20] avg loss: 0.016724214534277634		[learning rate: 0.00027679]
	Learning Rate: 0.000276795
	LOSS [training: 0.01768049513437509 | validation: 0.020496916744793134]
	TIME [epoch: 8.8 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01821869478818346		[learning rate: 0.00027646]
		[batch 20/20] avg loss: 0.02551025937991283		[learning rate: 0.00027612]
	Learning Rate: 0.000276125
	LOSS [training: 0.021864477084048144 | validation: 0.01563055617000921]
	TIME [epoch: 8.79 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02010659913035883		[learning rate: 0.00027579]
		[batch 20/20] avg loss: 0.016489999388385766		[learning rate: 0.00027546]
	Learning Rate: 0.000275456
	LOSS [training: 0.018298299259372295 | validation: 0.012656587328488933]
	TIME [epoch: 8.78 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009933158940180348		[learning rate: 0.00027512]
		[batch 20/20] avg loss: 0.019790230193607837		[learning rate: 0.00027479]
	Learning Rate: 0.000274789
	LOSS [training: 0.01486169456689409 | validation: 0.010110005039326202]
	TIME [epoch: 8.78 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011716235741869762		[learning rate: 0.00027446]
		[batch 20/20] avg loss: 0.019801943896137004		[learning rate: 0.00027412]
	Learning Rate: 0.000274124
	LOSS [training: 0.015759089819003383 | validation: 0.022579024114446664]
	TIME [epoch: 8.78 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017453617485779355		[learning rate: 0.00027379]
		[batch 20/20] avg loss: 0.015563590663504896		[learning rate: 0.00027346]
	Learning Rate: 0.000273461
	LOSS [training: 0.016508604074642125 | validation: 0.007423564152858744]
	TIME [epoch: 8.8 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01901786307378987		[learning rate: 0.00027313]
		[batch 20/20] avg loss: 0.022174158102715384		[learning rate: 0.0002728]
	Learning Rate: 0.000272799
	LOSS [training: 0.02059601058825263 | validation: 0.020393014936180388]
	TIME [epoch: 8.78 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02514867240155192		[learning rate: 0.00027247]
		[batch 20/20] avg loss: 0.01756120144770002		[learning rate: 0.00027214]
	Learning Rate: 0.000272138
	LOSS [training: 0.02135493692462597 | validation: 0.018582709915942863]
	TIME [epoch: 8.78 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01847852132062551		[learning rate: 0.00027181]
		[batch 20/20] avg loss: 0.007013307482818096		[learning rate: 0.00027148]
	Learning Rate: 0.000271479
	LOSS [training: 0.012745914401721802 | validation: 0.004145294365547317]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_1588.pth
	Model improved!!!
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01733026351079641		[learning rate: 0.00027115]
		[batch 20/20] avg loss: 0.015498839424366934		[learning rate: 0.00027082]
	Learning Rate: 0.000270822
	LOSS [training: 0.01641455146758167 | validation: 0.02091978006359026]
	TIME [epoch: 8.79 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013094835218429982		[learning rate: 0.00027049]
		[batch 20/20] avg loss: 0.015777671152995325		[learning rate: 0.00027017]
	Learning Rate: 0.000270167
	LOSS [training: 0.014436253185712653 | validation: 0.022217382563461608]
	TIME [epoch: 8.79 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018533163297734852		[learning rate: 0.00026984]
		[batch 20/20] avg loss: 0.02106652872856837		[learning rate: 0.00026951]
	Learning Rate: 0.000269513
	LOSS [training: 0.01979984601315161 | validation: 0.019812780767182812]
	TIME [epoch: 8.81 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02631026560903993		[learning rate: 0.00026919]
		[batch 20/20] avg loss: 0.01918190081594732		[learning rate: 0.00026886]
	Learning Rate: 0.00026886
	LOSS [training: 0.022746083212493627 | validation: 0.011199128630753716]
	TIME [epoch: 8.77 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01123037002378802		[learning rate: 0.00026853]
		[batch 20/20] avg loss: 0.016965373048193875		[learning rate: 0.00026821]
	Learning Rate: 0.000268209
	LOSS [training: 0.01409787153599095 | validation: 0.019988725316535647]
	TIME [epoch: 8.78 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011259315154916813		[learning rate: 0.00026788]
		[batch 20/20] avg loss: 0.016467734921109624		[learning rate: 0.00026756]
	Learning Rate: 0.00026756
	LOSS [training: 0.013863525038013216 | validation: 0.017435031312490103]
	TIME [epoch: 8.79 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0131326725019718		[learning rate: 0.00026724]
		[batch 20/20] avg loss: 0.017511976551603862		[learning rate: 0.00026691]
	Learning Rate: 0.000266912
	LOSS [training: 0.01532232452678783 | validation: 0.012688243647028113]
	TIME [epoch: 8.79 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013179114006949887		[learning rate: 0.00026659]
		[batch 20/20] avg loss: 0.010523966962689922		[learning rate: 0.00026627]
	Learning Rate: 0.000266266
	LOSS [training: 0.011851540484819905 | validation: 0.018404307347020168]
	TIME [epoch: 8.78 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012577352323072905		[learning rate: 0.00026594]
		[batch 20/20] avg loss: 0.02207605685237795		[learning rate: 0.00026562]
	Learning Rate: 0.000265621
	LOSS [training: 0.017326704587725426 | validation: 0.02562558219166666]
	TIME [epoch: 8.77 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019795607871526576		[learning rate: 0.0002653]
		[batch 20/20] avg loss: 0.019804608116916235		[learning rate: 0.00026498]
	Learning Rate: 0.000264978
	LOSS [training: 0.019800107994221404 | validation: 0.019265254561879257]
	TIME [epoch: 8.78 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01898504081254565		[learning rate: 0.00026466]
		[batch 20/20] avg loss: 0.01770259233539239		[learning rate: 0.00026434]
	Learning Rate: 0.000264337
	LOSS [training: 0.01834381657396902 | validation: 0.02095190631462849]
	TIME [epoch: 8.79 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018026361537644767		[learning rate: 0.00026402]
		[batch 20/20] avg loss: 0.014816903013570198		[learning rate: 0.0002637]
	Learning Rate: 0.000263697
	LOSS [training: 0.016421632275607483 | validation: 0.014900535036866338]
	TIME [epoch: 8.79 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01336579317319527		[learning rate: 0.00026338]
		[batch 20/20] avg loss: 0.009142220344748785		[learning rate: 0.00026306]
	Learning Rate: 0.000263059
	LOSS [training: 0.01125400675897203 | validation: 0.006878582794614946]
	TIME [epoch: 8.78 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01571594907738796		[learning rate: 0.00026274]
		[batch 20/20] avg loss: 0.018786693506775105		[learning rate: 0.00026242]
	Learning Rate: 0.000262422
	LOSS [training: 0.017251321292081533 | validation: 0.019354582299658744]
	TIME [epoch: 8.79 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020257289697517826		[learning rate: 0.0002621]
		[batch 20/20] avg loss: 0.017574096300967568		[learning rate: 0.00026179]
	Learning Rate: 0.000261787
	LOSS [training: 0.018915692999242697 | validation: 0.011775935123711511]
	TIME [epoch: 8.79 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013090307927553682		[learning rate: 0.00026147]
		[batch 20/20] avg loss: 0.01249287651511014		[learning rate: 0.00026115]
	Learning Rate: 0.000261153
	LOSS [training: 0.01279159222133191 | validation: 0.007874500749736973]
	TIME [epoch: 8.79 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009966946254487855		[learning rate: 0.00026084]
		[batch 20/20] avg loss: 0.013220597196585457		[learning rate: 0.00026052]
	Learning Rate: 0.000260521
	LOSS [training: 0.011593771725536658 | validation: 0.005290851324954628]
	TIME [epoch: 8.78 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006658279412088941		[learning rate: 0.00026021]
		[batch 20/20] avg loss: 0.022129008509740054		[learning rate: 0.00025989]
	Learning Rate: 0.00025989
	LOSS [training: 0.014393643960914502 | validation: 0.020318628080334057]
	TIME [epoch: 8.77 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012151640430315759		[learning rate: 0.00025958]
		[batch 20/20] avg loss: 0.017751825110724265		[learning rate: 0.00025926]
	Learning Rate: 0.000259261
	LOSS [training: 0.01495173277052001 | validation: 0.015332818431642478]
	TIME [epoch: 8.78 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02117944523888245		[learning rate: 0.00025895]
		[batch 20/20] avg loss: 0.016585985919900344		[learning rate: 0.00025863]
	Learning Rate: 0.000258633
	LOSS [training: 0.018882715579391394 | validation: 0.008525676669030044]
	TIME [epoch: 8.79 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015969692120998503		[learning rate: 0.00025832]
		[batch 20/20] avg loss: 0.01454015325986199		[learning rate: 0.00025801]
	Learning Rate: 0.000258007
	LOSS [training: 0.015254922690430241 | validation: 0.015832420979370942]
	TIME [epoch: 8.8 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012514026724545222		[learning rate: 0.00025769]
		[batch 20/20] avg loss: 0.01760221980131873		[learning rate: 0.00025738]
	Learning Rate: 0.000257382
	LOSS [training: 0.015058123262931978 | validation: 0.005310877646823139]
	TIME [epoch: 8.78 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013323197758653729		[learning rate: 0.00025707]
		[batch 20/20] avg loss: 0.01272258843297984		[learning rate: 0.00025676]
	Learning Rate: 0.000256759
	LOSS [training: 0.013022893095816784 | validation: 0.015468785332972997]
	TIME [epoch: 8.77 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011613611749661381		[learning rate: 0.00025645]
		[batch 20/20] avg loss: 0.013203263007187918		[learning rate: 0.00025614]
	Learning Rate: 0.000256138
	LOSS [training: 0.01240843737842465 | validation: 0.012407633707819889]
	TIME [epoch: 8.78 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01094641249684326		[learning rate: 0.00025583]
		[batch 20/20] avg loss: 0.012435470751388907		[learning rate: 0.00025552]
	Learning Rate: 0.000255518
	LOSS [training: 0.011690941624116083 | validation: 0.01670455861098677]
	TIME [epoch: 8.8 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019254776731418174		[learning rate: 0.00025521]
		[batch 20/20] avg loss: 0.012851308611491039		[learning rate: 0.0002549]
	Learning Rate: 0.000254899
	LOSS [training: 0.016053042671454607 | validation: 0.010469210787373326]
	TIME [epoch: 8.8 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0067912994927095795		[learning rate: 0.00025459]
		[batch 20/20] avg loss: 0.01572980607393867		[learning rate: 0.00025428]
	Learning Rate: 0.000254282
	LOSS [training: 0.011260552783324124 | validation: 0.019830620139179203]
	TIME [epoch: 8.8 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01236037907087042		[learning rate: 0.00025397]
		[batch 20/20] avg loss: 0.01301716246078891		[learning rate: 0.00025367]
	Learning Rate: 0.000253667
	LOSS [training: 0.012688770765829662 | validation: 0.01594151523219631]
	TIME [epoch: 8.79 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015226274510902957		[learning rate: 0.00025336]
		[batch 20/20] avg loss: 0.018135253156437364		[learning rate: 0.00025305]
	Learning Rate: 0.000253052
	LOSS [training: 0.016680763833670162 | validation: 0.015563229657415809]
	TIME [epoch: 8.79 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012522596243655587		[learning rate: 0.00025275]
		[batch 20/20] avg loss: 0.015038690701934601		[learning rate: 0.00025244]
	Learning Rate: 0.00025244
	LOSS [training: 0.013780643472795095 | validation: 0.007604689583807262]
	TIME [epoch: 8.79 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019344566336081816		[learning rate: 0.00025213]
		[batch 20/20] avg loss: 0.002647548480897516		[learning rate: 0.00025183]
	Learning Rate: 0.000251829
	LOSS [training: 0.010996057408489667 | validation: 0.007846178327338249]
	TIME [epoch: 8.78 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009657893535858593		[learning rate: 0.00025152]
		[batch 20/20] avg loss: 0.012270636452553343		[learning rate: 0.00025122]
	Learning Rate: 0.000251219
	LOSS [training: 0.010964264994205967 | validation: 0.007444139021433054]
	TIME [epoch: 8.77 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011590631198890906		[learning rate: 0.00025091]
		[batch 20/20] avg loss: 0.009895552720102018		[learning rate: 0.00025061]
	Learning Rate: 0.000250611
	LOSS [training: 0.010743091959496461 | validation: 0.00897997908684684]
	TIME [epoch: 8.77 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010668110246237584		[learning rate: 0.00025031]
		[batch 20/20] avg loss: 0.012346324511227877		[learning rate: 0.00025]
	Learning Rate: 0.000250004
	LOSS [training: 0.011507217378732731 | validation: 0.015297413823566792]
	TIME [epoch: 8.79 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015274802843980606		[learning rate: 0.0002497]
		[batch 20/20] avg loss: 0.0081424690891771		[learning rate: 0.0002494]
	Learning Rate: 0.000249399
	LOSS [training: 0.01170863596657885 | validation: 0.008802046152515031]
	TIME [epoch: 8.8 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01384260328473233		[learning rate: 0.0002491]
		[batch 20/20] avg loss: 0.009518185744007198		[learning rate: 0.0002488]
	Learning Rate: 0.000248795
	LOSS [training: 0.011680394514369762 | validation: 0.013588388945383806]
	TIME [epoch: 8.78 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01888378491852557		[learning rate: 0.00024849]
		[batch 20/20] avg loss: 0.024251381760840116		[learning rate: 0.00024819]
	Learning Rate: 0.000248193
	LOSS [training: 0.021567583339682846 | validation: 0.024891934731202634]
	TIME [epoch: 8.78 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02101719052834776		[learning rate: 0.00024789]
		[batch 20/20] avg loss: 0.02049295096563669		[learning rate: 0.00024759]
	Learning Rate: 0.000247592
	LOSS [training: 0.020755070746992225 | validation: 0.018858283354352735]
	TIME [epoch: 8.77 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01619396116927073		[learning rate: 0.00024729]
		[batch 20/20] avg loss: 0.01831196647380478		[learning rate: 0.00024699]
	Learning Rate: 0.000246993
	LOSS [training: 0.017252963821537754 | validation: 0.02910590411537553]
	TIME [epoch: 8.78 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015196471076954548		[learning rate: 0.00024669]
		[batch 20/20] avg loss: 0.0109248811047512		[learning rate: 0.00024639]
	Learning Rate: 0.000246395
	LOSS [training: 0.013060676090852874 | validation: 0.013184818685622702]
	TIME [epoch: 8.8 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0143706694485908		[learning rate: 0.0002461]
		[batch 20/20] avg loss: 0.0180764586283715		[learning rate: 0.0002458]
	Learning Rate: 0.000245798
	LOSS [training: 0.01622356403848115 | validation: 0.03385657513623193]
	TIME [epoch: 8.8 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03002381793333561		[learning rate: 0.0002455]
		[batch 20/20] avg loss: 0.012474940418413618		[learning rate: 0.0002452]
	Learning Rate: 0.000245203
	LOSS [training: 0.02124937917587462 | validation: 0.017869900222484608]
	TIME [epoch: 8.8 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009143960105296136		[learning rate: 0.00024491]
		[batch 20/20] avg loss: 0.013871238090438201		[learning rate: 0.00024461]
	Learning Rate: 0.00024461
	LOSS [training: 0.011507599097867168 | validation: 0.012875755642724343]
	TIME [epoch: 8.77 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008435341755878841		[learning rate: 0.00024431]
		[batch 20/20] avg loss: 0.007154534259020152		[learning rate: 0.00024402]
	Learning Rate: 0.000244018
	LOSS [training: 0.007794938007449495 | validation: 0.01175914996120529]
	TIME [epoch: 8.8 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012211726643707697		[learning rate: 0.00024372]
		[batch 20/20] avg loss: 0.008531326456903769		[learning rate: 0.00024343]
	Learning Rate: 0.000243427
	LOSS [training: 0.010371526550305732 | validation: 0.00531495831141547]
	TIME [epoch: 8.77 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014384077714862381		[learning rate: 0.00024313]
		[batch 20/20] avg loss: 0.013166283584493132		[learning rate: 0.00024284]
	Learning Rate: 0.000242838
	LOSS [training: 0.013775180649677757 | validation: 0.012901883476736643]
	TIME [epoch: 8.78 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015064236651341411		[learning rate: 0.00024254]
		[batch 20/20] avg loss: 0.01496000189049213		[learning rate: 0.00024225]
	Learning Rate: 0.00024225
	LOSS [training: 0.015012119270916766 | validation: 0.004393022171979527]
	TIME [epoch: 8.78 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01672919565328894		[learning rate: 0.00024196]
		[batch 20/20] avg loss: 0.011885028187538083		[learning rate: 0.00024166]
	Learning Rate: 0.000241663
	LOSS [training: 0.014307111920413512 | validation: 0.015659605848196197]
	TIME [epoch: 8.78 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011942678769785805		[learning rate: 0.00024137]
		[batch 20/20] avg loss: 0.016315084209975163		[learning rate: 0.00024108]
	Learning Rate: 0.000241078
	LOSS [training: 0.014128881489880483 | validation: 0.010492992786591031]
	TIME [epoch: 8.8 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018632788382803165		[learning rate: 0.00024079]
		[batch 20/20] avg loss: 0.008376642731378978		[learning rate: 0.00024049]
	Learning Rate: 0.000240495
	LOSS [training: 0.013504715557091072 | validation: 0.009259750939409263]
	TIME [epoch: 8.77 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012254232229436604		[learning rate: 0.0002402]
		[batch 20/20] avg loss: 0.011924589099281622		[learning rate: 0.00023991]
	Learning Rate: 0.000239912
	LOSS [training: 0.012089410664359113 | validation: 0.01847762253776254]
	TIME [epoch: 8.78 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01598038397926421		[learning rate: 0.00023962]
		[batch 20/20] avg loss: 0.013063408743548421		[learning rate: 0.00023933]
	Learning Rate: 0.000239332
	LOSS [training: 0.014521896361406317 | validation: 0.013975344000778983]
	TIME [epoch: 8.77 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014457756788376287		[learning rate: 0.00023904]
		[batch 20/20] avg loss: 0.02121050206923946		[learning rate: 0.00023875]
	Learning Rate: 0.000238752
	LOSS [training: 0.017834129428807875 | validation: 0.013473618321936411]
	TIME [epoch: 8.78 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010741595675836938		[learning rate: 0.00023846]
		[batch 20/20] avg loss: 0.013512286858241091		[learning rate: 0.00023817]
	Learning Rate: 0.000238174
	LOSS [training: 0.012126941267039015 | validation: 0.01587971164299889]
	TIME [epoch: 8.82 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012310141029229896		[learning rate: 0.00023789]
		[batch 20/20] avg loss: 0.012899312391153508		[learning rate: 0.0002376]
	Learning Rate: 0.000237598
	LOSS [training: 0.012604726710191704 | validation: 0.010718338648337608]
	TIME [epoch: 8.79 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010258099122957724		[learning rate: 0.00023731]
		[batch 20/20] avg loss: 0.011624179601599092		[learning rate: 0.00023702]
	Learning Rate: 0.000237022
	LOSS [training: 0.010941139362278408 | validation: 0.010935504684884535]
	TIME [epoch: 8.79 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011014592172353123		[learning rate: 0.00023674]
		[batch 20/20] avg loss: 0.01662129543506453		[learning rate: 0.00023645]
	Learning Rate: 0.000236449
	LOSS [training: 0.013817943803708826 | validation: 0.00941455201525713]
	TIME [epoch: 8.77 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016518261175214582		[learning rate: 0.00023616]
		[batch 20/20] avg loss: 0.016757914806749073		[learning rate: 0.00023588]
	Learning Rate: 0.000235876
	LOSS [training: 0.016638087990981827 | validation: 0.019482553558603774]
	TIME [epoch: 8.79 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019556262123709856		[learning rate: 0.00023559]
		[batch 20/20] avg loss: 0.012664277399049947		[learning rate: 0.00023531]
	Learning Rate: 0.000235305
	LOSS [training: 0.0161102697613799 | validation: 0.01147055309458729]
	TIME [epoch: 8.79 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014890428026905975		[learning rate: 0.00023502]
		[batch 20/20] avg loss: 0.011821568179125223		[learning rate: 0.00023474]
	Learning Rate: 0.000234736
	LOSS [training: 0.013355998103015595 | validation: 0.009926901866505772]
	TIME [epoch: 8.78 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01554436335365068		[learning rate: 0.00023445]
		[batch 20/20] avg loss: 0.013523390073481684		[learning rate: 0.00023417]
	Learning Rate: 0.000234167
	LOSS [training: 0.014533876713566185 | validation: 0.01793842949876233]
	TIME [epoch: 8.78 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01837417693896772		[learning rate: 0.00023388]
		[batch 20/20] avg loss: 0.01301781354645338		[learning rate: 0.0002336]
	Learning Rate: 0.0002336
	LOSS [training: 0.015695995242710547 | validation: 0.008399802654321633]
	TIME [epoch: 8.77 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011955422267931112		[learning rate: 0.00023332]
		[batch 20/20] avg loss: 0.00989246934176312		[learning rate: 0.00023303]
	Learning Rate: 0.000233035
	LOSS [training: 0.010923945804847115 | validation: 0.009884375854465747]
	TIME [epoch: 8.8 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008310084586568274		[learning rate: 0.00023275]
		[batch 20/20] avg loss: 0.01463966594211041		[learning rate: 0.00023247]
	Learning Rate: 0.000232471
	LOSS [training: 0.011474875264339341 | validation: 0.01810042916671688]
	TIME [epoch: 8.78 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018150363675136692		[learning rate: 0.00023219]
		[batch 20/20] avg loss: 0.011458895273186347		[learning rate: 0.00023191]
	Learning Rate: 0.000231908
	LOSS [training: 0.01480462947416152 | validation: 0.014078515389280417]
	TIME [epoch: 8.77 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012119726389890437		[learning rate: 0.00023163]
		[batch 20/20] avg loss: 0.019462569208906023		[learning rate: 0.00023135]
	Learning Rate: 0.000231347
	LOSS [training: 0.015791147799398227 | validation: 0.012799549969715109]
	TIME [epoch: 8.78 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01429620944155965		[learning rate: 0.00023107]
		[batch 20/20] avg loss: 0.010911325468252304		[learning rate: 0.00023079]
	Learning Rate: 0.000230787
	LOSS [training: 0.01260376745490598 | validation: 0.010792196358166077]
	TIME [epoch: 8.78 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013147204356523121		[learning rate: 0.00023051]
		[batch 20/20] avg loss: 0.012291315729727534		[learning rate: 0.00023023]
	Learning Rate: 0.000230228
	LOSS [training: 0.012719260043125328 | validation: 0.013570746973795213]
	TIME [epoch: 8.82 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01552041123889307		[learning rate: 0.00022995]
		[batch 20/20] avg loss: 0.013823759936682268		[learning rate: 0.00022967]
	Learning Rate: 0.000229671
	LOSS [training: 0.014672085587787667 | validation: 0.01017761047805448]
	TIME [epoch: 8.77 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01957554012818203		[learning rate: 0.00022939]
		[batch 20/20] avg loss: 0.012974794990254095		[learning rate: 0.00022911]
	Learning Rate: 0.000229115
	LOSS [training: 0.016275167559218064 | validation: 0.013354085530532238]
	TIME [epoch: 8.78 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015906061857534324		[learning rate: 0.00022884]
		[batch 20/20] avg loss: 0.013095655721115988		[learning rate: 0.00022856]
	Learning Rate: 0.00022856
	LOSS [training: 0.014500858789325152 | validation: 0.011923251481963368]
	TIME [epoch: 8.79 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015618683201525041		[learning rate: 0.00022828]
		[batch 20/20] avg loss: 0.012305193754981805		[learning rate: 0.00022801]
	Learning Rate: 0.000228007
	LOSS [training: 0.013961938478253421 | validation: 0.010222671599577201]
	TIME [epoch: 8.79 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013098133316831267		[learning rate: 0.00022773]
		[batch 20/20] avg loss: 0.012841238340791362		[learning rate: 0.00022745]
	Learning Rate: 0.000227455
	LOSS [training: 0.012969685828811315 | validation: 0.0023011427037448544]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_1661.pth
	Model improved!!!
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013655731799107187		[learning rate: 0.00022718]
		[batch 20/20] avg loss: 0.0100897666700616		[learning rate: 0.0002269]
	Learning Rate: 0.000226904
	LOSS [training: 0.011872749234584394 | validation: 0.014046412245578106]
	TIME [epoch: 8.78 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015153834528521365		[learning rate: 0.00022663]
		[batch 20/20] avg loss: 0.007552554397381714		[learning rate: 0.00022635]
	Learning Rate: 0.000226355
	LOSS [training: 0.01135319446295154 | validation: 0.013994071981326734]
	TIME [epoch: 8.78 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016910475235635953		[learning rate: 0.00022608]
		[batch 20/20] avg loss: 0.00807347771234882		[learning rate: 0.00022581]
	Learning Rate: 0.000225807
	LOSS [training: 0.012491976473992385 | validation: 0.013795063638356801]
	TIME [epoch: 8.78 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011514090391260848		[learning rate: 0.00022553]
		[batch 20/20] avg loss: 0.008027251088581204		[learning rate: 0.00022526]
	Learning Rate: 0.00022526
	LOSS [training: 0.009770670739921026 | validation: 0.013445234293597733]
	TIME [epoch: 8.78 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013914771627160708		[learning rate: 0.00022499]
		[batch 20/20] avg loss: 0.01868718723508716		[learning rate: 0.00022471]
	Learning Rate: 0.000224715
	LOSS [training: 0.016300979431123932 | validation: 0.013705987499962461]
	TIME [epoch: 8.79 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019748383937852325		[learning rate: 0.00022444]
		[batch 20/20] avg loss: 0.013134016185419883		[learning rate: 0.00022417]
	Learning Rate: 0.000224171
	LOSS [training: 0.016441200061636106 | validation: 0.0073832735899573435]
	TIME [epoch: 8.77 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011018629872117143		[learning rate: 0.0002239]
		[batch 20/20] avg loss: 0.01340893889406311		[learning rate: 0.00022363]
	Learning Rate: 0.000223628
	LOSS [training: 0.012213784383090126 | validation: 0.020477511717742484]
	TIME [epoch: 8.78 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013980338058536526		[learning rate: 0.00022336]
		[batch 20/20] avg loss: 0.015233989806235571		[learning rate: 0.00022309]
	Learning Rate: 0.000223087
	LOSS [training: 0.01460716393238605 | validation: 0.01168575972947779]
	TIME [epoch: 8.77 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015391944799345855		[learning rate: 0.00022282]
		[batch 20/20] avg loss: 0.017699213456033012		[learning rate: 0.00022255]
	Learning Rate: 0.000222547
	LOSS [training: 0.016545579127689433 | validation: 0.010072392418066853]
	TIME [epoch: 8.8 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020136453425007438		[learning rate: 0.00022228]
		[batch 20/20] avg loss: 0.015115852366448135		[learning rate: 0.00022201]
	Learning Rate: 0.000222008
	LOSS [training: 0.017626152895727786 | validation: 0.008400198151296338]
	TIME [epoch: 8.8 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012550374264101017		[learning rate: 0.00022174]
		[batch 20/20] avg loss: 0.016400712802256762		[learning rate: 0.00022147]
	Learning Rate: 0.00022147
	LOSS [training: 0.014475543533178889 | validation: 0.00679707468437883]
	TIME [epoch: 8.78 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013533704467882973		[learning rate: 0.0002212]
		[batch 20/20] avg loss: 0.016701575142681026		[learning rate: 0.00022093]
	Learning Rate: 0.000220934
	LOSS [training: 0.015117639805282001 | validation: 0.012448591526120083]
	TIME [epoch: 8.78 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01219514370705078		[learning rate: 0.00022067]
		[batch 20/20] avg loss: 0.009456689094212087		[learning rate: 0.0002204]
	Learning Rate: 0.000220399
	LOSS [training: 0.010825916400631432 | validation: 0.015506792049875541]
	TIME [epoch: 8.77 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01279831474266344		[learning rate: 0.00022013]
		[batch 20/20] avg loss: 0.009195175096995648		[learning rate: 0.00021987]
	Learning Rate: 0.000219866
	LOSS [training: 0.010996744919829543 | validation: 0.010041937001492605]
	TIME [epoch: 8.81 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008826135591911713		[learning rate: 0.0002196]
		[batch 20/20] avg loss: 0.008622882690490013		[learning rate: 0.00021933]
	Learning Rate: 0.000219334
	LOSS [training: 0.008724509141200864 | validation: 0.016077255386976342]
	TIME [epoch: 8.78 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015543431725880016		[learning rate: 0.00021907]
		[batch 20/20] avg loss: 0.011949488574518422		[learning rate: 0.0002188]
	Learning Rate: 0.000218803
	LOSS [training: 0.01374646015019922 | validation: 0.01113814828984403]
	TIME [epoch: 8.78 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009475746084076746		[learning rate: 0.00021854]
		[batch 20/20] avg loss: 0.013832712646355596		[learning rate: 0.00021827]
	Learning Rate: 0.000218273
	LOSS [training: 0.011654229365216172 | validation: 0.012858369125546578]
	TIME [epoch: 8.78 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011931211597656508		[learning rate: 0.00021801]
		[batch 20/20] avg loss: 0.011068321130830991		[learning rate: 0.00021774]
	Learning Rate: 0.000217745
	LOSS [training: 0.011499766364243748 | validation: 0.01743589029993185]
	TIME [epoch: 8.79 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00809960834377571		[learning rate: 0.00021748]
		[batch 20/20] avg loss: 0.014191813764044802		[learning rate: 0.00021722]
	Learning Rate: 0.000217217
	LOSS [training: 0.011145711053910256 | validation: 0.001510402949306375]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_1680.pth
	Model improved!!!
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015810893358186744		[learning rate: 0.00021695]
		[batch 20/20] avg loss: 0.0131443569851623		[learning rate: 0.00021669]
	Learning Rate: 0.000216692
	LOSS [training: 0.01447762517167452 | validation: 0.015183635255490449]
	TIME [epoch: 8.78 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01812292795882833		[learning rate: 0.00021643]
		[batch 20/20] avg loss: 0.011706495806072354		[learning rate: 0.00021617]
	Learning Rate: 0.000216167
	LOSS [training: 0.014914711882450343 | validation: 0.014961211126978664]
	TIME [epoch: 8.78 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014471994272858025		[learning rate: 0.00021591]
		[batch 20/20] avg loss: 0.016644121103586637		[learning rate: 0.00021564]
	Learning Rate: 0.000215644
	LOSS [training: 0.01555805768822233 | validation: 0.01180153591804182]
	TIME [epoch: 8.79 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014528929476965685		[learning rate: 0.00021538]
		[batch 20/20] avg loss: 0.024538775452564637		[learning rate: 0.00021512]
	Learning Rate: 0.000215122
	LOSS [training: 0.019533852464765154 | validation: 0.015076559326945401]
	TIME [epoch: 8.8 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014574493930549052		[learning rate: 0.00021486]
		[batch 20/20] avg loss: 0.012684109823438408		[learning rate: 0.0002146]
	Learning Rate: 0.000214601
	LOSS [training: 0.01362930187699373 | validation: 0.014060795439952187]
	TIME [epoch: 8.78 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017023078006743474		[learning rate: 0.00021434]
		[batch 20/20] avg loss: 0.01602966063468116		[learning rate: 0.00021408]
	Learning Rate: 0.000214081
	LOSS [training: 0.016526369320712317 | validation: 0.013848984086501244]
	TIME [epoch: 8.78 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012578675876295655		[learning rate: 0.00021382]
		[batch 20/20] avg loss: 0.006396445133708899		[learning rate: 0.00021356]
	Learning Rate: 0.000213563
	LOSS [training: 0.009487560505002279 | validation: 0.007330016503064367]
	TIME [epoch: 8.77 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016375611206859146		[learning rate: 0.0002133]
		[batch 20/20] avg loss: 0.012611866266103566		[learning rate: 0.00021305]
	Learning Rate: 0.000213046
	LOSS [training: 0.014493738736481358 | validation: 0.018532487673255726]
	TIME [epoch: 8.78 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01377780247755701		[learning rate: 0.00021279]
		[batch 20/20] avg loss: 0.008826302111906952		[learning rate: 0.00021253]
	Learning Rate: 0.00021253
	LOSS [training: 0.01130205229473198 | validation: 0.011391641130887333]
	TIME [epoch: 8.79 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008205095138231016		[learning rate: 0.00021227]
		[batch 20/20] avg loss: 0.016943828453883227		[learning rate: 0.00021202]
	Learning Rate: 0.000212016
	LOSS [training: 0.012574461796057122 | validation: 0.01377399484562987]
	TIME [epoch: 8.78 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012180910739191697		[learning rate: 0.00021176]
		[batch 20/20] avg loss: 0.011517464643141326		[learning rate: 0.0002115]
	Learning Rate: 0.000211503
	LOSS [training: 0.011849187691166512 | validation: 0.010778723482609354]
	TIME [epoch: 8.77 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011880080798307017		[learning rate: 0.00021125]
		[batch 20/20] avg loss: 0.011184203418698265		[learning rate: 0.00021099]
	Learning Rate: 0.000210991
	LOSS [training: 0.011532142108502643 | validation: 0.006833436179983882]
	TIME [epoch: 8.78 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0077724957396407305		[learning rate: 0.00021074]
		[batch 20/20] avg loss: 0.014594346598870938		[learning rate: 0.00021048]
	Learning Rate: 0.00021048
	LOSS [training: 0.011183421169255834 | validation: 0.013429245020261755]
	TIME [epoch: 8.78 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012614438391152148		[learning rate: 0.00021022]
		[batch 20/20] avg loss: 0.01896530574320341		[learning rate: 0.00020997]
	Learning Rate: 0.00020997
	LOSS [training: 0.01578987206717778 | validation: 0.01842882508643999]
	TIME [epoch: 8.78 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015560271132331369		[learning rate: 0.00020972]
		[batch 20/20] avg loss: 0.01567314076118142		[learning rate: 0.00020946]
	Learning Rate: 0.000209462
	LOSS [training: 0.015616705946756395 | validation: 0.012232626723685018]
	TIME [epoch: 8.78 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014570525681881014		[learning rate: 0.00020921]
		[batch 20/20] avg loss: 0.013091648796327257		[learning rate: 0.00020895]
	Learning Rate: 0.000208955
	LOSS [training: 0.013831087239104135 | validation: 0.00750587405929435]
	TIME [epoch: 8.78 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012660719759101064		[learning rate: 0.0002087]
		[batch 20/20] avg loss: 0.014634164463646168		[learning rate: 0.00020845]
	Learning Rate: 0.000208449
	LOSS [training: 0.013647442111373614 | validation: 0.003630610464046287]
	TIME [epoch: 8.79 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01331823987535033		[learning rate: 0.0002082]
		[batch 20/20] avg loss: 0.016833634981421024		[learning rate: 0.00020794]
	Learning Rate: 0.000207944
	LOSS [training: 0.015075937428385677 | validation: 0.0042119461248602994]
	TIME [epoch: 8.8 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013490888819843897		[learning rate: 0.00020769]
		[batch 20/20] avg loss: 0.011845784721964127		[learning rate: 0.00020744]
	Learning Rate: 0.000207441
	LOSS [training: 0.012668336770904013 | validation: 0.0061348924979568575]
	TIME [epoch: 8.77 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011449818039001282		[learning rate: 0.00020719]
		[batch 20/20] avg loss: 0.01397387812861826		[learning rate: 0.00020694]
	Learning Rate: 0.000206939
	LOSS [training: 0.012711848083809771 | validation: 0.00768766085441068]
	TIME [epoch: 8.77 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009457162938614164		[learning rate: 0.00020669]
		[batch 20/20] avg loss: 0.008393648328051527		[learning rate: 0.00020644]
	Learning Rate: 0.000206438
	LOSS [training: 0.008925405633332844 | validation: 0.004546229314660084]
	TIME [epoch: 8.77 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008353859162578334		[learning rate: 0.00020619]
		[batch 20/20] avg loss: 0.020556371388415824		[learning rate: 0.00020594]
	Learning Rate: 0.000205938
	LOSS [training: 0.01445511527549708 | validation: 0.015463407898543335]
	TIME [epoch: 8.77 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017219814373015518		[learning rate: 0.00020569]
		[batch 20/20] avg loss: 0.009304567819183904		[learning rate: 0.00020544]
	Learning Rate: 0.00020544
	LOSS [training: 0.013262191096099713 | validation: 0.006273982020273116]
	TIME [epoch: 8.8 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01107016220878932		[learning rate: 0.00020519]
		[batch 20/20] avg loss: 0.011331270172865091		[learning rate: 0.00020494]
	Learning Rate: 0.000204942
	LOSS [training: 0.011200716190827206 | validation: 0.011139972901540927]
	TIME [epoch: 8.77 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01527113685280288		[learning rate: 0.00020469]
		[batch 20/20] avg loss: 0.015621636324543244		[learning rate: 0.00020445]
	Learning Rate: 0.000204446
	LOSS [training: 0.015446386588673066 | validation: 0.01017994941941737]
	TIME [epoch: 8.78 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012657973722902139		[learning rate: 0.0002042]
		[batch 20/20] avg loss: 0.01490074812917092		[learning rate: 0.00020395]
	Learning Rate: 0.000203951
	LOSS [training: 0.013779360926036533 | validation: 0.009882835301908695]
	TIME [epoch: 8.76 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005799846326943731		[learning rate: 0.0002037]
		[batch 20/20] avg loss: 0.01719341026696087		[learning rate: 0.00020346]
	Learning Rate: 0.000203457
	LOSS [training: 0.0114966282969523 | validation: 0.02012774407254965]
	TIME [epoch: 8.77 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023872784080100278		[learning rate: 0.00020321]
		[batch 20/20] avg loss: 0.021379095838116993		[learning rate: 0.00020296]
	Learning Rate: 0.000202965
	LOSS [training: 0.022625939959108637 | validation: 0.011404328604937439]
	TIME [epoch: 8.8 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008572786188213463		[learning rate: 0.00020272]
		[batch 20/20] avg loss: 0.013658070590207788		[learning rate: 0.00020247]
	Learning Rate: 0.000202474
	LOSS [training: 0.011115428389210624 | validation: 0.019696834396948134]
	TIME [epoch: 8.79 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011577463044716161		[learning rate: 0.00020223]
		[batch 20/20] avg loss: 0.01583295377419557		[learning rate: 0.00020198]
	Learning Rate: 0.000201983
	LOSS [training: 0.013705208409455865 | validation: 0.019183882044544666]
	TIME [epoch: 8.79 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016519892372827374		[learning rate: 0.00020174]
		[batch 20/20] avg loss: 0.013474915481937386		[learning rate: 0.00020149]
	Learning Rate: 0.000201495
	LOSS [training: 0.014997403927382377 | validation: 0.01832894570468142]
	TIME [epoch: 8.79 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009091833285748445		[learning rate: 0.00020125]
		[batch 20/20] avg loss: 0.013152429693272804		[learning rate: 0.00020101]
	Learning Rate: 0.000201007
	LOSS [training: 0.011122131489510625 | validation: 0.005252404814556185]
	TIME [epoch: 8.8 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014229317628063249		[learning rate: 0.00020076]
		[batch 20/20] avg loss: 0.016219377574385817		[learning rate: 0.00020052]
	Learning Rate: 0.00020052
	LOSS [training: 0.015224347601224531 | validation: 0.014056840292693152]
	TIME [epoch: 8.79 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013982573245749358		[learning rate: 0.00020028]
		[batch 20/20] avg loss: 0.004932305709927665		[learning rate: 0.00020003]
	Learning Rate: 0.000200035
	LOSS [training: 0.009457439477838513 | validation: 0.010153030784754984]
	TIME [epoch: 8.77 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008380412521109135		[learning rate: 0.00019979]
		[batch 20/20] avg loss: 0.011516891812489723		[learning rate: 0.00019955]
	Learning Rate: 0.00019955
	LOSS [training: 0.009948652166799429 | validation: 0.011036020079657892]
	TIME [epoch: 8.78 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015169367732109881		[learning rate: 0.00019931]
		[batch 20/20] avg loss: 0.020261094421062092		[learning rate: 0.00019907]
	Learning Rate: 0.000199067
	LOSS [training: 0.01771523107658599 | validation: 0.01259049537406816]
	TIME [epoch: 8.77 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015963914363900236		[learning rate: 0.00019883]
		[batch 20/20] avg loss: 0.010501873839069949		[learning rate: 0.00019859]
	Learning Rate: 0.000198585
	LOSS [training: 0.013232894101485093 | validation: 0.012744732372776834]
	TIME [epoch: 8.81 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014666256873209872		[learning rate: 0.00019834]
		[batch 20/20] avg loss: 0.007259117166901181		[learning rate: 0.0001981]
	Learning Rate: 0.000198105
	LOSS [training: 0.010962687020055527 | validation: 0.014108786629463365]
	TIME [epoch: 8.77 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015723392184758885		[learning rate: 0.00019786]
		[batch 20/20] avg loss: 0.012079741936231982		[learning rate: 0.00019763]
	Learning Rate: 0.000197625
	LOSS [training: 0.013901567060495435 | validation: 0.009116597728434926]
	TIME [epoch: 8.78 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011433533508696438		[learning rate: 0.00019739]
		[batch 20/20] avg loss: 0.013759110309380904		[learning rate: 0.00019715]
	Learning Rate: 0.000197147
	LOSS [training: 0.012596321909038668 | validation: 0.01351073543722389]
	TIME [epoch: 8.77 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011625368048189467		[learning rate: 0.00019691]
		[batch 20/20] avg loss: 0.009359163188206687		[learning rate: 0.00019667]
	Learning Rate: 0.000196669
	LOSS [training: 0.010492265618198078 | validation: 0.00945090436624141]
	TIME [epoch: 8.77 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015612728238369362		[learning rate: 0.00019643]
		[batch 20/20] avg loss: 0.015190985991538609		[learning rate: 0.00019619]
	Learning Rate: 0.000196193
	LOSS [training: 0.015401857114953985 | validation: 0.00905365208137023]
	TIME [epoch: 8.8 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007433999212225401		[learning rate: 0.00019596]
		[batch 20/20] avg loss: 0.017904192072753504		[learning rate: 0.00019572]
	Learning Rate: 0.000195718
	LOSS [training: 0.012669095642489452 | validation: 0.015463717135380126]
	TIME [epoch: 8.79 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018369392789690575		[learning rate: 0.00019548]
		[batch 20/20] avg loss: 0.01509400535743321		[learning rate: 0.00019524]
	Learning Rate: 0.000195245
	LOSS [training: 0.016731699073561895 | validation: 0.012122463761342569]
	TIME [epoch: 8.79 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015698738346644155		[learning rate: 0.00019501]
		[batch 20/20] avg loss: 0.016375224169896178		[learning rate: 0.00019477]
	Learning Rate: 0.000194772
	LOSS [training: 0.016036981258270164 | validation: 0.011798038472803326]
	TIME [epoch: 8.79 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016092837261302253		[learning rate: 0.00019454]
		[batch 20/20] avg loss: 0.013906388169880254		[learning rate: 0.0001943]
	Learning Rate: 0.0001943
	LOSS [training: 0.014999612715591254 | validation: 0.009516234055925481]
	TIME [epoch: 8.78 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01065088121807893		[learning rate: 0.00019407]
		[batch 20/20] avg loss: 0.013364921028124439		[learning rate: 0.00019383]
	Learning Rate: 0.00019383
	LOSS [training: 0.012007901123101685 | validation: 0.011124673808349639]
	TIME [epoch: 8.8 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013850466733274131		[learning rate: 0.0001936]
		[batch 20/20] avg loss: 0.008702017313336		[learning rate: 0.00019336]
	Learning Rate: 0.000193361
	LOSS [training: 0.011276242023305063 | validation: 0.013517376362846296]
	TIME [epoch: 8.77 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010716260057156344		[learning rate: 0.00019313]
		[batch 20/20] avg loss: 0.007885072435967266		[learning rate: 0.00019289]
	Learning Rate: 0.000192893
	LOSS [training: 0.009300666246561807 | validation: 0.009413857136329645]
	TIME [epoch: 8.78 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018650096462863277		[learning rate: 0.00019266]
		[batch 20/20] avg loss: 0.015253299321944386		[learning rate: 0.00019243]
	Learning Rate: 0.000192426
	LOSS [training: 0.01695169789240383 | validation: 0.013124376491808979]
	TIME [epoch: 8.77 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02103363874514718		[learning rate: 0.00019219]
		[batch 20/20] avg loss: 0.01009939515183417		[learning rate: 0.00019196]
	Learning Rate: 0.00019196
	LOSS [training: 0.015566516948490677 | validation: 0.01425189318006344]
	TIME [epoch: 8.79 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01530742047403887		[learning rate: 0.00019173]
		[batch 20/20] avg loss: 0.012418263364833735		[learning rate: 0.0001915]
	Learning Rate: 0.000191495
	LOSS [training: 0.0138628419194363 | validation: 0.0049479702138290164]
	TIME [epoch: 8.79 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013678225994125718		[learning rate: 0.00019126]
		[batch 20/20] avg loss: 0.009043194450677465		[learning rate: 0.00019103]
	Learning Rate: 0.000191032
	LOSS [training: 0.01136071022240159 | validation: 0.002465909092936012]
	TIME [epoch: 8.77 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01480613746546837		[learning rate: 0.0001908]
		[batch 20/20] avg loss: 0.008991174203437402		[learning rate: 0.00019057]
	Learning Rate: 0.000190569
	LOSS [training: 0.011898655834452885 | validation: 0.007849124339930185]
	TIME [epoch: 8.77 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00743560941286808		[learning rate: 0.00019034]
		[batch 20/20] avg loss: 0.010564114380005698		[learning rate: 0.00019011]
	Learning Rate: 0.000190108
	LOSS [training: 0.008999861896436891 | validation: 0.014338898982797955]
	TIME [epoch: 8.77 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01835524366641509		[learning rate: 0.00018988]
		[batch 20/20] avg loss: 0.011923647844241506		[learning rate: 0.00018965]
	Learning Rate: 0.000189648
	LOSS [training: 0.015139445755328296 | validation: 0.006509498317805838]
	TIME [epoch: 8.79 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00637056333009064		[learning rate: 0.00018942]
		[batch 20/20] avg loss: 0.016208640275282092		[learning rate: 0.00018919]
	Learning Rate: 0.000189189
	LOSS [training: 0.011289601802686363 | validation: 0.005477818349079346]
	TIME [epoch: 8.79 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008382160722214458		[learning rate: 0.00018896]
		[batch 20/20] avg loss: 0.006839757888241796		[learning rate: 0.00018873]
	Learning Rate: 0.000188731
	LOSS [training: 0.007610959305228125 | validation: 0.009952182831979439]
	TIME [epoch: 8.79 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015339498832796412		[learning rate: 0.0001885]
		[batch 20/20] avg loss: 0.012508592792878007		[learning rate: 0.00018827]
	Learning Rate: 0.000188274
	LOSS [training: 0.013924045812837207 | validation: 0.017127107531223415]
	TIME [epoch: 8.79 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014257998354326932		[learning rate: 0.00018805]
		[batch 20/20] avg loss: 0.014776814688054344		[learning rate: 0.00018782]
	Learning Rate: 0.000187818
	LOSS [training: 0.014517406521190638 | validation: 0.00874907141852195]
	TIME [epoch: 8.77 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007028284452863208		[learning rate: 0.00018759]
		[batch 20/20] avg loss: 0.011418419033516949		[learning rate: 0.00018736]
	Learning Rate: 0.000187363
	LOSS [training: 0.00922335174319008 | validation: 0.004919081018231309]
	TIME [epoch: 8.8 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006408546183687758		[learning rate: 0.00018714]
		[batch 20/20] avg loss: 0.011931427992361453		[learning rate: 0.00018691]
	Learning Rate: 0.00018691
	LOSS [training: 0.009169987088024604 | validation: 0.008580470766570734]
	TIME [epoch: 8.77 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013704450709204738		[learning rate: 0.00018668]
		[batch 20/20] avg loss: 0.011056679473779998		[learning rate: 0.00018646]
	Learning Rate: 0.000186457
	LOSS [training: 0.012380565091492369 | validation: 0.014494335336446477]
	TIME [epoch: 8.78 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01843996410697096		[learning rate: 0.00018623]
		[batch 20/20] avg loss: 0.011303429198197061		[learning rate: 0.00018601]
	Learning Rate: 0.000186006
	LOSS [training: 0.014871696652584013 | validation: 0.008445345526202306]
	TIME [epoch: 8.78 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005404932516817258		[learning rate: 0.00018578]
		[batch 20/20] avg loss: 0.013317909101453302		[learning rate: 0.00018556]
	Learning Rate: 0.000185555
	LOSS [training: 0.009361420809135278 | validation: 0.0015725798867343996]
	TIME [epoch: 8.79 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01983616460475903		[learning rate: 0.00018533]
		[batch 20/20] avg loss: 0.004014639655533836		[learning rate: 0.00018511]
	Learning Rate: 0.000185106
	LOSS [training: 0.011925402130146433 | validation: 0.013166571080307488]
	TIME [epoch: 8.79 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007312044850175128		[learning rate: 0.00018488]
		[batch 20/20] avg loss: 0.011303312916087497		[learning rate: 0.00018466]
	Learning Rate: 0.000184658
	LOSS [training: 0.009307678883131314 | validation: 0.0037781518162062332]
	TIME [epoch: 8.77 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011633345349938446		[learning rate: 0.00018443]
		[batch 20/20] avg loss: 0.006716654078887196		[learning rate: 0.00018421]
	Learning Rate: 0.000184211
	LOSS [training: 0.009174999714412822 | validation: 0.014610088763251547]
	TIME [epoch: 8.78 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012431622776011482		[learning rate: 0.00018399]
		[batch 20/20] avg loss: 0.010131711664414513		[learning rate: 0.00018377]
	Learning Rate: 0.000183765
	LOSS [training: 0.011281667220212998 | validation: 0.009825182408918215]
	TIME [epoch: 8.78 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012470238260570483		[learning rate: 0.00018354]
		[batch 20/20] avg loss: 0.007728882686861676		[learning rate: 0.00018332]
	Learning Rate: 0.00018332
	LOSS [training: 0.010099560473716079 | validation: 0.00955943385817174]
	TIME [epoch: 8.79 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015016418591050375		[learning rate: 0.0001831]
		[batch 20/20] avg loss: 0.013746339648902095		[learning rate: 0.00018288]
	Learning Rate: 0.000182876
	LOSS [training: 0.014381379119976237 | validation: 0.011251140788431344]
	TIME [epoch: 8.81 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013451856416700218		[learning rate: 0.00018266]
		[batch 20/20] avg loss: 0.014714987212248124		[learning rate: 0.00018243]
	Learning Rate: 0.000182434
	LOSS [training: 0.014083421814474173 | validation: 0.019626247569095135]
	TIME [epoch: 8.78 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018297731733289455		[learning rate: 0.00018221]
		[batch 20/20] avg loss: 0.007483037128000099		[learning rate: 0.00018199]
	Learning Rate: 0.000181992
	LOSS [training: 0.012890384430644778 | validation: 0.008285085741897995]
	TIME [epoch: 8.78 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00960387891083617		[learning rate: 0.00018177]
		[batch 20/20] avg loss: 0.023444827432713065		[learning rate: 0.00018155]
	Learning Rate: 0.000181552
	LOSS [training: 0.016524353171774614 | validation: 0.00894896575739721]
	TIME [epoch: 8.77 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011875552830384764		[learning rate: 0.00018133]
		[batch 20/20] avg loss: 0.006574816298822872		[learning rate: 0.00018111]
	Learning Rate: 0.000181112
	LOSS [training: 0.009225184564603818 | validation: 0.012682174519743919]
	TIME [epoch: 8.79 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005559846518044817		[learning rate: 0.00018089]
		[batch 20/20] avg loss: 0.014071846850751116		[learning rate: 0.00018067]
	Learning Rate: 0.000180674
	LOSS [training: 0.009815846684397968 | validation: 0.016195792721200718]
	TIME [epoch: 8.78 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011835527656349105		[learning rate: 0.00018045]
		[batch 20/20] avg loss: 0.018022620173501123		[learning rate: 0.00018024]
	Learning Rate: 0.000180236
	LOSS [training: 0.014929073914925118 | validation: 0.0014108959832737486]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_1757.pth
	Model improved!!!
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007940322961122152		[learning rate: 0.00018002]
		[batch 20/20] avg loss: 0.013434947971602898		[learning rate: 0.0001798]
	Learning Rate: 0.0001798
	LOSS [training: 0.010687635466362524 | validation: 0.008673248544718284]
	TIME [epoch: 8.78 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013849623136225692		[learning rate: 0.00017958]
		[batch 20/20] avg loss: 0.02285205396207104		[learning rate: 0.00017936]
	Learning Rate: 0.000179365
	LOSS [training: 0.018350838549148367 | validation: 0.00889774636677859]
	TIME [epoch: 8.78 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01436347468333388		[learning rate: 0.00017915]
		[batch 20/20] avg loss: 0.02087590987945394		[learning rate: 0.00017893]
	Learning Rate: 0.00017893
	LOSS [training: 0.017619692281393907 | validation: 0.016945715487980833]
	TIME [epoch: 8.79 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01559851451807249		[learning rate: 0.00017871]
		[batch 20/20] avg loss: 0.01356873324926145		[learning rate: 0.0001785]
	Learning Rate: 0.000178497
	LOSS [training: 0.014583623883666968 | validation: 0.009392704319384958]
	TIME [epoch: 8.78 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016128440963576664		[learning rate: 0.00017828]
		[batch 20/20] avg loss: 0.012090349933037162		[learning rate: 0.00017807]
	Learning Rate: 0.000178065
	LOSS [training: 0.014109395448306911 | validation: 0.01001329688894739]
	TIME [epoch: 8.77 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015479770408819787		[learning rate: 0.00017785]
		[batch 20/20] avg loss: 0.0159489856055738		[learning rate: 0.00017763]
	Learning Rate: 0.000177634
	LOSS [training: 0.015714378007196793 | validation: 0.01718846186784868]
	TIME [epoch: 8.78 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009480081765746651		[learning rate: 0.00017742]
		[batch 20/20] avg loss: 0.009664417242016934		[learning rate: 0.0001772]
	Learning Rate: 0.000177204
	LOSS [training: 0.009572249503881792 | validation: 0.012525525500220907]
	TIME [epoch: 8.78 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005886247700192479		[learning rate: 0.00017699]
		[batch 20/20] avg loss: 0.013660117853914721		[learning rate: 0.00017678]
	Learning Rate: 0.000176775
	LOSS [training: 0.0097731827770536 | validation: 0.01188953152832255]
	TIME [epoch: 8.8 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016662656332637187		[learning rate: 0.00017656]
		[batch 20/20] avg loss: 0.012038730239574134		[learning rate: 0.00017635]
	Learning Rate: 0.000176347
	LOSS [training: 0.01435069328610566 | validation: 0.007617829346714745]
	TIME [epoch: 8.78 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010291880263102252		[learning rate: 0.00017613]
		[batch 20/20] avg loss: 0.007628614988600629		[learning rate: 0.00017592]
	Learning Rate: 0.00017592
	LOSS [training: 0.008960247625851441 | validation: 0.0038181388493463207]
	TIME [epoch: 8.77 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005383835667724514		[learning rate: 0.00017571]
		[batch 20/20] avg loss: 0.016806961703558713		[learning rate: 0.00017549]
	Learning Rate: 0.000175494
	LOSS [training: 0.011095398685641614 | validation: 0.011165043965599513]
	TIME [epoch: 8.78 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011438689859338809		[learning rate: 0.00017528]
		[batch 20/20] avg loss: 0.013550340777057551		[learning rate: 0.00017507]
	Learning Rate: 0.00017507
	LOSS [training: 0.01249451531819818 | validation: 0.011704415580604354]
	TIME [epoch: 8.79 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01228277141949693		[learning rate: 0.00017486]
		[batch 20/20] avg loss: 0.015955133125177746		[learning rate: 0.00017465]
	Learning Rate: 0.000174646
	LOSS [training: 0.014118952272337337 | validation: 0.007872807245673241]
	TIME [epoch: 8.78 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015821213989148607		[learning rate: 0.00017443]
		[batch 20/20] avg loss: 0.004315010398787357		[learning rate: 0.00017422]
	Learning Rate: 0.000174223
	LOSS [training: 0.010068112193967983 | validation: 0.0027735159910059633]
	TIME [epoch: 8.77 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016612940822415996		[learning rate: 0.00017401]
		[batch 20/20] avg loss: 0.009629317831283792		[learning rate: 0.0001738]
	Learning Rate: 0.000173801
	LOSS [training: 0.013121129326849893 | validation: 0.009100142098438334]
	TIME [epoch: 8.78 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013769587485271722		[learning rate: 0.00017359]
		[batch 20/20] avg loss: 0.013117571407212		[learning rate: 0.00017338]
	Learning Rate: 0.00017338
	LOSS [training: 0.01344357944624186 | validation: 0.010577547612825347]
	TIME [epoch: 8.78 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014593218080850363		[learning rate: 0.00017317]
		[batch 20/20] avg loss: 0.010460938212536094		[learning rate: 0.00017296]
	Learning Rate: 0.000172961
	LOSS [training: 0.01252707814669323 | validation: 0.0036887302416406744]
	TIME [epoch: 8.79 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008952432187872911		[learning rate: 0.00017275]
		[batch 20/20] avg loss: 0.009586303777260573		[learning rate: 0.00017254]
	Learning Rate: 0.000172542
	LOSS [training: 0.009269367982566742 | validation: 0.011555737359810038]
	TIME [epoch: 8.78 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00884033611714444		[learning rate: 0.00017233]
		[batch 20/20] avg loss: 0.016775529323637094		[learning rate: 0.00017212]
	Learning Rate: 0.000172124
	LOSS [training: 0.012807932720390769 | validation: 0.006267495637826103]
	TIME [epoch: 8.78 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0064816093791542715		[learning rate: 0.00017192]
		[batch 20/20] avg loss: 0.01945539715861175		[learning rate: 0.00017171]
	Learning Rate: 0.000171708
	LOSS [training: 0.01296850326888301 | validation: 0.012549774952203285]
	TIME [epoch: 8.78 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008640438965515645		[learning rate: 0.0001715]
		[batch 20/20] avg loss: 0.00863506984113439		[learning rate: 0.00017129]
	Learning Rate: 0.000171292
	LOSS [training: 0.008637754403325017 | validation: 0.010025131116777825]
	TIME [epoch: 8.8 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012365731577789385		[learning rate: 0.00017108]
		[batch 20/20] avg loss: 0.006817030376022057		[learning rate: 0.00017088]
	Learning Rate: 0.000170877
	LOSS [training: 0.009591380976905721 | validation: 0.0051285974631787665]
	TIME [epoch: 8.81 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015181505000022624		[learning rate: 0.00017067]
		[batch 20/20] avg loss: 0.01488888502845719		[learning rate: 0.00017046]
	Learning Rate: 0.000170464
	LOSS [training: 0.015035195014239906 | validation: 0.0018745208479735282]
	TIME [epoch: 8.79 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01128666599727134		[learning rate: 0.00017026]
		[batch 20/20] avg loss: 0.013563383973714604		[learning rate: 0.00017005]
	Learning Rate: 0.000170051
	LOSS [training: 0.012425024985492972 | validation: 0.01946021813684694]
	TIME [epoch: 8.77 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015921277152549523		[learning rate: 0.00016984]
		[batch 20/20] avg loss: 0.015145316087069654		[learning rate: 0.00016964]
	Learning Rate: 0.000169639
	LOSS [training: 0.01553329661980959 | validation: 0.00431127863920152]
	TIME [epoch: 8.78 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007969568702908676		[learning rate: 0.00016943]
		[batch 20/20] avg loss: 0.013029284037673974		[learning rate: 0.00016923]
	Learning Rate: 0.000169229
	LOSS [training: 0.010499426370291326 | validation: 0.0058611872995086136]
	TIME [epoch: 8.78 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010340374178895249		[learning rate: 0.00016902]
		[batch 20/20] avg loss: 0.012107456970553756		[learning rate: 0.00016882]
	Learning Rate: 0.000168819
	LOSS [training: 0.0112239155747245 | validation: 0.026203525065925555]
	TIME [epoch: 8.79 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017082658852994505		[learning rate: 0.00016861]
		[batch 20/20] avg loss: 0.012699854328166985		[learning rate: 0.00016841]
	Learning Rate: 0.00016841
	LOSS [training: 0.014891256590580746 | validation: 0.01827795066749374]
	TIME [epoch: 8.78 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015631842371780592		[learning rate: 0.00016821]
		[batch 20/20] avg loss: 0.008996665905338506		[learning rate: 0.000168]
	Learning Rate: 0.000168003
	LOSS [training: 0.012314254138559548 | validation: 0.005132443947848105]
	TIME [epoch: 8.77 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010305065534200263		[learning rate: 0.0001678]
		[batch 20/20] avg loss: 0.009620198818888662		[learning rate: 0.0001676]
	Learning Rate: 0.000167596
	LOSS [training: 0.009962632176544462 | validation: 0.01372432675549952]
	TIME [epoch: 8.78 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0077413886119220236		[learning rate: 0.00016739]
		[batch 20/20] avg loss: 0.012719741696143733		[learning rate: 0.00016719]
	Learning Rate: 0.00016719
	LOSS [training: 0.010230565154032877 | validation: 0.007029978159924538]
	TIME [epoch: 8.8 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01119796714239937		[learning rate: 0.00016699]
		[batch 20/20] avg loss: 0.010452596586073625		[learning rate: 0.00016679]
	Learning Rate: 0.000166785
	LOSS [training: 0.010825281864236497 | validation: 0.014689245000357842]
	TIME [epoch: 8.77 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014514699504894072		[learning rate: 0.00016658]
		[batch 20/20] avg loss: 0.013677877463049918		[learning rate: 0.00016638]
	Learning Rate: 0.000166382
	LOSS [training: 0.014096288483971996 | validation: 0.011872386251590792]
	TIME [epoch: 8.78 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013837916073347953		[learning rate: 0.00016618]
		[batch 20/20] avg loss: 0.006634106647132126		[learning rate: 0.00016598]
	Learning Rate: 0.000165979
	LOSS [training: 0.010236011360240039 | validation: 0.009732045996599756]
	TIME [epoch: 8.77 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008693809923978713		[learning rate: 0.00016578]
		[batch 20/20] avg loss: 0.01148287579030218		[learning rate: 0.00016558]
	Learning Rate: 0.000165577
	LOSS [training: 0.010088342857140446 | validation: 0.013916193012102847]
	TIME [epoch: 8.79 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01229157243851876		[learning rate: 0.00016538]
		[batch 20/20] avg loss: 0.012589364431580045		[learning rate: 0.00016518]
	Learning Rate: 0.000165176
	LOSS [training: 0.012440468435049399 | validation: 0.0028673415889756996]
	TIME [epoch: 8.81 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010438267361187687		[learning rate: 0.00016498]
		[batch 20/20] avg loss: 0.01281913675052009		[learning rate: 0.00016478]
	Learning Rate: 0.000164776
	LOSS [training: 0.011628702055853887 | validation: 0.011947544017028376]
	TIME [epoch: 8.78 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011834573076333932		[learning rate: 0.00016458]
		[batch 20/20] avg loss: 0.016293355951517968		[learning rate: 0.00016438]
	Learning Rate: 0.000164377
	LOSS [training: 0.01406396451392595 | validation: 0.011116595432593097]
	TIME [epoch: 8.78 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013328216190646298		[learning rate: 0.00016418]
		[batch 20/20] avg loss: 0.011979509465477966		[learning rate: 0.00016398]
	Learning Rate: 0.000163979
	LOSS [training: 0.012653862828062131 | validation: 0.014397487017901257]
	TIME [epoch: 8.78 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013857056489575659		[learning rate: 0.00016378]
		[batch 20/20] avg loss: 0.011582847957682088		[learning rate: 0.00016358]
	Learning Rate: 0.000163583
	LOSS [training: 0.012719952223628872 | validation: 0.009251165287624628]
	TIME [epoch: 8.79 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009813902305893601		[learning rate: 0.00016338]
		[batch 20/20] avg loss: 0.009685871462085287		[learning rate: 0.00016319]
	Learning Rate: 0.000163187
	LOSS [training: 0.009749886883989445 | validation: 0.004799718999434306]
	TIME [epoch: 8.78 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01427840549100893		[learning rate: 0.00016299]
		[batch 20/20] avg loss: 0.006829337377167931		[learning rate: 0.00016279]
	Learning Rate: 0.000162791
	LOSS [training: 0.01055387143408843 | validation: 0.01302162492116297]
	TIME [epoch: 8.78 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013567224891587737		[learning rate: 0.00016259]
		[batch 20/20] avg loss: 0.007205747574183472		[learning rate: 0.0001624]
	Learning Rate: 0.000162397
	LOSS [training: 0.010386486232885603 | validation: 0.0071762943446277965]
	TIME [epoch: 8.77 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010485897319690258		[learning rate: 0.0001622]
		[batch 20/20] avg loss: 0.008413888904191627		[learning rate: 0.000162]
	Learning Rate: 0.000162004
	LOSS [training: 0.00944989311194094 | validation: 0.005146428781474142]
	TIME [epoch: 8.78 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01280865314364873		[learning rate: 0.00016181]
		[batch 20/20] avg loss: 0.013018432257943299		[learning rate: 0.00016161]
	Learning Rate: 0.000161612
	LOSS [training: 0.012913542700796013 | validation: 0.010418047351720514]
	TIME [epoch: 8.8 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005483821273119427		[learning rate: 0.00016142]
		[batch 20/20] avg loss: 0.015383842506611112		[learning rate: 0.00016122]
	Learning Rate: 0.000161221
	LOSS [training: 0.01043383188986527 | validation: 0.011477178412813427]
	TIME [epoch: 8.78 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012821761083512124		[learning rate: 0.00016103]
		[batch 20/20] avg loss: 0.015918126904151775		[learning rate: 0.00016083]
	Learning Rate: 0.000160831
	LOSS [training: 0.014369943993831949 | validation: 0.015718851628703564]
	TIME [epoch: 8.78 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012740688905462344		[learning rate: 0.00016064]
		[batch 20/20] avg loss: 0.006078074163444686		[learning rate: 0.00016044]
	Learning Rate: 0.000160441
	LOSS [training: 0.009409381534453512 | validation: 0.010605451382556691]
	TIME [epoch: 8.77 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008318313684733514		[learning rate: 0.00016025]
		[batch 20/20] avg loss: 0.01346355390198016		[learning rate: 0.00016005]
	Learning Rate: 0.000160053
	LOSS [training: 0.010890933793356837 | validation: 0.008926167445731697]
	TIME [epoch: 8.77 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01276475443341218		[learning rate: 0.00015986]
		[batch 20/20] avg loss: 0.01516317797380284		[learning rate: 0.00015967]
	Learning Rate: 0.000159665
	LOSS [training: 0.01396396620360751 | validation: 0.014737541220106717]
	TIME [epoch: 8.8 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012456911352281984		[learning rate: 0.00015947]
		[batch 20/20] avg loss: 0.0111542666233816		[learning rate: 0.00015928]
	Learning Rate: 0.000159279
	LOSS [training: 0.011805588987831793 | validation: 0.010022440605270858]
	TIME [epoch: 8.77 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011781148184977698		[learning rate: 0.00015909]
		[batch 20/20] avg loss: 0.011332758289035674		[learning rate: 0.00015889]
	Learning Rate: 0.000158893
	LOSS [training: 0.011556953237006683 | validation: 0.010871735711621037]
	TIME [epoch: 8.77 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011570733274880752		[learning rate: 0.0001587]
		[batch 20/20] avg loss: 0.009811119455554605		[learning rate: 0.00015851]
	Learning Rate: 0.000158509
	LOSS [training: 0.010690926365217677 | validation: 0.01571495317862891]
	TIME [epoch: 8.77 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012611243998380955		[learning rate: 0.00015832]
		[batch 20/20] avg loss: 0.004728995172855422		[learning rate: 0.00015812]
	Learning Rate: 0.000158125
	LOSS [training: 0.008670119585618192 | validation: 0.00602686545030099]
	TIME [epoch: 8.77 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010493831970081188		[learning rate: 0.00015793]
		[batch 20/20] avg loss: 0.00889197726886606		[learning rate: 0.00015774]
	Learning Rate: 0.000157742
	LOSS [training: 0.009692904619473622 | validation: 0.012218954286460729]
	TIME [epoch: 8.8 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009802263439932667		[learning rate: 0.00015755]
		[batch 20/20] avg loss: 0.011750184310849694		[learning rate: 0.00015736]
	Learning Rate: 0.00015736
	LOSS [training: 0.010776223875391178 | validation: 0.005559155929096393]
	TIME [epoch: 8.78 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014645970168161077		[learning rate: 0.00015717]
		[batch 20/20] avg loss: 0.012281232845935781		[learning rate: 0.00015698]
	Learning Rate: 0.000156979
	LOSS [training: 0.013463601507048426 | validation: 0.0048641606671857626]
	TIME [epoch: 8.78 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006795192329406394		[learning rate: 0.00015679]
		[batch 20/20] avg loss: 0.012030485262489484		[learning rate: 0.0001566]
	Learning Rate: 0.000156599
	LOSS [training: 0.00941283879594794 | validation: 0.006933511960776007]
	TIME [epoch: 8.76 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008523418678564128		[learning rate: 0.00015641]
		[batch 20/20] avg loss: 0.012847010247182633		[learning rate: 0.00015622]
	Learning Rate: 0.00015622
	LOSS [training: 0.01068521446287338 | validation: 0.008344798723205116]
	TIME [epoch: 8.78 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008016679866515535		[learning rate: 0.00015603]
		[batch 20/20] avg loss: 0.011146696124360877		[learning rate: 0.00015584]
	Learning Rate: 0.000155842
	LOSS [training: 0.009581687995438206 | validation: 0.009926665408576632]
	TIME [epoch: 8.79 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010169108539140628		[learning rate: 0.00015565]
		[batch 20/20] avg loss: 0.0157683904071707		[learning rate: 0.00015546]
	Learning Rate: 0.000155465
	LOSS [training: 0.012968749473155664 | validation: 0.01641941262332889]
	TIME [epoch: 8.78 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011499792312210799		[learning rate: 0.00015528]
		[batch 20/20] avg loss: 0.014188288291320132		[learning rate: 0.00015509]
	Learning Rate: 0.000155088
	LOSS [training: 0.012844040301765466 | validation: 0.010531132617848396]
	TIME [epoch: 8.79 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009656821960013504		[learning rate: 0.0001549]
		[batch 20/20] avg loss: 0.008961704874274275		[learning rate: 0.00015471]
	Learning Rate: 0.000154713
	LOSS [training: 0.009309263417143889 | validation: 0.00829154855802632]
	TIME [epoch: 8.78 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006400613553140799		[learning rate: 0.00015453]
		[batch 20/20] avg loss: 0.010012621527953921		[learning rate: 0.00015434]
	Learning Rate: 0.000154338
	LOSS [training: 0.008206617540547361 | validation: 0.0016027472869066352]
	TIME [epoch: 8.8 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007223626492081465		[learning rate: 0.00015415]
		[batch 20/20] avg loss: 0.011919840405951499		[learning rate: 0.00015396]
	Learning Rate: 0.000153965
	LOSS [training: 0.009571733449016484 | validation: 0.00852133690520538]
	TIME [epoch: 8.78 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011082187893955495		[learning rate: 0.00015378]
		[batch 20/20] avg loss: 0.014657203178205522		[learning rate: 0.00015359]
	Learning Rate: 0.000153592
	LOSS [training: 0.01286969553608051 | validation: 0.010089089544179445]
	TIME [epoch: 8.77 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00645403489268107		[learning rate: 0.00015341]
		[batch 20/20] avg loss: 0.008523369869357976		[learning rate: 0.00015322]
	Learning Rate: 0.00015322
	LOSS [training: 0.007488702381019524 | validation: 0.016031757446675232]
	TIME [epoch: 8.78 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013836415605581862		[learning rate: 0.00015303]
		[batch 20/20] avg loss: 0.009542868947216492		[learning rate: 0.00015285]
	Learning Rate: 0.000152849
	LOSS [training: 0.011689642276399178 | validation: 0.010372761579132896]
	TIME [epoch: 8.78 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012321672781136854		[learning rate: 0.00015266]
		[batch 20/20] avg loss: 0.006906325347972713		[learning rate: 0.00015248]
	Learning Rate: 0.000152479
	LOSS [training: 0.009613999064554781 | validation: 0.006978249933767587]
	TIME [epoch: 8.8 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008811233584699018		[learning rate: 0.00015229]
		[batch 20/20] avg loss: 0.015263060597667633		[learning rate: 0.00015211]
	Learning Rate: 0.00015211
	LOSS [training: 0.012037147091183326 | validation: 0.015270059686096831]
	TIME [epoch: 8.78 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009352565806021432		[learning rate: 0.00015193]
		[batch 20/20] avg loss: 0.01701946993568703		[learning rate: 0.00015174]
	Learning Rate: 0.000151742
	LOSS [training: 0.01318601787085423 | validation: 0.020606555431042436]
	TIME [epoch: 8.78 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015340020295548238		[learning rate: 0.00015156]
		[batch 20/20] avg loss: 0.0098343888599916		[learning rate: 0.00015137]
	Learning Rate: 0.000151374
	LOSS [training: 0.01258720457776992 | validation: 0.014737822287700401]
	TIME [epoch: 8.77 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011684036427797372		[learning rate: 0.00015119]
		[batch 20/20] avg loss: 0.010883616980891509		[learning rate: 0.00015101]
	Learning Rate: 0.000151008
	LOSS [training: 0.01128382670434444 | validation: 0.010094144847548479]
	TIME [epoch: 8.77 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01337318003141745		[learning rate: 0.00015083]
		[batch 20/20] avg loss: 0.011652479071872468		[learning rate: 0.00015064]
	Learning Rate: 0.000150642
	LOSS [training: 0.012512829551644958 | validation: 0.004540121951987591]
	TIME [epoch: 8.8 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009277393084593692		[learning rate: 0.00015046]
		[batch 20/20] avg loss: 0.005552017215792148		[learning rate: 0.00015028]
	Learning Rate: 0.000150278
	LOSS [training: 0.007414705150192921 | validation: 0.00859365803802554]
	TIME [epoch: 8.79 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010972550683876333		[learning rate: 0.0001501]
		[batch 20/20] avg loss: 0.011410067157317239		[learning rate: 0.00014991]
	Learning Rate: 0.000149914
	LOSS [training: 0.011191308920596787 | validation: 0.01367050075506022]
	TIME [epoch: 8.79 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012046837228234922		[learning rate: 0.00014973]
		[batch 20/20] avg loss: 0.005281579611888788		[learning rate: 0.00014955]
	Learning Rate: 0.000149551
	LOSS [training: 0.008664208420061855 | validation: 0.010889264533818005]
	TIME [epoch: 8.78 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009659290173190466		[learning rate: 0.00014937]
		[batch 20/20] avg loss: 0.013525128406306078		[learning rate: 0.00014919]
	Learning Rate: 0.000149189
	LOSS [training: 0.011592209289748275 | validation: 0.017331043910627772]
	TIME [epoch: 8.78 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00962006715999979		[learning rate: 0.00014901]
		[batch 20/20] avg loss: 0.008776227749346341		[learning rate: 0.00014883]
	Learning Rate: 0.000148828
	LOSS [training: 0.009198147454673067 | validation: 0.016783189699765652]
	TIME [epoch: 8.79 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012849320616080387		[learning rate: 0.00014865]
		[batch 20/20] avg loss: 0.016423418921694254		[learning rate: 0.00014847]
	Learning Rate: 0.000148468
	LOSS [training: 0.01463636976888732 | validation: 0.010714639112142915]
	TIME [epoch: 8.77 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011211825253812903		[learning rate: 0.00014829]
		[batch 20/20] avg loss: 0.008445572680695162		[learning rate: 0.00014811]
	Learning Rate: 0.000148108
	LOSS [training: 0.00982869896725403 | validation: 0.01824866698789203]
	TIME [epoch: 8.77 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010978645815991162		[learning rate: 0.00014793]
		[batch 20/20] avg loss: 0.004386750410436657		[learning rate: 0.00014775]
	Learning Rate: 0.00014775
	LOSS [training: 0.00768269811321391 | validation: 0.016474136606998112]
	TIME [epoch: 8.78 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012681320219326858		[learning rate: 0.00014757]
		[batch 20/20] avg loss: 0.01021416007025833		[learning rate: 0.00014739]
	Learning Rate: 0.000147392
	LOSS [training: 0.011447740144792596 | validation: 0.01332290204912859]
	TIME [epoch: 8.79 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008722315813179364		[learning rate: 0.00014721]
		[batch 20/20] avg loss: 0.00907883899195874		[learning rate: 0.00014704]
	Learning Rate: 0.000147035
	LOSS [training: 0.008900577402569054 | validation: 0.017164790217001645]
	TIME [epoch: 8.78 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01768626355559482		[learning rate: 0.00014686]
		[batch 20/20] avg loss: 0.01060847556553342		[learning rate: 0.00014668]
	Learning Rate: 0.000146679
	LOSS [training: 0.014147369560564121 | validation: 0.010805495804598945]
	TIME [epoch: 8.77 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021699030527464632		[learning rate: 0.0001465]
		[batch 20/20] avg loss: 0.007087945805824206		[learning rate: 0.00014632]
	Learning Rate: 0.000146324
	LOSS [training: 0.014393488166644422 | validation: 0.00901744822941127]
	TIME [epoch: 8.78 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009217026655178491		[learning rate: 0.00014615]
		[batch 20/20] avg loss: 0.014291545657822369		[learning rate: 0.00014597]
	Learning Rate: 0.00014597
	LOSS [training: 0.01175428615650043 | validation: 0.01685368656925372]
	TIME [epoch: 8.77 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010095827608663387		[learning rate: 0.00014579]
		[batch 20/20] avg loss: 0.006721468015618943		[learning rate: 0.00014562]
	Learning Rate: 0.000145616
	LOSS [training: 0.008408647812141165 | validation: 0.003367581332567641]
	TIME [epoch: 8.82 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01129077294769975		[learning rate: 0.00014544]
		[batch 20/20] avg loss: 0.010124020250808146		[learning rate: 0.00014526]
	Learning Rate: 0.000145264
	LOSS [training: 0.01070739659925395 | validation: 0.019184130397981947]
	TIME [epoch: 8.79 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011354834354746265		[learning rate: 0.00014509]
		[batch 20/20] avg loss: 0.011843331026786974		[learning rate: 0.00014491]
	Learning Rate: 0.000144912
	LOSS [training: 0.011599082690766618 | validation: 0.015971063521942185]
	TIME [epoch: 8.79 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013484486360783993		[learning rate: 0.00014474]
		[batch 20/20] avg loss: 0.014918686495527193		[learning rate: 0.00014456]
	Learning Rate: 0.000144561
	LOSS [training: 0.014201586428155593 | validation: 0.011849063381761499]
	TIME [epoch: 8.79 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005561980649886423		[learning rate: 0.00014439]
		[batch 20/20] avg loss: 0.012442682490994473		[learning rate: 0.00014421]
	Learning Rate: 0.000144212
	LOSS [training: 0.009002331570440445 | validation: 0.012463477661914921]
	TIME [epoch: 8.78 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0141467320594421		[learning rate: 0.00014404]
		[batch 20/20] avg loss: 0.01262267015641468		[learning rate: 0.00014386]
	Learning Rate: 0.000143862
	LOSS [training: 0.013384701107928393 | validation: 0.015566865177795416]
	TIME [epoch: 8.79 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0163912895365253		[learning rate: 0.00014369]
		[batch 20/20] avg loss: 0.007352687197504687		[learning rate: 0.00014351]
	Learning Rate: 0.000143514
	LOSS [training: 0.011871988367014991 | validation: 0.013553516285306802]
	TIME [epoch: 8.78 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012736626111647258		[learning rate: 0.00014334]
		[batch 20/20] avg loss: 0.008442349455491615		[learning rate: 0.00014317]
	Learning Rate: 0.000143167
	LOSS [training: 0.010589487783569437 | validation: 0.006263258222013983]
	TIME [epoch: 8.78 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011000120459630813		[learning rate: 0.00014299]
		[batch 20/20] avg loss: 0.005280143202980298		[learning rate: 0.00014282]
	Learning Rate: 0.00014282
	LOSS [training: 0.008140131831305557 | validation: 0.014472605380912648]
	TIME [epoch: 8.78 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018425547194377577		[learning rate: 0.00014265]
		[batch 20/20] avg loss: 0.007212133033378145		[learning rate: 0.00014247]
	Learning Rate: 0.000142474
	LOSS [training: 0.01281884011387786 | validation: 0.01424331188451487]
	TIME [epoch: 8.79 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01775072110368295		[learning rate: 0.0001423]
		[batch 20/20] avg loss: 0.005313593592929943		[learning rate: 0.00014213]
	Learning Rate: 0.000142129
	LOSS [training: 0.011532157348306449 | validation: 0.01863210344356179]
	TIME [epoch: 8.78 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014776226459694388		[learning rate: 0.00014196]
		[batch 20/20] avg loss: 0.008326580753308584		[learning rate: 0.00014179]
	Learning Rate: 0.000141785
	LOSS [training: 0.011551403606501488 | validation: 0.008785839720933568]
	TIME [epoch: 8.77 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007399543943125944		[learning rate: 0.00014161]
		[batch 20/20] avg loss: 0.013961684023515325		[learning rate: 0.00014144]
	Learning Rate: 0.000141442
	LOSS [training: 0.010680613983320635 | validation: 0.013415818749205163]
	TIME [epoch: 8.77 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013271701533629032		[learning rate: 0.00014127]
		[batch 20/20] avg loss: 0.009002125437321196		[learning rate: 0.0001411]
	Learning Rate: 0.0001411
	LOSS [training: 0.011136913485475113 | validation: 0.012901292062460006]
	TIME [epoch: 8.78 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003847717969850132		[learning rate: 0.00014093]
		[batch 20/20] avg loss: 0.011526302543343993		[learning rate: 0.00014076]
	Learning Rate: 0.000140758
	LOSS [training: 0.007687010256597064 | validation: 0.01920966549084698]
	TIME [epoch: 8.8 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00926857158456591		[learning rate: 0.00014059]
		[batch 20/20] avg loss: 0.018010897429805033		[learning rate: 0.00014042]
	Learning Rate: 0.000140417
	LOSS [training: 0.01363973450718547 | validation: 0.023582953828745288]
	TIME [epoch: 8.79 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014036223332363174		[learning rate: 0.00014025]
		[batch 20/20] avg loss: 0.020827890254787058		[learning rate: 0.00014008]
	Learning Rate: 0.000140078
	LOSS [training: 0.017432056793575117 | validation: 0.019750079529727738]
	TIME [epoch: 8.78 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0074916697647900545		[learning rate: 0.00013991]
		[batch 20/20] avg loss: 0.011692819690349143		[learning rate: 0.00013974]
	Learning Rate: 0.000139738
	LOSS [training: 0.009592244727569598 | validation: 0.01096029821603742]
	TIME [epoch: 8.78 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00791236305392797		[learning rate: 0.00013957]
		[batch 20/20] avg loss: 0.011296803882699203		[learning rate: 0.0001394]
	Learning Rate: 0.0001394
	LOSS [training: 0.009604583468313586 | validation: 0.017311367831350608]
	TIME [epoch: 8.77 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007029071518871116		[learning rate: 0.00013923]
		[batch 20/20] avg loss: 0.018397181576123266		[learning rate: 0.00013906]
	Learning Rate: 0.000139063
	LOSS [training: 0.012713126547497192 | validation: 0.009196043719592298]
	TIME [epoch: 8.8 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006425479093303051		[learning rate: 0.00013889]
		[batch 20/20] avg loss: 0.008451410487196803		[learning rate: 0.00013873]
	Learning Rate: 0.000138726
	LOSS [training: 0.007438444790249926 | validation: 0.012284017312966576]
	TIME [epoch: 8.78 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012191841031266252		[learning rate: 0.00013856]
		[batch 20/20] avg loss: 0.007259266730479166		[learning rate: 0.00013839]
	Learning Rate: 0.00013839
	LOSS [training: 0.00972555388087271 | validation: 0.011712755932023164]
	TIME [epoch: 8.77 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00950284186050708		[learning rate: 0.00013822]
		[batch 20/20] avg loss: 0.008974714095230232		[learning rate: 0.00013806]
	Learning Rate: 0.000138055
	LOSS [training: 0.009238777977868656 | validation: 0.00933051633733905]
	TIME [epoch: 8.78 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00869184815200845		[learning rate: 0.00013789]
		[batch 20/20] avg loss: 0.00941138005050704		[learning rate: 0.00013772]
	Learning Rate: 0.000137721
	LOSS [training: 0.009051614101257746 | validation: 0.017010389507184052]
	TIME [epoch: 8.79 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011344775457188193		[learning rate: 0.00013755]
		[batch 20/20] avg loss: 0.007472318610724203		[learning rate: 0.00013739]
	Learning Rate: 0.000137388
	LOSS [training: 0.009408547033956197 | validation: 0.009715693865980327]
	TIME [epoch: 8.78 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011676110507927403		[learning rate: 0.00013722]
		[batch 20/20] avg loss: 0.01239199118858399		[learning rate: 0.00013705]
	Learning Rate: 0.000137055
	LOSS [training: 0.012034050848255696 | validation: 0.014264954392607547]
	TIME [epoch: 8.77 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016076579704140406		[learning rate: 0.00013689]
		[batch 20/20] avg loss: 0.012923570801478037		[learning rate: 0.00013672]
	Learning Rate: 0.000136723
	LOSS [training: 0.014500075252809216 | validation: 0.005649499259113385]
	TIME [epoch: 8.77 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012113578724246798		[learning rate: 0.00013656]
		[batch 20/20] avg loss: 0.0086892454374357		[learning rate: 0.00013639]
	Learning Rate: 0.000136392
	LOSS [training: 0.01040141208084125 | validation: 0.015466829771068778]
	TIME [epoch: 8.78 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009799702002525147		[learning rate: 0.00013623]
		[batch 20/20] avg loss: 0.00888830253205219		[learning rate: 0.00013606]
	Learning Rate: 0.000136062
	LOSS [training: 0.009344002267288667 | validation: 0.011667776253085417]
	TIME [epoch: 8.8 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008675475143561814		[learning rate: 0.0001359]
		[batch 20/20] avg loss: 0.009991979618242695		[learning rate: 0.00013573]
	Learning Rate: 0.000135733
	LOSS [training: 0.009333727380902254 | validation: 0.008128832267513703]
	TIME [epoch: 8.79 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015081279800402556		[learning rate: 0.00013557]
		[batch 20/20] avg loss: 0.012532820129115241		[learning rate: 0.0001354]
	Learning Rate: 0.000135404
	LOSS [training: 0.0138070499647589 | validation: 0.012836628214999675]
	TIME [epoch: 8.79 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01808049605638373		[learning rate: 0.00013524]
		[batch 20/20] avg loss: 0.00910419760334963		[learning rate: 0.00013508]
	Learning Rate: 0.000135076
	LOSS [training: 0.013592346829866684 | validation: 0.009116847615493978]
	TIME [epoch: 8.79 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01101853366859672		[learning rate: 0.00013491]
		[batch 20/20] avg loss: 0.009675698732569198		[learning rate: 0.00013475]
	Learning Rate: 0.000134749
	LOSS [training: 0.01034711620058296 | validation: 0.002855618734165436]
	TIME [epoch: 8.79 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010294609712484628		[learning rate: 0.00013459]
		[batch 20/20] avg loss: 0.014369386892883532		[learning rate: 0.00013442]
	Learning Rate: 0.000134423
	LOSS [training: 0.012331998302684083 | validation: 0.01149386683742395]
	TIME [epoch: 8.78 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01089494636821878		[learning rate: 0.00013426]
		[batch 20/20] avg loss: 0.016124094726702252		[learning rate: 0.0001341]
	Learning Rate: 0.000134098
	LOSS [training: 0.013509520547460518 | validation: 0.010667672114307589]
	TIME [epoch: 8.79 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006063149754955568		[learning rate: 0.00013394]
		[batch 20/20] avg loss: 0.010252250995882284		[learning rate: 0.00013377]
	Learning Rate: 0.000133773
	LOSS [training: 0.008157700375418925 | validation: 0.015599648725961777]
	TIME [epoch: 8.78 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01005236238312735		[learning rate: 0.00013361]
		[batch 20/20] avg loss: 0.010086303571688623		[learning rate: 0.00013345]
	Learning Rate: 0.000133449
	LOSS [training: 0.010069332977407986 | validation: 0.00914751961682965]
	TIME [epoch: 8.78 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012188006031697826		[learning rate: 0.00013329]
		[batch 20/20] avg loss: 0.0006256303123136347		[learning rate: 0.00013313]
	Learning Rate: 0.000133126
	LOSS [training: 0.006406818172005731 | validation: 0.009802130024440407]
	TIME [epoch: 8.79 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019561581259548596		[learning rate: 0.00013296]
		[batch 20/20] avg loss: 0.01074003458272563		[learning rate: 0.0001328]
	Learning Rate: 0.000132804
	LOSS [training: 0.01515080792113711 | validation: 0.013691434972148132]
	TIME [epoch: 8.8 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009616633603563709		[learning rate: 0.00013264]
		[batch 20/20] avg loss: 0.009739918816432411		[learning rate: 0.00013248]
	Learning Rate: 0.000132482
	LOSS [training: 0.00967827620999806 | validation: 0.018016163447912972]
	TIME [epoch: 8.78 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013110540001837975		[learning rate: 0.00013232]
		[batch 20/20] avg loss: 0.01207834392361299		[learning rate: 0.00013216]
	Learning Rate: 0.000132162
	LOSS [training: 0.012594441962725484 | validation: 0.013290582344460188]
	TIME [epoch: 8.78 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008268768509119723		[learning rate: 0.000132]
		[batch 20/20] avg loss: 0.011061307001614269		[learning rate: 0.00013184]
	Learning Rate: 0.000131842
	LOSS [training: 0.009665037755366996 | validation: 0.022284131442677737]
	TIME [epoch: 8.79 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009744651196044656		[learning rate: 0.00013168]
		[batch 20/20] avg loss: 0.007592018882163984		[learning rate: 0.00013152]
	Learning Rate: 0.000131522
	LOSS [training: 0.00866833503910432 | validation: 0.007631370382591107]
	TIME [epoch: 8.81 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007742477300645092		[learning rate: 0.00013136]
		[batch 20/20] avg loss: 0.014228110332798461		[learning rate: 0.0001312]
	Learning Rate: 0.000131204
	LOSS [training: 0.010985293816721777 | validation: 0.012147324715371036]
	TIME [epoch: 8.81 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012626112025805417		[learning rate: 0.00013105]
		[batch 20/20] avg loss: 0.01568549032974938		[learning rate: 0.00013089]
	Learning Rate: 0.000130886
	LOSS [training: 0.0141558011777774 | validation: 0.017538480696235658]
	TIME [epoch: 8.79 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014473768619001048		[learning rate: 0.00013073]
		[batch 20/20] avg loss: 0.011147230196155489		[learning rate: 0.00013057]
	Learning Rate: 0.00013057
	LOSS [training: 0.012810499407578271 | validation: 0.019875142515985433]
	TIME [epoch: 8.78 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011227221791430868		[learning rate: 0.00013041]
		[batch 20/20] avg loss: 0.01506786537078098		[learning rate: 0.00013025]
	Learning Rate: 0.000130254
	LOSS [training: 0.013147543581105925 | validation: 0.013168136156984308]
	TIME [epoch: 8.78 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01121485823824014		[learning rate: 0.0001301]
		[batch 20/20] avg loss: 0.007034972093003985		[learning rate: 0.00012994]
	Learning Rate: 0.000129938
	LOSS [training: 0.009124915165622062 | validation: 0.008710095041946973]
	TIME [epoch: 8.81 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005276673242953585		[learning rate: 0.00012978]
		[batch 20/20] avg loss: 0.013166804060258944		[learning rate: 0.00012962]
	Learning Rate: 0.000129624
	LOSS [training: 0.009221738651606266 | validation: 0.013522917736250863]
	TIME [epoch: 8.78 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010348175189988078		[learning rate: 0.00012947]
		[batch 20/20] avg loss: 0.007273364043177684		[learning rate: 0.00012931]
	Learning Rate: 0.00012931
	LOSS [training: 0.008810769616582882 | validation: 0.009076606572663206]
	TIME [epoch: 8.8 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013638818268051864		[learning rate: 0.00012915]
		[batch 20/20] avg loss: 0.008754176831806518		[learning rate: 0.000129]
	Learning Rate: 0.000128997
	LOSS [training: 0.01119649754992919 | validation: 0.009639729810355319]
	TIME [epoch: 8.79 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00556790333674442		[learning rate: 0.00012884]
		[batch 20/20] avg loss: 0.008693183520493392		[learning rate: 0.00012868]
	Learning Rate: 0.000128685
	LOSS [training: 0.007130543428618906 | validation: 0.01684691063786362]
	TIME [epoch: 8.79 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010397180958046726		[learning rate: 0.00012853]
		[batch 20/20] avg loss: 0.007329694967233828		[learning rate: 0.00012837]
	Learning Rate: 0.000128373
	LOSS [training: 0.008863437962640275 | validation: 0.01208238401636329]
	TIME [epoch: 8.81 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010373392351529374		[learning rate: 0.00012822]
		[batch 20/20] avg loss: 0.009090911086967362		[learning rate: 0.00012806]
	Learning Rate: 0.000128062
	LOSS [training: 0.009732151719248369 | validation: 0.00920008607376179]
	TIME [epoch: 8.78 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013254291868051282		[learning rate: 0.00012791]
		[batch 20/20] avg loss: 0.014120491650082783		[learning rate: 0.00012775]
	Learning Rate: 0.000127752
	LOSS [training: 0.013687391759067033 | validation: 0.006297127192577581]
	TIME [epoch: 8.79 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009868866613036573		[learning rate: 0.0001276]
		[batch 20/20] avg loss: 0.012064462016659653		[learning rate: 0.00012744]
	Learning Rate: 0.000127443
	LOSS [training: 0.01096666431484811 | validation: 0.016560128036903675]
	TIME [epoch: 8.8 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007564059242357242		[learning rate: 0.00012729]
		[batch 20/20] avg loss: 0.017560162887136646		[learning rate: 0.00012713]
	Learning Rate: 0.000127134
	LOSS [training: 0.012562111064746945 | validation: 0.006747122932834797]
	TIME [epoch: 8.81 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010166404191278938		[learning rate: 0.00012698]
		[batch 20/20] avg loss: 0.011023272583740544		[learning rate: 0.00012683]
	Learning Rate: 0.000126827
	LOSS [training: 0.010594838387509738 | validation: 0.01001787079736744]
	TIME [epoch: 8.82 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007408525252623125		[learning rate: 0.00012667]
		[batch 20/20] avg loss: 0.014772634737630944		[learning rate: 0.00012652]
	Learning Rate: 0.00012652
	LOSS [training: 0.011090579995127033 | validation: 0.0146986037424542]
	TIME [epoch: 8.78 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009553203400011593		[learning rate: 0.00012637]
		[batch 20/20] avg loss: 0.016347382283924867		[learning rate: 0.00012621]
	Learning Rate: 0.000126213
	LOSS [training: 0.01295029284196823 | validation: 0.010636237562072048]
	TIME [epoch: 8.79 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008497932375370547		[learning rate: 0.00012606]
		[batch 20/20] avg loss: 0.017508547781916316		[learning rate: 0.00012591]
	Learning Rate: 0.000125908
	LOSS [training: 0.013003240078643433 | validation: 0.016763918359288883]
	TIME [epoch: 8.78 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013875651156675096		[learning rate: 0.00012576]
		[batch 20/20] avg loss: 0.011830639500553849		[learning rate: 0.0001256]
	Learning Rate: 0.000125603
	LOSS [training: 0.012853145328614474 | validation: 0.01154442366818852]
	TIME [epoch: 8.8 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009911979301765603		[learning rate: 0.00012545]
		[batch 20/20] avg loss: 0.008961285584719871		[learning rate: 0.0001253]
	Learning Rate: 0.000125299
	LOSS [training: 0.009436632443242738 | validation: 0.011301123816446029]
	TIME [epoch: 8.79 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009764749976720133		[learning rate: 0.00012515]
		[batch 20/20] avg loss: 0.01899540000776779		[learning rate: 0.000125]
	Learning Rate: 0.000124996
	LOSS [training: 0.014380074992243964 | validation: 0.011206432982297499]
	TIME [epoch: 8.78 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00459256548136734		[learning rate: 0.00012484]
		[batch 20/20] avg loss: 0.013283550739064207		[learning rate: 0.00012469]
	Learning Rate: 0.000124693
	LOSS [training: 0.008938058110215776 | validation: 0.004960304374732216]
	TIME [epoch: 8.79 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005467190313810172		[learning rate: 0.00012454]
		[batch 20/20] avg loss: 0.008906936786463369		[learning rate: 0.00012439]
	Learning Rate: 0.000124391
	LOSS [training: 0.00718706355013677 | validation: 0.0108184172793999]
	TIME [epoch: 8.77 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00699254419137927		[learning rate: 0.00012424]
		[batch 20/20] avg loss: 0.013599977248643983		[learning rate: 0.00012409]
	Learning Rate: 0.00012409
	LOSS [training: 0.010296260720011628 | validation: 0.008053653605977979]
	TIME [epoch: 8.8 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005462115799581805		[learning rate: 0.00012394]
		[batch 20/20] avg loss: 0.012286159732744103		[learning rate: 0.00012379]
	Learning Rate: 0.00012379
	LOSS [training: 0.008874137766162953 | validation: 0.014344097499482856]
	TIME [epoch: 8.77 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006429128179665606		[learning rate: 0.00012364]
		[batch 20/20] avg loss: 0.005199409693350723		[learning rate: 0.00012349]
	Learning Rate: 0.00012349
	LOSS [training: 0.0058142689365081645 | validation: 0.00924099052464845]
	TIME [epoch: 8.79 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009021799430487976		[learning rate: 0.00012334]
		[batch 20/20] avg loss: 0.010572163086433651		[learning rate: 0.00012319]
	Learning Rate: 0.000123191
	LOSS [training: 0.009796981258460813 | validation: 0.008668860461245976]
	TIME [epoch: 8.79 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010198953939276667		[learning rate: 0.00012304]
		[batch 20/20] avg loss: 0.008076858152369701		[learning rate: 0.00012289]
	Learning Rate: 0.000122893
	LOSS [training: 0.009137906045823183 | validation: 0.005053397802642572]
	TIME [epoch: 8.79 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009491359232507242		[learning rate: 0.00012274]
		[batch 20/20] avg loss: 0.014076017925833805		[learning rate: 0.0001226]
	Learning Rate: 0.000122595
	LOSS [training: 0.011783688579170525 | validation: 0.008152393431521961]
	TIME [epoch: 8.81 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019230116243566842		[learning rate: 0.00012245]
		[batch 20/20] avg loss: 0.012889323415363094		[learning rate: 0.0001223]
	Learning Rate: 0.000122298
	LOSS [training: 0.01605971982946497 | validation: 0.009059039994891986]
	TIME [epoch: 8.77 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01836984721289483		[learning rate: 0.00012215]
		[batch 20/20] avg loss: 0.014205250727642282		[learning rate: 0.000122]
	Learning Rate: 0.000122002
	LOSS [training: 0.016287548970268552 | validation: 0.009046208578288969]
	TIME [epoch: 8.78 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004728130122154811		[learning rate: 0.00012185]
		[batch 20/20] avg loss: 0.009985970046771975		[learning rate: 0.00012171]
	Learning Rate: 0.000121707
	LOSS [training: 0.0073570500844633915 | validation: 0.004725065070151196]
	TIME [epoch: 8.78 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011545284103330961		[learning rate: 0.00012156]
		[batch 20/20] avg loss: 0.008886157195369709		[learning rate: 0.00012141]
	Learning Rate: 0.000121412
	LOSS [training: 0.010215720649350335 | validation: 0.015013397634078851]
	TIME [epoch: 8.79 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014253809055374805		[learning rate: 0.00012127]
		[batch 20/20] avg loss: 0.0168037402832452		[learning rate: 0.00012112]
	Learning Rate: 0.000121119
	LOSS [training: 0.015528774669310002 | validation: 0.021479305978871033]
	TIME [epoch: 8.79 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01501994347547327		[learning rate: 0.00012097]
		[batch 20/20] avg loss: 0.014936700253734714		[learning rate: 0.00012083]
	Learning Rate: 0.000120825
	LOSS [training: 0.014978321864603992 | validation: 0.012593675645022766]
	TIME [epoch: 8.78 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01263867689235495		[learning rate: 0.00012068]
		[batch 20/20] avg loss: 0.009563413354853981		[learning rate: 0.00012053]
	Learning Rate: 0.000120533
	LOSS [training: 0.011101045123604467 | validation: 0.012792655346678826]
	TIME [epoch: 8.77 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009853515480398813		[learning rate: 0.00012039]
		[batch 20/20] avg loss: 0.014066750352091723		[learning rate: 0.00012024]
	Learning Rate: 0.000120241
	LOSS [training: 0.011960132916245269 | validation: 0.021740234240442382]
	TIME [epoch: 8.77 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01640482763107539		[learning rate: 0.0001201]
		[batch 20/20] avg loss: 0.01666356306536273		[learning rate: 0.00011995]
	Learning Rate: 0.00011995
	LOSS [training: 0.01653419534821906 | validation: 0.01204863494098463]
	TIME [epoch: 8.79 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003622827990297661		[learning rate: 0.0001198]
		[batch 20/20] avg loss: 0.012209531609564417		[learning rate: 0.00011966]
	Learning Rate: 0.00011966
	LOSS [training: 0.007916179799931039 | validation: 0.014147040149375123]
	TIME [epoch: 8.78 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00907884548416404		[learning rate: 0.00011951]
		[batch 20/20] avg loss: 0.012396837218641205		[learning rate: 0.00011937]
	Learning Rate: 0.00011937
	LOSS [training: 0.010737841351402625 | validation: 0.012547609357278767]
	TIME [epoch: 8.78 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016158191351927715		[learning rate: 0.00011923]
		[batch 20/20] avg loss: 0.011550815892865371		[learning rate: 0.00011908]
	Learning Rate: 0.000119081
	LOSS [training: 0.013854503622396545 | validation: 0.009849564500575388]
	TIME [epoch: 8.79 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010527664696834377		[learning rate: 0.00011894]
		[batch 20/20] avg loss: 0.01107790647725097		[learning rate: 0.00011879]
	Learning Rate: 0.000118793
	LOSS [training: 0.010802785587042675 | validation: 0.018274913482455662]
	TIME [epoch: 8.78 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005365691830360159		[learning rate: 0.00011865]
		[batch 20/20] avg loss: 0.010917170627088187		[learning rate: 0.00011851]
	Learning Rate: 0.000118505
	LOSS [training: 0.008141431228724173 | validation: 0.00908726391700173]
	TIME [epoch: 8.79 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008416495448559753		[learning rate: 0.00011836]
		[batch 20/20] avg loss: 0.007053444531982768		[learning rate: 0.00011822]
	Learning Rate: 0.000118218
	LOSS [training: 0.0077349699902712595 | validation: 0.00861594697301685]
	TIME [epoch: 8.78 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013567895531746815		[learning rate: 0.00011808]
		[batch 20/20] avg loss: 0.009359971692352333		[learning rate: 0.00011793]
	Learning Rate: 0.000117932
	LOSS [training: 0.011463933612049573 | validation: 0.007967274617463948]
	TIME [epoch: 8.78 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011452626217286689		[learning rate: 0.00011779]
		[batch 20/20] avg loss: 0.011561421101716634		[learning rate: 0.00011765]
	Learning Rate: 0.000117646
	LOSS [training: 0.011507023659501658 | validation: 0.0094041697701067]
	TIME [epoch: 8.78 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008785171965672838		[learning rate: 0.0001175]
		[batch 20/20] avg loss: 0.009404465618247551		[learning rate: 0.00011736]
	Learning Rate: 0.000117362
	LOSS [training: 0.009094818791960193 | validation: 0.014277007493786192]
	TIME [epoch: 8.78 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010233294103565153		[learning rate: 0.00011722]
		[batch 20/20] avg loss: 0.014940156814912356		[learning rate: 0.00011708]
	Learning Rate: 0.000117078
	LOSS [training: 0.012586725459238755 | validation: 0.0132667595622732]
	TIME [epoch: 8.79 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004965534652232		[learning rate: 0.00011694]
		[batch 20/20] avg loss: 0.010232268483891315		[learning rate: 0.00011679]
	Learning Rate: 0.000116794
	LOSS [training: 0.007598901568061658 | validation: 0.003679640740610768]
	TIME [epoch: 8.78 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010136201843434245		[learning rate: 0.00011665]
		[batch 20/20] avg loss: 0.007938911980454134		[learning rate: 0.00011651]
	Learning Rate: 0.000116511
	LOSS [training: 0.009037556911944189 | validation: 0.005808521904959606]
	TIME [epoch: 8.77 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013963434769465608		[learning rate: 0.00011637]
		[batch 20/20] avg loss: 0.008400050490125802		[learning rate: 0.00011623]
	Learning Rate: 0.000116229
	LOSS [training: 0.011181742629795705 | validation: 0.012075227127084764]
	TIME [epoch: 8.77 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010431889599593475		[learning rate: 0.00011609]
		[batch 20/20] avg loss: 0.006148228038367139		[learning rate: 0.00011595]
	Learning Rate: 0.000115948
	LOSS [training: 0.008290058818980307 | validation: 0.007730159787265423]
	TIME [epoch: 8.78 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011355791050779026		[learning rate: 0.00011581]
		[batch 20/20] avg loss: 0.010993762111970123		[learning rate: 0.00011567]
	Learning Rate: 0.000115667
	LOSS [training: 0.011174776581374574 | validation: 0.009424748025003465]
	TIME [epoch: 8.79 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009862481640431863		[learning rate: 0.00011553]
		[batch 20/20] avg loss: 0.0084616389184111		[learning rate: 0.00011539]
	Learning Rate: 0.000115387
	LOSS [training: 0.009162060279421482 | validation: 0.006543829945962271]
	TIME [epoch: 8.78 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007481292120846986		[learning rate: 0.00011525]
		[batch 20/20] avg loss: 0.012494594595906437		[learning rate: 0.00011511]
	Learning Rate: 0.000115108
	LOSS [training: 0.009987943358376713 | validation: 0.012644632374845846]
	TIME [epoch: 8.78 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016638190624664217		[learning rate: 0.00011497]
		[batch 20/20] avg loss: 0.011335270473476594		[learning rate: 0.00011483]
	Learning Rate: 0.000114829
	LOSS [training: 0.013986730549070403 | validation: 0.006356154648733335]
	TIME [epoch: 8.78 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010147912876596468		[learning rate: 0.00011469]
		[batch 20/20] avg loss: 0.011572920528421677		[learning rate: 0.00011455]
	Learning Rate: 0.000114551
	LOSS [training: 0.010860416702509073 | validation: 0.007623393064500156]
	TIME [epoch: 8.79 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005535490428927671		[learning rate: 0.00011441]
		[batch 20/20] avg loss: 0.0029811410791907777		[learning rate: 0.00011427]
	Learning Rate: 0.000114274
	LOSS [training: 0.004258315754059224 | validation: 0.008978994338652912]
	TIME [epoch: 8.79 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013379820887743438		[learning rate: 0.00011414]
		[batch 20/20] avg loss: 0.006935480851244296		[learning rate: 0.000114]
	Learning Rate: 0.000113997
	LOSS [training: 0.010157650869493867 | validation: 0.012256392959604095]
	TIME [epoch: 8.77 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007798717209922807		[learning rate: 0.00011386]
		[batch 20/20] avg loss: 0.00436154011875696		[learning rate: 0.00011372]
	Learning Rate: 0.000113721
	LOSS [training: 0.006080128664339882 | validation: 0.006338023052675924]
	TIME [epoch: 8.78 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011039919751331144		[learning rate: 0.00011358]
		[batch 20/20] avg loss: 0.009374682001013837		[learning rate: 0.00011345]
	Learning Rate: 0.000113446
	LOSS [training: 0.010207300876172491 | validation: 0.0052080876474875975]
	TIME [epoch: 8.77 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006634473481425519		[learning rate: 0.00011331]
		[batch 20/20] avg loss: 0.004703872497641084		[learning rate: 0.00011317]
	Learning Rate: 0.000113171
	LOSS [training: 0.005669172989533301 | validation: 0.011799398797665749]
	TIME [epoch: 8.79 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005745854550500397		[learning rate: 0.00011303]
		[batch 20/20] avg loss: 0.008986502707785378		[learning rate: 0.0001129]
	Learning Rate: 0.000112897
	LOSS [training: 0.007366178629142889 | validation: 0.010340201290232187]
	TIME [epoch: 8.77 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010310484661988622		[learning rate: 0.00011276]
		[batch 20/20] avg loss: 0.013678568635599842		[learning rate: 0.00011262]
	Learning Rate: 0.000112624
	LOSS [training: 0.011994526648794232 | validation: 0.012158749535708627]
	TIME [epoch: 8.77 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010502958018442424		[learning rate: 0.00011249]
		[batch 20/20] avg loss: 0.011960167447709245		[learning rate: 0.00011235]
	Learning Rate: 0.000112352
	LOSS [training: 0.011231562733075833 | validation: 0.008398024747571887]
	TIME [epoch: 8.77 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01194674195743341		[learning rate: 0.00011222]
		[batch 20/20] avg loss: 0.0066636723910298845		[learning rate: 0.00011208]
	Learning Rate: 0.00011208
	LOSS [training: 0.009305207174231648 | validation: 0.004799597160014296]
	TIME [epoch: 8.79 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009287469950973157		[learning rate: 0.00011194]
		[batch 20/20] avg loss: 0.01058949874061558		[learning rate: 0.00011181]
	Learning Rate: 0.000111808
	LOSS [training: 0.009938484345794368 | validation: 0.0019127338614922593]
	TIME [epoch: 8.79 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004937002074998095		[learning rate: 0.00011167]
		[batch 20/20] avg loss: 0.007773384536425616		[learning rate: 0.00011154]
	Learning Rate: 0.000111538
	LOSS [training: 0.0063551933057118554 | validation: -0.0001954101077969036]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240219_183142/states/model_tr_study1_1955.pth
	Model improved!!!
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010872826277466165		[learning rate: 0.0001114]
		[batch 20/20] avg loss: 0.008783011126132816		[learning rate: 0.00011127]
	Learning Rate: 0.000111268
	LOSS [training: 0.009827918701799492 | validation: 0.01018194343712747]
	TIME [epoch: 8.77 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014589852452913471		[learning rate: 0.00011113]
		[batch 20/20] avg loss: -0.001545759352078621		[learning rate: 0.000111]
	Learning Rate: 0.000110998
	LOSS [training: 0.006522046550417422 | validation: 0.012499459333221902]
	TIME [epoch: 8.77 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011602003795246497		[learning rate: 0.00011086]
		[batch 20/20] avg loss: 0.0074395439212259174		[learning rate: 0.00011073]
	Learning Rate: 0.000110729
	LOSS [training: 0.009520773858236208 | validation: 0.006859605379541667]
	TIME [epoch: 8.78 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011380593847566279		[learning rate: 0.0001106]
		[batch 20/20] avg loss: 0.007688834565202546		[learning rate: 0.00011046]
	Learning Rate: 0.000110461
	LOSS [training: 0.009534714206384413 | validation: 0.00936902096264048]
	TIME [epoch: 8.77 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009394390367145688		[learning rate: 0.00011033]
		[batch 20/20] avg loss: 0.008073715810674393		[learning rate: 0.00011019]
	Learning Rate: 0.000110194
	LOSS [training: 0.008734053088910041 | validation: 0.008951412253930858]
	TIME [epoch: 8.78 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009842101860861827		[learning rate: 0.00011006]
		[batch 20/20] avg loss: 0.008204790212050502		[learning rate: 0.00010993]
	Learning Rate: 0.000109927
	LOSS [training: 0.009023446036456165 | validation: 0.0072575398518215095]
	TIME [epoch: 8.77 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012452710252468874		[learning rate: 0.00010979]
		[batch 20/20] avg loss: 0.012256240056552997		[learning rate: 0.00010966]
	Learning Rate: 0.000109661
	LOSS [training: 0.012354475154510932 | validation: 0.005622156410439874]
	TIME [epoch: 8.77 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010326834676297374		[learning rate: 0.00010953]
		[batch 20/20] avg loss: 0.010674649804869318		[learning rate: 0.0001094]
	Learning Rate: 0.000109396
	LOSS [training: 0.010500742240583347 | validation: 0.010513659334927055]
	TIME [epoch: 8.8 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01607488216114443		[learning rate: 0.00010926]
		[batch 20/20] avg loss: 0.00839195357341711		[learning rate: 0.00010913]
	Learning Rate: 0.000109131
	LOSS [training: 0.012233417867280768 | validation: 0.012419394252491667]
	TIME [epoch: 8.77 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015384152056976325		[learning rate: 0.000109]
		[batch 20/20] avg loss: 0.004960069969330455		[learning rate: 0.00010887]
	Learning Rate: 0.000108867
	LOSS [training: 0.01017211101315339 | validation: 0.009874506604201685]
	TIME [epoch: 8.78 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007082413963398962		[learning rate: 0.00010873]
		[batch 20/20] avg loss: 0.01149008979259869		[learning rate: 0.0001086]
	Learning Rate: 0.000108603
	LOSS [training: 0.009286251877998825 | validation: 0.010309931123220211]
	TIME [epoch: 8.77 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012319353263755614		[learning rate: 0.00010847]
		[batch 20/20] avg loss: 0.011915181576880865		[learning rate: 0.00010834]
	Learning Rate: 0.00010834
	LOSS [training: 0.012117267420318243 | validation: 0.005648341591511481]
	TIME [epoch: 8.77 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0063505278807408		[learning rate: 0.00010821]
		[batch 20/20] avg loss: 0.012679762870027447		[learning rate: 0.00010808]
	Learning Rate: 0.000108078
	LOSS [training: 0.009515145375384126 | validation: 0.004790940171275808]
	TIME [epoch: 8.79 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003775416957847462		[learning rate: 0.00010795]
		[batch 20/20] avg loss: 0.01268032118139209		[learning rate: 0.00010782]
	Learning Rate: 0.000107816
	LOSS [training: 0.008227869069619776 | validation: 0.008000907956767478]
	TIME [epoch: 8.78 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008102148645246724		[learning rate: 0.00010769]
		[batch 20/20] avg loss: 0.008534147598390957		[learning rate: 0.00010756]
	Learning Rate: 0.000107555
	LOSS [training: 0.00831814812181884 | validation: 0.010618861755432363]
	TIME [epoch: 8.78 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015219839360223761		[learning rate: 0.00010742]
		[batch 20/20] avg loss: 0.010915775717671803		[learning rate: 0.00010729]
	Learning Rate: 0.000107295
	LOSS [training: 0.013067807538947781 | validation: 0.011035998215284948]
	TIME [epoch: 8.77 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0066767059807626775		[learning rate: 0.00010716]
		[batch 20/20] avg loss: 0.014080016463612774		[learning rate: 0.00010704]
	Learning Rate: 0.000107035
	LOSS [training: 0.010378361222187724 | validation: 0.0056523924991033715]
	TIME [epoch: 8.78 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012639199524782804		[learning rate: 0.00010691]
		[batch 20/20] avg loss: 0.004729921292380419		[learning rate: 0.00010678]
	Learning Rate: 0.000106776
	LOSS [training: 0.008684560408581612 | validation: 0.012708233576738661]
	TIME [epoch: 8.78 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011915597154211737		[learning rate: 0.00010665]
		[batch 20/20] avg loss: 0.010010074848488367		[learning rate: 0.00010652]
	Learning Rate: 0.000106518
	LOSS [training: 0.01096283600135005 | validation: 0.006748596355066242]
	TIME [epoch: 8.77 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01016563826927994		[learning rate: 0.00010639]
		[batch 20/20] avg loss: 0.0061792225291585024		[learning rate: 0.00010626]
	Learning Rate: 0.00010626
	LOSS [training: 0.008172430399219223 | validation: 0.0056837299609552505]
	TIME [epoch: 8.77 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011968487262740229		[learning rate: 0.00010613]
		[batch 20/20] avg loss: 0.00332781519674372		[learning rate: 0.000106]
	Learning Rate: 0.000106002
	LOSS [training: 0.007648151229741974 | validation: 0.010189304939087644]
	TIME [epoch: 8.77 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008709273276461378		[learning rate: 0.00010587]
		[batch 20/20] avg loss: 0.009596837042830967		[learning rate: 0.00010575]
	Learning Rate: 0.000105746
	LOSS [training: 0.009153055159646173 | validation: 0.011406161670468382]
	TIME [epoch: 8.79 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005089995286048018		[learning rate: 0.00010562]
		[batch 20/20] avg loss: 0.010739863835232757		[learning rate: 0.00010549]
	Learning Rate: 0.00010549
	LOSS [training: 0.007914929560640386 | validation: 0.0038937755542446948]
	TIME [epoch: 8.76 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007878381419599218		[learning rate: 0.00010536]
		[batch 20/20] avg loss: 0.007092628732767006		[learning rate: 0.00010523]
	Learning Rate: 0.000105234
	LOSS [training: 0.007485505076183112 | validation: 0.011602303407131854]
	TIME [epoch: 8.77 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009195995468270398		[learning rate: 0.00010511]
		[batch 20/20] avg loss: 0.007144247138337454		[learning rate: 0.00010498]
	Learning Rate: 0.00010498
	LOSS [training: 0.008170121303303926 | validation: 0.00860353976981364]
	TIME [epoch: 8.77 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006700070869525142		[learning rate: 0.00010485]
		[batch 20/20] avg loss: 0.00988906219878748		[learning rate: 0.00010473]
	Learning Rate: 0.000104726
	LOSS [training: 0.008294566534156313 | validation: 0.011264420390057022]
	TIME [epoch: 8.77 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009855086627303429		[learning rate: 0.0001046]
		[batch 20/20] avg loss: 0.009795229303547036		[learning rate: 0.00010447]
	Learning Rate: 0.000104472
	LOSS [training: 0.00982515796542523 | validation: 0.01060757454658938]
	TIME [epoch: 8.81 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011021456756711503		[learning rate: 0.00010435]
		[batch 20/20] avg loss: 0.004562704685428268		[learning rate: 0.00010422]
	Learning Rate: 0.000104219
	LOSS [training: 0.007792080721069886 | validation: 0.004015757157850325]
	TIME [epoch: 8.79 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012512317268217974		[learning rate: 0.00010409]
		[batch 20/20] avg loss: 0.010317667206787024		[learning rate: 0.00010397]
	Learning Rate: 0.000103967
	LOSS [training: 0.011414992237502499 | validation: 0.009630601986945323]
	TIME [epoch: 8.78 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009644179120934862		[learning rate: 0.00010384]
		[batch 20/20] avg loss: 0.013863557753238113		[learning rate: 0.00010372]
	Learning Rate: 0.000103715
	LOSS [training: 0.01175386843708649 | validation: 0.009271827963932722]
	TIME [epoch: 8.77 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011356471177538553		[learning rate: 0.00010359]
		[batch 20/20] avg loss: 0.014895727925669802		[learning rate: 0.00010346]
	Learning Rate: 0.000103464
	LOSS [training: 0.013126099551604178 | validation: 0.008325974703553608]
	TIME [epoch: 8.78 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013549593293872985		[learning rate: 0.00010334]
		[batch 20/20] avg loss: 0.008930074331687443		[learning rate: 0.00010321]
	Learning Rate: 0.000103214
	LOSS [training: 0.011239833812780214 | validation: 0.005887159717997508]
	TIME [epoch: 8.79 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012900510638710824		[learning rate: 0.00010309]
		[batch 20/20] avg loss: 0.01278361826642875		[learning rate: 0.00010296]
	Learning Rate: 0.000102964
	LOSS [training: 0.012842064452569787 | validation: 0.006850391777227974]
	TIME [epoch: 8.77 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00682151210902093		[learning rate: 0.00010284]
		[batch 20/20] avg loss: 0.008884165440806346		[learning rate: 0.00010271]
	Learning Rate: 0.000102714
	LOSS [training: 0.007852838774913638 | validation: 0.00893417360609911]
	TIME [epoch: 8.78 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010439474009419799		[learning rate: 0.00010259]
		[batch 20/20] avg loss: 0.006242639397534199		[learning rate: 0.00010247]
	Learning Rate: 0.000102466
	LOSS [training: 0.008341056703476995 | validation: 0.008899529846737576]
	TIME [epoch: 8.76 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011782710918450536		[learning rate: 0.00010234]
		[batch 20/20] avg loss: 0.012389129329260586		[learning rate: 0.00010222]
	Learning Rate: 0.000102218
	LOSS [training: 0.012085920123855561 | validation: 0.008007307933962208]
	TIME [epoch: 8.79 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0063856240210120405		[learning rate: 0.00010209]
		[batch 20/20] avg loss: 0.009335878285122624		[learning rate: 0.00010197]
	Learning Rate: 0.00010197
	LOSS [training: 0.007860751153067332 | validation: 0.007176583274553616]
	TIME [epoch: 8.78 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015214851857845244		[learning rate: 0.00010185]
		[batch 20/20] avg loss: 0.008610884419824641		[learning rate: 0.00010172]
	Learning Rate: 0.000101723
	LOSS [training: 0.011912868138834942 | validation: 0.005030008776719442]
	TIME [epoch: 8.77 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007968902719418797		[learning rate: 0.0001016]
		[batch 20/20] avg loss: 0.006862106349364547		[learning rate: 0.00010148]
	Learning Rate: 0.000101477
	LOSS [training: 0.007415504534391671 | validation: 0.008082419227458055]
	TIME [epoch: 8.77 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01286486844027701		[learning rate: 0.00010135]
		[batch 20/20] avg loss: 0.01110956236067653		[learning rate: 0.00010123]
	Learning Rate: 0.000101232
	LOSS [training: 0.011987215400476772 | validation: 0.012453350865753299]
	TIME [epoch: 8.77 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011542788280836986		[learning rate: 0.00010111]
		[batch 20/20] avg loss: 0.015458428543001033		[learning rate: 0.00010099]
	Learning Rate: 0.000100986
	LOSS [training: 0.013500608411919007 | validation: 0.014794803957630455]
	TIME [epoch: 8.8 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010418028095893806		[learning rate: 0.00010086]
		[batch 20/20] avg loss: 0.013608885267045217		[learning rate: 0.00010074]
	Learning Rate: 0.000100742
	LOSS [training: 0.01201345668146951 | validation: 0.0008605833079868841]
	TIME [epoch: 8.79 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008719962819986285		[learning rate: 0.00010062]
		[batch 20/20] avg loss: 0.0044632896635030525		[learning rate: 0.0001005]
	Learning Rate: 0.000100498
	LOSS [training: 0.006591626241744669 | validation: 0.007521215748553687]
	TIME [epoch: 8.77 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013364520469210977		[learning rate: 0.00010038]
		[batch 20/20] avg loss: 0.010413267369365766		[learning rate: 0.00010025]
	Learning Rate: 0.000100255
	LOSS [training: 0.011888893919288372 | validation: 0.008399265695429212]
	TIME [epoch: 8.78 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015981964393911204		[learning rate: 0.00010013]
		[batch 20/20] avg loss: 0.010076539582340976		[learning rate: 0.00010001]
	Learning Rate: 0.000100012
	LOSS [training: 0.01302925198812609 | validation: 0.011514578748315546]
	TIME [epoch: 8.82 sec]
Finished training in 17704.858 seconds.
