Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r1', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3524501139

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.648702030676615		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.424401242233476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.536551636455046 | validation: 6.776262491101882]
	TIME [epoch: 47.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.124520308354491		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.8802291826533635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.0023747455039285 | validation: 7.18880577512287]
	TIME [epoch: 8.86 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.693007903362161		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.658732694064473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.675870298713318 | validation: 6.480939843309609]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.6888451668719995		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.394835467757575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.541840317314788 | validation: 6.424091941766378]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.345997790379942		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.364808194771376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.355402992575659 | validation: 6.460104827784133]
	TIME [epoch: 8.85 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.4050309128452865		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.191661586188269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.2983462495167775 | validation: 5.862095177774091]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.741571126411388		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.669170072325597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.705370599368492 | validation: 5.6344832024621985]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.620176353486361		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.525177508010023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.572676930748193 | validation: 5.622934421592658]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.52659086095515		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.604802922633192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.565696891794173 | validation: 5.4266303191118554]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.488258366976448		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.5086443943388685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.4984513806576585 | validation: 5.512745104787829]
	TIME [epoch: 8.84 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.483202106761836		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.662156742907997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.572679424834917 | validation: 5.6041471185709675]
	TIME [epoch: 8.85 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.694181329183335		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.65214009066697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6731607099251535 | validation: 5.315557689002997]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.518099568574383		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.629323736080925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.573711652327654 | validation: 5.497640780924771]
	TIME [epoch: 8.85 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.420447017993409		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.482797409177083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.451622213585245 | validation: 5.44953143320662]
	TIME [epoch: 8.85 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.484881534582486		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.354621245265395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.419751389923941 | validation: 5.423042512823816]
	TIME [epoch: 8.86 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.373940607465618		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.317395043833197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.345667825649408 | validation: 5.470728550584632]
	TIME [epoch: 8.85 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.338734841980479		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.401544830023584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.370139836002031 | validation: 5.24429992541161]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.177982354156042		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.224735625671529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2013589899137855 | validation: 5.257880770234509]
	TIME [epoch: 8.85 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.133445976201729		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.133059247928907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.133252612065318 | validation: 5.324493245470353]
	TIME [epoch: 8.87 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.034012052223189		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.153490523549334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.093751287886263 | validation: 4.994136135403883]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.819303131479615		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.303805651884708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0615543916821615 | validation: 4.883276494978323]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.668674007357636		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.335175331796203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.501924669576921 | validation: 4.364174673957259]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.249505999816809		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.881789098785935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.065647549301371 | validation: 3.2409315093159146]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.185871850558136		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8599194687523117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.022895659655224 | validation: 3.8231632896195844]
	TIME [epoch: 8.86 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.707389046710183		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6390665707308614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.673227808720522 | validation: 2.560178359661575]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.172149681602674		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3862135646032696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.279181623102972 | validation: 2.9796543826273716]
	TIME [epoch: 8.85 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1716122511832507		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8111847213322743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9913984862577632 | validation: 1.5871705860376244]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.671895562495898		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5580959551446356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6149957588202668 | validation: 1.524728502825478]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5959313248165319		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5501037737963155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5730175493064236 | validation: 1.0944425859984062]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4494081068421152		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4177856098797696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4335968583609424 | validation: 0.93232862563655]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3990179606994457		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6283500855077424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5136840231035942 | validation: 1.297803643881867]
	TIME [epoch: 8.84 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.171802396019936		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4594236703558043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3156130331878704 | validation: 1.4392062602146618]
	TIME [epoch: 8.85 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1776457751492424		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5309061827546704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3542759789519565 | validation: 2.283092930004714]
	TIME [epoch: 8.83 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3891500020868603		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1498522020487079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.269501102067784 | validation: 0.9162021221820219]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0532041360839528		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1665513359786637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1098777360313083 | validation: 1.2810785174111767]
	TIME [epoch: 8.83 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4264716052076771		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.195771057734681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.311121331471179 | validation: 1.2047770252241574]
	TIME [epoch: 8.85 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.190992897963928		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6501869168421073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4205899074030177 | validation: 1.1264588333913466]
	TIME [epoch: 8.83 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1469095363903308		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1218248654904248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1343672009403778 | validation: 0.9893755603348536]
	TIME [epoch: 8.83 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3086433478011483		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1025004138976562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.205571880849402 | validation: 0.7561054764266377]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.405185001453357		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7004843520351482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5528346767442527 | validation: 1.2069411153587506]
	TIME [epoch: 8.84 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1787626082491678		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0995501948849182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.139156401567043 | validation: 1.4591795324272994]
	TIME [epoch: 8.85 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2345409084837198		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3823430347461605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3084419716149402 | validation: 0.8840224220698687]
	TIME [epoch: 8.83 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9634453496452341		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9572895314361535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9603674405406938 | validation: 0.7378538353635666]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8901958244578809		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2100683126918788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0501320685748798 | validation: 0.7185154424717244]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0545623706538971		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1898368445621528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.122199607608025 | validation: 0.8815125988735081]
	TIME [epoch: 8.85 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9955724767147164		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.326929759366592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1612511180406542 | validation: 0.9766168273991538]
	TIME [epoch: 8.83 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0554074894383056		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0850418903698547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0702246899040802 | validation: 0.7638611337799388]
	TIME [epoch: 8.82 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0637784844895037		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2486718638496317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1562251741695675 | validation: 0.7761272456145977]
	TIME [epoch: 8.82 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.016657726310061		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.123476169341578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0700669478258198 | validation: 0.9405026670426554]
	TIME [epoch: 8.85 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9229189281609692		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1478855975288753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0354022628449224 | validation: 1.3294057395042547]
	TIME [epoch: 8.83 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0489727668660334		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2196482279066152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1343104973863243 | validation: 0.7509990606864874]
	TIME [epoch: 8.83 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.036544699486376		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0394692087530821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0380069541197288 | validation: 1.2287014750201453]
	TIME [epoch: 8.83 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0345718934838743		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.176551384353464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1055616389186693 | validation: 0.7489873932703427]
	TIME [epoch: 8.86 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0311400799539727		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9741101422292179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0026251110915956 | validation: 0.7623399503222095]
	TIME [epoch: 8.84 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9642438205714431		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1087663761611766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0365050983663096 | validation: 1.125204700304249]
	TIME [epoch: 8.83 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9087988352293384		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2299043101466545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0693515726879963 | validation: 1.0286881570900177]
	TIME [epoch: 8.85 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1944433747568342		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9045609374045099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.049502156080672 | validation: 0.6983220545660633]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9818559773121892		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0540607394897605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.017958358400975 | validation: 1.0986527631443739]
	TIME [epoch: 8.85 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.08506318763809		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.360595547664182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.222829367651136 | validation: 0.7926330625022133]
	TIME [epoch: 8.83 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9242741318148978		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1444846346067123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0343793832108052 | validation: 1.2511154553117891]
	TIME [epoch: 8.82 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.090142094730037		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8674194992514224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9787807969907298 | validation: 0.584818220282553]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9055598703489818		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8938597574227491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8997098138858654 | validation: 0.7087044561345379]
	TIME [epoch: 8.86 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.022319412766388		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0038804546411602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0130999337037738 | validation: 2.1525102582700137]
	TIME [epoch: 8.84 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0731429470118863		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8722504842220241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9726967156169553 | validation: 0.7685421131258903]
	TIME [epoch: 8.83 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9107082398275914		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9513112642755871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9310097520515892 | validation: 0.579778705411667]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9183308479443827		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.986689697997534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9525102729709586 | validation: 0.9867586396477505]
	TIME [epoch: 8.86 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9393473185568052		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8625507472428241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9009490328998149 | validation: 0.7409539968550702]
	TIME [epoch: 8.84 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4196724694964789		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9563765189669713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1880244942317248 | validation: 0.6735187908385603]
	TIME [epoch: 8.83 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.880778679611432		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9252911884200803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9030349340157562 | validation: 0.9641803115850732]
	TIME [epoch: 8.82 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9413200319949727		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.860819710662361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9010698713286669 | validation: 0.8994528903106882]
	TIME [epoch: 8.84 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.948696291213397		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7876597474223581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8681780193178776 | validation: 0.7505432567632235]
	TIME [epoch: 8.84 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8044695501108954		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1647853859594455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9846274680351705 | validation: 0.8505622826016044]
	TIME [epoch: 8.83 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0127158746206788		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8815268153508388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9471213449857586 | validation: 3.007194290166136]
	TIME [epoch: 8.84 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3676735496673855		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9905971444462753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1791353470568304 | validation: 0.7509485320050628]
	TIME [epoch: 8.83 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7504732823704254		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9490957735289636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8497845279496945 | validation: 1.355001311991256]
	TIME [epoch: 8.84 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8631811450057931		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9017011312147097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8824411381102515 | validation: 1.0801750701301152]
	TIME [epoch: 8.83 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0788386923207567		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7658342162128886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9223364542668225 | validation: 0.5503691544798736]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8035592290719873		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7310634138314271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7673113214517072 | validation: 0.5871020698596361]
	TIME [epoch: 8.84 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.832481335760565		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7191783948492341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7758298653048995 | validation: 1.1536480492258459]
	TIME [epoch: 8.86 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9990975619955499		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9372107984714664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.968154180233508 | validation: 0.7909309356924014]
	TIME [epoch: 8.82 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8164005141986159		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8542931544588356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8353468343287258 | validation: 0.6054711736242987]
	TIME [epoch: 8.84 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8984490315598647		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8581655402603434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8783072859101042 | validation: 0.5191554454052398]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9609760278659494		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.865213898018545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9130949629422472 | validation: 0.8303695063503508]
	TIME [epoch: 8.85 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8286608926745688		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1391081426393772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9838845176569728 | validation: 0.9818186338641737]
	TIME [epoch: 8.84 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7969276942216487		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9548645087916092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8758961015066291 | validation: 0.9153483832457059]
	TIME [epoch: 8.84 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8629648210015912		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9731102218540162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9180375214278035 | validation: 0.7257264403379153]
	TIME [epoch: 8.84 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8335796603140884		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8355180681653591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8345488642397237 | validation: 0.5064472784663141]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1156087014837		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9093632560259468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0124859787548235 | validation: 0.5302820706623803]
	TIME [epoch: 8.86 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8270522788316248		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7994759513294064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8132641150805157 | validation: 1.3841545477486397]
	TIME [epoch: 8.83 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0115106981591044		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9888012423848032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0001559702719536 | validation: 0.866521138610427]
	TIME [epoch: 8.84 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8781063272283172		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8539994815742384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8660529044012778 | validation: 0.7600306151950138]
	TIME [epoch: 8.84 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7507835065539783		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0014986141628837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.876141060358431 | validation: 1.104723098164245]
	TIME [epoch: 8.85 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7934527234660232		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0320581026488935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9127554130574584 | validation: 0.7719841711524109]
	TIME [epoch: 8.84 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8597918140887009		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9208970356506458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8903444248696735 | validation: 1.1674138063534787]
	TIME [epoch: 8.84 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8725522751502715		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0172301743653906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9448912247578312 | validation: 0.8772947074851007]
	TIME [epoch: 8.84 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8251653103757975		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9481344811461611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8866498957609792 | validation: 0.562624956056154]
	TIME [epoch: 8.86 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6408101631100478		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.714929510905524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6778698370077858 | validation: 0.6420885692393792]
	TIME [epoch: 8.84 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7795384278905622		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7598969223943371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7697176751424496 | validation: 0.628001603030621]
	TIME [epoch: 8.83 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8551422955436069		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5885066958008695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7218244956722383 | validation: 0.6378119274984466]
	TIME [epoch: 8.84 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9223729641200133		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8244615797190541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8734172719195339 | validation: 0.6302736202305004]
	TIME [epoch: 8.85 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6307905672659327		[learning rate: 0.0099891]
		[batch 20/20] avg loss: 0.790964089567491		[learning rate: 0.009977]
	Learning Rate: 0.009977
	LOSS [training: 0.7108773284167118 | validation: 1.7169009944287823]
	TIME [epoch: 8.86 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0277727343184448		[learning rate: 0.0099649]
		[batch 20/20] avg loss: 0.7179058645513152		[learning rate: 0.0099528]
	Learning Rate: 0.00995285
	LOSS [training: 0.87283929943488 | validation: 0.387301768384963]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6744018317075057		[learning rate: 0.0099408]
		[batch 20/20] avg loss: 0.6748973636315946		[learning rate: 0.0099288]
	Learning Rate: 0.00992875
	LOSS [training: 0.6746495976695501 | validation: 0.631546352622463]
	TIME [epoch: 8.83 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7350948598040209		[learning rate: 0.0099167]
		[batch 20/20] avg loss: 0.6814794648965119		[learning rate: 0.0099047]
	Learning Rate: 0.00990472
	LOSS [training: 0.7082871623502662 | validation: 0.5407706016147373]
	TIME [epoch: 8.84 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7422139047221754		[learning rate: 0.0098927]
		[batch 20/20] avg loss: 0.8038701240150926		[learning rate: 0.0098807]
	Learning Rate: 0.00988074
	LOSS [training: 0.7730420143686338 | validation: 0.5100315040157873]
	TIME [epoch: 8.86 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7748147459283842		[learning rate: 0.0098688]
		[batch 20/20] avg loss: 0.7736114193843899		[learning rate: 0.0098568]
	Learning Rate: 0.00985682
	LOSS [training: 0.7742130826563869 | validation: 0.38978149145082364]
	TIME [epoch: 8.84 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6580996973009619		[learning rate: 0.0098449]
		[batch 20/20] avg loss: 0.6677919157295451		[learning rate: 0.009833]
	Learning Rate: 0.00983296
	LOSS [training: 0.6629458065152535 | validation: 0.44195247476266286]
	TIME [epoch: 8.84 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6111914958669915		[learning rate: 0.009821]
		[batch 20/20] avg loss: 0.7926979755107111		[learning rate: 0.0098092]
	Learning Rate: 0.00980915
	LOSS [training: 0.7019447356888511 | validation: 0.39947875661372256]
	TIME [epoch: 8.83 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7234093166245616		[learning rate: 0.0097973]
		[batch 20/20] avg loss: 0.8568258592830821		[learning rate: 0.0097854]
	Learning Rate: 0.00978541
	LOSS [training: 0.7901175879538218 | validation: 0.4945146432411725]
	TIME [epoch: 8.85 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7423054162573305		[learning rate: 0.0097736]
		[batch 20/20] avg loss: 0.8431270184569947		[learning rate: 0.0097617]
	Learning Rate: 0.00976172
	LOSS [training: 0.7927162173571625 | validation: 0.5852466838307209]
	TIME [epoch: 8.84 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5931110678253314		[learning rate: 0.0097499]
		[batch 20/20] avg loss: 0.7114745759558618		[learning rate: 0.0097381]
	Learning Rate: 0.00973809
	LOSS [training: 0.6522928218905967 | validation: 0.6863311653733735]
	TIME [epoch: 8.83 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6408704161876407		[learning rate: 0.0097263]
		[batch 20/20] avg loss: 0.7971867343468243		[learning rate: 0.0097145]
	Learning Rate: 0.00971451
	LOSS [training: 0.7190285752672325 | validation: 0.5061715441220292]
	TIME [epoch: 8.83 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5008516193433616		[learning rate: 0.0097027]
		[batch 20/20] avg loss: 0.6100354499174577		[learning rate: 0.009691]
	Learning Rate: 0.009691
	LOSS [training: 0.5554435346304095 | validation: 0.5692159651994162]
	TIME [epoch: 8.85 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8032733955149819		[learning rate: 0.0096793]
		[batch 20/20] avg loss: 0.6451398624155893		[learning rate: 0.0096675]
	Learning Rate: 0.00966754
	LOSS [training: 0.7242066289652854 | validation: 1.0185265305780082]
	TIME [epoch: 8.84 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.685889055512132		[learning rate: 0.0096558]
		[batch 20/20] avg loss: 0.6582197835304926		[learning rate: 0.0096441]
	Learning Rate: 0.00964413
	LOSS [training: 0.6720544195213123 | validation: 0.4736002926138517]
	TIME [epoch: 8.84 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5648733180349393		[learning rate: 0.0096325]
		[batch 20/20] avg loss: 0.7694101926621826		[learning rate: 0.0096208]
	Learning Rate: 0.00962078
	LOSS [training: 0.6671417553485609 | validation: 0.9182360989771566]
	TIME [epoch: 8.84 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6609452771263645		[learning rate: 0.0096091]
		[batch 20/20] avg loss: 0.5542843041313464		[learning rate: 0.0095975]
	Learning Rate: 0.00959749
	LOSS [training: 0.6076147906288555 | validation: 0.5151928092046454]
	TIME [epoch: 8.84 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4575094236420395		[learning rate: 0.0095859]
		[batch 20/20] avg loss: 0.6345777479096283		[learning rate: 0.0095743]
	Learning Rate: 0.00957426
	LOSS [training: 0.5460435857758339 | validation: 0.4043879755790487]
	TIME [epoch: 8.85 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.604266956999723		[learning rate: 0.0095627]
		[batch 20/20] avg loss: 0.5680475053489981		[learning rate: 0.0095511]
	Learning Rate: 0.00955108
	LOSS [training: 0.5861572311743604 | validation: 0.9077258851071448]
	TIME [epoch: 8.84 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7207944792291527		[learning rate: 0.0095395]
		[batch 20/20] avg loss: 0.6067366114606297		[learning rate: 0.009528]
	Learning Rate: 0.00952796
	LOSS [training: 0.6637655453448914 | validation: 0.8177623138572425]
	TIME [epoch: 8.85 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6923246203576925		[learning rate: 0.0095164]
		[batch 20/20] avg loss: 0.5352902235856127		[learning rate: 0.0095049]
	Learning Rate: 0.0095049
	LOSS [training: 0.6138074219716526 | validation: 0.6698875394416468]
	TIME [epoch: 8.84 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5577088132936558		[learning rate: 0.0094934]
		[batch 20/20] avg loss: 0.6587766440658906		[learning rate: 0.0094819]
	Learning Rate: 0.00948189
	LOSS [training: 0.6082427286797734 | validation: 0.29414959878631053]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.504631596953115		[learning rate: 0.0094704]
		[batch 20/20] avg loss: 0.6534445514659184		[learning rate: 0.0094589]
	Learning Rate: 0.00945893
	LOSS [training: 0.5790380742095169 | validation: 0.6609933820360412]
	TIME [epoch: 8.84 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6779827464993827		[learning rate: 0.0094475]
		[batch 20/20] avg loss: 0.685091685947474		[learning rate: 0.009436]
	Learning Rate: 0.00943603
	LOSS [training: 0.6815372162234283 | validation: 0.3069861068304326]
	TIME [epoch: 8.83 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7846958511326133		[learning rate: 0.0094246]
		[batch 20/20] avg loss: 0.5497334719571916		[learning rate: 0.0094132]
	Learning Rate: 0.00941319
	LOSS [training: 0.6672146615449023 | validation: 0.31746553566912517]
	TIME [epoch: 8.85 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4194779062760482		[learning rate: 0.0094018]
		[batch 20/20] avg loss: 0.6115551214284547		[learning rate: 0.0093904]
	Learning Rate: 0.0093904
	LOSS [training: 0.5155165138522514 | validation: 0.9731685767094211]
	TIME [epoch: 8.86 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5315152415113694		[learning rate: 0.009379]
		[batch 20/20] avg loss: 0.7322020412606964		[learning rate: 0.0093677]
	Learning Rate: 0.00936767
	LOSS [training: 0.6318586413860329 | validation: 0.6220565401451853]
	TIME [epoch: 8.84 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.565632457878017		[learning rate: 0.0093563]
		[batch 20/20] avg loss: 0.542083723241866		[learning rate: 0.009345]
	Learning Rate: 0.00934499
	LOSS [training: 0.5538580905599415 | validation: 0.623294297840459]
	TIME [epoch: 8.83 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6486601408102758		[learning rate: 0.0093337]
		[batch 20/20] avg loss: 0.5939456393576237		[learning rate: 0.0093224]
	Learning Rate: 0.00932237
	LOSS [training: 0.6213028900839499 | validation: 0.6973556478261537]
	TIME [epoch: 8.84 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5206858880246262		[learning rate: 0.0093111]
		[batch 20/20] avg loss: 0.758079289336809		[learning rate: 0.0092998]
	Learning Rate: 0.0092998
	LOSS [training: 0.6393825886807176 | validation: 0.40853110154551064]
	TIME [epoch: 8.86 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9280173275685517		[learning rate: 0.0092885]
		[batch 20/20] avg loss: 0.6500490583412121		[learning rate: 0.0092773]
	Learning Rate: 0.00927729
	LOSS [training: 0.789033192954882 | validation: 0.7253225957155311]
	TIME [epoch: 8.84 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6678443962454707		[learning rate: 0.0092661]
		[batch 20/20] avg loss: 0.6419173762544658		[learning rate: 0.0092548]
	Learning Rate: 0.00925483
	LOSS [training: 0.6548808862499682 | validation: 0.7288647863437135]
	TIME [epoch: 8.85 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6514364148343074		[learning rate: 0.0092436]
		[batch 20/20] avg loss: 0.7539030935923028		[learning rate: 0.0092324]
	Learning Rate: 0.00923242
	LOSS [training: 0.702669754213305 | validation: 0.44755043919920373]
	TIME [epoch: 8.84 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49839150707183777		[learning rate: 0.0092212]
		[batch 20/20] avg loss: 0.5590252594815219		[learning rate: 0.0092101]
	Learning Rate: 0.00921007
	LOSS [training: 0.5287083832766798 | validation: 0.8513218114806506]
	TIME [epoch: 8.85 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6429311429349858		[learning rate: 0.0091989]
		[batch 20/20] avg loss: 0.5760584105004758		[learning rate: 0.0091878]
	Learning Rate: 0.00918778
	LOSS [training: 0.6094947767177308 | validation: 0.45124142479933094]
	TIME [epoch: 8.85 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.615714601094245		[learning rate: 0.0091767]
		[batch 20/20] avg loss: 0.5830966425788651		[learning rate: 0.0091655]
	Learning Rate: 0.00916554
	LOSS [training: 0.599405621836555 | validation: 0.8932846886524237]
	TIME [epoch: 8.84 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.745213710647662		[learning rate: 0.0091544]
		[batch 20/20] avg loss: 0.6000065343401081		[learning rate: 0.0091433]
	Learning Rate: 0.00914335
	LOSS [training: 0.6726101224938851 | validation: 0.6336346506990829]
	TIME [epoch: 8.84 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6375524334848894		[learning rate: 0.0091323]
		[batch 20/20] avg loss: 0.5574580368080807		[learning rate: 0.0091212]
	Learning Rate: 0.00912121
	LOSS [training: 0.597505235146485 | validation: 0.3325020192829366]
	TIME [epoch: 8.84 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5956189016116674		[learning rate: 0.0091102]
		[batch 20/20] avg loss: 0.5361912900399407		[learning rate: 0.0090991]
	Learning Rate: 0.00909913
	LOSS [training: 0.5659050958258041 | validation: 0.4537502575019364]
	TIME [epoch: 8.87 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.646759838656022		[learning rate: 0.0090881]
		[batch 20/20] avg loss: 0.7249186891556736		[learning rate: 0.0090771]
	Learning Rate: 0.0090771
	LOSS [training: 0.6858392639058479 | validation: 1.1136557592173926]
	TIME [epoch: 8.84 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5788848307634629		[learning rate: 0.0090661]
		[batch 20/20] avg loss: 0.501371841583241		[learning rate: 0.0090551]
	Learning Rate: 0.00905513
	LOSS [training: 0.540128336173352 | validation: 0.8525527987668423]
	TIME [epoch: 8.84 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6068393999721678		[learning rate: 0.0090442]
		[batch 20/20] avg loss: 0.4989538462801759		[learning rate: 0.0090332]
	Learning Rate: 0.00903321
	LOSS [training: 0.5528966231261719 | validation: 0.43381912793197086]
	TIME [epoch: 8.84 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5105106899856308		[learning rate: 0.0090223]
		[batch 20/20] avg loss: 0.5459216404375536		[learning rate: 0.0090113]
	Learning Rate: 0.00901134
	LOSS [training: 0.5282161652115922 | validation: 0.49921164174980837]
	TIME [epoch: 8.86 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.573244591304338		[learning rate: 0.0090004]
		[batch 20/20] avg loss: 0.5518719887391483		[learning rate: 0.0089895]
	Learning Rate: 0.00898953
	LOSS [training: 0.5625582900217432 | validation: 0.8866316417515542]
	TIME [epoch: 8.85 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5821486444423928		[learning rate: 0.0089786]
		[batch 20/20] avg loss: 0.5352999879272691		[learning rate: 0.0089678]
	Learning Rate: 0.00896776
	LOSS [training: 0.558724316184831 | validation: 0.3445460617158062]
	TIME [epoch: 8.84 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6045174057006509		[learning rate: 0.0089569]
		[batch 20/20] avg loss: 0.6386437539403537		[learning rate: 0.0089461]
	Learning Rate: 0.00894605
	LOSS [training: 0.6215805798205022 | validation: 0.33467914795570386]
	TIME [epoch: 8.84 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6466081327424107		[learning rate: 0.0089352]
		[batch 20/20] avg loss: 0.527965619974808		[learning rate: 0.0089244]
	Learning Rate: 0.0089244
	LOSS [training: 0.5872868763586094 | validation: 0.28747778640782834]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6799584176415328		[learning rate: 0.0089136]
		[batch 20/20] avg loss: 0.583530635331082		[learning rate: 0.0089028]
	Learning Rate: 0.00890279
	LOSS [training: 0.6317445264863075 | validation: 0.9090604084497297]
	TIME [epoch: 8.86 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4725027264615701		[learning rate: 0.008892]
		[batch 20/20] avg loss: 0.5983247435186758		[learning rate: 0.0088812]
	Learning Rate: 0.00888124
	LOSS [training: 0.5354137349901229 | validation: 0.42983936492793595]
	TIME [epoch: 8.84 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7508430164580949		[learning rate: 0.0088705]
		[batch 20/20] avg loss: 0.4985883018376242		[learning rate: 0.0088597]
	Learning Rate: 0.00885974
	LOSS [training: 0.6247156591478595 | validation: 0.7167664082332381]
	TIME [epoch: 8.83 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4446681609559838		[learning rate: 0.008849]
		[batch 20/20] avg loss: 0.5856984315373754		[learning rate: 0.0088383]
	Learning Rate: 0.00883829
	LOSS [training: 0.5151832962466797 | validation: 0.6603338800264086]
	TIME [epoch: 8.85 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5310579240425645		[learning rate: 0.0088276]
		[batch 20/20] avg loss: 0.5469416202576932		[learning rate: 0.0088169]
	Learning Rate: 0.0088169
	LOSS [training: 0.5389997721501288 | validation: 0.4575043799610996]
	TIME [epoch: 8.86 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4486438698464806		[learning rate: 0.0088062]
		[batch 20/20] avg loss: 0.5144255734341978		[learning rate: 0.0087956]
	Learning Rate: 0.00879555
	LOSS [training: 0.4815347216403392 | validation: 0.6164476779369887]
	TIME [epoch: 8.84 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6234993904468933		[learning rate: 0.0087849]
		[batch 20/20] avg loss: 0.5191872669791439		[learning rate: 0.0087743]
	Learning Rate: 0.00877426
	LOSS [training: 0.5713433287130186 | validation: 0.30215892906402]
	TIME [epoch: 8.85 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4746903102607593		[learning rate: 0.0087636]
		[batch 20/20] avg loss: 0.5228686036558707		[learning rate: 0.008753]
	Learning Rate: 0.00875302
	LOSS [training: 0.49877945695831494 | validation: 0.5174875542094411]
	TIME [epoch: 8.83 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4518087322828964		[learning rate: 0.0087424]
		[batch 20/20] avg loss: 0.4810457692070799		[learning rate: 0.0087318]
	Learning Rate: 0.00873183
	LOSS [training: 0.46642725074498814 | validation: 0.6188398499160765]
	TIME [epoch: 8.85 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5522207720527963		[learning rate: 0.0087213]
		[batch 20/20] avg loss: 0.5586897134258999		[learning rate: 0.0087107]
	Learning Rate: 0.00871069
	LOSS [training: 0.5554552427393482 | validation: 0.36461390329874366]
	TIME [epoch: 8.84 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.515394621708049		[learning rate: 0.0087001]
		[batch 20/20] avg loss: 0.6148944204655744		[learning rate: 0.0086896]
	Learning Rate: 0.0086896
	LOSS [training: 0.5651445210868116 | validation: 0.2765554426629895]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6037192940319345		[learning rate: 0.0086791]
		[batch 20/20] avg loss: 0.511322845552866		[learning rate: 0.0086686]
	Learning Rate: 0.00866857
	LOSS [training: 0.5575210697924001 | validation: 0.28913131712409695]
	TIME [epoch: 8.84 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5182770075136662		[learning rate: 0.0086581]
		[batch 20/20] avg loss: 0.5399484069720071		[learning rate: 0.0086476]
	Learning Rate: 0.00864758
	LOSS [training: 0.5291127072428365 | validation: 0.5419062799931117]
	TIME [epoch: 8.86 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4652324305782072		[learning rate: 0.0086371]
		[batch 20/20] avg loss: 0.6714393950986455		[learning rate: 0.0086266]
	Learning Rate: 0.00862665
	LOSS [training: 0.5683359128384263 | validation: 0.9822873992371356]
	TIME [epoch: 8.84 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5711997320516409		[learning rate: 0.0086162]
		[batch 20/20] avg loss: 0.4688274515763037		[learning rate: 0.0086058]
	Learning Rate: 0.00860576
	LOSS [training: 0.5200135918139722 | validation: 0.5870469184336286]
	TIME [epoch: 8.83 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7562148957047521		[learning rate: 0.0085953]
		[batch 20/20] avg loss: 0.5533176478613931		[learning rate: 0.0085849]
	Learning Rate: 0.00858493
	LOSS [training: 0.6547662717830727 | validation: 0.5230669827259696]
	TIME [epoch: 8.83 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41274023189844894		[learning rate: 0.0085745]
		[batch 20/20] avg loss: 0.48732214782089683		[learning rate: 0.0085641]
	Learning Rate: 0.00856415
	LOSS [training: 0.45003118985967305 | validation: 0.227584807428639]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240219_183142/states/model_tr_study1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44882369191812826		[learning rate: 0.0085538]
		[batch 20/20] avg loss: 0.4906111799987893		[learning rate: 0.0085434]
	Learning Rate: 0.00854342
	LOSS [training: 0.4697174359584587 | validation: 0.3031448571405846]
	TIME [epoch: 8.85 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6336863805906352		[learning rate: 0.0085331]
		[batch 20/20] avg loss: 0.5401201572738447		[learning rate: 0.0085227]
	Learning Rate: 0.00852273
	LOSS [training: 0.5869032689322399 | validation: 0.33969196386897077]
	TIME [epoch: 8.83 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49363355431317135		[learning rate: 0.0085124]
		[batch 20/20] avg loss: 0.5491036361370961		[learning rate: 0.0085021]
	Learning Rate: 0.0085021
	LOSS [training: 0.5213685952251337 | validation: 0.2925564140768029]
	TIME [epoch: 8.84 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49627121883720127		[learning rate: 0.0084918]
		[batch 20/20] avg loss: 0.5669617287901152		[learning rate: 0.0084815]
	Learning Rate: 0.00848152
	LOSS [training: 0.5316164738136583 | validation: 0.42098765976743724]
	TIME [epoch: 8.84 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5206697959557174		[learning rate: 0.0084712]
		[batch 20/20] avg loss: 0.4457898299766055		[learning rate: 0.008461]
	Learning Rate: 0.00846099
	LOSS [training: 0.48322981296616146 | validation: 0.4063239664043769]
	TIME [epoch: 8.86 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43471849869531587		[learning rate: 0.0084507]
		[batch 20/20] avg loss: 0.5775409768299059		[learning rate: 0.0084405]
	Learning Rate: 0.0084405
	LOSS [training: 0.5061297377626108 | validation: 0.5767956433391348]
	TIME [epoch: 8.83 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4101474806458782		[learning rate: 0.0084303]
		[batch 20/20] avg loss: 0.4953582573487078		[learning rate: 0.0084201]
	Learning Rate: 0.00842007
	LOSS [training: 0.45275286899729306 | validation: 0.6552067094735962]
	TIME [epoch: 8.83 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4883692376182679		[learning rate: 0.0084099]
		[batch 20/20] avg loss: 0.3573313266757431		[learning rate: 0.0083997]
	Learning Rate: 0.00839969
	LOSS [training: 0.42285028214700543 | validation: 0.43468353735707643]
	TIME [epoch: 8.83 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.537253621734932		[learning rate: 0.0083895]
		[batch 20/20] avg loss: 0.5194770952492893		[learning rate: 0.0083794]
	Learning Rate: 0.00837935
	LOSS [training: 0.5283653584921106 | validation: 0.3359045611823567]
	TIME [epoch: 8.86 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33436119849132495		[learning rate: 0.0083692]
		[batch 20/20] avg loss: 0.6012238268629629		[learning rate: 0.0083591]
	Learning Rate: 0.00835907
	LOSS [training: 0.46779251267714395 | validation: 0.288667709812657]
	TIME [epoch: 8.84 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4804484565731487		[learning rate: 0.0083489]
		[batch 20/20] avg loss: 0.5063557974067497		[learning rate: 0.0083388]
	Learning Rate: 0.00833883
	LOSS [training: 0.4934021269899492 | validation: 0.8540241166335132]
	TIME [epoch: 8.84 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5078495830571517		[learning rate: 0.0083287]
		[batch 20/20] avg loss: 0.46166422209012997		[learning rate: 0.0083186]
	Learning Rate: 0.00831864
	LOSS [training: 0.48475690257364085 | validation: 0.33013583734345814]
	TIME [epoch: 8.83 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6971533855826719		[learning rate: 0.0083086]
		[batch 20/20] avg loss: 0.6801061076626761		[learning rate: 0.0082985]
	Learning Rate: 0.00829851
	LOSS [training: 0.688629746622674 | validation: 0.5057340752802036]
	TIME [epoch: 8.86 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4304916027263709		[learning rate: 0.0082885]
		[batch 20/20] avg loss: 0.6691801210456431		[learning rate: 0.0082784]
	Learning Rate: 0.00827842
	LOSS [training: 0.549835861886007 | validation: 0.83514883280877]
	TIME [epoch: 8.84 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5402840654127649		[learning rate: 0.0082684]
		[batch 20/20] avg loss: 0.47417056159544335		[learning rate: 0.0082584]
	Learning Rate: 0.00825838
	LOSS [training: 0.5072273135041042 | validation: 0.9384088714320511]
	TIME [epoch: 8.83 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5524626732610594		[learning rate: 0.0082484]
		[batch 20/20] avg loss: 0.512229903137355		[learning rate: 0.0082384]
	Learning Rate: 0.00823839
	LOSS [training: 0.5323462881992072 | validation: 0.5231428899914418]
	TIME [epoch: 8.83 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42206862803231554		[learning rate: 0.0082284]
		[batch 20/20] avg loss: 0.545606564059684		[learning rate: 0.0082184]
	Learning Rate: 0.00821844
	LOSS [training: 0.48383759604599985 | validation: 0.41160491983757774]
	TIME [epoch: 8.85 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5055171847799861		[learning rate: 0.0082085]
		[batch 20/20] avg loss: 0.4975148127902079		[learning rate: 0.0081985]
	Learning Rate: 0.00819855
	LOSS [training: 0.501515998785097 | validation: 0.7277831805933401]
	TIME [epoch: 8.84 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4893870900584435		[learning rate: 0.0081886]
		[batch 20/20] avg loss: 0.4863539141707288		[learning rate: 0.0081787]
	Learning Rate: 0.0081787
	LOSS [training: 0.4878705021145862 | validation: 0.4367934965315642]
	TIME [epoch: 8.83 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5359159082188802		[learning rate: 0.0081688]
		[batch 20/20] avg loss: 0.40370979307131816		[learning rate: 0.0081589]
	Learning Rate: 0.0081589
	LOSS [training: 0.4698128506450992 | validation: 0.7380592682740609]
	TIME [epoch: 8.83 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8597523532508806		[learning rate: 0.008149]
		[batch 20/20] avg loss: 5.084211657770144		[learning rate: 0.0081391]
	Learning Rate: 0.00813915
	LOSS [training: 3.4719820055105126 | validation: 5.772941300087364]
	TIME [epoch: 8.83 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.067265672524903		[learning rate: 0.0081293]
		[batch 20/20] avg loss: 8.08037310838998		[learning rate: 0.0081194]
	Learning Rate: 0.00811944
	LOSS [training: 7.07381939045744 | validation: 8.277219844651157]
	TIME [epoch: 8.86 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9547237195082925		[learning rate: 0.0081096]
		[batch 20/20] avg loss: 7.5129990430526545		[learning rate: 0.0080998]
	Learning Rate: 0.00809979
	LOSS [training: 7.733861381280475 | validation: 7.85633463433456]
	TIME [epoch: 8.83 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.293906773191134		[learning rate: 0.00809]
		[batch 20/20] avg loss: 4.016999587474506		[learning rate: 0.0080802]
	Learning Rate: 0.00808018
	LOSS [training: 5.655453180332819 | validation: 1.6820326375090686]
	TIME [epoch: 8.84 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9782134105466675		[learning rate: 0.0080704]
		[batch 20/20] avg loss: 1.6391314701554252		[learning rate: 0.0080606]
	Learning Rate: 0.00806062
	LOSS [training: 1.8086724403510466 | validation: 1.8747970505063847]
	TIME [epoch: 8.83 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5666596481037198		[learning rate: 0.0080509]
		[batch 20/20] avg loss: 1.355568181357787		[learning rate: 0.0080411]
	Learning Rate: 0.00804111
	LOSS [training: 1.4611139147307532 | validation: 1.1785527138900425]
	TIME [epoch: 8.86 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6413933030271035		[learning rate: 0.0080314]
		[batch 20/20] avg loss: 1.6795371670465493		[learning rate: 0.0080216]
	Learning Rate: 0.00802164
	LOSS [training: 1.6604652350368265 | validation: 1.1863936321617166]
	TIME [epoch: 8.84 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.355618389874047		[learning rate: 0.0080119]
		[batch 20/20] avg loss: 1.3794915155428866		[learning rate: 0.0080022]
	Learning Rate: 0.00800222
	LOSS [training: 1.3675549527084667 | validation: 0.970627345195329]
	TIME [epoch: 8.84 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.070060823347681		[learning rate: 0.0079925]
		[batch 20/20] avg loss: 1.0592388149636593		[learning rate: 0.0079828]
	Learning Rate: 0.00798285
	LOSS [training: 1.0646498191556704 | validation: 0.9214783763108354]
	TIME [epoch: 8.84 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0169732137135956		[learning rate: 0.0079732]
		[batch 20/20] avg loss: 1.0483427499818139		[learning rate: 0.0079635]
	Learning Rate: 0.00796352
	LOSS [training: 1.0326579818477049 | validation: 1.0043421982568947]
	TIME [epoch: 8.85 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0433258610594884		[learning rate: 0.0079539]
		[batch 20/20] avg loss: 0.9507671489389331		[learning rate: 0.0079442]
	Learning Rate: 0.00794424
	LOSS [training: 0.9970465049992108 | validation: 1.1599776858665587]
	TIME [epoch: 8.85 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2626520805549135		[learning rate: 0.0079346]
		[batch 20/20] avg loss: 1.024373918133673		[learning rate: 0.007925]
	Learning Rate: 0.00792501
	LOSS [training: 1.1435129993442934 | validation: 0.8415304480415977]
	TIME [epoch: 8.84 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0562826500117237		[learning rate: 0.0079154]
		[batch 20/20] avg loss: 0.9988462966877899		[learning rate: 0.0079058]
	Learning Rate: 0.00790583
	LOSS [training: 1.0275644733497566 | validation: 0.8062582749961713]
	TIME [epoch: 8.84 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9683883026298986		[learning rate: 0.0078963]
		[batch 20/20] avg loss: 0.971520764648532		[learning rate: 0.0078867]
	Learning Rate: 0.00788669
	LOSS [training: 0.9699545336392154 | validation: 2.043181030058157]
	TIME [epoch: 8.85 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1635325696571839		[learning rate: 0.0078771]
		[batch 20/20] avg loss: 0.7955367555380414		[learning rate: 0.0078676]
	Learning Rate: 0.0078676
	LOSS [training: 0.9795346625976125 | validation: 0.6453492759234061]
	TIME [epoch: 8.85 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7625559283214144		[learning rate: 0.0078581]
		[batch 20/20] avg loss: 0.9915411391907792		[learning rate: 0.0078486]
	Learning Rate: 0.00784855
	LOSS [training: 0.8770485337560968 | validation: 0.8830369734105177]
	TIME [epoch: 8.84 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7567759609606663		[learning rate: 0.007839]
		[batch 20/20] avg loss: 0.8193367885898895		[learning rate: 0.0078296]
	Learning Rate: 0.00782955
	LOSS [training: 0.7880563747752778 | validation: 1.7351905755179327]
	TIME [epoch: 8.84 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0864084333347124		[learning rate: 0.0078201]
		[batch 20/20] avg loss: 0.7351064772969091		[learning rate: 0.0078106]
	Learning Rate: 0.0078106
	LOSS [training: 0.9107574553158109 | validation: 0.8336729170210873]
	TIME [epoch: 8.83 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9329912759089061		[learning rate: 0.0078011]
		[batch 20/20] avg loss: 1.0003995265743661		[learning rate: 0.0077917]
	Learning Rate: 0.00779169
	LOSS [training: 0.966695401241636 | validation: 0.7463242653220467]
	TIME [epoch: 8.86 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7042904036946526		[learning rate: 0.0077823]
		[batch 20/20] avg loss: 0.8078358216857474		[learning rate: 0.0077728]
	Learning Rate: 0.00777283
	LOSS [training: 0.7560631126902001 | validation: 0.7446834333635319]
	TIME [epoch: 8.84 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.856156080636338		[learning rate: 0.0077634]
		[batch 20/20] avg loss: 0.6433260064704728		[learning rate: 0.007754]
	Learning Rate: 0.00775401
	LOSS [training: 0.7497410435534054 | validation: 0.4561803978680007]
	TIME [epoch: 8.84 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.735593613169872		[learning rate: 0.0077446]
		[batch 20/20] avg loss: 0.7947118664266337		[learning rate: 0.0077352]
	Learning Rate: 0.00773524
	LOSS [training: 0.7651527397982528 | validation: 0.7860526655685625]
	TIME [epoch: 8.83 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7468611865819159		[learning rate: 0.0077259]
		[batch 20/20] avg loss: 0.8299458933150723		[learning rate: 0.0077165]
	Learning Rate: 0.00771651
	LOSS [training: 0.788403539948494 | validation: 0.599477056037466]
	TIME [epoch: 8.86 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8336013966880957		[learning rate: 0.0077072]
		[batch 20/20] avg loss: 0.7025999045252572		[learning rate: 0.0076978]
	Learning Rate: 0.00769783
	LOSS [training: 0.7681006506066763 | validation: 0.6189247538729072]
	TIME [epoch: 8.84 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.664235387396249		[learning rate: 0.0076885]
		[batch 20/20] avg loss: 0.6825454035443307		[learning rate: 0.0076792]
	Learning Rate: 0.0076792
	LOSS [training: 0.6733903954702898 | validation: 0.5441248992001919]
	TIME [epoch: 8.84 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7463811271113641		[learning rate: 0.0076699]
		[batch 20/20] avg loss: 0.8908144486306575		[learning rate: 0.0076606]
	Learning Rate: 0.00766061
	LOSS [training: 0.8185977878710109 | validation: 0.6801817652050903]
	TIME [epoch: 8.84 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7747949801562973		[learning rate: 0.0076513]
		[batch 20/20] avg loss: 0.838766784685687		[learning rate: 0.0076421]
	Learning Rate: 0.00764206
	LOSS [training: 0.8067808824209923 | validation: 0.8396521705523111]
	TIME [epoch: 8.85 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6204944317379074		[learning rate: 0.0076328]
		[batch 20/20] avg loss: 0.6997032025276881		[learning rate: 0.0076236]
	Learning Rate: 0.00762356
	LOSS [training: 0.6600988171327977 | validation: 0.79363071490841]
	TIME [epoch: 8.85 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7231597674731178		[learning rate: 0.0076143]
		[batch 20/20] avg loss: 0.7142174769394499		[learning rate: 0.0076051]
	Learning Rate: 0.00760511
	LOSS [training: 0.7186886222062838 | validation: 0.6078962899778488]
	TIME [epoch: 8.84 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7070814390593048		[learning rate: 0.0075959]
		[batch 20/20] avg loss: 0.7267945018271488		[learning rate: 0.0075867]
	Learning Rate: 0.00758669
	LOSS [training: 0.7169379704432266 | validation: 0.5195929729274285]
	TIME [epoch: 8.83 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6873040868210627		[learning rate: 0.0075775]
		[batch 20/20] avg loss: 0.7094749665570186		[learning rate: 0.0075683]
	Learning Rate: 0.00756833
	LOSS [training: 0.6983895266890408 | validation: 0.6432419720872923]
	TIME [epoch: 8.85 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4380990998469363		[learning rate: 0.0075592]
		[batch 20/20] avg loss: 3.5898577638471387		[learning rate: 0.00755]
	Learning Rate: 0.00755001
	LOSS [training: 3.013978431847037 | validation: 2.8803005802275754]
	TIME [epoch: 8.85 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.754898039292704		[learning rate: 0.0075409]
		[batch 20/20] avg loss: 5.218738462064396		[learning rate: 0.0075317]
	Learning Rate: 0.00753173
	LOSS [training: 3.9868182506785494 | validation: 3.792624267199019]
	TIME [epoch: 8.84 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.770800935309429		[learning rate: 0.0075226]
		[batch 20/20] avg loss: 6.051423001722938		[learning rate: 0.0075135]
	Learning Rate: 0.0075135
	LOSS [training: 4.911111968516184 | validation: 3.9443701575377688]
	TIME [epoch: 8.84 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9642384925838146		[learning rate: 0.0075044]
		[batch 20/20] avg loss: 4.168549903483453		[learning rate: 0.0074953]
	Learning Rate: 0.00749531
	LOSS [training: 4.066394198033635 | validation: 3.6676623880621038]
	TIME [epoch: 8.83 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.138908405104008		[learning rate: 0.0074862]
		[batch 20/20] avg loss: 4.220773465669875		[learning rate: 0.0074772]
	Learning Rate: 0.00747716
	LOSS [training: 4.179840935386943 | validation: 4.1396632392150625]
	TIME [epoch: 8.86 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.671621597173249		[learning rate: 0.0074681]
		[batch 20/20] avg loss: 3.905377011671599		[learning rate: 0.0074591]
	Learning Rate: 0.00745906
	LOSS [training: 4.288499304422424 | validation: 3.6422793978009587]
	TIME [epoch: 8.83 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.1612182462092075		[learning rate: 0.00745]
		[batch 20/20] avg loss: 4.1308696943917464		[learning rate: 0.007441]
	Learning Rate: 0.007441
	LOSS [training: 4.146043970300478 | validation: 3.9957041072224175]
	TIME [epoch: 8.83 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.076538974102213		[learning rate: 0.007432]
		[batch 20/20] avg loss: 4.189446235206033		[learning rate: 0.007423]
	Learning Rate: 0.00742299
	LOSS [training: 4.132992604654122 | validation: 3.6263911011110403]
	TIME [epoch: 8.84 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9600572833366536		[learning rate: 0.007414]
		[batch 20/20] avg loss: 4.406906604648766		[learning rate: 0.007405]
ERROR:
nan encountered in epoch 223 (validation loss).
