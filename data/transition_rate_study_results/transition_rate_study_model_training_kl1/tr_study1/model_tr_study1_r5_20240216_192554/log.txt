Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r5', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 39557939

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.858666811790654		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.666342824986021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.262504818388338 | validation: 8.7995701139404]
	TIME [epoch: 48.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.057174747025964		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.283375390564949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.6702750687954575 | validation: 7.5568020721262]
	TIME [epoch: 9.07 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.153706334141056		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.906805091518088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.0302557128295735 | validation: 6.485321251715018]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.626797527750921		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.839523791105323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.733160659428121 | validation: 6.214612237794426]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.231192505285234		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.469741463127764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.350466984206499 | validation: 6.504781772045819]
	TIME [epoch: 9.05 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.279795629316483		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.892750065404602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.086272847360542 | validation: 5.67409786218692]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.30268977651787		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.228740276380152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.76571502644901 | validation: 5.174144185118736]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.536787148531565		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.311102816791336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.423944982661451 | validation: 5.707536759703789]
	TIME [epoch: 9.06 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.4472047667121215		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.7398945864141195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0935496765631205 | validation: 5.170773089545763]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.077938186287073		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.194376043490618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.636157114888847 | validation: 4.0020664113263305]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.168801483432368		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.61776439205036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.893282937741364 | validation: 3.478696240517142]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.813491725940427		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.372424978644973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5929583522926998 | validation: 2.7641734357504113]
	TIME [epoch: 9.07 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9743784879297896		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.915925939365123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9451522136474564 | validation: 2.6403465007389224]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.717009531313365		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.392652463834616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.55483099757399 | validation: 2.5461408512082597]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4125640968094286		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.107949222054372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2602566594319002 | validation: 2.0350837972578057]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.233737951175411		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2705998778535994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.252168914514505 | validation: 1.8827969720584048]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.996386458638849		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.890013143901733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9431998012702905 | validation: 2.2883820042812206]
	TIME [epoch: 9.08 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0537642983753566		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7469530229894659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9003586606824112 | validation: 1.552484309309251]
	TIME [epoch: 9.07 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.131563997561641		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7040162007618587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9177900991617496 | validation: 1.6000375493942354]
	TIME [epoch: 9.08 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7913399912342485		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7047392554816216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7480396233579352 | validation: 1.5920283956004289]
	TIME [epoch: 9.07 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7053595313761343		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4884741000951087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5969168157356215 | validation: 1.493484059158246]
	TIME [epoch: 9.07 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.608190181411248		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.845898309207763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7270442453095054 | validation: 1.371330144475614]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.744478473964591		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6091938688882237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.676836171426407 | validation: 1.105226986995553]
	TIME [epoch: 9.07 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.508942990108018		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6950465399232946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6019947650156563 | validation: 1.3314944610679056]
	TIME [epoch: 9.05 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5394023929185643		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5341758534215058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5367891231700352 | validation: 1.3054357948195157]
	TIME [epoch: 9.04 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4730971992187938		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.470849773022897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4719734861208453 | validation: 1.3245026628148413]
	TIME [epoch: 9.05 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.506499224563441		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4608478539399712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4836735392517064 | validation: 1.2111570338446747]
	TIME [epoch: 9.07 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3910176874543814		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7571484232387864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5740830553465837 | validation: 1.4765074677719778]
	TIME [epoch: 9.06 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3349922497695743		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4313765239899625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3831843868797686 | validation: 1.453370210832987]
	TIME [epoch: 9.07 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.327447651484585		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.390706888174933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3590772698297588 | validation: 1.2428141057320607]
	TIME [epoch: 9.07 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.434822558266373		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.348566941613752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3916947499400625 | validation: 1.2480382460553892]
	TIME [epoch: 9.07 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2593926076770543		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2899595924334617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.274676100055258 | validation: 1.423093518375622]
	TIME [epoch: 9.09 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2977902091980291		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4047819208379182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3512860650179734 | validation: 1.266874347327632]
	TIME [epoch: 9.07 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3626456061903127		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1942073054501736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2784264558202434 | validation: 1.0807652375434702]
	TIME [epoch: 9.07 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5022613784884897		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2316677027673975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3669645406279436 | validation: 1.2785008216166625]
	TIME [epoch: 9.06 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1836014371498935		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.507510734373224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3455560857615587 | validation: 1.1532063299061621]
	TIME [epoch: 9.05 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2398419815173478		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.12786170190562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1838518417114838 | validation: 0.9950173398152612]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.350539188125318		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1998925687359019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.27521587843061 | validation: 0.9359534905059875]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2336575836684465		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3446835738846954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.289170578776571 | validation: 1.3687244611567273]
	TIME [epoch: 9.06 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3707930716641383		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2822608968852727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3265269842747056 | validation: 1.0333705317225097]
	TIME [epoch: 9.06 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1841702771372575		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3084755627048028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2463229199210302 | validation: 1.283858221368856]
	TIME [epoch: 9.05 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.233576725881167		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0957064378144186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1646415818477927 | validation: 0.8334886721806919]
	TIME [epoch: 9.09 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3201773935283234		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0955647746485615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2078710840884423 | validation: 0.9941630913918768]
	TIME [epoch: 9.05 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2957587484817306		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8422202592195416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5689895038506358 | validation: 1.310585504989596]
	TIME [epoch: 9.06 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2464632905570843		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.114754624637579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1806089575973315 | validation: 0.9544878545227605]
	TIME [epoch: 9.06 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2231202766357387		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2138468802654223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2184835784505805 | validation: 1.4663737876693617]
	TIME [epoch: 9.05 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2260957332971236		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4219844401010164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3240400866990698 | validation: 1.1469109671280548]
	TIME [epoch: 9.07 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2918648530291343		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1808235963327658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2363442246809497 | validation: 0.8851646401320713]
	TIME [epoch: 9.05 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2825483092076384		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1190264408939312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.200787375050785 | validation: 1.0427827261328602]
	TIME [epoch: 9.05 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6078151079736736		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.080961537670331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3443883228220022 | validation: 0.9928468269468491]
	TIME [epoch: 9.05 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1939955192766525		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1634011982432955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.178698358759974 | validation: 1.0470124258876938]
	TIME [epoch: 9.06 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1450056225529128		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.09827359385571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1216396082043114 | validation: 1.065084063433166]
	TIME [epoch: 9.07 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3601488807065327		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1553482005155638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2577485406110485 | validation: 0.8787977844269048]
	TIME [epoch: 9.06 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9394937876949088		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3029250525722251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.121209420133567 | validation: 1.1209409931888423]
	TIME [epoch: 9.06 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1774798233115766		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1025908038582077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.140035313584892 | validation: 1.009561704761453]
	TIME [epoch: 9.04 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0720665406405114		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1852448567597567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.128655698700134 | validation: 1.0683972838439053]
	TIME [epoch: 9.06 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.171913728936713		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0248281263124575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0983709276245852 | validation: 1.2942806450544326]
	TIME [epoch: 9.13 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1857186930811656		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1924282477006387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.189073470390902 | validation: 0.9339969608749947]
	TIME [epoch: 9.05 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0317822919166315		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2486939036837272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1402380978001792 | validation: 1.214815541227055]
	TIME [epoch: 9.06 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1202683484872442		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1156160654373943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1179422069623193 | validation: 0.9337047134905394]
	TIME [epoch: 9.06 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2374313595816235		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9636129575360102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.100522158558817 | validation: 1.226030973520025]
	TIME [epoch: 9.05 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1611754366517912		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0107586947046692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0859670656782303 | validation: 1.0593123044148594]
	TIME [epoch: 9.07 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9613747499286909		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9634173043209933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9623960271248417 | validation: 0.8013216556938784]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9508683831741095		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0075732038207668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9792207934974382 | validation: 0.6754618632309363]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.919348770696231		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.08106643165292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0002076011745755 | validation: 0.7189601692088277]
	TIME [epoch: 9.06 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8331206745398523		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9342989081808983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8837097913603754 | validation: 0.7962948827598008]
	TIME [epoch: 9.09 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9942723880483557		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9275783707729083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9609253794106323 | validation: 0.6973172874906038]
	TIME [epoch: 9.08 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8806699675680131		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1399666067461593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.010318287157086 | validation: 0.7480062306162559]
	TIME [epoch: 9.07 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9911564919203035		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9818578535554081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9865071727378556 | validation: 0.5944069415968563]
	TIME [epoch: 9.07 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8974831097634135		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8039705904421751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8507268501027943 | validation: 0.5152568851832187]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7001176845934161		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0999042636113316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9000109741023741 | validation: 1.0342444694284183]
	TIME [epoch: 9.07 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.281576577579449		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8416318364975476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0616042070384983 | validation: 0.6790579945392501]
	TIME [epoch: 9.07 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7966327161303436		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7450943090673654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7708635125988543 | validation: 0.6601671232505211]
	TIME [epoch: 9.06 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9363969865039523		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8079229814003996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.872159983952176 | validation: 0.42838111064304063]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2820736853438701		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7240684692521232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0030710772979967 | validation: 0.6292219299558053]
	TIME [epoch: 9.05 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7429529036870663		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8731664348250844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8080596692560753 | validation: 0.646245093486187]
	TIME [epoch: 9.05 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7104004066166858		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7838618251767138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7471311158966997 | validation: 1.2035332272560149]
	TIME [epoch: 9.08 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.884034147250625		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7888510845489953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8364426158998102 | validation: 1.1206793981834842]
	TIME [epoch: 9.06 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9451762808912006		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9960011561791232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9705887185351619 | validation: 0.5642151722460201]
	TIME [epoch: 9.06 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8540765181289702		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8039737651585688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8290251416437696 | validation: 0.569470588879353]
	TIME [epoch: 9.06 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6892886522374827		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8191928822561488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7542407672468158 | validation: 0.6118875950786618]
	TIME [epoch: 9.06 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6920623738261243		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7255390016900509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7088006877580877 | validation: 0.471511037879459]
	TIME [epoch: 9.09 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.820957471527642		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8292810974098233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8251192844687326 | validation: 0.7963266861146501]
	TIME [epoch: 9.06 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8383239451646387		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8407414357590388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8395326904618388 | validation: 0.3691725418907159]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7397478478803642		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9942626178114429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8670052328459036 | validation: 1.2289907904995483]
	TIME [epoch: 9.06 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7832868390699727		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0195704449246141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9014286419972933 | validation: 1.2081969678383793]
	TIME [epoch: 9.06 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8169930072423132		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7608920779630025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7889425426026577 | validation: 1.1324681931258058]
	TIME [epoch: 9.09 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8907702482339609		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7922703772235782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8415203127287694 | validation: 0.5085801968269775]
	TIME [epoch: 9.06 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9445403100440547		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9146054267442458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9295728683941503 | validation: 1.0885440328906473]
	TIME [epoch: 9.06 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9639033214695033		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8140871386258303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8889952300476667 | validation: 0.564938648480807]
	TIME [epoch: 9.44 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9651311274910921		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9372871134142716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9512091204526818 | validation: 0.5361785385102485]
	TIME [epoch: 9.06 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7706525318713571		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7789412677494318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7747968998103947 | validation: 0.5671905055921903]
	TIME [epoch: 9.09 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.740881328157333		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.075977852051745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.908429590104539 | validation: 0.7007851847470596]
	TIME [epoch: 9.05 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7849984463058528		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7191655441137534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.752081995209803 | validation: 0.7305395015577165]
	TIME [epoch: 9.05 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8045598824231469		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9594675148779214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8820136986505339 | validation: 0.6677369970136436]
	TIME [epoch: 9.06 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8054839139949566		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6716925815645328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7385882477797446 | validation: 1.020094168161136]
	TIME [epoch: 9.06 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7664067436195612		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6511826080409218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7087946758302415 | validation: 0.32293721154684096]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5879879598575799		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6191934096599534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6035906847587665 | validation: 0.3772817793493119]
	TIME [epoch: 9.06 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.679500048503416		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9482307663746585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8138654074390373 | validation: 0.7212524944737326]
	TIME [epoch: 9.05 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9781564869092906		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9538021470393963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9659793169743434 | validation: 1.099794840473087]
	TIME [epoch: 9.06 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.044811994625118		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8038768443835729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9243444195043453 | validation: 1.0363079542684586]
	TIME [epoch: 9.06 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7843937688379431		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7320018639158038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7581978163768734 | validation: 0.470273593537682]
	TIME [epoch: 9.09 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6823386347147912		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6922655812748868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6873021079948389 | validation: 0.3847522875223095]
	TIME [epoch: 9.06 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7284495346892271		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.710815530677416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7196325326833216 | validation: 0.8190110504885919]
	TIME [epoch: 9.07 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8812474460256892		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6364089207477329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7588281833867111 | validation: 0.4741169948845466]
	TIME [epoch: 9.06 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7290696526961596		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8247481884743288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7769089205852443 | validation: 0.6259275382616625]
	TIME [epoch: 9.05 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7585943775634869		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.846405722589818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8025000500766526 | validation: 0.6046759574634868]
	TIME [epoch: 9.09 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7850352360519717		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7090181264993765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7470266812756741 | validation: 1.0464064162006004]
	TIME [epoch: 9.08 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7388564913167455		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48357434239235986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6112154168545527 | validation: 3.146205857309078]
	TIME [epoch: 9.07 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0506408402174237		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5447331859521997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7976870130848117 | validation: 3.298599115884586]
	TIME [epoch: 9.06 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9441237011186034		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8258318184907031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8849777598046533 | validation: 0.593998643050734]
	TIME [epoch: 9.06 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5724870562755784		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6220589187788133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5972729875271957 | validation: 1.1114944735471626]
	TIME [epoch: 9.08 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6739221881675042		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5005202271183139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5872212076429091 | validation: 0.40609721455383907]
	TIME [epoch: 9.07 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.584984046020249		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8116521161352577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6983180810777533 | validation: 0.33337026202803655]
	TIME [epoch: 9.06 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6207382784103757		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7461156496721494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6834269640412625 | validation: 0.4492122975198752]
	TIME [epoch: 9.07 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6101787870296151		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6366722852363698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6234255361329925 | validation: 0.567946374438372]
	TIME [epoch: 9.05 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6332251491425187		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8203142566346264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7267697028885725 | validation: 0.6290861101820471]
	TIME [epoch: 9.08 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5737018320038911		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.901399077117752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7375504545608215 | validation: 0.4095365283569635]
	TIME [epoch: 9.07 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4736613392595637		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5363140943197633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5049877167896635 | validation: 0.6154715700067847]
	TIME [epoch: 9.06 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7345353279242257		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.582725008938587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6586301684314064 | validation: 0.7564498214541616]
	TIME [epoch: 9.06 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5787043455222667		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5410350786553861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5598697120888264 | validation: 0.5598408097676792]
	TIME [epoch: 9.06 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8753735868363366		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7381313229150719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8067524548757042 | validation: 1.1261852197750868]
	TIME [epoch: 9.08 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5954311707190706		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5846564706705905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5900438206948305 | validation: 0.6067637423847754]
	TIME [epoch: 9.06 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.545280653384264		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6128644330960458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5790725432401549 | validation: 0.47577612869759645]
	TIME [epoch: 9.12 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4551699458029903		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.654013059600441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5545915027017158 | validation: 0.5786004629802916]
	TIME [epoch: 9.05 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9012883552859339		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9748029213733032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9380456383296186 | validation: 0.6857157982710902]
	TIME [epoch: 9.05 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6458497213274011		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5765721362128186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6112109287701097 | validation: 0.7660100171030055]
	TIME [epoch: 9.07 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7011658912231898		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4584025933623492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5797842422927696 | validation: 0.43162620751173864]
	TIME [epoch: 9.06 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5009334265740824		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9189290513699122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7099312389719972 | validation: 0.4217377101276747]
	TIME [epoch: 9.06 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5350343150321173		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4940909006720345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5145626078520759 | validation: 0.4873961288766063]
	TIME [epoch: 9.06 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8451718346091338		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5640485867363465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7046102106727401 | validation: 0.48216351372324945]
	TIME [epoch: 9.05 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5963903086026632		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6458674037224073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.621128856162535 | validation: 0.46809227082185456]
	TIME [epoch: 9.09 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7279003198014913		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6181919890965881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6730461544490396 | validation: 0.5376947723048047]
	TIME [epoch: 9.06 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5058688771399648		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3536547649286592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.929761821034312 | validation: 0.8845814370083802]
	TIME [epoch: 9.06 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6488790230909102		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5185323382624729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5837056806766914 | validation: 0.9580154200406407]
	TIME [epoch: 9.05 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7316766554357252		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5232452302199764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6274609428278509 | validation: 0.6719143649876238]
	TIME [epoch: 9.06 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6976421128465012		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5087611560818639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6032016344641824 | validation: 0.49244403376744994]
	TIME [epoch: 9.08 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.537077502787543		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.475643673335159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5063605880613509 | validation: 0.3831515149812792]
	TIME [epoch: 9.06 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5144717724298529		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7354137004942478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6249427364620502 | validation: 0.9104557363417478]
	TIME [epoch: 9.06 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6779082007851595		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8920725749164147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.784990387850787 | validation: 0.7081478747841605]
	TIME [epoch: 9.06 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6418047751322178		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.696562249138698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6691835121354579 | validation: 0.4873694111088537]
	TIME [epoch: 9.06 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5853747070271746		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45349635885775497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5194355329424649 | validation: 0.40342107884420164]
	TIME [epoch: 9.08 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5458150419442265		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5215457857527918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5336804138485092 | validation: 0.334883594415624]
	TIME [epoch: 9.06 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4299604503884433		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.44410467892306116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43703256465575224 | validation: 0.3418252803606528]
	TIME [epoch: 9.05 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4817305709950781		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.42620341490072444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45396699294790127 | validation: 0.5674642389964553]
	TIME [epoch: 9.04 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.540120203448267		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5645401571060225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5523301802771445 | validation: 0.3590390473683511]
	TIME [epoch: 9.05 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4353366348413738		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8748616156855334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6550991252634536 | validation: 0.530185195779689]
	TIME [epoch: 9.08 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5033895341264082		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5236490201069681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5135192771166881 | validation: 0.7625167452551337]
	TIME [epoch: 9.05 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5224932127714897		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5601285012148914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5413108569931907 | validation: 0.28985371977580676]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4564467893935736		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.685472506534486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5709596479640298 | validation: 0.6709261792759162]
	TIME [epoch: 9.05 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5623784326638648		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7925168475127276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6774476400882963 | validation: 1.078799924129013]
	TIME [epoch: 9.04 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5397888616889797		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5522677211184288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5460282914037042 | validation: 0.42236429038708356]
	TIME [epoch: 9.08 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4150984718610384		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5422516248815151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47867504837127683 | validation: 0.33187939940235245]
	TIME [epoch: 9.07 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4687015121809103		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48600754451926936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4773545283500898 | validation: 0.20778832097542832]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5425740255237067		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4980109057671041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5202924656454054 | validation: 0.18968784509943398]
	TIME [epoch: 9.07 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47993770778578176		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5480004780604155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5139690929230987 | validation: 0.34793490098347624]
	TIME [epoch: 9.32 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5142492985508724		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46427107932346773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48926018893717005 | validation: 1.2097364125828574]
	TIME [epoch: 9.08 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6718564989665472		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5254487755574766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5986526372620121 | validation: 0.6745070692626268]
	TIME [epoch: 9.06 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8658437603633239		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8367704244785337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.851307092420929 | validation: 1.8560407696073038]
	TIME [epoch: 9.06 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7496100445794598		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7026391463386927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7261245954590763 | validation: 0.7369060723932676]
	TIME [epoch: 9.06 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6103911100989267		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1417583368511395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8760747234750331 | validation: 1.536076626649118]
	TIME [epoch: 9.06 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6963398873736434		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8467757386278484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7715578130007459 | validation: 0.5150492109502962]
	TIME [epoch: 9.09 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5713303148544746		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5271353536521708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5492328342533227 | validation: 0.41492639408112403]
	TIME [epoch: 9.06 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5772164426572983		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5079663636311992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5425914031442488 | validation: 0.42018158972415354]
	TIME [epoch: 9.06 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4790974236500246		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6690629491070337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5740801863785291 | validation: 0.3403611879633114]
	TIME [epoch: 9.05 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7832085260556105		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49331055576092336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6382595409082669 | validation: 0.37874956367344687]
	TIME [epoch: 9.06 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6294667591366047		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6915139851207608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6604903721286828 | validation: 0.7358262247095715]
	TIME [epoch: 9.07 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9031391649920486		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5821015715347398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7426203682633943 | validation: 0.28762615045178563]
	TIME [epoch: 9.06 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5768603002762184		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5391393666721949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5579998334742067 | validation: 0.3182179940139703]
	TIME [epoch: 9.05 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5201233028257951		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6133920971589955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5667576999923953 | validation: 0.5977710080510947]
	TIME [epoch: 9.06 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0177457367092093		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5977358605513189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8077407986302643 | validation: 0.4218054227038919]
	TIME [epoch: 9.06 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5042898708169061		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5253615821996287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5148257265082675 | validation: 0.6949830208608408]
	TIME [epoch: 9.09 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4745599795602823		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6254673075045869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5500136435324346 | validation: 0.8624496174247231]
	TIME [epoch: 9.06 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6884489838246031		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5812704036625906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6348596937435969 | validation: 0.4839075824945665]
	TIME [epoch: 9.07 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6869180518604502		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5957161161197975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6413170839901239 | validation: 0.4557167610026595]
	TIME [epoch: 9.06 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8323547280951933		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48049196609514067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.656423347095167 | validation: 0.4534259138787863]
	TIME [epoch: 9.06 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5620477416325412		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5733157028436227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5676817222380819 | validation: 0.5407334817235565]
	TIME [epoch: 9.08 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6353609261663644		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8393258612384592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7373433937024118 | validation: 0.8588642087075433]
	TIME [epoch: 9.06 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6138279823203329		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6443346790120921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6290813306662123 | validation: 0.3488036971165863]
	TIME [epoch: 9.05 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5101553183616444		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5530045621573422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5315799402594934 | validation: 0.7811930127920467]
	TIME [epoch: 9.05 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49879219365548033		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.524007295516063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5113997445857718 | validation: 0.35246579334102857]
	TIME [epoch: 9.04 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5909215101492571		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5912840444810279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5911027773151424 | validation: 0.62359089328911]
	TIME [epoch: 9.08 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5139258699371271		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48524150067570987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4995836853064185 | validation: 0.28428938645983237]
	TIME [epoch: 9.06 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6736923124012858		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5702022010486676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6219472567249766 | validation: 0.8163971482989559]
	TIME [epoch: 9.07 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6634542924062481		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5973858354869634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6304200639466059 | validation: 0.5895338164254194]
	TIME [epoch: 9.06 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5988613200307056		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4860802097454851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5424707648880953 | validation: 0.3176746739498283]
	TIME [epoch: 9.07 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4620946275605623		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5647826670176016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.513438647289082 | validation: 0.3652928532202522]
	TIME [epoch: 9.07 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43637457563726567		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5402334944193391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48830403502830233 | validation: 0.5639830515868801]
	TIME [epoch: 9.07 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44966483636482907		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5244789561112304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4870718962380297 | validation: 0.21052503623119814]
	TIME [epoch: 9.06 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.441518771334705		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5586463499097226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5000825606222138 | validation: 0.723610695003058]
	TIME [epoch: 9.06 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8406642577408971		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6644482198627384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7525562388018179 | validation: 0.47803210191558726]
	TIME [epoch: 9.05 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4738625016866478		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4734173097533841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4736399057200159 | validation: 0.3707334199014436]
	TIME [epoch: 9.07 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48294653409236776		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8486910848289433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6658188094606554 | validation: 0.43749769460223387]
	TIME [epoch: 9.06 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.503850997135145		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45935248329277345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4816017402139593 | validation: 0.3719045219787688]
	TIME [epoch: 9.05 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5257619346568938		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5667318017032785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5462468681800862 | validation: 0.414395762535906]
	TIME [epoch: 9.05 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4407269832312969		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5324267911167018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4865768871739992 | validation: 0.36467428159572846]
	TIME [epoch: 9.05 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46475060926517575		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9747644642373462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.719757536751261 | validation: 1.5140138450102942]
	TIME [epoch: 9.08 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9425656565678582		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4681699296705399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7053677931191988 | validation: 0.6727218503872032]
	TIME [epoch: 9.05 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.51999425142682		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5466686108969403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5333314311618802 | validation: 0.5291308429905806]
	TIME [epoch: 9.05 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7466042354393799		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5700700146441883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.658337125041784 | validation: 0.43070328111330203]
	TIME [epoch: 9.05 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4847824769629548		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4335706759256669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4591765764443109 | validation: 0.31369958308673057]
	TIME [epoch: 9.06 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.600993133824389		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.705832601616839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.653412867720614 | validation: 0.4359364733640635]
	TIME [epoch: 9.07 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5017625605694882		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.556188989920974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5289757752452312 | validation: 0.49832875973149804]
	TIME [epoch: 9.06 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6342196117795341		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.557172979422446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5956962956009901 | validation: 1.0140555439265564]
	TIME [epoch: 9.05 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5731801562368152		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.60380130249755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5884907293671826 | validation: 0.3814568478493374]
	TIME [epoch: 9.06 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45395588755649163		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4513526366838726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45265426212018206 | validation: 0.3079500955896407]
	TIME [epoch: 9.06 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4318078860701801		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48087415254489424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4563410193075372 | validation: 0.7267410115859632]
	TIME [epoch: 9.07 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.513490422156768		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4179454584469715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4657179403018697 | validation: 0.6037697570210872]
	TIME [epoch: 9.06 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5435093378632933		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4156540462530602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4795816920581767 | validation: 0.38671450891051584]
	TIME [epoch: 9.06 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7765969114371873		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48777111757915287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6321840145081701 | validation: 0.3329781136831835]
	TIME [epoch: 9.06 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.567400003800337		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.500294005315837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5338470045580871 | validation: 0.37634831052861417]
	TIME [epoch: 9.06 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5068126863228326		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5445241394046904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5256684128637615 | validation: 0.5024713677520849]
	TIME [epoch: 9.08 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4546203267324276		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4963078805691713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4754641036507995 | validation: 0.47397730988510367]
	TIME [epoch: 9.07 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5722515305459639		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5971500961586527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5847008133523082 | validation: 0.6259517561525038]
	TIME [epoch: 9.05 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4185822618266094		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40258251832957354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41058239007809155 | validation: 0.45735418045700715]
	TIME [epoch: 9.06 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6471916571483413		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4797973070490043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5634944820986727 | validation: 0.42333634852094]
	TIME [epoch: 9.06 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46302114995507315		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6753269004800818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5691740252175774 | validation: 0.7936620007710506]
	TIME [epoch: 9.08 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48366099423125686		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5216403240526604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5026506591419586 | validation: 0.5470156618199287]
	TIME [epoch: 9.06 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4122005632920729		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6365595964904889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5243800798912808 | validation: 0.3461280947415638]
	TIME [epoch: 9.06 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4398979360437415		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.50170959215181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4708037640977757 | validation: 0.2833720524842266]
	TIME [epoch: 9.05 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43304007778120734		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4156951122304564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4243675950058319 | validation: 0.5074848236328513]
	TIME [epoch: 9.06 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5697639708457907		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6549300341738604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6123470025098254 | validation: 0.4867056222819545]
	TIME [epoch: 9.07 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6640928846219939		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4844360530799136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5742644688509537 | validation: 0.8543444831361446]
	TIME [epoch: 9.06 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7364905854137124		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4653497660494553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6009201757315838 | validation: 0.6849249056484867]
	TIME [epoch: 9.05 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.547187242845272		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5402469540540145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5437170984496433 | validation: 0.3658085051880805]
	TIME [epoch: 9.06 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45592335099974735		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4518506620555388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45388700652764313 | validation: 0.7850542741971143]
	TIME [epoch: 9.06 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5580643737384662		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4185943011890888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4883293374637775 | validation: 0.3154913482998038]
	TIME [epoch: 9.08 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5567596815756582		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3440097663401038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.450384723957881 | validation: 0.2822188105279941]
	TIME [epoch: 9.06 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4360627561320466		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.529596589271773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48282967270190974 | validation: 0.4313486661774324]
	TIME [epoch: 9.05 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49240456887228445		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47499013767548626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4836973532738854 | validation: 0.8322416652199021]
	TIME [epoch: 9.05 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5683342594031633		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3669894693343781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4676618643687706 | validation: 0.5704167700975016]
	TIME [epoch: 9.05 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5229671291782695		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5476140848897989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5352906070340342 | validation: 0.48660126794719705]
	TIME [epoch: 9.07 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38336127073004084		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4743460465062669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42885365861815394 | validation: 0.4295770959891343]
	TIME [epoch: 9.07 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49041399204824776		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3983772360997778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4443956140740128 | validation: 0.3947425672825099]
	TIME [epoch: 9.06 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4806656310860644		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5009722192108887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49081892514847664 | validation: 0.25317667868708393]
	TIME [epoch: 9.06 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49758397039229674		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.379488538447398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43853625441984745 | validation: 0.31154593401140446]
	TIME [epoch: 9.06 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42902729719007765		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48046945406942204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4547483756297498 | validation: 0.3829205234811348]
	TIME [epoch: 9.07 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4932134762070698		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3263287087856367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40977109249635324 | validation: 0.5670567009516476]
	TIME [epoch: 9.06 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49696586136267945		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.509282866001157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5031243636819182 | validation: 0.35070603684538737]
	TIME [epoch: 9.05 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43980278445261495		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6093918687148363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5245973265837256 | validation: 0.36113282168998906]
	TIME [epoch: 9.06 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4138171743727678		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3734660875581226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3936416309654453 | validation: 0.20091812431594813]
	TIME [epoch: 9.05 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3698154122207037		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4244777197552566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3971465659879802 | validation: 0.5749770087726734]
	TIME [epoch: 9.07 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8019919077480651		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5630783624569532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6825351351025092 | validation: 0.347733968291877]
	TIME [epoch: 9.06 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4175870394772307		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4216301593802113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.419608599428721 | validation: 0.5019239945087914]
	TIME [epoch: 9.06 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5019322889083667		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5186157553110521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5102740221097093 | validation: 0.3333729490085803]
	TIME [epoch: 9.05 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3986672981189104		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5141512065515976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45640925233525387 | validation: 0.6929659328184564]
	TIME [epoch: 9.07 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.443993672996435		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40494012963398474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4244669013152099 | validation: 0.4659612653279408]
	TIME [epoch: 9.08 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4882252454569603		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.37922854406207074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43372689475951554 | validation: 0.44414562469752117]
	TIME [epoch: 9.07 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32193522877154956		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4586085942613206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3902719115164351 | validation: 0.586748557542958]
	TIME [epoch: 9.06 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3818026445463165		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3598217477445024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3708121961454095 | validation: 0.29560313073689126]
	TIME [epoch: 9.06 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37444070150988173		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32274842923293817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34859456537141 | validation: 0.35959072083463706]
	TIME [epoch: 9.06 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4391182550524892		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4997418216713433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4694300383619163 | validation: 0.8963829504821423]
	TIME [epoch: 9.08 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6659753016387954		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.44243551355211574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5542054075954554 | validation: 0.5877036672886902]
	TIME [epoch: 9.06 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5218453129164066		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5780230906913727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5499342018038896 | validation: 0.6038699256338934]
	TIME [epoch: 9.06 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46057614505504807		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5946799792243909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5276280621397195 | validation: 0.2612780843481779]
	TIME [epoch: 9.05 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41042658592032727		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5011026262090128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4557646060646701 | validation: 0.26607110005865214]
	TIME [epoch: 9.05 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5411456228380758		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4794683086961597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5103069657671178 | validation: 0.46502665895466466]
	TIME [epoch: 9.07 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4738099647671934		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.732176950492495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1029934576298441 | validation: 0.395588858575727]
	TIME [epoch: 9.06 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1240628006909237		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7541247399566134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9390937703237686 | validation: 0.8869093643440265]
	TIME [epoch: 9.05 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7159613634014098		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6806455169504178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6983034401759138 | validation: 0.6423659236833295]
	TIME [epoch: 9.06 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8154396907536015		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4695943073482569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6425169990509294 | validation: 0.3759286685824461]
	TIME [epoch: 9.06 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6313956047017729		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6705303919031798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6509629983024764 | validation: 0.3338039136588007]
	TIME [epoch: 9.09 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6576198998755981		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4898912457218533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5737555727987258 | validation: 0.6363395235178191]
	TIME [epoch: 9.06 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43093124578738917		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4370441805056898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43398771314653956 | validation: 0.6224565175943]
	TIME [epoch: 9.06 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3854763102869112		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46721449396975273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42634540212833205 | validation: 0.8447937839769035]
	TIME [epoch: 9.05 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6266720912904319		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.440297984963817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5334850381271244 | validation: 0.4042708001980097]
	TIME [epoch: 9.06 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49066659299969356		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6059521959292709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5483093944644821 | validation: 0.5654088640705792]
	TIME [epoch: 9.07 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.547544459349987		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4999616159936249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5237530376718058 | validation: 0.5752187616597788]
	TIME [epoch: 9.06 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47862275507870206		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6176021364918065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5481124457852544 | validation: 0.6735110966529712]
	TIME [epoch: 9.05 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49476095263672687		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.41814179204922297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4564513723429749 | validation: 0.27652509331190706]
	TIME [epoch: 9.05 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5562426932037424		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43526366097554553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49575317708964406 | validation: 0.46489840243729097]
	TIME [epoch: 9.06 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5421675100390189		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3393005309524479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44073402049573335 | validation: 0.2900291578439983]
	TIME [epoch: 9.06 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36851365825686916		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4244205587128442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39646710848485667 | validation: 0.3586422118753375]
	TIME [epoch: 9.07 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3994857615908235		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46192267212481697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4307042168578203 | validation: 0.29345412958798067]
	TIME [epoch: 9.06 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5108192563984006		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5468246290473642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5288219427228824 | validation: 0.37359229431632157]
	TIME [epoch: 9.05 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45938910815389866		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.34953912490819583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40446411653104714 | validation: 0.6163990488307517]
	TIME [epoch: 9.06 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4267260424981312		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.39104634713901054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4088861948185708 | validation: 0.5216570854735855]
	TIME [epoch: 9.07 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3488208385156485		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3990567487387045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37393879362717647 | validation: 0.2959111814109724]
	TIME [epoch: 9.06 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3868113769869383		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46640382501464284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42660760100079065 | validation: 0.19693933513175663]
	TIME [epoch: 9.05 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31200234637359536		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.34221770048941125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32711002343150336 | validation: 0.3225097332118179]
	TIME [epoch: 9.05 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34207962186250124		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3821336670249547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.362106644443728 | validation: 0.20341966129738293]
	TIME [epoch: 9.05 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34381626590725006		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4380367471643689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39092650653580946 | validation: 0.2577775574241239]
	TIME [epoch: 9.07 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4066840080037317		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38397928700598344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3953316475048576 | validation: 0.5834324025168842]
	TIME [epoch: 9.06 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34668928859157133		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.41097185157440536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37883057008298837 | validation: 0.310782068477374]
	TIME [epoch: 9.05 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3687299389261731		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.41897820434988586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39385407163802955 | validation: 0.47518660775784605]
	TIME [epoch: 9.06 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44817045640314407		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4464694447129852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44731995055806467 | validation: 0.22715611529677138]
	TIME [epoch: 9.04 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3761867177429471		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4113261735536988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.393756445648323 | validation: 0.5516930470857263]
	TIME [epoch: 9.07 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3508140933570899		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5655644989726627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4581892961648763 | validation: 0.638998985434741]
	TIME [epoch: 9.06 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4107110434659241		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.37944301794841545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3950770307071698 | validation: 0.5439373237479868]
	TIME [epoch: 9.06 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38774483670386195		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3210756080426938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35441022237327796 | validation: 0.30691348747250874]
	TIME [epoch: 9.05 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41615108998157513		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4363919574013738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42627152369147436 | validation: 0.1825952254231686]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35811677606476433		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4124441708099794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38528047343737176 | validation: 0.5029077864362249]
	TIME [epoch: 9.07 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3745751314709132		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4180728486431394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39632399005702623 | validation: 0.3556017918362189]
	TIME [epoch: 9.06 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47894114005892563		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48333576634994857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48113845320443716 | validation: 0.7183874898707774]
	TIME [epoch: 9.05 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4418682764999608		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48430242573335686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46308535111665894 | validation: 0.25599586505049954]
	TIME [epoch: 9.05 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3918363445525551		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45878032418107917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42530833436681714 | validation: 0.47865772847018384]
	TIME [epoch: 9.05 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32195987157734773		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3452668883734858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33361337997541674 | validation: 0.7142083113238514]
	TIME [epoch: 9.07 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4474796997647127		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7363866254584208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5919331626115668 | validation: 0.3659938678067781]
	TIME [epoch: 9.06 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4310438211168317		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4562934001773768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4436686106471043 | validation: 1.1300488525519068]
	TIME [epoch: 9.05 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5007794048091955		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38363810649842567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4422087556538107 | validation: 0.32336671749953383]
	TIME [epoch: 9.05 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42158017451551616		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4515938817833609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4365870281494385 | validation: 0.4321941288916806]
	TIME [epoch: 9.05 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3112076710046869		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3476352300875815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32942145054613425 | validation: 0.28677742278561413]
	TIME [epoch: 9.06 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44183966501249855		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5092422614102692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4755409632113837 | validation: 0.31965500824810217]
	TIME [epoch: 9.06 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.317523830639145		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.35862056710306917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3380721988711072 | validation: 0.2977490838293704]
	TIME [epoch: 9.05 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40912598656756216		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38769474135227594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3984103639599191 | validation: 0.20987510140418864]
	TIME [epoch: 9.05 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4265369339740691		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.335463681330209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38100030765213905 | validation: 0.3604321954109112]
	TIME [epoch: 9.05 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2862748549385951		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3710754390737426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3286751470061688 | validation: 0.37766503168659205]
	TIME [epoch: 9.07 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24210549329435746		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36450215343360404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3033038233639808 | validation: 0.3793547566488072]
	TIME [epoch: 9.07 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37374929614115693		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4613004325218165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4175248643314867 | validation: 0.20101466800273382]
	TIME [epoch: 9.05 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.375945366996814		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45482711281128924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41538623990405166 | validation: 0.501469479264328]
	TIME [epoch: 9.05 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39234569117329177		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4227563200356843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40755100560448804 | validation: 0.20336182784774787]
	TIME [epoch: 9.05 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2744094232343839		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4386425540298302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3565259886321071 | validation: 0.27229645751279186]
	TIME [epoch: 9.06 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.469862503711919		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29006593880273635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37996422125732765 | validation: 0.3043813899227762]
	TIME [epoch: 9.06 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37631730448772194		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43935071643275914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4078340104602406 | validation: 0.5816493193469626]
	TIME [epoch: 9.05 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39026065047989256		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3024505918400201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3463556211599564 | validation: 0.24507807703974194]
	TIME [epoch: 9.06 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3782845291003896		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46244216306977143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4203633460850805 | validation: 0.340810411359344]
	TIME [epoch: 9.05 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2501239121022826		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36221757274606314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3061707424241728 | validation: 0.31342930061399127]
	TIME [epoch: 9.05 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3375285487248326		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43581128646672795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38666991759578034 | validation: 0.2608185839193524]
	TIME [epoch: 9.07 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4164417020630986		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3726885971814025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39456514962225053 | validation: 0.32711099371709335]
	TIME [epoch: 9.05 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2897671977851601		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4413447881119903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36555599294857516 | validation: 0.4391701054349605]
	TIME [epoch: 9.06 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3404487086989908		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.33402046094679766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33723458482289426 | validation: 0.5589057424613587]
	TIME [epoch: 9.06 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3598024073747071		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36639458981188555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36309849859329635 | validation: 0.22167774067650264]
	TIME [epoch: 9.06 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36120226661764826		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.309545967567573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33537411709261067 | validation: 0.3128800464963592]
	TIME [epoch: 9.08 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3609444252695029		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4068486069850822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3838965161272926 | validation: 0.3489712485240707]
	TIME [epoch: 9.05 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3861470496255997		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4615838791135989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42386546436959927 | validation: 0.5065964501969895]
	TIME [epoch: 9.05 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3682813506296082		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.42584265079181083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3970620007107095 | validation: 0.27219746631228126]
	TIME [epoch: 9.06 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3766358693931016		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32822874028119736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3524323048371495 | validation: 0.3977977953236856]
	TIME [epoch: 9.05 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3814708622027581		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38408708862734414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3827789754150511 | validation: 0.32506967091155364]
	TIME [epoch: 9.09 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29727318359152044		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3979683088512725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34762074622139655 | validation: 0.34861256511189065]
	TIME [epoch: 9.05 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4064564907412075		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43772727626063607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42209188350092186 | validation: 0.34808750317684056]
	TIME [epoch: 9.05 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3489128698245877		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5242161516819317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4365645107532597 | validation: 0.27060516023347997]
	TIME [epoch: 9.05 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35367681802012635		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4314523019779773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3925645599990518 | validation: 0.32338531128050796]
	TIME [epoch: 9.05 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29264153703433643		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25682360560905804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27473257132169726 | validation: 0.24543083363317206]
	TIME [epoch: 9.07 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3005081505002437		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.39046040579045205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3454842781453478 | validation: 0.3746833842082124]
	TIME [epoch: 9.05 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34790322307474597		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.454033961058269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4009685920665075 | validation: 0.36942138049212947]
	TIME [epoch: 9.91 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33786370625906714		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2973515928521554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3176076495556112 | validation: 0.40496558704563873]
	TIME [epoch: 9.05 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47484127629932393		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4406160646483575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4577286704738407 | validation: 0.2693613936337512]
	TIME [epoch: 9.05 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3576010626524059		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4126910589575254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3851460608049656 | validation: 1.001908260575131]
	TIME [epoch: 9.07 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4741717354994564		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3831719399958601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4286718377476582 | validation: 0.20837659990914478]
	TIME [epoch: 9.06 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3473931148546542		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2659229544643096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3066580346594819 | validation: 0.2998163413397001]
	TIME [epoch: 9.05 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3152450073348741		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4357293821936863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3754871947642802 | validation: 0.2945752397359154]
	TIME [epoch: 9.05 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2755929466562394		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2755839764044538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2755884615303466 | validation: 0.1871583560285567]
	TIME [epoch: 9.05 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5508973666361981		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.33843225140124866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4446648090187234 | validation: 0.3096697983625871]
	TIME [epoch: 9.08 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43861877753867395		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.31805502629801924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37833690191834657 | validation: 0.3182058709165928]
	TIME [epoch: 9.05 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.403510860445914		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3535673542514988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3785391073487063 | validation: 0.20986552925260904]
	TIME [epoch: 9.05 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3277446268208246		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3089982012877911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31837141405430786 | validation: 0.4519879173282051]
	TIME [epoch: 9.06 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37273985697256834		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3505710372741898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36165544712337916 | validation: 0.28114091582691675]
	TIME [epoch: 9.05 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.423766216520643		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48306438359182396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4534153000562335 | validation: 0.4318665484885047]
	TIME [epoch: 9.08 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3588659006833311		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3240954796925898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3414806901879604 | validation: 0.5382977802219852]
	TIME [epoch: 9.05 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3153449588401459		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3999990872990623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35767202306960405 | validation: 0.3372537597617272]
	TIME [epoch: 9.05 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35513022358869795		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.337676616004673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34640341979668554 | validation: 0.3057705792188356]
	TIME [epoch: 9.05 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33015222854402054		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4610265595689664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39558939405649346 | validation: 0.4827239272444769]
	TIME [epoch: 9.06 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4029648338182226		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.35954956434362273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38125719908092265 | validation: 0.4752234387529526]
	TIME [epoch: 9.09 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3575733460642281		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32123448005232613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33940391305827705 | validation: 0.6723560404017634]
	TIME [epoch: 9.06 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4466429812449128		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32987263319503274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3882578072199728 | validation: 0.5419081392201344]
	TIME [epoch: 9.06 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4592476984911354		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2960084674636672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37762808297740136 | validation: 0.4114558260850311]
	TIME [epoch: 9.06 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3090458573427993		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.404624573720882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35683521553184067 | validation: 0.3761674383147574]
	TIME [epoch: 9.05 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.545831463597514		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.34559268115022285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4457120723738683 | validation: 0.5094952339006829]
	TIME [epoch: 9.1 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2956325812242756		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.42255591803359904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35909424962893727 | validation: 0.42905970324487974]
	TIME [epoch: 9.05 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.466522805963113		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3829799897808852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4247513978719991 | validation: 0.20925153454477954]
	TIME [epoch: 9.05 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2350751402856981		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32394435103856484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2795097456621315 | validation: 0.40208633558615026]
	TIME [epoch: 9.05 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42342309221461827		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3431292948919813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3832761935532997 | validation: 0.33568721362542464]
	TIME [epoch: 9.05 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38824810731423554		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3628091174580709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37552861238615326 | validation: 0.45346641419692235]
	TIME [epoch: 9.08 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36736680868368443		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2677424770617852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3175546428727348 | validation: 0.4245122068372713]
	TIME [epoch: 9.06 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.311752995920861		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25695821974194083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2843556078314009 | validation: 0.4328997553878762]
	TIME [epoch: 9.05 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3762942104361987		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3864996465036463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3813969284699225 | validation: 0.34573643991864245]
	TIME [epoch: 9.05 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3720748252267884		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.402623341240543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3873490832336657 | validation: 0.2266590077654256]
	TIME [epoch: 9.06 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2873823872679982		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30671578390720416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2970490855876012 | validation: 0.31216935616896757]
	TIME [epoch: 9.1 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3225020880247611		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3809646103468187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35173334918578986 | validation: 0.3020784576006784]
	TIME [epoch: 9.05 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3741179902756075		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3132544940310366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.343686242153322 | validation: 0.2745539284813314]
	TIME [epoch: 9.05 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4731077661530555		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.37141083943341824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.422259302793237 | validation: 0.3373939825035843]
	TIME [epoch: 9.05 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3360377412267247		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3383821790501571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33720996013844085 | validation: 0.48329504643755117]
	TIME [epoch: 9.05 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32031043561370326		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2826466989589952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30147856728634914 | validation: 0.3066482782689761]
	TIME [epoch: 9.07 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36072016530557505		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43773726077784614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39922871304171065 | validation: 0.31174174545969735]
	TIME [epoch: 9.05 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3193723899402159		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.333652718916595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32651255442840543 | validation: 0.2663887892826117]
	TIME [epoch: 9.06 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2736784084723545		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4418782904194286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3577783494458915 | validation: 0.34594110155267577]
	TIME [epoch: 9.04 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3377518393813538		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.35911076166662187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34843130052398774 | validation: 0.5329668236786795]
	TIME [epoch: 9.06 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.385764508753347		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29621047660174576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3409874926775464 | validation: 0.1880657733173971]
	TIME [epoch: 9.08 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3157663508550873		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.39568097849329087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35572366467418914 | validation: 0.28413538442459113]
	TIME [epoch: 9.06 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35959377945770743		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3839305621234467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3717621707905771 | validation: 0.3108272661250458]
	TIME [epoch: 9.05 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3340668237301193		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29748469477558936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31577575925285434 | validation: 0.48643350837398064]
	TIME [epoch: 9.05 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34226698676692		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.356586934318195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34942696054255756 | validation: 0.49869594263568334]
	TIME [epoch: 9.04 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7410167214617139		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3510653807141059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.54604105108791 | validation: 0.28103382891172085]
	TIME [epoch: 9.07 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2829132998597046		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38080506021603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3318591800378673 | validation: 0.37483629312311223]
	TIME [epoch: 9.05 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3533253982404593		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3232976689732924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3383115336068758 | validation: 0.30635578300232114]
	TIME [epoch: 9.05 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2942879114971239		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2540804203386184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2741841659178712 | validation: 0.3930607763465732]
	TIME [epoch: 9.05 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28796555891904274		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2594438266614145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27370469279022863 | validation: 0.23404390831799166]
	TIME [epoch: 9.05 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2917519529034257		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2848789738084364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.288315463355931 | validation: 0.1809863269372668]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2233425432201151		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22879624990838168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2260693965642484 | validation: 0.19978613769443587]
	TIME [epoch: 9.05 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4685422544854282		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30262091413146325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3855815843084458 | validation: 0.337983929585248]
	TIME [epoch: 9.05 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4379895561589552		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30668118052456433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37233536834175973 | validation: 0.2987321270061367]
	TIME [epoch: 9.05 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28816607158762564		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2982783590921607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2932222153398932 | validation: 0.284190314361489]
	TIME [epoch: 9.05 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2943018450654617		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29832498304015914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2963134140528105 | validation: 0.17451905063206433]
	TIME [epoch: 9.07 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3471059261543911		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3995444602696596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37332519321202534 | validation: 0.22269923119326177]
	TIME [epoch: 9.05 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0292029265071219		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4718938779835509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2505484022453364 | validation: 0.39139792185261635]
	TIME [epoch: 9.05 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42365239206872485		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3225570486407433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3731047203547341 | validation: 0.20104928200653038]
	TIME [epoch: 9.05 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26862526058065295		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3944249792753748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3315251199280139 | validation: 0.14420185219908843]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31758909444055805		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2987842819776466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30818668820910233 | validation: 0.2982911669238247]
	TIME [epoch: 9.08 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4389170007197382		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.35659395671618366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39775547871796085 | validation: 0.30285993255783844]
	TIME [epoch: 9.05 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33659820886734837		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3907024493642338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36365032911579104 | validation: 0.4963029509437621]
	TIME [epoch: 9.05 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3599681220160812		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23233352619241626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2961508241042487 | validation: 0.1792696921091973]
	TIME [epoch: 9.05 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2887108530039481		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.33364613206002186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31117849253198504 | validation: 0.23858411424470072]
	TIME [epoch: 9.05 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32242261191276766		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2653414006458751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2938820062793214 | validation: 0.39065115509688375]
	TIME [epoch: 9.07 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38960178379573074		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36000642881543066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37480410630558064 | validation: 0.2736805878751732]
	TIME [epoch: 9.04 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3333785773317754		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4673655197818734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40037204855682446 | validation: 0.31726055967027394]
	TIME [epoch: 9.04 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33575026887251447		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4173163947475159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37653333181001514 | validation: 0.30234636487958155]
	TIME [epoch: 9.05 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2919244455947602		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.35294718351922594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32243581455699316 | validation: 0.3218146940539436]
	TIME [epoch: 9.05 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3439398824046861		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3600011720978776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3519705272512818 | validation: 0.29514132121912573]
	TIME [epoch: 9.06 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29600802244270913		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29972488636103223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29786645440187065 | validation: 0.25492270328573374]
	TIME [epoch: 9.05 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3185694590294522		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4868028997828994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4026861794061759 | validation: 0.22789752264025442]
	TIME [epoch: 9.04 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28737855860287237		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2991539660377809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7932662623203266 | validation: 0.6717367489423998]
	TIME [epoch: 9.04 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.217224413510835		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.027482292573631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.622353353042233 | validation: 2.6789025144860243]
	TIME [epoch: 9.04 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3979179739700918		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2384960467762185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.318207010373155 | validation: 2.6866278126129646]
	TIME [epoch: 9.07 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.787632383970592		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5859489725100041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1867906782402984 | validation: 0.6742588408347567]
	TIME [epoch: 9.05 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5772492561516616		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7387152440093209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6579822500804913 | validation: 0.6804030090437515]
	TIME [epoch: 9.05 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9709997158206768		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6802848817518388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8256422987862579 | validation: 0.4855606203083191]
	TIME [epoch: 9.05 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5349136088653252		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7561051156693608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6455093622673431 | validation: 0.930258053878271]
	TIME [epoch: 9.05 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6542251509464687		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5875110999991386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.620868125472804 | validation: 0.3976023878255444]
	TIME [epoch: 9.07 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5419209922596699		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6731762378935621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6075486150766161 | validation: 0.5962560464296105]
	TIME [epoch: 9.05 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9480919134702072		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6319590219696085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.790025467719908 | validation: 0.7104574602955468]
	TIME [epoch: 9.04 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7618140445623242		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7356442878131182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7487291661877211 | validation: 1.3041525106983052]
	TIME [epoch: 9.09 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7249951986422657		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.42322874869476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.574111973668513 | validation: 0.2931395942467569]
	TIME [epoch: 9.04 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37009723945588		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.37281167552666106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37145445749127054 | validation: 0.3163077717803497]
	TIME [epoch: 9.07 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45990709650745903		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3893642303072116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42463566340733533 | validation: 0.39325338477248317]
	TIME [epoch: 9.05 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5490856758613033		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5566821573327918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5528839165970476 | validation: 0.3377644592291317]
	TIME [epoch: 9.04 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7064852129933394		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.568716616571103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6376009147822213 | validation: 0.4974496782048785]
	TIME [epoch: 9.04 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4667898112901584		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4385092568933396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.452649534091749 | validation: 0.5052078144199804]
	TIME [epoch: 9.05 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5514844715991378		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1445977586160185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3480411151075784 | validation: 6.556637962741508]
	TIME [epoch: 9.06 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.449448405982909		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.117591721767213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.783520063875061 | validation: 6.124829265372853]
	TIME [epoch: 9.05 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.822710313959133		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.629349442267124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7260298781131285 | validation: 6.0921091000686864]
	TIME [epoch: 9.05 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.671544349880133		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.872502024244907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.772023187062521 | validation: 6.132156844834144]
	TIME [epoch: 9.05 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.75861445822215		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.56004698537114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.659330721796644 | validation: 6.145573029112706]
	TIME [epoch: 9.04 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.450441257664517		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.6867210296032145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.568581143633867 | validation: 5.619367215818986]
	TIME [epoch: 9.07 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.686262194640069		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.444802798101074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0655324963705715 | validation: 3.321149080683536]
	TIME [epoch: 9.05 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.023006292906104		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.138961208072597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.08098375048935 | validation: 2.089486003941301]
	TIME [epoch: 9.04 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9146970987152496		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.652732319602152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7837147091587007 | validation: 1.4697793340300183]
	TIME [epoch: 9.05 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4307274096370768		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4042406089856638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4174840093113703 | validation: 1.4187205563927356]
	TIME [epoch: 9.04 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2488562986413736		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.37657201221741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.312714155429392 | validation: 1.2495888797100712]
	TIME [epoch: 9.07 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4109923774473598		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1756078440754767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2933001107614184 | validation: 1.4264046701999837]
	TIME [epoch: 9.05 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2432145535099965		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1728631948845456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2080388741972714 | validation: 1.1632565805889352]
	TIME [epoch: 9.05 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2055517172958896		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1435551805014073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1745534488986487 | validation: 1.0979207265863342]
	TIME [epoch: 9.05 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1240433864773893		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0271346727619464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0755890296196677 | validation: 1.0495615884867577]
	TIME [epoch: 9.04 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0262641731701503		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0522892353547533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.039276704262452 | validation: 0.9458718883414631]
	TIME [epoch: 9.07 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.944394079905417		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0497249536622328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.997059516783825 | validation: 0.8515236630564138]
	TIME [epoch: 9.05 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.901056351455982		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9772864843084497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9391714178822159 | validation: 0.9752537175720586]
	TIME [epoch: 9.04 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9352837995167329		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9700142201285337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9526490098226332 | validation: 1.0525566054774105]
	TIME [epoch: 9.04 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9370786996788848		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.109023197198184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0230509484385344 | validation: 1.0001549933373914]
	TIME [epoch: 9.04 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9342642607806552		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9700102267249724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9521372437528137 | validation: 0.9212410313387979]
	TIME [epoch: 9.08 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1864452201694335		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9535955081427062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0700203641560697 | validation: 0.9060199570222883]
	TIME [epoch: 9.05 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9068236760918967		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9035448346470565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9051842553694767 | validation: 0.8072930677037742]
	TIME [epoch: 9.05 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0700818855483083		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9396861276139908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0048840065811497 | validation: 0.8208891262569824]
	TIME [epoch: 9.04 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8653522442156089		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8390607686412949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8522065064284519 | validation: 0.6965371497986694]
	TIME [epoch: 9.04 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8470758017000509		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8624211628530022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8547484822765267 | validation: 0.7719405898018638]
	TIME [epoch: 9.07 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9630701053054531		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6971205401567617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8300953227311074 | validation: 1.7321708193696712]
	TIME [epoch: 9.06 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.867637981185266		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.011951673908909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9397948275470874 | validation: 0.7454582949067847]
	TIME [epoch: 9.05 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9570148266778213		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8061597750346436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8815873008562324 | validation: 0.9687183171549699]
	TIME [epoch: 9.05 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8220407542897895		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7879350176662808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8049878859780352 | validation: 1.2104819297555443]
	TIME [epoch: 9.05 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8703752557137285		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8061445428975118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8382598993056203 | validation: 0.76556082374491]
	TIME [epoch: 9.07 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.860959629327032		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8784845132384174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8697220712827246 | validation: 0.7448044444646258]
	TIME [epoch: 9.05 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8750598823755549		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.728194210793288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8016270465844215 | validation: 0.8696495944882205]
	TIME [epoch: 9.04 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.097591644702826		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7835847789269905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9405882118149081 | validation: 0.8799703047562877]
	TIME [epoch: 9.05 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9013348269944117		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8516630499020635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8764989384482377 | validation: 0.8487086789893832]
	TIME [epoch: 9.05 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7979647567265148		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7644177521952551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7811912544608848 | validation: 1.0355534316581576]
	TIME [epoch: 9.07 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7789978580951307		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.857499174484756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8182485162899432 | validation: 1.244203993741839]
	TIME [epoch: 9.06 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9031381537195946		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8581723363908118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8806552450552034 | validation: 0.7720826291821762]
	TIME [epoch: 9.05 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8324737443113474		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9162611175272483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8743674309192977 | validation: 1.0129291403794167]
	TIME [epoch: 9.05 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0930567164320668		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.054967985958403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0740123511952349 | validation: 0.9611525302191153]
	TIME [epoch: 9.05 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.974447417300313		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8578929440078455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9161701806540794 | validation: 0.6631205948927013]
	TIME [epoch: 9.06 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7639871549869292		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8293194630668431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7966533090268861 | validation: 1.3853585171783518]
	TIME [epoch: 9.05 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9213163632654859		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8385546707719584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8799355170187221 | validation: 0.9565917805099798]
	TIME [epoch: 9.05 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7922105825770465		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7696279386612546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7809192606191505 | validation: 0.7491273810001666]
	TIME [epoch: 9.05 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8836342614860133		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8960755450906296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8898549032883215 | validation: 1.063763504836868]
	TIME [epoch: 9.05 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8697768787613782		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.964484978421319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9171309285913487 | validation: 0.6556572943953614]
	TIME [epoch: 9.07 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7683352102272406		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7226599174480272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7454975638376338 | validation: 0.7697189389416288]
	TIME [epoch: 9.06 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9028180872272051		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8049010980203388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.853859592623772 | validation: 0.58537107494727]
	TIME [epoch: 9.05 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8051160043757418		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7238136991778351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7644648517767886 | validation: 0.7372323643609088]
	TIME [epoch: 9.04 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8436016044919097		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9175679261435267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8805847653177181 | validation: 0.6982472347625616]
	TIME [epoch: 9.05 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6328640048651917		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7529302019226722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6928971033939321 | validation: 0.9976851557347417]
	TIME [epoch: 9.06 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.762803115400385		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9959655336181236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8793843245092543 | validation: 0.7414794156248521]
	TIME [epoch: 9.05 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7142127652335293		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6796649750631824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6969388701483558 | validation: 0.5847106542401894]
	TIME [epoch: 9.05 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.79580077328836		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8014826513759317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7986417123321459 | validation: 0.6519390757646195]
	TIME [epoch: 9.05 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8777424623425795		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.691221070280362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7844817663114706 | validation: 0.5917329064624569]
	TIME [epoch: 9.05 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8134229136389255		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6335915651849815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7235072394119536 | validation: 0.6521975023042513]
	TIME [epoch: 9.07 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5873075049379537		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8856362588504394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7364718818941965 | validation: 0.6756583749020204]
	TIME [epoch: 9.08 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7628217461843677		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7036932579577735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7332575020710705 | validation: 0.7356342199929158]
	TIME [epoch: 9.05 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9108361078238472		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.869577072986146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8902065904049966 | validation: 0.7732599688582985]
	TIME [epoch: 9.05 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7270674631637745		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6360732837199673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6815703734418708 | validation: 0.6263020291746014]
	TIME [epoch: 9.05 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7035296130756392		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8272023817127113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7653659973941752 | validation: 0.587172370365319]
	TIME [epoch: 9.07 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.699824739670856		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.714263788058993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7070442638649246 | validation: 0.5569054147522408]
	TIME [epoch: 9.05 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6227428859949314		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7947102568781197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7087265714365254 | validation: 0.7233530059920479]
	TIME [epoch: 9.05 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7502724779025122		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7064540855837265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7283632817431193 | validation: 0.8364904092229422]
	TIME [epoch: 9.05 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7452602828316897		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5821911059960433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6637256944138665 | validation: 0.6662796924735532]
	TIME [epoch: 9.05 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8277989328191581		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7769628801702403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8023809064946992 | validation: 0.8507936153367054]
	TIME [epoch: 9.06 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7967185862018396		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.826666647594562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8116926168982006 | validation: 0.6086779396369104]
	TIME [epoch: 9.06 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6867348757638072		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6716218609230534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6791783683434303 | validation: 0.9061970455370912]
	TIME [epoch: 9.05 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6621677276672491		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9239086294612413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7930381785642451 | validation: 1.103076286190031]
	TIME [epoch: 9.05 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6929334334083651		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7914035874717905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7421685104400779 | validation: 0.6945186829341401]
	TIME [epoch: 9.06 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6115933537262638		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7406090505313305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6761012021287972 | validation: 0.7660984077662168]
	TIME [epoch: 9.07 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7927404320848598		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8884754999611981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.840607966023029 | validation: 0.6217615990363696]
	TIME [epoch: 9.06 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6746319970120039		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6643830658862819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.669507531449143 | validation: 0.5782159119633883]
	TIME [epoch: 9.06 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5756395558303998		[learning rate: 0.0099862]
		[batch 20/20] avg loss: 0.7331829428860825		[learning rate: 0.0099709]
	Learning Rate: 0.00997088
	LOSS [training: 0.6544112493582412 | validation: 0.47450105326578795]
	TIME [epoch: 9.05 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.646352375120631		[learning rate: 0.0099556]
		[batch 20/20] avg loss: 0.6155030224545275		[learning rate: 0.0099403]
	Learning Rate: 0.00994031
	LOSS [training: 0.6309276987875793 | validation: 0.6897026178218203]
	TIME [epoch: 9.06 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6399532113980813		[learning rate: 0.0099251]
		[batch 20/20] avg loss: 0.5621847400580384		[learning rate: 0.0099098]
	Learning Rate: 0.00990984
	LOSS [training: 0.6010689757280598 | validation: 0.5779522874897296]
	TIME [epoch: 9.07 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7495372398978015		[learning rate: 0.0098946]
		[batch 20/20] avg loss: 0.5858780445745664		[learning rate: 0.0098795]
	Learning Rate: 0.00987946
	LOSS [training: 0.667707642236184 | validation: 0.5843079493037445]
	TIME [epoch: 9.06 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5709862249499792		[learning rate: 0.0098643]
		[batch 20/20] avg loss: 0.6540021151644287		[learning rate: 0.0098492]
	Learning Rate: 0.00984918
	LOSS [training: 0.6124941700572041 | validation: 0.4665722399259883]
	TIME [epoch: 9.06 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.593929286880825		[learning rate: 0.0098341]
		[batch 20/20] avg loss: 0.8638698703152283		[learning rate: 0.009819]
	Learning Rate: 0.00981899
	LOSS [training: 0.7288995785980267 | validation: 0.695839603204919]
	TIME [epoch: 9.05 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6313608736461455		[learning rate: 0.0098039]
		[batch 20/20] avg loss: 0.5421390016288506		[learning rate: 0.0097889]
	Learning Rate: 0.00978889
	LOSS [training: 0.5867499376374979 | validation: 0.582935822237404]
	TIME [epoch: 9.05 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7284869537395753		[learning rate: 0.0097739]
		[batch 20/20] avg loss: 0.6582104928034597		[learning rate: 0.0097589]
	Learning Rate: 0.00975888
	LOSS [training: 0.6933487232715174 | validation: 0.5980559264314164]
	TIME [epoch: 9.07 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7586485670341596		[learning rate: 0.0097439]
		[batch 20/20] avg loss: 0.701982036415638		[learning rate: 0.009729]
	Learning Rate: 0.00972897
	LOSS [training: 0.7303153017248988 | validation: 0.8177792960588486]
	TIME [epoch: 9.07 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.609781695011725		[learning rate: 0.009714]
		[batch 20/20] avg loss: 0.6912912080352462		[learning rate: 0.0096991]
	Learning Rate: 0.00969914
	LOSS [training: 0.6505364515234857 | validation: 0.6716523042660283]
	TIME [epoch: 9.05 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.705567989041009		[learning rate: 0.0096843]
		[batch 20/20] avg loss: 0.6354891397680607		[learning rate: 0.0096694]
	Learning Rate: 0.00966941
	LOSS [training: 0.6705285644045348 | validation: 0.6594348440730678]
	TIME [epoch: 9.05 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5729956349211951		[learning rate: 0.0096546]
		[batch 20/20] avg loss: 0.6378261187752018		[learning rate: 0.0096398]
	Learning Rate: 0.00963977
	LOSS [training: 0.6054108768481984 | validation: 0.5601820486244373]
	TIME [epoch: 9.16 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.675612962769189		[learning rate: 0.009625]
		[batch 20/20] avg loss: 0.6345847849375479		[learning rate: 0.0096102]
	Learning Rate: 0.00961022
	LOSS [training: 0.6550988738533684 | validation: 0.45968216776138415]
	TIME [epoch: 9.07 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8621566088089885		[learning rate: 0.0095955]
		[batch 20/20] avg loss: 0.7094321438701451		[learning rate: 0.0095808]
	Learning Rate: 0.00958076
	LOSS [training: 0.785794376339567 | validation: 0.6318737521884714]
	TIME [epoch: 9.06 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5979367886942616		[learning rate: 0.0095661]
		[batch 20/20] avg loss: 0.603157778734109		[learning rate: 0.0095514]
	Learning Rate: 0.00955139
	LOSS [training: 0.6005472837141853 | validation: 0.5778619596744304]
	TIME [epoch: 9.06 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6585866364454459		[learning rate: 0.0095367]
		[batch 20/20] avg loss: 0.7126911888712082		[learning rate: 0.0095221]
	Learning Rate: 0.00952211
	LOSS [training: 0.6856389126583271 | validation: 0.5422160986810093]
	TIME [epoch: 9.05 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7195074392837933		[learning rate: 0.0095075]
		[batch 20/20] avg loss: 0.6598448545541984		[learning rate: 0.0094929]
	Learning Rate: 0.00949292
	LOSS [training: 0.6896761469189958 | validation: 0.5536929425882264]
	TIME [epoch: 9.05 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5683122730164463		[learning rate: 0.0094784]
		[batch 20/20] avg loss: 0.5372504261572435		[learning rate: 0.0094638]
	Learning Rate: 0.00946382
	LOSS [training: 0.5527813495868448 | validation: 0.9073696709957905]
	TIME [epoch: 9.07 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6921995414620039		[learning rate: 0.0094493]
		[batch 20/20] avg loss: 0.720779979408069		[learning rate: 0.0094348]
	Learning Rate: 0.00943481
	LOSS [training: 0.7064897604350364 | validation: 0.6500294529404409]
	TIME [epoch: 9.07 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8864647101658333		[learning rate: 0.0094203]
		[batch 20/20] avg loss: 0.5987778183208111		[learning rate: 0.0094059]
	Learning Rate: 0.00940589
	LOSS [training: 0.7426212642433223 | validation: 0.6822955109982284]
	TIME [epoch: 9.05 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3364681371702456		[learning rate: 0.0093915]
		[batch 20/20] avg loss: 0.9990616660530607		[learning rate: 0.0093771]
	Learning Rate: 0.00937706
	LOSS [training: 1.1677649016116534 | validation: 0.6492464749501105]
	TIME [epoch: 9.05 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.639589514663142		[learning rate: 0.0093627]
		[batch 20/20] avg loss: 0.6726242404759758		[learning rate: 0.0093483]
	Learning Rate: 0.00934831
	LOSS [training: 0.6561068775695589 | validation: 0.5474807582278716]
	TIME [epoch: 9.05 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6025697593257014		[learning rate: 0.009334]
		[batch 20/20] avg loss: 0.7267895806587065		[learning rate: 0.0093197]
	Learning Rate: 0.00931966
	LOSS [training: 0.6646796699922041 | validation: 0.5408743291918618]
	TIME [epoch: 9.07 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5167700790732412		[learning rate: 0.0093054]
		[batch 20/20] avg loss: 0.6991535162582239		[learning rate: 0.0092911]
	Learning Rate: 0.00929109
	LOSS [training: 0.6079617976657327 | validation: 0.7154309729559953]
	TIME [epoch: 9.06 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7536071520320766		[learning rate: 0.0092768]
		[batch 20/20] avg loss: 0.59086029566102		[learning rate: 0.0092626]
	Learning Rate: 0.00926261
	LOSS [training: 0.6722337238465481 | validation: 0.4633462294603748]
	TIME [epoch: 9.05 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6481826076275378		[learning rate: 0.0092484]
		[batch 20/20] avg loss: 0.6741856481127675		[learning rate: 0.0092342]
	Learning Rate: 0.00923422
	LOSS [training: 0.6611841278701526 | validation: 0.8040037728161136]
	TIME [epoch: 9.05 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6371721778055905		[learning rate: 0.0092201]
		[batch 20/20] avg loss: 0.6133190716484865		[learning rate: 0.0092059]
	Learning Rate: 0.00920591
	LOSS [training: 0.6252456247270386 | validation: 0.5408012571498573]
	TIME [epoch: 9.05 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5185855320408775		[learning rate: 0.0091918]
		[batch 20/20] avg loss: 0.589057976060567		[learning rate: 0.0091777]
	Learning Rate: 0.00917769
	LOSS [training: 0.5538217540507222 | validation: 0.6234866632871146]
	TIME [epoch: 9.06 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7224503009939867		[learning rate: 0.0091636]
		[batch 20/20] avg loss: 0.7344940252058568		[learning rate: 0.0091496]
	Learning Rate: 0.00914956
	LOSS [training: 0.7284721630999218 | validation: 0.630019223694772]
	TIME [epoch: 9.07 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6777256269997525		[learning rate: 0.0091355]
		[batch 20/20] avg loss: 0.6012211011465556		[learning rate: 0.0091215]
	Learning Rate: 0.00912151
	LOSS [training: 0.639473364073154 | validation: 0.542947197289851]
	TIME [epoch: 9.06 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5455953925747161		[learning rate: 0.0091075]
		[batch 20/20] avg loss: 0.5798779106861754		[learning rate: 0.0090935]
	Learning Rate: 0.00909355
	LOSS [training: 0.5627366516304458 | validation: 0.5117079701048459]
	TIME [epoch: 9.06 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6818159496999738		[learning rate: 0.0090796]
		[batch 20/20] avg loss: 0.855838112695411		[learning rate: 0.0090657]
	Learning Rate: 0.00906567
	LOSS [training: 0.7688270311976924 | validation: 0.9177683779256142]
	TIME [epoch: 9.06 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9046722120615293		[learning rate: 0.0090518]
		[batch 20/20] avg loss: 0.7606823745324025		[learning rate: 0.0090379]
	Learning Rate: 0.00903788
	LOSS [training: 0.8326772932969659 | validation: 0.6611273303373808]
	TIME [epoch: 9.07 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6659752089352583		[learning rate: 0.009024]
		[batch 20/20] avg loss: 0.6153699816426905		[learning rate: 0.0090102]
	Learning Rate: 0.00901018
	LOSS [training: 0.6406725952889744 | validation: 0.5255097673478977]
	TIME [epoch: 9.07 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5783322924057771		[learning rate: 0.0089964]
		[batch 20/20] avg loss: 0.6990928042416007		[learning rate: 0.0089826]
	Learning Rate: 0.00898256
	LOSS [training: 0.6387125483236888 | validation: 0.5120127821032041]
	TIME [epoch: 9.06 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7879647946387557		[learning rate: 0.0089688]
		[batch 20/20] avg loss: 0.5726080955098801		[learning rate: 0.008955]
	Learning Rate: 0.00895502
	LOSS [training: 0.6802864450743179 | validation: 0.569661328042095]
	TIME [epoch: 9.05 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6323981764816591		[learning rate: 0.0089413]
		[batch 20/20] avg loss: 0.711657166419173		[learning rate: 0.0089276]
	Learning Rate: 0.00892757
	LOSS [training: 0.672027671450416 | validation: 0.610004940828694]
	TIME [epoch: 9.05 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6651460307128245		[learning rate: 0.0089139]
		[batch 20/20] avg loss: 0.5584638771980039		[learning rate: 0.0089002]
	Learning Rate: 0.0089002
	LOSS [training: 0.6118049539554143 | validation: 0.5850735124755262]
	TIME [epoch: 9.07 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6425700326423632		[learning rate: 0.0088866]
		[batch 20/20] avg loss: 0.5246281463312108		[learning rate: 0.0088729]
	Learning Rate: 0.00887292
	LOSS [training: 0.5835990894867871 | validation: 0.5721252597862209]
	TIME [epoch: 9.07 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7473541434620381		[learning rate: 0.0088593]
		[batch 20/20] avg loss: 0.7883103428896415		[learning rate: 0.0088457]
	Learning Rate: 0.00884572
	LOSS [training: 0.76783224317584 | validation: 0.6420289870929532]
	TIME [epoch: 9.05 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7150907724246853		[learning rate: 0.0088322]
		[batch 20/20] avg loss: 0.5807314710360773		[learning rate: 0.0088186]
	Learning Rate: 0.00881861
	LOSS [training: 0.6479111217303815 | validation: 0.6267567855735079]
	TIME [epoch: 9.05 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7543293812094434		[learning rate: 0.0088051]
		[batch 20/20] avg loss: 0.6626367608227784		[learning rate: 0.0087916]
	Learning Rate: 0.00879157
	LOSS [training: 0.7084830710161111 | validation: 0.7733323037644382]
	TIME [epoch: 9.06 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7456164530493854		[learning rate: 0.0087781]
		[batch 20/20] avg loss: 0.596201947045964		[learning rate: 0.0087646]
	Learning Rate: 0.00876462
	LOSS [training: 0.6709092000476746 | validation: 0.7104509965800772]
	TIME [epoch: 9.07 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6147514687958198		[learning rate: 0.0087512]
		[batch 20/20] avg loss: 0.6188505805186922		[learning rate: 0.0087378]
	Learning Rate: 0.00873776
	LOSS [training: 0.6168010246572561 | validation: 0.6569897868292347]
	TIME [epoch: 9.08 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5749848621069773		[learning rate: 0.0087244]
		[batch 20/20] avg loss: 0.6253966244995112		[learning rate: 0.008711]
	Learning Rate: 0.00871097
	LOSS [training: 0.6001907433032441 | validation: 0.6718437923287467]
	TIME [epoch: 9.05 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8233671711086015		[learning rate: 0.0086976]
		[batch 20/20] avg loss: 0.5091593965432286		[learning rate: 0.0086843]
	Learning Rate: 0.00868427
	LOSS [training: 0.666263283825915 | validation: 0.6593632324732277]
	TIME [epoch: 9.05 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6085507746020712		[learning rate: 0.0086709]
		[batch 20/20] avg loss: 0.9371338680917752		[learning rate: 0.0086576]
	Learning Rate: 0.00865765
	LOSS [training: 0.7728423213469232 | validation: 0.5965842949728034]
	TIME [epoch: 9.06 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7334453917511092		[learning rate: 0.0086444]
		[batch 20/20] avg loss: 0.49737812679819526		[learning rate: 0.0086311]
	Learning Rate: 0.00863111
	LOSS [training: 0.6154117592746522 | validation: 0.551643275390402]
	TIME [epoch: 9.06 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6934172350264147		[learning rate: 0.0086179]
		[batch 20/20] avg loss: 0.5959812180053177		[learning rate: 0.0086047]
	Learning Rate: 0.00860465
	LOSS [training: 0.6446992265158663 | validation: 0.5044541314381823]
	TIME [epoch: 9.09 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49865049620817103		[learning rate: 0.0085915]
		[batch 20/20] avg loss: 0.6265717173176506		[learning rate: 0.0085783]
	Learning Rate: 0.00857828
	LOSS [training: 0.5626111067629108 | validation: 0.4553051286343727]
	TIME [epoch: 9.06 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5777707666225883		[learning rate: 0.0085651]
		[batch 20/20] avg loss: 0.4911300453136248		[learning rate: 0.008552]
	Learning Rate: 0.00855198
	LOSS [training: 0.5344504059681066 | validation: 0.5869651953136625]
	TIME [epoch: 9.06 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6459644191712001		[learning rate: 0.0085389]
		[batch 20/20] avg loss: 0.5786704230006968		[learning rate: 0.0085258]
	Learning Rate: 0.00852576
	LOSS [training: 0.6123174210859484 | validation: 0.5970642533413172]
	TIME [epoch: 9.06 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.583466675748403		[learning rate: 0.0085127]
		[batch 20/20] avg loss: 0.5108907506598135		[learning rate: 0.0084996]
	Learning Rate: 0.00849963
	LOSS [training: 0.5471787132041082 | validation: 0.6038481409390326]
	TIME [epoch: 9.06 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.665230097084139		[learning rate: 0.0084866]
		[batch 20/20] avg loss: 0.5147231576025194		[learning rate: 0.0084736]
	Learning Rate: 0.00847358
	LOSS [training: 0.5899766273433293 | validation: 0.873766204991587]
	TIME [epoch: 9.09 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6435950981212606		[learning rate: 0.0084606]
		[batch 20/20] avg loss: 0.5815211870292541		[learning rate: 0.0084476]
	Learning Rate: 0.0084476
	LOSS [training: 0.6125581425752575 | validation: 0.6344294239808378]
	TIME [epoch: 9.06 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5426517570013875		[learning rate: 0.0084346]
		[batch 20/20] avg loss: 0.6411553012925474		[learning rate: 0.0084217]
	Learning Rate: 0.0084217
	LOSS [training: 0.5919035291469674 | validation: 0.44865857939444637]
	TIME [epoch: 9.06 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6012937061479019		[learning rate: 0.0084088]
		[batch 20/20] avg loss: 0.5107822659937965		[learning rate: 0.0083959]
	Learning Rate: 0.00839589
	LOSS [training: 0.5560379860708493 | validation: 1.145500181205516]
	TIME [epoch: 9.06 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6478084324397471		[learning rate: 0.008383]
		[batch 20/20] avg loss: 0.6550923860418283		[learning rate: 0.0083702]
	Learning Rate: 0.00837015
	LOSS [training: 0.6514504092407878 | validation: 0.5657355090185444]
	TIME [epoch: 9.06 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6102702001177895		[learning rate: 0.0083573]
		[batch 20/20] avg loss: 0.553075661990545		[learning rate: 0.0083445]
	Learning Rate: 0.00834449
	LOSS [training: 0.5816729310541673 | validation: 0.5720257610083357]
	TIME [epoch: 9.08 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.192325939790371		[learning rate: 0.0083317]
		[batch 20/20] avg loss: 0.637428478789966		[learning rate: 0.0083189]
	Learning Rate: 0.00831891
	LOSS [training: 0.9148772092901686 | validation: 0.6419185843290405]
	TIME [epoch: 9.05 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6539760956546347		[learning rate: 0.0083062]
		[batch 20/20] avg loss: 0.5562377216329024		[learning rate: 0.0082934]
	Learning Rate: 0.00829341
	LOSS [training: 0.6051069086437684 | validation: 0.48213401398998523]
	TIME [epoch: 9.06 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4823694759074166		[learning rate: 0.0082807]
		[batch 20/20] avg loss: 0.6218744778564146		[learning rate: 0.008268]
	Learning Rate: 0.00826799
	LOSS [training: 0.5521219768819153 | validation: 0.546663629891492]
	TIME [epoch: 9.06 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6307693935867664		[learning rate: 0.0082553]
		[batch 20/20] avg loss: 0.5793625292889153		[learning rate: 0.0082426]
	Learning Rate: 0.00824265
	LOSS [training: 0.6050659614378409 | validation: 0.5499309078726475]
	TIME [epoch: 9.06 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6060147394285493		[learning rate: 0.00823]
		[batch 20/20] avg loss: 0.6393309733780425		[learning rate: 0.0082174]
	Learning Rate: 0.00821738
	LOSS [training: 0.6226728564032961 | validation: 0.5249179849064198]
	TIME [epoch: 9.08 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6341949783596683		[learning rate: 0.0082048]
		[batch 20/20] avg loss: 0.5241161350983845		[learning rate: 0.0081922]
	Learning Rate: 0.00819219
	LOSS [training: 0.5791555567290264 | validation: 0.48502278296063456]
	TIME [epoch: 9.05 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5797873186920244		[learning rate: 0.0081796]
		[batch 20/20] avg loss: 0.6060098782841844		[learning rate: 0.0081671]
	Learning Rate: 0.00816708
	LOSS [training: 0.5928985984881044 | validation: 0.4072527592651708]
	TIME [epoch: 9.05 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6258528982434559		[learning rate: 0.0081545]
		[batch 20/20] avg loss: 0.5963427184502481		[learning rate: 0.008142]
	Learning Rate: 0.00814204
	LOSS [training: 0.611097808346852 | validation: 0.5870858678084011]
	TIME [epoch: 9.05 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6287981875508759		[learning rate: 0.0081296]
		[batch 20/20] avg loss: 0.5220709353911032		[learning rate: 0.0081171]
	Learning Rate: 0.00811708
	LOSS [training: 0.5754345614709896 | validation: 0.5561226514893406]
	TIME [epoch: 9.07 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5071398432484107		[learning rate: 0.0081046]
		[batch 20/20] avg loss: 0.4894251741845547		[learning rate: 0.0080922]
	Learning Rate: 0.0080922
	LOSS [training: 0.49828250871648266 | validation: 1.0421128606424683]
	TIME [epoch: 9.08 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5404656400461281		[learning rate: 0.0080798]
		[batch 20/20] avg loss: 0.4459811935274501		[learning rate: 0.0080674]
	Learning Rate: 0.00806739
	LOSS [training: 0.49322341678678905 | validation: 0.47895197875564943]
	TIME [epoch: 9.05 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5041820090753009		[learning rate: 0.008055]
		[batch 20/20] avg loss: 0.4338824183266567		[learning rate: 0.0080427]
	Learning Rate: 0.00804267
	LOSS [training: 0.46903221370097886 | validation: 0.41014162054485653]
	TIME [epoch: 9.05 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47127462729559405		[learning rate: 0.0080303]
		[batch 20/20] avg loss: 0.4871257025309926		[learning rate: 0.008018]
	Learning Rate: 0.00801801
	LOSS [training: 0.4792001649132933 | validation: 0.56365679169382]
	TIME [epoch: 9.06 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44733343779980983		[learning rate: 0.0080057]
		[batch 20/20] avg loss: 0.6558757935649233		[learning rate: 0.0079934]
	Learning Rate: 0.00799343
	LOSS [training: 0.5516046156823666 | validation: 0.4564313449159303]
	TIME [epoch: 9.05 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45582328296798574		[learning rate: 0.0079812]
		[batch 20/20] avg loss: 0.346024250712186		[learning rate: 0.0079689]
	Learning Rate: 0.00796893
	LOSS [training: 0.4009237668400859 | validation: 0.3842514750698587]
	TIME [epoch: 9.08 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32634794863120514		[learning rate: 0.0079567]
		[batch 20/20] avg loss: 0.4182580370186284		[learning rate: 0.0079445]
	Learning Rate: 0.0079445
	LOSS [training: 0.37230299282491675 | validation: 0.28654575110028874]
	TIME [epoch: 9.04 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3650460176304612		[learning rate: 0.0079323]
		[batch 20/20] avg loss: 0.407890399508994		[learning rate: 0.0079201]
	Learning Rate: 0.00792015
	LOSS [training: 0.3864682085697276 | validation: 0.39629592103761124]
	TIME [epoch: 9.05 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33669024721745566		[learning rate: 0.007908]
		[batch 20/20] avg loss: 0.34357384014992937		[learning rate: 0.0078959]
	Learning Rate: 0.00789587
	LOSS [training: 0.3401320436836925 | validation: 0.2821018977320062]
	TIME [epoch: 9.04 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3731621367008238		[learning rate: 0.0078838]
		[batch 20/20] avg loss: 0.43161170861648523		[learning rate: 0.0078717]
	Learning Rate: 0.00787167
	LOSS [training: 0.40238692265865444 | validation: 0.2130876526091301]
	TIME [epoch: 9.05 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35155846391090045		[learning rate: 0.0078596]
		[batch 20/20] avg loss: 0.30639173242169077		[learning rate: 0.0078475]
	Learning Rate: 0.00784754
	LOSS [training: 0.3289750981662956 | validation: 0.21730675718082387]
	TIME [epoch: 9.07 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34744396612076306		[learning rate: 0.0078355]
		[batch 20/20] avg loss: 0.3679688172385177		[learning rate: 0.0078235]
	Learning Rate: 0.00782348
	LOSS [training: 0.35770639167964025 | validation: 0.24601092405519187]
	TIME [epoch: 9.05 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3173355774961306		[learning rate: 0.0078115]
		[batch 20/20] avg loss: 0.38544650992854856		[learning rate: 0.0077995]
	Learning Rate: 0.0077995
	LOSS [training: 0.35139104371233965 | validation: 0.2374592527884944]
	TIME [epoch: 9.05 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3757976410942431		[learning rate: 0.0077875]
		[batch 20/20] avg loss: 0.3665519099416915		[learning rate: 0.0077756]
	Learning Rate: 0.00777559
	LOSS [training: 0.37117477551796735 | validation: 0.38024022465025425]
	TIME [epoch: 9.05 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34798736862720947		[learning rate: 0.0077637]
		[batch 20/20] avg loss: 0.35427684802814713		[learning rate: 0.0077518]
	Learning Rate: 0.00775175
	LOSS [training: 0.3511321083276783 | validation: 0.19937652222260938]
	TIME [epoch: 9.05 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4253289380777593		[learning rate: 0.0077399]
		[batch 20/20] avg loss: 0.39016862580871703		[learning rate: 0.007728]
	Learning Rate: 0.00772799
	LOSS [training: 0.4077487819432382 | validation: 0.2851231229109522]
	TIME [epoch: 9.08 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3702876068548767		[learning rate: 0.0077161]
		[batch 20/20] avg loss: 0.36609717305307043		[learning rate: 0.0077043]
	Learning Rate: 0.0077043
	LOSS [training: 0.3681923899539736 | validation: 0.39587982068051686]
	TIME [epoch: 9.05 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3670215444612805		[learning rate: 0.0076925]
		[batch 20/20] avg loss: 0.4150724070638698		[learning rate: 0.0076807]
	Learning Rate: 0.00768069
	LOSS [training: 0.39104697576257513 | validation: 0.28815841271557335]
	TIME [epoch: 9.05 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4131993633461476		[learning rate: 0.0076689]
		[batch 20/20] avg loss: 0.3269571293228647		[learning rate: 0.0076571]
	Learning Rate: 0.00765714
	LOSS [training: 0.37007824633450614 | validation: 0.2754385280413557]
	TIME [epoch: 9.05 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3339337360771094		[learning rate: 0.0076454]
		[batch 20/20] avg loss: 0.393044616456628		[learning rate: 0.0076337]
	Learning Rate: 0.00763367
	LOSS [training: 0.3634891762668687 | validation: 0.36162978244843363]
	TIME [epoch: 9.05 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33634660294490015		[learning rate: 0.007622]
		[batch 20/20] avg loss: 0.4120207651878591		[learning rate: 0.0076103]
	Learning Rate: 0.00761027
	LOSS [training: 0.37418368406637964 | validation: 0.6237425221517593]
	TIME [epoch: 9.08 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3160966002079292		[learning rate: 0.0075986]
		[batch 20/20] avg loss: 0.36767154102618704		[learning rate: 0.0075869]
	Learning Rate: 0.00758694
	LOSS [training: 0.341884070617058 | validation: 0.38868845326956003]
	TIME [epoch: 9.87 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4509847901741682		[learning rate: 0.0075753]
		[batch 20/20] avg loss: 0.4852291801663431		[learning rate: 0.0075637]
	Learning Rate: 0.00756368
	LOSS [training: 0.4681069851702556 | validation: 0.6227135605533998]
	TIME [epoch: 9.05 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4633001442043769		[learning rate: 0.0075521]
		[batch 20/20] avg loss: 0.4877931953745966		[learning rate: 0.0075405]
	Learning Rate: 0.0075405
	LOSS [training: 0.47554666978948684 | validation: 0.3460292392338532]
	TIME [epoch: 9.05 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3126180119843701		[learning rate: 0.0075289]
		[batch 20/20] avg loss: 0.407623633134141		[learning rate: 0.0075174]
	Learning Rate: 0.00751738
	LOSS [training: 0.3601208225592555 | validation: 0.3477928092211156]
	TIME [epoch: 9.05 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43447674687312476		[learning rate: 0.0075059]
		[batch 20/20] avg loss: 0.3367339913374951		[learning rate: 0.0074943]
	Learning Rate: 0.00749434
	LOSS [training: 0.38560536910530996 | validation: 0.3361578109040068]
	TIME [epoch: 9.08 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2875299812395952		[learning rate: 0.0074828]
		[batch 20/20] avg loss: 0.3216513528058988		[learning rate: 0.0074714]
	Learning Rate: 0.00747137
	LOSS [training: 0.30459066702274706 | validation: 0.30697803038487936]
	TIME [epoch: 9.05 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2872542878557824		[learning rate: 0.0074599]
		[batch 20/20] avg loss: 0.3788597403537217		[learning rate: 0.0074485]
	Learning Rate: 0.00744846
	LOSS [training: 0.333057014104752 | validation: 0.3918412197905051]
	TIME [epoch: 9.06 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4748113400615458		[learning rate: 0.007437]
		[batch 20/20] avg loss: 0.3260491702841118		[learning rate: 0.0074256]
	Learning Rate: 0.00742563
	LOSS [training: 0.4004302551728288 | validation: 0.17859268849424756]
	TIME [epoch: 9.05 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2714523080282926		[learning rate: 0.0074142]
		[batch 20/20] avg loss: 0.3259810548617801		[learning rate: 0.0074029]
	Learning Rate: 0.00740287
	LOSS [training: 0.2987166814450363 | validation: 0.2027904369423097]
	TIME [epoch: 9.05 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3001732211246727		[learning rate: 0.0073915]
		[batch 20/20] avg loss: 0.28460430905554185		[learning rate: 0.0073802]
	Learning Rate: 0.00738017
	LOSS [training: 0.29238876509010725 | validation: 0.22571908144119063]
	TIME [epoch: 9.07 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.427945702712778		[learning rate: 0.0073689]
		[batch 20/20] avg loss: 0.36778118931260756		[learning rate: 0.0073576]
	Learning Rate: 0.00735755
	LOSS [training: 0.39786344601269275 | validation: 0.2230849279572263]
	TIME [epoch: 9.17 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4014366497357562		[learning rate: 0.0073463]
		[batch 20/20] avg loss: 0.3311274025304492		[learning rate: 0.007335]
	Learning Rate: 0.007335
	LOSS [training: 0.3662820261331027 | validation: 0.15866257697759478]
	TIME [epoch: 9.05 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34980085168743963		[learning rate: 0.0073237]
		[batch 20/20] avg loss: 0.2935449722961043		[learning rate: 0.0073125]
	Learning Rate: 0.00731251
	LOSS [training: 0.32167291199177195 | validation: 0.4922487425107126]
	TIME [epoch: 9.05 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4431790290764381		[learning rate: 0.0073013]
		[batch 20/20] avg loss: 0.3414777622658334		[learning rate: 0.0072901]
	Learning Rate: 0.0072901
	LOSS [training: 0.39232839567113575 | validation: 0.18558969311916831]
	TIME [epoch: 9.05 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3057489541585951		[learning rate: 0.0072789]
		[batch 20/20] avg loss: 0.3181005846345307		[learning rate: 0.0072678]
	Learning Rate: 0.00726775
	LOSS [training: 0.3119247693965629 | validation: 0.2645113594915002]
	TIME [epoch: 9.07 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3418343589305596		[learning rate: 0.0072566]
		[batch 20/20] avg loss: 0.3179780997328274		[learning rate: 0.0072455]
	Learning Rate: 0.00724547
	LOSS [training: 0.32990622933169356 | validation: 0.26957635184641515]
	TIME [epoch: 9.06 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27976850116876884		[learning rate: 0.0072344]
		[batch 20/20] avg loss: 0.27714897971463126		[learning rate: 0.0072233]
	Learning Rate: 0.00722326
	LOSS [training: 0.27845874044170005 | validation: 0.5642257777192907]
	TIME [epoch: 9.05 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3559747744160805		[learning rate: 0.0072122]
		[batch 20/20] avg loss: 0.43180135344491044		[learning rate: 0.0072011]
	Learning Rate: 0.00720112
	LOSS [training: 0.39388806393049547 | validation: 0.2948461456716422]
	TIME [epoch: 9.05 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41273410147696926		[learning rate: 0.0071901]
		[batch 20/20] avg loss: 0.32253360792454283		[learning rate: 0.007179]
	Learning Rate: 0.00717904
	LOSS [training: 0.36763385470075605 | validation: 0.22896823105076755]
	TIME [epoch: 9.05 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31306789257450296		[learning rate: 0.007168]
		[batch 20/20] avg loss: 0.3736656938750602		[learning rate: 0.007157]
	Learning Rate: 0.00715704
	LOSS [training: 0.3433667932247816 | validation: 0.22741929707000294]
	TIME [epoch: 9.06 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3715947089982681		[learning rate: 0.0071461]
		[batch 20/20] avg loss: 0.3053134590241486		[learning rate: 0.0071351]
	Learning Rate: 0.0071351
	LOSS [training: 0.33845408401120836 | validation: 0.2109226468241392]
	TIME [epoch: 9.05 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3388873958430184		[learning rate: 0.0071242]
		[batch 20/20] avg loss: 0.3456051425353617		[learning rate: 0.0071132]
	Learning Rate: 0.00711323
	LOSS [training: 0.34224626918919 | validation: 0.31441773125062344]
	TIME [epoch: 9.04 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3368782893955974		[learning rate: 0.0071023]
		[batch 20/20] avg loss: 0.3170998558570247		[learning rate: 0.0070914]
	Learning Rate: 0.00709142
	LOSS [training: 0.3269890726263111 | validation: 0.29737097362293063]
	TIME [epoch: 9.05 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2588873876894362		[learning rate: 0.0070805]
		[batch 20/20] avg loss: 0.3331986655706835		[learning rate: 0.0070697]
	Learning Rate: 0.00706968
	LOSS [training: 0.2960430266300599 | validation: 0.26390841612108745]
	TIME [epoch: 9.05 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2678488646365787		[learning rate: 0.0070588]
		[batch 20/20] avg loss: 0.47527158896157484		[learning rate: 0.007048]
	Learning Rate: 0.00704801
	LOSS [training: 0.37156022679907674 | validation: 0.15053703377736877]
	TIME [epoch: 9.07 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3707931974999106		[learning rate: 0.0070372]
		[batch 20/20] avg loss: 0.34025997384484874		[learning rate: 0.0070264]
	Learning Rate: 0.00702641
	LOSS [training: 0.3555265856723796 | validation: 0.4784667189154213]
	TIME [epoch: 9.05 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29074790499997577		[learning rate: 0.0070156]
		[batch 20/20] avg loss: 0.355293173966569		[learning rate: 0.0070049]
	Learning Rate: 0.00700487
	LOSS [training: 0.3230205394832724 | validation: 0.29329397224949605]
	TIME [epoch: 9.04 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34247670330596425		[learning rate: 0.0069941]
		[batch 20/20] avg loss: 0.31782503401354206		[learning rate: 0.0069834]
	Learning Rate: 0.0069834
	LOSS [training: 0.3301508686597531 | validation: 0.2776104561694633]
	TIME [epoch: 9.05 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2873181888678039		[learning rate: 0.0069727]
		[batch 20/20] avg loss: 0.31966591079228435		[learning rate: 0.006962]
	Learning Rate: 0.00696199
	LOSS [training: 0.30349204983004413 | validation: 0.1674530636361363]
	TIME [epoch: 9.04 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2693695927459811		[learning rate: 0.0069513]
		[batch 20/20] avg loss: 0.2892641250373026		[learning rate: 0.0069406]
	Learning Rate: 0.00694065
	LOSS [training: 0.27931685889164193 | validation: 0.19700276180920825]
	TIME [epoch: 9.07 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29046298074389415		[learning rate: 0.00693]
		[batch 20/20] avg loss: 0.3247442476412588		[learning rate: 0.0069194]
	Learning Rate: 0.00691937
	LOSS [training: 0.3076036141925764 | validation: 0.4763112920646284]
	TIME [epoch: 9.05 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3599087249637901		[learning rate: 0.0069088]
		[batch 20/20] avg loss: 0.3676805201718186		[learning rate: 0.0068982]
	Learning Rate: 0.00689816
	LOSS [training: 0.3637946225678044 | validation: 0.32520425456007973]
	TIME [epoch: 9.05 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36541223847960524		[learning rate: 0.0068876]
		[batch 20/20] avg loss: 0.3204972050313395		[learning rate: 0.006877]
	Learning Rate: 0.00687702
	LOSS [training: 0.34295472175547237 | validation: 0.3721884972669846]
	TIME [epoch: 9.04 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3670381233799961		[learning rate: 0.0068665]
		[batch 20/20] avg loss: 0.2522047250020444		[learning rate: 0.0068559]
	Learning Rate: 0.00685593
	LOSS [training: 0.30962142419102023 | validation: 0.18786426683747706]
	TIME [epoch: 9.05 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42943694770994495		[learning rate: 0.0068454]
		[batch 20/20] avg loss: 0.2377305221367772		[learning rate: 0.0068349]
	Learning Rate: 0.00683492
	LOSS [training: 0.33358373492336096 | validation: 0.22973277832125577]
	TIME [epoch: 9.06 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27870222276377854		[learning rate: 0.0068244]
		[batch 20/20] avg loss: 0.2658200394396185		[learning rate: 0.006814]
	Learning Rate: 0.00681397
	LOSS [training: 0.2722611311016985 | validation: 0.3750244820459523]
	TIME [epoch: 9.05 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3841971127551267		[learning rate: 0.0068035]
		[batch 20/20] avg loss: 0.27847174592530316		[learning rate: 0.0067931]
	Learning Rate: 0.00679308
	LOSS [training: 0.3313344293402149 | validation: 0.755420760966793]
	TIME [epoch: 9.05 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3094187303829681		[learning rate: 0.0067827]
		[batch 20/20] avg loss: 0.2998226050023753		[learning rate: 0.0067723]
	Learning Rate: 0.00677225
	LOSS [training: 0.30462066769267165 | validation: 0.18757146764962312]
	TIME [epoch: 9.05 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2699579853379027		[learning rate: 0.0067619]
		[batch 20/20] avg loss: 0.39669622506531393		[learning rate: 0.0067515]
	Learning Rate: 0.0067515
	LOSS [training: 0.33332710520160835 | validation: 0.4452437603170506]
	TIME [epoch: 9.05 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33860519287363633		[learning rate: 0.0067411]
		[batch 20/20] avg loss: 0.2381166821881092		[learning rate: 0.0067308]
	Learning Rate: 0.0067308
	LOSS [training: 0.2883609375308728 | validation: 0.3404858925695432]
	TIME [epoch: 9.07 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28966727339081916		[learning rate: 0.0067205]
		[batch 20/20] avg loss: 0.3502058563610999		[learning rate: 0.0067102]
	Learning Rate: 0.00671017
	LOSS [training: 0.3199365648759595 | validation: 0.2124400672576747]
	TIME [epoch: 9.05 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26695834329253776		[learning rate: 0.0066999]
		[batch 20/20] avg loss: 0.3682505002735358		[learning rate: 0.0066896]
	Learning Rate: 0.0066896
	LOSS [training: 0.3176044217830368 | validation: 0.23384218030594128]
	TIME [epoch: 9.06 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3672729208102594		[learning rate: 0.0066793]
		[batch 20/20] avg loss: 0.2793539224325038		[learning rate: 0.0066691]
	Learning Rate: 0.00666909
	LOSS [training: 0.32331342162138155 | validation: 0.2301854710575319]
	TIME [epoch: 9.05 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3010209444228217		[learning rate: 0.0066589]
		[batch 20/20] avg loss: 0.3409370621246876		[learning rate: 0.0066486]
	Learning Rate: 0.00664865
	LOSS [training: 0.3209790032737546 | validation: 0.3134981261013566]
	TIME [epoch: 9.35 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2970442170250432		[learning rate: 0.0066384]
		[batch 20/20] avg loss: 0.3130318735033082		[learning rate: 0.0066283]
	Learning Rate: 0.00662827
	LOSS [training: 0.30503804526417566 | validation: 0.22803902280828284]
	TIME [epoch: 9.07 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3142228741939355		[learning rate: 0.0066181]
		[batch 20/20] avg loss: 0.2735824952145775		[learning rate: 0.0066079]
	Learning Rate: 0.00660795
	LOSS [training: 0.2939026847042565 | validation: 0.2720446318079899]
	TIME [epoch: 9.05 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25670276690156413		[learning rate: 0.0065978]
		[batch 20/20] avg loss: 0.24444411981545686		[learning rate: 0.0065877]
	Learning Rate: 0.00658769
	LOSS [training: 0.2505734433585105 | validation: 0.36099464563510264]
	TIME [epoch: 9.05 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3739746108260866		[learning rate: 0.0065776]
		[batch 20/20] avg loss: 0.22177042666505087		[learning rate: 0.0065675]
	Learning Rate: 0.0065675
	LOSS [training: 0.29787251874556886 | validation: 0.2263701727380865]
	TIME [epoch: 9.04 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27674907697793794		[learning rate: 0.0065574]
		[batch 20/20] avg loss: 0.3888023569174661		[learning rate: 0.0065474]
	Learning Rate: 0.00654737
	LOSS [training: 0.3327757169477021 | validation: 0.32094062749671165]
	TIME [epoch: 9.05 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2957676904722056		[learning rate: 0.0065373]
		[batch 20/20] avg loss: 0.36964653805461667		[learning rate: 0.0065273]
	Learning Rate: 0.0065273
	LOSS [training: 0.3327071142634111 | validation: 0.49200119895104816]
	TIME [epoch: 9.07 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29587626595370103		[learning rate: 0.0065173]
		[batch 20/20] avg loss: 0.28177131638117614		[learning rate: 0.0065073]
	Learning Rate: 0.00650729
	LOSS [training: 0.2888237911674386 | validation: 0.2140468722229879]
	TIME [epoch: 9.06 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.287105856589418		[learning rate: 0.0064973]
		[batch 20/20] avg loss: 0.2516241481101743		[learning rate: 0.0064873]
	Learning Rate: 0.00648734
	LOSS [training: 0.26936500234979616 | validation: 0.11452868468279981]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_641.pth
	Model improved!!!
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2451566827993808		[learning rate: 0.0064774]
		[batch 20/20] avg loss: 0.31293228455502253		[learning rate: 0.0064675]
	Learning Rate: 0.00646745
	LOSS [training: 0.27904448367720164 | validation: 0.2389012619449064]
	TIME [epoch: 9.04 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24904244068859885		[learning rate: 0.0064575]
		[batch 20/20] avg loss: 0.2756063493157335		[learning rate: 0.0064476]
	Learning Rate: 0.00644763
	LOSS [training: 0.2623243950021662 | validation: 0.16766930777726868]
	TIME [epoch: 9.06 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23194521899020387		[learning rate: 0.0064377]
		[batch 20/20] avg loss: 0.34534389938678534		[learning rate: 0.0064279]
	Learning Rate: 0.00642786
	LOSS [training: 0.28864455918849463 | validation: 0.26387914828238707]
	TIME [epoch: 9.07 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22293881977778338		[learning rate: 0.006418]
		[batch 20/20] avg loss: 0.28571063444450107		[learning rate: 0.0064082]
	Learning Rate: 0.00640816
	LOSS [training: 0.25432472711114223 | validation: 0.23435242379120588]
	TIME [epoch: 9.06 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2158373045381067		[learning rate: 0.0063983]
		[batch 20/20] avg loss: 0.23696144637353087		[learning rate: 0.0063885]
	Learning Rate: 0.00638852
	LOSS [training: 0.22639937545581884 | validation: 0.18202827129931726]
	TIME [epoch: 9.06 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2772834932535509		[learning rate: 0.0063787]
		[batch 20/20] avg loss: 0.314653369834147		[learning rate: 0.0063689]
	Learning Rate: 0.00636893
	LOSS [training: 0.2959684315438489 | validation: 0.17389641442111875]
	TIME [epoch: 9.06 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.354464168454842		[learning rate: 0.0063592]
		[batch 20/20] avg loss: 0.2437525097791835		[learning rate: 0.0063494]
	Learning Rate: 0.00634941
	LOSS [training: 0.29910833911701273 | validation: 0.16948892185599518]
	TIME [epoch: 9.05 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2901224256920939		[learning rate: 0.0063397]
		[batch 20/20] avg loss: 0.2636780739779418		[learning rate: 0.0063299]
	Learning Rate: 0.00632995
	LOSS [training: 0.2769002498350179 | validation: 0.47356342029719967]
	TIME [epoch: 9.08 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25521814813934524		[learning rate: 0.0063202]
		[batch 20/20] avg loss: 0.22979481397935902		[learning rate: 0.0063105]
	Learning Rate: 0.00631054
	LOSS [training: 0.24250648105935216 | validation: 0.2699718331509308]
	TIME [epoch: 9.05 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2642464379869608		[learning rate: 0.0063009]
		[batch 20/20] avg loss: 0.2896730648176861		[learning rate: 0.0062912]
	Learning Rate: 0.0062912
	LOSS [training: 0.27695975140232354 | validation: 0.31277953241509326]
	TIME [epoch: 9.05 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23108602760849095		[learning rate: 0.0062815]
		[batch 20/20] avg loss: 0.18402308901706316		[learning rate: 0.0062719]
	Learning Rate: 0.00627191
	LOSS [training: 0.207554558312777 | validation: 0.1465407955043802]
	TIME [epoch: 9.05 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21511971052595427		[learning rate: 0.0062623]
		[batch 20/20] avg loss: 0.21155170290794204		[learning rate: 0.0062527]
	Learning Rate: 0.00625269
	LOSS [training: 0.21333570671694813 | validation: 0.12361050373678584]
	TIME [epoch: 9.05 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.318755979592385		[learning rate: 0.0062431]
		[batch 20/20] avg loss: 0.26172300788012204		[learning rate: 0.0062335]
	Learning Rate: 0.00623352
	LOSS [training: 0.2902394937362536 | validation: 0.2915013853406297]
	TIME [epoch: 9.08 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3164292710795324		[learning rate: 0.006224]
		[batch 20/20] avg loss: 0.253559764995714		[learning rate: 0.0062144]
	Learning Rate: 0.00621441
	LOSS [training: 0.2849945180376232 | validation: 0.19589454402436124]
	TIME [epoch: 9.06 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3034665872240191		[learning rate: 0.0062049]
		[batch 20/20] avg loss: 0.28952750301713526		[learning rate: 0.0061954]
	Learning Rate: 0.00619536
	LOSS [training: 0.2964970451205772 | validation: 0.33679311452440663]
	TIME [epoch: 9.06 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30190461185572914		[learning rate: 0.0061859]
		[batch 20/20] avg loss: 0.44232510609346554		[learning rate: 0.0061764]
	Learning Rate: 0.00617637
	LOSS [training: 0.3721148589745974 | validation: 0.1601295832135585]
	TIME [epoch: 9.05 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2662010586163522		[learning rate: 0.0061669]
		[batch 20/20] avg loss: 0.2649734531720508		[learning rate: 0.0061574]
	Learning Rate: 0.00615744
	LOSS [training: 0.2655872558942015 | validation: 0.32343622772953873]
	TIME [epoch: 9.04 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.52847508331019		[learning rate: 0.006148]
		[batch 20/20] avg loss: 0.4564160231290898		[learning rate: 0.0061386]
	Learning Rate: 0.00613856
	LOSS [training: 0.4924455532196398 | validation: 0.25820498272365855]
	TIME [epoch: 9.07 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26426248709586464		[learning rate: 0.0061291]
		[batch 20/20] avg loss: 0.25628909442514053		[learning rate: 0.0061197]
	Learning Rate: 0.00611974
	LOSS [training: 0.26027579076050267 | validation: 0.41069387284763176]
	TIME [epoch: 9.05 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2848949691229915		[learning rate: 0.0061104]
		[batch 20/20] avg loss: 0.2840719489411745		[learning rate: 0.006101]
	Learning Rate: 0.00610099
	LOSS [training: 0.28448345903208294 | validation: 0.2903060937888838]
	TIME [epoch: 9.05 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.267396458574195		[learning rate: 0.0060916]
		[batch 20/20] avg loss: 0.2542082285483523		[learning rate: 0.0060823]
	Learning Rate: 0.00608228
	LOSS [training: 0.2608023435612737 | validation: 0.1657219707441271]
	TIME [epoch: 9.05 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22425234774039488		[learning rate: 0.006073]
		[batch 20/20] avg loss: 0.2942716262541497		[learning rate: 0.0060636]
	Learning Rate: 0.00606364
	LOSS [training: 0.2592619869972723 | validation: 0.16296464149624423]
	TIME [epoch: 9.05 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2501924606595417		[learning rate: 0.0060543]
		[batch 20/20] avg loss: 0.22870885760776893		[learning rate: 0.0060451]
	Learning Rate: 0.00604505
	LOSS [training: 0.23945065913365532 | validation: 0.24804314743516984]
	TIME [epoch: 9.08 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2580156404149319		[learning rate: 0.0060358]
		[batch 20/20] avg loss: 0.24239858475660023		[learning rate: 0.0060265]
	Learning Rate: 0.00602652
	LOSS [training: 0.25020711258576606 | validation: 0.25314044186585455]
	TIME [epoch: 9.05 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21739652187841677		[learning rate: 0.0060173]
		[batch 20/20] avg loss: 0.21684105409865584		[learning rate: 0.006008]
	Learning Rate: 0.00600805
	LOSS [training: 0.2171187879885363 | validation: 0.1487612674462741]
	TIME [epoch: 9.05 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21327454003482793		[learning rate: 0.0059988]
		[batch 20/20] avg loss: 0.1999846936350119		[learning rate: 0.0059896]
	Learning Rate: 0.00598963
	LOSS [training: 0.2066296168349199 | validation: 0.11180693166637387]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_667.pth
	Model improved!!!
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23303947877645775		[learning rate: 0.0059804]
		[batch 20/20] avg loss: 0.24785384068230742		[learning rate: 0.0059713]
	Learning Rate: 0.00597127
	LOSS [training: 0.24044665972938262 | validation: 0.2526855952735678]
	TIME [epoch: 9.07 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2084545029427931		[learning rate: 0.0059621]
		[batch 20/20] avg loss: 0.27942218269507546		[learning rate: 0.005953]
	Learning Rate: 0.00595297
	LOSS [training: 0.24393834281893428 | validation: 0.26294342174839225]
	TIME [epoch: 9.08 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.238369533695631		[learning rate: 0.0059438]
		[batch 20/20] avg loss: 0.3554016206804003		[learning rate: 0.0059347]
	Learning Rate: 0.00593472
	LOSS [training: 0.2968855771880156 | validation: 0.237310977964914]
	TIME [epoch: 9.06 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3198817908525282		[learning rate: 0.0059256]
		[batch 20/20] avg loss: 0.29809577727636105		[learning rate: 0.0059165]
	Learning Rate: 0.00591652
	LOSS [training: 0.30898878406444463 | validation: 0.22476345194539696]
	TIME [epoch: 9.05 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2595093097736013		[learning rate: 0.0059074]
		[batch 20/20] avg loss: 0.283185935198715		[learning rate: 0.0058984]
	Learning Rate: 0.00589839
	LOSS [training: 0.2713476224861581 | validation: 0.3402139177702871]
	TIME [epoch: 9.06 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2634009577759894		[learning rate: 0.0058893]
		[batch 20/20] avg loss: 0.24973950136152703		[learning rate: 0.0058803]
	Learning Rate: 0.00588031
	LOSS [training: 0.2565702295687582 | validation: 0.2628054382631446]
	TIME [epoch: 9.05 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24836829999204085		[learning rate: 0.0058713]
		[batch 20/20] avg loss: 0.24441475378106134		[learning rate: 0.0058623]
	Learning Rate: 0.00586228
	LOSS [training: 0.24639152688655114 | validation: 0.23816940903714046]
	TIME [epoch: 9.08 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2336490101902152		[learning rate: 0.0058533]
		[batch 20/20] avg loss: 0.18932215351843285		[learning rate: 0.0058443]
	Learning Rate: 0.00584431
	LOSS [training: 0.21148558185432403 | validation: 0.1371489931733752]
	TIME [epoch: 9.05 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24409063988076313		[learning rate: 0.0058353]
		[batch 20/20] avg loss: 0.2033671072754196		[learning rate: 0.0058264]
	Learning Rate: 0.0058264
	LOSS [training: 0.22372887357809135 | validation: 0.322679977158303]
	TIME [epoch: 9.06 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35456277369899003		[learning rate: 0.0058175]
		[batch 20/20] avg loss: 0.2724804494019795		[learning rate: 0.0058085]
	Learning Rate: 0.00580854
	LOSS [training: 0.31352161155048475 | validation: 0.21416537562118448]
	TIME [epoch: 9.05 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2102250609457983		[learning rate: 0.0057996]
		[batch 20/20] avg loss: 0.2790921854125532		[learning rate: 0.0057907]
	Learning Rate: 0.00579073
	LOSS [training: 0.2446586231791757 | validation: 0.21108991484988773]
	TIME [epoch: 9.06 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2689942327981627		[learning rate: 0.0057818]
		[batch 20/20] avg loss: 0.2726823873957538		[learning rate: 0.005773]
	Learning Rate: 0.00577298
	LOSS [training: 0.2708383100969582 | validation: 0.2633537313497139]
	TIME [epoch: 9.08 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2647846035196246		[learning rate: 0.0057641]
		[batch 20/20] avg loss: 0.2476070118382702		[learning rate: 0.0057553]
	Learning Rate: 0.00575528
	LOSS [training: 0.25619580767894734 | validation: 0.107518295025269]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24103155349442268		[learning rate: 0.0057465]
		[batch 20/20] avg loss: 0.18946770209267133		[learning rate: 0.0057376]
	Learning Rate: 0.00573764
	LOSS [training: 0.215249627793547 | validation: 0.20332711192862324]
	TIME [epoch: 9.06 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2297052328621189		[learning rate: 0.0057288]
		[batch 20/20] avg loss: 0.1807293313619403		[learning rate: 0.0057201]
	Learning Rate: 0.00572005
	LOSS [training: 0.20521728211202964 | validation: 0.12052637949202338]
	TIME [epoch: 9.06 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21261394777772993		[learning rate: 0.0057113]
		[batch 20/20] avg loss: 0.26223804652901916		[learning rate: 0.0057025]
	Learning Rate: 0.00570252
	LOSS [training: 0.23742599715337454 | validation: 0.18740869588084866]
	TIME [epoch: 9.05 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20932992774446157		[learning rate: 0.0056938]
		[batch 20/20] avg loss: 0.24375775674702277		[learning rate: 0.005685]
	Learning Rate: 0.00568504
	LOSS [training: 0.22654384224574214 | validation: 0.17188180170095507]
	TIME [epoch: 9.08 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.202240035867075		[learning rate: 0.0056763]
		[batch 20/20] avg loss: 0.21025516540051084		[learning rate: 0.0056676]
	Learning Rate: 0.00566761
	LOSS [training: 0.20624760063379294 | validation: 0.15476857574158048]
	TIME [epoch: 9.05 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22853873890664658		[learning rate: 0.0056589]
		[batch 20/20] avg loss: 0.22807695017933352		[learning rate: 0.0056502]
	Learning Rate: 0.00565024
	LOSS [training: 0.22830784454299008 | validation: 0.2259862676774459]
	TIME [epoch: 9.06 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23801091166939292		[learning rate: 0.0056416]
		[batch 20/20] avg loss: 0.1753262148511276		[learning rate: 0.0056329]
	Learning Rate: 0.00563292
	LOSS [training: 0.20666856326026029 | validation: 0.17910454583864432]
	TIME [epoch: 9.06 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21809688074891914		[learning rate: 0.0056243]
		[batch 20/20] avg loss: 0.17024653200740353		[learning rate: 0.0056156]
	Learning Rate: 0.00561565
	LOSS [training: 0.19417170637816136 | validation: 0.11256862090077246]
	TIME [epoch: 9.06 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24253526819687066		[learning rate: 0.005607]
		[batch 20/20] avg loss: 0.22453250012811962		[learning rate: 0.0055984]
	Learning Rate: 0.00559843
	LOSS [training: 0.2335338841624952 | validation: 0.21127982844549184]
	TIME [epoch: 9.08 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2853493555467478		[learning rate: 0.0055898]
		[batch 20/20] avg loss: 0.25759313674775175		[learning rate: 0.0055813]
	Learning Rate: 0.00558127
	LOSS [training: 0.2714712461472497 | validation: 0.11619620301434967]
	TIME [epoch: 9.07 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1894975962362665		[learning rate: 0.0055727]
		[batch 20/20] avg loss: 0.18958537194628988		[learning rate: 0.0055642]
	Learning Rate: 0.00556416
	LOSS [training: 0.1895414840912782 | validation: 0.18374721341609013]
	TIME [epoch: 9.06 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1708861809052376		[learning rate: 0.0055556]
		[batch 20/20] avg loss: 0.16397342203078927		[learning rate: 0.0055471]
	Learning Rate: 0.00554711
	LOSS [training: 0.16742980146801342 | validation: 0.21246438359165382]
	TIME [epoch: 9.06 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17464122894144166		[learning rate: 0.0055386]
		[batch 20/20] avg loss: 0.21444087483392765		[learning rate: 0.0055301]
	Learning Rate: 0.0055301
	LOSS [training: 0.19454105188768464 | validation: 0.2093145003811781]
	TIME [epoch: 9.06 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22424866576572025		[learning rate: 0.0055216]
		[batch 20/20] avg loss: 0.21211386168040164		[learning rate: 0.0055132]
	Learning Rate: 0.00551315
	LOSS [training: 0.2181812637230609 | validation: 0.2690463634336997]
	TIME [epoch: 9.08 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27306070231977625		[learning rate: 0.0055047]
		[batch 20/20] avg loss: 0.17312668302234518		[learning rate: 0.0054963]
	Learning Rate: 0.00549625
	LOSS [training: 0.22309369267106066 | validation: 0.22456498870708602]
	TIME [epoch: 9.06 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22044677226303538		[learning rate: 0.0054878]
		[batch 20/20] avg loss: 0.24755085113603692		[learning rate: 0.0054794]
	Learning Rate: 0.0054794
	LOSS [training: 0.23399881169953612 | validation: 0.36674041980132344]
	TIME [epoch: 9.06 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25433446436802415		[learning rate: 0.005471]
		[batch 20/20] avg loss: 0.21462415481452024		[learning rate: 0.0054626]
	Learning Rate: 0.00546261
	LOSS [training: 0.23447930959127214 | validation: 0.21330077954201765]
	TIME [epoch: 9.06 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2073988279654523		[learning rate: 0.0054542]
		[batch 20/20] avg loss: 0.18323520084206343		[learning rate: 0.0054459]
	Learning Rate: 0.00544586
	LOSS [training: 0.19531701440375787 | validation: 0.27915687539701417]
	TIME [epoch: 9.06 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17730522369754298		[learning rate: 0.0054375]
		[batch 20/20] avg loss: 0.18441279478479372		[learning rate: 0.0054292]
	Learning Rate: 0.00542917
	LOSS [training: 0.18085900924116838 | validation: 0.15350957298392473]
	TIME [epoch: 9.08 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23818841835930615		[learning rate: 0.0054208]
		[batch 20/20] avg loss: 0.31882291591677686		[learning rate: 0.0054125]
	Learning Rate: 0.00541253
	LOSS [training: 0.2785056671380416 | validation: 0.2173683790421243]
	TIME [epoch: 9.06 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24963983676180096		[learning rate: 0.0054042]
		[batch 20/20] avg loss: 0.23807308971901736		[learning rate: 0.0053959]
	Learning Rate: 0.00539593
	LOSS [training: 0.24385646324040913 | validation: 0.24118551984652614]
	TIME [epoch: 9.06 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25088000628789475		[learning rate: 0.0053877]
		[batch 20/20] avg loss: 0.19668705759177812		[learning rate: 0.0053794]
	Learning Rate: 0.00537939
	LOSS [training: 0.22378353193983647 | validation: 0.07589544610628586]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_702.pth
	Model improved!!!
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18108954368321056		[learning rate: 0.0053711]
		[batch 20/20] avg loss: 0.2096156570922149		[learning rate: 0.0053629]
	Learning Rate: 0.0053629
	LOSS [training: 0.19535260038771277 | validation: 0.2634453049722457]
	TIME [epoch: 9.06 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2670347711263506		[learning rate: 0.0053547]
		[batch 20/20] avg loss: 0.23256286858459446		[learning rate: 0.0053465]
	Learning Rate: 0.00534646
	LOSS [training: 0.2497988198554725 | validation: 0.11057101536707034]
	TIME [epoch: 9.07 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1728242436475715		[learning rate: 0.0053383]
		[batch 20/20] avg loss: 0.17623767145547964		[learning rate: 0.0053301]
	Learning Rate: 0.00533008
	LOSS [training: 0.1745309575515256 | validation: 0.1281034245483357]
	TIME [epoch: 9.06 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22770363875257355		[learning rate: 0.0053219]
		[batch 20/20] avg loss: 0.36310298434466526		[learning rate: 0.0053137]
	Learning Rate: 0.00531374
	LOSS [training: 0.2954033115486194 | validation: 0.14707568407702562]
	TIME [epoch: 9.04 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19422058464903874		[learning rate: 0.0053056]
		[batch 20/20] avg loss: 0.23277930829642407		[learning rate: 0.0052974]
	Learning Rate: 0.00529745
	LOSS [training: 0.2134999464727314 | validation: 0.24441118677729937]
	TIME [epoch: 9.05 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1899538533363963		[learning rate: 0.0052893]
		[batch 20/20] avg loss: 0.24654770895396155		[learning rate: 0.0052812]
	Learning Rate: 0.00528121
	LOSS [training: 0.21825078114517896 | validation: 0.1952106569945813]
	TIME [epoch: 9.04 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19283164714838696		[learning rate: 0.0052731]
		[batch 20/20] avg loss: 0.22938201979329692		[learning rate: 0.005265]
	Learning Rate: 0.00526502
	LOSS [training: 0.21110683347084191 | validation: 0.24614543947169298]
	TIME [epoch: 9.07 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17569132906371937		[learning rate: 0.0052569]
		[batch 20/20] avg loss: 0.20043190231814684		[learning rate: 0.0052489]
	Learning Rate: 0.00524888
	LOSS [training: 0.18806161569093308 | validation: 0.11960668906294604]
	TIME [epoch: 9.04 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24114020988192048		[learning rate: 0.0052408]
		[batch 20/20] avg loss: 0.22858240873353686		[learning rate: 0.0052328]
	Learning Rate: 0.00523279
	LOSS [training: 0.23486130930772867 | validation: 0.29221763960945285]
	TIME [epoch: 9.05 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.198087300288731		[learning rate: 0.0052248]
		[batch 20/20] avg loss: 0.3354074737206103		[learning rate: 0.0052167]
	Learning Rate: 0.00521675
	LOSS [training: 0.26674738700467066 | validation: 0.1587883398270872]
	TIME [epoch: 9.05 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2259658903369443		[learning rate: 0.0052087]
		[batch 20/20] avg loss: 0.2114865776957971		[learning rate: 0.0052008]
	Learning Rate: 0.00520076
	LOSS [training: 0.2187262340163707 | validation: 0.15311649570786087]
	TIME [epoch: 9.05 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2065579864806363		[learning rate: 0.0051928]
		[batch 20/20] avg loss: 0.27158900583225826		[learning rate: 0.0051848]
	Learning Rate: 0.00518482
	LOSS [training: 0.2390734961564472 | validation: 0.302702162434526]
	TIME [epoch: 9.07 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19784433668018814		[learning rate: 0.0051769]
		[batch 20/20] avg loss: 0.19087118524968935		[learning rate: 0.0051689]
	Learning Rate: 0.00516892
	LOSS [training: 0.19435776096493879 | validation: 0.16382499878814183]
	TIME [epoch: 9.06 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23212364453205864		[learning rate: 0.005161]
		[batch 20/20] avg loss: 0.20933480220320783		[learning rate: 0.0051531]
	Learning Rate: 0.00515308
	LOSS [training: 0.22072922336763323 | validation: 0.18091165571662607]
	TIME [epoch: 9.05 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26944782403379086		[learning rate: 0.0051452]
		[batch 20/20] avg loss: 0.26148339061730985		[learning rate: 0.0051373]
	Learning Rate: 0.00513728
	LOSS [training: 0.26546560732555036 | validation: 0.20987370485386228]
	TIME [epoch: 9.06 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2768009210205008		[learning rate: 0.0051294]
		[batch 20/20] avg loss: 0.28489237990594146		[learning rate: 0.0051215]
	Learning Rate: 0.00512153
	LOSS [training: 0.28084665046322116 | validation: 0.1559098702122704]
	TIME [epoch: 9.05 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24191365912078916		[learning rate: 0.0051137]
		[batch 20/20] avg loss: 0.258191077949709		[learning rate: 0.0051058]
	Learning Rate: 0.00510583
	LOSS [training: 0.25005236853524904 | validation: 0.1413139951519884]
	TIME [epoch: 9.07 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21337739223666338		[learning rate: 0.005098]
		[batch 20/20] avg loss: 0.22163377343870616		[learning rate: 0.0050902]
	Learning Rate: 0.00509018
	LOSS [training: 0.21750558283768476 | validation: 0.20410927053681277]
	TIME [epoch: 9.06 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19150243708855685		[learning rate: 0.0050824]
		[batch 20/20] avg loss: 0.2594915265492645		[learning rate: 0.0050746]
	Learning Rate: 0.00507458
	LOSS [training: 0.2254969818189107 | validation: 0.16463283044983495]
	TIME [epoch: 9.05 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20796552009115313		[learning rate: 0.0050668]
		[batch 20/20] avg loss: 0.18405707647830877		[learning rate: 0.005059]
	Learning Rate: 0.00505902
	LOSS [training: 0.19601129828473093 | validation: 0.17062652955321989]
	TIME [epoch: 9.06 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20437407227780185		[learning rate: 0.0050513]
		[batch 20/20] avg loss: 0.16874300566227704		[learning rate: 0.0050435]
	Learning Rate: 0.00504352
	LOSS [training: 0.1865585389700395 | validation: 0.3982998645809791]
	TIME [epoch: 9.05 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22627164423553398		[learning rate: 0.0050358]
		[batch 20/20] avg loss: 0.1978968012197843		[learning rate: 0.0050281]
	Learning Rate: 0.00502805
	LOSS [training: 0.21208422272765914 | validation: 0.15212017493818356]
	TIME [epoch: 9.08 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2085009681644892		[learning rate: 0.0050203]
		[batch 20/20] avg loss: 0.20346081589050447		[learning rate: 0.0050126]
	Learning Rate: 0.00501264
	LOSS [training: 0.20598089202749686 | validation: 0.18804849664705592]
	TIME [epoch: 9.06 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24329826730756327		[learning rate: 0.005005]
		[batch 20/20] avg loss: 0.183511108054651		[learning rate: 0.0049973]
	Learning Rate: 0.00499728
	LOSS [training: 0.21340468768110715 | validation: 0.1161299295390189]
	TIME [epoch: 9.06 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14471682335097974		[learning rate: 0.0049896]
		[batch 20/20] avg loss: 0.22621748685208462		[learning rate: 0.004982]
	Learning Rate: 0.00498196
	LOSS [training: 0.1854671551015322 | validation: 0.13984470277132402]
	TIME [epoch: 9.05 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19876957076853222		[learning rate: 0.0049743]
		[batch 20/20] avg loss: 0.21229328958736585		[learning rate: 0.0049667]
	Learning Rate: 0.00496669
	LOSS [training: 0.205531430177949 | validation: 0.1459345588328569]
	TIME [epoch: 9.38 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2075658440379223		[learning rate: 0.0049591]
		[batch 20/20] avg loss: 0.2707972314269454		[learning rate: 0.0049515]
	Learning Rate: 0.00495146
	LOSS [training: 0.23918153773243384 | validation: 0.2070090625169771]
	TIME [epoch: 9.08 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2470418036158281		[learning rate: 0.0049439]
		[batch 20/20] avg loss: 0.23192067244752607		[learning rate: 0.0049363]
	Learning Rate: 0.00493628
	LOSS [training: 0.239481238031677 | validation: 0.16917538490720288]
	TIME [epoch: 9.06 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18449533793661038		[learning rate: 0.0049287]
		[batch 20/20] avg loss: 0.18144451270557913		[learning rate: 0.0049211]
	Learning Rate: 0.00492115
	LOSS [training: 0.18296992532109474 | validation: 0.17895773994459033]
	TIME [epoch: 9.05 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2080036538413327		[learning rate: 0.0049136]
		[batch 20/20] avg loss: 0.24076131402687265		[learning rate: 0.0049061]
	Learning Rate: 0.00490607
	LOSS [training: 0.2243824839341027 | validation: 0.31531840832172564]
	TIME [epoch: 9.06 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22262031604774762		[learning rate: 0.0048985]
		[batch 20/20] avg loss: 0.1907703737457209		[learning rate: 0.004891]
	Learning Rate: 0.00489103
	LOSS [training: 0.20669534489673427 | validation: 0.3004420813646472]
	TIME [epoch: 9.05 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2532066936431155		[learning rate: 0.0048835]
		[batch 20/20] avg loss: 0.25093448101046156		[learning rate: 0.004876]
	Learning Rate: 0.00487603
	LOSS [training: 0.25207058732678855 | validation: 0.2302546034032783]
	TIME [epoch: 9.07 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19715884253705313		[learning rate: 0.0048686]
		[batch 20/20] avg loss: 0.19480152949538135		[learning rate: 0.0048611]
	Learning Rate: 0.00486109
	LOSS [training: 0.19598018601621728 | validation: 0.13014950363295816]
	TIME [epoch: 9.07 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2002875255108733		[learning rate: 0.0048536]
		[batch 20/20] avg loss: 0.23526766485329126		[learning rate: 0.0048462]
	Learning Rate: 0.00484618
	LOSS [training: 0.21777759518208226 | validation: 0.2616998834088824]
	TIME [epoch: 9.05 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24724464282547615		[learning rate: 0.0048388]
		[batch 20/20] avg loss: 0.19063834439563143		[learning rate: 0.0048313]
	Learning Rate: 0.00483133
	LOSS [training: 0.21894149361055376 | validation: 0.16508237404015513]
	TIME [epoch: 9.05 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19164573882593225		[learning rate: 0.0048239]
		[batch 20/20] avg loss: 0.2710864757654519		[learning rate: 0.0048165]
	Learning Rate: 0.00481652
	LOSS [training: 0.23136610729569212 | validation: 0.20633496009881214]
	TIME [epoch: 9.05 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19156006485096605		[learning rate: 0.0048091]
		[batch 20/20] avg loss: 0.32188391679868883		[learning rate: 0.0048018]
	Learning Rate: 0.00480176
	LOSS [training: 0.25672199082482744 | validation: 0.28078881579069]
	TIME [epoch: 9.07 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2805824379881944		[learning rate: 0.0047944]
		[batch 20/20] avg loss: 0.19034281997738972		[learning rate: 0.004787]
	Learning Rate: 0.00478704
	LOSS [training: 0.23546262898279205 | validation: 0.38351860757426764]
	TIME [epoch: 9.06 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2696959221879159		[learning rate: 0.0047797]
		[batch 20/20] avg loss: 0.22554501635277308		[learning rate: 0.0047724]
	Learning Rate: 0.00477236
	LOSS [training: 0.24762046927034445 | validation: 0.13018458250790307]
	TIME [epoch: 9.06 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17817490238189895		[learning rate: 0.004765]
		[batch 20/20] avg loss: 0.2095052588408554		[learning rate: 0.0047577]
	Learning Rate: 0.00475773
	LOSS [training: 0.19384008061137717 | validation: 0.20823890300693015]
	TIME [epoch: 9.06 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2437804923682076		[learning rate: 0.0047504]
		[batch 20/20] avg loss: 0.2071698800943654		[learning rate: 0.0047431]
	Learning Rate: 0.00474315
	LOSS [training: 0.22547518623128654 | validation: 0.10052270503908832]
	TIME [epoch: 9.06 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1760030264186063		[learning rate: 0.0047359]
		[batch 20/20] avg loss: 0.17602738558944164		[learning rate: 0.0047286]
	Learning Rate: 0.00472861
	LOSS [training: 0.17601520600402398 | validation: 0.11435805308714792]
	TIME [epoch: 9.07 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1887653440688866		[learning rate: 0.0047214]
		[batch 20/20] avg loss: 0.21418809273741793		[learning rate: 0.0047141]
	Learning Rate: 0.00471411
	LOSS [training: 0.20147671840315223 | validation: 0.08584792064786971]
	TIME [epoch: 9.06 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18516330371376868		[learning rate: 0.0047069]
		[batch 20/20] avg loss: 0.27438763908124403		[learning rate: 0.0046997]
	Learning Rate: 0.00469966
	LOSS [training: 0.2297754713975063 | validation: 0.16539991551934308]
	TIME [epoch: 9.05 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2648452607768942		[learning rate: 0.0046925]
		[batch 20/20] avg loss: 0.26409191084621453		[learning rate: 0.0046853]
	Learning Rate: 0.00468526
	LOSS [training: 0.26446858581155436 | validation: 0.18736635542297086]
	TIME [epoch: 9.06 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1841674483154574		[learning rate: 0.0046781]
		[batch 20/20] avg loss: 0.2283125744908588		[learning rate: 0.0046709]
	Learning Rate: 0.00467089
	LOSS [training: 0.2062400114031581 | validation: 0.1889552231945082]
	TIME [epoch: 9.05 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1923439594121314		[learning rate: 0.0046637]
		[batch 20/20] avg loss: 0.18077562559651153		[learning rate: 0.0046566]
	Learning Rate: 0.00465658
	LOSS [training: 0.1865597925043215 | validation: 0.19440300298826863]
	TIME [epoch: 9.07 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19843773432647455		[learning rate: 0.0046494]
		[batch 20/20] avg loss: 0.18504688671858321		[learning rate: 0.0046423]
	Learning Rate: 0.0046423
	LOSS [training: 0.19174231052252885 | validation: 0.10922651082278884]
	TIME [epoch: 9.05 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18299507867690507		[learning rate: 0.0046352]
		[batch 20/20] avg loss: 0.21348478543940347		[learning rate: 0.0046281]
	Learning Rate: 0.00462807
	LOSS [training: 0.19823993205815424 | validation: 0.12371219595319222]
	TIME [epoch: 9.05 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2935644851905248		[learning rate: 0.004621]
		[batch 20/20] avg loss: 0.2162123116119606		[learning rate: 0.0046139]
	Learning Rate: 0.00461388
	LOSS [training: 0.25488839840124267 | validation: 0.14253118023122985]
	TIME [epoch: 9.06 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2783174493413194		[learning rate: 0.0046068]
		[batch 20/20] avg loss: 0.22397440832490062		[learning rate: 0.0045997]
	Learning Rate: 0.00459974
	LOSS [training: 0.25114592883311 | validation: 0.2821402813189576]
	TIME [epoch: 9.06 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22413901244569967		[learning rate: 0.0045927]
		[batch 20/20] avg loss: 0.20207887919234216		[learning rate: 0.0045856]
	Learning Rate: 0.00458564
	LOSS [training: 0.2131089458190209 | validation: 0.4196908119101772]
	TIME [epoch: 9.06 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2281315517386319		[learning rate: 0.0045786]
		[batch 20/20] avg loss: 0.19701598847906415		[learning rate: 0.0045716]
	Learning Rate: 0.00457158
	LOSS [training: 0.21257377010884798 | validation: 0.0996400263889519]
	TIME [epoch: 9.06 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17011623717320018		[learning rate: 0.0045646]
		[batch 20/20] avg loss: 0.2308305479346669		[learning rate: 0.0045576]
	Learning Rate: 0.00455757
	LOSS [training: 0.20047339255393357 | validation: 0.15299080079100758]
	TIME [epoch: 9.05 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2709914891783208		[learning rate: 0.0045506]
		[batch 20/20] avg loss: 0.19257278153963314		[learning rate: 0.0045436]
	Learning Rate: 0.0045436
	LOSS [training: 0.23178213535897693 | validation: 0.09956619592482216]
	TIME [epoch: 9.04 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18923867982598036		[learning rate: 0.0045366]
		[batch 20/20] avg loss: 0.17833451386057506		[learning rate: 0.0045297]
	Learning Rate: 0.00452967
	LOSS [training: 0.1837865968432777 | validation: 0.3230449968158962]
	TIME [epoch: 9.06 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1767014303639583		[learning rate: 0.0045227]
		[batch 20/20] avg loss: 0.2028504278902979		[learning rate: 0.0045158]
	Learning Rate: 0.00451579
	LOSS [training: 0.1897759291271281 | validation: 0.17769626325554896]
	TIME [epoch: 9.07 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1750714740162601		[learning rate: 0.0045089]
		[batch 20/20] avg loss: 0.22678624943697448		[learning rate: 0.0045019]
	Learning Rate: 0.00450194
	LOSS [training: 0.2009288617266173 | validation: 0.26571974136294485]
	TIME [epoch: 9.06 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2392114700497845		[learning rate: 0.004495]
		[batch 20/20] avg loss: 0.20547135751814327		[learning rate: 0.0044881]
	Learning Rate: 0.00448814
	LOSS [training: 0.22234141378396388 | validation: 0.14040211741745862]
	TIME [epoch: 9.06 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19215380349677016		[learning rate: 0.0044813]
		[batch 20/20] avg loss: 0.220863791148909		[learning rate: 0.0044744]
	Learning Rate: 0.00447438
	LOSS [training: 0.20650879732283958 | validation: 0.13765489985313092]
	TIME [epoch: 9.05 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1653382982162201		[learning rate: 0.0044675]
		[batch 20/20] avg loss: 0.21027646456251206		[learning rate: 0.0044607]
	Learning Rate: 0.00446067
	LOSS [training: 0.1878073813893661 | validation: 0.10920499322256863]
	TIME [epoch: 9.05 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17542795410121964		[learning rate: 0.0044538]
		[batch 20/20] avg loss: 0.22846791085874824		[learning rate: 0.004447]
	Learning Rate: 0.00444699
	LOSS [training: 0.20194793247998394 | validation: 0.3564287751396798]
	TIME [epoch: 9.07 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18331746818512396		[learning rate: 0.0044402]
		[batch 20/20] avg loss: 0.17392696673887373		[learning rate: 0.0044334]
	Learning Rate: 0.00443336
	LOSS [training: 0.17862221746199886 | validation: 0.24424558598300877]
	TIME [epoch: 9.06 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17258567807333572		[learning rate: 0.0044266]
		[batch 20/20] avg loss: 0.15465253719061037		[learning rate: 0.0044198]
	Learning Rate: 0.00441977
	LOSS [training: 0.16361910763197302 | validation: 0.25626140496354466]
	TIME [epoch: 9.05 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16348555234205603		[learning rate: 0.004413]
		[batch 20/20] avg loss: 0.17502985029137524		[learning rate: 0.0044062]
	Learning Rate: 0.00440622
	LOSS [training: 0.1692577013167156 | validation: 0.16556386963238343]
	TIME [epoch: 9.05 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14712144927140833		[learning rate: 0.0043995]
		[batch 20/20] avg loss: 0.1724277175964153		[learning rate: 0.0043927]
	Learning Rate: 0.00439272
	LOSS [training: 0.1597745834339118 | validation: 0.1444386594873106]
	TIME [epoch: 9.05 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14866254487338107		[learning rate: 0.004386]
		[batch 20/20] avg loss: 0.20753287893071554		[learning rate: 0.0043793]
	Learning Rate: 0.00437925
	LOSS [training: 0.17809771190204832 | validation: 0.12498041281212546]
	TIME [epoch: 9.07 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17302254603758777		[learning rate: 0.0043725]
		[batch 20/20] avg loss: 0.1721897880578247		[learning rate: 0.0043658]
	Learning Rate: 0.00436583
	LOSS [training: 0.17260616704770623 | validation: 0.2026670779997658]
	TIME [epoch: 9.06 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2001649124337872		[learning rate: 0.0043591]
		[batch 20/20] avg loss: 0.1777650468878798		[learning rate: 0.0043524]
	Learning Rate: 0.00435245
	LOSS [training: 0.1889649796608335 | validation: 0.10485258783219875]
	TIME [epoch: 9.05 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16771446980086174		[learning rate: 0.0043458]
		[batch 20/20] avg loss: 0.15828746098680577		[learning rate: 0.0043391]
	Learning Rate: 0.0043391
	LOSS [training: 0.16300096539383374 | validation: 0.23177160817639753]
	TIME [epoch: 9.05 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1854564322776539		[learning rate: 0.0043324]
		[batch 20/20] avg loss: 0.12858921425901734		[learning rate: 0.0043258]
	Learning Rate: 0.0043258
	LOSS [training: 0.15702282326833564 | validation: 0.10827451898688309]
	TIME [epoch: 9.05 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20173809596778716		[learning rate: 0.0043192]
		[batch 20/20] avg loss: 0.18962426614011943		[learning rate: 0.0043125]
	Learning Rate: 0.00431254
	LOSS [training: 0.1956811810539533 | validation: 0.13098136290029755]
	TIME [epoch: 9.08 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19696794450519256		[learning rate: 0.0043059]
		[batch 20/20] avg loss: 0.16033666977730965		[learning rate: 0.0042993]
	Learning Rate: 0.00429932
	LOSS [training: 0.17865230714125116 | validation: 0.11128172531100781]
	TIME [epoch: 9.06 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17245051840470899		[learning rate: 0.0042927]
		[batch 20/20] avg loss: 0.18245212076525735		[learning rate: 0.0042861]
	Learning Rate: 0.00428614
	LOSS [training: 0.17745131958498317 | validation: 0.214016282026749]
	TIME [epoch: 9.05 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1677026968811766		[learning rate: 0.0042796]
		[batch 20/20] avg loss: 0.15383243219546425		[learning rate: 0.004273]
	Learning Rate: 0.004273
	LOSS [training: 0.16076756453832042 | validation: 0.13937269541792052]
	TIME [epoch: 9.05 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.153009405872014		[learning rate: 0.0042664]
		[batch 20/20] avg loss: 0.16786258894731296		[learning rate: 0.0042599]
	Learning Rate: 0.00425991
	LOSS [training: 0.16043599740966347 | validation: 0.07818826224746658]
	TIME [epoch: 9.05 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17398803193529472		[learning rate: 0.0042534]
		[batch 20/20] avg loss: 0.22941143221811036		[learning rate: 0.0042468]
	Learning Rate: 0.00424685
	LOSS [training: 0.20169973207670253 | validation: 0.12335848766578719]
	TIME [epoch: 9.06 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1230479652015453		[learning rate: 0.0042403]
		[batch 20/20] avg loss: 0.1440061221781562		[learning rate: 0.0042338]
	Learning Rate: 0.00423383
	LOSS [training: 0.13352704368985074 | validation: 0.15467776063590777]
	TIME [epoch: 9.06 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20303655411763444		[learning rate: 0.0042273]
		[batch 20/20] avg loss: 0.17444134364576314		[learning rate: 0.0042209]
	Learning Rate: 0.00422085
	LOSS [training: 0.1887389488816988 | validation: 0.1584054988914077]
	TIME [epoch: 9.05 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21031525010108038		[learning rate: 0.0042144]
		[batch 20/20] avg loss: 0.14773255928044401		[learning rate: 0.0042079]
	Learning Rate: 0.00420791
	LOSS [training: 0.1790239046907622 | validation: 0.19389843355683056]
	TIME [epoch: 9.05 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1805053237134869		[learning rate: 0.0042015]
		[batch 20/20] avg loss: 0.2035078177086862		[learning rate: 0.004195]
	Learning Rate: 0.00419501
	LOSS [training: 0.19200657071108657 | validation: 0.09694779751821742]
	TIME [epoch: 9.06 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1653091361393058		[learning rate: 0.0041886]
		[batch 20/20] avg loss: 0.17683105186784337		[learning rate: 0.0041822]
	Learning Rate: 0.00418215
	LOSS [training: 0.1710700940035746 | validation: 0.1399766150213496]
	TIME [epoch: 9.08 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2307035449141618		[learning rate: 0.0041757]
		[batch 20/20] avg loss: 0.19803788300055486		[learning rate: 0.0041693]
	Learning Rate: 0.00416933
	LOSS [training: 0.2143707139573583 | validation: 0.33843467174363584]
	TIME [epoch: 9.06 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21757062869141178		[learning rate: 0.0041629]
		[batch 20/20] avg loss: 0.2445182068346549		[learning rate: 0.0041566]
	Learning Rate: 0.00415655
	LOSS [training: 0.2310444177630333 | validation: 0.14646538419041308]
	TIME [epoch: 9.05 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18784750371875578		[learning rate: 0.0041502]
		[batch 20/20] avg loss: 0.2273696059930738		[learning rate: 0.0041438]
	Learning Rate: 0.00414381
	LOSS [training: 0.20760855485591478 | validation: 0.16699813498967658]
	TIME [epoch: 9.05 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15437258013115526		[learning rate: 0.0041375]
		[batch 20/20] avg loss: 0.1686835405330361		[learning rate: 0.0041311]
	Learning Rate: 0.00413111
	LOSS [training: 0.1615280603320957 | validation: 0.09316809526920806]
	TIME [epoch: 9.06 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13014634928328078		[learning rate: 0.0041248]
		[batch 20/20] avg loss: 0.1783573791736171		[learning rate: 0.0041184]
	Learning Rate: 0.00411845
	LOSS [training: 0.15425186422844897 | validation: 0.08572264854519802]
	TIME [epoch: 9.07 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13291831266176954		[learning rate: 0.0041121]
		[batch 20/20] avg loss: 0.1873998170928051		[learning rate: 0.0041058]
	Learning Rate: 0.00410582
	LOSS [training: 0.1601590648772873 | validation: 0.13431336447441825]
	TIME [epoch: 9.06 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19507937553784677		[learning rate: 0.0040995]
		[batch 20/20] avg loss: 0.14251289637385534		[learning rate: 0.0040932]
	Learning Rate: 0.00409323
	LOSS [training: 0.16879613595585102 | validation: 0.19522619607165909]
	TIME [epoch: 9.06 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18230535726239894		[learning rate: 0.004087]
		[batch 20/20] avg loss: 0.16402746977738739		[learning rate: 0.0040807]
	Learning Rate: 0.00408069
	LOSS [training: 0.17316641351989315 | validation: 0.18528224774072716]
	TIME [epoch: 9.05 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.185989346551489		[learning rate: 0.0040744]
		[batch 20/20] avg loss: 0.2580493964385706		[learning rate: 0.0040682]
	Learning Rate: 0.00406818
	LOSS [training: 0.22201937149502987 | validation: 0.15043114855370823]
	TIME [epoch: 9.06 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1895837191126124		[learning rate: 0.0040619]
		[batch 20/20] avg loss: 0.18232529553318072		[learning rate: 0.0040557]
	Learning Rate: 0.00405571
	LOSS [training: 0.18595450732289656 | validation: 0.1289170800401362]
	TIME [epoch: 9.07 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2035550787286346		[learning rate: 0.0040495]
		[batch 20/20] avg loss: 0.16065067432092156		[learning rate: 0.0040433]
	Learning Rate: 0.00404328
	LOSS [training: 0.18210287652477813 | validation: 0.17073627382461254]
	TIME [epoch: 9.06 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16972927215161931		[learning rate: 0.0040371]
		[batch 20/20] avg loss: 0.22617760590498262		[learning rate: 0.0040309]
	Learning Rate: 0.00403088
	LOSS [training: 0.19795343902830095 | validation: 0.12693276253269756]
	TIME [epoch: 9.05 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16729305316410126		[learning rate: 0.0040247]
		[batch 20/20] avg loss: 0.17110862735179802		[learning rate: 0.0040185]
	Learning Rate: 0.00401852
	LOSS [training: 0.16920084025794963 | validation: 0.12139875693048566]
	TIME [epoch: 9.05 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16072569218038704		[learning rate: 0.0040124]
		[batch 20/20] avg loss: 0.1427213122763429		[learning rate: 0.0040062]
	Learning Rate: 0.00400621
	LOSS [training: 0.15172350222836495 | validation: 0.10062457417619317]
	TIME [epoch: 9.06 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16664256274334782		[learning rate: 0.0040001]
		[batch 20/20] avg loss: 0.16385911590136454		[learning rate: 0.0039939]
	Learning Rate: 0.00399393
	LOSS [training: 0.1652508393223562 | validation: 0.09054566213856141]
	TIME [epoch: 9.07 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13635046159726746		[learning rate: 0.0039878]
		[batch 20/20] avg loss: 0.1633390958723802		[learning rate: 0.0039817]
	Learning Rate: 0.00398168
	LOSS [training: 0.14984477873482388 | validation: 0.10954829485265435]
	TIME [epoch: 9.05 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16142044042754783		[learning rate: 0.0039756]
		[batch 20/20] avg loss: 0.16565197690464184		[learning rate: 0.0039695]
	Learning Rate: 0.00396948
	LOSS [training: 0.16353620866609483 | validation: 0.19740779876247766]
	TIME [epoch: 9.05 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14597277557157542		[learning rate: 0.0039634]
		[batch 20/20] avg loss: 0.16412754998883025		[learning rate: 0.0039573]
	Learning Rate: 0.00395731
	LOSS [training: 0.15505016278020284 | validation: 0.11763725522540366]
	TIME [epoch: 9.04 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20193387122384482		[learning rate: 0.0039512]
		[batch 20/20] avg loss: 0.17386164028968154		[learning rate: 0.0039452]
	Learning Rate: 0.00394518
	LOSS [training: 0.18789775575676315 | validation: 0.1989750174430675]
	TIME [epoch: 9.05 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16197252769200374		[learning rate: 0.0039391]
		[batch 20/20] avg loss: 0.16719457238323973		[learning rate: 0.0039331]
	Learning Rate: 0.00393308
	LOSS [training: 0.16458355003762176 | validation: 0.12267244220884502]
	TIME [epoch: 9.06 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16278737249592193		[learning rate: 0.0039271]
		[batch 20/20] avg loss: 0.18607992099094797		[learning rate: 0.003921]
	Learning Rate: 0.00392103
	LOSS [training: 0.17443364674343492 | validation: 0.13205908500370914]
	TIME [epoch: 9.07 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15564346236151436		[learning rate: 0.003915]
		[batch 20/20] avg loss: 0.16691783184967662		[learning rate: 0.003909]
	Learning Rate: 0.00390901
	LOSS [training: 0.16128064710559548 | validation: 0.06708357075095725]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_806.pth
	Model improved!!!
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09303625878482369		[learning rate: 0.003903]
		[batch 20/20] avg loss: 0.23010650737918864		[learning rate: 0.003897]
	Learning Rate: 0.00389703
	LOSS [training: 0.16157138308200616 | validation: 0.11036607623533194]
	TIME [epoch: 9.05 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16951324480800822		[learning rate: 0.003891]
		[batch 20/20] avg loss: 0.162329853664335		[learning rate: 0.0038851]
	Learning Rate: 0.00388508
	LOSS [training: 0.1659215492361716 | validation: 0.17342782525710848]
	TIME [epoch: 9.05 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18362254599784988		[learning rate: 0.0038791]
		[batch 20/20] avg loss: 0.18181271026127915		[learning rate: 0.0038732]
	Learning Rate: 0.00387317
	LOSS [training: 0.1827176281295645 | validation: 0.10375128952502133]
	TIME [epoch: 9.07 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17667374089826632		[learning rate: 0.0038672]
		[batch 20/20] avg loss: 0.19384912987269343		[learning rate: 0.0038613]
	Learning Rate: 0.0038613
	LOSS [training: 0.1852614353854799 | validation: 0.13702184358632263]
	TIME [epoch: 9.07 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15076589531650048		[learning rate: 0.0038554]
		[batch 20/20] avg loss: 0.19461788556717938		[learning rate: 0.0038495]
	Learning Rate: 0.00384946
	LOSS [training: 0.1726918904418399 | validation: 0.25682870313852996]
	TIME [epoch: 9.08 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1863723577885154		[learning rate: 0.0038436]
		[batch 20/20] avg loss: 0.19042028563610158		[learning rate: 0.0038377]
	Learning Rate: 0.00383766
	LOSS [training: 0.18839632171230852 | validation: 0.27593026181998676]
	TIME [epoch: 9.08 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17636917838057753		[learning rate: 0.0038318]
		[batch 20/20] avg loss: 0.14244326475497376		[learning rate: 0.0038259]
	Learning Rate: 0.0038259
	LOSS [training: 0.15940622156777565 | validation: 0.05032858423782022]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_813.pth
	Model improved!!!
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15318749835369594		[learning rate: 0.00382]
		[batch 20/20] avg loss: 0.15157776592070182		[learning rate: 0.0038142]
	Learning Rate: 0.00381417
	LOSS [training: 0.15238263213719888 | validation: 0.11444475645122343]
	TIME [epoch: 9.09 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12724206186540465		[learning rate: 0.0038083]
		[batch 20/20] avg loss: 0.14948610175087035		[learning rate: 0.0038025]
	Learning Rate: 0.00380248
	LOSS [training: 0.13836408180813753 | validation: 0.0974185593693403]
	TIME [epoch: 9.07 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18733122431290322		[learning rate: 0.0037966]
		[batch 20/20] avg loss: 0.13575793052304583		[learning rate: 0.0037908]
	Learning Rate: 0.00379082
	LOSS [training: 0.1615445774179745 | validation: 0.06640711479916032]
	TIME [epoch: 9.07 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22306238415127355		[learning rate: 0.003785]
		[batch 20/20] avg loss: 0.12978388155712398		[learning rate: 0.0037792]
	Learning Rate: 0.0037792
	LOSS [training: 0.17642313285419878 | validation: 0.16639219986750617]
	TIME [epoch: 9.07 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15192731084805022		[learning rate: 0.0037734]
		[batch 20/20] avg loss: 0.16686375819772592		[learning rate: 0.0037676]
	Learning Rate: 0.00376762
	LOSS [training: 0.15939553452288807 | validation: 0.1343096596558616]
	TIME [epoch: 9.05 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1651895001837424		[learning rate: 0.0037618]
		[batch 20/20] avg loss: 0.1442820289619845		[learning rate: 0.0037561]
	Learning Rate: 0.00375607
	LOSS [training: 0.15473576457286342 | validation: 0.09631203262146532]
	TIME [epoch: 9.07 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14331902303605423		[learning rate: 0.0037503]
		[batch 20/20] avg loss: 0.16486167865376689		[learning rate: 0.0037446]
	Learning Rate: 0.00374455
	LOSS [training: 0.15409035084491055 | validation: 0.18251138005600664]
	TIME [epoch: 9.05 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19942490489256465		[learning rate: 0.0037388]
		[batch 20/20] avg loss: 0.17887180468166075		[learning rate: 0.0037331]
	Learning Rate: 0.00373307
	LOSS [training: 0.18914835478711267 | validation: 0.1132477734732712]
	TIME [epoch: 9.05 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.170265250962453		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 0.15211208959125463		[learning rate: 0.0037216]
	Learning Rate: 0.00372163
	LOSS [training: 0.16118867027685385 | validation: 0.20499400892014774]
	TIME [epoch: 9.04 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16122392066339558		[learning rate: 0.0037159]
		[batch 20/20] avg loss: 0.1573106043311434		[learning rate: 0.0037102]
	Learning Rate: 0.00371022
	LOSS [training: 0.1592672624972695 | validation: 0.11219398319912202]
	TIME [epoch: 9.04 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13638523609509587		[learning rate: 0.0037045]
		[batch 20/20] avg loss: 0.11441245441944761		[learning rate: 0.0036988]
	Learning Rate: 0.00369885
	LOSS [training: 0.1253988452572717 | validation: 0.17482597653835485]
	TIME [epoch: 9.08 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14974854697263124		[learning rate: 0.0036932]
		[batch 20/20] avg loss: 0.14337343784143186		[learning rate: 0.0036875]
	Learning Rate: 0.00368751
	LOSS [training: 0.14656099240703158 | validation: 0.13549236837075507]
	TIME [epoch: 9.06 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15852158104250383		[learning rate: 0.0036819]
		[batch 20/20] avg loss: 0.16141892891100335		[learning rate: 0.0036762]
	Learning Rate: 0.00367621
	LOSS [training: 0.1599702549767536 | validation: 0.09697664499345666]
	TIME [epoch: 9.05 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1273126038510206		[learning rate: 0.0036706]
		[batch 20/20] avg loss: 0.14987906522710195		[learning rate: 0.0036649]
	Learning Rate: 0.00366494
	LOSS [training: 0.13859583453906127 | validation: 0.13422610432359286]
	TIME [epoch: 9.05 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16421596415666198		[learning rate: 0.0036593]
		[batch 20/20] avg loss: 0.1629733286930322		[learning rate: 0.0036537]
	Learning Rate: 0.0036537
	LOSS [training: 0.16359464642484708 | validation: 0.30658549809838215]
	TIME [epoch: 9.05 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16999741812668295		[learning rate: 0.0036481]
		[batch 20/20] avg loss: 0.12535585445845343		[learning rate: 0.0036425]
	Learning Rate: 0.0036425
	LOSS [training: 0.14767663629256822 | validation: 0.10823534071792315]
	TIME [epoch: 9.05 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10807725287228273		[learning rate: 0.0036369]
		[batch 20/20] avg loss: 0.17662625588510433		[learning rate: 0.0036313]
	Learning Rate: 0.00363134
	LOSS [training: 0.14235175437869355 | validation: 0.07880482239790654]
	TIME [epoch: 9.07 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1273373836716745		[learning rate: 0.0036258]
		[batch 20/20] avg loss: 0.21053570294180157		[learning rate: 0.0036202]
	Learning Rate: 0.00362021
	LOSS [training: 0.16893654330673805 | validation: 0.17979964835146534]
	TIME [epoch: 9.04 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13509591303495114		[learning rate: 0.0036147]
		[batch 20/20] avg loss: 0.15343269045953092		[learning rate: 0.0036091]
	Learning Rate: 0.00360911
	LOSS [training: 0.144264301747241 | validation: 0.21097206726089374]
	TIME [epoch: 9.05 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13116232153006305		[learning rate: 0.0036036]
		[batch 20/20] avg loss: 0.14328350341153334		[learning rate: 0.003598]
	Learning Rate: 0.00359805
	LOSS [training: 0.13722291247079815 | validation: 0.15672323008337866]
	TIME [epoch: 9.05 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15838040442180684		[learning rate: 0.0035925]
		[batch 20/20] avg loss: 0.14602950564358114		[learning rate: 0.003587]
	Learning Rate: 0.00358702
	LOSS [training: 0.152204955032694 | validation: 0.12386512516428859]
	TIME [epoch: 9.06 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21805605896584507		[learning rate: 0.0035815]
		[batch 20/20] avg loss: 0.1510919231136894		[learning rate: 0.003576]
	Learning Rate: 0.00357602
	LOSS [training: 0.18457399103976724 | validation: 0.1180804057954907]
	TIME [epoch: 9.06 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16349820275889096		[learning rate: 0.0035705]
		[batch 20/20] avg loss: 0.1696041423769241		[learning rate: 0.0035651]
	Learning Rate: 0.00356506
	LOSS [training: 0.1665511725679075 | validation: 0.10571826024106865]
	TIME [epoch: 9.05 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13902167718189914		[learning rate: 0.0035596]
		[batch 20/20] avg loss: 0.13042372276777786		[learning rate: 0.0035541]
	Learning Rate: 0.00355413
	LOSS [training: 0.1347226999748385 | validation: 0.14840985043324514]
	TIME [epoch: 9.05 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1256284862227515		[learning rate: 0.0035487]
		[batch 20/20] avg loss: 0.17083003281337739		[learning rate: 0.0035432]
	Learning Rate: 0.00354324
	LOSS [training: 0.14822925951806443 | validation: 0.12103650176394005]
	TIME [epoch: 9.04 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18604253723663872		[learning rate: 0.0035378]
		[batch 20/20] avg loss: 0.14019463155460127		[learning rate: 0.0035324]
	Learning Rate: 0.00353237
	LOSS [training: 0.16311858439561996 | validation: 0.10694933638108711]
	TIME [epoch: 9.06 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15184072277369906		[learning rate: 0.003527]
		[batch 20/20] avg loss: 0.16577645943769018		[learning rate: 0.0035215]
	Learning Rate: 0.00352155
	LOSS [training: 0.15880859110569462 | validation: 0.1281127838050348]
	TIME [epoch: 9.06 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15381163034337725		[learning rate: 0.0035161]
		[batch 20/20] avg loss: 0.2149346156231699		[learning rate: 0.0035108]
	Learning Rate: 0.00351075
	LOSS [training: 0.1843731229832736 | validation: 0.1123898791105861]
	TIME [epoch: 9.04 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12394230412106855		[learning rate: 0.0035054]
		[batch 20/20] avg loss: 0.17715708844762118		[learning rate: 0.0035]
	Learning Rate: 0.00349999
	LOSS [training: 0.15054969628434486 | validation: 0.1419010710071979]
	TIME [epoch: 9.05 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19440779235985975		[learning rate: 0.0034946]
		[batch 20/20] avg loss: 0.12016419533374419		[learning rate: 0.0034893]
	Learning Rate: 0.00348926
	LOSS [training: 0.15728599384680192 | validation: 0.1364997828274258]
	TIME [epoch: 9.05 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16823853262133473		[learning rate: 0.0034839]
		[batch 20/20] avg loss: 0.19145117252122068		[learning rate: 0.0034786]
	Learning Rate: 0.00347856
	LOSS [training: 0.17984485257127772 | validation: 0.07680334585039422]
	TIME [epoch: 9.05 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12025376851259631		[learning rate: 0.0034732]
		[batch 20/20] avg loss: 0.15177877906476595		[learning rate: 0.0034679]
	Learning Rate: 0.0034679
	LOSS [training: 0.1360162737886811 | validation: 0.1399811899850592]
	TIME [epoch: 9.08 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19090322396011822		[learning rate: 0.0034626]
		[batch 20/20] avg loss: 0.17122716204106536		[learning rate: 0.0034573]
	Learning Rate: 0.00345727
	LOSS [training: 0.18106519300059176 | validation: 0.21099157849448183]
	TIME [epoch: 9.04 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17946391691295632		[learning rate: 0.003452]
		[batch 20/20] avg loss: 0.18143839900687464		[learning rate: 0.0034467]
	Learning Rate: 0.00344667
	LOSS [training: 0.18045115795991545 | validation: 0.1748861237181039]
	TIME [epoch: 9.05 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18001924193067675		[learning rate: 0.0034414]
		[batch 20/20] avg loss: 0.1523427302028147		[learning rate: 0.0034361]
	Learning Rate: 0.00343611
	LOSS [training: 0.16618098606674572 | validation: 0.18980521012998824]
	TIME [epoch: 9.05 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.152925923217418		[learning rate: 0.0034308]
		[batch 20/20] avg loss: 0.1922632588642941		[learning rate: 0.0034256]
	Learning Rate: 0.00342557
	LOSS [training: 0.172594591040856 | validation: 0.3004418379552112]
	TIME [epoch: 9.05 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16263844189229468		[learning rate: 0.0034203]
		[batch 20/20] avg loss: 0.1259241378306725		[learning rate: 0.0034151]
	Learning Rate: 0.00341507
	LOSS [training: 0.14428128986148356 | validation: 0.12334416448360416]
	TIME [epoch: 9.07 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21300715031405426		[learning rate: 0.0034098]
		[batch 20/20] avg loss: 0.13001748624699783		[learning rate: 0.0034046]
	Learning Rate: 0.0034046
	LOSS [training: 0.17151231828052604 | validation: 0.11567382696030817]
	TIME [epoch: 9.06 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.124554248643191		[learning rate: 0.0033994]
		[batch 20/20] avg loss: 0.17546728828345914		[learning rate: 0.0033942]
	Learning Rate: 0.00339417
	LOSS [training: 0.1500107684633251 | validation: 0.20854517970569592]
	TIME [epoch: 9.05 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13056567725807963		[learning rate: 0.003389]
		[batch 20/20] avg loss: 0.1321838095994043		[learning rate: 0.0033838]
	Learning Rate: 0.00338376
	LOSS [training: 0.13137474342874192 | validation: 0.10415369325886249]
	TIME [epoch: 9.05 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19058418430603388		[learning rate: 0.0033786]
		[batch 20/20] avg loss: 0.21547334040718263		[learning rate: 0.0033734]
	Learning Rate: 0.00337339
	LOSS [training: 0.20302876235660822 | validation: 0.11319779659937113]
	TIME [epoch: 9.05 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.159839075134103		[learning rate: 0.0033682]
		[batch 20/20] avg loss: 0.15891211281668666		[learning rate: 0.0033631]
	Learning Rate: 0.00336305
	LOSS [training: 0.15937559397539486 | validation: 0.11806434811781857]
	TIME [epoch: 9.07 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13318474522427565		[learning rate: 0.0033579]
		[batch 20/20] avg loss: 0.15399224612330972		[learning rate: 0.0033527]
	Learning Rate: 0.00335274
	LOSS [training: 0.1435884956737927 | validation: 0.0906899687925438]
	TIME [epoch: 9.03 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1447832566268673		[learning rate: 0.0033476]
		[batch 20/20] avg loss: 0.13267440908022124		[learning rate: 0.0033425]
	Learning Rate: 0.00334246
	LOSS [training: 0.13872883285354426 | validation: 0.10654782369509086]
	TIME [epoch: 9.06 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11673658567270415		[learning rate: 0.0033373]
		[batch 20/20] avg loss: 0.13048278940190122		[learning rate: 0.0033322]
	Learning Rate: 0.00333222
	LOSS [training: 0.1236096875373027 | validation: 0.10343492064674692]
	TIME [epoch: 9.05 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13271636685065485		[learning rate: 0.0033271]
		[batch 20/20] avg loss: 0.12564975682032958		[learning rate: 0.003322]
	Learning Rate: 0.003322
	LOSS [training: 0.1291830618354922 | validation: 0.18190529500515148]
	TIME [epoch: 9.05 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16546605421482963		[learning rate: 0.0033169]
		[batch 20/20] avg loss: 0.16421105335174263		[learning rate: 0.0033118]
	Learning Rate: 0.00331182
	LOSS [training: 0.1648385537832861 | validation: 0.08479819661463288]
	TIME [epoch: 9.08 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1365028847318077		[learning rate: 0.0033067]
		[batch 20/20] avg loss: 0.14213247683422398		[learning rate: 0.0033017]
	Learning Rate: 0.00330167
	LOSS [training: 0.13931768078301582 | validation: 0.12338490030503978]
	TIME [epoch: 9.05 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1578276231293485		[learning rate: 0.0032966]
		[batch 20/20] avg loss: 0.15231550026472135		[learning rate: 0.0032915]
	Learning Rate: 0.00329155
	LOSS [training: 0.15507156169703498 | validation: 0.08549162037455585]
	TIME [epoch: 9.05 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13583239688678936		[learning rate: 0.0032865]
		[batch 20/20] avg loss: 0.1461539950200985		[learning rate: 0.0032815]
	Learning Rate: 0.00328146
	LOSS [training: 0.14099319595344395 | validation: 0.10184252840964539]
	TIME [epoch: 9.05 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.128935557250883		[learning rate: 0.0032764]
		[batch 20/20] avg loss: 0.15632135640416783		[learning rate: 0.0032714]
	Learning Rate: 0.0032714
	LOSS [training: 0.1426284568275254 | validation: 0.05464173017797384]
	TIME [epoch: 9.05 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1415566597411678		[learning rate: 0.0032664]
		[batch 20/20] avg loss: 0.11495046621919627		[learning rate: 0.0032614]
	Learning Rate: 0.00326137
	LOSS [training: 0.12825356298018203 | validation: 0.18137925581568126]
	TIME [epoch: 9.08 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14333931244419612		[learning rate: 0.0032564]
		[batch 20/20] avg loss: 0.1592213855173899		[learning rate: 0.0032514]
	Learning Rate: 0.00325137
	LOSS [training: 0.15128034898079304 | validation: 0.24260483327887133]
	TIME [epoch: 9.04 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13060958868092998		[learning rate: 0.0032464]
		[batch 20/20] avg loss: 0.15719563903517636		[learning rate: 0.0032414]
	Learning Rate: 0.00324141
	LOSS [training: 0.14390261385805317 | validation: 0.06549616281692552]
	TIME [epoch: 9.05 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10726510365992932		[learning rate: 0.0032364]
		[batch 20/20] avg loss: 0.15880027918193385		[learning rate: 0.0032315]
	Learning Rate: 0.00323147
	LOSS [training: 0.13303269142093155 | validation: 0.1605985411719345]
	TIME [epoch: 9.04 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16127044849827282		[learning rate: 0.0032265]
		[batch 20/20] avg loss: 0.1437788256077041		[learning rate: 0.0032216]
	Learning Rate: 0.00322156
	LOSS [training: 0.15252463705298847 | validation: 0.13659781471556343]
	TIME [epoch: 9.04 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17790400124904288		[learning rate: 0.0032166]
		[batch 20/20] avg loss: 0.1584213734911479		[learning rate: 0.0032117]
	Learning Rate: 0.00321169
	LOSS [training: 0.16816268737009535 | validation: 0.12313790894328962]
	TIME [epoch: 9.08 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14382484080976435		[learning rate: 0.0032068]
		[batch 20/20] avg loss: 0.09657724001408811		[learning rate: 0.0032018]
	Learning Rate: 0.00320184
	LOSS [training: 0.12020104041192622 | validation: 0.1578283259815537]
	TIME [epoch: 9.05 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14890087393311044		[learning rate: 0.0031969]
		[batch 20/20] avg loss: 0.12662203383144582		[learning rate: 0.003192]
	Learning Rate: 0.00319203
	LOSS [training: 0.13776145388227815 | validation: 0.11287447875875144]
	TIME [epoch: 9.05 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15338491521241515		[learning rate: 0.0031871]
		[batch 20/20] avg loss: 0.13229273244203502		[learning rate: 0.0031822]
	Learning Rate: 0.00318224
	LOSS [training: 0.14283882382722507 | validation: 0.11037600363765036]
	TIME [epoch: 9.06 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13409146393814786		[learning rate: 0.0031774]
		[batch 20/20] avg loss: 0.18562161928522847		[learning rate: 0.0031725]
	Learning Rate: 0.00317249
	LOSS [training: 0.15985654161168816 | validation: 0.0974468985042674]
	TIME [epoch: 9.04 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1285559670399392		[learning rate: 0.0031676]
		[batch 20/20] avg loss: 0.165096200442967		[learning rate: 0.0031628]
	Learning Rate: 0.00316276
	LOSS [training: 0.14682608374145306 | validation: 0.09030232857057169]
	TIME [epoch: 9.08 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11273983917157589		[learning rate: 0.0031579]
		[batch 20/20] avg loss: 0.1359540876234735		[learning rate: 0.0031531]
	Learning Rate: 0.00315307
	LOSS [training: 0.12434696339752471 | validation: 0.07049816535848032]
	TIME [epoch: 9.05 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14268513173253647		[learning rate: 0.0031482]
		[batch 20/20] avg loss: 0.14625164458714274		[learning rate: 0.0031434]
	Learning Rate: 0.0031434
	LOSS [training: 0.1444683881598396 | validation: 0.28012067935073476]
	TIME [epoch: 9.06 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1850537896081516		[learning rate: 0.0031386]
		[batch 20/20] avg loss: 0.20299713687195461		[learning rate: 0.0031338]
	Learning Rate: 0.00313377
	LOSS [training: 0.19402546324005313 | validation: 0.1292117997172001]
	TIME [epoch: 9.05 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1177940750724498		[learning rate: 0.003129]
		[batch 20/20] avg loss: 0.18341027110467936		[learning rate: 0.0031242]
	Learning Rate: 0.00312416
	LOSS [training: 0.15060217308856458 | validation: 0.07864544407308369]
	TIME [epoch: 9.05 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13925292245762833		[learning rate: 0.0031194]
		[batch 20/20] avg loss: 0.16003782469554076		[learning rate: 0.0031146]
	Learning Rate: 0.00311458
	LOSS [training: 0.14964537357658453 | validation: 0.11084093790699429]
	TIME [epoch: 9.07 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1389934719637977		[learning rate: 0.0031098]
		[batch 20/20] avg loss: 0.16146198639747833		[learning rate: 0.003105]
	Learning Rate: 0.00310504
	LOSS [training: 0.15022772918063804 | validation: 0.12547191880676573]
	TIME [epoch: 9.05 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11500259245763263		[learning rate: 0.0031003]
		[batch 20/20] avg loss: 0.10190612993222159		[learning rate: 0.0030955]
	Learning Rate: 0.00309552
	LOSS [training: 0.10845436119492713 | validation: 0.19123091858404292]
	TIME [epoch: 9.05 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15094231008777853		[learning rate: 0.0030908]
		[batch 20/20] avg loss: 0.163757343412261		[learning rate: 0.003086]
	Learning Rate: 0.00308603
	LOSS [training: 0.15734982675001977 | validation: 0.06187152434571002]
	TIME [epoch: 9.05 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18871350166577608		[learning rate: 0.0030813]
		[batch 20/20] avg loss: 0.1524626242330182		[learning rate: 0.0030766]
	Learning Rate: 0.00307657
	LOSS [training: 0.1705880629493971 | validation: 0.10752291267133116]
	TIME [epoch: 9.06 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12283143820678984		[learning rate: 0.0030718]
		[batch 20/20] avg loss: 0.1294242144802386		[learning rate: 0.0030671]
	Learning Rate: 0.00306714
	LOSS [training: 0.12612782634351422 | validation: 0.07693089213976802]
	TIME [epoch: 9.08 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1345387695898961		[learning rate: 0.0030624]
		[batch 20/20] avg loss: 0.14928547224206773		[learning rate: 0.0030577]
	Learning Rate: 0.00305774
	LOSS [training: 0.14191212091598193 | validation: 0.3037499983568685]
	TIME [epoch: 9.06 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20337159058414905		[learning rate: 0.003053]
		[batch 20/20] avg loss: 0.13634411344124484		[learning rate: 0.0030484]
	Learning Rate: 0.00304836
	LOSS [training: 0.16985785201269699 | validation: 0.12520647271203278]
	TIME [epoch: 9.05 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14007788834252988		[learning rate: 0.0030437]
		[batch 20/20] avg loss: 0.18636708020946793		[learning rate: 0.003039]
	Learning Rate: 0.00303902
	LOSS [training: 0.16322248427599892 | validation: 0.09786970046781208]
	TIME [epoch: 9.05 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13975958840669991		[learning rate: 0.0030344]
		[batch 20/20] avg loss: 0.18388419798482847		[learning rate: 0.0030297]
	Learning Rate: 0.0030297
	LOSS [training: 0.16182189319576418 | validation: 0.09005049287676023]
	TIME [epoch: 9.05 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1288266460469837		[learning rate: 0.0030251]
		[batch 20/20] avg loss: 0.1308491966933863		[learning rate: 0.0030204]
	Learning Rate: 0.00302042
	LOSS [training: 0.12983792137018496 | validation: 0.12674718504582483]
	TIME [epoch: 9.08 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1394341602458305		[learning rate: 0.0030158]
		[batch 20/20] avg loss: 0.15380303946039633		[learning rate: 0.0030112]
	Learning Rate: 0.00301116
	LOSS [training: 0.14661859985311337 | validation: 0.08789856172105993]
	TIME [epoch: 9.05 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12853721998750237		[learning rate: 0.0030065]
		[batch 20/20] avg loss: 0.11171770793528038		[learning rate: 0.0030019]
	Learning Rate: 0.00300193
	LOSS [training: 0.12012746396139136 | validation: 0.12941192383570274]
	TIME [epoch: 9.05 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13558447321178882		[learning rate: 0.0029973]
		[batch 20/20] avg loss: 0.11785139135122095		[learning rate: 0.0029927]
	Learning Rate: 0.00299272
	LOSS [training: 0.1267179322815049 | validation: 0.16488827150682736]
	TIME [epoch: 9.05 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1704123426880698		[learning rate: 0.0029881]
		[batch 20/20] avg loss: 0.1202019234224384		[learning rate: 0.0029835]
	Learning Rate: 0.00298355
	LOSS [training: 0.14530713305525414 | validation: 0.1386077100904867]
	TIME [epoch: 9.06 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15532838588904865		[learning rate: 0.002979]
		[batch 20/20] avg loss: 0.12385945322055636		[learning rate: 0.0029744]
	Learning Rate: 0.0029744
	LOSS [training: 0.1395939195548025 | validation: 0.10093133397348662]
	TIME [epoch: 9.08 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1600055735383198		[learning rate: 0.0029698]
		[batch 20/20] avg loss: 0.12410423038931521		[learning rate: 0.0029653]
	Learning Rate: 0.00296529
	LOSS [training: 0.14205490196381748 | validation: 0.07583793496487427]
	TIME [epoch: 9.06 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16954076546536007		[learning rate: 0.0029607]
		[batch 20/20] avg loss: 0.19850555616154555		[learning rate: 0.0029562]
	Learning Rate: 0.0029562
	LOSS [training: 0.18402316081345277 | validation: 0.1979138297743612]
	TIME [epoch: 9.06 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15837574987532604		[learning rate: 0.0029517]
		[batch 20/20] avg loss: 0.14178290063295723		[learning rate: 0.0029471]
	Learning Rate: 0.00294713
	LOSS [training: 0.15007932525414164 | validation: 0.23167824166763284]
	TIME [epoch: 9.06 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1639204821901852		[learning rate: 0.0029426]
		[batch 20/20] avg loss: 0.1600788155871542		[learning rate: 0.0029381]
	Learning Rate: 0.0029381
	LOSS [training: 0.1619996488886697 | validation: 0.08189421215228483]
	TIME [epoch: 9.07 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15023222738977624		[learning rate: 0.0029336]
		[batch 20/20] avg loss: 0.1668266858221397		[learning rate: 0.0029291]
	Learning Rate: 0.00292909
	LOSS [training: 0.158529456605958 | validation: 0.20274010937789144]
	TIME [epoch: 9.08 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11327944239612284		[learning rate: 0.0029246]
		[batch 20/20] avg loss: 0.13008462480879873		[learning rate: 0.0029201]
	Learning Rate: 0.00292011
	LOSS [training: 0.1216820336024608 | validation: 0.12302168000065186]
	TIME [epoch: 9.05 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11417294786414733		[learning rate: 0.0029156]
		[batch 20/20] avg loss: 0.11389040686712401		[learning rate: 0.0029112]
	Learning Rate: 0.00291116
	LOSS [training: 0.11403167736563569 | validation: 0.2804367235133872]
	TIME [epoch: 9.05 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1784438989085538		[learning rate: 0.0029067]
		[batch 20/20] avg loss: 0.1463994681584806		[learning rate: 0.0029022]
	Learning Rate: 0.00290224
	LOSS [training: 0.1624216835335172 | validation: 0.14600205007137343]
	TIME [epoch: 9.05 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17945035505593637		[learning rate: 0.0028978]
		[batch 20/20] avg loss: 0.11164036714416423		[learning rate: 0.0028933]
	Learning Rate: 0.00289334
	LOSS [training: 0.14554536110005029 | validation: 0.08309361754628433]
	TIME [epoch: 9.05 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15404358529867784		[learning rate: 0.0028889]
		[batch 20/20] avg loss: 0.1605720695011735		[learning rate: 0.0028845]
	Learning Rate: 0.00288447
	LOSS [training: 0.15730782739992563 | validation: 0.08576311319300718]
	TIME [epoch: 9.08 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09112087421883214		[learning rate: 0.00288]
		[batch 20/20] avg loss: 0.1530626003604691		[learning rate: 0.0028756]
	Learning Rate: 0.00287563
	LOSS [training: 0.12209173728965059 | validation: 0.15664123819502507]
	TIME [epoch: 9.06 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19800075105186715		[learning rate: 0.0028712]
		[batch 20/20] avg loss: 0.11666705031771626		[learning rate: 0.0028668]
	Learning Rate: 0.00286682
	LOSS [training: 0.15733390068479172 | validation: 0.19149421854920212]
	TIME [epoch: 9.06 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13736744146852595		[learning rate: 0.0028624]
		[batch 20/20] avg loss: 0.13969583019903156		[learning rate: 0.002858]
	Learning Rate: 0.00285803
	LOSS [training: 0.13853163583377878 | validation: 0.13566889690593345]
	TIME [epoch: 9.06 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16741655269719563		[learning rate: 0.0028536]
		[batch 20/20] avg loss: 0.1522348559517334		[learning rate: 0.0028493]
	Learning Rate: 0.00284927
	LOSS [training: 0.15982570432446455 | validation: 0.10971455966991647]
	TIME [epoch: 9.06 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1426304092356965		[learning rate: 0.0028449]
		[batch 20/20] avg loss: 0.12460370577313555		[learning rate: 0.0028405]
	Learning Rate: 0.00284053
	LOSS [training: 0.13361705750441605 | validation: 0.07800209867200252]
	TIME [epoch: 9.08 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11429442726580681		[learning rate: 0.0028362]
		[batch 20/20] avg loss: 0.14444988478291437		[learning rate: 0.0028318]
	Learning Rate: 0.00283183
	LOSS [training: 0.12937215602436059 | validation: 0.10501451514021043]
	TIME [epoch: 9.06 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09538133732815722		[learning rate: 0.0028275]
		[batch 20/20] avg loss: 0.12530997462647758		[learning rate: 0.0028231]
	Learning Rate: 0.00282315
	LOSS [training: 0.11034565597731741 | validation: 0.0780180071354881]
	TIME [epoch: 9.05 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12477914805186394		[learning rate: 0.0028188]
		[batch 20/20] avg loss: 0.13742003222533977		[learning rate: 0.0028145]
	Learning Rate: 0.00281449
	LOSS [training: 0.1310995901386019 | validation: 0.1305468444112171]
	TIME [epoch: 9.05 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15542989494020074		[learning rate: 0.0028102]
		[batch 20/20] avg loss: 0.14298190240183845		[learning rate: 0.0028059]
	Learning Rate: 0.00280586
	LOSS [training: 0.14920589867101958 | validation: 0.06844817461990389]
	TIME [epoch: 9.07 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18219850645033903		[learning rate: 0.0028016]
		[batch 20/20] avg loss: 0.12633081869639767		[learning rate: 0.0027973]
	Learning Rate: 0.00279726
	LOSS [training: 0.15426466257336838 | validation: 0.05042277539836682]
	TIME [epoch: 9.1 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11224673247791787		[learning rate: 0.002793]
		[batch 20/20] avg loss: 0.16463123696348092		[learning rate: 0.0027887]
	Learning Rate: 0.00278869
	LOSS [training: 0.1384389847206994 | validation: 0.14346233667815916]
	TIME [epoch: 9.08 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12304131469182455		[learning rate: 0.0027844]
		[batch 20/20] avg loss: 0.10624096467280893		[learning rate: 0.0027801]
	Learning Rate: 0.00278014
	LOSS [training: 0.11464113968231673 | validation: 0.05294060286831476]
	TIME [epoch: 9.07 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16930259678302448		[learning rate: 0.0027759]
		[batch 20/20] avg loss: 0.1303102288806453		[learning rate: 0.0027716]
	Learning Rate: 0.00277162
	LOSS [training: 0.1498064128318349 | validation: 0.11211406333518853]
	TIME [epoch: 9.07 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12134547143073988		[learning rate: 0.0027674]
		[batch 20/20] avg loss: 0.09975040504758342		[learning rate: 0.0027631]
	Learning Rate: 0.00276312
	LOSS [training: 0.11054793823916165 | validation: 0.09056028317551604]
	TIME [epoch: 9.07 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1267624510288963		[learning rate: 0.0027589]
		[batch 20/20] avg loss: 0.10589388772911597		[learning rate: 0.0027547]
	Learning Rate: 0.00275465
	LOSS [training: 0.11632816937900611 | validation: 0.0733710862145136]
	TIME [epoch: 9.1 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12247483098743621		[learning rate: 0.0027504]
		[batch 20/20] avg loss: 0.12904606964236184		[learning rate: 0.0027462]
	Learning Rate: 0.00274621
	LOSS [training: 0.12576045031489902 | validation: 0.06617415946052743]
	TIME [epoch: 9.06 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1092053680640623		[learning rate: 0.002742]
		[batch 20/20] avg loss: 0.13809715014053717		[learning rate: 0.0027378]
	Learning Rate: 0.00273779
	LOSS [training: 0.12365125910229975 | validation: 0.09081919203282399]
	TIME [epoch: 9.06 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11880302321209983		[learning rate: 0.0027336]
		[batch 20/20] avg loss: 0.13792218161460662		[learning rate: 0.0027294]
	Learning Rate: 0.0027294
	LOSS [training: 0.12836260241335323 | validation: 0.12790628697382123]
	TIME [epoch: 9.05 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1381919927056189		[learning rate: 0.0027252]
		[batch 20/20] avg loss: 0.13620719263511405		[learning rate: 0.002721]
	Learning Rate: 0.00272103
	LOSS [training: 0.13719959267036647 | validation: 0.10847058159410897]
	TIME [epoch: 9.05 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11489223432997475		[learning rate: 0.0027169]
		[batch 20/20] avg loss: 0.11608808106032571		[learning rate: 0.0027127]
	Learning Rate: 0.00271269
	LOSS [training: 0.11549015769515025 | validation: 0.15999881990018164]
	TIME [epoch: 9.07 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14520821565956252		[learning rate: 0.0027085]
		[batch 20/20] avg loss: 0.11609995578838073		[learning rate: 0.0027044]
	Learning Rate: 0.00270437
	LOSS [training: 0.13065408572397164 | validation: 0.10215359158379446]
	TIME [epoch: 9.06 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10274298648293638		[learning rate: 0.0027002]
		[batch 20/20] avg loss: 0.12179702755110748		[learning rate: 0.0026961]
	Learning Rate: 0.00269608
	LOSS [training: 0.11227000701702192 | validation: 0.09591488617269729]
	TIME [epoch: 9.05 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11828213960304937		[learning rate: 0.0026919]
		[batch 20/20] avg loss: 0.1272091283213684		[learning rate: 0.0026878]
	Learning Rate: 0.00268782
	LOSS [training: 0.1227456339622089 | validation: 0.060924942946673236]
	TIME [epoch: 9.05 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11505106921001074		[learning rate: 0.0026837]
		[batch 20/20] avg loss: 0.11495613593150363		[learning rate: 0.0026796]
	Learning Rate: 0.00267958
	LOSS [training: 0.11500360257075719 | validation: 0.08326550056939312]
	TIME [epoch: 9.06 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1792164051375991		[learning rate: 0.0026755]
		[batch 20/20] avg loss: 0.15573627546143803		[learning rate: 0.0026714]
	Learning Rate: 0.00267137
	LOSS [training: 0.16747634029951855 | validation: 0.08890844655906832]
	TIME [epoch: 9.07 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11717346475325482		[learning rate: 0.0026673]
		[batch 20/20] avg loss: 0.10357292905413314		[learning rate: 0.0026632]
	Learning Rate: 0.00266318
	LOSS [training: 0.110373196903694 | validation: 0.1763265813400161]
	TIME [epoch: 9.07 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1420312642809017		[learning rate: 0.0026591]
		[batch 20/20] avg loss: 0.11432274910574033		[learning rate: 0.002655]
	Learning Rate: 0.00265501
	LOSS [training: 0.12817700669332105 | validation: 0.1384919742247534]
	TIME [epoch: 9.06 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1749141452485572		[learning rate: 0.0026509]
		[batch 20/20] avg loss: 0.18533137313338863		[learning rate: 0.0026469]
	Learning Rate: 0.00264687
	LOSS [training: 0.18012275919097293 | validation: 0.14060427552002144]
	TIME [epoch: 9.06 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1165801738973413		[learning rate: 0.0026428]
		[batch 20/20] avg loss: 0.17418084734903033		[learning rate: 0.0026388]
	Learning Rate: 0.00263876
	LOSS [training: 0.1453805106231858 | validation: 0.09742257368513717]
	TIME [epoch: 9.05 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15271208588363439		[learning rate: 0.0026347]
		[batch 20/20] avg loss: 0.12752097941473983		[learning rate: 0.0026307]
	Learning Rate: 0.00263067
	LOSS [training: 0.14011653264918708 | validation: 0.1124527381589702]
	TIME [epoch: 9.08 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18402250071760853		[learning rate: 0.0026266]
		[batch 20/20] avg loss: 0.1805488363418182		[learning rate: 0.0026226]
	Learning Rate: 0.00262261
	LOSS [training: 0.18228566852971334 | validation: 0.08887848466291831]
	TIME [epoch: 9.07 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1382901265890033		[learning rate: 0.0026186]
		[batch 20/20] avg loss: 0.15292589368840842		[learning rate: 0.0026146]
	Learning Rate: 0.00261457
	LOSS [training: 0.14560801013870586 | validation: 0.052594004238592604]
	TIME [epoch: 9.06 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10575254638025333		[learning rate: 0.0026106]
		[batch 20/20] avg loss: 0.08570301255696725		[learning rate: 0.0026066]
	Learning Rate: 0.00260655
	LOSS [training: 0.0957277794686103 | validation: 0.07500457526289217]
	TIME [epoch: 9.06 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1222234369914009		[learning rate: 0.0026026]
		[batch 20/20] avg loss: 0.11450111290337524		[learning rate: 0.0025986]
	Learning Rate: 0.00259856
	LOSS [training: 0.11836227494738807 | validation: 0.12966037606998077]
	TIME [epoch: 9.06 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12657134015201135		[learning rate: 0.0025946]
		[batch 20/20] avg loss: 0.17631727056739777		[learning rate: 0.0025906]
	Learning Rate: 0.0025906
	LOSS [training: 0.15144430535970452 | validation: 0.052433043931933467]
	TIME [epoch: 9.08 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11885795948468958		[learning rate: 0.0025866]
		[batch 20/20] avg loss: 0.10969854152718117		[learning rate: 0.0025827]
	Learning Rate: 0.00258266
	LOSS [training: 0.11427825050593539 | validation: 0.15178376247273523]
	TIME [epoch: 9.06 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09870707802410776		[learning rate: 0.0025787]
		[batch 20/20] avg loss: 0.13180539693414356		[learning rate: 0.0025747]
	Learning Rate: 0.00257474
	LOSS [training: 0.11525623747912568 | validation: 0.053932458789754764]
	TIME [epoch: 9.06 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2158486516412498		[learning rate: 0.0025708]
		[batch 20/20] avg loss: 0.13891229409652456		[learning rate: 0.0025668]
	Learning Rate: 0.00256685
	LOSS [training: 0.17738047286888717 | validation: 0.10014186082269766]
	TIME [epoch: 9.05 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1397077090262484		[learning rate: 0.0025629]
		[batch 20/20] avg loss: 0.11567320830265133		[learning rate: 0.002559]
	Learning Rate: 0.00255898
	LOSS [training: 0.12769045866444986 | validation: 0.15310691334262402]
	TIME [epoch: 9.06 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11436369866330875		[learning rate: 0.0025551]
		[batch 20/20] avg loss: 0.12425316453890825		[learning rate: 0.0025511]
	Learning Rate: 0.00255113
	LOSS [training: 0.11930843160110849 | validation: 0.07315756503428134]
	TIME [epoch: 9.09 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10086204859340038		[learning rate: 0.0025472]
		[batch 20/20] avg loss: 0.09326142669375863		[learning rate: 0.0025433]
	Learning Rate: 0.00254331
	LOSS [training: 0.0970617376435795 | validation: 0.08028681506641425]
	TIME [epoch: 9.07 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15388623575363627		[learning rate: 0.0025394]
		[batch 20/20] avg loss: 0.11786325330858002		[learning rate: 0.0025355]
	Learning Rate: 0.00253552
	LOSS [training: 0.13587474453110812 | validation: 0.1727584444300423]
	TIME [epoch: 9.06 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12410317478689006		[learning rate: 0.0025316]
		[batch 20/20] avg loss: 0.09861919887247732		[learning rate: 0.0025277]
	Learning Rate: 0.00252774
	LOSS [training: 0.11136118682968368 | validation: 0.08732296789928337]
	TIME [epoch: 9.06 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1247480800642615		[learning rate: 0.0025239]
		[batch 20/20] avg loss: 0.14884140386410571		[learning rate: 0.00252]
	Learning Rate: 0.00252
	LOSS [training: 0.1367947419641836 | validation: 0.059450220602416184]
	TIME [epoch: 9.06 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09837134992780974		[learning rate: 0.0025161]
		[batch 20/20] avg loss: 0.12160274331013574		[learning rate: 0.0025123]
	Learning Rate: 0.00251227
	LOSS [training: 0.10998704661897277 | validation: 0.055687293526994336]
	TIME [epoch: 9.09 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09059947315216167		[learning rate: 0.0025084]
		[batch 20/20] avg loss: 0.1094482222613397		[learning rate: 0.0025046]
	Learning Rate: 0.00250457
	LOSS [training: 0.1000238477067507 | validation: 0.08021072659073636]
	TIME [epoch: 9.07 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11225192503715861		[learning rate: 0.0025007]
		[batch 20/20] avg loss: 0.09038262979114067		[learning rate: 0.0024969]
	Learning Rate: 0.00249689
	LOSS [training: 0.10131727741414964 | validation: 0.10515324379370587]
	TIME [epoch: 9.06 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08626342033939176		[learning rate: 0.0024931]
		[batch 20/20] avg loss: 0.09128288160421831		[learning rate: 0.0024892]
	Learning Rate: 0.00248924
	LOSS [training: 0.08877315097180505 | validation: 0.13770588331486025]
	TIME [epoch: 9.06 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10912816042317945		[learning rate: 0.0024854]
		[batch 20/20] avg loss: 0.08414479815174251		[learning rate: 0.0024816]
	Learning Rate: 0.00248161
	LOSS [training: 0.09663647928746098 | validation: 0.1327365291379755]
	TIME [epoch: 9.07 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12046202088779741		[learning rate: 0.0024778]
		[batch 20/20] avg loss: 0.13137146270865832		[learning rate: 0.002474]
	Learning Rate: 0.002474
	LOSS [training: 0.12591674179822784 | validation: 0.09572276263596015]
	TIME [epoch: 9.09 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0957610707232716		[learning rate: 0.0024702]
		[batch 20/20] avg loss: 0.11273346028334577		[learning rate: 0.0024664]
	Learning Rate: 0.00246642
	LOSS [training: 0.10424726550330868 | validation: 0.06083710671287919]
	TIME [epoch: 9.06 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12485594251407299		[learning rate: 0.0024626]
		[batch 20/20] avg loss: 0.1279374237569908		[learning rate: 0.0024589]
	Learning Rate: 0.00245886
	LOSS [training: 0.1263966831355319 | validation: 0.06866141526765782]
	TIME [epoch: 9.06 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11628085720394128		[learning rate: 0.0024551]
		[batch 20/20] avg loss: 0.10569939639045853		[learning rate: 0.0024513]
	Learning Rate: 0.00245132
	LOSS [training: 0.11099012679719991 | validation: 0.08712024735894028]
	TIME [epoch: 9.06 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09440877025739687		[learning rate: 0.0024476]
		[batch 20/20] avg loss: 0.12312607612028278		[learning rate: 0.0024438]
	Learning Rate: 0.00244381
	LOSS [training: 0.10876742318883985 | validation: 0.10928366231609665]
	TIME [epoch: 9.06 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11518762618557135		[learning rate: 0.0024401]
		[batch 20/20] avg loss: 0.14426620535460183		[learning rate: 0.0024363]
	Learning Rate: 0.00243631
	LOSS [training: 0.12972691577008658 | validation: 0.09562044377653964]
	TIME [epoch: 9.08 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10606177372489042		[learning rate: 0.0024326]
		[batch 20/20] avg loss: 0.09997229294592254		[learning rate: 0.0024288]
	Learning Rate: 0.00242885
	LOSS [training: 0.10301703333540649 | validation: 0.12974345133280452]
	TIME [epoch: 9.06 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11296048971325287		[learning rate: 0.0024251]
		[batch 20/20] avg loss: 0.10224449335957637		[learning rate: 0.0024214]
	Learning Rate: 0.0024214
	LOSS [training: 0.10760249153641459 | validation: 0.06815357872053031]
	TIME [epoch: 9.07 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09666269628764967		[learning rate: 0.0024177]
		[batch 20/20] avg loss: 0.12382172996199332		[learning rate: 0.002414]
	Learning Rate: 0.00241398
	LOSS [training: 0.11024221312482148 | validation: 0.12433882001077878]
	TIME [epoch: 9.06 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07887645563759939		[learning rate: 0.0024103]
		[batch 20/20] avg loss: 0.1509724173872517		[learning rate: 0.0024066]
	Learning Rate: 0.00240658
	LOSS [training: 0.11492443651242554 | validation: 0.08169449180332074]
	TIME [epoch: 9.07 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10824590433720942		[learning rate: 0.0024029]
		[batch 20/20] avg loss: 0.08143755374584116		[learning rate: 0.0023992]
	Learning Rate: 0.0023992
	LOSS [training: 0.09484172904152528 | validation: 0.08020738609450521]
	TIME [epoch: 9.08 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10747581582067492		[learning rate: 0.0023955]
		[batch 20/20] avg loss: 0.09178667346354001		[learning rate: 0.0023918]
	Learning Rate: 0.00239185
	LOSS [training: 0.09963124464210746 | validation: 0.10861918555107765]
	TIME [epoch: 9.06 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14770019894215297		[learning rate: 0.0023882]
		[batch 20/20] avg loss: 0.10070006893110099		[learning rate: 0.0023845]
	Learning Rate: 0.00238451
	LOSS [training: 0.12420013393662696 | validation: 0.08336745529692229]
	TIME [epoch: 9.06 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12352848591407513		[learning rate: 0.0023809]
		[batch 20/20] avg loss: 0.1163389964922706		[learning rate: 0.0023772]
	Learning Rate: 0.00237721
	LOSS [training: 0.11993374120317288 | validation: 0.1644645473531214]
	TIME [epoch: 9.06 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16161682252392726		[learning rate: 0.0023736]
		[batch 20/20] avg loss: 0.14136122761895922		[learning rate: 0.0023699]
	Learning Rate: 0.00236992
	LOSS [training: 0.1514890250714432 | validation: 0.09838749128338337]
	TIME [epoch: 9.06 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15119272629828984		[learning rate: 0.0023663]
		[batch 20/20] avg loss: 0.110205164851837		[learning rate: 0.0023627]
	Learning Rate: 0.00236265
	LOSS [training: 0.1306989455750634 | validation: 0.07839722812557667]
	TIME [epoch: 9.09 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1098407612051739		[learning rate: 0.002359]
		[batch 20/20] avg loss: 0.13705808708458028		[learning rate: 0.0023554]
	Learning Rate: 0.00235541
	LOSS [training: 0.12344942414487707 | validation: 0.09445844276526753]
	TIME [epoch: 9.06 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10955801552501021		[learning rate: 0.0023518]
		[batch 20/20] avg loss: 0.1634569954661792		[learning rate: 0.0023482]
	Learning Rate: 0.00234819
	LOSS [training: 0.1365075054955947 | validation: 0.11019394025946477]
	TIME [epoch: 9.06 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13400195190574676		[learning rate: 0.0023446]
		[batch 20/20] avg loss: 0.14493757430734494		[learning rate: 0.002341]
	Learning Rate: 0.00234099
	LOSS [training: 0.13946976310654588 | validation: 0.13048084010962668]
	TIME [epoch: 9.06 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15100605858872862		[learning rate: 0.0023374]
		[batch 20/20] avg loss: 0.1291966108709203		[learning rate: 0.0023338]
	Learning Rate: 0.00233382
	LOSS [training: 0.14010133472982447 | validation: 0.11410240570602365]
	TIME [epoch: 9.06 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15217718431802624		[learning rate: 0.0023302]
		[batch 20/20] avg loss: 0.13706542553333584		[learning rate: 0.0023267]
	Learning Rate: 0.00232666
	LOSS [training: 0.14462130492568107 | validation: 0.1334170275602028]
	TIME [epoch: 9.08 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10571929100437176		[learning rate: 0.0023231]
		[batch 20/20] avg loss: 0.12553126668849868		[learning rate: 0.0023195]
	Learning Rate: 0.00231953
	LOSS [training: 0.11562527884643523 | validation: 0.10733376715233986]
	TIME [epoch: 9.06 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10154346255796369		[learning rate: 0.002316]
		[batch 20/20] avg loss: 0.1290108557956822		[learning rate: 0.0023124]
	Learning Rate: 0.00231242
	LOSS [training: 0.11527715917682295 | validation: 0.11170447130188767]
	TIME [epoch: 9.06 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11101126413928115		[learning rate: 0.0023089]
		[batch 20/20] avg loss: 0.14496754721949373		[learning rate: 0.0023053]
	Learning Rate: 0.00230533
	LOSS [training: 0.12798940567938746 | validation: 0.21297480959102943]
	TIME [epoch: 9.06 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13356159647870014		[learning rate: 0.0023018]
		[batch 20/20] avg loss: 0.0844346222825564		[learning rate: 0.0022983]
	Learning Rate: 0.00229826
	LOSS [training: 0.10899810938062826 | validation: 0.09001100101091046]
	TIME [epoch: 9.06 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07611732689378657		[learning rate: 0.0022947]
		[batch 20/20] avg loss: 0.11653702786813339		[learning rate: 0.0022912]
	Learning Rate: 0.00229122
	LOSS [training: 0.09632717738095999 | validation: 0.12469273774329162]
	TIME [epoch: 9.09 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.100902189520471		[learning rate: 0.0022877]
		[batch 20/20] avg loss: 0.111710264801796		[learning rate: 0.0022842]
	Learning Rate: 0.0022842
	LOSS [training: 0.10630622716113351 | validation: 0.11671132792667022]
	TIME [epoch: 9.06 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10346261980039444		[learning rate: 0.0022807]
		[batch 20/20] avg loss: 0.10823665140277569		[learning rate: 0.0022772]
	Learning Rate: 0.00227719
	LOSS [training: 0.10584963560158504 | validation: 0.05975900312710668]
	TIME [epoch: 9.06 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12390597597259398		[learning rate: 0.0022737]
		[batch 20/20] avg loss: 0.09782323373924359		[learning rate: 0.0022702]
	Learning Rate: 0.00227021
	LOSS [training: 0.1108646048559188 | validation: 0.10747721345986935]
	TIME [epoch: 9.06 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1078553803122925		[learning rate: 0.0022667]
		[batch 20/20] avg loss: 0.13199626437104536		[learning rate: 0.0022633]
	Learning Rate: 0.00226325
	LOSS [training: 0.11992582234166893 | validation: 0.09793123265967865]
	TIME [epoch: 9.06 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12409515050767124		[learning rate: 0.0022598]
		[batch 20/20] avg loss: 0.10456605965419849		[learning rate: 0.0022563]
	Learning Rate: 0.00225632
	LOSS [training: 0.11433060508093489 | validation: 0.1174449232173374]
	TIME [epoch: 9.09 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12381373894062439		[learning rate: 0.0022529]
		[batch 20/20] avg loss: 0.11036172591415871		[learning rate: 0.0022494]
	Learning Rate: 0.0022494
	LOSS [training: 0.11708773242739157 | validation: 0.09347441637310433]
	TIME [epoch: 9.06 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10744560590762169		[learning rate: 0.0022459]
		[batch 20/20] avg loss: 0.1253721048296673		[learning rate: 0.0022425]
	Learning Rate: 0.0022425
	LOSS [training: 0.1164088553686445 | validation: 0.1189509982617086]
	TIME [epoch: 9.07 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11667784845624987		[learning rate: 0.0022391]
		[batch 20/20] avg loss: 0.13552083458259218		[learning rate: 0.0022356]
	Learning Rate: 0.00223563
	LOSS [training: 0.126099341519421 | validation: 0.1529217411484645]
	TIME [epoch: 9.06 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10289127207161668		[learning rate: 0.0022322]
		[batch 20/20] avg loss: 0.08780146584274058		[learning rate: 0.0022288]
	Learning Rate: 0.00222878
	LOSS [training: 0.09534636895717863 | validation: 0.06077450347258674]
	TIME [epoch: 9.07 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11180526841093512		[learning rate: 0.0022254]
		[batch 20/20] avg loss: 0.09765396806013273		[learning rate: 0.0022219]
	Learning Rate: 0.00222194
	LOSS [training: 0.10472961823553392 | validation: 0.09717519963670695]
	TIME [epoch: 9.09 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09688818091565513		[learning rate: 0.0022185]
		[batch 20/20] avg loss: 0.09717880596137231		[learning rate: 0.0022151]
	Learning Rate: 0.00221513
	LOSS [training: 0.09703349343851372 | validation: 0.06951353782792462]
	TIME [epoch: 9.07 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09037890560659828		[learning rate: 0.0022117]
		[batch 20/20] avg loss: 0.11347456370587136		[learning rate: 0.0022083]
	Learning Rate: 0.00220834
	LOSS [training: 0.10192673465623484 | validation: 0.09735557440964868]
	TIME [epoch: 9.06 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12291547805353042		[learning rate: 0.002205]
		[batch 20/20] avg loss: 0.10494572183151703		[learning rate: 0.0022016]
	Learning Rate: 0.00220157
	LOSS [training: 0.11393059994252372 | validation: 0.1004266482744383]
	TIME [epoch: 9.44 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11933336075672826		[learning rate: 0.0021982]
		[batch 20/20] avg loss: 0.12287852478381392		[learning rate: 0.0021948]
	Learning Rate: 0.00219483
	LOSS [training: 0.12110594277027109 | validation: 0.09391444953181916]
	TIME [epoch: 9.06 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1473280995283079		[learning rate: 0.0021915]
		[batch 20/20] avg loss: 0.11417250030787107		[learning rate: 0.0021881]
	Learning Rate: 0.0021881
	LOSS [training: 0.13075029991808948 | validation: 0.12746217964885392]
	TIME [epoch: 9.08 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11665748952484534		[learning rate: 0.0021847]
		[batch 20/20] avg loss: 0.15289952536811832		[learning rate: 0.0021814]
	Learning Rate: 0.00218139
	LOSS [training: 0.13477850744648187 | validation: 0.1561316630551094]
	TIME [epoch: 9.06 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13062971947412666		[learning rate: 0.002178]
		[batch 20/20] avg loss: 0.12438924365091808		[learning rate: 0.0021747]
	Learning Rate: 0.0021747
	LOSS [training: 0.12750948156252234 | validation: 0.08700572164184045]
	TIME [epoch: 9.06 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09716974232513616		[learning rate: 0.0021714]
		[batch 20/20] avg loss: 0.16182464151595072		[learning rate: 0.002168]
	Learning Rate: 0.00216804
	LOSS [training: 0.12949719192054346 | validation: 0.15133004702812977]
	TIME [epoch: 9.06 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12786687008667635		[learning rate: 0.0021647]
		[batch 20/20] avg loss: 0.09656244303260018		[learning rate: 0.0021614]
	Learning Rate: 0.00216139
	LOSS [training: 0.11221465655963828 | validation: 0.06101721938632898]
	TIME [epoch: 9.07 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10960134425304573		[learning rate: 0.0021581]
		[batch 20/20] avg loss: 0.10709796306097649		[learning rate: 0.0021548]
	Learning Rate: 0.00215477
	LOSS [training: 0.1083496536570111 | validation: 0.12230629690741951]
	TIME [epoch: 9.08 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1298806420949409		[learning rate: 0.0021515]
		[batch 20/20] avg loss: 0.12561470969163363		[learning rate: 0.0021482]
	Learning Rate: 0.00214816
	LOSS [training: 0.12774767589328725 | validation: 0.20127395725522224]
	TIME [epoch: 9.07 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13744708730392913		[learning rate: 0.0021449]
		[batch 20/20] avg loss: 0.11267667630074155		[learning rate: 0.0021416]
	Learning Rate: 0.00214158
	LOSS [training: 0.12506188180233538 | validation: 0.13283135463557028]
	TIME [epoch: 9.06 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14552147957917766		[learning rate: 0.0021383]
		[batch 20/20] avg loss: 0.11748009667592929		[learning rate: 0.002135]
	Learning Rate: 0.00213501
	LOSS [training: 0.13150078812755348 | validation: 0.06502693785824987]
	TIME [epoch: 9.05 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.111345189378117		[learning rate: 0.0021317]
		[batch 20/20] avg loss: 0.13952247343188712		[learning rate: 0.0021285]
	Learning Rate: 0.00212847
	LOSS [training: 0.12543383140500206 | validation: 0.1661111233081169]
	TIME [epoch: 9.07 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11409039888439312		[learning rate: 0.0021252]
		[batch 20/20] avg loss: 0.1713946503330615		[learning rate: 0.0021219]
	Learning Rate: 0.00212194
	LOSS [training: 0.1427425246087273 | validation: 0.10536873161312921]
	TIME [epoch: 9.08 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12414541588800734		[learning rate: 0.0021187]
		[batch 20/20] avg loss: 0.12784112736971864		[learning rate: 0.0021154]
	Learning Rate: 0.00211544
	LOSS [training: 0.12599327162886298 | validation: 0.12700201395753477]
	TIME [epoch: 9.12 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12539059454551377		[learning rate: 0.0021122]
		[batch 20/20] avg loss: 0.14888368697003945		[learning rate: 0.002109]
	Learning Rate: 0.00210895
	LOSS [training: 0.13713714075777658 | validation: 0.14020838701995095]
	TIME [epoch: 9.06 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13073007138115073		[learning rate: 0.0021057]
		[batch 20/20] avg loss: 0.13638815645974817		[learning rate: 0.0021025]
	Learning Rate: 0.00210249
	LOSS [training: 0.13355911392044947 | validation: 0.07979841100958676]
	TIME [epoch: 9.06 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11844390892605161		[learning rate: 0.0020993]
		[batch 20/20] avg loss: 0.11149780504673079		[learning rate: 0.002096]
	Learning Rate: 0.00209604
	LOSS [training: 0.1149708569863912 | validation: 0.07754206119210841]
	TIME [epoch: 9.06 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08667976284512034		[learning rate: 0.0020928]
		[batch 20/20] avg loss: 0.11026697219680429		[learning rate: 0.0020896]
	Learning Rate: 0.00208962
	LOSS [training: 0.09847336752096233 | validation: 0.11837103705027159]
	TIME [epoch: 9.08 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10547669197661724		[learning rate: 0.0020864]
		[batch 20/20] avg loss: 0.14567410015083898		[learning rate: 0.0020832]
	Learning Rate: 0.00208321
	LOSS [training: 0.1255753960637281 | validation: 0.16355006332752853]
	TIME [epoch: 9.05 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.135846727062944		[learning rate: 0.00208]
		[batch 20/20] avg loss: 0.12224594929098949		[learning rate: 0.0020768]
	Learning Rate: 0.00207683
	LOSS [training: 0.12904633817696676 | validation: 0.07798078604527964]
	TIME [epoch: 9.06 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09333979221402885		[learning rate: 0.0020736]
		[batch 20/20] avg loss: 0.09436333190729636		[learning rate: 0.0020705]
	Learning Rate: 0.00207046
	LOSS [training: 0.09385156206066263 | validation: 0.16439937012194042]
	TIME [epoch: 9.07 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11203524154186273		[learning rate: 0.0020673]
		[batch 20/20] avg loss: 0.09456786308361023		[learning rate: 0.0020641]
	Learning Rate: 0.00206411
	LOSS [training: 0.1033015523127365 | validation: 0.09464120829557762]
	TIME [epoch: 9.05 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09666565719760568		[learning rate: 0.0020609]
		[batch 20/20] avg loss: 0.1412678976458309		[learning rate: 0.0020578]
	Learning Rate: 0.00205778
	LOSS [training: 0.11896677742171828 | validation: 0.09559806418161665]
	TIME [epoch: 9.08 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09647535277521453		[learning rate: 0.0020546]
		[batch 20/20] avg loss: 0.09115618248562016		[learning rate: 0.0020515]
	Learning Rate: 0.00205148
	LOSS [training: 0.09381576763041735 | validation: 0.09577052589700795]
	TIME [epoch: 9.05 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08965720600031554		[learning rate: 0.0020483]
		[batch 20/20] avg loss: 0.0854284854629493		[learning rate: 0.0020452]
	Learning Rate: 0.00204519
	LOSS [training: 0.08754284573163242 | validation: 0.05055057269062664]
	TIME [epoch: 9.05 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0961152584277743		[learning rate: 0.0020421]
		[batch 20/20] avg loss: 0.1131265374817166		[learning rate: 0.0020389]
	Learning Rate: 0.00203892
	LOSS [training: 0.10462089795474545 | validation: 0.10354411835664586]
	TIME [epoch: 9.06 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11449484385916289		[learning rate: 0.0020358]
		[batch 20/20] avg loss: 0.1318805097747336		[learning rate: 0.0020327]
	Learning Rate: 0.00203267
	LOSS [training: 0.12318767681694824 | validation: 0.10401539409030379]
	TIME [epoch: 9.06 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16619965998982025		[learning rate: 0.0020296]
		[batch 20/20] avg loss: 0.09571822529625433		[learning rate: 0.0020264]
	Learning Rate: 0.00202644
	LOSS [training: 0.13095894264303728 | validation: 0.053594598556266526]
	TIME [epoch: 9.08 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07977050061917498		[learning rate: 0.0020233]
		[batch 20/20] avg loss: 0.09043882632392526		[learning rate: 0.0020202]
	Learning Rate: 0.00202023
	LOSS [training: 0.08510466347155013 | validation: 0.07946124279715894]
	TIME [epoch: 9.06 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10944190165561393		[learning rate: 0.0020171]
		[batch 20/20] avg loss: 0.0935614792722282		[learning rate: 0.002014]
	Learning Rate: 0.00201403
	LOSS [training: 0.10150169046392106 | validation: 0.07659379598405137]
	TIME [epoch: 9.07 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10019068109679549		[learning rate: 0.0020109]
		[batch 20/20] avg loss: 0.07649746147575287		[learning rate: 0.0020079]
	Learning Rate: 0.00200786
	LOSS [training: 0.0883440712862742 | validation: 0.07325432078169586]
	TIME [epoch: 9.05 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.105952564045702		[learning rate: 0.0020048]
		[batch 20/20] avg loss: 0.09053619859781176		[learning rate: 0.0020017]
	Learning Rate: 0.0020017
	LOSS [training: 0.09824438132175686 | validation: 0.06386364223114893]
	TIME [epoch: 9.05 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10029056816160364		[learning rate: 0.0019986]
		[batch 20/20] avg loss: 0.0863158046131457		[learning rate: 0.0019956]
	Learning Rate: 0.00199557
	LOSS [training: 0.09330318638737466 | validation: 0.08767314311364263]
	TIME [epoch: 9.08 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12386400837536567		[learning rate: 0.0019925]
		[batch 20/20] avg loss: 0.08730734561896414		[learning rate: 0.0019895]
	Learning Rate: 0.00198945
	LOSS [training: 0.10558567699716492 | validation: 0.06884158037038088]
	TIME [epoch: 9.06 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10711863768455387		[learning rate: 0.0019864]
		[batch 20/20] avg loss: 0.10473159186397		[learning rate: 0.0019834]
	Learning Rate: 0.00198335
	LOSS [training: 0.10592511477426195 | validation: 0.059085292657363404]
	TIME [epoch: 9.05 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07733753121091089		[learning rate: 0.0019803]
		[batch 20/20] avg loss: 0.1059359795375737		[learning rate: 0.0019773]
	Learning Rate: 0.00197727
	LOSS [training: 0.09163675537424229 | validation: 0.030355355739808722]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_1028.pth
	Model improved!!!
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10735486566464938		[learning rate: 0.0019742]
		[batch 20/20] avg loss: 0.08149582891951666		[learning rate: 0.0019712]
	Learning Rate: 0.00197121
	LOSS [training: 0.09442534729208303 | validation: 0.10450163214379989]
	TIME [epoch: 9.07 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10433563041099747		[learning rate: 0.0019682]
		[batch 20/20] avg loss: 0.0948515439339526		[learning rate: 0.0019652]
	Learning Rate: 0.00196517
	LOSS [training: 0.09959358717247503 | validation: 0.06296593229181172]
	TIME [epoch: 9.08 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09271701131209133		[learning rate: 0.0019622]
		[batch 20/20] avg loss: 0.09606068703673888		[learning rate: 0.0019591]
	Learning Rate: 0.00195915
	LOSS [training: 0.0943888491744151 | validation: 0.11650251050806731]
	TIME [epoch: 9.05 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09154641460529965		[learning rate: 0.0019561]
		[batch 20/20] avg loss: 0.12581586104525105		[learning rate: 0.0019531]
	Learning Rate: 0.00195314
	LOSS [training: 0.10868113782527536 | validation: 0.07978162952621952]
	TIME [epoch: 9.05 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10284405978458352		[learning rate: 0.0019501]
		[batch 20/20] avg loss: 0.08052487545094969		[learning rate: 0.0019472]
	Learning Rate: 0.00194715
	LOSS [training: 0.09168446761776658 | validation: 0.07640269298220456]
	TIME [epoch: 9.05 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0777965516271186		[learning rate: 0.0019442]
		[batch 20/20] avg loss: 0.11682865611003257		[learning rate: 0.0019412]
	Learning Rate: 0.00194118
	LOSS [training: 0.09731260386857558 | validation: 0.08194468591194005]
	TIME [epoch: 9.05 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13066532117819668		[learning rate: 0.0019382]
		[batch 20/20] avg loss: 0.1517984097103257		[learning rate: 0.0019352]
	Learning Rate: 0.00193523
	LOSS [training: 0.1412318654442612 | validation: 0.08872426938168604]
	TIME [epoch: 9.08 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1292669276189075		[learning rate: 0.0019323]
		[batch 20/20] avg loss: 0.09045977255918242		[learning rate: 0.0019293]
	Learning Rate: 0.0019293
	LOSS [training: 0.10986335008904498 | validation: 0.05636207633207462]
	TIME [epoch: 9.06 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08886381760851222		[learning rate: 0.0019263]
		[batch 20/20] avg loss: 0.13341482373768623		[learning rate: 0.0019234]
	Learning Rate: 0.00192339
	LOSS [training: 0.11113932067309924 | validation: 0.1955069935365709]
	TIME [epoch: 9.05 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12873879335309035		[learning rate: 0.0019204]
		[batch 20/20] avg loss: 0.09422053661115713		[learning rate: 0.0019175]
	Learning Rate: 0.00191749
	LOSS [training: 0.11147966498212374 | validation: 0.11461317629767263]
	TIME [epoch: 9.05 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10037852863285832		[learning rate: 0.0019145]
		[batch 20/20] avg loss: 0.07809804323536983		[learning rate: 0.0019116]
	Learning Rate: 0.00191161
	LOSS [training: 0.08923828593411406 | validation: 0.05676385724277497]
	TIME [epoch: 9.05 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07829160783556312		[learning rate: 0.0019087]
		[batch 20/20] avg loss: 0.08132390863299398		[learning rate: 0.0019058]
	Learning Rate: 0.00190575
	LOSS [training: 0.07980775823427855 | validation: 0.09598179908618501]
	TIME [epoch: 9.07 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09248924056009747		[learning rate: 0.0019028]
		[batch 20/20] avg loss: 0.07604957025764357		[learning rate: 0.0018999]
	Learning Rate: 0.00189991
	LOSS [training: 0.08426940540887051 | validation: 0.12952098387440147]
	TIME [epoch: 9.06 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10236276046951298		[learning rate: 0.001897]
		[batch 20/20] avg loss: 0.09204347237938748		[learning rate: 0.0018941]
	Learning Rate: 0.00189409
	LOSS [training: 0.09720311642445024 | validation: 0.07336864476327871]
	TIME [epoch: 9.05 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09304606470685721		[learning rate: 0.0018912]
		[batch 20/20] avg loss: 0.09500926936164153		[learning rate: 0.0018883]
	Learning Rate: 0.00188828
	LOSS [training: 0.09402766703424939 | validation: 0.11501082373229389]
	TIME [epoch: 9.05 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12078873490481909		[learning rate: 0.0018854]
		[batch 20/20] avg loss: 0.10824777587759227		[learning rate: 0.0018825]
	Learning Rate: 0.00188249
	LOSS [training: 0.11451825539120566 | validation: 0.07490658222733725]
	TIME [epoch: 9.05 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0841821292390984		[learning rate: 0.0018796]
		[batch 20/20] avg loss: 0.12961801225418976		[learning rate: 0.0018767]
	Learning Rate: 0.00187672
	LOSS [training: 0.10690007074664407 | validation: 0.1221121939138995]
	TIME [epoch: 9.07 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11139966381603687		[learning rate: 0.0018738]
		[batch 20/20] avg loss: 0.09980164503244868		[learning rate: 0.001871]
	Learning Rate: 0.00187097
	LOSS [training: 0.10560065442424278 | validation: 0.059146645008438595]
	TIME [epoch: 9.07 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09343376573506018		[learning rate: 0.0018681]
		[batch 20/20] avg loss: 0.1103493031938096		[learning rate: 0.0018652]
	Learning Rate: 0.00186523
	LOSS [training: 0.10189153446443491 | validation: 0.09526981546258421]
	TIME [epoch: 9.05 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08733415621654868		[learning rate: 0.0018624]
		[batch 20/20] avg loss: 0.10710923288612066		[learning rate: 0.0018595]
	Learning Rate: 0.00185952
	LOSS [training: 0.09722169455133464 | validation: 0.07477875115235622]
	TIME [epoch: 9.06 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10840016594737112		[learning rate: 0.0018567]
		[batch 20/20] avg loss: 0.10089712618110838		[learning rate: 0.0018538]
	Learning Rate: 0.00185382
	LOSS [training: 0.10464864606423976 | validation: 0.06779902598708115]
	TIME [epoch: 9.05 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10262516291563036		[learning rate: 0.001851]
		[batch 20/20] avg loss: 0.10460616277767135		[learning rate: 0.0018481]
	Learning Rate: 0.00184813
	LOSS [training: 0.10361566284665083 | validation: 0.07884315437732141]
	TIME [epoch: 9.06 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09199942288861088		[learning rate: 0.0018453]
		[batch 20/20] avg loss: 0.09150896029317614		[learning rate: 0.0018425]
	Learning Rate: 0.00184247
	LOSS [training: 0.09175419159089351 | validation: 0.11160555143692533]
	TIME [epoch: 9.06 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09584456450007584		[learning rate: 0.0018396]
		[batch 20/20] avg loss: 0.07559642252923178		[learning rate: 0.0018368]
	Learning Rate: 0.00183682
	LOSS [training: 0.0857204935146538 | validation: 0.07463945907968697]
	TIME [epoch: 9.05 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06566155374278997		[learning rate: 0.001834]
		[batch 20/20] avg loss: 0.08344908813619542		[learning rate: 0.0018312]
	Learning Rate: 0.00183119
	LOSS [training: 0.07455532093949269 | validation: 0.08098824465808645]
	TIME [epoch: 9.05 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09039260386140607		[learning rate: 0.0018284]
		[batch 20/20] avg loss: 0.05368312740006659		[learning rate: 0.0018256]
	Learning Rate: 0.00182558
	LOSS [training: 0.07203786563073633 | validation: 0.06502340898733414]
	TIME [epoch: 9.05 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10363089057715466		[learning rate: 0.0018228]
		[batch 20/20] avg loss: 0.10323436322399897		[learning rate: 0.00182]
	Learning Rate: 0.00181998
	LOSS [training: 0.10343262690057684 | validation: 0.05447388469030448]
	TIME [epoch: 9.07 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07984002748387097		[learning rate: 0.0018172]
		[batch 20/20] avg loss: 0.07413639287991161		[learning rate: 0.0018144]
	Learning Rate: 0.0018144
	LOSS [training: 0.0769882101818913 | validation: 0.06776989155224085]
	TIME [epoch: 9.06 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.069414729930274		[learning rate: 0.0018116]
		[batch 20/20] avg loss: 0.08541999791438143		[learning rate: 0.0018088]
	Learning Rate: 0.00180884
	LOSS [training: 0.0774173639223277 | validation: 0.06494509526004762]
	TIME [epoch: 9.04 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1113153857211302		[learning rate: 0.0018061]
		[batch 20/20] avg loss: 0.06848839428221651		[learning rate: 0.0018033]
	Learning Rate: 0.00180329
	LOSS [training: 0.08990189000167334 | validation: 0.04674269583662519]
	TIME [epoch: 9.05 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09084317719855213		[learning rate: 0.0018005]
		[batch 20/20] avg loss: 0.09950040171118066		[learning rate: 0.0017978]
	Learning Rate: 0.00179777
	LOSS [training: 0.09517178945486639 | validation: 0.09187492035958818]
	TIME [epoch: 9.05 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08728314776237038		[learning rate: 0.001795]
		[batch 20/20] avg loss: 0.10783466710211598		[learning rate: 0.0017923]
	Learning Rate: 0.00179226
	LOSS [training: 0.09755890743224317 | validation: 0.07195423127918998]
	TIME [epoch: 9.07 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08021702146343615		[learning rate: 0.0017895]
		[batch 20/20] avg loss: 0.1230135094299462		[learning rate: 0.0017868]
	Learning Rate: 0.00178676
	LOSS [training: 0.1016152654466912 | validation: 0.06496917885147699]
	TIME [epoch: 9.06 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07288387692159784		[learning rate: 0.001784]
		[batch 20/20] avg loss: 0.07777154503490048		[learning rate: 0.0017813]
	Learning Rate: 0.00178128
	LOSS [training: 0.07532771097824917 | validation: 0.06849169857802044]
	TIME [epoch: 9.05 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09795672308791867		[learning rate: 0.0017786]
		[batch 20/20] avg loss: 0.07136296145131067		[learning rate: 0.0017758]
	Learning Rate: 0.00177582
	LOSS [training: 0.08465984226961466 | validation: 0.06808254642329627]
	TIME [epoch: 9.05 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10288951986726022		[learning rate: 0.0017731]
		[batch 20/20] avg loss: 0.08656341144894439		[learning rate: 0.0017704]
	Learning Rate: 0.00177038
	LOSS [training: 0.0947264656581023 | validation: 0.05458851714612533]
	TIME [epoch: 9.06 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07820233310545947		[learning rate: 0.0017677]
		[batch 20/20] avg loss: 0.07015422358888551		[learning rate: 0.001765]
	Learning Rate: 0.00176495
	LOSS [training: 0.07417827834717249 | validation: 0.04591056623260373]
	TIME [epoch: 9.06 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06881453945780693		[learning rate: 0.0017622]
		[batch 20/20] avg loss: 0.07578995883098155		[learning rate: 0.0017595]
	Learning Rate: 0.00175954
	LOSS [training: 0.07230224914439423 | validation: 0.05005458063312737]
	TIME [epoch: 9.06 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0714879794106931		[learning rate: 0.0017568]
		[batch 20/20] avg loss: 0.09215265058907733		[learning rate: 0.0017541]
	Learning Rate: 0.00175415
	LOSS [training: 0.08182031499988521 | validation: 0.12229545817551199]
	TIME [epoch: 9.05 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09453158060009972		[learning rate: 0.0017515]
		[batch 20/20] avg loss: 0.0901717001413358		[learning rate: 0.0017488]
	Learning Rate: 0.00174877
	LOSS [training: 0.09235164037071776 | validation: 0.08873962121874757]
	TIME [epoch: 9.04 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08329801780013198		[learning rate: 0.0017461]
		[batch 20/20] avg loss: 0.09052559721586857		[learning rate: 0.0017434]
	Learning Rate: 0.00174341
	LOSS [training: 0.08691180750800026 | validation: 0.061513123628203874]
	TIME [epoch: 9.05 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07552285551242989		[learning rate: 0.0017407]
		[batch 20/20] avg loss: 0.1361623508478355		[learning rate: 0.0017381]
	Learning Rate: 0.00173807
	LOSS [training: 0.10584260318013272 | validation: 0.22757263894192759]
	TIME [epoch: 9.06 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12368851373243306		[learning rate: 0.0017354]
		[batch 20/20] avg loss: 0.09460851629822575		[learning rate: 0.0017327]
	Learning Rate: 0.00173274
	LOSS [training: 0.1091485150153294 | validation: 0.04496823132735188]
	TIME [epoch: 9.05 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0801667721556915		[learning rate: 0.0017301]
		[batch 20/20] avg loss: 0.0794250926142159		[learning rate: 0.0017274]
	Learning Rate: 0.00172743
	LOSS [training: 0.07979593238495372 | validation: 0.08444220662647059]
	TIME [epoch: 9.05 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11867878741879563		[learning rate: 0.0017248]
		[batch 20/20] avg loss: 0.08661413992085737		[learning rate: 0.0017221]
	Learning Rate: 0.00172213
	LOSS [training: 0.10264646366982651 | validation: 0.08978861246405172]
	TIME [epoch: 9.04 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08159293834894231		[learning rate: 0.0017195]
		[batch 20/20] avg loss: 0.08001163375143751		[learning rate: 0.0017169]
	Learning Rate: 0.00171685
	LOSS [training: 0.08080228605018991 | validation: 0.04864455867169679]
	TIME [epoch: 9.06 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07425150365395408		[learning rate: 0.0017142]
		[batch 20/20] avg loss: 0.11526051309731336		[learning rate: 0.0017116]
	Learning Rate: 0.00171159
	LOSS [training: 0.09475600837563371 | validation: 0.10647739332689994]
	TIME [epoch: 9.07 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07508212194887573		[learning rate: 0.001709]
		[batch 20/20] avg loss: 0.06924042711998077		[learning rate: 0.0017063]
	Learning Rate: 0.00170634
	LOSS [training: 0.07216127453442825 | validation: 0.04900673937238549]
	TIME [epoch: 9.06 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09519391646589667		[learning rate: 0.0017037]
		[batch 20/20] avg loss: 0.13194595895201056		[learning rate: 0.0017011]
	Learning Rate: 0.00170111
	LOSS [training: 0.11356993770895363 | validation: 0.10066227864787786]
	TIME [epoch: 9.04 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07000113041922566		[learning rate: 0.0016985]
		[batch 20/20] avg loss: 0.11204972250967105		[learning rate: 0.0016959]
	Learning Rate: 0.0016959
	LOSS [training: 0.09102542646444833 | validation: 0.05975027607548544]
	TIME [epoch: 9.05 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07360099964419144		[learning rate: 0.0016933]
		[batch 20/20] avg loss: 0.10105252189107128		[learning rate: 0.0016907]
	Learning Rate: 0.0016907
	LOSS [training: 0.08732676076763134 | validation: 0.07194316367406735]
	TIME [epoch: 9.05 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09141180523111554		[learning rate: 0.0016881]
		[batch 20/20] avg loss: 0.07556934216921718		[learning rate: 0.0016855]
	Learning Rate: 0.00168552
	LOSS [training: 0.08349057370016635 | validation: 0.08542756210954607]
	TIME [epoch: 9.06 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10432660577548378		[learning rate: 0.0016829]
		[batch 20/20] avg loss: 0.08662216710495323		[learning rate: 0.0016804]
	Learning Rate: 0.00168035
	LOSS [training: 0.0954743864402185 | validation: 0.10851605262284894]
	TIME [epoch: 9.05 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09520614729306856		[learning rate: 0.0016778]
		[batch 20/20] avg loss: 0.08129501801970691		[learning rate: 0.0016752]
	Learning Rate: 0.0016752
	LOSS [training: 0.08825058265638773 | validation: 0.055961718471514516]
	TIME [epoch: 9.04 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061444286593283735		[learning rate: 0.0016726]
		[batch 20/20] avg loss: 0.06993584169314193		[learning rate: 0.0016701]
	Learning Rate: 0.00167006
	LOSS [training: 0.06569006414321285 | validation: 0.08108924739442014]
	TIME [epoch: 9.04 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07563000829764786		[learning rate: 0.0016675]
		[batch 20/20] avg loss: 0.11052510819404618		[learning rate: 0.0016649]
	Learning Rate: 0.00166495
	LOSS [training: 0.09307755824584701 | validation: 0.060179884268644596]
	TIME [epoch: 9.04 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09801355411119149		[learning rate: 0.0016624]
		[batch 20/20] avg loss: 0.11733322775919612		[learning rate: 0.0016598]
	Learning Rate: 0.00165984
	LOSS [training: 0.10767339093519383 | validation: 0.04800485471924153]
	TIME [epoch: 9.06 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059828643148774605		[learning rate: 0.0016573]
		[batch 20/20] avg loss: 0.09813912298255353		[learning rate: 0.0016548]
	Learning Rate: 0.00165475
	LOSS [training: 0.07898388306566406 | validation: 0.0513075049830793]
	TIME [epoch: 9.05 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0987073477328348		[learning rate: 0.0016522]
		[batch 20/20] avg loss: 0.08003600281462755		[learning rate: 0.0016497]
	Learning Rate: 0.00164968
	LOSS [training: 0.08937167527373116 | validation: 0.052682399491499665]
	TIME [epoch: 9.04 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09388762947983056		[learning rate: 0.0016472]
		[batch 20/20] avg loss: 0.09222642746437437		[learning rate: 0.0016446]
	Learning Rate: 0.00164462
	LOSS [training: 0.09305702847210247 | validation: 0.12303907544053701]
	TIME [epoch: 9.04 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08952233744816251		[learning rate: 0.0016421]
		[batch 20/20] avg loss: 0.09347069348526862		[learning rate: 0.0016396]
	Learning Rate: 0.00163958
	LOSS [training: 0.09149651546671558 | validation: 0.0587402833202426]
	TIME [epoch: 9.04 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0835406368021182		[learning rate: 0.0016371]
		[batch 20/20] avg loss: 0.07820877785969878		[learning rate: 0.0016346]
	Learning Rate: 0.00163456
	LOSS [training: 0.0808747073309085 | validation: 0.05535882218208567]
	TIME [epoch: 9.06 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09627358108715678		[learning rate: 0.001632]
		[batch 20/20] avg loss: 0.10051228991285507		[learning rate: 0.0016295]
	Learning Rate: 0.00162955
	LOSS [training: 0.09839293550000591 | validation: 0.0792140271640617]
	TIME [epoch: 9.06 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0779121051460962		[learning rate: 0.001627]
		[batch 20/20] avg loss: 0.0849923123925205		[learning rate: 0.0016246]
	Learning Rate: 0.00162455
	LOSS [training: 0.08145220876930837 | validation: 0.047636487724717354]
	TIME [epoch: 9.05 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09307025945007195		[learning rate: 0.0016221]
		[batch 20/20] avg loss: 0.07525041966194654		[learning rate: 0.0016196]
	Learning Rate: 0.00161957
	LOSS [training: 0.08416033955600925 | validation: 0.03577970874448259]
	TIME [epoch: 9.06 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07619184118804481		[learning rate: 0.0016171]
		[batch 20/20] avg loss: 0.08171593252552463		[learning rate: 0.0016146]
	Learning Rate: 0.00161461
	LOSS [training: 0.07895388685678471 | validation: 0.11874408505961577]
	TIME [epoch: 9.06 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09638387485805391		[learning rate: 0.0016121]
		[batch 20/20] avg loss: 0.07794908100351498		[learning rate: 0.0016097]
	Learning Rate: 0.00160966
	LOSS [training: 0.08716647793078447 | validation: 0.06823752054577628]
	TIME [epoch: 9.07 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09286846030735237		[learning rate: 0.0016072]
		[batch 20/20] avg loss: 0.10526486420499266		[learning rate: 0.0016047]
	Learning Rate: 0.00160472
	LOSS [training: 0.09906666225617253 | validation: 0.06086896005903294]
	TIME [epoch: 9.06 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10853080172704568		[learning rate: 0.0016023]
		[batch 20/20] avg loss: 0.0982058792905378		[learning rate: 0.0015998]
	Learning Rate: 0.0015998
	LOSS [training: 0.10336834050879176 | validation: 0.035085055863206205]
	TIME [epoch: 9.05 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08506896036850659		[learning rate: 0.0015973]
		[batch 20/20] avg loss: 0.0879917062049676		[learning rate: 0.0015949]
	Learning Rate: 0.0015949
	LOSS [training: 0.0865303332867371 | validation: 0.05948515495776967]
	TIME [epoch: 9.04 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08894069410457581		[learning rate: 0.0015925]
		[batch 20/20] avg loss: 0.10059897122672316		[learning rate: 0.00159]
	Learning Rate: 0.00159001
	LOSS [training: 0.09476983266564948 | validation: 0.14042726148267573]
	TIME [epoch: 9.05 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1013580866032939		[learning rate: 0.0015876]
		[batch 20/20] avg loss: 0.10156135161158612		[learning rate: 0.0015851]
	Learning Rate: 0.00158514
	LOSS [training: 0.10145971910744003 | validation: 0.05729435612026113]
	TIME [epoch: 9.07 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09507998501099284		[learning rate: 0.0015827]
		[batch 20/20] avg loss: 0.07919544429799819		[learning rate: 0.0015803]
	Learning Rate: 0.00158028
	LOSS [training: 0.08713771465449552 | validation: 0.07413952349439407]
	TIME [epoch: 9.05 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09255789245290906		[learning rate: 0.0015779]
		[batch 20/20] avg loss: 0.07788738101297144		[learning rate: 0.0015754]
	Learning Rate: 0.00157543
	LOSS [training: 0.08522263673294025 | validation: 0.07509481989510661]
	TIME [epoch: 9.05 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08267261819134351		[learning rate: 0.001573]
		[batch 20/20] avg loss: 0.08475524685099531		[learning rate: 0.0015706]
	Learning Rate: 0.0015706
	LOSS [training: 0.08371393252116943 | validation: 0.06691110780298094]
	TIME [epoch: 9.04 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06654803510781826		[learning rate: 0.0015682]
		[batch 20/20] avg loss: 0.07027215444583693		[learning rate: 0.0015658]
	Learning Rate: 0.00156579
	LOSS [training: 0.0684100947768276 | validation: 0.09812937935392127]
	TIME [epoch: 9.05 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08119649971614377		[learning rate: 0.0015634]
		[batch 20/20] avg loss: 0.08842338295289116		[learning rate: 0.001561]
	Learning Rate: 0.00156099
	LOSS [training: 0.08480994133451748 | validation: 0.07004493065746664]
	TIME [epoch: 9.07 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09842067425061544		[learning rate: 0.0015586]
		[batch 20/20] avg loss: 0.08632447408365354		[learning rate: 0.0015562]
	Learning Rate: 0.0015562
	LOSS [training: 0.09237257416713449 | validation: 0.048110079096916794]
	TIME [epoch: 9.06 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06868554376089768		[learning rate: 0.0015538]
		[batch 20/20] avg loss: 0.09099779344815287		[learning rate: 0.0015514]
	Learning Rate: 0.00155143
	LOSS [training: 0.07984166860452528 | validation: 0.06890029230320976]
	TIME [epoch: 9.05 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07587510529736644		[learning rate: 0.0015491]
		[batch 20/20] avg loss: 0.06749852718096726		[learning rate: 0.0015467]
	Learning Rate: 0.00154668
	LOSS [training: 0.07168681623916685 | validation: 0.05500793640674028]
	TIME [epoch: 9.04 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06667039206502282		[learning rate: 0.0015443]
		[batch 20/20] avg loss: 0.10418527498639277		[learning rate: 0.0015419]
	Learning Rate: 0.00154194
	LOSS [training: 0.08542783352570779 | validation: 0.08320662810823069]
	TIME [epoch: 9.04 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07887944098529485		[learning rate: 0.0015396]
		[batch 20/20] avg loss: 0.09027949195816055		[learning rate: 0.0015372]
	Learning Rate: 0.00153721
	LOSS [training: 0.0845794664717277 | validation: 0.0834768919152946]
	TIME [epoch: 9.06 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08051031529042468		[learning rate: 0.0015349]
		[batch 20/20] avg loss: 0.0720125176024703		[learning rate: 0.0015325]
	Learning Rate: 0.0015325
	LOSS [training: 0.07626141644644749 | validation: 0.08237564021631338]
	TIME [epoch: 9.05 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08943211030741291		[learning rate: 0.0015301]
		[batch 20/20] avg loss: 0.13012652323816684		[learning rate: 0.0015278]
	Learning Rate: 0.0015278
	LOSS [training: 0.10977931677278992 | validation: 0.0893748304991344]
	TIME [epoch: 9.06 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08871497941778897		[learning rate: 0.0015255]
		[batch 20/20] avg loss: 0.07364362270154436		[learning rate: 0.0015231]
	Learning Rate: 0.00152312
	LOSS [training: 0.08117930105966667 | validation: 0.07261824414408938]
	TIME [epoch: 9.05 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07073032544651126		[learning rate: 0.0015208]
		[batch 20/20] avg loss: 0.06829873954670831		[learning rate: 0.0015184]
	Learning Rate: 0.00151845
	LOSS [training: 0.06951453249660978 | validation: 0.061084425603699026]
	TIME [epoch: 9.05 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07716687702559495		[learning rate: 0.0015161]
		[batch 20/20] avg loss: 0.08406855332882288		[learning rate: 0.0015138]
	Learning Rate: 0.00151379
	LOSS [training: 0.0806177151772089 | validation: 0.056453842226790944]
	TIME [epoch: 9.07 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07094593585970467		[learning rate: 0.0015115]
		[batch 20/20] avg loss: 0.07630002590399487		[learning rate: 0.0015092]
	Learning Rate: 0.00150915
	LOSS [training: 0.07362298088184975 | validation: 0.05235961864062773]
	TIME [epoch: 9.06 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07354863060288772		[learning rate: 0.0015068]
		[batch 20/20] avg loss: 0.10609015038493048		[learning rate: 0.0015045]
	Learning Rate: 0.00150453
	LOSS [training: 0.08981939049390911 | validation: 0.10132971546467193]
	TIME [epoch: 9.04 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08844442084891871		[learning rate: 0.0015022]
		[batch 20/20] avg loss: 0.07073366254147441		[learning rate: 0.0014999]
	Learning Rate: 0.00149991
	LOSS [training: 0.07958904169519654 | validation: 0.05744416353432058]
	TIME [epoch: 9.04 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1081087605371803		[learning rate: 0.0014976]
		[batch 20/20] avg loss: 0.05674205631978261		[learning rate: 0.0014953]
	Learning Rate: 0.00149532
	LOSS [training: 0.08242540842848144 | validation: 0.10968205266144321]
	TIME [epoch: 9.05 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09396098679906775		[learning rate: 0.001493]
		[batch 20/20] avg loss: 0.07197432079875885		[learning rate: 0.0014907]
	Learning Rate: 0.00149073
	LOSS [training: 0.0829676537989133 | validation: 0.05628606679523871]
	TIME [epoch: 9.07 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07885225798577992		[learning rate: 0.0014884]
		[batch 20/20] avg loss: 0.07234266976761505		[learning rate: 0.0014862]
	Learning Rate: 0.00148616
	LOSS [training: 0.0755974638766975 | validation: 0.051852460991257995]
	TIME [epoch: 9.05 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0869688098848853		[learning rate: 0.0014839]
		[batch 20/20] avg loss: 0.08840685671902808		[learning rate: 0.0014816]
	Learning Rate: 0.00148161
	LOSS [training: 0.08768783330195667 | validation: 0.07970928881060377]
	TIME [epoch: 9.48 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08125303458465974		[learning rate: 0.0014793]
		[batch 20/20] avg loss: 0.08001822013923524		[learning rate: 0.0014771]
	Learning Rate: 0.00147707
	LOSS [training: 0.08063562736194749 | validation: 0.08941203760860479]
	TIME [epoch: 9.05 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06820284618481393		[learning rate: 0.0014748]
		[batch 20/20] avg loss: 0.06585150556853459		[learning rate: 0.0014725]
	Learning Rate: 0.00147254
	LOSS [training: 0.06702717587667426 | validation: 0.1361456337541026]
	TIME [epoch: 9.05 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08968896630494438		[learning rate: 0.0014703]
		[batch 20/20] avg loss: 0.07960432305840463		[learning rate: 0.001468]
	Learning Rate: 0.00146802
	LOSS [training: 0.08464664468167452 | validation: 0.06044212271112605]
	TIME [epoch: 9.06 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060307156501758455		[learning rate: 0.0014658]
		[batch 20/20] avg loss: 0.09914122662230611		[learning rate: 0.0014635]
	Learning Rate: 0.00146352
	LOSS [training: 0.0797241915620323 | validation: 0.08103746464249181]
	TIME [epoch: 9.07 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09576006058512346		[learning rate: 0.0014613]
		[batch 20/20] avg loss: 0.10492077287670076		[learning rate: 0.001459]
	Learning Rate: 0.00145904
	LOSS [training: 0.1003404167309121 | validation: 0.17900515720392896]
	TIME [epoch: 9.06 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08667246731215635		[learning rate: 0.0014568]
		[batch 20/20] avg loss: 0.09272064826129904		[learning rate: 0.0014546]
	Learning Rate: 0.00145457
	LOSS [training: 0.0896965577867277 | validation: 0.05363013015670612]
	TIME [epoch: 9.05 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06623296804821568		[learning rate: 0.0014523]
		[batch 20/20] avg loss: 0.06968873391287264		[learning rate: 0.0014501]
	Learning Rate: 0.00145011
	LOSS [training: 0.06796085098054415 | validation: 0.08117522140746881]
	TIME [epoch: 9.05 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08884096719594739		[learning rate: 0.0014479]
		[batch 20/20] avg loss: 0.12045196022718327		[learning rate: 0.0014457]
	Learning Rate: 0.00144566
	LOSS [training: 0.10464646371156532 | validation: 0.04348157085447288]
	TIME [epoch: 9.07 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07711589082386416		[learning rate: 0.0014434]
		[batch 20/20] avg loss: 0.07554882139709523		[learning rate: 0.0014412]
	Learning Rate: 0.00144123
	LOSS [training: 0.07633235611047968 | validation: 0.07550837585989803]
	TIME [epoch: 9.08 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08204907012148961		[learning rate: 0.001439]
		[batch 20/20] avg loss: 0.0907688932125468		[learning rate: 0.0014368]
	Learning Rate: 0.00143681
	LOSS [training: 0.0864089816670182 | validation: 0.06717089659836073]
	TIME [epoch: 9.05 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0769859436994747		[learning rate: 0.0014346]
		[batch 20/20] avg loss: 0.07371612122728967		[learning rate: 0.0014324]
	Learning Rate: 0.00143241
	LOSS [training: 0.07535103246338218 | validation: 0.04649158052716347]
	TIME [epoch: 9.06 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06568741916111218		[learning rate: 0.0014302]
		[batch 20/20] avg loss: 0.10731511785823673		[learning rate: 0.001428]
	Learning Rate: 0.00142802
	LOSS [training: 0.08650126850967446 | validation: 0.06230030783320036]
	TIME [epoch: 9.06 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10492262480182571		[learning rate: 0.0014258]
		[batch 20/20] avg loss: 0.09359060675337996		[learning rate: 0.0014236]
	Learning Rate: 0.00142364
	LOSS [training: 0.09925661577760284 | validation: 0.11538549033554737]
	TIME [epoch: 9.07 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0869547296169614		[learning rate: 0.0014215]
		[batch 20/20] avg loss: 0.07568122212366868		[learning rate: 0.0014193]
	Learning Rate: 0.00141928
	LOSS [training: 0.08131797587031504 | validation: 0.07088437940225704]
	TIME [epoch: 9.07 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07815954314836597		[learning rate: 0.0014171]
		[batch 20/20] avg loss: 0.08035055827951176		[learning rate: 0.0014149]
	Learning Rate: 0.00141492
	LOSS [training: 0.07925505071393886 | validation: 0.048725264528350465]
	TIME [epoch: 9.05 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06611119784567798		[learning rate: 0.0014128]
		[batch 20/20] avg loss: 0.08220945663422821		[learning rate: 0.0014106]
	Learning Rate: 0.00141059
	LOSS [training: 0.0741603272399531 | validation: 0.1365731361094394]
	TIME [epoch: 9.07 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16012551473656905		[learning rate: 0.0014084]
		[batch 20/20] avg loss: 0.12523682534944122		[learning rate: 0.0014063]
	Learning Rate: 0.00140626
	LOSS [training: 0.14268117004300512 | validation: 0.11264710513951345]
	TIME [epoch: 9.09 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08288428997651394		[learning rate: 0.0014041]
		[batch 20/20] avg loss: 0.10069595162328045		[learning rate: 0.001402]
	Learning Rate: 0.00140195
	LOSS [training: 0.09179012079989719 | validation: 0.07433623411148332]
	TIME [epoch: 9.09 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0643071565947921		[learning rate: 0.0013998]
		[batch 20/20] avg loss: 0.08828430459282186		[learning rate: 0.0013977]
	Learning Rate: 0.00139765
	LOSS [training: 0.07629573059380698 | validation: 0.05261454453215882]
	TIME [epoch: 9.09 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08173350156680148		[learning rate: 0.0013955]
		[batch 20/20] avg loss: 0.07827471783674565		[learning rate: 0.0013934]
	Learning Rate: 0.00139337
	LOSS [training: 0.08000410970177356 | validation: 0.0447949941928971]
	TIME [epoch: 9.07 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060550796998858146		[learning rate: 0.0013912]
		[batch 20/20] avg loss: 0.09671516242033887		[learning rate: 0.0013891]
	Learning Rate: 0.0013891
	LOSS [training: 0.07863297970959851 | validation: 0.06221579111393481]
	TIME [epoch: 9.06 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07236889621998939		[learning rate: 0.001387]
		[batch 20/20] avg loss: 0.06582636630819319		[learning rate: 0.0013848]
	Learning Rate: 0.00138484
	LOSS [training: 0.06909763126409131 | validation: 0.12224941108106477]
	TIME [epoch: 9.07 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07644116954547325		[learning rate: 0.0013827]
		[batch 20/20] avg loss: 0.06766422396070981		[learning rate: 0.0013806]
	Learning Rate: 0.0013806
	LOSS [training: 0.07205269675309153 | validation: 0.0667245438989115]
	TIME [epoch: 9.07 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06647645683756725		[learning rate: 0.0013785]
		[batch 20/20] avg loss: 0.08011922137609094		[learning rate: 0.0013764]
	Learning Rate: 0.00137636
	LOSS [training: 0.07329783910682909 | validation: 0.0811119406308454]
	TIME [epoch: 9.07 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07050472178424935		[learning rate: 0.0013743]
		[batch 20/20] avg loss: 0.07569872750324362		[learning rate: 0.0013721]
	Learning Rate: 0.00137214
	LOSS [training: 0.07310172464374648 | validation: 0.06767075514690823]
	TIME [epoch: 9.05 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07559903928959003		[learning rate: 0.00137]
		[batch 20/20] avg loss: 0.08232226011381126		[learning rate: 0.0013679]
	Learning Rate: 0.00136794
	LOSS [training: 0.07896064970170065 | validation: 0.13620189038464867]
	TIME [epoch: 9.05 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09589023747596873		[learning rate: 0.0013658]
		[batch 20/20] avg loss: 0.06370648141670003		[learning rate: 0.0013637]
	Learning Rate: 0.00136375
	LOSS [training: 0.07979835944633437 | validation: 0.09348161228629653]
	TIME [epoch: 9.05 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07434456665453934		[learning rate: 0.0013617]
		[batch 20/20] avg loss: 0.09151340411698315		[learning rate: 0.0013596]
	Learning Rate: 0.00135956
	LOSS [training: 0.08292898538576124 | validation: 0.06578593857619348]
	TIME [epoch: 9.06 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09000513183214742		[learning rate: 0.0013575]
		[batch 20/20] avg loss: 0.08509326330899025		[learning rate: 0.0013554]
	Learning Rate: 0.0013554
	LOSS [training: 0.08754919757056882 | validation: 0.05097068662616233]
	TIME [epoch: 9.06 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06318997002616097		[learning rate: 0.0013533]
		[batch 20/20] avg loss: 0.08952508617499269		[learning rate: 0.0013512]
	Learning Rate: 0.00135124
	LOSS [training: 0.0763575281005768 | validation: 0.03830805993379499]
	TIME [epoch: 9.05 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06266414562170539		[learning rate: 0.0013492]
		[batch 20/20] avg loss: 0.07384603862108274		[learning rate: 0.0013471]
	Learning Rate: 0.0013471
	LOSS [training: 0.06825509212139408 | validation: 0.04166013941675803]
	TIME [epoch: 9.05 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058562110769110776		[learning rate: 0.001345]
		[batch 20/20] avg loss: 0.06823581799752743		[learning rate: 0.001343]
	Learning Rate: 0.00134297
	LOSS [training: 0.0633989643833191 | validation: 0.03001987130690028]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_1154.pth
	Model improved!!!
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061822209775306225		[learning rate: 0.0013409]
		[batch 20/20] avg loss: 0.09777325541320823		[learning rate: 0.0013389]
	Learning Rate: 0.00133885
	LOSS [training: 0.07979773259425724 | validation: 0.06623852338880368]
	TIME [epoch: 9.06 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0701826340672148		[learning rate: 0.0013368]
		[batch 20/20] avg loss: 0.10666369177206356		[learning rate: 0.0013348]
	Learning Rate: 0.00133475
	LOSS [training: 0.08842316291963918 | validation: 0.07034604240934199]
	TIME [epoch: 9.05 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07233745088217383		[learning rate: 0.0013327]
		[batch 20/20] avg loss: 0.08126567909336024		[learning rate: 0.0013307]
	Learning Rate: 0.00133066
	LOSS [training: 0.07680156498776705 | validation: 0.05667800272082378]
	TIME [epoch: 9.04 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08483873625526919		[learning rate: 0.0013286]
		[batch 20/20] avg loss: 0.07434060527394254		[learning rate: 0.0013266]
	Learning Rate: 0.00132658
	LOSS [training: 0.07958967076460585 | validation: 0.09148207004027731]
	TIME [epoch: 9.04 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0685117521167265		[learning rate: 0.0013245]
		[batch 20/20] avg loss: 0.06750146541733171		[learning rate: 0.0013225]
	Learning Rate: 0.00132251
	LOSS [training: 0.0680066087670291 | validation: 0.11098562012749495]
	TIME [epoch: 9.05 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07473348686063219		[learning rate: 0.0013205]
		[batch 20/20] avg loss: 0.05564244759417712		[learning rate: 0.0013185]
	Learning Rate: 0.00131846
	LOSS [training: 0.06518796722740465 | validation: 0.06379867404434562]
	TIME [epoch: 9.05 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06894625967884396		[learning rate: 0.0013164]
		[batch 20/20] avg loss: 0.0688757623806153		[learning rate: 0.0013144]
	Learning Rate: 0.00131442
	LOSS [training: 0.06891101102972964 | validation: 0.06339825046500643]
	TIME [epoch: 9.07 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07416112503735951		[learning rate: 0.0013124]
		[batch 20/20] avg loss: 0.07687190407547027		[learning rate: 0.0013104]
	Learning Rate: 0.00131039
	LOSS [training: 0.0755165145564149 | validation: 0.04966110201903765]
	TIME [epoch: 9.05 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05571149827860252		[learning rate: 0.0013084]
		[batch 20/20] avg loss: 0.09018399657066543		[learning rate: 0.0013064]
	Learning Rate: 0.00130637
	LOSS [training: 0.07294774742463399 | validation: 0.09044591357029816]
	TIME [epoch: 9.04 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07821345539500597		[learning rate: 0.0013044]
		[batch 20/20] avg loss: 0.056893544499657486		[learning rate: 0.0013024]
	Learning Rate: 0.00130237
	LOSS [training: 0.06755349994733173 | validation: 0.04618821165362654]
	TIME [epoch: 9.04 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06161580348891678		[learning rate: 0.0013004]
		[batch 20/20] avg loss: 0.07650709837847364		[learning rate: 0.0012984]
	Learning Rate: 0.00129837
	LOSS [training: 0.0690614509336952 | validation: 0.07735246229026653]
	TIME [epoch: 9.05 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06816229301151147		[learning rate: 0.0012964]
		[batch 20/20] avg loss: 0.07738505906181328		[learning rate: 0.0012944]
	Learning Rate: 0.00129439
	LOSS [training: 0.07277367603666238 | validation: 0.03644188002106315]
	TIME [epoch: 9.07 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08416374446637573		[learning rate: 0.0012924]
		[batch 20/20] avg loss: 0.07112080586332573		[learning rate: 0.0012904]
	Learning Rate: 0.00129043
	LOSS [training: 0.07764227516485075 | validation: 0.05104084817363384]
	TIME [epoch: 9.03 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052744780386383784		[learning rate: 0.0012884]
		[batch 20/20] avg loss: 0.07857593261966787		[learning rate: 0.0012865]
	Learning Rate: 0.00128647
	LOSS [training: 0.06566035650302585 | validation: 0.05495378136385248]
	TIME [epoch: 9.04 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08511891427991866		[learning rate: 0.0012845]
		[batch 20/20] avg loss: 0.075945187752643		[learning rate: 0.0012825]
	Learning Rate: 0.00128253
	LOSS [training: 0.08053205101628083 | validation: 0.04687026956325246]
	TIME [epoch: 9.04 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07998152110365794		[learning rate: 0.0012806]
		[batch 20/20] avg loss: 0.0849443176469239		[learning rate: 0.0012786]
	Learning Rate: 0.0012786
	LOSS [training: 0.08246291937529092 | validation: 0.05949116902189246]
	TIME [epoch: 9.04 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10217338711396873		[learning rate: 0.0012766]
		[batch 20/20] avg loss: 0.07582731236228407		[learning rate: 0.0012747]
	Learning Rate: 0.00127468
	LOSS [training: 0.08900034973812641 | validation: 0.07365117414628497]
	TIME [epoch: 9.06 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07487271303305887		[learning rate: 0.0012727]
		[batch 20/20] avg loss: 0.09116593702108783		[learning rate: 0.0012708]
	Learning Rate: 0.00127077
	LOSS [training: 0.08301932502707335 | validation: 0.07666121469163245]
	TIME [epoch: 9.03 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07804188960849281		[learning rate: 0.0012688]
		[batch 20/20] avg loss: 0.06728406056419045		[learning rate: 0.0012669]
	Learning Rate: 0.00126687
	LOSS [training: 0.07266297508634163 | validation: 0.034516636571682205]
	TIME [epoch: 9.03 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09690486357713204		[learning rate: 0.0012649]
		[batch 20/20] avg loss: 0.07014516929634253		[learning rate: 0.001263]
	Learning Rate: 0.00126299
	LOSS [training: 0.08352501643673729 | validation: 0.05868365835584966]
	TIME [epoch: 9.04 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08588344904495221		[learning rate: 0.0012611]
		[batch 20/20] avg loss: 0.08854989037975713		[learning rate: 0.0012591]
	Learning Rate: 0.00125912
	LOSS [training: 0.08721666971235466 | validation: 0.0649356413974963]
	TIME [epoch: 9.05 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06905323933766341		[learning rate: 0.0012572]
		[batch 20/20] avg loss: 0.08159614074087543		[learning rate: 0.0012553]
	Learning Rate: 0.00125526
	LOSS [training: 0.07532469003926942 | validation: 0.060778658118412376]
	TIME [epoch: 9.08 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06879072616586421		[learning rate: 0.0012533]
		[batch 20/20] avg loss: 0.07126841190492067		[learning rate: 0.0012514]
	Learning Rate: 0.00125141
	LOSS [training: 0.07002956903539245 | validation: 0.06806927878601023]
	TIME [epoch: 9.04 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06872493789757932		[learning rate: 0.0012495]
		[batch 20/20] avg loss: 0.08400895882357758		[learning rate: 0.0012476]
	Learning Rate: 0.00124757
	LOSS [training: 0.07636694836057846 | validation: 0.060793744173260354]
	TIME [epoch: 9.05 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06430271849169121		[learning rate: 0.0012457]
		[batch 20/20] avg loss: 0.07137269967178361		[learning rate: 0.0012438]
	Learning Rate: 0.00124375
	LOSS [training: 0.0678377090817374 | validation: 0.0535601945334735]
	TIME [epoch: 9.04 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07961591792087339		[learning rate: 0.0012418]
		[batch 20/20] avg loss: 0.06146660127646177		[learning rate: 0.0012399]
	Learning Rate: 0.00123994
	LOSS [training: 0.07054125959866758 | validation: 0.035852309451665905]
	TIME [epoch: 9.04 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08158331185402877		[learning rate: 0.001238]
		[batch 20/20] avg loss: 0.07089500813487605		[learning rate: 0.0012361]
	Learning Rate: 0.00123614
	LOSS [training: 0.0762391599944524 | validation: 0.06416944156497718]
	TIME [epoch: 9.07 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06962289963013447		[learning rate: 0.0012342]
		[batch 20/20] avg loss: 0.07226796487901604		[learning rate: 0.0012323]
	Learning Rate: 0.00123235
	LOSS [training: 0.07094543225457525 | validation: 0.1303788499781453]
	TIME [epoch: 9.04 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08964015124632582		[learning rate: 0.0012305]
		[batch 20/20] avg loss: 0.08498237236800414		[learning rate: 0.0012286]
	Learning Rate: 0.00122857
	LOSS [training: 0.08731126180716499 | validation: 0.06318631469177777]
	TIME [epoch: 9.04 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07081413811841483		[learning rate: 0.0012267]
		[batch 20/20] avg loss: 0.06007989675159046		[learning rate: 0.0012248]
	Learning Rate: 0.0012248
	LOSS [training: 0.06544701743500266 | validation: 0.04861010418661637]
	TIME [epoch: 9.04 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06058687786798006		[learning rate: 0.0012229]
		[batch 20/20] avg loss: 0.08393902046514515		[learning rate: 0.001221]
	Learning Rate: 0.00122105
	LOSS [training: 0.0722629491665626 | validation: 0.09766947267314023]
	TIME [epoch: 9.05 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06834017216355529		[learning rate: 0.0012192]
		[batch 20/20] avg loss: 0.08982656265829095		[learning rate: 0.0012173]
	Learning Rate: 0.00121731
	LOSS [training: 0.07908336741092312 | validation: 0.04271013486399375]
	TIME [epoch: 9.07 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06578540625683205		[learning rate: 0.0012154]
		[batch 20/20] avg loss: 0.05340942150717222		[learning rate: 0.0012136]
	Learning Rate: 0.00121357
	LOSS [training: 0.059597413882002126 | validation: 0.04849657590687805]
	TIME [epoch: 9.05 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07541669687841207		[learning rate: 0.0012117]
		[batch 20/20] avg loss: 0.06065383692388175		[learning rate: 0.0012099]
	Learning Rate: 0.00120985
	LOSS [training: 0.06803526690114689 | validation: 0.03014119246032592]
	TIME [epoch: 9.05 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05416107434257635		[learning rate: 0.001208]
		[batch 20/20] avg loss: 0.06042671783323371		[learning rate: 0.0012061]
	Learning Rate: 0.00120615
	LOSS [training: 0.05729389608790505 | validation: 0.04702919250999411]
	TIME [epoch: 9.04 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0505670346593194		[learning rate: 0.0012043]
		[batch 20/20] avg loss: 0.0559094064543992		[learning rate: 0.0012024]
	Learning Rate: 0.00120245
	LOSS [training: 0.053238220556859304 | validation: 0.04611672322827953]
	TIME [epoch: 9.04 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06500083383111406		[learning rate: 0.0012006]
		[batch 20/20] avg loss: 0.095338352187334		[learning rate: 0.0011988]
	Learning Rate: 0.00119876
	LOSS [training: 0.08016959300922402 | validation: 0.13333636490104828]
	TIME [epoch: 9.06 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0893900591125514		[learning rate: 0.0011969]
		[batch 20/20] avg loss: 0.08429338705123698		[learning rate: 0.0011951]
	Learning Rate: 0.00119509
	LOSS [training: 0.0868417230818942 | validation: 0.05126716810065621]
	TIME [epoch: 9.05 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07988837174910329		[learning rate: 0.0011933]
		[batch 20/20] avg loss: 0.06278095921108465		[learning rate: 0.0011914]
	Learning Rate: 0.00119142
	LOSS [training: 0.07133466548009397 | validation: 0.05288464791392191]
	TIME [epoch: 9.05 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057530851076422175		[learning rate: 0.0011896]
		[batch 20/20] avg loss: 0.07180531962107556		[learning rate: 0.0011878]
	Learning Rate: 0.00118777
	LOSS [training: 0.06466808534874886 | validation: 0.05562297945287348]
	TIME [epoch: 9.04 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06808778241013091		[learning rate: 0.001186]
		[batch 20/20] avg loss: 0.05760069718935943		[learning rate: 0.0011841]
	Learning Rate: 0.00118413
	LOSS [training: 0.06284423979974516 | validation: 0.0374054878747713]
	TIME [epoch: 9.04 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06644405102712389		[learning rate: 0.0011823]
		[batch 20/20] avg loss: 0.05447678587888518		[learning rate: 0.0011805]
	Learning Rate: 0.0011805
	LOSS [training: 0.06046041845300454 | validation: 0.0464200241957634]
	TIME [epoch: 9.07 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0763998756193776		[learning rate: 0.0011787]
		[batch 20/20] avg loss: 0.07546118674042948		[learning rate: 0.0011769]
	Learning Rate: 0.00117688
	LOSS [training: 0.07593053117990353 | validation: 0.0715118749304513]
	TIME [epoch: 9.04 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08195374059333668		[learning rate: 0.0011751]
		[batch 20/20] avg loss: 0.05261410772390533		[learning rate: 0.0011733]
	Learning Rate: 0.00117328
	LOSS [training: 0.06728392415862101 | validation: 0.09349504497297122]
	TIME [epoch: 9.05 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05855664648268262		[learning rate: 0.0011715]
		[batch 20/20] avg loss: 0.062051807821388416		[learning rate: 0.0011697]
	Learning Rate: 0.00116968
	LOSS [training: 0.060304227152035504 | validation: 0.05980480626092624]
	TIME [epoch: 9.05 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06417800317180218		[learning rate: 0.0011679]
		[batch 20/20] avg loss: 0.0563512532329565		[learning rate: 0.0011661]
	Learning Rate: 0.00116609
	LOSS [training: 0.060264628202379336 | validation: 0.031134137179762364]
	TIME [epoch: 9.04 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055370593016758306		[learning rate: 0.0011643]
		[batch 20/20] avg loss: 0.08390581521764769		[learning rate: 0.0011625]
	Learning Rate: 0.00116252
	LOSS [training: 0.06963820411720302 | validation: 0.07134855770837664]
	TIME [epoch: 9.08 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04966006576005723		[learning rate: 0.0011607]
		[batch 20/20] avg loss: 0.07679404778830133		[learning rate: 0.001159]
	Learning Rate: 0.00115896
	LOSS [training: 0.06322705677417927 | validation: 0.053391538440426475]
	TIME [epoch: 9.05 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07695065859845207		[learning rate: 0.0011572]
		[batch 20/20] avg loss: 0.056819552245373106		[learning rate: 0.0011554]
	Learning Rate: 0.0011554
	LOSS [training: 0.06688510542191259 | validation: 0.04653590054370357]
	TIME [epoch: 9.04 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06461271804796465		[learning rate: 0.0011536]
		[batch 20/20] avg loss: 0.07460647858021852		[learning rate: 0.0011519]
	Learning Rate: 0.00115186
	LOSS [training: 0.06960959831409158 | validation: 0.05383385576182145]
	TIME [epoch: 9.04 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061231243017252954		[learning rate: 0.0011501]
		[batch 20/20] avg loss: 0.05666513110294561		[learning rate: 0.0011483]
	Learning Rate: 0.00114833
	LOSS [training: 0.0589481870600993 | validation: 0.027168145352786527]
	TIME [epoch: 9.04 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_1205.pth
	Model improved!!!
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06163275001575552		[learning rate: 0.0011466]
		[batch 20/20] avg loss: 0.0780255167536592		[learning rate: 0.0011448]
	Learning Rate: 0.00114481
	LOSS [training: 0.06982913338470735 | validation: 0.05421125699818506]
	TIME [epoch: 9.07 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05994956331503115		[learning rate: 0.0011431]
		[batch 20/20] avg loss: 0.0585109928774034		[learning rate: 0.0011413]
	Learning Rate: 0.0011413
	LOSS [training: 0.05923027809621727 | validation: 0.03989556873961222]
	TIME [epoch: 9.04 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06762537316386297		[learning rate: 0.0011395]
		[batch 20/20] avg loss: 0.08353399719566275		[learning rate: 0.0011378]
	Learning Rate: 0.0011378
	LOSS [training: 0.07557968517976285 | validation: 0.07430273398088846]
	TIME [epoch: 9.05 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06811329737326988		[learning rate: 0.0011361]
		[batch 20/20] avg loss: 0.0657131822577065		[learning rate: 0.0011343]
	Learning Rate: 0.00113431
	LOSS [training: 0.06691323981548816 | validation: 0.03986407109050212]
	TIME [epoch: 9.05 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06886190844158051		[learning rate: 0.0011326]
		[batch 20/20] avg loss: 0.06607730836926798		[learning rate: 0.0011308]
	Learning Rate: 0.00113084
	LOSS [training: 0.06746960840542425 | validation: 0.045628240772787355]
	TIME [epoch: 9.05 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06960407976786455		[learning rate: 0.0011291]
		[batch 20/20] avg loss: 0.05006535467377097		[learning rate: 0.0011274]
	Learning Rate: 0.00112737
	LOSS [training: 0.059834717220817756 | validation: 0.04512370711391171]
	TIME [epoch: 9.07 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04973635573890724		[learning rate: 0.0011256]
		[batch 20/20] avg loss: 0.05781577628699858		[learning rate: 0.0011239]
	Learning Rate: 0.00112391
	LOSS [training: 0.05377606601295291 | validation: 0.07099568879860915]
	TIME [epoch: 9.05 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05566873535863729		[learning rate: 0.0011222]
		[batch 20/20] avg loss: 0.080595464828769		[learning rate: 0.0011205]
	Learning Rate: 0.00112047
	LOSS [training: 0.06813210009370314 | validation: 0.09720791168289966]
	TIME [epoch: 9.05 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09258402905968903		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.0588981143047199		[learning rate: 0.001117]
	Learning Rate: 0.00111703
	LOSS [training: 0.07574107168220447 | validation: 0.04682496254261122]
	TIME [epoch: 9.05 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06879740733287808		[learning rate: 0.0011153]
		[batch 20/20] avg loss: 0.0602693220260891		[learning rate: 0.0011136]
	Learning Rate: 0.00111361
	LOSS [training: 0.06453336467948359 | validation: 0.03826676486880444]
	TIME [epoch: 9.05 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07485645697970282		[learning rate: 0.0011119]
		[batch 20/20] avg loss: 0.05824038537559516		[learning rate: 0.0011102]
	Learning Rate: 0.0011102
	LOSS [training: 0.06654842117764899 | validation: 0.040336086284332426]
	TIME [epoch: 9.07 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06201528251880206		[learning rate: 0.0011085]
		[batch 20/20] avg loss: 0.06038939650560449		[learning rate: 0.0011068]
	Learning Rate: 0.00110679
	LOSS [training: 0.061202339512203274 | validation: 0.053569911249591005]
	TIME [epoch: 9.04 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08543675703679465		[learning rate: 0.0011051]
		[batch 20/20] avg loss: 0.10249333682912272		[learning rate: 0.0011034]
	Learning Rate: 0.0011034
	LOSS [training: 0.09396504693295868 | validation: 0.10169219436705942]
	TIME [epoch: 9.03 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061332144403067154		[learning rate: 0.0011017]
		[batch 20/20] avg loss: 0.07309230851990604		[learning rate: 0.0011]
	Learning Rate: 0.00110002
	LOSS [training: 0.06721222646148661 | validation: 0.03767755809783292]
	TIME [epoch: 9.04 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050621808578669325		[learning rate: 0.0010983]
		[batch 20/20] avg loss: 0.07766850537728043		[learning rate: 0.0010966]
	Learning Rate: 0.00109665
	LOSS [training: 0.06414515697797488 | validation: 0.06151168829906242]
	TIME [epoch: 9.04 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060344107348047474		[learning rate: 0.001095]
		[batch 20/20] avg loss: 0.05597890637408093		[learning rate: 0.0010933]
	Learning Rate: 0.00109328
	LOSS [training: 0.05816150686106421 | validation: 0.07561658847602021]
	TIME [epoch: 9.06 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06688620785951072		[learning rate: 0.0010916]
		[batch 20/20] avg loss: 0.0923133962261936		[learning rate: 0.0010899]
	Learning Rate: 0.00108993
	LOSS [training: 0.07959980204285215 | validation: 0.0996955214366492]
	TIME [epoch: 9.04 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08605429986540525		[learning rate: 0.0010883]
		[batch 20/20] avg loss: 0.0686062256991048		[learning rate: 0.0010866]
	Learning Rate: 0.00108659
	LOSS [training: 0.07733026278225502 | validation: 0.08563039968398455]
	TIME [epoch: 9.05 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07193186683958219		[learning rate: 0.0010849]
		[batch 20/20] avg loss: 0.06530991035260753		[learning rate: 0.0010833]
	Learning Rate: 0.00108326
	LOSS [training: 0.06862088859609486 | validation: 0.06094397276388854]
	TIME [epoch: 9.05 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07340635726407717		[learning rate: 0.0010816]
		[batch 20/20] avg loss: 0.07372185874323642		[learning rate: 0.0010799]
	Learning Rate: 0.00107994
	LOSS [training: 0.07356410800365679 | validation: 0.06330341422236722]
	TIME [epoch: 9.04 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07304024239184502		[learning rate: 0.0010783]
		[batch 20/20] avg loss: 0.08321621352278276		[learning rate: 0.0010766]
	Learning Rate: 0.00107663
	LOSS [training: 0.07812822795731388 | validation: 0.038582948539123765]
	TIME [epoch: 9.07 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05219626421968639		[learning rate: 0.001075]
		[batch 20/20] avg loss: 0.0604006341481272		[learning rate: 0.0010733]
	Learning Rate: 0.00107333
	LOSS [training: 0.05629844918390679 | validation: 0.03290162566313388]
	TIME [epoch: 9.05 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061042026761669886		[learning rate: 0.0010717]
		[batch 20/20] avg loss: 0.050561292849704474		[learning rate: 0.00107]
	Learning Rate: 0.00107004
	LOSS [training: 0.05580165980568719 | validation: 0.03429210738076554]
	TIME [epoch: 9.04 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06276703801323674		[learning rate: 0.0010684]
		[batch 20/20] avg loss: 0.05739750523544114		[learning rate: 0.0010668]
	Learning Rate: 0.00106676
	LOSS [training: 0.06008227162433895 | validation: 0.061294945111467554]
	TIME [epoch: 9.04 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08490070699136482		[learning rate: 0.0010651]
		[batch 20/20] avg loss: 0.05080737118624803		[learning rate: 0.0010635]
	Learning Rate: 0.00106349
	LOSS [training: 0.06785403908880644 | validation: 0.0342947007671166]
	TIME [epoch: 9.04 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0517459413598156		[learning rate: 0.0010619]
		[batch 20/20] avg loss: 0.05010154977271062		[learning rate: 0.0010602]
	Learning Rate: 0.00106023
	LOSS [training: 0.05092374556626311 | validation: 0.036121796856169966]
	TIME [epoch: 9.06 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049125982647670964		[learning rate: 0.0010586]
		[batch 20/20] avg loss: 0.07975177757240251		[learning rate: 0.001057]
	Learning Rate: 0.00105698
	LOSS [training: 0.06443888011003675 | validation: 0.06789062695019277]
	TIME [epoch: 9.04 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08629705350041825		[learning rate: 0.0010554]
		[batch 20/20] avg loss: 0.0772365786186699		[learning rate: 0.0010537]
	Learning Rate: 0.00105374
	LOSS [training: 0.08176681605954407 | validation: 0.06256247060882164]
	TIME [epoch: 9.03 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0641375946349929		[learning rate: 0.0010521]
		[batch 20/20] avg loss: 0.054389176448500595		[learning rate: 0.0010505]
	Learning Rate: 0.00105051
	LOSS [training: 0.059263385541746746 | validation: 0.05866360369925054]
	TIME [epoch: 9.04 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07384482462431775		[learning rate: 0.0010489]
		[batch 20/20] avg loss: 0.06487529677810544		[learning rate: 0.0010473]
	Learning Rate: 0.00104729
	LOSS [training: 0.06936006070121159 | validation: 0.03827534749996811]
	TIME [epoch: 9.04 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053082630959590135		[learning rate: 0.0010457]
		[batch 20/20] avg loss: 0.08221566445097399		[learning rate: 0.0010441]
	Learning Rate: 0.00104408
	LOSS [training: 0.06764914770528208 | validation: 0.0618763570606216]
	TIME [epoch: 9.06 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07581191297375317		[learning rate: 0.0010425]
		[batch 20/20] avg loss: 0.055198298028597714		[learning rate: 0.0010409]
	Learning Rate: 0.00104088
	LOSS [training: 0.06550510550117546 | validation: 0.03261121544914009]
	TIME [epoch: 9.04 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04996392688534189		[learning rate: 0.0010393]
		[batch 20/20] avg loss: 0.060637638463118225		[learning rate: 0.0010377]
	Learning Rate: 0.00103769
	LOSS [training: 0.05530078267423007 | validation: 0.04410328488045654]
	TIME [epoch: 9.03 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07216161143596299		[learning rate: 0.0010361]
		[batch 20/20] avg loss: 0.04501059873950845		[learning rate: 0.0010345]
	Learning Rate: 0.00103451
	LOSS [training: 0.05858610508773572 | validation: 0.05847906008482809]
	TIME [epoch: 9.04 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07945956529052277		[learning rate: 0.0010329]
		[batch 20/20] avg loss: 0.05512556479340001		[learning rate: 0.0010313]
	Learning Rate: 0.00103134
	LOSS [training: 0.06729256504196139 | validation: 0.031374616444167946]
	TIME [epoch: 9.04 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052384316974473086		[learning rate: 0.0010298]
		[batch 20/20] avg loss: 0.06541994098599846		[learning rate: 0.0010282]
	Learning Rate: 0.00102817
	LOSS [training: 0.058902128980235766 | validation: 0.08491800595027992]
	TIME [epoch: 9.06 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09283695179785907		[learning rate: 0.0010266]
		[batch 20/20] avg loss: 0.06600443617472095		[learning rate: 0.001025]
	Learning Rate: 0.00102502
	LOSS [training: 0.07942069398629001 | validation: 0.035058498815239335]
	TIME [epoch: 9.04 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0822840977767098		[learning rate: 0.0010234]
		[batch 20/20] avg loss: 0.06890258027092425		[learning rate: 0.0010219]
	Learning Rate: 0.00102188
	LOSS [training: 0.07559333902381701 | validation: 0.042639637227250435]
	TIME [epoch: 9.04 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05650823087127368		[learning rate: 0.0010203]
		[batch 20/20] avg loss: 0.05578714638795579		[learning rate: 0.0010187]
	Learning Rate: 0.00101875
	LOSS [training: 0.056147688629614736 | validation: 0.06344783944422844]
	TIME [epoch: 9.03 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06371464344973955		[learning rate: 0.0010172]
		[batch 20/20] avg loss: 0.06034523758433334		[learning rate: 0.0010156]
	Learning Rate: 0.00101562
	LOSS [training: 0.06202994051703644 | validation: 0.07416568570287005]
	TIME [epoch: 9.03 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06391591077568137		[learning rate: 0.0010141]
		[batch 20/20] avg loss: 0.0645743952430541		[learning rate: 0.0010125]
	Learning Rate: 0.00101251
	LOSS [training: 0.06424515300936773 | validation: 0.05210387308784972]
	TIME [epoch: 9.05 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07246802386894342		[learning rate: 0.001011]
		[batch 20/20] avg loss: 0.06369505011204889		[learning rate: 0.0010094]
	Learning Rate: 0.00100941
	LOSS [training: 0.06808153699049616 | validation: 0.05965342409226255]
	TIME [epoch: 9.04 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10659685052864305		[learning rate: 0.0010079]
		[batch 20/20] avg loss: 0.0746197240813857		[learning rate: 0.0010063]
	Learning Rate: 0.00100631
	LOSS [training: 0.09060828730501436 | validation: 0.05250161265608333]
	TIME [epoch: 9.03 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07168382611451884		[learning rate: 0.0010048]
		[batch 20/20] avg loss: 0.05540240814099649		[learning rate: 0.0010032]
	Learning Rate: 0.00100323
	LOSS [training: 0.06354311712775768 | validation: 0.04450785397175004]
	TIME [epoch: 9.04 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06975229664123526		[learning rate: 0.0010017]
		[batch 20/20] avg loss: 0.07136055371184064		[learning rate: 0.0010002]
	Learning Rate: 0.00100015
	LOSS [training: 0.07055642517653794 | validation: 0.06546596948584048]
	TIME [epoch: 9.04 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05465765156714014		[learning rate: 0.00099862]
		[batch 20/20] avg loss: 0.06880623422665554		[learning rate: 0.00099709]
	Learning Rate: 0.000997087
	LOSS [training: 0.06173194289689783 | validation: 0.05518097157632217]
	TIME [epoch: 9.06 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07386312142324167		[learning rate: 0.00099556]
		[batch 20/20] avg loss: 0.07151830566928485		[learning rate: 0.00099403]
	Learning Rate: 0.000994031
	LOSS [training: 0.07269071354626326 | validation: 0.06569887859930564]
	TIME [epoch: 9.05 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05557294935928556		[learning rate: 0.00099251]
		[batch 20/20] avg loss: 0.05940568726620191		[learning rate: 0.00099098]
	Learning Rate: 0.000990984
	LOSS [training: 0.057489318312743745 | validation: 0.027689604899357168]
	TIME [epoch: 9.04 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06081970474506735		[learning rate: 0.00098946]
		[batch 20/20] avg loss: 0.06155039970626571		[learning rate: 0.00098795]
	Learning Rate: 0.000987946
	LOSS [training: 0.06118505222566652 | validation: 0.04755966120594082]
	TIME [epoch: 9.04 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07528849066377467		[learning rate: 0.00098643]
		[batch 20/20] avg loss: 0.05248655449745125		[learning rate: 0.00098492]
	Learning Rate: 0.000984918
	LOSS [training: 0.06388752258061295 | validation: 0.03451301024862108]
	TIME [epoch: 9.04 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05990084867846255		[learning rate: 0.00098341]
		[batch 20/20] avg loss: 0.0570771628769642		[learning rate: 0.0009819]
	Learning Rate: 0.000981899
	LOSS [training: 0.058489005777713376 | validation: 0.038180086621959784]
	TIME [epoch: 9.06 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06019380761972691		[learning rate: 0.00098039]
		[batch 20/20] avg loss: 0.052954973645460215		[learning rate: 0.00097889]
	Learning Rate: 0.000978889
	LOSS [training: 0.05657439063259356 | validation: 0.04390841672853513]
	TIME [epoch: 9.04 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059628304963161814		[learning rate: 0.00097739]
		[batch 20/20] avg loss: 0.05596302795116235		[learning rate: 0.00097589]
	Learning Rate: 0.000975888
	LOSS [training: 0.05779566645716209 | validation: 0.029783742833111526]
	TIME [epoch: 9.14 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05261329918748029		[learning rate: 0.00097439]
		[batch 20/20] avg loss: 0.06720306207563748		[learning rate: 0.0009729]
	Learning Rate: 0.000972897
	LOSS [training: 0.05990818063155888 | validation: 0.033108209586753104]
	TIME [epoch: 9.03 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06881431346222489		[learning rate: 0.0009714]
		[batch 20/20] avg loss: 0.08274598322984221		[learning rate: 0.00096991]
	Learning Rate: 0.000969914
	LOSS [training: 0.07578014834603357 | validation: 0.024564773511140385]
	TIME [epoch: 9.04 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_1260.pth
	Model improved!!!
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06205135173434327		[learning rate: 0.00096843]
		[batch 20/20] avg loss: 0.058778455344318956		[learning rate: 0.00096694]
	Learning Rate: 0.000966941
	LOSS [training: 0.06041490353933112 | validation: 0.029362518696577338]
	TIME [epoch: 9.07 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048020454313905565		[learning rate: 0.00096546]
		[batch 20/20] avg loss: 0.0512792579679199		[learning rate: 0.00096398]
	Learning Rate: 0.000963977
	LOSS [training: 0.04964985614091273 | validation: 0.03963370086212032]
	TIME [epoch: 9.04 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05614908421334015		[learning rate: 0.0009625]
		[batch 20/20] avg loss: 0.052941877648162605		[learning rate: 0.00096102]
	Learning Rate: 0.000961022
	LOSS [training: 0.05454548093075138 | validation: 0.05879679394395687]
	TIME [epoch: 9.05 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055139645274335305		[learning rate: 0.00095955]
		[batch 20/20] avg loss: 0.08893814306755436		[learning rate: 0.00095808]
	Learning Rate: 0.000958076
	LOSS [training: 0.07203889417094482 | validation: 0.06459858089091795]
	TIME [epoch: 9.04 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06873448364073917		[learning rate: 0.00095661]
		[batch 20/20] avg loss: 0.08978558325248286		[learning rate: 0.00095514]
	Learning Rate: 0.000955139
	LOSS [training: 0.079260033446611 | validation: 0.07092781367706791]
	TIME [epoch: 9.04 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07541326929850595		[learning rate: 0.00095367]
		[batch 20/20] avg loss: 0.060952592562320995		[learning rate: 0.00095221]
	Learning Rate: 0.000952211
	LOSS [training: 0.06818293093041347 | validation: 0.038592630735118066]
	TIME [epoch: 9.06 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05360870509390967		[learning rate: 0.00095075]
		[batch 20/20] avg loss: 0.05512597701428288		[learning rate: 0.00094929]
	Learning Rate: 0.000949292
	LOSS [training: 0.05436734105409626 | validation: 0.035989958654309484]
	TIME [epoch: 9.04 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05739296350051041		[learning rate: 0.00094784]
		[batch 20/20] avg loss: 0.06979408870742217		[learning rate: 0.00094638]
	Learning Rate: 0.000946382
	LOSS [training: 0.06359352610396628 | validation: 0.04980643816692821]
	TIME [epoch: 9.04 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0546913065351853		[learning rate: 0.00094493]
		[batch 20/20] avg loss: 0.06444339484234671		[learning rate: 0.00094348]
	Learning Rate: 0.000943481
	LOSS [training: 0.059567350688765994 | validation: 0.055140097492659516]
	TIME [epoch: 9.04 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08262978406299101		[learning rate: 0.00094203]
		[batch 20/20] avg loss: 0.06366944607671955		[learning rate: 0.00094059]
	Learning Rate: 0.000940589
	LOSS [training: 0.07314961506985526 | validation: 0.02813265885034655]
	TIME [epoch: 9.04 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06677323038517322		[learning rate: 0.00093915]
		[batch 20/20] avg loss: 0.05994894846820291		[learning rate: 0.00093771]
	Learning Rate: 0.000937706
	LOSS [training: 0.06336108942668806 | validation: 0.040835798964280426]
	TIME [epoch: 9.06 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057874811637550386		[learning rate: 0.00093627]
		[batch 20/20] avg loss: 0.06907278553232801		[learning rate: 0.00093483]
	Learning Rate: 0.000934831
	LOSS [training: 0.0634737985849392 | validation: 0.08198535112238031]
	TIME [epoch: 9.05 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07619626051878121		[learning rate: 0.0009334]
		[batch 20/20] avg loss: 0.05446312399959796		[learning rate: 0.00093197]
	Learning Rate: 0.000931966
	LOSS [training: 0.06532969225918958 | validation: 0.05707222385250585]
	TIME [epoch: 9.05 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060411377401035284		[learning rate: 0.00093054]
		[batch 20/20] avg loss: 0.04810917080449292		[learning rate: 0.00092911]
	Learning Rate: 0.000929109
	LOSS [training: 0.05426027410276411 | validation: 0.0857468792966647]
	TIME [epoch: 9.03 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06737785325500587		[learning rate: 0.00092768]
		[batch 20/20] avg loss: 0.07522930517558546		[learning rate: 0.00092626]
	Learning Rate: 0.000926261
	LOSS [training: 0.07130357921529565 | validation: 0.04719930558690785]
	TIME [epoch: 9.04 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053597749034390366		[learning rate: 0.00092484]
		[batch 20/20] avg loss: 0.05900320844612589		[learning rate: 0.00092342]
	Learning Rate: 0.000923421
	LOSS [training: 0.056300478740258134 | validation: 0.0446442729301821]
	TIME [epoch: 9.06 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07003152025912256		[learning rate: 0.00092201]
		[batch 20/20] avg loss: 0.05460314815606697		[learning rate: 0.00092059]
	Learning Rate: 0.000920591
	LOSS [training: 0.06231733420759476 | validation: 0.052489744557003205]
	TIME [epoch: 9.05 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049104992574524206		[learning rate: 0.00091918]
		[batch 20/20] avg loss: 0.06115389948511994		[learning rate: 0.00091777]
	Learning Rate: 0.000917769
	LOSS [training: 0.055129446029822074 | validation: 0.05870851900598538]
	TIME [epoch: 9.04 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06335453343895278		[learning rate: 0.00091636]
		[batch 20/20] avg loss: 0.06315382970855991		[learning rate: 0.00091496]
	Learning Rate: 0.000914956
	LOSS [training: 0.06325418157375635 | validation: 0.0321366163808439]
	TIME [epoch: 9.04 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04477855463237489		[learning rate: 0.00091355]
		[batch 20/20] avg loss: 0.05854697820998295		[learning rate: 0.00091215]
	Learning Rate: 0.000912151
	LOSS [training: 0.05166276642117892 | validation: 0.06647031048097575]
	TIME [epoch: 9.04 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05112059133816005		[learning rate: 0.00091075]
		[batch 20/20] avg loss: 0.059304194677109026		[learning rate: 0.00090935]
	Learning Rate: 0.000909355
	LOSS [training: 0.055212393007634544 | validation: 0.10797179769510648]
	TIME [epoch: 9.07 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06456094193904756		[learning rate: 0.00090796]
		[batch 20/20] avg loss: 0.060981853918704663		[learning rate: 0.00090657]
	Learning Rate: 0.000906567
	LOSS [training: 0.06277139792887612 | validation: 0.03193060781725738]
	TIME [epoch: 9.04 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06540834398056658		[learning rate: 0.00090518]
		[batch 20/20] avg loss: 0.05319654893601403		[learning rate: 0.00090379]
	Learning Rate: 0.000903788
	LOSS [training: 0.05930244645829032 | validation: 0.054666163234863896]
	TIME [epoch: 9.05 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05780504248532044		[learning rate: 0.0009024]
		[batch 20/20] avg loss: 0.0630444144679296		[learning rate: 0.00090102]
	Learning Rate: 0.000901018
	LOSS [training: 0.06042472847662502 | validation: 0.0340756934146326]
	TIME [epoch: 9.05 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06183578078594826		[learning rate: 0.00089964]
		[batch 20/20] avg loss: 0.04344562164323936		[learning rate: 0.00089826]
	Learning Rate: 0.000898256
	LOSS [training: 0.0526407012145938 | validation: 0.0258586419663949]
	TIME [epoch: 9.04 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041715340796928735		[learning rate: 0.00089688]
		[batch 20/20] avg loss: 0.06475000277476148		[learning rate: 0.0008955]
	Learning Rate: 0.000895502
	LOSS [training: 0.053232671785845107 | validation: 0.047830325280810514]
	TIME [epoch: 9.06 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05347648400882221		[learning rate: 0.00089413]
		[batch 20/20] avg loss: 0.05468540925794193		[learning rate: 0.00089276]
	Learning Rate: 0.000892757
	LOSS [training: 0.05408094663338207 | validation: 0.07045762826544706]
	TIME [epoch: 9.04 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07024190134163827		[learning rate: 0.00089139]
		[batch 20/20] avg loss: 0.04358095600168745		[learning rate: 0.00089002]
	Learning Rate: 0.00089002
	LOSS [training: 0.05691142867166286 | validation: 0.027531414435391276]
	TIME [epoch: 9.04 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042201177151618405		[learning rate: 0.00088866]
		[batch 20/20] avg loss: 0.05870698207394006		[learning rate: 0.00088729]
	Learning Rate: 0.000887292
	LOSS [training: 0.050454079612779236 | validation: 0.12565067799423163]
	TIME [epoch: 9.03 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06929276366276685		[learning rate: 0.00088593]
		[batch 20/20] avg loss: 0.06614878516040087		[learning rate: 0.00088457]
	Learning Rate: 0.000884572
	LOSS [training: 0.06772077441158389 | validation: 0.03999661473019684]
	TIME [epoch: 9.04 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05271198870704841		[learning rate: 0.00088322]
		[batch 20/20] avg loss: 0.06688386744752023		[learning rate: 0.00088186]
	Learning Rate: 0.000881861
	LOSS [training: 0.059797928077284325 | validation: 0.04436716482463663]
	TIME [epoch: 9.07 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08046055377583851		[learning rate: 0.00088051]
		[batch 20/20] avg loss: 0.08607438940359721		[learning rate: 0.00087916]
	Learning Rate: 0.000879157
	LOSS [training: 0.08326747158971785 | validation: 0.04636110234418309]
	TIME [epoch: 9.04 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056666397552036195		[learning rate: 0.00087781]
		[batch 20/20] avg loss: 0.07124665629321655		[learning rate: 0.00087646]
	Learning Rate: 0.000876462
	LOSS [training: 0.06395652692262636 | validation: 0.09543878456669806]
	TIME [epoch: 9.04 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08391823692486741		[learning rate: 0.00087512]
		[batch 20/20] avg loss: 0.070666542530553		[learning rate: 0.00087378]
	Learning Rate: 0.000873776
	LOSS [training: 0.0772923897277102 | validation: 0.029424507042665467]
	TIME [epoch: 9.04 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0588176637937675		[learning rate: 0.00087244]
		[batch 20/20] avg loss: 0.07183092027314464		[learning rate: 0.0008711]
	Learning Rate: 0.000871097
	LOSS [training: 0.06532429203345605 | validation: 0.030585629867402604]
	TIME [epoch: 9.04 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07720529380520984		[learning rate: 0.00086976]
		[batch 20/20] avg loss: 0.05161459482980672		[learning rate: 0.00086843]
	Learning Rate: 0.000868427
	LOSS [training: 0.06440994431750827 | validation: 0.01927145680743099]
	TIME [epoch: 9.07 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_1296.pth
	Model improved!!!
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048656460481492206		[learning rate: 0.00086709]
		[batch 20/20] avg loss: 0.049779017180193424		[learning rate: 0.00086576]
	Learning Rate: 0.000865765
	LOSS [training: 0.049217738830842815 | validation: 0.03395362992337311]
	TIME [epoch: 9.05 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07440297035990606		[learning rate: 0.00086444]
		[batch 20/20] avg loss: 0.057596000141428225		[learning rate: 0.00086311]
	Learning Rate: 0.000863111
	LOSS [training: 0.06599948525066714 | validation: 0.032941539556843015]
	TIME [epoch: 9.04 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0619281862718021		[learning rate: 0.00086179]
		[batch 20/20] avg loss: 0.06274735856872633		[learning rate: 0.00086047]
	Learning Rate: 0.000860465
	LOSS [training: 0.06233777242026421 | validation: 0.06402706649850952]
	TIME [epoch: 9.04 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061215606868910324		[learning rate: 0.00085915]
		[batch 20/20] avg loss: 0.07466184448858497		[learning rate: 0.00085783]
	Learning Rate: 0.000857828
	LOSS [training: 0.06793872567874763 | validation: 0.06607015915242738]
	TIME [epoch: 9.04 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08600261485321273		[learning rate: 0.00085651]
		[batch 20/20] avg loss: 0.057899897632277306		[learning rate: 0.0008552]
	Learning Rate: 0.000855198
	LOSS [training: 0.07195125624274501 | validation: 0.0747823380705239]
	TIME [epoch: 9.07 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05902459466875067		[learning rate: 0.00085389]
		[batch 20/20] avg loss: 0.09581765274497708		[learning rate: 0.00085258]
	Learning Rate: 0.000852576
	LOSS [training: 0.07742112370686387 | validation: 0.05435776300140323]
	TIME [epoch: 9.05 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07544022635348345		[learning rate: 0.00085127]
		[batch 20/20] avg loss: 0.05692226852847286		[learning rate: 0.00084996]
	Learning Rate: 0.000849963
	LOSS [training: 0.06618124744097817 | validation: 0.031057843295725834]
	TIME [epoch: 9.04 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056720528749581355		[learning rate: 0.00084866]
		[batch 20/20] avg loss: 0.045661696063257096		[learning rate: 0.00084736]
	Learning Rate: 0.000847357
	LOSS [training: 0.051191112406419215 | validation: 0.03873164532664189]
	TIME [epoch: 9.04 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050987616590032946		[learning rate: 0.00084606]
		[batch 20/20] avg loss: 0.053445279019500594		[learning rate: 0.00084476]
	Learning Rate: 0.00084476
	LOSS [training: 0.05221644780476675 | validation: 0.045385237195808836]
	TIME [epoch: 9.04 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05027282183719539		[learning rate: 0.00084346]
		[batch 20/20] avg loss: 0.05715712075828261		[learning rate: 0.00084217]
	Learning Rate: 0.00084217
	LOSS [training: 0.053714971297739014 | validation: 0.04311668610352057]
	TIME [epoch: 9.08 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04953448781346605		[learning rate: 0.00084088]
		[batch 20/20] avg loss: 0.04779286100325367		[learning rate: 0.00083959]
	Learning Rate: 0.000839589
	LOSS [training: 0.04866367440835986 | validation: 0.03891358073243361]
	TIME [epoch: 9.04 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050548528637013514		[learning rate: 0.0008383]
		[batch 20/20] avg loss: 0.0571311293657618		[learning rate: 0.00083702]
	Learning Rate: 0.000837015
	LOSS [training: 0.05383982900138766 | validation: 0.056078197396888727]
	TIME [epoch: 9.05 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06616362844262971		[learning rate: 0.00083573]
		[batch 20/20] avg loss: 0.07525013934081541		[learning rate: 0.00083445]
	Learning Rate: 0.000834449
	LOSS [training: 0.07070688389172257 | validation: 0.0933446785831702]
	TIME [epoch: 9.05 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05487782986921811		[learning rate: 0.00083317]
		[batch 20/20] avg loss: 0.05465168756412055		[learning rate: 0.00083189]
	Learning Rate: 0.000831891
	LOSS [training: 0.054764758716669325 | validation: 0.036898985269954714]
	TIME [epoch: 9.05 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04955638450066663		[learning rate: 0.00083062]
		[batch 20/20] avg loss: 0.11403737055586831		[learning rate: 0.00082934]
	Learning Rate: 0.000829341
	LOSS [training: 0.08179687752826748 | validation: 0.038230624553970055]
	TIME [epoch: 9.07 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05145994289317424		[learning rate: 0.00082807]
		[batch 20/20] avg loss: 0.05013500586889341		[learning rate: 0.0008268]
	Learning Rate: 0.000826799
	LOSS [training: 0.050797474381033825 | validation: 0.03722618454017606]
	TIME [epoch: 9.04 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05936787057571065		[learning rate: 0.00082553]
		[batch 20/20] avg loss: 0.07622757860085291		[learning rate: 0.00082426]
	Learning Rate: 0.000824265
	LOSS [training: 0.06779772458828177 | validation: 0.051654153782013164]
	TIME [epoch: 9.04 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05182419090903275		[learning rate: 0.000823]
		[batch 20/20] avg loss: 0.054784724472203596		[learning rate: 0.00082174]
	Learning Rate: 0.000821738
	LOSS [training: 0.05330445769061818 | validation: 0.035552189927423095]
	TIME [epoch: 9.04 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04939391736842715		[learning rate: 0.00082048]
		[batch 20/20] avg loss: 0.05052271265387598		[learning rate: 0.00081922]
	Learning Rate: 0.000819219
	LOSS [training: 0.04995831501115157 | validation: 0.05764398845143857]
	TIME [epoch: 9.04 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052160033104798845		[learning rate: 0.00081796]
		[batch 20/20] avg loss: 0.04458184986496847		[learning rate: 0.00081671]
	Learning Rate: 0.000816708
	LOSS [training: 0.048370941484883655 | validation: 0.030508721796118125]
	TIME [epoch: 9.07 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038639801113006425		[learning rate: 0.00081545]
		[batch 20/20] avg loss: 0.05210228221337728		[learning rate: 0.0008142]
	Learning Rate: 0.000814204
	LOSS [training: 0.045371041663191856 | validation: 0.02905356543194517]
	TIME [epoch: 9.04 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05093594714370966		[learning rate: 0.00081296]
		[batch 20/20] avg loss: 0.058049964413893484		[learning rate: 0.00081171]
	Learning Rate: 0.000811708
	LOSS [training: 0.05449295577880157 | validation: 0.08775891115041395]
	TIME [epoch: 9.04 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07315604470745604		[learning rate: 0.00081046]
		[batch 20/20] avg loss: 0.05750798126875502		[learning rate: 0.00080922]
	Learning Rate: 0.00080922
	LOSS [training: 0.06533201298810552 | validation: 0.1224575175703329]
	TIME [epoch: 9.05 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0763964479902212		[learning rate: 0.00080798]
		[batch 20/20] avg loss: 0.056064396546528505		[learning rate: 0.00080674]
	Learning Rate: 0.000806739
	LOSS [training: 0.06623042226837483 | validation: 0.04204187159889161]
	TIME [epoch: 9.04 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05611615138420636		[learning rate: 0.0008055]
		[batch 20/20] avg loss: 0.05065534885979654		[learning rate: 0.00080427]
	Learning Rate: 0.000804267
	LOSS [training: 0.05338575012200145 | validation: 0.0578512825619777]
	TIME [epoch: 9.07 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059408220068660554		[learning rate: 0.00080303]
		[batch 20/20] avg loss: 0.04310322207119488		[learning rate: 0.0008018]
	Learning Rate: 0.000801801
	LOSS [training: 0.05125572106992772 | validation: 0.03720886650198048]
	TIME [epoch: 9.05 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049672945656371266		[learning rate: 0.00080057]
		[batch 20/20] avg loss: 0.07317062000372575		[learning rate: 0.00079934]
	Learning Rate: 0.000799343
	LOSS [training: 0.061421782830048496 | validation: 0.07094513261021818]
	TIME [epoch: 9.05 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053918642652210046		[learning rate: 0.00079812]
		[batch 20/20] avg loss: 0.05796537839500698		[learning rate: 0.00079689]
	Learning Rate: 0.000796893
	LOSS [training: 0.05594201052360852 | validation: 0.04098285097619036]
	TIME [epoch: 9.05 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04290002331551277		[learning rate: 0.00079567]
		[batch 20/20] avg loss: 0.05555196723666757		[learning rate: 0.00079445]
	Learning Rate: 0.00079445
	LOSS [training: 0.04922599527609017 | validation: 0.04288372952880277]
	TIME [epoch: 9.05 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048254781390435576		[learning rate: 0.00079323]
		[batch 20/20] avg loss: 0.0713112728381107		[learning rate: 0.00079201]
	Learning Rate: 0.000792015
	LOSS [training: 0.05978302711427312 | validation: 0.041225630149147076]
	TIME [epoch: 9.07 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048214877532166425		[learning rate: 0.0007908]
		[batch 20/20] avg loss: 0.042629290939703406		[learning rate: 0.00078959]
	Learning Rate: 0.000789587
	LOSS [training: 0.04542208423593492 | validation: 0.03606331103979203]
	TIME [epoch: 9.06 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05280342555219712		[learning rate: 0.00078838]
		[batch 20/20] avg loss: 0.06569620828201292		[learning rate: 0.00078717]
	Learning Rate: 0.000787166
	LOSS [training: 0.059249816917105014 | validation: 0.06403220832617619]
	TIME [epoch: 9.04 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06495107148182792		[learning rate: 0.00078596]
		[batch 20/20] avg loss: 0.060768122071028385		[learning rate: 0.00078475]
	Learning Rate: 0.000784754
	LOSS [training: 0.06285959677642813 | validation: 0.02845314482800941]
	TIME [epoch: 9.04 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060836109521099156		[learning rate: 0.00078355]
		[batch 20/20] avg loss: 0.05522065665803928		[learning rate: 0.00078235]
	Learning Rate: 0.000782348
	LOSS [training: 0.0580283830895692 | validation: 0.04481191455690662]
	TIME [epoch: 9.04 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06054672046946933		[learning rate: 0.00078115]
		[batch 20/20] avg loss: 0.05224078497470186		[learning rate: 0.00077995]
	Learning Rate: 0.00077995
	LOSS [training: 0.05639375272208559 | validation: 0.04451176042855354]
	TIME [epoch: 9.05 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06894198525644653		[learning rate: 0.00077875]
		[batch 20/20] avg loss: 0.056762155126532945		[learning rate: 0.00077756]
	Learning Rate: 0.000777559
	LOSS [training: 0.06285207019148974 | validation: 0.05666022155338995]
	TIME [epoch: 9.05 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06271214908607879		[learning rate: 0.00077637]
		[batch 20/20] avg loss: 0.05193678942014943		[learning rate: 0.00077518]
	Learning Rate: 0.000775175
	LOSS [training: 0.057324469253114116 | validation: 0.04246880912160331]
	TIME [epoch: 9.04 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04843581132005754		[learning rate: 0.00077399]
		[batch 20/20] avg loss: 0.0693007639137306		[learning rate: 0.0007728]
	Learning Rate: 0.000772799
	LOSS [training: 0.058868287616894076 | validation: 0.03136747677874784]
	TIME [epoch: 9.05 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059466353827640886		[learning rate: 0.00077161]
		[batch 20/20] avg loss: 0.0392743143177659		[learning rate: 0.00077043]
	Learning Rate: 0.00077043
	LOSS [training: 0.04937033407270339 | validation: 0.03478711931700515]
	TIME [epoch: 9.04 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05541111747444335		[learning rate: 0.00076925]
		[batch 20/20] avg loss: 0.05233706818138001		[learning rate: 0.00076807]
	Learning Rate: 0.000768068
	LOSS [training: 0.05387409282791168 | validation: 0.04425450369715622]
	TIME [epoch: 9.06 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05203663398747747		[learning rate: 0.00076689]
		[batch 20/20] avg loss: 0.05560417066894059		[learning rate: 0.00076571]
	Learning Rate: 0.000765714
	LOSS [training: 0.05382040232820904 | validation: 0.05881520853843641]
	TIME [epoch: 9.05 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0641484928510728		[learning rate: 0.00076454]
		[batch 20/20] avg loss: 0.06238133625325916		[learning rate: 0.00076337]
	Learning Rate: 0.000763367
	LOSS [training: 0.06326491455216597 | validation: 0.04804056530148906]
	TIME [epoch: 9.04 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06315324714058969		[learning rate: 0.0007622]
		[batch 20/20] avg loss: 0.06594630525910788		[learning rate: 0.00076103]
	Learning Rate: 0.000761027
	LOSS [training: 0.06454977619984881 | validation: 0.05428594720681254]
	TIME [epoch: 9.05 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04676326218486878		[learning rate: 0.00075986]
		[batch 20/20] avg loss: 0.043593695050625735		[learning rate: 0.00075869]
	Learning Rate: 0.000758694
	LOSS [training: 0.04517847861774726 | validation: 0.05022284518757961]
	TIME [epoch: 9.04 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048850123349856195		[learning rate: 0.00075753]
		[batch 20/20] avg loss: 0.059810751487290315		[learning rate: 0.00075637]
	Learning Rate: 0.000756368
	LOSS [training: 0.054330437418573276 | validation: 0.09117418254060584]
	TIME [epoch: 9.06 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06382216549297846		[learning rate: 0.00075521]
		[batch 20/20] avg loss: 0.048491122797537936		[learning rate: 0.00075405]
	Learning Rate: 0.00075405
	LOSS [training: 0.056156644145258214 | validation: 0.045514574488256056]
	TIME [epoch: 9.05 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06046090366867365		[learning rate: 0.00075289]
		[batch 20/20] avg loss: 0.04912727289744505		[learning rate: 0.00075174]
	Learning Rate: 0.000751738
	LOSS [training: 0.05479408828305936 | validation: 0.04523961503711829]
	TIME [epoch: 9.05 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06295184864497452		[learning rate: 0.00075059]
		[batch 20/20] avg loss: 0.07608439928589814		[learning rate: 0.00074943]
	Learning Rate: 0.000749434
	LOSS [training: 0.06951812396543633 | validation: 0.06017532092377206]
	TIME [epoch: 9.04 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050544940152117204		[learning rate: 0.00074828]
		[batch 20/20] avg loss: 0.06054149452263789		[learning rate: 0.00074714]
	Learning Rate: 0.000747137
	LOSS [training: 0.05554321733737756 | validation: 0.044479418102685256]
	TIME [epoch: 9.04 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062491068999897106		[learning rate: 0.00074599]
		[batch 20/20] avg loss: 0.06837994941719208		[learning rate: 0.00074485]
	Learning Rate: 0.000744846
	LOSS [training: 0.06543550920854459 | validation: 0.045465204760131696]
	TIME [epoch: 9.07 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05367538125749359		[learning rate: 0.0007437]
		[batch 20/20] avg loss: 0.05329917639746821		[learning rate: 0.00074256]
	Learning Rate: 0.000742563
	LOSS [training: 0.05348727882748089 | validation: 0.047511034880180686]
	TIME [epoch: 9.06 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05119669473852846		[learning rate: 0.00074142]
		[batch 20/20] avg loss: 0.06875569611359537		[learning rate: 0.00074029]
	Learning Rate: 0.000740287
	LOSS [training: 0.05997619542606192 | validation: 0.04273548790318982]
	TIME [epoch: 9.04 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06138882122775534		[learning rate: 0.00073915]
		[batch 20/20] avg loss: 0.0554590001582983		[learning rate: 0.00073802]
	Learning Rate: 0.000738017
	LOSS [training: 0.05842391069302681 | validation: 0.059085398544998384]
	TIME [epoch: 9.04 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06156384070229742		[learning rate: 0.00073689]
		[batch 20/20] avg loss: 0.05300515652508333		[learning rate: 0.00073576]
	Learning Rate: 0.000735755
	LOSS [training: 0.05728449861369038 | validation: 0.07066240289610977]
	TIME [epoch: 9.05 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06382502760914274		[learning rate: 0.00073463]
		[batch 20/20] avg loss: 0.058967858686538886		[learning rate: 0.0007335]
	Learning Rate: 0.0007335
	LOSS [training: 0.06139644314784082 | validation: 0.041204934674576504]
	TIME [epoch: 9.05 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0607746954595743		[learning rate: 0.00073237]
		[batch 20/20] avg loss: 0.056282581665481925		[learning rate: 0.00073125]
	Learning Rate: 0.000731251
	LOSS [training: 0.05852863856252812 | validation: 0.04279623004736853]
	TIME [epoch: 9.05 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07623198759974098		[learning rate: 0.00073013]
		[batch 20/20] avg loss: 0.053013767438122014		[learning rate: 0.00072901]
	Learning Rate: 0.00072901
	LOSS [training: 0.0646228775189315 | validation: 0.038300158332700444]
	TIME [epoch: 9.05 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07111570904743747		[learning rate: 0.00072789]
		[batch 20/20] avg loss: 0.06115637575401589		[learning rate: 0.00072677]
	Learning Rate: 0.000726775
	LOSS [training: 0.06613604240072667 | validation: 0.035030109972967065]
	TIME [epoch: 9.04 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06600771750787358		[learning rate: 0.00072566]
		[batch 20/20] avg loss: 0.057691113942924435		[learning rate: 0.00072455]
	Learning Rate: 0.000724547
	LOSS [training: 0.061849415725399 | validation: 0.03244937777292642]
	TIME [epoch: 9.04 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05437751315108922		[learning rate: 0.00072344]
		[batch 20/20] avg loss: 0.047361081350700766		[learning rate: 0.00072233]
	Learning Rate: 0.000722326
	LOSS [training: 0.05086929725089499 | validation: 0.04895523868003887]
	TIME [epoch: 9.06 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0666865552144481		[learning rate: 0.00072122]
		[batch 20/20] avg loss: 0.06659689083904216		[learning rate: 0.00072011]
	Learning Rate: 0.000720112
	LOSS [training: 0.06664172302674512 | validation: 0.036800473118730984]
	TIME [epoch: 9.04 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055661622976569944		[learning rate: 0.00071901]
		[batch 20/20] avg loss: 0.06060676005030433		[learning rate: 0.0007179]
	Learning Rate: 0.000717904
	LOSS [training: 0.05813419151343714 | validation: 0.05954486735511732]
	TIME [epoch: 9.03 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05666199879520748		[learning rate: 0.0007168]
		[batch 20/20] avg loss: 0.046791208313974555		[learning rate: 0.0007157]
	Learning Rate: 0.000715704
	LOSS [training: 0.05172660355459101 | validation: 0.03625469209516262]
	TIME [epoch: 9.03 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04868541957973223		[learning rate: 0.00071461]
		[batch 20/20] avg loss: 0.05511032324228898		[learning rate: 0.00071351]
	Learning Rate: 0.00071351
	LOSS [training: 0.05189787141101061 | validation: 0.04956398647427176]
	TIME [epoch: 9.04 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05079989469631814		[learning rate: 0.00071242]
		[batch 20/20] avg loss: 0.04929483249260942		[learning rate: 0.00071132]
	Learning Rate: 0.000711323
	LOSS [training: 0.05004736359446378 | validation: 0.01591076609608349]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_1361.pth
	Model improved!!!
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043256030117781445		[learning rate: 0.00071023]
		[batch 20/20] avg loss: 0.05529516899794988		[learning rate: 0.00070914]
	Learning Rate: 0.000709142
	LOSS [training: 0.04927559955786565 | validation: 0.032501284648337014]
	TIME [epoch: 9.04 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04666172659222135		[learning rate: 0.00070805]
		[batch 20/20] avg loss: 0.050363325826414305		[learning rate: 0.00070697]
	Learning Rate: 0.000706968
	LOSS [training: 0.04851252620931783 | validation: 0.033509932357665626]
	TIME [epoch: 9.03 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04216592299824448		[learning rate: 0.00070588]
		[batch 20/20] avg loss: 0.05155995268336189		[learning rate: 0.0007048]
	Learning Rate: 0.000704801
	LOSS [training: 0.0468629378408032 | validation: 0.07620735134180298]
	TIME [epoch: 9.03 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05679420836689375		[learning rate: 0.00070372]
		[batch 20/20] avg loss: 0.05516440178350011		[learning rate: 0.00070264]
	Learning Rate: 0.000702641
	LOSS [training: 0.055979305075196925 | validation: 0.027552717413335683]
	TIME [epoch: 9.04 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.066399486630037		[learning rate: 0.00070156]
		[batch 20/20] avg loss: 0.04738267897542472		[learning rate: 0.00070049]
	Learning Rate: 0.000700487
	LOSS [training: 0.056891082802730863 | validation: 0.058388804481794315]
	TIME [epoch: 9.06 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05374143457443955		[learning rate: 0.00069941]
		[batch 20/20] avg loss: 0.05106645254368637		[learning rate: 0.00069834]
	Learning Rate: 0.000698339
	LOSS [training: 0.05240394355906296 | validation: 0.05019746286450501]
	TIME [epoch: 9.05 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04870734844455179		[learning rate: 0.00069727]
		[batch 20/20] avg loss: 0.04179830436029808		[learning rate: 0.0006962]
	Learning Rate: 0.000696199
	LOSS [training: 0.04525282640242494 | validation: 0.03867282421711418]
	TIME [epoch: 9.04 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04372563964088246		[learning rate: 0.00069513]
		[batch 20/20] avg loss: 0.048012160719230645		[learning rate: 0.00069406]
	Learning Rate: 0.000694065
	LOSS [training: 0.04586890018005656 | validation: 0.03517475148143478]
	TIME [epoch: 9.03 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048760723844168595		[learning rate: 0.000693]
		[batch 20/20] avg loss: 0.06185350586289688		[learning rate: 0.00069194]
	Learning Rate: 0.000691937
	LOSS [training: 0.055307114853532735 | validation: 0.05528544083959572]
	TIME [epoch: 9.03 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06386080381154569		[learning rate: 0.00069088]
		[batch 20/20] avg loss: 0.04302478500413404		[learning rate: 0.00068982]
	Learning Rate: 0.000689816
	LOSS [training: 0.05344279440783985 | validation: 0.03594227055112919]
	TIME [epoch: 9.06 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04964717599645229		[learning rate: 0.00068876]
		[batch 20/20] avg loss: 0.07079741300587364		[learning rate: 0.0006877]
	Learning Rate: 0.000687701
	LOSS [training: 0.060222294501162975 | validation: 0.0552697700938084]
	TIME [epoch: 9.04 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053907553890142824		[learning rate: 0.00068665]
		[batch 20/20] avg loss: 0.06852912820752716		[learning rate: 0.00068559]
	Learning Rate: 0.000685593
	LOSS [training: 0.06121834104883498 | validation: 0.04752540971617801]
	TIME [epoch: 9.04 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048172083443854256		[learning rate: 0.00068454]
		[batch 20/20] avg loss: 0.05422981446878441		[learning rate: 0.00068349]
	Learning Rate: 0.000683492
	LOSS [training: 0.05120094895631934 | validation: 0.056059038393526425]
	TIME [epoch: 9.03 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05011590371797707		[learning rate: 0.00068244]
		[batch 20/20] avg loss: 0.05506273450745416		[learning rate: 0.0006814]
	Learning Rate: 0.000681397
	LOSS [training: 0.05258931911271562 | validation: 0.048086945659775816]
	TIME [epoch: 9.03 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06771938357398458		[learning rate: 0.00068035]
		[batch 20/20] avg loss: 0.045112091149301786		[learning rate: 0.00067931]
	Learning Rate: 0.000679308
	LOSS [training: 0.056415737361643195 | validation: 0.02985003198147544]
	TIME [epoch: 9.05 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047056803648299075		[learning rate: 0.00067827]
		[batch 20/20] avg loss: 0.04397791969374541		[learning rate: 0.00067723]
	Learning Rate: 0.000677225
	LOSS [training: 0.045517361671022244 | validation: 0.046348800240560005]
	TIME [epoch: 9.04 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05952660686396248		[learning rate: 0.00067619]
		[batch 20/20] avg loss: 0.06782324704230427		[learning rate: 0.00067515]
	Learning Rate: 0.000675149
	LOSS [training: 0.06367492695313338 | validation: 0.04634533232842463]
	TIME [epoch: 9.03 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.072317282387945		[learning rate: 0.00067411]
		[batch 20/20] avg loss: 0.06454245872840007		[learning rate: 0.00067308]
	Learning Rate: 0.00067308
	LOSS [training: 0.06842987055817254 | validation: 0.05336843177944937]
	TIME [epoch: 9.03 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052910354435098436		[learning rate: 0.00067205]
		[batch 20/20] avg loss: 0.04947440797043627		[learning rate: 0.00067102]
	Learning Rate: 0.000671017
	LOSS [training: 0.051192381202767354 | validation: 0.05764375863812724]
	TIME [epoch: 9.03 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05034431187303381		[learning rate: 0.00066999]
		[batch 20/20] avg loss: 0.0474404908684258		[learning rate: 0.00066896]
	Learning Rate: 0.00066896
	LOSS [training: 0.0488924013707298 | validation: 0.034598314430061804]
	TIME [epoch: 9.05 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0391207604372379		[learning rate: 0.00066793]
		[batch 20/20] avg loss: 0.052818048104253036		[learning rate: 0.00066691]
	Learning Rate: 0.000666909
	LOSS [training: 0.045969404270745466 | validation: 0.036636211363008146]
	TIME [epoch: 9.04 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04812750558651874		[learning rate: 0.00066589]
		[batch 20/20] avg loss: 0.05386350133197528		[learning rate: 0.00066486]
	Learning Rate: 0.000664865
	LOSS [training: 0.050995503459247006 | validation: 0.038325594936261476]
	TIME [epoch: 9.04 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04055902744373387		[learning rate: 0.00066384]
		[batch 20/20] avg loss: 0.055400558583160106		[learning rate: 0.00066283]
	Learning Rate: 0.000662827
	LOSS [training: 0.047979793013446986 | validation: 0.051514427350382565]
	TIME [epoch: 9.04 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05529944087866738		[learning rate: 0.00066181]
		[batch 20/20] avg loss: 0.06083331478302072		[learning rate: 0.00066079]
	Learning Rate: 0.000660795
	LOSS [training: 0.05806637783084405 | validation: 0.05050254094976123]
	TIME [epoch: 9.04 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060291765865139534		[learning rate: 0.00065978]
		[batch 20/20] avg loss: 0.05373291146666763		[learning rate: 0.00065877]
	Learning Rate: 0.000658769
	LOSS [training: 0.05701233866590359 | validation: 0.04122542296200696]
	TIME [epoch: 9.06 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05620446020254401		[learning rate: 0.00065776]
		[batch 20/20] avg loss: 0.046478208207971015		[learning rate: 0.00065675]
	Learning Rate: 0.00065675
	LOSS [training: 0.051341334205257515 | validation: 0.034932205395383664]
	TIME [epoch: 9.04 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045488697851783666		[learning rate: 0.00065574]
		[batch 20/20] avg loss: 0.05542725512199507		[learning rate: 0.00065474]
	Learning Rate: 0.000654737
	LOSS [training: 0.05045797648688936 | validation: 0.04180500009284966]
	TIME [epoch: 9.03 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046761259960872395		[learning rate: 0.00065373]
		[batch 20/20] avg loss: 0.041341254038967816		[learning rate: 0.00065273]
	Learning Rate: 0.00065273
	LOSS [training: 0.0440512569999201 | validation: 0.026291149029138823]
	TIME [epoch: 9.04 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04988941409704396		[learning rate: 0.00065173]
		[batch 20/20] avg loss: 0.047890374203769556		[learning rate: 0.00065073]
	Learning Rate: 0.000650729
	LOSS [training: 0.04888989415040675 | validation: 0.0331983645774272]
	TIME [epoch: 9.04 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041591608968685016		[learning rate: 0.00064973]
		[batch 20/20] avg loss: 0.05391835150675879		[learning rate: 0.00064873]
	Learning Rate: 0.000648734
	LOSS [training: 0.0477549802377219 | validation: 0.03358682507191685]
	TIME [epoch: 9.06 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04338233626280739		[learning rate: 0.00064774]
		[batch 20/20] avg loss: 0.047463601754915985		[learning rate: 0.00064675]
	Learning Rate: 0.000646745
	LOSS [training: 0.0454229690088617 | validation: 0.027768228550508877]
	TIME [epoch: 9.05 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05055802810170005		[learning rate: 0.00064575]
		[batch 20/20] avg loss: 0.04934187062493563		[learning rate: 0.00064476]
	Learning Rate: 0.000644763
	LOSS [training: 0.049949949363317835 | validation: 0.02839500029988605]
	TIME [epoch: 9.04 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04337010286525074		[learning rate: 0.00064377]
		[batch 20/20] avg loss: 0.04885181634886292		[learning rate: 0.00064279]
	Learning Rate: 0.000642786
	LOSS [training: 0.046110959607056835 | validation: 0.04869665365680839]
	TIME [epoch: 9.03 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06932998835318058		[learning rate: 0.0006418]
		[batch 20/20] avg loss: 0.05275166174453708		[learning rate: 0.00064082]
	Learning Rate: 0.000640816
	LOSS [training: 0.06104082504885883 | validation: 0.04480208614492504]
	TIME [epoch: 9.03 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047224913783873654		[learning rate: 0.00063983]
		[batch 20/20] avg loss: 0.043922364677832046		[learning rate: 0.00063885]
	Learning Rate: 0.000638852
	LOSS [training: 0.04557363923085285 | validation: 0.05072139835428025]
	TIME [epoch: 9.05 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05112233326603487		[learning rate: 0.00063787]
		[batch 20/20] avg loss: 0.04218273179123762		[learning rate: 0.00063689]
	Learning Rate: 0.000636893
	LOSS [training: 0.04665253252863624 | validation: 0.05015855497318908]
	TIME [epoch: 9.06 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0630274674771806		[learning rate: 0.00063592]
		[batch 20/20] avg loss: 0.05234621077095834		[learning rate: 0.00063494]
	Learning Rate: 0.000634941
	LOSS [training: 0.05768683912406947 | validation: 0.028338027532128128]
	TIME [epoch: 9.04 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04468738366293874		[learning rate: 0.00063397]
		[batch 20/20] avg loss: 0.044821240239512734		[learning rate: 0.00063299]
	Learning Rate: 0.000632994
	LOSS [training: 0.044754311951225735 | validation: 0.037711084355573396]
	TIME [epoch: 9.04 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04176180707911795		[learning rate: 0.00063202]
		[batch 20/20] avg loss: 0.037921646676474643		[learning rate: 0.00063105]
	Learning Rate: 0.000631054
	LOSS [training: 0.0398417268777963 | validation: 0.04271223573015481]
	TIME [epoch: 9.04 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041227038891430035		[learning rate: 0.00063009]
		[batch 20/20] avg loss: 0.04297012378618537		[learning rate: 0.00062912]
	Learning Rate: 0.00062912
	LOSS [training: 0.0420985813388077 | validation: 0.031148790053703607]
	TIME [epoch: 9.06 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04427287464631456		[learning rate: 0.00062815]
		[batch 20/20] avg loss: 0.05595515548103656		[learning rate: 0.00062719]
	Learning Rate: 0.000627191
	LOSS [training: 0.050114015063675556 | validation: 0.05370757140694916]
	TIME [epoch: 9.03 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04956365327924755		[learning rate: 0.00062623]
		[batch 20/20] avg loss: 0.04497772311261593		[learning rate: 0.00062527]
	Learning Rate: 0.000625269
	LOSS [training: 0.047270688195931734 | validation: 0.05273055462911473]
	TIME [epoch: 9.03 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05280906143621089		[learning rate: 0.00062431]
		[batch 20/20] avg loss: 0.043236659461999484		[learning rate: 0.00062335]
	Learning Rate: 0.000623352
	LOSS [training: 0.048022860449105184 | validation: 0.045759223828385404]
	TIME [epoch: 9.03 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04791003427644293		[learning rate: 0.0006224]
		[batch 20/20] avg loss: 0.04999895937546663		[learning rate: 0.00062144]
	Learning Rate: 0.000621441
	LOSS [training: 0.048954496825954784 | validation: 0.033796556298177495]
	TIME [epoch: 9.04 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03782492581838358		[learning rate: 0.00062049]
		[batch 20/20] avg loss: 0.06210142964483516		[learning rate: 0.00061954]
	Learning Rate: 0.000619536
	LOSS [training: 0.049963177731609375 | validation: 0.041775046936483866]
	TIME [epoch: 9.03 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049932532497367205		[learning rate: 0.00061859]
		[batch 20/20] avg loss: 0.061513991445906205		[learning rate: 0.00061764]
	Learning Rate: 0.000617637
	LOSS [training: 0.0557232619716367 | validation: 0.03632638724070392]
	TIME [epoch: 9.05 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050667340196127966		[learning rate: 0.00061669]
		[batch 20/20] avg loss: 0.04129410918310478		[learning rate: 0.00061574]
	Learning Rate: 0.000615744
	LOSS [training: 0.045980724689616365 | validation: 0.03488206487726822]
	TIME [epoch: 9.02 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06247185717564485		[learning rate: 0.0006148]
		[batch 20/20] avg loss: 0.05694258110432913		[learning rate: 0.00061386]
	Learning Rate: 0.000613856
	LOSS [training: 0.059707219139987 | validation: 0.04769238438221636]
	TIME [epoch: 9.04 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04932561739615112		[learning rate: 0.00061291]
		[batch 20/20] avg loss: 0.061935311798236234		[learning rate: 0.00061197]
	Learning Rate: 0.000611974
	LOSS [training: 0.05563046459719369 | validation: 0.0263026166870215]
	TIME [epoch: 9.03 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04716735876403638		[learning rate: 0.00061104]
		[batch 20/20] avg loss: 0.04964468652039745		[learning rate: 0.0006101]
	Learning Rate: 0.000610099
	LOSS [training: 0.04840602264221692 | validation: 0.026215727172069343]
	TIME [epoch: 9.05 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06216010819697091		[learning rate: 0.00060916]
		[batch 20/20] avg loss: 0.043726518036273025		[learning rate: 0.00060823]
	Learning Rate: 0.000608228
	LOSS [training: 0.05294331311662197 | validation: 0.03135220889922267]
	TIME [epoch: 9.06 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04050408026623034		[learning rate: 0.0006073]
		[batch 20/20] avg loss: 0.06609755887754945		[learning rate: 0.00060636]
	Learning Rate: 0.000606364
	LOSS [training: 0.0533008195718899 | validation: 0.05795197840585167]
	TIME [epoch: 9.04 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0538901999446212		[learning rate: 0.00060543]
		[batch 20/20] avg loss: 0.04735041525225076		[learning rate: 0.00060451]
	Learning Rate: 0.000604505
	LOSS [training: 0.05062030759843598 | validation: 0.045065963513490785]
	TIME [epoch: 9.03 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05383582657444656		[learning rate: 0.00060358]
		[batch 20/20] avg loss: 0.04580480920561884		[learning rate: 0.00060265]
	Learning Rate: 0.000602652
	LOSS [training: 0.0498203178900327 | validation: 0.03802909603712484]
	TIME [epoch: 9.04 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05338941121714408		[learning rate: 0.00060173]
		[batch 20/20] avg loss: 0.05840422335048249		[learning rate: 0.0006008]
	Learning Rate: 0.000600805
	LOSS [training: 0.05589681728381328 | validation: 0.03198740133334979]
	TIME [epoch: 9.04 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049035180333301884		[learning rate: 0.00059988]
		[batch 20/20] avg loss: 0.061108821918110776		[learning rate: 0.00059896]
	Learning Rate: 0.000598963
	LOSS [training: 0.05507200112570633 | validation: 0.07093140013313945]
	TIME [epoch: 9.06 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05489862580983629		[learning rate: 0.00059804]
		[batch 20/20] avg loss: 0.04330213535455567		[learning rate: 0.00059713]
	Learning Rate: 0.000597127
	LOSS [training: 0.04910038058219598 | validation: 0.0351183042825164]
	TIME [epoch: 9.04 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0491243879237603		[learning rate: 0.00059621]
		[batch 20/20] avg loss: 0.06740647895957713		[learning rate: 0.0005953]
	Learning Rate: 0.000595296
	LOSS [training: 0.058265433441668714 | validation: 0.02827573919746458]
	TIME [epoch: 9.04 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048931490009851045		[learning rate: 0.00059438]
		[batch 20/20] avg loss: 0.04302368498097246		[learning rate: 0.00059347]
	Learning Rate: 0.000593472
	LOSS [training: 0.04597758749541176 | validation: 0.03136583472897883]
	TIME [epoch: 9.05 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037348661366432015		[learning rate: 0.00059256]
		[batch 20/20] avg loss: 0.04904657632383036		[learning rate: 0.00059165]
	Learning Rate: 0.000591652
	LOSS [training: 0.043197618845131194 | validation: 0.03072758171007453]
	TIME [epoch: 9.04 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04260418143085722		[learning rate: 0.00059074]
		[batch 20/20] avg loss: 0.04546319816609623		[learning rate: 0.00058984]
	Learning Rate: 0.000589839
	LOSS [training: 0.04403368979847673 | validation: 0.05312224128348978]
	TIME [epoch: 9.06 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06476818077398898		[learning rate: 0.00058893]
		[batch 20/20] avg loss: 0.047200240417172266		[learning rate: 0.00058803]
	Learning Rate: 0.000588031
	LOSS [training: 0.05598421059558063 | validation: 0.03519935076641148]
	TIME [epoch: 9.04 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0457160073173881		[learning rate: 0.00058713]
		[batch 20/20] avg loss: 0.054930929000546524		[learning rate: 0.00058623]
	Learning Rate: 0.000586228
	LOSS [training: 0.05032346815896731 | validation: 0.02642751087153333]
	TIME [epoch: 9.04 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05444298864962864		[learning rate: 0.00058533]
		[batch 20/20] avg loss: 0.042546557305258345		[learning rate: 0.00058443]
	Learning Rate: 0.000584431
	LOSS [training: 0.04849477297744349 | validation: 0.0311323975549877]
	TIME [epoch: 9.04 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05581908338541004		[learning rate: 0.00058353]
		[batch 20/20] avg loss: 0.04442907561190925		[learning rate: 0.00058264]
	Learning Rate: 0.000582639
	LOSS [training: 0.05012407949865964 | validation: 0.029261865918797043]
	TIME [epoch: 9.03 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04799574057112514		[learning rate: 0.00058175]
		[batch 20/20] avg loss: 0.04602041993843997		[learning rate: 0.00058085]
	Learning Rate: 0.000580854
	LOSS [training: 0.047008080254782564 | validation: 0.05609189961082413]
	TIME [epoch: 9.06 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03882466199704433		[learning rate: 0.00057996]
		[batch 20/20] avg loss: 0.03959052650408022		[learning rate: 0.00057907]
	Learning Rate: 0.000579073
	LOSS [training: 0.03920759425056229 | validation: 0.034852366298507034]
	TIME [epoch: 9.04 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04447288367524793		[learning rate: 0.00057818]
		[batch 20/20] avg loss: 0.05421097560539464		[learning rate: 0.0005773]
	Learning Rate: 0.000577298
	LOSS [training: 0.04934192964032129 | validation: 0.0632555039166496]
	TIME [epoch: 9.04 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051059872671649534		[learning rate: 0.00057641]
		[batch 20/20] avg loss: 0.053015051330404836		[learning rate: 0.00057553]
	Learning Rate: 0.000575528
	LOSS [training: 0.05203746200102718 | validation: 0.0420653702572747]
	TIME [epoch: 9.03 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06861360934066439		[learning rate: 0.00057465]
		[batch 20/20] avg loss: 0.03833502182258938		[learning rate: 0.00057376]
	Learning Rate: 0.000573764
	LOSS [training: 0.05347431558162689 | validation: 0.034205659281746514]
	TIME [epoch: 9.04 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061715526306163504		[learning rate: 0.00057288]
		[batch 20/20] avg loss: 0.050475763728534864		[learning rate: 0.00057201]
	Learning Rate: 0.000572005
	LOSS [training: 0.056095645017349184 | validation: 0.038542273526186555]
	TIME [epoch: 9.06 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04840203791607594		[learning rate: 0.00057113]
		[batch 20/20] avg loss: 0.04734484010471338		[learning rate: 0.00057025]
	Learning Rate: 0.000570252
	LOSS [training: 0.04787343901039466 | validation: 0.037837531012625325]
	TIME [epoch: 9.03 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04364017977271285		[learning rate: 0.00056938]
		[batch 20/20] avg loss: 0.07346156258400932		[learning rate: 0.0005685]
	Learning Rate: 0.000568504
	LOSS [training: 0.058550871178361094 | validation: 0.04018299795985708]
	TIME [epoch: 9.04 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046651060673399775		[learning rate: 0.00056763]
		[batch 20/20] avg loss: 0.060974722343442755		[learning rate: 0.00056676]
	Learning Rate: 0.000566761
	LOSS [training: 0.05381289150842126 | validation: 0.01995899589004588]
	TIME [epoch: 9.04 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043302467573362285		[learning rate: 0.00056589]
		[batch 20/20] avg loss: 0.04932943457774241		[learning rate: 0.00056502]
	Learning Rate: 0.000565024
	LOSS [training: 0.046315951075552336 | validation: 0.03368560403267265]
	TIME [epoch: 9.04 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0416640088046279		[learning rate: 0.00056416]
		[batch 20/20] avg loss: 0.03364972318304773		[learning rate: 0.00056329]
	Learning Rate: 0.000563292
	LOSS [training: 0.037656865993837815 | validation: 0.027859625258797018]
	TIME [epoch: 9.06 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05201250114804688		[learning rate: 0.00056243]
		[batch 20/20] avg loss: 0.04514889260206504		[learning rate: 0.00056156]
	Learning Rate: 0.000561565
	LOSS [training: 0.04858069687505596 | validation: 0.02977629117238902]
	TIME [epoch: 9.04 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04274442207425885		[learning rate: 0.0005607]
		[batch 20/20] avg loss: 0.04028421772160931		[learning rate: 0.00055984]
	Learning Rate: 0.000559844
	LOSS [training: 0.041514319897934075 | validation: 0.02548510481842709]
	TIME [epoch: 9.04 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05239982307903039		[learning rate: 0.00055898]
		[batch 20/20] avg loss: 0.03762343403030223		[learning rate: 0.00055813]
	Learning Rate: 0.000558127
	LOSS [training: 0.04501162855466632 | validation: 0.05275031425102607]
	TIME [epoch: 9.05 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041987438336106894		[learning rate: 0.00055727]
		[batch 20/20] avg loss: 0.04853917475337873		[learning rate: 0.00055642]
	Learning Rate: 0.000556416
	LOSS [training: 0.045263306544742817 | validation: 0.032588844516705066]
	TIME [epoch: 9.05 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050396258823081776		[learning rate: 0.00055556]
		[batch 20/20] avg loss: 0.04788020604423367		[learning rate: 0.00055471]
	Learning Rate: 0.000554711
	LOSS [training: 0.04913823243365771 | validation: 0.030411807679698297]
	TIME [epoch: 9.12 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039415166929483166		[learning rate: 0.00055386]
		[batch 20/20] avg loss: 0.04831545891699594		[learning rate: 0.00055301]
	Learning Rate: 0.00055301
	LOSS [training: 0.04386531292323956 | validation: 0.03586707672365734]
	TIME [epoch: 9.04 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0547433984432946		[learning rate: 0.00055216]
		[batch 20/20] avg loss: 0.054526494641674826		[learning rate: 0.00055132]
	Learning Rate: 0.000551315
	LOSS [training: 0.05463494654248472 | validation: 0.038879642428200044]
	TIME [epoch: 9.04 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04999477397728676		[learning rate: 0.00055047]
		[batch 20/20] avg loss: 0.052238459547366056		[learning rate: 0.00054963]
	Learning Rate: 0.000549625
	LOSS [training: 0.051116616762326414 | validation: 0.04669316969359699]
	TIME [epoch: 9.04 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04577475159900013		[learning rate: 0.00054878]
		[batch 20/20] avg loss: 0.0627314631505845		[learning rate: 0.00054794]
	Learning Rate: 0.00054794
	LOSS [training: 0.05425310737479231 | validation: 0.03563050325537154]
	TIME [epoch: 9.04 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03732185115957217		[learning rate: 0.0005471]
		[batch 20/20] avg loss: 0.05386370807820191		[learning rate: 0.00054626]
	Learning Rate: 0.000546261
	LOSS [training: 0.045592779618887046 | validation: 0.028749958929043917]
	TIME [epoch: 9.06 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0416537489939932		[learning rate: 0.00054542]
		[batch 20/20] avg loss: 0.06291680602197686		[learning rate: 0.00054459]
	Learning Rate: 0.000544586
	LOSS [training: 0.05228527750798503 | validation: 0.03716851567237402]
	TIME [epoch: 9.03 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053896012300197825		[learning rate: 0.00054375]
		[batch 20/20] avg loss: 0.053313574707197706		[learning rate: 0.00054292]
	Learning Rate: 0.000542917
	LOSS [training: 0.053604793503697755 | validation: 0.03717392986821264]
	TIME [epoch: 9.03 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03883806882172991		[learning rate: 0.00054208]
		[batch 20/20] avg loss: 0.04274464224401953		[learning rate: 0.00054125]
	Learning Rate: 0.000541253
	LOSS [training: 0.040791355532874725 | validation: 0.03392354929386]
	TIME [epoch: 9.04 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04613413945048718		[learning rate: 0.00054042]
		[batch 20/20] avg loss: 0.03550103250375196		[learning rate: 0.00053959]
	Learning Rate: 0.000539593
	LOSS [training: 0.040817585977119576 | validation: 0.03770326286962184]
	TIME [epoch: 9.04 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046637675289203526		[learning rate: 0.00053877]
		[batch 20/20] avg loss: 0.043221371517501925		[learning rate: 0.00053794]
	Learning Rate: 0.000537939
	LOSS [training: 0.044929523403352725 | validation: 0.03204392964872734]
	TIME [epoch: 9.06 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04055882949679044		[learning rate: 0.00053711]
		[batch 20/20] avg loss: 0.04641237494987986		[learning rate: 0.00053629]
	Learning Rate: 0.00053629
	LOSS [training: 0.043485602223335154 | validation: 0.0340770544492671]
	TIME [epoch: 9.03 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05028545221957621		[learning rate: 0.00053547]
		[batch 20/20] avg loss: 0.055662761107938975		[learning rate: 0.00053465]
	Learning Rate: 0.000534646
	LOSS [training: 0.05297410666375759 | validation: 0.03484962857771537]
	TIME [epoch: 9.04 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056612615720590495		[learning rate: 0.00053383]
		[batch 20/20] avg loss: 0.05723690228732352		[learning rate: 0.00053301]
	Learning Rate: 0.000533007
	LOSS [training: 0.05692475900395703 | validation: 0.03947276511549181]
	TIME [epoch: 9.04 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059040403001693276		[learning rate: 0.00053219]
		[batch 20/20] avg loss: 0.06265293316254425		[learning rate: 0.00053137]
	Learning Rate: 0.000531374
	LOSS [training: 0.06084666808211877 | validation: 0.042872699904536085]
	TIME [epoch: 9.04 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05072122228114166		[learning rate: 0.00053056]
		[batch 20/20] avg loss: 0.04079843949371052		[learning rate: 0.00052974]
	Learning Rate: 0.000529745
	LOSS [training: 0.04575983088742609 | validation: 0.0319900739971088]
	TIME [epoch: 9.06 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04855745656949409		[learning rate: 0.00052893]
		[batch 20/20] avg loss: 0.045571096219945526		[learning rate: 0.00052812]
	Learning Rate: 0.000528121
	LOSS [training: 0.04706427639471981 | validation: 0.03443043226861055]
	TIME [epoch: 9.04 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042599470474926196		[learning rate: 0.00052731]
		[batch 20/20] avg loss: 0.04643282303806257		[learning rate: 0.0005265]
	Learning Rate: 0.000526502
	LOSS [training: 0.044516146756494374 | validation: 0.04445682555522409]
	TIME [epoch: 9.04 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08949869421677398		[learning rate: 0.00052569]
		[batch 20/20] avg loss: 0.04565464084035122		[learning rate: 0.00052489]
	Learning Rate: 0.000524888
	LOSS [training: 0.06757666752856259 | validation: 0.027701341938388836]
	TIME [epoch: 9.04 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041230373308155356		[learning rate: 0.00052408]
		[batch 20/20] avg loss: 0.03955591498247242		[learning rate: 0.00052328]
	Learning Rate: 0.000523279
	LOSS [training: 0.04039314414531389 | validation: 0.06472639160425529]
	TIME [epoch: 9.04 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041964488524584335		[learning rate: 0.00052248]
		[batch 20/20] avg loss: 0.05651802801325386		[learning rate: 0.00052167]
	Learning Rate: 0.000521675
	LOSS [training: 0.049241258268919105 | validation: 0.030935549853955198]
	TIME [epoch: 9.05 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04030115097668939		[learning rate: 0.00052087]
		[batch 20/20] avg loss: 0.05707418515646273		[learning rate: 0.00052008]
	Learning Rate: 0.000520076
	LOSS [training: 0.04868766806657606 | validation: 0.035188747937876776]
	TIME [epoch: 9.04 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04603968018028591		[learning rate: 0.00051928]
		[batch 20/20] avg loss: 0.06462290665171364		[learning rate: 0.00051848]
	Learning Rate: 0.000518482
	LOSS [training: 0.05533129341599977 | validation: 0.03804073736171837]
	TIME [epoch: 9.03 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03654413512512223		[learning rate: 0.00051769]
		[batch 20/20] avg loss: 0.047827518935418076		[learning rate: 0.00051689]
	Learning Rate: 0.000516892
	LOSS [training: 0.04218582703027016 | validation: 0.03839013610287345]
	TIME [epoch: 9.04 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03955750756688996		[learning rate: 0.0005161]
		[batch 20/20] avg loss: 0.046806862178935184		[learning rate: 0.00051531]
	Learning Rate: 0.000515308
	LOSS [training: 0.04318218487291257 | validation: 0.04945189121138414]
	TIME [epoch: 9.04 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05261745963147091		[learning rate: 0.00051452]
		[batch 20/20] avg loss: 0.07497381903119646		[learning rate: 0.00051373]
	Learning Rate: 0.000513728
	LOSS [training: 0.06379563933133368 | validation: 0.050162417746713026]
	TIME [epoch: 9.06 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04246677361602215		[learning rate: 0.00051294]
		[batch 20/20] avg loss: 0.040616650726638455		[learning rate: 0.00051215]
	Learning Rate: 0.000512153
	LOSS [training: 0.041541712171330306 | validation: 0.02671296914059277]
	TIME [epoch: 9.05 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04641059448992134		[learning rate: 0.00051137]
		[batch 20/20] avg loss: 0.056210442008234385		[learning rate: 0.00051058]
	Learning Rate: 0.000510583
	LOSS [training: 0.05131051824907786 | validation: 0.04620031996858905]
	TIME [epoch: 9.04 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04548191658246017		[learning rate: 0.0005098]
		[batch 20/20] avg loss: 0.042201741996274834		[learning rate: 0.00050902]
	Learning Rate: 0.000509018
	LOSS [training: 0.0438418292893675 | validation: 0.027133480235649775]
	TIME [epoch: 9.04 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04963029307859053		[learning rate: 0.00050824]
		[batch 20/20] avg loss: 0.04450120291803961		[learning rate: 0.00050746]
	Learning Rate: 0.000507458
	LOSS [training: 0.04706574799831506 | validation: 0.03187901505951717]
	TIME [epoch: 9.04 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04445909587972572		[learning rate: 0.00050668]
		[batch 20/20] avg loss: 0.04224105892743161		[learning rate: 0.0005059]
	Learning Rate: 0.000505902
	LOSS [training: 0.04335007740357867 | validation: 0.047894372037946595]
	TIME [epoch: 9.05 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055544625246622584		[learning rate: 0.00050513]
		[batch 20/20] avg loss: 0.055318449665838		[learning rate: 0.00050435]
	Learning Rate: 0.000504351
	LOSS [training: 0.055431537456230286 | validation: 0.027591487852973606]
	TIME [epoch: 9.04 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04629013096179649		[learning rate: 0.00050358]
		[batch 20/20] avg loss: 0.0538315964424914		[learning rate: 0.00050281]
	Learning Rate: 0.000502806
	LOSS [training: 0.050060863702143944 | validation: 0.04201796045332516]
	TIME [epoch: 9.04 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04708332137623496		[learning rate: 0.00050203]
		[batch 20/20] avg loss: 0.047274487728946414		[learning rate: 0.00050126]
	Learning Rate: 0.000501264
	LOSS [training: 0.04717890455259069 | validation: 0.027582772363858726]
	TIME [epoch: 9.03 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06548491946244808		[learning rate: 0.0005005]
		[batch 20/20] avg loss: 0.03853087206971496		[learning rate: 0.00049973]
	Learning Rate: 0.000499728
	LOSS [training: 0.0520078957660815 | validation: 0.03016598997870385]
	TIME [epoch: 9.04 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052039709661613184		[learning rate: 0.00049896]
		[batch 20/20] avg loss: 0.04391549853901906		[learning rate: 0.0004982]
	Learning Rate: 0.000498196
	LOSS [training: 0.04797760410031613 | validation: 0.025216379743961882]
	TIME [epoch: 9.06 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03878771183117402		[learning rate: 0.00049743]
		[batch 20/20] avg loss: 0.04319687238386229		[learning rate: 0.00049667]
	Learning Rate: 0.000496668
	LOSS [training: 0.040992292107518154 | validation: 0.032496020073317665]
	TIME [epoch: 9.05 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04550351462356008		[learning rate: 0.00049591]
		[batch 20/20] avg loss: 0.03523500379776151		[learning rate: 0.00049515]
	Learning Rate: 0.000495146
	LOSS [training: 0.04036925921066079 | validation: 0.03604394215299659]
	TIME [epoch: 9.03 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048475213490675594		[learning rate: 0.00049439]
		[batch 20/20] avg loss: 0.0481133078766964		[learning rate: 0.00049363]
	Learning Rate: 0.000493628
	LOSS [training: 0.048294260683685995 | validation: 0.03367681266971833]
	TIME [epoch: 9.04 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04708083026179378		[learning rate: 0.00049287]
		[batch 20/20] avg loss: 0.05589353472990109		[learning rate: 0.00049211]
	Learning Rate: 0.000492115
	LOSS [training: 0.05148718249584744 | validation: 0.052437671422539224]
	TIME [epoch: 9.04 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057540421151714025		[learning rate: 0.00049136]
		[batch 20/20] avg loss: 0.049861821369972165		[learning rate: 0.00049061]
	Learning Rate: 0.000490606
	LOSS [training: 0.0537011212608431 | validation: 0.01943225074610906]
	TIME [epoch: 9.06 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04051844289060318		[learning rate: 0.00048985]
		[batch 20/20] avg loss: 0.03471656678742506		[learning rate: 0.0004891]
	Learning Rate: 0.000489103
	LOSS [training: 0.03761750483901412 | validation: 0.03316557284983131]
	TIME [epoch: 9.04 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041861782799998234		[learning rate: 0.00048835]
		[batch 20/20] avg loss: 0.05136112975210183		[learning rate: 0.0004876]
	Learning Rate: 0.000487603
	LOSS [training: 0.046611456276050026 | validation: 0.049180184250104814]
	TIME [epoch: 9.04 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06777194839317133		[learning rate: 0.00048686]
		[batch 20/20] avg loss: 0.04793869606613555		[learning rate: 0.00048611]
	Learning Rate: 0.000486109
	LOSS [training: 0.057855322229653436 | validation: 0.030806432918544527]
	TIME [epoch: 9.03 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042670096485975265		[learning rate: 0.00048536]
		[batch 20/20] avg loss: 0.04467981793857562		[learning rate: 0.00048462]
	Learning Rate: 0.000484619
	LOSS [training: 0.04367495721227544 | validation: 0.04236156885345825]
	TIME [epoch: 9.04 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04006712631436803		[learning rate: 0.00048388]
		[batch 20/20] avg loss: 0.06012437274052459		[learning rate: 0.00048313]
	Learning Rate: 0.000483133
	LOSS [training: 0.0500957495274463 | validation: 0.033490745775247216]
	TIME [epoch: 9.05 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04765000477632068		[learning rate: 0.00048239]
		[batch 20/20] avg loss: 0.042136523214480984		[learning rate: 0.00048165]
	Learning Rate: 0.000481652
	LOSS [training: 0.044893263995400834 | validation: 0.03932502362730951]
	TIME [epoch: 9.05 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046650070103561474		[learning rate: 0.00048091]
		[batch 20/20] avg loss: 0.05103605415451572		[learning rate: 0.00048018]
	Learning Rate: 0.000480175
	LOSS [training: 0.04884306212903859 | validation: 0.02064856531967422]
	TIME [epoch: 9.04 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04230310205268202		[learning rate: 0.00047944]
		[batch 20/20] avg loss: 0.04784360368030501		[learning rate: 0.0004787]
	Learning Rate: 0.000478704
	LOSS [training: 0.04507335286649351 | validation: 0.032246137313912994]
	TIME [epoch: 9.04 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04421672066841197		[learning rate: 0.00047797]
		[batch 20/20] avg loss: 0.03984897621245263		[learning rate: 0.00047724]
	Learning Rate: 0.000477236
	LOSS [training: 0.0420328484404323 | validation: 0.035170071810679854]
	TIME [epoch: 9.04 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040767767249920775		[learning rate: 0.0004765]
		[batch 20/20] avg loss: 0.042184805027559266		[learning rate: 0.00047577]
	Learning Rate: 0.000475773
	LOSS [training: 0.04147628613874001 | validation: 0.012835554107547546]
	TIME [epoch: 9.05 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240216_192554/states/model_tr_study1_1492.pth
	Model improved!!!
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046266855092150776		[learning rate: 0.00047504]
		[batch 20/20] avg loss: 0.03773083083385817		[learning rate: 0.00047431]
	Learning Rate: 0.000474315
	LOSS [training: 0.041998842963004476 | validation: 0.034354102501513994]
	TIME [epoch: 9.05 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044924717278375645		[learning rate: 0.00047359]
		[batch 20/20] avg loss: 0.03729484564929905		[learning rate: 0.00047286]
	Learning Rate: 0.000472861
	LOSS [training: 0.04110978146383735 | validation: 0.06261368187940776]
	TIME [epoch: 9.04 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056576162656871945		[learning rate: 0.00047214]
		[batch 20/20] avg loss: 0.036307254570772406		[learning rate: 0.00047141]
	Learning Rate: 0.000471411
	LOSS [training: 0.04644170861382217 | validation: 0.0552107936819306]
	TIME [epoch: 9.04 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04068672657145509		[learning rate: 0.00047069]
		[batch 20/20] avg loss: 0.054032122142837105		[learning rate: 0.00046997]
	Learning Rate: 0.000469966
	LOSS [training: 0.04735942435714609 | validation: 0.05222815113658147]
	TIME [epoch: 9.05 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0327295920420852		[learning rate: 0.00046925]
		[batch 20/20] avg loss: 0.04792685966581679		[learning rate: 0.00046853]
	Learning Rate: 0.000468526
	LOSS [training: 0.040328225853950996 | validation: 0.052255531135158774]
	TIME [epoch: 9.07 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05680633662600928		[learning rate: 0.00046781]
		[batch 20/20] avg loss: 0.03966486012084254		[learning rate: 0.00046709]
	Learning Rate: 0.000467089
	LOSS [training: 0.04823559837342591 | validation: 0.03593512120487314]
	TIME [epoch: 9.04 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04971126158204764		[learning rate: 0.00046637]
		[batch 20/20] avg loss: 0.03857099535995259		[learning rate: 0.00046566]
	Learning Rate: 0.000465658
	LOSS [training: 0.04414112847100011 | validation: 0.035045415593460055]
	TIME [epoch: 9.04 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051674286811635786		[learning rate: 0.00046494]
		[batch 20/20] avg loss: 0.05991886644691846		[learning rate: 0.00046423]
	Learning Rate: 0.00046423
	LOSS [training: 0.05579657662927713 | validation: 0.047939973437918955]
	TIME [epoch: 9.04 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04605505089926605		[learning rate: 0.00046352]
		[batch 20/20] avg loss: 0.04807257878590278		[learning rate: 0.00046281]
	Learning Rate: 0.000462807
	LOSS [training: 0.047063814842584414 | validation: 0.03250255691518918]
	TIME [epoch: 9.04 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04126368189462863		[learning rate: 0.0004621]
		[batch 20/20] avg loss: 0.04468738304135978		[learning rate: 0.00046139]
	Learning Rate: 0.000461388
	LOSS [training: 0.0429755324679942 | validation: 0.056677154725258125]
	TIME [epoch: 9.07 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05003042326315732		[learning rate: 0.00046068]
		[batch 20/20] avg loss: 0.037791349914204984		[learning rate: 0.00045997]
	Learning Rate: 0.000459974
	LOSS [training: 0.043910886588681156 | validation: 0.03194642182501208]
	TIME [epoch: 9.04 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04700464247098528		[learning rate: 0.00045927]
		[batch 20/20] avg loss: 0.04267910179231848		[learning rate: 0.00045856]
	Learning Rate: 0.000458564
	LOSS [training: 0.04484187213165189 | validation: 0.04209151724379985]
	TIME [epoch: 9.04 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04533429479242181		[learning rate: 0.00045786]
		[batch 20/20] avg loss: 0.040661209729991335		[learning rate: 0.00045716]
	Learning Rate: 0.000457158
	LOSS [training: 0.04299775226120657 | validation: 0.033299061641691774]
	TIME [epoch: 9.04 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04252265595598565		[learning rate: 0.00045646]
		[batch 20/20] avg loss: 0.06094706843540674		[learning rate: 0.00045576]
	Learning Rate: 0.000455757
	LOSS [training: 0.051734862195696195 | validation: 0.07427317663885716]
	TIME [epoch: 9.04 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045371947468092554		[learning rate: 0.00045506]
		[batch 20/20] avg loss: 0.053313131373760536		[learning rate: 0.00045436]
	Learning Rate: 0.00045436
	LOSS [training: 0.04934253942092654 | validation: 0.02134439576203659]
	TIME [epoch: 9.06 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03891797706372545		[learning rate: 0.00045366]
		[batch 20/20] avg loss: 0.043153683139698085		[learning rate: 0.00045297]
	Learning Rate: 0.000452967
	LOSS [training: 0.041035830101711766 | validation: 0.039925408185233074]
	TIME [epoch: 9.04 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032661414592299425		[learning rate: 0.00045227]
		[batch 20/20] avg loss: 0.04263189423494059		[learning rate: 0.00045158]
	Learning Rate: 0.000451579
	LOSS [training: 0.03764665441362001 | validation: 0.029741595284007834]
	TIME [epoch: 9.04 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043275135382202325		[learning rate: 0.00045089]
		[batch 20/20] avg loss: 0.0398061320501439		[learning rate: 0.00045019]
	Learning Rate: 0.000450194
	LOSS [training: 0.041540633716173114 | validation: 0.038158294592580876]
	TIME [epoch: 9.04 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04422494632714982		[learning rate: 0.0004495]
		[batch 20/20] avg loss: 0.041345451800867114		[learning rate: 0.00044881]
	Learning Rate: 0.000448814
	LOSS [training: 0.042785199064008475 | validation: 0.03702521950658715]
	TIME [epoch: 10.2 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04008175766815208		[learning rate: 0.00044813]
		[batch 20/20] avg loss: 0.04128466072405529		[learning rate: 0.00044744]
	Learning Rate: 0.000447439
	LOSS [training: 0.04068320919610369 | validation: 0.03112215665284898]
	TIME [epoch: 9.06 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04431245517793249		[learning rate: 0.00044675]
		[batch 20/20] avg loss: 0.032219378045794264		[learning rate: 0.00044607]
	Learning Rate: 0.000446067
	LOSS [training: 0.038265916611863376 | validation: 0.03366451227725979]
	TIME [epoch: 9.04 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04879415251822039		[learning rate: 0.00044538]
		[batch 20/20] avg loss: 0.03597302979524018		[learning rate: 0.0004447]
	Learning Rate: 0.0004447
	LOSS [training: 0.04238359115673029 | validation: 0.0432411651123893]
	TIME [epoch: 9.04 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04273246893156264		[learning rate: 0.00044402]
		[batch 20/20] avg loss: 0.03485377520030637		[learning rate: 0.00044334]
	Learning Rate: 0.000443336
	LOSS [training: 0.03879312206593451 | validation: 0.041336910601812826]
	TIME [epoch: 9.05 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053851848213184915		[learning rate: 0.00044266]
		[batch 20/20] avg loss: 0.042506771319302326		[learning rate: 0.00044198]
	Learning Rate: 0.000441977
	LOSS [training: 0.04817930976624362 | validation: 0.04263327900304787]
	TIME [epoch: 9.04 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04339980741275052		[learning rate: 0.0004413]
		[batch 20/20] avg loss: 0.04677510480237357		[learning rate: 0.00044062]
	Learning Rate: 0.000440622
	LOSS [training: 0.04508745610756204 | validation: 0.023059783045466947]
	TIME [epoch: 9.07 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04011999663025722		[learning rate: 0.00043995]
		[batch 20/20] avg loss: 0.04101259501705814		[learning rate: 0.00043927]
	Learning Rate: 0.000439272
	LOSS [training: 0.040566295823657665 | validation: 0.028025418763942007]
	TIME [epoch: 9.05 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03950438556985878		[learning rate: 0.0004386]
		[batch 20/20] avg loss: 0.040378460097425614		[learning rate: 0.00043793]
	Learning Rate: 0.000437925
	LOSS [training: 0.0399414228336422 | validation: 0.02226951277023121]
	TIME [epoch: 9.05 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030892956747412898		[learning rate: 0.00043725]
		[batch 20/20] avg loss: 0.06501759721450336		[learning rate: 0.00043658]
	Learning Rate: 0.000436583
	LOSS [training: 0.047955276980958125 | validation: 0.023448025495041377]
	TIME [epoch: 9.05 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05012302820469514		[learning rate: 0.00043591]
		[batch 20/20] avg loss: 0.036356516133432014		[learning rate: 0.00043524]
	Learning Rate: 0.000435244
	LOSS [training: 0.04323977216906358 | validation: 0.030306914902511035]
	TIME [epoch: 9.05 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04529482400712137		[learning rate: 0.00043458]
		[batch 20/20] avg loss: 0.054903707268904776		[learning rate: 0.00043391]
	Learning Rate: 0.00043391
	LOSS [training: 0.05009926563801307 | validation: 0.02940437014157388]
	TIME [epoch: 9.06 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04376700531401866		[learning rate: 0.00043324]
		[batch 20/20] avg loss: 0.03511441103420636		[learning rate: 0.00043258]
	Learning Rate: 0.00043258
	LOSS [training: 0.039440708174112514 | validation: 0.02940336194751391]
	TIME [epoch: 9.05 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05148560433673362		[learning rate: 0.00043192]
		[batch 20/20] avg loss: 0.04578705852808324		[learning rate: 0.00043125]
	Learning Rate: 0.000431254
	LOSS [training: 0.04863633143240842 | validation: 0.028851369809014947]
	TIME [epoch: 9.04 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0476315295812946		[learning rate: 0.00043059]
		[batch 20/20] avg loss: 0.04247653382011988		[learning rate: 0.00042993]
	Learning Rate: 0.000429932
	LOSS [training: 0.045054031700707244 | validation: 0.02586496459935681]
	TIME [epoch: 9.05 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04367992697467782		[learning rate: 0.00042927]
		[batch 20/20] avg loss: 0.03549986107330175		[learning rate: 0.00042861]
	Learning Rate: 0.000428614
	LOSS [training: 0.03958989402398978 | validation: 0.0276570108405713]
	TIME [epoch: 9.04 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041322063607741974		[learning rate: 0.00042796]
		[batch 20/20] avg loss: 0.047356355098790395		[learning rate: 0.0004273]
	Learning Rate: 0.0004273
	LOSS [training: 0.044339209353266185 | validation: 0.03748536173738612]
	TIME [epoch: 9.07 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04361959242903193		[learning rate: 0.00042664]
		[batch 20/20] avg loss: 0.04211519105560012		[learning rate: 0.00042599]
	Learning Rate: 0.000425991
	LOSS [training: 0.04286739174231603 | validation: 0.04199839723179541]
	TIME [epoch: 9.05 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04196777523400163		[learning rate: 0.00042534]
		[batch 20/20] avg loss: 0.05151052870142915		[learning rate: 0.00042468]
	Learning Rate: 0.000424685
	LOSS [training: 0.0467391519677154 | validation: 0.03980107673689559]
	TIME [epoch: 9.05 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03569462117813145		[learning rate: 0.00042403]
		[batch 20/20] avg loss: 0.048436331465808155		[learning rate: 0.00042338]
	Learning Rate: 0.000423383
	LOSS [training: 0.042065476321969796 | validation: 0.028057867361639746]
	TIME [epoch: 9.05 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03545130362162186		[learning rate: 0.00042273]
		[batch 20/20] avg loss: 0.0459567985485123		[learning rate: 0.00042208]
	Learning Rate: 0.000422085
	LOSS [training: 0.04070405108506707 | validation: 0.030736747710006326]
	TIME [epoch: 9.05 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041597422259510186		[learning rate: 0.00042144]
		[batch 20/20] avg loss: 0.04011063831698336		[learning rate: 0.00042079]
	Learning Rate: 0.000420791
	LOSS [training: 0.04085403028824678 | validation: 0.04095357166755094]
	TIME [epoch: 9.06 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05137003485655937		[learning rate: 0.00042015]
		[batch 20/20] avg loss: 0.04911982843881825		[learning rate: 0.0004195]
	Learning Rate: 0.000419501
	LOSS [training: 0.05024493164768882 | validation: 0.03243259620176278]
	TIME [epoch: 9.04 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03598807168837912		[learning rate: 0.00041886]
		[batch 20/20] avg loss: 0.042624814096573246		[learning rate: 0.00041822]
	Learning Rate: 0.000418215
	LOSS [training: 0.03930644289247617 | validation: 0.02300792300828811]
	TIME [epoch: 9.05 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03921213509633077		[learning rate: 0.00041757]
		[batch 20/20] avg loss: 0.040923189314026026		[learning rate: 0.00041693]
	Learning Rate: 0.000416933
	LOSS [training: 0.0400676622051784 | validation: 0.02470861708353305]
	TIME [epoch: 9.05 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033621403469850535		[learning rate: 0.00041629]
		[batch 20/20] avg loss: 0.04315652802881059		[learning rate: 0.00041566]
	Learning Rate: 0.000415655
	LOSS [training: 0.03838896574933055 | validation: 0.02475928223158671]
	TIME [epoch: 9.04 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05199031143704101		[learning rate: 0.00041502]
		[batch 20/20] avg loss: 0.049398292603456906		[learning rate: 0.00041438]
	Learning Rate: 0.000414381
	LOSS [training: 0.05069430202024895 | validation: 0.024487429568790617]
	TIME [epoch: 9.06 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041107228514338935		[learning rate: 0.00041375]
		[batch 20/20] avg loss: 0.04775374624571262		[learning rate: 0.00041311]
	Learning Rate: 0.000413111
	LOSS [training: 0.04443048738002578 | validation: 0.027637020135381343]
	TIME [epoch: 9.04 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03942007425103451		[learning rate: 0.00041248]
		[batch 20/20] avg loss: 0.04846091251265894		[learning rate: 0.00041184]
	Learning Rate: 0.000411845
	LOSS [training: 0.04394049338184673 | validation: 0.03625620739721558]
	TIME [epoch: 9.04 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038947986662020304		[learning rate: 0.00041121]
		[batch 20/20] avg loss: 0.03179393240604326		[learning rate: 0.00041058]
	Learning Rate: 0.000410582
	LOSS [training: 0.035370959534031784 | validation: 0.028978679828265616]
	TIME [epoch: 9.05 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039931028083934825		[learning rate: 0.00040995]
		[batch 20/20] avg loss: 0.047312767912628846		[learning rate: 0.00040932]
	Learning Rate: 0.000409323
	LOSS [training: 0.04362189799828183 | validation: 0.028218718390673303]
	TIME [epoch: 9.04 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04428744139138937		[learning rate: 0.0004087]
		[batch 20/20] avg loss: 0.039694897874165566		[learning rate: 0.00040807]
	Learning Rate: 0.000408069
	LOSS [training: 0.04199116963277747 | validation: 0.039523174699961076]
	TIME [epoch: 9.07 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07362967202117612		[learning rate: 0.00040744]
		[batch 20/20] avg loss: 0.05375792832810411		[learning rate: 0.00040682]
	Learning Rate: 0.000406818
	LOSS [training: 0.0636938001746401 | validation: 0.04034820665064502]
	TIME [epoch: 9.04 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038012700100917764		[learning rate: 0.00040619]
		[batch 20/20] avg loss: 0.049090643439742856		[learning rate: 0.00040557]
	Learning Rate: 0.000405571
	LOSS [training: 0.0435516717703303 | validation: 0.03314107477858623]
	TIME [epoch: 9.05 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039336935787675754		[learning rate: 0.00040495]
		[batch 20/20] avg loss: 0.05351977280853228		[learning rate: 0.00040433]
	Learning Rate: 0.000404328
	LOSS [training: 0.04642835429810401 | validation: 0.03085687577131401]
	TIME [epoch: 9.04 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04701041303668509		[learning rate: 0.00040371]
		[batch 20/20] avg loss: 0.040066572822297745		[learning rate: 0.00040309]
	Learning Rate: 0.000403088
	LOSS [training: 0.04353849292949142 | validation: 0.016799824141755473]
	TIME [epoch: 9.04 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039323370150490124		[learning rate: 0.00040247]
		[batch 20/20] avg loss: 0.038209968169900986		[learning rate: 0.00040185]
	Learning Rate: 0.000401852
	LOSS [training: 0.038766669160195555 | validation: 0.03220748334634097]
	TIME [epoch: 9.06 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036526110904005135		[learning rate: 0.00040124]
		[batch 20/20] avg loss: 0.04020270623131605		[learning rate: 0.00040062]
	Learning Rate: 0.000400621
	LOSS [training: 0.03836440856766059 | validation: 0.02674712958178411]
	TIME [epoch: 9.05 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04615579590988939		[learning rate: 0.00040001]
		[batch 20/20] avg loss: 0.043265430493524035		[learning rate: 0.00039939]
	Learning Rate: 0.000399393
	LOSS [training: 0.04471061320170672 | validation: 0.03492560536826189]
	TIME [epoch: 9.04 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044677050971530775		[learning rate: 0.00039878]
		[batch 20/20] avg loss: 0.04230819251936524		[learning rate: 0.00039817]
	Learning Rate: 0.000398168
	LOSS [training: 0.043492621745448004 | validation: 0.028882843492249153]
	TIME [epoch: 9.05 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04359921580157191		[learning rate: 0.00039756]
		[batch 20/20] avg loss: 0.043506482109563205		[learning rate: 0.00039695]
	Learning Rate: 0.000396948
	LOSS [training: 0.04355284895556756 | validation: 0.03054028709994161]
	TIME [epoch: 9.04 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043692004261084175		[learning rate: 0.00039634]
		[batch 20/20] avg loss: 0.04678773263281539		[learning rate: 0.00039573]
	Learning Rate: 0.000395731
	LOSS [training: 0.045239868446949785 | validation: 0.045316146150826436]
	TIME [epoch: 9.08 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048939950111996346		[learning rate: 0.00039512]
		[batch 20/20] avg loss: 0.03896245466522066		[learning rate: 0.00039452]
	Learning Rate: 0.000394518
	LOSS [training: 0.043951202388608494 | validation: 0.03019679205056104]
	TIME [epoch: 9.05 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053387444445272283		[learning rate: 0.00039391]
		[batch 20/20] avg loss: 0.04382709002065158		[learning rate: 0.00039331]
	Learning Rate: 0.000393308
	LOSS [training: 0.04860726723296193 | validation: 0.022784167379339842]
	TIME [epoch: 9.05 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04223813414751271		[learning rate: 0.00039271]
		[batch 20/20] avg loss: 0.04050576230525456		[learning rate: 0.0003921]
	Learning Rate: 0.000392103
	LOSS [training: 0.041371948226383626 | validation: 0.037731719008560566]
	TIME [epoch: 9.05 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044304570078309076		[learning rate: 0.0003915]
		[batch 20/20] avg loss: 0.040335054148595256		[learning rate: 0.0003909]
	Learning Rate: 0.000390901
	LOSS [training: 0.04231981211345217 | validation: 0.024257869285610262]
	TIME [epoch: 9.05 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04069498148695663		[learning rate: 0.0003903]
		[batch 20/20] avg loss: 0.052074481560158425		[learning rate: 0.0003897]
	Learning Rate: 0.000389703
	LOSS [training: 0.04638473152355753 | validation: 0.02933157842074458]
	TIME [epoch: 9.07 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035546324540712534		[learning rate: 0.0003891]
		[batch 20/20] avg loss: 0.0428655364012835		[learning rate: 0.00038851]
	Learning Rate: 0.000388508
	LOSS [training: 0.03920593047099801 | validation: 0.04727243404124404]
	TIME [epoch: 9.05 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04299802662279722		[learning rate: 0.00038791]
		[batch 20/20] avg loss: 0.045113726231156094		[learning rate: 0.00038732]
	Learning Rate: 0.000387317
	LOSS [training: 0.04405587642697665 | validation: 0.016666046888392974]
	TIME [epoch: 9.05 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03995547883400807		[learning rate: 0.00038672]
		[batch 20/20] avg loss: 0.03650899432570228		[learning rate: 0.00038613]
	Learning Rate: 0.00038613
	LOSS [training: 0.03823223657985518 | validation: 0.04162579420635107]
	TIME [epoch: 9.05 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038523295297616014		[learning rate: 0.00038554]
		[batch 20/20] avg loss: 0.03907180326801972		[learning rate: 0.00038495]
	Learning Rate: 0.000384946
	LOSS [training: 0.038797549282817874 | validation: 0.03057737523346865]
	TIME [epoch: 9.05 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037261296155550974		[learning rate: 0.00038436]
		[batch 20/20] avg loss: 0.04427126357289392		[learning rate: 0.00038377]
	Learning Rate: 0.000383766
	LOSS [training: 0.040766279864222446 | validation: 0.025120719364338597]
	TIME [epoch: 9.07 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045188563717669494		[learning rate: 0.00038318]
		[batch 20/20] avg loss: 0.044002032224795276		[learning rate: 0.00038259]
	Learning Rate: 0.00038259
	LOSS [training: 0.044595297971232385 | validation: 0.027172596157777385]
	TIME [epoch: 9.06 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04255585017534032		[learning rate: 0.000382]
		[batch 20/20] avg loss: 0.03880729579344414		[learning rate: 0.00038142]
	Learning Rate: 0.000381417
	LOSS [training: 0.040681572984392234 | validation: 0.03578409892566891]
	TIME [epoch: 9.05 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045561110261912514		[learning rate: 0.00038083]
		[batch 20/20] avg loss: 0.043210105832423615		[learning rate: 0.00038025]
	Learning Rate: 0.000380248
	LOSS [training: 0.04438560804716806 | validation: 0.0228945645415313]
	TIME [epoch: 9.05 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03732883410453684		[learning rate: 0.00037966]
		[batch 20/20] avg loss: 0.03296623052551942		[learning rate: 0.00037908]
	Learning Rate: 0.000379082
	LOSS [training: 0.035147532315028124 | validation: 0.02842784949784288]
	TIME [epoch: 9.04 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040087927999550785		[learning rate: 0.0003785]
		[batch 20/20] avg loss: 0.04401757391812394		[learning rate: 0.00037792]
	Learning Rate: 0.00037792
	LOSS [training: 0.04205275095883736 | validation: 0.028518706448321815]
	TIME [epoch: 9.08 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04139870430529967		[learning rate: 0.00037734]
		[batch 20/20] avg loss: 0.03769006384429904		[learning rate: 0.00037676]
	Learning Rate: 0.000376762
	LOSS [training: 0.039544384074799356 | validation: 0.062338698511856847]
	TIME [epoch: 9.05 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0647697613861061		[learning rate: 0.00037618]
		[batch 20/20] avg loss: 0.041471054882609354		[learning rate: 0.00037561]
	Learning Rate: 0.000375607
	LOSS [training: 0.053120408134357734 | validation: 0.05014344187512278]
	TIME [epoch: 9.04 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0548803684374088		[learning rate: 0.00037503]
		[batch 20/20] avg loss: 0.03557493428032906		[learning rate: 0.00037446]
	Learning Rate: 0.000374455
	LOSS [training: 0.045227651358868935 | validation: 0.03064529254475079]
	TIME [epoch: 9.05 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03663846228566896		[learning rate: 0.00037388]
		[batch 20/20] avg loss: 0.03931869360614847		[learning rate: 0.00037331]
	Learning Rate: 0.000373307
	LOSS [training: 0.03797857794590871 | validation: 0.043846248692321614]
	TIME [epoch: 9.05 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04398278260479142		[learning rate: 0.00037273]
		[batch 20/20] avg loss: 0.039518408731360644		[learning rate: 0.00037216]
	Learning Rate: 0.000372163
	LOSS [training: 0.04175059566807603 | validation: 0.037242061789439784]
	TIME [epoch: 9.07 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036858969353988844		[learning rate: 0.00037159]
		[batch 20/20] avg loss: 0.0459488844031629		[learning rate: 0.00037102]
	Learning Rate: 0.000371022
	LOSS [training: 0.04140392687857587 | validation: 0.047978895513709416]
	TIME [epoch: 9.05 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04255614444973951		[learning rate: 0.00037045]
		[batch 20/20] avg loss: 0.03622372997008172		[learning rate: 0.00036988]
	Learning Rate: 0.000369885
	LOSS [training: 0.03938993720991061 | validation: 0.03558653458784533]
	TIME [epoch: 9.05 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038991682904537875		[learning rate: 0.00036932]
		[batch 20/20] avg loss: 0.03977488998546481		[learning rate: 0.00036875]
	Learning Rate: 0.000368751
	LOSS [training: 0.039383286445001345 | validation: 0.028590518792629395]
	TIME [epoch: 9.05 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03825362286672106		[learning rate: 0.00036819]
		[batch 20/20] avg loss: 0.03757191418649962		[learning rate: 0.00036762]
	Learning Rate: 0.000367621
	LOSS [training: 0.03791276852661034 | validation: 0.042264785287759024]
	TIME [epoch: 9.05 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047398691979643345		[learning rate: 0.00036706]
		[batch 20/20] avg loss: 0.03307813738092165		[learning rate: 0.00036649]
	Learning Rate: 0.000366494
	LOSS [training: 0.04023841468028251 | validation: 0.030342266749363352]
	TIME [epoch: 9.07 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04137733949816625		[learning rate: 0.00036593]
		[batch 20/20] avg loss: 0.0403678986704946		[learning rate: 0.00036537]
	Learning Rate: 0.00036537
	LOSS [training: 0.04087261908433042 | validation: 0.03919312528761082]
	TIME [epoch: 9.05 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05096644685576738		[learning rate: 0.00036481]
		[batch 20/20] avg loss: 0.03727946175445018		[learning rate: 0.00036425]
	Learning Rate: 0.00036425
	LOSS [training: 0.044122954305108784 | validation: 0.03002485110673066]
	TIME [epoch: 9.04 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03526304025067471		[learning rate: 0.00036369]
		[batch 20/20] avg loss: 0.04531922011605367		[learning rate: 0.00036313]
	Learning Rate: 0.000363134
	LOSS [training: 0.0402911301833642 | validation: 0.03176527908249873]
	TIME [epoch: 9.05 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04096870965626985		[learning rate: 0.00036258]
		[batch 20/20] avg loss: 0.031278197573257054		[learning rate: 0.00036202]
	Learning Rate: 0.000362021
	LOSS [training: 0.03612345361476345 | validation: 0.03161052142199045]
	TIME [epoch: 9.04 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040720881906079705		[learning rate: 0.00036147]
		[batch 20/20] avg loss: 0.035541152285420166		[learning rate: 0.00036091]
	Learning Rate: 0.000360911
	LOSS [training: 0.03813101709574994 | validation: 0.02549652772443865]
	TIME [epoch: 9.07 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035040162288496414		[learning rate: 0.00036036]
		[batch 20/20] avg loss: 0.04656140782324674		[learning rate: 0.0003598]
	Learning Rate: 0.000359805
	LOSS [training: 0.040800785055871584 | validation: 0.0335552511446866]
	TIME [epoch: 9.05 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04164977876448763		[learning rate: 0.00035925]
		[batch 20/20] avg loss: 0.0431909015269157		[learning rate: 0.0003587]
	Learning Rate: 0.000358702
	LOSS [training: 0.042420340145701674 | validation: 0.04357089349306313]
	TIME [epoch: 9.04 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03447716396003303		[learning rate: 0.00035815]
		[batch 20/20] avg loss: 0.04460025064422547		[learning rate: 0.0003576]
	Learning Rate: 0.000357602
	LOSS [training: 0.03953870730212926 | validation: 0.04613462563882901]
	TIME [epoch: 9.05 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05496935377399522		[learning rate: 0.00035705]
		[batch 20/20] avg loss: 0.03799732732844378		[learning rate: 0.00035651]
	Learning Rate: 0.000356506
	LOSS [training: 0.0464833405512195 | validation: 0.03174348577278614]
	TIME [epoch: 9.05 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04201878669314772		[learning rate: 0.00035596]
		[batch 20/20] avg loss: 0.044536024797177706		[learning rate: 0.00035541]
	Learning Rate: 0.000355413
	LOSS [training: 0.04327740574516271 | validation: 0.03278438853193858]
	TIME [epoch: 9.07 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05133873815923704		[learning rate: 0.00035487]
		[batch 20/20] avg loss: 0.0537822746754945		[learning rate: 0.00035432]
	Learning Rate: 0.000354323
	LOSS [training: 0.05256050641736576 | validation: 0.037533687903733404]
	TIME [epoch: 9.05 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03802887858022445		[learning rate: 0.00035378]
		[batch 20/20] avg loss: 0.03899871787324915		[learning rate: 0.00035324]
	Learning Rate: 0.000353237
	LOSS [training: 0.0385137982267368 | validation: 0.03883277833774835]
	TIME [epoch: 9.05 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039467422018658826		[learning rate: 0.0003527]
		[batch 20/20] avg loss: 0.04357661318833431		[learning rate: 0.00035215]
	Learning Rate: 0.000352155
	LOSS [training: 0.04152201760349656 | validation: 0.04066305154405584]
	TIME [epoch: 9.05 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03969738198483182		[learning rate: 0.00035161]
		[batch 20/20] avg loss: 0.04059736669175354		[learning rate: 0.00035108]
	Learning Rate: 0.000351075
	LOSS [training: 0.04014737433829268 | validation: 0.038319184982399855]
	TIME [epoch: 9.04 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0506819517770703		[learning rate: 0.00035054]
		[batch 20/20] avg loss: 0.05219261398186416		[learning rate: 0.00035]
	Learning Rate: 0.000349999
	LOSS [training: 0.05143728287946723 | validation: 0.027243301273221936]
	TIME [epoch: 9.06 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027676627622743942		[learning rate: 0.00034946]
		[batch 20/20] avg loss: 0.04819385705816059		[learning rate: 0.00034893]
	Learning Rate: 0.000348926
	LOSS [training: 0.03793524234045227 | validation: 0.037488806657925616]
	TIME [epoch: 9.05 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039411645153646334		[learning rate: 0.00034839]
		[batch 20/20] avg loss: 0.03701594664864516		[learning rate: 0.00034786]
	Learning Rate: 0.000347856
	LOSS [training: 0.03821379590114575 | validation: 0.04757200156009735]
	TIME [epoch: 9.04 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044309115869468986		[learning rate: 0.00034732]
		[batch 20/20] avg loss: 0.0436196868723141		[learning rate: 0.00034679]
	Learning Rate: 0.00034679
	LOSS [training: 0.04396440137089154 | validation: 0.03299914549408991]
	TIME [epoch: 9.04 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03806999769532392		[learning rate: 0.00034626]
		[batch 20/20] avg loss: 0.04655210429736113		[learning rate: 0.00034573]
	Learning Rate: 0.000345727
	LOSS [training: 0.04231105099634253 | validation: 0.05880923882073118]
	TIME [epoch: 9.05 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05660196463326782		[learning rate: 0.0003452]
		[batch 20/20] avg loss: 0.04673924615386592		[learning rate: 0.00034467]
	Learning Rate: 0.000344667
	LOSS [training: 0.05167060539356687 | validation: 0.04033149453608298]
	TIME [epoch: 9.06 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039375287286645626		[learning rate: 0.00034414]
		[batch 20/20] avg loss: 0.05114501225786329		[learning rate: 0.00034361]
	Learning Rate: 0.000343611
	LOSS [training: 0.045260149772254454 | validation: 0.038220301568299836]
	TIME [epoch: 9.05 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033102076675943636		[learning rate: 0.00034308]
		[batch 20/20] avg loss: 0.051780968890163204		[learning rate: 0.00034256]
	Learning Rate: 0.000342557
	LOSS [training: 0.04244152278305342 | validation: 0.024907517024136495]
	TIME [epoch: 9.04 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036575145581730376		[learning rate: 0.00034203]
		[batch 20/20] avg loss: 0.04025215111796262		[learning rate: 0.00034151]
	Learning Rate: 0.000341507
	LOSS [training: 0.0384136483498465 | validation: 0.026293674229658316]
	TIME [epoch: 9.04 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03952640385773922		[learning rate: 0.00034098]
		[batch 20/20] avg loss: 0.030550311816109788		[learning rate: 0.00034046]
	Learning Rate: 0.00034046
	LOSS [training: 0.0350383578369245 | validation: 0.034040504780263985]
	TIME [epoch: 9.05 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04692243745894547		[learning rate: 0.00033994]
		[batch 20/20] avg loss: 0.05836157500204273		[learning rate: 0.00033942]
	Learning Rate: 0.000339417
	LOSS [training: 0.0526420062304941 | validation: 0.04344569305492846]
	TIME [epoch: 9.05 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043017492116311416		[learning rate: 0.0003389]
		[batch 20/20] avg loss: 0.041699485777538646		[learning rate: 0.00033838]
	Learning Rate: 0.000338376
	LOSS [training: 0.04235848894692504 | validation: 0.036774492958285596]
	TIME [epoch: 9.05 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04613838192399765		[learning rate: 0.00033786]
		[batch 20/20] avg loss: 0.03729884114717014		[learning rate: 0.00033734]
	Learning Rate: 0.000337339
	LOSS [training: 0.04171861153558389 | validation: 0.030647200508392758]
	TIME [epoch: 9.04 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03894249186135089		[learning rate: 0.00033682]
		[batch 20/20] avg loss: 0.03610143784582085		[learning rate: 0.0003363]
	Learning Rate: 0.000336305
	LOSS [training: 0.03752196485358587 | validation: 0.020769347848130282]
	TIME [epoch: 9.05 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03678054938461114		[learning rate: 0.00033579]
		[batch 20/20] avg loss: 0.04105306844786931		[learning rate: 0.00033527]
	Learning Rate: 0.000335274
	LOSS [training: 0.03891680891624022 | validation: 0.01812440137341955]
	TIME [epoch: 9.04 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034194306059847884		[learning rate: 0.00033476]
		[batch 20/20] avg loss: 0.04175301935996142		[learning rate: 0.00033425]
	Learning Rate: 0.000334246
	LOSS [training: 0.03797366270990465 | validation: 0.04377281258681462]
	TIME [epoch: 9.06 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04092704056992884		[learning rate: 0.00033373]
		[batch 20/20] avg loss: 0.04191228900529703		[learning rate: 0.00033322]
	Learning Rate: 0.000333222
	LOSS [training: 0.04141966478761293 | validation: 0.018687707208000236]
	TIME [epoch: 9.05 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04075035431606069		[learning rate: 0.00033271]
		[batch 20/20] avg loss: 0.04252380773774599		[learning rate: 0.0003322]
	Learning Rate: 0.0003322
	LOSS [training: 0.04163708102690335 | validation: 0.033438591243337756]
	TIME [epoch: 9.04 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04679337454515764		[learning rate: 0.00033169]
		[batch 20/20] avg loss: 0.03800534498794494		[learning rate: 0.00033118]
	Learning Rate: 0.000331182
	LOSS [training: 0.042399359766551285 | validation: 0.033077883141337985]
	TIME [epoch: 9.04 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04385275282465019		[learning rate: 0.00033067]
		[batch 20/20] avg loss: 0.0384997552116957		[learning rate: 0.00033017]
	Learning Rate: 0.000330167
	LOSS [training: 0.041176254018172945 | validation: 0.02791897857468303]
	TIME [epoch: 9.04 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03824668010582105		[learning rate: 0.00032966]
		[batch 20/20] avg loss: 0.04416013007126502		[learning rate: 0.00032915]
	Learning Rate: 0.000329155
	LOSS [training: 0.04120340508854303 | validation: 0.03935841501163289]
	TIME [epoch: 9.06 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036157271706960516		[learning rate: 0.00032865]
		[batch 20/20] avg loss: 0.04865747289567452		[learning rate: 0.00032815]
	Learning Rate: 0.000328146
	LOSS [training: 0.04240737230131751 | validation: 0.04650763006800627]
	TIME [epoch: 9.05 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0488913625198311		[learning rate: 0.00032764]
		[batch 20/20] avg loss: 0.03806834668116663		[learning rate: 0.00032714]
	Learning Rate: 0.00032714
	LOSS [training: 0.04347985460049887 | validation: 0.03190703224425597]
	TIME [epoch: 9.05 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03946220848078928		[learning rate: 0.00032664]
		[batch 20/20] avg loss: 0.04167065699263388		[learning rate: 0.00032614]
	Learning Rate: 0.000326137
	LOSS [training: 0.04056643273671158 | validation: 0.029900020457429174]
	TIME [epoch: 9.04 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05092047912863725		[learning rate: 0.00032564]
		[batch 20/20] avg loss: 0.040810477870213545		[learning rate: 0.00032514]
	Learning Rate: 0.000325137
	LOSS [training: 0.045865478499425405 | validation: 0.03264276253428752]
	TIME [epoch: 9.05 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04529064684885455		[learning rate: 0.00032464]
		[batch 20/20] avg loss: 0.03953197716008341		[learning rate: 0.00032414]
	Learning Rate: 0.000324141
	LOSS [training: 0.04241131200446899 | validation: 0.04840074953022814]
	TIME [epoch: 9.07 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04837786707246357		[learning rate: 0.00032364]
		[batch 20/20] avg loss: 0.04122226352909171		[learning rate: 0.00032315]
	Learning Rate: 0.000323147
	LOSS [training: 0.04480006530077764 | validation: 0.020226778689803618]
	TIME [epoch: 9.05 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03747189088758151		[learning rate: 0.00032265]
		[batch 20/20] avg loss: 0.04441570020188944		[learning rate: 0.00032216]
	Learning Rate: 0.000322156
	LOSS [training: 0.04094379554473547 | validation: 0.046877639192368446]
	TIME [epoch: 9.04 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044770521545517035		[learning rate: 0.00032166]
		[batch 20/20] avg loss: 0.03792091123523427		[learning rate: 0.00032117]
	Learning Rate: 0.000321169
	LOSS [training: 0.04134571639037564 | validation: 0.03259262626331262]
	TIME [epoch: 9.04 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03846239489712959		[learning rate: 0.00032068]
		[batch 20/20] avg loss: 0.045706303632091036		[learning rate: 0.00032018]
	Learning Rate: 0.000320184
	LOSS [training: 0.04208434926461032 | validation: 0.02493581865942727]
	TIME [epoch: 9.04 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04550072745711211		[learning rate: 0.00031969]
		[batch 20/20] avg loss: 0.034549040296405556		[learning rate: 0.0003192]
	Learning Rate: 0.000319203
	LOSS [training: 0.04002488387675884 | validation: 0.0214900810794887]
	TIME [epoch: 9.06 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04057556041051072		[learning rate: 0.00031871]
		[batch 20/20] avg loss: 0.05629517427938426		[learning rate: 0.00031822]
	Learning Rate: 0.000318224
	LOSS [training: 0.0484353673449475 | validation: 0.03566819408119971]
	TIME [epoch: 9.05 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04527632370454623		[learning rate: 0.00031774]
		[batch 20/20] avg loss: 0.03111481861451195		[learning rate: 0.00031725]
	Learning Rate: 0.000317249
	LOSS [training: 0.03819557115952908 | validation: 0.029883580861914685]
	TIME [epoch: 9.04 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03438779926811243		[learning rate: 0.00031676]
		[batch 20/20] avg loss: 0.03733460574618828		[learning rate: 0.00031628]
	Learning Rate: 0.000316276
	LOSS [training: 0.03586120250715035 | validation: 0.04207329709742459]
	TIME [epoch: 9.04 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03893114296972312		[learning rate: 0.00031579]
		[batch 20/20] avg loss: 0.04350014045853929		[learning rate: 0.00031531]
	Learning Rate: 0.000315307
	LOSS [training: 0.0412156417141312 | validation: 0.02566423651784197]
	TIME [epoch: 9.04 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033533736837021814		[learning rate: 0.00031482]
		[batch 20/20] avg loss: 0.04731742761251122		[learning rate: 0.00031434]
	Learning Rate: 0.00031434
	LOSS [training: 0.04042558222476651 | validation: 0.04685346893453244]
	TIME [epoch: 9.06 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046283386497291454		[learning rate: 0.00031386]
		[batch 20/20] avg loss: 0.040383034989351516		[learning rate: 0.00031338]
	Learning Rate: 0.000313377
	LOSS [training: 0.043333210743321485 | validation: 0.040839091099084145]
	TIME [epoch: 9.05 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039764535708711184		[learning rate: 0.0003129]
		[batch 20/20] avg loss: 0.0415033099374687		[learning rate: 0.00031242]
	Learning Rate: 0.000312416
	LOSS [training: 0.04063392282308994 | validation: 0.02590186402952381]
	TIME [epoch: 9.04 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03890542794609862		[learning rate: 0.00031194]
		[batch 20/20] avg loss: 0.03718144432670996		[learning rate: 0.00031146]
	Learning Rate: 0.000311458
	LOSS [training: 0.03804343613640429 | validation: 0.03721834169906881]
	TIME [epoch: 9.04 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03817996882628334		[learning rate: 0.00031098]
		[batch 20/20] avg loss: 0.04667033539635547		[learning rate: 0.0003105]
	Learning Rate: 0.000310504
	LOSS [training: 0.042425152111319406 | validation: 0.043224970640204155]
	TIME [epoch: 9.04 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04737075835345627		[learning rate: 0.00031003]
		[batch 20/20] avg loss: 0.06619306326257185		[learning rate: 0.00030955]
	Learning Rate: 0.000309552
	LOSS [training: 0.056781910808014056 | validation: 0.046706065073349394]
	TIME [epoch: 9.06 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042209631283287585		[learning rate: 0.00030908]
		[batch 20/20] avg loss: 0.04270983689340836		[learning rate: 0.0003086]
	Learning Rate: 0.000308603
	LOSS [training: 0.042459734088347974 | validation: 0.03466027103863083]
	TIME [epoch: 9.07 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047536068040874124		[learning rate: 0.00030813]
		[batch 20/20] avg loss: 0.04766135828086283		[learning rate: 0.00030766]
	Learning Rate: 0.000307657
	LOSS [training: 0.04759871316086847 | validation: 0.025841429317628797]
	TIME [epoch: 9.05 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04438723665459252		[learning rate: 0.00030718]
		[batch 20/20] avg loss: 0.03889491894348226		[learning rate: 0.00030671]
	Learning Rate: 0.000306714
	LOSS [training: 0.04164107779903738 | validation: 0.030067773993670242]
	TIME [epoch: 9.04 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04279802215872719		[learning rate: 0.00030624]
		[batch 20/20] avg loss: 0.03821824763924102		[learning rate: 0.00030577]
	Learning Rate: 0.000305774
	LOSS [training: 0.040508134898984104 | validation: 0.04192581064825719]
	TIME [epoch: 9.04 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040502361932765206		[learning rate: 0.0003053]
		[batch 20/20] avg loss: 0.05282755047162836		[learning rate: 0.00030484]
	Learning Rate: 0.000304836
	LOSS [training: 0.04666495620219678 | validation: 0.029610177110158883]
	TIME [epoch: 9.06 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04776381414008862		[learning rate: 0.00030437]
		[batch 20/20] avg loss: 0.047878632270267306		[learning rate: 0.0003039]
	Learning Rate: 0.000303902
	LOSS [training: 0.04782122320517797 | validation: 0.03615552153124975]
	TIME [epoch: 9.05 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04087578748365894		[learning rate: 0.00030344]
		[batch 20/20] avg loss: 0.05113141511734241		[learning rate: 0.00030297]
	Learning Rate: 0.00030297
	LOSS [training: 0.04600360130050068 | validation: 0.03674277068255906]
	TIME [epoch: 9.04 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04383192294680495		[learning rate: 0.00030251]
		[batch 20/20] avg loss: 0.03809661992703428		[learning rate: 0.00030204]
	Learning Rate: 0.000302042
	LOSS [training: 0.040964271436919615 | validation: 0.02187974761936999]
	TIME [epoch: 9.04 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0373443006508327		[learning rate: 0.00030158]
		[batch 20/20] avg loss: 0.040598080670732214		[learning rate: 0.00030112]
	Learning Rate: 0.000301116
	LOSS [training: 0.03897119066078245 | validation: 0.03106508182148023]
	TIME [epoch: 9.04 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04131304218243867		[learning rate: 0.00030065]
		[batch 20/20] avg loss: 0.03611353682937576		[learning rate: 0.00030019]
	Learning Rate: 0.000300193
	LOSS [training: 0.038713289505907224 | validation: 0.028395144091510376]
	TIME [epoch: 9.06 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04015330684515998		[learning rate: 0.00029973]
		[batch 20/20] avg loss: 0.03230980085978451		[learning rate: 0.00029927]
	Learning Rate: 0.000299272
	LOSS [training: 0.036231553852472245 | validation: 0.02734324815342227]
	TIME [epoch: 9.05 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0473346533812634		[learning rate: 0.00029881]
		[batch 20/20] avg loss: 0.03452373002982223		[learning rate: 0.00029835]
	Learning Rate: 0.000298355
	LOSS [training: 0.040929191705542815 | validation: 0.04362302593231276]
	TIME [epoch: 9.04 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036543219609265994		[learning rate: 0.0002979]
		[batch 20/20] avg loss: 0.046035856159828234		[learning rate: 0.00029744]
	Learning Rate: 0.00029744
	LOSS [training: 0.04128953788454712 | validation: 0.04031104766769687]
	TIME [epoch: 9.04 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03765535351831018		[learning rate: 0.00029698]
		[batch 20/20] avg loss: 0.038761886946049505		[learning rate: 0.00029653]
	Learning Rate: 0.000296529
	LOSS [training: 0.038208620232179843 | validation: 0.027168427189468727]
	TIME [epoch: 9.05 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04281921565803319		[learning rate: 0.00029607]
		[batch 20/20] avg loss: 0.04306350023769719		[learning rate: 0.00029562]
	Learning Rate: 0.00029562
	LOSS [training: 0.04294135794786519 | validation: 0.025206310637308195]
	TIME [epoch: 9.06 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03505976337348954		[learning rate: 0.00029517]
		[batch 20/20] avg loss: 0.028465973090401758		[learning rate: 0.00029471]
	Learning Rate: 0.000294713
	LOSS [training: 0.03176286823194564 | validation: 0.03370626296303954]
	TIME [epoch: 9.05 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038949646516251685		[learning rate: 0.00029426]
		[batch 20/20] avg loss: 0.03497311809573485		[learning rate: 0.00029381]
	Learning Rate: 0.00029381
	LOSS [training: 0.036961382305993266 | validation: 0.029020066668367518]
	TIME [epoch: 9.04 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03300846794511733		[learning rate: 0.00029336]
		[batch 20/20] avg loss: 0.035816585254149144		[learning rate: 0.00029291]
	Learning Rate: 0.000292909
	LOSS [training: 0.034412526599633245 | validation: 0.0313808341826724]
	TIME [epoch: 9.04 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03814451078802002		[learning rate: 0.00029246]
		[batch 20/20] avg loss: 0.029349050877499165		[learning rate: 0.00029201]
	Learning Rate: 0.000292011
	LOSS [training: 0.033746780832759594 | validation: 0.024184991346606406]
	TIME [epoch: 9.04 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03248442188242677		[learning rate: 0.00029156]
		[batch 20/20] avg loss: 0.04002182351914556		[learning rate: 0.00029112]
	Learning Rate: 0.000291116
	LOSS [training: 0.03625312270078616 | validation: 0.05523492116398215]
	TIME [epoch: 9.07 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04322172161477185		[learning rate: 0.00029067]
		[batch 20/20] avg loss: 0.04253606434397053		[learning rate: 0.00029022]
	Learning Rate: 0.000290224
	LOSS [training: 0.04287889297937118 | validation: 0.02465769717564414]
	TIME [epoch: 9.05 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0324635897225784		[learning rate: 0.00028978]
		[batch 20/20] avg loss: 0.0414092840659007		[learning rate: 0.00028933]
	Learning Rate: 0.000289334
	LOSS [training: 0.03693643689423955 | validation: 0.039040278692346796]
	TIME [epoch: 9.05 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04594379913285267		[learning rate: 0.00028889]
		[batch 20/20] avg loss: 0.040594523664984075		[learning rate: 0.00028845]
	Learning Rate: 0.000288447
	LOSS [training: 0.043269161398918364 | validation: 0.048531983196065]
	TIME [epoch: 9.05 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04091909796020025		[learning rate: 0.000288]
		[batch 20/20] avg loss: 0.036361818250711095		[learning rate: 0.00028756]
	Learning Rate: 0.000287563
	LOSS [training: 0.03864045810545567 | validation: 0.04009728901720963]
	TIME [epoch: 9.05 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034392865797857185		[learning rate: 0.00028712]
		[batch 20/20] avg loss: 0.03543705986080524		[learning rate: 0.00028668]
	Learning Rate: 0.000286682
	LOSS [training: 0.034914962829331216 | validation: 0.026840993164564113]
	TIME [epoch: 9.07 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03198381582076344		[learning rate: 0.00028624]
		[batch 20/20] avg loss: 0.03560107709167472		[learning rate: 0.0002858]
	Learning Rate: 0.000285803
	LOSS [training: 0.03379244645621908 | validation: 0.027556180605742116]
	TIME [epoch: 9.05 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0394872077490369		[learning rate: 0.00028536]
		[batch 20/20] avg loss: 0.047211049306809104		[learning rate: 0.00028493]
	Learning Rate: 0.000284927
	LOSS [training: 0.043349128527923 | validation: 0.026027455956219935]
	TIME [epoch: 9.04 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03398719066712681		[learning rate: 0.00028449]
		[batch 20/20] avg loss: 0.04700223563511301		[learning rate: 0.00028405]
	Learning Rate: 0.000284053
	LOSS [training: 0.04049471315111991 | validation: 0.043408358603784336]
	TIME [epoch: 9.05 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04885264697517189		[learning rate: 0.00028362]
		[batch 20/20] avg loss: 0.039431367658895794		[learning rate: 0.00028318]
	Learning Rate: 0.000283183
	LOSS [training: 0.04414200731703385 | validation: 0.020464205078152022]
	TIME [epoch: 9.06 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04029651536773767		[learning rate: 0.00028275]
		[batch 20/20] avg loss: 0.03594494205325887		[learning rate: 0.00028231]
	Learning Rate: 0.000282315
	LOSS [training: 0.03812072871049828 | validation: 0.024026585870267826]
	TIME [epoch: 9.06 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040873283521720746		[learning rate: 0.00028188]
		[batch 20/20] avg loss: 0.036647638558202536		[learning rate: 0.00028145]
	Learning Rate: 0.000281449
	LOSS [training: 0.03876046103996165 | validation: 0.01920753960535891]
	TIME [epoch: 9.06 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030316392509437923		[learning rate: 0.00028102]
		[batch 20/20] avg loss: 0.042044507747385676		[learning rate: 0.00028059]
	Learning Rate: 0.000280586
	LOSS [training: 0.0361804501284118 | validation: 0.032201558868010446]
	TIME [epoch: 9.05 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05323839931038958		[learning rate: 0.00028016]
		[batch 20/20] avg loss: 0.05393237034650387		[learning rate: 0.00027973]
	Learning Rate: 0.000279726
	LOSS [training: 0.053585384828446714 | validation: 0.03308146950538338]
	TIME [epoch: 9.05 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04096015858733814		[learning rate: 0.0002793]
		[batch 20/20] avg loss: 0.039408257777334504		[learning rate: 0.00027887]
	Learning Rate: 0.000278869
	LOSS [training: 0.040184208182336326 | validation: 0.04201692142979887]
	TIME [epoch: 9.05 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041984921063175897		[learning rate: 0.00027844]
		[batch 20/20] avg loss: 0.029582902631112372		[learning rate: 0.00027801]
	Learning Rate: 0.000278014
	LOSS [training: 0.035783911847144134 | validation: 0.03127757847557625]
	TIME [epoch: 9.06 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04155804915087298		[learning rate: 0.00027759]
		[batch 20/20] avg loss: 0.047442538541756486		[learning rate: 0.00027716]
	Learning Rate: 0.000277162
	LOSS [training: 0.04450029384631473 | validation: 0.04152173029076023]
	TIME [epoch: 9.06 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03994031493093349		[learning rate: 0.00027674]
		[batch 20/20] avg loss: 0.050996205147004224		[learning rate: 0.00027631]
	Learning Rate: 0.000276312
	LOSS [training: 0.04546826003896886 | validation: 0.020949423942952607]
	TIME [epoch: 9.05 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048179604460428106		[learning rate: 0.00027589]
		[batch 20/20] avg loss: 0.03142109654317425		[learning rate: 0.00027547]
	Learning Rate: 0.000275465
	LOSS [training: 0.039800350501801175 | validation: 0.02632761651434995]
	TIME [epoch: 9.05 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0368985288250013		[learning rate: 0.00027504]
		[batch 20/20] avg loss: 0.0483183340588325		[learning rate: 0.00027462]
	Learning Rate: 0.000274621
	LOSS [training: 0.0426084314419169 | validation: 0.041080294545623224]
	TIME [epoch: 9.05 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05049424672909749		[learning rate: 0.0002742]
		[batch 20/20] avg loss: 0.04049470923990313		[learning rate: 0.00027378]
	Learning Rate: 0.000273779
	LOSS [training: 0.045494477984500306 | validation: 0.020459877977109152]
	TIME [epoch: 9.06 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03890046801424604		[learning rate: 0.00027336]
		[batch 20/20] avg loss: 0.04505754858708621		[learning rate: 0.00027294]
	Learning Rate: 0.00027294
	LOSS [training: 0.041979008300666136 | validation: 0.024260897358364994]
	TIME [epoch: 9.05 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034335839682007215		[learning rate: 0.00027252]
		[batch 20/20] avg loss: 0.030850040813647783		[learning rate: 0.0002721]
	Learning Rate: 0.000272103
	LOSS [training: 0.032592940247827495 | validation: 0.019020848807340472]
	TIME [epoch: 9.05 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03811075134057397		[learning rate: 0.00027169]
		[batch 20/20] avg loss: 0.03823258984552467		[learning rate: 0.00027127]
	Learning Rate: 0.000271269
	LOSS [training: 0.03817167059304932 | validation: 0.019566378140507892]
	TIME [epoch: 9.05 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04206513679048679		[learning rate: 0.00027085]
		[batch 20/20] avg loss: 0.037235177888006565		[learning rate: 0.00027044]
	Learning Rate: 0.000270437
	LOSS [training: 0.03965015733924667 | validation: 0.023776889758289923]
	TIME [epoch: 9.04 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0357939074432163		[learning rate: 0.00027002]
		[batch 20/20] avg loss: 0.04017872458288612		[learning rate: 0.00026961]
	Learning Rate: 0.000269608
	LOSS [training: 0.037986316013051205 | validation: 0.022393385201673253]
	TIME [epoch: 9.06 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03542268207750258		[learning rate: 0.00026919]
		[batch 20/20] avg loss: 0.03409419898094364		[learning rate: 0.00026878]
	Learning Rate: 0.000268782
	LOSS [training: 0.03475844052922311 | validation: 0.02853308525139904]
	TIME [epoch: 9.05 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049887311617033706		[learning rate: 0.00026837]
		[batch 20/20] avg loss: 0.03664683881678824		[learning rate: 0.00026796]
	Learning Rate: 0.000267958
	LOSS [training: 0.043267075216910963 | validation: 0.0350946376658068]
	TIME [epoch: 9.04 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037668575445281446		[learning rate: 0.00026755]
		[batch 20/20] avg loss: 0.02901668408203308		[learning rate: 0.00026714]
	Learning Rate: 0.000267137
	LOSS [training: 0.03334262976365727 | validation: 0.030900238257806217]
	TIME [epoch: 9.04 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04828113774837567		[learning rate: 0.00026673]
		[batch 20/20] avg loss: 0.041314388945915125		[learning rate: 0.00026632]
	Learning Rate: 0.000266318
	LOSS [training: 0.04479776334714539 | validation: 0.031847912901129115]
	TIME [epoch: 9.04 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03289468046698742		[learning rate: 0.00026591]
		[batch 20/20] avg loss: 0.0442302165983835		[learning rate: 0.0002655]
	Learning Rate: 0.000265501
	LOSS [training: 0.038562448532685464 | validation: 0.02723292213032474]
	TIME [epoch: 9.07 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037067571200949695		[learning rate: 0.00026509]
		[batch 20/20] avg loss: 0.039109313475501256		[learning rate: 0.00026469]
	Learning Rate: 0.000264687
	LOSS [training: 0.03808844233822547 | validation: 0.03653269322579028]
	TIME [epoch: 9.05 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05090658113274249		[learning rate: 0.00026428]
		[batch 20/20] avg loss: 0.03617402474964399		[learning rate: 0.00026388]
	Learning Rate: 0.000263876
	LOSS [training: 0.043540302941193244 | validation: 0.028137286761266783]
	TIME [epoch: 9.04 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03554245092799456		[learning rate: 0.00026347]
		[batch 20/20] avg loss: 0.038057810423387836		[learning rate: 0.00026307]
	Learning Rate: 0.000263067
	LOSS [training: 0.036800130675691184 | validation: 0.02652957970469496]
	TIME [epoch: 9.04 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03749684787354723		[learning rate: 0.00026266]
		[batch 20/20] avg loss: 0.03435399870268587		[learning rate: 0.00026226]
	Learning Rate: 0.000262261
	LOSS [training: 0.035925423288116556 | validation: 0.02811917339433633]
	TIME [epoch: 9.04 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03655644246644911		[learning rate: 0.00026186]
		[batch 20/20] avg loss: 0.0383587054610839		[learning rate: 0.00026146]
	Learning Rate: 0.000261457
	LOSS [training: 0.037457573963766505 | validation: 0.042799320084665575]
	TIME [epoch: 9.06 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04229885355690771		[learning rate: 0.00026106]
		[batch 20/20] avg loss: 0.04189619527775382		[learning rate: 0.00026066]
	Learning Rate: 0.000260655
	LOSS [training: 0.042097524417330764 | validation: 0.03374969644088811]
	TIME [epoch: 9.06 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0349959968205511		[learning rate: 0.00026026]
		[batch 20/20] avg loss: 0.03272046615123011		[learning rate: 0.00025986]
	Learning Rate: 0.000259856
	LOSS [training: 0.03385823148589061 | validation: 0.024672673545176806]
	TIME [epoch: 9.04 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04408710106629786		[learning rate: 0.00025946]
		[batch 20/20] avg loss: 0.033450919108529736		[learning rate: 0.00025906]
	Learning Rate: 0.00025906
	LOSS [training: 0.038769010087413805 | validation: 0.026530509290489973]
	TIME [epoch: 9.04 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030754158168232966		[learning rate: 0.00025866]
		[batch 20/20] avg loss: 0.038507480502121626		[learning rate: 0.00025827]
	Learning Rate: 0.000258266
	LOSS [training: 0.034630819335177296 | validation: 0.028703264108061788]
	TIME [epoch: 9.04 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04571663637220968		[learning rate: 0.00025787]
		[batch 20/20] avg loss: 0.034401493532769264		[learning rate: 0.00025747]
	Learning Rate: 0.000257474
	LOSS [training: 0.04005906495248947 | validation: 0.025298384334632003]
	TIME [epoch: 9.05 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03759554459200482		[learning rate: 0.00025708]
		[batch 20/20] avg loss: 0.03390907447513024		[learning rate: 0.00025668]
	Learning Rate: 0.000256685
	LOSS [training: 0.03575230953356753 | validation: 0.025111009923724774]
	TIME [epoch: 9.06 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036652929995428345		[learning rate: 0.00025629]
		[batch 20/20] avg loss: 0.031967194075506224		[learning rate: 0.0002559]
	Learning Rate: 0.000255898
	LOSS [training: 0.034310062035467284 | validation: 0.035383488746309685]
	TIME [epoch: 9.04 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039574718183863364		[learning rate: 0.00025551]
		[batch 20/20] avg loss: 0.036856278759127115		[learning rate: 0.00025511]
	Learning Rate: 0.000255113
	LOSS [training: 0.03821549847149523 | validation: 0.032690170683261716]
	TIME [epoch: 9.04 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03927949206288428		[learning rate: 0.00025472]
		[batch 20/20] avg loss: 0.0348195329637824		[learning rate: 0.00025433]
	Learning Rate: 0.000254331
	LOSS [training: 0.03704951251333334 | validation: 0.03200878713858789]
	TIME [epoch: 9.04 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04744190314648798		[learning rate: 0.00025394]
		[batch 20/20] avg loss: 0.04614940607978242		[learning rate: 0.00025355]
	Learning Rate: 0.000253552
	LOSS [training: 0.0467956546131352 | validation: 0.03750280885576692]
	TIME [epoch: 9.06 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038316244710677856		[learning rate: 0.00025316]
		[batch 20/20] avg loss: 0.03187074462891679		[learning rate: 0.00025277]
	Learning Rate: 0.000252775
	LOSS [training: 0.03509349466979732 | validation: 0.03309492431306636]
	TIME [epoch: 9.06 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03269538171568569		[learning rate: 0.00025239]
		[batch 20/20] avg loss: 0.03919914136279688		[learning rate: 0.000252]
	Learning Rate: 0.000252
	LOSS [training: 0.03594726153924129 | validation: 0.025340071699688452]
	TIME [epoch: 9.04 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0402651316697883		[learning rate: 0.00025161]
		[batch 20/20] avg loss: 0.04135024684299362		[learning rate: 0.00025123]
	Learning Rate: 0.000251227
	LOSS [training: 0.04080768925639095 | validation: 0.037660236479042083]
	TIME [epoch: 9.05 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03685310930916249		[learning rate: 0.00025084]
		[batch 20/20] avg loss: 0.0347778996449406		[learning rate: 0.00025046]
	Learning Rate: 0.000250457
	LOSS [training: 0.03581550447705155 | validation: 0.03476443349548846]
	TIME [epoch: 9.04 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03534924120874864		[learning rate: 0.00025007]
		[batch 20/20] avg loss: 0.04623985082120409		[learning rate: 0.00024969]
	Learning Rate: 0.000249689
	LOSS [training: 0.04079454601497636 | validation: 0.04388887547104662]
	TIME [epoch: 9.04 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035198346664352516		[learning rate: 0.00024931]
		[batch 20/20] avg loss: 0.03592859801411199		[learning rate: 0.00024892]
	Learning Rate: 0.000248924
	LOSS [training: 0.03556347233923226 | validation: 0.03220126139812871]
	TIME [epoch: 9.06 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041477625972259194		[learning rate: 0.00024854]
		[batch 20/20] avg loss: 0.04296363499186777		[learning rate: 0.00024816]
	Learning Rate: 0.000248161
	LOSS [training: 0.04222063048206349 | validation: 0.03733217906150784]
	TIME [epoch: 9.05 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05124635355573215		[learning rate: 0.00024778]
		[batch 20/20] avg loss: 0.03870656729565978		[learning rate: 0.0002474]
	Learning Rate: 0.0002474
	LOSS [training: 0.04497646042569596 | validation: 0.023042928042507693]
	TIME [epoch: 9.04 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03998207524907968		[learning rate: 0.00024702]
		[batch 20/20] avg loss: 0.03438103954349524		[learning rate: 0.00024664]
	Learning Rate: 0.000246642
	LOSS [training: 0.03718155739628746 | validation: 0.029491693461823097]
	TIME [epoch: 9.04 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03690381221538658		[learning rate: 0.00024626]
		[batch 20/20] avg loss: 0.032594809661996196		[learning rate: 0.00024589]
	Learning Rate: 0.000245886
	LOSS [training: 0.034749310938691394 | validation: 0.028071632410661296]
	TIME [epoch: 9.05 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0352379909025657		[learning rate: 0.00024551]
		[batch 20/20] avg loss: 0.03962487963721848		[learning rate: 0.00024513]
	Learning Rate: 0.000245132
	LOSS [training: 0.03743143526989209 | validation: 0.02012690347752164]
	TIME [epoch: 9.06 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03906470907048956		[learning rate: 0.00024476]
		[batch 20/20] avg loss: 0.0339254898488651		[learning rate: 0.00024438]
	Learning Rate: 0.000244381
	LOSS [training: 0.03649509945967733 | validation: 0.03321314188391669]
	TIME [epoch: 9.11 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03664950983934624		[learning rate: 0.00024401]
		[batch 20/20] avg loss: 0.03467390771311774		[learning rate: 0.00024363]
	Learning Rate: 0.000243631
	LOSS [training: 0.035661708776231996 | validation: 0.03386532868240027]
	TIME [epoch: 9.04 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03152674894073809		[learning rate: 0.00024326]
		[batch 20/20] avg loss: 0.03691943442367929		[learning rate: 0.00024288]
	Learning Rate: 0.000242885
	LOSS [training: 0.03422309168220869 | validation: 0.03233188493795688]
	TIME [epoch: 9.04 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040660131555472045		[learning rate: 0.00024251]
		[batch 20/20] avg loss: 0.03270831951490324		[learning rate: 0.00024214]
	Learning Rate: 0.00024214
	LOSS [training: 0.03668422553518764 | validation: 0.02405379808146861]
	TIME [epoch: 9.04 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030634584205034128		[learning rate: 0.00024177]
		[batch 20/20] avg loss: 0.034087078980786056		[learning rate: 0.0002414]
	Learning Rate: 0.000241398
	LOSS [training: 0.032360831592910096 | validation: 0.0277466974451814]
	TIME [epoch: 9.07 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03205506864092418		[learning rate: 0.00024103]
		[batch 20/20] avg loss: 0.0322836208434511		[learning rate: 0.00024066]
	Learning Rate: 0.000240658
	LOSS [training: 0.03216934474218764 | validation: 0.03324206379152699]
	TIME [epoch: 9.04 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03648427711130997		[learning rate: 0.00024029]
		[batch 20/20] avg loss: 0.028184942786090118		[learning rate: 0.00023992]
	Learning Rate: 0.00023992
	LOSS [training: 0.03233460994870005 | validation: 0.02940041268068632]
	TIME [epoch: 9.05 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032094478507835776		[learning rate: 0.00023955]
		[batch 20/20] avg loss: 0.0333540043516272		[learning rate: 0.00023918]
	Learning Rate: 0.000239185
	LOSS [training: 0.032724241429731495 | validation: 0.027161170742573883]
	TIME [epoch: 9.04 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03761642353268914		[learning rate: 0.00023882]
		[batch 20/20] avg loss: 0.03539196597150403		[learning rate: 0.00023845]
	Learning Rate: 0.000238451
	LOSS [training: 0.036504194752096576 | validation: 0.026920684372062693]
	TIME [epoch: 9.04 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03508140447486139		[learning rate: 0.00023809]
		[batch 20/20] avg loss: 0.032269615865847194		[learning rate: 0.00023772]
	Learning Rate: 0.000237721
	LOSS [training: 0.03367551017035429 | validation: 0.02251411360773105]
	TIME [epoch: 9.07 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03674155506718139		[learning rate: 0.00023736]
		[batch 20/20] avg loss: 0.030052754141336274		[learning rate: 0.00023699]
	Learning Rate: 0.000236992
	LOSS [training: 0.03339715460425883 | validation: 0.03024430161289997]
	TIME [epoch: 9.32 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03478405480289216		[learning rate: 0.00023663]
		[batch 20/20] avg loss: 0.04114360668749313		[learning rate: 0.00023627]
	Learning Rate: 0.000236265
	LOSS [training: 0.03796383074519265 | validation: 0.04117424356053315]
	TIME [epoch: 9.05 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037223142242561694		[learning rate: 0.0002359]
		[batch 20/20] avg loss: 0.0397207584282088		[learning rate: 0.00023554]
	Learning Rate: 0.000235541
	LOSS [training: 0.038471950335385245 | validation: 0.035136703020684855]
	TIME [epoch: 9.04 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03994840919539289		[learning rate: 0.00023518]
		[batch 20/20] avg loss: 0.033867821194967875		[learning rate: 0.00023482]
	Learning Rate: 0.000234819
	LOSS [training: 0.03690811519518038 | validation: 0.038595878831684784]
	TIME [epoch: 9.04 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03291151769155272		[learning rate: 0.00023446]
		[batch 20/20] avg loss: 0.04375634836679587		[learning rate: 0.0002341]
	Learning Rate: 0.000234099
	LOSS [training: 0.038333933029174296 | validation: 0.039744366784888116]
	TIME [epoch: 9.06 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036865514622150555		[learning rate: 0.00023374]
		[batch 20/20] avg loss: 0.03159650968782719		[learning rate: 0.00023338]
	Learning Rate: 0.000233382
	LOSS [training: 0.034231012154988874 | validation: 0.03394467944652604]
	TIME [epoch: 9.04 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03341260947508544		[learning rate: 0.00023302]
		[batch 20/20] avg loss: 0.03291669160070948		[learning rate: 0.00023267]
	Learning Rate: 0.000232666
	LOSS [training: 0.03316465053789745 | validation: 0.028500401842036824]
	TIME [epoch: 9.04 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033415962241246294		[learning rate: 0.00023231]
		[batch 20/20] avg loss: 0.038730608276336155		[learning rate: 0.00023195]
	Learning Rate: 0.000231953
	LOSS [training: 0.03607328525879123 | validation: 0.0392203414859336]
	TIME [epoch: 9.13 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03771625483016726		[learning rate: 0.0002316]
		[batch 20/20] avg loss: 0.03110055016854999		[learning rate: 0.00023124]
	Learning Rate: 0.000231242
	LOSS [training: 0.03440840249935863 | validation: 0.03212839150394366]
	TIME [epoch: 9.04 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043957129408178586		[learning rate: 0.00023089]
		[batch 20/20] avg loss: 0.038001175830294484		[learning rate: 0.00023053]
	Learning Rate: 0.000230533
	LOSS [training: 0.04097915261923653 | validation: 0.0306548382513368]
	TIME [epoch: 9.07 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03919267835612117		[learning rate: 0.00023018]
		[batch 20/20] avg loss: 0.031780554409573544		[learning rate: 0.00022983]
	Learning Rate: 0.000229826
	LOSS [training: 0.03548661638284736 | validation: 0.02812743797118817]
	TIME [epoch: 9.04 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030854252856215923		[learning rate: 0.00022947]
		[batch 20/20] avg loss: 0.036182521604499765		[learning rate: 0.00022912]
	Learning Rate: 0.000229122
	LOSS [training: 0.03351838723035784 | validation: 0.03169288949665349]
	TIME [epoch: 9.05 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038091995430056466		[learning rate: 0.00022877]
		[batch 20/20] avg loss: 0.033561407434536644		[learning rate: 0.00022842]
	Learning Rate: 0.00022842
	LOSS [training: 0.035826701432296555 | validation: 0.02412547238906014]
	TIME [epoch: 9.04 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03114158609417735		[learning rate: 0.00022807]
		[batch 20/20] avg loss: 0.030793502793440015		[learning rate: 0.00022772]
	Learning Rate: 0.000227719
	LOSS [training: 0.03096754444380868 | validation: 0.026825646710653486]
	TIME [epoch: 9.04 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033497079394139144		[learning rate: 0.00022737]
		[batch 20/20] avg loss: 0.0348122278759085		[learning rate: 0.00022702]
	Learning Rate: 0.000227021
	LOSS [training: 0.034154653635023825 | validation: 0.01810365835654416]
	TIME [epoch: 9.07 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0275312274408747		[learning rate: 0.00022667]
		[batch 20/20] avg loss: 0.03785988082326557		[learning rate: 0.00022633]
	Learning Rate: 0.000226325
	LOSS [training: 0.03269555413207014 | validation: 0.03610833939032617]
	TIME [epoch: 9.04 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03538546234616993		[learning rate: 0.00022598]
		[batch 20/20] avg loss: 0.03795004871780368		[learning rate: 0.00022563]
	Learning Rate: 0.000225632
	LOSS [training: 0.036667755531986805 | validation: 0.03020219492693106]
	TIME [epoch: 9.05 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037483486884357126		[learning rate: 0.00022529]
		[batch 20/20] avg loss: 0.041416056453704955		[learning rate: 0.00022494]
	Learning Rate: 0.00022494
	LOSS [training: 0.03944977166903105 | validation: 0.028799664001005032]
	TIME [epoch: 9.04 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042501465057149655		[learning rate: 0.00022459]
		[batch 20/20] avg loss: 0.03175872480546494		[learning rate: 0.00022425]
	Learning Rate: 0.00022425
	LOSS [training: 0.037130094931307304 | validation: 0.025299339945500224]
	TIME [epoch: 9.06 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03645219407305784		[learning rate: 0.00022391]
		[batch 20/20] avg loss: 0.031231241861882097		[learning rate: 0.00022356]
	Learning Rate: 0.000223563
	LOSS [training: 0.033841717967469966 | validation: 0.02698826650052836]
	TIME [epoch: 9.07 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029456064138135434		[learning rate: 0.00022322]
		[batch 20/20] avg loss: 0.03555757693156232		[learning rate: 0.00022288]
	Learning Rate: 0.000222878
	LOSS [training: 0.03250682053484887 | validation: 0.03224730229251844]
	TIME [epoch: 9.05 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038929040916547676		[learning rate: 0.00022254]
		[batch 20/20] avg loss: 0.04169918447704445		[learning rate: 0.00022219]
	Learning Rate: 0.000222195
	LOSS [training: 0.040314112696796064 | validation: 0.036448977603578236]
	TIME [epoch: 9.04 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03529691921302322		[learning rate: 0.00022185]
		[batch 20/20] avg loss: 0.03337852466463716		[learning rate: 0.00022151]
	Learning Rate: 0.000221513
	LOSS [training: 0.03433772193883018 | validation: 0.024953418482167092]
	TIME [epoch: 9.05 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02998754127024934		[learning rate: 0.00022117]
		[batch 20/20] avg loss: 0.03333594256195038		[learning rate: 0.00022083]
	Learning Rate: 0.000220834
	LOSS [training: 0.03166174191609986 | validation: 0.035367914031391175]
	TIME [epoch: 9.24 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033443441196553944		[learning rate: 0.0002205]
		[batch 20/20] avg loss: 0.029363472347413787		[learning rate: 0.00022016]
	Learning Rate: 0.000220157
	LOSS [training: 0.03140345677198387 | validation: 0.027622615429542016]
	TIME [epoch: 9.07 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0382523321740786		[learning rate: 0.00021982]
		[batch 20/20] avg loss: 0.04136594591896082		[learning rate: 0.00021948]
	Learning Rate: 0.000219483
	LOSS [training: 0.03980913904651971 | validation: 0.023394968479538966]
	TIME [epoch: 9.04 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03419782248292485		[learning rate: 0.00021915]
		[batch 20/20] avg loss: 0.03356938883428691		[learning rate: 0.00021881]
	Learning Rate: 0.00021881
	LOSS [training: 0.03388360565860588 | validation: 0.02272237807803115]
	TIME [epoch: 9.04 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03711716624210471		[learning rate: 0.00021847]
		[batch 20/20] avg loss: 0.02973839901770877		[learning rate: 0.00021814]
	Learning Rate: 0.000218139
	LOSS [training: 0.03342778262990674 | validation: 0.029606047819797317]
	TIME [epoch: 9.04 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030403970668966468		[learning rate: 0.0002178]
		[batch 20/20] avg loss: 0.03716043500984299		[learning rate: 0.00021747]
	Learning Rate: 0.00021747
	LOSS [training: 0.03378220283940474 | validation: 0.02651456479475163]
	TIME [epoch: 9.05 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03798323712634895		[learning rate: 0.00021714]
		[batch 20/20] avg loss: 0.029737605869778777		[learning rate: 0.0002168]
	Learning Rate: 0.000216804
	LOSS [training: 0.033860421498063856 | validation: 0.027577245697888664]
	TIME [epoch: 9.07 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025328465780166932		[learning rate: 0.00021647]
		[batch 20/20] avg loss: 0.03320803814670999		[learning rate: 0.00021614]
	Learning Rate: 0.000216139
	LOSS [training: 0.029268251963438463 | validation: 0.02521653468281279]
	TIME [epoch: 9.04 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0373829754280017		[learning rate: 0.00021581]
		[batch 20/20] avg loss: 0.029057946289738755		[learning rate: 0.00021548]
	Learning Rate: 0.000215477
	LOSS [training: 0.033220460858870225 | validation: 0.028891376557885737]
	TIME [epoch: 9.04 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025827464048331045		[learning rate: 0.00021515]
		[batch 20/20] avg loss: 0.03819502027079465		[learning rate: 0.00021482]
	Learning Rate: 0.000214816
	LOSS [training: 0.032011242159562844 | validation: 0.028539187742585195]
	TIME [epoch: 9.04 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03193995002183912		[learning rate: 0.00021449]
		[batch 20/20] avg loss: 0.03706688101636744		[learning rate: 0.00021416]
	Learning Rate: 0.000214157
	LOSS [training: 0.034503415519103284 | validation: 0.02993552929809773]
	TIME [epoch: 9.05 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03209262337393875		[learning rate: 0.00021383]
		[batch 20/20] avg loss: 0.03136124383369624		[learning rate: 0.0002135]
	Learning Rate: 0.000213501
	LOSS [training: 0.031726933603817495 | validation: 0.023377601157551102]
	TIME [epoch: 9.06 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030275885741327186		[learning rate: 0.00021317]
		[batch 20/20] avg loss: 0.039286401009439906		[learning rate: 0.00021285]
	Learning Rate: 0.000212847
	LOSS [training: 0.03478114337538354 | validation: 0.028514259735203325]
	TIME [epoch: 9.05 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04131888757085033		[learning rate: 0.00021252]
		[batch 20/20] avg loss: 0.028833768456447428		[learning rate: 0.00021219]
	Learning Rate: 0.000212194
	LOSS [training: 0.035076328013648884 | validation: 0.028044418353934016]
	TIME [epoch: 9.08 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042818869873761496		[learning rate: 0.00021187]
		[batch 20/20] avg loss: 0.040805317038248676		[learning rate: 0.00021154]
	Learning Rate: 0.000211544
	LOSS [training: 0.04181209345600508 | validation: 0.023746255260790928]
	TIME [epoch: 9.04 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03742817549961957		[learning rate: 0.00021122]
		[batch 20/20] avg loss: 0.03244114769146981		[learning rate: 0.0002109]
	Learning Rate: 0.000210895
	LOSS [training: 0.03493466159554469 | validation: 0.02364350492666207]
	TIME [epoch: 9.04 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031709949803005344		[learning rate: 0.00021057]
		[batch 20/20] avg loss: 0.03933845031326621		[learning rate: 0.00021025]
	Learning Rate: 0.000210249
	LOSS [training: 0.03552420005813578 | validation: 0.02643620693082815]
	TIME [epoch: 9.07 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03235858974858991		[learning rate: 0.00020993]
		[batch 20/20] avg loss: 0.03451985565975268		[learning rate: 0.0002096]
	Learning Rate: 0.000209604
	LOSS [training: 0.0334392227041713 | validation: 0.027380878528637784]
	TIME [epoch: 9.04 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028331592045635796		[learning rate: 0.00020928]
		[batch 20/20] avg loss: 0.03135252505590697		[learning rate: 0.00020896]
	Learning Rate: 0.000208962
	LOSS [training: 0.029842058550771378 | validation: 0.02649588151325486]
	TIME [epoch: 9.04 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033468472035400806		[learning rate: 0.00020864]
		[batch 20/20] avg loss: 0.04114953642364053		[learning rate: 0.00020832]
	Learning Rate: 0.000208321
	LOSS [training: 0.03730900422952067 | validation: 0.02393019029893769]
	TIME [epoch: 9.04 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032285720207138796		[learning rate: 0.000208]
		[batch 20/20] avg loss: 0.030805620536075145		[learning rate: 0.00020768]
	Learning Rate: 0.000207683
	LOSS [training: 0.03154567037160697 | validation: 0.027566761325224155]
	TIME [epoch: 9.05 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03344068505333199		[learning rate: 0.00020736]
		[batch 20/20] avg loss: 0.03479988347391702		[learning rate: 0.00020705]
	Learning Rate: 0.000207046
	LOSS [training: 0.03412028426362451 | validation: 0.02420484219829623]
	TIME [epoch: 9.06 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043722105059129796		[learning rate: 0.00020673]
		[batch 20/20] avg loss: 0.03180564305400378		[learning rate: 0.00020641]
	Learning Rate: 0.000206411
	LOSS [training: 0.03776387405656678 | validation: 0.01928942338676783]
	TIME [epoch: 9.05 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041251470995867665		[learning rate: 0.00020609]
		[batch 20/20] avg loss: 0.03223933269729621		[learning rate: 0.00020578]
	Learning Rate: 0.000205778
	LOSS [training: 0.036745401846581933 | validation: 0.022600705299706085]
	TIME [epoch: 9.04 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03424538918634387		[learning rate: 0.00020546]
		[batch 20/20] avg loss: 0.03125331537078581		[learning rate: 0.00020515]
	Learning Rate: 0.000205148
	LOSS [training: 0.03274935227856484 | validation: 0.029504348175660066]
	TIME [epoch: 9.04 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02845343961589253		[learning rate: 0.00020483]
		[batch 20/20] avg loss: 0.04556597036944032		[learning rate: 0.00020452]
	Learning Rate: 0.000204519
	LOSS [training: 0.037009704992666426 | validation: 0.021936657963357736]
	TIME [epoch: 9.04 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03438157367769802		[learning rate: 0.00020421]
		[batch 20/20] avg loss: 0.03338446341746389		[learning rate: 0.00020389]
	Learning Rate: 0.000203892
	LOSS [training: 0.03388301854758097 | validation: 0.03615479160093986]
	TIME [epoch: 9.06 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0435284248016567		[learning rate: 0.00020358]
		[batch 20/20] avg loss: 0.035370277748368165		[learning rate: 0.00020327]
	Learning Rate: 0.000203267
	LOSS [training: 0.03944935127501244 | validation: 0.022997484588318438]
	TIME [epoch: 9.04 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03487004672884918		[learning rate: 0.00020296]
		[batch 20/20] avg loss: 0.03913668586099004		[learning rate: 0.00020264]
	Learning Rate: 0.000202644
	LOSS [training: 0.03700336629491961 | validation: 0.024530507211504404]
	TIME [epoch: 9.04 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036225807734607054		[learning rate: 0.00020233]
		[batch 20/20] avg loss: 0.03225958424262158		[learning rate: 0.00020202]
	Learning Rate: 0.000202023
	LOSS [training: 0.03424269598861431 | validation: 0.026203635576783894]
	TIME [epoch: 9.04 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034281517737876466		[learning rate: 0.00020171]
		[batch 20/20] avg loss: 0.03128144359224659		[learning rate: 0.0002014]
	Learning Rate: 0.000201403
	LOSS [training: 0.03278148066506152 | validation: 0.023395918153651878]
	TIME [epoch: 9.04 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03567619434364753		[learning rate: 0.00020109]
		[batch 20/20] avg loss: 0.03658469743957151		[learning rate: 0.00020079]
	Learning Rate: 0.000200786
	LOSS [training: 0.03613044589160951 | validation: 0.032087868983796834]
	TIME [epoch: 9.06 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02964760460856374		[learning rate: 0.00020048]
		[batch 20/20] avg loss: 0.03178577026021927		[learning rate: 0.00020017]
	Learning Rate: 0.00020017
	LOSS [training: 0.030716687434391494 | validation: 0.01703361029937793]
	TIME [epoch: 9.04 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03505157799775969		[learning rate: 0.00019986]
		[batch 20/20] avg loss: 0.03397857225304416		[learning rate: 0.00019956]
	Learning Rate: 0.000199557
	LOSS [training: 0.034515075125401924 | validation: 0.021403382048623607]
	TIME [epoch: 9.04 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029661426587232798		[learning rate: 0.00019925]
		[batch 20/20] avg loss: 0.042008313650268476		[learning rate: 0.00019895]
	Learning Rate: 0.000198945
	LOSS [training: 0.03583487011875064 | validation: 0.026716497102747802]
	TIME [epoch: 9.05 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03489327670990285		[learning rate: 0.00019864]
		[batch 20/20] avg loss: 0.030148344735085052		[learning rate: 0.00019834]
	Learning Rate: 0.000198335
	LOSS [training: 0.03252081072249395 | validation: 0.02827515922760092]
	TIME [epoch: 9.04 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034932173059193196		[learning rate: 0.00019803]
		[batch 20/20] avg loss: 0.03480763849615014		[learning rate: 0.00019773]
	Learning Rate: 0.000197727
	LOSS [training: 0.03486990577767167 | validation: 0.03271462549400486]
	TIME [epoch: 9.06 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03776170985429633		[learning rate: 0.00019742]
		[batch 20/20] avg loss: 0.031389588747465696		[learning rate: 0.00019712]
	Learning Rate: 0.000197121
	LOSS [training: 0.03457564930088102 | validation: 0.015133029934041708]
	TIME [epoch: 9.04 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0386645202949971		[learning rate: 0.00019682]
		[batch 20/20] avg loss: 0.02952228382114236		[learning rate: 0.00019652]
	Learning Rate: 0.000196517
	LOSS [training: 0.034093402058069724 | validation: 0.029836680317033695]
	TIME [epoch: 9.04 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037063642675592735		[learning rate: 0.00019622]
		[batch 20/20] avg loss: 0.04523349216079546		[learning rate: 0.00019591]
	Learning Rate: 0.000195915
	LOSS [training: 0.041148567418194094 | validation: 0.03362866313813201]
	TIME [epoch: 9.04 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031596507527943754		[learning rate: 0.00019561]
		[batch 20/20] avg loss: 0.03965465076694695		[learning rate: 0.00019531]
	Learning Rate: 0.000195314
	LOSS [training: 0.03562557914744535 | validation: 0.03325889288825559]
	TIME [epoch: 9.04 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032702720479474014		[learning rate: 0.00019501]
		[batch 20/20] avg loss: 0.03724491219631575		[learning rate: 0.00019472]
	Learning Rate: 0.000194715
	LOSS [training: 0.034973816337894884 | validation: 0.032723759052333615]
	TIME [epoch: 9.06 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038003842656304784		[learning rate: 0.00019442]
		[batch 20/20] avg loss: 0.04057729481950702		[learning rate: 0.00019412]
	Learning Rate: 0.000194118
	LOSS [training: 0.0392905687379059 | validation: 0.02921235366986754]
	TIME [epoch: 9.04 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032709651216151284		[learning rate: 0.00019382]
		[batch 20/20] avg loss: 0.035181688157583865		[learning rate: 0.00019352]
	Learning Rate: 0.000193523
	LOSS [training: 0.03394566968686758 | validation: 0.027658768803487013]
	TIME [epoch: 9.04 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04405834828532958		[learning rate: 0.00019323]
		[batch 20/20] avg loss: 0.03488790047053951		[learning rate: 0.00019293]
	Learning Rate: 0.00019293
	LOSS [training: 0.03947312437793454 | validation: 0.036054735967400296]
	TIME [epoch: 9.04 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031518789067211325		[learning rate: 0.00019263]
		[batch 20/20] avg loss: 0.03545699972079576		[learning rate: 0.00019234]
	Learning Rate: 0.000192339
	LOSS [training: 0.03348789439400354 | validation: 0.02412420640415035]
	TIME [epoch: 9.04 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03350559800701551		[learning rate: 0.00019204]
		[batch 20/20] avg loss: 0.030123395190483588		[learning rate: 0.00019175]
	Learning Rate: 0.000191749
	LOSS [training: 0.031814496598749555 | validation: 0.02278792656339022]
	TIME [epoch: 9.06 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03184816764547423		[learning rate: 0.00019145]
		[batch 20/20] avg loss: 0.042320604351224346		[learning rate: 0.00019116]
	Learning Rate: 0.000191161
	LOSS [training: 0.03708438599834928 | validation: 0.03535383789865304]
	TIME [epoch: 9.04 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037300213500623305		[learning rate: 0.00019087]
		[batch 20/20] avg loss: 0.035565591603542876		[learning rate: 0.00019058]
	Learning Rate: 0.000190575
	LOSS [training: 0.0364329025520831 | validation: 0.022805762180985213]
	TIME [epoch: 9.05 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033129616452583784		[learning rate: 0.00019028]
		[batch 20/20] avg loss: 0.02974381029833854		[learning rate: 0.00018999]
	Learning Rate: 0.000189991
	LOSS [training: 0.03143671337546116 | validation: 0.0331081705950104]
	TIME [epoch: 9.04 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03239571223217054		[learning rate: 0.0001897]
		[batch 20/20] avg loss: 0.03441010097527587		[learning rate: 0.00018941]
	Learning Rate: 0.000189409
	LOSS [training: 0.033402906603723206 | validation: 0.02556615408465624]
	TIME [epoch: 9.04 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03762972263772622		[learning rate: 0.00018912]
		[batch 20/20] avg loss: 0.03561769996387655		[learning rate: 0.00018883]
	Learning Rate: 0.000188828
	LOSS [training: 0.03662371130080137 | validation: 0.025404764114801158]
	TIME [epoch: 9.06 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02356259416249694		[learning rate: 0.00018854]
		[batch 20/20] avg loss: 0.03729918626630374		[learning rate: 0.00018825]
	Learning Rate: 0.000188249
	LOSS [training: 0.03043089021440034 | validation: 0.027939784365532877]
	TIME [epoch: 9.04 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03115126041967488		[learning rate: 0.00018796]
		[batch 20/20] avg loss: 0.029892696785625016		[learning rate: 0.00018767]
	Learning Rate: 0.000187672
	LOSS [training: 0.030521978602649952 | validation: 0.019593985334326294]
	TIME [epoch: 9.04 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03150977090570027		[learning rate: 0.00018738]
		[batch 20/20] avg loss: 0.03584537535252426		[learning rate: 0.0001871]
	Learning Rate: 0.000187097
	LOSS [training: 0.03367757312911226 | validation: 0.025901301406078832]
	TIME [epoch: 9.04 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03784099058901354		[learning rate: 0.00018681]
		[batch 20/20] avg loss: 0.04134492285307555		[learning rate: 0.00018652]
	Learning Rate: 0.000186523
	LOSS [training: 0.039592956721044535 | validation: 0.044705647504491984]
	TIME [epoch: 9.04 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03946112650697797		[learning rate: 0.00018624]
		[batch 20/20] avg loss: 0.03153599417417915		[learning rate: 0.00018595]
	Learning Rate: 0.000185952
	LOSS [training: 0.035498560340578564 | validation: 0.028414080139961814]
	TIME [epoch: 9.06 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039497035903593644		[learning rate: 0.00018567]
		[batch 20/20] avg loss: 0.05194698314872233		[learning rate: 0.00018538]
	Learning Rate: 0.000185382
	LOSS [training: 0.04572200952615799 | validation: 0.03477837237876695]
	TIME [epoch: 9.04 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03606462757749259		[learning rate: 0.0001851]
		[batch 20/20] avg loss: 0.040017126183851844		[learning rate: 0.00018481]
	Learning Rate: 0.000184813
	LOSS [training: 0.03804087688067222 | validation: 0.030507406074308137]
	TIME [epoch: 9.04 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04343106980804519		[learning rate: 0.00018453]
		[batch 20/20] avg loss: 0.032894084250522454		[learning rate: 0.00018425]
	Learning Rate: 0.000184247
	LOSS [training: 0.038162577029283815 | validation: 0.030147271176286322]
	TIME [epoch: 9.04 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03467150283695352		[learning rate: 0.00018396]
		[batch 20/20] avg loss: 0.033189425705372325		[learning rate: 0.00018368]
	Learning Rate: 0.000183682
	LOSS [training: 0.03393046427116292 | validation: 0.021852297214987007]
	TIME [epoch: 9.04 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03761458001397104		[learning rate: 0.0001834]
		[batch 20/20] avg loss: 0.0315384407419998		[learning rate: 0.00018312]
	Learning Rate: 0.000183119
	LOSS [training: 0.034576510377985425 | validation: 0.0210779705979056]
	TIME [epoch: 9.15 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0330524405002274		[learning rate: 0.00018284]
		[batch 20/20] avg loss: 0.03691850392162357		[learning rate: 0.00018256]
	Learning Rate: 0.000182558
	LOSS [training: 0.03498547221092548 | validation: 0.030118125697687657]
	TIME [epoch: 9.04 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035247582500621315		[learning rate: 0.00018228]
		[batch 20/20] avg loss: 0.03701843252256423		[learning rate: 0.000182]
	Learning Rate: 0.000181998
	LOSS [training: 0.03613300751159278 | validation: 0.022366408991691605]
	TIME [epoch: 9.04 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03978308128407466		[learning rate: 0.00018172]
		[batch 20/20] avg loss: 0.028832903797661347		[learning rate: 0.00018144]
	Learning Rate: 0.00018144
	LOSS [training: 0.034307992540868006 | validation: 0.030612241684804767]
	TIME [epoch: 9.04 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030223867870049277		[learning rate: 0.00018116]
		[batch 20/20] avg loss: 0.03315587153398699		[learning rate: 0.00018088]
	Learning Rate: 0.000180884
	LOSS [training: 0.031689869702018134 | validation: 0.025129299833207896]
	TIME [epoch: 9.04 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031896984933161704		[learning rate: 0.00018061]
		[batch 20/20] avg loss: 0.03479388057135999		[learning rate: 0.00018033]
	Learning Rate: 0.000180329
	LOSS [training: 0.033345432752260845 | validation: 0.021821159679911887]
	TIME [epoch: 9.06 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036055528007250745		[learning rate: 0.00018005]
		[batch 20/20] avg loss: 0.03181785667201366		[learning rate: 0.00017978]
	Learning Rate: 0.000179777
	LOSS [training: 0.0339366923396322 | validation: 0.01916785562670426]
	TIME [epoch: 9.04 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02820945613327013		[learning rate: 0.0001795]
		[batch 20/20] avg loss: 0.03044993709007527		[learning rate: 0.00017923]
	Learning Rate: 0.000179226
	LOSS [training: 0.0293296966116727 | validation: 0.03147998056152589]
	TIME [epoch: 9.05 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03195759463542403		[learning rate: 0.00017895]
		[batch 20/20] avg loss: 0.03611075389084292		[learning rate: 0.00017868]
	Learning Rate: 0.000178676
	LOSS [training: 0.03403417426313348 | validation: 0.031607287155587534]
	TIME [epoch: 9.04 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03256925062336384		[learning rate: 0.0001784]
		[batch 20/20] avg loss: 0.04029229462035956		[learning rate: 0.00017813]
	Learning Rate: 0.000178128
	LOSS [training: 0.03643077262186171 | validation: 0.033426402913921675]
	TIME [epoch: 9.04 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036892049935229855		[learning rate: 0.00017786]
		[batch 20/20] avg loss: 0.03561812646897631		[learning rate: 0.00017758]
	Learning Rate: 0.000177582
	LOSS [training: 0.03625508820210308 | validation: 0.0314684384043718]
	TIME [epoch: 9.07 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030471566317000542		[learning rate: 0.00017731]
		[batch 20/20] avg loss: 0.038727693951938695		[learning rate: 0.00017704]
	Learning Rate: 0.000177038
	LOSS [training: 0.034599630134469615 | validation: 0.02395564694368129]
	TIME [epoch: 9.04 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031458189639545844		[learning rate: 0.00017677]
		[batch 20/20] avg loss: 0.03667171155771442		[learning rate: 0.0001765]
	Learning Rate: 0.000176495
	LOSS [training: 0.03406495059863013 | validation: 0.04009996823017974]
	TIME [epoch: 9.05 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036309877175643504		[learning rate: 0.00017622]
		[batch 20/20] avg loss: 0.03240569850742375		[learning rate: 0.00017595]
	Learning Rate: 0.000175954
	LOSS [training: 0.034357787841533635 | validation: 0.032754069370449064]
	TIME [epoch: 9.04 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03052995077553261		[learning rate: 0.00017568]
		[batch 20/20] avg loss: 0.04409856860548336		[learning rate: 0.00017541]
	Learning Rate: 0.000175415
	LOSS [training: 0.037314259690507987 | validation: 0.02704360701345968]
	TIME [epoch: 9.04 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039444745885305596		[learning rate: 0.00017515]
		[batch 20/20] avg loss: 0.034398329133662045		[learning rate: 0.00017488]
	Learning Rate: 0.000174877
	LOSS [training: 0.03692153750948382 | validation: 0.021159007157292516]
	TIME [epoch: 9.06 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04161380734621479		[learning rate: 0.00017461]
		[batch 20/20] avg loss: 0.0327555062095255		[learning rate: 0.00017434]
	Learning Rate: 0.000174341
	LOSS [training: 0.03718465677787015 | validation: 0.014959433352888702]
	TIME [epoch: 9.04 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03417968578809273		[learning rate: 0.00017407]
		[batch 20/20] avg loss: 0.031853298427536134		[learning rate: 0.00017381]
	Learning Rate: 0.000173807
	LOSS [training: 0.033016492107814426 | validation: 0.03177720219031691]
	TIME [epoch: 9.04 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03211685805133118		[learning rate: 0.00017354]
		[batch 20/20] avg loss: 0.03984938239978265		[learning rate: 0.00017327]
	Learning Rate: 0.000173274
	LOSS [training: 0.035983120225556914 | validation: 0.028731694513193087]
	TIME [epoch: 9.04 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035086629932726494		[learning rate: 0.00017301]
		[batch 20/20] avg loss: 0.03748469388387471		[learning rate: 0.00017274]
	Learning Rate: 0.000172743
	LOSS [training: 0.036285661908300605 | validation: 0.03700452860000072]
	TIME [epoch: 9.04 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029663762025731545		[learning rate: 0.00017248]
		[batch 20/20] avg loss: 0.03340324847766514		[learning rate: 0.00017221]
	Learning Rate: 0.000172213
	LOSS [training: 0.031533505251698346 | validation: 0.021734964999702018]
	TIME [epoch: 9.06 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03698983872705987		[learning rate: 0.00017195]
		[batch 20/20] avg loss: 0.03407213107406574		[learning rate: 0.00017169]
	Learning Rate: 0.000171685
	LOSS [training: 0.03553098490056279 | validation: 0.0331981294223235]
	TIME [epoch: 9.04 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03144457471369253		[learning rate: 0.00017142]
		[batch 20/20] avg loss: 0.04072477901485443		[learning rate: 0.00017116]
	Learning Rate: 0.000171159
	LOSS [training: 0.03608467686427348 | validation: 0.022106767946203358]
	TIME [epoch: 9.04 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03725342268084727		[learning rate: 0.0001709]
		[batch 20/20] avg loss: 0.03367757151801723		[learning rate: 0.00017063]
	Learning Rate: 0.000170634
	LOSS [training: 0.035465497099432246 | validation: 0.02487478698286369]
	TIME [epoch: 9.04 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03658305352303641		[learning rate: 0.00017037]
		[batch 20/20] avg loss: 0.0384567313635633		[learning rate: 0.00017011]
	Learning Rate: 0.000170111
	LOSS [training: 0.037519892443299854 | validation: 0.033871073914770664]
	TIME [epoch: 9.04 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0432222097131776		[learning rate: 0.00016985]
		[batch 20/20] avg loss: 0.03523568138128548		[learning rate: 0.00016959]
	Learning Rate: 0.00016959
	LOSS [training: 0.03922894554723153 | validation: 0.03758879353485785]
	TIME [epoch: 9.06 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033474969604005055		[learning rate: 0.00016933]
		[batch 20/20] avg loss: 0.030716954086859392		[learning rate: 0.00016907]
	Learning Rate: 0.00016907
	LOSS [training: 0.03209596184543223 | validation: 0.03342191246404122]
	TIME [epoch: 9.05 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03116153991322147		[learning rate: 0.00016881]
		[batch 20/20] avg loss: 0.02468844859564738		[learning rate: 0.00016855]
	Learning Rate: 0.000168552
	LOSS [training: 0.027924994254434427 | validation: 0.03275161119658507]
	TIME [epoch: 9.04 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03245236484585715		[learning rate: 0.00016829]
		[batch 20/20] avg loss: 0.034134625044403705		[learning rate: 0.00016804]
	Learning Rate: 0.000168035
	LOSS [training: 0.033293494945130425 | validation: 0.0270920773938994]
	TIME [epoch: 9.04 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028796689448977435		[learning rate: 0.00016778]
		[batch 20/20] avg loss: 0.03439185881219189		[learning rate: 0.00016752]
	Learning Rate: 0.00016752
	LOSS [training: 0.03159427413058467 | validation: 0.024781649741344513]
	TIME [epoch: 9.04 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029383594572551536		[learning rate: 0.00016726]
		[batch 20/20] avg loss: 0.0376042169772828		[learning rate: 0.00016701]
	Learning Rate: 0.000167006
	LOSS [training: 0.03349390577491717 | validation: 0.026533266828658825]
	TIME [epoch: 9.06 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034824473536773363		[learning rate: 0.00016675]
		[batch 20/20] avg loss: 0.029574409698515287		[learning rate: 0.00016649]
	Learning Rate: 0.000166495
	LOSS [training: 0.03219944161764431 | validation: 0.022394133005471255]
	TIME [epoch: 9.04 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03601815795080275		[learning rate: 0.00016624]
		[batch 20/20] avg loss: 0.034997216552826185		[learning rate: 0.00016598]
	Learning Rate: 0.000165984
	LOSS [training: 0.03550768725181447 | validation: 0.019624440502710356]
	TIME [epoch: 9.04 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028550102040563186		[learning rate: 0.00016573]
		[batch 20/20] avg loss: 0.03409674230129647		[learning rate: 0.00016548]
	Learning Rate: 0.000165475
	LOSS [training: 0.03132342217092983 | validation: 0.023117241111027302]
	TIME [epoch: 9.04 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03272578586543505		[learning rate: 0.00016522]
		[batch 20/20] avg loss: 0.030751045907064463		[learning rate: 0.00016497]
	Learning Rate: 0.000164968
	LOSS [training: 0.03173841588624975 | validation: 0.024803013995499827]
	TIME [epoch: 9.04 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02861994240914566		[learning rate: 0.00016472]
		[batch 20/20] avg loss: 0.0403678338784816		[learning rate: 0.00016446]
	Learning Rate: 0.000164462
	LOSS [training: 0.03449388814381363 | validation: 0.04161899287209882]
	TIME [epoch: 9.07 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03584644295606647		[learning rate: 0.00016421]
		[batch 20/20] avg loss: 0.038258071898970125		[learning rate: 0.00016396]
	Learning Rate: 0.000163958
	LOSS [training: 0.037052257427518304 | validation: 0.02443715392472896]
	TIME [epoch: 9.04 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029675837893641456		[learning rate: 0.00016371]
		[batch 20/20] avg loss: 0.035133703829327026		[learning rate: 0.00016346]
	Learning Rate: 0.000163456
	LOSS [training: 0.03240477086148424 | validation: 0.025006546837093063]
	TIME [epoch: 9.04 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025661906820212254		[learning rate: 0.0001632]
		[batch 20/20] avg loss: 0.03345206970672515		[learning rate: 0.00016295]
	Learning Rate: 0.000162955
	LOSS [training: 0.029556988263468705 | validation: 0.026844495230880373]
	TIME [epoch: 9.04 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03358628964489539		[learning rate: 0.0001627]
		[batch 20/20] avg loss: 0.030672018362155256		[learning rate: 0.00016246]
	Learning Rate: 0.000162455
	LOSS [training: 0.032129154003525326 | validation: 0.029325541318732797]
	TIME [epoch: 9.05 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03732338244779883		[learning rate: 0.00016221]
		[batch 20/20] avg loss: 0.03816025773462106		[learning rate: 0.00016196]
	Learning Rate: 0.000161957
	LOSS [training: 0.03774182009120995 | validation: 0.03573652595895796]
	TIME [epoch: 9.07 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03530581625782128		[learning rate: 0.00016171]
		[batch 20/20] avg loss: 0.03270590861962393		[learning rate: 0.00016146]
	Learning Rate: 0.000161461
	LOSS [training: 0.03400586243872261 | validation: 0.032241157223472885]
	TIME [epoch: 9.11 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031716324379862125		[learning rate: 0.00016121]
		[batch 20/20] avg loss: 0.03495372274736942		[learning rate: 0.00016097]
	Learning Rate: 0.000160966
	LOSS [training: 0.03333502356361577 | validation: 0.023027160872981345]
	TIME [epoch: 9.05 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03510929412537959		[learning rate: 0.00016072]
		[batch 20/20] avg loss: 0.039315895507964785		[learning rate: 0.00016047]
	Learning Rate: 0.000160472
	LOSS [training: 0.03721259481667218 | validation: 0.03292025078643941]
	TIME [epoch: 9.04 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04037930326255539		[learning rate: 0.00016023]
		[batch 20/20] avg loss: 0.04141462899513296		[learning rate: 0.00015998]
	Learning Rate: 0.00015998
	LOSS [training: 0.04089696612884418 | validation: 0.037540601908178296]
	TIME [epoch: 9.04 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03831821728635728		[learning rate: 0.00015973]
		[batch 20/20] avg loss: 0.03607332672561926		[learning rate: 0.00015949]
	Learning Rate: 0.00015949
	LOSS [training: 0.03719577200598827 | validation: 0.03371025725211127]
	TIME [epoch: 9.07 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040790352851838796		[learning rate: 0.00015925]
		[batch 20/20] avg loss: 0.042308291763574826		[learning rate: 0.000159]
	Learning Rate: 0.000159001
	LOSS [training: 0.041549322307706804 | validation: 0.030175817159230796]
	TIME [epoch: 9.04 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03760725338924271		[learning rate: 0.00015876]
		[batch 20/20] avg loss: 0.04212127949433218		[learning rate: 0.00015851]
	Learning Rate: 0.000158514
	LOSS [training: 0.039864266441787435 | validation: 0.041180751207504196]
	TIME [epoch: 9.04 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04339125106813494		[learning rate: 0.00015827]
		[batch 20/20] avg loss: 0.04602457461383193		[learning rate: 0.00015803]
	Learning Rate: 0.000158028
	LOSS [training: 0.04470791284098343 | validation: 0.027010569392534295]
	TIME [epoch: 9.04 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04500562519841519		[learning rate: 0.00015779]
		[batch 20/20] avg loss: 0.030334312174690552		[learning rate: 0.00015754]
	Learning Rate: 0.000157543
	LOSS [training: 0.03766996868655288 | validation: 0.03156107660954279]
	TIME [epoch: 9.04 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03480565857308569		[learning rate: 0.0001573]
		[batch 20/20] avg loss: 0.03758169121888753		[learning rate: 0.00015706]
	Learning Rate: 0.00015706
	LOSS [training: 0.03619367489598661 | validation: 0.02716804480885711]
	TIME [epoch: 9.06 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03597506846034662		[learning rate: 0.00015682]
		[batch 20/20] avg loss: 0.04025680162312393		[learning rate: 0.00015658]
	Learning Rate: 0.000156579
	LOSS [training: 0.03811593504173528 | validation: 0.030918519069570296]
	TIME [epoch: 9.05 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03266567511539257		[learning rate: 0.00015634]
		[batch 20/20] avg loss: 0.02977196951617278		[learning rate: 0.0001561]
	Learning Rate: 0.000156099
	LOSS [training: 0.031218822315782675 | validation: 0.024228376448836256]
	TIME [epoch: 9.05 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036954456640351935		[learning rate: 0.00015586]
		[batch 20/20] avg loss: 0.02363302681423278		[learning rate: 0.00015562]
	Learning Rate: 0.00015562
	LOSS [training: 0.030293741727292357 | validation: 0.024554689023935882]
	TIME [epoch: 9.04 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029884475605566962		[learning rate: 0.00015538]
		[batch 20/20] avg loss: 0.026385013166266635		[learning rate: 0.00015514]
	Learning Rate: 0.000155143
	LOSS [training: 0.028134744385916798 | validation: 0.01862410480361794]
	TIME [epoch: 9.05 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038677315581991896		[learning rate: 0.00015491]
		[batch 20/20] avg loss: 0.03825004159617884		[learning rate: 0.00015467]
	Learning Rate: 0.000154668
	LOSS [training: 0.03846367858908537 | validation: 0.023306872481729524]
	TIME [epoch: 9.06 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033362818514557094		[learning rate: 0.00015443]
		[batch 20/20] avg loss: 0.03332264307333521		[learning rate: 0.00015419]
	Learning Rate: 0.000154194
	LOSS [training: 0.03334273079394615 | validation: 0.014269250799352194]
	TIME [epoch: 9.04 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03591201780800629		[learning rate: 0.00015396]
		[batch 20/20] avg loss: 0.02826417423808223		[learning rate: 0.00015372]
	Learning Rate: 0.000153721
	LOSS [training: 0.03208809602304426 | validation: 0.02860122632247153]
	TIME [epoch: 9.04 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027342014971977458		[learning rate: 0.00015349]
		[batch 20/20] avg loss: 0.03394245899257327		[learning rate: 0.00015325]
	Learning Rate: 0.00015325
	LOSS [training: 0.03064223698227537 | validation: 0.026760465912995288]
	TIME [epoch: 9.05 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032230659714702896		[learning rate: 0.00015301]
		[batch 20/20] avg loss: 0.03214481272607535		[learning rate: 0.00015278]
	Learning Rate: 0.00015278
	LOSS [training: 0.03218773622038913 | validation: 0.01951991269312784]
	TIME [epoch: 9.04 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03383256351540488		[learning rate: 0.00015255]
		[batch 20/20] avg loss: 0.03394710740805795		[learning rate: 0.00015231]
	Learning Rate: 0.000152312
	LOSS [training: 0.03388983546173142 | validation: 0.02394103316964101]
	TIME [epoch: 9.07 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04190659391521882		[learning rate: 0.00015208]
		[batch 20/20] avg loss: 0.03284990559629388		[learning rate: 0.00015184]
	Learning Rate: 0.000151845
	LOSS [training: 0.037378249755756356 | validation: 0.019719300866565655]
	TIME [epoch: 9.05 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03301353628646918		[learning rate: 0.00015161]
		[batch 20/20] avg loss: 0.03141450366091063		[learning rate: 0.00015138]
	Learning Rate: 0.000151379
	LOSS [training: 0.0322140199736899 | validation: 0.030012433016451803]
	TIME [epoch: 9.04 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03131660466934707		[learning rate: 0.00015115]
		[batch 20/20] avg loss: 0.034324145309413696		[learning rate: 0.00015092]
	Learning Rate: 0.000150915
	LOSS [training: 0.03282037498938038 | validation: 0.0307369211769404]
	TIME [epoch: 9.04 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03178234807937323		[learning rate: 0.00015068]
		[batch 20/20] avg loss: 0.03792985724320053		[learning rate: 0.00015045]
	Learning Rate: 0.000150453
	LOSS [training: 0.03485610266128688 | validation: 0.025559380185939826]
	TIME [epoch: 9.04 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02995036446387901		[learning rate: 0.00015022]
		[batch 20/20] avg loss: 0.031212425468633058		[learning rate: 0.00014999]
	Learning Rate: 0.000149991
	LOSS [training: 0.03058139496625604 | validation: 0.02559511494707696]
	TIME [epoch: 9.08 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0358412252432917		[learning rate: 0.00014976]
		[batch 20/20] avg loss: 0.03490379341478039		[learning rate: 0.00014953]
	Learning Rate: 0.000149532
	LOSS [training: 0.03537250932903604 | validation: 0.026845869360387208]
	TIME [epoch: 9.04 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03418645283826788		[learning rate: 0.0001493]
		[batch 20/20] avg loss: 0.029973863004374646		[learning rate: 0.00014907]
	Learning Rate: 0.000149073
	LOSS [training: 0.03208015792132126 | validation: 0.02365164071121773]
	TIME [epoch: 9.04 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03132365886204117		[learning rate: 0.00014884]
		[batch 20/20] avg loss: 0.035515241573900416		[learning rate: 0.00014862]
	Learning Rate: 0.000148616
	LOSS [training: 0.03341945021797079 | validation: 0.03173254143460308]
	TIME [epoch: 9.04 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035217486782721456		[learning rate: 0.00014839]
		[batch 20/20] avg loss: 0.034517179479718295		[learning rate: 0.00014816]
	Learning Rate: 0.000148161
	LOSS [training: 0.03486733313121988 | validation: 0.034174570985873225]
	TIME [epoch: 9.03 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028534574219054375		[learning rate: 0.00014793]
		[batch 20/20] avg loss: 0.035757400518719853		[learning rate: 0.00014771]
	Learning Rate: 0.000147707
	LOSS [training: 0.03214598736888711 | validation: 0.031377707472493604]
	TIME [epoch: 9.06 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038693104600890524		[learning rate: 0.00014748]
		[batch 20/20] avg loss: 0.03195725492851378		[learning rate: 0.00014725]
	Learning Rate: 0.000147254
	LOSS [training: 0.03532517976470216 | validation: 0.03764170838948437]
	TIME [epoch: 9.04 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04195946770827229		[learning rate: 0.00014703]
		[batch 20/20] avg loss: 0.036939272752018554		[learning rate: 0.0001468]
	Learning Rate: 0.000146802
	LOSS [training: 0.03944937023014543 | validation: 0.01999335400991391]
	TIME [epoch: 9.04 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02753541204813286		[learning rate: 0.00014658]
		[batch 20/20] avg loss: 0.03074033513413494		[learning rate: 0.00014635]
	Learning Rate: 0.000146352
	LOSS [training: 0.0291378735911339 | validation: 0.025703713415613998]
	TIME [epoch: 9.04 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030122106264021876		[learning rate: 0.00014613]
		[batch 20/20] avg loss: 0.039761479418686616		[learning rate: 0.0001459]
	Learning Rate: 0.000145904
	LOSS [training: 0.03494179284135425 | validation: 0.031213285082611644]
	TIME [epoch: 9.04 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040583870381120817		[learning rate: 0.00014568]
		[batch 20/20] avg loss: 0.030730367526247443		[learning rate: 0.00014546]
	Learning Rate: 0.000145457
	LOSS [training: 0.03565711895368413 | validation: 0.028930599034495717]
	TIME [epoch: 9.07 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039360669958366366		[learning rate: 0.00014523]
		[batch 20/20] avg loss: 0.031811937295341465		[learning rate: 0.00014501]
	Learning Rate: 0.000145011
	LOSS [training: 0.03558630362685391 | validation: 0.02487516915971743]
	TIME [epoch: 9.04 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03415087183845742		[learning rate: 0.00014479]
		[batch 20/20] avg loss: 0.03066859338524718		[learning rate: 0.00014457]
	Learning Rate: 0.000144566
	LOSS [training: 0.0324097326118523 | validation: 0.02502490428143729]
	TIME [epoch: 9.04 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028122684983874796		[learning rate: 0.00014434]
		[batch 20/20] avg loss: 0.03459922337243597		[learning rate: 0.00014412]
	Learning Rate: 0.000144123
	LOSS [training: 0.03136095417815538 | validation: 0.02054775453360992]
	TIME [epoch: 9.04 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03973699809871638		[learning rate: 0.0001439]
		[batch 20/20] avg loss: 0.028432337194126238		[learning rate: 0.00014368]
	Learning Rate: 0.000143681
	LOSS [training: 0.034084667646421304 | validation: 0.025625912850205157]
	TIME [epoch: 9.04 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033617013920624356		[learning rate: 0.00014346]
		[batch 20/20] avg loss: 0.03695695879229049		[learning rate: 0.00014324]
	Learning Rate: 0.000143241
	LOSS [training: 0.035286986356457425 | validation: 0.02548210230787521]
	TIME [epoch: 9.07 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032395854109281426		[learning rate: 0.00014302]
		[batch 20/20] avg loss: 0.03157083237029234		[learning rate: 0.0001428]
	Learning Rate: 0.000142802
	LOSS [training: 0.031983343239786875 | validation: 0.026421978818122235]
	TIME [epoch: 9.04 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02857387124318919		[learning rate: 0.00014258]
		[batch 20/20] avg loss: 0.03603302056691083		[learning rate: 0.00014236]
	Learning Rate: 0.000142364
	LOSS [training: 0.03230344590505001 | validation: 0.026092893217979585]
	TIME [epoch: 9.04 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03154391777271111		[learning rate: 0.00014215]
		[batch 20/20] avg loss: 0.04036100250959028		[learning rate: 0.00014193]
	Learning Rate: 0.000141928
	LOSS [training: 0.035952460141150694 | validation: 0.041528714613798194]
	TIME [epoch: 9.04 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034012693924577425		[learning rate: 0.00014171]
		[batch 20/20] avg loss: 0.040425471380097625		[learning rate: 0.00014149]
	Learning Rate: 0.000141492
	LOSS [training: 0.03721908265233752 | validation: 0.0324200783289466]
	TIME [epoch: 9.04 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03821194368952906		[learning rate: 0.00014128]
		[batch 20/20] avg loss: 0.02961335372017678		[learning rate: 0.00014106]
	Learning Rate: 0.000141059
	LOSS [training: 0.03391264870485293 | validation: 0.02849572358169977]
	TIME [epoch: 9.06 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03306377541987142		[learning rate: 0.00014084]
		[batch 20/20] avg loss: 0.029436863585297124		[learning rate: 0.00014063]
	Learning Rate: 0.000140626
	LOSS [training: 0.03125031950258427 | validation: 0.021735945301160686]
	TIME [epoch: 9.04 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038924022187894		[learning rate: 0.00014041]
		[batch 20/20] avg loss: 0.028676060819451864		[learning rate: 0.0001402]
	Learning Rate: 0.000140195
	LOSS [training: 0.03380004150367293 | validation: 0.028745562441360572]
	TIME [epoch: 9.04 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03549586802608958		[learning rate: 0.00013998]
		[batch 20/20] avg loss: 0.03208315600535498		[learning rate: 0.00013977]
	Learning Rate: 0.000139765
	LOSS [training: 0.033789512015722285 | validation: 0.023636410478153055]
	TIME [epoch: 9.04 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03509669979093056		[learning rate: 0.00013955]
		[batch 20/20] avg loss: 0.03076831018876821		[learning rate: 0.00013934]
	Learning Rate: 0.000139337
	LOSS [training: 0.032932504989849384 | validation: 0.02045551367485077]
	TIME [epoch: 9.04 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03173216534324007		[learning rate: 0.00013912]
		[batch 20/20] avg loss: 0.03561615191820493		[learning rate: 0.00013891]
	Learning Rate: 0.00013891
	LOSS [training: 0.0336741586307225 | validation: 0.024061592148050346]
	TIME [epoch: 9.06 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030470051266805947		[learning rate: 0.0001387]
		[batch 20/20] avg loss: 0.03512525485765083		[learning rate: 0.00013848]
	Learning Rate: 0.000138484
	LOSS [training: 0.03279765306222839 | validation: 0.023379315995976122]
	TIME [epoch: 9.05 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03264351767915178		[learning rate: 0.00013827]
		[batch 20/20] avg loss: 0.030147579840269924		[learning rate: 0.00013806]
	Learning Rate: 0.00013806
	LOSS [training: 0.03139554875971085 | validation: 0.02762909494065317]
	TIME [epoch: 9.04 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0395937804208337		[learning rate: 0.00013785]
		[batch 20/20] avg loss: 0.0302005820045943		[learning rate: 0.00013764]
	Learning Rate: 0.000137636
	LOSS [training: 0.034897181212714 | validation: 0.03210461302909531]
	TIME [epoch: 9.04 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03043403509685766		[learning rate: 0.00013743]
		[batch 20/20] avg loss: 0.03491699189088284		[learning rate: 0.00013721]
	Learning Rate: 0.000137215
	LOSS [training: 0.032675513493870254 | validation: 0.02347365866597805]
	TIME [epoch: 9.04 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033115886491305015		[learning rate: 0.000137]
		[batch 20/20] avg loss: 0.033709379276719534		[learning rate: 0.00013679]
	Learning Rate: 0.000136794
	LOSS [training: 0.033412632884012264 | validation: 0.032087961360176484]
	TIME [epoch: 9.06 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03625991900309028		[learning rate: 0.00013658]
		[batch 20/20] avg loss: 0.034168712142172465		[learning rate: 0.00013637]
	Learning Rate: 0.000136375
	LOSS [training: 0.03521431557263137 | validation: 0.020502415976878092]
	TIME [epoch: 9.04 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035230082391619795		[learning rate: 0.00013617]
		[batch 20/20] avg loss: 0.02859098239721685		[learning rate: 0.00013596]
	Learning Rate: 0.000135956
	LOSS [training: 0.03191053239441832 | validation: 0.02357111120188231]
	TIME [epoch: 9.04 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026866340074021944		[learning rate: 0.00013575]
		[batch 20/20] avg loss: 0.03891577646114662		[learning rate: 0.00013554]
	Learning Rate: 0.00013554
	LOSS [training: 0.03289105826758428 | validation: 0.02487943114618711]
	TIME [epoch: 9.05 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041442385113636286		[learning rate: 0.00013533]
		[batch 20/20] avg loss: 0.03856132596128044		[learning rate: 0.00013512]
	Learning Rate: 0.000135124
	LOSS [training: 0.040001855537458365 | validation: 0.01735238253453745]
	TIME [epoch: 9.05 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03202752881130216		[learning rate: 0.00013492]
		[batch 20/20] avg loss: 0.03894309839993052		[learning rate: 0.00013471]
	Learning Rate: 0.00013471
	LOSS [training: 0.03548531360561634 | validation: 0.021970329685223996]
	TIME [epoch: 9.06 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03180938012579074		[learning rate: 0.0001345]
		[batch 20/20] avg loss: 0.027909571703986707		[learning rate: 0.0001343]
	Learning Rate: 0.000134297
	LOSS [training: 0.029859475914888724 | validation: 0.020425215214839962]
	TIME [epoch: 9.05 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03977320471576175		[learning rate: 0.00013409]
		[batch 20/20] avg loss: 0.030204896403177917		[learning rate: 0.00013389]
	Learning Rate: 0.000133885
	LOSS [training: 0.03498905055946984 | validation: 0.038352140460867885]
	TIME [epoch: 9.05 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03661495695162145		[learning rate: 0.00013368]
		[batch 20/20] avg loss: 0.04105181658999136		[learning rate: 0.00013347]
	Learning Rate: 0.000133475
	LOSS [training: 0.0388333867708064 | validation: 0.017754121153987987]
	TIME [epoch: 9.04 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0304986248782884		[learning rate: 0.00013327]
		[batch 20/20] avg loss: 0.03794293029063621		[learning rate: 0.00013307]
	Learning Rate: 0.000133066
	LOSS [training: 0.034220777584462306 | validation: 0.018682590079030885]
	TIME [epoch: 9.05 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03250945790554795		[learning rate: 0.00013286]
		[batch 20/20] avg loss: 0.03761788492951419		[learning rate: 0.00013266]
	Learning Rate: 0.000132658
	LOSS [training: 0.03506367141753107 | validation: 0.0221994583203677]
	TIME [epoch: 9.06 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03234745645387372		[learning rate: 0.00013245]
		[batch 20/20] avg loss: 0.036579450833802804		[learning rate: 0.00013225]
	Learning Rate: 0.000132251
	LOSS [training: 0.03446345364383825 | validation: 0.03248965380531138]
	TIME [epoch: 9.05 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027002422168748003		[learning rate: 0.00013205]
		[batch 20/20] avg loss: 0.034010100351503		[learning rate: 0.00013185]
	Learning Rate: 0.000131846
	LOSS [training: 0.0305062612601255 | validation: 0.028113842942489458]
	TIME [epoch: 9.04 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037392837945444825		[learning rate: 0.00013164]
		[batch 20/20] avg loss: 0.03409653475262002		[learning rate: 0.00013144]
	Learning Rate: 0.000131442
	LOSS [training: 0.03574468634903242 | validation: 0.03754840344595555]
	TIME [epoch: 9.04 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0353535319790587		[learning rate: 0.00013124]
		[batch 20/20] avg loss: 0.03376836808344138		[learning rate: 0.00013104]
	Learning Rate: 0.000131039
	LOSS [training: 0.034560950031250044 | validation: 0.030489062039204085]
	TIME [epoch: 9.04 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029767565437729755		[learning rate: 0.00013084]
		[batch 20/20] avg loss: 0.03142587938184071		[learning rate: 0.00013064]
	Learning Rate: 0.000130637
	LOSS [training: 0.030596722409785237 | validation: 0.0381765843844788]
	TIME [epoch: 9.06 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03523108403806878		[learning rate: 0.00013044]
		[batch 20/20] avg loss: 0.029610296952495074		[learning rate: 0.00013024]
	Learning Rate: 0.000130237
	LOSS [training: 0.03242069049528193 | validation: 0.031225133369124726]
	TIME [epoch: 9.05 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03156147351551464		[learning rate: 0.00013004]
		[batch 20/20] avg loss: 0.03186860462656012		[learning rate: 0.00012984]
	Learning Rate: 0.000129837
	LOSS [training: 0.031715039071037385 | validation: 0.024336675938160295]
	TIME [epoch: 9.04 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04412709964936937		[learning rate: 0.00012964]
		[batch 20/20] avg loss: 0.03361955789682557		[learning rate: 0.00012944]
	Learning Rate: 0.000129439
	LOSS [training: 0.03887332877309747 | validation: 0.0228334778249416]
	TIME [epoch: 9.04 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03793277217676437		[learning rate: 0.00012924]
		[batch 20/20] avg loss: 0.033021785406628396		[learning rate: 0.00012904]
	Learning Rate: 0.000129043
	LOSS [training: 0.03547727879169639 | validation: 0.019053470321475113]
	TIME [epoch: 9.04 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03448904427785156		[learning rate: 0.00012884]
		[batch 20/20] avg loss: 0.030802113585474446		[learning rate: 0.00012865]
	Learning Rate: 0.000128647
	LOSS [training: 0.032645578931663 | validation: 0.01857870819584108]
	TIME [epoch: 9.06 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03459607575000892		[learning rate: 0.00012845]
		[batch 20/20] avg loss: 0.02863749027471213		[learning rate: 0.00012825]
	Learning Rate: 0.000128253
	LOSS [training: 0.031616783012360525 | validation: 0.02802799850953728]
	TIME [epoch: 9.05 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034790251432639675		[learning rate: 0.00012806]
		[batch 20/20] avg loss: 0.03737367692629782		[learning rate: 0.00012786]
	Learning Rate: 0.00012786
	LOSS [training: 0.03608196417946875 | validation: 0.023328655736115023]
	TIME [epoch: 9.04 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03065218603766815		[learning rate: 0.00012766]
		[batch 20/20] avg loss: 0.02966960014600507		[learning rate: 0.00012747]
	Learning Rate: 0.000127468
	LOSS [training: 0.03016089309183661 | validation: 0.028444809112435847]
	TIME [epoch: 9.03 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03235485940193064		[learning rate: 0.00012727]
		[batch 20/20] avg loss: 0.04106564567718885		[learning rate: 0.00012708]
	Learning Rate: 0.000127077
	LOSS [training: 0.03671025253955974 | validation: 0.031966959049178664]
	TIME [epoch: 9.04 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04198657106927749		[learning rate: 0.00012688]
		[batch 20/20] avg loss: 0.02689658402441223		[learning rate: 0.00012669]
	Learning Rate: 0.000126687
	LOSS [training: 0.034441577546844855 | validation: 0.020384218135789153]
	TIME [epoch: 9.06 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03001262479009139		[learning rate: 0.00012649]
		[batch 20/20] avg loss: 0.031150319278768223		[learning rate: 0.0001263]
	Learning Rate: 0.000126299
	LOSS [training: 0.030581472034429808 | validation: 0.026214438026416553]
	TIME [epoch: 9.04 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035232851690518055		[learning rate: 0.00012611]
		[batch 20/20] avg loss: 0.0350976367290503		[learning rate: 0.00012591]
	Learning Rate: 0.000125912
	LOSS [training: 0.03516524420978418 | validation: 0.03107407291639756]
	TIME [epoch: 9.04 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03573184814579862		[learning rate: 0.00012572]
		[batch 20/20] avg loss: 0.026984413323057503		[learning rate: 0.00012553]
	Learning Rate: 0.000125526
	LOSS [training: 0.03135813073442807 | validation: 0.025706322670648294]
	TIME [epoch: 9.04 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03230835721598386		[learning rate: 0.00012533]
		[batch 20/20] avg loss: 0.030003696216180882		[learning rate: 0.00012514]
	Learning Rate: 0.000125141
	LOSS [training: 0.03115602671608237 | validation: 0.031142810149208954]
	TIME [epoch: 9.04 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031231992512711686		[learning rate: 0.00012495]
		[batch 20/20] avg loss: 0.034881763455974754		[learning rate: 0.00012476]
	Learning Rate: 0.000124757
	LOSS [training: 0.033056877984343216 | validation: 0.03304442933363928]
	TIME [epoch: 9.06 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029334050971185723		[learning rate: 0.00012457]
		[batch 20/20] avg loss: 0.034487055498073946		[learning rate: 0.00012438]
	Learning Rate: 0.000124375
	LOSS [training: 0.03191055323462984 | validation: 0.027451499015389305]
	TIME [epoch: 9.04 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036839888714426464		[learning rate: 0.00012418]
		[batch 20/20] avg loss: 0.03134046896298372		[learning rate: 0.00012399]
	Learning Rate: 0.000123994
	LOSS [training: 0.03409017883870509 | validation: 0.020090801465234055]
	TIME [epoch: 9.04 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026563767999593847		[learning rate: 0.0001238]
		[batch 20/20] avg loss: 0.027590533348693096		[learning rate: 0.00012361]
	Learning Rate: 0.000123614
	LOSS [training: 0.027077150674143468 | validation: 0.019666578620151073]
	TIME [epoch: 9.04 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03520759659462465		[learning rate: 0.00012342]
		[batch 20/20] avg loss: 0.027871546041973207		[learning rate: 0.00012323]
	Learning Rate: 0.000123235
	LOSS [training: 0.031539571318298924 | validation: 0.02367158813486077]
	TIME [epoch: 9.04 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03188180448936693		[learning rate: 0.00012305]
		[batch 20/20] avg loss: 0.03384582107591001		[learning rate: 0.00012286]
	Learning Rate: 0.000122857
	LOSS [training: 0.03286381278263846 | validation: 0.029384274102979575]
	TIME [epoch: 9.05 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036197734009961215		[learning rate: 0.00012267]
		[batch 20/20] avg loss: 0.02377896657795279		[learning rate: 0.00012248]
	Learning Rate: 0.00012248
	LOSS [training: 0.029988350293957 | validation: 0.03036781265310782]
	TIME [epoch: 9.05 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030950724986393875		[learning rate: 0.00012229]
		[batch 20/20] avg loss: 0.03682768352189632		[learning rate: 0.0001221]
	Learning Rate: 0.000122105
	LOSS [training: 0.0338892042541451 | validation: 0.028376934890539295]
	TIME [epoch: 9.04 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033935914099670866		[learning rate: 0.00012192]
		[batch 20/20] avg loss: 0.042558298041465804		[learning rate: 0.00012173]
	Learning Rate: 0.000121731
	LOSS [training: 0.03824710607056835 | validation: 0.029782907199239754]
	TIME [epoch: 9.03 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02957327817973916		[learning rate: 0.00012154]
		[batch 20/20] avg loss: 0.03524448604313958		[learning rate: 0.00012136]
	Learning Rate: 0.000121357
	LOSS [training: 0.03240888211143937 | validation: 0.019098227191070287]
	TIME [epoch: 9.04 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02858882543012529		[learning rate: 0.00012117]
		[batch 20/20] avg loss: 0.032838335137372096		[learning rate: 0.00012099]
	Learning Rate: 0.000120985
	LOSS [training: 0.030713580283748692 | validation: 0.028471217450935886]
	TIME [epoch: 9.06 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030966049257224293		[learning rate: 0.0001208]
		[batch 20/20] avg loss: 0.02724804841986574		[learning rate: 0.00012061]
	Learning Rate: 0.000120615
	LOSS [training: 0.029107048838545018 | validation: 0.02718947362086224]
	TIME [epoch: 9.04 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029922307112917296		[learning rate: 0.00012043]
		[batch 20/20] avg loss: 0.03222509651250921		[learning rate: 0.00012024]
	Learning Rate: 0.000120245
	LOSS [training: 0.03107370181271325 | validation: 0.024657890976062944]
	TIME [epoch: 9.03 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03353850046881542		[learning rate: 0.00012006]
		[batch 20/20] avg loss: 0.031464691194891085		[learning rate: 0.00011988]
	Learning Rate: 0.000119876
	LOSS [training: 0.03250159583185325 | validation: 0.022236527924812274]
	TIME [epoch: 9.04 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030068232672679017		[learning rate: 0.00011969]
		[batch 20/20] avg loss: 0.03391921862963457		[learning rate: 0.00011951]
	Learning Rate: 0.000119509
	LOSS [training: 0.0319937256511568 | validation: 0.023994995914789964]
	TIME [epoch: 9.04 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0309802174739613		[learning rate: 0.00011933]
		[batch 20/20] avg loss: 0.03204540564737861		[learning rate: 0.00011914]
	Learning Rate: 0.000119142
	LOSS [training: 0.031512811560669964 | validation: 0.02350235632402023]
	TIME [epoch: 9.05 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04028841196089181		[learning rate: 0.00011896]
		[batch 20/20] avg loss: 0.031119342650331182		[learning rate: 0.00011878]
	Learning Rate: 0.000118777
	LOSS [training: 0.035703877305611496 | validation: 0.022320429452613835]
	TIME [epoch: 9.04 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032845695811610905		[learning rate: 0.0001186]
		[batch 20/20] avg loss: 0.02952520490956368		[learning rate: 0.00011841]
	Learning Rate: 0.000118413
	LOSS [training: 0.031185450360587296 | validation: 0.026507078861807053]
	TIME [epoch: 9.04 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025114530035666084		[learning rate: 0.00011823]
		[batch 20/20] avg loss: 0.0328308491209776		[learning rate: 0.00011805]
	Learning Rate: 0.00011805
	LOSS [training: 0.02897268957832184 | validation: 0.023888424573773184]
	TIME [epoch: 9.04 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029615842935469167		[learning rate: 0.00011787]
		[batch 20/20] avg loss: 0.032471146142722604		[learning rate: 0.00011769]
	Learning Rate: 0.000117688
	LOSS [training: 0.031043494539095884 | validation: 0.024318343480878087]
	TIME [epoch: 9.03 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026582030626118237		[learning rate: 0.00011751]
		[batch 20/20] avg loss: 0.03732171341032926		[learning rate: 0.00011733]
	Learning Rate: 0.000117328
	LOSS [training: 0.031951872018223755 | validation: 0.02474056408326372]
	TIME [epoch: 9.06 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03249537412056791		[learning rate: 0.00011715]
		[batch 20/20] avg loss: 0.029127033091960318		[learning rate: 0.00011697]
	Learning Rate: 0.000116968
	LOSS [training: 0.030811203606264114 | validation: 0.016062693121063905]
	TIME [epoch: 9.05 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028433699226596713		[learning rate: 0.00011679]
		[batch 20/20] avg loss: 0.03202218153508751		[learning rate: 0.00011661]
	Learning Rate: 0.000116609
	LOSS [training: 0.030227940380842123 | validation: 0.03446104573792656]
	TIME [epoch: 9.04 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039188317325868485		[learning rate: 0.00011643]
		[batch 20/20] avg loss: 0.03348895138098085		[learning rate: 0.00011625]
	Learning Rate: 0.000116252
	LOSS [training: 0.036338634353424666 | validation: 0.03335177557391107]
	TIME [epoch: 9.03 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029369621537553244		[learning rate: 0.00011607]
		[batch 20/20] avg loss: 0.033689249542365525		[learning rate: 0.0001159]
	Learning Rate: 0.000115896
	LOSS [training: 0.03152943553995938 | validation: 0.03614768607014217]
	TIME [epoch: 9.04 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03317528012527227		[learning rate: 0.00011572]
		[batch 20/20] avg loss: 0.028266221041164774		[learning rate: 0.00011554]
	Learning Rate: 0.00011554
	LOSS [training: 0.03072075058321852 | validation: 0.026191849465827782]
	TIME [epoch: 9.05 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020657711989710448		[learning rate: 0.00011536]
		[batch 20/20] avg loss: 0.0320847920114145		[learning rate: 0.00011519]
	Learning Rate: 0.000115186
	LOSS [training: 0.02637125200056248 | validation: 0.030010985544215107]
	TIME [epoch: 9.04 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032185410886926044		[learning rate: 0.00011501]
		[batch 20/20] avg loss: 0.032364566331214546		[learning rate: 0.00011483]
	Learning Rate: 0.000114833
	LOSS [training: 0.032274988609070285 | validation: 0.025379886277392637]
	TIME [epoch: 9.04 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029253650380294937		[learning rate: 0.00011466]
		[batch 20/20] avg loss: 0.03990020804292793		[learning rate: 0.00011448]
	Learning Rate: 0.000114481
	LOSS [training: 0.03457692921161143 | validation: 0.03092550645419814]
	TIME [epoch: 9.04 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03041652633637546		[learning rate: 0.00011431]
		[batch 20/20] avg loss: 0.029761061889324886		[learning rate: 0.00011413]
	Learning Rate: 0.00011413
	LOSS [training: 0.030088794112850176 | validation: 0.023504868978658902]
	TIME [epoch: 9.04 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03165668231447631		[learning rate: 0.00011395]
		[batch 20/20] avg loss: 0.03016938885978799		[learning rate: 0.00011378]
	Learning Rate: 0.00011378
	LOSS [training: 0.030913035587132143 | validation: 0.025593274055999114]
	TIME [epoch: 9.06 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029680580647072817		[learning rate: 0.00011361]
		[batch 20/20] avg loss: 0.031027694199451776		[learning rate: 0.00011343]
	Learning Rate: 0.000113431
	LOSS [training: 0.030354137423262297 | validation: 0.024894398547136515]
	TIME [epoch: 9.05 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033993767855218925		[learning rate: 0.00011326]
		[batch 20/20] avg loss: 0.02734988554005762		[learning rate: 0.00011308]
	Learning Rate: 0.000113084
	LOSS [training: 0.030671826697638273 | validation: 0.030841464567534723]
	TIME [epoch: 9.04 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02785173555969441		[learning rate: 0.00011291]
		[batch 20/20] avg loss: 0.036849689479947934		[learning rate: 0.00011274]
	Learning Rate: 0.000112737
	LOSS [training: 0.032350712519821176 | validation: 0.021548733828024143]
	TIME [epoch: 9.04 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03112533745073922		[learning rate: 0.00011256]
		[batch 20/20] avg loss: 0.03420498236762674		[learning rate: 0.00011239]
	Learning Rate: 0.000112391
	LOSS [training: 0.03266515990918299 | validation: 0.024218470932406286]
	TIME [epoch: 9.04 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030899956377352593		[learning rate: 0.00011222]
		[batch 20/20] avg loss: 0.025214730087633763		[learning rate: 0.00011205]
	Learning Rate: 0.000112047
	LOSS [training: 0.028057343232493176 | validation: 0.0289551413912449]
	TIME [epoch: 9.05 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03483735217465157		[learning rate: 0.00011188]
		[batch 20/20] avg loss: 0.03640642805276489		[learning rate: 0.0001117]
	Learning Rate: 0.000111703
	LOSS [training: 0.035621890113708234 | validation: 0.023958044885934567]
	TIME [epoch: 9.04 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027902839379232318		[learning rate: 0.00011153]
		[batch 20/20] avg loss: 0.033749019859668807		[learning rate: 0.00011136]
	Learning Rate: 0.000111361
	LOSS [training: 0.030825929619450566 | validation: 0.028673531077706524]
	TIME [epoch: 9.04 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033893116228828256		[learning rate: 0.00011119]
		[batch 20/20] avg loss: 0.031892517847552766		[learning rate: 0.00011102]
	Learning Rate: 0.00011102
	LOSS [training: 0.03289281703819052 | validation: 0.02300228033138106]
	TIME [epoch: 9.04 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031866189773443784		[learning rate: 0.00011085]
		[batch 20/20] avg loss: 0.034915138246559826		[learning rate: 0.00011068]
	Learning Rate: 0.000110679
	LOSS [training: 0.03339066401000181 | validation: 0.027745366063008994]
	TIME [epoch: 9.05 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027792253533796968		[learning rate: 0.00011051]
		[batch 20/20] avg loss: 0.03468205201143826		[learning rate: 0.00011034]
	Learning Rate: 0.00011034
	LOSS [training: 0.03123715277261761 | validation: 0.026077149189956975]
	TIME [epoch: 9.06 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025656488737829337		[learning rate: 0.00011017]
		[batch 20/20] avg loss: 0.030742859095330095		[learning rate: 0.00011]
	Learning Rate: 0.000110002
	LOSS [training: 0.02819967391657971 | validation: 0.019045603590688305]
	TIME [epoch: 9.04 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03125024301730237		[learning rate: 0.00010983]
		[batch 20/20] avg loss: 0.030795825186533232		[learning rate: 0.00010966]
	Learning Rate: 0.000109665
	LOSS [training: 0.031023034101917805 | validation: 0.025941171115600646]
	TIME [epoch: 9.04 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03406792092711632		[learning rate: 0.0001095]
		[batch 20/20] avg loss: 0.031488510029536634		[learning rate: 0.00010933]
	Learning Rate: 0.000109328
	LOSS [training: 0.032778215478326475 | validation: 0.023273830797512145]
	TIME [epoch: 9.04 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035878771566288575		[learning rate: 0.00010916]
		[batch 20/20] avg loss: 0.03135070871921608		[learning rate: 0.00010899]
	Learning Rate: 0.000108993
	LOSS [training: 0.03361474014275233 | validation: 0.02377003051343204]
	TIME [epoch: 9.04 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030650382392674008		[learning rate: 0.00010883]
		[batch 20/20] avg loss: 0.026320945294709804		[learning rate: 0.00010866]
	Learning Rate: 0.000108659
	LOSS [training: 0.0284856638436919 | validation: 0.024592403627510535]
	TIME [epoch: 9.06 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026938157284597697		[learning rate: 0.00010849]
		[batch 20/20] avg loss: 0.035692684430288356		[learning rate: 0.00010833]
	Learning Rate: 0.000108326
	LOSS [training: 0.03131542085744303 | validation: 0.023609157146549216]
	TIME [epoch: 9.05 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03220655967246626		[learning rate: 0.00010816]
		[batch 20/20] avg loss: 0.028472953977463612		[learning rate: 0.00010799]
	Learning Rate: 0.000107994
	LOSS [training: 0.030339756824964946 | validation: 0.035330226529746524]
	TIME [epoch: 9.03 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03576765485722025		[learning rate: 0.00010783]
		[batch 20/20] avg loss: 0.03410256269329461		[learning rate: 0.00010766]
	Learning Rate: 0.000107663
	LOSS [training: 0.03493510877525743 | validation: 0.023385680791043563]
	TIME [epoch: 9.04 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02990431442346783		[learning rate: 0.0001075]
		[batch 20/20] avg loss: 0.030818993526008292		[learning rate: 0.00010733]
	Learning Rate: 0.000107333
	LOSS [training: 0.030361653974738063 | validation: 0.026955270782105287]
	TIME [epoch: 9.03 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029652860061310966		[learning rate: 0.00010717]
		[batch 20/20] avg loss: 0.029517077702460332		[learning rate: 0.000107]
	Learning Rate: 0.000107004
	LOSS [training: 0.029584968881885654 | validation: 0.02916981351783208]
	TIME [epoch: 9.05 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03540327735287084		[learning rate: 0.00010684]
		[batch 20/20] avg loss: 0.031720800782188925		[learning rate: 0.00010668]
	Learning Rate: 0.000106676
	LOSS [training: 0.033562039067529886 | validation: 0.021498882153177024]
	TIME [epoch: 9.04 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02910976299261		[learning rate: 0.00010651]
		[batch 20/20] avg loss: 0.03202744741180695		[learning rate: 0.00010635]
	Learning Rate: 0.000106349
	LOSS [training: 0.03056860520220847 | validation: 0.02020957844994678]
	TIME [epoch: 9.04 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025777741128521353		[learning rate: 0.00010619]
		[batch 20/20] avg loss: 0.04022736768897396		[learning rate: 0.00010602]
	Learning Rate: 0.000106023
	LOSS [training: 0.03300255440874765 | validation: 0.02401700036395367]
	TIME [epoch: 9.04 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026729089201292494		[learning rate: 0.00010586]
		[batch 20/20] avg loss: 0.03327228693985388		[learning rate: 0.0001057]
	Learning Rate: 0.000105698
	LOSS [training: 0.030000688070573177 | validation: 0.027578133276315486]
	TIME [epoch: 9.03 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033573735728431406		[learning rate: 0.00010554]
		[batch 20/20] avg loss: 0.02768615240916375		[learning rate: 0.00010537]
	Learning Rate: 0.000105374
	LOSS [training: 0.03062994406879757 | validation: 0.02203580917772087]
	TIME [epoch: 9.06 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031135118827072806		[learning rate: 0.00010521]
		[batch 20/20] avg loss: 0.03384667144152187		[learning rate: 0.00010505]
	Learning Rate: 0.000105051
	LOSS [training: 0.032490895134297336 | validation: 0.02183531236734445]
	TIME [epoch: 9.13 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03539939619768131		[learning rate: 0.00010489]
		[batch 20/20] avg loss: 0.023880347079827096		[learning rate: 0.00010473]
	Learning Rate: 0.000104729
	LOSS [training: 0.0296398716387542 | validation: 0.02036052449538942]
	TIME [epoch: 9.03 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026547278393841613		[learning rate: 0.00010457]
		[batch 20/20] avg loss: 0.037075888644301974		[learning rate: 0.00010441]
	Learning Rate: 0.000104408
	LOSS [training: 0.0318115835190718 | validation: 0.0243968186128883]
	TIME [epoch: 9.08 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030191550417271108		[learning rate: 0.00010425]
		[batch 20/20] avg loss: 0.031828666801125415		[learning rate: 0.00010409]
	Learning Rate: 0.000104088
	LOSS [training: 0.031010108609198263 | validation: 0.025552032684401288]
	TIME [epoch: 9.07 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03615628596265798		[learning rate: 0.00010393]
		[batch 20/20] avg loss: 0.02958805189085823		[learning rate: 0.00010377]
	Learning Rate: 0.000103769
	LOSS [training: 0.0328721689267581 | validation: 0.027212870344835237]
	TIME [epoch: 9.08 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03359718079979903		[learning rate: 0.00010361]
		[batch 20/20] avg loss: 0.02797030465066792		[learning rate: 0.00010345]
	Learning Rate: 0.000103451
	LOSS [training: 0.03078374272523347 | validation: 0.020622389499311236]
	TIME [epoch: 9.08 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02902272030044923		[learning rate: 0.00010329]
		[batch 20/20] avg loss: 0.03645506204441726		[learning rate: 0.00010313]
	Learning Rate: 0.000103134
	LOSS [training: 0.03273889117243324 | validation: 0.02583633484936422]
	TIME [epoch: 9.04 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0353577316047502		[learning rate: 0.00010298]
		[batch 20/20] avg loss: 0.026420106809605715		[learning rate: 0.00010282]
	Learning Rate: 0.000102817
	LOSS [training: 0.030888919207177956 | validation: 0.02242162683732333]
	TIME [epoch: 9.07 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035128958819474865		[learning rate: 0.00010266]
		[batch 20/20] avg loss: 0.0345660239003217		[learning rate: 0.0001025]
	Learning Rate: 0.000102502
	LOSS [training: 0.034847491359898286 | validation: 0.028020913808641516]
	TIME [epoch: 9.05 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03106402102724513		[learning rate: 0.00010235]
		[batch 20/20] avg loss: 0.03293263874060699		[learning rate: 0.00010219]
	Learning Rate: 0.000102188
	LOSS [training: 0.031998329883926056 | validation: 0.026401572964791162]
	TIME [epoch: 9.05 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032390352562700994		[learning rate: 0.00010203]
		[batch 20/20] avg loss: 0.03375358345267365		[learning rate: 0.00010187]
	Learning Rate: 0.000101875
	LOSS [training: 0.03307196800768732 | validation: 0.022159702398358627]
	TIME [epoch: 9.04 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02696697873672745		[learning rate: 0.00010172]
		[batch 20/20] avg loss: 0.02989167569824267		[learning rate: 0.00010156]
	Learning Rate: 0.000101562
	LOSS [training: 0.028429327217485055 | validation: 0.015017499201236052]
	TIME [epoch: 9.03 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027949904987190726		[learning rate: 0.00010141]
		[batch 20/20] avg loss: 0.03282228862541162		[learning rate: 0.00010125]
	Learning Rate: 0.000101251
	LOSS [training: 0.030386096806301173 | validation: 0.01781475586868295]
	TIME [epoch: 9.04 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028560046713726785		[learning rate: 0.0001011]
		[batch 20/20] avg loss: 0.030108085599982147		[learning rate: 0.00010094]
	Learning Rate: 0.000100941
	LOSS [training: 0.029334066156854473 | validation: 0.024326057158138674]
	TIME [epoch: 9.04 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030744403612972794		[learning rate: 0.00010079]
		[batch 20/20] avg loss: 0.03296410349523261		[learning rate: 0.00010063]
	Learning Rate: 0.000100631
	LOSS [training: 0.03185425355410271 | validation: 0.022680739005279338]
	TIME [epoch: 9.05 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03825926367710043		[learning rate: 0.00010048]
		[batch 20/20] avg loss: 0.022888244807911934		[learning rate: 0.00010032]
	Learning Rate: 0.000100323
	LOSS [training: 0.03057375424250618 | validation: 0.024880230932480214]
	TIME [epoch: 9.04 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03487261265566437		[learning rate: 0.00010017]
		[batch 20/20] avg loss: 0.02572830639847215		[learning rate: 0.00010002]
	Learning Rate: 0.000100015
	LOSS [training: 0.03030045952706826 | validation: 0.0336300544933144]
	TIME [epoch: 9.03 sec]
Finished training in 18265.915 seconds.
