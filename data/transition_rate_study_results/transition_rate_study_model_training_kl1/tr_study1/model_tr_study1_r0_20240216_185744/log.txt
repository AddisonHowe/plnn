Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r0', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2086804891

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.334771757233758		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.95549264916884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.645132203201301 | validation: 9.266472112921686]
	TIME [epoch: 49.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.855701249715052		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.877801128042263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.366751188878656 | validation: 7.912828659931067]
	TIME [epoch: 8.91 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.269366119020563		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.92055898892378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.094962553972172 | validation: 6.714221006344394]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.71292775131496		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.675319966357591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.194123858836276 | validation: 4.3051184939397995]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.269347259803886		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.08944577315634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.179396516480114 | validation: 4.653348126009805]
	TIME [epoch: 8.86 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.655784378820639		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5973178539781627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6265511163994004 | validation: 9.44929053938142]
	TIME [epoch: 8.85 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.518215681402017		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.593238196930849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.055726939166433 | validation: 3.3048117939241077]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2194271171681343		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1999391614177433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.209683139292939 | validation: 3.3047468168092253]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9850917784340263		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1176360522270845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.051363915330556 | validation: 2.8529111028673935]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.276332713098823		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7297235979188463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0030281555088343 | validation: 2.7114842379327553]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.567063643466061		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5768369186532114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.571950281059636 | validation: 3.0100813496406156]
	TIME [epoch: 8.86 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.515192185230718		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6604940838387643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.587843134534741 | validation: 3.136571641624372]
	TIME [epoch: 8.85 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.270154941640267		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1810066377026494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2255807896714583 | validation: 2.1023882711433695]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.973232555874468		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0935420524939445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0333873041842065 | validation: 2.008208703491391]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9290082561483348		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.872250981014967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.900629618581651 | validation: 1.7540703702042855]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8327102350987245		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9080354234000416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8703728292493829 | validation: 2.163317384298053]
	TIME [epoch: 8.85 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8587147940283362		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.780697966138575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8197063800834559 | validation: 1.5412492496128498]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6574781686026838		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8218061600321165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7396421643173998 | validation: 1.845884210621309]
	TIME [epoch: 8.86 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6355441059704545		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9318033718440257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7836737389072403 | validation: 2.026085332657531]
	TIME [epoch: 8.86 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7786660370976208		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7013257772505088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.739995907174065 | validation: 1.5616998704990426]
	TIME [epoch: 8.85 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6253015454270294		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8290179486818587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.727159747054444 | validation: 1.3005974765961201]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6512393217105021		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6849005853779455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6680699535442236 | validation: 1.3172006793759212]
	TIME [epoch: 8.87 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5376822838610171		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3658523626208439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4517673232409305 | validation: 2.2161081080582936]
	TIME [epoch: 8.87 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7255508899923995		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8518310305523005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.78869096027235 | validation: 1.1645855061848134]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5829735560381797		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4006072830494358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4917904195438076 | validation: 1.3222835369415538]
	TIME [epoch: 8.86 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3583508192240044		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3423690371638055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3503599281939054 | validation: 1.5461454194118769]
	TIME [epoch: 8.86 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3049908329126771		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.581321160085466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4431559964990717 | validation: 1.7968206409217093]
	TIME [epoch: 8.88 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.304238869951563		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3755802241663355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3399095470589493 | validation: 1.2428029514829875]
	TIME [epoch: 8.86 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3473319001899677		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2423853809246144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2948586405572908 | validation: 1.670705706405693]
	TIME [epoch: 8.86 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.210766462204134		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.559893060165607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3853297611848707 | validation: 1.0284450782274768]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.249023369800871		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4583070704504657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3536652201256685 | validation: 2.492986203467826]
	TIME [epoch: 8.87 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4384699330460164		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1699257460034291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3041978395247225 | validation: 1.0292411120119533]
	TIME [epoch: 8.87 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2122984436658826		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2416454427876982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2269719432267903 | validation: 0.9069906600409845]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5362523238369241		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2841647718265774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.410208547831751 | validation: 0.933378789009745]
	TIME [epoch: 8.86 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.061015786061451		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1308299534949948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.095922869778223 | validation: 1.1435115626948993]
	TIME [epoch: 8.85 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1323224427378045		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.191311012334988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1618167275363964 | validation: 1.462749226917107]
	TIME [epoch: 8.88 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1335978660884791		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0287046379215112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0811512520049953 | validation: 0.7522227282980789]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3931217920954608		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1276327358805713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2603772639880162 | validation: 0.9689476147441203]
	TIME [epoch: 8.85 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.319536604831042		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1902571005845712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2548968527078066 | validation: 1.3610093650968618]
	TIME [epoch: 8.85 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.370831184939639		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2838306115656493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.327330898252644 | validation: 0.9143584993476908]
	TIME [epoch: 8.88 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2432309973699947		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0472672841626531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1452491407663237 | validation: 1.309082721583644]
	TIME [epoch: 8.86 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1041548152724245		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2132367558172255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.158695785544825 | validation: 1.0255177015328272]
	TIME [epoch: 8.85 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0660878255352904		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9242367248754654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9951622752053779 | validation: 1.1709034347637917]
	TIME [epoch: 8.85 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1326688942237395		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.00615204650165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0694104703626945 | validation: 1.1160283816230248]
	TIME [epoch: 8.86 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1360648139450582		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0745659068492444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.105315360397151 | validation: 1.1917272897360838]
	TIME [epoch: 13.4 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1602754637817028		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1866241259568533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1734497948692781 | validation: 0.9247161795341948]
	TIME [epoch: 8.85 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.236181410615599		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.256599740881542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2463905757485705 | validation: 1.5209131052324376]
	TIME [epoch: 8.85 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0004835899222844		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1809423662370908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0907129780796876 | validation: 0.9794299698124203]
	TIME [epoch: 8.86 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9894713106248094		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9548422932659205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9721568019453649 | validation: 0.8786943153934367]
	TIME [epoch: 8.87 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1012473598348422		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3172887328934189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2092680463641305 | validation: 1.1443833511112436]
	TIME [epoch: 8.85 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1193525642408413		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0488827266614777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0841176454511596 | validation: 2.661209790594489]
	TIME [epoch: 8.85 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3284210404016037		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0482866957382284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1883538680699162 | validation: 0.766658962790891]
	TIME [epoch: 8.85 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.003329950875358		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9979350013316406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0006324761034995 | validation: 1.3347938751304484]
	TIME [epoch: 8.88 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0448658474875872		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2079732415318145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1264195445097005 | validation: 0.7324982852740525]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2534253312542745		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1218992287476448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1876622800009595 | validation: 1.3434966536929862]
	TIME [epoch: 8.85 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0761349961682671		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9321117541798681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0041233751740675 | validation: 1.4269562383134176]
	TIME [epoch: 8.85 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.10053834729706		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8552402159433319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9778892816201961 | validation: 0.9267702131819595]
	TIME [epoch: 8.87 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9062909736297785		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0607171971755736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9835040854026762 | validation: 0.6167130621823415]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9437124176725339		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1396456548135308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0416790362430326 | validation: 1.2252543579540291]
	TIME [epoch: 8.85 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8809228182623041		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9142896211672655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8976062197147847 | validation: 1.1578947582151573]
	TIME [epoch: 8.85 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0127050163195839		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0488901413268836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.030797578823234 | validation: 0.9201213489870373]
	TIME [epoch: 8.86 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9975908425400142		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3469800597666635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1722854511533387 | validation: 1.0903877202074175]
	TIME [epoch: 8.85 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8566825217772905		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0266535754071655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9416680485922282 | validation: 0.860124847691907]
	TIME [epoch: 8.85 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.001234707495973		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9359446911361691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9685896993160711 | validation: 0.8077183359939148]
	TIME [epoch: 8.85 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9507176084783959		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8672506463452974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9089841274118466 | validation: 1.0183237302211088]
	TIME [epoch: 8.84 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9569692227609883		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0250272924296973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9909982575953429 | validation: 0.9737716220026631]
	TIME [epoch: 8.87 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8705763996674554		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0945987814095566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.982587590538506 | validation: 1.4497440484373194]
	TIME [epoch: 8.85 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1036704238447659		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9623570140861913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0330137189654787 | validation: 1.0915152958587084]
	TIME [epoch: 12.5 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1075686539004952		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1301718900381568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.118870271969326 | validation: 1.190375143914676]
	TIME [epoch: 8.85 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1168871097027246		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1731912617936646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1450391857481943 | validation: 1.0886120434093078]
	TIME [epoch: 8.87 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0981640216162147		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.110244766793211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1042043942047128 | validation: 0.9130065311566924]
	TIME [epoch: 8.85 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9296081237446308		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0297051155421957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9796566196434133 | validation: 3.018040285292309]
	TIME [epoch: 8.84 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4578024970581067		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2010937834403153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.329448140249211 | validation: 0.8687483459312849]
	TIME [epoch: 8.84 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0675805772849818		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1700693473130916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.118824962299037 | validation: 0.8522766765352237]
	TIME [epoch: 8.86 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0887262628738963		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1658106123335963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1272684376037463 | validation: 0.8288976059860599]
	TIME [epoch: 8.86 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.246347884106234		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7261652192185846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4862565516624093 | validation: 1.069630261066304]
	TIME [epoch: 8.84 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0721951114303332		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.039099170035293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.055647140732813 | validation: 1.0956775327013806]
	TIME [epoch: 8.84 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8641228296140847		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9549858512541316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9095543404341082 | validation: 0.7458811102773558]
	TIME [epoch: 8.86 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8172653431604523		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0029465604456715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9101059518030621 | validation: 0.7688415854549868]
	TIME [epoch: 8.86 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8323884488355204		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8760489343641547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8542186915998377 | validation: 0.48884760818210654]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8346693313939977		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0309920087119668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9328306700529823 | validation: 0.6058552460920839]
	TIME [epoch: 8.85 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6488343258426357		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9829576592291442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.81589599253589 | validation: 0.8763373170137781]
	TIME [epoch: 8.85 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.69040486708431		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8797951547888946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7851000109366024 | validation: 0.6973092352022557]
	TIME [epoch: 8.87 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8890197697179774		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8515291798465533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8702744747822655 | validation: 0.6179331830924852]
	TIME [epoch: 8.85 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9263172793231329		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9701713604262094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9482443198746713 | validation: 0.6921382605149677]
	TIME [epoch: 8.85 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8376625891706476		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7678609773537245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8027617832621863 | validation: 0.9601166585068488]
	TIME [epoch: 8.85 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3295330650530426		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8437326071813664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0866328361172042 | validation: 0.751048577684477]
	TIME [epoch: 8.87 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8314719059684039		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8587751168059408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8451235113871725 | validation: 0.4416247855601847]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8184482275397089		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8678450973847301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8431466624622196 | validation: 0.9564635615544276]
	TIME [epoch: 8.84 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8470421601534763		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.820918383773068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8339802719632722 | validation: 0.579071174034763]
	TIME [epoch: 8.84 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7763150432806136		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9014814896005543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8388982664405841 | validation: 0.4497740552708571]
	TIME [epoch: 8.85 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8086227547379796		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7578555147867749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7832391347623774 | validation: 0.8008008408500382]
	TIME [epoch: 8.86 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7202150993126432		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.754640121258414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7374276102855285 | validation: 0.839988975656999]
	TIME [epoch: 8.86 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9195020235592771		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8537053232890323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8866036734241545 | validation: 0.6754944817217059]
	TIME [epoch: 8.85 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8521688833557265		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8030861709830223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8276275271693745 | validation: 0.8575500912772449]
	TIME [epoch: 8.86 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.657876860589044		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9378147924300017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7978458265095228 | validation: 0.8855980883358793]
	TIME [epoch: 8.88 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8552910216465041		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8237179032787918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8395044624626479 | validation: 0.5730710896943212]
	TIME [epoch: 8.85 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6266466241963716		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6952880572710678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6609673407337197 | validation: 1.499338002072829]
	TIME [epoch: 8.84 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9719232766464133		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.091795130887987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0318592037672003 | validation: 1.1058385728499216]
	TIME [epoch: 8.85 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8126466881493363		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6804180036566534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7465323459029949 | validation: 0.5557424657766386]
	TIME [epoch: 8.87 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6249286444154805		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8188149394611102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7218717919382952 | validation: 0.5179817526096057]
	TIME [epoch: 8.85 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8008993548965128		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6923023577616301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7466008563290714 | validation: 0.46897028757255177]
	TIME [epoch: 8.84 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8807538050668512		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7191836598983798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7999687324826155 | validation: 0.5383727638253126]
	TIME [epoch: 8.85 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7067037183141283		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8404578345588428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7735807764364855 | validation: 0.4993715765322242]
	TIME [epoch: 8.89 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.862704122136105		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6937722093519778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7782381657440414 | validation: 0.8913309155229838]
	TIME [epoch: 8.91 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7639470279041953		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8119076583309319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7879273431175636 | validation: 1.417967457128551]
	TIME [epoch: 8.86 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7645657462603418		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7902318544672805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.777398800363811 | validation: 0.678301640274741]
	TIME [epoch: 8.84 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.830288432787917		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7919102005742014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8110993166810593 | validation: 0.7402428366448688]
	TIME [epoch: 8.87 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9305641089882488		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6691227036940539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7998434063411514 | validation: 1.425958305461129]
	TIME [epoch: 8.86 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9017658711507754		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8665033749713131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8841346230610441 | validation: 0.5804457245527714]
	TIME [epoch: 8.85 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6656271305978816		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7124425715955092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6890348510966954 | validation: 0.5350058656829477]
	TIME [epoch: 8.85 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8029846934331317		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8406023860009635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8217935397170475 | validation: 0.7134015238601803]
	TIME [epoch: 8.85 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9915183241289606		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7944615463387448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8929899352338525 | validation: 0.7361375103173309]
	TIME [epoch: 8.87 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8318808134398017		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.76341536620103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7976480898204158 | validation: 0.5197593941178734]
	TIME [epoch: 8.86 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6130735282101776		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8208774044106478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7169754663104128 | validation: 0.9833003279637471]
	TIME [epoch: 8.85 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7208443604215875		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6579952817251283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.689419821073358 | validation: 0.5034333599007618]
	TIME [epoch: 8.85 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.680280192624698		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7949004431226953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7375903178736968 | validation: 0.3139868370455581]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r0_20240216_185744/states/model_tr_study1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6612790888195531		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7137894649556464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6875342768875996 | validation: 0.5787229510140715]
	TIME [epoch: 8.86 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7383239913719959		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6146275336439097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6764757625079526 | validation: 1.0104952069956732]
	TIME [epoch: 8.85 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8517781734562254		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7682252147699268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8100016941130761 | validation: 0.9479330138991673]
	TIME [epoch: 8.85 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8805935627563246		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8818303364269887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8812119495916566 | validation: 0.6059349683210985]
	TIME [epoch: 8.87 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6111251603526651		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7558916452522251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6835084028024451 | validation: 0.6788492847707323]
	TIME [epoch: 8.88 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8958126555915344		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7680797901334289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8319462228624817 | validation: 0.6844308572750358]
	TIME [epoch: 8.86 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7640388452144298		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7626335435505751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7633361943825026 | validation: 0.8497934896782471]
	TIME [epoch: 8.85 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7711238684861155		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0602337064985687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9156787874923422 | validation: 0.9868322374755023]
	TIME [epoch: 8.85 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7143796662040593		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.867532493508526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7909560798562929 | validation: 0.5438896397375732]
	TIME [epoch: 8.88 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7842685345459625		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7834870454156277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7838777899807952 | validation: 0.6376147241204853]
	TIME [epoch: 8.85 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7673414197961049		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7550319739204352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7611866968582702 | validation: 0.5372340396914728]
	TIME [epoch: 8.85 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7341405675443737		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7075465583536017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.720843562948988 | validation: 0.539223918055663]
	TIME [epoch: 8.86 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7765827114037916		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7238180843901817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7502003978969867 | validation: 0.4387839732018305]
	TIME [epoch: 8.88 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7550806486879105		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6451267141892892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001036814385999 | validation: 0.7367978984244722]
	TIME [epoch: 8.87 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.65504055412403		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8357130004357888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7453767772799094 | validation: 0.615911884113165]
	TIME [epoch: 8.85 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.707945389483181		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3865071205152506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.047226254999216 | validation: 1.1343688559431047]
	TIME [epoch: 8.86 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8628363959953577		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6715068585131815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7671716272542696 | validation: 1.0572996719809304]
	TIME [epoch: 8.87 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8859657704738233		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9078724599898038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8969191152318136 | validation: 1.1329649079572748]
	TIME [epoch: 8.87 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7470232861928048		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7604109794482724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7537171328205386 | validation: 0.8170733039208973]
	TIME [epoch: 8.86 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7861776624982564		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9085418515214885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8473597570098725 | validation: 0.7092581125362155]
	TIME [epoch: 8.85 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6960549620183898		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.094689256629513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8953721093239514 | validation: 0.9145127030029699]
	TIME [epoch: 8.91 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7320246161957031		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6401602646584198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6860924404270614 | validation: 0.9785004323943358]
	TIME [epoch: 8.88 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7843816956069114		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7457826255236331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7650821605652721 | validation: 0.789284525540368]
	TIME [epoch: 8.85 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2332711464263912		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6633181790967877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9482946627615896 | validation: 0.6697364218291816]
	TIME [epoch: 8.84 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9371357025492623		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6519408016833996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.794538252116331 | validation: 0.4117829844526014]
	TIME [epoch: 8.85 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7291377845386531		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7808653642440605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.755001574391357 | validation: 0.6410043611698905]
	TIME [epoch: 8.87 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.57868980371017		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6175747172434912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5981322604768307 | validation: 0.49075500950098494]
	TIME [epoch: 8.85 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5814748004206969		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7940969613521591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687785880886428 | validation: 0.7192194252604146]
	TIME [epoch: 8.85 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7638599856853705		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7726597898313072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7682598877583389 | validation: 0.6600273916274224]
	TIME [epoch: 8.85 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6797615540277747		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.664332757652669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6720471558402219 | validation: 0.8370676326138484]
	TIME [epoch: 8.87 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6595217521168721		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6753087148544824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6674152334856773 | validation: 1.0964329961187236]
	TIME [epoch: 8.86 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5790335035014518		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6639402703112331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6214868869063425 | validation: 0.7311719061862995]
	TIME [epoch: 8.84 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6007778289376603		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6666794687262617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6337286488319609 | validation: 0.40821570687829034]
	TIME [epoch: 8.84 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6197803526073261		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5592366148396957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.589508483723511 | validation: 0.49196580166734843]
	TIME [epoch: 8.86 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6132299212139045		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7741081421397504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6936690316768275 | validation: 0.5636946924676881]
	TIME [epoch: 8.87 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5399897624758122		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7026437578468792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6213167601613456 | validation: 0.6009292536107846]
	TIME [epoch: 8.85 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5293546343135		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7700419085355265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6496982714245132 | validation: 0.9337728368286936]
	TIME [epoch: 8.85 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6458178997545186		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.559637335209479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6027276174819985 | validation: 0.6721759845765076]
	TIME [epoch: 8.85 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6470199680763373		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6266149894567641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6368174787665508 | validation: 0.6136951335450218]
	TIME [epoch: 8.87 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7851609213119155		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7526047551235479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7688828382177316 | validation: 0.3931273907674832]
	TIME [epoch: 8.85 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7494400897294795		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7567071394102874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7530736145698834 | validation: 0.6698731051290195]
	TIME [epoch: 8.85 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6907468708168085		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5720275209375894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6313871958771988 | validation: 0.39284043340597496]
	TIME [epoch: 8.86 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6701219829146451		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7615687007474948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7158453418310698 | validation: 0.581318924008394]
	TIME [epoch: 8.88 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7553092709345118		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6995711800303688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7274402254824404 | validation: 0.8159349110280035]
	TIME [epoch: 8.86 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7851167165451376		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6915148352413362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7383157758932369 | validation: 0.5004761971671208]
	TIME [epoch: 8.87 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5621469381647662		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7051261713847186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6336365547747425 | validation: 0.54543121244986]
	TIME [epoch: 8.86 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6638087381166806		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.583411260558849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6236099993377648 | validation: 0.967278942034852]
	TIME [epoch: 8.87 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8106138927265988		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6343273400930378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7224706164098182 | validation: 0.6304784002825479]
	TIME [epoch: 8.87 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.59082915773165		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7497861362807029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6703076470061765 | validation: 0.6881807972518148]
	TIME [epoch: 8.86 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7079846319128876		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7114491491761457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7097168905445164 | validation: 0.664403207097579]
	TIME [epoch: 8.85 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7184103636923378		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.827822049776292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7731162067343149 | validation: 0.9047174091398047]
	TIME [epoch: 8.85 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6653637810150415		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6504736235259365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6579187022704891 | validation: 0.866086693214608]
	TIME [epoch: 8.87 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7098369796145116		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7338171890075393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7218270843110253 | validation: 0.7050978655906581]
	TIME [epoch: 8.85 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5527007197560911		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6971099468472934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6249053333016923 | validation: 0.9612899565772581]
	TIME [epoch: 8.85 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7646695445873912		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7009698836569309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.732819714122161 | validation: 0.4661577781512648]
	TIME [epoch: 8.86 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7395087391349117		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6625101087185722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7010094239267419 | validation: 0.7792008529866314]
	TIME [epoch: 8.87 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.704370047398851		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6797505614876713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6920603044432612 | validation: 0.9428653918082176]
	TIME [epoch: 8.85 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8180071357183634		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6936527754432589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7558299555808113 | validation: 0.6241184234077781]
	TIME [epoch: 8.85 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5999610142030337		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.724091069040899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6620260416219664 | validation: 0.6604079503757008]
	TIME [epoch: 8.85 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8636308524069094		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6965231350346007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7800769937207551 | validation: 0.5623809657874289]
	TIME [epoch: 8.87 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8411722461424296		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7107943537910176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7759832999667233 | validation: 0.5861319511811781]
	TIME [epoch: 8.86 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7482853860397372		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7942472782095653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7712663321246513 | validation: 0.5551618211137908]
	TIME [epoch: 8.85 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7232025892858864		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7168089476698132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7200057684778497 | validation: 0.4680824199995135]
	TIME [epoch: 8.86 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7605965897288836		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6814329341411132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7210147619349982 | validation: 0.9046715752444217]
	TIME [epoch: 8.85 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7040283550789808		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6255586509085135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6647935029937471 | validation: 0.9398377406002982]
	TIME [epoch: 8.85 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6849514721272397		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7263828552518243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7056671636895321 | validation: 0.9932961468668169]
	TIME [epoch: 8.86 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6382897464359735		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.556034372075913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5971620592559432 | validation: 0.4566049197388493]
	TIME [epoch: 8.86 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6804756073817024		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8928054678740274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7866405376278649 | validation: 0.9599518590746255]
	TIME [epoch: 8.87 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7567863008969078		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6521278953305443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7044570981137263 | validation: 0.7640391192128866]
	TIME [epoch: 8.9 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6238497510029033		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8089518942198035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.716400822611353 | validation: 3.2957334052646816]
	TIME [epoch: 8.87 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.06103486159724		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6668186062105876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.863926733903914 | validation: 3.7454690692966857]
	TIME [epoch: 8.86 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8505009198486606		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9760821042028396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9132915120257508 | validation: 2.8488821955159453]
	TIME [epoch: 8.88 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9260287623147936		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5501180850465786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.738073423680686 | validation: 2.593562622829208]
	TIME [epoch: 8.89 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.646322886362017		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8723409489616785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7593319176618487 | validation: 5.149144610345956]
	TIME [epoch: 8.87 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.252511123226816		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.090704363851177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.171607743538997 | validation: 3.5925034671049403]
	TIME [epoch: 8.85 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1415362569872882		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6494628024258224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8954995297065556 | validation: 2.289795370540082]
	TIME [epoch: 8.86 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3159819946362337		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2596605793714595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2878212870038466 | validation: 2.1146138592540056]
	TIME [epoch: 8.87 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2877189834630496		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.294819873978542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2912694287207964 | validation: 2.0314394111124723]
	TIME [epoch: 8.87 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.613667620499811		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.721263191448324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.167465405974069 | validation: 6.9535222344545105]
	TIME [epoch: 8.84 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.794166973419822		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0312498694308116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.412708421425316 | validation: 2.5040978558199964]
	TIME [epoch: 8.85 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9974500747206987		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2495854418241295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6235177582724143 | validation: 2.0910680618683384]
	TIME [epoch: 8.84 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9991551309177518		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.21571077484232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1074329528800355 | validation: 1.8753849808124712]
	TIME [epoch: 8.89 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2361013501275773		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4512758593570974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.343688604742337 | validation: 2.237207479412663]
	TIME [epoch: 8.85 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.077516396185163		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.935029839192123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.506273117688643 | validation: 7.054217002566054]
	TIME [epoch: 8.86 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.431878819079375		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.125004011420751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.2784414152500645 | validation: 6.541575951378397]
	TIME [epoch: 8.86 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.59205035537582		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.843969080718091		[learning rate: 0.01]
ERROR:
nan encountered in epoch 202 (validation loss).
