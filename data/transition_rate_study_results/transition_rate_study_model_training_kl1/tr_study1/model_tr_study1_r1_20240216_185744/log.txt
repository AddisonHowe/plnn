Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r1', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4016960985

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.468562277559114		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.475442207648408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.972002242603761 | validation: 9.130811903172699]
	TIME [epoch: 49.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.339471738858514		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.3564221686535065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.84794695375601 | validation: 6.727706016427589]
	TIME [epoch: 8.91 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.327255198836479		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.948452047303514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.137853623069998 | validation: 6.2935887357657805]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.887406005776847		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.626727337292193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7570666715345205 | validation: 5.938541942671712]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.879814789624312		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.306375225225684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.593095007424999 | validation: 5.590986745876416]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.339646024222893		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.208388631053891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.274017327638392 | validation: 5.061000104687785]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.747581653048302		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.286676191958614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.517128922503458 | validation: 4.669727887721146]
	TIME [epoch: 8.98 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.195550028131968		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.1460728865409004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.170811457336433 | validation: 4.10696044631659]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.886187180955494		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8119974958499854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.849092338402739 | validation: 4.575887638324913]
	TIME [epoch: 8.87 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.756339957311726		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.645282915412028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.700811436361877 | validation: 3.9248638257370336]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.562540307621325		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5435182658795568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5530292867504416 | validation: 3.9859620585663205]
	TIME [epoch: 8.87 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.35086925232534		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.546576604169296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.448722928247318 | validation: 3.540670286535951]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.546212796839078		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5248815598662993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.535547178352689 | validation: 3.5119018435620157]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.400256049652815		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3778073787724963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3890317142126563 | validation: 3.6505118314727634]
	TIME [epoch: 8.89 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.24028964964144		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3616816087464203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.30098562919393 | validation: 3.4015195843024615]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3546614369634895		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.4994777457945885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.42706959137904 | validation: 3.94785814093484]
	TIME [epoch: 8.86 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3608144589435733		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2826856202864882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3217500396150306 | validation: 3.857652709608379]
	TIME [epoch: 8.86 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3786348542503206		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5050246164474075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.441829735348864 | validation: 3.9858030419416988]
	TIME [epoch: 8.88 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3267089358119484		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0789335943671974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2028212650895727 | validation: 3.8170879110381533]
	TIME [epoch: 8.88 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0673849816752736		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1533458509229453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.110365416299109 | validation: 3.48695681047516]
	TIME [epoch: 8.86 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.839376858623057		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1155570071044782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.977466932863768 | validation: 3.3260129401764513]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1679074066681414		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.714561563907758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.94123448528795 | validation: 3.3000217334875455]
	TIME [epoch: 8.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9115417092804172		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.31420034603898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1128710276596983 | validation: 3.5701266323495435]
	TIME [epoch: 8.89 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2548317459307876		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.287073657401586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.270952701666187 | validation: 3.314918627658133]
	TIME [epoch: 8.88 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2909319457605557		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8141939989981255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0525629723793397 | validation: 3.417750063531203]
	TIME [epoch: 8.87 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8641019632219735		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1074573441538456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.985779653687909 | validation: 4.288213334410807]
	TIME [epoch: 8.88 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2733047490258222		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.637184919643418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9552448343346196 | validation: 3.145395943202575]
	TIME [epoch: 8.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.801350458181134		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.795995544920641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7986730015508874 | validation: 3.4134821005707616]
	TIME [epoch: 8.89 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9244251682813953		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8491198906713615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8867725294763784 | validation: 3.340702531440975]
	TIME [epoch: 8.88 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7316139308646736		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7847878909198256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.758200910892249 | validation: 3.1890271652463027]
	TIME [epoch: 8.88 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.841992761479425		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1861768351955453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0140847983374854 | validation: 3.986517558396355]
	TIME [epoch: 8.88 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.242188130538932		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2033207700068203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.222754450272876 | validation: 4.057972985040352]
	TIME [epoch: 8.89 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1281499030736657		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.095306125165954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.11172801411981 | validation: 3.366579459169584]
	TIME [epoch: 8.88 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.709333011432358		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.799797814170592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7545654128014747 | validation: 4.205462587926449]
	TIME [epoch: 8.88 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4226494084758863		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.471067900483086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.946858654479486 | validation: 0.9263101816165259]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2732787563951053		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3455102417265787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3093944990608422 | validation: 1.1052997491364285]
	TIME [epoch: 8.9 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2547897149156264		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.100522916703707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1776563158096667 | validation: 1.5310651185220938]
	TIME [epoch: 8.87 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.175995227462988		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0646134277482147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1203043276056017 | validation: 1.3987945116786598]
	TIME [epoch: 8.87 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.064407999240282		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1091359598770065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0867719795586444 | validation: 1.3661511255452714]
	TIME [epoch: 8.87 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.053449834130838		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1443686990996986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0989092666152682 | validation: 2.2444715826918573]
	TIME [epoch: 8.89 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.120885060049586		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3841201168269595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2525025884382728 | validation: 1.1937180741180926]
	TIME [epoch: 8.88 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0546020526557744		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2659322317238475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.160267142189811 | validation: 1.5639855348121658]
	TIME [epoch: 8.88 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2628767709211586		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9426250263429983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1027508986320784 | validation: 1.107599116114895]
	TIME [epoch: 8.87 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2356947876925264		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2710177711184083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2533562794054673 | validation: 1.4061414883452368]
	TIME [epoch: 8.88 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1093282806561564		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0683162121200733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.088822246388115 | validation: 0.798018816153012]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0611697085443286		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5524764846316266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3068230965879777 | validation: 0.8304651358039911]
	TIME [epoch: 8.87 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9052206650981918		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2636887366738396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0844547008860155 | validation: 0.7571163186862369]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2122457364812917		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9439623413464459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0781040389138687 | validation: 1.2096407280870842]
	TIME [epoch: 8.88 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2456918437316689		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8732280780392321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0594599608854505 | validation: 1.1764805836702537]
	TIME [epoch: 8.88 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.169377562682723		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8523553626027551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.010866462642739 | validation: 0.7731765398676598]
	TIME [epoch: 8.87 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9099648383909354		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9138882517347803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9119265450628576 | validation: 1.0032415950339522]
	TIME [epoch: 8.87 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8733421028792353		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1604239641158551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0168830334975454 | validation: 0.706153843944562]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9380118640991562		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0912291516372064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0146205078681811 | validation: 2.3018444142158025]
	TIME [epoch: 8.9 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1477241112326424		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0665500668725596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.107137089052601 | validation: 0.9644237938727306]
	TIME [epoch: 8.86 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9014787214115912		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8522834459921667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8768810837018792 | validation: 0.970556315389225]
	TIME [epoch: 8.87 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0140705720727992		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7251844691916938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8696275206322465 | validation: 1.2192426537680894]
	TIME [epoch: 8.86 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9185450792802896		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7191826096799765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8188638444801329 | validation: 2.076752549900049]
	TIME [epoch: 8.89 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9932403999263515		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7628009885093388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8780206942178452 | validation: 0.7546835065577375]
	TIME [epoch: 8.87 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1905511343492723		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8770652886068747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0338082114780736 | validation: 0.6993876364993432]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7622210659881122		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6469258983086409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7045734821483765 | validation: 1.1981076860616806]
	TIME [epoch: 8.87 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6705560335562474		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9216815395076947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7961187865319711 | validation: 1.1810709186008252]
	TIME [epoch: 8.88 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9659830030421711		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8428896865238699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9044363447830206 | validation: 1.0777076734205786]
	TIME [epoch: 8.88 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2076900798617825		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8676674731901194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.537678776525951 | validation: 0.6992486565072092]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.915777459945693		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7053698533783488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8105736566620209 | validation: 0.5617490070347492]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.072310687079023		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5874123364601626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8298615117695929 | validation: 1.319975492537098]
	TIME [epoch: 8.87 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2171895387263683		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7478221712416799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9825058549840241 | validation: 0.7307701646291995]
	TIME [epoch: 8.87 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7084186597553621		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6359238267351801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6721712432452711 | validation: 0.3795743192753763]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.840221262602854		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8375562720766127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8388887673397333 | validation: 1.8492928853122539]
	TIME [epoch: 8.86 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8812628703245966		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.668741172150079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7750020212373379 | validation: 1.2234867489214651]
	TIME [epoch: 8.86 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7591161441848762		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7476858890599221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.753401016622399 | validation: 1.1767738314771288]
	TIME [epoch: 8.9 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8685842067038706		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7681356661735801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8183599364387254 | validation: 0.46402481108961524]
	TIME [epoch: 8.87 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7561536757032881		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6075422428075684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6818479592554282 | validation: 1.2365905357758415]
	TIME [epoch: 8.86 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8267294348661333		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7945654380591602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8106474364626468 | validation: 0.39276305091468633]
	TIME [epoch: 8.87 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.66744235959522		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.920824809343072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.794133584469146 | validation: 0.6645359872776478]
	TIME [epoch: 8.89 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7352872402069737		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7250048515574185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7301460458821959 | validation: 0.7118731925611701]
	TIME [epoch: 8.87 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7577399768667049		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6494848933199829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7036124350933439 | validation: 0.37229751311968334]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7591267791828571		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5969538968306334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6780403380067452 | validation: 0.6288835286988899]
	TIME [epoch: 8.86 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6672073550383639		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5621056812591616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6146565181487629 | validation: 0.5364653172345265]
	TIME [epoch: 8.87 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6596563849648993		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6656574669316424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6626569259482709 | validation: 0.2955530034020324]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6503901321989372		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5125836382058847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.581486885202411 | validation: 0.31066514849279625]
	TIME [epoch: 8.86 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7049125336675784		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5358662639791081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6203893988233432 | validation: 0.5833362685244319]
	TIME [epoch: 8.86 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6267503198146304		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7943640785816618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7105571991981461 | validation: 0.5985724788913644]
	TIME [epoch: 8.87 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6343659562892916		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.534091879125848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5842289177075698 | validation: 1.732210576851342]
	TIME [epoch: 8.88 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6347764048146465		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5108256206217091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5728010127181776 | validation: 0.6386292531980254]
	TIME [epoch: 8.86 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5788604638618866		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49082865301156237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5348445584367245 | validation: 0.6388172144949537]
	TIME [epoch: 8.86 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5310494589720487		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7752121322460349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.653130795609042 | validation: 0.8567802581971311]
	TIME [epoch: 8.86 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6374030262677224		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0761996456047958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.856801335936259 | validation: 0.9158294705924466]
	TIME [epoch: 8.89 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6984520375254405		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49590209273294106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5971770651291908 | validation: 0.6458670070956216]
	TIME [epoch: 8.87 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8022366901685285		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.595014771045075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6986257306068018 | validation: 0.4592567642645262]
	TIME [epoch: 8.86 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8396437461265096		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.580988527259346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7103161366929278 | validation: 0.5110854984263746]
	TIME [epoch: 8.86 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6602916605899007		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5023511350124928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5813213978011966 | validation: 0.7493538927366348]
	TIME [epoch: 8.88 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6230887706510342		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8173009875954884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7201948791232613 | validation: 0.45138087564841045]
	TIME [epoch: 8.87 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4703645487047726		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4749227586843707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4726436536945716 | validation: 0.5546354747141526]
	TIME [epoch: 8.87 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5821616935309628		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5816833244417123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5819225089863374 | validation: 0.3741926177996905]
	TIME [epoch: 8.86 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5328308787948		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6092498362994531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5710403575471265 | validation: 0.5382530844460507]
	TIME [epoch: 8.87 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8446664548258976		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7410629491394514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7928647019826746 | validation: 0.730449764836838]
	TIME [epoch: 8.88 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5326364428420736		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5329486338690828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5327925383555783 | validation: 0.9733212125302648]
	TIME [epoch: 8.86 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5899753416317093		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5832550388140549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.586615190222882 | validation: 0.5122429443178598]
	TIME [epoch: 8.86 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7058886619713188		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5136080549232378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6097483584472783 | validation: 0.3558940082629086]
	TIME [epoch: 8.86 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6137281214123871		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9441526351022007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7789403782572939 | validation: 0.7305092596563073]
	TIME [epoch: 8.88 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6194755863967277		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5932360299816858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6063558081892068 | validation: 0.4250805558544957]
	TIME [epoch: 8.86 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48594432905539425		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5696752687774914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5278097989164428 | validation: 0.8311902134713625]
	TIME [epoch: 8.86 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8659748191711121		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5561155530546561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7110451861128839 | validation: 0.33631570805483213]
	TIME [epoch: 8.86 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6036541869050414		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.483936163556256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5437951752306487 | validation: 0.8956532877787484]
	TIME [epoch: 8.89 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6107353864332447		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6887127453038444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6497240658685446 | validation: 0.9141495277129423]
	TIME [epoch: 8.87 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5240567001257468		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5676443689773583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5458505345515526 | validation: 0.6831479090231452]
	TIME [epoch: 8.87 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6291453787186146		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6580046432164491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6435750109675319 | validation: 0.5705859088716194]
	TIME [epoch: 8.86 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5462184648300598		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7508910192859194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6485547420579894 | validation: 0.7356446392006362]
	TIME [epoch: 8.89 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7878702835747591		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.663456055088574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7256631693316665 | validation: 0.3053070020034977]
	TIME [epoch: 8.87 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.610125160738143		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6384246216884826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6242748912133127 | validation: 0.8015106656942028]
	TIME [epoch: 8.87 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5818183915106261		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5474450158553765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5646317036830013 | validation: 0.737239859573628]
	TIME [epoch: 8.86 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5952684757546247		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5155573521889506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5554129139717876 | validation: 0.3865701699470906]
	TIME [epoch: 8.88 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5694014681135273		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5737137057482647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.571557586930896 | validation: 1.4012555709375492]
	TIME [epoch: 8.88 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.678968426083584		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4485521673130567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5637602966983202 | validation: 1.098643849341328]
	TIME [epoch: 8.87 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5451642022571342		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6235066656408452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5843354339489898 | validation: 0.7903376094943233]
	TIME [epoch: 8.86 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6363151602902366		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4908141727559518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5635646665230942 | validation: 0.714801506585939]
	TIME [epoch: 8.86 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5917438585756136		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5145101885825796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5531270235790966 | validation: 0.3051131781678849]
	TIME [epoch: 8.89 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47729772587420644		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5697509151157505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5235243204949783 | validation: 0.31148381244350387]
	TIME [epoch: 8.86 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5721516007890783		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5772375628596851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5746945818243816 | validation: 0.39173970836870364]
	TIME [epoch: 8.86 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5056951572061954		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.504568867974319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5051320125902572 | validation: 0.3550786789784922]
	TIME [epoch: 8.86 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.618454888445613		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4668656831031736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5426602857743934 | validation: 0.18650908462132687]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.738739120258396		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6301104303088019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.684424775283599 | validation: 0.4026253617749571]
	TIME [epoch: 8.87 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5005788239886726		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4521066719072839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47634274794797815 | validation: 0.4984597460931838]
	TIME [epoch: 8.86 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4980476818474239		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49217058728862034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4951091345680222 | validation: 0.7582585811932269]
	TIME [epoch: 8.86 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5303956133107847		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.534697623522531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5325466184166578 | validation: 0.4362216562804936]
	TIME [epoch: 8.87 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7127075665786657		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40814124852401135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5604244075513385 | validation: 0.30366285689382755]
	TIME [epoch: 8.89 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5070142351213696		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5219144025099245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.514464318815647 | validation: 0.919734841595132]
	TIME [epoch: 8.86 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5757312706204796		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5555588859113567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5656450782659181 | validation: 0.48518398944422325]
	TIME [epoch: 8.86 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6653169559025185		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6764578512148287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6708874035586737 | validation: 0.5222222217495777]
	TIME [epoch: 8.86 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47471886323333495		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4333939171106427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45405639017198884 | validation: 0.5329231081458161]
	TIME [epoch: 8.89 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46259565555293064		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6811557804747768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5718757180138537 | validation: 0.4794935695532905]
	TIME [epoch: 8.86 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47289237861602124		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5292411557373805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5010667671767008 | validation: 0.5362743914096693]
	TIME [epoch: 8.86 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5644593906605399		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4962471992398501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.530353294950195 | validation: 0.4904254845723958]
	TIME [epoch: 8.86 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5317136764804946		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49924571880379603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5154796976421454 | validation: 0.24378355094838067]
	TIME [epoch: 8.89 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4487570934098885		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5393623881492287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49405974077955844 | validation: 0.30523045321954334]
	TIME [epoch: 8.87 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4661273215943484		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6496065740772929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5578669478358206 | validation: 0.2932272852343173]
	TIME [epoch: 8.86 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5048408230302991		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4765764708258887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4907086469280938 | validation: 0.6108286605098578]
	TIME [epoch: 8.86 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5174940927284288		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.551374856402684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5344344745655565 | validation: 0.9373358992402808]
	TIME [epoch: 8.89 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5198724515561841		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4963037185998366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5080880850780104 | validation: 0.4983268322107624]
	TIME [epoch: 8.87 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45560546110453615		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5845831128198823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5200942869622092 | validation: 0.3985350772479045]
	TIME [epoch: 8.86 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5368858256110863		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5334571547126106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5351714901618484 | validation: 0.22787221091439686]
	TIME [epoch: 8.86 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5497254111339954		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6934341230618212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6215797670979083 | validation: 0.8304036523672378]
	TIME [epoch: 8.87 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5930735575191498		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6460945023915956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6195840299553727 | validation: 0.6106626793543526]
	TIME [epoch: 8.88 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5661653168493219		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5554720413172021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5608186790832619 | validation: 0.30307192673138217]
	TIME [epoch: 8.87 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5430131511840108		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6125146636903253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5777639074371681 | validation: 0.5964811363258409]
	TIME [epoch: 8.86 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4896560033016172		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6876494334965956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5886527183991062 | validation: 0.46868979402243977]
	TIME [epoch: 8.87 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5227246523140057		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5610947471653398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5419096997396726 | validation: 0.5041732941028496]
	TIME [epoch: 8.89 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4721712177867488		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.630582644390939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5513769310888439 | validation: 0.5404954122098684]
	TIME [epoch: 8.86 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6544549197411322		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5615559973997103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6080054585704212 | validation: 0.5980481460157956]
	TIME [epoch: 8.88 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4808971040466502		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4837897622067541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48234343312670214 | validation: 0.42699112083750657]
	TIME [epoch: 8.86 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6183184745021892		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4898202860946818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5540693802984356 | validation: 0.665669374290224]
	TIME [epoch: 8.89 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44725811181470787		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5399131812193796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49358564651704384 | validation: 0.43047169173275357]
	TIME [epoch: 8.87 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5301552991776798		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5109680113261458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5205616552519127 | validation: 0.3205283129635742]
	TIME [epoch: 8.87 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.565412456811085		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4729619040032345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5191871804071597 | validation: 0.5489832577351086]
	TIME [epoch: 8.86 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5405079143206059		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5888533541959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5646806342582529 | validation: 0.5000761138812547]
	TIME [epoch: 8.88 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44281353228389675		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.558110480143853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5004620062138749 | validation: 0.7316379984884144]
	TIME [epoch: 8.88 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6482537019080852		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48213626625815265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5651949840831191 | validation: 0.27197411994869497]
	TIME [epoch: 8.86 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4315455262428601		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5196907869788978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.475618156610879 | validation: 0.5675386394360096]
	TIME [epoch: 8.86 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5419309601622231		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4570707171696248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4995008386659238 | validation: 0.24515276999069915]
	TIME [epoch: 8.87 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4273213407936941		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.646530226824673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5369257838091835 | validation: 0.35672343896725783]
	TIME [epoch: 8.88 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4047034129424564		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.526618611477012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46566101220973427 | validation: 0.5118898439274475]
	TIME [epoch: 8.86 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4749499216027708		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5379005504483494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5064252360255601 | validation: 0.34431905890017617]
	TIME [epoch: 8.86 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5294891259712322		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5337803558073165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5316347408892744 | validation: 0.8319238196519277]
	TIME [epoch: 8.86 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5457056809089185		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4580348532451385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5018702670770284 | validation: 0.5461093336400298]
	TIME [epoch: 8.89 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5098394995494853		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4458116177896948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47782555866959003 | validation: 0.5182432371057812]
	TIME [epoch: 8.87 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5231957911481139		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6826496575580563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6029227243530851 | validation: 0.3267557680245268]
	TIME [epoch: 8.86 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44113164822393325		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5296638233208839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4853977357724085 | validation: 0.347342759582822]
	TIME [epoch: 8.86 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4766510829005662		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6475349143167298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5620929986086481 | validation: 0.6757664369393073]
	TIME [epoch: 8.89 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4592828617470717		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5892426466771574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5242627542121145 | validation: 0.3838479688139948]
	TIME [epoch: 10 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5044576949648586		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.44195021305567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4732039540102642 | validation: 0.18179316216981745]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r1_20240216_185744/states/model_tr_study1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40396450141335877		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6034180360060633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.503691268709711 | validation: 0.8933485277197113]
	TIME [epoch: 8.86 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5845468943186741		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40448465160071995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49451577295969706 | validation: 0.45637397077315156]
	TIME [epoch: 8.87 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4355446890772		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5051427693188814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4703437291980406 | validation: 0.23259443551418135]
	TIME [epoch: 8.87 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5259748183589652		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5147238672026424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5203493427808038 | validation: 0.8381124547270133]
	TIME [epoch: 8.86 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5043173828085286		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4285871494555211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46645226613202484 | validation: 0.6071202799442802]
	TIME [epoch: 8.85 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45918492725667176		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5039686648932438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4815767960749578 | validation: 0.2676153393901668]
	TIME [epoch: 8.85 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4825554742828997		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4293977784928008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4559766263878503 | validation: 0.34660723079442546]
	TIME [epoch: 8.89 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32723779615523935		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38849747186290634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3578676340090728 | validation: 0.3807913421775827]
	TIME [epoch: 8.86 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4589732325580407		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40441740341443155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4316953179862361 | validation: 0.49211616181328427]
	TIME [epoch: 8.86 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7914696539261648		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48388213889844345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6376758964123039 | validation: 0.5559297926245256]
	TIME [epoch: 8.86 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5596992390619027		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5701196539256858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5649094464937942 | validation: 0.4489268946513728]
	TIME [epoch: 8.87 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3362318345001577		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4938201047118005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41502596960597915 | validation: 0.6035383884143123]
	TIME [epoch: 8.84 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4802007369408729		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5124007825654106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49630075975314175 | validation: 0.533711373825983]
	TIME [epoch: 8.89 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39785328846692947		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45592941066381154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4268913495653706 | validation: 0.31432490661546275]
	TIME [epoch: 8.89 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5188947037210516		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4595719706360663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4892333371785589 | validation: 0.3068726746448488]
	TIME [epoch: 8.9 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4433393924907456		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5534151024204188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4983772474555822 | validation: 0.7335386898999801]
	TIME [epoch: 8.91 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5760148619320759		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.37902314277820615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4775190023551411 | validation: 0.39931309778449453]
	TIME [epoch: 8.88 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5513479223228612		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5135514167551994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5324496695390304 | validation: 0.28913489809496495]
	TIME [epoch: 8.86 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4458847541797321		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.656071989597401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5509783718885666 | validation: 0.32218920211550617]
	TIME [epoch: 8.89 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4351894116119178		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4596691545440037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4474292830779608 | validation: 0.6794925801784671]
	TIME [epoch: 8.89 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4799409513171886		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5014476525402819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4906943019287352 | validation: 0.6638442341290083]
	TIME [epoch: 8.86 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5879141356717967		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5249856850004276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5564499103361121 | validation: 0.5080741218166661]
	TIME [epoch: 8.85 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5436774983188215		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5816981274818133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5626878129003174 | validation: 0.5566101386209112]
	TIME [epoch: 8.86 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4890337475358784		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.547034477844219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5180341126900487 | validation: 0.2606801432885152]
	TIME [epoch: 8.88 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48598059879822475		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4532539692331158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46961728401567016 | validation: 1.4209591616539916]
	TIME [epoch: 8.86 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7794239377942522		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5786208537801252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6790223957871887 | validation: 0.7826666291218102]
	TIME [epoch: 8.86 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6537461897589283		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4431719924768324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5484590911178804 | validation: 0.9928128780665173]
	TIME [epoch: 8.86 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.515196032711806		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48852370491652863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5018598688141673 | validation: 0.4692929694508829]
	TIME [epoch: 8.88 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47726687098618636		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.526028465494241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5016476682402137 | validation: 0.8242841617916018]
	TIME [epoch: 8.86 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4672737433710153		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.50354999536178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4854118693663977 | validation: 0.5086669009081876]
	TIME [epoch: 8.85 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48771583324961576		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5567159260592539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.522215879654435 | validation: 0.6100495449114587]
	TIME [epoch: 8.84 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4883508437005187		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.582201454499818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5352761491001683 | validation: 0.5997601767801626]
	TIME [epoch: 8.86 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5877392063730578		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5123764370369616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5500578217050097 | validation: 0.6037389609573218]
	TIME [epoch: 9.76 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48171459868664046		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7658696419938721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6237921203402562 | validation: 0.619501684139075]
	TIME [epoch: 8.85 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6122417290495485		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4769249442779494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.544583336663749 | validation: 0.34366707059886153]
	TIME [epoch: 8.88 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5799967008268785		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5964632792310331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5882299900289558 | validation: 0.30709438784224324]
	TIME [epoch: 8.89 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5134366508913171		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4507841771228197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48211041400706833 | validation: 0.5180287859145585]
	TIME [epoch: 8.9 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7220803341148113		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4065705782295904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5643254561722009 | validation: 0.4329953474114441]
	TIME [epoch: 8.88 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7417103862035692		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9468226841553715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8442665351794704 | validation: 1.059479382243842]
	TIME [epoch: 8.84 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6677090905749538		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.573570834258081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6206399624165174 | validation: 0.74445228434126]
	TIME [epoch: 8.87 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4311294127374793		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5251433731545179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4781363929459985 | validation: 0.4578029769568134]
	TIME [epoch: 8.9 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4471871461602584		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5619273832061138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.504557264683186 | validation: 0.33323388662864084]
	TIME [epoch: 8.86 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5184419369438386		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4965132147038552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5074775758238468 | validation: 0.3257517182014793]
	TIME [epoch: 8.85 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5952005039875651		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5592678652357801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5772341846116728 | validation: 0.5286313928013662]
	TIME [epoch: 8.85 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7534961554969059		[learning rate: 0.01]
		[batch 20/20] avg loss: nan		[learning rate: 0.01]
ERROR:
nan encountered in epoch 214 (training loss).
