Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r5', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3899088934

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.717067901952761		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.401094442980147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.059081172466453 | validation: 8.57089936876987]
	TIME [epoch: 53.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.182201748375142		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.891946492371538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.03707412037334 | validation: 7.238254247458054]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.7736255281346915		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.160069343312991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.46684743572384 | validation: 5.331860801270866]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.219050583928825		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.108555321620376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1638029527746 | validation: 8.615324713343865]
	TIME [epoch: 8.44 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.309497656359754		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.68628601981784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.4978918380887976 | validation: 5.060994091893754]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.247725791652102		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.005936295212305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.126831043432205 | validation: 5.8934880953973945]
	TIME [epoch: 8.46 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.105027730534116		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.837230282167572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.971129006350844 | validation: 5.088051176977217]
	TIME [epoch: 8.44 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.166121589455474		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.7403681595354685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.953244874495471 | validation: 5.130465215205136]
	TIME [epoch: 8.45 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.012636436902514		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.655333615981972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.833985026442242 | validation: 4.692999838048724]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.565117442421756		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.335290157034104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.45020379972793 | validation: 4.687383212579753]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.418652909617829		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.3560636986770245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.387358304147427 | validation: 4.24341019684529]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.257236204330854		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.229761525742772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.243498865036813 | validation: 4.5712622727251055]
	TIME [epoch: 8.46 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.086018082809533		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.050567641044692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.068292861927112 | validation: 4.12498912509287]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.953552024640215		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.212719271171811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.083135647906014 | validation: 3.8739955636389256]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.64784117074688		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.803585191010743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.725713180878811 | validation: 5.564382035183855]
	TIME [epoch: 8.44 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.639334229907528		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.541303607735787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.590318918821659 | validation: 3.4961615768608945]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.447702595284252		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.385824880859542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.416763738071896 | validation: 3.335890562455562]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.240227264084373		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.428166719774869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3341969919296215 | validation: 3.5637705692137516]
	TIME [epoch: 8.43 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9963362622175262		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.381124602630554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.18873043242404 | validation: 3.143135185719537]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9869683356195282		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.1381577462542385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.062563040936883 | validation: 2.958296312240657]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.144168636638826		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8017469247066096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9729577806727177 | validation: 3.1303755750919393]
	TIME [epoch: 8.45 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8686693663157996		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.1434726080201765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0060709871679885 | validation: 2.994185108108382]
	TIME [epoch: 8.43 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9556262423746587		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7164397085630703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8360329754688642 | validation: 3.1616444351489195]
	TIME [epoch: 8.43 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.780064015010873		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6783072991461507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7291856570785114 | validation: 2.9451065054701826]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.592275989811995		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6365070474590615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6143915186355287 | validation: 2.6072566237448944]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8635465589148317		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9178842603508106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3907154096328216 | validation: 1.7818613366586693]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8834690109729184		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9925818712624785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9380254411176985 | validation: 1.912701742791718]
	TIME [epoch: 8.45 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0307548257819		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0010657010433293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0159102634126143 | validation: 2.0239023616565155]
	TIME [epoch: 8.45 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.954111326608817		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9579480362583184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9560296814335676 | validation: 1.4318221331091963]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7241385413438757		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7724313997808712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7482849705623735 | validation: 2.4836759107516064]
	TIME [epoch: 8.45 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8629810228510266		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7494888584305408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8062349406407834 | validation: 1.24690480732855]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7744902738971677		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.051999199233085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.913244736565126 | validation: 1.5606558906113865]
	TIME [epoch: 8.43 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6921605452729023		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.689966684454621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6910636148637619 | validation: 1.3605047416063853]
	TIME [epoch: 8.48 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5619377880002525		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.804097579769768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.68301768388501 | validation: 1.259854386595187]
	TIME [epoch: 8.44 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5027776539769904		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5147813019898873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5087794779834387 | validation: 1.8453742828291149]
	TIME [epoch: 8.45 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.76703315873465		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5223976907437586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6447154247392046 | validation: 2.5073756255471666]
	TIME [epoch: 8.44 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7170505743158713		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6599349886950256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6884927815054485 | validation: 1.1892186120945878]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.481967760198173		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6717756531121837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5768717066551778 | validation: 1.2573008335426878]
	TIME [epoch: 8.44 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5682840696800553		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.421494016787514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4948890432337847 | validation: 1.266867770292461]
	TIME [epoch: 8.44 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5242387160442439		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4192460919532048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4717424039987241 | validation: 1.4656544716706703]
	TIME [epoch: 8.43 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3542851356030572		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6986163332658577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5264507344344576 | validation: 1.4561919895495266]
	TIME [epoch: 8.48 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5690811512274319		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5788089424819542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5739450468546932 | validation: 1.4012891366655251]
	TIME [epoch: 8.43 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.622977827862605		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.457006347561263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.539992087711934 | validation: 1.3612649831934904]
	TIME [epoch: 8.43 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4193766876821365		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5068573222594248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4631170049707807 | validation: 0.986230292891007]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3092040143822172		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.06916583051789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6891849224500537 | validation: 1.1332941651673791]
	TIME [epoch: 8.47 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5985526629238533		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5696261093818973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.084089386152876 | validation: 1.0798047179141]
	TIME [epoch: 8.43 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3685676855686268		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5569829570775162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4627753213230714 | validation: 1.5105947029758817]
	TIME [epoch: 8.43 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5153717562386904		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5590873588689371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5372295575538135 | validation: 1.4323274673189537]
	TIME [epoch: 8.45 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3083609419461548		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.48742662307404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3978937825100974 | validation: 1.3007887723443625]
	TIME [epoch: 8.45 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5004442320463884		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4129914210409837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4567178265436858 | validation: 1.501560491721176]
	TIME [epoch: 8.43 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.503512165469484		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3717941295153708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4376531474924275 | validation: 1.1404846588264101]
	TIME [epoch: 8.43 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2886395213243085		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5551978600508891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4219186906875985 | validation: 1.1943539467410866]
	TIME [epoch: 8.44 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.480113210773149		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.342343637780901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.411228424277025 | validation: 1.1336731656314778]
	TIME [epoch: 8.45 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3420561954220593		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.401908934657119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.371982565039589 | validation: 1.1918798309341825]
	TIME [epoch: 8.43 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4179149118443224		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3297587621276867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3738368369860043 | validation: 1.1335053800448722]
	TIME [epoch: 8.43 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.307762095488447		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4427503022059767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.375256198847212 | validation: 0.9418000921507876]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.510446754634764		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.44557331816326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4780100363990119 | validation: 1.3208136861086046]
	TIME [epoch: 8.44 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.644115873325331		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3335576029189442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4888367381221375 | validation: 1.5185651282998378]
	TIME [epoch: 8.42 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2810726399438157		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2219289066115446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2515007732776804 | validation: 1.4695516457540945]
	TIME [epoch: 8.43 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3349926265243348		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2586522262661666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2968224263952504 | validation: 0.8215152379070646]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.232843504244467		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3239698167840195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.278406660514243 | validation: 1.5802977852888123]
	TIME [epoch: 8.43 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2568087639371561		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3270967604162167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2919527621766862 | validation: 0.9558467089941813]
	TIME [epoch: 8.42 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3541128484570943		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2293176713176643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2917152598873798 | validation: 1.0403108755158677]
	TIME [epoch: 8.43 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.251443114992203		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3136560772173562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2825495961047797 | validation: 1.799223831512415]
	TIME [epoch: 8.45 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4456362931557263		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2894637937603448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3675500434580359 | validation: 1.1576230644047378]
	TIME [epoch: 8.43 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3152216266894206		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2933820160984077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3043018213939142 | validation: 1.4009343705419797]
	TIME [epoch: 8.42 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.502893617250241		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3575805414188422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4302370793345414 | validation: 1.0122186246766773]
	TIME [epoch: 8.46 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.180354007074539		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2531054761939973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2167297416342682 | validation: 0.9143879776784054]
	TIME [epoch: 8.44 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4373070606058347		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1096965007608424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2735017806833384 | validation: 1.1839265615160148]
	TIME [epoch: 8.43 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3640439271355649		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5698537081466095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4669488176410872 | validation: 1.0258805894777163]
	TIME [epoch: 8.44 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3287903391579128		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3537789782664549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.341284658712184 | validation: 1.7689170619303343]
	TIME [epoch: 8.46 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5622355032758848		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3818081543880247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.472021828831955 | validation: 1.4611329801672888]
	TIME [epoch: 8.43 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2738570399834257		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2276043698267787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.250730704905102 | validation: 1.5634827166087493]
	TIME [epoch: 8.45 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.213763052053438		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2341039299900498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.223933491021744 | validation: 0.8921019967206547]
	TIME [epoch: 8.44 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2492741189116034		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3231207301361363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2861974245238699 | validation: 0.873038754505806]
	TIME [epoch: 8.45 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1996215530120091		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2425682465186294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2210948997653195 | validation: 0.8888722233516301]
	TIME [epoch: 8.44 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.34747715328453		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1749543155702438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2612157344273869 | validation: 1.0217742526543159]
	TIME [epoch: 8.43 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1786439294056412		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.064434655364346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1215392923849936 | validation: 1.2569436881181992]
	TIME [epoch: 8.45 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2441675029129498		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1835615792506917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2138645410818207 | validation: 1.0866911584639316]
	TIME [epoch: 8.43 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2261950007664577		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1852795283980573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2057372645822577 | validation: 1.0210076787357145]
	TIME [epoch: 8.48 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2385097847459516		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1382405491408178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1883751669433849 | validation: 1.3759555096545175]
	TIME [epoch: 8.44 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3207450852669171		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2213569891265545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2710510371967358 | validation: 1.346237322436536]
	TIME [epoch: 8.47 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0714571590383364		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0994773119679875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0854672355031616 | validation: 0.7722120950945661]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1106080978784576		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0858833486043644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0982457232414107 | validation: 1.1003905419555968]
	TIME [epoch: 8.46 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0753743763699197		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0850367441414273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0802055602556737 | validation: 1.4120876057433556]
	TIME [epoch: 8.45 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.134259132038954		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2412435112771234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1877513216580389 | validation: 0.9641730866457712]
	TIME [epoch: 8.44 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1304872761911422		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.332154617125796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2313209466584694 | validation: 0.9966753721389893]
	TIME [epoch: 8.43 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4744381851668964		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0603258607947283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2673820229808124 | validation: 1.039083508811605]
	TIME [epoch: 8.42 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3628358861324483		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1251287878515208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2439823369919845 | validation: 1.2337041767867527]
	TIME [epoch: 8.45 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0519377710115627		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.33877571248398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1953567417477713 | validation: 0.7802298876501479]
	TIME [epoch: 8.45 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9751437775965256		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3098045681596229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1424741728780743 | validation: 1.361616180705444]
	TIME [epoch: 8.44 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.304205830449413		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.070052282033134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1871290562412735 | validation: 1.1397421916324142]
	TIME [epoch: 8.42 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0322621548764026		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0861750368056122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0592185958410074 | validation: 0.8962692408061557]
	TIME [epoch: 8.46 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9938362771341943		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0639863937913723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0289113354627832 | validation: 0.8231734712963308]
	TIME [epoch: 8.42 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0580177622826872		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0750422119238539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0665299871032705 | validation: 1.0403086765002696]
	TIME [epoch: 8.44 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0545970873002781		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1266661600495587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0906316236749183 | validation: 0.7659218776090024]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8769319350883688		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.119102855128698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9980173951085332 | validation: 0.9009874310919619]
	TIME [epoch: 8.45 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9534774173626935		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.281064183529619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1172708004461565 | validation: 0.6480063634509703]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.358566686455814		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1291694686976197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2438680775767166 | validation: 1.244116834402437]
	TIME [epoch: 8.45 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0167984802598231		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.109874156698605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0633363184792142 | validation: 0.9741007281829042]
	TIME [epoch: 8.48 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0840273989184692		[learning rate: 0.0099891]
		[batch 20/20] avg loss: 1.0202114479612407		[learning rate: 0.009977]
	Learning Rate: 0.009977
	LOSS [training: 1.0521194234398548 | validation: 1.1875049479983635]
	TIME [epoch: 8.45 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3130588753892776		[learning rate: 0.0099649]
		[batch 20/20] avg loss: 1.1538578549373968		[learning rate: 0.0099528]
	Learning Rate: 0.00995285
	LOSS [training: 1.233458365163337 | validation: 1.0865547320741789]
	TIME [epoch: 8.46 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.20338930382699		[learning rate: 0.0099408]
		[batch 20/20] avg loss: 1.095982566165155		[learning rate: 0.0099288]
	Learning Rate: 0.00992875
	LOSS [training: 1.1496859349960724 | validation: 1.1637270021630572]
	TIME [epoch: 8.44 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9990079411450546		[learning rate: 0.0099167]
		[batch 20/20] avg loss: 1.068000412592292		[learning rate: 0.0099047]
	Learning Rate: 0.00990472
	LOSS [training: 1.0335041768686732 | validation: 0.8355744125327911]
	TIME [epoch: 8.46 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1037633245618474		[learning rate: 0.0098927]
		[batch 20/20] avg loss: 1.284597926546288		[learning rate: 0.0098807]
	Learning Rate: 0.00988074
	LOSS [training: 1.1941806255540677 | validation: 0.6481694208215036]
	TIME [epoch: 8.46 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0522111122120594		[learning rate: 0.0098688]
		[batch 20/20] avg loss: 1.143268172221564		[learning rate: 0.0098568]
	Learning Rate: 0.00985682
	LOSS [training: 1.0977396422168115 | validation: 1.4807594194962923]
	TIME [epoch: 8.46 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2736839330159708		[learning rate: 0.0098449]
		[batch 20/20] avg loss: 1.1571828828355384		[learning rate: 0.009833]
	Learning Rate: 0.00983296
	LOSS [training: 1.2154334079257545 | validation: 0.8512451825971898]
	TIME [epoch: 8.5 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.116937887111591		[learning rate: 0.009821]
		[batch 20/20] avg loss: 1.30906868559863		[learning rate: 0.0098092]
	Learning Rate: 0.00980915
	LOSS [training: 1.2130032863551103 | validation: 1.0204523752941435]
	TIME [epoch: 8.49 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3243658125749551		[learning rate: 0.0097973]
		[batch 20/20] avg loss: 1.1178706076331881		[learning rate: 0.0097854]
	Learning Rate: 0.00978541
	LOSS [training: 1.221118210104072 | validation: 1.1420816868352561]
	TIME [epoch: 8.45 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7591215696747717		[learning rate: 0.0097736]
		[batch 20/20] avg loss: 1.073954983021092		[learning rate: 0.0097617]
	Learning Rate: 0.00976172
	LOSS [training: 1.416538276347932 | validation: 0.8193875056747538]
	TIME [epoch: 8.51 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0491668752889336		[learning rate: 0.0097499]
		[batch 20/20] avg loss: 1.0341882990209772		[learning rate: 0.0097381]
	Learning Rate: 0.00973809
	LOSS [training: 1.0416775871549555 | validation: 0.9605779173659934]
	TIME [epoch: 8.45 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9618614262964791		[learning rate: 0.0097263]
		[batch 20/20] avg loss: 0.8829975105501771		[learning rate: 0.0097145]
	Learning Rate: 0.00971451
	LOSS [training: 0.9224294684233281 | validation: 1.0190701444515997]
	TIME [epoch: 8.46 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9824204576217113		[learning rate: 0.0097027]
		[batch 20/20] avg loss: 1.5336670188252068		[learning rate: 0.009691]
	Learning Rate: 0.009691
	LOSS [training: 1.2580437382234593 | validation: 1.2680872197047997]
	TIME [epoch: 8.46 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0227425807915063		[learning rate: 0.0096793]
		[batch 20/20] avg loss: 1.0757919566688994		[learning rate: 0.0096675]
	Learning Rate: 0.00966754
	LOSS [training: 1.0492672687302025 | validation: 0.7844137557055813]
	TIME [epoch: 8.46 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9165161046107798		[learning rate: 0.0096558]
		[batch 20/20] avg loss: 0.902937195450348		[learning rate: 0.0096441]
	Learning Rate: 0.00964413
	LOSS [training: 0.909726650030564 | validation: 0.5366907045521518]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8282542126389696		[learning rate: 0.0096325]
		[batch 20/20] avg loss: 0.7286419939399427		[learning rate: 0.0096208]
	Learning Rate: 0.00962078
	LOSS [training: 0.7784481032894564 | validation: 0.5242543203543378]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8065298099479834		[learning rate: 0.0096091]
		[batch 20/20] avg loss: 0.7927840993125236		[learning rate: 0.0095975]
	Learning Rate: 0.00959749
	LOSS [training: 0.7996569546302534 | validation: 0.5966538093672452]
	TIME [epoch: 8.45 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8628970111373118		[learning rate: 0.0095859]
		[batch 20/20] avg loss: 0.7168644379896211		[learning rate: 0.0095743]
	Learning Rate: 0.00957426
	LOSS [training: 0.7898807245634664 | validation: 0.4245484244395772]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7447236287523055		[learning rate: 0.0095627]
		[batch 20/20] avg loss: 0.8878962945538589		[learning rate: 0.0095511]
	Learning Rate: 0.00955108
	LOSS [training: 0.8163099616530822 | validation: 0.7460673072852038]
	TIME [epoch: 8.46 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8135574114217597		[learning rate: 0.0095395]
		[batch 20/20] avg loss: 0.8426076115951482		[learning rate: 0.009528]
	Learning Rate: 0.00952796
	LOSS [training: 0.8280825115084541 | validation: 0.8016835495021344]
	TIME [epoch: 8.46 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9760627685412917		[learning rate: 0.0095164]
		[batch 20/20] avg loss: 1.0398186695104823		[learning rate: 0.0095049]
	Learning Rate: 0.0095049
	LOSS [training: 1.0079407190258869 | validation: 1.0717255680225333]
	TIME [epoch: 8.42 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8133517428564098		[learning rate: 0.0094934]
		[batch 20/20] avg loss: 0.9244931195162092		[learning rate: 0.0094819]
	Learning Rate: 0.00948189
	LOSS [training: 0.8689224311863095 | validation: 0.9160833575527599]
	TIME [epoch: 8.42 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7441425240614857		[learning rate: 0.0094704]
		[batch 20/20] avg loss: 0.8658514289264783		[learning rate: 0.0094589]
	Learning Rate: 0.00945893
	LOSS [training: 0.804996976493982 | validation: 0.4947840680465565]
	TIME [epoch: 8.44 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.834886483310205		[learning rate: 0.0094475]
		[batch 20/20] avg loss: 0.7153304235070329		[learning rate: 0.009436]
	Learning Rate: 0.00943603
	LOSS [training: 0.7751084534086191 | validation: 0.42768096943672673]
	TIME [epoch: 8.44 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9087018417908844		[learning rate: 0.0094246]
		[batch 20/20] avg loss: 0.9383180743731518		[learning rate: 0.0094132]
	Learning Rate: 0.00941319
	LOSS [training: 0.9235099580820181 | validation: 0.7137508315592147]
	TIME [epoch: 8.43 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1777816306002689		[learning rate: 0.0094018]
		[batch 20/20] avg loss: 0.9275653596791276		[learning rate: 0.0093904]
	Learning Rate: 0.0093904
	LOSS [training: 1.052673495139698 | validation: 0.6951874861654405]
	TIME [epoch: 8.42 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9044891714984378		[learning rate: 0.009379]
		[batch 20/20] avg loss: 0.9810325943727726		[learning rate: 0.0093677]
	Learning Rate: 0.00936767
	LOSS [training: 0.9427608829356051 | validation: 0.9663847048018838]
	TIME [epoch: 8.45 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7821061765218729		[learning rate: 0.0093563]
		[batch 20/20] avg loss: 0.8871840271440956		[learning rate: 0.009345]
	Learning Rate: 0.00934499
	LOSS [training: 0.8346451018329842 | validation: 0.8650046676911107]
	TIME [epoch: 8.43 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6796468510925822		[learning rate: 0.0093337]
		[batch 20/20] avg loss: 0.7533396860414071		[learning rate: 0.0093224]
	Learning Rate: 0.00932237
	LOSS [training: 0.7164932685669946 | validation: 0.5083020373496061]
	TIME [epoch: 8.42 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6539746527454231		[learning rate: 0.0093111]
		[batch 20/20] avg loss: 1.0172027460849118		[learning rate: 0.0092998]
	Learning Rate: 0.0092998
	LOSS [training: 0.8355886994151673 | validation: 0.871839926078763]
	TIME [epoch: 8.45 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8038731566139873		[learning rate: 0.0092885]
		[batch 20/20] avg loss: 0.8465330885184231		[learning rate: 0.0092773]
	Learning Rate: 0.00927729
	LOSS [training: 0.8252031225662055 | validation: 0.7251807560534527]
	TIME [epoch: 8.42 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.680879939516991		[learning rate: 0.0092661]
		[batch 20/20] avg loss: 0.8629315447806594		[learning rate: 0.0092548]
	Learning Rate: 0.00925483
	LOSS [training: 0.7719057421488251 | validation: 0.6043585395234112]
	TIME [epoch: 8.43 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6930923156713309		[learning rate: 0.0092436]
		[batch 20/20] avg loss: 0.6793036753427584		[learning rate: 0.0092324]
	Learning Rate: 0.00923242
	LOSS [training: 0.6861979955070446 | validation: 0.8358899500146033]
	TIME [epoch: 8.45 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6791955613739632		[learning rate: 0.0092212]
		[batch 20/20] avg loss: 0.7407002047097548		[learning rate: 0.0092101]
	Learning Rate: 0.00921007
	LOSS [training: 0.709947883041859 | validation: 1.3269740021033203]
	TIME [epoch: 8.48 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8613400150291544		[learning rate: 0.0091989]
		[batch 20/20] avg loss: 0.6221564618752637		[learning rate: 0.0091878]
	Learning Rate: 0.00918778
	LOSS [training: 0.7417482384522092 | validation: 0.6418324683405424]
	TIME [epoch: 8.41 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6048363462987575		[learning rate: 0.0091767]
		[batch 20/20] avg loss: 0.7650965505093916		[learning rate: 0.0091655]
	Learning Rate: 0.00916554
	LOSS [training: 0.6849664484040745 | validation: 0.7594775287638479]
	TIME [epoch: 8.43 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6775178398457423		[learning rate: 0.0091544]
		[batch 20/20] avg loss: 0.734347964191931		[learning rate: 0.0091433]
	Learning Rate: 0.00914335
	LOSS [training: 0.7059329020188368 | validation: 0.6595670615193874]
	TIME [epoch: 8.43 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6815747281964056		[learning rate: 0.0091323]
		[batch 20/20] avg loss: 0.6329879731427039		[learning rate: 0.0091212]
	Learning Rate: 0.00912121
	LOSS [training: 0.6572813506695548 | validation: 0.39008492629902186]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6346390068131342		[learning rate: 0.0091102]
		[batch 20/20] avg loss: 0.627617245683125		[learning rate: 0.0090991]
	Learning Rate: 0.00909913
	LOSS [training: 0.6311281262481295 | validation: 0.6206455547441065]
	TIME [epoch: 8.44 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6986559533953083		[learning rate: 0.0090881]
		[batch 20/20] avg loss: 0.8130564773910501		[learning rate: 0.0090771]
	Learning Rate: 0.0090771
	LOSS [training: 0.7558562153931793 | validation: 0.44033005055680696]
	TIME [epoch: 8.42 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5706137287243226		[learning rate: 0.0090661]
		[batch 20/20] avg loss: 0.5660970105084208		[learning rate: 0.0090551]
	Learning Rate: 0.00905513
	LOSS [training: 0.5683553696163716 | validation: 0.6361705174034067]
	TIME [epoch: 8.46 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6357688084907591		[learning rate: 0.0090442]
		[batch 20/20] avg loss: 0.6439003250758047		[learning rate: 0.0090332]
	Learning Rate: 0.00903321
	LOSS [training: 0.6398345667832819 | validation: 0.6458598526202975]
	TIME [epoch: 8.42 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6995250795110767		[learning rate: 0.0090223]
		[batch 20/20] avg loss: 0.5706121237942584		[learning rate: 0.0090113]
	Learning Rate: 0.00901134
	LOSS [training: 0.6350686016526674 | validation: 0.5311879658923375]
	TIME [epoch: 8.42 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6493807428072593		[learning rate: 0.0090004]
		[batch 20/20] avg loss: 0.6411064114991276		[learning rate: 0.0089895]
	Learning Rate: 0.00898953
	LOSS [training: 0.6452435771531935 | validation: 0.42610631159174195]
	TIME [epoch: 8.43 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6846770488883885		[learning rate: 0.0089786]
		[batch 20/20] avg loss: 0.7182583719104163		[learning rate: 0.0089678]
	Learning Rate: 0.00896776
	LOSS [training: 0.7014677103994026 | validation: 0.3743933163027039]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6881457509362854		[learning rate: 0.0089569]
		[batch 20/20] avg loss: 0.5722459808390912		[learning rate: 0.0089461]
	Learning Rate: 0.00894605
	LOSS [training: 0.6301958658876884 | validation: 0.5323097389240482]
	TIME [epoch: 8.44 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5903504416630506		[learning rate: 0.0089352]
		[batch 20/20] avg loss: 0.791128968556387		[learning rate: 0.0089244]
	Learning Rate: 0.0089244
	LOSS [training: 0.6907397051097188 | validation: 0.6813710769942449]
	TIME [epoch: 8.43 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6046142655138182		[learning rate: 0.0089136]
		[batch 20/20] avg loss: 0.5946621268855762		[learning rate: 0.0089028]
	Learning Rate: 0.00890279
	LOSS [training: 0.5996381961996973 | validation: 0.3586262311803267]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6929524471895027		[learning rate: 0.008892]
		[batch 20/20] avg loss: 0.7714107182564628		[learning rate: 0.0088812]
	Learning Rate: 0.00888124
	LOSS [training: 0.732181582722983 | validation: 0.6299095681069128]
	TIME [epoch: 8.48 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5290189686250091		[learning rate: 0.0088705]
		[batch 20/20] avg loss: 0.812345138120534		[learning rate: 0.0088597]
	Learning Rate: 0.00885974
	LOSS [training: 0.6706820533727715 | validation: 0.5775154914659885]
	TIME [epoch: 8.43 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6977376552268926		[learning rate: 0.008849]
		[batch 20/20] avg loss: 0.6689527219165902		[learning rate: 0.0088383]
	Learning Rate: 0.00883829
	LOSS [training: 0.6833451885717414 | validation: 0.5832709881831297]
	TIME [epoch: 8.43 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7290734065723016		[learning rate: 0.0088276]
		[batch 20/20] avg loss: 0.8756984826426433		[learning rate: 0.0088169]
	Learning Rate: 0.0088169
	LOSS [training: 0.8023859446074724 | validation: 0.9488096048305221]
	TIME [epoch: 8.48 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7528935193165122		[learning rate: 0.0088062]
		[batch 20/20] avg loss: 0.75200492057151		[learning rate: 0.0087956]
	Learning Rate: 0.00879555
	LOSS [training: 0.7524492199440113 | validation: 0.515355722464298]
	TIME [epoch: 8.44 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.601440704677807		[learning rate: 0.0087849]
		[batch 20/20] avg loss: 0.6162196370358182		[learning rate: 0.0087743]
	Learning Rate: 0.00877426
	LOSS [training: 0.6088301708568128 | validation: 0.37980520486757696]
	TIME [epoch: 8.43 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.729793197643899		[learning rate: 0.0087636]
		[batch 20/20] avg loss: 0.743889458847639		[learning rate: 0.008753]
	Learning Rate: 0.00875302
	LOSS [training: 0.736841328245769 | validation: 0.689907229081998]
	TIME [epoch: 8.48 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7435405068959684		[learning rate: 0.0087424]
		[batch 20/20] avg loss: 0.653591548434326		[learning rate: 0.0087318]
	Learning Rate: 0.00873183
	LOSS [training: 0.6985660276651472 | validation: 0.7375468173812552]
	TIME [epoch: 8.45 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6409205778361584		[learning rate: 0.0087213]
		[batch 20/20] avg loss: 0.8549200706479729		[learning rate: 0.0087107]
	Learning Rate: 0.00871069
	LOSS [training: 0.7479203242420656 | validation: 0.6908536273486628]
	TIME [epoch: 8.43 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6844159910365969		[learning rate: 0.0087001]
		[batch 20/20] avg loss: 0.686871384884436		[learning rate: 0.0086896]
	Learning Rate: 0.0086896
	LOSS [training: 0.6856436879605166 | validation: 0.7344903074703736]
	TIME [epoch: 8.43 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7924249390896516		[learning rate: 0.0086791]
		[batch 20/20] avg loss: 0.8145111667947619		[learning rate: 0.0086686]
	Learning Rate: 0.00866857
	LOSS [training: 0.8034680529422069 | validation: 0.681092397272537]
	TIME [epoch: 8.43 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6716200949504548		[learning rate: 0.0086581]
		[batch 20/20] avg loss: 0.9428390973412488		[learning rate: 0.0086476]
	Learning Rate: 0.00864758
	LOSS [training: 0.8072295961458519 | validation: 0.5253229707013898]
	TIME [epoch: 8.46 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.738909077905762		[learning rate: 0.0086371]
		[batch 20/20] avg loss: 0.7227576093791761		[learning rate: 0.0086266]
	Learning Rate: 0.00862665
	LOSS [training: 0.730833343642469 | validation: 0.541290498948071]
	TIME [epoch: 8.42 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5308616312840436		[learning rate: 0.0086162]
		[batch 20/20] avg loss: 0.5707002827582749		[learning rate: 0.0086058]
	Learning Rate: 0.00860576
	LOSS [training: 0.5507809570211591 | validation: 0.6548754573577248]
	TIME [epoch: 8.49 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6932167163457261		[learning rate: 0.0085953]
		[batch 20/20] avg loss: 0.7622071790152505		[learning rate: 0.0085849]
	Learning Rate: 0.00858493
	LOSS [training: 0.7277119476804884 | validation: 0.717122815498336]
	TIME [epoch: 8.43 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5420508153643413		[learning rate: 0.0085745]
		[batch 20/20] avg loss: 0.6041670624544256		[learning rate: 0.0085641]
	Learning Rate: 0.00856415
	LOSS [training: 0.5731089389093833 | validation: 1.0349536748696861]
	TIME [epoch: 8.44 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6666113919745678		[learning rate: 0.0085538]
		[batch 20/20] avg loss: 0.590759394191941		[learning rate: 0.0085434]
	Learning Rate: 0.00854342
	LOSS [training: 0.6286853930832544 | validation: 0.6536858785113857]
	TIME [epoch: 8.43 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6109886176005657		[learning rate: 0.0085331]
		[batch 20/20] avg loss: 0.6426134501886416		[learning rate: 0.0085227]
	Learning Rate: 0.00852273
	LOSS [training: 0.6268010338946036 | validation: 0.9213202537458237]
	TIME [epoch: 8.44 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7482121873425114		[learning rate: 0.0085124]
		[batch 20/20] avg loss: 0.5319245790166197		[learning rate: 0.0085021]
	Learning Rate: 0.0085021
	LOSS [training: 0.6400683831795656 | validation: 0.4549833750639347]
	TIME [epoch: 8.46 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5830948844003261		[learning rate: 0.0084918]
		[batch 20/20] avg loss: 0.7110538810068361		[learning rate: 0.0084815]
	Learning Rate: 0.00848152
	LOSS [training: 0.6470743827035811 | validation: 0.82436821929497]
	TIME [epoch: 8.44 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.722265161513578		[learning rate: 0.0084712]
		[batch 20/20] avg loss: 0.553210782648888		[learning rate: 0.008461]
	Learning Rate: 0.00846099
	LOSS [training: 0.6377379720812331 | validation: 0.3756807215050246]
	TIME [epoch: 8.42 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5911645676991002		[learning rate: 0.0084507]
		[batch 20/20] avg loss: 0.5359869213283808		[learning rate: 0.0084405]
	Learning Rate: 0.0084405
	LOSS [training: 0.5635757445137404 | validation: 0.5439788368444549]
	TIME [epoch: 8.46 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6535308131644612		[learning rate: 0.0084303]
		[batch 20/20] avg loss: 0.45561895349669584		[learning rate: 0.0084201]
	Learning Rate: 0.00842007
	LOSS [training: 0.5545748833305785 | validation: 0.2767362825110139]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5597225789826503		[learning rate: 0.0084099]
		[batch 20/20] avg loss: 0.5938604199399795		[learning rate: 0.0083997]
	Learning Rate: 0.00839969
	LOSS [training: 0.5767914994613148 | validation: 0.3392902446060608]
	TIME [epoch: 8.43 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5909459828411023		[learning rate: 0.0083895]
		[batch 20/20] avg loss: 0.5428553192148029		[learning rate: 0.0083794]
	Learning Rate: 0.00837935
	LOSS [training: 0.5669006510279526 | validation: 0.6240009010295409]
	TIME [epoch: 8.43 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5424764638662214		[learning rate: 0.0083692]
		[batch 20/20] avg loss: 0.7597168747170511		[learning rate: 0.0083591]
	Learning Rate: 0.00835907
	LOSS [training: 0.6510966692916361 | validation: 0.6082351564918813]
	TIME [epoch: 8.45 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4595259976033274		[learning rate: 0.0083489]
		[batch 20/20] avg loss: 0.5813762329801708		[learning rate: 0.0083388]
	Learning Rate: 0.00833883
	LOSS [training: 0.5204511152917493 | validation: 0.39075166730347677]
	TIME [epoch: 8.45 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6074295348934757		[learning rate: 0.0083287]
		[batch 20/20] avg loss: 0.5514826532095445		[learning rate: 0.0083186]
	Learning Rate: 0.00831864
	LOSS [training: 0.5794560940515101 | validation: 0.48192950976914856]
	TIME [epoch: 8.46 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.623687188199308		[learning rate: 0.0083086]
		[batch 20/20] avg loss: 0.5221489217658754		[learning rate: 0.0082985]
	Learning Rate: 0.00829851
	LOSS [training: 0.5729180549825916 | validation: 0.8828327706709176]
	TIME [epoch: 8.42 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8130537856406784		[learning rate: 0.0082885]
		[batch 20/20] avg loss: 0.6472606077982427		[learning rate: 0.0082784]
	Learning Rate: 0.00827842
	LOSS [training: 0.7301571967194607 | validation: 0.7494219896750213]
	TIME [epoch: 8.46 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6136337764609047		[learning rate: 0.0082684]
		[batch 20/20] avg loss: 0.589729477576947		[learning rate: 0.0082584]
	Learning Rate: 0.00825838
	LOSS [training: 0.6016816270189258 | validation: 0.745898329062819]
	TIME [epoch: 8.43 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6631941922522667		[learning rate: 0.0082484]
		[batch 20/20] avg loss: 0.60230771087016		[learning rate: 0.0082384]
	Learning Rate: 0.00823839
	LOSS [training: 0.6327509515612133 | validation: 0.5911239858548455]
	TIME [epoch: 8.43 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5301584230163314		[learning rate: 0.0082284]
		[batch 20/20] avg loss: 0.568768456322458		[learning rate: 0.0082184]
	Learning Rate: 0.00821844
	LOSS [training: 0.5494634396693947 | validation: 0.2314726009302805]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r5_20240219_183143/states/model_tr_study1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5551673463605657		[learning rate: 0.0082085]
		[batch 20/20] avg loss: 0.4896262168944916		[learning rate: 0.0081985]
	Learning Rate: 0.00819855
	LOSS [training: 0.5223967816275288 | validation: 0.7349459499650595]
	TIME [epoch: 8.46 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7487379032808492		[learning rate: 0.0081886]
		[batch 20/20] avg loss: 0.47615487585779154		[learning rate: 0.0081787]
	Learning Rate: 0.0081787
	LOSS [training: 0.6124463895693204 | validation: 0.4087309236708917]
	TIME [epoch: 8.42 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6268280400691433		[learning rate: 0.0081688]
		[batch 20/20] avg loss: 0.5244238239423608		[learning rate: 0.0081589]
	Learning Rate: 0.0081589
	LOSS [training: 0.575625932005752 | validation: 0.3972836745047622]
	TIME [epoch: 8.44 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6918609347112871		[learning rate: 0.008149]
		[batch 20/20] avg loss: 0.5300440030433262		[learning rate: 0.0081391]
	Learning Rate: 0.00813915
	LOSS [training: 0.6109524688773067 | validation: 0.3120437799324617]
	TIME [epoch: 8.43 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.633204001647601		[learning rate: 0.0081293]
		[batch 20/20] avg loss: 0.6813497803056823		[learning rate: 0.0081194]
	Learning Rate: 0.00811944
	LOSS [training: 0.6572768909766418 | validation: 0.6478414658432412]
	TIME [epoch: 8.44 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7343648779611545		[learning rate: 0.0081096]
		[batch 20/20] avg loss: 0.5189868203382423		[learning rate: 0.0080998]
	Learning Rate: 0.00809979
	LOSS [training: 0.6266758491496983 | validation: 0.5683421643641617]
	TIME [epoch: 8.43 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5890047467031247		[learning rate: 0.00809]
		[batch 20/20] avg loss: 0.5623080444313852		[learning rate: 0.0080802]
	Learning Rate: 0.00808018
	LOSS [training: 0.5756563955672548 | validation: 0.6489924861661551]
	TIME [epoch: 8.44 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6606813665177584		[learning rate: 0.0080704]
		[batch 20/20] avg loss: 0.8018866629946816		[learning rate: 0.0080606]
	Learning Rate: 0.00806062
	LOSS [training: 0.7312840147562201 | validation: 0.335973107536796]
	TIME [epoch: 8.42 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5975660834089878		[learning rate: 0.0080509]
		[batch 20/20] avg loss: 0.5338345957577769		[learning rate: 0.0080411]
	Learning Rate: 0.00804111
	LOSS [training: 0.5657003395833824 | validation: 0.30267990290010816]
	TIME [epoch: 8.44 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5685895399751913		[learning rate: 0.0080314]
		[batch 20/20] avg loss: 0.739244973073782		[learning rate: 0.0080216]
	Learning Rate: 0.00802164
	LOSS [training: 0.6539172565244867 | validation: 0.2725485380674249]
	TIME [epoch: 8.42 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7136638083093187		[learning rate: 0.0080119]
		[batch 20/20] avg loss: 0.6253107096055326		[learning rate: 0.0080022]
	Learning Rate: 0.00800222
	LOSS [training: 0.6694872589574257 | validation: 0.7560155659873499]
	TIME [epoch: 8.42 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5156572463240139		[learning rate: 0.0079925]
		[batch 20/20] avg loss: 0.5618762626262648		[learning rate: 0.0079828]
	Learning Rate: 0.00798285
	LOSS [training: 0.5387667544751393 | validation: 0.6571853094613902]
	TIME [epoch: 8.46 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5894586656161824		[learning rate: 0.0079732]
		[batch 20/20] avg loss: 0.5064864313497928		[learning rate: 0.0079635]
	Learning Rate: 0.00796352
	LOSS [training: 0.5479725484829876 | validation: 0.32591627432220105]
	TIME [epoch: 8.44 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4841096699774633		[learning rate: 0.0079539]
		[batch 20/20] avg loss: 0.6235810618601234		[learning rate: 0.0079442]
	Learning Rate: 0.00794424
	LOSS [training: 0.5538453659187934 | validation: 0.30810104153383955]
	TIME [epoch: 8.45 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5425450841183072		[learning rate: 0.0079346]
		[batch 20/20] avg loss: 0.6767110041539774		[learning rate: 0.007925]
	Learning Rate: 0.00792501
	LOSS [training: 0.6096280441361424 | validation: 0.5766846615973337]
	TIME [epoch: 8.47 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4800438954035114		[learning rate: 0.0079154]
		[batch 20/20] avg loss: 0.6776153431624204		[learning rate: 0.0079058]
	Learning Rate: 0.00790583
	LOSS [training: 0.5788296192829658 | validation: 0.28074889631482625]
	TIME [epoch: 8.45 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4908952716122184		[learning rate: 0.0078963]
		[batch 20/20] avg loss: 0.48949328399509306		[learning rate: 0.0078867]
	Learning Rate: 0.00788669
	LOSS [training: 0.4901942778036557 | validation: 0.31079892835123263]
	TIME [epoch: 8.46 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5553317248572929		[learning rate: 0.0078771]
		[batch 20/20] avg loss: 0.5521519034932941		[learning rate: 0.0078676]
	Learning Rate: 0.0078676
	LOSS [training: 0.5537418141752936 | validation: 0.3335966372502463]
	TIME [epoch: 8.45 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.536727922365084		[learning rate: 0.0078581]
		[batch 20/20] avg loss: 0.4100856684194355		[learning rate: 0.0078486]
	Learning Rate: 0.00784855
	LOSS [training: 0.4734067953922597 | validation: 0.4790657422106526]
	TIME [epoch: 8.45 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5936549460284166		[learning rate: 0.007839]
		[batch 20/20] avg loss: 0.6162404110447111		[learning rate: 0.0078296]
	Learning Rate: 0.00782955
	LOSS [training: 0.6049476785365637 | validation: 0.7357029179863193]
	TIME [epoch: 8.44 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5291140897896407		[learning rate: 0.0078201]
		[batch 20/20] avg loss: 0.5188354241994803		[learning rate: 0.0078106]
	Learning Rate: 0.0078106
	LOSS [training: 0.5239747569945603 | validation: 0.8153283049497606]
	TIME [epoch: 8.44 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7971417957948499		[learning rate: 0.0078011]
		[batch 20/20] avg loss: 0.5824339640281508		[learning rate: 0.0077917]
	Learning Rate: 0.00779169
	LOSS [training: 0.6897878799115004 | validation: 0.5086917874225985]
	TIME [epoch: 8.46 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48503099098470626		[learning rate: 0.0077823]
		[batch 20/20] avg loss: 0.46885150786006974		[learning rate: 0.0077728]
	Learning Rate: 0.00777283
	LOSS [training: 0.476941249422388 | validation: 0.3677666027159335]
	TIME [epoch: 8.45 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6653227501369405		[learning rate: 0.0077634]
		[batch 20/20] avg loss: 0.5116574798881572		[learning rate: 0.007754]
	Learning Rate: 0.00775401
	LOSS [training: 0.5884901150125488 | validation: 0.4156239699280926]
	TIME [epoch: 8.42 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6557530927005132		[learning rate: 0.0077446]
		[batch 20/20] avg loss: 0.59895364307989		[learning rate: 0.0077352]
	Learning Rate: 0.00773524
	LOSS [training: 0.6273533678902016 | validation: 0.4043980380224991]
	TIME [epoch: 8.44 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8028618477782722		[learning rate: 0.0077259]
		[batch 20/20] avg loss: 0.5545685850221236		[learning rate: 0.0077165]
	Learning Rate: 0.00771651
	LOSS [training: 0.678715216400198 | validation: 0.31201584782671077]
	TIME [epoch: 8.45 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6917572687144511		[learning rate: 0.0077072]
		[batch 20/20] avg loss: 0.5037135972728445		[learning rate: 0.0076978]
	Learning Rate: 0.00769783
	LOSS [training: 0.5977354329936478 | validation: 1.115803943518634]
	TIME [epoch: 8.44 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6136997812304611		[learning rate: 0.0076885]
		[batch 20/20] avg loss: 0.6153146883711779		[learning rate: 0.0076792]
	Learning Rate: 0.0076792
	LOSS [training: 0.6145072348008195 | validation: 0.8536698547226483]
	TIME [epoch: 8.46 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.587974938944716		[learning rate: 0.0076699]
		[batch 20/20] avg loss: 0.7376676507751582		[learning rate: 0.0076606]
	Learning Rate: 0.00766061
	LOSS [training: 0.6628212948599371 | validation: 0.28789499087944925]
	TIME [epoch: 8.44 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7504747538463341		[learning rate: 0.0076513]
		[batch 20/20] avg loss: 0.4695149182736749		[learning rate: 0.0076421]
	Learning Rate: 0.00764206
	LOSS [training: 0.6099948360600045 | validation: 0.4240909938070079]
	TIME [epoch: 8.45 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.644245176969392		[learning rate: 0.0076328]
		[batch 20/20] avg loss: 0.5324066499808808		[learning rate: 0.0076236]
	Learning Rate: 0.00762356
	LOSS [training: 0.5883259134751364 | validation: 0.3029953921110585]
	TIME [epoch: 8.42 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7678032116372219		[learning rate: 0.0076143]
		[batch 20/20] avg loss: 0.535079947900388		[learning rate: 0.0076051]
	Learning Rate: 0.00760511
	LOSS [training: 0.6514415797688049 | validation: 0.3253642043477594]
	TIME [epoch: 8.43 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5497108205002623		[learning rate: 0.0075959]
		[batch 20/20] avg loss: 0.687567074134163		[learning rate: 0.0075867]
	Learning Rate: 0.00758669
	LOSS [training: 0.6186389473172127 | validation: 0.430936413103882]
	TIME [epoch: 8.44 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.517759141469029		[learning rate: 0.0075775]
		[batch 20/20] avg loss: 0.4608987454430853		[learning rate: 0.0075683]
	Learning Rate: 0.00756833
	LOSS [training: 0.489328943456057 | validation: 0.8749356531991327]
	TIME [epoch: 8.44 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6035258856208386		[learning rate: 0.0075592]
		[batch 20/20] avg loss: 0.5659235633779983		[learning rate: 0.00755]
	Learning Rate: 0.00755001
	LOSS [training: 0.5847247244994185 | validation: 0.4850782670375082]
	TIME [epoch: 8.43 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.690301625806153		[learning rate: 0.0075409]
		[batch 20/20] avg loss: 0.5660972394780148		[learning rate: 0.0075317]
	Learning Rate: 0.00753173
	LOSS [training: 0.6281994326420838 | validation: 0.6888222089136069]
	TIME [epoch: 8.42 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5302858220670149		[learning rate: 0.0075226]
		[batch 20/20] avg loss: 0.6210853362282985		[learning rate: 0.0075135]
	Learning Rate: 0.0075135
	LOSS [training: 0.5756855791476567 | validation: 0.3513907767017278]
	TIME [epoch: 8.45 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5570780819385119		[learning rate: 0.0075044]
		[batch 20/20] avg loss: 0.6563703483021289		[learning rate: 0.0074953]
	Learning Rate: 0.00749531
	LOSS [training: 0.6067242151203203 | validation: 0.4900027406589083]
	TIME [epoch: 8.45 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5070925469407064		[learning rate: 0.0074862]
		[batch 20/20] avg loss: 0.7597734898046762		[learning rate: 0.0074772]
	Learning Rate: 0.00747716
	LOSS [training: 0.6334330183726913 | validation: 0.5160190446602813]
	TIME [epoch: 8.44 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6439949314598319		[learning rate: 0.0074681]
		[batch 20/20] avg loss: 0.5033964695493663		[learning rate: 0.0074591]
	Learning Rate: 0.00745906
	LOSS [training: 0.5736957005045993 | validation: 0.3138895003615266]
	TIME [epoch: 8.43 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5527305881899112		[learning rate: 0.00745]
		[batch 20/20] avg loss: 0.5669273963252216		[learning rate: 0.007441]
	Learning Rate: 0.007441
	LOSS [training: 0.5598289922575664 | validation: 0.4814520379717323]
	TIME [epoch: 8.45 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5868882287431266		[learning rate: 0.007432]
		[batch 20/20] avg loss: 0.6228635759050972		[learning rate: 0.007423]
	Learning Rate: 0.00742299
	LOSS [training: 0.6048759023241119 | validation: 0.6230946276270917]
	TIME [epoch: 8.44 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6519949507131362		[learning rate: 0.007414]
		[batch 20/20] avg loss: 1.054422462050258		[learning rate: 0.007405]
	Learning Rate: 0.00740502
	LOSS [training: 0.853208706381697 | validation: 0.6515376180042869]
	TIME [epoch: 8.41 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7987352544129898		[learning rate: 0.0073961]
		[batch 20/20] avg loss: 0.45153114103202424		[learning rate: 0.0073871]
	Learning Rate: 0.0073871
	LOSS [training: 0.6251331977225069 | validation: 0.3084029522496499]
	TIME [epoch: 8.47 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5902936492456518		[learning rate: 0.0073781]
		[batch 20/20] avg loss: 0.568004883562738		[learning rate: 0.0073692]
	Learning Rate: 0.00736921
	LOSS [training: 0.5791492664041951 | validation: 0.4300578890973439]
	TIME [epoch: 8.42 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4863449837446942		[learning rate: 0.0073603]
		[batch 20/20] avg loss: 0.5096614163781028		[learning rate: 0.0073514]
	Learning Rate: 0.00735137
	LOSS [training: 0.4980032000613984 | validation: 0.5520602020318357]
	TIME [epoch: 8.44 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5264744591688393		[learning rate: 0.0073425]
		[batch 20/20] avg loss: 0.6974715329253501		[learning rate: 0.0073336]
	Learning Rate: 0.00733358
	LOSS [training: 0.6119729960470945 | validation: 0.4672727131009245]
	TIME [epoch: 8.42 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7050169769086707		[learning rate: 0.0073247]
		[batch 20/20] avg loss: 0.6346980340847939		[learning rate: 0.0073158]
	Learning Rate: 0.00731582
	LOSS [training: 0.6698575054967324 | validation: 0.4076002380007564]
	TIME [epoch: 8.46 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.596752844598855		[learning rate: 0.007307]
		[batch 20/20] avg loss: 0.6109812340442657		[learning rate: 0.0072981]
	Learning Rate: 0.00729811
	LOSS [training: 0.6038670393215603 | validation: 0.473069278798642]
	TIME [epoch: 8.44 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9153093014230945		[learning rate: 0.0072893]
		[batch 20/20] avg loss: 0.6496279024743818		[learning rate: 0.0072804]
	Learning Rate: 0.00728044
	LOSS [training: 0.7824686019487382 | validation: 0.4810739836470485]
	TIME [epoch: 8.43 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6544965086794547		[learning rate: 0.0072716]
		[batch 20/20] avg loss: 0.6572287204752194		[learning rate: 0.0072628]
	Learning Rate: 0.00726282
	LOSS [training: 0.6558626145773371 | validation: 0.6195226339445723]
	TIME [epoch: 8.45 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6731475101581803		[learning rate: 0.007254]
		[batch 20/20] avg loss: 0.6307344416119145		[learning rate: 0.0072452]
	Learning Rate: 0.00724524
	LOSS [training: 0.6519409758850474 | validation: 0.5974965005209469]
	TIME [epoch: 8.45 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7291517390868502		[learning rate: 0.0072365]
		[batch 20/20] avg loss: 0.7828923988192538		[learning rate: 0.0072277]
	Learning Rate: 0.0072277
	LOSS [training: 0.756022068953052 | validation: 0.8319424506710962]
	TIME [epoch: 8.42 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.820606485110946		[learning rate: 0.0072189]
		[batch 20/20] avg loss: 0.809647250652597		[learning rate: 0.0072102]
	Learning Rate: 0.0072102
	LOSS [training: 0.8151268678817715 | validation: 0.9819078586955925]
	TIME [epoch: 8.43 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8252097619230142		[learning rate: 0.0072015]
		[batch 20/20] avg loss: 0.7729064456077326		[learning rate: 0.0071927]
	Learning Rate: 0.00719275
	LOSS [training: 0.7990581037653735 | validation: 0.667580024232114]
	TIME [epoch: 8.45 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7148238880247149		[learning rate: 0.007184]
		[batch 20/20] avg loss: 0.7302954441272472		[learning rate: 0.0071753]
	Learning Rate: 0.00717533
	LOSS [training: 0.7225596660759812 | validation: 0.9695190426687779]
	TIME [epoch: 8.44 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6780949197839327		[learning rate: 0.0071666]
		[batch 20/20] avg loss: 0.7550135896044917		[learning rate: 0.007158]
	Learning Rate: 0.00715796
	LOSS [training: 0.7165542546942121 | validation: 0.4687957332151146]
	TIME [epoch: 8.44 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6980041058680767		[learning rate: 0.0071493]
		[batch 20/20] avg loss: 1.0677623113833419		[learning rate: 0.0071406]
	Learning Rate: 0.00714064
	LOSS [training: 0.882883208625709 | validation: 1.3599283316831356]
	TIME [epoch: 8.45 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.796621096120879		[learning rate: 0.007132]
		[batch 20/20] avg loss: 0.9249559571666163		[learning rate: 0.0071233]
	Learning Rate: 0.00712335
	LOSS [training: 0.8607885266437476 | validation: 1.1006589588112172]
	TIME [epoch: 8.43 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7979578472628222		[learning rate: 0.0071147]
		[batch 20/20] avg loss: 0.632833584164819		[learning rate: 0.0071061]
	Learning Rate: 0.0071061
	LOSS [training: 0.7153957157138204 | validation: 0.45472285193159634]
	TIME [epoch: 8.43 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8125962374658255		[learning rate: 0.0070975]
		[batch 20/20] avg loss: 0.6545710573132961		[learning rate: 0.0070889]
	Learning Rate: 0.0070889
	LOSS [training: 0.7335836473895609 | validation: 0.534804092429869]
	TIME [epoch: 8.45 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7520287915039932		[learning rate: 0.0070803]
		[batch 20/20] avg loss: 0.8148496880655494		[learning rate: 0.0070717]
	Learning Rate: 0.00707174
	LOSS [training: 0.7834392397847711 | validation: 0.5040683432259891]
	TIME [epoch: 8.45 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8337995992942024		[learning rate: 0.0070632]
		[batch 20/20] avg loss: 1.0817506089355167		[learning rate: 0.0070546]
	Learning Rate: 0.00705462
	LOSS [training: 0.9577751041148597 | validation: 1.1695364617286845]
	TIME [epoch: 8.43 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0839506879492673		[learning rate: 0.0070461]
		[batch 20/20] avg loss: 0.8952856423066183		[learning rate: 0.0070375]
	Learning Rate: 0.00703754
	LOSS [training: 0.9896181651279428 | validation: 0.6835206777865791]
	TIME [epoch: 8.43 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.02024200459408		[learning rate: 0.007029]
		[batch 20/20] avg loss: 0.9981064527316394		[learning rate: 0.0070205]
	Learning Rate: 0.00702051
	LOSS [training: 1.00917422866286 | validation: 0.9480882140320865]
	TIME [epoch: 8.43 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8347935594078375		[learning rate: 0.007012]
		[batch 20/20] avg loss: 1.0485094285280783		[learning rate: 0.0070035]
	Learning Rate: 0.00700351
	LOSS [training: 0.941651493967958 | validation: 1.5466768020577728]
	TIME [epoch: 8.45 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.954929408217746		[learning rate: 0.006995]
		[batch 20/20] avg loss: 0.8908731494836605		[learning rate: 0.0069866]
	Learning Rate: 0.00698656
	LOSS [training: 0.9229012788507032 | validation: 0.8732384874239136]
	TIME [epoch: 8.46 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.826525495348473		[learning rate: 0.0069781]
		[batch 20/20] avg loss: 0.7612042604842978		[learning rate: 0.0069696]
	Learning Rate: 0.00696964
	LOSS [training: 0.7938648779163854 | validation: 0.6979064059409689]
	TIME [epoch: 8.43 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7528722854155466		[learning rate: 0.0069612]
		[batch 20/20] avg loss: 0.6744262523195071		[learning rate: 0.0069528]
	Learning Rate: 0.00695277
	LOSS [training: 0.7136492688675269 | validation: 0.5745244436000772]
	TIME [epoch: 8.44 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8760187310500271		[learning rate: 0.0069443]
		[batch 20/20] avg loss: 0.8137065623329487		[learning rate: 0.0069359]
	Learning Rate: 0.00693594
	LOSS [training: 0.8448626466914879 | validation: 0.8039548502056071]
	TIME [epoch: 8.43 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9348681771926357		[learning rate: 0.0069275]
		[batch 20/20] avg loss: 0.7311492317402613		[learning rate: 0.0069191]
	Learning Rate: 0.00691915
	LOSS [training: 0.8330087044664485 | validation: 0.6891226627170647]
	TIME [epoch: 8.42 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7472969801044165		[learning rate: 0.0069108]
		[batch 20/20] avg loss: 0.7196525968012394		[learning rate: 0.0069024]
	Learning Rate: 0.0069024
	LOSS [training: 0.733474788452828 | validation: 0.8344585528654622]
	TIME [epoch: 8.43 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8364328999088221		[learning rate: 0.006894]
		[batch 20/20] avg loss: 0.7575235107917112		[learning rate: 0.0068857]
	Learning Rate: 0.00688569
	LOSS [training: 0.7969782053502665 | validation: 0.7743215866167517]
	TIME [epoch: 8.47 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9021358540878118		[learning rate: 0.0068773]
		[batch 20/20] avg loss: 0.837135896545448		[learning rate: 0.006869]
	Learning Rate: 0.00686902
	LOSS [training: 0.8696358753166299 | validation: 0.9577870290477002]
	TIME [epoch: 8.46 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7935943304099295		[learning rate: 0.0068607]
		[batch 20/20] avg loss: 0.8418468283359395		[learning rate: 0.0068524]
	Learning Rate: 0.00685239
	LOSS [training: 0.8177205793729346 | validation: 0.8098045329617564]
	TIME [epoch: 8.42 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1013952269744443		[learning rate: 0.0068441]
		[batch 20/20] avg loss: 0.9142704123651477		[learning rate: 0.0068358]
	Learning Rate: 0.0068358
	LOSS [training: 1.0078328196697959 | validation: 1.6554682757485293]
	TIME [epoch: 8.45 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7426424041900338		[learning rate: 0.0068275]
		[batch 20/20] avg loss: 0.9318401936563061		[learning rate: 0.0068193]
	Learning Rate: 0.00681925
	LOSS [training: 0.8372412989231698 | validation: 0.7312203760203515]
	TIME [epoch: 8.45 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7026984155579803		[learning rate: 0.006811]
		[batch 20/20] avg loss: 0.8994033685115399		[learning rate: 0.0068027]
	Learning Rate: 0.00680275
	LOSS [training: 0.8010508920347601 | validation: 0.8964150047460608]
	TIME [epoch: 8.42 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7106927938924794		[learning rate: 0.0067945]
		[batch 20/20] avg loss: 0.8133990269885123		[learning rate: 0.0067863]
	Learning Rate: 0.00678628
	LOSS [training: 0.7620459104404959 | validation: 0.494114716549653]
	TIME [epoch: 8.42 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7628913414218667		[learning rate: 0.0067781]
		[batch 20/20] avg loss: 0.655088593300797		[learning rate: 0.0067698]
	Learning Rate: 0.00676985
	LOSS [training: 0.7089899673613318 | validation: 0.44559329512400053]
	TIME [epoch: 8.45 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6642664369325795		[learning rate: 0.0067616]
		[batch 20/20] avg loss: 0.794552073192486		[learning rate: 0.0067535]
	Learning Rate: 0.00675346
	LOSS [training: 0.7294092550625326 | validation: 0.5183441483869982]
	TIME [epoch: 8.44 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7324573219173187		[learning rate: 0.0067453]
		[batch 20/20] avg loss: 0.638752027871524		[learning rate: 0.0067371]
	Learning Rate: 0.00673711
	LOSS [training: 0.6856046748944211 | validation: 0.8854362464332447]
	TIME [epoch: 8.43 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6675893987751194		[learning rate: 0.006729]
		[batch 20/20] avg loss: 0.909844827415009		[learning rate: 0.0067208]
	Learning Rate: 0.0067208
	LOSS [training: 0.7887171130950643 | validation: 0.6957376295057913]
	TIME [epoch: 8.44 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9324660952163466		[learning rate: 0.0067127]
		[batch 20/20] avg loss: 1.0088558582819716		[learning rate: 0.0067045]
	Learning Rate: 0.00670453
	LOSS [training: 0.9706609767491587 | validation: 1.1332833765462695]
	TIME [epoch: 8.46 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.131790322685665		[learning rate: 0.0066964]
		[batch 20/20] avg loss: 0.7308013390077738		[learning rate: 0.0066883]
	Learning Rate: 0.0066883
	LOSS [training: 0.9312958308467195 | validation: 0.7742196508330458]
	TIME [epoch: 8.46 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7412724960943293		[learning rate: 0.0066802]
		[batch 20/20] avg loss: 0.7600807012257086		[learning rate: 0.0066721]
	Learning Rate: 0.00667211
	LOSS [training: 0.750676598660019 | validation: 0.625695539633721]
	TIME [epoch: 8.43 sec]
EPOCH 268/2000:
	Training over batches...
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.05
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.025
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.0125
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.00625
