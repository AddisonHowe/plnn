Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r3', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2323492838

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.082521856572		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.815308429685205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.448915143128604 | validation: 8.15145010534073]
	TIME [epoch: 49.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.527450545799443		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.3127597337906804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.920105139795062 | validation: 6.527464767082547]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.799690359252547		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.7919772313466185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.7958337952995835 | validation: 6.47685767783305]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.625577344227139		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.708449182876528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.667013263551834 | validation: 6.313451528536602]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.555049948439441		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.703044261440806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.629047104940123 | validation: 6.274094643255015]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.4826871749701755		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.278890700242118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.880788937606146 | validation: 5.721693639388926]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.638198756898161		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.377031068163476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.507614912530817 | validation: 4.920069651238835]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.528963472350432		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.192382544956213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.360673008653324 | validation: 5.030443670478218]
	TIME [epoch: 8.86 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.189643357442362		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.05287349223062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.121258424836491 | validation: 5.096002546277445]
	TIME [epoch: 8.85 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.979058971683374		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.010795042442524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.99492700706295 | validation: 5.321526432503701]
	TIME [epoch: 8.85 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.262605272252625		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.012910334847971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.137757803550298 | validation: 4.687280853004811]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.734201960575445		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.43932186858915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.586761914582298 | validation: 4.12486602656776]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.413717720210218		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.9856304419797324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.199674081094974 | validation: 4.1793310539119055]
	TIME [epoch: 8.82 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7851041167270276		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.505779262708016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6454416897175212 | validation: 3.1019248929445724]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1459553233712527		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.160004616692794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.152979970032023 | validation: 3.0093343823055783]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.712841428040377		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6807586951624294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6968000616014036 | validation: 3.1058031761095473]
	TIME [epoch: 8.83 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6152707246205797		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.68528795052237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.650279337571475 | validation: 2.1078172974086975]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4962778759390423		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.452730491864331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.474504183901687 | validation: 2.1440415689497208]
	TIME [epoch: 8.81 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.240167286434033		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2775130292082126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2588401578211226 | validation: 1.9681417200503035]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.099717965792197		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4086903033527927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.254204134572494 | validation: 1.8470288788833948]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0452073725323494		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0933455926856683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.069276482609009 | validation: 1.974397447549398]
	TIME [epoch: 8.79 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2068453333417826		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.098396691330332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.152621012336057 | validation: 1.9890701910794002]
	TIME [epoch: 8.78 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9243068282770537		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1552788484885244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0397928383827884 | validation: 1.8567932584939402]
	TIME [epoch: 8.78 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9209621935353862		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8258964567622797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8734293251488332 | validation: 3.0379826318100784]
	TIME [epoch: 8.81 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7980694998170725		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.775751604780634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7869105522988533 | validation: 2.538582629725286]
	TIME [epoch: 8.79 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.724068450236707		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6644002285900694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6942343394133879 | validation: 1.30255401442664]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6821209863335014		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6044401420071555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6432805641703283 | validation: 1.3757848443427316]
	TIME [epoch: 8.81 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.716372176165129		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5788272955988516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6475997358819903 | validation: 1.1947019789525979]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4929022703286672		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4062129390470273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4495576046878473 | validation: 1.3136947295946095]
	TIME [epoch: 8.82 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5476312975768485		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3239826702893918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.43580698393312 | validation: 1.1760347028643254]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6888715721427325		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3535509776321464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5212112748874396 | validation: 1.0822499715122462]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4016896805071286		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6389246050920545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5203071427995916 | validation: 1.0625083660357753]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3236863369833898		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2681524034960492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2959193702397198 | validation: 1.228233873816042]
	TIME [epoch: 8.86 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3369016076767315		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3552523153785194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3460769615276251 | validation: 1.092664503661199]
	TIME [epoch: 8.85 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1295723013547543		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.191913898185424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1607430997700896 | validation: 1.1198817574938156]
	TIME [epoch: 8.84 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1615195516415104		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3141983813845555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.237858966513033 | validation: 1.218141981898909]
	TIME [epoch: 8.84 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2948582240719588		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2482023451771924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2715302846245755 | validation: 1.0026031461157248]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1011056101997727		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0798755932223896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0904906017110811 | validation: 1.825292366789124]
	TIME [epoch: 8.83 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2151525693158853		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1935866207240609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.204369595019973 | validation: 0.791702767634785]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.08504265565185		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.214328620455114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.149685638053482 | validation: 0.6744975306602534]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9068455240183807		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9046236833808899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9057346036996353 | validation: 1.2324747817251904]
	TIME [epoch: 8.79 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.130989311250986		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9739857744830696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0524875428670275 | validation: 1.4546346717261858]
	TIME [epoch: 8.83 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.095302929834643		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0266859717913563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0609944508129998 | validation: 1.2410256617277104]
	TIME [epoch: 8.81 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0866731641924425		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0191623247997235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0529177444960829 | validation: 0.753358238694382]
	TIME [epoch: 8.8 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9269156647379129		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1494817837429465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0381987242404298 | validation: 0.6224891465456953]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9476398677890835		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8583637964874313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9030018321382576 | validation: 0.6585737358338578]
	TIME [epoch: 8.79 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9126182913168247		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.974939214855939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9437787530863819 | validation: 1.052418185346813]
	TIME [epoch: 8.8 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9233934579501497		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8582333912154398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8908134245827949 | validation: 0.6565667174847061]
	TIME [epoch: 8.82 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7947256901584235		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.773962821130729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7843442556445761 | validation: 0.777747068758663]
	TIME [epoch: 8.8 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7713304284557196		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9474756325393866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8594030304975531 | validation: 1.0109147185142677]
	TIME [epoch: 8.79 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9048124323247286		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7807593868764476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8427859096005882 | validation: 0.7550883203662921]
	TIME [epoch: 8.78 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.75099389330163		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0693388027702717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9101663480359508 | validation: 1.9923975946706838]
	TIME [epoch: 8.8 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8758043813276476		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8485590220589933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8621817016933205 | validation: 0.7077020745392366]
	TIME [epoch: 8.8 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9411800869964579		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8396455984087702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8904128427026141 | validation: 0.5816549629639378]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.756684197669605		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6939477054769121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7253159515732587 | validation: 0.6713935303666914]
	TIME [epoch: 8.81 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7910726670860717		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.678357350589111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7347150088375913 | validation: 0.8751166534226944]
	TIME [epoch: 8.83 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8082490842137764		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6845621778002561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7464056310070162 | validation: 1.0353998898190797]
	TIME [epoch: 8.84 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9398482885646186		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7507068327969902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8452775606808043 | validation: 0.5285094878378102]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7252326222654909		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7555305989828356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7403816106241633 | validation: 0.503784990810807]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6819843218814923		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0762718352971965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8791280785893443 | validation: 1.0728036993604393]
	TIME [epoch: 8.81 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6686897463595032		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6598042587043621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6642470025319327 | validation: 0.5142931904341478]
	TIME [epoch: 8.83 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8236544880259584		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0746755950222169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9491650415240878 | validation: 0.6373124763955695]
	TIME [epoch: 8.82 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6163131668438012		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8678780903462089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7420956285950051 | validation: 0.5737426863008116]
	TIME [epoch: 8.79 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.64956722346459		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6976803174085779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.673623770436584 | validation: 0.5758007318250276]
	TIME [epoch: 8.8 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6273513978190229		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.569326486070774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5983389419448985 | validation: 0.45245509982944254]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6200399161959279		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6803266722245462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.650183294210237 | validation: 0.5600096691393822]
	TIME [epoch: 8.84 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6003169410817528		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5964711885119439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5983940647968484 | validation: 0.6557130701543088]
	TIME [epoch: 8.82 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7175010395461219		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8116132648412769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7645571521936995 | validation: 0.6365046481511673]
	TIME [epoch: 8.84 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6959442570544676		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6930641554359321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6945042062451998 | validation: 0.30479472447498784]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.642538839802712		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8479524436440048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7452456417233583 | validation: 0.6500611188902498]
	TIME [epoch: 8.83 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7935422567009964		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6999288490385794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7467355528697879 | validation: 0.6754313174966315]
	TIME [epoch: 8.83 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7112356125118182		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6983417759322486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7047886942220334 | validation: 0.7715130727180286]
	TIME [epoch: 8.8 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5635240603162674		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6616549215473343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6125894909318009 | validation: 0.5034222031534759]
	TIME [epoch: 8.8 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5873217428921431		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.648745134472317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6180334386822299 | validation: 0.410721334163456]
	TIME [epoch: 8.81 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5656211348214057		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6857833573134028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6257022460674043 | validation: 1.0474569260513205]
	TIME [epoch: 8.83 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6469160215638875		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.516114367379183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5815151944715352 | validation: 0.5524884240337282]
	TIME [epoch: 8.84 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.679584926227309		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6597948874503399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6696899068388245 | validation: 0.6689503959053065]
	TIME [epoch: 8.82 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6582630767067029		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5451665021846888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6017147894456958 | validation: 0.5437228915126094]
	TIME [epoch: 8.81 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5980140724736718		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4894670594123105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5437405659429911 | validation: 0.34659004944433147]
	TIME [epoch: 8.83 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6787664602590591		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5525580731144462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6156622666867525 | validation: 0.7768177121930054]
	TIME [epoch: 8.81 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7309312807052462		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6592290095565341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6950801451308901 | validation: 0.6091686478535092]
	TIME [epoch: 8.8 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.581037085721605		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6550024488877064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6180197673046556 | validation: 0.6004306212926023]
	TIME [epoch: 8.79 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6213090609834848		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5147080774080749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5680085691957798 | validation: 0.6045578373353933]
	TIME [epoch: 8.81 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6638758232879276		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6023398371130623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6331078302004951 | validation: 0.26076128455101333]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5406770413819098		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5397875481451906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5402322947635502 | validation: 0.5703155757196385]
	TIME [epoch: 8.84 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7890370761201491		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5805509903401169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6847940332301331 | validation: 0.3308979198017421]
	TIME [epoch: 8.79 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49425104737263015		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5642256746968437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5292383610347369 | validation: 0.6568092809786888]
	TIME [epoch: 8.81 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5372474433328426		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.44680405393142075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4920257486321316 | validation: 0.4008232112313767]
	TIME [epoch: 8.78 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5254003989592703		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5459997163763292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5357000576677996 | validation: 0.5552304855242223]
	TIME [epoch: 8.81 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5165591957016236		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5053134091043838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5109363024030038 | validation: 0.3239271723367118]
	TIME [epoch: 8.83 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4880496821287627		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5150654237958256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5015575529622943 | validation: 0.5112246799874129]
	TIME [epoch: 8.82 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5158998215440053		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5552211409989347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5355604812714698 | validation: 0.25325225672622803]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5827658844465196		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8587524947080822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7207591895773009 | validation: 0.5759110316378007]
	TIME [epoch: 8.81 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42656358907263503		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5172551609388344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4719093750057347 | validation: 0.6762348096392836]
	TIME [epoch: 8.85 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47718435577457907		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5711194844381773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5241519201063782 | validation: 0.49024776679077936]
	TIME [epoch: 8.83 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5791903505658949		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6860432240321263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6326167872990104 | validation: 0.2550649061463066]
	TIME [epoch: 8.82 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3820088928793098		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.606892886266933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49445088957312144 | validation: 0.7953085638352269]
	TIME [epoch: 8.82 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5431356496387997		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48432475640672684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5137302030227634 | validation: 0.20698548417165186]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5504634032114163		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45975385631099996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5051086297612082 | validation: 0.7441981201946357]
	TIME [epoch: 8.85 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5244850144688754		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.598906530449382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5616957724591287 | validation: 0.4780614282605413]
	TIME [epoch: 8.82 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4547512509199219		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4789704856491501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.466860868284536 | validation: 0.6301855539529273]
	TIME [epoch: 8.83 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4481968905871364		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5821485886807257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.515172739633931 | validation: 0.7124106843249995]
	TIME [epoch: 8.83 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6146766609691137		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5912450709167361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6029608659429251 | validation: 0.5760076198887516]
	TIME [epoch: 8.83 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4392116174315124		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5546817724142719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4969466949228922 | validation: 0.3924696287693757]
	TIME [epoch: 8.83 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44876642935330946		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6178975575887985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.533331993471054 | validation: 0.2886736583754165]
	TIME [epoch: 8.81 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4826017646215293		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8560837703398516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6693427674806904 | validation: 0.33925950803866634]
	TIME [epoch: 8.8 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5462095302380795		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8824075160632481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7143085231506636 | validation: 0.4829263900256384]
	TIME [epoch: 8.81 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.517286456057563		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7933433132284065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6553148846429848 | validation: 0.3625647530674896]
	TIME [epoch: 8.84 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6132114618667563		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48684444554656015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5500279537066584 | validation: 0.8908148837698366]
	TIME [epoch: 8.84 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4641966313772542		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7235321653262518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.593864398351753 | validation: 0.6371062978102444]
	TIME [epoch: 8.83 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47151677257274577		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46244664042943384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46698170650108983 | validation: 0.4014985704996983]
	TIME [epoch: 8.85 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7398497668957712		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49916107272582255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.619505419810797 | validation: 0.3556900680399777]
	TIME [epoch: 8.85 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5266855754516018		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5412913153811592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5339884454163807 | validation: 0.5668940383753975]
	TIME [epoch: 8.87 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6611103425633805		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.630467761946613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6457890522549967 | validation: 0.9864130717241756]
	TIME [epoch: 8.84 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6399788425917505		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6435655577137094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6417722001527298 | validation: 0.5525278411043404]
	TIME [epoch: 8.83 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6289684390043475		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6053760836633892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6171722613338683 | validation: 0.8245360404803554]
	TIME [epoch: 8.84 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5106161692987465		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.520571619593389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5155938944460677 | validation: 0.5096023291298847]
	TIME [epoch: 8.83 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5444068891826158		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5563476322502675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5503772607164417 | validation: 0.3245250878170157]
	TIME [epoch: 8.86 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46300885978165274		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8445924900144085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6538006748980307 | validation: 0.8573657993048645]
	TIME [epoch: 9.23 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5403090790739699		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7753738063665733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6578414427202715 | validation: 0.6429844004234065]
	TIME [epoch: 8.82 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8026919723079615		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6042996099741378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7034957911410497 | validation: 0.5126168146329131]
	TIME [epoch: 8.82 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.513446936898874		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5438702186919623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5286585777954181 | validation: 0.6067150656953604]
	TIME [epoch: 8.82 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5525486983782958		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.714071829516701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6333102639474985 | validation: 0.8036597650482142]
	TIME [epoch: 8.85 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7084264804173748		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.128284868344879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.918355674381127 | validation: 0.784775916766892]
	TIME [epoch: 8.81 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6527982822975908		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5582469246039666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6055226034507787 | validation: 0.5608156335015153]
	TIME [epoch: 8.83 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5727617758165645		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49670634056049534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5347340581885299 | validation: 0.4268469416355366]
	TIME [epoch: 9.02 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45598193883445515		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5675264608181981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5117541998263266 | validation: 0.5294409574035382]
	TIME [epoch: 8.84 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5476960624221203		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4523029673735316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4999995148978261 | validation: 0.39122432841169136]
	TIME [epoch: 8.83 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6107468212676811		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5171645711090893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5639556961883851 | validation: 0.38339262657980255]
	TIME [epoch: 8.83 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7614231367699356		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8552280913846747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8083256140773051 | validation: 0.43802907459619644]
	TIME [epoch: 8.82 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5684581780275326		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4113602459633333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.489909211995433 | validation: 0.4096861288635321]
	TIME [epoch: 8.82 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6145017718411148		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5576463083077052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.58607404007441 | validation: 0.43384172684184097]
	TIME [epoch: 8.84 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.648439864385649		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4522335555332392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5503367099594442 | validation: 0.5213822182485114]
	TIME [epoch: 8.83 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5691610572648227		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7735392893622917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6713501733135572 | validation: 0.6394255271023388]
	TIME [epoch: 8.82 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4554244770205713		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4553712772705608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4553978771455662 | validation: 0.38554334996561446]
	TIME [epoch: 8.83 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4401725837729823		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5049109555097686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47254176964137545 | validation: 0.4258230292696681]
	TIME [epoch: 8.82 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5130027080533344		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6566958231772928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5848492656153136 | validation: 0.37759432511510377]
	TIME [epoch: 8.88 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6072859651279143		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43445765682544907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5208718109766817 | validation: 0.44928148862216005]
	TIME [epoch: 8.85 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4094241541400082		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0053622725154034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.707393213327706 | validation: 0.30308040346802695]
	TIME [epoch: 9.23 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49142663349061644		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48102449676689235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4862255651287544 | validation: 0.4306691161116176]
	TIME [epoch: 8.79 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4167284883187438		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.42327773390586626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42000311111230515 | validation: 0.7807639314189737]
	TIME [epoch: 8.82 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6515580284857985		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5173637608642415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5844608946750199 | validation: 0.27334886042789536]
	TIME [epoch: 8.84 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44403158407360055		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6453801376340349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5447058608538177 | validation: 0.3911138461930356]
	TIME [epoch: 8.82 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46140548790448516		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5138423065489854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48762389722673516 | validation: 0.64158196913854]
	TIME [epoch: 8.82 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48427940553622184		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7170278887539866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6006536471451043 | validation: 0.7180035275240122]
	TIME [epoch: 8.81 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6150210617843335		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5779170745396466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.59646906816199 | validation: 0.3239018858677668]
	TIME [epoch: 8.82 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4826438595986251		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5951567134677049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5389002865331649 | validation: 0.4885128119287012]
	TIME [epoch: 8.8 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5843250973800558		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4308737225431429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5075994099615994 | validation: 0.5484416792603528]
	TIME [epoch: 8.81 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5054966515441472		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.605446467882147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.555471559713147 | validation: 0.2613340872987924]
	TIME [epoch: 8.83 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4445975313192392		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.739858557176291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5922280442477652 | validation: 0.47423602648250607]
	TIME [epoch: 8.83 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5124737879368534		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7685081788736398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6404909834052466 | validation: 0.6164265731166524]
	TIME [epoch: 8.85 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8823308441569058		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6325758132730942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.757453328715 | validation: 0.4152449018843428]
	TIME [epoch: 8.84 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6053459003308843		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5495678868572087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5774568935940466 | validation: 0.8286021300669638]
	TIME [epoch: 8.82 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5157957722997881		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49303430924013397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.504415040769961 | validation: 0.9467383711592549]
	TIME [epoch: 8.83 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5748191241599457		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4858434321621211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5303312781610334 | validation: 0.2448244298629129]
	TIME [epoch: 8.83 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6821160128849949		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49069382839102077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5864049206380079 | validation: 0.3225149854048587]
	TIME [epoch: 8.84 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4991178331930886		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4784559026361291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4887868679146089 | validation: 0.4101239119756499]
	TIME [epoch: 8.82 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47644554654646243		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40596189328284915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44120371991465585 | validation: 0.4114725017697817]
	TIME [epoch: 8.81 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47256736054659243		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5270734334610099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4998203970038011 | validation: 0.34742579528117695]
	TIME [epoch: 8.83 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.504925889117086		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5918333691408242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5483796291289551 | validation: 0.223631087598213]
	TIME [epoch: 8.85 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6200164495595577		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.487746029492618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5538812395260879 | validation: 0.6091834489005123]
	TIME [epoch: 8.86 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4412096734047635		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.823457190062171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6323334317334671 | validation: 2.9024673055900174]
	TIME [epoch: 8.82 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.987324007058137		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.44346337806963143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7153936925638843 | validation: 0.4188135913706273]
	TIME [epoch: 8.8 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5001137925269811		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0692653129065597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7846895527167703 | validation: 0.3892941306096128]
	TIME [epoch: 8.81 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34992869609078575		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6215673093253218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4857480027080537 | validation: 0.27253756997775425]
	TIME [epoch: 8.82 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.435781637847665		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4550798923145042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44543076508108453 | validation: 0.446054665583969]
	TIME [epoch: 8.79 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5743180500489948		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5029096147814212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5386138324152079 | validation: 0.3793878626521491]
	TIME [epoch: 8.8 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4764631719193139		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4902823803483523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48337277613383306 | validation: 0.4857406539455189]
	TIME [epoch: 8.81 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5397036446653483		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.55306544888008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5463845467727142 | validation: 0.4078454646880658]
	TIME [epoch: 8.8 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43098820794803566		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5792956894846851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5051419487163604 | validation: 0.4242350723891713]
	TIME [epoch: 8.82 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5828513916173411		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5219773314596892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5524143615385151 | validation: 0.27206935029590446]
	TIME [epoch: 8.82 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44969309775241484		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6089603274839577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5293267126181862 | validation: 0.47373967934994154]
	TIME [epoch: 8.79 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6885366645864213		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5296254324166076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6090810485015145 | validation: 0.7814889019028677]
	TIME [epoch: 8.8 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5029420009088648		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3696466312836198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43629431609624225 | validation: 0.20582886602950914]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5117315968009549		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43612401578657234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4739278062937637 | validation: 0.5781916183984763]
	TIME [epoch: 8.89 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.531078737906237		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5663014942671272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5486901160866821 | validation: 0.7541816192400885]
	TIME [epoch: 8.85 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5792699890954709		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2339787811162561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9066243851058637 | validation: 0.47321009962325067]
	TIME [epoch: 8.86 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4432146159357348		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4861188763801058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4646667461579203 | validation: 0.27498366980126077]
	TIME [epoch: 8.87 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4295300378675907		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6078487787929726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5186894083302817 | validation: 0.39035141222550773]
	TIME [epoch: 8.89 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4417388977094009		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5410421097308468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49139050372012394 | validation: 0.33257606286697183]
	TIME [epoch: 8.87 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3859025684701943		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5521557061416515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4690291373059229 | validation: 0.26075460677792356]
	TIME [epoch: 8.86 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6065417011478823		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5767458554590649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5916437783034736 | validation: 0.4811684143500389]
	TIME [epoch: 8.91 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6441493184971289		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.41298676156621994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5285680400316746 | validation: 0.5395164543464716]
	TIME [epoch: 8.9 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4225168787277561		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6863068784951566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5544118786114565 | validation: 0.5552532248632275]
	TIME [epoch: 8.9 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6596246804341599		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5515363536980624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.605580517066111 | validation: 0.4238439890885559]
	TIME [epoch: 8.84 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6250221147488698		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7423552385509011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6836886766498855 | validation: 0.7791259986090795]
	TIME [epoch: 8.87 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5820818768389279		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6020338913282781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.592057884083603 | validation: 0.3988288701715541]
	TIME [epoch: 8.86 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5477526580372857		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5409345202878619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5443435891625737 | validation: 0.8133447246543906]
	TIME [epoch: 8.85 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5678231265226014		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4970333555198721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5324282410212368 | validation: 0.37009265620458476]
	TIME [epoch: 8.86 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4729335511319177		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4797398936740155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47633672240296654 | validation: 0.2667888298601302]
	TIME [epoch: 8.88 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5360466720952941		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.469361351121226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.50270401160826 | validation: 0.4481005233631331]
	TIME [epoch: 8.87 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7785044589232022		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.495895135989476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6371997974563391 | validation: 0.5172414711369658]
	TIME [epoch: 8.85 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5481391515596499		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4737673502169244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.510953250888287 | validation: 0.19879476781871258]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5218671044670826		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4987874579700642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5103272812185734 | validation: 0.2790222518777168]
	TIME [epoch: 8.82 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.543559391295082		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45294946629422866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49825442879465537 | validation: 0.4911262576649583]
	TIME [epoch: 8.82 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48019267867875814		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5003946325335681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49029365560616317 | validation: 0.5910768775703821]
	TIME [epoch: 8.84 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5470863339299458		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.555958034764511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5515221843472283 | validation: 0.32112299393428884]
	TIME [epoch: 8.83 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.55379983459275		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4479202329473706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5008600337700603 | validation: 0.5602631527397964]
	TIME [epoch: 8.86 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.421763103014697		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5038597027955529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46281140290512496 | validation: 0.417339940836869]
	TIME [epoch: 8.83 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5241367143503305		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40768517249820996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46591094342427014 | validation: 0.2437504382214352]
	TIME [epoch: 8.82 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44941667099110705		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4116725843818932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4305446276865002 | validation: 0.321984001999629]
	TIME [epoch: 8.82 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4299677946808633		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.528906770631766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4794372826563147 | validation: 0.7837293562447251]
	TIME [epoch: 8.83 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5143726943464202		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45250700597185595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4834398501591382 | validation: 0.9479217670915154]
	TIME [epoch: 8.84 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4519568152452992		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48740650333792457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.469681659291612 | validation: 0.3809683982741978]
	TIME [epoch: 8.84 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3968052179517333		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.42373605320559526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4102706355786642 | validation: 0.3211192569709372]
	TIME [epoch: 8.83 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4759124935146632		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43217946220950115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45404597786208206 | validation: 0.6953932673675929]
	TIME [epoch: 8.82 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4182518244623096		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4506677748551501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43445979965872983 | validation: 1.4675145571781945]
	TIME [epoch: 8.8 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5481557165968577		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6194412423751005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5837984794859791 | validation: 0.44784961748286317]
	TIME [epoch: 8.84 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4824789570896756		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4787276711648545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48060331412726487 | validation: 0.4149951382422058]
	TIME [epoch: 8.82 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4220760006669271		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4476480810292771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.434862040848102 | validation: 0.2908988695025901]
	TIME [epoch: 8.83 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4823246602652137		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36845887966413976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42539176996467687 | validation: 0.4090927393258405]
	TIME [epoch: 8.81 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4331828254812856		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6892601781844323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.561221501832859 | validation: 0.5239259686482415]
	TIME [epoch: 8.84 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5286862326504435		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5412265036227213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5349563681365825 | validation: 0.46427752067428196]
	TIME [epoch: 8.83 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47814740404251854		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5121085665604623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49512798530149044 | validation: 0.6899979101887206]
	TIME [epoch: 8.82 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47221765067269983		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4454733798786714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4588455152756857 | validation: 0.38698477187525027]
	TIME [epoch: 8.82 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5687993859208034		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.44393144550116226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5063654157109829 | validation: 0.23747901266473712]
	TIME [epoch: 8.83 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4135828182815037		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5400588012509531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4768208097662284 | validation: 0.23977199590463216]
	TIME [epoch: 8.89 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40331490458806474		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5649631134839291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4841390090359968 | validation: 0.3781869195298819]
	TIME [epoch: 8.88 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49258944408921534		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4089805326484212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45078498836881814 | validation: 0.7803129757976489]
	TIME [epoch: 8.82 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5196035561939254		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.37847255355224946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4490380548730875 | validation: 0.22670562284204535]
	TIME [epoch: 8.81 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40984320634052607		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.35922153741997487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38453237188025047 | validation: 0.2698078229114944]
	TIME [epoch: 8.83 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5450268436314669		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7230533375147099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6340400905730884 | validation: 0.3253794749247598]
	TIME [epoch: 8.84 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44487321443269334		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4165131856112703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43069320002198186 | validation: 0.41060176847115315]
	TIME [epoch: 8.84 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6344011406320111		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5698548228841359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6021279817580736 | validation: 0.22063341082948423]
	TIME [epoch: 8.82 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49870849526234656		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4113912378528609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4550498665576037 | validation: 0.43785936476707255]
	TIME [epoch: 8.82 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4629120610114696		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4926604721701409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47778626659080536 | validation: 0.2921292068925102]
	TIME [epoch: 8.83 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5506930977010087		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5210098298110621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5358514637560353 | validation: 0.33236971356712497]
	TIME [epoch: 8.84 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.431297839591228		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4202325451713884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4257651923813082 | validation: 0.2989539847136351]
	TIME [epoch: 8.82 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42313222420603447		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5005968207136703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46186452245985227 | validation: 0.1836748739995629]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r3_20240216_185744/states/model_tr_study1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6702797824359074		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.518036906155664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5941583442957857 | validation: 0.626937333732976]
	TIME [epoch: 8.82 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6218920706977797		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43145631796095724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5266741943293685 | validation: 0.28416596499963864]
	TIME [epoch: 8.85 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.544894253534819		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4857257675010024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5153100105179107 | validation: 0.4669471513117718]
	TIME [epoch: 8.83 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5471005143398984		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5619095602977653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5545050373188318 | validation: 0.18463425423011845]
	TIME [epoch: 8.83 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4302389211693051		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.41981771358949604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4250283173794006 | validation: 0.22039693159077867]
	TIME [epoch: 8.82 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3614096717502638		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5374816391461368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4494456554482002 | validation: 0.19815182818892393]
	TIME [epoch: 8.82 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5484845670718939		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.506654217503579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5275693922877365 | validation: 0.4947825270932837]
	TIME [epoch: 8.85 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4600945116022418		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38897860981747173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42453656070985685 | validation: 0.2354148250273921]
	TIME [epoch: 8.84 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48787880908683806		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46222394373583137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47505137641133466 | validation: 1.2928042280684857]
	TIME [epoch: 8.83 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5167187513329878		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4137027760398217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4652107636864048 | validation: 0.186035085624775]
	TIME [epoch: 8.83 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8345463742504113		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5929668305771438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7137566024137776 | validation: 0.38889552944550176]
	TIME [epoch: 8.82 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.364345344769117		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4847647441732027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4245550444711599 | validation: 0.27014882208559055]
	TIME [epoch: 8.84 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4290475709529519		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3956481973356357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4123478841442938 | validation: 0.3903359068592455]
	TIME [epoch: 8.83 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7822611357474827		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4742015636659872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6282313497067349 | validation: 0.351755614245713]
	TIME [epoch: 8.84 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4521256350582856		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4876492450315545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46988744004492 | validation: 0.34225265047035613]
	TIME [epoch: 8.83 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4509864879972271		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3323448677848302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3916656778910287 | validation: 0.4002619928489364]
	TIME [epoch: 8.85 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48965951230488186		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.44777278897999623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4687161506424391 | validation: 0.4085019342669972]
	TIME [epoch: 8.83 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6163717118899038		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6440849592656301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6302283355777669 | validation: 0.2609790616621634]
	TIME [epoch: 8.83 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43434363238404955		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5764325872098527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5053881097969513 | validation: 0.34357685635045176]
	TIME [epoch: 9.92 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4524565373177928		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43750077039689933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44497865385734603 | validation: 0.3025948408994955]
	TIME [epoch: 8.83 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.655295980332135		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.772464187686666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7138800840094005 | validation: 0.5959254751047295]
	TIME [epoch: 8.86 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5179394596088297		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7230135580197399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6204765088142847 | validation: 0.4262874900548989]
	TIME [epoch: 8.85 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5529521208027465		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5942485601222909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5736003404625186 | validation: 0.4164997521144629]
	TIME [epoch: 8.83 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4793589950677126		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48101521660345475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48018710583558366 | validation: 0.3897502454109686]
	TIME [epoch: 8.83 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47822870471438267		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5150684021659203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49664855344015163 | validation: 0.531210800229035]
	TIME [epoch: 8.83 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.724004333086335		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4163593797750062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5701818564306707 | validation: 0.42168680495671546]
	TIME [epoch: 8.87 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34264430933933054		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6292433579893062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4859438336643184 | validation: 0.3120830228721557]
	TIME [epoch: 8.85 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48236768673256		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4686895999781141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4755286433553372 | validation: 0.31792268718438443]
	TIME [epoch: 8.84 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4846991289147337		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4821334852741853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4834163070944594 | validation: 0.4253604033157842]
	TIME [epoch: 8.86 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45833876824494774		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.39849692628275996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42841784726385396 | validation: 0.3315959765665899]
	TIME [epoch: 8.85 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5432235967993495		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40489086541712044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47405723110823494 | validation: 0.2743271752405576]
	TIME [epoch: 8.87 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4414501258366136		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4657378056748528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4535939657557333 | validation: 0.7645484854196964]
	TIME [epoch: 8.85 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4725370586730756		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48570260528528486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4791198319791802 | validation: 0.5554493207932931]
	TIME [epoch: 8.86 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6304231532212239		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.41083343985957776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5206282965404008 | validation: 0.7768114395981048]
	TIME [epoch: 8.85 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5449670921448986		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5638217203396338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5543944062422662 | validation: 0.56530886716519]
	TIME [epoch: 8.87 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4114882332706741		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5194475245744985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46546787892258623 | validation: 0.2994684020586459]
	TIME [epoch: 8.85 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43198145438258956		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4137152382375243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.422848346310057 | validation: 0.5535894565916766]
	TIME [epoch: 8.85 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48699790657123054		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7687347044687434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6278663055199869 | validation: 0.43338088130067753]
	TIME [epoch: 8.85 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5159997378154615		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5525173969344351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5342585673749483 | validation: 0.6418383029605172]
	TIME [epoch: 8.84 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7047852878209087		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8359828560565437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.770384071938726 | validation: 0.4877306860964583]
	TIME [epoch: 8.86 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5549993230983785		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4493278122521658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5021635676752723 | validation: 0.4759831004061951]
	TIME [epoch: 8.84 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5903322909078919		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5258406946717391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5580864927898155 | validation: 0.7568147141618122]
	TIME [epoch: 8.82 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5419115420096693		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5399496205925294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5409305813010993 | validation: 0.26656949693875687]
	TIME [epoch: 8.85 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4648186447583826		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5296860889333058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4972523668458441 | validation: 0.3989941399651785]
	TIME [epoch: 8.84 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4497476828087361		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.525728361397302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48773802210301903 | validation: 0.7132356452310366]
	TIME [epoch: 8.87 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49477582545301024		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.64664167066253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5707087480577702 | validation: 0.4234100008500439]
	TIME [epoch: 8.83 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4723085800793876		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6934566729728016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5828826265260946 | validation: 1.0826344001204071]
	TIME [epoch: 8.82 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5689399447359078		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5392823868509922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.55411116579345 | validation: 0.49764480058640503]
	TIME [epoch: 8.84 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40084827829629155		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5929400655760768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49689417193618424 | validation: 0.7362456124832706]
	TIME [epoch: 8.87 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45505210771081306		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4633782102831717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4592151589969923 | validation: 0.5274331916198823]
	TIME [epoch: 8.85 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48419785578111385		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5046555272635804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4944266915223472 | validation: 0.49900195060946884]
	TIME [epoch: 8.83 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44454268709884526		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5603791233914832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5024609052451643 | validation: 0.35314790213624325]
	TIME [epoch: 8.84 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.415072326724572		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5976478528799343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5063600898022532 | validation: 0.3364631582541847]
	TIME [epoch: 8.84 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4295046001099713		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5918196169307012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5106621085203363 | validation: 0.4512901404513952]
	TIME [epoch: 8.85 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5409578906536153		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5267895002936503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5338736954736328 | validation: 0.6101236078760143]
	TIME [epoch: 8.84 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5873035577537606		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5454737295726713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.566388643663216 | validation: 0.7599779232376822]
	TIME [epoch: 8.85 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4995913297698191		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5027218047473686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5011565672585937 | validation: 0.33154961177136566]
	TIME [epoch: 8.83 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4920423196275817		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47330319013825306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4826727548829176 | validation: 0.37784774658402276]
	TIME [epoch: 8.85 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46689188428671535		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5613452776892454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5141185809879802 | validation: 0.3210415822523877]
	TIME [epoch: 8.87 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7167670638397806		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5161927981595986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6164799309996896 | validation: 0.3360335054045786]
	TIME [epoch: 8.84 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4361098598933477		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48822098124515245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4621654205692501 | validation: 0.38367295236462995]
	TIME [epoch: 8.84 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5492655400159638		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8944478956044438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7218567178102038 | validation: 0.5210739646907154]
	TIME [epoch: 8.84 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5880966826540773		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5654185398917941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5767576112729357 | validation: 0.40899751520343053]
	TIME [epoch: 8.84 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.840736628398217		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7828549618982137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3117957951482153 | validation: 0.4606661153209966]
	TIME [epoch: 8.88 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6540459893615228		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9841991947027171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8191225920321198 | validation: 0.6639642366171397]
	TIME [epoch: 8.84 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9760535248904209		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.24332451347008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1096890191802504 | validation: 1.0457922741851333]
	TIME [epoch: 8.84 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.98186896206851		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.447924227724196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2148965948963528 | validation: 0.7500521640999473]
	TIME [epoch: 8.84 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0656099177126932		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0044505770832943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.535030247397994 | validation: 1.5469717659781166]
	TIME [epoch: 8.86 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0499201763248154		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0674390632898718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5586796198073436 | validation: 0.9178262868269466]
	TIME [epoch: 8.86 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9505348065984898		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0271547086866657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9888447576425776 | validation: 0.9219891932156654]
	TIME [epoch: 8.84 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9517393008392302		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7406084009199785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8461738508796042 | validation: 0.7563988642816757]
	TIME [epoch: 8.86 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7395033020542684		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6802450325790967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7098741673166824 | validation: 0.922962139201854]
	TIME [epoch: 8.84 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0972721233590146		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8946091024718825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9959406129154488 | validation: 0.6373121883422006]
	TIME [epoch: 8.88 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6701646321851037		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5664984720826731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6183315521338882 | validation: 0.374265110441485]
	TIME [epoch: 8.85 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6395234820572917		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.632517813929311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6360206479933013 | validation: 0.6373261689706055]
	TIME [epoch: 8.84 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6915931294200173		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7122009164461731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7018970229330953 | validation: 1.3642920712411934]
	TIME [epoch: 8.85 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8286519224074874		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8903192700988889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8594855962531881 | validation: 0.5056159874388161]
	TIME [epoch: 8.86 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5775443416799966		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7464113193615427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6619778305207696 | validation: 0.49106882080811465]
	TIME [epoch: 8.87 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7292080048829482		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5980129400500652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6636104724665067 | validation: 0.36339977601216716]
	TIME [epoch: 8.84 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5185737547123287		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0512507089046268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7849122318084778 | validation: 0.5319630956107061]
	TIME [epoch: 8.83 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6656154306181699		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7333882531801624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6995018418991661 | validation: 0.3674236249982601]
	TIME [epoch: 8.85 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.545464993779184		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7484172969804133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6469411453797986 | validation: 0.48949866193467884]
	TIME [epoch: 8.87 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.651343055858095		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5437694615669981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5975562587125467 | validation: 1.071707077167643]
	TIME [epoch: 8.85 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6195561370512314		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5254773759231031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5725167564871673 | validation: 0.500031244338482]
	TIME [epoch: 8.84 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8179554678036377		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47041309069995163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6441842792517947 | validation: 0.5384884045884981]
	TIME [epoch: 8.84 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.541366221015485		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5322602497008802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5368132353581826 | validation: 0.5386920214439017]
	TIME [epoch: 8.83 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4733420659642662		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6520124149605315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5626772404623989 | validation: 0.4263936984836335]
	TIME [epoch: 8.87 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42584738085396223		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6293852853492938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5276163331016279 | validation: 0.42808419557138866]
	TIME [epoch: 8.85 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46677881094395407		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8447143974364412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6557466041901977 | validation: 0.5947848307410428]
	TIME [epoch: 8.83 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7574065320325326		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5778567792672644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6676316556498985 | validation: 0.28793931986660604]
	TIME [epoch: 8.85 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5377769212475496		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.628762055292791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5832694882701702 | validation: 0.6505653923232839]
	TIME [epoch: 8.85 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5342841343798795		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5011805151425184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5177323247611991 | validation: 0.2542033804508914]
	TIME [epoch: 8.88 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6174731178758166		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.600904683286473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6091889005811448 | validation: 0.7471049174408881]
	TIME [epoch: 8.86 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6096893359289572		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5262248837871008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5679571098580289 | validation: 0.6956637550727531]
	TIME [epoch: 8.85 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5105556538273736		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5741265317599182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.542341092793646 | validation: 0.8862697227614038]
	TIME [epoch: 8.85 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6147377047358308		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6142238596627736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6144807821993021 | validation: 0.7491729888459617]
	TIME [epoch: 8.84 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.569110177238513		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5945192368923023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5818147070654075 | validation: 0.42664775913264114]
	TIME [epoch: 8.87 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5382668463541924		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5237167194572068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5309917829056996 | validation: 0.9804904993586485]
	TIME [epoch: 8.85 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.743066818256447		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.818664815100051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7808658166782492 | validation: 0.7240895949404574]
	TIME [epoch: 8.84 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.558913290559536		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6456124243294635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6022628574444997 | validation: 0.5481844849324022]
	TIME [epoch: 8.84 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6746477394777266		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6887132710971924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6816805052874596 | validation: 0.4233927432562669]
	TIME [epoch: 8.86 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5247177807628505		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6352262592592721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5799720200110612 | validation: 0.42008855771780035]
	TIME [epoch: 8.85 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7551240075862307		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7470575579164593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.751090782751345 | validation: 0.6374837085129349]
	TIME [epoch: 8.85 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5029748177178294		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5183226051999906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.51064871145891 | validation: 1.0425395186105129]
	TIME [epoch: 8.84 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6874990608685538		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6252890534276376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6563940571480958 | validation: 0.29461069616771907]
	TIME [epoch: 8.84 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5328013117827327		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0597944292550436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962978705188882 | validation: 0.5895121788558096]
	TIME [epoch: 8.87 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6241805703245293		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5553095100612653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5897450401928973 | validation: 0.8555419240228924]
	TIME [epoch: 8.86 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6892839143582317		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5812567526684497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6352703335133405 | validation: 1.0949948013676092]
	TIME [epoch: 8.83 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5927036304547844		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5494777415954392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5710906860251119 | validation: 0.6084817457560359]
	TIME [epoch: 8.87 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6267652264952488		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6889016775396309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6578334520174399 | validation: 0.56711410325273]
	TIME [epoch: 8.87 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7288453925340689		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5467730867617188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6378092396478938 | validation: 0.3604227627974704]
	TIME [epoch: 8.87 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5273834638481676		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5286812103749506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5280323371115592 | validation: 0.5371920984589836]
	TIME [epoch: 8.85 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5274268897702242		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5081759675125914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5178014286414079 | validation: 0.6231872854474817]
	TIME [epoch: 8.85 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46568996323844764		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5002847787339288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4829873709861884 | validation: 0.7879470274989492]
	TIME [epoch: 8.84 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4706963985411446		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5986751409928568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5346857697670008 | validation: 0.7579230418596437]
	TIME [epoch: 8.87 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5810738127492182		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6117418386786573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5964078257139379 | validation: 0.7088632060767726]
	TIME [epoch: 8.87 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7479278945070404		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5588361037612379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6533819991341391 | validation: 0.48654207691357126]
	TIME [epoch: 8.85 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6244162202693581		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5787896434597587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6016029318645586 | validation: 0.33136503365291115]
	TIME [epoch: 8.86 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48860028927671656		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5928736804032957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5407369848400062 | validation: 0.3648131516390255]
	TIME [epoch: 8.85 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5930982036530762		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5908825796506696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.591990391651873 | validation: 0.6885496230132955]
	TIME [epoch: 8.87 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6321497581487799		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7377872475042164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6849685028264981 | validation: 0.5702965729072206]
	TIME [epoch: 8.85 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5719239098431621		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5306161034444707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5512700066438163 | validation: 0.7824926730574491]
	TIME [epoch: 8.85 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5627089585295346		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4891239466831399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5259164526063372 | validation: 0.30623590869705136]
	TIME [epoch: 8.85 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6369825725308795		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.731293387459828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6841379799953538 | validation: 0.4160160875304476]
	TIME [epoch: 8.85 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.610154021376464		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.696473908091608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6533139647340362 | validation: 0.5602785231836612]
	TIME [epoch: 8.88 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5448574239695313		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5237776200663881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5343175220179598 | validation: 0.6089610625462326]
	TIME [epoch: 8.85 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9273154630788827		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6826223123059694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8049688876924261 | validation: 0.4233422582496654]
	TIME [epoch: 8.85 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7510443261831424		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5079628137537239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.629503569968433 | validation: 0.3747146272961481]
	TIME [epoch: 8.86 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6373242595131051		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5784862784085848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6079052689608451 | validation: 0.3863081420544979]
	TIME [epoch: 8.86 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44862186754578576		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7193610237673849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0839914456565851 | validation: 2.435970844122619]
	TIME [epoch: 8.89 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0537799841817233		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.647168589797754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8504742869897393 | validation: 3.6576652362906237]
	TIME [epoch: 8.86 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5257845758101554		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0759454015794505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3008649886948027 | validation: 0.43849827313330153]
	TIME [epoch: 8.85 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6481715938189562		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5276177025629833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5878946481909697 | validation: 0.537632306901263]
	TIME [epoch: 8.85 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5466492919385852		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6057172213475629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.576183256643074 | validation: 0.6205276055238684]
	TIME [epoch: 8.87 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6194949686576398		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48244682918271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5509708989201748 | validation: 0.9358556184326895]
	TIME [epoch: 8.85 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.759527970549293		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5207943198205406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6401611451849168 | validation: 0.3792295474887835]
	TIME [epoch: 8.85 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5346576165338297		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46929358945796446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.501975602995897 | validation: 0.37267654740306144]
	TIME [epoch: 8.85 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45794577521557933		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6738238940331296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5658848346243543 | validation: 1.0216640326114825]
	TIME [epoch: 8.86 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6338982025721537		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5092694421392322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5715838223556929 | validation: 0.6451193636900454]
	TIME [epoch: 8.88 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6045607540694947		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.621205592079073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6128831730742839 | validation: 0.4396051910392058]
	TIME [epoch: 8.86 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5497651989070128		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.486027845444773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5178965221758929 | validation: 0.6322689668036152]
	TIME [epoch: 8.85 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5159563986216729		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5142528757012326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5151046371614526 | validation: 1.1153222386190436]
	TIME [epoch: 8.85 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7400081599292772		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.137603491726051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9388058258276644 | validation: 8.063103026093957]
	TIME [epoch: 8.87 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: nan		[learning rate: 0.01]
		[batch 20/20] avg loss: nan		[learning rate: 0.01]
ERROR:
nan encountered in epoch 372 (training loss).
