Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r2', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4157605031

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.932099742364432		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.662165555378587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.297132648871509 | validation: 8.029916344358742]
	TIME [epoch: 47.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.108885895691635		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.784794261803038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.946840078747336 | validation: 7.1229490060943075]
	TIME [epoch: 8.91 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.1895351592142855		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.097972199146729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.1437536791805085 | validation: 6.291260782882356]
	TIME [epoch: 8.93 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.343584777295122		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.922747508685416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.133166142990268 | validation: 5.516181028711047]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.695173460149078		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.595502201307735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.645337830728407 | validation: 5.749704036003252]
	TIME [epoch: 8.89 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.282859425908906		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.11717765314903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.200018539528967 | validation: 5.273880768209134]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.967278717987082		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.102670639718164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.034974678852622 | validation: 5.005767901675967]
	TIME [epoch: 8.91 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.689028830595293		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.4659696162840365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5774992234396645 | validation: 4.376627238235943]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.29675623689005		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.9254001211028893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.11107817899647 | validation: 3.357687454743415]
	TIME [epoch: 8.91 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1751124412873266		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.724048387903924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.949580414595625 | validation: 2.504660623446742]
	TIME [epoch: 8.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4350411554600084		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0898538918333633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.262447523646686 | validation: 1.8297860772101204]
	TIME [epoch: 8.92 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8194974981927874		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8867709654013907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8531342317970896 | validation: 1.6149828617777993]
	TIME [epoch: 8.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.740385868588446		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5836911030598837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6620384858241644 | validation: 2.291110788290231]
	TIME [epoch: 8.89 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6467245291773327		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.000507561719872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8236160454486026 | validation: 1.6240987110709204]
	TIME [epoch: 8.88 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.479295552361055		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.673332183219315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5763138677901851 | validation: 2.1251162485978234]
	TIME [epoch: 8.9 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.650336212012118		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5160870852142836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.583211648613201 | validation: 1.446561895230249]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8369091034842722		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.511104135682348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.67400661958331 | validation: 1.472553987973668]
	TIME [epoch: 8.87 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4989091509459544		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3021818758820387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4005455134139968 | validation: 1.2802660296050297]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.688356725593423		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4146604250960455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5515085753447342 | validation: 1.4699207551221227]
	TIME [epoch: 8.89 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2097527294824277		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.535905589708162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3728291595952948 | validation: 1.3642784999667363]
	TIME [epoch: 8.88 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3993931289509596		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.483886314252533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.441639721601746 | validation: 1.5791047208911742]
	TIME [epoch: 8.88 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4302884470515083		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.519520436235607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4749044416435577 | validation: 1.641577891021159]
	TIME [epoch: 8.88 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2705089895291046		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4493841147176223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3599465521233636 | validation: 1.2082794493545173]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4145612352921264		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.520316409238791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4674388222654584 | validation: 1.4423676703029675]
	TIME [epoch: 8.89 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3148443626337167		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3488007326760112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.331822547654864 | validation: 1.3952535217448645]
	TIME [epoch: 8.87 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4334092068006492		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.371938046591209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.402673626695929 | validation: 1.6762065610313386]
	TIME [epoch: 8.87 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3127383277003353		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.41066446526696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3617013964836475 | validation: 1.5283717250451432]
	TIME [epoch: 8.88 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3881653235666414		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.187066462558456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.287615893062549 | validation: 1.2764368380402487]
	TIME [epoch: 8.89 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.885198035069796		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.238720982210708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.561959508640252 | validation: 1.8322152649856567]
	TIME [epoch: 8.87 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2839114051590768		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.272919876048175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.278415640603626 | validation: 1.32766515629376]
	TIME [epoch: 8.87 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3594658388692749		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2126102391697073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.286038039019491 | validation: 1.4416341788845317]
	TIME [epoch: 8.86 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0845386799824754		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.262596273363606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1735674766730404 | validation: 1.4566527599446912]
	TIME [epoch: 8.89 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1510390363872305		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1582852927168352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1546621645520323 | validation: 1.3039262640468963]
	TIME [epoch: 8.86 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1044573606861072		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.147674179204599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1260657699453531 | validation: 1.2547193105199996]
	TIME [epoch: 8.87 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.175297919262311		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1631153573391797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1692066383007451 | validation: 1.197446234844107]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1690724679652789		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1334616549596566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1512670614624678 | validation: 1.2703109712128553]
	TIME [epoch: 8.88 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1483834320922743		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3558108379357559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2520971350140155 | validation: 1.165676128665838]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1590974976086648		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2584219067999487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2087597022043066 | validation: 1.621212881166115]
	TIME [epoch: 8.87 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3471491995493736		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3246138309680133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3358815152586936 | validation: 1.4039714845942222]
	TIME [epoch: 8.88 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.175722074237759		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.038903350033604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1073127121356818 | validation: 1.0384091867143475]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1052827528692148		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1663512128011013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1358169828351579 | validation: 1.2753494032517851]
	TIME [epoch: 8.9 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0289843487177817		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1135918828605664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.071288115789174 | validation: 1.1868119761595302]
	TIME [epoch: 8.87 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.133182584189001		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0555582985663952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.094370441377698 | validation: 1.3529028136259815]
	TIME [epoch: 8.88 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0597556523654073		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0952453074248418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0775004798951244 | validation: 1.9462302379261076]
	TIME [epoch: 8.89 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2805277252559037		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.076561300228181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1785445127420422 | validation: 1.1233746833961384]
	TIME [epoch: 8.91 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0899967105805286		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2104457765617311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.15022124357113 | validation: 1.3044461760829122]
	TIME [epoch: 8.9 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2092869830381647		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9782945364914879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0937907597648266 | validation: 1.2946932365794255]
	TIME [epoch: 8.89 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0463919211940644		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9957451338547727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0210685275244187 | validation: 1.41533077393588]
	TIME [epoch: 8.89 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1458314075651408		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2004584246948413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.173144916129991 | validation: 1.1441894736487428]
	TIME [epoch: 8.91 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9645018733966113		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0845932966785914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0245475850376011 | validation: 1.2056045169381826]
	TIME [epoch: 8.9 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.027370245994632		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1753890637658595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.101379654880246 | validation: 1.1858160028578293]
	TIME [epoch: 8.89 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0017590315901912		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0956510166970834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0487050241436378 | validation: 1.021513357501408]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1712408604168147		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9731045352411141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.072172697828964 | validation: 1.140966753823508]
	TIME [epoch: 8.9 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8831983484084887		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8611867345242976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8721925414663932 | validation: 1.4425549255355974]
	TIME [epoch: 8.92 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8725108823617032		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0032456104647598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9378782464132314 | validation: 1.036776041944315]
	TIME [epoch: 8.89 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1022324618717214		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2294263303835666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1658293961276438 | validation: 1.5040057283879955]
	TIME [epoch: 8.9 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0054792058549444		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0793729891697785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0424260975123616 | validation: 1.168990385074873]
	TIME [epoch: 8.89 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2442230512696617		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0644656897197493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1543443704947056 | validation: 1.5316482597045256]
	TIME [epoch: 8.89 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0779527361786068		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0968687735227514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0874107548506793 | validation: 1.1479742847220953]
	TIME [epoch: 8.92 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9110037415286852		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.022182394270522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9665930678996038 | validation: 1.2587173460718302]
	TIME [epoch: 8.89 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9079709682476338		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8130074967154872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8604892324815603 | validation: 0.9077995159716753]
	TIME [epoch: 8.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9954927390807622		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.034874992971313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0151838660260375 | validation: 1.1566406051912743]
	TIME [epoch: 8.87 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8638856851308161		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9365160879655308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9002008865481734 | validation: 1.0705324523570146]
	TIME [epoch: 8.9 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0713110554875847		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9131251251510173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.992218090319301 | validation: 1.1025819872757172]
	TIME [epoch: 8.89 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0217167435073229		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3897218890113827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2057193162593527 | validation: 1.109787342421036]
	TIME [epoch: 8.88 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9808025977635477		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9965566064130582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9886796020883033 | validation: 1.2119078465769337]
	TIME [epoch: 8.88 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.928282171183952		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8219441066406595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8751131389123058 | validation: 1.6147288250037035]
	TIME [epoch: 8.87 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8578035812377907		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9891683648314317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9234859730346112 | validation: 0.9533239201035351]
	TIME [epoch: 8.9 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9761549515803409		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9322791180047251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9542170347925332 | validation: 0.9712531547126463]
	TIME [epoch: 8.88 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9159929818432749		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8790082660572205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8975006239502477 | validation: 1.0065801309734772]
	TIME [epoch: 8.88 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9081558335398064		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9384779065542466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9233168700470266 | validation: 1.1482560571904108]
	TIME [epoch: 8.87 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8561043024365386		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8498192178945783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8529617601655584 | validation: 0.8946808931526458]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8839901446350321		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8062944073114389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8451422759732358 | validation: 1.1597750867068515]
	TIME [epoch: 8.89 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8624371736692797		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8742223970076457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8683297853384628 | validation: 1.1894690034337443]
	TIME [epoch: 8.86 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8992662399262622		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8580255615403407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8786459007333016 | validation: 1.8869661143340684]
	TIME [epoch: 8.86 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0876245411884757		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1058736219746748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0967490815815755 | validation: 1.233977566150455]
	TIME [epoch: 8.86 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8709745679125295		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8850639159595041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8780192419360168 | validation: 1.0593113871472257]
	TIME [epoch: 8.9 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7195833826693113		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9469584515020124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8332709170856617 | validation: 1.1918140682203056]
	TIME [epoch: 8.88 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.884845855785389		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9854984900871043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9351721729362469 | validation: 1.4706975778503095]
	TIME [epoch: 8.87 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9346974535960255		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9188634245924904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9267804390942581 | validation: 1.3337518411376459]
	TIME [epoch: 8.87 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8211715381188898		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.851992344285209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8365819412020492 | validation: 0.7978389705186753]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9251056425462792		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8352314153138215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8801685289300503 | validation: 1.1165645418975636]
	TIME [epoch: 8.89 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7402977538222707		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7600720703990678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7501849121106692 | validation: 1.2716845230334364]
	TIME [epoch: 8.87 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.823444653287465		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9535437924173336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8884942228523991 | validation: 0.8706961553229693]
	TIME [epoch: 8.86 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8588672982407146		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7774486394864899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8181579688636024 | validation: 0.7522684629268654]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.822875714233757		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7507915134560419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7868336138448994 | validation: 1.2299806215537683]
	TIME [epoch: 8.87 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9358665541922548		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9911320444482961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9634992993202756 | validation: 1.170832446754845]
	TIME [epoch: 8.89 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8494234608116809		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7850897665890099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8172566137003454 | validation: 0.9090083508324895]
	TIME [epoch: 8.87 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8354431350179838		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7607373391176449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7980902370678143 | validation: 1.1575965949138602]
	TIME [epoch: 8.87 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8583203150844139		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7689484117722231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8136343634283183 | validation: 0.9974310182179247]
	TIME [epoch: 8.87 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8039039219442004		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8088777414855945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8063908317148976 | validation: 1.0481447902364227]
	TIME [epoch: 8.89 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7537225407849846		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7137446818207616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.733733611302873 | validation: 1.0640662049471732]
	TIME [epoch: 8.89 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6787515907750271		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7308363735980896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7047939821865583 | validation: 0.8142424624041689]
	TIME [epoch: 8.88 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7404291590732706		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8853078594810325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8128685092771514 | validation: 1.077501045457208]
	TIME [epoch: 8.87 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2141032200455797		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6622481188102288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9381756694279042 | validation: 0.7684139241417719]
	TIME [epoch: 8.87 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6439582488971394		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8063847884673427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.725171518682241 | validation: 0.872566094860023]
	TIME [epoch: 8.89 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6399432712444209		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.198676860805595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9193100660250078 | validation: 0.8107647554987135]
	TIME [epoch: 8.88 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.686018177815816		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6868986278883487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6864584028520823 | validation: 0.7119018576828888]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7247870243924557		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.629439438949356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6771132316709058 | validation: 0.6745764044038635]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7182294786135122		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.763887735833886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7410586072236991 | validation: 0.5782168400716969]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0495153581735308		[learning rate: 0.0099891]
		[batch 20/20] avg loss: 0.7111290930273564		[learning rate: 0.009977]
	Learning Rate: 0.009977
	LOSS [training: 0.8803222256004435 | validation: 0.7769630038375134]
	TIME [epoch: 8.89 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8093662373874503		[learning rate: 0.0099649]
		[batch 20/20] avg loss: 0.9254351361530745		[learning rate: 0.0099528]
	Learning Rate: 0.00995285
	LOSS [training: 0.8674006867702623 | validation: 1.185799000861573]
	TIME [epoch: 8.85 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6272651981650165		[learning rate: 0.0099408]
		[batch 20/20] avg loss: 0.709990519697262		[learning rate: 0.0099288]
	Learning Rate: 0.00992875
	LOSS [training: 0.6686278589311394 | validation: 0.8857036310609994]
	TIME [epoch: 8.85 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5852389624309817		[learning rate: 0.0099167]
		[batch 20/20] avg loss: 0.6001580184506773		[learning rate: 0.0099047]
	Learning Rate: 0.00990472
	LOSS [training: 0.5926984904408295 | validation: 0.4872788514370664]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7103967649487524		[learning rate: 0.0098927]
		[batch 20/20] avg loss: 0.6925080023038037		[learning rate: 0.0098807]
	Learning Rate: 0.00988074
	LOSS [training: 0.701452383626278 | validation: 1.148738506764007]
	TIME [epoch: 8.92 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8297301185147464		[learning rate: 0.0098688]
		[batch 20/20] avg loss: 0.7302568330868153		[learning rate: 0.0098568]
	Learning Rate: 0.00985682
	LOSS [training: 0.7799934758007809 | validation: 1.1032507699342613]
	TIME [epoch: 8.91 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.647517722543079		[learning rate: 0.0098449]
		[batch 20/20] avg loss: 0.5747302337522946		[learning rate: 0.009833]
	Learning Rate: 0.00983296
	LOSS [training: 0.6111239781476869 | validation: 0.5462291864928419]
	TIME [epoch: 8.9 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6374938647245882		[learning rate: 0.009821]
		[batch 20/20] avg loss: 0.6127356303223558		[learning rate: 0.0098092]
	Learning Rate: 0.00980915
	LOSS [training: 0.6251147475234718 | validation: 0.7023728808594007]
	TIME [epoch: 8.89 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6190225059674088		[learning rate: 0.0097973]
		[batch 20/20] avg loss: 0.6315747210040746		[learning rate: 0.0097854]
	Learning Rate: 0.00978541
	LOSS [training: 0.6252986134857418 | validation: 0.49634458313540575]
	TIME [epoch: 8.89 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6789199485081269		[learning rate: 0.0097736]
		[batch 20/20] avg loss: 0.7680716060350028		[learning rate: 0.0097617]
	Learning Rate: 0.00976172
	LOSS [training: 0.7234957772715649 | validation: 0.7924147697895463]
	TIME [epoch: 8.92 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5707175579582673		[learning rate: 0.0097499]
		[batch 20/20] avg loss: 0.7553774981901297		[learning rate: 0.0097381]
	Learning Rate: 0.00973809
	LOSS [training: 0.6630475280741984 | validation: 0.7997991913990277]
	TIME [epoch: 8.89 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5825607222259721		[learning rate: 0.0097263]
		[batch 20/20] avg loss: 0.695336533480661		[learning rate: 0.0097145]
	Learning Rate: 0.00971451
	LOSS [training: 0.6389486278533166 | validation: 1.2112659368681682]
	TIME [epoch: 8.89 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6133392751290978		[learning rate: 0.0097027]
		[batch 20/20] avg loss: 0.8802431832255546		[learning rate: 0.009691]
	Learning Rate: 0.009691
	LOSS [training: 0.7467912291773262 | validation: 0.8122684987525783]
	TIME [epoch: 8.89 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5712769138454686		[learning rate: 0.0096793]
		[batch 20/20] avg loss: 0.9824589774481545		[learning rate: 0.0096675]
	Learning Rate: 0.00966754
	LOSS [training: 0.7768679456468115 | validation: 0.5209682968023973]
	TIME [epoch: 8.88 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47075707005700984		[learning rate: 0.0096558]
		[batch 20/20] avg loss: 0.5782635323675049		[learning rate: 0.0096441]
	Learning Rate: 0.00964413
	LOSS [training: 0.5245103012122574 | validation: 0.910362382953273]
	TIME [epoch: 8.93 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5695276868217267		[learning rate: 0.0096325]
		[batch 20/20] avg loss: 0.6851636940351117		[learning rate: 0.0096208]
	Learning Rate: 0.00962078
	LOSS [training: 0.6273456904284191 | validation: 0.7345185724152302]
	TIME [epoch: 8.89 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7373277321389521		[learning rate: 0.0096091]
		[batch 20/20] avg loss: 0.8062173144188465		[learning rate: 0.0095975]
	Learning Rate: 0.00959749
	LOSS [training: 0.7717725232788993 | validation: 1.9421097256915885]
	TIME [epoch: 8.89 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7778385248474835		[learning rate: 0.0095859]
		[batch 20/20] avg loss: 0.5744661580376885		[learning rate: 0.0095743]
	Learning Rate: 0.00957426
	LOSS [training: 0.676152341442586 | validation: 0.5817985984153243]
	TIME [epoch: 8.9 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6247219328517326		[learning rate: 0.0095627]
		[batch 20/20] avg loss: 0.5604360997460359		[learning rate: 0.0095511]
	Learning Rate: 0.00955108
	LOSS [training: 0.5925790162988843 | validation: 1.4241752302403907]
	TIME [epoch: 8.91 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6435588523534213		[learning rate: 0.0095395]
		[batch 20/20] avg loss: 0.5030689347648328		[learning rate: 0.009528]
	Learning Rate: 0.00952796
	LOSS [training: 0.5733138935591271 | validation: 0.5299210266923247]
	TIME [epoch: 8.91 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5562304758451385		[learning rate: 0.0095164]
		[batch 20/20] avg loss: 0.56043469844763		[learning rate: 0.0095049]
	Learning Rate: 0.0095049
	LOSS [training: 0.5583325871463843 | validation: 1.0192474363888844]
	TIME [epoch: 8.89 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6843956339359998		[learning rate: 0.0094934]
		[batch 20/20] avg loss: 0.5230123165126674		[learning rate: 0.0094819]
	Learning Rate: 0.00948189
	LOSS [training: 0.6037039752243336 | validation: 0.7769910826881594]
	TIME [epoch: 8.89 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4736161953533764		[learning rate: 0.0094704]
		[batch 20/20] avg loss: 0.576468373116999		[learning rate: 0.0094589]
	Learning Rate: 0.00945893
	LOSS [training: 0.5250422842351876 | validation: 0.9447069447336451]
	TIME [epoch: 8.89 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7047142327901168		[learning rate: 0.0094475]
		[batch 20/20] avg loss: 0.730759594906667		[learning rate: 0.009436]
	Learning Rate: 0.00943603
	LOSS [training: 0.7177369138483919 | validation: 0.4917292382245291]
	TIME [epoch: 8.92 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6700875586091619		[learning rate: 0.0094246]
		[batch 20/20] avg loss: 0.6388487760840948		[learning rate: 0.0094132]
	Learning Rate: 0.00941319
	LOSS [training: 0.6544681673466284 | validation: 1.1739154204361433]
	TIME [epoch: 8.9 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6106495698509896		[learning rate: 0.0094018]
		[batch 20/20] avg loss: 0.6208760721636546		[learning rate: 0.0093904]
	Learning Rate: 0.0093904
	LOSS [training: 0.615762821007322 | validation: 0.644985269124529]
	TIME [epoch: 8.89 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5500501427004013		[learning rate: 0.009379]
		[batch 20/20] avg loss: 0.4749477803366098		[learning rate: 0.0093677]
	Learning Rate: 0.00936767
	LOSS [training: 0.5124989615185057 | validation: 0.6476863225526823]
	TIME [epoch: 8.89 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5281709483404965		[learning rate: 0.0093563]
		[batch 20/20] avg loss: 0.434447864509211		[learning rate: 0.009345]
	Learning Rate: 0.00934499
	LOSS [training: 0.48130940642485376 | validation: 0.3015759463276313]
	TIME [epoch: 8.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5838140294405847		[learning rate: 0.0093337]
		[batch 20/20] avg loss: 0.5685722755043912		[learning rate: 0.0093224]
	Learning Rate: 0.00932237
	LOSS [training: 0.5761931524724878 | validation: 0.4710079253861702]
	TIME [epoch: 8.92 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49244234008411203		[learning rate: 0.0093111]
		[batch 20/20] avg loss: 0.451494700711702		[learning rate: 0.0092998]
	Learning Rate: 0.0092998
	LOSS [training: 0.4719685203979068 | validation: 0.7191880946163876]
	TIME [epoch: 8.89 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5406551425486066		[learning rate: 0.0092885]
		[batch 20/20] avg loss: 0.457046520603714		[learning rate: 0.0092773]
	Learning Rate: 0.00927729
	LOSS [training: 0.4988508315761603 | validation: 0.6232716799280205]
	TIME [epoch: 8.89 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5722329336823306		[learning rate: 0.0092661]
		[batch 20/20] avg loss: 0.5220087448326488		[learning rate: 0.0092548]
	Learning Rate: 0.00925483
	LOSS [training: 0.5471208392574899 | validation: 0.695886399442067]
	TIME [epoch: 8.89 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5331269624843793		[learning rate: 0.0092436]
		[batch 20/20] avg loss: 0.4887697738461448		[learning rate: 0.0092324]
	Learning Rate: 0.00923242
	LOSS [training: 0.510948368165262 | validation: 0.4027933632297206]
	TIME [epoch: 8.91 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4353135123275805		[learning rate: 0.0092212]
		[batch 20/20] avg loss: 0.4491676546225877		[learning rate: 0.0092101]
	Learning Rate: 0.00921007
	LOSS [training: 0.44224058347508405 | validation: 0.5403954718022796]
	TIME [epoch: 8.89 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5855342719433081		[learning rate: 0.0091989]
		[batch 20/20] avg loss: 0.47540770673839755		[learning rate: 0.0091878]
	Learning Rate: 0.00918778
	LOSS [training: 0.5304709893408528 | validation: 0.23664972218876812]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5045770451034327		[learning rate: 0.0091767]
		[batch 20/20] avg loss: 0.41548661552140687		[learning rate: 0.0091655]
	Learning Rate: 0.00916554
	LOSS [training: 0.46003183031241973 | validation: 0.5118865374689475]
	TIME [epoch: 8.88 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4940211333625918		[learning rate: 0.0091544]
		[batch 20/20] avg loss: 0.5043715198679115		[learning rate: 0.0091433]
	Learning Rate: 0.00914335
	LOSS [training: 0.4991963266152517 | validation: 0.8013222764056066]
	TIME [epoch: 8.89 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5925689248031744		[learning rate: 0.0091323]
		[batch 20/20] avg loss: 0.5061297774765635		[learning rate: 0.0091212]
	Learning Rate: 0.00912121
	LOSS [training: 0.5493493511398689 | validation: 0.5328967536174527]
	TIME [epoch: 8.9 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5037656225329156		[learning rate: 0.0091102]
		[batch 20/20] avg loss: 0.6625513092640533		[learning rate: 0.0090991]
	Learning Rate: 0.00909913
	LOSS [training: 0.5831584658984845 | validation: 0.388153506946665]
	TIME [epoch: 8.88 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32348933593967855		[learning rate: 0.0090881]
		[batch 20/20] avg loss: 0.5025361173279814		[learning rate: 0.0090771]
	Learning Rate: 0.0090771
	LOSS [training: 0.41301272663383004 | validation: 0.3495451966985383]
	TIME [epoch: 8.87 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5281978192959543		[learning rate: 0.0090661]
		[batch 20/20] avg loss: 0.47170640416968423		[learning rate: 0.0090551]
	Learning Rate: 0.00905513
	LOSS [training: 0.4999521117328192 | validation: 0.3937444996253268]
	TIME [epoch: 8.87 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5443640138896213		[learning rate: 0.0090442]
		[batch 20/20] avg loss: 0.5429590220615133		[learning rate: 0.0090332]
	Learning Rate: 0.00903321
	LOSS [training: 0.5436615179755673 | validation: 0.3646630474393051]
	TIME [epoch: 8.87 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4760584711029295		[learning rate: 0.0090223]
		[batch 20/20] avg loss: 0.5711635825259394		[learning rate: 0.0090113]
	Learning Rate: 0.00901134
	LOSS [training: 0.5236110268144344 | validation: 0.9705397265479101]
	TIME [epoch: 8.9 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4805084238954474		[learning rate: 0.0090004]
		[batch 20/20] avg loss: 0.4457486237969661		[learning rate: 0.0089895]
	Learning Rate: 0.00898953
	LOSS [training: 0.4631285238462066 | validation: 0.28451627452374834]
	TIME [epoch: 8.87 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4354530180672861		[learning rate: 0.0089786]
		[batch 20/20] avg loss: 0.45394080835217904		[learning rate: 0.0089678]
	Learning Rate: 0.00896776
	LOSS [training: 0.4446969132097326 | validation: 0.23352981434146164]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5976993373470665		[learning rate: 0.0089569]
		[batch 20/20] avg loss: 0.5059850719308436		[learning rate: 0.0089461]
	Learning Rate: 0.00894605
	LOSS [training: 0.551842204638955 | validation: 0.5217939659804212]
	TIME [epoch: 8.88 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4421665734833179		[learning rate: 0.0089352]
		[batch 20/20] avg loss: 0.5191171757298982		[learning rate: 0.0089244]
	Learning Rate: 0.0089244
	LOSS [training: 0.48064187460660807 | validation: 0.36765521031414805]
	TIME [epoch: 8.89 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4076949750197107		[learning rate: 0.0089136]
		[batch 20/20] avg loss: 0.4814363752219381		[learning rate: 0.0089028]
	Learning Rate: 0.00890279
	LOSS [training: 0.44456567512082434 | validation: 0.34828815511474975]
	TIME [epoch: 8.88 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44221549465826177		[learning rate: 0.008892]
		[batch 20/20] avg loss: 0.585800468118641		[learning rate: 0.0088812]
	Learning Rate: 0.00888124
	LOSS [training: 0.5140079813884514 | validation: 0.2808508834780634]
	TIME [epoch: 8.86 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42468488569889623		[learning rate: 0.0088705]
		[batch 20/20] avg loss: 0.7077481091060767		[learning rate: 0.0088597]
	Learning Rate: 0.00885974
	LOSS [training: 0.5662164974024865 | validation: 0.46465145536060903]
	TIME [epoch: 8.87 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49532746079503287		[learning rate: 0.008849]
		[batch 20/20] avg loss: 0.5947857309234513		[learning rate: 0.0088383]
	Learning Rate: 0.00883829
	LOSS [training: 0.5450565958592423 | validation: 0.7369115018651695]
	TIME [epoch: 8.86 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5954482153259081		[learning rate: 0.0088276]
		[batch 20/20] avg loss: 0.5652099033454961		[learning rate: 0.0088169]
	Learning Rate: 0.0088169
	LOSS [training: 0.580329059335702 | validation: 0.8011874656301099]
	TIME [epoch: 8.9 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5724561083032248		[learning rate: 0.0088062]
		[batch 20/20] avg loss: 0.44944599019811005		[learning rate: 0.0087956]
	Learning Rate: 0.00879555
	LOSS [training: 0.5109510492506674 | validation: 0.4103606588551131]
	TIME [epoch: 8.87 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5395694052755171		[learning rate: 0.0087849]
		[batch 20/20] avg loss: 0.43982427712911665		[learning rate: 0.0087743]
	Learning Rate: 0.00877426
	LOSS [training: 0.48969684120231693 | validation: 0.8779190222475566]
	TIME [epoch: 8.87 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49691139582383614		[learning rate: 0.0087636]
		[batch 20/20] avg loss: 0.46329236608642743		[learning rate: 0.008753]
	Learning Rate: 0.00875302
	LOSS [training: 0.48010188095513173 | validation: 0.4857807865869179]
	TIME [epoch: 8.87 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44616642734259904		[learning rate: 0.0087424]
		[batch 20/20] avg loss: 0.4310846959125344		[learning rate: 0.0087318]
	Learning Rate: 0.00873183
	LOSS [training: 0.4386255616275667 | validation: 0.40633787272582444]
	TIME [epoch: 8.86 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4412192664751321		[learning rate: 0.0087213]
		[batch 20/20] avg loss: 0.49753219934809134		[learning rate: 0.0087107]
	Learning Rate: 0.00871069
	LOSS [training: 0.4693757329116117 | validation: 0.29276646008049234]
	TIME [epoch: 8.9 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.419178909087913		[learning rate: 0.0087001]
		[batch 20/20] avg loss: 0.49899439930625783		[learning rate: 0.0086896]
	Learning Rate: 0.0086896
	LOSS [training: 0.4590866541970854 | validation: 0.7188060561168076]
	TIME [epoch: 8.87 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5142229170414423		[learning rate: 0.0086791]
		[batch 20/20] avg loss: 0.4287893986261693		[learning rate: 0.0086686]
	Learning Rate: 0.00866857
	LOSS [training: 0.4715061578338058 | validation: 1.0794688220678528]
	TIME [epoch: 8.87 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4631815560580388		[learning rate: 0.0086581]
		[batch 20/20] avg loss: 0.4224127424496363		[learning rate: 0.0086476]
	Learning Rate: 0.00864758
	LOSS [training: 0.44279714925383756 | validation: 0.22709289435227004]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41394992897907457		[learning rate: 0.0086371]
		[batch 20/20] avg loss: 0.5224099217106299		[learning rate: 0.0086266]
	Learning Rate: 0.00862665
	LOSS [training: 0.46817992534485225 | validation: 0.59559901994402]
	TIME [epoch: 8.89 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42362809244524435		[learning rate: 0.0086162]
		[batch 20/20] avg loss: 0.3367388829597818		[learning rate: 0.0086058]
	Learning Rate: 0.00860576
	LOSS [training: 0.38018348770251303 | validation: 0.5843991862653845]
	TIME [epoch: 8.88 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5843949217998292		[learning rate: 0.0085953]
		[batch 20/20] avg loss: 0.3627800500251017		[learning rate: 0.0085849]
	Learning Rate: 0.00858493
	LOSS [training: 0.4735874859124654 | validation: 0.7090557764852079]
	TIME [epoch: 8.87 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4098793481240671		[learning rate: 0.0085745]
		[batch 20/20] avg loss: 0.3646034303222508		[learning rate: 0.0085641]
	Learning Rate: 0.00856415
	LOSS [training: 0.387241389223159 | validation: 0.48358782597796857]
	TIME [epoch: 8.86 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4526849398052629		[learning rate: 0.0085538]
		[batch 20/20] avg loss: 0.6278592150595463		[learning rate: 0.0085434]
	Learning Rate: 0.00854342
	LOSS [training: 0.5402720774324046 | validation: 0.6768815313377806]
	TIME [epoch: 8.87 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.505327119499232		[learning rate: 0.0085331]
		[batch 20/20] avg loss: 0.44286421710803775		[learning rate: 0.0085227]
	Learning Rate: 0.00852273
	LOSS [training: 0.4740956683036349 | validation: 0.4339736045383915]
	TIME [epoch: 8.89 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5194819545227021		[learning rate: 0.0085124]
		[batch 20/20] avg loss: 0.5578350417611587		[learning rate: 0.0085021]
	Learning Rate: 0.0085021
	LOSS [training: 0.5386584981419305 | validation: 0.2861703509015778]
	TIME [epoch: 8.88 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4975768372366904		[learning rate: 0.0084918]
		[batch 20/20] avg loss: 0.40864108854257547		[learning rate: 0.0084815]
	Learning Rate: 0.00848152
	LOSS [training: 0.45310896288963304 | validation: 0.2645126793037884]
	TIME [epoch: 8.87 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4597404374983022		[learning rate: 0.0084712]
		[batch 20/20] avg loss: 0.5386975206491033		[learning rate: 0.008461]
	Learning Rate: 0.00846099
	LOSS [training: 0.49921897907370266 | validation: 0.3220650629285296]
	TIME [epoch: 8.86 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39668896376099866		[learning rate: 0.0084507]
		[batch 20/20] avg loss: 0.4185736557398023		[learning rate: 0.0084405]
	Learning Rate: 0.0084405
	LOSS [training: 0.40763130975040046 | validation: 0.53530817945118]
	TIME [epoch: 8.87 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48515341078869456		[learning rate: 0.0084303]
		[batch 20/20] avg loss: 0.47191000704501673		[learning rate: 0.0084201]
	Learning Rate: 0.00842007
	LOSS [training: 0.4785317089168557 | validation: 0.7118616891710248]
	TIME [epoch: 8.9 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39811689743591977		[learning rate: 0.0084099]
		[batch 20/20] avg loss: 0.47505685295450534		[learning rate: 0.0083997]
	Learning Rate: 0.00839969
	LOSS [training: 0.43658687519521255 | validation: 1.0067979061316406]
	TIME [epoch: 8.87 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5200485388887814		[learning rate: 0.0083895]
		[batch 20/20] avg loss: 0.48042271365641553		[learning rate: 0.0083794]
	Learning Rate: 0.00837935
	LOSS [training: 0.5002356262725984 | validation: 0.5564167509143302]
	TIME [epoch: 8.87 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4188483319949466		[learning rate: 0.0083692]
		[batch 20/20] avg loss: 0.43748520308680866		[learning rate: 0.0083591]
	Learning Rate: 0.00835907
	LOSS [training: 0.4281667675408777 | validation: 0.3010671827556808]
	TIME [epoch: 8.87 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4603861964604919		[learning rate: 0.0083489]
		[batch 20/20] avg loss: 0.4388482173001499		[learning rate: 0.0083388]
	Learning Rate: 0.00833883
	LOSS [training: 0.4496172068803209 | validation: 0.43517114674777474]
	TIME [epoch: 8.89 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4967505993144939		[learning rate: 0.0083287]
		[batch 20/20] avg loss: 0.439870985638897		[learning rate: 0.0083186]
	Learning Rate: 0.00831864
	LOSS [training: 0.46831079247669544 | validation: 0.7091765308669745]
	TIME [epoch: 8.88 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5204041315914989		[learning rate: 0.0083086]
		[batch 20/20] avg loss: 0.40174366666167566		[learning rate: 0.0082985]
	Learning Rate: 0.00829851
	LOSS [training: 0.4610738991265874 | validation: 0.2832030547196991]
	TIME [epoch: 8.88 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36587420425844297		[learning rate: 0.0082885]
		[batch 20/20] avg loss: 0.42237008560338135		[learning rate: 0.0082784]
	Learning Rate: 0.00827842
	LOSS [training: 0.39412214493091224 | validation: 0.3535888545533576]
	TIME [epoch: 8.87 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4237451915387328		[learning rate: 0.0082684]
		[batch 20/20] avg loss: 0.38971531061966846		[learning rate: 0.0082584]
	Learning Rate: 0.00825838
	LOSS [training: 0.4067302510792006 | validation: 0.40261698527570267]
	TIME [epoch: 8.88 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35410599427628964		[learning rate: 0.0082484]
		[batch 20/20] avg loss: 0.5901567940373533		[learning rate: 0.0082384]
	Learning Rate: 0.00823839
	LOSS [training: 0.4721313941568215 | validation: 0.6478227161549482]
	TIME [epoch: 8.9 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42047913357256084		[learning rate: 0.0082284]
		[batch 20/20] avg loss: 0.5052054658414654		[learning rate: 0.0082184]
	Learning Rate: 0.00821844
	LOSS [training: 0.4628422997070133 | validation: 0.38731844681643923]
	TIME [epoch: 8.87 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4330879937347819		[learning rate: 0.0082085]
		[batch 20/20] avg loss: 0.509336356874291		[learning rate: 0.0081985]
	Learning Rate: 0.00819855
	LOSS [training: 0.4712121753045365 | validation: 0.2902698377899419]
	TIME [epoch: 8.88 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40592737400850554		[learning rate: 0.0081886]
		[batch 20/20] avg loss: 0.4287848191553024		[learning rate: 0.0081787]
	Learning Rate: 0.0081787
	LOSS [training: 0.417356096581904 | validation: 0.5160907405201642]
	TIME [epoch: 8.87 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.453002095019338		[learning rate: 0.0081688]
		[batch 20/20] avg loss: 0.42420184354790963		[learning rate: 0.0081589]
	Learning Rate: 0.0081589
	LOSS [training: 0.4386019692836238 | validation: 0.2857624796523558]
	TIME [epoch: 8.88 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4930855961327314		[learning rate: 0.008149]
		[batch 20/20] avg loss: 0.5739044372736618		[learning rate: 0.0081391]
	Learning Rate: 0.00813915
	LOSS [training: 0.5334950167031967 | validation: 0.4278222637416015]
	TIME [epoch: 8.9 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4480967809861836		[learning rate: 0.0081293]
		[batch 20/20] avg loss: 0.42637187968066714		[learning rate: 0.0081194]
	Learning Rate: 0.00811944
	LOSS [training: 0.43723433033342535 | validation: 0.3395834589768515]
	TIME [epoch: 8.88 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5352575041686758		[learning rate: 0.0081096]
		[batch 20/20] avg loss: 0.46408060007738416		[learning rate: 0.0080998]
	Learning Rate: 0.00809979
	LOSS [training: 0.4996690521230299 | validation: 0.2516793617599685]
	TIME [epoch: 8.88 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4042591032494142		[learning rate: 0.00809]
		[batch 20/20] avg loss: 0.3881232134109606		[learning rate: 0.0080802]
	Learning Rate: 0.00808018
	LOSS [training: 0.3961911583301874 | validation: 0.5448121258343761]
	TIME [epoch: 8.87 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4857022837868132		[learning rate: 0.0080704]
		[batch 20/20] avg loss: 0.4080266561476349		[learning rate: 0.0080606]
	Learning Rate: 0.00806062
	LOSS [training: 0.44686446996722406 | validation: 0.37407213407825846]
	TIME [epoch: 8.88 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3599314941766911		[learning rate: 0.0080509]
		[batch 20/20] avg loss: 0.4516453145547482		[learning rate: 0.0080411]
	Learning Rate: 0.00804111
	LOSS [training: 0.40578840436571967 | validation: 0.4401419713718576]
	TIME [epoch: 8.88 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38811731363865015		[learning rate: 0.0080314]
		[batch 20/20] avg loss: 0.3550733559632028		[learning rate: 0.0080216]
	Learning Rate: 0.00802164
	LOSS [training: 0.37159533480092644 | validation: 0.4914329730410707]
	TIME [epoch: 8.87 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5452942225438053		[learning rate: 0.0080119]
		[batch 20/20] avg loss: 0.44895042827303777		[learning rate: 0.0080022]
	Learning Rate: 0.00800222
	LOSS [training: 0.4971223254084215 | validation: 0.5202098143501686]
	TIME [epoch: 8.87 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37370025380048416		[learning rate: 0.0079925]
		[batch 20/20] avg loss: 0.5086607340426184		[learning rate: 0.0079828]
	Learning Rate: 0.00798285
	LOSS [training: 0.44118049392155123 | validation: 0.7696262495731021]
	TIME [epoch: 8.86 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4105593686824271		[learning rate: 0.0079732]
		[batch 20/20] avg loss: 0.4297481740914046		[learning rate: 0.0079635]
	Learning Rate: 0.00796352
	LOSS [training: 0.42015377138691584 | validation: 0.3749876416446744]
	TIME [epoch: 8.9 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44046913016414535		[learning rate: 0.0079539]
		[batch 20/20] avg loss: 0.6420053094357465		[learning rate: 0.0079442]
	Learning Rate: 0.00794424
	LOSS [training: 0.5412372197999459 | validation: 0.27739960575396044]
	TIME [epoch: 8.88 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3898240948874412		[learning rate: 0.0079346]
		[batch 20/20] avg loss: 0.39594223104916326		[learning rate: 0.007925]
	Learning Rate: 0.00792501
	LOSS [training: 0.39288316296830217 | validation: 0.282542136100155]
	TIME [epoch: 8.87 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4322262049069526		[learning rate: 0.0079154]
		[batch 20/20] avg loss: 0.40680445621843936		[learning rate: 0.0079058]
	Learning Rate: 0.00790583
	LOSS [training: 0.4195153305626961 | validation: 0.2945946865975876]
	TIME [epoch: 8.88 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42941466116521687		[learning rate: 0.0078963]
		[batch 20/20] avg loss: 0.3947193240096011		[learning rate: 0.0078867]
	Learning Rate: 0.00788669
	LOSS [training: 0.412066992587409 | validation: 0.2817199913622568]
	TIME [epoch: 8.87 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4980230343104702		[learning rate: 0.0078771]
		[batch 20/20] avg loss: 0.3911497827075359		[learning rate: 0.0078676]
	Learning Rate: 0.0078676
	LOSS [training: 0.44458640850900305 | validation: 0.37152824114961014]
	TIME [epoch: 8.91 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4150687505710122		[learning rate: 0.0078581]
		[batch 20/20] avg loss: 0.45265361826569805		[learning rate: 0.0078486]
	Learning Rate: 0.00784855
	LOSS [training: 0.4338611844183551 | validation: 0.2980843222852425]
	TIME [epoch: 8.88 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4472099637170722		[learning rate: 0.007839]
		[batch 20/20] avg loss: 0.3679010457398991		[learning rate: 0.0078296]
	Learning Rate: 0.00782955
	LOSS [training: 0.4075555047284857 | validation: 0.6008800454218652]
	TIME [epoch: 8.87 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34119869790672464		[learning rate: 0.0078201]
		[batch 20/20] avg loss: 0.46688171687319535		[learning rate: 0.0078106]
	Learning Rate: 0.0078106
	LOSS [training: 0.40404020738995994 | validation: 0.45280072752134504]
	TIME [epoch: 8.87 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43814523625735385		[learning rate: 0.0078011]
		[batch 20/20] avg loss: 0.36539921730107294		[learning rate: 0.0077917]
	Learning Rate: 0.00779169
	LOSS [training: 0.4017722267792134 | validation: 0.26415425801346515]
	TIME [epoch: 8.89 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3471215030268296		[learning rate: 0.0077823]
		[batch 20/20] avg loss: 0.43760538475098826		[learning rate: 0.0077728]
	Learning Rate: 0.00777283
	LOSS [training: 0.392363443888909 | validation: 0.794047820390255]
	TIME [epoch: 8.88 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44721540305329793		[learning rate: 0.0077634]
		[batch 20/20] avg loss: 0.4505055812857385		[learning rate: 0.007754]
	Learning Rate: 0.00775401
	LOSS [training: 0.4488604921695183 | validation: 0.33481275027218976]
	TIME [epoch: 8.88 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4419253845512654		[learning rate: 0.0077446]
		[batch 20/20] avg loss: 0.4080030330678016		[learning rate: 0.0077352]
	Learning Rate: 0.00773524
	LOSS [training: 0.4249642088095335 | validation: 1.0028019769646923]
	TIME [epoch: 8.87 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4155835646766417		[learning rate: 0.0077259]
		[batch 20/20] avg loss: 0.37496286727405137		[learning rate: 0.0077165]
	Learning Rate: 0.00771651
	LOSS [training: 0.39527321597534654 | validation: 0.6472587232620244]
	TIME [epoch: 8.88 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3592051223764471		[learning rate: 0.0077072]
		[batch 20/20] avg loss: 0.4120628465901712		[learning rate: 0.0076978]
	Learning Rate: 0.00769783
	LOSS [training: 0.38563398448330927 | validation: 0.3022790842167166]
	TIME [epoch: 8.9 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4467853150104615		[learning rate: 0.0076885]
		[batch 20/20] avg loss: 0.3652941722545893		[learning rate: 0.0076792]
	Learning Rate: 0.0076792
	LOSS [training: 0.40603974363252543 | validation: 0.2540204807332517]
	TIME [epoch: 8.89 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3980468338911788		[learning rate: 0.0076699]
		[batch 20/20] avg loss: 0.34857139935518927		[learning rate: 0.0076606]
	Learning Rate: 0.00766061
	LOSS [training: 0.373309116623184 | validation: 0.6806607973937897]
	TIME [epoch: 8.88 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5826162714481249		[learning rate: 0.0076513]
		[batch 20/20] avg loss: 0.4238715639650814		[learning rate: 0.0076421]
	Learning Rate: 0.00764206
	LOSS [training: 0.5032439177066033 | validation: 0.6515736253089393]
	TIME [epoch: 8.88 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41390632465623234		[learning rate: 0.0076328]
		[batch 20/20] avg loss: 0.3484284377513216		[learning rate: 0.0076236]
	Learning Rate: 0.00762356
	LOSS [training: 0.3811673812037769 | validation: 0.6298815433324687]
	TIME [epoch: 8.88 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43453325254728864		[learning rate: 0.0076143]
		[batch 20/20] avg loss: 0.369531229528271		[learning rate: 0.0076051]
	Learning Rate: 0.00760511
	LOSS [training: 0.40203224103777985 | validation: 0.33158102562810354]
	TIME [epoch: 8.91 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2940489422876046		[learning rate: 0.0075959]
		[batch 20/20] avg loss: 0.4119992582549896		[learning rate: 0.0075867]
	Learning Rate: 0.00758669
	LOSS [training: 0.35302410027129705 | validation: 0.24877269199824595]
	TIME [epoch: 8.89 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3501915157999828		[learning rate: 0.0075775]
		[batch 20/20] avg loss: 0.42396560971020014		[learning rate: 0.0075683]
	Learning Rate: 0.00756833
	LOSS [training: 0.3870785627550914 | validation: 0.37025305682118465]
	TIME [epoch: 8.88 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42447099276379047		[learning rate: 0.0075592]
		[batch 20/20] avg loss: 0.3762015160194873		[learning rate: 0.00755]
	Learning Rate: 0.00755001
	LOSS [training: 0.40033625439163883 | validation: 0.3267865441847673]
	TIME [epoch: 8.88 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39602503023084434		[learning rate: 0.0075409]
		[batch 20/20] avg loss: 0.3423863812180509		[learning rate: 0.0075317]
	Learning Rate: 0.00753173
	LOSS [training: 0.3692057057244476 | validation: 0.5184471246642615]
	TIME [epoch: 8.88 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30821163602830376		[learning rate: 0.0075226]
		[batch 20/20] avg loss: 0.40799103886703963		[learning rate: 0.0075135]
	Learning Rate: 0.0075135
	LOSS [training: 0.3581013374476717 | validation: 0.28280998253995854]
	TIME [epoch: 8.89 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.321015499271447		[learning rate: 0.0075044]
		[batch 20/20] avg loss: 0.3708949155036019		[learning rate: 0.0074953]
	Learning Rate: 0.00749531
	LOSS [training: 0.3459552073875244 | validation: 0.2306129239070291]
	TIME [epoch: 8.89 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4297695752746968		[learning rate: 0.0074862]
		[batch 20/20] avg loss: 0.24829069037207924		[learning rate: 0.0074772]
	Learning Rate: 0.00747716
	LOSS [training: 0.33903013282338806 | validation: 0.18391863201599734]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3745580673392058		[learning rate: 0.0074681]
		[batch 20/20] avg loss: 0.3890191610542759		[learning rate: 0.0074591]
	Learning Rate: 0.00745906
	LOSS [training: 0.3817886141967408 | validation: 0.4423991328158679]
	TIME [epoch: 8.87 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2925898462890134		[learning rate: 0.00745]
		[batch 20/20] avg loss: 0.45805013546784645		[learning rate: 0.007441]
	Learning Rate: 0.007441
	LOSS [training: 0.37531999087843 | validation: 0.3438895105148453]
	TIME [epoch: 8.89 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2793117824151313		[learning rate: 0.007432]
		[batch 20/20] avg loss: 0.3118981749260485		[learning rate: 0.007423]
	Learning Rate: 0.00742299
	LOSS [training: 0.2956049786705899 | validation: 0.18699209138463616]
	TIME [epoch: 8.86 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38222676528353816		[learning rate: 0.007414]
		[batch 20/20] avg loss: 0.31109503671576644		[learning rate: 0.007405]
	Learning Rate: 0.00740502
	LOSS [training: 0.3466609009996523 | validation: 0.22373053605546084]
	TIME [epoch: 8.84 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2537972171395354		[learning rate: 0.0073961]
		[batch 20/20] avg loss: 0.3496325439892535		[learning rate: 0.0073871]
	Learning Rate: 0.0073871
	LOSS [training: 0.30171488056439444 | validation: 0.39620348906618474]
	TIME [epoch: 8.87 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3857627619638952		[learning rate: 0.0073781]
		[batch 20/20] avg loss: 0.4068777987495002		[learning rate: 0.0073692]
	Learning Rate: 0.00736921
	LOSS [training: 0.39632028035669775 | validation: 0.23387015985109028]
	TIME [epoch: 8.95 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3454513903224078		[learning rate: 0.0073603]
		[batch 20/20] avg loss: 0.22574515672557424		[learning rate: 0.0073514]
	Learning Rate: 0.00735137
	LOSS [training: 0.285598273523991 | validation: 0.459943033107887]
	TIME [epoch: 8.94 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2741852861953331		[learning rate: 0.0073425]
		[batch 20/20] avg loss: 0.32779500879837087		[learning rate: 0.0073336]
	Learning Rate: 0.00733358
	LOSS [training: 0.300990147496852 | validation: 0.23138644860149263]
	TIME [epoch: 8.92 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2929347213768823		[learning rate: 0.0073247]
		[batch 20/20] avg loss: 0.3200836583088892		[learning rate: 0.0073158]
	Learning Rate: 0.00731582
	LOSS [training: 0.3065091898428857 | validation: 0.6459468664267751]
	TIME [epoch: 8.91 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32292607773345905		[learning rate: 0.007307]
		[batch 20/20] avg loss: 0.37554029072301953		[learning rate: 0.0072981]
	Learning Rate: 0.00729811
	LOSS [training: 0.34923318422823935 | validation: 0.16433934157574087]
	TIME [epoch: 8.91 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24098542337856044		[learning rate: 0.0072893]
		[batch 20/20] avg loss: 0.37337933101373677		[learning rate: 0.0072804]
	Learning Rate: 0.00728044
	LOSS [training: 0.30718237719614855 | validation: 0.3772114168460669]
	TIME [epoch: 8.93 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45163606712059917		[learning rate: 0.0072716]
		[batch 20/20] avg loss: 0.30001131937820114		[learning rate: 0.0072628]
	Learning Rate: 0.00726282
	LOSS [training: 0.37582369324940024 | validation: 0.376222331614036]
	TIME [epoch: 8.88 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3896580052802313		[learning rate: 0.007254]
		[batch 20/20] avg loss: 0.6461199074296037		[learning rate: 0.0072452]
	Learning Rate: 0.00724524
	LOSS [training: 0.5178889563549175 | validation: 0.2801778130931699]
	TIME [epoch: 8.89 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3701461293684413		[learning rate: 0.0072365]
		[batch 20/20] avg loss: 0.3564196551365975		[learning rate: 0.0072277]
	Learning Rate: 0.0072277
	LOSS [training: 0.3632828922525194 | validation: 0.47665373568882036]
	TIME [epoch: 8.9 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35489278992811896		[learning rate: 0.0072189]
		[batch 20/20] avg loss: 0.30823872573007427		[learning rate: 0.0072102]
	Learning Rate: 0.0072102
	LOSS [training: 0.3315657578290965 | validation: 0.27639488900823783]
	TIME [epoch: 8.85 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3234194186228129		[learning rate: 0.0072015]
		[batch 20/20] avg loss: 0.36177194521119		[learning rate: 0.0071927]
	Learning Rate: 0.00719275
	LOSS [training: 0.34259568191700146 | validation: 0.3114008432170022]
	TIME [epoch: 8.86 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27519106881530453		[learning rate: 0.007184]
		[batch 20/20] avg loss: 0.46684188352411315		[learning rate: 0.0071753]
	Learning Rate: 0.00717533
	LOSS [training: 0.3710164761697089 | validation: 0.47794774444181065]
	TIME [epoch: 8.85 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.336713573860917		[learning rate: 0.0071666]
		[batch 20/20] avg loss: 0.33320583668049847		[learning rate: 0.007158]
	Learning Rate: 0.00715796
	LOSS [training: 0.33495970527070773 | validation: 0.30717538353019935]
	TIME [epoch: 8.85 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27325767348586677		[learning rate: 0.0071493]
		[batch 20/20] avg loss: 0.29609373187164695		[learning rate: 0.0071406]
	Learning Rate: 0.00714064
	LOSS [training: 0.2846757026787569 | validation: 0.22979338956156134]
	TIME [epoch: 8.85 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3257800388015936		[learning rate: 0.007132]
		[batch 20/20] avg loss: 0.3490340276965143		[learning rate: 0.0071233]
	Learning Rate: 0.00712335
	LOSS [training: 0.337407033249054 | validation: 0.36221082707235575]
	TIME [epoch: 8.84 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40253787945538794		[learning rate: 0.0071147]
		[batch 20/20] avg loss: 0.32575048032706116		[learning rate: 0.0071061]
	Learning Rate: 0.0071061
	LOSS [training: 0.36414417989122455 | validation: 0.34227755950003247]
	TIME [epoch: 8.88 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33504391541064327		[learning rate: 0.0070975]
		[batch 20/20] avg loss: 0.3561731156738216		[learning rate: 0.0070889]
	Learning Rate: 0.0070889
	LOSS [training: 0.3456085155422324 | validation: 0.15974235135782386]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study1/model_tr_study1_r2_20240219_183142/states/model_tr_study1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2388600325048285		[learning rate: 0.0070803]
		[batch 20/20] avg loss: 0.30721438334306483		[learning rate: 0.0070717]
	Learning Rate: 0.00707174
	LOSS [training: 0.2730372079239467 | validation: 0.17417865931694376]
	TIME [epoch: 8.86 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44804338214238265		[learning rate: 0.0070632]
		[batch 20/20] avg loss: 0.2960936739261497		[learning rate: 0.0070546]
	Learning Rate: 0.00705462
	LOSS [training: 0.3720685280342661 | validation: 0.22657141759541904]
	TIME [epoch: 8.85 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26999570501844294		[learning rate: 0.0070461]
		[batch 20/20] avg loss: 0.41897145393444174		[learning rate: 0.0070375]
	Learning Rate: 0.00703754
	LOSS [training: 0.34448357947644237 | validation: 0.26253161824851673]
	TIME [epoch: 8.85 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23959955423709026		[learning rate: 0.007029]
		[batch 20/20] avg loss: 0.29523449893824455		[learning rate: 0.0070205]
	Learning Rate: 0.00702051
	LOSS [training: 0.2674170265876675 | validation: 0.6469125774913966]
	TIME [epoch: 8.87 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3954912136770973		[learning rate: 0.007012]
		[batch 20/20] avg loss: 0.35865191776365063		[learning rate: 0.0070035]
	Learning Rate: 0.00700351
	LOSS [training: 0.3770715657203739 | validation: 0.34324848665628255]
	TIME [epoch: 8.83 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28432196171090585		[learning rate: 0.006995]
		[batch 20/20] avg loss: 0.29759045300892273		[learning rate: 0.0069866]
	Learning Rate: 0.00698656
	LOSS [training: 0.2909562073599143 | validation: 0.2656749005057096]
	TIME [epoch: 8.84 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2476185644980863		[learning rate: 0.0069781]
		[batch 20/20] avg loss: 0.32628388982437795		[learning rate: 0.0069696]
	Learning Rate: 0.00696964
	LOSS [training: 0.2869512271612321 | validation: 0.2528913115524517]
	TIME [epoch: 8.85 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3543574924144817		[learning rate: 0.0069612]
		[batch 20/20] avg loss: 0.4566351383896558		[learning rate: 0.0069528]
	Learning Rate: 0.00695277
	LOSS [training: 0.40549631540206876 | validation: 0.6881842665272799]
	TIME [epoch: 8.87 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34963433178444975		[learning rate: 0.0069443]
		[batch 20/20] avg loss: 0.2693642417450736		[learning rate: 0.0069359]
	Learning Rate: 0.00693594
	LOSS [training: 0.30949928676476174 | validation: 0.38996658021481745]
	TIME [epoch: 8.86 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3147408433855358		[learning rate: 0.0069275]
		[batch 20/20] avg loss: 0.33347348202820976		[learning rate: 0.0069191]
	Learning Rate: 0.00691915
	LOSS [training: 0.3241071627068728 | validation: 0.20783127865170117]
	TIME [epoch: 8.84 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3681294923003537		[learning rate: 0.0069108]
		[batch 20/20] avg loss: 0.2620948455316434		[learning rate: 0.0069024]
	Learning Rate: 0.0069024
	LOSS [training: 0.31511216891599847 | validation: 0.24641028705953355]
	TIME [epoch: 8.86 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.250106223593049		[learning rate: 0.006894]
		[batch 20/20] avg loss: 0.3189190019686026		[learning rate: 0.0068857]
	Learning Rate: 0.00688569
	LOSS [training: 0.28451261278082585 | validation: 0.4375207548392853]
	TIME [epoch: 8.85 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23947122352065892		[learning rate: 0.0068773]
		[batch 20/20] avg loss: 0.33121067891857964		[learning rate: 0.006869]
	Learning Rate: 0.00686902
	LOSS [training: 0.28534095121961933 | validation: 0.5851122165524801]
	TIME [epoch: 8.87 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3329093141902529		[learning rate: 0.0068607]
		[batch 20/20] avg loss: 0.29832601720119134		[learning rate: 0.0068524]
	Learning Rate: 0.00685239
	LOSS [training: 0.3156176656957221 | validation: 0.26826439110357436]
	TIME [epoch: 8.85 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2130744821971929		[learning rate: 0.0068441]
		[batch 20/20] avg loss: 0.30157002825186785		[learning rate: 0.0068358]
	Learning Rate: 0.0068358
	LOSS [training: 0.25732225522453045 | validation: 0.2045830351014973]
	TIME [epoch: 8.83 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24371476813481974		[learning rate: 0.0068275]
		[batch 20/20] avg loss: 0.2804727660291854		[learning rate: 0.0068193]
	Learning Rate: 0.00681925
	LOSS [training: 0.2620937670820026 | validation: 0.20896534222862262]
	TIME [epoch: 8.84 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.346680438167517		[learning rate: 0.006811]
		[batch 20/20] avg loss: 0.27317497879886776		[learning rate: 0.0068027]
	Learning Rate: 0.00680275
	LOSS [training: 0.3099277084831924 | validation: 0.3900812399058917]
	TIME [epoch: 8.84 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31732235244389495		[learning rate: 0.0067945]
		[batch 20/20] avg loss: 0.3027565328730561		[learning rate: 0.0067863]
	Learning Rate: 0.00678628
	LOSS [training: 0.3100394426584755 | validation: 0.45093448333037467]
	TIME [epoch: 8.88 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27378499149773255		[learning rate: 0.0067781]
		[batch 20/20] avg loss: 0.8130453319307194		[learning rate: 0.0067698]
	Learning Rate: 0.00676985
	LOSS [training: 0.5434151617142261 | validation: 1.0091989269107609]
	TIME [epoch: 8.85 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6069811291129736		[learning rate: 0.0067616]
		[batch 20/20] avg loss: 0.599824220857872		[learning rate: 0.0067535]
	Learning Rate: 0.00675346
	LOSS [training: 0.6034026749854229 | validation: 0.770309251822606]
	TIME [epoch: 8.85 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5682025773725591		[learning rate: 0.0067453]
		[batch 20/20] avg loss: 0.5873742354500404		[learning rate: 0.0067371]
	Learning Rate: 0.00673711
	LOSS [training: 0.5777884064112996 | validation: 1.3760219887264586]
	TIME [epoch: 8.84 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6028772882626444		[learning rate: 0.006729]
		[batch 20/20] avg loss: 0.793928490858544		[learning rate: 0.0067208]
	Learning Rate: 0.0067208
	LOSS [training: 0.698402889560594 | validation: 1.2155145395757987]
	TIME [epoch: 8.86 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5678128383949069		[learning rate: 0.0067127]
		[batch 20/20] avg loss: 0.5478166335791825		[learning rate: 0.0067045]
	Learning Rate: 0.00670453
	LOSS [training: 0.5578147359870448 | validation: 1.4959673214835096]
	TIME [epoch: 8.85 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6383713321942611		[learning rate: 0.0066964]
		[batch 20/20] avg loss: 0.47558247335778636		[learning rate: 0.0066883]
	Learning Rate: 0.0066883
	LOSS [training: 0.556976902776024 | validation: 0.6189576883142569]
	TIME [epoch: 8.85 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5004849649492922		[learning rate: 0.0066802]
		[batch 20/20] avg loss: 0.5230545637979651		[learning rate: 0.0066721]
	Learning Rate: 0.00667211
	LOSS [training: 0.5117697643736288 | validation: 0.4718950952575234]
	TIME [epoch: 8.84 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4960268449195537		[learning rate: 0.006664]
		[batch 20/20] avg loss: 1.8563529559929992		[learning rate: 0.006656]
	Learning Rate: 0.00665596
	LOSS [training: 1.1761899004562768 | validation: 3.1998938296964536]
	TIME [epoch: 8.85 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.644889263781248		[learning rate: 0.0066479]
		[batch 20/20] avg loss: 9.672530775254511		[learning rate: 0.0066398]
	Learning Rate: 0.00663984
	LOSS [training: 8.658710019517878 | validation: 9.060346374300016]
	TIME [epoch: 8.87 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.41487528893712		[learning rate: 0.0066318]
		[batch 20/20] avg loss: 7.52624757113931		[learning rate: 0.0066238]
	Learning Rate: 0.00662377
	LOSS [training: 7.970561430038215 | validation: 8.390276668212683]
	TIME [epoch: 8.86 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.724620841096598		[learning rate: 0.0066157]
		[batch 20/20] avg loss: 9.972136632688969		[learning rate: 0.0066077]
	Learning Rate: 0.00660774
	LOSS [training: 9.348378736892784 | validation: 9.760309909171578]
	TIME [epoch: 8.86 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.525118707784207		[learning rate: 0.0065997]
		[batch 20/20] avg loss: 7.808480669943846		[learning rate: 0.0065917]
	Learning Rate: 0.00659174
	LOSS [training: 8.166799688864028 | validation: 6.998959241627226]
	TIME [epoch: 8.85 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.653161580471365		[learning rate: 0.0065838]
		[batch 20/20] avg loss: 7.020983104556066		[learning rate: 0.0065758]
	Learning Rate: 0.00657578
	LOSS [training: 7.337072342513716 | validation: 6.431165117159656]
	TIME [epoch: 8.86 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.901982805645993		[learning rate: 0.0065678]
		[batch 20/20] avg loss: 5.417391788667291		[learning rate: 0.0065599]
	Learning Rate: 0.00655986
	LOSS [training: 5.659687297156642 | validation: 5.161174962394141]
	TIME [epoch: 8.88 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.86180711298118		[learning rate: 0.0065519]
		[batch 20/20] avg loss: 5.163422102314594		[learning rate: 0.006544]
	Learning Rate: 0.00654398
	LOSS [training: 5.012614607647887 | validation: 5.467331997422056]
	TIME [epoch: 8.86 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.115115987966586		[learning rate: 0.0065361]
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.05
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.025
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.0125
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.00625
