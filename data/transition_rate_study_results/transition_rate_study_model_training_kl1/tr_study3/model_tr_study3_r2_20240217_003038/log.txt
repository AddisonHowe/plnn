Args:
Namespace(name='model_tr_study3', outdir='out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2', training_data='data/transition_rate_studies/tr_study3/tr_study3_training/r2', validation_data='data/transition_rate_studies/tr_study3/tr_study3_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1051886793

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.690556479546817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.690556479546817 | validation: 13.02502456453788]
	TIME [epoch: 48.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.16428633821305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.16428633821305 | validation: 9.444768720535087]
	TIME [epoch: 9.07 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.861019222922307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.861019222922307 | validation: 8.167051234142273]
	TIME [epoch: 9.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.182284359265404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.182284359265404 | validation: 7.95645432376225]
	TIME [epoch: 9.07 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.097297558419713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.097297558419713 | validation: 9.351294738790237]
	TIME [epoch: 9.1 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.31151468889968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.31151468889968 | validation: 7.689167387460358]
	TIME [epoch: 9.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.342074212822061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.342074212822061 | validation: 7.17605981072561]
	TIME [epoch: 9.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.763310170717925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.763310170717925 | validation: 6.9157549007916455]
	TIME [epoch: 9.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.411024068358733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.411024068358733 | validation: 5.989990863273079]
	TIME [epoch: 9.09 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.1886990239177875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1886990239177875 | validation: 6.052110429061036]
	TIME [epoch: 9.13 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.530534469570395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.530534469570395 | validation: 5.912038621111889]
	TIME [epoch: 9.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.850586957350346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.850586957350346 | validation: 5.506560860443981]
	TIME [epoch: 9.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.686171848480903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.686171848480903 | validation: 5.092609300149558]
	TIME [epoch: 9.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.928579585793202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.928579585793202 | validation: 6.977363505982922]
	TIME [epoch: 9.11 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.974923205853433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.974923205853433 | validation: 5.200690172509228]
	TIME [epoch: 9.1 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.178839882773694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.178839882773694 | validation: 5.455934815480776]
	TIME [epoch: 9.09 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.1775914620330585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1775914620330585 | validation: 5.05799341033577]
	TIME [epoch: 9.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.10300432147177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.10300432147177 | validation: 8.544883550725272]
	TIME [epoch: 9.08 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.287055003317481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.287055003317481 | validation: 6.022079970665146]
	TIME [epoch: 9.07 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.785406430002976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.785406430002976 | validation: 7.796823409364766]
	TIME [epoch: 9.11 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.448223485185248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.448223485185248 | validation: 7.221014296213278]
	TIME [epoch: 9.09 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.861668903824836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.861668903824836 | validation: 6.547426334342821]
	TIME [epoch: 9.08 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.250556106849383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.250556106849383 | validation: 6.037443827627313]
	TIME [epoch: 9.1 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.068471369962652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.068471369962652 | validation: 5.843169803501489]
	TIME [epoch: 9.09 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.5776072901036295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5776072901036295 | validation: 5.005644285822745]
	TIME [epoch: 9.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.315870769322673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.315870769322673 | validation: 5.039320355565615]
	TIME [epoch: 9.09 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.943290872119833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.943290872119833 | validation: 4.666478189323855]
	TIME [epoch: 9.09 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.331512469375294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.331512469375294 | validation: 8.222987922709697]
	TIME [epoch: 9.11 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.442204797474344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.442204797474344 | validation: 4.617832724112026]
	TIME [epoch: 9.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.594442288894251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.594442288894251 | validation: 4.622644897988392]
	TIME [epoch: 9.22 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.685734104148497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.685734104148497 | validation: 4.164916541446932]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.137004457340458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.137004457340458 | validation: 3.752881247540624]
	TIME [epoch: 9.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5404069016358486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5404069016358486 | validation: 3.267972211101643]
	TIME [epoch: 9.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.237535862912336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.237535862912336 | validation: 2.8098458957833015]
	TIME [epoch: 9.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.62709524078504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.62709524078504 | validation: 2.6359450109108566]
	TIME [epoch: 9.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7752730373084282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7752730373084282 | validation: 2.4270159570672583]
	TIME [epoch: 9.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.551152711530075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.551152711530075 | validation: 2.374625896480475]
	TIME [epoch: 9.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3249613724788913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3249613724788913 | validation: 2.774862508254328]
	TIME [epoch: 9.12 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8139976207947335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8139976207947335 | validation: 1.9260388623990998]
	TIME [epoch: 9.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0159407504069575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0159407504069575 | validation: 2.4213191974676156]
	TIME [epoch: 9.14 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4961372817546303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4961372817546303 | validation: 2.4360964445289026]
	TIME [epoch: 9.1 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9768077309961476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9768077309961476 | validation: 1.9141325156793378]
	TIME [epoch: 9.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.075610951957895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.075610951957895 | validation: 2.202174690223723]
	TIME [epoch: 9.13 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9372793096287952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9372793096287952 | validation: 1.7754462427900295]
	TIME [epoch: 9.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8179352015014998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8179352015014998 | validation: 1.6750123638534322]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0720968427613684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0720968427613684 | validation: 2.4887162944389725]
	TIME [epoch: 9.14 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1102033148873987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1102033148873987 | validation: 2.3299627025939844]
	TIME [epoch: 9.12 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9517627117746918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9517627117746918 | validation: 1.6023988729961784]
	TIME [epoch: 9.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7197406757280091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7197406757280091 | validation: 1.9106696135033847]
	TIME [epoch: 9.13 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9829616506714713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9829616506714713 | validation: 1.8802209038488173]
	TIME [epoch: 9.14 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8552617743515216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8552617743515216 | validation: 1.2278089470880427]
	TIME [epoch: 9.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.829638397619722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.829638397619722 | validation: 1.9535163044558848]
	TIME [epoch: 9.1 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0142404142637704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0142404142637704 | validation: 1.465605570447873]
	TIME [epoch: 9.14 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7394434280079274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7394434280079274 | validation: 1.1810417337768802]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6133350310823524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6133350310823524 | validation: 1.7172133377061884]
	TIME [epoch: 9.13 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.334288468257133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.334288468257133 | validation: 3.670289524686395]
	TIME [epoch: 9.12 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.300654144443719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.300654144443719 | validation: 2.0461735758516975]
	TIME [epoch: 9.1 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0766925974839645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0766925974839645 | validation: 2.0289395881359393]
	TIME [epoch: 9.14 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.03018309071335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.03018309071335 | validation: 1.8233913379742672]
	TIME [epoch: 9.16 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9175059491309434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9175059491309434 | validation: 1.507070540259296]
	TIME [epoch: 9.14 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9503707629629772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9503707629629772 | validation: 1.5357905582308824]
	TIME [epoch: 9.11 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.831891341742006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.831891341742006 | validation: 2.3371805634218754]
	TIME [epoch: 9.14 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8524495406405965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8524495406405965 | validation: 1.9121624877458792]
	TIME [epoch: 9.1 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5852925643056026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5852925643056026 | validation: 1.1286308964665355]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.616242829340888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.616242829340888 | validation: 1.4050894617521241]
	TIME [epoch: 9.11 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.61860571072445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.61860571072445 | validation: 1.927517242191346]
	TIME [epoch: 9.11 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0179635725093275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0179635725093275 | validation: 2.143785547864873]
	TIME [epoch: 9.14 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.006228414995244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.006228414995244 | validation: 1.4529660542544107]
	TIME [epoch: 9.15 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5808788075433262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5808788075433262 | validation: 1.188723890454819]
	TIME [epoch: 9.19 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.439161363863389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.439161363863389 | validation: 1.4294807693024612]
	TIME [epoch: 9.19 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7562213067827357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7562213067827357 | validation: 1.2086437580057219]
	TIME [epoch: 9.18 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8404981459766756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8404981459766756 | validation: 2.2154100472341405]
	TIME [epoch: 9.18 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0317627223945904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0317627223945904 | validation: 1.7747598771635438]
	TIME [epoch: 9.18 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3735678714596662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3735678714596662 | validation: 1.1913922760889006]
	TIME [epoch: 9.18 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5335220078380458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5335220078380458 | validation: 1.6695945954358775]
	TIME [epoch: 9.17 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5764211736007883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5764211736007883 | validation: 1.9252712774653147]
	TIME [epoch: 9.17 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.128494482359996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.128494482359996 | validation: 1.5227228258849688]
	TIME [epoch: 9.16 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7747863521874467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7747863521874467 | validation: 1.0831005116494157]
	TIME [epoch: 9.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2531359932875443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2531359932875443 | validation: 0.9671283158530852]
	TIME [epoch: 9.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3628717430288528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3628717430288528 | validation: 1.5767419506576665]
	TIME [epoch: 9.11 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6576288358365499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6576288358365499 | validation: 1.3132544029017066]
	TIME [epoch: 9.11 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.594781003245275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.594781003245275 | validation: 1.800052884626131]
	TIME [epoch: 9.09 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0603287129172196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0603287129172196 | validation: 1.7221608006483815]
	TIME [epoch: 9.1 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8392567430020705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8392567430020705 | validation: 1.7551654612227732]
	TIME [epoch: 9.12 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7958624600435833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7958624600435833 | validation: 1.6337602653028305]
	TIME [epoch: 9.1 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7147334265230783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7147334265230783 | validation: 1.2985609018666884]
	TIME [epoch: 9.11 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.357559657842922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.357559657842922 | validation: 1.0524305930565068]
	TIME [epoch: 9.11 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6688546118722833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6688546118722833 | validation: 1.174238674088601]
	TIME [epoch: 9.1 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4297501768399479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4297501768399479 | validation: 1.0020865247174275]
	TIME [epoch: 9.11 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4303772018582195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4303772018582195 | validation: 1.1920408580437694]
	TIME [epoch: 9.1 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.494704254366279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.494704254366279 | validation: 1.267883168933563]
	TIME [epoch: 9.1 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.604253836558038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.604253836558038 | validation: 1.2348707696023813]
	TIME [epoch: 9.11 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.155869482023943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.155869482023943 | validation: 1.1381332132852573]
	TIME [epoch: 9.11 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.586254479336443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.586254479336443 | validation: 1.1360059487535814]
	TIME [epoch: 9.13 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3459159907767813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3459159907767813 | validation: 1.0214603029496434]
	TIME [epoch: 9.11 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.247349394832815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.247349394832815 | validation: 0.8350582390815574]
	TIME [epoch: 9.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3728557784892075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3728557784892075 | validation: 2.0042756620693005]
	TIME [epoch: 9.1 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.350795359646186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.350795359646186 | validation: 1.513488301606825]
	TIME [epoch: 9.12 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1485941914784594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1485941914784594 | validation: 0.8821659117898937]
	TIME [epoch: 9.13 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0542833255029755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0542833255029755 | validation: 0.8049035420960811]
	TIME [epoch: 9.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.925927594717065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.925927594717065 | validation: 1.1836840318353135]
	TIME [epoch: 9.12 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2358030264332007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2358030264332007 | validation: 0.811747165350422]
	TIME [epoch: 9.14 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7130893476819935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7130893476819935 | validation: 1.0115932542071067]
	TIME [epoch: 9.13 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4628508882554483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4628508882554483 | validation: 1.3285978723357714]
	TIME [epoch: 9.13 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.524237905342745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.524237905342745 | validation: 1.7698876075719339]
	TIME [epoch: 9.12 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3279556776157786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3279556776157786 | validation: 1.11840220736925]
	TIME [epoch: 9.11 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0582918377238288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0582918377238288 | validation: 0.790156626383764]
	TIME [epoch: 9.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4974098852103692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4974098852103692 | validation: 2.0013124581596147]
	TIME [epoch: 9.09 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7386883132995443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7386883132995443 | validation: 1.7509381977988134]
	TIME [epoch: 9.12 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.835000371867626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.835000371867626 | validation: 1.6786871299132935]
	TIME [epoch: 9.1 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7329797305200354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7329797305200354 | validation: 1.4831126592769133]
	TIME [epoch: 9.09 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4827193574809128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4827193574809128 | validation: 1.6946240547185447]
	TIME [epoch: 9.1 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7403554398871386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7403554398871386 | validation: 1.41177103935479]
	TIME [epoch: 9.1 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1979920942703726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1979920942703726 | validation: 1.1762215045705218]
	TIME [epoch: 9.12 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4090333332547762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4090333332547762 | validation: 1.2360815942595837]
	TIME [epoch: 9.11 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4079472372598034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4079472372598034 | validation: 1.2118994073868596]
	TIME [epoch: 9.11 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.001635234263648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.001635234263648 | validation: 0.5493862353195325]
	TIME [epoch: 9.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2039755737413587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2039755737413587 | validation: 2.1332519208348506]
	TIME [epoch: 9.1 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8440237344727735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8440237344727735 | validation: 1.643142091639474]
	TIME [epoch: 9.15 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.760044275171466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.760044275171466 | validation: 1.6419785292572713]
	TIME [epoch: 9.11 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8316806581926712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8316806581926712 | validation: 1.6957818026545282]
	TIME [epoch: 9.12 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.629827136020306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.629827136020306 | validation: 1.6969104655065514]
	TIME [epoch: 9.11 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7764376898519536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7764376898519536 | validation: 1.4309046798707783]
	TIME [epoch: 9.12 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5522078791154614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5522078791154614 | validation: 1.7097327081613298]
	TIME [epoch: 9.13 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4566816293804972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4566816293804972 | validation: 1.6569528232705362]
	TIME [epoch: 9.12 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.534167482630392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.534167482630392 | validation: 1.1977589928783048]
	TIME [epoch: 9.11 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.285141150984203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.285141150984203 | validation: 1.3080916191377905]
	TIME [epoch: 9.13 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3622328092355713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3622328092355713 | validation: 1.1291636561236538]
	TIME [epoch: 9.13 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3383998560486077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3383998560486077 | validation: 0.9114311696924172]
	TIME [epoch: 9.14 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.231749923059805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.231749923059805 | validation: 0.8446211641794311]
	TIME [epoch: 9.13 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9080157445516838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9080157445516838 | validation: 0.5225745710228967]
	TIME [epoch: 9.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9514772263461844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9514772263461844 | validation: 1.9475982937509793]
	TIME [epoch: 9.1 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1795918400730123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1795918400730123 | validation: 0.8869860762844002]
	TIME [epoch: 9.1 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9482606756450673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9482606756450673 | validation: 0.6838889279607987]
	TIME [epoch: 9.13 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9727768993916026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9727768993916026 | validation: 1.1838178356519071]
	TIME [epoch: 9.1 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9948668054231795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9948668054231795 | validation: 0.6459002675028678]
	TIME [epoch: 9.11 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3205108697294479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3205108697294479 | validation: 0.8764083275049075]
	TIME [epoch: 9.12 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3673145839107486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3673145839107486 | validation: 0.5955433801701271]
	TIME [epoch: 9.15 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1641459914581227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1641459914581227 | validation: 1.585299673181817]
	TIME [epoch: 9.16 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1266285205887934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1266285205887934 | validation: 0.8388564945468906]
	TIME [epoch: 9.15 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3433989199196452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3433989199196452 | validation: 1.1355487729584846]
	TIME [epoch: 9.14 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0109062846070138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0109062846070138 | validation: 1.0789585617906932]
	TIME [epoch: 9.13 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0541038840380073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0541038840380073 | validation: 1.0384052063938611]
	TIME [epoch: 9.13 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0934220232608705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0934220232608705 | validation: 0.97184071330221]
	TIME [epoch: 9.14 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4092288694697999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4092288694697999 | validation: 0.7968015377883502]
	TIME [epoch: 9.14 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8379687523078377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8379687523078377 | validation: 1.0396716699184134]
	TIME [epoch: 9.14 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0147882491134772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0147882491134772 | validation: 0.8091356081018466]
	TIME [epoch: 9.14 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9237612358858144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9237612358858144 | validation: 0.9347821753008547]
	TIME [epoch: 9.12 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0418631299455048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0418631299455048 | validation: 1.0515008558152004]
	TIME [epoch: 9.14 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1229775135277174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1229775135277174 | validation: 1.1756287567945476]
	TIME [epoch: 9.13 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0814645912503544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0814645912503544 | validation: 0.8143012559049143]
	TIME [epoch: 9.12 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1870985475687283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1870985475687283 | validation: 1.2732035844886063]
	TIME [epoch: 9.13 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2944846088979707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2944846088979707 | validation: 1.1271063508485608]
	TIME [epoch: 9.13 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1260983210890498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1260983210890498 | validation: 0.9153287063569586]
	TIME [epoch: 9.15 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0564298876448264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0564298876448264 | validation: 0.9695080849976467]
	TIME [epoch: 9.12 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0251481551312742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0251481551312742 | validation: 0.607312517175197]
	TIME [epoch: 9.12 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.01653683280285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.01653683280285 | validation: 0.7627762839692331]
	TIME [epoch: 9.14 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9664079620556609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9664079620556609 | validation: 1.0990623709908576]
	TIME [epoch: 9.14 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3269910273004952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3269910273004952 | validation: 1.1884496965959208]
	TIME [epoch: 9.17 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3719604375422898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3719604375422898 | validation: 1.3165123764696285]
	TIME [epoch: 9.13 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2744910779291305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2744910779291305 | validation: 1.2970749474101118]
	TIME [epoch: 9.13 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1500699654195308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1500699654195308 | validation: 1.759968136068213]
	TIME [epoch: 9.14 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6029377264851319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6029377264851319 | validation: 1.3580415716343892]
	TIME [epoch: 9.14 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2648691001641386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2648691001641386 | validation: 1.1496936822559025]
	TIME [epoch: 9.15 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2777724066250284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2777724066250284 | validation: 1.020106762991156]
	TIME [epoch: 9.14 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2811775599038082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2811775599038082 | validation: 0.982442585405353]
	TIME [epoch: 9.16 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1390764435040688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1390764435040688 | validation: 1.5392424697961773]
	TIME [epoch: 9.14 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2937957007902128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2937957007902128 | validation: 1.3676036447038995]
	TIME [epoch: 9.16 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1260222326818226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1260222326818226 | validation: 1.1800804397716997]
	TIME [epoch: 9.16 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1344820056483171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1344820056483171 | validation: 0.8218518254700302]
	TIME [epoch: 9.14 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8078061499801421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8078061499801421 | validation: 1.3092441279082516]
	TIME [epoch: 9.15 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0380483079830918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0380483079830918 | validation: 0.6064736577564671]
	TIME [epoch: 9.14 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1279187779545283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1279187779545283 | validation: 0.6732327963696523]
	TIME [epoch: 9.14 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0726109475483543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0726109475483543 | validation: 0.8224847621085832]
	TIME [epoch: 9.15 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9899983072651386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9899983072651386 | validation: 0.7131847862286647]
	TIME [epoch: 9.14 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8201467025763705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8201467025763705 | validation: 0.7684932306425748]
	TIME [epoch: 9.14 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8502557892232371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8502557892232371 | validation: 0.8207279045838841]
	TIME [epoch: 9.14 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0816553231452448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0816553231452448 | validation: 0.7386728112895835]
	TIME [epoch: 9.16 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9391336921793452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9391336921793452 | validation: 1.2578682424998295]
	TIME [epoch: 9.15 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0876907471739448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0876907471739448 | validation: 1.150379843121611]
	TIME [epoch: 9.12 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8873883025130876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8873883025130876 | validation: 0.41489337679456584]
	TIME [epoch: 9.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r2_20240217_003038/states/model_tr_study3_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7913489042364101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7913489042364101 | validation: 0.8246481442151016]
	TIME [epoch: 9.14 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.811844084628258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.811844084628258 | validation: 1.1120843306132762]
	TIME [epoch: 9.16 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8720960808281573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8720960808281573 | validation: 0.7554965269312012]
	TIME [epoch: 9.13 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8158178369614403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8158178369614403 | validation: 0.4525993244534222]
	TIME [epoch: 9.12 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9567326627343625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9567326627343625 | validation: 0.8455117170860365]
	TIME [epoch: 9.12 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9886294576777699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9886294576777699 | validation: 0.8605532105056347]
	TIME [epoch: 9.11 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8736115372390298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8736115372390298 | validation: 0.7059242628592881]
	TIME [epoch: 9.14 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0028831182740827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0028831182740827 | validation: 0.9987543432711925]
	TIME [epoch: 9.12 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0764536251684684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0764536251684684 | validation: 0.6751810627546566]
	TIME [epoch: 9.13 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.40816772773762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.40816772773762 | validation: 1.0699250323940883]
	TIME [epoch: 9.12 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.136714263087773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.136714263087773 | validation: 0.6496895356118186]
	TIME [epoch: 9.13 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2173564264201464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2173564264201464 | validation: 0.557905327449483]
	TIME [epoch: 9.15 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2464742425036264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2464742425036264 | validation: 0.8643382042152248]
	TIME [epoch: 9.15 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9350526821382589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9350526821382589 | validation: 0.5756197588669396]
	TIME [epoch: 9.14 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7875296997996234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7875296997996234 | validation: 0.5924242848352252]
	TIME [epoch: 9.11 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.039490531944972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.039490531944972 | validation: 1.0310321950246868]
	TIME [epoch: 9.12 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2356583366506866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2356583366506866 | validation: 1.3341377500623168]
	TIME [epoch: 9.14 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0574920336901605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0574920336901605 | validation: 0.5265470526996585]
	TIME [epoch: 9.14 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0970039184827374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0970039184827374 | validation: 0.6987420139252081]
	TIME [epoch: 9.12 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.006571862315681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.006571862315681 | validation: 0.794270833955848]
	TIME [epoch: 9.13 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.811332843149789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.811332843149789 | validation: 0.4420613130722937]
	TIME [epoch: 9.13 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7671246523950399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7671246523950399 | validation: 0.435270400944497]
	TIME [epoch: 9.17 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9124283526087916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9124283526087916 | validation: 0.6208466479531922]
	TIME [epoch: 9.13 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.026364271457767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.026364271457767 | validation: 0.687804189490675]
	TIME [epoch: 9.14 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8716926197053292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8716926197053292 | validation: 0.6563442928980865]
	TIME [epoch: 9.14 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7042962706878125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7042962706878125 | validation: 0.9210885511759708]
	TIME [epoch: 9.12 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.822335480560078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.822335480560078 | validation: 0.7991415149070131]
	TIME [epoch: 9.15 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1138476768711505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1138476768711505 | validation: 0.7600971847159731]
	TIME [epoch: 9.13 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0204073752320402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0204073752320402 | validation: 0.5601659363870066]
	TIME [epoch: 9.14 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9375606402723393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9375606402723393 | validation: 0.7830091244946774]
	TIME [epoch: 9.13 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0090117650005244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0090117650005244 | validation: 0.7824384685920887]
	TIME [epoch: 9.15 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9718074910420993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9718074910420993 | validation: 0.8589117828734236]
	TIME [epoch: 9.16 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9224908145024692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9224908145024692 | validation: 1.0216219636705515]
	TIME [epoch: 9.15 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9604239097786437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9604239097786437 | validation: 2.1296791323111344]
	TIME [epoch: 9.13 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0523871095122268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0523871095122268 | validation: 0.73333617342324]
	TIME [epoch: 9.14 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.156441800891184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.156441800891184 | validation: 0.6854032564501635]
	TIME [epoch: 9.13 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5246121665756238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5246121665756238 | validation: 1.8330238336033693]
	TIME [epoch: 9.16 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2764667515967776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2764667515967776 | validation: 1.2783497387181768]
	TIME [epoch: 9.15 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3439142911547464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3439142911547464 | validation: 1.0182003065950858]
	TIME [epoch: 9.16 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0360548192075405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0360548192075405 | validation: 0.8134654387956888]
	TIME [epoch: 9.15 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7029463945456054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7029463945456054 | validation: 0.862473247407983]
	TIME [epoch: 9.16 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8888991201082499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8888991201082499 | validation: 1.0495776301449045]
	TIME [epoch: 9.18 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0308849136672733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0308849136672733 | validation: 1.5094640895844087]
	TIME [epoch: 9.14 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6065208666333746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6065208666333746 | validation: 1.6087210624120234]
	TIME [epoch: 9.17 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0025560313103845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0025560313103845 | validation: 1.165319959156835]
	TIME [epoch: 9.08 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0933643437522815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0933643437522815 | validation: 1.1653801685626153]
	TIME [epoch: 9.08 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7654719774307981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7654719774307981 | validation: 2.004656208648994]
	TIME [epoch: 9.11 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.149191167116627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.149191167116627 | validation: 1.5197966449239018]
	TIME [epoch: 9.11 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2112516345878208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2112516345878208 | validation: 1.4832369136925134]
	TIME [epoch: 9.13 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4819075168754976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4819075168754976 | validation: 0.6801895819652455]
	TIME [epoch: 9.1 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2757090826135922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2757090826135922 | validation: 0.981479371621389]
	TIME [epoch: 9.11 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1281095069093965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1281095069093965 | validation: 1.6527944658275282]
	TIME [epoch: 9.16 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0579780947661008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0579780947661008 | validation: 0.8148488828846795]
	TIME [epoch: 9.15 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9529688860043171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9529688860043171 | validation: 1.0277889591526759]
	TIME [epoch: 9.14 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5103836501695032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5103836501695032 | validation: 1.1403083017988587]
	TIME [epoch: 9.17 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.267399361540629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.267399361540629 | validation: 1.0292270992831063]
	TIME [epoch: 9.15 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1051436957372238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1051436957372238 | validation: 1.6679634026444048]
	TIME [epoch: 9.18 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9698550528803729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9698550528803729 | validation: 0.7815955800182529]
	TIME [epoch: 9.15 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4163152847599054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4163152847599054 | validation: 1.0151524841831574]
	TIME [epoch: 9.14 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0842674690920397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0842674690920397 | validation: 1.317639066556667]
	TIME [epoch: 9.15 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1642159685165954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1642159685165954 | validation: 0.8754250676431414]
	TIME [epoch: 9.14 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9837056949069914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9837056949069914 | validation: 0.9126721351453528]
	TIME [epoch: 9.18 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2359625936297143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2359625936297143 | validation: 1.0462145653364237]
	TIME [epoch: 9.16 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0014208622012302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0014208622012302 | validation: 0.5148746952415582]
	TIME [epoch: 9.16 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9534588468815395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9534588468815395 | validation: 0.813552214921972]
	TIME [epoch: 9.15 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0731074755007008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0731074755007008 | validation: 1.9207266504660183]
	TIME [epoch: 9.14 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2316875746060005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2316875746060005 | validation: 0.845495794965253]
	TIME [epoch: 9.19 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9880121071554069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9880121071554069 | validation: 1.0955998664221873]
	TIME [epoch: 9.18 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8805838451011064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8805838451011064 | validation: 1.0411413484412868]
	TIME [epoch: 9.16 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1620233737532462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1620233737532462 | validation: 1.0420051840493811]
	TIME [epoch: 9.15 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6687694433653213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6687694433653213 | validation: 0.8188421603626402]
	TIME [epoch: 9.15 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3734561409797919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3734561409797919 | validation: 0.9775801756082366]
	TIME [epoch: 9.18 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3134188835904663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3134188835904663 | validation: 0.7858170311243755]
	TIME [epoch: 9.17 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.196441465705737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.196441465705737 | validation: 1.1130534917703434]
	TIME [epoch: 9.15 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1767570301576107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1767570301576107 | validation: 0.9823405601864355]
	TIME [epoch: 9.17 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5126701140611964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5126701140611964 | validation: 1.0228309251614824]
	TIME [epoch: 9.16 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2077225191600618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2077225191600618 | validation: 1.082968418493449]
	TIME [epoch: 9.19 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3471243762247516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3471243762247516 | validation: 0.9450544546981834]
	TIME [epoch: 9.16 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1566787553040259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1566787553040259 | validation: 1.2591304744227647]
	TIME [epoch: 9.17 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.130249541808158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.130249541808158 | validation: 0.9574112775571197]
	TIME [epoch: 9.15 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5378012794385723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5378012794385723 | validation: 1.3182389583993324]
	TIME [epoch: 9.18 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.172001262464974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.172001262464974 | validation: 0.8149137001047242]
	TIME [epoch: 9.19 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1056765548828806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1056765548828806 | validation: 1.4313594016956768]
	TIME [epoch: 9.16 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5389928153192862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5389928153192862 | validation: 0.6283176231841348]
	TIME [epoch: 9.15 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0762503309835538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0762503309835538 | validation: 0.7787520232098482]
	TIME [epoch: 9.16 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4032748192316835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4032748192316835 | validation: 1.0714184567839167]
	TIME [epoch: 9.17 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2601524113604057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2601524113604057 | validation: 1.0035717319392805]
	TIME [epoch: 9.18 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3177812971318033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3177812971318033 | validation: 1.9765920941273518]
	TIME [epoch: 9.16 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3652993399089075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3652993399089075 | validation: 5.4762091720055]
	TIME [epoch: 9.15 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7953656411944423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7953656411944423 | validation: 1.1145742181963174]
	TIME [epoch: 9.15 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1702685763409078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1702685763409078 | validation: 0.9582698224408259]
	TIME [epoch: 9.15 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9159683608220736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9159683608220736 | validation: 0.8995527048520389]
	TIME [epoch: 9.17 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0023676319934731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0023676319934731 | validation: 1.0993654847289756]
	TIME [epoch: 9.16 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.612249727573441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.612249727573441 | validation: 1.247759508928605]
	TIME [epoch: 9.14 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1371642654004384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1371642654004384 | validation: 1.1822426179063563]
	TIME [epoch: 9.13 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1865915055920806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1865915055920806 | validation: 0.918432737091208]
	TIME [epoch: 9.13 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3904881489977898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3904881489977898 | validation: 0.9393743020961494]
	TIME [epoch: 9.15 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2296223981202634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2296223981202634 | validation: 0.9949929606421877]
	TIME [epoch: 9.14 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0018534361218743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0018534361218743 | validation: 1.2560509172454712]
	TIME [epoch: 9.15 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0863297503208187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0863297503208187 | validation: 0.9224670280400653]
	TIME [epoch: 9.13 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4095686898278403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4095686898278403 | validation: 1.9387586663910854]
	TIME [epoch: 9.17 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4391015291267106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4391015291267106 | validation: 1.0156000937373955]
	TIME [epoch: 9.16 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1373242094799827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1373242094799827 | validation: 1.1310092471127695]
	TIME [epoch: 9.15 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4055827492216368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4055827492216368 | validation: 2.4260734203253262]
	TIME [epoch: 9.15 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4432892922237888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4432892922237888 | validation: 0.9686763825794706]
	TIME [epoch: 9.14 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3197717285571136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3197717285571136 | validation: 1.437752717032313]
	TIME [epoch: 9.16 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6692395309312502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6692395309312502 | validation: 0.9807453447876782]
	TIME [epoch: 9.18 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7933248257307652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7933248257307652 | validation: 1.2347259502836776]
	TIME [epoch: 9.13 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.701403978571199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.701403978571199 | validation: 0.7117464532864882]
	TIME [epoch: 9.16 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5150876923123608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5150876923123608 | validation: 0.9480269502826759]
	TIME [epoch: 9.14 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.22170218522204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.22170218522204 | validation: 0.725701756910483]
	TIME [epoch: 9.14 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9690815528749266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9690815528749266 | validation: 0.8011800395893715]
	TIME [epoch: 9.14 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7010746219392563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7010746219392563 | validation: 6.191862268630726]
	TIME [epoch: 9.14 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6672896068230827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6672896068230827 | validation: 1.4946288866352329]
	TIME [epoch: 9.16 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1501533639558985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1501533639558985 | validation: 0.63650543600355]
	TIME [epoch: 9.18 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.266678449120853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.266678449120853 | validation: 2.4685857069386135]
	TIME [epoch: 9.16 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.351049269789297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.351049269789297 | validation: 0.7295538751915736]
	TIME [epoch: 9.18 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0553499593778137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0553499593778137 | validation: 0.9976190706535322]
	TIME [epoch: 9.14 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0240583561453165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0240583561453165 | validation: 1.3391456274136853]
	TIME [epoch: 9.16 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.123583251277149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.123583251277149 | validation: 0.9268063027856557]
	TIME [epoch: 9.31 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1433434403802254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1433434403802254 | validation: 0.6721406522780244]
	TIME [epoch: 9.18 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5693454474538555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5693454474538555 | validation: 1.0222174195779887]
	TIME [epoch: 9.19 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9420494351957638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9420494351957638 | validation: 0.9816873865525363]
	TIME [epoch: 9.17 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1316008413120304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1316008413120304 | validation: 0.8899458971595934]
	TIME [epoch: 9.16 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2520964662519347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2520964662519347 | validation: 0.8660490624747113]
	TIME [epoch: 9.16 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1654441368506143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1654441368506143 | validation: 2.203739049292482]
	TIME [epoch: 9.16 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3974372959785621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3974372959785621 | validation: 1.2715328368716148]
	TIME [epoch: 9.15 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.266328246277531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.266328246277531 | validation: 0.9675345367168477]
	TIME [epoch: 9.15 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1057542790790964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1057542790790964 | validation: 0.9014370383435752]
	TIME [epoch: 9.17 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2609326425747966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2609326425747966 | validation: 0.7446401745209243]
	TIME [epoch: 9.16 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.104753646832569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.104753646832569 | validation: 0.704347367498251]
	TIME [epoch: 9.17 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.378883415132219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.378883415132219 | validation: 2.089065694848985]
	TIME [epoch: 9.16 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2010886238106675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2010886238106675 | validation: 0.8642166881531379]
	TIME [epoch: 9.14 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0913995081457464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0913995081457464 | validation: 1.8446647171034178]
	TIME [epoch: 9.17 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2502397336321667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2502397336321667 | validation: 0.7013099796158233]
	TIME [epoch: 9.15 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.770250445611561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.770250445611561 | validation: 6.265354671497221]
	TIME [epoch: 9.16 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.237005240751746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.237005240751746 | validation: 6.205757634435363]
	TIME [epoch: 9.15 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.09167105756479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.09167105756479 | validation: 6.303573107292062]
	TIME [epoch: 9.14 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.112864327026892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.112864327026892 | validation: 6.987247227806428]
	TIME [epoch: 9.14 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.255300817824649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.255300817824649 | validation: 6.554745499037031]
	TIME [epoch: 9.14 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.177395127697339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.177395127697339 | validation: 6.246249623268551]
	TIME [epoch: 9.16 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.277682505096815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.277682505096815 | validation: 2.2251656159304996]
	TIME [epoch: 9.16 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.447251384430989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.447251384430989 | validation: 1.1115540480501482]
	TIME [epoch: 9.12 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1003747888862825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1003747888862825 | validation: 0.8323837540824028]
	TIME [epoch: 9.12 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9886017396471871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9886017396471871 | validation: 2.4755800255146285]
	TIME [epoch: 9.13 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.955874513029856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.955874513029856 | validation: 1.6333879678217003]
	TIME [epoch: 9.15 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.390858919811439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.390858919811439 | validation: 1.3046465754348902]
	TIME [epoch: 9.14 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2630132571987864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2630132571987864 | validation: 1.450042313276077]
	TIME [epoch: 9.15 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2517882435790884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2517882435790884 | validation: 0.9154305466275655]
	TIME [epoch: 9.14 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.569171190949897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.569171190949897 | validation: 1.0606851861944782]
	TIME [epoch: 9.13 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2872326588041285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2872326588041285 | validation: 1.1168021722685375]
	TIME [epoch: 9.15 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0852233712554966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0852233712554966 | validation: 1.3471809652274325]
	TIME [epoch: 9.15 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.152968120517475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.152968120517475 | validation: 1.1095871035068512]
	TIME [epoch: 9.16 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6609050487658858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6609050487658858 | validation: 1.2115351888352484]
	TIME [epoch: 9.19 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0973907192151051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0973907192151051 | validation: 0.8633117642643486]
	TIME [epoch: 9.18 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.168192001209243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.168192001209243 | validation: 1.3216070520945207]
	TIME [epoch: 9.21 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4784220095731937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4784220095731937 | validation: 1.351261516414237]
	TIME [epoch: 9.19 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0413721060058694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0413721060058694 | validation: 0.9112036923810853]
	TIME [epoch: 9.18 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0066345464574342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0066345464574342 | validation: 0.8330314416562536]
	TIME [epoch: 9.17 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.715820294849744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.715820294849744 | validation: 1.1221173468863572]
	TIME [epoch: 9.17 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1837739147047324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1837739147047324 | validation: 3.2062399075099526]
	TIME [epoch: 9.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.446647682679647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.446647682679647 | validation: 0.741266420709295]
	TIME [epoch: 9.18 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.089860833093372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.089860833093372 | validation: 0.7584599963596623]
	TIME [epoch: 9.19 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3378650950543074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3378650950543074 | validation: 3.8611844248865372]
	TIME [epoch: 9.16 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1909390259991484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1909390259991484 | validation: 1.8057279115388063]
	TIME [epoch: 9.16 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3727082568615248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3727082568615248 | validation: 1.1223644224965077]
	TIME [epoch: 9.19 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5379652222944171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5379652222944171 | validation: 2.6567223952387145]
	TIME [epoch: 9.18 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4295878212217583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4295878212217583 | validation: 0.7505590481122515]
	TIME [epoch: 9.18 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1879713192261812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1879713192261812 | validation: 0.8644291241778017]
	TIME [epoch: 9.19 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1912204822488728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1912204822488728 | validation: 0.9038715797119344]
	TIME [epoch: 9.18 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0625539430589082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0625539430589082 | validation: 0.9501791712884562]
	TIME [epoch: 9.21 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1971519839080929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1971519839080929 | validation: 0.6933129737502506]
	TIME [epoch: 9.18 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4984082467255775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4984082467255775 | validation: 1.1529458017660787]
	TIME [epoch: 9.19 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0031787642448562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0031787642448562 | validation: 0.9605724352744938]
	TIME [epoch: 9.18 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2945469104232472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2945469104232472 | validation: 1.6282810033795814]
	TIME [epoch: 9.19 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1464039436248572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1464039436248572 | validation: 0.7827735723709941]
	TIME [epoch: 9.2 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1770703533544211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1770703533544211 | validation: 0.8560023857009684]
	TIME [epoch: 9.17 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2369937950509364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2369937950509364 | validation: 1.0762250272097615]
	TIME [epoch: 9.16 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1546498643299365		[learning rate: 0.01]
ERROR:
nan encountered in epoch 359 (validation loss).
