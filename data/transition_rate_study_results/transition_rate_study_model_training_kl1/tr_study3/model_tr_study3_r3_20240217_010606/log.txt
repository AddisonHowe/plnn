Args:
Namespace(name='model_tr_study3', outdir='out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3', training_data='data/transition_rate_studies/tr_study3/tr_study3_training/r3', validation_data='data/transition_rate_studies/tr_study3/tr_study3_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1896424786

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.005405145222644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.005405145222644 | validation: 9.44151109343445]
	TIME [epoch: 48.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.933183758897615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.933183758897615 | validation: 9.004056016678323]
	TIME [epoch: 9.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.549826203044093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.549826203044093 | validation: 7.547218166722789]
	TIME [epoch: 9.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.60084041441853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.60084041441853 | validation: 6.900338135926233]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.131434307014598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.131434307014598 | validation: 6.326299266595623]
	TIME [epoch: 9.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.125895130732744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.125895130732744 | validation: 6.6761210485426155]
	TIME [epoch: 9.15 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.041672994793285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.041672994793285 | validation: 6.487667543178458]
	TIME [epoch: 9.18 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.892077228346139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.892077228346139 | validation: 6.329097863188183]
	TIME [epoch: 9.16 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.738164843820195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.738164843820195 | validation: 6.2682751696705425]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.508579015257816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.508579015257816 | validation: 5.78087017962264]
	TIME [epoch: 9.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.178779756586243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.178779756586243 | validation: 5.7143751071514135]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.155241820628939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.155241820628939 | validation: 5.8498230686706]
	TIME [epoch: 9.16 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.942052651184548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.942052651184548 | validation: 5.612067693538148]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.976741618134676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.976741618134676 | validation: 5.58688644195305]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.832996675939375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.832996675939375 | validation: 5.680377904011824]
	TIME [epoch: 9.16 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.84921250130871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.84921250130871 | validation: 5.607263549527632]
	TIME [epoch: 9.17 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.886418017299031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.886418017299031 | validation: 5.594671306170848]
	TIME [epoch: 9.15 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.7628081734481444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7628081734481444 | validation: 5.68425762788468]
	TIME [epoch: 9.13 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.78595204613395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.78595204613395 | validation: 5.501741730662373]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.769266764872919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.769266764872919 | validation: 5.557736365981985]
	TIME [epoch: 9.17 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.748392601982665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.748392601982665 | validation: 5.452536782797473]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.634455843037036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.634455843037036 | validation: 5.552678182181087]
	TIME [epoch: 9.14 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.672290828357305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.672290828357305 | validation: 5.64475127131394]
	TIME [epoch: 9.14 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.644013089489314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.644013089489314 | validation: 5.372103627856268]
	TIME [epoch: 9.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.62256371428885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.62256371428885 | validation: 5.374727411910952]
	TIME [epoch: 9.16 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.754498141255245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.754498141255245 | validation: 5.580414386518952]
	TIME [epoch: 9.15 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.70119218354221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.70119218354221 | validation: 5.431438440050483]
	TIME [epoch: 9.16 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.619068288255292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.619068288255292 | validation: 5.357163396284779]
	TIME [epoch: 9.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.540610764299613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.540610764299613 | validation: 5.329250087191395]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.599686902892741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.599686902892741 | validation: 5.229655176637389]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.683395924797689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.683395924797689 | validation: 5.618119069810343]
	TIME [epoch: 9.15 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.54472635791336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.54472635791336 | validation: 5.029653683812443]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.301154671965439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.301154671965439 | validation: 5.0213557622794305]
	TIME [epoch: 9.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.559822857877006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.559822857877006 | validation: 5.1261361354216275]
	TIME [epoch: 9.15 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.263139944770318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.263139944770318 | validation: 4.953502686388248]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.983046663962971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.983046663962971 | validation: 5.037896644405759]
	TIME [epoch: 9.17 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.6020898833925195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6020898833925195 | validation: 4.152538995357598]
	TIME [epoch: 9.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.551534929437279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.551534929437279 | validation: 3.792744655738434]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.050335582319315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.050335582319315 | validation: 4.925921888648664]
	TIME [epoch: 9.16 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.059824919034581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.059824919034581 | validation: 3.554837074669022]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.526070335863227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.526070335863227 | validation: 3.8900742130027544]
	TIME [epoch: 9.17 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.804025753717707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.804025753717707 | validation: 2.9404469033128127]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9883758260256883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9883758260256883 | validation: 2.6829043600633176]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9241854990121463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9241854990121463 | validation: 2.3304158671380595]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.781821247105703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.781821247105703 | validation: 2.118560776137683]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.343977294465435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.343977294465435 | validation: 2.0187245809396046]
	TIME [epoch: 9.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.191103110684963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.191103110684963 | validation: 3.314249595210747]
	TIME [epoch: 9.16 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.199575093299432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.199575093299432 | validation: 3.354806653504169]
	TIME [epoch: 9.15 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0382851974371876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0382851974371876 | validation: 1.992041128500973]
	TIME [epoch: 9.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.900228466092008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.900228466092008 | validation: 2.0337879920414426]
	TIME [epoch: 9.14 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4445600912915677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4445600912915677 | validation: 1.5273720118411362]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7899431718817795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7899431718817795 | validation: 2.4571843967760447]
	TIME [epoch: 9.15 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1050575448934725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1050575448934725 | validation: 2.1462918063966607]
	TIME [epoch: 9.16 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7020421105139019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7020421105139019 | validation: 1.8741306877361836]
	TIME [epoch: 9.14 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.716087403182895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.716087403182895 | validation: 2.1689147104065762]
	TIME [epoch: 9.14 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.468554750223816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.468554750223816 | validation: 3.263544534384041]
	TIME [epoch: 9.14 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6682592844598365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6682592844598365 | validation: 1.710250685306086]
	TIME [epoch: 9.15 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.448047137896316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.448047137896316 | validation: 1.244605218899287]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5483586625262098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5483586625262098 | validation: 1.0619455522771326]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4865222860293021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4865222860293021 | validation: 1.4993145556835414]
	TIME [epoch: 9.14 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4474976534014259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4474976534014259 | validation: 1.0459290346066332]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.357534216402167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.357534216402167 | validation: 5.59169877756797]
	TIME [epoch: 9.16 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1383818166547286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1383818166547286 | validation: 1.180421202349263]
	TIME [epoch: 9.17 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4038249357477006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4038249357477006 | validation: 1.6990147968194442]
	TIME [epoch: 9.15 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3382503886807071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3382503886807071 | validation: 1.937868460572707]
	TIME [epoch: 9.15 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.294473702493021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.294473702493021 | validation: 1.0471324029263962]
	TIME [epoch: 9.18 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3615979519912949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3615979519912949 | validation: 1.044123227639099]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1944794605451157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1944794605451157 | validation: 1.3281924866053978]
	TIME [epoch: 9.16 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1391370050799243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1391370050799243 | validation: 1.5959972459360303]
	TIME [epoch: 9.15 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3866436294252151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3866436294252151 | validation: 0.8693651597726346]
	TIME [epoch: 9.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.276531318629204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.276531318629204 | validation: 0.7722547640232524]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.676042349034369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.676042349034369 | validation: 1.3586091048445805]
	TIME [epoch: 9.16 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7016312284133694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7016312284133694 | validation: 2.338169078118047]
	TIME [epoch: 9.15 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5741390997514164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5741390997514164 | validation: 0.8451412867131987]
	TIME [epoch: 9.16 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.316897567345243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.316897567345243 | validation: 1.0865416795981753]
	TIME [epoch: 9.17 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4975518368696248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4975518368696248 | validation: 1.2977088355880868]
	TIME [epoch: 9.15 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.115961953847037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.115961953847037 | validation: 1.2174097474007533]
	TIME [epoch: 9.15 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.041951081914753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.041951081914753 | validation: 1.2918958346076717]
	TIME [epoch: 9.15 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3189401715105724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3189401715105724 | validation: 1.764528823526134]
	TIME [epoch: 9.17 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2223203887592877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2223203887592877 | validation: 1.4767354732627072]
	TIME [epoch: 9.16 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1057290057096982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1057290057096982 | validation: 2.780935485489657]
	TIME [epoch: 9.15 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2626043473748123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2626043473748123 | validation: 0.8923635655910502]
	TIME [epoch: 9.16 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1967747257295425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1967747257295425 | validation: 0.7515981624898928]
	TIME [epoch: 9.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0393398275784735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0393398275784735 | validation: 0.5931040645526684]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.50077593177451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.50077593177451 | validation: 1.391359762691165]
	TIME [epoch: 9.14 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2029634821340283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2029634821340283 | validation: 1.0395748876287143]
	TIME [epoch: 9.15 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.64870247488974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.64870247488974 | validation: 1.2154574930905622]
	TIME [epoch: 9.15 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1986343673566788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1986343673566788 | validation: 1.0961741127712097]
	TIME [epoch: 9.16 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0076652328217004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0076652328217004 | validation: 0.8454828130201093]
	TIME [epoch: 9.14 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.467704053807444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.467704053807444 | validation: 1.6336519635717495]
	TIME [epoch: 9.14 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3778990776847266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3778990776847266 | validation: 0.6815572045126803]
	TIME [epoch: 9.14 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0715924626583022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0715924626583022 | validation: 0.7722661630310117]
	TIME [epoch: 9.17 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3014851703822004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3014851703822004 | validation: 1.737085628562128]
	TIME [epoch: 9.15 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4434430021847304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4434430021847304 | validation: 1.4618242556804182]
	TIME [epoch: 9.14 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6073051073820745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6073051073820745 | validation: 1.0408847733936726]
	TIME [epoch: 9.14 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.963743945871979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.963743945871979 | validation: 7.1295000985410155]
	TIME [epoch: 9.15 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.392088629228058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.392088629228058 | validation: 7.044054568686444]
	TIME [epoch: 9.16 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.381440516874217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.381440516874217 | validation: 7.108743872172834]
	TIME [epoch: 9.14 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.429062618280176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.429062618280176 | validation: 7.185226788460573]
	TIME [epoch: 9.14 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.043533018481191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.043533018481191 | validation: 1.7184510230310583]
	TIME [epoch: 9.18 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5004839565950179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5004839565950179 | validation: 1.4557480390932884]
	TIME [epoch: 9.16 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2421038931391741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2421038931391741 | validation: 1.047091683709124]
	TIME [epoch: 9.14 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1505690649189073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1505690649189073 | validation: 1.4613563229588022]
	TIME [epoch: 9.14 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.22866271532691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.22866271532691 | validation: 0.7554855652987046]
	TIME [epoch: 9.14 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.17253548902673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.17253548902673 | validation: 1.186616948448143]
	TIME [epoch: 9.16 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.039534094035901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.039534094035901 | validation: 1.0718326124999664]
	TIME [epoch: 9.14 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9931192928336408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9931192928336408 | validation: 0.9751858948523148]
	TIME [epoch: 9.14 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8430542697160968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8430542697160968 | validation: 1.103448176351248]
	TIME [epoch: 9.15 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1894183530170896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1894183530170896 | validation: 2.8867772564277336]
	TIME [epoch: 9.16 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4765931681524764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4765931681524764 | validation: 0.8387454951404556]
	TIME [epoch: 9.14 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.029673338814452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.029673338814452 | validation: 1.014475453014666]
	TIME [epoch: 9.14 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.283873231720292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.283873231720292 | validation: 0.9627019028211736]
	TIME [epoch: 9.14 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9796177631044163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9796177631044163 | validation: 0.8405385177753406]
	TIME [epoch: 9.16 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9485642160555067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9485642160555067 | validation: 0.8990538345068128]
	TIME [epoch: 9.14 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8654449531304922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8654449531304922 | validation: 1.0995006798788667]
	TIME [epoch: 9.14 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8899906847603521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8899906847603521 | validation: 1.6597840869461735]
	TIME [epoch: 9.14 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9115633188026576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9115633188026576 | validation: 0.5917684178991198]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8825173352464203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8825173352464203 | validation: 0.6135957407983459]
	TIME [epoch: 9.14 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7970546897269649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7970546897269649 | validation: 0.9742962657453988]
	TIME [epoch: 9.13 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0838830403792368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0838830403792368 | validation: 1.2398501501465344]
	TIME [epoch: 9.13 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8266420477655604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8266420477655604 | validation: 2.095432185294624]
	TIME [epoch: 9.14 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9108201767045541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9108201767045541 | validation: 0.6192691783940646]
	TIME [epoch: 9.15 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8931333920729045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8931333920729045 | validation: 0.5151091601447189]
	TIME [epoch: 9.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8466251242429091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8466251242429091 | validation: 0.8791329335335806]
	TIME [epoch: 9.14 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8048813002767574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8048813002767574 | validation: 3.096307897497036]
	TIME [epoch: 9.14 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0842168852175051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0842168852175051 | validation: 1.3076725701458034]
	TIME [epoch: 9.17 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1305043161142831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1305043161142831 | validation: 0.6974224744379415]
	TIME [epoch: 9.14 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7917627127546691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7917627127546691 | validation: 0.713846461393278]
	TIME [epoch: 9.14 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6844470643422556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6844470643422556 | validation: 1.0340862425589812]
	TIME [epoch: 9.16 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9173483119868692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9173483119868692 | validation: 0.8149332530546636]
	TIME [epoch: 9.2 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7067199081119047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7067199081119047 | validation: 0.44115656488745963]
	TIME [epoch: 9.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6183673346687518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6183673346687518 | validation: 0.6451569774401877]
	TIME [epoch: 9.16 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8516017936100692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8516017936100692 | validation: 0.44255936559029074]
	TIME [epoch: 9.15 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4278167406095172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4278167406095172 | validation: 1.0634350945539792]
	TIME [epoch: 9.16 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.032199876722297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.032199876722297 | validation: 1.0522553606002671]
	TIME [epoch: 9.17 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8014031848376654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8014031848376654 | validation: 1.1662651495988432]
	TIME [epoch: 9.15 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9376413721386566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9376413721386566 | validation: 0.7913451157556686]
	TIME [epoch: 9.14 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.652959078268039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.652959078268039 | validation: 0.4609318444049695]
	TIME [epoch: 9.14 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0790795585126083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0790795585126083 | validation: 1.1988672735125965]
	TIME [epoch: 9.16 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8225198394519714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8225198394519714 | validation: 0.9663211462540435]
	TIME [epoch: 9.14 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1095877545573742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1095877545573742 | validation: 0.575136940774948]
	TIME [epoch: 9.14 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7256143423771212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7256143423771212 | validation: 0.7034572440352864]
	TIME [epoch: 9.13 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1503595025599733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1503595025599733 | validation: 1.1848004801856804]
	TIME [epoch: 9.16 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9693794864520469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9693794864520469 | validation: 0.9243717262199763]
	TIME [epoch: 9.14 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0499393924744367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0499393924744367 | validation: 0.6615929968769179]
	TIME [epoch: 9.13 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.68457091687594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.68457091687594 | validation: 1.2781260245136012]
	TIME [epoch: 9.14 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8914785736754085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8914785736754085 | validation: 0.8482035032981061]
	TIME [epoch: 9.16 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7021044290619426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7021044290619426 | validation: 0.895034835704169]
	TIME [epoch: 9.15 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.736269496342929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.736269496342929 | validation: 0.4438739284875086]
	TIME [epoch: 9.14 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8952513704054329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8952513704054329 | validation: 0.8008093899897668]
	TIME [epoch: 9.14 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9923293699095534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9923293699095534 | validation: 1.069243960626959]
	TIME [epoch: 9.15 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615180439724992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.615180439724992 | validation: 7.261606628334624]
	TIME [epoch: 9.14 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.417375669950351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.417375669950351 | validation: 6.952572601418485]
	TIME [epoch: 9.13 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.365210588898804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.365210588898804 | validation: 6.938087228941166]
	TIME [epoch: 9.13 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.3229971280966994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3229971280966994 | validation: 6.935393062118623]
	TIME [epoch: 9.14 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.323608607938233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.323608607938233 | validation: 6.877507027398799]
	TIME [epoch: 9.15 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8752100009253083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8752100009253083 | validation: 0.8887437506970447]
	TIME [epoch: 9.14 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3648788366537958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3648788366537958 | validation: 0.7976474227870105]
	TIME [epoch: 9.13 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9397496465956376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9397496465956376 | validation: 0.9399548044478072]
	TIME [epoch: 9.13 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9677189573590119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9677189573590119 | validation: 0.8840072875688632]
	TIME [epoch: 9.15 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2612587150519157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2612587150519157 | validation: 0.6470379074717378]
	TIME [epoch: 9.13 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8153016823503216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8153016823503216 | validation: 0.743063370026076]
	TIME [epoch: 9.13 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.841090725906106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.841090725906106 | validation: 0.7688142555874699]
	TIME [epoch: 9.14 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9653308018729749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9653308018729749 | validation: 0.5126172857693478]
	TIME [epoch: 9.16 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7321384542427225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7321384542427225 | validation: 0.704606185141142]
	TIME [epoch: 9.13 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7565920714471043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7565920714471043 | validation: 0.7878563014668627]
	TIME [epoch: 9.13 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7161602413558219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7161602413558219 | validation: 1.0509559358484721]
	TIME [epoch: 9.13 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8295539286392817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8295539286392817 | validation: 0.5586958485694355]
	TIME [epoch: 9.15 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6118053098231491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6118053098231491 | validation: 1.495041529534741]
	TIME [epoch: 9.13 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7715301512544948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7715301512544948 | validation: 0.8868416929762185]
	TIME [epoch: 9.13 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6729731196684825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6729731196684825 | validation: 0.7401861453409834]
	TIME [epoch: 9.14 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7098737547256582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7098737547256582 | validation: 0.8079798619837963]
	TIME [epoch: 9.16 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7761779889457544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7761779889457544 | validation: 0.4228395565117675]
	TIME [epoch: 9.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6363518697343384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6363518697343384 | validation: 1.59372245464524]
	TIME [epoch: 9.14 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8715600012521861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8715600012521861 | validation: 0.6689457898526727]
	TIME [epoch: 9.13 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7194649778338266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7194649778338266 | validation: 1.0384312564244438]
	TIME [epoch: 9.15 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9634535390416241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9634535390416241 | validation: 1.379808870157616]
	TIME [epoch: 9.15 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8866277191056653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8866277191056653 | validation: 0.5567604833614226]
	TIME [epoch: 9.14 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9092279670187808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9092279670187808 | validation: 1.166334738153749]
	TIME [epoch: 9.13 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.823940267964477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.823940267964477 | validation: 1.7644931623017222]
	TIME [epoch: 9.13 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7254015573548127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7254015573548127 | validation: 0.4231142191290182]
	TIME [epoch: 9.16 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7706258019759874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7706258019759874 | validation: 0.8532786906969408]
	TIME [epoch: 9.14 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6306407218459968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6306407218459968 | validation: 1.118861662076124]
	TIME [epoch: 9.14 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6404359436954407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6404359436954407 | validation: 0.3937803090986543]
	TIME [epoch: 9.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6817338259136763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6817338259136763 | validation: 4.1538662990673405]
	TIME [epoch: 9.16 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0579509072563027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0579509072563027 | validation: 0.6231119448270283]
	TIME [epoch: 9.14 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.704908952446597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.704908952446597 | validation: 0.4904096551184427]
	TIME [epoch: 9.14 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9773469205201051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9773469205201051 | validation: 1.3654530652705983]
	TIME [epoch: 9.14 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2850645723459748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2850645723459748 | validation: 0.7738411571816459]
	TIME [epoch: 9.15 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.702920137854395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.702920137854395 | validation: 0.6226523967459925]
	TIME [epoch: 9.15 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7551670685901908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7551670685901908 | validation: 0.6365416718117238]
	TIME [epoch: 9.13 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7674512777295538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7674512777295538 | validation: 0.7997329620641557]
	TIME [epoch: 9.14 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6588427635000765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6588427635000765 | validation: 0.4953622830548924]
	TIME [epoch: 9.14 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6386963164220181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6386963164220181 | validation: 0.7316049013146542]
	TIME [epoch: 9.15 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2154949544915157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2154949544915157 | validation: 0.5173990042246159]
	TIME [epoch: 9.14 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0598351645756012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0598351645756012 | validation: 0.8193046861709334]
	TIME [epoch: 9.13 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9104345469210475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9104345469210475 | validation: 0.6757866119949079]
	TIME [epoch: 9.13 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7284413303379791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7284413303379791 | validation: 0.8230377009076191]
	TIME [epoch: 9.15 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7902685619604305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7902685619604305 | validation: 0.726477278788862]
	TIME [epoch: 9.12 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5886070114932244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5886070114932244 | validation: 0.5876250254813303]
	TIME [epoch: 9.13 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8699329676267468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8699329676267468 | validation: 1.036167578559895]
	TIME [epoch: 9.13 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7583293688654114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7583293688654114 | validation: 0.7776318619452554]
	TIME [epoch: 9.15 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7375548057376641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7375548057376641 | validation: 0.668630474610616]
	TIME [epoch: 9.14 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6470596246352309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6470596246352309 | validation: 0.467419216055849]
	TIME [epoch: 9.13 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.619261967351129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.619261967351129 | validation: 0.6216456466502465]
	TIME [epoch: 9.13 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5835578244959354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5835578244959354 | validation: 1.3258530838233864]
	TIME [epoch: 9.16 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7624794925830263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7624794925830263 | validation: 0.8295701528267394]
	TIME [epoch: 9.13 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7818789914089435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7818789914089435 | validation: 1.4515590358335908]
	TIME [epoch: 9.13 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9457632074439383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9457632074439383 | validation: 0.7320749843927734]
	TIME [epoch: 9.13 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9296880795854883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9296880795854883 | validation: 0.6104749549038928]
	TIME [epoch: 9.15 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7292651129965786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7292651129965786 | validation: 0.8298402330386055]
	TIME [epoch: 9.14 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.774374513849019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.774374513849019 | validation: 0.4743747148208636]
	TIME [epoch: 9.13 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6664127561794915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6664127561794915 | validation: 0.4149800762175193]
	TIME [epoch: 9.13 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8660550135176397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8660550135176397 | validation: 0.7424875315916133]
	TIME [epoch: 9.14 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.687270769852699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687270769852699 | validation: 0.5596530434881002]
	TIME [epoch: 9.15 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8144792748128763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8144792748128763 | validation: 1.0233123738770575]
	TIME [epoch: 9.13 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8824084926779863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8824084926779863 | validation: 0.6534813415781444]
	TIME [epoch: 9.13 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6420979850968778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6420979850968778 | validation: 0.6700366653791634]
	TIME [epoch: 9.14 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7987299533136173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7987299533136173 | validation: 0.4275224608698036]
	TIME [epoch: 9.16 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.777546198247241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.777546198247241 | validation: 0.6398203624108536]
	TIME [epoch: 9.13 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2241837902288553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2241837902288553 | validation: 6.729843907292602]
	TIME [epoch: 9.13 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2299742501766486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2299742501766486 | validation: 0.9309465798193006]
	TIME [epoch: 9.13 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8630477282110449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8630477282110449 | validation: 0.6249746168498668]
	TIME [epoch: 9.16 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9754437837522676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9754437837522676 | validation: 1.3005845016753357]
	TIME [epoch: 9.14 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9244095484404052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9244095484404052 | validation: 0.6545545852764387]
	TIME [epoch: 9.13 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6874855752393274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6874855752393274 | validation: 1.0616145934529992]
	TIME [epoch: 9.13 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9134649189411789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9134649189411789 | validation: 0.9735092371711409]
	TIME [epoch: 9.15 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.781768801923417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.781768801923417 | validation: 0.5377145974046904]
	TIME [epoch: 9.15 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5969236011578011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5969236011578011 | validation: 1.643284322395433]
	TIME [epoch: 9.13 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.16757065954794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.16757065954794 | validation: 0.6615347152450713]
	TIME [epoch: 9.14 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6771487836215676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6771487836215676 | validation: 0.5987255583132501]
	TIME [epoch: 9.15 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7220574422298467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7220574422298467 | validation: 0.8457461942847522]
	TIME [epoch: 9.14 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7292940454029022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7292940454029022 | validation: 0.4952991461861893]
	TIME [epoch: 9.14 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6828362892898132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6828362892898132 | validation: 0.38067921509010394]
	TIME [epoch: 9.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6567334255684716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6567334255684716 | validation: 0.42846334501861966]
	TIME [epoch: 9.16 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7204734374458204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7204734374458204 | validation: 0.4110853481302048]
	TIME [epoch: 9.14 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7751780362381803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7751780362381803 | validation: 1.3603125327851775]
	TIME [epoch: 9.13 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8072598442685525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8072598442685525 | validation: 0.553682555236593]
	TIME [epoch: 9.13 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6888210626164377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6888210626164377 | validation: 0.7955436996012111]
	TIME [epoch: 9.13 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6572015516030375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6572015516030375 | validation: 0.8083450298230913]
	TIME [epoch: 9.15 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6690862904703139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6690862904703139 | validation: 0.6507288872516142]
	TIME [epoch: 9.13 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6628145661757129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6628145661757129 | validation: 0.7234163513344629]
	TIME [epoch: 9.13 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.612628410567152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.612628410567152 | validation: 0.9387524373868403]
	TIME [epoch: 9.12 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5732768063587392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5732768063587392 | validation: 0.6772949701773123]
	TIME [epoch: 9.15 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7656112362953238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7656112362953238 | validation: 1.1075896103586471]
	TIME [epoch: 9.13 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5385246812741435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5385246812741435 | validation: 0.9240773061999311]
	TIME [epoch: 9.13 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6854377555466467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6854377555466467 | validation: 1.5000312293823592]
	TIME [epoch: 9.13 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2939000119886248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2939000119886248 | validation: 0.6867992475462548]
	TIME [epoch: 9.14 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.357156114852642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.357156114852642 | validation: 0.7191427569901288]
	TIME [epoch: 9.13 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.754093801678773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.754093801678773 | validation: 0.7614271273816703]
	TIME [epoch: 9.13 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6146805697570705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6146805697570705 | validation: 0.4784355087100459]
	TIME [epoch: 9.12 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.768431188062754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.768431188062754 | validation: 0.4430352820096153]
	TIME [epoch: 9.15 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1556483399918083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1556483399918083 | validation: 0.8604635023910397]
	TIME [epoch: 9.14 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7485959441462775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7485959441462775 | validation: 0.5594073674315152]
	TIME [epoch: 9.14 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6879102188400493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6879102188400493 | validation: 0.6273631681867866]
	TIME [epoch: 9.13 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6409516574452532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6409516574452532 | validation: 1.0060647005972922]
	TIME [epoch: 9.14 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1115518774469835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1115518774469835 | validation: 0.4889215742338623]
	TIME [epoch: 9.14 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.736532131365626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.736532131365626 | validation: 0.4686693461320553]
	TIME [epoch: 9.12 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5936419768968051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5936419768968051 | validation: 0.6016734943390203]
	TIME [epoch: 9.13 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7047572453889753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7047572453889753 | validation: 0.5682731116836524]
	TIME [epoch: 9.14 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6962017102545011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6962017102545011 | validation: 1.8090025833653698]
	TIME [epoch: 9.14 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0746461748516736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0746461748516736 | validation: 0.6787733655881463]
	TIME [epoch: 9.14 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8655083363775857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8655083363775857 | validation: 1.5028040045461726]
	TIME [epoch: 9.14 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8148200921955645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8148200921955645 | validation: 0.6970561459679161]
	TIME [epoch: 9.13 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.796839996466529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.796839996466529 | validation: 0.6058768953473055]
	TIME [epoch: 9.15 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7260002278437423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7260002278437423 | validation: 0.6911617852105251]
	TIME [epoch: 9.13 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8696006422462149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8696006422462149 | validation: 0.742723770056517]
	TIME [epoch: 9.12 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7242017236776588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7242017236776588 | validation: 0.7548328497594543]
	TIME [epoch: 9.13 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9053505480946841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9053505480946841 | validation: 0.819825609316055]
	TIME [epoch: 9.15 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7716272822841063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7716272822841063 | validation: 0.44193951544694365]
	TIME [epoch: 9.13 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7261901143368231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7261901143368231 | validation: 0.8607989205063299]
	TIME [epoch: 9.12 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.722648245162627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.722648245162627 | validation: 0.37236034308986465]
	TIME [epoch: 9.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8036061042710096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8036061042710096 | validation: 1.0262717335632172]
	TIME [epoch: 9.15 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7902482628891002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7902482628891002 | validation: 0.6254301794386765]
	TIME [epoch: 9.13 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6889724772208699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6889724772208699 | validation: 0.992091629671317]
	TIME [epoch: 9.12 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7680748490948426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7680748490948426 | validation: 0.5253171736552632]
	TIME [epoch: 9.12 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6109610137521806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6109610137521806 | validation: 0.4408320439208484]
	TIME [epoch: 9.13 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.661022844085781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.661022844085781 | validation: 0.6102390586573689]
	TIME [epoch: 9.13 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7760831083070672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7760831083070672 | validation: 0.4592281946743333]
	TIME [epoch: 9.12 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9823799304202426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9823799304202426 | validation: 0.5502708317230023]
	TIME [epoch: 9.13 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6137621421388209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6137621421388209 | validation: 1.1837696889769602]
	TIME [epoch: 9.12 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9588883268212633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9588883268212633 | validation: 1.6551997031940133]
	TIME [epoch: 9.14 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9587171791131078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9587171791131078 | validation: 0.4772957641405396]
	TIME [epoch: 9.12 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6072627235490493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6072627235490493 | validation: 0.48643167940696597]
	TIME [epoch: 9.12 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5860134969045528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5860134969045528 | validation: 1.5057378427227648]
	TIME [epoch: 9.12 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7817768046048976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7817768046048976 | validation: 0.5018057041103028]
	TIME [epoch: 9.15 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7098086437727946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7098086437727946 | validation: 0.3918030659672779]
	TIME [epoch: 9.12 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8794470881050083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8794470881050083 | validation: 1.7626878992524762]
	TIME [epoch: 9.13 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.739573992581311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.739573992581311 | validation: 6.411928234523657]
	TIME [epoch: 9.11 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.155683137641497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.155683137641497 | validation: 7.361821936891346]
	TIME [epoch: 9.15 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.240392180460587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.240392180460587 | validation: 3.1110060751619875]
	TIME [epoch: 9.13 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5518903610761448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5518903610761448 | validation: 0.7043684254949301]
	TIME [epoch: 9.12 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8860282155019222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8860282155019222 | validation: 1.4475961206335792]
	TIME [epoch: 9.12 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8356556403818114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8356556403818114 | validation: 1.1660970622322666]
	TIME [epoch: 9.14 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.029249449680375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.029249449680375 | validation: 0.723165396850395]
	TIME [epoch: 9.12 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0096169677874798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0096169677874798 | validation: 0.6956650100171775]
	TIME [epoch: 9.12 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9648823557759647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9648823557759647 | validation: 0.8365143877196426]
	TIME [epoch: 9.12 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9228838906780836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9228838906780836 | validation: 0.7074417768216136]
	TIME [epoch: 9.14 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.717141519174433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.717141519174433 | validation: 0.4883279870220123]
	TIME [epoch: 9.13 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6983654636845806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6983654636845806 | validation: 1.0998609337071699]
	TIME [epoch: 9.12 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6667901325587233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6667901325587233 | validation: 0.5031313471795179]
	TIME [epoch: 9.12 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6456816300554742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6456816300554742 | validation: 0.813997471561402]
	TIME [epoch: 9.14 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6932548126535497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6932548126535497 | validation: 0.9152564662389542]
	TIME [epoch: 9.14 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.777242313277857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.777242313277857 | validation: 0.4562244993689205]
	TIME [epoch: 9.13 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8515106416420852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8515106416420852 | validation: 0.45871612046820465]
	TIME [epoch: 9.13 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6218112448060241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6218112448060241 | validation: 0.7295508581403645]
	TIME [epoch: 9.12 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6810439740706722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6810439740706722 | validation: 0.5735713721418161]
	TIME [epoch: 9.15 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5898699580370144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5898699580370144 | validation: 0.5602180432986081]
	TIME [epoch: 9.13 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7939684261825766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7939684261825766 | validation: 0.3835949014448642]
	TIME [epoch: 9.12 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5798526353153955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5798526353153955 | validation: 0.6498011816600615]
	TIME [epoch: 9.12 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6120528884081233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6120528884081233 | validation: 1.4390661921466539]
	TIME [epoch: 9.15 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1309266577264872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1309266577264872 | validation: 0.44496888148583114]
	TIME [epoch: 9.12 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8467866017618173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8467866017618173 | validation: 0.7027016777630863]
	TIME [epoch: 9.13 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6994520408374909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6994520408374909 | validation: 0.9967932001668103]
	TIME [epoch: 9.13 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.864437321747263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.864437321747263 | validation: 0.3881494964670599]
	TIME [epoch: 9.15 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6233779239474703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6233779239474703 | validation: 0.7964796126300053]
	TIME [epoch: 9.13 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7863452433361722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7863452433361722 | validation: 0.4278310644867181]
	TIME [epoch: 9.13 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6194316362706657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6194316362706657 | validation: 0.3148863357243141]
	TIME [epoch: 9.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9515538290276908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9515538290276908 | validation: 1.038270416884013]
	TIME [epoch: 9.15 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7274378346588303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7274378346588303 | validation: 0.8295341604913729]
	TIME [epoch: 9.12 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6657042664421748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6657042664421748 | validation: 0.6931651154836815]
	TIME [epoch: 9.12 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6808143260074038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6808143260074038 | validation: 0.7349933305554964]
	TIME [epoch: 9.11 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8806676297533407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8806676297533407 | validation: 0.2985189319373103]
	TIME [epoch: 9.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study3/model_tr_study3_r3_20240217_010606/states/model_tr_study3_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7036451474438128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7036451474438128 | validation: 0.5697431895957252]
	TIME [epoch: 9.15 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0040850414825635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0040850414825635 | validation: 0.703640459080386]
	TIME [epoch: 9.12 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8223227123930821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8223227123930821 | validation: 0.829147685483282]
	TIME [epoch: 9.12 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.581959385372525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.581959385372525 | validation: 1.1665445057667423]
	TIME [epoch: 9.12 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1312336729585546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1312336729585546 | validation: 0.8130218904999514]
	TIME [epoch: 9.14 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.702253823342428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.702253823342428 | validation: 0.5568779867363327]
	TIME [epoch: 9.13 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.884195152039821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.884195152039821 | validation: 0.7375318777895474]
	TIME [epoch: 9.12 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.841751532602778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.841751532602778 | validation: 0.9608209785879354]
	TIME [epoch: 9.12 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7391976791836852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7391976791836852 | validation: 1.169765421747472]
	TIME [epoch: 9.13 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8260365513189634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8260365513189634 | validation: 0.5191073590085943]
	TIME [epoch: 9.14 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8236881483348467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8236881483348467 | validation: 0.9199574944178658]
	TIME [epoch: 9.12 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0097623982936086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0097623982936086 | validation: 0.6254706143259978]
	TIME [epoch: 9.12 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8494713528921061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8494713528921061 | validation: 1.246460089319533]
	TIME [epoch: 9.13 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7822221077520347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7822221077520347 | validation: 0.7300523690114266]
	TIME [epoch: 9.13 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8362211935500005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8362211935500005 | validation: 0.5032876341929182]
	TIME [epoch: 9.11 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8328182949968331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8328182949968331 | validation: 1.0339611230708603]
	TIME [epoch: 9.12 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9018924562680745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9018924562680745 | validation: 0.5030520879998785]
	TIME [epoch: 9.12 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.735627734185711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.735627734185711 | validation: 1.3157560109447712]
	TIME [epoch: 9.14 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.807515918695581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.807515918695581 | validation: 0.4248158559434787]
	TIME [epoch: 9.12 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7220633746536811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7220633746536811 | validation: 0.8769787798369628]
	TIME [epoch: 9.12 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9652943073894639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9652943073894639 | validation: 1.0189202556932022]
	TIME [epoch: 9.12 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7467992996353789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7467992996353789 | validation: 0.7999356313986361]
	TIME [epoch: 9.14 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7989375336061747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7989375336061747 | validation: 0.5810765703524414]
	TIME [epoch: 9.12 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7316685564417498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7316685564417498 | validation: 0.4212290833386888]
	TIME [epoch: 9.11 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6389455136301986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6389455136301986 | validation: 0.7705398911380736]
	TIME [epoch: 9.12 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6682285513211129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6682285513211129 | validation: 0.6447493789495164]
	TIME [epoch: 9.14 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7265626655817485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7265626655817485 | validation: 0.5582628299027244]
	TIME [epoch: 9.13 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7028819469589651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7028819469589651 | validation: 0.8780191398421759]
	TIME [epoch: 9.11 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7600905749779785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7600905749779785 | validation: 0.43317164769052363]
	TIME [epoch: 9.11 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6323716164406833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6323716164406833 | validation: 0.6188435387345861]
	TIME [epoch: 9.13 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7773392129930848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7773392129930848 | validation: 0.9100042144511624]
	TIME [epoch: 9.12 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7910957200754969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7910957200754969 | validation: 0.5840794623626311]
	TIME [epoch: 9.11 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7999238071844372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7999238071844372 | validation: 0.7269813927909068]
	TIME [epoch: 9.11 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.912206281677269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.912206281677269 | validation: 0.8365652932290121]
	TIME [epoch: 9.13 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7535595744168834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7535595744168834 | validation: 0.9349009658914369]
	TIME [epoch: 9.14 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7208904500492118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7208904500492118 | validation: 0.7947104022184911]
	TIME [epoch: 9.12 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7991948507197864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7991948507197864 | validation: 0.43779799059924845]
	TIME [epoch: 9.12 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7283947010449324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7283947010449324 | validation: 0.550939210010718]
	TIME [epoch: 9.13 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7109449791712856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7109449791712856 | validation: 0.41629096640508023]
	TIME [epoch: 9.14 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7132706412785255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7132706412785255 | validation: 0.4203787601805884]
	TIME [epoch: 9.12 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8391028039511017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8391028039511017 | validation: 0.657342332709663]
	TIME [epoch: 9.13 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7357338634391433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7357338634391433 | validation: 1.665657023551221]
	TIME [epoch: 9.12 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.940765399873486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.940765399873486 | validation: 0.4870217608963898]
	TIME [epoch: 9.14 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9068992449997145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9068992449997145 | validation: 0.6223703835354973]
	TIME [epoch: 9.12 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6342615228312507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6342615228312507 | validation: 0.573522404836291]
	TIME [epoch: 9.12 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6304231979897155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6304231979897155 | validation: 0.7701482733351188]
	TIME [epoch: 9.12 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6475215980883484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6475215980883484 | validation: 1.0429712868101286]
	TIME [epoch: 9.15 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.632241994905445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.632241994905445 | validation: 4.358868283113424]
	TIME [epoch: 9.13 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.458929109158733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.458929109158733 | validation: 5.194200141074457]
	TIME [epoch: 9.13 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.097059018805024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.097059018805024 | validation: 6.255514136276525]
	TIME [epoch: 9.13 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.3918233712326975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3918233712326975 | validation: 7.858717601911396]
	TIME [epoch: 9.15 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.246627987658874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.246627987658874 | validation: 8.878879928394605]
	TIME [epoch: 9.13 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.796126429264628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.796126429264628 | validation: 8.238744305815917]
	TIME [epoch: 9.13 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.05375199575885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.05375199575885 | validation: 7.068274366723328]
	TIME [epoch: 9.13 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.499919248699359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.499919248699359 | validation: 7.567725044651825]
	TIME [epoch: 9.14 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.590867347355774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.590867347355774 | validation: 7.324172884656637]
	TIME [epoch: 9.13 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.562633136933798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.562633136933798 | validation: 7.2633449515595885]
	TIME [epoch: 9.12 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.416681008659322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.416681008659322 | validation: 7.0183168993036595]
	TIME [epoch: 9.12 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.763040055097479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.763040055097479 | validation: 1.3943896438206633]
	TIME [epoch: 9.14 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2714827700362117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2714827700362117 | validation: 1.5268338459459097]
	TIME [epoch: 9.13 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3331119583334652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3331119583334652 | validation: 1.2620005534308065]
	TIME [epoch: 9.11 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6921803582848345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6921803582848345 | validation: 2.82792892260068]
	TIME [epoch: 9.11 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5224413468656677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5224413468656677 | validation: 1.1177957440881368]
	TIME [epoch: 9.12 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3403431826372683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3403431826372683 | validation: 1.0869604939837265]
	TIME [epoch: 9.13 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0802153731098902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0802153731098902 | validation: 1.2538909007543504]
	TIME [epoch: 9.11 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6870356716411599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6870356716411599 | validation: 0.9831540076226382]
	TIME [epoch: 9.11 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.332244442767577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.332244442767577 | validation: 1.0770212314435004]
	TIME [epoch: 9.13 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.536444542986195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.536444542986195 | validation: 7.042439907571245]
	TIME [epoch: 9.12 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.160930220388506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.160930220388506 | validation: 10.443076432014806]
	TIME [epoch: 9.12 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.581764310048333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.581764310048333 | validation: 9.74293632741408]
	TIME [epoch: 9.12 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.902367200567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.902367200567 | validation: 11.04206853048412]
	TIME [epoch: 9.12 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/10] avg loss: 10.673958266154514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.673958266154514 | validation: 9.04245158119835]
	TIME [epoch: 9.15 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.756978201571181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.756978201571181 | validation: 8.502386311904058]
	TIME [epoch: 9.12 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.936628537125465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.936628537125465 | validation: 7.293068564644278]
	TIME [epoch: 9.13 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.313307672780488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.313307672780488 | validation: 5.742192380790385]
	TIME [epoch: 9.13 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.4167168776645145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.4167168776645145 | validation: 4.662130763949594]
	TIME [epoch: 9.15 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8708624464379304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8708624464379304 | validation: 7.0813779237917265]
	TIME [epoch: 9.13 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.8561807144978815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.8561807144978815 | validation: 9.008297402447099]
	TIME [epoch: 9.12 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.980092322911315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.980092322911315 | validation: 13.022843567733373]
	TIME [epoch: 9.12 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/10] avg loss: 10.620161516131038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.620161516131038 | validation: 11.008371514230499]
	TIME [epoch: 9.14 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/10] avg loss: 10.987498891748716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.987498891748716 | validation: 10.886189656260932]
	TIME [epoch: 9.13 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/10] avg loss: 10.649744505996718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.649744505996718 | validation: 10.490973698363216]
	TIME [epoch: 9.11 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.790606870251423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.790606870251423 | validation: 8.397898691454241]
	TIME [epoch: 9.12 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.718065043048137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.718065043048137 | validation: 10.253525145127863]
	TIME [epoch: 9.14 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.885531873468306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.885531873468306 | validation: 9.94959612810658]
	TIME [epoch: 9.13 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/10] avg loss: nan		[learning rate: 0.01]
ERROR:
nan encountered in epoch 408 (training loss).
