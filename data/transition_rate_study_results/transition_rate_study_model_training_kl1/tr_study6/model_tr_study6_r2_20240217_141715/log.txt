Args:
Namespace(name='model_tr_study6', outdir='out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2', training_data='data/transition_rate_studies/tr_study6/tr_study6_training/r2', validation_data='data/transition_rate_studies/tr_study6/tr_study6_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2775564526

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.377780660207588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.377780660207588 | validation: 8.247072331272557]
	TIME [epoch: 49.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.062962684002857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.062962684002857 | validation: 9.766779261497266]
	TIME [epoch: 10.3 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.375525181519157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.375525181519157 | validation: 5.363185824590756]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.4446312836376265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.4446312836376265 | validation: 6.3887089003220785]
	TIME [epoch: 10.3 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.0875323589613455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0875323589613455 | validation: 6.041226817993959]
	TIME [epoch: 10.3 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.849758091088816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.849758091088816 | validation: 5.868320351008256]
	TIME [epoch: 10.3 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.506591566542329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.506591566542329 | validation: 5.009229022660736]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.01787307728941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.01787307728941 | validation: 4.457615242577471]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.162721743423307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.162721743423307 | validation: 4.928458873251897]
	TIME [epoch: 10.3 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.804278230192102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.804278230192102 | validation: 3.926663231731393]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.66622409448016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.66622409448016 | validation: 5.306948094044301]
	TIME [epoch: 10.3 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.972244766668574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.972244766668574 | validation: 4.824729774770444]
	TIME [epoch: 10.3 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.547788044575455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.547788044575455 | validation: 4.507338205369136]
	TIME [epoch: 10.3 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.967718440175372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.967718440175372 | validation: 3.8259108597967852]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.105517594558246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.105517594558246 | validation: 4.296397755662771]
	TIME [epoch: 10.3 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.336000925856351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.336000925856351 | validation: 3.6116675660945248]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0389620302323745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0389620302323745 | validation: 3.7190384128733527]
	TIME [epoch: 10.3 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.332907762443316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.332907762443316 | validation: 7.032329625901021]
	TIME [epoch: 10.3 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.425362292482154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.425362292482154 | validation: 4.496823470253297]
	TIME [epoch: 10.3 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.275921940575914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.275921940575914 | validation: 3.651782100981597]
	TIME [epoch: 10.3 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.023154606438498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.023154606438498 | validation: 3.588156381488675]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7994654643315457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7994654643315457 | validation: 3.760796785403571]
	TIME [epoch: 10.3 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.732243400684388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.732243400684388 | validation: 3.5095297430845527]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6073726165366367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6073726165366367 | validation: 3.379680700250858]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.193705241837934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.193705241837934 | validation: 3.8357559973840103]
	TIME [epoch: 10.3 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5999071967944745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5999071967944745 | validation: 3.4816923998832388]
	TIME [epoch: 10.3 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.686680305953844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.686680305953844 | validation: 3.6545252686036584]
	TIME [epoch: 10.3 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5636481419237738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5636481419237738 | validation: 3.3283950620670586]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5418257833140805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5418257833140805 | validation: 3.442179079850115]
	TIME [epoch: 10.3 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3922682312033254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3922682312033254 | validation: 3.319605216210504]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4211446128168665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4211446128168665 | validation: 3.4553078136536635]
	TIME [epoch: 10.3 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.356798372258345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.356798372258345 | validation: 3.26736095953121]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4007818081663244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4007818081663244 | validation: 3.9250965638939763]
	TIME [epoch: 10.3 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4709146521290632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4709146521290632 | validation: 3.171931545449427]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3422602910466717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3422602910466717 | validation: 3.0394055294832896]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.155288624185487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.155288624185487 | validation: 3.10612078756678]
	TIME [epoch: 10.3 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.42323797552782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.42323797552782 | validation: 2.950854664308553]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3485388126840823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3485388126840823 | validation: 3.2317237814156647]
	TIME [epoch: 10.3 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.405721920181837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.405721920181837 | validation: 3.0573472380486213]
	TIME [epoch: 10.3 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8302430192072165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8302430192072165 | validation: 4.5252625118705545]
	TIME [epoch: 10.3 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.436652102857218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.436652102857218 | validation: 2.989906471138962]
	TIME [epoch: 10.3 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.398536882114229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.398536882114229 | validation: 6.263333503714405]
	TIME [epoch: 10.3 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.58339920963465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.58339920963465 | validation: 3.238635195541343]
	TIME [epoch: 10.3 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.247125265331673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.247125265331673 | validation: 3.5371120220399757]
	TIME [epoch: 10.3 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9851312157956102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9851312157956102 | validation: 3.263807444656076]
	TIME [epoch: 10.3 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5828200652063265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5828200652063265 | validation: 3.53547390504859]
	TIME [epoch: 10.3 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0077702872429875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0077702872429875 | validation: 3.6679015453504067]
	TIME [epoch: 10.3 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5112765571661315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5112765571661315 | validation: 2.9819903328202177]
	TIME [epoch: 10.3 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1763950941929933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1763950941929933 | validation: 3.6371845290550913]
	TIME [epoch: 10.3 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3297594209262646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3297594209262646 | validation: 3.0520449870988147]
	TIME [epoch: 10.3 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1810191139737154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1810191139737154 | validation: 3.108516461720946]
	TIME [epoch: 10.3 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.158323610866499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.158323610866499 | validation: 2.890891407996621]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6419061837453945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6419061837453945 | validation: 3.4903293969999916]
	TIME [epoch: 10.3 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0894826738441723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0894826738441723 | validation: 2.93140929353449]
	TIME [epoch: 10.3 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0132645308171377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0132645308171377 | validation: 2.965299943906523]
	TIME [epoch: 10.3 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7229219189530527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7229219189530527 | validation: 3.5420305489735338]
	TIME [epoch: 10.3 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.40733731285322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.40733731285322 | validation: 2.8924405242915907]
	TIME [epoch: 10.3 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8248313918307373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8248313918307373 | validation: 2.717806711402454]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.831233417836942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.831233417836942 | validation: 3.0594233567251514]
	TIME [epoch: 10.3 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7915145107796837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7915145107796837 | validation: 2.4643680303740347]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5326164066090073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5326164066090073 | validation: 3.1505087306575548]
	TIME [epoch: 10.3 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.446756281881143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.446756281881143 | validation: 2.9154288744363304]
	TIME [epoch: 10.3 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6235796827987325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6235796827987325 | validation: 2.289488790312755]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.691408002691323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.691408002691323 | validation: 2.755436688616807]
	TIME [epoch: 10.3 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959534541326889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.959534541326889 | validation: 2.505485285406791]
	TIME [epoch: 10.3 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4626368174129896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4626368174129896 | validation: 2.191113512850447]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.533205333900591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.533205333900591 | validation: 2.142930309718369]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3445671395715153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3445671395715153 | validation: 3.5763053354210523]
	TIME [epoch: 10.3 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.400652101729375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.400652101729375 | validation: 3.349297986334669]
	TIME [epoch: 10.3 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.459159423136951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.459159423136951 | validation: 5.2938512357291]
	TIME [epoch: 10.3 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.371953931948814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.371953931948814 | validation: 3.2616964335640377]
	TIME [epoch: 10.3 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9229866686312787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9229866686312787 | validation: 2.6200719040419096]
	TIME [epoch: 10.3 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8992227352829447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8992227352829447 | validation: 2.418099129836946]
	TIME [epoch: 10.3 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4942225431633593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4942225431633593 | validation: 2.3842863357202297]
	TIME [epoch: 10.3 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4730529612037997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4730529612037997 | validation: 2.271240900961543]
	TIME [epoch: 10.3 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5268472742772063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5268472742772063 | validation: 2.9322757478954675]
	TIME [epoch: 10.3 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.484567922419346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.484567922419346 | validation: 2.0512710809998236]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1610468043099775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1610468043099775 | validation: 1.8401083220075531]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.414729929643111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.414729929643111 | validation: 4.832659682965207]
	TIME [epoch: 10.3 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.177913068687849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.177913068687849 | validation: 2.677049066056101]
	TIME [epoch: 10.3 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1891364627822196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1891364627822196 | validation: 2.979884173654526]
	TIME [epoch: 10.3 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.749224034700745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.749224034700745 | validation: 3.439815779289508]
	TIME [epoch: 10.3 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8999247286946686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8999247286946686 | validation: 1.8810749102013427]
	TIME [epoch: 10.3 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1942635827462937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1942635827462937 | validation: 2.5145310900296343]
	TIME [epoch: 10.3 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.173885628920042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.173885628920042 | validation: 1.9562546400372651]
	TIME [epoch: 10.3 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.838660207415752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.838660207415752 | validation: 2.884789214925562]
	TIME [epoch: 10.3 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2546740600335253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2546740600335253 | validation: 3.21392050333086]
	TIME [epoch: 10.3 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4796542987206265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4796542987206265 | validation: 3.513830237849526]
	TIME [epoch: 10.3 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7619397410298463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7619397410298463 | validation: 3.209921007987258]
	TIME [epoch: 10.3 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1011024675932424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1011024675932424 | validation: 2.857567159879407]
	TIME [epoch: 10.3 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4680660371152094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4680660371152094 | validation: 2.789711466613627]
	TIME [epoch: 10.3 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.397275416514185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.397275416514185 | validation: 2.7117230333712263]
	TIME [epoch: 10.3 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9867422242988035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9867422242988035 | validation: 3.8827422666400104]
	TIME [epoch: 10.3 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.363494711412662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.363494711412662 | validation: 2.0557870824336244]
	TIME [epoch: 10.3 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3086081625826465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3086081625826465 | validation: 2.1399068759035336]
	TIME [epoch: 10.3 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.284571338290271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.284571338290271 | validation: 2.319841942708751]
	TIME [epoch: 10.3 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6407196757306672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6407196757306672 | validation: 3.142389700649618]
	TIME [epoch: 10.3 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.941334647499558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.941334647499558 | validation: 3.602115162711002]
	TIME [epoch: 10.3 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2887009186370184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2887009186370184 | validation: 2.5941314202706343]
	TIME [epoch: 10.3 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3328895618463634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3328895618463634 | validation: 3.016425062147428]
	TIME [epoch: 10.3 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2025316279635687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2025316279635687 | validation: 2.859421663901868]
	TIME [epoch: 10.3 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.798564562076531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.798564562076531 | validation: 2.9836780249722885]
	TIME [epoch: 10.3 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.008859102502937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.008859102502937 | validation: 3.4593029184969106]
	TIME [epoch: 10.3 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0967948079340206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0967948079340206 | validation: 2.836933044652984]
	TIME [epoch: 10.3 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.724348559366488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.724348559366488 | validation: 3.766513506805886]
	TIME [epoch: 10.3 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.445747577246402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.445747577246402 | validation: 2.7748156862220856]
	TIME [epoch: 10.3 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9540061109053144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9540061109053144 | validation: 2.6334174462481066]
	TIME [epoch: 10.3 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3036467361860877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3036467361860877 | validation: 2.6436134417531822]
	TIME [epoch: 10.3 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0758624556346335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0758624556346335 | validation: 3.4936445483857006]
	TIME [epoch: 10.3 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.566824598800804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.566824598800804 | validation: 2.6060145995631134]
	TIME [epoch: 10.3 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.156657513322744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.156657513322744 | validation: 2.737822423582579]
	TIME [epoch: 10.3 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.155667818287825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.155667818287825 | validation: 2.955902115981717]
	TIME [epoch: 10.3 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2589051635642634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2589051635642634 | validation: 3.0033403100488214]
	TIME [epoch: 10.3 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3456287375482128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3456287375482128 | validation: 3.159636737575445]
	TIME [epoch: 10.3 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3640561506536293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3640561506536293 | validation: 3.3945377416092333]
	TIME [epoch: 10.3 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8763824606438995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8763824606438995 | validation: 3.697288017482597]
	TIME [epoch: 10.3 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.540352479531594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.540352479531594 | validation: 2.927349944750559]
	TIME [epoch: 10.3 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3268932603317922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3268932603317922 | validation: 2.9511998029068383]
	TIME [epoch: 10.3 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.210627650248287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.210627650248287 | validation: 2.660349745106158]
	TIME [epoch: 10.3 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.222371565308717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.222371565308717 | validation: 3.0543726874928825]
	TIME [epoch: 10.3 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.551750931100842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.551750931100842 | validation: 2.741642268211581]
	TIME [epoch: 10.3 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.305849223993569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.305849223993569 | validation: 2.61156491541996]
	TIME [epoch: 10.3 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500384614344411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.500384614344411 | validation: 3.0804618238289483]
	TIME [epoch: 10.3 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3627938921546607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3627938921546607 | validation: 3.3163920725923686]
	TIME [epoch: 10.3 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6519315860021373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6519315860021373 | validation: 3.8194632712943384]
	TIME [epoch: 10.3 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4844158026333703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4844158026333703 | validation: 2.6581334250224495]
	TIME [epoch: 10.3 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.285697398124115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.285697398124115 | validation: 3.310511251331976]
	TIME [epoch: 10.3 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5614128772512372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5614128772512372 | validation: 2.600231761831188]
	TIME [epoch: 10.3 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9120244557606134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9120244557606134 | validation: 2.37160152722187]
	TIME [epoch: 10.3 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0416707480246554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0416707480246554 | validation: 2.5670968943752586]
	TIME [epoch: 10.3 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949507176492755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.949507176492755 | validation: 2.400240091109026]
	TIME [epoch: 10.3 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8233238611692357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8233238611692357 | validation: 2.5349470614636354]
	TIME [epoch: 10.3 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9875133599520916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9875133599520916 | validation: 2.300220807790471]
	TIME [epoch: 10.3 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.112138183143572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.112138183143572 | validation: 3.727341601553208]
	TIME [epoch: 10.3 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6204461496870812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6204461496870812 | validation: 3.1538931142099194]
	TIME [epoch: 10.3 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.102260556689624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.102260556689624 | validation: 2.4379967345551283]
	TIME [epoch: 10.3 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.710492461069191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.710492461069191 | validation: 2.2979725222893963]
	TIME [epoch: 10.3 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.81882693876874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.81882693876874 | validation: 2.685473058618216]
	TIME [epoch: 10.3 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0326932028115983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0326932028115983 | validation: 2.209552258897919]
	TIME [epoch: 10.3 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7767357497069174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7767357497069174 | validation: 2.920562381132846]
	TIME [epoch: 10.3 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0783089394459453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0783089394459453 | validation: 2.6111941194047312]
	TIME [epoch: 10.3 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0374814605722076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0374814605722076 | validation: 2.3885019673730072]
	TIME [epoch: 10.3 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.902190189743263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.902190189743263 | validation: 2.371014591685746]
	TIME [epoch: 10.3 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2312294252055134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2312294252055134 | validation: 2.5945619923698167]
	TIME [epoch: 10.3 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2792516639463294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2792516639463294 | validation: 2.6762075718131544]
	TIME [epoch: 10.3 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2077876984516727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2077876984516727 | validation: 2.431064506902605]
	TIME [epoch: 10.3 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8349697699498906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8349697699498906 | validation: 2.4204426316968948]
	TIME [epoch: 10.3 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5661822842148503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5661822842148503 | validation: 3.0611266593696564]
	TIME [epoch: 10.3 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.459210471403999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.459210471403999 | validation: 3.9538589098912946]
	TIME [epoch: 10.3 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.405705468431951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.405705468431951 | validation: 3.1606915387792447]
	TIME [epoch: 10.3 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1744409677159937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1744409677159937 | validation: 2.378646354918304]
	TIME [epoch: 10.3 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7260000460420573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7260000460420573 | validation: 2.3364966445538835]
	TIME [epoch: 10.3 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9783021032467647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9783021032467647 | validation: 2.597013098625356]
	TIME [epoch: 10.3 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9769979548502103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9769979548502103 | validation: 2.4069715369892557]
	TIME [epoch: 10.3 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.705157969313374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.705157969313374 | validation: 3.1730444529777735]
	TIME [epoch: 10.3 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7388884778960305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7388884778960305 | validation: 2.7812773867483265]
	TIME [epoch: 10.3 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.580263306853046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.580263306853046 | validation: 2.625271690003358]
	TIME [epoch: 10.3 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.054836852741309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.054836852741309 | validation: 2.6261930351884604]
	TIME [epoch: 10.3 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0154959407195507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0154959407195507 | validation: 2.52241883756572]
	TIME [epoch: 10.3 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.926996230947304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.926996230947304 | validation: 2.6116798326227046]
	TIME [epoch: 10.3 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7882502141485057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7882502141485057 | validation: 2.271966033797437]
	TIME [epoch: 10.3 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1864764258978977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1864764258978977 | validation: 1.898392731062345]
	TIME [epoch: 10.3 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9851583914111663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9851583914111663 | validation: 1.9226788811415847]
	TIME [epoch: 10.3 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9418640695636722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9418640695636722 | validation: 1.8716573000600298]
	TIME [epoch: 10.3 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.634683990046875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.634683990046875 | validation: 3.1709219802691515]
	TIME [epoch: 10.3 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4776659337259077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4776659337259077 | validation: 2.768294777744269]
	TIME [epoch: 10.3 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.005578566445902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.005578566445902 | validation: 2.838801624328175]
	TIME [epoch: 10.3 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4744680253031164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4744680253031164 | validation: 3.4792909528360063]
	TIME [epoch: 10.3 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.479902493643192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.479902493643192 | validation: 3.515287213881543]
	TIME [epoch: 10.3 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3659155661174758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3659155661174758 | validation: 2.571071470743392]
	TIME [epoch: 10.3 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.102626508554765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.102626508554765 | validation: 2.435701581581114]
	TIME [epoch: 10.3 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.191263304666917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.191263304666917 | validation: 2.9582272656574697]
	TIME [epoch: 10.3 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7728658226558567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7728658226558567 | validation: 2.9676183333790185]
	TIME [epoch: 10.3 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5514593262137657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5514593262137657 | validation: 2.5463188169581152]
	TIME [epoch: 10.3 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.035682983719126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.035682983719126 | validation: 3.416807739106746]
	TIME [epoch: 10.3 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3957344118368535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3957344118368535 | validation: 5.042674653389837]
	TIME [epoch: 10.3 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3712753339984065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3712753339984065 | validation: 3.03497304811269]
	TIME [epoch: 10.3 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.939809208564963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.939809208564963 | validation: 2.8247896095726923]
	TIME [epoch: 10.3 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.881425083653598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.881425083653598 | validation: 2.2848110461039526]
	TIME [epoch: 10.3 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1195845418204007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1195845418204007 | validation: 1.8897095823711119]
	TIME [epoch: 10.3 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0639957836174516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0639957836174516 | validation: 2.907585548505381]
	TIME [epoch: 10.3 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.100495134488825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.100495134488825 | validation: 2.164351893115015]
	TIME [epoch: 10.3 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.866234552842204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.866234552842204 | validation: 1.7581883715275648]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8614117737073752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8614117737073752 | validation: 1.8737262570936326]
	TIME [epoch: 10.3 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7773731906381252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7773731906381252 | validation: 1.6412533764636639]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6611773891989583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6611773891989583 | validation: 1.6134793565184173]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5911829255728485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5911829255728485 | validation: 1.6399521648067241]
	TIME [epoch: 10.3 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.551096092466563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.551096092466563 | validation: 1.9359418827860275]
	TIME [epoch: 10.3 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7644106183269055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7644106183269055 | validation: 1.385991006099299]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5449593890183455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5449593890183455 | validation: 1.4562676322419634]
	TIME [epoch: 10.3 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6731830021824083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6731830021824083 | validation: 2.1665809141881303]
	TIME [epoch: 10.3 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.65107355160543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.65107355160543 | validation: 1.3423149358786066]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.350546504183567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.350546504183567 | validation: 1.3315611475229017]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3456495769687424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3456495769687424 | validation: 1.1491703768927495]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4375015798337487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4375015798337487 | validation: 1.212593048992377]
	TIME [epoch: 10.3 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2671508890679897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2671508890679897 | validation: 1.1182501483663672]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3895413457746844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3895413457746844 | validation: 1.0778797593246119]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.437649698116503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.437649698116503 | validation: 1.4228310829908453]
	TIME [epoch: 10.3 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2743638853301924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2743638853301924 | validation: 1.0663701793136102]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1812209641671418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1812209641671418 | validation: 1.6736732553976583]
	TIME [epoch: 10.3 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4060821831334085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4060821831334085 | validation: 1.1418604999181066]
	TIME [epoch: 10.3 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1954804581668466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1954804581668466 | validation: 1.3887554727242821]
	TIME [epoch: 10.3 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.094348867112737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.094348867112737 | validation: 0.9254837520482587]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9605023864556538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9605023864556538 | validation: 1.2480433395595965]
	TIME [epoch: 10.3 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0923872671384935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0923872671384935 | validation: 1.4187206448593497]
	TIME [epoch: 10.3 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1356169637654747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1356169637654747 | validation: 0.8479374206666239]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0419570622833443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0419570622833443 | validation: 1.0104685812970762]
	TIME [epoch: 10.3 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.152593260845999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.152593260845999 | validation: 1.4188165275940794]
	TIME [epoch: 10.3 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3049111334351224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3049111334351224 | validation: 1.3395500204050566]
	TIME [epoch: 10.3 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2574542924532457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2574542924532457 | validation: 1.1287232924686839]
	TIME [epoch: 10.3 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.224887104132445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.224887104132445 | validation: 0.989748522484409]
	TIME [epoch: 10.3 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1248381247953483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1248381247953483 | validation: 0.9317763524222]
	TIME [epoch: 10.3 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.059492950200184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.059492950200184 | validation: 1.0230802355930317]
	TIME [epoch: 10.3 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1388507408378428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1388507408378428 | validation: 0.9985648584306538]
	TIME [epoch: 10.3 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2831308359161595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2831308359161595 | validation: 1.0457647637934782]
	TIME [epoch: 10.3 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1816790256361642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1816790256361642 | validation: 0.9564888129158637]
	TIME [epoch: 10.3 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0515633703187108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0515633703187108 | validation: 1.014069446174922]
	TIME [epoch: 10.3 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0285236146421624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0285236146421624 | validation: 1.3462925139484834]
	TIME [epoch: 10.3 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4313073697153436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4313073697153436 | validation: 1.585191191160686]
	TIME [epoch: 10.3 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1209829341117117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1209829341117117 | validation: 0.7905379412917553]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.89012203101165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.89012203101165 | validation: 0.7327795688300961]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8423036205831955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8423036205831955 | validation: 0.7804939395044947]
	TIME [epoch: 10.3 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7532141882901158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7532141882901158 | validation: 4.424775298908239]
	TIME [epoch: 10.3 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.106512428225519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.106512428225519 | validation: 4.2540251559555875]
	TIME [epoch: 10.3 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9324223329156154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9324223329156154 | validation: 4.280451861307699]
	TIME [epoch: 10.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.826146198839828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.826146198839828 | validation: 4.221844659286732]
	TIME [epoch: 10.3 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.899349497821084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.899349497821084 | validation: 4.225135530048965]
	TIME [epoch: 10.3 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.822656910008965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.822656910008965 | validation: 4.233064558229092]
	TIME [epoch: 10.3 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.44986806723318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.44986806723318 | validation: 4.200705086478049]
	TIME [epoch: 10.3 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.693009499032832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.693009499032832 | validation: 0.8459078975426476]
	TIME [epoch: 10.3 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7769916416590462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7769916416590462 | validation: 0.7483591622357171]
	TIME [epoch: 10.3 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0441731667256817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0441731667256817 | validation: 1.1688069849207154]
	TIME [epoch: 10.3 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0328643857084672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0328643857084672 | validation: 0.976826992585813]
	TIME [epoch: 10.3 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7834290157264121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7834290157264121 | validation: 0.7517788375871726]
	TIME [epoch: 10.3 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.782970841331407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.782970841331407 | validation: 0.9841955827129049]
	TIME [epoch: 10.3 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1920220733764226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1920220733764226 | validation: 0.837503837062802]
	TIME [epoch: 10.3 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7496824447786575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7496824447786575 | validation: 0.9358482769496326]
	TIME [epoch: 10.3 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8350350158473729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8350350158473729 | validation: 0.8446153704958509]
	TIME [epoch: 10.3 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7943577448475627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7943577448475627 | validation: 0.8032221333853755]
	TIME [epoch: 10.3 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8093392414152643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8093392414152643 | validation: 1.0733922636597317]
	TIME [epoch: 10.3 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7473117730942382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7473117730942382 | validation: 0.7843443684406143]
	TIME [epoch: 10.3 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0793721093761088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0793721093761088 | validation: 2.3543361018078985]
	TIME [epoch: 10.3 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0894453075841035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0894453075841035 | validation: 0.9458337807157517]
	TIME [epoch: 10.3 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9523082006327561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9523082006327561 | validation: 0.8528536221227316]
	TIME [epoch: 10.3 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8937294287525765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8937294287525765 | validation: 0.980615294967898]
	TIME [epoch: 10.3 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7983379059532432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7983379059532432 | validation: 0.681449914859071]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6611166777092787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6611166777092787 | validation: 0.5566287522370424]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7028590070492495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7028590070492495 | validation: 1.0001405131542287]
	TIME [epoch: 10.3 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8331923264198359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8331923264198359 | validation: 0.703037094374988]
	TIME [epoch: 10.3 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0651125394646406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0651125394646406 | validation: 0.9113044852137262]
	TIME [epoch: 10.3 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8314152290814147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8314152290814147 | validation: 0.7960722867118096]
	TIME [epoch: 10.3 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7808314570724659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7808314570724659 | validation: 0.6769865042827959]
	TIME [epoch: 10.3 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0468108606434101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0468108606434101 | validation: 0.8472310060194745]
	TIME [epoch: 10.3 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8265894121053616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8265894121053616 | validation: 0.6694023913406401]
	TIME [epoch: 10.3 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6442035469920789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6442035469920789 | validation: 1.139733259180528]
	TIME [epoch: 10.3 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2730339695232988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2730339695232988 | validation: 2.733599839588082]
	TIME [epoch: 10.3 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9733313180163248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9733313180163248 | validation: 2.3737604907211742]
	TIME [epoch: 10.3 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9749806312170493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9749806312170493 | validation: 1.2834565065278076]
	TIME [epoch: 10.3 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0013210536769708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0013210536769708 | validation: 0.794615692847879]
	TIME [epoch: 10.3 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9376906443131283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9376906443131283 | validation: 0.8923407189150645]
	TIME [epoch: 10.3 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6837482949174396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6837482949174396 | validation: 0.9329612432992079]
	TIME [epoch: 10.3 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6840426745464357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6840426745464357 | validation: 0.6633804542384394]
	TIME [epoch: 10.3 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8020172215037491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8020172215037491 | validation: 1.1031816837738215]
	TIME [epoch: 10.3 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0096316523522553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0096316523522553 | validation: 0.8732845992811538]
	TIME [epoch: 10.3 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8802399125335552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8802399125335552 | validation: 0.6781673163067659]
	TIME [epoch: 10.3 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7112558099041955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7112558099041955 | validation: 1.4511298961627734]
	TIME [epoch: 10.3 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9105428812422833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9105428812422833 | validation: 0.8003807853320143]
	TIME [epoch: 10.3 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7188150799264359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7188150799264359 | validation: 0.5483867956583682]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8272384344157144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8272384344157144 | validation: 0.7976267354264848]
	TIME [epoch: 10.3 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7579204590534646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7579204590534646 | validation: 0.630765139595672]
	TIME [epoch: 10.3 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6377943176292914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6377943176292914 | validation: 0.6580699053292084]
	TIME [epoch: 10.3 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6237544441477028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6237544441477028 | validation: 0.5619782607896034]
	TIME [epoch: 10.3 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5642977363132033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5642977363132033 | validation: 2.4327526780930953]
	TIME [epoch: 10.3 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7142326589539327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7142326589539327 | validation: 0.7210208437246671]
	TIME [epoch: 10.3 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1556669977002674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1556669977002674 | validation: 0.6198776794565518]
	TIME [epoch: 10.3 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6133018669673735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6133018669673735 | validation: 0.5433564745296473]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5843268527888487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5843268527888487 | validation: 0.9643635079179507]
	TIME [epoch: 10.3 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.783661158448869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.783661158448869 | validation: 0.7177236539333695]
	TIME [epoch: 10.3 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9210985954491555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9210985954491555 | validation: 0.638010027600798]
	TIME [epoch: 10.3 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8168450406327304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8168450406327304 | validation: 0.7676179137867797]
	TIME [epoch: 10.3 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.716477422756529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.716477422756529 | validation: 0.7521008802991194]
	TIME [epoch: 10.3 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7090633048809165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7090633048809165 | validation: 0.8283639527402147]
	TIME [epoch: 10.3 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6027646794881483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6027646794881483 | validation: 1.3703008566501007]
	TIME [epoch: 10.3 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8778120469892293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8778120469892293 | validation: 0.6519093657672553]
	TIME [epoch: 10.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8319308629518162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8319308629518162 | validation: 0.7201866251169458]
	TIME [epoch: 10.3 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6614950659521371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6614950659521371 | validation: 0.5658620150529096]
	TIME [epoch: 10.3 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6268711781757714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6268711781757714 | validation: 0.5832721378803002]
	TIME [epoch: 10.3 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5601408623712388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5601408623712388 | validation: 1.0927153470472573]
	TIME [epoch: 10.3 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9755237333219753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9755237333219753 | validation: 0.42057330937950377]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7494535286395824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7494535286395824 | validation: 0.8702369850015933]
	TIME [epoch: 10.3 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6123373913447361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6123373913447361 | validation: 0.7602001504494975]
	TIME [epoch: 10.3 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6630987168116225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6630987168116225 | validation: 0.7272621637521527]
	TIME [epoch: 10.3 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6612175007999441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6612175007999441 | validation: 0.5258235235340248]
	TIME [epoch: 10.3 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7484860482005546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7484860482005546 | validation: 0.4950357910921128]
	TIME [epoch: 10.3 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.621579204193149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.621579204193149 | validation: 0.6044859382837454]
	TIME [epoch: 10.3 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6994404957836587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6994404957836587 | validation: 1.0012243796256828]
	TIME [epoch: 10.3 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8109881583207423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8109881583207423 | validation: 0.820407688100043]
	TIME [epoch: 10.3 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6314451316806393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6314451316806393 | validation: 1.0121797439197888]
	TIME [epoch: 10.3 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8031675037507335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8031675037507335 | validation: 0.797904296837753]
	TIME [epoch: 10.3 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5868721971687212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5868721971687212 | validation: 0.427866326367857]
	TIME [epoch: 10.3 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6153817003801232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6153817003801232 | validation: 0.5632398065749671]
	TIME [epoch: 10.3 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5621162401227482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5621162401227482 | validation: 0.5897796941838505]
	TIME [epoch: 10.3 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6171600182257001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6171600182257001 | validation: 0.4985022427560502]
	TIME [epoch: 10.3 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5668046445908603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5668046445908603 | validation: 0.6940391150853432]
	TIME [epoch: 10.3 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5989425174295767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5989425174295767 | validation: 0.4019644430108562]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5312236853081955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5312236853081955 | validation: 0.5095854391498625]
	TIME [epoch: 10.3 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5519172028922456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5519172028922456 | validation: 0.5677771950223901]
	TIME [epoch: 10.3 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5026331307362855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5026331307362855 | validation: 0.5909187361527115]
	TIME [epoch: 10.3 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6110513526801583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6110513526801583 | validation: 0.8990476354317657]
	TIME [epoch: 10.3 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.733744527496039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.733744527496039 | validation: 0.5442129681799039]
	TIME [epoch: 10.3 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8322554434886964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8322554434886964 | validation: 0.49489639431916327]
	TIME [epoch: 10.3 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4243989506133024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4243989506133024 | validation: 1.8416143144692985]
	TIME [epoch: 10.3 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0321077252690032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0321077252690032 | validation: 0.5382599020528903]
	TIME [epoch: 10.3 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5742255641848585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5742255641848585 | validation: 0.8730402628322914]
	TIME [epoch: 10.3 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8528161006943746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8528161006943746 | validation: 1.5609414130865527]
	TIME [epoch: 10.3 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0700458943490756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0700458943490756 | validation: 0.5066408403618854]
	TIME [epoch: 10.3 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5246461787397789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5246461787397789 | validation: 0.5067334662095951]
	TIME [epoch: 10.3 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5118282472089369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5118282472089369 | validation: 0.4727170200178116]
	TIME [epoch: 10.3 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8818393277592449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8818393277592449 | validation: 1.5583603952999534]
	TIME [epoch: 10.3 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.323557234353815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.323557234353815 | validation: 1.0528502683349674]
	TIME [epoch: 10.3 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8349995602038328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8349995602038328 | validation: 1.0555295864907148]
	TIME [epoch: 10.3 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.969748942514887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.969748942514887 | validation: 0.8257114561739493]
	TIME [epoch: 10.3 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5853723594909057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5853723594909057 | validation: 0.44588916751144203]
	TIME [epoch: 10.3 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40925520912006713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40925520912006713 | validation: 0.5064579727144087]
	TIME [epoch: 10.3 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6375684680551641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6375684680551641 | validation: 0.7672428471860597]
	TIME [epoch: 10.3 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6205353723009117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6205353723009117 | validation: 0.508754179941635]
	TIME [epoch: 10.3 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5609841028863742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5609841028863742 | validation: 1.017607581598666]
	TIME [epoch: 10.3 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8114443359318168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8114443359318168 | validation: 0.5546958612106087]
	TIME [epoch: 10.3 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6521628762898926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6521628762898926 | validation: 0.680914423251121]
	TIME [epoch: 10.3 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.562434742146259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.562434742146259 | validation: 0.8161663170089604]
	TIME [epoch: 10.3 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6026676231822654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6026676231822654 | validation: 0.721796108020937]
	TIME [epoch: 10.3 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5901457847296627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5901457847296627 | validation: 0.690888586493505]
	TIME [epoch: 10.3 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7363777786409039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7363777786409039 | validation: 0.7456615836369673]
	TIME [epoch: 10.3 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7057845285342612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7057845285342612 | validation: 0.5722467112908766]
	TIME [epoch: 10.3 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.001423741193834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.001423741193834 | validation: 1.3796579504390183]
	TIME [epoch: 10.3 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2456725022895447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2456725022895447 | validation: 0.7307195379304173]
	TIME [epoch: 10.3 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6653269612256529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6653269612256529 | validation: 0.5809750149507583]
	TIME [epoch: 10.3 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.784209037287934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.784209037287934 | validation: 0.7027966110661186]
	TIME [epoch: 10.3 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9358435318366911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9358435318366911 | validation: 0.8191036897347425]
	TIME [epoch: 10.3 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2121170496482603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2121170496482603 | validation: 0.7473881853228779]
	TIME [epoch: 10.3 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.662539972996426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.662539972996426 | validation: 0.9040637773757672]
	TIME [epoch: 10.3 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5057186006704257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5057186006704257 | validation: 0.6530442908136146]
	TIME [epoch: 10.3 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7039658694901674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7039658694901674 | validation: 0.5438608987863482]
	TIME [epoch: 10.3 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7612604614832106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7612604614832106 | validation: 0.513421615524959]
	TIME [epoch: 10.3 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5814879593249183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5814879593249183 | validation: 0.46832482236486833]
	TIME [epoch: 10.3 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9790067609598072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9790067609598072 | validation: 0.6217606551147937]
	TIME [epoch: 10.3 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8945043143718026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8945043143718026 | validation: 0.7255589101748271]
	TIME [epoch: 10.3 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9631046551022999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9631046551022999 | validation: 1.2397068493248151]
	TIME [epoch: 10.3 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7999524327013875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7999524327013875 | validation: 0.7366403639611702]
	TIME [epoch: 10.3 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6361611408380776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6361611408380776 | validation: 1.839967060326785]
	TIME [epoch: 10.3 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6193481582625815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6193481582625815 | validation: 0.7617697422448455]
	TIME [epoch: 10.3 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6482171934723902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6482171934723902 | validation: 0.8163618149597603]
	TIME [epoch: 10.3 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6213211069398572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6213211069398572 | validation: 0.5393265632651313]
	TIME [epoch: 10.3 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5278879262700269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5278879262700269 | validation: 0.43287538785351504]
	TIME [epoch: 10.3 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7903392489559518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7903392489559518 | validation: 0.6381060473126615]
	TIME [epoch: 10.3 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.766082151557349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.766082151557349 | validation: 0.7214514745034893]
	TIME [epoch: 10.3 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7227747100150002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7227747100150002 | validation: 0.7446618866694539]
	TIME [epoch: 10.3 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6951474635201987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6951474635201987 | validation: 0.7168283706935313]
	TIME [epoch: 10.3 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9605538223467021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9605538223467021 | validation: 0.6719196775645423]
	TIME [epoch: 10.3 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.652241246696774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.652241246696774 | validation: 0.7819951361317214]
	TIME [epoch: 10.3 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4704804979344258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4704804979344258 | validation: 0.7195947059380771]
	TIME [epoch: 10.3 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6456481708635805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6456481708635805 | validation: 1.394387563875597]
	TIME [epoch: 10.3 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7176625341732588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7176625341732588 | validation: 1.2069580118696992]
	TIME [epoch: 10.3 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.789972519408317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.789972519408317 | validation: 0.42295023466741855]
	TIME [epoch: 10.3 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6332766281834583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6332766281834583 | validation: 1.3357627722206462]
	TIME [epoch: 10.3 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3893816564136114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3893816564136114 | validation: 1.0810318966059589]
	TIME [epoch: 10.3 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1103374476303165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1103374476303165 | validation: 0.4504403547885895]
	TIME [epoch: 10.3 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5009748076780507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5009748076780507 | validation: 0.5260427626763764]
	TIME [epoch: 10.3 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5328714550006068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5328714550006068 | validation: 1.7068109890629166]
	TIME [epoch: 10.3 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2393757815826967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2393757815826967 | validation: 0.3312144455352488]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5233709981781685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5233709981781685 | validation: 0.8704394879735391]
	TIME [epoch: 10.3 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6151680104474021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6151680104474021 | validation: 0.5177598494549165]
	TIME [epoch: 10.3 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48707238848006557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48707238848006557 | validation: 0.5302716801786521]
	TIME [epoch: 10.3 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7554917067866505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7554917067866505 | validation: 0.5024217136712754]
	TIME [epoch: 10.3 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.580146061019415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.580146061019415 | validation: 0.6976995990561611]
	TIME [epoch: 10.3 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1122600589298102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1122600589298102 | validation: 0.9102180715767628]
	TIME [epoch: 10.3 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6330414682278196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6330414682278196 | validation: 0.5932784230602528]
	TIME [epoch: 10.3 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5766668728600929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5766668728600929 | validation: 0.6200542933365859]
	TIME [epoch: 10.3 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5050191268656955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5050191268656955 | validation: 0.6653875535369782]
	TIME [epoch: 10.3 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46368288777904165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46368288777904165 | validation: 0.23133397265613634]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r2_20240217_141715/states/model_tr_study6_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32907420258272124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32907420258272124 | validation: 0.5858881691319938]
	TIME [epoch: 10.3 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5988766574558874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5988766574558874 | validation: 0.819424737151518]
	TIME [epoch: 10.3 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7201950261034585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7201950261034585 | validation: 0.5908782608862422]
	TIME [epoch: 10.3 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1633197605200665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1633197605200665 | validation: 1.2532237198144547]
	TIME [epoch: 10.3 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7886488611356542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7886488611356542 | validation: 0.3783641342609771]
	TIME [epoch: 10.3 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38382754823609794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38382754823609794 | validation: 0.733153044545732]
	TIME [epoch: 10.3 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4968412421237117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4968412421237117 | validation: 0.42890533214573906]
	TIME [epoch: 10.3 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.510845391867121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.510845391867121 | validation: 0.23262582862109604]
	TIME [epoch: 10.3 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5408373512994218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5408373512994218 | validation: 0.4885579562584816]
	TIME [epoch: 10.3 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5518979181888183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5518979181888183 | validation: 0.4344554112786154]
	TIME [epoch: 10.3 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40772919704099336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40772919704099336 | validation: 0.6513524397183318]
	TIME [epoch: 10.3 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9860373827959142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9860373827959142 | validation: 0.5627839621832817]
	TIME [epoch: 10.3 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5844139114133988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5844139114133988 | validation: 1.4464819076032966]
	TIME [epoch: 10.3 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8772481496948363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8772481496948363 | validation: 1.1247795445566267]
	TIME [epoch: 10.3 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9402585664952943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9402585664952943 | validation: 0.5449674031438249]
	TIME [epoch: 10.3 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5435253565208273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5435253565208273 | validation: 2.7894001581840575]
	TIME [epoch: 10.3 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8382828492432775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8382828492432775 | validation: 0.8734133830387353]
	TIME [epoch: 10.3 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9117972440864808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9117972440864808 | validation: 0.35039370522128804]
	TIME [epoch: 10.3 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4715365146735747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4715365146735747 | validation: 1.481919103629046]
	TIME [epoch: 10.3 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0389212258174616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0389212258174616 | validation: 0.5424837515715447]
	TIME [epoch: 10.3 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5685209381845512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5685209381845512 | validation: 1.1906268515028582]
	TIME [epoch: 10.3 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8111662159893654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8111662159893654 | validation: 0.7318871133481841]
	TIME [epoch: 10.3 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4992109510553678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4992109510553678 | validation: 0.3248307592880083]
	TIME [epoch: 10.3 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4897580803616786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4897580803616786 | validation: 0.3611575927816783]
	TIME [epoch: 10.3 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36871873005668315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36871873005668315 | validation: 1.0603668542410938]
	TIME [epoch: 10.3 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.898632675063453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.898632675063453 | validation: 1.2799123165201693]
	TIME [epoch: 10.3 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9525856475494885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9525856475494885 | validation: 0.42652207496868444]
	TIME [epoch: 10.3 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7778386698887692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7778386698887692 | validation: 0.686702952148322]
	TIME [epoch: 10.3 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7564689523524991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7564689523524991 | validation: 0.4939199811032834]
	TIME [epoch: 10.3 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9982601621966289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9982601621966289 | validation: 0.7416256422267022]
	TIME [epoch: 10.3 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5643690993089905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5643690993089905 | validation: 0.5240749434736869]
	TIME [epoch: 10.3 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4375299338182117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4375299338182117 | validation: 1.0924056678638012]
	TIME [epoch: 10.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7711742757589132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7711742757589132 | validation: 0.5053242917298498]
	TIME [epoch: 10.3 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5256741421107518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5256741421107518 | validation: 0.5762829927758142]
	TIME [epoch: 10.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6256310273764295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6256310273764295 | validation: 1.2708020885061595]
	TIME [epoch: 10.3 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1821703947057478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1821703947057478 | validation: 0.7401280854457665]
	TIME [epoch: 10.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7561472937216726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7561472937216726 | validation: 0.7684382288730188]
	TIME [epoch: 10.3 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8115388089136303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8115388089136303 | validation: 0.9202097519165284]
	TIME [epoch: 10.3 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8702970776869184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8702970776869184 | validation: 0.5863259666275651]
	TIME [epoch: 10.3 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8170691093773949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8170691093773949 | validation: 0.8579028055676824]
	TIME [epoch: 10.3 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.773510074415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.773510074415 | validation: 0.5729752681768753]
	TIME [epoch: 10.3 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7156777184558294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7156777184558294 | validation: 0.471631803360532]
	TIME [epoch: 10.3 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7378781243768296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7378781243768296 | validation: 0.882568848327931]
	TIME [epoch: 10.3 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6705828716823687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6705828716823687 | validation: 0.28725429063476887]
	TIME [epoch: 10.3 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43626306005812693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43626306005812693 | validation: 0.3796852661339812]
	TIME [epoch: 10.3 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6294343500662511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6294343500662511 | validation: 1.55917939587885]
	TIME [epoch: 10.3 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4385679519790322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4385679519790322 | validation: 0.8672475173153353]
	TIME [epoch: 10.3 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6769174964382207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6769174964382207 | validation: 0.3870121927454866]
	TIME [epoch: 10.3 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4499979133075612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4499979133075612 | validation: 0.5535242698018529]
	TIME [epoch: 10.3 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5049463601420949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5049463601420949 | validation: 0.8958683929260707]
	TIME [epoch: 10.3 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5879786515981098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5879786515981098 | validation: 0.5566601778653887]
	TIME [epoch: 10.3 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48199400504254636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48199400504254636 | validation: 0.3402398165302155]
	TIME [epoch: 10.3 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3780825231556335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3780825231556335 | validation: 0.2531071875220956]
	TIME [epoch: 10.3 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4555172820036447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4555172820036447 | validation: 0.2758234068632441]
	TIME [epoch: 10.3 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3746746449894701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3746746449894701 | validation: 0.4254905455636555]
	TIME [epoch: 10.3 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6393205920460276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6393205920460276 | validation: 0.36499221225606654]
	TIME [epoch: 10.3 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.118311253706412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.118311253706412 | validation: 1.0111397243577926]
	TIME [epoch: 10.3 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9096175143343972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9096175143343972 | validation: 0.3380481250915735]
	TIME [epoch: 10.3 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5009592559149593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5009592559149593 | validation: 0.3335448851931433]
	TIME [epoch: 10.3 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.350226041075698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.350226041075698 | validation: 0.9136511471779347]
	TIME [epoch: 10.3 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.607975635004758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.607975635004758 | validation: 0.7072707161210853]
	TIME [epoch: 10.3 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2439364979589629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2439364979589629 | validation: 0.7976845619801154]
	TIME [epoch: 10.3 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4646566041881844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4646566041881844 | validation: 0.770985895627197]
	TIME [epoch: 10.3 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6896667246136553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6896667246136553 | validation: 0.5455541694753034]
	TIME [epoch: 10.3 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5183505844255274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5183505844255274 | validation: 0.64946181159135]
	TIME [epoch: 10.3 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5164962874475723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5164962874475723 | validation: 0.41342452614742514]
	TIME [epoch: 10.3 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6518197074795504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6518197074795504 | validation: 0.6026813149779824]
	TIME [epoch: 10.3 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5880084146711374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5880084146711374 | validation: 1.2630174037940871]
	TIME [epoch: 10.3 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7379799133215342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7379799133215342 | validation: 0.2840899073623046]
	TIME [epoch: 10.3 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34536188633289966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34536188633289966 | validation: 0.3136835299882424]
	TIME [epoch: 10.3 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5409915685198936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5409915685198936 | validation: 0.38709586997130857]
	TIME [epoch: 10.3 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5559440686646528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5559440686646528 | validation: 1.338490542056937]
	TIME [epoch: 10.3 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7643269232818216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7643269232818216 | validation: 0.5341693483496912]
	TIME [epoch: 10.3 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6370011345436246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6370011345436246 | validation: 1.5761686052751063]
	TIME [epoch: 10.3 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9139350102890174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9139350102890174 | validation: 0.29745538498141594]
	TIME [epoch: 10.3 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38182891820897125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38182891820897125 | validation: 0.4037622337582137]
	TIME [epoch: 10.3 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5346472438166059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5346472438166059 | validation: 0.6011150187026492]
	TIME [epoch: 10.3 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4410001423134225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4410001423134225 | validation: 0.46230525827285546]
	TIME [epoch: 10.3 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44955361490284174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44955361490284174 | validation: 0.3046182692439241]
	TIME [epoch: 10.3 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4301378228354701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4301378228354701 | validation: 0.23541807842661677]
	TIME [epoch: 10.3 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5464070870729407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5464070870729407 | validation: 1.0146128144255584]
	TIME [epoch: 10.3 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7985676368319943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7985676368319943 | validation: 1.125605198220183]
	TIME [epoch: 10.3 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8614567561208556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8614567561208556 | validation: 0.8980300639795863]
	TIME [epoch: 10.3 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6775850123532485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6775850123532485 | validation: 0.3622996869863292]
	TIME [epoch: 10.3 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6371969220419407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6371969220419407 | validation: 0.9578936183751234]
	TIME [epoch: 10.3 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.470017937610292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.470017937610292 | validation: 0.8024339950333232]
	TIME [epoch: 10.3 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9258358847682431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9258358847682431 | validation: 1.069159568539253]
	TIME [epoch: 10.3 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9280897102217228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9280897102217228 | validation: 0.8543470878265649]
	TIME [epoch: 10.3 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9010029975796974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9010029975796974 | validation: 1.4736641970856024]
	TIME [epoch: 10.3 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7210891088691571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7210891088691571 | validation: 0.3852135508422677]
	TIME [epoch: 10.3 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5301467054838213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5301467054838213 | validation: 0.4373466981390247]
	TIME [epoch: 10.3 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37573991905348236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37573991905348236 | validation: 0.3636999505369726]
	TIME [epoch: 10.3 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9170230036952436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9170230036952436 | validation: 1.2143735899503654]
	TIME [epoch: 10.3 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3631546330654714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3631546330654714 | validation: 1.041773355480642]
	TIME [epoch: 10.3 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7142698721282235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7142698721282235 | validation: 0.35269184068681525]
	TIME [epoch: 10.3 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.638143645768378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.638143645768378 | validation: 0.5052798633830932]
	TIME [epoch: 10.3 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5236362267975786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5236362267975786 | validation: 0.5990647977784631]
	TIME [epoch: 10.3 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6176913088554529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6176913088554529 | validation: 0.6567739090276808]
	TIME [epoch: 10.3 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6773408456860481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6773408456860481 | validation: 0.8259078043207028]
	TIME [epoch: 10.3 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6325739872993446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6325739872993446 | validation: 0.4988226634972804]
	TIME [epoch: 10.3 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4729320944301838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4729320944301838 | validation: 0.3642265271731452]
	TIME [epoch: 10.3 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5525488444370203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5525488444370203 | validation: 0.5291265145870584]
	TIME [epoch: 10.3 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.575961129624176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.575961129624176 | validation: 0.4575345930505979]
	TIME [epoch: 10.3 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.516377801224677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.516377801224677 | validation: 0.44037069920655186]
	TIME [epoch: 10.3 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42981243828473925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42981243828473925 | validation: 0.7961260472516491]
	TIME [epoch: 10.3 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.698027077886324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.698027077886324 | validation: 1.1733194562820806]
	TIME [epoch: 10.3 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8702467744933589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8702467744933589 | validation: 0.3816175801061322]
	TIME [epoch: 10.3 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3664947891957455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3664947891957455 | validation: 0.2526167693603319]
	TIME [epoch: 10.3 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6854309336293325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6854309336293325 | validation: 0.7605156044038514]
	TIME [epoch: 10.3 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7673297585343872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7673297585343872 | validation: 1.2752433137582173]
	TIME [epoch: 10.3 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1890655132746613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1890655132746613 | validation: 1.577991760216118]
	TIME [epoch: 10.3 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.41985602658634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.41985602658634 | validation: 0.5954295711358408]
	TIME [epoch: 10.3 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8695276149544909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8695276149544909 | validation: 0.6106084490351791]
	TIME [epoch: 10.3 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4103474014466684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4103474014466684 | validation: 0.43609288167472643]
	TIME [epoch: 10.3 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.347708103324788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.347708103324788 | validation: 0.362523148889013]
	TIME [epoch: 10.3 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7671097362198425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7671097362198425 | validation: 2.3353284857023757]
	TIME [epoch: 10.3 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7603755865205781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7603755865205781 | validation: 0.8985834678318029]
	TIME [epoch: 10.3 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6403342573278337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6403342573278337 | validation: 0.553279634396999]
	TIME [epoch: 10.3 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3825322049485324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3825322049485324 | validation: 0.3165621236880946]
	TIME [epoch: 10.3 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7437389864755628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7437389864755628 | validation: 1.8625667632779128]
	TIME [epoch: 10.3 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3437290357883303		[learning rate: 0.0099755]
	Learning Rate: 0.00997547
	LOSS [training: 1.3437290357883303 | validation: 0.7437648710543857]
	TIME [epoch: 10.3 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1876121374474207		[learning rate: 0.0099449]
	Learning Rate: 0.00994489
	LOSS [training: 1.1876121374474207 | validation: 2.4033605116962558]
	TIME [epoch: 10.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2558487097336504		[learning rate: 0.0099144]
	Learning Rate: 0.0099144
	LOSS [training: 2.2558487097336504 | validation: 1.6437565143486033]
	TIME [epoch: 10.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6028010477300783		[learning rate: 0.009884]
	Learning Rate: 0.00988401
	LOSS [training: 1.6028010477300783 | validation: 1.2018821301706673]
	TIME [epoch: 10.3 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5130870144603479		[learning rate: 0.0098537]
	Learning Rate: 0.00985371
	LOSS [training: 1.5130870144603479 | validation: 1.3618113295070895]
	TIME [epoch: 10.3 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4295590598910097		[learning rate: 0.0098235]
	Learning Rate: 0.00982351
	LOSS [training: 1.4295590598910097 | validation: 1.3635808886850713]
	TIME [epoch: 10.3 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.155421686864179		[learning rate: 0.0097934]
	Learning Rate: 0.0097934
	LOSS [training: 1.155421686864179 | validation: 0.8215179325080868]
	TIME [epoch: 10.3 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0829717087041901		[learning rate: 0.0097634]
	Learning Rate: 0.00976337
	LOSS [training: 1.0829717087041901 | validation: 1.2355736128106274]
	TIME [epoch: 10.3 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2601431670533918		[learning rate: 0.0097334]
	Learning Rate: 0.00973345
	LOSS [training: 1.2601431670533918 | validation: 0.7444096267169544]
	TIME [epoch: 10.3 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0107144811411346		[learning rate: 0.0097036]
	Learning Rate: 0.00970361
	LOSS [training: 1.0107144811411346 | validation: 1.2981139386955767]
	TIME [epoch: 10.3 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2844878885849034		[learning rate: 0.0096739]
	Learning Rate: 0.00967386
	LOSS [training: 1.2844878885849034 | validation: 0.726881502457234]
	TIME [epoch: 10.3 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2072479098945763		[learning rate: 0.0096442]
	Learning Rate: 0.00964421
	LOSS [training: 1.2072479098945763 | validation: 1.1379323163286352]
	TIME [epoch: 10.3 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1168235377243392		[learning rate: 0.0096146]
	Learning Rate: 0.00961465
	LOSS [training: 1.1168235377243392 | validation: 0.741008670605832]
	TIME [epoch: 10.3 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8595445627841529		[learning rate: 0.0095852]
	Learning Rate: 0.00958517
	LOSS [training: 0.8595445627841529 | validation: 0.8698215455185482]
	TIME [epoch: 10.3 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.789823384869		[learning rate: 0.0095558]
	Learning Rate: 0.00955579
	LOSS [training: 0.789823384869 | validation: 0.5791118149634721]
	TIME [epoch: 10.3 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8706301559400723		[learning rate: 0.0095265]
	Learning Rate: 0.0095265
	LOSS [training: 0.8706301559400723 | validation: 0.9773257059027592]
	TIME [epoch: 10.3 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1526112551115113		[learning rate: 0.0094973]
	Learning Rate: 0.0094973
	LOSS [training: 1.1526112551115113 | validation: 1.155868307349833]
	TIME [epoch: 10.3 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245955173741028		[learning rate: 0.0094682]
	Learning Rate: 0.00946818
	LOSS [training: 2.245955173741028 | validation: 1.355294515312564]
	TIME [epoch: 10.3 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.775800253477754		[learning rate: 0.0094392]
	Learning Rate: 0.00943916
	LOSS [training: 1.775800253477754 | validation: 1.2498860134704175]
	TIME [epoch: 10.3 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2355467898702033		[learning rate: 0.0094102]
	Learning Rate: 0.00941022
	LOSS [training: 1.2355467898702033 | validation: 0.5685687179752567]
	TIME [epoch: 10.3 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5653173926944068		[learning rate: 0.0093814]
	Learning Rate: 0.00938138
	LOSS [training: 0.5653173926944068 | validation: 0.5787606124375078]
	TIME [epoch: 10.3 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0967862444179477		[learning rate: 0.0093526]
	Learning Rate: 0.00935262
	LOSS [training: 1.0967862444179477 | validation: 0.8488983178739866]
	TIME [epoch: 10.3 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.655639305178356		[learning rate: 0.009324]
	Learning Rate: 0.00932395
	LOSS [training: 0.655639305178356 | validation: 0.6532008304886593]
	TIME [epoch: 10.3 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7145018635038272		[learning rate: 0.0092954]
	Learning Rate: 0.00929537
	LOSS [training: 0.7145018635038272 | validation: 0.8578536581900089]
	TIME [epoch: 10.3 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7531189571340529		[learning rate: 0.0092669]
	Learning Rate: 0.00926687
	LOSS [training: 0.7531189571340529 | validation: 0.7570616127868927]
	TIME [epoch: 10.3 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8859757144210771		[learning rate: 0.0092385]
	Learning Rate: 0.00923847
	LOSS [training: 0.8859757144210771 | validation: 0.8767714113517019]
	TIME [epoch: 10.3 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7814379898525303		[learning rate: 0.0092101]
	Learning Rate: 0.00921015
	LOSS [training: 0.7814379898525303 | validation: 0.6602761952812477]
	TIME [epoch: 10.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7101001910424689		[learning rate: 0.0091819]
	Learning Rate: 0.00918192
	LOSS [training: 0.7101001910424689 | validation: 0.6066753377008042]
	TIME [epoch: 10.3 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6220490027531718		[learning rate: 0.0091538]
	Learning Rate: 0.00915377
	LOSS [training: 0.6220490027531718 | validation: 0.596318133823525]
	TIME [epoch: 10.3 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7219641024650192		[learning rate: 0.0091257]
	Learning Rate: 0.00912571
	LOSS [training: 0.7219641024650192 | validation: 0.4730508643284169]
	TIME [epoch: 10.3 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7238368130048309		[learning rate: 0.0090977]
	Learning Rate: 0.00909774
	LOSS [training: 0.7238368130048309 | validation: 0.58171685707398]
	TIME [epoch: 10.3 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.609921731558212		[learning rate: 0.0090698]
	Learning Rate: 0.00906985
	LOSS [training: 0.609921731558212 | validation: 0.93875685312166]
	TIME [epoch: 10.3 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7305789389399593		[learning rate: 0.009042]
	Learning Rate: 0.00904205
	LOSS [training: 0.7305789389399593 | validation: 0.5988398166044399]
	TIME [epoch: 10.3 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: nan		[learning rate: 0.0090143]
ERROR:
nan encountered in epoch 533 (training loss).
