Args:
Namespace(name='model_tr_study6', outdir='out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3', training_data='data/transition_rate_studies/tr_study6/tr_study6_training/r3', validation_data='data/transition_rate_studies/tr_study6/tr_study6_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3708729222

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.147284166645974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.147284166645974 | validation: 9.333062902344325]
	TIME [epoch: 49.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.673765103054787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.673765103054787 | validation: 7.765315464022621]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.622542485448645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.622542485448645 | validation: 6.0008555169681]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.085959206498908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.085959206498908 | validation: 5.899863443223878]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.059174420713487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.059174420713487 | validation: 5.041650496590893]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.702264848722768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.702264848722768 | validation: 5.196982701563562]
	TIME [epoch: 10.6 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.853238602440044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.853238602440044 | validation: 3.6954629394180323]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0677274719672525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0677274719672525 | validation: 3.170248841967532]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.56787974650815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.56787974650815 | validation: 2.911063038438276]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2382849246816114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2382849246816114 | validation: 3.2615732744660115]
	TIME [epoch: 10.6 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.453798177372166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.453798177372166 | validation: 2.803128483176866]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1765345624547536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1765345624547536 | validation: 2.706926158638611]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.936457684177943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.936457684177943 | validation: 2.8314669052441683]
	TIME [epoch: 10.6 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0954177352586068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0954177352586068 | validation: 2.967496710143191]
	TIME [epoch: 10.6 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1000101995597387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1000101995597387 | validation: 2.863234011609352]
	TIME [epoch: 10.6 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.118380660592643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.118380660592643 | validation: 2.865514619615534]
	TIME [epoch: 10.6 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.034244831741062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.034244831741062 | validation: 2.693415914790769]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.144527212871374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.144527212871374 | validation: 2.766504532509232]
	TIME [epoch: 10.6 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0554929740242223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0554929740242223 | validation: 2.642915910397304]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9987106826661534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9987106826661534 | validation: 2.674961503153938]
	TIME [epoch: 10.6 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0877421691579303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0877421691579303 | validation: 2.666886488163786]
	TIME [epoch: 10.6 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.089166861041457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.089166861041457 | validation: 2.668533086394159]
	TIME [epoch: 10.6 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1535215240939545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1535215240939545 | validation: 2.919595081612506]
	TIME [epoch: 10.6 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0570230002296377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0570230002296377 | validation: 2.6284976180862594]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.054858526204612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.054858526204612 | validation: 2.785519142511987]
	TIME [epoch: 10.6 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0340610643127057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0340610643127057 | validation: 2.90663992599769]
	TIME [epoch: 10.6 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.043911560993174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.043911560993174 | validation: 2.713255062064154]
	TIME [epoch: 10.6 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.432864636573913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.432864636573913 | validation: 7.2225896854879865]
	TIME [epoch: 10.6 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.946352079904207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.946352079904207 | validation: 5.673553835110818]
	TIME [epoch: 10.6 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.3307528786726355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3307528786726355 | validation: 5.571987690448085]
	TIME [epoch: 10.6 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.315579418125485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.315579418125485 | validation: 5.702481757572762]
	TIME [epoch: 10.6 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.930527820008335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.930527820008335 | validation: 5.400826315793861]
	TIME [epoch: 10.6 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.1384604084154395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1384604084154395 | validation: 4.7080217140045555]
	TIME [epoch: 10.6 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7195389899538664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7195389899538664 | validation: 4.186155314574361]
	TIME [epoch: 10.6 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.40562273053659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.40562273053659 | validation: 3.391836876665627]
	TIME [epoch: 10.6 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.829785963814821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.829785963814821 | validation: 5.086841855986378]
	TIME [epoch: 10.6 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.64082235491792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.64082235491792 | validation: 3.112760604220316]
	TIME [epoch: 10.6 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3700154442837045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3700154442837045 | validation: 3.1243292614101263]
	TIME [epoch: 10.6 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.105420725875006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.105420725875006 | validation: 3.2236742181830427]
	TIME [epoch: 10.6 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502444484177925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.502444484177925 | validation: 3.0132906738341276]
	TIME [epoch: 10.6 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6148253275611224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6148253275611224 | validation: 3.0030802806300883]
	TIME [epoch: 10.6 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.070001318991998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.070001318991998 | validation: 2.5684369906214646]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.968321406705969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.968321406705969 | validation: 2.7507321834319045]
	TIME [epoch: 10.6 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8087880254120483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8087880254120483 | validation: 2.729970570506398]
	TIME [epoch: 10.6 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7240687829552184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7240687829552184 | validation: 3.1352331103946303]
	TIME [epoch: 10.6 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9039145153455506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9039145153455506 | validation: 2.919398025634473]
	TIME [epoch: 10.6 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2613592512294636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2613592512294636 | validation: 2.4359631850772225]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8277376743938016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8277376743938016 | validation: 2.2453356762680174]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4691470391323285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4691470391323285 | validation: 2.144439140650682]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4686891838063154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4686891838063154 | validation: 2.2846844434785107]
	TIME [epoch: 10.6 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.361502792600459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.361502792600459 | validation: 4.087482891882636]
	TIME [epoch: 10.6 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0645306262897307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0645306262897307 | validation: 3.070337359225034]
	TIME [epoch: 10.6 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3238628810656636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3238628810656636 | validation: 2.365438493006963]
	TIME [epoch: 10.6 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4065058831543524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4065058831543524 | validation: 2.268647250397334]
	TIME [epoch: 10.6 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.909947762629488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.909947762629488 | validation: 2.86396075459253]
	TIME [epoch: 10.6 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.850649122963272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.850649122963272 | validation: 2.5819496960605193]
	TIME [epoch: 10.6 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.724577149279912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.724577149279912 | validation: 2.58817413779692]
	TIME [epoch: 10.6 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9374519851210295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9374519851210295 | validation: 2.1236471919886086]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2894287887519162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2894287887519162 | validation: 1.8978076773416548]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.032296968254863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.032296968254863 | validation: 2.083462235304391]
	TIME [epoch: 10.6 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.316829815767229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.316829815767229 | validation: 1.8287066344656182]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.792237800932768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.792237800932768 | validation: 3.0035084492457806]
	TIME [epoch: 10.6 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.420223698232402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.420223698232402 | validation: 2.7596534805135957]
	TIME [epoch: 10.6 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.771199412667282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.771199412667282 | validation: 3.7269734321750683]
	TIME [epoch: 10.6 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7703672700267856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7703672700267856 | validation: 1.9866123576154093]
	TIME [epoch: 10.6 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.337240673766215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.337240673766215 | validation: 2.115164466273148]
	TIME [epoch: 10.6 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2943587165567676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2943587165567676 | validation: 4.691443958477318]
	TIME [epoch: 10.6 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.20280588514413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.20280588514413 | validation: 3.3787716012578586]
	TIME [epoch: 10.6 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.305825569935082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.305825569935082 | validation: 2.625347984449086]
	TIME [epoch: 10.6 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.537520676710408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.537520676710408 | validation: 2.600272326752823]
	TIME [epoch: 10.6 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9591576235996928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9591576235996928 | validation: 2.0522908625918275]
	TIME [epoch: 10.6 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9428539347161422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9428539347161422 | validation: 2.3549916725146356]
	TIME [epoch: 10.6 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4185937992568247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4185937992568247 | validation: 2.2138394133114927]
	TIME [epoch: 10.6 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.993722776380741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.993722776380741 | validation: 1.4976844146811072]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7244967825043283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7244967825043283 | validation: 1.4729570731369024]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4771347254238099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4771347254238099 | validation: 1.3162274292471432]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7193490689189583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7193490689189583 | validation: 2.3474769763078407]
	TIME [epoch: 10.6 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9793492150387149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9793492150387149 | validation: 2.0677639832011114]
	TIME [epoch: 10.6 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6682835394235453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6682835394235453 | validation: 1.2656439316757433]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4354858101033627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4354858101033627 | validation: 1.2589491031234552]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1592949084414226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1592949084414226 | validation: 1.283246704768152]
	TIME [epoch: 10.6 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4936807261598095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4936807261598095 | validation: 1.2818497615017583]
	TIME [epoch: 10.6 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4877497179267962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4877497179267962 | validation: 1.5809450976858295]
	TIME [epoch: 10.6 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.59101208433773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.59101208433773 | validation: 7.211185310321195]
	TIME [epoch: 10.6 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.047281831891402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.047281831891402 | validation: 4.560263753361727]
	TIME [epoch: 10.6 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.782732690816704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.782732690816704 | validation: 3.9175242302035618]
	TIME [epoch: 10.6 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2781901430597187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2781901430597187 | validation: 2.6681266096615692]
	TIME [epoch: 10.6 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.632528206219658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.632528206219658 | validation: 1.8581027872214697]
	TIME [epoch: 10.6 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5823735441747928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5823735441747928 | validation: 1.120820714764348]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.342059544092884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.342059544092884 | validation: 1.7794693215213775]
	TIME [epoch: 10.6 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.968371768354351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.968371768354351 | validation: 1.5517315953611783]
	TIME [epoch: 10.6 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3256458867851346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3256458867851346 | validation: 1.5236432961107114]
	TIME [epoch: 10.6 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7055751781672441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7055751781672441 | validation: 1.3672845563025715]
	TIME [epoch: 10.6 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3669647479240088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3669647479240088 | validation: 1.2634788934311905]
	TIME [epoch: 10.6 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.248515994515025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.248515994515025 | validation: 1.1399204659135245]
	TIME [epoch: 10.6 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2181932750635718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2181932750635718 | validation: 1.1940818550326406]
	TIME [epoch: 10.6 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.174945032167921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.174945032167921 | validation: 6.0477816123312165]
	TIME [epoch: 10.6 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.628256979412892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.628256979412892 | validation: 6.3186871862505996]
	TIME [epoch: 10.6 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.820752537447371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.820752537447371 | validation: 5.077197268936154]
	TIME [epoch: 10.6 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.354163861950095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.354163861950095 | validation: 4.911989769100753]
	TIME [epoch: 10.6 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.204624339588888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.204624339588888 | validation: 4.794725618317482]
	TIME [epoch: 10.6 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.081526046329964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.081526046329964 | validation: 4.676327573355686]
	TIME [epoch: 10.6 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8484844044916002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8484844044916002 | validation: 4.2071783855402]
	TIME [epoch: 10.6 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.704119587121687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.704119587121687 | validation: 4.099398205244653]
	TIME [epoch: 10.6 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.656233498098497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.656233498098497 | validation: 4.006361367342975]
	TIME [epoch: 10.6 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.462041905943218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.462041905943218 | validation: 3.7723580167993176]
	TIME [epoch: 10.6 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.245304841519986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.245304841519986 | validation: 2.467051998278962]
	TIME [epoch: 10.6 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3904484707764118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3904484707764118 | validation: 1.9056656834407721]
	TIME [epoch: 10.6 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8786817208117668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8786817208117668 | validation: 1.5488677141431948]
	TIME [epoch: 10.6 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9893116999330496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9893116999330496 | validation: 3.0332395028753036]
	TIME [epoch: 10.6 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.975413066418442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.975413066418442 | validation: 2.8758826262103145]
	TIME [epoch: 10.6 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1910773104104098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1910773104104098 | validation: 2.8253722363778615]
	TIME [epoch: 10.6 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.137594234882364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.137594234882364 | validation: 2.8219436672426883]
	TIME [epoch: 10.6 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8403873428317756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8403873428317756 | validation: 1.8096270685308868]
	TIME [epoch: 10.6 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6021317075664392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6021317075664392 | validation: 1.046532868711195]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.152533317216786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.152533317216786 | validation: 1.9538766948013817]
	TIME [epoch: 10.6 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4329027840973123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4329027840973123 | validation: 2.7714058204588308]
	TIME [epoch: 10.6 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.136333507626868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.136333507626868 | validation: 2.8030722037779396]
	TIME [epoch: 10.6 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1596707223716813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1596707223716813 | validation: 2.8895370235818176]
	TIME [epoch: 10.6 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1441996873321054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1441996873321054 | validation: 2.7038435394962654]
	TIME [epoch: 10.6 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.095238704974926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.095238704974926 | validation: 2.633754859609502]
	TIME [epoch: 10.6 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1201124611693367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1201124611693367 | validation: 2.7865057525512023]
	TIME [epoch: 10.6 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.084688877101205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.084688877101205 | validation: 2.8568818829072486]
	TIME [epoch: 10.6 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.227934050312835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.227934050312835 | validation: 2.728687234696323]
	TIME [epoch: 10.6 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.484578347570359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.484578347570359 | validation: 2.6547883593557966]
	TIME [epoch: 10.6 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.100047784409924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.100047784409924 | validation: 2.639950610946928]
	TIME [epoch: 10.6 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.055545449177374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.055545449177374 | validation: 2.7801102772358224]
	TIME [epoch: 10.6 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.099939413280069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.099939413280069 | validation: 3.10000571271061]
	TIME [epoch: 10.6 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1077802673721644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1077802673721644 | validation: 2.251012467813408]
	TIME [epoch: 10.6 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8833556238295053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8833556238295053 | validation: 2.9901548843456887]
	TIME [epoch: 10.6 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.173588589172707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.173588589172707 | validation: 1.2657214238266226]
	TIME [epoch: 10.6 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1903909407157738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1903909407157738 | validation: 0.9134960049640911]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9393738135107867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9393738135107867 | validation: 1.080743662047159]
	TIME [epoch: 10.6 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5960436567632268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5960436567632268 | validation: 1.0029461293575073]
	TIME [epoch: 10.6 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.211657570947913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.211657570947913 | validation: 1.8340303355634828]
	TIME [epoch: 10.6 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.558213718819265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.558213718819265 | validation: 2.769256634648847]
	TIME [epoch: 10.6 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1037088443129246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1037088443129246 | validation: 2.6858394042769955]
	TIME [epoch: 10.6 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1078344047371353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1078344047371353 | validation: 2.85270709273418]
	TIME [epoch: 10.6 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0797369420435285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0797369420435285 | validation: 2.725067719622359]
	TIME [epoch: 10.6 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1714601611602564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1714601611602564 | validation: 2.7969500935745284]
	TIME [epoch: 10.6 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4258656860525214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4258656860525214 | validation: 2.7541138530726146]
	TIME [epoch: 10.6 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0820529447051355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0820529447051355 | validation: 2.727727083785485]
	TIME [epoch: 10.6 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0214873953016057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0214873953016057 | validation: 2.2718084145788384]
	TIME [epoch: 10.6 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6922639695042971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6922639695042971 | validation: 1.208647641731892]
	TIME [epoch: 10.6 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6737662568748353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6737662568748353 | validation: 1.3347674787980057]
	TIME [epoch: 10.6 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1549140211328237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1549140211328237 | validation: 0.9838942518777398]
	TIME [epoch: 10.6 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9748019550019249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9748019550019249 | validation: 1.4810434348331485]
	TIME [epoch: 10.6 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2564328165440692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2564328165440692 | validation: 1.5206840012105118]
	TIME [epoch: 10.6 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1684477271421314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1684477271421314 | validation: 0.9679349280876047]
	TIME [epoch: 10.6 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3752057449576716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3752057449576716 | validation: 1.1270617013279607]
	TIME [epoch: 10.6 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9815622656101037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9815622656101037 | validation: 0.8669822873718456]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.859957777572078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.859957777572078 | validation: 3.031814269186593]
	TIME [epoch: 10.6 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2353506901207183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2353506901207183 | validation: 2.840889026518712]
	TIME [epoch: 10.6 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0913910631511987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0913910631511987 | validation: 2.7508223587855207]
	TIME [epoch: 10.6 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1081183991429846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1081183991429846 | validation: 2.7288925902724186]
	TIME [epoch: 10.6 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1117849393809403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1117849393809403 | validation: 2.829622978683108]
	TIME [epoch: 10.6 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0870552571171266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0870552571171266 | validation: 4.018707952974944]
	TIME [epoch: 10.6 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.508295606571754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.508295606571754 | validation: 1.6660474323542083]
	TIME [epoch: 10.6 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4929362972151625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4929362972151625 | validation: 1.6311395972219325]
	TIME [epoch: 10.6 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.167085403368821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.167085403368821 | validation: 0.9430027666784692]
	TIME [epoch: 10.6 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0009203434576928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0009203434576928 | validation: 1.009438453416578]
	TIME [epoch: 10.6 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0223732651964632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0223732651964632 | validation: 0.8231612338926987]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1190667441341815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1190667441341815 | validation: 0.8315618357906415]
	TIME [epoch: 10.6 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9555854047194032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9555854047194032 | validation: 2.7361136594572337]
	TIME [epoch: 10.6 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.216229409746519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.216229409746519 | validation: 0.8727482973558242]
	TIME [epoch: 10.6 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.864183974885871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.864183974885871 | validation: 1.0689033873698452]
	TIME [epoch: 10.6 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.108451268269524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.108451268269524 | validation: 0.9769372850138498]
	TIME [epoch: 10.6 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7488385621188829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7488385621188829 | validation: 1.3884251179815932]
	TIME [epoch: 10.6 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.247212288949292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.247212288949292 | validation: 1.3344877486108624]
	TIME [epoch: 10.6 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1778215393348779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1778215393348779 | validation: 0.8797192346045382]
	TIME [epoch: 10.6 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9340678678563687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9340678678563687 | validation: 0.8535916315173379]
	TIME [epoch: 10.6 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.080541994321671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.080541994321671 | validation: 1.1030367637466072]
	TIME [epoch: 10.6 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1298525226359213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1298525226359213 | validation: 0.8655776684953367]
	TIME [epoch: 10.6 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8965787018447603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8965787018447603 | validation: 0.946202241039644]
	TIME [epoch: 10.6 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.894623580125827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.894623580125827 | validation: 0.8821901355138245]
	TIME [epoch: 10.6 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9723341400908904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9723341400908904 | validation: 0.8127249321420601]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9850940731548666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9850940731548666 | validation: 1.1118565633495319]
	TIME [epoch: 10.6 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.077017192457266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.077017192457266 | validation: 0.9199943213372717]
	TIME [epoch: 10.6 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9373866676473112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9373866676473112 | validation: 0.9412498566986934]
	TIME [epoch: 10.6 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2189059431618225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2189059431618225 | validation: 1.0308408932418776]
	TIME [epoch: 10.6 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.954275287562844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.954275287562844 | validation: 0.7767560003331743]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8666524459547086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8666524459547086 | validation: 0.7347949012418127]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4981397465895385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4981397465895385 | validation: 1.0979363109678053]
	TIME [epoch: 10.6 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0558547109828218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0558547109828218 | validation: 0.9293861649577584]
	TIME [epoch: 10.6 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0283408439572264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0283408439572264 | validation: 0.7869392034069452]
	TIME [epoch: 10.6 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9421490436214421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9421490436214421 | validation: 0.6825212766499579]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8093279730720344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8093279730720344 | validation: 1.0558182666351075]
	TIME [epoch: 10.6 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1707564385481377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1707564385481377 | validation: 0.9528253773825217]
	TIME [epoch: 10.6 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7392137037092694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7392137037092694 | validation: 2.7441054652850307]
	TIME [epoch: 10.6 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8343252544059983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8343252544059983 | validation: 1.6323606956116965]
	TIME [epoch: 10.6 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4051255335872284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4051255335872284 | validation: 1.1312923963077797]
	TIME [epoch: 10.6 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3364858821238472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3364858821238472 | validation: 1.570495717184048]
	TIME [epoch: 10.6 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3768896032959692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3768896032959692 | validation: 0.8962639761007759]
	TIME [epoch: 10.6 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3353659906483284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3353659906483284 | validation: 1.0850137051972066]
	TIME [epoch: 10.6 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.01489691475799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.01489691475799 | validation: 0.9359475863457766]
	TIME [epoch: 10.6 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0189097952029444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0189097952029444 | validation: 0.8449844476852844]
	TIME [epoch: 10.6 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9418676311824747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9418676311824747 | validation: 0.9252149905802033]
	TIME [epoch: 10.6 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8819701149829022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8819701149829022 | validation: 0.8234528519355628]
	TIME [epoch: 10.6 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8320659444749475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8320659444749475 | validation: 0.7310375846883386]
	TIME [epoch: 10.6 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.875487097965818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.875487097965818 | validation: 0.7897973853460425]
	TIME [epoch: 10.6 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0333333832725473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0333333832725473 | validation: 1.053952417421392]
	TIME [epoch: 10.6 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9889903112591003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9889903112591003 | validation: 0.7000047747313588]
	TIME [epoch: 10.6 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8622803417478725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8622803417478725 | validation: 0.825588440946342]
	TIME [epoch: 10.6 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8436305598039902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8436305598039902 | validation: 0.7877610871243726]
	TIME [epoch: 10.6 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8191844291816619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8191844291816619 | validation: 0.893620929368336]
	TIME [epoch: 10.6 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1033197966401769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1033197966401769 | validation: 0.9210680652937265]
	TIME [epoch: 10.6 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0311498582025735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0311498582025735 | validation: 1.020997980788543]
	TIME [epoch: 10.6 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0799194529486909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0799194529486909 | validation: 0.9658079493791768]
	TIME [epoch: 10.6 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0729030797404597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0729030797404597 | validation: 1.1191017307025708]
	TIME [epoch: 10.6 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0287775945523987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0287775945523987 | validation: 0.9297352734757851]
	TIME [epoch: 10.6 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.922167695098975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.922167695098975 | validation: 0.7941264725982934]
	TIME [epoch: 10.6 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8788965842347842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8788965842347842 | validation: 0.6824520966090768]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8809706316027779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8809706316027779 | validation: 1.126146003981781]
	TIME [epoch: 10.6 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2356708735436033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2356708735436033 | validation: 0.9373619161016844]
	TIME [epoch: 10.6 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2286610614186642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2286610614186642 | validation: 0.9868609304647399]
	TIME [epoch: 10.6 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5447734700012665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5447734700012665 | validation: 4.92984926748112]
	TIME [epoch: 10.6 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.168008676964034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.168008676964034 | validation: 4.810815191886891]
	TIME [epoch: 10.6 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.126891743834523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.126891743834523 | validation: 4.809150532538308]
	TIME [epoch: 10.6 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2597494332016215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2597494332016215 | validation: 4.687884725605411]
	TIME [epoch: 10.6 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.411704079107014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.411704079107014 | validation: 4.633955377857986]
	TIME [epoch: 10.6 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.065324411827961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.065324411827961 | validation: 4.616753707263855]
	TIME [epoch: 10.6 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.942041977831657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.942041977831657 | validation: 4.751053086234971]
	TIME [epoch: 10.6 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0132267881071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0132267881071 | validation: 4.345610632027917]
	TIME [epoch: 10.6 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.910564138969517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.910564138969517 | validation: 4.21553974101615]
	TIME [epoch: 10.6 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.698381879277142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.698381879277142 | validation: 4.460900247747382]
	TIME [epoch: 10.6 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.876469581151423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.876469581151423 | validation: 3.917590835660182]
	TIME [epoch: 10.6 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8591443053266112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8591443053266112 | validation: 4.034362878712591]
	TIME [epoch: 10.6 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6009084233984083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6009084233984083 | validation: 3.8837076876186685]
	TIME [epoch: 10.6 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.50349410056048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.50349410056048 | validation: 3.836744704628111]
	TIME [epoch: 10.6 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.430425016528715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.430425016528715 | validation: 3.7545897194476474]
	TIME [epoch: 10.6 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5883057731913284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5883057731913284 | validation: 3.6210857800452794]
	TIME [epoch: 10.6 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.394805053621334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.394805053621334 | validation: 3.7004225132394937]
	TIME [epoch: 10.6 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4040346952860823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4040346952860823 | validation: 3.8101976724724516]
	TIME [epoch: 10.6 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.336316715352974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.336316715352974 | validation: 3.759759221408574]
	TIME [epoch: 10.6 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.315873883995612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.315873883995612 | validation: 3.577295222977948]
	TIME [epoch: 10.6 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0987727633478657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0987727633478657 | validation: 3.2744617509151897]
	TIME [epoch: 10.6 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.141240311608267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.141240311608267 | validation: 4.2157622321642565]
	TIME [epoch: 10.6 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5038529015985858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5038529015985858 | validation: 3.2466845869194554]
	TIME [epoch: 10.6 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9235426549771306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9235426549771306 | validation: 3.0405370835385725]
	TIME [epoch: 10.6 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9037706903414593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9037706903414593 | validation: 2.4286762628652827]
	TIME [epoch: 10.6 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6876604376766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6876604376766 | validation: 4.840261778357022]
	TIME [epoch: 10.6 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.70494374723755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.70494374723755 | validation: 3.4638617146916055]
	TIME [epoch: 10.6 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.140588801806088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.140588801806088 | validation: 2.253807007609043]
	TIME [epoch: 10.6 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.231951158534093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.231951158534093 | validation: 2.0243371536436627]
	TIME [epoch: 10.6 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.78838433655836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.78838433655836 | validation: 1.7650802775308203]
	TIME [epoch: 10.6 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8840152736955305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8840152736955305 | validation: 2.027674229217122]
	TIME [epoch: 10.6 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9221576019462845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9221576019462845 | validation: 1.456705504311945]
	TIME [epoch: 10.6 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4598240205924617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4598240205924617 | validation: 1.4437649160026649]
	TIME [epoch: 10.6 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.379727515697191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.379727515697191 | validation: 1.7016810649408498]
	TIME [epoch: 10.6 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4723198555444532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4723198555444532 | validation: 1.1667544434110197]
	TIME [epoch: 10.6 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.547214623743678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.547214623743678 | validation: 1.519502468083532]
	TIME [epoch: 10.6 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6407588052264668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6407588052264668 | validation: 1.8106189559346861]
	TIME [epoch: 10.6 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9250565577472956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9250565577472956 | validation: 1.5162952181945082]
	TIME [epoch: 10.6 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9027374100433154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9027374100433154 | validation: 1.477843199048387]
	TIME [epoch: 10.6 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2623793431861947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2623793431861947 | validation: 1.0197484618149435]
	TIME [epoch: 10.6 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2650324796900558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2650324796900558 | validation: 1.0315145326714774]
	TIME [epoch: 10.6 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0938592535065008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0938592535065008 | validation: 0.8606560371337932]
	TIME [epoch: 10.6 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8861210485631075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8861210485631075 | validation: 0.9946206282214681]
	TIME [epoch: 10.6 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2426593401429984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2426593401429984 | validation: 1.3032688513007094]
	TIME [epoch: 10.6 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1984772571515638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1984772571515638 | validation: 0.9911033164752698]
	TIME [epoch: 10.6 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2392234912613351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2392234912613351 | validation: 1.1003301681196793]
	TIME [epoch: 10.6 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9949638927750206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9949638927750206 | validation: 0.8834239046823262]
	TIME [epoch: 10.6 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1085942710735832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1085942710735832 | validation: 1.30633267983952]
	TIME [epoch: 10.6 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4412621866648934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4412621866648934 | validation: 2.234239118038982]
	TIME [epoch: 10.6 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4851220938941325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4851220938941325 | validation: 2.0778416991576716]
	TIME [epoch: 10.6 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3435552238514856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3435552238514856 | validation: 0.9675001131275971]
	TIME [epoch: 10.6 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1676426537306113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1676426537306113 | validation: 0.9291249124115004]
	TIME [epoch: 10.6 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9304068384228799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9304068384228799 | validation: 0.9326978522249573]
	TIME [epoch: 10.6 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9205771246658584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9205771246658584 | validation: 1.1579377443145902]
	TIME [epoch: 10.6 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9686457845095235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9686457845095235 | validation: 1.0292450702704552]
	TIME [epoch: 10.6 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8822612890051632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8822612890051632 | validation: 0.7301595739867071]
	TIME [epoch: 10.6 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8729261757056802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8729261757056802 | validation: 0.7700162853038297]
	TIME [epoch: 10.6 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8492955800700587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8492955800700587 | validation: 1.0471834357559846]
	TIME [epoch: 10.6 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8984241950244053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8984241950244053 | validation: 0.9512254108849111]
	TIME [epoch: 10.6 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0756548764741818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0756548764741818 | validation: 2.3310925955358393]
	TIME [epoch: 10.6 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1452128377290416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1452128377290416 | validation: 1.7061144759646947]
	TIME [epoch: 10.6 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2737564984599905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2737564984599905 | validation: 0.9044284718975564]
	TIME [epoch: 10.6 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0130097287790363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0130097287790363 | validation: 1.1514276627376379]
	TIME [epoch: 10.6 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0533868892499365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0533868892499365 | validation: 1.4041549375330962]
	TIME [epoch: 10.6 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3272375958172282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3272375958172282 | validation: 1.826713330080073]
	TIME [epoch: 10.6 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9489186215845904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9489186215845904 | validation: 1.7144948058371123]
	TIME [epoch: 10.6 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6633065929415358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6633065929415358 | validation: 1.178915701501287]
	TIME [epoch: 10.6 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.029249857639347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.029249857639347 | validation: 2.3245474415611174]
	TIME [epoch: 10.6 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7368958236930854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7368958236930854 | validation: 2.2786198483425966]
	TIME [epoch: 10.6 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4031009653953377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4031009653953377 | validation: 1.2583904705644908]
	TIME [epoch: 10.6 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.445676587661839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.445676587661839 | validation: 1.9216860019336608]
	TIME [epoch: 10.6 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9064926727290967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9064926727290967 | validation: 1.3560823907981037]
	TIME [epoch: 10.6 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4194054673022596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4194054673022596 | validation: 1.4704991053695113]
	TIME [epoch: 10.6 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7142321115910673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7142321115910673 | validation: 1.258164059882336]
	TIME [epoch: 10.6 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0901637977140974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0901637977140974 | validation: 0.823523156730592]
	TIME [epoch: 10.6 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9259290562409539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9259290562409539 | validation: 0.9414813382634543]
	TIME [epoch: 10.6 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7406559222029805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7406559222029805 | validation: 3.270982546479437]
	TIME [epoch: 10.6 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5522394254575564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5522394254575564 | validation: 3.1722459184035756]
	TIME [epoch: 10.6 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5218121967603557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5218121967603557 | validation: 3.030325584988137]
	TIME [epoch: 10.6 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.393030496236675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.393030496236675 | validation: 2.9730156635363705]
	TIME [epoch: 10.6 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3138185293610873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3138185293610873 | validation: 2.8354469828641595]
	TIME [epoch: 10.6 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2297443274612965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2297443274612965 | validation: 2.9299837070935997]
	TIME [epoch: 10.6 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.236731330009506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.236731330009506 | validation: 3.2482248090323305]
	TIME [epoch: 10.6 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.836312090424358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.836312090424358 | validation: 1.8632309421976112]
	TIME [epoch: 10.6 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3119283424976953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3119283424976953 | validation: 0.9497413711849236]
	TIME [epoch: 10.6 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1329319903239923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1329319903239923 | validation: 1.2258915194584799]
	TIME [epoch: 10.6 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.317371685882562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.317371685882562 | validation: 1.1351924855084232]
	TIME [epoch: 10.6 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2111966614929308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2111966614929308 | validation: 0.8876267070774009]
	TIME [epoch: 10.6 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0537573279050831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0537573279050831 | validation: 0.8259648218516042]
	TIME [epoch: 10.6 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1003319798778053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1003319798778053 | validation: 5.985852243452294]
	TIME [epoch: 10.6 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.316944309217656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.316944309217656 | validation: 0.8820695855621895]
	TIME [epoch: 10.6 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2022748353944661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2022748353944661 | validation: 0.991309079770115]
	TIME [epoch: 10.6 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.941426994703472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.941426994703472 | validation: 0.942188337140978]
	TIME [epoch: 10.6 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.97600564237598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.97600564237598 | validation: 1.189288697005755]
	TIME [epoch: 10.6 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9511527826994197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9511527826994197 | validation: 0.7930257977493176]
	TIME [epoch: 10.6 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.061925309527178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.061925309527178 | validation: 0.8581030598171999]
	TIME [epoch: 10.6 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9939082279238219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9939082279238219 | validation: 0.9285911549057064]
	TIME [epoch: 10.6 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2508650642201706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2508650642201706 | validation: 3.1984048781254133]
	TIME [epoch: 10.6 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.577365626486359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.577365626486359 | validation: 1.93067964161888]
	TIME [epoch: 10.6 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5038941876132808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5038941876132808 | validation: 1.0278744401386524]
	TIME [epoch: 10.6 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.050781582408578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.050781582408578 | validation: 1.039246039519145]
	TIME [epoch: 10.6 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0377746740324827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0377746740324827 | validation: 0.8827748130288873]
	TIME [epoch: 10.6 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9196370180724374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9196370180724374 | validation: 1.6157063978866517]
	TIME [epoch: 10.6 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2868375303651143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2868375303651143 | validation: 1.1507067761886847]
	TIME [epoch: 10.6 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.259369183055492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.259369183055492 | validation: 1.2511565242074156]
	TIME [epoch: 10.6 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4743257501668972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4743257501668972 | validation: 1.4445876730009406]
	TIME [epoch: 10.6 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2137788436883716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2137788436883716 | validation: 0.8797173997541796]
	TIME [epoch: 10.6 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8426185480431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8426185480431 | validation: 0.9009221911803837]
	TIME [epoch: 10.6 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8883669312980922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8883669312980922 | validation: 0.8224356321732128]
	TIME [epoch: 10.6 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8951755656425103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8951755656425103 | validation: 0.7265829669089078]
	TIME [epoch: 10.6 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9151330885176865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9151330885176865 | validation: 0.8227926271915057]
	TIME [epoch: 10.6 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.826157355953088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.826157355953088 | validation: 1.4288671600329792]
	TIME [epoch: 10.6 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0595851010254314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0595851010254314 | validation: 0.994518512920002]
	TIME [epoch: 10.6 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0850049832760793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0850049832760793 | validation: 1.1629928655202695]
	TIME [epoch: 10.6 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9659661073266657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9659661073266657 | validation: 0.9881545102524861]
	TIME [epoch: 10.6 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.038882402300656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.038882402300656 | validation: 1.0589348625921071]
	TIME [epoch: 10.6 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0050070581129371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0050070581129371 | validation: 0.8581491337344287]
	TIME [epoch: 10.6 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.791797113597377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.791797113597377 | validation: 0.7188752782480988]
	TIME [epoch: 10.6 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8493866810759453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8493866810759453 | validation: 0.9314560558596635]
	TIME [epoch: 10.6 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0257748191190879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0257748191190879 | validation: 1.0243383776752837]
	TIME [epoch: 10.6 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8822439128402693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8822439128402693 | validation: 0.8620990214807247]
	TIME [epoch: 10.6 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9104062114244856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9104062114244856 | validation: 0.808086733315555]
	TIME [epoch: 10.6 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8710194457508507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8710194457508507 | validation: 0.8149562618306041]
	TIME [epoch: 10.6 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8524111198568646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8524111198568646 | validation: 0.7623385288713354]
	TIME [epoch: 10.6 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8173312546520364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8173312546520364 | validation: 0.8033122248746513]
	TIME [epoch: 10.6 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9450215166490142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9450215166490142 | validation: 0.8639809354578037]
	TIME [epoch: 10.6 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8289247360094623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8289247360094623 | validation: 0.7731464592396824]
	TIME [epoch: 10.6 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1206473192089617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1206473192089617 | validation: 0.6514682976609052]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7698154378412726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7698154378412726 | validation: 0.7456905813315206]
	TIME [epoch: 10.6 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8831547615186552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8831547615186552 | validation: 0.9702380603933076]
	TIME [epoch: 10.6 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8873616351569578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8873616351569578 | validation: 0.778189260781421]
	TIME [epoch: 10.6 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8333772927037199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8333772927037199 | validation: 0.8177170170516083]
	TIME [epoch: 10.6 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.085432164563398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.085432164563398 | validation: 1.0880774158398576]
	TIME [epoch: 10.6 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0514084193090905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0514084193090905 | validation: 1.3210011875006233]
	TIME [epoch: 10.6 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0080783751976046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0080783751976046 | validation: 0.8662189394477774]
	TIME [epoch: 10.6 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9219254780677877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9219254780677877 | validation: 0.8542944060606895]
	TIME [epoch: 10.6 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3635034079292594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3635034079292594 | validation: 0.821178454201641]
	TIME [epoch: 10.6 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.965349236939088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.965349236939088 | validation: 0.9297100808071241]
	TIME [epoch: 10.6 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.949452315909882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.949452315909882 | validation: 0.8850310487144978]
	TIME [epoch: 10.6 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8068315424531676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8068315424531676 | validation: 1.4430606996444135]
	TIME [epoch: 10.6 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3475824450720282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3475824450720282 | validation: 1.0452884257859234]
	TIME [epoch: 10.6 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.061678595990939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.061678595990939 | validation: 0.9523890291645404]
	TIME [epoch: 10.6 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0940369409936888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0940369409936888 | validation: 0.8117548139012118]
	TIME [epoch: 10.6 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9160084568113277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9160084568113277 | validation: 0.891003674538545]
	TIME [epoch: 10.6 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8527386294264401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8527386294264401 | validation: 1.1780830931946933]
	TIME [epoch: 10.6 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9416566466345779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9416566466345779 | validation: 1.0146384326549847]
	TIME [epoch: 10.6 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1400893562369776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1400893562369776 | validation: 1.3939956705641081]
	TIME [epoch: 10.6 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2506288518003226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2506288518003226 | validation: 0.8586820351036706]
	TIME [epoch: 10.6 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1509836981466914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1509836981466914 | validation: 1.1203755987260784]
	TIME [epoch: 10.6 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0254991938813014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0254991938813014 | validation: 0.8354848369166713]
	TIME [epoch: 10.6 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0442467023257325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0442467023257325 | validation: 1.418300422984786]
	TIME [epoch: 10.6 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3081256419625586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3081256419625586 | validation: 1.0386928586979967]
	TIME [epoch: 10.6 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.269296628066408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.269296628066408 | validation: 1.0830096684237402]
	TIME [epoch: 10.6 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0392407389130134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0392407389130134 | validation: 0.8117431797241541]
	TIME [epoch: 10.6 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8364999536225604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8364999536225604 | validation: 0.8363295793396123]
	TIME [epoch: 10.6 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9056624863777962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9056624863777962 | validation: 1.0617881123450348]
	TIME [epoch: 10.6 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0226662351321978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0226662351321978 | validation: 0.8559043291219689]
	TIME [epoch: 10.6 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8912953311785261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8912953311785261 | validation: 0.8980442490106928]
	TIME [epoch: 10.6 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9780779129671737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9780779129671737 | validation: 0.8497406893157513]
	TIME [epoch: 10.6 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8688573801201956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8688573801201956 | validation: 0.939613506347978]
	TIME [epoch: 10.6 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8557993926264755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8557993926264755 | validation: 0.8535253392860548]
	TIME [epoch: 10.6 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9929695708207227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9929695708207227 | validation: 2.9094935647910343]
	TIME [epoch: 10.6 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0938352718543447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0938352718543447 | validation: 0.9509581768866295]
	TIME [epoch: 10.6 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0051690405593368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0051690405593368 | validation: 0.9299647825381583]
	TIME [epoch: 10.6 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2274084398204146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2274084398204146 | validation: 1.1189807598467396]
	TIME [epoch: 10.6 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1742053546205882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1742053546205882 | validation: 1.0994159526581264]
	TIME [epoch: 10.6 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0067713866434322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0067713866434322 | validation: 0.784163467864053]
	TIME [epoch: 10.6 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8609341853607028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8609341853607028 | validation: 0.8079690903938016]
	TIME [epoch: 10.6 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8397038015482542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8397038015482542 | validation: 0.8304950623367239]
	TIME [epoch: 10.6 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8771135634033802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8771135634033802 | validation: 0.7773936758087134]
	TIME [epoch: 10.6 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8962140002973598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8962140002973598 | validation: 1.0194262362405575]
	TIME [epoch: 10.6 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8773419924701467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8773419924701467 | validation: 0.8505132104522218]
	TIME [epoch: 10.6 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1473465167121464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1473465167121464 | validation: 1.4360181255641953]
	TIME [epoch: 10.6 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2255656559493853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2255656559493853 | validation: 1.1104995740465216]
	TIME [epoch: 10.6 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8673656474409009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8673656474409009 | validation: 1.257706238397613]
	TIME [epoch: 10.6 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9635367432788101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9635367432788101 | validation: 0.8423064125473428]
	TIME [epoch: 10.6 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1692630034005937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1692630034005937 | validation: 0.9289376850905302]
	TIME [epoch: 10.6 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8806557773829866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8806557773829866 | validation: 0.7751041508215956]
	TIME [epoch: 10.6 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0401354555211795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0401354555211795 | validation: 1.5283498103462876]
	TIME [epoch: 10.6 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6095722357926043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6095722357926043 | validation: 1.487796553657297]
	TIME [epoch: 10.6 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5701743575505283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5701743575505283 | validation: 1.1115321190452299]
	TIME [epoch: 10.6 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9920089134164092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9920089134164092 | validation: 0.7325194128161887]
	TIME [epoch: 10.6 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8617924366662338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8617924366662338 | validation: 0.7195579032203676]
	TIME [epoch: 10.6 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.773112554122781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.773112554122781 | validation: 0.6532556990644669]
	TIME [epoch: 10.6 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7766346412902513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7766346412902513 | validation: 0.7281383262307096]
	TIME [epoch: 10.6 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8408706899355648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8408706899355648 | validation: 0.7114483978113667]
	TIME [epoch: 10.6 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7607834986301869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7607834986301869 | validation: 0.6570998451288065]
	TIME [epoch: 10.6 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7695598118702549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7695598118702549 | validation: 0.7905205708701749]
	TIME [epoch: 10.6 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9008437174047561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9008437174047561 | validation: 0.9391819555941612]
	TIME [epoch: 10.6 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.077154708574312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.077154708574312 | validation: 0.7238207707808891]
	TIME [epoch: 10.6 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.92927541622694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.92927541622694 | validation: 1.2598753045325728]
	TIME [epoch: 10.6 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1856057483367244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1856057483367244 | validation: 0.9652794900678799]
	TIME [epoch: 10.6 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8424299061576889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8424299061576889 | validation: 0.8694208689975015]
	TIME [epoch: 10.6 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9615723542806347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9615723542806347 | validation: 0.8159092665255244]
	TIME [epoch: 10.6 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8516434944168058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8516434944168058 | validation: 0.8481196121935028]
	TIME [epoch: 10.6 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8814703590768772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8814703590768772 | validation: 0.9429507718002293]
	TIME [epoch: 10.6 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8920491683772243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8920491683772243 | validation: 0.6870934182545917]
	TIME [epoch: 10.6 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8520685983818554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8520685983818554 | validation: 0.7987111133242095]
	TIME [epoch: 10.6 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8876329551313471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8876329551313471 | validation: 0.7841589738096155]
	TIME [epoch: 10.6 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7896549005653332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7896549005653332 | validation: 0.8226587284361969]
	TIME [epoch: 10.6 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8238830336112386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8238830336112386 | validation: 0.760919080005217]
	TIME [epoch: 10.6 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8099601428742588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8099601428742588 | validation: 0.7407299251411389]
	TIME [epoch: 10.6 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9212011588774928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9212011588774928 | validation: 0.8126739937237073]
	TIME [epoch: 10.6 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8679684309410234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8679684309410234 | validation: 0.8948321878229493]
	TIME [epoch: 10.6 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9440249806375093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9440249806375093 | validation: 0.9217265087935184]
	TIME [epoch: 10.6 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8882482350048096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8882482350048096 | validation: 0.7931709579786346]
	TIME [epoch: 10.6 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8949485378794492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8949485378794492 | validation: 0.8791706142836263]
	TIME [epoch: 10.6 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9324401270572927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9324401270572927 | validation: 0.83725994305993]
	TIME [epoch: 10.6 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0489396194501308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0489396194501308 | validation: 1.1064644136635389]
	TIME [epoch: 10.6 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3229557929198823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3229557929198823 | validation: 1.1345794590222267]
	TIME [epoch: 10.6 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3165621245297334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3165621245297334 | validation: 1.2020959716531265]
	TIME [epoch: 10.6 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2273347204160685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2273347204160685 | validation: 0.7829029875123672]
	TIME [epoch: 10.6 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8715233327167311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8715233327167311 | validation: 0.7394841413229682]
	TIME [epoch: 10.6 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8012324856375835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8012324856375835 | validation: 0.9307189913884184]
	TIME [epoch: 10.6 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8562435498751787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8562435498751787 | validation: 0.9687063497399145]
	TIME [epoch: 10.6 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.075067372799284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.075067372799284 | validation: 1.4808879825136276]
	TIME [epoch: 10.6 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.280211994948064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.280211994948064 | validation: 1.2943432809018693]
	TIME [epoch: 10.6 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.969234491915883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.969234491915883 | validation: 0.9950234286910913]
	TIME [epoch: 10.6 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.951862935531485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.951862935531485 | validation: 1.6016867442056042]
	TIME [epoch: 10.6 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1050751025863221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1050751025863221 | validation: 0.6646022976056836]
	TIME [epoch: 10.6 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7848262607723254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7848262607723254 | validation: 0.7898453325093858]
	TIME [epoch: 10.6 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8712785925931262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8712785925931262 | validation: 0.7762440700484211]
	TIME [epoch: 10.6 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8295070103849913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8295070103849913 | validation: 0.7629583240791695]
	TIME [epoch: 10.6 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8458005689144714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8458005689144714 | validation: 0.8288150545241127]
	TIME [epoch: 10.6 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.128393950188602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.128393950188602 | validation: 1.224573516975841]
	TIME [epoch: 10.6 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4282474502805906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4282474502805906 | validation: 1.248685057393011]
	TIME [epoch: 10.6 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.181693782999235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.181693782999235 | validation: 1.2925303563078459]
	TIME [epoch: 10.6 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6176624627597456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6176624627597456 | validation: 1.4873715985812754]
	TIME [epoch: 10.6 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.501080175083165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.501080175083165 | validation: 1.139014962867224]
	TIME [epoch: 10.6 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1061400450307581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1061400450307581 | validation: 1.0732080779086428]
	TIME [epoch: 10.6 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.018787596480094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.018787596480094 | validation: 0.8871461226394763]
	TIME [epoch: 10.6 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9948975326871106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9948975326871106 | validation: 0.9200108986725907]
	TIME [epoch: 10.6 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.861879125423149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.861879125423149 | validation: 0.9713242039875359]
	TIME [epoch: 10.6 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9562041209365375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9562041209365375 | validation: 1.004769756542632]
	TIME [epoch: 10.6 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8951972498050736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8951972498050736 | validation: 0.8023027436363933]
	TIME [epoch: 10.6 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8735768316859959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8735768316859959 | validation: 0.7005307713860583]
	TIME [epoch: 10.6 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9306343094849152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9306343094849152 | validation: 1.4870084617583161]
	TIME [epoch: 10.6 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1086439293107397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1086439293107397 | validation: 0.9147746235139695]
	TIME [epoch: 10.6 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9724953606932315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9724953606932315 | validation: 0.7464469704778582]
	TIME [epoch: 10.6 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7979833989212597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7979833989212597 | validation: 0.7950153469369019]
	TIME [epoch: 10.6 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7841151510262373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7841151510262373 | validation: 1.101598326989524]
	TIME [epoch: 10.6 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9227787529016702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9227787529016702 | validation: 0.779813141250811]
	TIME [epoch: 10.6 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0170583014231631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0170583014231631 | validation: 1.2856175068722602]
	TIME [epoch: 10.6 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1116816116010582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1116816116010582 | validation: 1.444534634099203]
	TIME [epoch: 10.6 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3737748373197873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3737748373197873 | validation: 1.2704782055250956]
	TIME [epoch: 10.6 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2147451316289133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2147451316289133 | validation: 1.2492047185091255]
	TIME [epoch: 10.6 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0475697999737918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0475697999737918 | validation: 1.0509555234638666]
	TIME [epoch: 10.6 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0776376914429266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0776376914429266 | validation: 0.9358952723785692]
	TIME [epoch: 10.6 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8351366425635168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8351366425635168 | validation: 1.388531116189556]
	TIME [epoch: 10.6 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9050622354258626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9050622354258626 | validation: 0.6875333621805839]
	TIME [epoch: 10.6 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7798569233061761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7798569233061761 | validation: 1.4168770212831168]
	TIME [epoch: 10.6 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9775596073037647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9775596073037647 | validation: 0.9199013938708838]
	TIME [epoch: 10.6 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0286183524258605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0286183524258605 | validation: 1.257778512336996]
	TIME [epoch: 10.6 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0414203003237905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0414203003237905 | validation: 0.8695711023522509]
	TIME [epoch: 10.6 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9100191774488152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9100191774488152 | validation: 1.0837335242404147]
	TIME [epoch: 10.6 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8934022578522726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8934022578522726 | validation: 0.6845432943960372]
	TIME [epoch: 10.6 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9013109965556587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9013109965556587 | validation: 0.9184645451865535]
	TIME [epoch: 10.6 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8518410859086746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8518410859086746 | validation: 0.7946141682564556]
	TIME [epoch: 10.6 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9161453393153611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9161453393153611 | validation: 0.7231042306602905]
	TIME [epoch: 10.6 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8333732501177724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8333732501177724 | validation: 0.7091400973789002]
	TIME [epoch: 10.6 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8730715268407439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8730715268407439 | validation: 0.6969695697931461]
	TIME [epoch: 10.6 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8984558697970986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8984558697970986 | validation: 0.935158276567514]
	TIME [epoch: 10.6 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8925121517904563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8925121517904563 | validation: 0.7162719712099608]
	TIME [epoch: 10.6 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7719670035385364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7719670035385364 | validation: 0.8203551633893943]
	TIME [epoch: 10.6 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9166453226428984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9166453226428984 | validation: 0.7406741437526121]
	TIME [epoch: 10.6 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8670889625941353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8670889625941353 | validation: 0.8339043000277744]
	TIME [epoch: 10.6 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.499483486941045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.499483486941045 | validation: 2.603345420548883]
	TIME [epoch: 10.6 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4142542951591486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4142542951591486 | validation: 0.8739821811229074]
	TIME [epoch: 10.6 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8798307463571995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8798307463571995 | validation: 0.7162211658346671]
	TIME [epoch: 10.6 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8355969515551014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8355969515551014 | validation: 0.967497503528383]
	TIME [epoch: 10.6 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8451207037074207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8451207037074207 | validation: 1.3216135535224611]
	TIME [epoch: 10.6 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0316599146776786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0316599146776786 | validation: 0.9120936158216939]
	TIME [epoch: 10.6 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9135443151783786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9135443151783786 | validation: 0.8851597695713416]
	TIME [epoch: 10.6 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9023532642284053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9023532642284053 | validation: 0.796322421174506]
	TIME [epoch: 10.6 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9460166671480593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9460166671480593 | validation: 0.9148378554901132]
	TIME [epoch: 10.6 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0256732356929517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0256732356929517 | validation: 0.8711669328544959]
	TIME [epoch: 10.6 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8644767938624047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8644767938624047 | validation: 1.1148510130195057]
	TIME [epoch: 10.6 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9087352129416602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9087352129416602 | validation: 0.8942625636227733]
	TIME [epoch: 10.6 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8508124076760396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8508124076760396 | validation: 0.6826255015222914]
	TIME [epoch: 10.6 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.957479435213575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.957479435213575 | validation: 0.7720009393847835]
	TIME [epoch: 10.6 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.018999858486341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.018999858486341 | validation: 0.7592084249040838]
	TIME [epoch: 10.6 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9131215512439897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9131215512439897 | validation: 1.2184556734457561]
	TIME [epoch: 10.6 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1720692390142442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1720692390142442 | validation: 1.3190233981641655]
	TIME [epoch: 10.6 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1532017565063304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1532017565063304 | validation: 0.9177478479335776]
	TIME [epoch: 10.6 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9895141996185457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9895141996185457 | validation: 1.3689536016031008]
	TIME [epoch: 10.6 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0833400003867202		[learning rate: 0.0099755]
	Learning Rate: 0.00997547
	LOSS [training: 1.0833400003867202 | validation: 0.8204603880023389]
	TIME [epoch: 10.6 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8839631371800929		[learning rate: 0.0099449]
	Learning Rate: 0.00994489
	LOSS [training: 0.8839631371800929 | validation: 1.059190148390989]
	TIME [epoch: 10.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.888195930032914		[learning rate: 0.0099144]
	Learning Rate: 0.0099144
	LOSS [training: 0.888195930032914 | validation: 0.7171779038330587]
	TIME [epoch: 10.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8783675114471368		[learning rate: 0.009884]
	Learning Rate: 0.00988401
	LOSS [training: 0.8783675114471368 | validation: 0.8970686746667997]
	TIME [epoch: 10.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8158821904349478		[learning rate: 0.0098537]
	Learning Rate: 0.00985371
	LOSS [training: 0.8158821904349478 | validation: 0.8380828797492812]
	TIME [epoch: 10.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1298499877689083		[learning rate: 0.0098235]
	Learning Rate: 0.00982351
	LOSS [training: 1.1298499877689083 | validation: 1.8327557013509577]
	TIME [epoch: 10.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1258840292993033		[learning rate: 0.0097934]
	Learning Rate: 0.0097934
	LOSS [training: 1.1258840292993033 | validation: 0.721281721345816]
	TIME [epoch: 10.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9216413851899723		[learning rate: 0.0097634]
	Learning Rate: 0.00976337
	LOSS [training: 0.9216413851899723 | validation: 0.7402670398479287]
	TIME [epoch: 10.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8473867241032288		[learning rate: 0.0097334]
	Learning Rate: 0.00973345
	LOSS [training: 0.8473867241032288 | validation: 0.7748618445700192]
	TIME [epoch: 10.6 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8932108008640245		[learning rate: 0.0097036]
	Learning Rate: 0.00970361
	LOSS [training: 0.8932108008640245 | validation: 0.9460499860553881]
	TIME [epoch: 10.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8798744523185323		[learning rate: 0.0096739]
	Learning Rate: 0.00967386
	LOSS [training: 0.8798744523185323 | validation: 1.0381870866675393]
	TIME [epoch: 10.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9049921172066998		[learning rate: 0.0096442]
	Learning Rate: 0.00964421
	LOSS [training: 0.9049921172066998 | validation: 0.8173189625614923]
	TIME [epoch: 10.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9929247671090563		[learning rate: 0.0096146]
	Learning Rate: 0.00961465
	LOSS [training: 0.9929247671090563 | validation: 0.8461317572512617]
	TIME [epoch: 10.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9909727468040422		[learning rate: 0.0095852]
	Learning Rate: 0.00958517
	LOSS [training: 0.9909727468040422 | validation: 0.8339824991103431]
	TIME [epoch: 10.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9274663599638808		[learning rate: 0.0095558]
	Learning Rate: 0.00955579
	LOSS [training: 0.9274663599638808 | validation: 0.9556269896409612]
	TIME [epoch: 10.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.927324775432395		[learning rate: 0.0095265]
	Learning Rate: 0.0095265
	LOSS [training: 0.927324775432395 | validation: 0.7344150990067516]
	TIME [epoch: 10.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8469860238381116		[learning rate: 0.0094973]
	Learning Rate: 0.0094973
	LOSS [training: 0.8469860238381116 | validation: 1.1103871129631302]
	TIME [epoch: 10.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1569214918443655		[learning rate: 0.0094682]
	Learning Rate: 0.00946818
	LOSS [training: 1.1569214918443655 | validation: 0.9631192789188043]
	TIME [epoch: 10.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1584605868209574		[learning rate: 0.0094392]
	Learning Rate: 0.00943916
	LOSS [training: 1.1584605868209574 | validation: 1.3340111766880531]
	TIME [epoch: 10.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3644818220287764		[learning rate: 0.0094102]
	Learning Rate: 0.00941022
	LOSS [training: 1.3644818220287764 | validation: 0.9911829333610876]
	TIME [epoch: 10.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2151916929475794		[learning rate: 0.0093814]
	Learning Rate: 0.00938138
	LOSS [training: 1.2151916929475794 | validation: 1.318121383231246]
	TIME [epoch: 10.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1727027825596281		[learning rate: 0.0093526]
	Learning Rate: 0.00935262
	LOSS [training: 1.1727027825596281 | validation: 0.846348687794274]
	TIME [epoch: 10.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0627664499704426		[learning rate: 0.009324]
	Learning Rate: 0.00932395
	LOSS [training: 1.0627664499704426 | validation: 1.3063050143419621]
	TIME [epoch: 10.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3734361176197585		[learning rate: 0.0092954]
	Learning Rate: 0.00929537
	LOSS [training: 1.3734361176197585 | validation: 1.3913937225376978]
	TIME [epoch: 10.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2711425034545132		[learning rate: 0.0092669]
	Learning Rate: 0.00926687
	LOSS [training: 1.2711425034545132 | validation: 1.2010536377185606]
	TIME [epoch: 10.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8876931283708662		[learning rate: 0.0092385]
	Learning Rate: 0.00923847
	LOSS [training: 0.8876931283708662 | validation: 0.7838586480169453]
	TIME [epoch: 10.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1253380092788894		[learning rate: 0.0092101]
	Learning Rate: 0.00921015
	LOSS [training: 1.1253380092788894 | validation: 1.4038436284009408]
	TIME [epoch: 10.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.277093423530362		[learning rate: 0.0091819]
	Learning Rate: 0.00918192
	LOSS [training: 1.277093423530362 | validation: 0.9101903935904859]
	TIME [epoch: 10.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2721422885170282		[learning rate: 0.0091538]
	Learning Rate: 0.00915377
	LOSS [training: 1.2721422885170282 | validation: 1.6562612641649264]
	TIME [epoch: 10.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2722433283300547		[learning rate: 0.0091257]
	Learning Rate: 0.00912571
	LOSS [training: 1.2722433283300547 | validation: 1.20328892009576]
	TIME [epoch: 10.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0208299842882327		[learning rate: 0.0090977]
	Learning Rate: 0.00909774
	LOSS [training: 1.0208299842882327 | validation: 0.7690502685513463]
	TIME [epoch: 10.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9113168752929592		[learning rate: 0.0090698]
	Learning Rate: 0.00906985
	LOSS [training: 0.9113168752929592 | validation: 1.0739725129815758]
	TIME [epoch: 10.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9588875816549519		[learning rate: 0.009042]
	Learning Rate: 0.00904205
	LOSS [training: 0.9588875816549519 | validation: 0.7545670322511805]
	TIME [epoch: 10.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8621291978320871		[learning rate: 0.0090143]
	Learning Rate: 0.00901433
	LOSS [training: 0.8621291978320871 | validation: 0.94239711734721]
	TIME [epoch: 10.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9339675367431214		[learning rate: 0.0089867]
	Learning Rate: 0.00898669
	LOSS [training: 0.9339675367431214 | validation: 1.0461866097438572]
	TIME [epoch: 10.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0703338079940212		[learning rate: 0.0089591]
	Learning Rate: 0.00895915
	LOSS [training: 1.0703338079940212 | validation: 0.8365820473135365]
	TIME [epoch: 10.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9964639220018168		[learning rate: 0.0089317]
	Learning Rate: 0.00893168
	LOSS [training: 0.9964639220018168 | validation: 0.8054988023599355]
	TIME [epoch: 10.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9749177871704371		[learning rate: 0.0089043]
	Learning Rate: 0.0089043
	LOSS [training: 0.9749177871704371 | validation: 1.1079293470094487]
	TIME [epoch: 10.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0178580298542947		[learning rate: 0.008877]
	Learning Rate: 0.00887701
	LOSS [training: 1.0178580298542947 | validation: 0.8557346095693176]
	TIME [epoch: 10.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8846971082573465		[learning rate: 0.0088498]
	Learning Rate: 0.0088498
	LOSS [training: 0.8846971082573465 | validation: 0.8837315882981095]
	TIME [epoch: 10.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3219608356548989		[learning rate: 0.0088227]
	Learning Rate: 0.00882267
	LOSS [training: 1.3219608356548989 | validation: 0.9465579111391722]
	TIME [epoch: 10.6 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7463218459617724		[learning rate: 0.0087956]
	Learning Rate: 0.00879562
	LOSS [training: 1.7463218459617724 | validation: 1.9726964738636557]
	TIME [epoch: 10.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7966006786501532		[learning rate: 0.0087687]
	Learning Rate: 0.00876866
	LOSS [training: 1.7966006786501532 | validation: 1.4572785211045642]
	TIME [epoch: 10.6 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7424759244225991		[learning rate: 0.0087418]
	Learning Rate: 0.00874178
	LOSS [training: 1.7424759244225991 | validation: 1.7760938141697258]
	TIME [epoch: 10.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4662126524116412		[learning rate: 0.008715]
	Learning Rate: 0.00871499
	LOSS [training: 1.4662126524116412 | validation: 2.139730486374111]
	TIME [epoch: 10.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6606719467927333		[learning rate: 0.0086883]
	Learning Rate: 0.00868827
	LOSS [training: 1.6606719467927333 | validation: 1.2531757191241166]
	TIME [epoch: 10.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1566146821709495		[learning rate: 0.0086616]
	Learning Rate: 0.00866164
	LOSS [training: 1.1566146821709495 | validation: 0.9047642407007341]
	TIME [epoch: 10.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9631786314546436		[learning rate: 0.0086351]
	Learning Rate: 0.00863509
	LOSS [training: 0.9631786314546436 | validation: 0.6956822430018954]
	TIME [epoch: 10.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9970142174929997		[learning rate: 0.0086086]
	Learning Rate: 0.00860862
	LOSS [training: 0.9970142174929997 | validation: 0.7895874276933269]
	TIME [epoch: 10.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8875803967877373		[learning rate: 0.0085822]
	Learning Rate: 0.00858223
	LOSS [training: 0.8875803967877373 | validation: 0.7448579459537297]
	TIME [epoch: 10.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.893724373445381		[learning rate: 0.0085559]
	Learning Rate: 0.00855592
	LOSS [training: 0.893724373445381 | validation: 1.9857009522163134]
	TIME [epoch: 10.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.267730172033927		[learning rate: 0.0085297]
	Learning Rate: 0.00852969
	LOSS [training: 1.267730172033927 | validation: 0.8430895553662074]
	TIME [epoch: 10.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.429834923169767		[learning rate: 0.0085035]
	Learning Rate: 0.00850354
	LOSS [training: 1.429834923169767 | validation: 2.2507287295745653]
	TIME [epoch: 10.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8967980172634185		[learning rate: 0.0084775]
	Learning Rate: 0.00847748
	LOSS [training: 1.8967980172634185 | validation: 1.1484410821372424]
	TIME [epoch: 10.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2390441997761918		[learning rate: 0.0084515]
	Learning Rate: 0.00845149
	LOSS [training: 1.2390441997761918 | validation: 1.16735081326121]
	TIME [epoch: 10.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2351632945392037		[learning rate: 0.0084256]
	Learning Rate: 0.00842558
	LOSS [training: 1.2351632945392037 | validation: 1.0527636173039836]
	TIME [epoch: 10.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.173419817492548		[learning rate: 0.0083998]
	Learning Rate: 0.00839976
	LOSS [training: 1.173419817492548 | validation: 1.2115468110875853]
	TIME [epoch: 10.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.019614828944291		[learning rate: 0.008374]
	Learning Rate: 0.00837401
	LOSS [training: 1.019614828944291 | validation: 0.8270714865540708]
	TIME [epoch: 10.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8889496719877874		[learning rate: 0.0083483]
	Learning Rate: 0.00834834
	LOSS [training: 0.8889496719877874 | validation: 1.4734086797689567]
	TIME [epoch: 10.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1044412807417485		[learning rate: 0.0083227]
	Learning Rate: 0.00832275
	LOSS [training: 1.1044412807417485 | validation: 0.8218695050308452]
	TIME [epoch: 10.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0263974334894241		[learning rate: 0.0082972]
	Learning Rate: 0.00829723
	LOSS [training: 1.0263974334894241 | validation: 0.7674356447098757]
	TIME [epoch: 10.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.923580034960883		[learning rate: 0.0082718]
	Learning Rate: 0.0082718
	LOSS [training: 0.923580034960883 | validation: 0.8254467061534058]
	TIME [epoch: 10.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1738347037309045		[learning rate: 0.0082464]
	Learning Rate: 0.00824644
	LOSS [training: 1.1738347037309045 | validation: 1.4932164621358965]
	TIME [epoch: 10.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2561820928173455		[learning rate: 0.0082212]
	Learning Rate: 0.00822116
	LOSS [training: 1.2561820928173455 | validation: 1.0392490230964182]
	TIME [epoch: 10.6 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0778541347444244		[learning rate: 0.008196]
	Learning Rate: 0.00819596
	LOSS [training: 1.0778541347444244 | validation: 0.9514176530265966]
	TIME [epoch: 10.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9628198572451447		[learning rate: 0.0081708]
	Learning Rate: 0.00817084
	LOSS [training: 0.9628198572451447 | validation: 0.6999238859966141]
	TIME [epoch: 10.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8889965052946112		[learning rate: 0.0081458]
	Learning Rate: 0.00814579
	LOSS [training: 0.8889965052946112 | validation: 1.4117075571616045]
	TIME [epoch: 10.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0024935072672434		[learning rate: 0.0081208]
	Learning Rate: 0.00812082
	LOSS [training: 1.0024935072672434 | validation: 0.76869210069803]
	TIME [epoch: 10.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9389774400649216		[learning rate: 0.0080959]
	Learning Rate: 0.00809593
	LOSS [training: 0.9389774400649216 | validation: 1.0028260988013304]
	TIME [epoch: 10.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0905714620033469		[learning rate: 0.0080711]
	Learning Rate: 0.00807111
	LOSS [training: 1.0905714620033469 | validation: 0.955042770307754]
	TIME [epoch: 10.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.008022766882525		[learning rate: 0.0080464]
	Learning Rate: 0.00804637
	LOSS [training: 1.008022766882525 | validation: 1.2457111964626868]
	TIME [epoch: 10.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0354781692078936		[learning rate: 0.0080217]
	Learning Rate: 0.0080217
	LOSS [training: 1.0354781692078936 | validation: 0.817672212598153]
	TIME [epoch: 10.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8956487381718465		[learning rate: 0.0079971]
	Learning Rate: 0.00799711
	LOSS [training: 0.8956487381718465 | validation: 0.8109289297483779]
	TIME [epoch: 10.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9489857812448242		[learning rate: 0.0079726]
	Learning Rate: 0.0079726
	LOSS [training: 0.9489857812448242 | validation: 1.1509856046765616]
	TIME [epoch: 10.6 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8404067824650581		[learning rate: 0.0079482]
	Learning Rate: 0.00794816
	LOSS [training: 0.8404067824650581 | validation: 0.7499664920025486]
	TIME [epoch: 10.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.792957007262908		[learning rate: 0.0079238]
	Learning Rate: 0.0079238
	LOSS [training: 0.792957007262908 | validation: 1.2553150332095944]
	TIME [epoch: 10.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8449232746565987		[learning rate: 0.0078995]
	Learning Rate: 0.00789951
	LOSS [training: 0.8449232746565987 | validation: 0.6138428496623091]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7557110865853961		[learning rate: 0.0078753]
	Learning Rate: 0.00787529
	LOSS [training: 0.7557110865853961 | validation: 0.913831948421194]
	TIME [epoch: 10.6 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8866933536036361		[learning rate: 0.0078512]
	Learning Rate: 0.00785115
	LOSS [training: 0.8866933536036361 | validation: 0.7122956328186548]
	TIME [epoch: 10.6 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.766820310237969		[learning rate: 0.0078271]
	Learning Rate: 0.00782708
	LOSS [training: 0.766820310237969 | validation: 0.8819743368625163]
	TIME [epoch: 10.6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8813212892704136		[learning rate: 0.0078031]
	Learning Rate: 0.00780309
	LOSS [training: 0.8813212892704136 | validation: 0.9243677258209556]
	TIME [epoch: 10.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9174057598231545		[learning rate: 0.0077792]
	Learning Rate: 0.00777917
	LOSS [training: 0.9174057598231545 | validation: 0.8879403738275413]
	TIME [epoch: 10.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8847964176124824		[learning rate: 0.0077553]
	Learning Rate: 0.00775532
	LOSS [training: 0.8847964176124824 | validation: 0.7546075581870002]
	TIME [epoch: 10.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9666275654285184		[learning rate: 0.0077316]
	Learning Rate: 0.00773155
	LOSS [training: 0.9666275654285184 | validation: 0.8831425958611334]
	TIME [epoch: 10.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9781939340863601		[learning rate: 0.0077079]
	Learning Rate: 0.00770785
	LOSS [training: 0.9781939340863601 | validation: 1.0287132105102326]
	TIME [epoch: 10.6 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.916445092839426		[learning rate: 0.0076842]
	Learning Rate: 0.00768422
	LOSS [training: 0.916445092839426 | validation: 1.206708532335067]
	TIME [epoch: 10.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9897783572781457		[learning rate: 0.0076607]
	Learning Rate: 0.00766067
	LOSS [training: 0.9897783572781457 | validation: 1.1620470448245395]
	TIME [epoch: 10.6 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3254405268410232		[learning rate: 0.0076372]
	Learning Rate: 0.00763718
	LOSS [training: 1.3254405268410232 | validation: 0.7827354626411825]
	TIME [epoch: 10.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9134903205605062		[learning rate: 0.0076138]
	Learning Rate: 0.00761377
	LOSS [training: 0.9134903205605062 | validation: 0.8386640259378829]
	TIME [epoch: 10.6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8587517705744052		[learning rate: 0.0075904]
	Learning Rate: 0.00759043
	LOSS [training: 0.8587517705744052 | validation: 0.96967785901973]
	TIME [epoch: 10.6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8933007067061134		[learning rate: 0.0075672]
	Learning Rate: 0.00756717
	LOSS [training: 0.8933007067061134 | validation: 0.7271059188794975]
	TIME [epoch: 10.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8088136715303813		[learning rate: 0.007544]
	Learning Rate: 0.00754397
	LOSS [training: 0.8088136715303813 | validation: 0.9023140193595136]
	TIME [epoch: 10.6 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8337857453685384		[learning rate: 0.0075208]
	Learning Rate: 0.00752085
	LOSS [training: 0.8337857453685384 | validation: 0.8675465953239947]
	TIME [epoch: 10.6 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8027407444584818		[learning rate: 0.0074978]
	Learning Rate: 0.00749779
	LOSS [training: 0.8027407444584818 | validation: 0.7900779486330113]
	TIME [epoch: 10.6 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7812549877622279		[learning rate: 0.0074748]
	Learning Rate: 0.00747481
	LOSS [training: 0.7812549877622279 | validation: 0.6855335929142633]
	TIME [epoch: 10.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7310804034986387		[learning rate: 0.0074519]
	Learning Rate: 0.00745189
	LOSS [training: 0.7310804034986387 | validation: 0.8561860283961551]
	TIME [epoch: 10.6 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7697083038477486		[learning rate: 0.0074291]
	Learning Rate: 0.00742905
	LOSS [training: 0.7697083038477486 | validation: 0.7230352248930626]
	TIME [epoch: 10.6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9352163870818169		[learning rate: 0.0074063]
	Learning Rate: 0.00740628
	LOSS [training: 0.9352163870818169 | validation: 1.5848316734655348]
	TIME [epoch: 10.6 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9461016208246473		[learning rate: 0.0073836]
	Learning Rate: 0.00738357
	LOSS [training: 0.9461016208246473 | validation: 1.2976727153034426]
	TIME [epoch: 10.6 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9051773571483677		[learning rate: 0.0073609]
	Learning Rate: 0.00736094
	LOSS [training: 0.9051773571483677 | validation: 0.7941400076541922]
	TIME [epoch: 10.6 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8341947988102911		[learning rate: 0.0073384]
	Learning Rate: 0.00733838
	LOSS [training: 0.8341947988102911 | validation: 0.7219590053846792]
	TIME [epoch: 10.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7462914040835169		[learning rate: 0.0073159]
	Learning Rate: 0.00731588
	LOSS [training: 0.7462914040835169 | validation: 0.6343844187295595]
	TIME [epoch: 10.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.763109279737588		[learning rate: 0.0072935]
	Learning Rate: 0.00729345
	LOSS [training: 0.763109279737588 | validation: 0.812635472980918]
	TIME [epoch: 10.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7714018903320348		[learning rate: 0.0072711]
	Learning Rate: 0.0072711
	LOSS [training: 0.7714018903320348 | validation: 0.6869084636035181]
	TIME [epoch: 10.6 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7341837260475323		[learning rate: 0.0072488]
	Learning Rate: 0.00724881
	LOSS [training: 0.7341837260475323 | validation: 0.8754484255930507]
	TIME [epoch: 10.6 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8851351799954166		[learning rate: 0.0072266]
	Learning Rate: 0.00722659
	LOSS [training: 0.8851351799954166 | validation: 0.8748433913466805]
	TIME [epoch: 10.6 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6452469507124189		[learning rate: 0.0072044]
	Learning Rate: 0.00720444
	LOSS [training: 0.6452469507124189 | validation: 0.6205138066785896]
	TIME [epoch: 10.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7156440719037678		[learning rate: 0.0071824]
	Learning Rate: 0.00718235
	LOSS [training: 0.7156440719037678 | validation: 0.6459736954773009]
	TIME [epoch: 10.6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7330447652322949		[learning rate: 0.0071603]
	Learning Rate: 0.00716033
	LOSS [training: 0.7330447652322949 | validation: 0.5452198136262995]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6637325727823378		[learning rate: 0.0071384]
	Learning Rate: 0.00713838
	LOSS [training: 0.6637325727823378 | validation: 0.6051802537236748]
	TIME [epoch: 10.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6637146903659553		[learning rate: 0.0071165]
	Learning Rate: 0.0071165
	LOSS [training: 0.6637146903659553 | validation: 0.6256948447646081]
	TIME [epoch: 10.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9007359644180433		[learning rate: 0.0070947]
	Learning Rate: 0.00709469
	LOSS [training: 0.9007359644180433 | validation: 1.097385612573533]
	TIME [epoch: 10.6 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.735744326118416		[learning rate: 0.0070729]
	Learning Rate: 0.00707294
	LOSS [training: 0.735744326118416 | validation: 0.658594385648246]
	TIME [epoch: 10.6 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6439134144435799		[learning rate: 0.0070513]
	Learning Rate: 0.00705126
	LOSS [training: 0.6439134144435799 | validation: 0.8402566350319282]
	TIME [epoch: 10.6 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6534066456959309		[learning rate: 0.0070296]
	Learning Rate: 0.00702964
	LOSS [training: 0.6534066456959309 | validation: 0.5956558782363688]
	TIME [epoch: 10.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6332498094095513		[learning rate: 0.0070081]
	Learning Rate: 0.00700809
	LOSS [training: 0.6332498094095513 | validation: 0.6813800465469649]
	TIME [epoch: 10.6 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6614503591187081		[learning rate: 0.0069866]
	Learning Rate: 0.00698661
	LOSS [training: 0.6614503591187081 | validation: 0.5053777858142003]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7065351468298906		[learning rate: 0.0069652]
	Learning Rate: 0.0069652
	LOSS [training: 0.7065351468298906 | validation: 0.5892742548627108]
	TIME [epoch: 10.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7114617179367478		[learning rate: 0.0069438]
	Learning Rate: 0.00694384
	LOSS [training: 0.7114617179367478 | validation: 0.5201004468602753]
	TIME [epoch: 10.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7187757549925426		[learning rate: 0.0069226]
	Learning Rate: 0.00692256
	LOSS [training: 0.7187757549925426 | validation: 0.4700976355094062]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6028046743709392		[learning rate: 0.0069013]
	Learning Rate: 0.00690134
	LOSS [training: 0.6028046743709392 | validation: 0.5356943612135407]
	TIME [epoch: 10.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6551080591622154		[learning rate: 0.0068802]
	Learning Rate: 0.00688018
	LOSS [training: 0.6551080591622154 | validation: 0.5740599569774608]
	TIME [epoch: 10.6 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5519890941210882		[learning rate: 0.0068591]
	Learning Rate: 0.00685909
	LOSS [training: 0.5519890941210882 | validation: 0.5555799289372353]
	TIME [epoch: 10.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5224201655693074		[learning rate: 0.0068381]
	Learning Rate: 0.00683807
	LOSS [training: 0.5224201655693074 | validation: 0.5690078290691549]
	TIME [epoch: 10.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5964816182913465		[learning rate: 0.0068171]
	Learning Rate: 0.0068171
	LOSS [training: 0.5964816182913465 | validation: 1.032651178003314]
	TIME [epoch: 10.6 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.642123192994345		[learning rate: 0.0067962]
	Learning Rate: 0.00679621
	LOSS [training: 0.642123192994345 | validation: 0.44984478321880017]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5994935677419948		[learning rate: 0.0067754]
	Learning Rate: 0.00677537
	LOSS [training: 0.5994935677419948 | validation: 0.5925947344913507]
	TIME [epoch: 10.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6420582897855039		[learning rate: 0.0067546]
	Learning Rate: 0.00675461
	LOSS [training: 0.6420582897855039 | validation: 0.6301650604357166]
	TIME [epoch: 10.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6225925146235988		[learning rate: 0.0067339]
	Learning Rate: 0.0067339
	LOSS [training: 0.6225925146235988 | validation: 0.8250841010974854]
	TIME [epoch: 10.5 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6409258393997672		[learning rate: 0.0067133]
	Learning Rate: 0.00671326
	LOSS [training: 0.6409258393997672 | validation: 0.6973531094957568]
	TIME [epoch: 10.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7466755783698614		[learning rate: 0.0066927]
	Learning Rate: 0.00669268
	LOSS [training: 0.7466755783698614 | validation: 0.8698246460328093]
	TIME [epoch: 10.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7003816001837633		[learning rate: 0.0066722]
	Learning Rate: 0.00667216
	LOSS [training: 0.7003816001837633 | validation: 0.6032585857474162]
	TIME [epoch: 10.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5448530591714322		[learning rate: 0.0066517]
	Learning Rate: 0.00665171
	LOSS [training: 0.5448530591714322 | validation: 0.49247662807109166]
	TIME [epoch: 10.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6003190696233862		[learning rate: 0.0066313]
	Learning Rate: 0.00663132
	LOSS [training: 0.6003190696233862 | validation: 0.4350712467431026]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_634.pth
	Model improved!!!
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5450023480944679		[learning rate: 0.006611]
	Learning Rate: 0.00661099
	LOSS [training: 0.5450023480944679 | validation: 0.5674847921721794]
	TIME [epoch: 10.6 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6062600867408217		[learning rate: 0.0065907]
	Learning Rate: 0.00659073
	LOSS [training: 0.6062600867408217 | validation: 1.2754876385277463]
	TIME [epoch: 10.5 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0622372641860454		[learning rate: 0.0065705]
	Learning Rate: 0.00657052
	LOSS [training: 1.0622372641860454 | validation: 0.7996243959381903]
	TIME [epoch: 10.6 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7881790903891146		[learning rate: 0.0065504]
	Learning Rate: 0.00655038
	LOSS [training: 0.7881790903891146 | validation: 0.5926026625173391]
	TIME [epoch: 10.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6513433963737988		[learning rate: 0.0065303]
	Learning Rate: 0.0065303
	LOSS [training: 0.6513433963737988 | validation: 0.6420536250291571]
	TIME [epoch: 10.6 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5189494118900565		[learning rate: 0.0065103]
	Learning Rate: 0.00651028
	LOSS [training: 0.5189494118900565 | validation: 0.6761495821758854]
	TIME [epoch: 10.6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6701964431610381		[learning rate: 0.0064903]
	Learning Rate: 0.00649033
	LOSS [training: 0.6701964431610381 | validation: 0.5543880138191805]
	TIME [epoch: 10.6 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.53033901354845		[learning rate: 0.0064704]
	Learning Rate: 0.00647043
	LOSS [training: 0.53033901354845 | validation: 0.5330939307876816]
	TIME [epoch: 10.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7086026210594196		[learning rate: 0.0064506]
	Learning Rate: 0.0064506
	LOSS [training: 0.7086026210594196 | validation: 0.6005618427235703]
	TIME [epoch: 10.5 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7012502677550634		[learning rate: 0.0064308]
	Learning Rate: 0.00643082
	LOSS [training: 0.7012502677550634 | validation: 0.6898157936481026]
	TIME [epoch: 10.6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8090187689026687		[learning rate: 0.0064111]
	Learning Rate: 0.00641111
	LOSS [training: 0.8090187689026687 | validation: 0.6739629644409196]
	TIME [epoch: 10.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5725976380572718		[learning rate: 0.0063915]
	Learning Rate: 0.00639146
	LOSS [training: 0.5725976380572718 | validation: 0.7967929230001295]
	TIME [epoch: 10.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7271599428562568		[learning rate: 0.0063719]
	Learning Rate: 0.00637187
	LOSS [training: 0.7271599428562568 | validation: 0.6299023727611724]
	TIME [epoch: 10.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7601835886950268		[learning rate: 0.0063523]
	Learning Rate: 0.00635233
	LOSS [training: 0.7601835886950268 | validation: 0.5724296411301518]
	TIME [epoch: 10.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5779001704191192		[learning rate: 0.0063329]
	Learning Rate: 0.00633286
	LOSS [training: 0.5779001704191192 | validation: 0.7151479846542255]
	TIME [epoch: 10.5 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6644965970397921		[learning rate: 0.0063134]
	Learning Rate: 0.00631345
	LOSS [training: 0.6644965970397921 | validation: 0.6945739236230438]
	TIME [epoch: 10.6 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7310512323390849		[learning rate: 0.0062941]
	Learning Rate: 0.0062941
	LOSS [training: 0.7310512323390849 | validation: 0.5916339519774568]
	TIME [epoch: 10.5 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5855410511531781		[learning rate: 0.0062748]
	Learning Rate: 0.0062748
	LOSS [training: 0.5855410511531781 | validation: 0.5931756113551829]
	TIME [epoch: 10.6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8627183976697548		[learning rate: 0.0062556]
	Learning Rate: 0.00625557
	LOSS [training: 0.8627183976697548 | validation: 0.7698817918647555]
	TIME [epoch: 10.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7028529755812408		[learning rate: 0.0062364]
	Learning Rate: 0.00623639
	LOSS [training: 0.7028529755812408 | validation: 0.7368245020019183]
	TIME [epoch: 10.6 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6819397470400766		[learning rate: 0.0062173]
	Learning Rate: 0.00621727
	LOSS [training: 0.6819397470400766 | validation: 0.6879796044686944]
	TIME [epoch: 10.6 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6525451375763776		[learning rate: 0.0061982]
	Learning Rate: 0.00619822
	LOSS [training: 0.6525451375763776 | validation: 0.5223332582367659]
	TIME [epoch: 10.5 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5731796752587961		[learning rate: 0.0061792]
	Learning Rate: 0.00617922
	LOSS [training: 0.5731796752587961 | validation: 0.7691936975843242]
	TIME [epoch: 10.6 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6286304284401798		[learning rate: 0.0061603]
	Learning Rate: 0.00616027
	LOSS [training: 0.6286304284401798 | validation: 0.6564789475308499]
	TIME [epoch: 10.5 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6288052651264485		[learning rate: 0.0061414]
	Learning Rate: 0.00614139
	LOSS [training: 0.6288052651264485 | validation: 0.5007463656743627]
	TIME [epoch: 10.6 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.58897658577594		[learning rate: 0.0061226]
	Learning Rate: 0.00612256
	LOSS [training: 0.58897658577594 | validation: 0.5800603402838149]
	TIME [epoch: 10.5 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5385894521723655		[learning rate: 0.0061038]
	Learning Rate: 0.0061038
	LOSS [training: 0.5385894521723655 | validation: 0.4876948818688801]
	TIME [epoch: 10.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.539070187473442		[learning rate: 0.0060851]
	Learning Rate: 0.00608508
	LOSS [training: 0.539070187473442 | validation: 0.5373900071936896]
	TIME [epoch: 10.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5631802793703241		[learning rate: 0.0060664]
	Learning Rate: 0.00606643
	LOSS [training: 0.5631802793703241 | validation: 0.547248286669671]
	TIME [epoch: 10.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5599988803330462		[learning rate: 0.0060478]
	Learning Rate: 0.00604784
	LOSS [training: 0.5599988803330462 | validation: 0.5849335162667059]
	TIME [epoch: 10.5 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5366302026318497		[learning rate: 0.0060293]
	Learning Rate: 0.0060293
	LOSS [training: 0.5366302026318497 | validation: 0.5756936122983995]
	TIME [epoch: 10.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5786462451335216		[learning rate: 0.0060108]
	Learning Rate: 0.00601081
	LOSS [training: 0.5786462451335216 | validation: 0.6031206012887924]
	TIME [epoch: 10.6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6220392281661618		[learning rate: 0.0059924]
	Learning Rate: 0.00599239
	LOSS [training: 0.6220392281661618 | validation: 0.5700438777931461]
	TIME [epoch: 10.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5706271394011176		[learning rate: 0.005974]
	Learning Rate: 0.00597402
	LOSS [training: 0.5706271394011176 | validation: 0.5736809021492161]
	TIME [epoch: 10.6 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5317218397193153		[learning rate: 0.0059557]
	Learning Rate: 0.00595571
	LOSS [training: 0.5317218397193153 | validation: 0.4979062999286417]
	TIME [epoch: 10.6 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4754502757699398		[learning rate: 0.0059375]
	Learning Rate: 0.00593745
	LOSS [training: 0.4754502757699398 | validation: 0.5838608525521228]
	TIME [epoch: 10.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5578768243216078		[learning rate: 0.0059192]
	Learning Rate: 0.00591925
	LOSS [training: 0.5578768243216078 | validation: 0.5425489250423117]
	TIME [epoch: 10.6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5221619133811402		[learning rate: 0.0059011]
	Learning Rate: 0.0059011
	LOSS [training: 0.5221619133811402 | validation: 0.5618273855384426]
	TIME [epoch: 10.6 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5716600595282497		[learning rate: 0.005883]
	Learning Rate: 0.00588302
	LOSS [training: 0.5716600595282497 | validation: 0.5546742968186265]
	TIME [epoch: 10.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6315858514475302		[learning rate: 0.005865]
	Learning Rate: 0.00586498
	LOSS [training: 0.6315858514475302 | validation: 0.46836052371558906]
	TIME [epoch: 10.6 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46169750440151913		[learning rate: 0.005847]
	Learning Rate: 0.005847
	LOSS [training: 0.46169750440151913 | validation: 0.46397780080885964]
	TIME [epoch: 10.6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5373271060802877		[learning rate: 0.0058291]
	Learning Rate: 0.00582908
	LOSS [training: 0.5373271060802877 | validation: 0.6290758657679654]
	TIME [epoch: 10.5 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5353782127455428		[learning rate: 0.0058112]
	Learning Rate: 0.00581121
	LOSS [training: 0.5353782127455428 | validation: 0.6965019164599616]
	TIME [epoch: 10.5 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5460496899482001		[learning rate: 0.0057934]
	Learning Rate: 0.0057934
	LOSS [training: 0.5460496899482001 | validation: 0.39136350962320055]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_678.pth
	Model improved!!!
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4963612650955266		[learning rate: 0.0057756]
	Learning Rate: 0.00577564
	LOSS [training: 0.4963612650955266 | validation: 0.6334232398558802]
	TIME [epoch: 10.6 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6152440034493761		[learning rate: 0.0057579]
	Learning Rate: 0.00575793
	LOSS [training: 0.6152440034493761 | validation: 0.6111883528407586]
	TIME [epoch: 10.5 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5600386634793064		[learning rate: 0.0057403]
	Learning Rate: 0.00574028
	LOSS [training: 0.5600386634793064 | validation: 0.6564498025833759]
	TIME [epoch: 10.5 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6075647878823018		[learning rate: 0.0057227]
	Learning Rate: 0.00572269
	LOSS [training: 0.6075647878823018 | validation: 0.5309559203939048]
	TIME [epoch: 10.6 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5614788018739045		[learning rate: 0.0057051]
	Learning Rate: 0.00570514
	LOSS [training: 0.5614788018739045 | validation: 0.542615241232485]
	TIME [epoch: 10.5 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6608503431227191		[learning rate: 0.0056877]
	Learning Rate: 0.00568766
	LOSS [training: 0.6608503431227191 | validation: 0.7195041487590963]
	TIME [epoch: 10.5 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5511569698652279		[learning rate: 0.0056702]
	Learning Rate: 0.00567022
	LOSS [training: 0.5511569698652279 | validation: 0.5734454089682858]
	TIME [epoch: 10.6 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5023262537539084		[learning rate: 0.0056528]
	Learning Rate: 0.00565284
	LOSS [training: 0.5023262537539084 | validation: 0.5170315773670687]
	TIME [epoch: 10.5 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.451068289400986		[learning rate: 0.0056355]
	Learning Rate: 0.00563551
	LOSS [training: 0.451068289400986 | validation: 0.45876474118427846]
	TIME [epoch: 10.5 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42783706079480766		[learning rate: 0.0056182]
	Learning Rate: 0.00561824
	LOSS [training: 0.42783706079480766 | validation: 0.47073284025923157]
	TIME [epoch: 10.5 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46281600279985896		[learning rate: 0.005601]
	Learning Rate: 0.00560101
	LOSS [training: 0.46281600279985896 | validation: 0.5139468227726514]
	TIME [epoch: 10.5 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46883664988197093		[learning rate: 0.0055838]
	Learning Rate: 0.00558384
	LOSS [training: 0.46883664988197093 | validation: 0.5291314445751165]
	TIME [epoch: 10.5 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5085521478099501		[learning rate: 0.0055667]
	Learning Rate: 0.00556673
	LOSS [training: 0.5085521478099501 | validation: 0.790245409617523]
	TIME [epoch: 10.6 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5956545704992933		[learning rate: 0.0055497]
	Learning Rate: 0.00554966
	LOSS [training: 0.5956545704992933 | validation: 0.5705412100321562]
	TIME [epoch: 10.6 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5511756513779706		[learning rate: 0.0055327]
	Learning Rate: 0.00553265
	LOSS [training: 0.5511756513779706 | validation: 0.538626359226797]
	TIME [epoch: 10.5 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5895871539519308		[learning rate: 0.0055157]
	Learning Rate: 0.00551569
	LOSS [training: 0.5895871539519308 | validation: 0.5005252636607053]
	TIME [epoch: 10.5 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49805735529512773		[learning rate: 0.0054988]
	Learning Rate: 0.00549878
	LOSS [training: 0.49805735529512773 | validation: 0.4058118814451339]
	TIME [epoch: 10.6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4403947257500572		[learning rate: 0.0054819]
	Learning Rate: 0.00548193
	LOSS [training: 0.4403947257500572 | validation: 0.6578122741592568]
	TIME [epoch: 10.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.560514933651731		[learning rate: 0.0054651]
	Learning Rate: 0.00546512
	LOSS [training: 0.560514933651731 | validation: 1.0128938426734408]
	TIME [epoch: 10.5 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8107584087141266		[learning rate: 0.0054484]
	Learning Rate: 0.00544837
	LOSS [training: 0.8107584087141266 | validation: 0.4762651199562549]
	TIME [epoch: 10.5 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49865351016960185		[learning rate: 0.0054317]
	Learning Rate: 0.00543167
	LOSS [training: 0.49865351016960185 | validation: 0.4947934992509494]
	TIME [epoch: 10.6 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41508345390154755		[learning rate: 0.005415]
	Learning Rate: 0.00541502
	LOSS [training: 0.41508345390154755 | validation: 0.3509708524100904]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_700.pth
	Model improved!!!
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4249167750488704		[learning rate: 0.0053984]
	Learning Rate: 0.00539842
	LOSS [training: 0.4249167750488704 | validation: 0.47400883208856687]
	TIME [epoch: 10.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5449160553659226		[learning rate: 0.0053819]
	Learning Rate: 0.00538187
	LOSS [training: 0.5449160553659226 | validation: 0.5460976775149066]
	TIME [epoch: 10.6 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5329094967801419		[learning rate: 0.0053654]
	Learning Rate: 0.00536537
	LOSS [training: 0.5329094967801419 | validation: 0.6001135845051369]
	TIME [epoch: 10.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47808920736125965		[learning rate: 0.0053489]
	Learning Rate: 0.00534893
	LOSS [training: 0.47808920736125965 | validation: 0.4176812063982044]
	TIME [epoch: 10.6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5271732379088571		[learning rate: 0.0053325]
	Learning Rate: 0.00533253
	LOSS [training: 0.5271732379088571 | validation: 0.5376982854222238]
	TIME [epoch: 10.6 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8916940711438517		[learning rate: 0.0053162]
	Learning Rate: 0.00531618
	LOSS [training: 0.8916940711438517 | validation: 0.882289446843171]
	TIME [epoch: 10.6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.373096781726626		[learning rate: 0.0052999]
	Learning Rate: 0.00529989
	LOSS [training: 1.373096781726626 | validation: 1.0239421400591633]
	TIME [epoch: 10.6 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4234992791973144		[learning rate: 0.0052836]
	Learning Rate: 0.00528364
	LOSS [training: 1.4234992791973144 | validation: 1.7974136619281715]
	TIME [epoch: 10.6 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0205283492959512		[learning rate: 0.0052674]
	Learning Rate: 0.00526744
	LOSS [training: 1.0205283492959512 | validation: 0.7875052659581775]
	TIME [epoch: 10.6 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9129956526524563		[learning rate: 0.0052513]
	Learning Rate: 0.0052513
	LOSS [training: 0.9129956526524563 | validation: 0.5765647980572597]
	TIME [epoch: 10.6 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5784021297572994		[learning rate: 0.0052352]
	Learning Rate: 0.0052352
	LOSS [training: 0.5784021297572994 | validation: 0.6076416316103992]
	TIME [epoch: 10.6 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5257153323389765		[learning rate: 0.0052192]
	Learning Rate: 0.00521915
	LOSS [training: 0.5257153323389765 | validation: 0.4806413638131704]
	TIME [epoch: 10.6 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4803454526854346		[learning rate: 0.0052032]
	Learning Rate: 0.00520315
	LOSS [training: 0.4803454526854346 | validation: 0.4791422997071301]
	TIME [epoch: 10.6 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49693859399294765		[learning rate: 0.0051872]
	Learning Rate: 0.0051872
	LOSS [training: 0.49693859399294765 | validation: 0.6205961787153619]
	TIME [epoch: 10.6 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5956607837758583		[learning rate: 0.0051713]
	Learning Rate: 0.0051713
	LOSS [training: 0.5956607837758583 | validation: 0.683725424410891]
	TIME [epoch: 10.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5843255816761024		[learning rate: 0.0051555]
	Learning Rate: 0.00515545
	LOSS [training: 0.5843255816761024 | validation: 0.604353474878452]
	TIME [epoch: 10.5 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6129602177853608		[learning rate: 0.0051396]
	Learning Rate: 0.00513965
	LOSS [training: 0.6129602177853608 | validation: 0.6830307299988988]
	TIME [epoch: 10.6 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7521244403468568		[learning rate: 0.0051239]
	Learning Rate: 0.00512389
	LOSS [training: 0.7521244403468568 | validation: 0.6251405858426731]
	TIME [epoch: 10.6 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6588579518430908		[learning rate: 0.0051082]
	Learning Rate: 0.00510819
	LOSS [training: 0.6588579518430908 | validation: 0.8287438843726258]
	TIME [epoch: 10.6 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6947229088779692		[learning rate: 0.0050925]
	Learning Rate: 0.00509253
	LOSS [training: 0.6947229088779692 | validation: 0.6093965368329687]
	TIME [epoch: 10.6 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7158973191319337		[learning rate: 0.0050769]
	Learning Rate: 0.00507692
	LOSS [training: 0.7158973191319337 | validation: 0.5956472192499933]
	TIME [epoch: 10.6 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7372946497633013		[learning rate: 0.0050614]
	Learning Rate: 0.00506135
	LOSS [training: 0.7372946497633013 | validation: 0.670737466880033]
	TIME [epoch: 10.5 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6589100059660769		[learning rate: 0.0050458]
	Learning Rate: 0.00504584
	LOSS [training: 0.6589100059660769 | validation: 0.7373028699213379]
	TIME [epoch: 10.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6799349286804567		[learning rate: 0.0050304]
	Learning Rate: 0.00503037
	LOSS [training: 0.6799349286804567 | validation: 0.786202994892808]
	TIME [epoch: 10.6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.974338846888022		[learning rate: 0.005015]
	Learning Rate: 0.00501495
	LOSS [training: 0.974338846888022 | validation: 0.8518153774318341]
	TIME [epoch: 10.6 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7693128499472456		[learning rate: 0.0049996]
	Learning Rate: 0.00499958
	LOSS [training: 0.7693128499472456 | validation: 0.6264901825571848]
	TIME [epoch: 10.6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6264154541926443		[learning rate: 0.0049843]
	Learning Rate: 0.00498425
	LOSS [training: 0.6264154541926443 | validation: 0.7716194856130557]
	TIME [epoch: 10.5 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6556975270620651		[learning rate: 0.004969]
	Learning Rate: 0.00496897
	LOSS [training: 0.6556975270620651 | validation: 0.5200958227841207]
	TIME [epoch: 10.5 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5394171882558745		[learning rate: 0.0049537]
	Learning Rate: 0.00495374
	LOSS [training: 0.5394171882558745 | validation: 0.5007339569299544]
	TIME [epoch: 10.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5221827595521117		[learning rate: 0.0049386]
	Learning Rate: 0.00493856
	LOSS [training: 0.5221827595521117 | validation: 0.5574684487890202]
	TIME [epoch: 10.6 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5016020636647415		[learning rate: 0.0049234]
	Learning Rate: 0.00492342
	LOSS [training: 0.5016020636647415 | validation: 0.6148734098166938]
	TIME [epoch: 10.6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5288465055528785		[learning rate: 0.0049083]
	Learning Rate: 0.00490832
	LOSS [training: 0.5288465055528785 | validation: 0.6395064144949366]
	TIME [epoch: 10.6 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5995348113856372		[learning rate: 0.0048933]
	Learning Rate: 0.00489328
	LOSS [training: 0.5995348113856372 | validation: 0.6652345391510893]
	TIME [epoch: 10.5 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6356133772642623		[learning rate: 0.0048783]
	Learning Rate: 0.00487828
	LOSS [training: 0.6356133772642623 | validation: 0.637460000478656]
	TIME [epoch: 10.6 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5732026818156581		[learning rate: 0.0048633]
	Learning Rate: 0.00486333
	LOSS [training: 0.5732026818156581 | validation: 0.705907906011989]
	TIME [epoch: 10.6 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.702374936759435		[learning rate: 0.0048484]
	Learning Rate: 0.00484842
	LOSS [training: 0.702374936759435 | validation: 1.1699002000368819]
	TIME [epoch: 10.6 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9130859412055321		[learning rate: 0.0048336]
	Learning Rate: 0.00483355
	LOSS [training: 0.9130859412055321 | validation: 0.7040304051191143]
	TIME [epoch: 10.6 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6389558598884135		[learning rate: 0.0048187]
	Learning Rate: 0.00481874
	LOSS [training: 0.6389558598884135 | validation: 0.5480456852608848]
	TIME [epoch: 10.6 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5666786062214663		[learning rate: 0.004804]
	Learning Rate: 0.00480397
	LOSS [training: 0.5666786062214663 | validation: 0.5952390367370629]
	TIME [epoch: 10.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5664830198012327		[learning rate: 0.0047892]
	Learning Rate: 0.00478924
	LOSS [training: 0.5664830198012327 | validation: 0.5733292268901825]
	TIME [epoch: 10.6 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6814847183184936		[learning rate: 0.0047746]
	Learning Rate: 0.00477456
	LOSS [training: 0.6814847183184936 | validation: 0.834820561195717]
	TIME [epoch: 10.5 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7288237832113966		[learning rate: 0.0047599]
	Learning Rate: 0.00475992
	LOSS [training: 0.7288237832113966 | validation: 0.6606484465462498]
	TIME [epoch: 10.6 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5928394681042046		[learning rate: 0.0047453]
	Learning Rate: 0.00474533
	LOSS [training: 0.5928394681042046 | validation: 0.6876219563250209]
	TIME [epoch: 10.5 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7555842727015583		[learning rate: 0.0047308]
	Learning Rate: 0.00473079
	LOSS [training: 0.7555842727015583 | validation: 0.7556212329458103]
	TIME [epoch: 10.5 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6701867757616303		[learning rate: 0.0047163]
	Learning Rate: 0.00471628
	LOSS [training: 0.6701867757616303 | validation: 0.555170476511079]
	TIME [epoch: 10.6 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6050499445001051		[learning rate: 0.0047018]
	Learning Rate: 0.00470183
	LOSS [training: 0.6050499445001051 | validation: 0.5354315395974539]
	TIME [epoch: 10.6 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.545717790073794		[learning rate: 0.0046874]
	Learning Rate: 0.00468741
	LOSS [training: 0.545717790073794 | validation: 0.5240491951857547]
	TIME [epoch: 10.6 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5860796075149095		[learning rate: 0.004673]
	Learning Rate: 0.00467305
	LOSS [training: 0.5860796075149095 | validation: 0.5550900509851936]
	TIME [epoch: 10.6 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5099073177096475		[learning rate: 0.0046587]
	Learning Rate: 0.00465872
	LOSS [training: 0.5099073177096475 | validation: 0.5617449348159778]
	TIME [epoch: 10.6 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5274807530814811		[learning rate: 0.0046444]
	Learning Rate: 0.00464444
	LOSS [training: 0.5274807530814811 | validation: 0.5892801673474648]
	TIME [epoch: 10.6 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5079785255619723		[learning rate: 0.0046302]
	Learning Rate: 0.0046302
	LOSS [training: 0.5079785255619723 | validation: 0.46579543577318655]
	TIME [epoch: 10.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5325881709149346		[learning rate: 0.004616]
	Learning Rate: 0.00461601
	LOSS [training: 0.5325881709149346 | validation: 0.5786350862816223]
	TIME [epoch: 10.6 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5164748175614351		[learning rate: 0.0046019]
	Learning Rate: 0.00460186
	LOSS [training: 0.5164748175614351 | validation: 0.48397197916364193]
	TIME [epoch: 10.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5481893622375239		[learning rate: 0.0045878]
	Learning Rate: 0.00458775
	LOSS [training: 0.5481893622375239 | validation: 0.5996249355011055]
	TIME [epoch: 10.6 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6497164856713745		[learning rate: 0.0045737]
	Learning Rate: 0.00457369
	LOSS [training: 0.6497164856713745 | validation: 0.607184655692845]
	TIME [epoch: 10.6 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6313666629992409		[learning rate: 0.0045597]
	Learning Rate: 0.00455967
	LOSS [training: 0.6313666629992409 | validation: 0.5753433657457666]
	TIME [epoch: 10.6 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5564862213864693		[learning rate: 0.0045457]
	Learning Rate: 0.00454569
	LOSS [training: 0.5564862213864693 | validation: 0.5450038331345306]
	TIME [epoch: 10.6 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5699094164754057		[learning rate: 0.0045318]
	Learning Rate: 0.00453176
	LOSS [training: 0.5699094164754057 | validation: 0.5166902325948354]
	TIME [epoch: 10.6 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5441000898838035		[learning rate: 0.0045179]
	Learning Rate: 0.00451787
	LOSS [training: 0.5441000898838035 | validation: 0.5541909047241897]
	TIME [epoch: 10.6 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4947918656511156		[learning rate: 0.004504]
	Learning Rate: 0.00450402
	LOSS [training: 0.4947918656511156 | validation: 0.4773671601700625]
	TIME [epoch: 10.6 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44712784382460014		[learning rate: 0.0044902]
	Learning Rate: 0.00449021
	LOSS [training: 0.44712784382460014 | validation: 0.7072173109535614]
	TIME [epoch: 10.6 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.074810236336717		[learning rate: 0.0044764]
	Learning Rate: 0.00447645
	LOSS [training: 1.074810236336717 | validation: 0.8363878864459788]
	TIME [epoch: 10.5 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7483441207775092		[learning rate: 0.0044627]
	Learning Rate: 0.00446272
	LOSS [training: 0.7483441207775092 | validation: 0.5230459677961511]
	TIME [epoch: 10.5 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6287397986771351		[learning rate: 0.004449]
	Learning Rate: 0.00444904
	LOSS [training: 0.6287397986771351 | validation: 0.8856020468449345]
	TIME [epoch: 10.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7037153262859164		[learning rate: 0.0044354]
	Learning Rate: 0.0044354
	LOSS [training: 0.7037153262859164 | validation: 0.5921840852523133]
	TIME [epoch: 10.6 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6527371434022609		[learning rate: 0.0044218]
	Learning Rate: 0.00442181
	LOSS [training: 0.6527371434022609 | validation: 0.5916160886165227]
	TIME [epoch: 10.6 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5323010484234417		[learning rate: 0.0044083]
	Learning Rate: 0.00440825
	LOSS [training: 0.5323010484234417 | validation: 0.535172630531948]
	TIME [epoch: 10.6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5048419935510551		[learning rate: 0.0043947]
	Learning Rate: 0.00439474
	LOSS [training: 0.5048419935510551 | validation: 0.49487257260214257]
	TIME [epoch: 10.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44967204059780286		[learning rate: 0.0043813]
	Learning Rate: 0.00438127
	LOSS [training: 0.44967204059780286 | validation: 0.4412907579475521]
	TIME [epoch: 10.6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8177671023443105		[learning rate: 0.0043678]
	Learning Rate: 0.00436784
	LOSS [training: 0.8177671023443105 | validation: 0.5597901407550071]
	TIME [epoch: 10.6 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7384362779694746		[learning rate: 0.0043544]
	Learning Rate: 0.00435445
	LOSS [training: 0.7384362779694746 | validation: 1.1346905125976472]
	TIME [epoch: 10.6 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7634087902486126		[learning rate: 0.0043411]
	Learning Rate: 0.0043411
	LOSS [training: 0.7634087902486126 | validation: 0.6965976289777026]
	TIME [epoch: 10.6 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7505801414347564		[learning rate: 0.0043278]
	Learning Rate: 0.0043278
	LOSS [training: 0.7505801414347564 | validation: 0.7697377067622182]
	TIME [epoch: 10.6 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7240737791819003		[learning rate: 0.0043145]
	Learning Rate: 0.00431453
	LOSS [training: 0.7240737791819003 | validation: 0.8344040141950805]
	TIME [epoch: 10.6 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6920095667492758		[learning rate: 0.0043013]
	Learning Rate: 0.0043013
	LOSS [training: 0.6920095667492758 | validation: 0.5486361484357711]
	TIME [epoch: 10.6 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5775914577969729		[learning rate: 0.0042881]
	Learning Rate: 0.00428812
	LOSS [training: 0.5775914577969729 | validation: 0.7039738279961716]
	TIME [epoch: 10.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7818849393953592		[learning rate: 0.004275]
	Learning Rate: 0.00427497
	LOSS [training: 0.7818849393953592 | validation: 0.8407827086587281]
	TIME [epoch: 10.6 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6100869064817334		[learning rate: 0.0042619]
	Learning Rate: 0.00426187
	LOSS [training: 0.6100869064817334 | validation: 0.4983388677345997]
	TIME [epoch: 10.6 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6485378564318368		[learning rate: 0.0042488]
	Learning Rate: 0.0042488
	LOSS [training: 0.6485378564318368 | validation: 0.5859556368997006]
	TIME [epoch: 10.6 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5338142820190539		[learning rate: 0.0042358]
	Learning Rate: 0.00423578
	LOSS [training: 0.5338142820190539 | validation: 0.5372734737900611]
	TIME [epoch: 10.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5181263660315103		[learning rate: 0.0042228]
	Learning Rate: 0.00422279
	LOSS [training: 0.5181263660315103 | validation: 0.4625184279079153]
	TIME [epoch: 10.6 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5588751027543188		[learning rate: 0.0042098]
	Learning Rate: 0.00420985
	LOSS [training: 0.5588751027543188 | validation: 0.553827065420708]
	TIME [epoch: 10.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5014493709065568		[learning rate: 0.0041969]
	Learning Rate: 0.00419695
	LOSS [training: 0.5014493709065568 | validation: 0.4273502564970957]
	TIME [epoch: 10.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4213031187140846		[learning rate: 0.0041841]
	Learning Rate: 0.00418408
	LOSS [training: 0.4213031187140846 | validation: 0.3837632593670249]
	TIME [epoch: 10.6 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46984596414929836		[learning rate: 0.0041713]
	Learning Rate: 0.00417125
	LOSS [training: 0.46984596414929836 | validation: 0.3887113039640925]
	TIME [epoch: 10.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38668324514906277		[learning rate: 0.0041585]
	Learning Rate: 0.00415847
	LOSS [training: 0.38668324514906277 | validation: 0.3897230234959391]
	TIME [epoch: 10.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40144623100176524		[learning rate: 0.0041457]
	Learning Rate: 0.00414572
	LOSS [training: 0.40144623100176524 | validation: 0.42667139644569163]
	TIME [epoch: 10.6 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41925096645737253		[learning rate: 0.004133]
	Learning Rate: 0.00413301
	LOSS [training: 0.41925096645737253 | validation: 0.5236205046354716]
	TIME [epoch: 10.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3987695417598186		[learning rate: 0.0041203]
	Learning Rate: 0.00412034
	LOSS [training: 0.3987695417598186 | validation: 0.40479787199677547]
	TIME [epoch: 10.6 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3838988199356176		[learning rate: 0.0041077]
	Learning Rate: 0.00410771
	LOSS [training: 0.3838988199356176 | validation: 0.4361779161051672]
	TIME [epoch: 10.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38054538335024135		[learning rate: 0.0040951]
	Learning Rate: 0.00409512
	LOSS [training: 0.38054538335024135 | validation: 0.5490078771782948]
	TIME [epoch: 10.6 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4488246870457483		[learning rate: 0.0040826]
	Learning Rate: 0.00408257
	LOSS [training: 0.4488246870457483 | validation: 0.4205011354839567]
	TIME [epoch: 10.6 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42871178729044496		[learning rate: 0.0040701]
	Learning Rate: 0.00407005
	LOSS [training: 0.42871178729044496 | validation: 0.6996564430872504]
	TIME [epoch: 10.6 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5163477328587522		[learning rate: 0.0040576]
	Learning Rate: 0.00405758
	LOSS [training: 0.5163477328587522 | validation: 0.4427288802913325]
	TIME [epoch: 10.6 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4422498958365736		[learning rate: 0.0040451]
	Learning Rate: 0.00404514
	LOSS [training: 0.4422498958365736 | validation: 0.5837083699709587]
	TIME [epoch: 10.6 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5132819967323397		[learning rate: 0.0040327]
	Learning Rate: 0.00403274
	LOSS [training: 0.5132819967323397 | validation: 0.4625546784857011]
	TIME [epoch: 10.6 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43672759561480345		[learning rate: 0.0040204]
	Learning Rate: 0.00402038
	LOSS [training: 0.43672759561480345 | validation: 0.42802475901339065]
	TIME [epoch: 10.6 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4169737288391572		[learning rate: 0.0040081]
	Learning Rate: 0.00400805
	LOSS [training: 0.4169737288391572 | validation: 0.4214068516568844]
	TIME [epoch: 10.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4299438712505709		[learning rate: 0.0039958]
	Learning Rate: 0.00399577
	LOSS [training: 0.4299438712505709 | validation: 0.42204132280357315]
	TIME [epoch: 10.6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4148371203149058		[learning rate: 0.0039835]
	Learning Rate: 0.00398352
	LOSS [training: 0.4148371203149058 | validation: 0.42034904213230306]
	TIME [epoch: 10.5 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.474770595616293		[learning rate: 0.0039713]
	Learning Rate: 0.00397131
	LOSS [training: 0.474770595616293 | validation: 0.4398255290063193]
	TIME [epoch: 10.5 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4137867016028266		[learning rate: 0.0039591]
	Learning Rate: 0.00395913
	LOSS [training: 0.4137867016028266 | validation: 0.406277052833811]
	TIME [epoch: 10.6 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3840359747540818		[learning rate: 0.003947]
	Learning Rate: 0.003947
	LOSS [training: 0.3840359747540818 | validation: 0.3780205826756799]
	TIME [epoch: 10.6 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.394558180120797		[learning rate: 0.0039349]
	Learning Rate: 0.0039349
	LOSS [training: 0.394558180120797 | validation: 0.3640402705329152]
	TIME [epoch: 10.6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35432931498606646		[learning rate: 0.0039228]
	Learning Rate: 0.00392283
	LOSS [training: 0.35432931498606646 | validation: 0.39318970560086697]
	TIME [epoch: 10.6 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3892265070241633		[learning rate: 0.0039108]
	Learning Rate: 0.00391081
	LOSS [training: 0.3892265070241633 | validation: 0.3701343709174572]
	TIME [epoch: 10.6 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3743511659012865		[learning rate: 0.0038988]
	Learning Rate: 0.00389882
	LOSS [training: 0.3743511659012865 | validation: 0.3994388314123891]
	TIME [epoch: 10.6 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3760602658331197		[learning rate: 0.0038869]
	Learning Rate: 0.00388687
	LOSS [training: 0.3760602658331197 | validation: 0.35809620573799505]
	TIME [epoch: 10.6 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38325493611138745		[learning rate: 0.003875]
	Learning Rate: 0.00387495
	LOSS [training: 0.38325493611138745 | validation: 0.40022348461310053]
	TIME [epoch: 10.6 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37262375491804745		[learning rate: 0.0038631]
	Learning Rate: 0.00386308
	LOSS [training: 0.37262375491804745 | validation: 0.37646280351228917]
	TIME [epoch: 10.5 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38732821403131024		[learning rate: 0.0038512]
	Learning Rate: 0.00385123
	LOSS [training: 0.38732821403131024 | validation: 0.3680903605266035]
	TIME [epoch: 10.6 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45675395685956294		[learning rate: 0.0038394]
	Learning Rate: 0.00383943
	LOSS [training: 0.45675395685956294 | validation: 0.4566422585701298]
	TIME [epoch: 10.6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40010691137834115		[learning rate: 0.0038277]
	Learning Rate: 0.00382766
	LOSS [training: 0.40010691137834115 | validation: 0.3511578770853012]
	TIME [epoch: 10.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36719973742591		[learning rate: 0.0038159]
	Learning Rate: 0.00381593
	LOSS [training: 0.36719973742591 | validation: 0.32800018661064095]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_814.pth
	Model improved!!!
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3714037912631456		[learning rate: 0.0038042]
	Learning Rate: 0.00380423
	LOSS [training: 0.3714037912631456 | validation: 0.36728347206193634]
	TIME [epoch: 10.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35519928566766407		[learning rate: 0.0037926]
	Learning Rate: 0.00379257
	LOSS [training: 0.35519928566766407 | validation: 0.36116816400916146]
	TIME [epoch: 10.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4174122047158959		[learning rate: 0.0037809]
	Learning Rate: 0.00378094
	LOSS [training: 0.4174122047158959 | validation: 0.39069705455591375]
	TIME [epoch: 10.6 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.603281486950556		[learning rate: 0.0037694]
	Learning Rate: 0.00376935
	LOSS [training: 0.603281486950556 | validation: 0.6269871619835233]
	TIME [epoch: 10.6 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6163835251955941		[learning rate: 0.0037578]
	Learning Rate: 0.0037578
	LOSS [training: 0.6163835251955941 | validation: 0.45368298386243433]
	TIME [epoch: 10.6 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4174307186853833		[learning rate: 0.0037463]
	Learning Rate: 0.00374628
	LOSS [training: 0.4174307186853833 | validation: 0.36928737209636764]
	TIME [epoch: 10.6 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3929963484879373		[learning rate: 0.0037348]
	Learning Rate: 0.00373479
	LOSS [training: 0.3929963484879373 | validation: 0.4397616861660607]
	TIME [epoch: 10.6 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5386253415038468		[learning rate: 0.0037233]
	Learning Rate: 0.00372335
	LOSS [training: 0.5386253415038468 | validation: 0.3876866743573205]
	TIME [epoch: 10.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4316303011608424		[learning rate: 0.0037119]
	Learning Rate: 0.00371193
	LOSS [training: 0.4316303011608424 | validation: 0.4837892106276149]
	TIME [epoch: 10.5 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4253268968485201		[learning rate: 0.0037006]
	Learning Rate: 0.00370055
	LOSS [training: 0.4253268968485201 | validation: 0.44021046025863897]
	TIME [epoch: 10.6 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3935363621573099		[learning rate: 0.0036892]
	Learning Rate: 0.00368921
	LOSS [training: 0.3935363621573099 | validation: 0.42010111814742473]
	TIME [epoch: 10.6 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4053553248247428		[learning rate: 0.0036779]
	Learning Rate: 0.0036779
	LOSS [training: 0.4053553248247428 | validation: 0.36273938545942025]
	TIME [epoch: 10.6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41450722014960933		[learning rate: 0.0036666]
	Learning Rate: 0.00366663
	LOSS [training: 0.41450722014960933 | validation: 0.4499874841996482]
	TIME [epoch: 10.6 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5217070393443951		[learning rate: 0.0036554]
	Learning Rate: 0.00365539
	LOSS [training: 0.5217070393443951 | validation: 0.6051899753741673]
	TIME [epoch: 10.6 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5221965987597802		[learning rate: 0.0036442]
	Learning Rate: 0.00364418
	LOSS [training: 0.5221965987597802 | validation: 0.5456318214327179]
	TIME [epoch: 10.6 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5765758202640963		[learning rate: 0.003633]
	Learning Rate: 0.00363301
	LOSS [training: 0.5765758202640963 | validation: 0.7172924244722313]
	TIME [epoch: 10.5 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6044891346701216		[learning rate: 0.0036219]
	Learning Rate: 0.00362187
	LOSS [training: 0.6044891346701216 | validation: 0.5643737256332618]
	TIME [epoch: 10.6 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6303908028984089		[learning rate: 0.0036108]
	Learning Rate: 0.00361077
	LOSS [training: 0.6303908028984089 | validation: 0.6368243188832271]
	TIME [epoch: 10.6 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5936175214034587		[learning rate: 0.0035997]
	Learning Rate: 0.0035997
	LOSS [training: 0.5936175214034587 | validation: 0.47220575839406265]
	TIME [epoch: 10.6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41916601293649985		[learning rate: 0.0035887]
	Learning Rate: 0.00358867
	LOSS [training: 0.41916601293649985 | validation: 0.40176050801607815]
	TIME [epoch: 10.6 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3870069491752214		[learning rate: 0.0035777]
	Learning Rate: 0.00357767
	LOSS [training: 0.3870069491752214 | validation: 0.5265698408270864]
	TIME [epoch: 10.6 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4566077994536066		[learning rate: 0.0035667]
	Learning Rate: 0.0035667
	LOSS [training: 0.4566077994536066 | validation: 0.48051225233276257]
	TIME [epoch: 10.5 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38669494103390206		[learning rate: 0.0035558]
	Learning Rate: 0.00355577
	LOSS [training: 0.38669494103390206 | validation: 0.4427643527842605]
	TIME [epoch: 10.6 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46725081600835966		[learning rate: 0.0035449]
	Learning Rate: 0.00354487
	LOSS [training: 0.46725081600835966 | validation: 0.4152322874975499]
	TIME [epoch: 10.6 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4584913069924327		[learning rate: 0.003534]
	Learning Rate: 0.003534
	LOSS [training: 0.4584913069924327 | validation: 0.48302885854701744]
	TIME [epoch: 10.6 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42915581251599244		[learning rate: 0.0035232]
	Learning Rate: 0.00352317
	LOSS [training: 0.42915581251599244 | validation: 0.335308311450147]
	TIME [epoch: 10.6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3370547265807488		[learning rate: 0.0035124]
	Learning Rate: 0.00351237
	LOSS [training: 0.3370547265807488 | validation: 0.3786913840001887]
	TIME [epoch: 10.6 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.453510149157228		[learning rate: 0.0035016]
	Learning Rate: 0.0035016
	LOSS [training: 0.453510149157228 | validation: 0.5686063626488548]
	TIME [epoch: 10.6 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46656299221972264		[learning rate: 0.0034909]
	Learning Rate: 0.00349087
	LOSS [training: 0.46656299221972264 | validation: 0.46914878263768184]
	TIME [epoch: 10.6 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38778841077788206		[learning rate: 0.0034802]
	Learning Rate: 0.00348017
	LOSS [training: 0.38778841077788206 | validation: 0.4119369717748539]
	TIME [epoch: 10.6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40571673835830635		[learning rate: 0.0034695]
	Learning Rate: 0.0034695
	LOSS [training: 0.40571673835830635 | validation: 0.5165859688739883]
	TIME [epoch: 10.6 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40168858944073804		[learning rate: 0.0034589]
	Learning Rate: 0.00345886
	LOSS [training: 0.40168858944073804 | validation: 0.3803997043541695]
	TIME [epoch: 10.6 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40153748283724394		[learning rate: 0.0034483]
	Learning Rate: 0.00344826
	LOSS [training: 0.40153748283724394 | validation: 0.40775438302009975]
	TIME [epoch: 10.5 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3687529822983821		[learning rate: 0.0034377]
	Learning Rate: 0.00343769
	LOSS [training: 0.3687529822983821 | validation: 0.38072066702259777]
	TIME [epoch: 10.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38265220266588174		[learning rate: 0.0034272]
	Learning Rate: 0.00342715
	LOSS [training: 0.38265220266588174 | validation: 0.38024201017235215]
	TIME [epoch: 10.6 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3696950020194477		[learning rate: 0.0034166]
	Learning Rate: 0.00341665
	LOSS [training: 0.3696950020194477 | validation: 0.37874030314268353]
	TIME [epoch: 10.6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3704972607725699		[learning rate: 0.0034062]
	Learning Rate: 0.00340617
	LOSS [training: 0.3704972607725699 | validation: 0.36106839701423116]
	TIME [epoch: 10.6 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3409571179015935		[learning rate: 0.0033957]
	Learning Rate: 0.00339573
	LOSS [training: 0.3409571179015935 | validation: 0.3144616289008094]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_852.pth
	Model improved!!!
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32914329255910724		[learning rate: 0.0033853]
	Learning Rate: 0.00338532
	LOSS [training: 0.32914329255910724 | validation: 0.4689917442895705]
	TIME [epoch: 10.6 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3594060185126058		[learning rate: 0.0033749]
	Learning Rate: 0.00337494
	LOSS [training: 0.3594060185126058 | validation: 0.360221608549188]
	TIME [epoch: 10.6 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3409132621498646		[learning rate: 0.0033646]
	Learning Rate: 0.0033646
	LOSS [training: 0.3409132621498646 | validation: 0.38896674666554004]
	TIME [epoch: 10.6 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35171556344904625		[learning rate: 0.0033543]
	Learning Rate: 0.00335428
	LOSS [training: 0.35171556344904625 | validation: 0.4259321189314051]
	TIME [epoch: 10.6 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33769060222745406		[learning rate: 0.003344]
	Learning Rate: 0.003344
	LOSS [training: 0.33769060222745406 | validation: 0.39412799952685384]
	TIME [epoch: 10.5 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41689954791671513		[learning rate: 0.0033338]
	Learning Rate: 0.00333375
	LOSS [training: 0.41689954791671513 | validation: 0.44637224092887656]
	TIME [epoch: 10.6 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4021938433545701		[learning rate: 0.0033235]
	Learning Rate: 0.00332353
	LOSS [training: 0.4021938433545701 | validation: 0.3890946346563456]
	TIME [epoch: 10.6 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3457118509818571		[learning rate: 0.0033133]
	Learning Rate: 0.00331334
	LOSS [training: 0.3457118509818571 | validation: 0.4133791877438197]
	TIME [epoch: 10.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3704204456590319		[learning rate: 0.0033032]
	Learning Rate: 0.00330319
	LOSS [training: 0.3704204456590319 | validation: 0.450449090420218]
	TIME [epoch: 10.6 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3880029572175955		[learning rate: 0.0032931]
	Learning Rate: 0.00329306
	LOSS [training: 0.3880029572175955 | validation: 0.3865855595761468]
	TIME [epoch: 10.6 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3691555180067535		[learning rate: 0.003283]
	Learning Rate: 0.00328297
	LOSS [training: 0.3691555180067535 | validation: 0.45429509881274727]
	TIME [epoch: 10.6 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4316452824092494		[learning rate: 0.0032729]
	Learning Rate: 0.0032729
	LOSS [training: 0.4316452824092494 | validation: 0.45829494618831634]
	TIME [epoch: 10.6 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4064316251929932		[learning rate: 0.0032629]
	Learning Rate: 0.00326287
	LOSS [training: 0.4064316251929932 | validation: 0.41197272807695073]
	TIME [epoch: 10.6 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.347821728221161		[learning rate: 0.0032529]
	Learning Rate: 0.00325287
	LOSS [training: 0.347821728221161 | validation: 0.4241337225352655]
	TIME [epoch: 10.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3968518945816087		[learning rate: 0.0032429]
	Learning Rate: 0.0032429
	LOSS [training: 0.3968518945816087 | validation: 0.38260692932619395]
	TIME [epoch: 10.6 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3597075848333552		[learning rate: 0.003233]
	Learning Rate: 0.00323296
	LOSS [training: 0.3597075848333552 | validation: 0.40081395887613497]
	TIME [epoch: 10.6 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.360844308862725		[learning rate: 0.003223]
	Learning Rate: 0.00322305
	LOSS [training: 0.360844308862725 | validation: 0.3558360085086794]
	TIME [epoch: 10.6 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3762450353586312		[learning rate: 0.0032132]
	Learning Rate: 0.00321317
	LOSS [training: 0.3762450353586312 | validation: 0.48028872924559113]
	TIME [epoch: 10.6 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4217340795014108		[learning rate: 0.0032033]
	Learning Rate: 0.00320332
	LOSS [training: 0.4217340795014108 | validation: 0.324740668785176]
	TIME [epoch: 10.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35194774248776295		[learning rate: 0.0031935]
	Learning Rate: 0.0031935
	LOSS [training: 0.35194774248776295 | validation: 0.38014276584183615]
	TIME [epoch: 10.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3717604231546708		[learning rate: 0.0031837]
	Learning Rate: 0.00318371
	LOSS [training: 0.3717604231546708 | validation: 0.3610264490615174]
	TIME [epoch: 10.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3547134941568081		[learning rate: 0.0031739]
	Learning Rate: 0.00317395
	LOSS [training: 0.3547134941568081 | validation: 0.469409929885303]
	TIME [epoch: 10.6 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4333270280397688		[learning rate: 0.0031642]
	Learning Rate: 0.00316422
	LOSS [training: 0.4333270280397688 | validation: 0.36847037466224136]
	TIME [epoch: 10.6 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49914750276786696		[learning rate: 0.0031545]
	Learning Rate: 0.00315452
	LOSS [training: 0.49914750276786696 | validation: 0.46567777905110336]
	TIME [epoch: 10.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5033509554584268		[learning rate: 0.0031449]
	Learning Rate: 0.00314485
	LOSS [training: 0.5033509554584268 | validation: 0.42248093831485806]
	TIME [epoch: 10.6 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42624991660824973		[learning rate: 0.0031352]
	Learning Rate: 0.00313521
	LOSS [training: 0.42624991660824973 | validation: 0.4306736925274143]
	TIME [epoch: 10.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4074141274168846		[learning rate: 0.0031256]
	Learning Rate: 0.0031256
	LOSS [training: 0.4074141274168846 | validation: 0.34454596746534416]
	TIME [epoch: 10.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39240269852237064		[learning rate: 0.003116]
	Learning Rate: 0.00311602
	LOSS [training: 0.39240269852237064 | validation: 0.44631910217486265]
	TIME [epoch: 10.6 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4735805251153229		[learning rate: 0.0031065]
	Learning Rate: 0.00310647
	LOSS [training: 0.4735805251153229 | validation: 0.4737415391708927]
	TIME [epoch: 10.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48793793300839916		[learning rate: 0.0030969]
	Learning Rate: 0.00309694
	LOSS [training: 0.48793793300839916 | validation: 0.4796674868875661]
	TIME [epoch: 10.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47852195021836436		[learning rate: 0.0030875]
	Learning Rate: 0.00308745
	LOSS [training: 0.47852195021836436 | validation: 0.4759949321351657]
	TIME [epoch: 10.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4726258656697543		[learning rate: 0.003078]
	Learning Rate: 0.00307799
	LOSS [training: 0.4726258656697543 | validation: 0.37623012838857384]
	TIME [epoch: 10.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4272620213766468		[learning rate: 0.0030686]
	Learning Rate: 0.00306855
	LOSS [training: 0.4272620213766468 | validation: 0.35623389103904957]
	TIME [epoch: 10.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44475381921071955		[learning rate: 0.0030591]
	Learning Rate: 0.00305914
	LOSS [training: 0.44475381921071955 | validation: 0.38689531119649107]
	TIME [epoch: 10.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4322831003572888		[learning rate: 0.0030498]
	Learning Rate: 0.00304977
	LOSS [training: 0.4322831003572888 | validation: 0.35115031057933893]
	TIME [epoch: 10.6 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38311645758333407		[learning rate: 0.0030404]
	Learning Rate: 0.00304042
	LOSS [training: 0.38311645758333407 | validation: 0.3771445392175581]
	TIME [epoch: 10.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3863279024318487		[learning rate: 0.0030311]
	Learning Rate: 0.0030311
	LOSS [training: 0.3863279024318487 | validation: 0.37318191389199046]
	TIME [epoch: 10.6 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37275885718286805		[learning rate: 0.0030218]
	Learning Rate: 0.00302181
	LOSS [training: 0.37275885718286805 | validation: 0.3111871474569763]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_890.pth
	Model improved!!!
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3750116934128101		[learning rate: 0.0030125]
	Learning Rate: 0.00301254
	LOSS [training: 0.3750116934128101 | validation: 0.34340181260310515]
	TIME [epoch: 10.5 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3571200938136486		[learning rate: 0.0030033]
	Learning Rate: 0.00300331
	LOSS [training: 0.3571200938136486 | validation: 0.34126089856903347]
	TIME [epoch: 10.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3350101834847995		[learning rate: 0.0029941]
	Learning Rate: 0.0029941
	LOSS [training: 0.3350101834847995 | validation: 0.3329782591541367]
	TIME [epoch: 10.6 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31921467036017204		[learning rate: 0.0029849]
	Learning Rate: 0.00298492
	LOSS [training: 0.31921467036017204 | validation: 0.3346936685317196]
	TIME [epoch: 10.6 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39159873841187		[learning rate: 0.0029758]
	Learning Rate: 0.00297577
	LOSS [training: 0.39159873841187 | validation: 0.41200694579046593]
	TIME [epoch: 10.6 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3578921660354094		[learning rate: 0.0029667]
	Learning Rate: 0.00296665
	LOSS [training: 0.3578921660354094 | validation: 0.3418274512015876]
	TIME [epoch: 10.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3342647252029368		[learning rate: 0.0029576]
	Learning Rate: 0.00295756
	LOSS [training: 0.3342647252029368 | validation: 0.3230046902009557]
	TIME [epoch: 10.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3414378380076056		[learning rate: 0.0029485]
	Learning Rate: 0.00294849
	LOSS [training: 0.3414378380076056 | validation: 0.40150440838073154]
	TIME [epoch: 10.6 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3532958642150831		[learning rate: 0.0029395]
	Learning Rate: 0.00293945
	LOSS [training: 0.3532958642150831 | validation: 0.34832714945844456]
	TIME [epoch: 10.6 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3247960265654741		[learning rate: 0.0029304]
	Learning Rate: 0.00293044
	LOSS [training: 0.3247960265654741 | validation: 0.3334807494559219]
	TIME [epoch: 10.6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3110779169350001		[learning rate: 0.0029215]
	Learning Rate: 0.00292146
	LOSS [training: 0.3110779169350001 | validation: 0.3339673499923682]
	TIME [epoch: 10.6 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37329672964007354		[learning rate: 0.0029125]
	Learning Rate: 0.0029125
	LOSS [training: 0.37329672964007354 | validation: 0.3985888642768352]
	TIME [epoch: 10.5 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3836294876477272		[learning rate: 0.0029036]
	Learning Rate: 0.00290358
	LOSS [training: 0.3836294876477272 | validation: 0.38684002749139823]
	TIME [epoch: 10.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.391811600534639		[learning rate: 0.0028947]
	Learning Rate: 0.00289468
	LOSS [training: 0.391811600534639 | validation: 0.4465839557175849]
	TIME [epoch: 10.6 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5992749707874965		[learning rate: 0.0028858]
	Learning Rate: 0.0028858
	LOSS [training: 0.5992749707874965 | validation: 0.6785078244182015]
	TIME [epoch: 10.6 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6741369436174347		[learning rate: 0.002877]
	Learning Rate: 0.00287696
	LOSS [training: 0.6741369436174347 | validation: 0.4661866805378584]
	TIME [epoch: 10.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45850739018876424		[learning rate: 0.0028681]
	Learning Rate: 0.00286814
	LOSS [training: 0.45850739018876424 | validation: 0.3408937412449069]
	TIME [epoch: 10.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4910814283686772		[learning rate: 0.0028593]
	Learning Rate: 0.00285935
	LOSS [training: 0.4910814283686772 | validation: 0.7215064257253152]
	TIME [epoch: 10.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6928087658279616		[learning rate: 0.0028506]
	Learning Rate: 0.00285058
	LOSS [training: 0.6928087658279616 | validation: 0.4380119337343427]
	TIME [epoch: 10.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4142297801486835		[learning rate: 0.0028418]
	Learning Rate: 0.00284184
	LOSS [training: 0.4142297801486835 | validation: 0.3900443498113475]
	TIME [epoch: 10.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3489807708276632		[learning rate: 0.0028331]
	Learning Rate: 0.00283313
	LOSS [training: 0.3489807708276632 | validation: 0.3748160111753235]
	TIME [epoch: 10.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3869102672196948		[learning rate: 0.0028244]
	Learning Rate: 0.00282445
	LOSS [training: 0.3869102672196948 | validation: 0.5094826667044042]
	TIME [epoch: 10.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38882811421151786		[learning rate: 0.0028158]
	Learning Rate: 0.00281579
	LOSS [training: 0.38882811421151786 | validation: 0.39440982846249123]
	TIME [epoch: 10.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3630589639040346		[learning rate: 0.0028072]
	Learning Rate: 0.00280716
	LOSS [training: 0.3630589639040346 | validation: 0.40581837930618336]
	TIME [epoch: 10.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3935354145785965		[learning rate: 0.0027986]
	Learning Rate: 0.00279855
	LOSS [training: 0.3935354145785965 | validation: 0.3816491536401797]
	TIME [epoch: 10.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4020528666090982		[learning rate: 0.00279]
	Learning Rate: 0.00278997
	LOSS [training: 0.4020528666090982 | validation: 0.3876163912674958]
	TIME [epoch: 10.6 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41302390918163495		[learning rate: 0.0027814]
	Learning Rate: 0.00278142
	LOSS [training: 0.41302390918163495 | validation: 0.4988227963243116]
	TIME [epoch: 10.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41892462032089384		[learning rate: 0.0027729]
	Learning Rate: 0.00277289
	LOSS [training: 0.41892462032089384 | validation: 0.46183706021424964]
	TIME [epoch: 10.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43067727989020294		[learning rate: 0.0027644]
	Learning Rate: 0.00276439
	LOSS [training: 0.43067727989020294 | validation: 0.4527780189510296]
	TIME [epoch: 10.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40721579658541013		[learning rate: 0.0027559]
	Learning Rate: 0.00275592
	LOSS [training: 0.40721579658541013 | validation: 0.4260916197367425]
	TIME [epoch: 10.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44034444364025316		[learning rate: 0.0027475]
	Learning Rate: 0.00274747
	LOSS [training: 0.44034444364025316 | validation: 0.4100775562153406]
	TIME [epoch: 10.6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4330788988851787		[learning rate: 0.002739]
	Learning Rate: 0.00273905
	LOSS [training: 0.4330788988851787 | validation: 0.5364923056780463]
	TIME [epoch: 10.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5201098734210354		[learning rate: 0.0027307]
	Learning Rate: 0.00273065
	LOSS [training: 0.5201098734210354 | validation: 0.5037253436359663]
	TIME [epoch: 10.6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5636491696426004		[learning rate: 0.0027223]
	Learning Rate: 0.00272228
	LOSS [training: 0.5636491696426004 | validation: 0.45943446603073185]
	TIME [epoch: 10.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42517401900218266		[learning rate: 0.0027139]
	Learning Rate: 0.00271394
	LOSS [training: 0.42517401900218266 | validation: 0.38340296832138765]
	TIME [epoch: 10.6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35768567028582454		[learning rate: 0.0027056]
	Learning Rate: 0.00270562
	LOSS [training: 0.35768567028582454 | validation: 0.41729167060930417]
	TIME [epoch: 10.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.397105954871342		[learning rate: 0.0026973]
	Learning Rate: 0.00269733
	LOSS [training: 0.397105954871342 | validation: 0.4057085178141104]
	TIME [epoch: 10.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4228987251799473		[learning rate: 0.0026891]
	Learning Rate: 0.00268906
	LOSS [training: 0.4228987251799473 | validation: 0.43840977118481306]
	TIME [epoch: 10.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40214310104195317		[learning rate: 0.0026808]
	Learning Rate: 0.00268081
	LOSS [training: 0.40214310104195317 | validation: 0.5079097342363404]
	TIME [epoch: 10.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4076121066393924		[learning rate: 0.0026726]
	Learning Rate: 0.0026726
	LOSS [training: 0.4076121066393924 | validation: 0.36799235854524803]
	TIME [epoch: 10.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3783283924480501		[learning rate: 0.0026644]
	Learning Rate: 0.0026644
	LOSS [training: 0.3783283924480501 | validation: 0.46937812157328823]
	TIME [epoch: 10.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4434362352533541		[learning rate: 0.0026562]
	Learning Rate: 0.00265624
	LOSS [training: 0.4434362352533541 | validation: 0.6317955351660521]
	TIME [epoch: 10.6 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7650757710305472		[learning rate: 0.0026481]
	Learning Rate: 0.00264809
	LOSS [training: 0.7650757710305472 | validation: 0.5214691108418897]
	TIME [epoch: 10.6 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.427244211075847		[learning rate: 0.00264]
	Learning Rate: 0.00263998
	LOSS [training: 0.427244211075847 | validation: 0.40900152877513885]
	TIME [epoch: 10.6 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49584886190148864		[learning rate: 0.0026319]
	Learning Rate: 0.00263188
	LOSS [training: 0.49584886190148864 | validation: 0.4917311407404566]
	TIME [epoch: 10.6 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5502491778287236		[learning rate: 0.0026238]
	Learning Rate: 0.00262382
	LOSS [training: 0.5502491778287236 | validation: 0.49958021008719344]
	TIME [epoch: 10.6 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4754988796457087		[learning rate: 0.0026158]
	Learning Rate: 0.00261577
	LOSS [training: 0.4754988796457087 | validation: 0.42794385275526653]
	TIME [epoch: 10.6 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42834585658720814		[learning rate: 0.0026078]
	Learning Rate: 0.00260775
	LOSS [training: 0.42834585658720814 | validation: 0.41709734588432384]
	TIME [epoch: 10.6 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44940841496788175		[learning rate: 0.0025998]
	Learning Rate: 0.00259976
	LOSS [training: 0.44940841496788175 | validation: 0.40933529760896487]
	TIME [epoch: 10.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36656848034052136		[learning rate: 0.0025918]
	Learning Rate: 0.00259179
	LOSS [training: 0.36656848034052136 | validation: 0.32510419503230004]
	TIME [epoch: 10.6 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3218739033704514		[learning rate: 0.0025838]
	Learning Rate: 0.00258385
	LOSS [training: 0.3218739033704514 | validation: 0.3500913870382395]
	TIME [epoch: 10.6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3622715147211544		[learning rate: 0.0025759]
	Learning Rate: 0.00257593
	LOSS [training: 0.3622715147211544 | validation: 0.3732591877702147]
	TIME [epoch: 10.6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3777456579269026		[learning rate: 0.002568]
	Learning Rate: 0.00256803
	LOSS [training: 0.3777456579269026 | validation: 0.39557927688514954]
	TIME [epoch: 10.6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36712463214989627		[learning rate: 0.0025602]
	Learning Rate: 0.00256016
	LOSS [training: 0.36712463214989627 | validation: 0.3668330778515154]
	TIME [epoch: 10.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3646577298845565		[learning rate: 0.0025523]
	Learning Rate: 0.00255231
	LOSS [training: 0.3646577298845565 | validation: 0.34083210628670374]
	TIME [epoch: 10.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3372873426524591		[learning rate: 0.0025445]
	Learning Rate: 0.00254449
	LOSS [training: 0.3372873426524591 | validation: 0.3969172068046352]
	TIME [epoch: 10.6 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44465945578217747		[learning rate: 0.0025367]
	Learning Rate: 0.00253669
	LOSS [training: 0.44465945578217747 | validation: 0.42584678718208885]
	TIME [epoch: 10.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3865692587569793		[learning rate: 0.0025289]
	Learning Rate: 0.00252891
	LOSS [training: 0.3865692587569793 | validation: 0.3957896254284297]
	TIME [epoch: 10.6 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4248186250317131		[learning rate: 0.0025212]
	Learning Rate: 0.00252116
	LOSS [training: 0.4248186250317131 | validation: 0.40813645297572715]
	TIME [epoch: 10.6 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3704130713178233		[learning rate: 0.0025134]
	Learning Rate: 0.00251343
	LOSS [training: 0.3704130713178233 | validation: 0.39054068411306175]
	TIME [epoch: 10.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40166978827194555		[learning rate: 0.0025057]
	Learning Rate: 0.00250572
	LOSS [training: 0.40166978827194555 | validation: 0.443023861298108]
	TIME [epoch: 10.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3838043019142023		[learning rate: 0.002498]
	Learning Rate: 0.00249804
	LOSS [training: 0.3838043019142023 | validation: 0.41844196951802964]
	TIME [epoch: 10.6 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4124609949254653		[learning rate: 0.0024904]
	Learning Rate: 0.00249039
	LOSS [training: 0.4124609949254653 | validation: 0.42549309443759004]
	TIME [epoch: 10.6 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39317118147987673		[learning rate: 0.0024828]
	Learning Rate: 0.00248275
	LOSS [training: 0.39317118147987673 | validation: 0.3851524266197935]
	TIME [epoch: 10.6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35483790486522204		[learning rate: 0.0024751]
	Learning Rate: 0.00247514
	LOSS [training: 0.35483790486522204 | validation: 0.3892193522235358]
	TIME [epoch: 10.6 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33399659485270555		[learning rate: 0.0024676]
	Learning Rate: 0.00246755
	LOSS [training: 0.33399659485270555 | validation: 0.3695330385885711]
	TIME [epoch: 10.6 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3300471873076682		[learning rate: 0.00246]
	Learning Rate: 0.00245999
	LOSS [training: 0.3300471873076682 | validation: 0.34752788096147336]
	TIME [epoch: 10.6 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3492516858148348		[learning rate: 0.0024524]
	Learning Rate: 0.00245245
	LOSS [training: 0.3492516858148348 | validation: 0.3438185284493826]
	TIME [epoch: 10.6 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3076721684927957		[learning rate: 0.0024449]
	Learning Rate: 0.00244493
	LOSS [training: 0.3076721684927957 | validation: 0.3576624129655386]
	TIME [epoch: 10.6 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3614620130667208		[learning rate: 0.0024374]
	Learning Rate: 0.00243744
	LOSS [training: 0.3614620130667208 | validation: 0.33743679405605775]
	TIME [epoch: 10.6 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33766591519885286		[learning rate: 0.00243]
	Learning Rate: 0.00242996
	LOSS [training: 0.33766591519885286 | validation: 0.36505372990685786]
	TIME [epoch: 10.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3301785294512233		[learning rate: 0.0024225]
	Learning Rate: 0.00242252
	LOSS [training: 0.3301785294512233 | validation: 0.3179036372844425]
	TIME [epoch: 10.6 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32467590140304325		[learning rate: 0.0024151]
	Learning Rate: 0.00241509
	LOSS [training: 0.32467590140304325 | validation: 0.3162842548931528]
	TIME [epoch: 10.6 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31999140402171095		[learning rate: 0.0024077]
	Learning Rate: 0.00240769
	LOSS [training: 0.31999140402171095 | validation: 0.33259418813476693]
	TIME [epoch: 10.6 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36591214203149747		[learning rate: 0.0024003]
	Learning Rate: 0.00240031
	LOSS [training: 0.36591214203149747 | validation: 0.37998932312743094]
	TIME [epoch: 10.6 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32315820066409795		[learning rate: 0.0023929]
	Learning Rate: 0.00239295
	LOSS [training: 0.32315820066409795 | validation: 0.3222274941653668]
	TIME [epoch: 10.6 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32978723646967734		[learning rate: 0.0023856]
	Learning Rate: 0.00238561
	LOSS [training: 0.32978723646967734 | validation: 0.34223894322456544]
	TIME [epoch: 10.6 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35063444262860555		[learning rate: 0.0023783]
	Learning Rate: 0.0023783
	LOSS [training: 0.35063444262860555 | validation: 0.4751462364663254]
	TIME [epoch: 10.6 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41803500783744285		[learning rate: 0.002371]
	Learning Rate: 0.00237101
	LOSS [training: 0.41803500783744285 | validation: 0.4394098902971157]
	TIME [epoch: 10.6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4121745647033578		[learning rate: 0.0023637]
	Learning Rate: 0.00236374
	LOSS [training: 0.4121745647033578 | validation: 0.41720917985565786]
	TIME [epoch: 10.6 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41008663661055367		[learning rate: 0.0023565]
	Learning Rate: 0.0023565
	LOSS [training: 0.41008663661055367 | validation: 0.44062515529971286]
	TIME [epoch: 10.6 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4146699589540611		[learning rate: 0.0023493]
	Learning Rate: 0.00234927
	LOSS [training: 0.4146699589540611 | validation: 0.4153001112156403]
	TIME [epoch: 10.6 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4137876712471978		[learning rate: 0.0023421]
	Learning Rate: 0.00234207
	LOSS [training: 0.4137876712471978 | validation: 0.3974831302012386]
	TIME [epoch: 10.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4128402895669067		[learning rate: 0.0023349]
	Learning Rate: 0.00233489
	LOSS [training: 0.4128402895669067 | validation: 0.3921981353663749]
	TIME [epoch: 10.6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36976127628720434		[learning rate: 0.0023277]
	Learning Rate: 0.00232773
	LOSS [training: 0.36976127628720434 | validation: 0.4062702866748083]
	TIME [epoch: 10.6 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3954917985713059		[learning rate: 0.0023206]
	Learning Rate: 0.0023206
	LOSS [training: 0.3954917985713059 | validation: 0.5913290662442067]
	TIME [epoch: 10.6 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.433129257628429		[learning rate: 0.0023135]
	Learning Rate: 0.00231348
	LOSS [training: 0.433129257628429 | validation: 0.34663194977532036]
	TIME [epoch: 10.6 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3406907494190482		[learning rate: 0.0023064]
	Learning Rate: 0.00230639
	LOSS [training: 0.3406907494190482 | validation: 0.41406285971073303]
	TIME [epoch: 10.6 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3870393045722527		[learning rate: 0.0022993]
	Learning Rate: 0.00229932
	LOSS [training: 0.3870393045722527 | validation: 0.38547527999842973]
	TIME [epoch: 10.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43313831550010634		[learning rate: 0.0022923]
	Learning Rate: 0.00229227
	LOSS [training: 0.43313831550010634 | validation: 0.43508265776694766]
	TIME [epoch: 10.6 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40875465438519143		[learning rate: 0.0022852]
	Learning Rate: 0.00228525
	LOSS [training: 0.40875465438519143 | validation: 0.37046553710361807]
	TIME [epoch: 10.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3642180652917497		[learning rate: 0.0022782]
	Learning Rate: 0.00227824
	LOSS [training: 0.3642180652917497 | validation: 0.3383632115874291]
	TIME [epoch: 10.6 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3288256008244751		[learning rate: 0.0022713]
	Learning Rate: 0.00227126
	LOSS [training: 0.3288256008244751 | validation: 0.3485093755731627]
	TIME [epoch: 10.6 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38934591186279804		[learning rate: 0.0022643]
	Learning Rate: 0.0022643
	LOSS [training: 0.38934591186279804 | validation: 0.46967171828553944]
	TIME [epoch: 10.6 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3979507605354252		[learning rate: 0.0022574]
	Learning Rate: 0.00225736
	LOSS [training: 0.3979507605354252 | validation: 0.48400777077730844]
	TIME [epoch: 10.6 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3925633965909617		[learning rate: 0.0022504]
	Learning Rate: 0.00225044
	LOSS [training: 0.3925633965909617 | validation: 0.3825418054800812]
	TIME [epoch: 10.6 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3340739535991755		[learning rate: 0.0022435]
	Learning Rate: 0.00224354
	LOSS [training: 0.3340739535991755 | validation: 0.3638060938978196]
	TIME [epoch: 10.6 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.334076522889834		[learning rate: 0.0022367]
	Learning Rate: 0.00223666
	LOSS [training: 0.334076522889834 | validation: 0.38310089457396723]
	TIME [epoch: 10.6 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.446424255498003		[learning rate: 0.0022298]
	Learning Rate: 0.0022298
	LOSS [training: 0.446424255498003 | validation: 0.3958700416881694]
	TIME [epoch: 10.6 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36465801652282187		[learning rate: 0.002223]
	Learning Rate: 0.00222297
	LOSS [training: 0.36465801652282187 | validation: 0.4052181282591036]
	TIME [epoch: 10.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35431795839813934		[learning rate: 0.0022162]
	Learning Rate: 0.00221615
	LOSS [training: 0.35431795839813934 | validation: 0.35903940296187964]
	TIME [epoch: 10.6 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3784726622004578		[learning rate: 0.0022094]
	Learning Rate: 0.00220936
	LOSS [training: 0.3784726622004578 | validation: 0.3258970828971398]
	TIME [epoch: 10.6 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35718180168013347		[learning rate: 0.0022026]
	Learning Rate: 0.00220259
	LOSS [training: 0.35718180168013347 | validation: 0.45825528183075515]
	TIME [epoch: 10.6 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.435319555756854		[learning rate: 0.0021958]
	Learning Rate: 0.00219584
	LOSS [training: 0.435319555756854 | validation: 0.5399228145769088]
	TIME [epoch: 10.6 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4942125047824984		[learning rate: 0.0021891]
	Learning Rate: 0.00218911
	LOSS [training: 0.4942125047824984 | validation: 0.4393946772669818]
	TIME [epoch: 10.6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5234836273755198		[learning rate: 0.0021824]
	Learning Rate: 0.00218239
	LOSS [training: 0.5234836273755198 | validation: 0.6624068092400548]
	TIME [epoch: 10.6 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5286427330412911		[learning rate: 0.0021757]
	Learning Rate: 0.00217571
	LOSS [training: 0.5286427330412911 | validation: 0.3329442307478898]
	TIME [epoch: 10.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3895400066536223		[learning rate: 0.002169]
	Learning Rate: 0.00216904
	LOSS [training: 0.3895400066536223 | validation: 0.5022143909507781]
	TIME [epoch: 10.6 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5246111959864852		[learning rate: 0.0021624]
	Learning Rate: 0.00216239
	LOSS [training: 0.5246111959864852 | validation: 0.44859880288846965]
	TIME [epoch: 10.6 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46681004262504755		[learning rate: 0.0021558]
	Learning Rate: 0.00215576
	LOSS [training: 0.46681004262504755 | validation: 0.5287167298392429]
	TIME [epoch: 10.6 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5197107124256789		[learning rate: 0.0021491]
	Learning Rate: 0.00214915
	LOSS [training: 0.5197107124256789 | validation: 0.5860239450411728]
	TIME [epoch: 10.6 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6600347223904184		[learning rate: 0.0021426]
	Learning Rate: 0.00214256
	LOSS [training: 0.6600347223904184 | validation: 0.5728826060629312]
	TIME [epoch: 10.6 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5452873464555854		[learning rate: 0.002136]
	Learning Rate: 0.00213599
	LOSS [training: 0.5452873464555854 | validation: 0.4349480196126713]
	TIME [epoch: 10.6 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4954656617204384		[learning rate: 0.0021294]
	Learning Rate: 0.00212945
	LOSS [training: 0.4954656617204384 | validation: 0.4567396564368647]
	TIME [epoch: 10.6 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49263673160800386		[learning rate: 0.0021229]
	Learning Rate: 0.00212292
	LOSS [training: 0.49263673160800386 | validation: 0.5079964066550763]
	TIME [epoch: 10.6 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.591730611217878		[learning rate: 0.0021164]
	Learning Rate: 0.00211641
	LOSS [training: 0.591730611217878 | validation: 0.4950131921164336]
	TIME [epoch: 10.6 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6240767628191677		[learning rate: 0.0021099]
	Learning Rate: 0.00210992
	LOSS [training: 0.6240767628191677 | validation: 0.4990738074286361]
	TIME [epoch: 10.6 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45197146219085005		[learning rate: 0.0021035]
	Learning Rate: 0.00210346
	LOSS [training: 0.45197146219085005 | validation: 0.442187657871065]
	TIME [epoch: 10.6 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.585664567890356		[learning rate: 0.002097]
	Learning Rate: 0.00209701
	LOSS [training: 0.585664567890356 | validation: 0.7600410603987396]
	TIME [epoch: 10.6 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6847951769624556		[learning rate: 0.0020906]
	Learning Rate: 0.00209058
	LOSS [training: 0.6847951769624556 | validation: 0.6914236237677983]
	TIME [epoch: 10.6 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6807940528731591		[learning rate: 0.0020842]
	Learning Rate: 0.00208417
	LOSS [training: 0.6807940528731591 | validation: 0.45749461227079236]
	TIME [epoch: 10.6 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49839580260749		[learning rate: 0.0020778]
	Learning Rate: 0.00207778
	LOSS [training: 0.49839580260749 | validation: 0.415094063651025]
	TIME [epoch: 10.6 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4090210856943159		[learning rate: 0.0020714]
	Learning Rate: 0.00207141
	LOSS [training: 0.4090210856943159 | validation: 0.3864751246570218]
	TIME [epoch: 10.6 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36568968875217045		[learning rate: 0.0020651]
	Learning Rate: 0.00206506
	LOSS [training: 0.36568968875217045 | validation: 0.34266461153540334]
	TIME [epoch: 10.6 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37631109782376426		[learning rate: 0.0020587]
	Learning Rate: 0.00205873
	LOSS [training: 0.37631109782376426 | validation: 0.45529503012357664]
	TIME [epoch: 10.6 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3796203185394891		[learning rate: 0.0020524]
	Learning Rate: 0.00205242
	LOSS [training: 0.3796203185394891 | validation: 0.3298315789945992]
	TIME [epoch: 10.6 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34743702470295224		[learning rate: 0.0020461]
	Learning Rate: 0.00204613
	LOSS [training: 0.34743702470295224 | validation: 0.35278302689039465]
	TIME [epoch: 10.6 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3391169083904739		[learning rate: 0.0020399]
	Learning Rate: 0.00203986
	LOSS [training: 0.3391169083904739 | validation: 0.3639864186769812]
	TIME [epoch: 10.6 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3625976982980714		[learning rate: 0.0020336]
	Learning Rate: 0.00203361
	LOSS [training: 0.3625976982980714 | validation: 0.3714342136394164]
	TIME [epoch: 10.6 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3556041351310594		[learning rate: 0.0020274]
	Learning Rate: 0.00202737
	LOSS [training: 0.3556041351310594 | validation: 0.4740087073821321]
	TIME [epoch: 10.6 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5126500274603074		[learning rate: 0.0020212]
	Learning Rate: 0.00202116
	LOSS [training: 0.5126500274603074 | validation: 0.44509328853243063]
	TIME [epoch: 10.6 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35960902080501356		[learning rate: 0.002015]
	Learning Rate: 0.00201496
	LOSS [training: 0.35960902080501356 | validation: 0.4157810204951439]
	TIME [epoch: 10.6 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36305314248115544		[learning rate: 0.0020088]
	Learning Rate: 0.00200878
	LOSS [training: 0.36305314248115544 | validation: 0.3114846033410594]
	TIME [epoch: 10.6 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3091269403493907		[learning rate: 0.0020026]
	Learning Rate: 0.00200263
	LOSS [training: 0.3091269403493907 | validation: 0.36233160781042073]
	TIME [epoch: 10.6 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3235612924818054		[learning rate: 0.0019965]
	Learning Rate: 0.00199649
	LOSS [training: 0.3235612924818054 | validation: 0.3240973526995191]
	TIME [epoch: 10.6 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3092229972517145		[learning rate: 0.0019904]
	Learning Rate: 0.00199037
	LOSS [training: 0.3092229972517145 | validation: 0.3822545919281196]
	TIME [epoch: 10.6 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32259816782922723		[learning rate: 0.0019843]
	Learning Rate: 0.00198427
	LOSS [training: 0.32259816782922723 | validation: 0.30280838318179704]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_1027.pth
	Model improved!!!
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3086413474296387		[learning rate: 0.0019782]
	Learning Rate: 0.00197818
	LOSS [training: 0.3086413474296387 | validation: 0.27382793612872347]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_1028.pth
	Model improved!!!
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3008328966899018		[learning rate: 0.0019721]
	Learning Rate: 0.00197212
	LOSS [training: 0.3008328966899018 | validation: 0.3150687263322739]
	TIME [epoch: 10.8 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2977160886172492		[learning rate: 0.0019661]
	Learning Rate: 0.00196607
	LOSS [training: 0.2977160886172492 | validation: 0.34110107952512153]
	TIME [epoch: 10.6 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.326455852856662		[learning rate: 0.00196]
	Learning Rate: 0.00196005
	LOSS [training: 0.326455852856662 | validation: 0.3058315853247312]
	TIME [epoch: 10.6 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3327226117431488		[learning rate: 0.001954]
	Learning Rate: 0.00195404
	LOSS [training: 0.3327226117431488 | validation: 0.3628321940529112]
	TIME [epoch: 10.6 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3547937307415686		[learning rate: 0.001948]
	Learning Rate: 0.00194805
	LOSS [training: 0.3547937307415686 | validation: 0.34274353049258965]
	TIME [epoch: 10.6 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3549910823108887		[learning rate: 0.0019421]
	Learning Rate: 0.00194208
	LOSS [training: 0.3549910823108887 | validation: 0.33518004415896613]
	TIME [epoch: 10.6 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3248294732078625		[learning rate: 0.0019361]
	Learning Rate: 0.00193612
	LOSS [training: 0.3248294732078625 | validation: 0.37003695840167267]
	TIME [epoch: 10.6 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36271755093681907		[learning rate: 0.0019302]
	Learning Rate: 0.00193019
	LOSS [training: 0.36271755093681907 | validation: 0.376185332965556]
	TIME [epoch: 10.6 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39382715423419273		[learning rate: 0.0019243]
	Learning Rate: 0.00192427
	LOSS [training: 0.39382715423419273 | validation: 0.435539026122009]
	TIME [epoch: 10.6 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4761183194398503		[learning rate: 0.0019184]
	Learning Rate: 0.00191837
	LOSS [training: 0.4761183194398503 | validation: 0.3883107312002564]
	TIME [epoch: 10.6 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3577818414615245		[learning rate: 0.0019125]
	Learning Rate: 0.00191249
	LOSS [training: 0.3577818414615245 | validation: 0.3654091329154484]
	TIME [epoch: 10.6 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34659659036257695		[learning rate: 0.0019066]
	Learning Rate: 0.00190663
	LOSS [training: 0.34659659036257695 | validation: 0.3447023949384317]
	TIME [epoch: 10.6 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31688429992653233		[learning rate: 0.0019008]
	Learning Rate: 0.00190079
	LOSS [training: 0.31688429992653233 | validation: 0.32009282046526316]
	TIME [epoch: 10.6 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3220082521876606		[learning rate: 0.001895]
	Learning Rate: 0.00189496
	LOSS [training: 0.3220082521876606 | validation: 0.32390489307177106]
	TIME [epoch: 10.6 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3071165885463621		[learning rate: 0.0018892]
	Learning Rate: 0.00188915
	LOSS [training: 0.3071165885463621 | validation: 0.3015259291279119]
	TIME [epoch: 10.6 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2898873804024912		[learning rate: 0.0018834]
	Learning Rate: 0.00188336
	LOSS [training: 0.2898873804024912 | validation: 0.3249258357959714]
	TIME [epoch: 10.6 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3119474207343729		[learning rate: 0.0018776]
	Learning Rate: 0.00187759
	LOSS [training: 0.3119474207343729 | validation: 0.32708107770157346]
	TIME [epoch: 10.6 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3227955404806634		[learning rate: 0.0018718]
	Learning Rate: 0.00187183
	LOSS [training: 0.3227955404806634 | validation: 0.30609198305341606]
	TIME [epoch: 10.6 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3031410296965375		[learning rate: 0.0018661]
	Learning Rate: 0.00186609
	LOSS [training: 0.3031410296965375 | validation: 0.34640116116249914]
	TIME [epoch: 10.6 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3237547798896977		[learning rate: 0.0018604]
	Learning Rate: 0.00186037
	LOSS [training: 0.3237547798896977 | validation: 0.3256708803023544]
	TIME [epoch: 10.6 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3132798844154288		[learning rate: 0.0018547]
	Learning Rate: 0.00185467
	LOSS [training: 0.3132798844154288 | validation: 0.328815333643864]
	TIME [epoch: 10.6 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35488333258439525		[learning rate: 0.001849]
	Learning Rate: 0.00184898
	LOSS [training: 0.35488333258439525 | validation: 0.34332934195086157]
	TIME [epoch: 10.6 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31260331598763874		[learning rate: 0.0018433]
	Learning Rate: 0.00184332
	LOSS [training: 0.31260331598763874 | validation: 0.37034745512617334]
	TIME [epoch: 10.6 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3546644423706441		[learning rate: 0.0018377]
	Learning Rate: 0.00183767
	LOSS [training: 0.3546644423706441 | validation: 0.4570975355184839]
	TIME [epoch: 10.6 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4147985469709553		[learning rate: 0.001832]
	Learning Rate: 0.00183203
	LOSS [training: 0.4147985469709553 | validation: 0.3679455691603758]
	TIME [epoch: 10.6 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35700405547724356		[learning rate: 0.0018264]
	Learning Rate: 0.00182642
	LOSS [training: 0.35700405547724356 | validation: 0.3764615712718124]
	TIME [epoch: 10.6 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38164018346190065		[learning rate: 0.0018208]
	Learning Rate: 0.00182082
	LOSS [training: 0.38164018346190065 | validation: 0.39818787549687956]
	TIME [epoch: 10.6 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38310870147240783		[learning rate: 0.0018152]
	Learning Rate: 0.00181524
	LOSS [training: 0.38310870147240783 | validation: 0.39456721761164787]
	TIME [epoch: 10.6 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39511144300468387		[learning rate: 0.0018097]
	Learning Rate: 0.00180967
	LOSS [training: 0.39511144300468387 | validation: 0.382620386636076]
	TIME [epoch: 10.6 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3655220990206462		[learning rate: 0.0018041]
	Learning Rate: 0.00180412
	LOSS [training: 0.3655220990206462 | validation: 0.44316636655901664]
	TIME [epoch: 10.6 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39034031449011125		[learning rate: 0.0017986]
	Learning Rate: 0.00179859
	LOSS [training: 0.39034031449011125 | validation: 0.38376267977495787]
	TIME [epoch: 10.6 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3913099825221017		[learning rate: 0.0017931]
	Learning Rate: 0.00179308
	LOSS [training: 0.3913099825221017 | validation: 0.3799611010790139]
	TIME [epoch: 10.6 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3821483671792122		[learning rate: 0.0017876]
	Learning Rate: 0.00178758
	LOSS [training: 0.3821483671792122 | validation: 0.3960013774446037]
	TIME [epoch: 10.6 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37776744440823834		[learning rate: 0.0017821]
	Learning Rate: 0.00178211
	LOSS [training: 0.37776744440823834 | validation: 0.35834059438948684]
	TIME [epoch: 10.6 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35873971317304826		[learning rate: 0.0017766]
	Learning Rate: 0.00177664
	LOSS [training: 0.35873971317304826 | validation: 0.3724000173648983]
	TIME [epoch: 10.6 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32892406432688226		[learning rate: 0.0017712]
	Learning Rate: 0.0017712
	LOSS [training: 0.32892406432688226 | validation: 0.3240642581034723]
	TIME [epoch: 10.6 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3325463551152942		[learning rate: 0.0017658]
	Learning Rate: 0.00176577
	LOSS [training: 0.3325463551152942 | validation: 0.344429933368247]
	TIME [epoch: 10.6 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3201026534822139		[learning rate: 0.0017604]
	Learning Rate: 0.00176035
	LOSS [training: 0.3201026534822139 | validation: 0.31837521767990795]
	TIME [epoch: 10.6 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3232984637431381		[learning rate: 0.001755]
	Learning Rate: 0.00175496
	LOSS [training: 0.3232984637431381 | validation: 0.37129023793409444]
	TIME [epoch: 10.6 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3728926949670088		[learning rate: 0.0017496]
	Learning Rate: 0.00174958
	LOSS [training: 0.3728926949670088 | validation: 0.3603162357818148]
	TIME [epoch: 10.6 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33743251024723603		[learning rate: 0.0017442]
	Learning Rate: 0.00174421
	LOSS [training: 0.33743251024723603 | validation: 0.3829587594427592]
	TIME [epoch: 10.6 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.340545357008165		[learning rate: 0.0017389]
	Learning Rate: 0.00173887
	LOSS [training: 0.340545357008165 | validation: 0.3787858325237784]
	TIME [epoch: 10.6 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3508226975509872		[learning rate: 0.0017335]
	Learning Rate: 0.00173354
	LOSS [training: 0.3508226975509872 | validation: 0.355022411750697]
	TIME [epoch: 10.6 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.352073444119963		[learning rate: 0.0017282]
	Learning Rate: 0.00172822
	LOSS [training: 0.352073444119963 | validation: 0.36195268529144714]
	TIME [epoch: 10.6 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3499835887396205		[learning rate: 0.0017229]
	Learning Rate: 0.00172293
	LOSS [training: 0.3499835887396205 | validation: 0.3838917600206144]
	TIME [epoch: 10.6 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35342367433401983		[learning rate: 0.0017176]
	Learning Rate: 0.00171764
	LOSS [training: 0.35342367433401983 | validation: 0.3373302867972879]
	TIME [epoch: 10.6 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3570734147814286		[learning rate: 0.0017124]
	Learning Rate: 0.00171238
	LOSS [training: 0.3570734147814286 | validation: 0.30984008084637005]
	TIME [epoch: 10.6 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2905146637003815		[learning rate: 0.0017071]
	Learning Rate: 0.00170713
	LOSS [training: 0.2905146637003815 | validation: 0.30024237233107054]
	TIME [epoch: 10.6 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2855563538337681		[learning rate: 0.0017019]
	Learning Rate: 0.0017019
	LOSS [training: 0.2855563538337681 | validation: 0.2962841918944504]
	TIME [epoch: 10.6 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29211375624330105		[learning rate: 0.0016967]
	Learning Rate: 0.00169668
	LOSS [training: 0.29211375624330105 | validation: 0.3096604755632591]
	TIME [epoch: 10.6 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29898735718077185		[learning rate: 0.0016915]
	Learning Rate: 0.00169148
	LOSS [training: 0.29898735718077185 | validation: 0.3565491244050516]
	TIME [epoch: 10.6 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33380723414414637		[learning rate: 0.0016863]
	Learning Rate: 0.00168629
	LOSS [training: 0.33380723414414637 | validation: 0.3240369119566063]
	TIME [epoch: 10.6 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29063978524621037		[learning rate: 0.0016811]
	Learning Rate: 0.00168113
	LOSS [training: 0.29063978524621037 | validation: 0.2921947255242259]
	TIME [epoch: 10.6 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3158038580625407		[learning rate: 0.001676]
	Learning Rate: 0.00167597
	LOSS [training: 0.3158038580625407 | validation: 0.3123461881472187]
	TIME [epoch: 10.6 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29908369782222477		[learning rate: 0.0016708]
	Learning Rate: 0.00167083
	LOSS [training: 0.29908369782222477 | validation: 0.3567036806718508]
	TIME [epoch: 10.6 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3323226810152736		[learning rate: 0.0016657]
	Learning Rate: 0.00166571
	LOSS [training: 0.3323226810152736 | validation: 0.3413506857872084]
	TIME [epoch: 10.6 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4012224336273308		[learning rate: 0.0016606]
	Learning Rate: 0.00166061
	LOSS [training: 0.4012224336273308 | validation: 0.4306496004020417]
	TIME [epoch: 10.6 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37863082149680066		[learning rate: 0.0016555]
	Learning Rate: 0.00165552
	LOSS [training: 0.37863082149680066 | validation: 0.37079155297838656]
	TIME [epoch: 10.6 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3494012135428177		[learning rate: 0.0016504]
	Learning Rate: 0.00165044
	LOSS [training: 0.3494012135428177 | validation: 0.378075816213409]
	TIME [epoch: 10.6 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3513778056481215		[learning rate: 0.0016454]
	Learning Rate: 0.00164538
	LOSS [training: 0.3513778056481215 | validation: 0.3639770795884877]
	TIME [epoch: 10.6 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3256742098805943		[learning rate: 0.0016403]
	Learning Rate: 0.00164034
	LOSS [training: 0.3256742098805943 | validation: 0.32190926095787]
	TIME [epoch: 10.6 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32235400269648984		[learning rate: 0.0016353]
	Learning Rate: 0.00163531
	LOSS [training: 0.32235400269648984 | validation: 0.3244861292190269]
	TIME [epoch: 10.6 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3578265362979779		[learning rate: 0.0016303]
	Learning Rate: 0.0016303
	LOSS [training: 0.3578265362979779 | validation: 0.35412615966927147]
	TIME [epoch: 10.6 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31223477136885924		[learning rate: 0.0016253]
	Learning Rate: 0.0016253
	LOSS [training: 0.31223477136885924 | validation: 0.3337004315014515]
	TIME [epoch: 10.6 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3133291714946559		[learning rate: 0.0016203]
	Learning Rate: 0.00162032
	LOSS [training: 0.3133291714946559 | validation: 0.332340606372668]
	TIME [epoch: 10.6 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31397952207754337		[learning rate: 0.0016154]
	Learning Rate: 0.00161535
	LOSS [training: 0.31397952207754337 | validation: 0.3202827830564269]
	TIME [epoch: 10.6 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31792664149244904		[learning rate: 0.0016104]
	Learning Rate: 0.0016104
	LOSS [training: 0.31792664149244904 | validation: 0.3173483098765599]
	TIME [epoch: 10.6 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3024493709042852		[learning rate: 0.0016055]
	Learning Rate: 0.00160546
	LOSS [training: 0.3024493709042852 | validation: 0.32620185260257417]
	TIME [epoch: 10.6 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2998624362066266		[learning rate: 0.0016005]
	Learning Rate: 0.00160054
	LOSS [training: 0.2998624362066266 | validation: 0.3253070468458399]
	TIME [epoch: 10.6 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3018091169729937		[learning rate: 0.0015956]
	Learning Rate: 0.00159563
	LOSS [training: 0.3018091169729937 | validation: 0.31426484559178774]
	TIME [epoch: 10.6 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3159905071756209		[learning rate: 0.0015907]
	Learning Rate: 0.00159074
	LOSS [training: 0.3159905071756209 | validation: 0.33246534098861863]
	TIME [epoch: 10.6 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3109821869190067		[learning rate: 0.0015859]
	Learning Rate: 0.00158587
	LOSS [training: 0.3109821869190067 | validation: 0.30827846930304154]
	TIME [epoch: 10.6 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2972937889962914		[learning rate: 0.001581]
	Learning Rate: 0.00158101
	LOSS [training: 0.2972937889962914 | validation: 0.3246480918277925]
	TIME [epoch: 10.6 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33875602157030177		[learning rate: 0.0015762]
	Learning Rate: 0.00157616
	LOSS [training: 0.33875602157030177 | validation: 0.3255246266197242]
	TIME [epoch: 10.6 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.332845031687396		[learning rate: 0.0015713]
	Learning Rate: 0.00157133
	LOSS [training: 0.332845031687396 | validation: 0.351810983328451]
	TIME [epoch: 10.6 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3403335883659251		[learning rate: 0.0015665]
	Learning Rate: 0.00156651
	LOSS [training: 0.3403335883659251 | validation: 0.4027315561861747]
	TIME [epoch: 10.6 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39101932824431246		[learning rate: 0.0015617]
	Learning Rate: 0.00156171
	LOSS [training: 0.39101932824431246 | validation: 0.39621503113096235]
	TIME [epoch: 10.6 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3542325715771184		[learning rate: 0.0015569]
	Learning Rate: 0.00155692
	LOSS [training: 0.3542325715771184 | validation: 0.38875230091359597]
	TIME [epoch: 10.6 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3624594363748773		[learning rate: 0.0015521]
	Learning Rate: 0.00155215
	LOSS [training: 0.3624594363748773 | validation: 0.35280531404738236]
	TIME [epoch: 10.6 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3490759685004535		[learning rate: 0.0015474]
	Learning Rate: 0.00154739
	LOSS [training: 0.3490759685004535 | validation: 0.3309519524959735]
	TIME [epoch: 10.6 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3228038282814014		[learning rate: 0.0015426]
	Learning Rate: 0.00154265
	LOSS [training: 0.3228038282814014 | validation: 0.32973617981613723]
	TIME [epoch: 10.6 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3029823177638274		[learning rate: 0.0015379]
	Learning Rate: 0.00153792
	LOSS [training: 0.3029823177638274 | validation: 0.3156625829978601]
	TIME [epoch: 10.6 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3073232821520362		[learning rate: 0.0015332]
	Learning Rate: 0.0015332
	LOSS [training: 0.3073232821520362 | validation: 0.30472230520863797]
	TIME [epoch: 10.6 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3433922052332462		[learning rate: 0.0015285]
	Learning Rate: 0.0015285
	LOSS [training: 0.3433922052332462 | validation: 0.3047937230017726]
	TIME [epoch: 10.6 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34074880363687776		[learning rate: 0.0015238]
	Learning Rate: 0.00152382
	LOSS [training: 0.34074880363687776 | validation: 0.3499504292332351]
	TIME [epoch: 10.6 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30775265550105313		[learning rate: 0.0015191]
	Learning Rate: 0.00151915
	LOSS [training: 0.30775265550105313 | validation: 0.3389312460347523]
	TIME [epoch: 10.6 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30573055930212995		[learning rate: 0.0015145]
	Learning Rate: 0.00151449
	LOSS [training: 0.30573055930212995 | validation: 0.3604768345748822]
	TIME [epoch: 10.6 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34280125907768466		[learning rate: 0.0015098]
	Learning Rate: 0.00150985
	LOSS [training: 0.34280125907768466 | validation: 0.3264353495543989]
	TIME [epoch: 10.6 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3181102857475252		[learning rate: 0.0015052]
	Learning Rate: 0.00150522
	LOSS [training: 0.3181102857475252 | validation: 0.35023913433604575]
	TIME [epoch: 10.6 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4084180668296707		[learning rate: 0.0015006]
	Learning Rate: 0.00150061
	LOSS [training: 0.4084180668296707 | validation: 0.42435760490444757]
	TIME [epoch: 10.6 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3474552989597836		[learning rate: 0.001496]
	Learning Rate: 0.00149601
	LOSS [training: 0.3474552989597836 | validation: 0.37180419059768705]
	TIME [epoch: 10.6 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3619969284480017		[learning rate: 0.0014914]
	Learning Rate: 0.00149142
	LOSS [training: 0.3619969284480017 | validation: 0.37520763046622185]
	TIME [epoch: 10.6 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36658741778033727		[learning rate: 0.0014868]
	Learning Rate: 0.00148685
	LOSS [training: 0.36658741778033727 | validation: 0.3874739141940166]
	TIME [epoch: 10.6 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3694735790535655		[learning rate: 0.0014823]
	Learning Rate: 0.00148229
	LOSS [training: 0.3694735790535655 | validation: 0.39146020307734164]
	TIME [epoch: 10.6 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3738072385193588		[learning rate: 0.0014777]
	Learning Rate: 0.00147775
	LOSS [training: 0.3738072385193588 | validation: 0.41295764872675905]
	TIME [epoch: 10.6 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36247266051579136		[learning rate: 0.0014732]
	Learning Rate: 0.00147322
	LOSS [training: 0.36247266051579136 | validation: 0.3691627879334399]
	TIME [epoch: 10.6 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35259296638358		[learning rate: 0.0014687]
	Learning Rate: 0.0014687
	LOSS [training: 0.35259296638358 | validation: 0.37492718987243434]
	TIME [epoch: 10.6 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3674032729565881		[learning rate: 0.0014642]
	Learning Rate: 0.0014642
	LOSS [training: 0.3674032729565881 | validation: 0.3946992203047712]
	TIME [epoch: 10.6 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3641905023178136		[learning rate: 0.0014597]
	Learning Rate: 0.00145971
	LOSS [training: 0.3641905023178136 | validation: 0.3747513744313822]
	TIME [epoch: 10.6 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3388533898373235		[learning rate: 0.0014552]
	Learning Rate: 0.00145524
	LOSS [training: 0.3388533898373235 | validation: 0.3572407219399122]
	TIME [epoch: 10.6 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35296913564762467		[learning rate: 0.0014508]
	Learning Rate: 0.00145077
	LOSS [training: 0.35296913564762467 | validation: 0.32716628557098704]
	TIME [epoch: 10.6 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33217419387692326		[learning rate: 0.0014463]
	Learning Rate: 0.00144633
	LOSS [training: 0.33217419387692326 | validation: 0.3604737529225635]
	TIME [epoch: 10.6 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34049553032612623		[learning rate: 0.0014419]
	Learning Rate: 0.00144189
	LOSS [training: 0.34049553032612623 | validation: 0.3801045057686606]
	TIME [epoch: 10.6 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34363307606701077		[learning rate: 0.0014375]
	Learning Rate: 0.00143747
	LOSS [training: 0.34363307606701077 | validation: 0.3579876576765885]
	TIME [epoch: 10.6 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3372238952136093		[learning rate: 0.0014331]
	Learning Rate: 0.00143307
	LOSS [training: 0.3372238952136093 | validation: 0.3643451833798031]
	TIME [epoch: 10.6 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32655036339517185		[learning rate: 0.0014287]
	Learning Rate: 0.00142867
	LOSS [training: 0.32655036339517185 | validation: 0.3429698056708418]
	TIME [epoch: 10.6 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34653580040225157		[learning rate: 0.0014243]
	Learning Rate: 0.0014243
	LOSS [training: 0.34653580040225157 | validation: 0.400706175499449]
	TIME [epoch: 10.6 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34818063000213756		[learning rate: 0.0014199]
	Learning Rate: 0.00141993
	LOSS [training: 0.34818063000213756 | validation: 0.38620820779609943]
	TIME [epoch: 10.6 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3461807676664557		[learning rate: 0.0014156]
	Learning Rate: 0.00141558
	LOSS [training: 0.3461807676664557 | validation: 0.3772511101285496]
	TIME [epoch: 10.6 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33484949705799927		[learning rate: 0.0014112]
	Learning Rate: 0.00141124
	LOSS [training: 0.33484949705799927 | validation: 0.4096507712938568]
	TIME [epoch: 10.6 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3606803243470912		[learning rate: 0.0014069]
	Learning Rate: 0.00140691
	LOSS [training: 0.3606803243470912 | validation: 0.35822165876177886]
	TIME [epoch: 10.6 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33766400307932526		[learning rate: 0.0014026]
	Learning Rate: 0.0014026
	LOSS [training: 0.33766400307932526 | validation: 0.37282385352724967]
	TIME [epoch: 10.6 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3683312064497296		[learning rate: 0.0013983]
	Learning Rate: 0.0013983
	LOSS [training: 0.3683312064497296 | validation: 0.35628476965982986]
	TIME [epoch: 10.6 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34694205760775343		[learning rate: 0.001394]
	Learning Rate: 0.00139401
	LOSS [training: 0.34694205760775343 | validation: 0.30861891376924006]
	TIME [epoch: 10.6 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3463925731921739		[learning rate: 0.0013897]
	Learning Rate: 0.00138974
	LOSS [training: 0.3463925731921739 | validation: 0.35277308284403164]
	TIME [epoch: 10.6 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32944719266710865		[learning rate: 0.0013855]
	Learning Rate: 0.00138548
	LOSS [training: 0.32944719266710865 | validation: 0.3392282970089005]
	TIME [epoch: 10.6 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.301824779292633		[learning rate: 0.0013812]
	Learning Rate: 0.00138123
	LOSS [training: 0.301824779292633 | validation: 0.31446598882469917]
	TIME [epoch: 10.6 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2987256904933188		[learning rate: 0.001377]
	Learning Rate: 0.001377
	LOSS [training: 0.2987256904933188 | validation: 0.30665669776832843]
	TIME [epoch: 10.6 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31345541917661723		[learning rate: 0.0013728]
	Learning Rate: 0.00137278
	LOSS [training: 0.31345541917661723 | validation: 0.34728229026579016]
	TIME [epoch: 10.6 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3023121845566944		[learning rate: 0.0013686]
	Learning Rate: 0.00136857
	LOSS [training: 0.3023121845566944 | validation: 0.3205925115659352]
	TIME [epoch: 10.6 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3132675476362866		[learning rate: 0.0013644]
	Learning Rate: 0.00136437
	LOSS [training: 0.3132675476362866 | validation: 0.3574452106683884]
	TIME [epoch: 10.6 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34925792734221905		[learning rate: 0.0013602]
	Learning Rate: 0.00136019
	LOSS [training: 0.34925792734221905 | validation: 0.3501092006343548]
	TIME [epoch: 10.6 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3309774958263823		[learning rate: 0.001356]
	Learning Rate: 0.00135602
	LOSS [training: 0.3309774958263823 | validation: 0.35086850694881483]
	TIME [epoch: 10.6 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34714466920516973		[learning rate: 0.0013519]
	Learning Rate: 0.00135187
	LOSS [training: 0.34714466920516973 | validation: 0.37605168526772204]
	TIME [epoch: 10.6 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3554754519962284		[learning rate: 0.0013477]
	Learning Rate: 0.00134772
	LOSS [training: 0.3554754519962284 | validation: 0.34925053420788016]
	TIME [epoch: 10.6 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33797814434113166		[learning rate: 0.0013436]
	Learning Rate: 0.00134359
	LOSS [training: 0.33797814434113166 | validation: 0.3265992883580168]
	TIME [epoch: 10.6 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3185746602594012		[learning rate: 0.0013395]
	Learning Rate: 0.00133947
	LOSS [training: 0.3185746602594012 | validation: 0.3310320066341689]
	TIME [epoch: 10.6 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3063401227316785		[learning rate: 0.0013354]
	Learning Rate: 0.00133536
	LOSS [training: 0.3063401227316785 | validation: 0.30179426183011276]
	TIME [epoch: 10.6 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3047273196528313		[learning rate: 0.0013313]
	Learning Rate: 0.00133127
	LOSS [training: 0.3047273196528313 | validation: 0.31951414885450785]
	TIME [epoch: 10.6 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29572797490811387		[learning rate: 0.0013272]
	Learning Rate: 0.00132719
	LOSS [training: 0.29572797490811387 | validation: 0.34280869864222013]
	TIME [epoch: 10.6 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.370968691418864		[learning rate: 0.0013231]
	Learning Rate: 0.00132312
	LOSS [training: 0.370968691418864 | validation: 0.4073669937354431]
	TIME [epoch: 10.6 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3301626147196156		[learning rate: 0.0013191]
	Learning Rate: 0.00131907
	LOSS [training: 0.3301626147196156 | validation: 0.33793400585106925]
	TIME [epoch: 10.6 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30245554112136314		[learning rate: 0.001315]
	Learning Rate: 0.00131502
	LOSS [training: 0.30245554112136314 | validation: 0.30767900950952026]
	TIME [epoch: 10.6 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2979662929678549		[learning rate: 0.001311]
	Learning Rate: 0.00131099
	LOSS [training: 0.2979662929678549 | validation: 0.2956504046071443]
	TIME [epoch: 10.6 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2999518234130176		[learning rate: 0.001307]
	Learning Rate: 0.00130697
	LOSS [training: 0.2999518234130176 | validation: 0.30364498862494804]
	TIME [epoch: 10.6 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31544006740705616		[learning rate: 0.001303]
	Learning Rate: 0.00130297
	LOSS [training: 0.31544006740705616 | validation: 0.3072804362961955]
	TIME [epoch: 10.6 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29133998672867895		[learning rate: 0.001299]
	Learning Rate: 0.00129897
	LOSS [training: 0.29133998672867895 | validation: 0.3362491113703521]
	TIME [epoch: 10.6 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3067132981412612		[learning rate: 0.001295]
	Learning Rate: 0.00129499
	LOSS [training: 0.3067132981412612 | validation: 0.31180532958648643]
	TIME [epoch: 10.6 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2935910633112118		[learning rate: 0.001291]
	Learning Rate: 0.00129102
	LOSS [training: 0.2935910633112118 | validation: 0.2775724398733302]
	TIME [epoch: 10.6 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.285190327970159		[learning rate: 0.0012871]
	Learning Rate: 0.00128706
	LOSS [training: 0.285190327970159 | validation: 0.2927331779799765]
	TIME [epoch: 10.6 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.308214065539553		[learning rate: 0.0012831]
	Learning Rate: 0.00128312
	LOSS [training: 0.308214065539553 | validation: 0.278632518153723]
	TIME [epoch: 10.6 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29312222361217155		[learning rate: 0.0012792]
	Learning Rate: 0.00127918
	LOSS [training: 0.29312222361217155 | validation: 0.2847138021806673]
	TIME [epoch: 10.6 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2870035224559455		[learning rate: 0.0012753]
	Learning Rate: 0.00127526
	LOSS [training: 0.2870035224559455 | validation: 0.2932691918381712]
	TIME [epoch: 10.6 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2990697479866096		[learning rate: 0.0012714]
	Learning Rate: 0.00127135
	LOSS [training: 0.2990697479866096 | validation: 0.30751044283974904]
	TIME [epoch: 10.6 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29530772164080576		[learning rate: 0.0012675]
	Learning Rate: 0.00126746
	LOSS [training: 0.29530772164080576 | validation: 0.3137630929474973]
	TIME [epoch: 10.6 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3064700315047607		[learning rate: 0.0012636]
	Learning Rate: 0.00126357
	LOSS [training: 0.3064700315047607 | validation: 0.3271986894256706]
	TIME [epoch: 10.6 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2911567702783305		[learning rate: 0.0012597]
	Learning Rate: 0.0012597
	LOSS [training: 0.2911567702783305 | validation: 0.3205598413319508]
	TIME [epoch: 10.6 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3042659473745259		[learning rate: 0.0012558]
	Learning Rate: 0.00125584
	LOSS [training: 0.3042659473745259 | validation: 0.30768048417234584]
	TIME [epoch: 10.6 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30443518038844164		[learning rate: 0.001252]
	Learning Rate: 0.00125199
	LOSS [training: 0.30443518038844164 | validation: 0.3339395470651198]
	TIME [epoch: 10.6 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3230324948680193		[learning rate: 0.0012481]
	Learning Rate: 0.00124815
	LOSS [training: 0.3230324948680193 | validation: 0.31763879522080574]
	TIME [epoch: 10.6 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29470101465207066		[learning rate: 0.0012443]
	Learning Rate: 0.00124432
	LOSS [training: 0.29470101465207066 | validation: 0.33199552402998167]
	TIME [epoch: 10.6 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2973716592645061		[learning rate: 0.0012405]
	Learning Rate: 0.00124051
	LOSS [training: 0.2973716592645061 | validation: 0.3125471364341623]
	TIME [epoch: 10.6 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29032504776483437		[learning rate: 0.0012367]
	Learning Rate: 0.00123671
	LOSS [training: 0.29032504776483437 | validation: 0.36224159018772784]
	TIME [epoch: 10.6 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3043611327289705		[learning rate: 0.0012329]
	Learning Rate: 0.00123292
	LOSS [training: 0.3043611327289705 | validation: 0.32083092051318174]
	TIME [epoch: 10.6 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2899624882439852		[learning rate: 0.0012291]
	Learning Rate: 0.00122914
	LOSS [training: 0.2899624882439852 | validation: 0.3328073166803967]
	TIME [epoch: 10.6 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2945585602303832		[learning rate: 0.0012254]
	Learning Rate: 0.00122537
	LOSS [training: 0.2945585602303832 | validation: 0.3330972922316684]
	TIME [epoch: 10.6 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32873422271109237		[learning rate: 0.0012216]
	Learning Rate: 0.00122161
	LOSS [training: 0.32873422271109237 | validation: 0.3274559160766207]
	TIME [epoch: 10.6 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2985907018272897		[learning rate: 0.0012179]
	Learning Rate: 0.00121787
	LOSS [training: 0.2985907018272897 | validation: 0.29332359765208493]
	TIME [epoch: 10.6 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2848851738453787		[learning rate: 0.0012141]
	Learning Rate: 0.00121413
	LOSS [training: 0.2848851738453787 | validation: 0.3134485370095592]
	TIME [epoch: 10.6 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2828836140706066		[learning rate: 0.0012104]
	Learning Rate: 0.00121041
	LOSS [training: 0.2828836140706066 | validation: 0.2867876928905514]
	TIME [epoch: 10.6 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31509496723937885		[learning rate: 0.0012067]
	Learning Rate: 0.0012067
	LOSS [training: 0.31509496723937885 | validation: 0.32122146073064434]
	TIME [epoch: 10.6 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2831569570190691		[learning rate: 0.001203]
	Learning Rate: 0.001203
	LOSS [training: 0.2831569570190691 | validation: 0.2913203925695201]
	TIME [epoch: 10.6 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2949246906791896		[learning rate: 0.0011993]
	Learning Rate: 0.00119932
	LOSS [training: 0.2949246906791896 | validation: 0.29742656629509556]
	TIME [epoch: 10.6 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28335137191541787		[learning rate: 0.0011956]
	Learning Rate: 0.00119564
	LOSS [training: 0.28335137191541787 | validation: 0.32012057427903706]
	TIME [epoch: 10.6 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29178827690587456		[learning rate: 0.001192]
	Learning Rate: 0.00119197
	LOSS [training: 0.29178827690587456 | validation: 0.27641584205478514]
	TIME [epoch: 10.6 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2835523740999095		[learning rate: 0.0011883]
	Learning Rate: 0.00118832
	LOSS [training: 0.2835523740999095 | validation: 0.30908582118481276]
	TIME [epoch: 10.6 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28169098329177544		[learning rate: 0.0011847]
	Learning Rate: 0.00118468
	LOSS [training: 0.28169098329177544 | validation: 0.31498863558670026]
	TIME [epoch: 10.6 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2955826967645894		[learning rate: 0.001181]
	Learning Rate: 0.00118105
	LOSS [training: 0.2955826967645894 | validation: 0.3187938166106959]
	TIME [epoch: 10.6 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2920144016614524		[learning rate: 0.0011774]
	Learning Rate: 0.00117743
	LOSS [training: 0.2920144016614524 | validation: 0.3569678064603265]
	TIME [epoch: 10.6 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31180250526269737		[learning rate: 0.0011738]
	Learning Rate: 0.00117382
	LOSS [training: 0.31180250526269737 | validation: 0.3635842365266598]
	TIME [epoch: 10.6 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34488029734596		[learning rate: 0.0011702]
	Learning Rate: 0.00117022
	LOSS [training: 0.34488029734596 | validation: 0.33957553262161105]
	TIME [epoch: 10.6 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32032845332413584		[learning rate: 0.0011666]
	Learning Rate: 0.00116663
	LOSS [training: 0.32032845332413584 | validation: 0.30468635771529295]
	TIME [epoch: 10.6 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3014724479140801		[learning rate: 0.0011631]
	Learning Rate: 0.00116305
	LOSS [training: 0.3014724479140801 | validation: 0.3229286628990192]
	TIME [epoch: 10.6 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2869417610761225		[learning rate: 0.0011595]
	Learning Rate: 0.00115949
	LOSS [training: 0.2869417610761225 | validation: 0.2865482816835237]
	TIME [epoch: 10.6 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3053284429701841		[learning rate: 0.0011559]
	Learning Rate: 0.00115593
	LOSS [training: 0.3053284429701841 | validation: 0.33278011898030435]
	TIME [epoch: 10.6 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32940081095853146		[learning rate: 0.0011524]
	Learning Rate: 0.00115239
	LOSS [training: 0.32940081095853146 | validation: 0.3406165488407822]
	TIME [epoch: 10.6 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3210376102075038		[learning rate: 0.0011489]
	Learning Rate: 0.00114886
	LOSS [training: 0.3210376102075038 | validation: 0.3681343626061948]
	TIME [epoch: 10.6 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3329335423834629		[learning rate: 0.0011453]
	Learning Rate: 0.00114534
	LOSS [training: 0.3329335423834629 | validation: 0.3274855461593042]
	TIME [epoch: 10.6 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3067268995421809		[learning rate: 0.0011418]
	Learning Rate: 0.00114183
	LOSS [training: 0.3067268995421809 | validation: 0.32341323651740217]
	TIME [epoch: 10.6 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31379741653841897		[learning rate: 0.0011383]
	Learning Rate: 0.00113833
	LOSS [training: 0.31379741653841897 | validation: 0.33833891340111877]
	TIME [epoch: 10.6 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3448736360665467		[learning rate: 0.0011348]
	Learning Rate: 0.00113484
	LOSS [training: 0.3448736360665467 | validation: 0.3374412068717058]
	TIME [epoch: 10.6 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3118073038160688		[learning rate: 0.0011314]
	Learning Rate: 0.00113136
	LOSS [training: 0.3118073038160688 | validation: 0.29421023120618656]
	TIME [epoch: 10.6 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29554921293442166		[learning rate: 0.0011279]
	Learning Rate: 0.00112789
	LOSS [training: 0.29554921293442166 | validation: 0.32069026114826515]
	TIME [epoch: 10.6 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29226376501681095		[learning rate: 0.0011244]
	Learning Rate: 0.00112443
	LOSS [training: 0.29226376501681095 | validation: 0.2999349151202275]
	TIME [epoch: 10.6 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.285695506691561		[learning rate: 0.001121]
	Learning Rate: 0.00112099
	LOSS [training: 0.285695506691561 | validation: 0.29972043479337507]
	TIME [epoch: 10.6 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2846170126980637		[learning rate: 0.0011175]
	Learning Rate: 0.00111755
	LOSS [training: 0.2846170126980637 | validation: 0.2970622740121598]
	TIME [epoch: 10.6 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29542708838343695		[learning rate: 0.0011141]
	Learning Rate: 0.00111412
	LOSS [training: 0.29542708838343695 | validation: 0.28277236669726497]
	TIME [epoch: 10.6 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27982253289045955		[learning rate: 0.0011107]
	Learning Rate: 0.00111071
	LOSS [training: 0.27982253289045955 | validation: 0.2943872699024864]
	TIME [epoch: 10.6 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28235426475970926		[learning rate: 0.0011073]
	Learning Rate: 0.0011073
	LOSS [training: 0.28235426475970926 | validation: 0.28052852963868075]
	TIME [epoch: 10.6 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2834660741566387		[learning rate: 0.0011039]
	Learning Rate: 0.00110391
	LOSS [training: 0.2834660741566387 | validation: 0.2899877470186269]
	TIME [epoch: 10.6 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26731672367586856		[learning rate: 0.0011005]
	Learning Rate: 0.00110053
	LOSS [training: 0.26731672367586856 | validation: 0.281387211901523]
	TIME [epoch: 10.6 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28177344860044606		[learning rate: 0.0010972]
	Learning Rate: 0.00109715
	LOSS [training: 0.28177344860044606 | validation: 0.30390736960050146]
	TIME [epoch: 10.6 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2858388511836807		[learning rate: 0.0010938]
	Learning Rate: 0.00109379
	LOSS [training: 0.2858388511836807 | validation: 0.33720155557595183]
	TIME [epoch: 10.6 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31295880167373585		[learning rate: 0.0010904]
	Learning Rate: 0.00109044
	LOSS [training: 0.31295880167373585 | validation: 0.2863900808279939]
	TIME [epoch: 10.6 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28720825309850817		[learning rate: 0.0010871]
	Learning Rate: 0.00108709
	LOSS [training: 0.28720825309850817 | validation: 0.35616007714155223]
	TIME [epoch: 10.6 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31664598460301524		[learning rate: 0.0010838]
	Learning Rate: 0.00108376
	LOSS [training: 0.31664598460301524 | validation: 0.2782813835646869]
	TIME [epoch: 10.6 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2887478097050211		[learning rate: 0.0010804]
	Learning Rate: 0.00108044
	LOSS [training: 0.2887478097050211 | validation: 0.3007809446314468]
	TIME [epoch: 10.6 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2867569401128163		[learning rate: 0.0010771]
	Learning Rate: 0.00107713
	LOSS [training: 0.2867569401128163 | validation: 0.30320513981033775]
	TIME [epoch: 10.6 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29178861049635885		[learning rate: 0.0010738]
	Learning Rate: 0.00107382
	LOSS [training: 0.29178861049635885 | validation: 0.2966172682684599]
	TIME [epoch: 10.6 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28717687414773074		[learning rate: 0.0010705]
	Learning Rate: 0.00107053
	LOSS [training: 0.28717687414773074 | validation: 0.31044429538628804]
	TIME [epoch: 10.6 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29592724426914985		[learning rate: 0.0010673]
	Learning Rate: 0.00106725
	LOSS [training: 0.29592724426914985 | validation: 0.3136698425722213]
	TIME [epoch: 10.6 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30667857758486683		[learning rate: 0.001064]
	Learning Rate: 0.00106398
	LOSS [training: 0.30667857758486683 | validation: 0.3285024597258459]
	TIME [epoch: 10.6 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2945950555120609		[learning rate: 0.0010607]
	Learning Rate: 0.00106072
	LOSS [training: 0.2945950555120609 | validation: 0.3063412900476757]
	TIME [epoch: 10.6 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31012799544490877		[learning rate: 0.0010575]
	Learning Rate: 0.00105747
	LOSS [training: 0.31012799544490877 | validation: 0.34473514212656514]
	TIME [epoch: 10.6 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31834427969107515		[learning rate: 0.0010542]
	Learning Rate: 0.00105422
	LOSS [training: 0.31834427969107515 | validation: 0.33323631692448247]
	TIME [epoch: 10.6 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3174051691872135		[learning rate: 0.001051]
	Learning Rate: 0.00105099
	LOSS [training: 0.3174051691872135 | validation: 0.3321068911201857]
	TIME [epoch: 10.6 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.300623604258138		[learning rate: 0.0010478]
	Learning Rate: 0.00104777
	LOSS [training: 0.300623604258138 | validation: 0.2882164847903255]
	TIME [epoch: 10.6 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2932019931966634		[learning rate: 0.0010446]
	Learning Rate: 0.00104456
	LOSS [training: 0.2932019931966634 | validation: 0.3193702533596805]
	TIME [epoch: 10.6 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3291484528199103		[learning rate: 0.0010414]
	Learning Rate: 0.00104136
	LOSS [training: 0.3291484528199103 | validation: 0.3624579953775507]
	TIME [epoch: 10.6 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3719064413943395		[learning rate: 0.0010382]
	Learning Rate: 0.00103817
	LOSS [training: 0.3719064413943395 | validation: 0.3370897166804632]
	TIME [epoch: 10.6 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3512404864001274		[learning rate: 0.001035]
	Learning Rate: 0.00103498
	LOSS [training: 0.3512404864001274 | validation: 0.3430543931723119]
	TIME [epoch: 10.6 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.312320828646635		[learning rate: 0.0010318]
	Learning Rate: 0.00103181
	LOSS [training: 0.312320828646635 | validation: 0.34255439879766186]
	TIME [epoch: 10.6 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30790267010368905		[learning rate: 0.0010286]
	Learning Rate: 0.00102865
	LOSS [training: 0.30790267010368905 | validation: 0.3004848621699705]
	TIME [epoch: 10.6 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33063611930353826		[learning rate: 0.0010255]
	Learning Rate: 0.00102549
	LOSS [training: 0.33063611930353826 | validation: 0.3263136371168263]
	TIME [epoch: 10.6 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34048374817446214		[learning rate: 0.0010224]
	Learning Rate: 0.00102235
	LOSS [training: 0.34048374817446214 | validation: 0.3860329119743201]
	TIME [epoch: 10.6 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36325491637673313		[learning rate: 0.0010192]
	Learning Rate: 0.00101922
	LOSS [training: 0.36325491637673313 | validation: 0.35365143117604747]
	TIME [epoch: 10.6 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31987031568869473		[learning rate: 0.0010161]
	Learning Rate: 0.00101609
	LOSS [training: 0.31987031568869473 | validation: 0.33247200531044846]
	TIME [epoch: 10.6 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3145399600591162		[learning rate: 0.001013]
	Learning Rate: 0.00101298
	LOSS [training: 0.3145399600591162 | validation: 0.33216242376096256]
	TIME [epoch: 10.6 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3157215356671309		[learning rate: 0.0010099]
	Learning Rate: 0.00100987
	LOSS [training: 0.3157215356671309 | validation: 0.3358196595143618]
	TIME [epoch: 10.6 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3340494325150983		[learning rate: 0.0010068]
	Learning Rate: 0.00100678
	LOSS [training: 0.3340494325150983 | validation: 0.34639376247304876]
	TIME [epoch: 10.6 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3353447913754304		[learning rate: 0.0010037]
	Learning Rate: 0.00100369
	LOSS [training: 0.3353447913754304 | validation: 0.34948477728346644]
	TIME [epoch: 10.6 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36173394845097173		[learning rate: 0.0010006]
	Learning Rate: 0.00100061
	LOSS [training: 0.36173394845097173 | validation: 0.3902202205231653]
	TIME [epoch: 10.6 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3407096438494465		[learning rate: 0.00099755]
	Learning Rate: 0.000997547
	LOSS [training: 0.3407096438494465 | validation: 0.3556290296369795]
	TIME [epoch: 10.6 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3249899713377104		[learning rate: 0.00099449]
	Learning Rate: 0.000994489
	LOSS [training: 0.3249899713377104 | validation: 0.3302343910026876]
	TIME [epoch: 10.6 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31240864947771946		[learning rate: 0.00099144]
	Learning Rate: 0.00099144
	LOSS [training: 0.31240864947771946 | validation: 0.3539045609171066]
	TIME [epoch: 10.6 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3056975863946471		[learning rate: 0.0009884]
	Learning Rate: 0.000988401
	LOSS [training: 0.3056975863946471 | validation: 0.30248731578490506]
	TIME [epoch: 10.6 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29778217122937856		[learning rate: 0.00098537]
	Learning Rate: 0.000985371
	LOSS [training: 0.29778217122937856 | validation: 0.3157373147911684]
	TIME [epoch: 10.6 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2964687619867449		[learning rate: 0.00098235]
	Learning Rate: 0.000982351
	LOSS [training: 0.2964687619867449 | validation: 0.3210947651478018]
	TIME [epoch: 10.6 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2961223991170243		[learning rate: 0.00097934]
	Learning Rate: 0.000979339
	LOSS [training: 0.2961223991170243 | validation: 0.29178216141074936]
	TIME [epoch: 10.6 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2675678449247706		[learning rate: 0.00097634]
	Learning Rate: 0.000976337
	LOSS [training: 0.2675678449247706 | validation: 0.3064014208791215]
	TIME [epoch: 10.6 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2891834825330569		[learning rate: 0.00097334]
	Learning Rate: 0.000973345
	LOSS [training: 0.2891834825330569 | validation: 0.3228869485954215]
	TIME [epoch: 10.6 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33094286355066116		[learning rate: 0.00097036]
	Learning Rate: 0.000970361
	LOSS [training: 0.33094286355066116 | validation: 0.3497977039650905]
	TIME [epoch: 10.6 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30601848994569353		[learning rate: 0.00096739]
	Learning Rate: 0.000967386
	LOSS [training: 0.30601848994569353 | validation: 0.2924623997433759]
	TIME [epoch: 10.6 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2747386391435306		[learning rate: 0.00096442]
	Learning Rate: 0.000964421
	LOSS [training: 0.2747386391435306 | validation: 0.28994377907101004]
	TIME [epoch: 10.6 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3173439404724498		[learning rate: 0.00096146]
	Learning Rate: 0.000961464
	LOSS [training: 0.3173439404724498 | validation: 0.33022018574298373]
	TIME [epoch: 10.6 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29867494222895585		[learning rate: 0.00095852]
	Learning Rate: 0.000958517
	LOSS [training: 0.29867494222895585 | validation: 0.28312805157861354]
	TIME [epoch: 10.6 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28272722771508135		[learning rate: 0.00095558]
	Learning Rate: 0.000955579
	LOSS [training: 0.28272722771508135 | validation: 0.29534020528662774]
	TIME [epoch: 10.6 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.285384459874016		[learning rate: 0.00095265]
	Learning Rate: 0.00095265
	LOSS [training: 0.285384459874016 | validation: 0.28488056510140547]
	TIME [epoch: 10.6 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28271678910775255		[learning rate: 0.00094973]
	Learning Rate: 0.00094973
	LOSS [training: 0.28271678910775255 | validation: 0.3024043221339065]
	TIME [epoch: 10.6 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2596151944606604		[learning rate: 0.00094682]
	Learning Rate: 0.000946818
	LOSS [training: 0.2596151944606604 | validation: 0.29637666804894386]
	TIME [epoch: 10.6 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27110702965611083		[learning rate: 0.00094392]
	Learning Rate: 0.000943916
	LOSS [training: 0.27110702965611083 | validation: 0.2893104661071319]
	TIME [epoch: 10.6 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27984376596224686		[learning rate: 0.00094102]
	Learning Rate: 0.000941023
	LOSS [training: 0.27984376596224686 | validation: 0.29161242849177915]
	TIME [epoch: 10.6 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2720081518930427		[learning rate: 0.00093814]
	Learning Rate: 0.000938138
	LOSS [training: 0.2720081518930427 | validation: 0.28466086766613236]
	TIME [epoch: 10.6 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.287186227504578		[learning rate: 0.00093526]
	Learning Rate: 0.000935262
	LOSS [training: 0.287186227504578 | validation: 0.33274322957952707]
	TIME [epoch: 10.6 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3170959719751232		[learning rate: 0.0009324]
	Learning Rate: 0.000932395
	LOSS [training: 0.3170959719751232 | validation: 0.2830792763018008]
	TIME [epoch: 10.6 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26994539495365283		[learning rate: 0.00092954]
	Learning Rate: 0.000929537
	LOSS [training: 0.26994539495365283 | validation: 0.2936001365740695]
	TIME [epoch: 10.6 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27789910453335065		[learning rate: 0.00092669]
	Learning Rate: 0.000926688
	LOSS [training: 0.27789910453335065 | validation: 0.2838550980520608]
	TIME [epoch: 10.6 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2875675979538574		[learning rate: 0.00092385]
	Learning Rate: 0.000923847
	LOSS [training: 0.2875675979538574 | validation: 0.3209050277515291]
	TIME [epoch: 10.6 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2824358443548203		[learning rate: 0.00092101]
	Learning Rate: 0.000921015
	LOSS [training: 0.2824358443548203 | validation: 0.30395848097969536]
	TIME [epoch: 10.6 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28986103205552827		[learning rate: 0.00091819]
	Learning Rate: 0.000918192
	LOSS [training: 0.28986103205552827 | validation: 0.28353499977836677]
	TIME [epoch: 10.6 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2877128635348356		[learning rate: 0.00091538]
	Learning Rate: 0.000915377
	LOSS [training: 0.2877128635348356 | validation: 0.2826909588602742]
	TIME [epoch: 10.6 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27645466205113595		[learning rate: 0.00091257]
	Learning Rate: 0.000912571
	LOSS [training: 0.27645466205113595 | validation: 0.28368238384620564]
	TIME [epoch: 10.6 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2737222560667525		[learning rate: 0.00090977]
	Learning Rate: 0.000909774
	LOSS [training: 0.2737222560667525 | validation: 0.2939750615381113]
	TIME [epoch: 10.6 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.290449920733161		[learning rate: 0.00090698]
	Learning Rate: 0.000906985
	LOSS [training: 0.290449920733161 | validation: 0.3108463142814009]
	TIME [epoch: 10.6 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30893809994349286		[learning rate: 0.0009042]
	Learning Rate: 0.000904204
	LOSS [training: 0.30893809994349286 | validation: 0.3470747586465043]
	TIME [epoch: 10.6 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32275463576030095		[learning rate: 0.00090143]
	Learning Rate: 0.000901433
	LOSS [training: 0.32275463576030095 | validation: 0.3726026948478806]
	TIME [epoch: 10.6 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34428742903628234		[learning rate: 0.00089867]
	Learning Rate: 0.000898669
	LOSS [training: 0.34428742903628234 | validation: 0.39254861236080546]
	TIME [epoch: 10.6 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3346685272326953		[learning rate: 0.00089591]
	Learning Rate: 0.000895915
	LOSS [training: 0.3346685272326953 | validation: 0.34483980590501423]
	TIME [epoch: 10.6 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31252843567482425		[learning rate: 0.00089317]
	Learning Rate: 0.000893168
	LOSS [training: 0.31252843567482425 | validation: 0.34162433428370426]
	TIME [epoch: 10.6 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3130897176643003		[learning rate: 0.00089043]
	Learning Rate: 0.00089043
	LOSS [training: 0.3130897176643003 | validation: 0.303680933252282]
	TIME [epoch: 10.6 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.299211501486197		[learning rate: 0.0008877]
	Learning Rate: 0.000887701
	LOSS [training: 0.299211501486197 | validation: 0.3206725732189415]
	TIME [epoch: 10.6 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3005512763551923		[learning rate: 0.00088498]
	Learning Rate: 0.00088498
	LOSS [training: 0.3005512763551923 | validation: 0.3271286654288654]
	TIME [epoch: 10.6 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29204856863526274		[learning rate: 0.00088227]
	Learning Rate: 0.000882267
	LOSS [training: 0.29204856863526274 | validation: 0.30283180602734555]
	TIME [epoch: 10.6 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2841630515007453		[learning rate: 0.00087956]
	Learning Rate: 0.000879562
	LOSS [training: 0.2841630515007453 | validation: 0.2946714765101661]
	TIME [epoch: 10.6 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.286430694040433		[learning rate: 0.00087687]
	Learning Rate: 0.000876866
	LOSS [training: 0.286430694040433 | validation: 0.30029705622719327]
	TIME [epoch: 10.6 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27561659800974586		[learning rate: 0.00087418]
	Learning Rate: 0.000874178
	LOSS [training: 0.27561659800974586 | validation: 0.31567850980121454]
	TIME [epoch: 10.6 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2852930784705722		[learning rate: 0.0008715]
	Learning Rate: 0.000871498
	LOSS [training: 0.2852930784705722 | validation: 0.2931406296864863]
	TIME [epoch: 10.6 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27581258035966594		[learning rate: 0.00086883]
	Learning Rate: 0.000868827
	LOSS [training: 0.27581258035966594 | validation: 0.28759937336156904]
	TIME [epoch: 10.6 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2787861486368997		[learning rate: 0.00086616]
	Learning Rate: 0.000866164
	LOSS [training: 0.2787861486368997 | validation: 0.28435331290224986]
	TIME [epoch: 10.6 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2780873570799415		[learning rate: 0.00086351]
	Learning Rate: 0.000863509
	LOSS [training: 0.2780873570799415 | validation: 0.30048580676509545]
	TIME [epoch: 10.6 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2655061507618737		[learning rate: 0.00086086]
	Learning Rate: 0.000860861
	LOSS [training: 0.2655061507618737 | validation: 0.27216986058808745]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_1299.pth
	Model improved!!!
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.264938580473815		[learning rate: 0.00085822]
	Learning Rate: 0.000858223
	LOSS [training: 0.264938580473815 | validation: 0.28314092989779666]
	TIME [epoch: 10.6 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29359081244155305		[learning rate: 0.00085559]
	Learning Rate: 0.000855592
	LOSS [training: 0.29359081244155305 | validation: 0.2860097442308106]
	TIME [epoch: 10.6 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2928438066431107		[learning rate: 0.00085297]
	Learning Rate: 0.000852969
	LOSS [training: 0.2928438066431107 | validation: 0.28975901270872]
	TIME [epoch: 10.6 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27971463727346085		[learning rate: 0.00085035]
	Learning Rate: 0.000850354
	LOSS [training: 0.27971463727346085 | validation: 0.2930536475894462]
	TIME [epoch: 10.6 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2912060785490079		[learning rate: 0.00084775]
	Learning Rate: 0.000847748
	LOSS [training: 0.2912060785490079 | validation: 0.2883854281675224]
	TIME [epoch: 10.6 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2886790415581634		[learning rate: 0.00084515]
	Learning Rate: 0.000845149
	LOSS [training: 0.2886790415581634 | validation: 0.29469730012646667]
	TIME [epoch: 10.6 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2653439221543465		[learning rate: 0.00084256]
	Learning Rate: 0.000842558
	LOSS [training: 0.2653439221543465 | validation: 0.32394291356886074]
	TIME [epoch: 10.6 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2748093822325651		[learning rate: 0.00083998]
	Learning Rate: 0.000839976
	LOSS [training: 0.2748093822325651 | validation: 0.283991465700477]
	TIME [epoch: 10.6 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2873384488424382		[learning rate: 0.0008374]
	Learning Rate: 0.000837401
	LOSS [training: 0.2873384488424382 | validation: 0.29324446047161984]
	TIME [epoch: 10.6 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2850352883646904		[learning rate: 0.00083483]
	Learning Rate: 0.000834834
	LOSS [training: 0.2850352883646904 | validation: 0.30575702184800685]
	TIME [epoch: 10.6 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2837547091169266		[learning rate: 0.00083227]
	Learning Rate: 0.000832274
	LOSS [training: 0.2837547091169266 | validation: 0.2948073151087379]
	TIME [epoch: 10.6 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29695831119488103		[learning rate: 0.00082972]
	Learning Rate: 0.000829723
	LOSS [training: 0.29695831119488103 | validation: 0.333698359160079]
	TIME [epoch: 10.6 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3206297445437492		[learning rate: 0.00082718]
	Learning Rate: 0.00082718
	LOSS [training: 0.3206297445437492 | validation: 0.3263989935737945]
	TIME [epoch: 10.6 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29673826498476785		[learning rate: 0.00082464]
	Learning Rate: 0.000824644
	LOSS [training: 0.29673826498476785 | validation: 0.2805013323318886]
	TIME [epoch: 10.6 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2883037378846962		[learning rate: 0.00082212]
	Learning Rate: 0.000822116
	LOSS [training: 0.2883037378846962 | validation: 0.29249007723659376]
	TIME [epoch: 10.6 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2815628077343688		[learning rate: 0.0008196]
	Learning Rate: 0.000819596
	LOSS [training: 0.2815628077343688 | validation: 0.2733523982648148]
	TIME [epoch: 10.6 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27006445688378683		[learning rate: 0.00081708]
	Learning Rate: 0.000817084
	LOSS [training: 0.27006445688378683 | validation: 0.2894240034203389]
	TIME [epoch: 10.6 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2715210190043578		[learning rate: 0.00081458]
	Learning Rate: 0.000814579
	LOSS [training: 0.2715210190043578 | validation: 0.2735547753540667]
	TIME [epoch: 10.6 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28487964170890717		[learning rate: 0.00081208]
	Learning Rate: 0.000812082
	LOSS [training: 0.28487964170890717 | validation: 0.2954851410580076]
	TIME [epoch: 10.6 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2916747959888833		[learning rate: 0.00080959]
	Learning Rate: 0.000809593
	LOSS [training: 0.2916747959888833 | validation: 0.2617485477610509]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_1319.pth
	Model improved!!!
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2734639886997869		[learning rate: 0.00080711]
	Learning Rate: 0.000807111
	LOSS [training: 0.2734639886997869 | validation: 0.2821777542182642]
	TIME [epoch: 10.6 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2791127901352479		[learning rate: 0.00080464]
	Learning Rate: 0.000804637
	LOSS [training: 0.2791127901352479 | validation: 0.29485566847972755]
	TIME [epoch: 10.6 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28606285625592237		[learning rate: 0.00080217]
	Learning Rate: 0.00080217
	LOSS [training: 0.28606285625592237 | validation: 0.2997865895859821]
	TIME [epoch: 10.6 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2906606521234987		[learning rate: 0.00079971]
	Learning Rate: 0.000799712
	LOSS [training: 0.2906606521234987 | validation: 0.28549544977480745]
	TIME [epoch: 10.6 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3086338143552411		[learning rate: 0.00079726]
	Learning Rate: 0.00079726
	LOSS [training: 0.3086338143552411 | validation: 0.29578941405583115]
	TIME [epoch: 10.6 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36256534822101055		[learning rate: 0.00079482]
	Learning Rate: 0.000794816
	LOSS [training: 0.36256534822101055 | validation: 0.3635730758684525]
	TIME [epoch: 10.6 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33804154409643544		[learning rate: 0.00079238]
	Learning Rate: 0.00079238
	LOSS [training: 0.33804154409643544 | validation: 0.308524896002094]
	TIME [epoch: 10.6 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33177875694544967		[learning rate: 0.00078995]
	Learning Rate: 0.000789951
	LOSS [training: 0.33177875694544967 | validation: 0.3173499169927393]
	TIME [epoch: 10.6 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3342479382478626		[learning rate: 0.00078753]
	Learning Rate: 0.000787529
	LOSS [training: 0.3342479382478626 | validation: 0.32773192852817395]
	TIME [epoch: 10.6 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3503086234771956		[learning rate: 0.00078512]
	Learning Rate: 0.000785115
	LOSS [training: 0.3503086234771956 | validation: 0.3219668681695107]
	TIME [epoch: 10.6 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3146295340941463		[learning rate: 0.00078271]
	Learning Rate: 0.000782708
	LOSS [training: 0.3146295340941463 | validation: 0.3017789187026844]
	TIME [epoch: 10.6 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.317403265141896		[learning rate: 0.00078031]
	Learning Rate: 0.000780309
	LOSS [training: 0.317403265141896 | validation: 0.32786161938904984]
	TIME [epoch: 10.6 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.370240774427342		[learning rate: 0.00077792]
	Learning Rate: 0.000777917
	LOSS [training: 0.370240774427342 | validation: 0.3286974963730897]
	TIME [epoch: 10.6 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3515419775541421		[learning rate: 0.00077553]
	Learning Rate: 0.000775533
	LOSS [training: 0.3515419775541421 | validation: 0.3153423188367291]
	TIME [epoch: 10.6 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32865225572912404		[learning rate: 0.00077316]
	Learning Rate: 0.000773155
	LOSS [training: 0.32865225572912404 | validation: 0.2978232106281843]
	TIME [epoch: 10.6 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3364404647696687		[learning rate: 0.00077079]
	Learning Rate: 0.000770785
	LOSS [training: 0.3364404647696687 | validation: 0.31399331508029793]
	TIME [epoch: 10.6 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33467411123438584		[learning rate: 0.00076842]
	Learning Rate: 0.000768422
	LOSS [training: 0.33467411123438584 | validation: 0.3368053442468694]
	TIME [epoch: 10.6 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34473924939513256		[learning rate: 0.00076607]
	Learning Rate: 0.000766067
	LOSS [training: 0.34473924939513256 | validation: 0.331194024797141]
	TIME [epoch: 10.6 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3472575704045652		[learning rate: 0.00076372]
	Learning Rate: 0.000763719
	LOSS [training: 0.3472575704045652 | validation: 0.3236167729140428]
	TIME [epoch: 10.6 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.340865390772385		[learning rate: 0.00076138]
	Learning Rate: 0.000761377
	LOSS [training: 0.340865390772385 | validation: 0.31493482933861744]
	TIME [epoch: 10.6 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3095046465555965		[learning rate: 0.00075904]
	Learning Rate: 0.000759043
	LOSS [training: 0.3095046465555965 | validation: 0.3138661293795599]
	TIME [epoch: 10.6 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2984352124128996		[learning rate: 0.00075672]
	Learning Rate: 0.000756717
	LOSS [training: 0.2984352124128996 | validation: 0.2919627381541134]
	TIME [epoch: 10.6 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28489737579454494		[learning rate: 0.0007544]
	Learning Rate: 0.000754397
	LOSS [training: 0.28489737579454494 | validation: 0.2965645618738089]
	TIME [epoch: 10.6 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27037207688908627		[learning rate: 0.00075208]
	Learning Rate: 0.000752084
	LOSS [training: 0.27037207688908627 | validation: 0.2636467860045189]
	TIME [epoch: 10.6 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2685271624334897		[learning rate: 0.00074978]
	Learning Rate: 0.000749779
	LOSS [training: 0.2685271624334897 | validation: 0.26716371383657583]
	TIME [epoch: 10.6 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2796124882657759		[learning rate: 0.00074748]
	Learning Rate: 0.000747481
	LOSS [training: 0.2796124882657759 | validation: 0.27337474502009534]
	TIME [epoch: 10.6 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2753917348006084		[learning rate: 0.00074519]
	Learning Rate: 0.000745189
	LOSS [training: 0.2753917348006084 | validation: 0.26815548143827084]
	TIME [epoch: 10.6 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26768406114393356		[learning rate: 0.0007429]
	Learning Rate: 0.000742905
	LOSS [training: 0.26768406114393356 | validation: 0.27616652687480875]
	TIME [epoch: 10.6 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2604169676556173		[learning rate: 0.00074063]
	Learning Rate: 0.000740628
	LOSS [training: 0.2604169676556173 | validation: 0.2717027047350083]
	TIME [epoch: 10.6 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2655978867242596		[learning rate: 0.00073836]
	Learning Rate: 0.000738357
	LOSS [training: 0.2655978867242596 | validation: 0.2789346190573894]
	TIME [epoch: 10.6 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2676838145346988		[learning rate: 0.00073609]
	Learning Rate: 0.000736094
	LOSS [training: 0.2676838145346988 | validation: 0.3032922531193602]
	TIME [epoch: 10.6 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28102261948257806		[learning rate: 0.00073384]
	Learning Rate: 0.000733838
	LOSS [training: 0.28102261948257806 | validation: 0.30855992045458996]
	TIME [epoch: 10.6 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27809894614723973		[learning rate: 0.00073159]
	Learning Rate: 0.000731588
	LOSS [training: 0.27809894614723973 | validation: 0.2849280584017089]
	TIME [epoch: 10.6 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29760165651906384		[learning rate: 0.00072935]
	Learning Rate: 0.000729345
	LOSS [training: 0.29760165651906384 | validation: 0.3310052858069082]
	TIME [epoch: 10.6 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30769439215420624		[learning rate: 0.00072711]
	Learning Rate: 0.00072711
	LOSS [training: 0.30769439215420624 | validation: 0.3303969020992639]
	TIME [epoch: 10.6 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31802355377009683		[learning rate: 0.00072488]
	Learning Rate: 0.000724881
	LOSS [training: 0.31802355377009683 | validation: 0.3237847804108679]
	TIME [epoch: 10.6 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30549984216663084		[learning rate: 0.00072266]
	Learning Rate: 0.000722659
	LOSS [training: 0.30549984216663084 | validation: 0.32774994733873003]
	TIME [epoch: 10.6 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.305409324695932		[learning rate: 0.00072044]
	Learning Rate: 0.000720444
	LOSS [training: 0.305409324695932 | validation: 0.32999782315354553]
	TIME [epoch: 10.6 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3189856643578196		[learning rate: 0.00071824]
	Learning Rate: 0.000718235
	LOSS [training: 0.3189856643578196 | validation: 0.31170609065636595]
	TIME [epoch: 10.6 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32551911642569054		[learning rate: 0.00071603]
	Learning Rate: 0.000716033
	LOSS [training: 0.32551911642569054 | validation: 0.3161076564176769]
	TIME [epoch: 10.6 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3130830391115075		[learning rate: 0.00071384]
	Learning Rate: 0.000713839
	LOSS [training: 0.3130830391115075 | validation: 0.3491710350628046]
	TIME [epoch: 10.6 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3131987892593159		[learning rate: 0.00071165]
	Learning Rate: 0.00071165
	LOSS [training: 0.3131987892593159 | validation: 0.33491273744505906]
	TIME [epoch: 10.6 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3254729719443551		[learning rate: 0.00070947]
	Learning Rate: 0.000709469
	LOSS [training: 0.3254729719443551 | validation: 0.3460415611251642]
	TIME [epoch: 10.6 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33510675368621284		[learning rate: 0.00070729]
	Learning Rate: 0.000707294
	LOSS [training: 0.33510675368621284 | validation: 0.33387407255480184]
	TIME [epoch: 10.6 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3245914067289622		[learning rate: 0.00070513]
	Learning Rate: 0.000705126
	LOSS [training: 0.3245914067289622 | validation: 0.3612885601477974]
	TIME [epoch: 10.6 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.348783475427376		[learning rate: 0.00070296]
	Learning Rate: 0.000702964
	LOSS [training: 0.348783475427376 | validation: 0.37240277307862213]
	TIME [epoch: 10.6 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33875604968649303		[learning rate: 0.00070081]
	Learning Rate: 0.00070081
	LOSS [training: 0.33875604968649303 | validation: 0.36367083398575467]
	TIME [epoch: 10.6 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3449049576661065		[learning rate: 0.00069866]
	Learning Rate: 0.000698661
	LOSS [training: 0.3449049576661065 | validation: 0.35798561883483493]
	TIME [epoch: 10.6 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3107671372014385		[learning rate: 0.00069652]
	Learning Rate: 0.000696519
	LOSS [training: 0.3107671372014385 | validation: 0.3461558218658624]
	TIME [epoch: 10.6 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3170070230479494		[learning rate: 0.00069438]
	Learning Rate: 0.000694384
	LOSS [training: 0.3170070230479494 | validation: 0.32236802791482194]
	TIME [epoch: 10.6 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3063715869648287		[learning rate: 0.00069226]
	Learning Rate: 0.000692256
	LOSS [training: 0.3063715869648287 | validation: 0.32761734534059356]
	TIME [epoch: 10.6 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.315354648832979		[learning rate: 0.00069013]
	Learning Rate: 0.000690134
	LOSS [training: 0.315354648832979 | validation: 0.33819949920603537]
	TIME [epoch: 10.6 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3430127462966346		[learning rate: 0.00068802]
	Learning Rate: 0.000688018
	LOSS [training: 0.3430127462966346 | validation: 0.3870382505081441]
	TIME [epoch: 10.6 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3478512054219167		[learning rate: 0.00068591]
	Learning Rate: 0.000685909
	LOSS [training: 0.3478512054219167 | validation: 0.35198482548423016]
	TIME [epoch: 10.6 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3272959642122266		[learning rate: 0.00068381]
	Learning Rate: 0.000683807
	LOSS [training: 0.3272959642122266 | validation: 0.3372259168869266]
	TIME [epoch: 10.6 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3249582009916398		[learning rate: 0.00068171]
	Learning Rate: 0.000681711
	LOSS [training: 0.3249582009916398 | validation: 0.32954466055398585]
	TIME [epoch: 10.6 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31240643124000933		[learning rate: 0.00067962]
	Learning Rate: 0.000679621
	LOSS [training: 0.31240643124000933 | validation: 0.35962283947096096]
	TIME [epoch: 10.6 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32188779909056164		[learning rate: 0.00067754]
	Learning Rate: 0.000677538
	LOSS [training: 0.32188779909056164 | validation: 0.3542702423806803]
	TIME [epoch: 10.6 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3072441736758759		[learning rate: 0.00067546]
	Learning Rate: 0.000675461
	LOSS [training: 0.3072441736758759 | validation: 0.3107930984022524]
	TIME [epoch: 10.6 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2948534451613648		[learning rate: 0.00067339]
	Learning Rate: 0.00067339
	LOSS [training: 0.2948534451613648 | validation: 0.31213465193828044]
	TIME [epoch: 10.6 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3086815372396487		[learning rate: 0.00067133]
	Learning Rate: 0.000671326
	LOSS [training: 0.3086815372396487 | validation: 0.31686271152623774]
	TIME [epoch: 10.6 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3186580884767757		[learning rate: 0.00066927]
	Learning Rate: 0.000669268
	LOSS [training: 0.3186580884767757 | validation: 0.33037198720333827]
	TIME [epoch: 10.6 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.299186354656335		[learning rate: 0.00066722]
	Learning Rate: 0.000667216
	LOSS [training: 0.299186354656335 | validation: 0.2841496353468698]
	TIME [epoch: 10.6 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2918016994548503		[learning rate: 0.00066517]
	Learning Rate: 0.000665171
	LOSS [training: 0.2918016994548503 | validation: 0.314782379744582]
	TIME [epoch: 10.6 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2951505323185391		[learning rate: 0.00066313]
	Learning Rate: 0.000663132
	LOSS [training: 0.2951505323185391 | validation: 0.32224830645570507]
	TIME [epoch: 10.6 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2916677789162148		[learning rate: 0.0006611]
	Learning Rate: 0.000661099
	LOSS [training: 0.2916677789162148 | validation: 0.30616960025686973]
	TIME [epoch: 10.6 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2872406722054089		[learning rate: 0.00065907]
	Learning Rate: 0.000659073
	LOSS [training: 0.2872406722054089 | validation: 0.3029730048009212]
	TIME [epoch: 10.6 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28039760081801035		[learning rate: 0.00065705]
	Learning Rate: 0.000657052
	LOSS [training: 0.28039760081801035 | validation: 0.3124186629432824]
	TIME [epoch: 10.6 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2942441779316101		[learning rate: 0.00065504]
	Learning Rate: 0.000655038
	LOSS [training: 0.2942441779316101 | validation: 0.30890559060737405]
	TIME [epoch: 10.6 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2865228541917155		[learning rate: 0.00065303]
	Learning Rate: 0.00065303
	LOSS [training: 0.2865228541917155 | validation: 0.30978061094065573]
	TIME [epoch: 10.6 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28521469304492897		[learning rate: 0.00065103]
	Learning Rate: 0.000651028
	LOSS [training: 0.28521469304492897 | validation: 0.28362719689478944]
	TIME [epoch: 10.6 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27820845121096416		[learning rate: 0.00064903]
	Learning Rate: 0.000649033
	LOSS [training: 0.27820845121096416 | validation: 0.28327996331446237]
	TIME [epoch: 10.6 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2745320318789167		[learning rate: 0.00064704]
	Learning Rate: 0.000647043
	LOSS [training: 0.2745320318789167 | validation: 0.28635210169924974]
	TIME [epoch: 10.6 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2867117873074753		[learning rate: 0.00064506]
	Learning Rate: 0.00064506
	LOSS [training: 0.2867117873074753 | validation: 0.2850311543481869]
	TIME [epoch: 10.6 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28379226618225734		[learning rate: 0.00064308]
	Learning Rate: 0.000643082
	LOSS [training: 0.28379226618225734 | validation: 0.3061440848142816]
	TIME [epoch: 10.6 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2858833726278487		[learning rate: 0.00064111]
	Learning Rate: 0.000641111
	LOSS [training: 0.2858833726278487 | validation: 0.3172561551807253]
	TIME [epoch: 10.6 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.298188968983061		[learning rate: 0.00063915]
	Learning Rate: 0.000639146
	LOSS [training: 0.298188968983061 | validation: 0.29866001360283667]
	TIME [epoch: 10.6 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.292288921500762		[learning rate: 0.00063719]
	Learning Rate: 0.000637187
	LOSS [training: 0.292288921500762 | validation: 0.3061920387328373]
	TIME [epoch: 10.6 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2978993301006083		[learning rate: 0.00063523]
	Learning Rate: 0.000635233
	LOSS [training: 0.2978993301006083 | validation: 0.3094515625055596]
	TIME [epoch: 10.6 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30854210545086447		[learning rate: 0.00063329]
	Learning Rate: 0.000633286
	LOSS [training: 0.30854210545086447 | validation: 0.31897337263594544]
	TIME [epoch: 10.6 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31713550972741816		[learning rate: 0.00063134]
	Learning Rate: 0.000631345
	LOSS [training: 0.31713550972741816 | validation: 0.3105359909167658]
	TIME [epoch: 10.6 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2953736901169735		[learning rate: 0.00062941]
	Learning Rate: 0.000629409
	LOSS [training: 0.2953736901169735 | validation: 0.30856763024345063]
	TIME [epoch: 10.6 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3064429858482356		[learning rate: 0.00062748]
	Learning Rate: 0.00062748
	LOSS [training: 0.3064429858482356 | validation: 0.290723183013704]
	TIME [epoch: 10.6 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.282875623677697		[learning rate: 0.00062556]
	Learning Rate: 0.000625557
	LOSS [training: 0.282875623677697 | validation: 0.31018087580267467]
	TIME [epoch: 10.6 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2853596902449274		[learning rate: 0.00062364]
	Learning Rate: 0.000623639
	LOSS [training: 0.2853596902449274 | validation: 0.29923575857865325]
	TIME [epoch: 10.6 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2821088209895711		[learning rate: 0.00062173]
	Learning Rate: 0.000621727
	LOSS [training: 0.2821088209895711 | validation: 0.2838057065179813]
	TIME [epoch: 10.6 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27487537451823		[learning rate: 0.00061982]
	Learning Rate: 0.000619821
	LOSS [training: 0.27487537451823 | validation: 0.28863963548183497]
	TIME [epoch: 10.6 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27587749623665064		[learning rate: 0.00061792]
	Learning Rate: 0.000617922
	LOSS [training: 0.27587749623665064 | validation: 0.2939618370856189]
	TIME [epoch: 10.6 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28375593201193133		[learning rate: 0.00061603]
	Learning Rate: 0.000616027
	LOSS [training: 0.28375593201193133 | validation: 0.2866333664067047]
	TIME [epoch: 10.6 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28036773420229943		[learning rate: 0.00061414]
	Learning Rate: 0.000614139
	LOSS [training: 0.28036773420229943 | validation: 0.3019510488487618]
	TIME [epoch: 10.6 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2696452659464181		[learning rate: 0.00061226]
	Learning Rate: 0.000612256
	LOSS [training: 0.2696452659464181 | validation: 0.3052706100145857]
	TIME [epoch: 10.6 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2870897819856212		[learning rate: 0.00061038]
	Learning Rate: 0.00061038
	LOSS [training: 0.2870897819856212 | validation: 0.2871886171198374]
	TIME [epoch: 10.6 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2795497286673902		[learning rate: 0.00060851]
	Learning Rate: 0.000608509
	LOSS [training: 0.2795497286673902 | validation: 0.3166102734031795]
	TIME [epoch: 10.6 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31378904210543956		[learning rate: 0.00060664]
	Learning Rate: 0.000606643
	LOSS [training: 0.31378904210543956 | validation: 0.3160210960783024]
	TIME [epoch: 10.6 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29840476385169873		[learning rate: 0.00060478]
	Learning Rate: 0.000604784
	LOSS [training: 0.29840476385169873 | validation: 0.31977376848975825]
	TIME [epoch: 10.6 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29213766393493834		[learning rate: 0.00060293]
	Learning Rate: 0.00060293
	LOSS [training: 0.29213766393493834 | validation: 0.32946076531472945]
	TIME [epoch: 10.6 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28977979036583446		[learning rate: 0.00060108]
	Learning Rate: 0.000601081
	LOSS [training: 0.28977979036583446 | validation: 0.3169528351681876]
	TIME [epoch: 10.6 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2939036598478898		[learning rate: 0.00059924]
	Learning Rate: 0.000599239
	LOSS [training: 0.2939036598478898 | validation: 0.3234238924080644]
	TIME [epoch: 10.6 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29449598131082444		[learning rate: 0.0005974]
	Learning Rate: 0.000597402
	LOSS [training: 0.29449598131082444 | validation: 0.304944449721375]
	TIME [epoch: 10.6 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2730605777051815		[learning rate: 0.00059557]
	Learning Rate: 0.000595571
	LOSS [training: 0.2730605777051815 | validation: 0.29440260406729957]
	TIME [epoch: 10.6 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27706833459337776		[learning rate: 0.00059375]
	Learning Rate: 0.000593745
	LOSS [training: 0.27706833459337776 | validation: 0.3011652633618843]
	TIME [epoch: 10.6 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2903947253630784		[learning rate: 0.00059192]
	Learning Rate: 0.000591925
	LOSS [training: 0.2903947253630784 | validation: 0.3133864377682251]
	TIME [epoch: 10.6 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29358035443050484		[learning rate: 0.00059011]
	Learning Rate: 0.00059011
	LOSS [training: 0.29358035443050484 | validation: 0.2909462677449801]
	TIME [epoch: 10.6 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.277892450117914		[learning rate: 0.0005883]
	Learning Rate: 0.000588302
	LOSS [training: 0.277892450117914 | validation: 0.2944635881418518]
	TIME [epoch: 10.6 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2781490286431712		[learning rate: 0.0005865]
	Learning Rate: 0.000586498
	LOSS [training: 0.2781490286431712 | validation: 0.3023530473522376]
	TIME [epoch: 10.6 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2777188307377916		[learning rate: 0.0005847]
	Learning Rate: 0.0005847
	LOSS [training: 0.2777188307377916 | validation: 0.3011550688041744]
	TIME [epoch: 10.6 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2944883705370597		[learning rate: 0.00058291]
	Learning Rate: 0.000582908
	LOSS [training: 0.2944883705370597 | validation: 0.2996523864658133]
	TIME [epoch: 10.6 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2834520623528202		[learning rate: 0.00058112]
	Learning Rate: 0.000581121
	LOSS [training: 0.2834520623528202 | validation: 0.32780803451479523]
	TIME [epoch: 10.6 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29316544701329617		[learning rate: 0.00057934]
	Learning Rate: 0.00057934
	LOSS [training: 0.29316544701329617 | validation: 0.312093772470563]
	TIME [epoch: 10.6 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2892701438762689		[learning rate: 0.00057756]
	Learning Rate: 0.000577564
	LOSS [training: 0.2892701438762689 | validation: 0.29302536786143113]
	TIME [epoch: 10.6 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2787454257305616		[learning rate: 0.00057579]
	Learning Rate: 0.000575793
	LOSS [training: 0.2787454257305616 | validation: 0.2967999706271277]
	TIME [epoch: 10.6 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2862006693622209		[learning rate: 0.00057403]
	Learning Rate: 0.000574028
	LOSS [training: 0.2862006693622209 | validation: 0.30605557659930605]
	TIME [epoch: 10.6 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29181187163758526		[learning rate: 0.00057227]
	Learning Rate: 0.000572269
	LOSS [training: 0.29181187163758526 | validation: 0.30093636595899914]
	TIME [epoch: 10.6 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2884626391309813		[learning rate: 0.00057051]
	Learning Rate: 0.000570514
	LOSS [training: 0.2884626391309813 | validation: 0.3068819551323223]
	TIME [epoch: 10.6 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2775310709456452		[learning rate: 0.00056877]
	Learning Rate: 0.000568766
	LOSS [training: 0.2775310709456452 | validation: 0.2670297630042797]
	TIME [epoch: 10.6 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27474619522269905		[learning rate: 0.00056702]
	Learning Rate: 0.000567022
	LOSS [training: 0.27474619522269905 | validation: 0.3065388812205662]
	TIME [epoch: 10.6 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2760645638068881		[learning rate: 0.00056528]
	Learning Rate: 0.000565284
	LOSS [training: 0.2760645638068881 | validation: 0.2908969473327651]
	TIME [epoch: 10.6 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2732917762789343		[learning rate: 0.00056355]
	Learning Rate: 0.000563551
	LOSS [training: 0.2732917762789343 | validation: 0.2867208542079067]
	TIME [epoch: 10.6 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2960590379665732		[learning rate: 0.00056182]
	Learning Rate: 0.000561824
	LOSS [training: 0.2960590379665732 | validation: 0.3054784960719958]
	TIME [epoch: 10.6 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29996104466788126		[learning rate: 0.0005601]
	Learning Rate: 0.000560101
	LOSS [training: 0.29996104466788126 | validation: 0.31833056923564956]
	TIME [epoch: 10.6 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29447701747144295		[learning rate: 0.00055838]
	Learning Rate: 0.000558385
	LOSS [training: 0.29447701747144295 | validation: 0.3055348647282558]
	TIME [epoch: 10.6 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2992361193942238		[learning rate: 0.00055667]
	Learning Rate: 0.000556673
	LOSS [training: 0.2992361193942238 | validation: 0.33115666261194476]
	TIME [epoch: 10.6 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3009432169519367		[learning rate: 0.00055497]
	Learning Rate: 0.000554966
	LOSS [training: 0.3009432169519367 | validation: 0.3237844929851916]
	TIME [epoch: 10.6 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29981903901590756		[learning rate: 0.00055327]
	Learning Rate: 0.000553265
	LOSS [training: 0.29981903901590756 | validation: 0.3042816828563984]
	TIME [epoch: 10.6 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30308568759732807		[learning rate: 0.00055157]
	Learning Rate: 0.000551569
	LOSS [training: 0.30308568759732807 | validation: 0.3184170968467947]
	TIME [epoch: 10.6 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2929931767771218		[learning rate: 0.00054988]
	Learning Rate: 0.000549878
	LOSS [training: 0.2929931767771218 | validation: 0.3495107978890792]
	TIME [epoch: 10.6 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33451070136795413		[learning rate: 0.00054819]
	Learning Rate: 0.000548193
	LOSS [training: 0.33451070136795413 | validation: 0.3318520132032447]
	TIME [epoch: 10.6 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33250095935493584		[learning rate: 0.00054651]
	Learning Rate: 0.000546512
	LOSS [training: 0.33250095935493584 | validation: 0.3446438991013743]
	TIME [epoch: 10.6 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33077299544038224		[learning rate: 0.00054484]
	Learning Rate: 0.000544837
	LOSS [training: 0.33077299544038224 | validation: 0.3391940283088003]
	TIME [epoch: 10.6 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2990993277190501		[learning rate: 0.00054317]
	Learning Rate: 0.000543167
	LOSS [training: 0.2990993277190501 | validation: 0.3231412166369234]
	TIME [epoch: 10.6 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31013234435924036		[learning rate: 0.0005415]
	Learning Rate: 0.000541502
	LOSS [training: 0.31013234435924036 | validation: 0.32431847134409497]
	TIME [epoch: 10.6 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3082964424820305		[learning rate: 0.00053984]
	Learning Rate: 0.000539842
	LOSS [training: 0.3082964424820305 | validation: 0.3191659654396006]
	TIME [epoch: 10.6 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32139935956660587		[learning rate: 0.00053819]
	Learning Rate: 0.000538187
	LOSS [training: 0.32139935956660587 | validation: 0.3372987650542431]
	TIME [epoch: 10.6 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31414286317651713		[learning rate: 0.00053654]
	Learning Rate: 0.000536537
	LOSS [training: 0.31414286317651713 | validation: 0.3417399986502167]
	TIME [epoch: 10.6 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3257264930044733		[learning rate: 0.00053489]
	Learning Rate: 0.000534893
	LOSS [training: 0.3257264930044733 | validation: 0.38795848368856367]
	TIME [epoch: 10.6 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3618925622181492		[learning rate: 0.00053325]
	Learning Rate: 0.000533253
	LOSS [training: 0.3618925622181492 | validation: 0.36891924704756723]
	TIME [epoch: 10.6 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34047610833730846		[learning rate: 0.00053162]
	Learning Rate: 0.000531618
	LOSS [training: 0.34047610833730846 | validation: 0.3370292488096834]
	TIME [epoch: 10.6 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31121355143128476		[learning rate: 0.00052999]
	Learning Rate: 0.000529989
	LOSS [training: 0.31121355143128476 | validation: 0.349051113488896]
	TIME [epoch: 10.6 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29639617292986375		[learning rate: 0.00052836]
	Learning Rate: 0.000528364
	LOSS [training: 0.29639617292986375 | validation: 0.30224683642254396]
	TIME [epoch: 10.6 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30028317765987644		[learning rate: 0.00052674]
	Learning Rate: 0.000526744
	LOSS [training: 0.30028317765987644 | validation: 0.31345193901381846]
	TIME [epoch: 10.6 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30820336655981395		[learning rate: 0.00052513]
	Learning Rate: 0.00052513
	LOSS [training: 0.30820336655981395 | validation: 0.29454741147483643]
	TIME [epoch: 10.6 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2834921606301221		[learning rate: 0.00052352]
	Learning Rate: 0.00052352
	LOSS [training: 0.2834921606301221 | validation: 0.28569786598583463]
	TIME [epoch: 10.6 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27667862363993495		[learning rate: 0.00052192]
	Learning Rate: 0.000521915
	LOSS [training: 0.27667862363993495 | validation: 0.2877514576803797]
	TIME [epoch: 10.6 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2730914597493996		[learning rate: 0.00052032]
	Learning Rate: 0.000520315
	LOSS [training: 0.2730914597493996 | validation: 0.29716696554612876]
	TIME [epoch: 10.6 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26763637084260006		[learning rate: 0.00051872]
	Learning Rate: 0.00051872
	LOSS [training: 0.26763637084260006 | validation: 0.3019945425065796]
	TIME [epoch: 10.6 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2702465909480132		[learning rate: 0.00051713]
	Learning Rate: 0.00051713
	LOSS [training: 0.2702465909480132 | validation: 0.2755013690478103]
	TIME [epoch: 10.6 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2669343320855358		[learning rate: 0.00051555]
	Learning Rate: 0.000515545
	LOSS [training: 0.2669343320855358 | validation: 0.27491484082822054]
	TIME [epoch: 10.6 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27043734550909676		[learning rate: 0.00051396]
	Learning Rate: 0.000513965
	LOSS [training: 0.27043734550909676 | validation: 0.2827082047273009]
	TIME [epoch: 10.6 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2741763678613885		[learning rate: 0.00051239]
	Learning Rate: 0.000512389
	LOSS [training: 0.2741763678613885 | validation: 0.2699097012373263]
	TIME [epoch: 10.6 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2890328287816975		[learning rate: 0.00051082]
	Learning Rate: 0.000510818
	LOSS [training: 0.2890328287816975 | validation: 0.2825073457499119]
	TIME [epoch: 10.6 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27720532283776095		[learning rate: 0.00050925]
	Learning Rate: 0.000509253
	LOSS [training: 0.27720532283776095 | validation: 0.25663789478804544]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_1470.pth
	Model improved!!!
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2752397267121907		[learning rate: 0.00050769]
	Learning Rate: 0.000507692
	LOSS [training: 0.2752397267121907 | validation: 0.2859428577968412]
	TIME [epoch: 10.6 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3150039774300114		[learning rate: 0.00050614]
	Learning Rate: 0.000506135
	LOSS [training: 0.3150039774300114 | validation: 0.32605636887649814]
	TIME [epoch: 10.6 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3086294803743336		[learning rate: 0.00050458]
	Learning Rate: 0.000504584
	LOSS [training: 0.3086294803743336 | validation: 0.290094815780488]
	TIME [epoch: 10.6 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29747510191041515		[learning rate: 0.00050304]
	Learning Rate: 0.000503037
	LOSS [training: 0.29747510191041515 | validation: 0.2922184809939566]
	TIME [epoch: 10.6 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29497090885103056		[learning rate: 0.00050149]
	Learning Rate: 0.000501495
	LOSS [training: 0.29497090885103056 | validation: 0.3000771682167763]
	TIME [epoch: 10.6 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3047177683114277		[learning rate: 0.00049996]
	Learning Rate: 0.000499958
	LOSS [training: 0.3047177683114277 | validation: 0.285714186163281]
	TIME [epoch: 10.6 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29846105597977585		[learning rate: 0.00049843]
	Learning Rate: 0.000498425
	LOSS [training: 0.29846105597977585 | validation: 0.2771941252259046]
	TIME [epoch: 10.6 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29620483038092027		[learning rate: 0.0004969]
	Learning Rate: 0.000496897
	LOSS [training: 0.29620483038092027 | validation: 0.2865716621984521]
	TIME [epoch: 10.6 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29201407049022715		[learning rate: 0.00049537]
	Learning Rate: 0.000495374
	LOSS [training: 0.29201407049022715 | validation: 0.3011539520373509]
	TIME [epoch: 10.6 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2817685830271475		[learning rate: 0.00049386]
	Learning Rate: 0.000493856
	LOSS [training: 0.2817685830271475 | validation: 0.2717030460720777]
	TIME [epoch: 10.6 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2685101033584557		[learning rate: 0.00049234]
	Learning Rate: 0.000492342
	LOSS [training: 0.2685101033584557 | validation: 0.25774158030818717]
	TIME [epoch: 10.6 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2676323810030597		[learning rate: 0.00049083]
	Learning Rate: 0.000490832
	LOSS [training: 0.2676323810030597 | validation: 0.2793273563333723]
	TIME [epoch: 10.6 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27295995980073184		[learning rate: 0.00048933]
	Learning Rate: 0.000489328
	LOSS [training: 0.27295995980073184 | validation: 0.2771143907187743]
	TIME [epoch: 10.6 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2709397213121418		[learning rate: 0.00048783]
	Learning Rate: 0.000487828
	LOSS [training: 0.2709397213121418 | validation: 0.2703954653597153]
	TIME [epoch: 10.6 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27777479212296574		[learning rate: 0.00048633]
	Learning Rate: 0.000486333
	LOSS [training: 0.27777479212296574 | validation: 0.2724992747978087]
	TIME [epoch: 10.6 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2739287409008116		[learning rate: 0.00048484]
	Learning Rate: 0.000484842
	LOSS [training: 0.2739287409008116 | validation: 0.27223080727730525]
	TIME [epoch: 10.6 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27277549566426285		[learning rate: 0.00048336]
	Learning Rate: 0.000483356
	LOSS [training: 0.27277549566426285 | validation: 0.2476811512212642]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_1487.pth
	Model improved!!!
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25727983411177757		[learning rate: 0.00048187]
	Learning Rate: 0.000481874
	LOSS [training: 0.25727983411177757 | validation: 0.2622451560340435]
	TIME [epoch: 10.6 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2692982647416536		[learning rate: 0.0004804]
	Learning Rate: 0.000480397
	LOSS [training: 0.2692982647416536 | validation: 0.2587666376481619]
	TIME [epoch: 10.6 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2713771573230465		[learning rate: 0.00047892]
	Learning Rate: 0.000478924
	LOSS [training: 0.2713771573230465 | validation: 0.29875200472541885]
	TIME [epoch: 10.6 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26941627784466754		[learning rate: 0.00047746]
	Learning Rate: 0.000477456
	LOSS [training: 0.26941627784466754 | validation: 0.29401764367679106]
	TIME [epoch: 10.6 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26772514774415146		[learning rate: 0.00047599]
	Learning Rate: 0.000475992
	LOSS [training: 0.26772514774415146 | validation: 0.271114115950434]
	TIME [epoch: 10.6 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2608749825486712		[learning rate: 0.00047453]
	Learning Rate: 0.000474533
	LOSS [training: 0.2608749825486712 | validation: 0.2628527186269413]
	TIME [epoch: 10.6 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2671336603617942		[learning rate: 0.00047308]
	Learning Rate: 0.000473079
	LOSS [training: 0.2671336603617942 | validation: 0.2719921462279106]
	TIME [epoch: 10.6 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2689613471183139		[learning rate: 0.00047163]
	Learning Rate: 0.000471628
	LOSS [training: 0.2689613471183139 | validation: 0.2625199832199975]
	TIME [epoch: 10.6 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2635250466961138		[learning rate: 0.00047018]
	Learning Rate: 0.000470183
	LOSS [training: 0.2635250466961138 | validation: 0.27382222835697917]
	TIME [epoch: 10.6 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2706351198180772		[learning rate: 0.00046874]
	Learning Rate: 0.000468741
	LOSS [training: 0.2706351198180772 | validation: 0.29313334225057597]
	TIME [epoch: 10.6 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2785944842407562		[learning rate: 0.0004673]
	Learning Rate: 0.000467304
	LOSS [training: 0.2785944842407562 | validation: 0.28141261392975603]
	TIME [epoch: 10.6 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26625535602070555		[learning rate: 0.00046587]
	Learning Rate: 0.000465872
	LOSS [training: 0.26625535602070555 | validation: 0.28485858492239113]
	TIME [epoch: 10.6 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2555203016877143		[learning rate: 0.00046444]
	Learning Rate: 0.000464444
	LOSS [training: 0.2555203016877143 | validation: 0.27644577264421005]
	TIME [epoch: 10.6 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2575223582112102		[learning rate: 0.00046302]
	Learning Rate: 0.00046302
	LOSS [training: 0.2575223582112102 | validation: 0.25966674814547014]
	TIME [epoch: 10.6 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.255057796176493		[learning rate: 0.0004616]
	Learning Rate: 0.000461601
	LOSS [training: 0.255057796176493 | validation: 0.2669810712433733]
	TIME [epoch: 10.6 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2560607459638128		[learning rate: 0.00046019]
	Learning Rate: 0.000460186
	LOSS [training: 0.2560607459638128 | validation: 0.27861996305719333]
	TIME [epoch: 10.6 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25775080844007153		[learning rate: 0.00045878]
	Learning Rate: 0.000458775
	LOSS [training: 0.25775080844007153 | validation: 0.2719873014791817]
	TIME [epoch: 10.6 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2526295899143983		[learning rate: 0.00045737]
	Learning Rate: 0.000457369
	LOSS [training: 0.2526295899143983 | validation: 0.2726090731217876]
	TIME [epoch: 10.6 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26856699342894136		[learning rate: 0.00045597]
	Learning Rate: 0.000455967
	LOSS [training: 0.26856699342894136 | validation: 0.26689695609945735]
	TIME [epoch: 10.6 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2643534286503071		[learning rate: 0.00045457]
	Learning Rate: 0.000454569
	LOSS [training: 0.2643534286503071 | validation: 0.2621464987621257]
	TIME [epoch: 10.6 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25510595283140935		[learning rate: 0.00045318]
	Learning Rate: 0.000453176
	LOSS [training: 0.25510595283140935 | validation: 0.26663703544886097]
	TIME [epoch: 10.6 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25788917408261847		[learning rate: 0.00045179]
	Learning Rate: 0.000451787
	LOSS [training: 0.25788917408261847 | validation: 0.2660361954726808]
	TIME [epoch: 10.6 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24805527981892875		[learning rate: 0.0004504]
	Learning Rate: 0.000450402
	LOSS [training: 0.24805527981892875 | validation: 0.25007642804839014]
	TIME [epoch: 10.6 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25887428840889976		[learning rate: 0.00044902]
	Learning Rate: 0.000449021
	LOSS [training: 0.25887428840889976 | validation: 0.2670333557940414]
	TIME [epoch: 10.6 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2728208173143026		[learning rate: 0.00044764]
	Learning Rate: 0.000447645
	LOSS [training: 0.2728208173143026 | validation: 0.267296503873891]
	TIME [epoch: 10.6 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25032154923541683		[learning rate: 0.00044627]
	Learning Rate: 0.000446272
	LOSS [training: 0.25032154923541683 | validation: 0.2428517562209447]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_1513.pth
	Model improved!!!
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25113001992152106		[learning rate: 0.0004449]
	Learning Rate: 0.000444904
	LOSS [training: 0.25113001992152106 | validation: 0.2655428791594195]
	TIME [epoch: 10.6 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25523847072352107		[learning rate: 0.00044354]
	Learning Rate: 0.000443541
	LOSS [training: 0.25523847072352107 | validation: 0.25445471472016]
	TIME [epoch: 10.6 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2558084724795895		[learning rate: 0.00044218]
	Learning Rate: 0.000442181
	LOSS [training: 0.2558084724795895 | validation: 0.25838655881192024]
	TIME [epoch: 10.6 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2643214391784805		[learning rate: 0.00044083]
	Learning Rate: 0.000440825
	LOSS [training: 0.2643214391784805 | validation: 0.261401483785599]
	TIME [epoch: 10.6 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2622787595423032		[learning rate: 0.00043947]
	Learning Rate: 0.000439474
	LOSS [training: 0.2622787595423032 | validation: 0.2642672897608787]
	TIME [epoch: 10.6 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25894239030886324		[learning rate: 0.00043813]
	Learning Rate: 0.000438127
	LOSS [training: 0.25894239030886324 | validation: 0.2557577629436056]
	TIME [epoch: 10.6 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26726124515826777		[learning rate: 0.00043678]
	Learning Rate: 0.000436784
	LOSS [training: 0.26726124515826777 | validation: 0.26916320122360954]
	TIME [epoch: 10.6 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2686941772853236		[learning rate: 0.00043544]
	Learning Rate: 0.000435445
	LOSS [training: 0.2686941772853236 | validation: 0.25303657049734024]
	TIME [epoch: 10.6 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2536026982917575		[learning rate: 0.00043411]
	Learning Rate: 0.00043411
	LOSS [training: 0.2536026982917575 | validation: 0.25564749168622475]
	TIME [epoch: 10.6 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2742495937520989		[learning rate: 0.00043278]
	Learning Rate: 0.00043278
	LOSS [training: 0.2742495937520989 | validation: 0.2720260271014974]
	TIME [epoch: 10.6 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2676157834644508		[learning rate: 0.00043145]
	Learning Rate: 0.000431453
	LOSS [training: 0.2676157834644508 | validation: 0.26599884766128684]
	TIME [epoch: 10.6 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2653196240186418		[learning rate: 0.00043013]
	Learning Rate: 0.00043013
	LOSS [training: 0.2653196240186418 | validation: 0.2651611697098502]
	TIME [epoch: 10.6 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2657461304844436		[learning rate: 0.00042881]
	Learning Rate: 0.000428812
	LOSS [training: 0.2657461304844436 | validation: 0.27369607785623784]
	TIME [epoch: 10.6 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26336775000142365		[learning rate: 0.0004275]
	Learning Rate: 0.000427497
	LOSS [training: 0.26336775000142365 | validation: 0.27088087673459965]
	TIME [epoch: 10.6 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26672277931262645		[learning rate: 0.00042619]
	Learning Rate: 0.000426187
	LOSS [training: 0.26672277931262645 | validation: 0.2657013537752873]
	TIME [epoch: 10.6 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26282152911903484		[learning rate: 0.00042488]
	Learning Rate: 0.00042488
	LOSS [training: 0.26282152911903484 | validation: 0.2785001199558149]
	TIME [epoch: 10.6 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2714626110005035		[learning rate: 0.00042358]
	Learning Rate: 0.000423578
	LOSS [training: 0.2714626110005035 | validation: 0.259225590178472]
	TIME [epoch: 10.6 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26107427395332544		[learning rate: 0.00042228]
	Learning Rate: 0.000422279
	LOSS [training: 0.26107427395332544 | validation: 0.2744980538000265]
	TIME [epoch: 10.6 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26135250611956373		[learning rate: 0.00042098]
	Learning Rate: 0.000420985
	LOSS [training: 0.26135250611956373 | validation: 0.26199180637370834]
	TIME [epoch: 10.6 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2609855270895572		[learning rate: 0.00041969]
	Learning Rate: 0.000419694
	LOSS [training: 0.2609855270895572 | validation: 0.2609150975635252]
	TIME [epoch: 10.6 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26262077488216357		[learning rate: 0.00041841]
	Learning Rate: 0.000418408
	LOSS [training: 0.26262077488216357 | validation: 0.29096791299638347]
	TIME [epoch: 10.6 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.286034425874304		[learning rate: 0.00041713]
	Learning Rate: 0.000417125
	LOSS [training: 0.286034425874304 | validation: 0.2609614997687174]
	TIME [epoch: 10.6 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2809382267024709		[learning rate: 0.00041585]
	Learning Rate: 0.000415847
	LOSS [training: 0.2809382267024709 | validation: 0.26534751836369286]
	TIME [epoch: 10.6 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27619508657358294		[learning rate: 0.00041457]
	Learning Rate: 0.000414572
	LOSS [training: 0.27619508657358294 | validation: 0.26227257324750725]
	TIME [epoch: 10.6 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26880694488575846		[learning rate: 0.0004133]
	Learning Rate: 0.000413301
	LOSS [training: 0.26880694488575846 | validation: 0.27605707300536914]
	TIME [epoch: 10.6 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27194143448271035		[learning rate: 0.00041203]
	Learning Rate: 0.000412034
	LOSS [training: 0.27194143448271035 | validation: 0.27892019364963083]
	TIME [epoch: 10.6 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26861735098866896		[learning rate: 0.00041077]
	Learning Rate: 0.000410771
	LOSS [training: 0.26861735098866896 | validation: 0.26286489095111504]
	TIME [epoch: 10.6 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2606666550675552		[learning rate: 0.00040951]
	Learning Rate: 0.000409512
	LOSS [training: 0.2606666550675552 | validation: 0.2564257932406383]
	TIME [epoch: 10.6 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2679568933545705		[learning rate: 0.00040826]
	Learning Rate: 0.000408257
	LOSS [training: 0.2679568933545705 | validation: 0.2716162127394441]
	TIME [epoch: 10.6 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2906686695136744		[learning rate: 0.00040701]
	Learning Rate: 0.000407005
	LOSS [training: 0.2906686695136744 | validation: 0.27760228700135997]
	TIME [epoch: 10.6 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2856128208184276		[learning rate: 0.00040576]
	Learning Rate: 0.000405758
	LOSS [training: 0.2856128208184276 | validation: 0.279354819805356]
	TIME [epoch: 10.6 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2858994191764067		[learning rate: 0.00040451]
	Learning Rate: 0.000404514
	LOSS [training: 0.2858994191764067 | validation: 0.26900793010175833]
	TIME [epoch: 10.6 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27740492490126567		[learning rate: 0.00040327]
	Learning Rate: 0.000403274
	LOSS [training: 0.27740492490126567 | validation: 0.2858712591569689]
	TIME [epoch: 10.6 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26627281316624013		[learning rate: 0.00040204]
	Learning Rate: 0.000402038
	LOSS [training: 0.26627281316624013 | validation: 0.26266663554165354]
	TIME [epoch: 10.6 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2738187009806014		[learning rate: 0.00040081]
	Learning Rate: 0.000400805
	LOSS [training: 0.2738187009806014 | validation: 0.2683640492952255]
	TIME [epoch: 10.6 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28512600041730485		[learning rate: 0.00039958]
	Learning Rate: 0.000399577
	LOSS [training: 0.28512600041730485 | validation: 0.29024333374946326]
	TIME [epoch: 10.6 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29353904470893377		[learning rate: 0.00039835]
	Learning Rate: 0.000398352
	LOSS [training: 0.29353904470893377 | validation: 0.27317615516026356]
	TIME [epoch: 10.6 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28198362067486177		[learning rate: 0.00039713]
	Learning Rate: 0.000397131
	LOSS [training: 0.28198362067486177 | validation: 0.2727304026410996]
	TIME [epoch: 10.6 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26010308386371184		[learning rate: 0.00039591]
	Learning Rate: 0.000395913
	LOSS [training: 0.26010308386371184 | validation: 0.271939931302363]
	TIME [epoch: 10.6 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2632772572368661		[learning rate: 0.0003947]
	Learning Rate: 0.0003947
	LOSS [training: 0.2632772572368661 | validation: 0.26303839295049464]
	TIME [epoch: 10.6 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2610831713021684		[learning rate: 0.00039349]
	Learning Rate: 0.00039349
	LOSS [training: 0.2610831713021684 | validation: 0.26634443915147576]
	TIME [epoch: 10.6 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2651126244018939		[learning rate: 0.00039228]
	Learning Rate: 0.000392283
	LOSS [training: 0.2651126244018939 | validation: 0.2845087805903826]
	TIME [epoch: 10.6 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2613531429058708		[learning rate: 0.00039108]
	Learning Rate: 0.000391081
	LOSS [training: 0.2613531429058708 | validation: 0.26053879786907813]
	TIME [epoch: 10.6 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2625601787034165		[learning rate: 0.00038988]
	Learning Rate: 0.000389882
	LOSS [training: 0.2625601787034165 | validation: 0.2582520890039763]
	TIME [epoch: 10.6 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2671991181247691		[learning rate: 0.00038869]
	Learning Rate: 0.000388687
	LOSS [training: 0.2671991181247691 | validation: 0.2860422598787443]
	TIME [epoch: 10.6 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2643752198033358		[learning rate: 0.0003875]
	Learning Rate: 0.000387495
	LOSS [training: 0.2643752198033358 | validation: 0.2786585991586758]
	TIME [epoch: 10.6 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26482340967736395		[learning rate: 0.00038631]
	Learning Rate: 0.000386308
	LOSS [training: 0.26482340967736395 | validation: 0.261189055497216]
	TIME [epoch: 10.6 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26520674874312195		[learning rate: 0.00038512]
	Learning Rate: 0.000385123
	LOSS [training: 0.26520674874312195 | validation: 0.2850118460703391]
	TIME [epoch: 10.6 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26961757870887265		[learning rate: 0.00038394]
	Learning Rate: 0.000383943
	LOSS [training: 0.26961757870887265 | validation: 0.29229049754123476]
	TIME [epoch: 10.6 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2745474758101984		[learning rate: 0.00038277]
	Learning Rate: 0.000382766
	LOSS [training: 0.2745474758101984 | validation: 0.2770083466523556]
	TIME [epoch: 10.6 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.284646890933608		[learning rate: 0.00038159]
	Learning Rate: 0.000381593
	LOSS [training: 0.284646890933608 | validation: 0.28476683511130757]
	TIME [epoch: 10.6 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.268722889713678		[learning rate: 0.00038042]
	Learning Rate: 0.000380423
	LOSS [training: 0.268722889713678 | validation: 0.2830293838609495]
	TIME [epoch: 10.6 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2631827769125922		[learning rate: 0.00037926]
	Learning Rate: 0.000379257
	LOSS [training: 0.2631827769125922 | validation: 0.27120284374659465]
	TIME [epoch: 10.6 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2604069463137641		[learning rate: 0.00037809]
	Learning Rate: 0.000378094
	LOSS [training: 0.2604069463137641 | validation: 0.27876794933590526]
	TIME [epoch: 10.6 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2732424848981502		[learning rate: 0.00037694]
	Learning Rate: 0.000376935
	LOSS [training: 0.2732424848981502 | validation: 0.27370577639802607]
	TIME [epoch: 10.6 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26235678290492626		[learning rate: 0.00037578]
	Learning Rate: 0.00037578
	LOSS [training: 0.26235678290492626 | validation: 0.2769419544721396]
	TIME [epoch: 10.6 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2610547225420804		[learning rate: 0.00037463]
	Learning Rate: 0.000374628
	LOSS [training: 0.2610547225420804 | validation: 0.28332613563502734]
	TIME [epoch: 10.6 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2749680503523947		[learning rate: 0.00037348]
	Learning Rate: 0.000373479
	LOSS [training: 0.2749680503523947 | validation: 0.2831884643269562]
	TIME [epoch: 10.6 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26077906888036045		[learning rate: 0.00037233]
	Learning Rate: 0.000372335
	LOSS [training: 0.26077906888036045 | validation: 0.3030106809140108]
	TIME [epoch: 10.6 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26740541371994075		[learning rate: 0.00037119]
	Learning Rate: 0.000371193
	LOSS [training: 0.26740541371994075 | validation: 0.29014874112733713]
	TIME [epoch: 10.6 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2757899267974228		[learning rate: 0.00037006]
	Learning Rate: 0.000370055
	LOSS [training: 0.2757899267974228 | validation: 0.30359059965232343]
	TIME [epoch: 10.6 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2853282296848163		[learning rate: 0.00036892]
	Learning Rate: 0.000368921
	LOSS [training: 0.2853282296848163 | validation: 0.30886985569084635]
	TIME [epoch: 10.6 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27774886264175624		[learning rate: 0.00036779]
	Learning Rate: 0.00036779
	LOSS [training: 0.27774886264175624 | validation: 0.29247114653647144]
	TIME [epoch: 10.6 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2722064819725968		[learning rate: 0.00036666]
	Learning Rate: 0.000366663
	LOSS [training: 0.2722064819725968 | validation: 0.3036824768450795]
	TIME [epoch: 10.6 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2747382524064887		[learning rate: 0.00036554]
	Learning Rate: 0.000365539
	LOSS [training: 0.2747382524064887 | validation: 0.2854407783195548]
	TIME [epoch: 10.6 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27350108016294555		[learning rate: 0.00036442]
	Learning Rate: 0.000364418
	LOSS [training: 0.27350108016294555 | validation: 0.26843206821309196]
	TIME [epoch: 10.6 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2731786379976814		[learning rate: 0.0003633]
	Learning Rate: 0.000363301
	LOSS [training: 0.2731786379976814 | validation: 0.2801546058466727]
	TIME [epoch: 10.6 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2718389140354701		[learning rate: 0.00036219]
	Learning Rate: 0.000362187
	LOSS [training: 0.2718389140354701 | validation: 0.28566564851636045]
	TIME [epoch: 10.6 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27570436829933076		[learning rate: 0.00036108]
	Learning Rate: 0.000361077
	LOSS [training: 0.27570436829933076 | validation: 0.2873817782563348]
	TIME [epoch: 10.6 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27358583428644784		[learning rate: 0.00035997]
	Learning Rate: 0.00035997
	LOSS [training: 0.27358583428644784 | validation: 0.3055980743138851]
	TIME [epoch: 10.6 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27533713884102007		[learning rate: 0.00035887]
	Learning Rate: 0.000358867
	LOSS [training: 0.27533713884102007 | validation: 0.28916076879709335]
	TIME [epoch: 10.6 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2734979906634144		[learning rate: 0.00035777]
	Learning Rate: 0.000357767
	LOSS [training: 0.2734979906634144 | validation: 0.30106385717017176]
	TIME [epoch: 10.6 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2808549821989733		[learning rate: 0.00035667]
	Learning Rate: 0.00035667
	LOSS [training: 0.2808549821989733 | validation: 0.30126338071138264]
	TIME [epoch: 10.6 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2592293333131274		[learning rate: 0.00035558]
	Learning Rate: 0.000355577
	LOSS [training: 0.2592293333131274 | validation: 0.2561902623379339]
	TIME [epoch: 10.6 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27304789576697563		[learning rate: 0.00035449]
	Learning Rate: 0.000354487
	LOSS [training: 0.27304789576697563 | validation: 0.29920490208268186]
	TIME [epoch: 10.5 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26905060588929003		[learning rate: 0.0003534]
	Learning Rate: 0.0003534
	LOSS [training: 0.26905060588929003 | validation: 0.2826055787757719]
	TIME [epoch: 10.6 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26794848878404026		[learning rate: 0.00035232]
	Learning Rate: 0.000352317
	LOSS [training: 0.26794848878404026 | validation: 0.2712259770293701]
	TIME [epoch: 10.6 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2697419388581769		[learning rate: 0.00035124]
	Learning Rate: 0.000351237
	LOSS [training: 0.2697419388581769 | validation: 0.2844370672971294]
	TIME [epoch: 10.6 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2753248901402451		[learning rate: 0.00035016]
	Learning Rate: 0.00035016
	LOSS [training: 0.2753248901402451 | validation: 0.28892945533609316]
	TIME [epoch: 10.5 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27178123296391166		[learning rate: 0.00034909]
	Learning Rate: 0.000349087
	LOSS [training: 0.27178123296391166 | validation: 0.2801853346520555]
	TIME [epoch: 10.6 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2669686641675436		[learning rate: 0.00034802]
	Learning Rate: 0.000348017
	LOSS [training: 0.2669686641675436 | validation: 0.29134101318622485]
	TIME [epoch: 10.6 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2755219966859964		[learning rate: 0.00034695]
	Learning Rate: 0.00034695
	LOSS [training: 0.2755219966859964 | validation: 0.3017269339684685]
	TIME [epoch: 10.6 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2799328230271325		[learning rate: 0.00034589]
	Learning Rate: 0.000345886
	LOSS [training: 0.2799328230271325 | validation: 0.30071178869337384]
	TIME [epoch: 10.6 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28542080515764023		[learning rate: 0.00034483]
	Learning Rate: 0.000344826
	LOSS [training: 0.28542080515764023 | validation: 0.292893684710714]
	TIME [epoch: 10.6 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2680653200752272		[learning rate: 0.00034377]
	Learning Rate: 0.000343769
	LOSS [training: 0.2680653200752272 | validation: 0.302136658164822]
	TIME [epoch: 10.6 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27608598717419114		[learning rate: 0.00034272]
	Learning Rate: 0.000342715
	LOSS [training: 0.27608598717419114 | validation: 0.2968444162148577]
	TIME [epoch: 10.6 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28298626957498174		[learning rate: 0.00034166]
	Learning Rate: 0.000341665
	LOSS [training: 0.28298626957498174 | validation: 0.28856897649470303]
	TIME [epoch: 10.6 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27492911393291675		[learning rate: 0.00034062]
	Learning Rate: 0.000340617
	LOSS [training: 0.27492911393291675 | validation: 0.28420713254900026]
	TIME [epoch: 10.6 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2706089512361716		[learning rate: 0.00033957]
	Learning Rate: 0.000339573
	LOSS [training: 0.2706089512361716 | validation: 0.2832086420623118]
	TIME [epoch: 10.6 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.269406805863124		[learning rate: 0.00033853]
	Learning Rate: 0.000338532
	LOSS [training: 0.269406805863124 | validation: 0.2860313848840957]
	TIME [epoch: 10.6 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26811284720617734		[learning rate: 0.00033749]
	Learning Rate: 0.000337494
	LOSS [training: 0.26811284720617734 | validation: 0.2860177659731738]
	TIME [epoch: 10.6 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27148609828664244		[learning rate: 0.00033646]
	Learning Rate: 0.00033646
	LOSS [training: 0.27148609828664244 | validation: 0.27458626033813743]
	TIME [epoch: 10.5 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2726950879542488		[learning rate: 0.00033543]
	Learning Rate: 0.000335428
	LOSS [training: 0.2726950879542488 | validation: 0.27197319381196733]
	TIME [epoch: 10.6 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2763984381286431		[learning rate: 0.0003344]
	Learning Rate: 0.0003344
	LOSS [training: 0.2763984381286431 | validation: 0.29448131699286423]
	TIME [epoch: 10.6 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26587260935034973		[learning rate: 0.00033338]
	Learning Rate: 0.000333375
	LOSS [training: 0.26587260935034973 | validation: 0.2889185134837686]
	TIME [epoch: 10.6 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2761423629206916		[learning rate: 0.00033235]
	Learning Rate: 0.000332353
	LOSS [training: 0.2761423629206916 | validation: 0.29076360851098576]
	TIME [epoch: 10.5 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2997851833468105		[learning rate: 0.00033133]
	Learning Rate: 0.000331334
	LOSS [training: 0.2997851833468105 | validation: 0.30439483335497336]
	TIME [epoch: 10.6 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31241516531611396		[learning rate: 0.00033032]
	Learning Rate: 0.000330319
	LOSS [training: 0.31241516531611396 | validation: 0.32915898934701704]
	TIME [epoch: 10.6 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3086374554878607		[learning rate: 0.00032931]
	Learning Rate: 0.000329306
	LOSS [training: 0.3086374554878607 | validation: 0.3166672348820037]
	TIME [epoch: 10.6 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.323822442705308		[learning rate: 0.0003283]
	Learning Rate: 0.000328297
	LOSS [training: 0.323822442705308 | validation: 0.31530859138034645]
	TIME [epoch: 10.6 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30265053256411434		[learning rate: 0.00032729]
	Learning Rate: 0.00032729
	LOSS [training: 0.30265053256411434 | validation: 0.29602712526870745]
	TIME [epoch: 10.5 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27144585380107633		[learning rate: 0.00032629]
	Learning Rate: 0.000326287
	LOSS [training: 0.27144585380107633 | validation: 0.27186832939321987]
	TIME [epoch: 10.6 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2773641492393447		[learning rate: 0.00032529]
	Learning Rate: 0.000325287
	LOSS [training: 0.2773641492393447 | validation: 0.27022830286010013]
	TIME [epoch: 10.6 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27799582154318414		[learning rate: 0.00032429]
	Learning Rate: 0.00032429
	LOSS [training: 0.27799582154318414 | validation: 0.2766893208690578]
	TIME [epoch: 10.6 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2849259052533566		[learning rate: 0.0003233]
	Learning Rate: 0.000323296
	LOSS [training: 0.2849259052533566 | validation: 0.29376667894190583]
	TIME [epoch: 10.6 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2871682056443947		[learning rate: 0.0003223]
	Learning Rate: 0.000322305
	LOSS [training: 0.2871682056443947 | validation: 0.2967217917853654]
	TIME [epoch: 10.6 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27760389948145736		[learning rate: 0.00032132]
	Learning Rate: 0.000321317
	LOSS [training: 0.27760389948145736 | validation: 0.3016332749130004]
	TIME [epoch: 10.6 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2842420315007959		[learning rate: 0.00032033]
	Learning Rate: 0.000320332
	LOSS [training: 0.2842420315007959 | validation: 0.3022674327472932]
	TIME [epoch: 10.6 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28864728113735943		[learning rate: 0.00031935]
	Learning Rate: 0.00031935
	LOSS [training: 0.28864728113735943 | validation: 0.3063331916308908]
	TIME [epoch: 10.6 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28521050721051316		[learning rate: 0.00031837]
	Learning Rate: 0.000318371
	LOSS [training: 0.28521050721051316 | validation: 0.2952319960782001]
	TIME [epoch: 10.5 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2913819105992066		[learning rate: 0.00031739]
	Learning Rate: 0.000317395
	LOSS [training: 0.2913819105992066 | validation: 0.28957101008098796]
	TIME [epoch: 10.6 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2807557641700905		[learning rate: 0.00031642]
	Learning Rate: 0.000316422
	LOSS [training: 0.2807557641700905 | validation: 0.2898604284459539]
	TIME [epoch: 10.6 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2819571979824481		[learning rate: 0.00031545]
	Learning Rate: 0.000315452
	LOSS [training: 0.2819571979824481 | validation: 0.28565265176837423]
	TIME [epoch: 10.6 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2915106113159744		[learning rate: 0.00031449]
	Learning Rate: 0.000314485
	LOSS [training: 0.2915106113159744 | validation: 0.3098488785373618]
	TIME [epoch: 10.6 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2880132165437115		[learning rate: 0.00031352]
	Learning Rate: 0.000313521
	LOSS [training: 0.2880132165437115 | validation: 0.3097710566074748]
	TIME [epoch: 10.6 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2904543614493281		[learning rate: 0.00031256]
	Learning Rate: 0.00031256
	LOSS [training: 0.2904543614493281 | validation: 0.30674197349432974]
	TIME [epoch: 10.6 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2783037611734071		[learning rate: 0.0003116]
	Learning Rate: 0.000311602
	LOSS [training: 0.2783037611734071 | validation: 0.2873134548385311]
	TIME [epoch: 10.6 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2747940382745271		[learning rate: 0.00031065]
	Learning Rate: 0.000310647
	LOSS [training: 0.2747940382745271 | validation: 0.2863816385103709]
	TIME [epoch: 10.6 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.289240233209798		[learning rate: 0.00030969]
	Learning Rate: 0.000309694
	LOSS [training: 0.289240233209798 | validation: 0.2976658093417449]
	TIME [epoch: 10.6 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28548800136998614		[learning rate: 0.00030874]
	Learning Rate: 0.000308745
	LOSS [training: 0.28548800136998614 | validation: 0.3015313516916382]
	TIME [epoch: 10.6 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2857788811754053		[learning rate: 0.0003078]
	Learning Rate: 0.000307799
	LOSS [training: 0.2857788811754053 | validation: 0.3012300901461365]
	TIME [epoch: 10.6 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29494399659104076		[learning rate: 0.00030686]
	Learning Rate: 0.000306855
	LOSS [training: 0.29494399659104076 | validation: 0.30045406871071645]
	TIME [epoch: 10.6 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2865664234380157		[learning rate: 0.00030591]
	Learning Rate: 0.000305914
	LOSS [training: 0.2865664234380157 | validation: 0.32608760240485796]
	TIME [epoch: 10.6 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2952039233681513		[learning rate: 0.00030498]
	Learning Rate: 0.000304977
	LOSS [training: 0.2952039233681513 | validation: 0.3091206109618597]
	TIME [epoch: 10.6 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29682834716886125		[learning rate: 0.00030404]
	Learning Rate: 0.000304042
	LOSS [training: 0.29682834716886125 | validation: 0.32150380529934214]
	TIME [epoch: 10.6 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29548683519531604		[learning rate: 0.00030311]
	Learning Rate: 0.00030311
	LOSS [training: 0.29548683519531604 | validation: 0.3180691600459577]
	TIME [epoch: 10.6 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29207137991750376		[learning rate: 0.00030218]
	Learning Rate: 0.000302181
	LOSS [training: 0.29207137991750376 | validation: 0.2932594183378806]
	TIME [epoch: 10.6 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2897943507606503		[learning rate: 0.00030125]
	Learning Rate: 0.000301254
	LOSS [training: 0.2897943507606503 | validation: 0.3077484610056692]
	TIME [epoch: 10.6 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2874068501947299		[learning rate: 0.00030033]
	Learning Rate: 0.000300331
	LOSS [training: 0.2874068501947299 | validation: 0.3018133510642886]
	TIME [epoch: 10.6 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2980173066167502		[learning rate: 0.00029941]
	Learning Rate: 0.00029941
	LOSS [training: 0.2980173066167502 | validation: 0.29899497326808383]
	TIME [epoch: 10.6 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30027759412993343		[learning rate: 0.00029849]
	Learning Rate: 0.000298492
	LOSS [training: 0.30027759412993343 | validation: 0.3175831391915833]
	TIME [epoch: 10.6 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29089579024621964		[learning rate: 0.00029758]
	Learning Rate: 0.000297577
	LOSS [training: 0.29089579024621964 | validation: 0.3082100301629629]
	TIME [epoch: 10.6 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2967404878574224		[learning rate: 0.00029667]
	Learning Rate: 0.000296665
	LOSS [training: 0.2967404878574224 | validation: 0.3055336224650391]
	TIME [epoch: 10.6 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3139900673839986		[learning rate: 0.00029576]
	Learning Rate: 0.000295756
	LOSS [training: 0.3139900673839986 | validation: 0.3178136753025681]
	TIME [epoch: 10.6 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30026770569978223		[learning rate: 0.00029485]
	Learning Rate: 0.000294849
	LOSS [training: 0.30026770569978223 | validation: 0.2899525193216476]
	TIME [epoch: 10.6 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29578831707285336		[learning rate: 0.00029395]
	Learning Rate: 0.000293945
	LOSS [training: 0.29578831707285336 | validation: 0.2984116390050299]
	TIME [epoch: 10.6 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27851871990444393		[learning rate: 0.00029304]
	Learning Rate: 0.000293044
	LOSS [training: 0.27851871990444393 | validation: 0.3013308515283423]
	TIME [epoch: 10.6 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28553051051819234		[learning rate: 0.00029215]
	Learning Rate: 0.000292146
	LOSS [training: 0.28553051051819234 | validation: 0.30326651042208075]
	TIME [epoch: 10.6 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2891852746078075		[learning rate: 0.00029125]
	Learning Rate: 0.00029125
	LOSS [training: 0.2891852746078075 | validation: 0.3172500624198756]
	TIME [epoch: 10.6 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28509203500848396		[learning rate: 0.00029036]
	Learning Rate: 0.000290358
	LOSS [training: 0.28509203500848396 | validation: 0.2946923987898197]
	TIME [epoch: 10.6 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2956127917631258		[learning rate: 0.00028947]
	Learning Rate: 0.000289468
	LOSS [training: 0.2956127917631258 | validation: 0.2996631565175701]
	TIME [epoch: 10.6 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28955518221183785		[learning rate: 0.00028858]
	Learning Rate: 0.00028858
	LOSS [training: 0.28955518221183785 | validation: 0.28995394047129713]
	TIME [epoch: 10.6 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2856596056213624		[learning rate: 0.0002877]
	Learning Rate: 0.000287696
	LOSS [training: 0.2856596056213624 | validation: 0.28735692670472984]
	TIME [epoch: 10.6 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28733294651421865		[learning rate: 0.00028681]
	Learning Rate: 0.000286814
	LOSS [training: 0.28733294651421865 | validation: 0.2802914516537641]
	TIME [epoch: 10.6 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2838904905173467		[learning rate: 0.00028593]
	Learning Rate: 0.000285935
	LOSS [training: 0.2838904905173467 | validation: 0.28228305714969304]
	TIME [epoch: 10.6 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28429280771150955		[learning rate: 0.00028506]
	Learning Rate: 0.000285058
	LOSS [training: 0.28429280771150955 | validation: 0.29778628904156024]
	TIME [epoch: 10.6 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27515610013405145		[learning rate: 0.00028418]
	Learning Rate: 0.000284184
	LOSS [training: 0.27515610013405145 | validation: 0.2989236047448683]
	TIME [epoch: 10.6 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2857358923389161		[learning rate: 0.00028331]
	Learning Rate: 0.000283313
	LOSS [training: 0.2857358923389161 | validation: 0.30007060409571634]
	TIME [epoch: 10.6 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28435737607789957		[learning rate: 0.00028244]
	Learning Rate: 0.000282445
	LOSS [training: 0.28435737607789957 | validation: 0.3128235768220247]
	TIME [epoch: 10.6 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27246541823578985		[learning rate: 0.00028158]
	Learning Rate: 0.000281579
	LOSS [training: 0.27246541823578985 | validation: 0.29555444292289246]
	TIME [epoch: 10.6 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2740781689685897		[learning rate: 0.00028072]
	Learning Rate: 0.000280716
	LOSS [training: 0.2740781689685897 | validation: 0.29569218719405727]
	TIME [epoch: 10.6 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27481077584155483		[learning rate: 0.00027986]
	Learning Rate: 0.000279855
	LOSS [training: 0.27481077584155483 | validation: 0.29215159748444725]
	TIME [epoch: 10.6 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2750508907074061		[learning rate: 0.000279]
	Learning Rate: 0.000278997
	LOSS [training: 0.2750508907074061 | validation: 0.27845505241755]
	TIME [epoch: 10.6 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2774160795404878		[learning rate: 0.00027814]
	Learning Rate: 0.000278142
	LOSS [training: 0.2774160795404878 | validation: 0.27686986606862735]
	TIME [epoch: 10.6 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27562698505198235		[learning rate: 0.00027729]
	Learning Rate: 0.000277289
	LOSS [training: 0.27562698505198235 | validation: 0.28668726583066156]
	TIME [epoch: 10.6 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2777577785911848		[learning rate: 0.00027644]
	Learning Rate: 0.000276439
	LOSS [training: 0.2777577785911848 | validation: 0.29168094871124456]
	TIME [epoch: 10.6 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26522341103128133		[learning rate: 0.00027559]
	Learning Rate: 0.000275592
	LOSS [training: 0.26522341103128133 | validation: 0.2911613303360005]
	TIME [epoch: 10.6 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2704237928992223		[learning rate: 0.00027475]
	Learning Rate: 0.000274747
	LOSS [training: 0.2704237928992223 | validation: 0.29104916156654737]
	TIME [epoch: 10.6 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26618354247221265		[learning rate: 0.00027391]
	Learning Rate: 0.000273905
	LOSS [training: 0.26618354247221265 | validation: 0.28505448019963675]
	TIME [epoch: 10.6 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26482072987755095		[learning rate: 0.00027307]
	Learning Rate: 0.000273065
	LOSS [training: 0.26482072987755095 | validation: 0.26099326535594414]
	TIME [epoch: 10.6 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2607653080026152		[learning rate: 0.00027223]
	Learning Rate: 0.000272228
	LOSS [training: 0.2607653080026152 | validation: 0.2659250964372328]
	TIME [epoch: 10.6 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26158420045974223		[learning rate: 0.00027139]
	Learning Rate: 0.000271394
	LOSS [training: 0.26158420045974223 | validation: 0.263994178540081]
	TIME [epoch: 10.6 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27135311890508895		[learning rate: 0.00027056]
	Learning Rate: 0.000270562
	LOSS [training: 0.27135311890508895 | validation: 0.26490093706710127]
	TIME [epoch: 10.6 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2692410851751489		[learning rate: 0.00026973]
	Learning Rate: 0.000269733
	LOSS [training: 0.2692410851751489 | validation: 0.25640360361537184]
	TIME [epoch: 10.6 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2633764347652344		[learning rate: 0.00026891]
	Learning Rate: 0.000268906
	LOSS [training: 0.2633764347652344 | validation: 0.26337794973693224]
	TIME [epoch: 10.6 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26391705123261777		[learning rate: 0.00026808]
	Learning Rate: 0.000268081
	LOSS [training: 0.26391705123261777 | validation: 0.2716297692763763]
	TIME [epoch: 10.6 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2635302965214811		[learning rate: 0.00026726]
	Learning Rate: 0.00026726
	LOSS [training: 0.2635302965214811 | validation: 0.28028876263012154]
	TIME [epoch: 10.6 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2693592758160004		[learning rate: 0.00026644]
	Learning Rate: 0.00026644
	LOSS [training: 0.2693592758160004 | validation: 0.2841684806969256]
	TIME [epoch: 10.6 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2684007166085375		[learning rate: 0.00026562]
	Learning Rate: 0.000265624
	LOSS [training: 0.2684007166085375 | validation: 0.2727195742585946]
	TIME [epoch: 10.6 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2659343495686182		[learning rate: 0.00026481]
	Learning Rate: 0.000264809
	LOSS [training: 0.2659343495686182 | validation: 0.2763419892043623]
	TIME [epoch: 10.6 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27681465862042554		[learning rate: 0.000264]
	Learning Rate: 0.000263998
	LOSS [training: 0.27681465862042554 | validation: 0.28109480948951315]
	TIME [epoch: 10.6 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2649583446240117		[learning rate: 0.00026319]
	Learning Rate: 0.000263188
	LOSS [training: 0.2649583446240117 | validation: 0.26449944129034636]
	TIME [epoch: 10.6 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27321339589449384		[learning rate: 0.00026238]
	Learning Rate: 0.000262382
	LOSS [training: 0.27321339589449384 | validation: 0.28328868783716477]
	TIME [epoch: 10.6 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2721170837447706		[learning rate: 0.00026158]
	Learning Rate: 0.000261577
	LOSS [training: 0.2721170837447706 | validation: 0.265015156755685]
	TIME [epoch: 10.6 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2743296806532521		[learning rate: 0.00026078]
	Learning Rate: 0.000260775
	LOSS [training: 0.2743296806532521 | validation: 0.28078969699503004]
	TIME [epoch: 10.6 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2728651278359024		[learning rate: 0.00025998]
	Learning Rate: 0.000259976
	LOSS [training: 0.2728651278359024 | validation: 0.2680258153079433]
	TIME [epoch: 10.6 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2676993083044981		[learning rate: 0.00025918]
	Learning Rate: 0.000259179
	LOSS [training: 0.2676993083044981 | validation: 0.2967573155847137]
	TIME [epoch: 10.6 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2677426183194598		[learning rate: 0.00025838]
	Learning Rate: 0.000258385
	LOSS [training: 0.2677426183194598 | validation: 0.27850177614260496]
	TIME [epoch: 10.6 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27512428243187276		[learning rate: 0.00025759]
	Learning Rate: 0.000257593
	LOSS [training: 0.27512428243187276 | validation: 0.26790403118174005]
	TIME [epoch: 10.6 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26670727913858383		[learning rate: 0.0002568]
	Learning Rate: 0.000256803
	LOSS [training: 0.26670727913858383 | validation: 0.2793973144595898]
	TIME [epoch: 10.6 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26151597430222767		[learning rate: 0.00025602]
	Learning Rate: 0.000256016
	LOSS [training: 0.26151597430222767 | validation: 0.2618693584290952]
	TIME [epoch: 10.6 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26493242633113573		[learning rate: 0.00025523]
	Learning Rate: 0.000255231
	LOSS [training: 0.26493242633113573 | validation: 0.2601175912032999]
	TIME [epoch: 10.5 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26270754725028106		[learning rate: 0.00025445]
	Learning Rate: 0.000254449
	LOSS [training: 0.26270754725028106 | validation: 0.2578255600010689]
	TIME [epoch: 10.6 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2736927366531606		[learning rate: 0.00025367]
	Learning Rate: 0.000253669
	LOSS [training: 0.2736927366531606 | validation: 0.27922842583019164]
	TIME [epoch: 10.6 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26884465496770155		[learning rate: 0.00025289]
	Learning Rate: 0.000252891
	LOSS [training: 0.26884465496770155 | validation: 0.28462613987368474]
	TIME [epoch: 10.6 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28442271532852825		[learning rate: 0.00025212]
	Learning Rate: 0.000252116
	LOSS [training: 0.28442271532852825 | validation: 0.29059339030420267]
	TIME [epoch: 10.6 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27673806404842327		[learning rate: 0.00025134]
	Learning Rate: 0.000251343
	LOSS [training: 0.27673806404842327 | validation: 0.27448163068778814]
	TIME [epoch: 10.6 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2761247436870123		[learning rate: 0.00025057]
	Learning Rate: 0.000250572
	LOSS [training: 0.2761247436870123 | validation: 0.2690492024845635]
	TIME [epoch: 10.6 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26353353854705847		[learning rate: 0.0002498]
	Learning Rate: 0.000249804
	LOSS [training: 0.26353353854705847 | validation: 0.27672062878138765]
	TIME [epoch: 10.6 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27002777312204584		[learning rate: 0.00024904]
	Learning Rate: 0.000249039
	LOSS [training: 0.27002777312204584 | validation: 0.26927009492392173]
	TIME [epoch: 10.6 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27038700731335136		[learning rate: 0.00024828]
	Learning Rate: 0.000248275
	LOSS [training: 0.27038700731335136 | validation: 0.2629192197792352]
	TIME [epoch: 10.6 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2825379958667171		[learning rate: 0.00024751]
	Learning Rate: 0.000247514
	LOSS [training: 0.2825379958667171 | validation: 0.27105868270486083]
	TIME [epoch: 10.6 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27680372314886825		[learning rate: 0.00024676]
	Learning Rate: 0.000246755
	LOSS [training: 0.27680372314886825 | validation: 0.26514238071381885]
	TIME [epoch: 10.5 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2736420031801887		[learning rate: 0.000246]
	Learning Rate: 0.000245999
	LOSS [training: 0.2736420031801887 | validation: 0.26075958310464353]
	TIME [epoch: 10.5 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2649250101076709		[learning rate: 0.00024524]
	Learning Rate: 0.000245245
	LOSS [training: 0.2649250101076709 | validation: 0.2678080539342962]
	TIME [epoch: 10.6 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25567869355810335		[learning rate: 0.00024449]
	Learning Rate: 0.000244493
	LOSS [training: 0.25567869355810335 | validation: 0.2829133772230685]
	TIME [epoch: 10.6 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2649119198786364		[learning rate: 0.00024374]
	Learning Rate: 0.000243744
	LOSS [training: 0.2649119198786364 | validation: 0.2688955711903616]
	TIME [epoch: 10.6 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25467434372527376		[learning rate: 0.000243]
	Learning Rate: 0.000242996
	LOSS [training: 0.25467434372527376 | validation: 0.2676467286482141]
	TIME [epoch: 10.6 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25878172706068847		[learning rate: 0.00024225]
	Learning Rate: 0.000242252
	LOSS [training: 0.25878172706068847 | validation: 0.27466075307191185]
	TIME [epoch: 10.6 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26759550165393564		[learning rate: 0.00024151]
	Learning Rate: 0.000241509
	LOSS [training: 0.26759550165393564 | validation: 0.2697707845462753]
	TIME [epoch: 10.6 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27401654278592014		[learning rate: 0.00024077]
	Learning Rate: 0.000240769
	LOSS [training: 0.27401654278592014 | validation: 0.2804649619851308]
	TIME [epoch: 10.6 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2676004752950167		[learning rate: 0.00024003]
	Learning Rate: 0.000240031
	LOSS [training: 0.2676004752950167 | validation: 0.28039955689848634]
	TIME [epoch: 10.6 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2612942762401761		[learning rate: 0.00023929]
	Learning Rate: 0.000239295
	LOSS [training: 0.2612942762401761 | validation: 0.2647137047558965]
	TIME [epoch: 10.6 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26561069348941435		[learning rate: 0.00023856]
	Learning Rate: 0.000238561
	LOSS [training: 0.26561069348941435 | validation: 0.2654432172127504]
	TIME [epoch: 10.6 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2699868282838646		[learning rate: 0.00023783]
	Learning Rate: 0.00023783
	LOSS [training: 0.2699868282838646 | validation: 0.28493388996981317]
	TIME [epoch: 10.6 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28555695573928047		[learning rate: 0.0002371]
	Learning Rate: 0.000237101
	LOSS [training: 0.28555695573928047 | validation: 0.2774031675036714]
	TIME [epoch: 10.6 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27434174099530895		[learning rate: 0.00023637]
	Learning Rate: 0.000236374
	LOSS [training: 0.27434174099530895 | validation: 0.2689898242329156]
	TIME [epoch: 10.6 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2701617670485309		[learning rate: 0.00023565]
	Learning Rate: 0.00023565
	LOSS [training: 0.2701617670485309 | validation: 0.260699759657579]
	TIME [epoch: 10.6 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2771949874279106		[learning rate: 0.00023493]
	Learning Rate: 0.000234927
	LOSS [training: 0.2771949874279106 | validation: 0.2749719666651114]
	TIME [epoch: 10.6 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.273357898984778		[learning rate: 0.00023421]
	Learning Rate: 0.000234207
	LOSS [training: 0.273357898984778 | validation: 0.2654641642947465]
	TIME [epoch: 10.6 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27574363768002164		[learning rate: 0.00023349]
	Learning Rate: 0.000233489
	LOSS [training: 0.27574363768002164 | validation: 0.2841138370634772]
	TIME [epoch: 10.6 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27751942316308253		[learning rate: 0.00023277]
	Learning Rate: 0.000232773
	LOSS [training: 0.27751942316308253 | validation: 0.28498638694463874]
	TIME [epoch: 10.6 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27433623002628293		[learning rate: 0.00023206]
	Learning Rate: 0.00023206
	LOSS [training: 0.27433623002628293 | validation: 0.27192577316608096]
	TIME [epoch: 10.6 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2781213893262928		[learning rate: 0.00023135]
	Learning Rate: 0.000231348
	LOSS [training: 0.2781213893262928 | validation: 0.28332350118261757]
	TIME [epoch: 10.6 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27891158954591094		[learning rate: 0.00023064]
	Learning Rate: 0.000230639
	LOSS [training: 0.27891158954591094 | validation: 0.2732289455964613]
	TIME [epoch: 10.6 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2677005535182616		[learning rate: 0.00022993]
	Learning Rate: 0.000229932
	LOSS [training: 0.2677005535182616 | validation: 0.266022551090626]
	TIME [epoch: 10.6 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2694622913915086		[learning rate: 0.00022923]
	Learning Rate: 0.000229227
	LOSS [training: 0.2694622913915086 | validation: 0.26156137693689124]
	TIME [epoch: 10.6 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2641618010791589		[learning rate: 0.00022852]
	Learning Rate: 0.000228525
	LOSS [training: 0.2641618010791589 | validation: 0.26939391557481196]
	TIME [epoch: 10.6 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2646710053669234		[learning rate: 0.00022782]
	Learning Rate: 0.000227824
	LOSS [training: 0.2646710053669234 | validation: 0.25313272173976037]
	TIME [epoch: 10.6 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26116946831469867		[learning rate: 0.00022713]
	Learning Rate: 0.000227126
	LOSS [training: 0.26116946831469867 | validation: 0.26049792767863944]
	TIME [epoch: 10.6 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27199275593393474		[learning rate: 0.00022643]
	Learning Rate: 0.00022643
	LOSS [training: 0.27199275593393474 | validation: 0.2694025832992647]
	TIME [epoch: 10.6 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2732766904284289		[learning rate: 0.00022574]
	Learning Rate: 0.000225736
	LOSS [training: 0.2732766904284289 | validation: 0.2730348089208991]
	TIME [epoch: 10.6 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2687505173302565		[learning rate: 0.00022504]
	Learning Rate: 0.000225044
	LOSS [training: 0.2687505173302565 | validation: 0.25593368847399356]
	TIME [epoch: 10.6 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27121517689769115		[learning rate: 0.00022435]
	Learning Rate: 0.000224354
	LOSS [training: 0.27121517689769115 | validation: 0.24499269731702583]
	TIME [epoch: 10.5 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26437187921862937		[learning rate: 0.00022367]
	Learning Rate: 0.000223666
	LOSS [training: 0.26437187921862937 | validation: 0.2753975127371107]
	TIME [epoch: 10.6 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2641327522155032		[learning rate: 0.00022298]
	Learning Rate: 0.00022298
	LOSS [training: 0.2641327522155032 | validation: 0.26638494381271355]
	TIME [epoch: 10.6 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2520105902760002		[learning rate: 0.0002223]
	Learning Rate: 0.000222297
	LOSS [training: 0.2520105902760002 | validation: 0.26999068797019254]
	TIME [epoch: 10.6 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26079006237575203		[learning rate: 0.00022162]
	Learning Rate: 0.000221615
	LOSS [training: 0.26079006237575203 | validation: 0.26683962336178146]
	TIME [epoch: 10.6 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26708989407000444		[learning rate: 0.00022094]
	Learning Rate: 0.000220936
	LOSS [training: 0.26708989407000444 | validation: 0.26483264438973125]
	TIME [epoch: 10.6 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2658924550108806		[learning rate: 0.00022026]
	Learning Rate: 0.000220259
	LOSS [training: 0.2658924550108806 | validation: 0.26098626186767093]
	TIME [epoch: 10.6 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2682032154196701		[learning rate: 0.00021958]
	Learning Rate: 0.000219584
	LOSS [training: 0.2682032154196701 | validation: 0.25974714317771896]
	TIME [epoch: 10.6 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2673612415687744		[learning rate: 0.00021891]
	Learning Rate: 0.000218911
	LOSS [training: 0.2673612415687744 | validation: 0.2774359598658642]
	TIME [epoch: 10.6 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2729118091132364		[learning rate: 0.00021824]
	Learning Rate: 0.000218239
	LOSS [training: 0.2729118091132364 | validation: 0.2648575952039069]
	TIME [epoch: 10.6 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2734200950155047		[learning rate: 0.00021757]
	Learning Rate: 0.00021757
	LOSS [training: 0.2734200950155047 | validation: 0.26433738395004686]
	TIME [epoch: 10.6 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26979098479118807		[learning rate: 0.0002169]
	Learning Rate: 0.000216904
	LOSS [training: 0.26979098479118807 | validation: 0.26653335498701564]
	TIME [epoch: 10.6 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2770336317431274		[learning rate: 0.00021624]
	Learning Rate: 0.000216239
	LOSS [training: 0.2770336317431274 | validation: 0.28142425617918665]
	TIME [epoch: 10.6 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2841463452250042		[learning rate: 0.00021558]
	Learning Rate: 0.000215576
	LOSS [training: 0.2841463452250042 | validation: 0.2681652578064296]
	TIME [epoch: 10.6 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27222043159632414		[learning rate: 0.00021491]
	Learning Rate: 0.000214915
	LOSS [training: 0.27222043159632414 | validation: 0.28054347163298005]
	TIME [epoch: 10.5 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27326861277277714		[learning rate: 0.00021426]
	Learning Rate: 0.000214256
	LOSS [training: 0.27326861277277714 | validation: 0.26027133609942577]
	TIME [epoch: 10.6 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27300389996429875		[learning rate: 0.0002136]
	Learning Rate: 0.000213599
	LOSS [training: 0.27300389996429875 | validation: 0.2633038215941347]
	TIME [epoch: 10.6 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2798485997855986		[learning rate: 0.00021294]
	Learning Rate: 0.000212945
	LOSS [training: 0.2798485997855986 | validation: 0.27247912251970674]
	TIME [epoch: 10.6 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2682556363541601		[learning rate: 0.00021229]
	Learning Rate: 0.000212292
	LOSS [training: 0.2682556363541601 | validation: 0.27666999828989214]
	TIME [epoch: 10.6 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27541828774309596		[learning rate: 0.00021164]
	Learning Rate: 0.000211641
	LOSS [training: 0.27541828774309596 | validation: 0.277983078193998]
	TIME [epoch: 10.6 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2683781326198852		[learning rate: 0.00021099]
	Learning Rate: 0.000210992
	LOSS [training: 0.2683781326198852 | validation: 0.2801395723113707]
	TIME [epoch: 10.6 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28684624892305755		[learning rate: 0.00021035]
	Learning Rate: 0.000210346
	LOSS [training: 0.28684624892305755 | validation: 0.2880994381310525]
	TIME [epoch: 10.6 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27820682733120583		[learning rate: 0.0002097]
	Learning Rate: 0.000209701
	LOSS [training: 0.27820682733120583 | validation: 0.2870093042359915]
	TIME [epoch: 10.6 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27629642802666854		[learning rate: 0.00020906]
	Learning Rate: 0.000209058
	LOSS [training: 0.27629642802666854 | validation: 0.2681119685501823]
	TIME [epoch: 10.6 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26084418639222495		[learning rate: 0.00020842]
	Learning Rate: 0.000208417
	LOSS [training: 0.26084418639222495 | validation: 0.25936892653845683]
	TIME [epoch: 10.6 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2718044531770194		[learning rate: 0.00020778]
	Learning Rate: 0.000207778
	LOSS [training: 0.2718044531770194 | validation: 0.2808956739837995]
	TIME [epoch: 10.6 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2698936641773556		[learning rate: 0.00020714]
	Learning Rate: 0.000207141
	LOSS [training: 0.2698936641773556 | validation: 0.27412500430742276]
	TIME [epoch: 10.6 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27815973700079094		[learning rate: 0.00020651]
	Learning Rate: 0.000206506
	LOSS [training: 0.27815973700079094 | validation: 0.25817667729664373]
	TIME [epoch: 10.6 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2692742922713899		[learning rate: 0.00020587]
	Learning Rate: 0.000205873
	LOSS [training: 0.2692742922713899 | validation: 0.27174294447516256]
	TIME [epoch: 10.6 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2682822513442985		[learning rate: 0.00020524]
	Learning Rate: 0.000205242
	LOSS [training: 0.2682822513442985 | validation: 0.2937875305945108]
	TIME [epoch: 10.6 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2686253310881749		[learning rate: 0.00020461]
	Learning Rate: 0.000204613
	LOSS [training: 0.2686253310881749 | validation: 0.26263780281677823]
	TIME [epoch: 10.5 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2644745285383364		[learning rate: 0.00020399]
	Learning Rate: 0.000203986
	LOSS [training: 0.2644745285383364 | validation: 0.2715148709339601]
	TIME [epoch: 10.6 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27098584687947425		[learning rate: 0.00020336]
	Learning Rate: 0.00020336
	LOSS [training: 0.27098584687947425 | validation: 0.26518197740621063]
	TIME [epoch: 10.6 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26831223924558895		[learning rate: 0.00020274]
	Learning Rate: 0.000202737
	LOSS [training: 0.26831223924558895 | validation: 0.2678414124365017]
	TIME [epoch: 10.6 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2604366199862		[learning rate: 0.00020212]
	Learning Rate: 0.000202116
	LOSS [training: 0.2604366199862 | validation: 0.27539776710242114]
	TIME [epoch: 10.6 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26131353775543537		[learning rate: 0.0002015]
	Learning Rate: 0.000201496
	LOSS [training: 0.26131353775543537 | validation: 0.2664180982598555]
	TIME [epoch: 10.6 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26406552020222696		[learning rate: 0.00020088]
	Learning Rate: 0.000200878
	LOSS [training: 0.26406552020222696 | validation: 0.25115930454377977]
	TIME [epoch: 10.5 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2675208415566571		[learning rate: 0.00020026]
	Learning Rate: 0.000200263
	LOSS [training: 0.2675208415566571 | validation: 0.2517433392394961]
	TIME [epoch: 10.6 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2684617127160037		[learning rate: 0.00019965]
	Learning Rate: 0.000199649
	LOSS [training: 0.2684617127160037 | validation: 0.27122294264091984]
	TIME [epoch: 10.6 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27244150940123313		[learning rate: 0.00019904]
	Learning Rate: 0.000199037
	LOSS [training: 0.27244150940123313 | validation: 0.2513111221726084]
	TIME [epoch: 10.6 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2703337082157525		[learning rate: 0.00019843]
	Learning Rate: 0.000198427
	LOSS [training: 0.2703337082157525 | validation: 0.2711175557304821]
	TIME [epoch: 10.6 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26608513959728775		[learning rate: 0.00019782]
	Learning Rate: 0.000197818
	LOSS [training: 0.26608513959728775 | validation: 0.26213622991175295]
	TIME [epoch: 10.6 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25991446381229244		[learning rate: 0.00019721]
	Learning Rate: 0.000197212
	LOSS [training: 0.25991446381229244 | validation: 0.2520464268775628]
	TIME [epoch: 10.6 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2551427048435061		[learning rate: 0.00019661]
	Learning Rate: 0.000196607
	LOSS [training: 0.2551427048435061 | validation: 0.2520947287278776]
	TIME [epoch: 10.6 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25860378824527974		[learning rate: 0.000196]
	Learning Rate: 0.000196005
	LOSS [training: 0.25860378824527974 | validation: 0.2614341188082385]
	TIME [epoch: 10.5 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25282949624900686		[learning rate: 0.0001954]
	Learning Rate: 0.000195404
	LOSS [training: 0.25282949624900686 | validation: 0.2672705172406204]
	TIME [epoch: 10.6 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25887476258061615		[learning rate: 0.0001948]
	Learning Rate: 0.000194805
	LOSS [training: 0.25887476258061615 | validation: 0.23792393270527368]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_1783.pth
	Model improved!!!
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25770839022670355		[learning rate: 0.00019421]
	Learning Rate: 0.000194208
	LOSS [training: 0.25770839022670355 | validation: 0.25927441265419576]
	TIME [epoch: 10.6 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.262368985773071		[learning rate: 0.00019361]
	Learning Rate: 0.000193612
	LOSS [training: 0.262368985773071 | validation: 0.2657940832673626]
	TIME [epoch: 10.6 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2642029708673615		[learning rate: 0.00019302]
	Learning Rate: 0.000193019
	LOSS [training: 0.2642029708673615 | validation: 0.27696474089109946]
	TIME [epoch: 10.6 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27223189858227365		[learning rate: 0.00019243]
	Learning Rate: 0.000192427
	LOSS [training: 0.27223189858227365 | validation: 0.26316034036372626]
	TIME [epoch: 10.6 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.272053174597737		[learning rate: 0.00019184]
	Learning Rate: 0.000191837
	LOSS [training: 0.272053174597737 | validation: 0.26670011857907333]
	TIME [epoch: 10.6 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2717887568913901		[learning rate: 0.00019125]
	Learning Rate: 0.000191249
	LOSS [training: 0.2717887568913901 | validation: 0.26590963511868454]
	TIME [epoch: 10.6 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2717044047898042		[learning rate: 0.00019066]
	Learning Rate: 0.000190663
	LOSS [training: 0.2717044047898042 | validation: 0.2692724401157346]
	TIME [epoch: 10.6 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26862169136704306		[learning rate: 0.00019008]
	Learning Rate: 0.000190079
	LOSS [training: 0.26862169136704306 | validation: 0.28776988063670433]
	TIME [epoch: 10.6 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27338214922347204		[learning rate: 0.0001895]
	Learning Rate: 0.000189496
	LOSS [training: 0.27338214922347204 | validation: 0.27090789822447947]
	TIME [epoch: 10.6 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2707554098377763		[learning rate: 0.00018892]
	Learning Rate: 0.000188915
	LOSS [training: 0.2707554098377763 | validation: 0.27245027055701054]
	TIME [epoch: 10.6 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25925970127739373		[learning rate: 0.00018834]
	Learning Rate: 0.000188336
	LOSS [training: 0.25925970127739373 | validation: 0.25513247123493]
	TIME [epoch: 10.5 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24927555061551326		[learning rate: 0.00018776]
	Learning Rate: 0.000187759
	LOSS [training: 0.24927555061551326 | validation: 0.2580889797401689]
	TIME [epoch: 10.6 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26285882910608266		[learning rate: 0.00018718]
	Learning Rate: 0.000187183
	LOSS [training: 0.26285882910608266 | validation: 0.2599291823317976]
	TIME [epoch: 10.6 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25456982022676033		[learning rate: 0.00018661]
	Learning Rate: 0.000186609
	LOSS [training: 0.25456982022676033 | validation: 0.24990839924207917]
	TIME [epoch: 10.6 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25711535181279477		[learning rate: 0.00018604]
	Learning Rate: 0.000186037
	LOSS [training: 0.25711535181279477 | validation: 0.24913877340899773]
	TIME [epoch: 10.5 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25968478365552183		[learning rate: 0.00018547]
	Learning Rate: 0.000185467
	LOSS [training: 0.25968478365552183 | validation: 0.25565759842368874]
	TIME [epoch: 10.6 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2553646941126089		[learning rate: 0.0001849]
	Learning Rate: 0.000184898
	LOSS [training: 0.2553646941126089 | validation: 0.2634650252965251]
	TIME [epoch: 10.6 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26317133501108547		[learning rate: 0.00018433]
	Learning Rate: 0.000184332
	LOSS [training: 0.26317133501108547 | validation: 0.2597931905646512]
	TIME [epoch: 10.5 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25881293203628875		[learning rate: 0.00018377]
	Learning Rate: 0.000183767
	LOSS [training: 0.25881293203628875 | validation: 0.27206900390821737]
	TIME [epoch: 10.5 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2576596805734777		[learning rate: 0.0001832]
	Learning Rate: 0.000183203
	LOSS [training: 0.2576596805734777 | validation: 0.2654749967635262]
	TIME [epoch: 10.6 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2551384461369198		[learning rate: 0.00018264]
	Learning Rate: 0.000182642
	LOSS [training: 0.2551384461369198 | validation: 0.25129321362429374]
	TIME [epoch: 10.6 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25234234041118303		[learning rate: 0.00018208]
	Learning Rate: 0.000182082
	LOSS [training: 0.25234234041118303 | validation: 0.26631737388368154]
	TIME [epoch: 10.6 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25328510764717765		[learning rate: 0.00018152]
	Learning Rate: 0.000181524
	LOSS [training: 0.25328510764717765 | validation: 0.26441722051559674]
	TIME [epoch: 10.6 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2559328119738248		[learning rate: 0.00018097]
	Learning Rate: 0.000180967
	LOSS [training: 0.2559328119738248 | validation: 0.2721292076262832]
	TIME [epoch: 10.5 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25549196007122454		[learning rate: 0.00018041]
	Learning Rate: 0.000180412
	LOSS [training: 0.25549196007122454 | validation: 0.26377300285150174]
	TIME [epoch: 10.6 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2559879860221429		[learning rate: 0.00017986]
	Learning Rate: 0.000179859
	LOSS [training: 0.2559879860221429 | validation: 0.28252161747763693]
	TIME [epoch: 10.5 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26733892635137285		[learning rate: 0.00017931]
	Learning Rate: 0.000179308
	LOSS [training: 0.26733892635137285 | validation: 0.2714462467792514]
	TIME [epoch: 10.6 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26539401915557115		[learning rate: 0.00017876]
	Learning Rate: 0.000178758
	LOSS [training: 0.26539401915557115 | validation: 0.2715549535679485]
	TIME [epoch: 10.5 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25634436713570763		[learning rate: 0.00017821]
	Learning Rate: 0.00017821
	LOSS [training: 0.25634436713570763 | validation: 0.28229982399072895]
	TIME [epoch: 10.6 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.254174037784776		[learning rate: 0.00017766]
	Learning Rate: 0.000177664
	LOSS [training: 0.254174037784776 | validation: 0.2729123284043879]
	TIME [epoch: 10.5 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25491730879243135		[learning rate: 0.00017712]
	Learning Rate: 0.00017712
	LOSS [training: 0.25491730879243135 | validation: 0.2663684269953366]
	TIME [epoch: 10.6 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25115354411990254		[learning rate: 0.00017658]
	Learning Rate: 0.000176577
	LOSS [training: 0.25115354411990254 | validation: 0.26044829619681864]
	TIME [epoch: 10.6 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2562902949074736		[learning rate: 0.00017604]
	Learning Rate: 0.000176035
	LOSS [training: 0.2562902949074736 | validation: 0.2590689731555569]
	TIME [epoch: 10.5 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2521025975322015		[learning rate: 0.0001755]
	Learning Rate: 0.000175496
	LOSS [training: 0.2521025975322015 | validation: 0.2828566445139277]
	TIME [epoch: 10.6 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25029058452777625		[learning rate: 0.00017496]
	Learning Rate: 0.000174958
	LOSS [training: 0.25029058452777625 | validation: 0.2677887631943168]
	TIME [epoch: 10.5 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25736037966432407		[learning rate: 0.00017442]
	Learning Rate: 0.000174421
	LOSS [training: 0.25736037966432407 | validation: 0.26691837934772233]
	TIME [epoch: 10.5 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24674149235880946		[learning rate: 0.00017389]
	Learning Rate: 0.000173887
	LOSS [training: 0.24674149235880946 | validation: 0.24150642183222354]
	TIME [epoch: 10.5 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25239962111219677		[learning rate: 0.00017335]
	Learning Rate: 0.000173354
	LOSS [training: 0.25239962111219677 | validation: 0.2740519143604247]
	TIME [epoch: 10.6 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2571490136413119		[learning rate: 0.00017282]
	Learning Rate: 0.000172822
	LOSS [training: 0.2571490136413119 | validation: 0.2522577874820511]
	TIME [epoch: 10.6 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25434823354694314		[learning rate: 0.00017229]
	Learning Rate: 0.000172293
	LOSS [training: 0.25434823354694314 | validation: 0.2504789775077245]
	TIME [epoch: 10.6 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25603154165714104		[learning rate: 0.00017176]
	Learning Rate: 0.000171764
	LOSS [training: 0.25603154165714104 | validation: 0.25895842186285906]
	TIME [epoch: 10.6 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2577736532694713		[learning rate: 0.00017124]
	Learning Rate: 0.000171238
	LOSS [training: 0.2577736532694713 | validation: 0.258650105437154]
	TIME [epoch: 10.6 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26224595208109636		[learning rate: 0.00017071]
	Learning Rate: 0.000170713
	LOSS [training: 0.26224595208109636 | validation: 0.2696170881317185]
	TIME [epoch: 10.5 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2564946751644558		[learning rate: 0.00017019]
	Learning Rate: 0.00017019
	LOSS [training: 0.2564946751644558 | validation: 0.2680893458556906]
	TIME [epoch: 10.6 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25998321083157905		[learning rate: 0.00016967]
	Learning Rate: 0.000169668
	LOSS [training: 0.25998321083157905 | validation: 0.26380036552110664]
	TIME [epoch: 10.6 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25753901620737096		[learning rate: 0.00016915]
	Learning Rate: 0.000169148
	LOSS [training: 0.25753901620737096 | validation: 0.25849833058551686]
	TIME [epoch: 10.6 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2518754340144942		[learning rate: 0.00016863]
	Learning Rate: 0.000168629
	LOSS [training: 0.2518754340144942 | validation: 0.2566715356612738]
	TIME [epoch: 10.6 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2558150816279486		[learning rate: 0.00016811]
	Learning Rate: 0.000168112
	LOSS [training: 0.2558150816279486 | validation: 0.26251075233335314]
	TIME [epoch: 10.5 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25986515268988364		[learning rate: 0.0001676]
	Learning Rate: 0.000167597
	LOSS [training: 0.25986515268988364 | validation: 0.2700310528042634]
	TIME [epoch: 10.6 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2551760985564238		[learning rate: 0.00016708]
	Learning Rate: 0.000167083
	LOSS [training: 0.2551760985564238 | validation: 0.2616534579605718]
	TIME [epoch: 10.6 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2578040052477476		[learning rate: 0.00016657]
	Learning Rate: 0.000166571
	LOSS [training: 0.2578040052477476 | validation: 0.2655742576659653]
	TIME [epoch: 10.6 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25834687466749406		[learning rate: 0.00016606]
	Learning Rate: 0.000166061
	LOSS [training: 0.25834687466749406 | validation: 0.26350652369023614]
	TIME [epoch: 10.6 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2541508081264225		[learning rate: 0.00016555]
	Learning Rate: 0.000165552
	LOSS [training: 0.2541508081264225 | validation: 0.26095840903098555]
	TIME [epoch: 10.6 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24951934894890715		[learning rate: 0.00016504]
	Learning Rate: 0.000165044
	LOSS [training: 0.24951934894890715 | validation: 0.24553023637679477]
	TIME [epoch: 10.6 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2519465164688913		[learning rate: 0.00016454]
	Learning Rate: 0.000164538
	LOSS [training: 0.2519465164688913 | validation: 0.2500398792646511]
	TIME [epoch: 10.6 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24457345940868694		[learning rate: 0.00016403]
	Learning Rate: 0.000164034
	LOSS [training: 0.24457345940868694 | validation: 0.250374031649623]
	TIME [epoch: 10.6 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25927229259392043		[learning rate: 0.00016353]
	Learning Rate: 0.000163531
	LOSS [training: 0.25927229259392043 | validation: 0.2625151598941925]
	TIME [epoch: 10.5 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2493379532796788		[learning rate: 0.00016303]
	Learning Rate: 0.00016303
	LOSS [training: 0.2493379532796788 | validation: 0.24952584876773215]
	TIME [epoch: 10.5 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25238836683742427		[learning rate: 0.00016253]
	Learning Rate: 0.00016253
	LOSS [training: 0.25238836683742427 | validation: 0.2465360694454334]
	TIME [epoch: 10.6 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24620907659698346		[learning rate: 0.00016203]
	Learning Rate: 0.000162032
	LOSS [training: 0.24620907659698346 | validation: 0.266895778200901]
	TIME [epoch: 10.5 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25133812118212145		[learning rate: 0.00016153]
	Learning Rate: 0.000161535
	LOSS [training: 0.25133812118212145 | validation: 0.2593866439808319]
	TIME [epoch: 10.6 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2460543302529187		[learning rate: 0.00016104]
	Learning Rate: 0.00016104
	LOSS [training: 0.2460543302529187 | validation: 0.25638085730239707]
	TIME [epoch: 10.6 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24696138748977076		[learning rate: 0.00016055]
	Learning Rate: 0.000160546
	LOSS [training: 0.24696138748977076 | validation: 0.2657042997986471]
	TIME [epoch: 10.6 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2477737433199163		[learning rate: 0.00016005]
	Learning Rate: 0.000160054
	LOSS [training: 0.2477737433199163 | validation: 0.25925385638517]
	TIME [epoch: 10.6 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25193824717111457		[learning rate: 0.00015956]
	Learning Rate: 0.000159563
	LOSS [training: 0.25193824717111457 | validation: 0.25364169466240755]
	TIME [epoch: 10.5 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25720762882391257		[learning rate: 0.00015907]
	Learning Rate: 0.000159074
	LOSS [training: 0.25720762882391257 | validation: 0.27833033669020624]
	TIME [epoch: 10.5 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2584928201589264		[learning rate: 0.00015859]
	Learning Rate: 0.000158587
	LOSS [training: 0.2584928201589264 | validation: 0.2613511056436939]
	TIME [epoch: 10.5 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25472128596067134		[learning rate: 0.0001581]
	Learning Rate: 0.000158101
	LOSS [training: 0.25472128596067134 | validation: 0.24703995939982218]
	TIME [epoch: 10.6 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24791656691107988		[learning rate: 0.00015762]
	Learning Rate: 0.000157616
	LOSS [training: 0.24791656691107988 | validation: 0.26138115533500383]
	TIME [epoch: 10.5 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2553580290532123		[learning rate: 0.00015713]
	Learning Rate: 0.000157133
	LOSS [training: 0.2553580290532123 | validation: 0.2548733831003824]
	TIME [epoch: 10.5 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24769811507448689		[learning rate: 0.00015665]
	Learning Rate: 0.000156651
	LOSS [training: 0.24769811507448689 | validation: 0.2679245503359772]
	TIME [epoch: 10.5 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2556101046438663		[learning rate: 0.00015617]
	Learning Rate: 0.000156171
	LOSS [training: 0.2556101046438663 | validation: 0.2564561854855023]
	TIME [epoch: 10.6 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2530902448012253		[learning rate: 0.00015569]
	Learning Rate: 0.000155692
	LOSS [training: 0.2530902448012253 | validation: 0.24551208891030868]
	TIME [epoch: 10.6 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2614171986615743		[learning rate: 0.00015521]
	Learning Rate: 0.000155215
	LOSS [training: 0.2614171986615743 | validation: 0.25848684080660334]
	TIME [epoch: 10.6 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2527510909375761		[learning rate: 0.00015474]
	Learning Rate: 0.000154739
	LOSS [training: 0.2527510909375761 | validation: 0.2628464478458078]
	TIME [epoch: 10.6 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2506146912355737		[learning rate: 0.00015426]
	Learning Rate: 0.000154265
	LOSS [training: 0.2506146912355737 | validation: 0.26032925265217033]
	TIME [epoch: 10.6 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25628818658958996		[learning rate: 0.00015379]
	Learning Rate: 0.000153792
	LOSS [training: 0.25628818658958996 | validation: 0.2629791128601237]
	TIME [epoch: 10.6 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2524234604514265		[learning rate: 0.00015332]
	Learning Rate: 0.00015332
	LOSS [training: 0.2524234604514265 | validation: 0.2579273347424721]
	TIME [epoch: 10.6 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25145058593990205		[learning rate: 0.00015285]
	Learning Rate: 0.00015285
	LOSS [training: 0.25145058593990205 | validation: 0.25436423958979687]
	TIME [epoch: 10.6 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2510517744618096		[learning rate: 0.00015238]
	Learning Rate: 0.000152382
	LOSS [training: 0.2510517744618096 | validation: 0.2668489693112022]
	TIME [epoch: 10.5 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2524909423218319		[learning rate: 0.00015191]
	Learning Rate: 0.000151915
	LOSS [training: 0.2524909423218319 | validation: 0.24576834659840036]
	TIME [epoch: 10.6 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24848916451762587		[learning rate: 0.00015145]
	Learning Rate: 0.000151449
	LOSS [training: 0.24848916451762587 | validation: 0.2670286696707928]
	TIME [epoch: 10.5 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24302044846672888		[learning rate: 0.00015098]
	Learning Rate: 0.000150985
	LOSS [training: 0.24302044846672888 | validation: 0.2476967630524525]
	TIME [epoch: 10.6 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25347592592546975		[learning rate: 0.00015052]
	Learning Rate: 0.000150522
	LOSS [training: 0.25347592592546975 | validation: 0.23098516674630418]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_1867.pth
	Model improved!!!
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2492779252200405		[learning rate: 0.00015006]
	Learning Rate: 0.000150061
	LOSS [training: 0.2492779252200405 | validation: 0.2520765891170703]
	TIME [epoch: 10.6 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23812295563058616		[learning rate: 0.0001496]
	Learning Rate: 0.000149601
	LOSS [training: 0.23812295563058616 | validation: 0.25954232391872595]
	TIME [epoch: 10.6 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24967756033114746		[learning rate: 0.00014914]
	Learning Rate: 0.000149142
	LOSS [training: 0.24967756033114746 | validation: 0.2602787508859249]
	TIME [epoch: 10.6 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24407336124171392		[learning rate: 0.00014868]
	Learning Rate: 0.000148685
	LOSS [training: 0.24407336124171392 | validation: 0.25142850410066897]
	TIME [epoch: 10.5 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24684043638125197		[learning rate: 0.00014823]
	Learning Rate: 0.000148229
	LOSS [training: 0.24684043638125197 | validation: 0.2481226753554461]
	TIME [epoch: 10.6 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2551385953089878		[learning rate: 0.00014777]
	Learning Rate: 0.000147775
	LOSS [training: 0.2551385953089878 | validation: 0.25919256203721835]
	TIME [epoch: 10.6 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2443472896972362		[learning rate: 0.00014732]
	Learning Rate: 0.000147322
	LOSS [training: 0.2443472896972362 | validation: 0.27483044729304235]
	TIME [epoch: 10.5 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24707645854036953		[learning rate: 0.00014687]
	Learning Rate: 0.00014687
	LOSS [training: 0.24707645854036953 | validation: 0.24770018922819453]
	TIME [epoch: 10.6 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23686194561672314		[learning rate: 0.00014642]
	Learning Rate: 0.00014642
	LOSS [training: 0.23686194561672314 | validation: 0.250966743198136]
	TIME [epoch: 10.6 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2508617840172735		[learning rate: 0.00014597]
	Learning Rate: 0.000145971
	LOSS [training: 0.2508617840172735 | validation: 0.26551644553742937]
	TIME [epoch: 10.6 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2501339665318601		[learning rate: 0.00014552]
	Learning Rate: 0.000145524
	LOSS [training: 0.2501339665318601 | validation: 0.2634780439600232]
	TIME [epoch: 10.5 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2511192906632416		[learning rate: 0.00014508]
	Learning Rate: 0.000145077
	LOSS [training: 0.2511192906632416 | validation: 0.26422932306017444]
	TIME [epoch: 10.6 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2508482203299673		[learning rate: 0.00014463]
	Learning Rate: 0.000144633
	LOSS [training: 0.2508482203299673 | validation: 0.23670959654808738]
	TIME [epoch: 10.5 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24771015962372922		[learning rate: 0.00014419]
	Learning Rate: 0.000144189
	LOSS [training: 0.24771015962372922 | validation: 0.25742451990080856]
	TIME [epoch: 10.6 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2553592247412954		[learning rate: 0.00014375]
	Learning Rate: 0.000143747
	LOSS [training: 0.2553592247412954 | validation: 0.25544841490662495]
	TIME [epoch: 10.6 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2497557444089598		[learning rate: 0.00014331]
	Learning Rate: 0.000143307
	LOSS [training: 0.2497557444089598 | validation: 0.26160952844383445]
	TIME [epoch: 10.6 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2536892878696667		[learning rate: 0.00014287]
	Learning Rate: 0.000142867
	LOSS [training: 0.2536892878696667 | validation: 0.26402554541309514]
	TIME [epoch: 10.5 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25818436172996545		[learning rate: 0.00014243]
	Learning Rate: 0.00014243
	LOSS [training: 0.25818436172996545 | validation: 0.26836497329529013]
	TIME [epoch: 10.6 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.265394935328347		[learning rate: 0.00014199]
	Learning Rate: 0.000141993
	LOSS [training: 0.265394935328347 | validation: 0.2528167911619866]
	TIME [epoch: 10.6 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2613316027854823		[learning rate: 0.00014156]
	Learning Rate: 0.000141558
	LOSS [training: 0.2613316027854823 | validation: 0.27076294110497007]
	TIME [epoch: 10.6 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2667737449236065		[learning rate: 0.00014112]
	Learning Rate: 0.000141124
	LOSS [training: 0.2667737449236065 | validation: 0.28239229808276395]
	TIME [epoch: 10.5 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.266339399677329		[learning rate: 0.00014069]
	Learning Rate: 0.000140691
	LOSS [training: 0.266339399677329 | validation: 0.26194241925164574]
	TIME [epoch: 10.6 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2544039705590338		[learning rate: 0.00014026]
	Learning Rate: 0.00014026
	LOSS [training: 0.2544039705590338 | validation: 0.2554962255352426]
	TIME [epoch: 10.6 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2604846755304984		[learning rate: 0.00013983]
	Learning Rate: 0.00013983
	LOSS [training: 0.2604846755304984 | validation: 0.26472029446240963]
	TIME [epoch: 10.6 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24596486721929808		[learning rate: 0.0001394]
	Learning Rate: 0.000139401
	LOSS [training: 0.24596486721929808 | validation: 0.2598917180503148]
	TIME [epoch: 10.5 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25769890158883946		[learning rate: 0.00013897]
	Learning Rate: 0.000138974
	LOSS [training: 0.25769890158883946 | validation: 0.2683116398452842]
	TIME [epoch: 10.5 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24933185022877677		[learning rate: 0.00013855]
	Learning Rate: 0.000138548
	LOSS [training: 0.24933185022877677 | validation: 0.24320219019490744]
	TIME [epoch: 10.6 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25347473784673136		[learning rate: 0.00013812]
	Learning Rate: 0.000138123
	LOSS [training: 0.25347473784673136 | validation: 0.2569823180968561]
	TIME [epoch: 10.6 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25323165598809594		[learning rate: 0.0001377]
	Learning Rate: 0.0001377
	LOSS [training: 0.25323165598809594 | validation: 0.2396086139378258]
	TIME [epoch: 10.6 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2534870015004743		[learning rate: 0.00013728]
	Learning Rate: 0.000137278
	LOSS [training: 0.2534870015004743 | validation: 0.27459251443698257]
	TIME [epoch: 10.6 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24662884088992915		[learning rate: 0.00013686]
	Learning Rate: 0.000136857
	LOSS [training: 0.24662884088992915 | validation: 0.26715943939823106]
	TIME [epoch: 10.6 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25102738951206843		[learning rate: 0.00013644]
	Learning Rate: 0.000136437
	LOSS [training: 0.25102738951206843 | validation: 0.24597086881093075]
	TIME [epoch: 10.5 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2517644192537646		[learning rate: 0.00013602]
	Learning Rate: 0.000136019
	LOSS [training: 0.2517644192537646 | validation: 0.2506098139475845]
	TIME [epoch: 10.6 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24661733437932		[learning rate: 0.0001356]
	Learning Rate: 0.000135602
	LOSS [training: 0.24661733437932 | validation: 0.25498457676019753]
	TIME [epoch: 10.6 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24874305726540719		[learning rate: 0.00013519]
	Learning Rate: 0.000135186
	LOSS [training: 0.24874305726540719 | validation: 0.24841078789528329]
	TIME [epoch: 10.6 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23895661862123654		[learning rate: 0.00013477]
	Learning Rate: 0.000134772
	LOSS [training: 0.23895661862123654 | validation: 0.2609260558483058]
	TIME [epoch: 10.6 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24756709553933542		[learning rate: 0.00013436]
	Learning Rate: 0.000134359
	LOSS [training: 0.24756709553933542 | validation: 0.2572377307558959]
	TIME [epoch: 10.5 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23850510314665882		[learning rate: 0.00013395]
	Learning Rate: 0.000133947
	LOSS [training: 0.23850510314665882 | validation: 0.25515452653405596]
	TIME [epoch: 10.5 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2472998782856637		[learning rate: 0.00013354]
	Learning Rate: 0.000133536
	LOSS [training: 0.2472998782856637 | validation: 0.2510269105270767]
	TIME [epoch: 10.6 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2462874718551144		[learning rate: 0.00013313]
	Learning Rate: 0.000133127
	LOSS [training: 0.2462874718551144 | validation: 0.24820679707828266]
	TIME [epoch: 10.6 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.242726788018813		[learning rate: 0.00013272]
	Learning Rate: 0.000132719
	LOSS [training: 0.242726788018813 | validation: 0.24261674982625941]
	TIME [epoch: 10.5 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24316712192398562		[learning rate: 0.00013231]
	Learning Rate: 0.000132312
	LOSS [training: 0.24316712192398562 | validation: 0.24579915502212207]
	TIME [epoch: 10.6 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2433880522806001		[learning rate: 0.00013191]
	Learning Rate: 0.000131907
	LOSS [training: 0.2433880522806001 | validation: 0.24083450520203628]
	TIME [epoch: 10.6 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2393823498852578		[learning rate: 0.0001315]
	Learning Rate: 0.000131502
	LOSS [training: 0.2393823498852578 | validation: 0.2556528288355521]
	TIME [epoch: 10.6 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24621259250036762		[learning rate: 0.0001311]
	Learning Rate: 0.000131099
	LOSS [training: 0.24621259250036762 | validation: 0.24903808290658164]
	TIME [epoch: 10.6 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24954681765542616		[learning rate: 0.0001307]
	Learning Rate: 0.000130697
	LOSS [training: 0.24954681765542616 | validation: 0.25128059001850483]
	TIME [epoch: 10.6 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24713399845143003		[learning rate: 0.0001303]
	Learning Rate: 0.000130297
	LOSS [training: 0.24713399845143003 | validation: 0.24646408047663884]
	TIME [epoch: 10.6 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2464665593720433		[learning rate: 0.0001299]
	Learning Rate: 0.000129897
	LOSS [training: 0.2464665593720433 | validation: 0.2535413542803609]
	TIME [epoch: 10.6 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24122335738449263		[learning rate: 0.0001295]
	Learning Rate: 0.000129499
	LOSS [training: 0.24122335738449263 | validation: 0.2578285248971292]
	TIME [epoch: 10.6 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24228843529661614		[learning rate: 0.0001291]
	Learning Rate: 0.000129102
	LOSS [training: 0.24228843529661614 | validation: 0.2415497280280716]
	TIME [epoch: 10.6 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2380574748009602		[learning rate: 0.00012871]
	Learning Rate: 0.000128706
	LOSS [training: 0.2380574748009602 | validation: 0.24174065280352167]
	TIME [epoch: 10.5 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23840964045870233		[learning rate: 0.00012831]
	Learning Rate: 0.000128312
	LOSS [training: 0.23840964045870233 | validation: 0.2493244842242239]
	TIME [epoch: 10.6 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24845371731306148		[learning rate: 0.00012792]
	Learning Rate: 0.000127918
	LOSS [training: 0.24845371731306148 | validation: 0.25495728539126616]
	TIME [epoch: 10.5 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23945465229667465		[learning rate: 0.00012753]
	Learning Rate: 0.000127526
	LOSS [training: 0.23945465229667465 | validation: 0.24602949346181305]
	TIME [epoch: 10.5 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24363778186875612		[learning rate: 0.00012714]
	Learning Rate: 0.000127135
	LOSS [training: 0.24363778186875612 | validation: 0.24820792122614982]
	TIME [epoch: 10.5 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24902470604010993		[learning rate: 0.00012675]
	Learning Rate: 0.000126746
	LOSS [training: 0.24902470604010993 | validation: 0.2514473589685668]
	TIME [epoch: 10.5 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24202081636904738		[learning rate: 0.00012636]
	Learning Rate: 0.000126357
	LOSS [training: 0.24202081636904738 | validation: 0.2513030312373091]
	TIME [epoch: 10.6 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24176342115956215		[learning rate: 0.00012597]
	Learning Rate: 0.00012597
	LOSS [training: 0.24176342115956215 | validation: 0.24533434662523035]
	TIME [epoch: 10.6 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24315974985914884		[learning rate: 0.00012558]
	Learning Rate: 0.000125584
	LOSS [training: 0.24315974985914884 | validation: 0.26512568405677006]
	TIME [epoch: 10.5 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2542453111010056		[learning rate: 0.0001252]
	Learning Rate: 0.000125199
	LOSS [training: 0.2542453111010056 | validation: 0.23664546917043405]
	TIME [epoch: 10.6 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24525440946015215		[learning rate: 0.00012481]
	Learning Rate: 0.000124815
	LOSS [training: 0.24525440946015215 | validation: 0.25494595290162575]
	TIME [epoch: 10.6 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24095797517925588		[learning rate: 0.00012443]
	Learning Rate: 0.000124432
	LOSS [training: 0.24095797517925588 | validation: 0.24954866457130542]
	TIME [epoch: 10.5 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24319867186429517		[learning rate: 0.00012405]
	Learning Rate: 0.000124051
	LOSS [training: 0.24319867186429517 | validation: 0.24328690361643118]
	TIME [epoch: 10.5 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24427899262742203		[learning rate: 0.00012367]
	Learning Rate: 0.000123671
	LOSS [training: 0.24427899262742203 | validation: 0.23776435641750523]
	TIME [epoch: 10.5 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24154220626407588		[learning rate: 0.00012329]
	Learning Rate: 0.000123292
	LOSS [training: 0.24154220626407588 | validation: 0.24903948400768738]
	TIME [epoch: 10.6 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23683640208736917		[learning rate: 0.00012291]
	Learning Rate: 0.000122914
	LOSS [training: 0.23683640208736917 | validation: 0.2488384991396802]
	TIME [epoch: 10.6 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24970396267130926		[learning rate: 0.00012254]
	Learning Rate: 0.000122537
	LOSS [training: 0.24970396267130926 | validation: 0.2415418842773835]
	TIME [epoch: 10.6 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24380736841496092		[learning rate: 0.00012216]
	Learning Rate: 0.000122161
	LOSS [training: 0.24380736841496092 | validation: 0.2489114048923297]
	TIME [epoch: 10.5 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24670695727195038		[learning rate: 0.00012179]
	Learning Rate: 0.000121787
	LOSS [training: 0.24670695727195038 | validation: 0.2561356288214184]
	TIME [epoch: 10.6 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25039475621118906		[learning rate: 0.00012141]
	Learning Rate: 0.000121413
	LOSS [training: 0.25039475621118906 | validation: 0.2634535066049002]
	TIME [epoch: 10.6 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24782302098029368		[learning rate: 0.00012104]
	Learning Rate: 0.000121041
	LOSS [training: 0.24782302098029368 | validation: 0.23587076208745125]
	TIME [epoch: 10.5 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24663804404093823		[learning rate: 0.00012067]
	Learning Rate: 0.00012067
	LOSS [training: 0.24663804404093823 | validation: 0.25356260014237836]
	TIME [epoch: 10.5 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23519524700522357		[learning rate: 0.0001203]
	Learning Rate: 0.0001203
	LOSS [training: 0.23519524700522357 | validation: 0.23031433339867008]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_1940.pth
	Model improved!!!
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23615651345898656		[learning rate: 0.00011993]
	Learning Rate: 0.000119932
	LOSS [training: 0.23615651345898656 | validation: 0.26132014729170006]
	TIME [epoch: 10.6 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23689512851743338		[learning rate: 0.00011956]
	Learning Rate: 0.000119564
	LOSS [training: 0.23689512851743338 | validation: 0.2344364585387288]
	TIME [epoch: 10.5 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24294894505477638		[learning rate: 0.0001192]
	Learning Rate: 0.000119197
	LOSS [training: 0.24294894505477638 | validation: 0.2361231173289206]
	TIME [epoch: 10.5 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24213313495737485		[learning rate: 0.00011883]
	Learning Rate: 0.000118832
	LOSS [training: 0.24213313495737485 | validation: 0.23001768430159672]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study6/model_tr_study6_r3_20240217_152728/states/model_tr_study6_1944.pth
	Model improved!!!
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2451490959512474		[learning rate: 0.00011847]
	Learning Rate: 0.000118468
	LOSS [training: 0.2451490959512474 | validation: 0.2626842849626927]
	TIME [epoch: 10.6 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24226897568643885		[learning rate: 0.0001181]
	Learning Rate: 0.000118105
	LOSS [training: 0.24226897568643885 | validation: 0.25391296475135133]
	TIME [epoch: 10.5 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2372804648220402		[learning rate: 0.00011774]
	Learning Rate: 0.000117743
	LOSS [training: 0.2372804648220402 | validation: 0.25065434914927487]
	TIME [epoch: 10.6 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24082123122560645		[learning rate: 0.00011738]
	Learning Rate: 0.000117382
	LOSS [training: 0.24082123122560645 | validation: 0.23080286150763726]
	TIME [epoch: 10.5 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2484832751644756		[learning rate: 0.00011702]
	Learning Rate: 0.000117022
	LOSS [training: 0.2484832751644756 | validation: 0.24583632447357176]
	TIME [epoch: 10.6 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2449744954802855		[learning rate: 0.00011666]
	Learning Rate: 0.000116663
	LOSS [training: 0.2449744954802855 | validation: 0.26504137310496734]
	TIME [epoch: 10.6 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25058325791909564		[learning rate: 0.00011631]
	Learning Rate: 0.000116305
	LOSS [training: 0.25058325791909564 | validation: 0.24822299295790465]
	TIME [epoch: 10.5 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23735921597002227		[learning rate: 0.00011595]
	Learning Rate: 0.000115949
	LOSS [training: 0.23735921597002227 | validation: 0.25357175716628916]
	TIME [epoch: 10.5 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2406154209836616		[learning rate: 0.00011559]
	Learning Rate: 0.000115593
	LOSS [training: 0.2406154209836616 | validation: 0.2465331750509333]
	TIME [epoch: 10.6 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24896055788396226		[learning rate: 0.00011524]
	Learning Rate: 0.000115239
	LOSS [training: 0.24896055788396226 | validation: 0.2501169319968459]
	TIME [epoch: 10.6 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24755367496657268		[learning rate: 0.00011489]
	Learning Rate: 0.000114886
	LOSS [training: 0.24755367496657268 | validation: 0.24994635106388052]
	TIME [epoch: 10.5 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24234965744359407		[learning rate: 0.00011453]
	Learning Rate: 0.000114534
	LOSS [training: 0.24234965744359407 | validation: 0.23667236480837195]
	TIME [epoch: 10.5 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24528597997358462		[learning rate: 0.00011418]
	Learning Rate: 0.000114183
	LOSS [training: 0.24528597997358462 | validation: 0.2435339085963973]
	TIME [epoch: 10.6 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24544616590757307		[learning rate: 0.00011383]
	Learning Rate: 0.000113833
	LOSS [training: 0.24544616590757307 | validation: 0.25659217866814643]
	TIME [epoch: 10.6 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24226328000061415		[learning rate: 0.00011348]
	Learning Rate: 0.000113484
	LOSS [training: 0.24226328000061415 | validation: 0.27477790122271534]
	TIME [epoch: 10.6 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2398671295234259		[learning rate: 0.00011314]
	Learning Rate: 0.000113136
	LOSS [training: 0.2398671295234259 | validation: 0.2533828461928504]
	TIME [epoch: 10.5 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2436895004990179		[learning rate: 0.00011279]
	Learning Rate: 0.000112789
	LOSS [training: 0.2436895004990179 | validation: 0.23781309364951006]
	TIME [epoch: 10.6 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24049002543304293		[learning rate: 0.00011244]
	Learning Rate: 0.000112443
	LOSS [training: 0.24049002543304293 | validation: 0.2664000453697504]
	TIME [epoch: 10.6 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24238165757067254		[learning rate: 0.0001121]
	Learning Rate: 0.000112099
	LOSS [training: 0.24238165757067254 | validation: 0.24158764293179275]
	TIME [epoch: 10.5 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2454431935795231		[learning rate: 0.00011175]
	Learning Rate: 0.000111755
	LOSS [training: 0.2454431935795231 | validation: 0.2507389594419529]
	TIME [epoch: 10.5 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24784978530819685		[learning rate: 0.00011141]
	Learning Rate: 0.000111412
	LOSS [training: 0.24784978530819685 | validation: 0.2522302974444223]
	TIME [epoch: 10.5 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24147151061482264		[learning rate: 0.00011107]
	Learning Rate: 0.000111071
	LOSS [training: 0.24147151061482264 | validation: 0.25594016290664046]
	TIME [epoch: 10.6 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25282383800543484		[learning rate: 0.00011073]
	Learning Rate: 0.00011073
	LOSS [training: 0.25282383800543484 | validation: 0.2505133209185636]
	TIME [epoch: 10.6 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24155798788933872		[learning rate: 0.00011039]
	Learning Rate: 0.000110391
	LOSS [training: 0.24155798788933872 | validation: 0.24639011021254958]
	TIME [epoch: 10.5 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24693515435126456		[learning rate: 0.00011005]
	Learning Rate: 0.000110053
	LOSS [training: 0.24693515435126456 | validation: 0.25961486760521746]
	TIME [epoch: 10.5 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24484308142804184		[learning rate: 0.00010972]
	Learning Rate: 0.000109715
	LOSS [training: 0.24484308142804184 | validation: 0.24904653781590202]
	TIME [epoch: 10.5 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2508782423214214		[learning rate: 0.00010938]
	Learning Rate: 0.000109379
	LOSS [training: 0.2508782423214214 | validation: 0.25068190465327844]
	TIME [epoch: 10.6 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2521688031267024		[learning rate: 0.00010904]
	Learning Rate: 0.000109044
	LOSS [training: 0.2521688031267024 | validation: 0.24727084139780964]
	TIME [epoch: 10.5 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24655009367056507		[learning rate: 0.00010871]
	Learning Rate: 0.000108709
	LOSS [training: 0.24655009367056507 | validation: 0.24826754336781698]
	TIME [epoch: 10.6 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24045401337090788		[learning rate: 0.00010838]
	Learning Rate: 0.000108376
	LOSS [training: 0.24045401337090788 | validation: 0.267477471668527]
	TIME [epoch: 10.6 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25507590452813245		[learning rate: 0.00010804]
	Learning Rate: 0.000108044
	LOSS [training: 0.25507590452813245 | validation: 0.2545887453815665]
	TIME [epoch: 10.6 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24423433752560003		[learning rate: 0.00010771]
	Learning Rate: 0.000107713
	LOSS [training: 0.24423433752560003 | validation: 0.2521116326812405]
	TIME [epoch: 10.5 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.246770838653191		[learning rate: 0.00010738]
	Learning Rate: 0.000107382
	LOSS [training: 0.246770838653191 | validation: 0.2521291333264021]
	TIME [epoch: 10.5 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24285568283839903		[learning rate: 0.00010705]
	Learning Rate: 0.000107053
	LOSS [training: 0.24285568283839903 | validation: 0.24869158993073634]
	TIME [epoch: 10.5 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2433997524672223		[learning rate: 0.00010673]
	Learning Rate: 0.000106725
	LOSS [training: 0.2433997524672223 | validation: 0.24655052560378385]
	TIME [epoch: 10.6 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24386614501698775		[learning rate: 0.0001064]
	Learning Rate: 0.000106398
	LOSS [training: 0.24386614501698775 | validation: 0.24754783897427493]
	TIME [epoch: 10.6 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23965438482285234		[learning rate: 0.00010607]
	Learning Rate: 0.000106072
	LOSS [training: 0.23965438482285234 | validation: 0.2582804346098493]
	TIME [epoch: 10.6 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24844375274739994		[learning rate: 0.00010575]
	Learning Rate: 0.000105747
	LOSS [training: 0.24844375274739994 | validation: 0.2450163361037224]
	TIME [epoch: 10.5 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24739910986885078		[learning rate: 0.00010542]
	Learning Rate: 0.000105423
	LOSS [training: 0.24739910986885078 | validation: 0.2680685507281189]
	TIME [epoch: 10.5 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2411874300420105		[learning rate: 0.0001051]
	Learning Rate: 0.000105099
	LOSS [training: 0.2411874300420105 | validation: 0.24032007266317326]
	TIME [epoch: 10.6 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23881580499606106		[learning rate: 0.00010478]
	Learning Rate: 0.000104777
	LOSS [training: 0.23881580499606106 | validation: 0.24862081710726933]
	TIME [epoch: 10.5 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23696916564300724		[learning rate: 0.00010446]
	Learning Rate: 0.000104456
	LOSS [training: 0.23696916564300724 | validation: 0.2391056722662775]
	TIME [epoch: 10.5 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25107047322184783		[learning rate: 0.00010414]
	Learning Rate: 0.000104136
	LOSS [training: 0.25107047322184783 | validation: 0.24058532505472663]
	TIME [epoch: 10.6 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24346538803463064		[learning rate: 0.00010382]
	Learning Rate: 0.000103817
	LOSS [training: 0.24346538803463064 | validation: 0.2458822397056675]
	TIME [epoch: 10.6 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2394103034478051		[learning rate: 0.0001035]
	Learning Rate: 0.000103498
	LOSS [training: 0.2394103034478051 | validation: 0.26072885683736646]
	TIME [epoch: 10.6 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.242706105385811		[learning rate: 0.00010318]
	Learning Rate: 0.000103181
	LOSS [training: 0.242706105385811 | validation: 0.23286961554569224]
	TIME [epoch: 10.5 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.242467265776449		[learning rate: 0.00010286]
	Learning Rate: 0.000102865
	LOSS [training: 0.242467265776449 | validation: 0.23515089224209015]
	TIME [epoch: 10.6 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24942098450050776		[learning rate: 0.00010255]
	Learning Rate: 0.000102549
	LOSS [training: 0.24942098450050776 | validation: 0.24592805204934273]
	TIME [epoch: 10.6 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24275087722589958		[learning rate: 0.00010224]
	Learning Rate: 0.000102235
	LOSS [training: 0.24275087722589958 | validation: 0.255598106985686]
	TIME [epoch: 10.5 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24669018610193452		[learning rate: 0.00010192]
	Learning Rate: 0.000101922
	LOSS [training: 0.24669018610193452 | validation: 0.23519568540922683]
	TIME [epoch: 10.6 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2504220167944248		[learning rate: 0.00010161]
	Learning Rate: 0.000101609
	LOSS [training: 0.2504220167944248 | validation: 0.23391355719396278]
	TIME [epoch: 10.5 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24266876335471926		[learning rate: 0.0001013]
	Learning Rate: 0.000101298
	LOSS [training: 0.24266876335471926 | validation: 0.23708260300128622]
	TIME [epoch: 10.6 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24771666459631952		[learning rate: 0.00010099]
	Learning Rate: 0.000100987
	LOSS [training: 0.24771666459631952 | validation: 0.24975055666794643]
	TIME [epoch: 10.6 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.251204155494935		[learning rate: 0.00010068]
	Learning Rate: 0.000100678
	LOSS [training: 0.251204155494935 | validation: 0.24271745540664177]
	TIME [epoch: 10.5 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24143912547451568		[learning rate: 0.00010037]
	Learning Rate: 0.000100369
	LOSS [training: 0.24143912547451568 | validation: 0.2543163628888561]
	TIME [epoch: 10.6 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24449269028899395		[learning rate: 0.00010006]
	Learning Rate: 0.000100061
	LOSS [training: 0.24449269028899395 | validation: 0.25454155604410156]
	TIME [epoch: 10.5 sec]
Finished training in 21249.479 seconds.
