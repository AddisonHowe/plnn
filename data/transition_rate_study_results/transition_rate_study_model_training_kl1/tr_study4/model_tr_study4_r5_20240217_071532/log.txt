Args:
Namespace(name='model_tr_study4', outdir='out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5', training_data='data/transition_rate_studies/tr_study4/tr_study4_training/r5', validation_data='data/transition_rate_studies/tr_study4/tr_study4_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 175677377

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.000147649545728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.000147649545728 | validation: 8.940778827531313]
	TIME [epoch: 48.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.340484766804889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.340484766804889 | validation: 5.914076281071354]
	TIME [epoch: 9.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.0853040645682785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0853040645682785 | validation: 6.6406011874558395]
	TIME [epoch: 9.11 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.921231630209105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.921231630209105 | validation: 4.0607397120287665]
	TIME [epoch: 9.09 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.476531793960996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.476531793960996 | validation: 4.185867083828336]
	TIME [epoch: 9.1 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.097560135803838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.097560135803838 | validation: 4.452441347031684]
	TIME [epoch: 9.08 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.953007985697498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.953007985697498 | validation: 3.906877133878925]
	TIME [epoch: 9.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8230832969246733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8230832969246733 | validation: 3.7549264267623013]
	TIME [epoch: 9.09 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6102256052932082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6102256052932082 | validation: 3.7590417174546387]
	TIME [epoch: 9.09 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.542843091548426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.542843091548426 | validation: 3.359106071324412]
	TIME [epoch: 9.09 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1864238458655607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1864238458655607 | validation: 3.498726353873908]
	TIME [epoch: 9.08 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9430363280891902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9430363280891902 | validation: 4.368595843638833]
	TIME [epoch: 9.1 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2093021747643555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2093021747643555 | validation: 3.3032502682870843]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2964508126238514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2964508126238514 | validation: 2.9483928660414023]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.892213030849741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.892213030849741 | validation: 3.1345679734672798]
	TIME [epoch: 9.09 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2636452152079705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2636452152079705 | validation: 2.026422388084821]
	TIME [epoch: 9.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0039195014007443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0039195014007443 | validation: 1.657505886953924]
	TIME [epoch: 9.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7735201014055133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7735201014055133 | validation: 1.3656861547372845]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4963795481202218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4963795481202218 | validation: 1.2451368650925647]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7452391981888777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7452391981888777 | validation: 2.806904354588909]
	TIME [epoch: 9.12 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5672507594016993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5672507594016993 | validation: 1.1644816597491745]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1617582870262357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1617582870262357 | validation: 1.3484445189455012]
	TIME [epoch: 9.09 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1860011707166023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1860011707166023 | validation: 1.0195063625997265]
	TIME [epoch: 9.09 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9365540668035918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9365540668035918 | validation: 0.678378526507004]
	TIME [epoch: 9.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2650508710734734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2650508710734734 | validation: 0.9827552294036652]
	TIME [epoch: 9.09 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9637859864405731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9637859864405731 | validation: 0.6393561214453107]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8080587526230575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8080587526230575 | validation: 1.4112637848248448]
	TIME [epoch: 9.08 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1641720711073678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1641720711073678 | validation: 0.903928908130775]
	TIME [epoch: 9.09 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8158541771830257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8158541771830257 | validation: 1.0769100597108943]
	TIME [epoch: 9.1 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1252703120205099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1252703120205099 | validation: 1.026019950765525]
	TIME [epoch: 9.08 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9657213330487842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9657213330487842 | validation: 0.6936569351650275]
	TIME [epoch: 9.08 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0509738790519072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0509738790519072 | validation: 0.8865518602155562]
	TIME [epoch: 9.08 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.810392861393141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.810392861393141 | validation: 0.9944743035962051]
	TIME [epoch: 9.09 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9224202412025294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9224202412025294 | validation: 0.6035461003093163]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.180326740075812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.180326740075812 | validation: 0.759628783779366]
	TIME [epoch: 9.09 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8515115193431576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8515115193431576 | validation: 1.112771306985954]
	TIME [epoch: 9.08 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8489406294913875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8489406294913875 | validation: 0.6784389709400835]
	TIME [epoch: 9.1 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8240014150415419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8240014150415419 | validation: 0.7581960561784415]
	TIME [epoch: 9.08 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8615652417969486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8615652417969486 | validation: 1.2140125528849737]
	TIME [epoch: 9.08 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9058097296461446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9058097296461446 | validation: 0.7035225790705126]
	TIME [epoch: 9.08 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.889306958969682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.889306958969682 | validation: 0.6559963934068265]
	TIME [epoch: 9.11 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8583217487086575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8583217487086575 | validation: 0.7082317132169612]
	TIME [epoch: 9.09 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7933394969471699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7933394969471699 | validation: 0.8810195849781237]
	TIME [epoch: 9.08 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2417533451562675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2417533451562675 | validation: 0.8037556629690361]
	TIME [epoch: 9.08 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8324720537703264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8324720537703264 | validation: 1.1752934027889688]
	TIME [epoch: 9.1 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9890877290354133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9890877290354133 | validation: 0.6613869939592851]
	TIME [epoch: 9.08 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6944534312547597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6944534312547597 | validation: 0.8019686892959677]
	TIME [epoch: 9.08 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8777347935056163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8777347935056163 | validation: 0.6784349762180533]
	TIME [epoch: 9.08 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8077390790635619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8077390790635619 | validation: 0.7049719928695224]
	TIME [epoch: 9.11 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8160849160849123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8160849160849123 | validation: 0.6639945936142913]
	TIME [epoch: 9.09 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7414272379498639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7414272379498639 | validation: 0.7585017796433151]
	TIME [epoch: 9.08 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0045549921957733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0045549921957733 | validation: 0.6726030221315814]
	TIME [epoch: 9.08 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7581939089536953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7581939089536953 | validation: 4.0065727506062565]
	TIME [epoch: 9.09 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5832749824638577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5832749824638577 | validation: 3.197000956968145]
	TIME [epoch: 9.09 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3096539810952224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3096539810952224 | validation: 3.0113040086405376]
	TIME [epoch: 9.08 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.359096263158108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.359096263158108 | validation: 3.1419300670026526]
	TIME [epoch: 9.08 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.404115061240325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.404115061240325 | validation: 3.097754229137221]
	TIME [epoch: 9.09 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3770809099664434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3770809099664434 | validation: 3.121905044678382]
	TIME [epoch: 9.09 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3590129503937325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3590129503937325 | validation: 3.20293477733472]
	TIME [epoch: 9.08 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4088582626280006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4088582626280006 | validation: 3.2569653728936023]
	TIME [epoch: 9.08 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.376074354397678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.376074354397678 | validation: 3.0904039737868993]
	TIME [epoch: 9.08 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1786216617587053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1786216617587053 | validation: 2.1322592268478875]
	TIME [epoch: 9.1 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.493497918888988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.493497918888988 | validation: 1.0539662408855586]
	TIME [epoch: 9.08 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8299298277588232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8299298277588232 | validation: 0.8779230505836098]
	TIME [epoch: 9.08 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8350637822675454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8350637822675454 | validation: 0.7570841905009273]
	TIME [epoch: 9.07 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7947347804602056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7947347804602056 | validation: 0.7575840109352794]
	TIME [epoch: 9.1 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7992546024759191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7992546024759191 | validation: 0.699605581551666]
	TIME [epoch: 9.07 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7678170868647124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7678170868647124 | validation: 0.8659538912740622]
	TIME [epoch: 9.07 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7552343718182881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7552343718182881 | validation: 0.7206560743742358]
	TIME [epoch: 9.08 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7006506782628281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7006506782628281 | validation: 0.6362788677741074]
	TIME [epoch: 9.1 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7001052610537617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001052610537617 | validation: 1.065503197825553]
	TIME [epoch: 9.08 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.825194253001818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.825194253001818 | validation: 1.0493260017014936]
	TIME [epoch: 9.08 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8701863189366913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8701863189366913 | validation: 1.3597786458972079]
	TIME [epoch: 9.07 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9102407571290234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9102407571290234 | validation: 0.646611109514387]
	TIME [epoch: 9.09 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.664793634141545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.664793634141545 | validation: 0.6144086623639228]
	TIME [epoch: 9.08 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7088718052607204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7088718052607204 | validation: 0.6650372645035227]
	TIME [epoch: 9.07 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7397700344015967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7397700344015967 | validation: 0.9371761805599066]
	TIME [epoch: 9.08 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7629735290881281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7629735290881281 | validation: 0.7181962135698936]
	TIME [epoch: 9.09 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6923684962726446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6923684962726446 | validation: 0.6381942002496466]
	TIME [epoch: 9.09 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6423776364806075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6423776364806075 | validation: 0.8056211233743149]
	TIME [epoch: 9.08 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7751610293936828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7751610293936828 | validation: 0.865476782392118]
	TIME [epoch: 9.07 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9203817669985114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9203817669985114 | validation: 0.9770723839006533]
	TIME [epoch: 9.09 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9048910683034037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9048910683034037 | validation: 0.6144392498514353]
	TIME [epoch: 9.09 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6760556550507724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6760556550507724 | validation: 0.8102121721908977]
	TIME [epoch: 9.08 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7477826984348414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7477826984348414 | validation: 0.6333147199291875]
	TIME [epoch: 9.07 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7039517846049284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7039517846049284 | validation: 0.7575308288372471]
	TIME [epoch: 9.08 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7597423705816528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7597423705816528 | validation: 0.7141969652913285]
	TIME [epoch: 9.1 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7577060021149464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7577060021149464 | validation: 0.6426438931879812]
	TIME [epoch: 9.08 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8297524619943148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8297524619943148 | validation: 0.655396602465921]
	TIME [epoch: 9.08 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.649021746130865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.649021746130865 | validation: 0.6365484242020414]
	TIME [epoch: 9.08 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7192124684992895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7192124684992895 | validation: 0.6598481556711215]
	TIME [epoch: 9.1 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7670690052792006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7670690052792006 | validation: 0.6372688961689092]
	TIME [epoch: 9.08 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.776524248840073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.776524248840073 | validation: 0.8388987780305568]
	TIME [epoch: 9.08 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7598140586639504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7598140586639504 | validation: 0.6439942079629877]
	TIME [epoch: 9.07 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.731345163462125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.731345163462125 | validation: 0.8223529086022077]
	TIME [epoch: 9.1 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8881960479629585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8881960479629585 | validation: 0.7233117838073162]
	TIME [epoch: 9.08 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.73995888187776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.73995888187776 | validation: 0.7000453518503651]
	TIME [epoch: 9.08 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.71402282971024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.71402282971024 | validation: 3.3386466316256715]
	TIME [epoch: 9.07 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9858774244533819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9858774244533819 | validation: 1.1821645857870857]
	TIME [epoch: 9.1 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7208263807397068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7208263807397068 | validation: 0.7465982087662368]
	TIME [epoch: 9.08 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7337704259448126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7337704259448126 | validation: 0.9579862578834849]
	TIME [epoch: 9.08 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6735023405060857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6735023405060857 | validation: 0.7482036921086053]
	TIME [epoch: 9.07 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6573658976983714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6573658976983714 | validation: 0.5395515190881002]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7277674174434261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7277674174434261 | validation: 1.316282221876061]
	TIME [epoch: 9.19 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7170344144741517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7170344144741517 | validation: 1.0141275136445056]
	TIME [epoch: 9.07 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7526371930140133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7526371930140133 | validation: 0.5711801874942934]
	TIME [epoch: 9.08 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7231783988207575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7231783988207575 | validation: 0.6703850236405345]
	TIME [epoch: 9.08 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5980910596736428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5980910596736428 | validation: 0.60058678571277]
	TIME [epoch: 9.09 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7297118297723983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7297118297723983 | validation: 0.5871169566205341]
	TIME [epoch: 9.07 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5124891647141077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5124891647141077 | validation: 2.973813664109568]
	TIME [epoch: 9.08 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4150338946996044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4150338946996044 | validation: 2.9061325296519356]
	TIME [epoch: 9.08 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3356837761172735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3356837761172735 | validation: 2.842879284931902]
	TIME [epoch: 9.1 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.360588757572773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.360588757572773 | validation: 1.0146539764691904]
	TIME [epoch: 9.08 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.990476171398153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.990476171398153 | validation: 1.158495457671277]
	TIME [epoch: 9.08 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7226298014419932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7226298014419932 | validation: 2.8591820769136325]
	TIME [epoch: 9.08 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9888810721726715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9888810721726715 | validation: 1.2240319733836227]
	TIME [epoch: 9.1 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6394127224789714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6394127224789714 | validation: 0.8828810355285541]
	TIME [epoch: 9.08 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.723221849211739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.723221849211739 | validation: 1.1160203713745438]
	TIME [epoch: 9.08 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7937642494174781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7937642494174781 | validation: 0.6997239509929246]
	TIME [epoch: 9.08 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6911227011473524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6911227011473524 | validation: 0.638621632400129]
	TIME [epoch: 9.09 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5793313351580117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5793313351580117 | validation: 0.5331428595485711]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.56786436444827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.56786436444827 | validation: 0.592696755802874]
	TIME [epoch: 9.08 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.536957506075787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.536957506075787 | validation: 0.8435463281016211]
	TIME [epoch: 9.07 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6750542964735661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6750542964735661 | validation: 0.717629246686551]
	TIME [epoch: 9.1 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8160840222195385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8160840222195385 | validation: 0.5854666637010645]
	TIME [epoch: 9.08 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8120199675115675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8120199675115675 | validation: 0.595327894754737]
	TIME [epoch: 9.08 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4412122084033988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4412122084033988 | validation: 0.40308653816755796]
	TIME [epoch: 9.07 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6251792224785419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6251792224785419 | validation: 0.5481527603164995]
	TIME [epoch: 9.09 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5247691398901658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5247691398901658 | validation: 0.37612498001365086]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8588570084206444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8588570084206444 | validation: 0.5326015015054818]
	TIME [epoch: 9.08 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5363056554025378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5363056554025378 | validation: 0.4541866400271808]
	TIME [epoch: 9.08 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5925820448036595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5925820448036595 | validation: 1.0500037765133599]
	TIME [epoch: 9.08 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7119340118746943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7119340118746943 | validation: 0.5585614279787532]
	TIME [epoch: 9.1 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45724343927343847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45724343927343847 | validation: 0.3990737778431187]
	TIME [epoch: 9.08 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4133524500841821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4133524500841821 | validation: 0.6895576691737457]
	TIME [epoch: 9.05 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6106267591324571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6106267591324571 | validation: 0.7869561292764857]
	TIME [epoch: 9.04 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47711174401285456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47711174401285456 | validation: 0.5855552156988375]
	TIME [epoch: 9.08 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5127493134352727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5127493134352727 | validation: 0.5154275209569329]
	TIME [epoch: 9.08 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4137575840304114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4137575840304114 | validation: 0.5108640284183323]
	TIME [epoch: 9.09 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49021403458728285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49021403458728285 | validation: 0.3497682747138239]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44300940081491536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44300940081491536 | validation: 0.5800366118510225]
	TIME [epoch: 9.1 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39486223184862973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39486223184862973 | validation: 0.22208951980164884]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4459734421878142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4459734421878142 | validation: 0.31901900425696383]
	TIME [epoch: 9.08 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36749985869981705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36749985869981705 | validation: 0.3697298062092738]
	TIME [epoch: 9.08 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39397617994104034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39397617994104034 | validation: 0.5169436059917947]
	TIME [epoch: 9.11 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.398418352261242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.398418352261242 | validation: 1.1827798804438372]
	TIME [epoch: 9.08 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38600840089690336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38600840089690336 | validation: 0.477762936018179]
	TIME [epoch: 9.06 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3839202396374263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3839202396374263 | validation: 0.48847323320069713]
	TIME [epoch: 9.05 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3558386994066387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3558386994066387 | validation: 0.421283616765451]
	TIME [epoch: 9.08 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40866297566320364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40866297566320364 | validation: 0.2672154117342993]
	TIME [epoch: 9.06 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34017477615984654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34017477615984654 | validation: 0.80156506372482]
	TIME [epoch: 9.07 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47883438962832114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47883438962832114 | validation: 0.2995239282135984]
	TIME [epoch: 9.06 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32420270543428464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32420270543428464 | validation: 0.3000191305839921]
	TIME [epoch: 9.06 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3635761193624041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3635761193624041 | validation: 0.3205872373606693]
	TIME [epoch: 9.13 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5292189116942816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5292189116942816 | validation: 0.5363631027800991]
	TIME [epoch: 9.11 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39884357204266513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39884357204266513 | validation: 0.3874823815050388]
	TIME [epoch: 9.11 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29167029873597716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29167029873597716 | validation: 0.2020937717200646]
	TIME [epoch: 9.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3789007048854785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3789007048854785 | validation: 0.2899744441955949]
	TIME [epoch: 9.09 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37571508692850797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37571508692850797 | validation: 0.532534634240392]
	TIME [epoch: 9.11 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3529712963264692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3529712963264692 | validation: 0.4491071263302293]
	TIME [epoch: 9.09 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3264635450825075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3264635450825075 | validation: 0.3089315934590897]
	TIME [epoch: 9.07 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44143904897838315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44143904897838315 | validation: 0.7617147088790247]
	TIME [epoch: 9.07 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5293622641539125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5293622641539125 | validation: 0.8578427107790139]
	TIME [epoch: 9.1 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6263243919384323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6263243919384323 | validation: 0.8452124798774033]
	TIME [epoch: 9.08 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32593237644628553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32593237644628553 | validation: 0.35665478191442657]
	TIME [epoch: 9.03 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3402459240724785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3402459240724785 | validation: 0.7318828612364547]
	TIME [epoch: 9.07 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5871359006649546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5871359006649546 | validation: 0.7300880352832029]
	TIME [epoch: 9.07 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40550790998149344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40550790998149344 | validation: 0.8708386573551148]
	TIME [epoch: 9.1 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5769890565187012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5769890565187012 | validation: 0.5037739967657748]
	TIME [epoch: 9.07 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3078634571976337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3078634571976337 | validation: 0.18496901808353278]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23960470662192374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23960470662192374 | validation: 0.2889546260030744]
	TIME [epoch: 9.07 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23376084712720088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23376084712720088 | validation: 0.3200433601738239]
	TIME [epoch: 9.09 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7893992449276604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7893992449276604 | validation: 0.6660028076050459]
	TIME [epoch: 9.07 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34276549904628467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34276549904628467 | validation: 0.4029180867396177]
	TIME [epoch: 9.07 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47357741714046686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47357741714046686 | validation: 0.4794998373738923]
	TIME [epoch: 9.07 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40658194899627825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40658194899627825 | validation: 0.4990180780008676]
	TIME [epoch: 9.07 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3726965954742037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3726965954742037 | validation: 0.3386799733807109]
	TIME [epoch: 9.09 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30648075817073095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30648075817073095 | validation: 1.0467170084094541]
	TIME [epoch: 9.07 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6273234463837738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6273234463837738 | validation: 0.27007168261939707]
	TIME [epoch: 9.07 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3700021431442325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3700021431442325 | validation: 0.22065076392740618]
	TIME [epoch: 9.07 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5795713163274185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5795713163274185 | validation: 0.4155068600767472]
	TIME [epoch: 9.09 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49541726726451907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49541726726451907 | validation: 0.29651620395997014]
	TIME [epoch: 9.08 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3513301298432247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3513301298432247 | validation: 0.2749474066577521]
	TIME [epoch: 9.07 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2948937750998843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2948937750998843 | validation: 0.24772347468006622]
	TIME [epoch: 9.08 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5418078517110968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5418078517110968 | validation: 0.37539801215857715]
	TIME [epoch: 9.08 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3725143420843083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3725143420843083 | validation: 0.38632549584257736]
	TIME [epoch: 9.1 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4406282740498906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4406282740498906 | validation: 0.3002040458788676]
	TIME [epoch: 9.07 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2781506236792017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2781506236792017 | validation: 0.8158516439097543]
	TIME [epoch: 9.07 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4795370888345798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4795370888345798 | validation: 0.35911438247375116]
	TIME [epoch: 9.07 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35177009891220357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35177009891220357 | validation: 0.4155210590466853]
	TIME [epoch: 9.07 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3565481316341634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3565481316341634 | validation: 0.4127857223191954]
	TIME [epoch: 9.09 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3339036815727231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3339036815727231 | validation: 0.3057098061928387]
	TIME [epoch: 9.08 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.291724507273317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.291724507273317 | validation: 0.24992794264268292]
	TIME [epoch: 9.07 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0239308618775511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0239308618775511 | validation: 2.565792997302055]
	TIME [epoch: 9.07 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0088992222720996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0088992222720996 | validation: 0.27157169425142264]
	TIME [epoch: 9.09 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2948336431292751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2948336431292751 | validation: 0.24846277722679233]
	TIME [epoch: 9.07 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.292468420672492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.292468420672492 | validation: 0.49324445896968705]
	TIME [epoch: 9.05 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3910721530168619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3910721530168619 | validation: 0.2573996339413616]
	TIME [epoch: 9 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3523921642836612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3523921642836612 | validation: 0.4331859753539905]
	TIME [epoch: 8.98 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2667321596442875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2667321596442875 | validation: 0.2675238686494166]
	TIME [epoch: 9.01 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31187719708892064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31187719708892064 | validation: 0.53811287513303]
	TIME [epoch: 9.05 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3437235068735557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3437235068735557 | validation: 0.35259123096734435]
	TIME [epoch: 9.08 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2920649322730798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2920649322730798 | validation: 0.32440102455767506]
	TIME [epoch: 9.08 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.295804510085404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.295804510085404 | validation: 0.3959110512436296]
	TIME [epoch: 9.1 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3425901464862035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3425901464862035 | validation: 0.4243721426684264]
	TIME [epoch: 9.09 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38302774429793074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38302774429793074 | validation: 0.3863443452474695]
	TIME [epoch: 9.08 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34320256121016157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34320256121016157 | validation: 0.38956657310022014]
	TIME [epoch: 9.08 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31958615289698894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31958615289698894 | validation: 0.3235955661839002]
	TIME [epoch: 9.1 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25675028015568657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25675028015568657 | validation: 0.29263165947693387]
	TIME [epoch: 9.11 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3638041631229752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3638041631229752 | validation: 0.49237525628764]
	TIME [epoch: 9.08 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38700602346083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38700602346083 | validation: 0.28885246703534084]
	TIME [epoch: 9.08 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29821075222814386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29821075222814386 | validation: 0.37823420621062603]
	TIME [epoch: 9.08 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2681621714035519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2681621714035519 | validation: 0.542330722713112]
	TIME [epoch: 9.08 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3618691116436186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3618691116436186 | validation: 0.3785229483655216]
	TIME [epoch: 9.11 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33636874230227465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33636874230227465 | validation: 0.39608827958134285]
	TIME [epoch: 9.09 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45050367488235893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45050367488235893 | validation: 0.251263019734]
	TIME [epoch: 9.09 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7316778527558195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7316778527558195 | validation: 0.46480126170508324]
	TIME [epoch: 9.08 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5464044601116385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5464044601116385 | validation: 2.8008373086139216]
	TIME [epoch: 9.1 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3786624341517273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3786624341517273 | validation: 0.4441657765735446]
	TIME [epoch: 9.09 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35526933977808933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35526933977808933 | validation: 0.268206952878439]
	TIME [epoch: 9.08 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35348910097876207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35348910097876207 | validation: 1.0222469704080834]
	TIME [epoch: 9.07 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40673924920758725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40673924920758725 | validation: 0.19735833936486796]
	TIME [epoch: 9.08 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31008442183844176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31008442183844176 | validation: 0.30900886506760905]
	TIME [epoch: 9.09 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27842334599128016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27842334599128016 | validation: 0.43610380512221913]
	TIME [epoch: 9.09 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34318371362149813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34318371362149813 | validation: 0.3946687003871406]
	TIME [epoch: 9.09 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.356919618683854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.356919618683854 | validation: 0.4082183608332715]
	TIME [epoch: 9.08 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3075084207439179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3075084207439179 | validation: 0.2518884496104818]
	TIME [epoch: 9.1 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26933046258953286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26933046258953286 | validation: 1.110875429108996]
	TIME [epoch: 9.08 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5877529636205142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5877529636205142 | validation: 1.8796404565799638]
	TIME [epoch: 9.08 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6128375785754867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6128375785754867 | validation: 0.3428303375537255]
	TIME [epoch: 9.09 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32766555566509475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32766555566509475 | validation: 0.4994798384599122]
	TIME [epoch: 9.07 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4256087613646297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4256087613646297 | validation: 0.36431626096626546]
	TIME [epoch: 9.11 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4073345729018187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4073345729018187 | validation: 0.45904607804779984]
	TIME [epoch: 9.12 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3492826160053794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3492826160053794 | validation: 0.2604950465286616]
	TIME [epoch: 9.09 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37422520622340305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37422520622340305 | validation: 0.1962705972287107]
	TIME [epoch: 9.1 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3383819665774619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3383819665774619 | validation: 0.644386623830266]
	TIME [epoch: 9.1 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28212937096709056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28212937096709056 | validation: 0.3601910731620666]
	TIME [epoch: 9.09 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3096854221242408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3096854221242408 | validation: 0.2031479887397284]
	TIME [epoch: 9.1 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3613060905810547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3613060905810547 | validation: 0.2161070674081707]
	TIME [epoch: 9.08 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28430230747461277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28430230747461277 | validation: 0.18554529728235888]
	TIME [epoch: 9.07 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4013092136459774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4013092136459774 | validation: 0.3142741118501082]
	TIME [epoch: 9.1 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40519233403046007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40519233403046007 | validation: 0.2214409190232993]
	TIME [epoch: 9.08 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40271370968137205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40271370968137205 | validation: 0.257673134066018]
	TIME [epoch: 9.07 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8019644512137445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8019644512137445 | validation: 0.35460257669158957]
	TIME [epoch: 9.09 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31383026897473393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31383026897473393 | validation: 0.4621335804215403]
	TIME [epoch: 9.08 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2823472328971143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2823472328971143 | validation: 0.24256052034411063]
	TIME [epoch: 9.1 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3089350483194544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3089350483194544 | validation: 0.246401033736381]
	TIME [epoch: 9.08 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3673477424424436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3673477424424436 | validation: 0.4691210218717548]
	TIME [epoch: 9.08 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41062564656323913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41062564656323913 | validation: 0.48973508699995383]
	TIME [epoch: 9.08 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29089028082102175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29089028082102175 | validation: 0.2806337024035289]
	TIME [epoch: 9.11 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26912223356494497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26912223356494497 | validation: 0.3805628640346449]
	TIME [epoch: 9.09 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2455263924889864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2455263924889864 | validation: 0.20848856471748453]
	TIME [epoch: 9.08 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2791082042915918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2791082042915918 | validation: 0.657190959087989]
	TIME [epoch: 9.07 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27310631372554844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27310631372554844 | validation: 0.19979652551901883]
	TIME [epoch: 9.09 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26915637842076034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26915637842076034 | validation: 0.23327496404327758]
	TIME [epoch: 9.09 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30854440665707344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30854440665707344 | validation: 0.6347052424910089]
	TIME [epoch: 9.09 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3550063837956186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3550063837956186 | validation: 0.4308305509824732]
	TIME [epoch: 9.09 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3196931978792682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3196931978792682 | validation: 0.23093500520389473]
	TIME [epoch: 9.08 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2249680516165248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2249680516165248 | validation: 0.24145738004795564]
	TIME [epoch: 9.08 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22996565341644115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22996565341644115 | validation: 0.21673219709290048]
	TIME [epoch: 9.09 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26370110126034535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26370110126034535 | validation: 0.1948850086271396]
	TIME [epoch: 9.08 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5050385955944544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5050385955944544 | validation: 0.3439959803772046]
	TIME [epoch: 9.08 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3032258091430914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3032258091430914 | validation: 0.45251850216215095]
	TIME [epoch: 9.08 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4608049845289692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4608049845289692 | validation: 0.28297209574019877]
	TIME [epoch: 9.1 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2160569668931068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2160569668931068 | validation: 0.3722517778120806]
	TIME [epoch: 9.08 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2992013579920898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2992013579920898 | validation: 0.2735869516533196]
	TIME [epoch: 9.08 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16345740363905042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16345740363905042 | validation: 0.22111216737818276]
	TIME [epoch: 9.09 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17268023771123633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17268023771123633 | validation: 0.1466357351959544]
	TIME [epoch: 9.08 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22686771003943215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22686771003943215 | validation: 0.22499723690299173]
	TIME [epoch: 9.1 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5501709643427495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5501709643427495 | validation: 0.45191407005455564]
	TIME [epoch: 9.07 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5110851980618402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5110851980618402 | validation: 0.4305404186249024]
	TIME [epoch: 9.07 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3705319456538942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3705319456538942 | validation: 0.2335428258100667]
	TIME [epoch: 9.06 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3116711141422518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3116711141422518 | validation: 0.3951860401303488]
	TIME [epoch: 9.08 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2664369510323024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2664369510323024 | validation: 0.1674945170973609]
	TIME [epoch: 9.07 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2372742306636244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2372742306636244 | validation: 0.23520454548292022]
	TIME [epoch: 9.06 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23236716754787973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23236716754787973 | validation: 0.6512006030366304]
	TIME [epoch: 9.07 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6279247502080516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6279247502080516 | validation: 0.6925171053251566]
	TIME [epoch: 9.06 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5018576233537122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5018576233537122 | validation: 0.29409487250279387]
	TIME [epoch: 9.09 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35714593016349594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35714593016349594 | validation: 2.0924972499858656]
	TIME [epoch: 9.07 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7073179459867953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7073179459867953 | validation: 0.25426081177539084]
	TIME [epoch: 9.07 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4739135330529149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4739135330529149 | validation: 0.282546304711768]
	TIME [epoch: 9.06 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29730502948034654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29730502948034654 | validation: 0.15383815768908055]
	TIME [epoch: 9.08 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2725226211006226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2725226211006226 | validation: 0.17304740392202705]
	TIME [epoch: 9.08 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.377659217208315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.377659217208315 | validation: 0.23400833563267184]
	TIME [epoch: 9.07 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3622170693770046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3622170693770046 | validation: 0.20286421076759809]
	TIME [epoch: 9.06 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.272557410849881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.272557410849881 | validation: 0.24907042573624105]
	TIME [epoch: 9.06 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46807266993647323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46807266993647323 | validation: 0.3868560996281754]
	TIME [epoch: 9.1 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7169241958304402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7169241958304402 | validation: 0.2508315110584655]
	TIME [epoch: 9.07 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2742217631975113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2742217631975113 | validation: 0.36832989541762123]
	TIME [epoch: 9.06 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28417620977429825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28417620977429825 | validation: 0.3066514595535468]
	TIME [epoch: 9.05 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2866680526292564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2866680526292564 | validation: 0.22686704866661866]
	TIME [epoch: 9.06 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4002691755052936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4002691755052936 | validation: 0.3422048817691514]
	TIME [epoch: 9.08 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4288183745501725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4288183745501725 | validation: 0.239159153946088]
	TIME [epoch: 9.07 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.304365480119864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.304365480119864 | validation: 0.37008458287101115]
	TIME [epoch: 9.06 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3226425107916962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3226425107916962 | validation: 0.7852020934855918]
	TIME [epoch: 9.06 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46877477477136154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46877477477136154 | validation: 0.29480895284678954]
	TIME [epoch: 9.08 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26046764225643365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26046764225643365 | validation: 0.31515560858280733]
	TIME [epoch: 9.07 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2395812852463619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2395812852463619 | validation: 0.7468989835181686]
	TIME [epoch: 9.06 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5758648793706999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5758648793706999 | validation: 0.21807945389174954]
	TIME [epoch: 9.05 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2145291521617294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2145291521617294 | validation: 0.3756080926073586]
	TIME [epoch: 9.05 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32729061331855014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32729061331855014 | validation: 0.2004920741177949]
	TIME [epoch: 9.08 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33060375376189355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33060375376189355 | validation: 0.7875189765142188]
	TIME [epoch: 9.06 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43775547594354425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43775547594354425 | validation: 0.36828320888851407]
	TIME [epoch: 9.05 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29408654308624277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29408654308624277 | validation: 0.18340482545755915]
	TIME [epoch: 9.06 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2419272975445362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2419272975445362 | validation: 0.33459553572284795]
	TIME [epoch: 9.08 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2881706906037464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2881706906037464 | validation: 1.0157541511675214]
	TIME [epoch: 9.07 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4354192724608884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4354192724608884 | validation: 0.26326554940380065]
	TIME [epoch: 9.06 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22955212609372605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22955212609372605 | validation: 0.3731197878012124]
	TIME [epoch: 9.07 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29915807968050007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29915807968050007 | validation: 0.2214950416016411]
	TIME [epoch: 9.06 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9043832271735834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9043832271735834 | validation: 1.3687918517462148]
	TIME [epoch: 9.09 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8358355040652506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8358355040652506 | validation: 0.8836721485141525]
	TIME [epoch: 9.06 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4592017104838117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4592017104838117 | validation: 0.3198544947231557]
	TIME [epoch: 9.07 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.306387771172241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.306387771172241 | validation: 0.30550113885688146]
	TIME [epoch: 9.07 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8317333996592087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8317333996592087 | validation: 0.8373036252154782]
	TIME [epoch: 9.07 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5363369521636993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5363369521636993 | validation: 0.25929803942746144]
	TIME [epoch: 9.09 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6200219043974159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6200219043974159 | validation: 0.4148616419024501]
	TIME [epoch: 9.07 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33410377652128764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33410377652128764 | validation: 0.35496864010949136]
	TIME [epoch: 9.07 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24307381458206206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24307381458206206 | validation: 0.23644192130481706]
	TIME [epoch: 9.07 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22091523981381286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22091523981381286 | validation: 0.2596332637004561]
	TIME [epoch: 9.08 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.856898929756808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.856898929756808 | validation: 0.3388198190714026]
	TIME [epoch: 9.08 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3531210925434698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3531210925434698 | validation: 0.2498436549888835]
	TIME [epoch: 9.15 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5992918079166911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5992918079166911 | validation: 0.31399796040214895]
	TIME [epoch: 9.07 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4310377005825471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4310377005825471 | validation: 0.26907955845143217]
	TIME [epoch: 9.07 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2886879569579109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2886879569579109 | validation: 0.4282706636929047]
	TIME [epoch: 9.09 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3723476793164672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3723476793164672 | validation: 0.2608698074817481]
	TIME [epoch: 9.06 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3552572994345084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3552572994345084 | validation: 0.4081892677501754]
	TIME [epoch: 9.05 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3651902182483193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3651902182483193 | validation: 0.25871062962869246]
	TIME [epoch: 9.06 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33350757410627835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33350757410627835 | validation: 0.2243780469870069]
	TIME [epoch: 9.06 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5535421188209159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5535421188209159 | validation: 0.4931573231459013]
	TIME [epoch: 9.04 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.598620007877541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.598620007877541 | validation: 0.4313268653522493]
	TIME [epoch: 9.03 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39092061875914313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39092061875914313 | validation: 0.20387355246800673]
	TIME [epoch: 9.05 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4019649423111805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4019649423111805 | validation: 0.4795396010814458]
	TIME [epoch: 9.07 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3043371670806937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3043371670806937 | validation: 0.5674578601287719]
	TIME [epoch: 9.09 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3170115411536244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3170115411536244 | validation: 0.3000858194126872]
	TIME [epoch: 9.06 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3099406673706028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3099406673706028 | validation: 0.18497150514913335]
	TIME [epoch: 9.06 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5126761450856324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5126761450856324 | validation: 0.39207673900848256]
	TIME [epoch: 9.06 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3570941345734325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3570941345734325 | validation: 0.279732358196788]
	TIME [epoch: 9.08 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42870243468856684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42870243468856684 | validation: 0.2796738726224582]
	TIME [epoch: 9.09 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31886738619676724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31886738619676724 | validation: 0.26423464702518457]
	TIME [epoch: 9.07 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5752244180264446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5752244180264446 | validation: 0.5016813706514698]
	TIME [epoch: 9.07 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4434066083107237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4434066083107237 | validation: 0.4360892569311573]
	TIME [epoch: 9.06 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46293174814055577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46293174814055577 | validation: 0.25459037872558776]
	TIME [epoch: 9.09 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2763221900463277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2763221900463277 | validation: 0.41244326734412784]
	TIME [epoch: 9.07 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5363124775088483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5363124775088483 | validation: 0.3908215560950703]
	TIME [epoch: 9.06 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40251201891686206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40251201891686206 | validation: 0.5451066291239719]
	TIME [epoch: 9.07 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2061496298643815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2061496298643815 | validation: 3.7189519010986776]
	TIME [epoch: 9.07 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9109024471344327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9109024471344327 | validation: 0.4226176796681917]
	TIME [epoch: 9.1 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.436686683165523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.436686683165523 | validation: 0.5596277897435556]
	TIME [epoch: 9.07 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44397270869946387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44397270869946387 | validation: 1.9145264826754709]
	TIME [epoch: 9.07 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6898498073503887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6898498073503887 | validation: 0.346432664761822]
	TIME [epoch: 9.06 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29332641587725916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29332641587725916 | validation: 0.27901380253117714]
	TIME [epoch: 9.08 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2590249623554164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2590249623554164 | validation: 0.23088843219821553]
	TIME [epoch: 9.07 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1907450290729737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1907450290729737 | validation: 0.2037219364433137]
	TIME [epoch: 9.07 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2110563924055512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2110563924055512 | validation: 0.4935427055828634]
	TIME [epoch: 9.07 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28122981385420887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28122981385420887 | validation: 0.347813773607283]
	TIME [epoch: 9.06 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.510804322189304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.510804322189304 | validation: 0.4351332806831836]
	TIME [epoch: 9.07 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6399980794768612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6399980794768612 | validation: 0.7372041793275612]
	TIME [epoch: 9.07 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.708844779623674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.708844779623674 | validation: 1.4246625241098827]
	TIME [epoch: 9.06 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7945600562795777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7945600562795777 | validation: 0.23842166849429253]
	TIME [epoch: 9.06 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3018065383294982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3018065383294982 | validation: 0.3892072863415367]
	TIME [epoch: 9.08 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44373497241290016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44373497241290016 | validation: 0.17838681689131597]
	TIME [epoch: 9.08 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3704732440576474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3704732440576474 | validation: 0.23729686367149944]
	TIME [epoch: 9.06 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1872690518959542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1872690518959542 | validation: 0.18420554198248296]
	TIME [epoch: 9.06 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26194007982405354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26194007982405354 | validation: 0.2874627290869034]
	TIME [epoch: 9.05 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18400572813255295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18400572813255295 | validation: 0.28139801963933186]
	TIME [epoch: 9.08 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3360081040712393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3360081040712393 | validation: 0.2376504005017828]
	TIME [epoch: 9.08 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24859391127420438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24859391127420438 | validation: 0.4004469027809259]
	TIME [epoch: 9.06 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2033302514243799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2033302514243799 | validation: 0.13541889456420403]
	TIME [epoch: 9.06 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2368462866338529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2368462866338529 | validation: 0.19422067469170506]
	TIME [epoch: 9.08 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5787065180567024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5787065180567024 | validation: 0.7072250024514026]
	TIME [epoch: 9.09 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4108957541605524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4108957541605524 | validation: 0.5382411648505863]
	TIME [epoch: 9.07 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38603523669043815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38603523669043815 | validation: 0.27765155780360273]
	TIME [epoch: 9.07 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2703637730241021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2703637730241021 | validation: 0.17693941164949092]
	TIME [epoch: 9.07 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.325024536563692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.325024536563692 | validation: 0.2660306780565911]
	TIME [epoch: 9.09 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20910401336296086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20910401336296086 | validation: 0.2894721660995997]
	TIME [epoch: 9.08 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.320959953530466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.320959953530466 | validation: 0.3060901863449994]
	TIME [epoch: 9.07 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3257768927762316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3257768927762316 | validation: 0.5569916210373687]
	TIME [epoch: 9.06 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38468846855144057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38468846855144057 | validation: 0.26776592249743014]
	TIME [epoch: 9.07 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18457504092769075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18457504092769075 | validation: 0.13734728493206072]
	TIME [epoch: 9.09 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25966630957601294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25966630957601294 | validation: 0.46335447104288363]
	TIME [epoch: 9.07 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2779161904505759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2779161904505759 | validation: 0.23736541347584544]
	TIME [epoch: 9.07 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2138774611972433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2138774611972433 | validation: 0.30407970297425835]
	TIME [epoch: 9.08 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6364794171262942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6364794171262942 | validation: 0.4362413521385553]
	TIME [epoch: 9.08 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24448658546755814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24448658546755814 | validation: 0.17064823499480486]
	TIME [epoch: 9.08 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2532020186539531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2532020186539531 | validation: 0.17825685680026537]
	TIME [epoch: 9.06 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3250202544371389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3250202544371389 | validation: 0.30475796940327027]
	TIME [epoch: 9.07 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1988597344466965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1988597344466965 | validation: 0.21298710760735196]
	TIME [epoch: 9.06 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8407705790676442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8407705790676442 | validation: 0.5771033378100114]
	TIME [epoch: 9.09 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5822518608449789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5822518608449789 | validation: 0.42723949215995227]
	TIME [epoch: 9.07 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3308659035738362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3308659035738362 | validation: 0.19695851667974076]
	TIME [epoch: 9.07 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3882004636680806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3882004636680806 | validation: 0.2651798553159687]
	TIME [epoch: 9.06 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2680397782250147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2680397782250147 | validation: 0.23130477443260783]
	TIME [epoch: 9.07 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23290978386422748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23290978386422748 | validation: 0.10090360912203195]
	TIME [epoch: 9.09 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study4/model_tr_study4_r5_20240217_071532/states/model_tr_study4_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15936816134690185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15936816134690185 | validation: 0.20993232207997475]
	TIME [epoch: 9.07 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3031187818231881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3031187818231881 | validation: 0.6141280771679855]
	TIME [epoch: 9.06 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3929429275816717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3929429275816717 | validation: 0.23880890121674161]
	TIME [epoch: 9.06 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32499472063332385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32499472063332385 | validation: 0.6086789165965245]
	TIME [epoch: 9.06 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3431440967498173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3431440967498173 | validation: 0.16923329535885587]
	TIME [epoch: 9.05 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17080421441888982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17080421441888982 | validation: 0.2594067626073129]
	TIME [epoch: 9.03 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19636815392962087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19636815392962087 | validation: 0.22963699829313539]
	TIME [epoch: 9.03 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22517035660990153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22517035660990153 | validation: 0.274109600837007]
	TIME [epoch: 9.07 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23071525500526352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23071525500526352 | validation: 0.12604863873010277]
	TIME [epoch: 9.08 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22577010661998723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22577010661998723 | validation: 0.17289759502609586]
	TIME [epoch: 9.07 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2249308319274473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2249308319274473 | validation: 0.21518207847474735]
	TIME [epoch: 9.07 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19769338833657746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19769338833657746 | validation: 0.1952669787166054]
	TIME [epoch: 9.06 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23829226278943808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23829226278943808 | validation: 0.30552680679462596]
	TIME [epoch: 9.09 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35976822585228857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35976822585228857 | validation: 0.2033080477364948]
	TIME [epoch: 9.07 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22248760812923846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22248760812923846 | validation: 0.21688496815487368]
	TIME [epoch: 9.07 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21114385842624936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21114385842624936 | validation: 0.16145956177000256]
	TIME [epoch: 9.07 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2683383126927186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2683383126927186 | validation: 0.150853184162267]
	TIME [epoch: 9.06 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.267413028490061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.267413028490061 | validation: 0.20098376893783593]
	TIME [epoch: 9.09 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18950864958286562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18950864958286562 | validation: 0.27217778323376907]
	TIME [epoch: 9.07 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15060925564377575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15060925564377575 | validation: 0.12816573085143626]
	TIME [epoch: 9.06 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3304033667549809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3304033667549809 | validation: 1.479966097749854]
	TIME [epoch: 9.06 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9917327645400487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9917327645400487 | validation: 0.2661348441831576]
	TIME [epoch: 9.06 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4285496222188662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4285496222188662 | validation: 0.4160962286995715]
	TIME [epoch: 9.09 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36045161811878856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36045161811878856 | validation: 2.5606866112579785]
	TIME [epoch: 9.06 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6127756271202981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6127756271202981 | validation: 0.38707930229121557]
	TIME [epoch: 9.07 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31093398558224367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31093398558224367 | validation: 0.2871881080173716]
	TIME [epoch: 9.06 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32062236814319645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32062236814319645 | validation: 0.3512717576365049]
	TIME [epoch: 9.09 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36261507522761993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36261507522761993 | validation: 0.4302929078185379]
	TIME [epoch: 9.07 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21288924066975778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21288924066975778 | validation: 0.17754070032366592]
	TIME [epoch: 9.06 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21435611030732044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21435611030732044 | validation: 1.3178473139601188]
	TIME [epoch: 9.06 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7879223052059556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7879223052059556 | validation: 2.252451249691918]
	TIME [epoch: 9.07 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6000767501122659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6000767501122659 | validation: 0.657788026451839]
	TIME [epoch: 9.08 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39375718003434346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39375718003434346 | validation: 0.20344242417147423]
	TIME [epoch: 9.07 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2313900637657884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2313900637657884 | validation: 0.3303259344571928]
	TIME [epoch: 9.06 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2551315752098483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2551315752098483 | validation: 0.4065216554571631]
	TIME [epoch: 9.06 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30248750540149355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30248750540149355 | validation: 0.1627791701916606]
	TIME [epoch: 9.09 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15746820164866032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15746820164866032 | validation: 0.26313507796620145]
	TIME [epoch: 9.07 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23363759690380825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23363759690380825 | validation: 0.19266908573865799]
	TIME [epoch: 9.08 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23726813355350954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23726813355350954 | validation: 0.24151975530781572]
	TIME [epoch: 9.07 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26816774025347867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26816774025347867 | validation: 0.1945449703630735]
	TIME [epoch: 9.07 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21746504485240106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21746504485240106 | validation: 0.22657520685742125]
	TIME [epoch: 9.09 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20117908729541362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20117908729541362 | validation: 0.21623652846375502]
	TIME [epoch: 9.07 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34158123804154217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34158123804154217 | validation: 0.33621614430956737]
	TIME [epoch: 9.06 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19176391333896606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19176391333896606 | validation: 0.10516837591779954]
	TIME [epoch: 9.06 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21409416596824093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21409416596824093 | validation: 0.15366966453625727]
	TIME [epoch: 9.06 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17406251032408965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17406251032408965 | validation: 0.17118389274463935]
	TIME [epoch: 9.09 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1864398316586457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1864398316586457 | validation: 0.14632765388485097]
	TIME [epoch: 9.06 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2188541358635277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2188541358635277 | validation: 0.18208394525915245]
	TIME [epoch: 9.07 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2530896680160257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2530896680160257 | validation: 0.29498887007872643]
	TIME [epoch: 9.07 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49205584483277154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49205584483277154 | validation: 0.38562856914225]
	TIME [epoch: 9.08 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48561875498585305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48561875498585305 | validation: 0.21763041916625742]
	TIME [epoch: 9.07 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32922311998483855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32922311998483855 | validation: 0.16278350472009373]
	TIME [epoch: 9.06 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24933523009176897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24933523009176897 | validation: 0.1877981394335151]
	TIME [epoch: 9.07 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23811979349691192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23811979349691192 | validation: 0.28966338864230223]
	TIME [epoch: 9.06 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26310706787896604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26310706787896604 | validation: 0.5303786343563897]
	TIME [epoch: 9.08 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28046947684287615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28046947684287615 | validation: 0.2234199397007975]
	TIME [epoch: 9.06 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1927343176764526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1927343176764526 | validation: 0.18924832292941318]
	TIME [epoch: 9.06 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17775404097541087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17775404097541087 | validation: 0.18645029007472674]
	TIME [epoch: 9.07 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18581172512225924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18581172512225924 | validation: 1.222853400548963]
	TIME [epoch: 9.09 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9927282546713126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9927282546713126 | validation: 0.22328814523256962]
	TIME [epoch: 9.07 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21459345171783678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21459345171783678 | validation: 0.1453443464280074]
	TIME [epoch: 9.07 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20684711731253347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20684711731253347 | validation: 0.5744395929119763]
	TIME [epoch: 9.06 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48849406835886555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48849406835886555 | validation: 0.23027925168512775]
	TIME [epoch: 9.07 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22786468724525683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22786468724525683 | validation: 0.4354815912276594]
	TIME [epoch: 9.08 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5177021989356312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5177021989356312 | validation: 0.42326130747259827]
	TIME [epoch: 9.07 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30386370359508696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30386370359508696 | validation: 0.32881278121008606]
	TIME [epoch: 9.07 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5107604820094854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5107604820094854 | validation: 0.2434751311791216]
	TIME [epoch: 9.06 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3573390457924834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3573390457924834 | validation: 0.3402175339447481]
	TIME [epoch: 9.07 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.313906722033623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.313906722033623 | validation: 0.15198600924968342]
	TIME [epoch: 9.09 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19389667856509513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19389667856509513 | validation: 0.19885976927682306]
	TIME [epoch: 9.06 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22866967917925973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22866967917925973 | validation: 0.3036393249982206]
	TIME [epoch: 9.07 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2519919218016444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2519919218016444 | validation: 0.20304917085978402]
	TIME [epoch: 9.06 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.404928528083937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.404928528083937 | validation: 0.4564483967707922]
	TIME [epoch: 9.08 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4735319069989032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4735319069989032 | validation: 0.30083907522786557]
	TIME [epoch: 9.07 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34436321086501037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34436321086501037 | validation: 0.21780474190520627]
	TIME [epoch: 9.07 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2088888673456905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2088888673456905 | validation: 0.37509129213734027]
	TIME [epoch: 9.06 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2761013143219155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2761013143219155 | validation: 0.2099511192395933]
	TIME [epoch: 9.06 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21948455562498861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21948455562498861 | validation: 0.23877035098255]
	TIME [epoch: 9.08 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20276445509592866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20276445509592866 | validation: 0.24834855195780647]
	TIME [epoch: 9.07 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1985744259869074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1985744259869074 | validation: 0.22383608654640308]
	TIME [epoch: 9.06 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2410587268990974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2410587268990974 | validation: 0.2556518255185849]
	TIME [epoch: 9.07 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18566717742547129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18566717742547129 | validation: 0.251817918822768]
	TIME [epoch: 9.09 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21744345963499448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21744345963499448 | validation: 0.13849046319919014]
	TIME [epoch: 9.07 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17494102636214087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17494102636214087 | validation: 0.1581002386458767]
	TIME [epoch: 9.06 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24505408590807334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24505408590807334 | validation: 0.14988782940233833]
	TIME [epoch: 9.06 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2896273431100817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2896273431100817 | validation: 0.11880590619530369]
	TIME [epoch: 9.06 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34194962512855315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34194962512855315 | validation: 0.42752355396358965]
	TIME [epoch: 9.09 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39259936204037477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39259936204037477 | validation: 0.19187876850093671]
	TIME [epoch: 9.06 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22350865667740055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22350865667740055 | validation: 0.3379950080207419]
	TIME [epoch: 9.06 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23766227510496446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23766227510496446 | validation: 0.23275192645328013]
	TIME [epoch: 9.06 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2712031236052895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2712031236052895 | validation: 0.18267380014158108]
	TIME [epoch: 9.07 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3850913931286286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3850913931286286 | validation: 0.29089308594820373]
	TIME [epoch: 9.08 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23745954343636305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23745954343636305 | validation: 0.2344794770147774]
	TIME [epoch: 9.06 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2679026290530931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2679026290530931 | validation: 0.21510676860449618]
	TIME [epoch: 9.06 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29214047248829395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29214047248829395 | validation: 0.20319478387152295]
	TIME [epoch: 9.06 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1513778571393493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1513778571393493 | validation: 0.20037993422056818]
	TIME [epoch: 9.07 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19414647604212654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19414647604212654 | validation: 0.33787881743557435]
	TIME [epoch: 9.07 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34233098222389147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34233098222389147 | validation: 0.3798650141799615]
	TIME [epoch: 9.06 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2404334390507466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2404334390507466 | validation: 0.3159133348399794]
	TIME [epoch: 9.06 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2047332793340623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2047332793340623 | validation: 0.4052266949294843]
	TIME [epoch: 9.06 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49864201806729735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49864201806729735 | validation: 0.201009157664973]
	TIME [epoch: 9.08 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21499252407760588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21499252407760588 | validation: 0.42901956312039613]
	TIME [epoch: 9.07 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9580688421498218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9580688421498218 | validation: 0.934414453789818]
	TIME [epoch: 9.07 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43288275122610964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43288275122610964 | validation: 0.2392680762037273]
	TIME [epoch: 9.09 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22211777259756174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22211777259756174 | validation: 0.3763153808693269]
	TIME [epoch: 9.09 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20586550391840838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20586550391840838 | validation: 0.17387768427142924]
	TIME [epoch: 9.09 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19324443150085038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19324443150085038 | validation: 0.26997227464707707]
	TIME [epoch: 9.06 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3992786289789147		[learning rate: 0.0099724]
	Learning Rate: 0.00997241
	LOSS [training: 0.3992786289789147 | validation: 0.2693436481587463]
	TIME [epoch: 9.07 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17844068122890255		[learning rate: 0.0099418]
	Learning Rate: 0.00994184
	LOSS [training: 0.17844068122890255 | validation: 0.20601767012682698]
	TIME [epoch: 9.07 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1580724875998742		[learning rate: 0.0099114]
	Learning Rate: 0.00991136
	LOSS [training: 0.1580724875998742 | validation: 0.11150718631324578]
	TIME [epoch: 9.07 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1528719925732343		[learning rate: 0.009881]
	Learning Rate: 0.00988098
	LOSS [training: 0.1528719925732343 | validation: 0.2555952969971323]
	TIME [epoch: 9.06 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2407021521389948		[learning rate: 0.0098507]
	Learning Rate: 0.00985069
	LOSS [training: 0.2407021521389948 | validation: 0.1733969824907382]
	TIME [epoch: 9.06 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19980816025763137		[learning rate: 0.0098205]
	Learning Rate: 0.00982049
	LOSS [training: 0.19980816025763137 | validation: 0.13283946084585854]
	TIME [epoch: 9.06 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18675805677824764		[learning rate: 0.0097904]
	Learning Rate: 0.00979039
	LOSS [training: 0.18675805677824764 | validation: 0.22020924306147097]
	TIME [epoch: 9.07 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28336109295712225		[learning rate: 0.0097604]
	Learning Rate: 0.00976038
	LOSS [training: 0.28336109295712225 | validation: 0.39488605254721865]
	TIME [epoch: 9.07 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22113032438013014		[learning rate: 0.0097305]
	Learning Rate: 0.00973046
	LOSS [training: 0.22113032438013014 | validation: 0.14603874917430792]
	TIME [epoch: 9.06 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1387999851032		[learning rate: 0.0097006]
	Learning Rate: 0.00970063
	LOSS [training: 0.1387999851032 | validation: 0.18148405930879202]
	TIME [epoch: 9.07 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1557287412193921		[learning rate: 0.0096709]
	Learning Rate: 0.00967089
	LOSS [training: 0.1557287412193921 | validation: 0.20976907421954324]
	TIME [epoch: 9.06 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23007178863839578		[learning rate: 0.0096412]
	Learning Rate: 0.00964125
	LOSS [training: 0.23007178863839578 | validation: 0.1888566734142668]
	TIME [epoch: 9.08 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22815076840936394		[learning rate: 0.0096117]
	Learning Rate: 0.0096117
	LOSS [training: 0.22815076840936394 | validation: 0.46246481603641676]
	TIME [epoch: 9.06 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3657017958700647		[learning rate: 0.0095822]
	Learning Rate: 0.00958223
	LOSS [training: 0.3657017958700647 | validation: 0.20482185016985877]
	TIME [epoch: 9.06 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34108823538687566		[learning rate: 0.0095529]
	Learning Rate: 0.00955286
	LOSS [training: 0.34108823538687566 | validation: 0.23351187698660608]
	TIME [epoch: 9.05 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24560091302422515		[learning rate: 0.0095236]
	Learning Rate: 0.00952357
	LOSS [training: 0.24560091302422515 | validation: 0.13479330607753565]
	TIME [epoch: 9.06 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16168023957768707		[learning rate: 0.0094944]
	Learning Rate: 0.00949438
	LOSS [training: 0.16168023957768707 | validation: 0.21590882188931654]
	TIME [epoch: 9.08 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18161044145452201		[learning rate: 0.0094653]
	Learning Rate: 0.00946528
	LOSS [training: 0.18161044145452201 | validation: 0.14536621835219543]
	TIME [epoch: 9.06 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16991744878204476		[learning rate: 0.0094363]
	Learning Rate: 0.00943626
	LOSS [training: 0.16991744878204476 | validation: 0.21835834787300362]
	TIME [epoch: 9.06 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18333952384412483		[learning rate: 0.0094073]
	Learning Rate: 0.00940734
	LOSS [training: 0.18333952384412483 | validation: 0.12752908640126487]
	TIME [epoch: 9.06 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20421973149096623		[learning rate: 0.0093785]
	Learning Rate: 0.0093785
	LOSS [training: 0.20421973149096623 | validation: 0.14818920307601563]
	TIME [epoch: 9.08 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19054147780650424		[learning rate: 0.0093497]
	Learning Rate: 0.00934975
	LOSS [training: 0.19054147780650424 | validation: 0.16854928000227515]
	TIME [epoch: 9.06 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15341951034145052		[learning rate: 0.0093211]
	Learning Rate: 0.00932109
	LOSS [training: 0.15341951034145052 | validation: 0.14433454085110292]
	TIME [epoch: 9.07 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1950175705886657		[learning rate: 0.0092925]
	Learning Rate: 0.00929252
	LOSS [training: 0.1950175705886657 | validation: 0.12296870216028226]
	TIME [epoch: 9.06 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15157776023031605		[learning rate: 0.009264]
	Learning Rate: 0.00926403
	LOSS [training: 0.15157776023031605 | validation: 0.14304543469474615]
	TIME [epoch: 9.07 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15028015289895497		[learning rate: 0.0092356]
	Learning Rate: 0.00923563
	LOSS [training: 0.15028015289895497 | validation: 0.2057839386643944]
	TIME [epoch: 9.08 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16365623876121602		[learning rate: 0.0092073]
	Learning Rate: 0.00920732
	LOSS [training: 0.16365623876121602 | validation: 0.17816530599497626]
	TIME [epoch: 9.07 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28769042183850146		[learning rate: 0.0091791]
	Learning Rate: 0.0091791
	LOSS [training: 0.28769042183850146 | validation: 0.17463664854836547]
	TIME [epoch: 9.06 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22557402643373906		[learning rate: 0.009151]
	Learning Rate: 0.00915096
	LOSS [training: 0.22557402643373906 | validation: 0.15150234309245592]
	TIME [epoch: 9.06 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19574278740776033		[learning rate: 0.0091229]
	Learning Rate: 0.00912291
	LOSS [training: 0.19574278740776033 | validation: 0.21014955934929874]
	TIME [epoch: 9.07 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23440413715544034		[learning rate: 0.0090949]
	Learning Rate: 0.00909494
	LOSS [training: 0.23440413715544034 | validation: 0.18931129147824374]
	TIME [epoch: 9.07 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20538997753072152		[learning rate: 0.0090671]
	Learning Rate: 0.00906706
	LOSS [training: 0.20538997753072152 | validation: 0.15755054771908128]
	TIME [epoch: 9.06 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22197832964040382		[learning rate: 0.0090393]
	Learning Rate: 0.00903927
	LOSS [training: 0.22197832964040382 | validation: 0.4859284990949687]
	TIME [epoch: 9.06 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.052365081698202		[learning rate: 0.0090116]
	Learning Rate: 0.00901156
	LOSS [training: 2.052365081698202 | validation: 3.1496916727454933]
	TIME [epoch: 9.06 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.210289899114154		[learning rate: 0.0089839]
	Learning Rate: 0.00898394
	LOSS [training: 2.210289899114154 | validation: 2.8871052792801173]
	TIME [epoch: 9.08 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9057618541727326		[learning rate: 0.0089564]
	Learning Rate: 0.0089564
	LOSS [training: 1.9057618541727326 | validation: 2.674878830711955]
	TIME [epoch: 9.07 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7661695999566007		[learning rate: 0.0089289]
	Learning Rate: 0.00892894
	LOSS [training: 1.7661695999566007 | validation: 2.4563736371543214]
	TIME [epoch: 9.06 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.724272807934416		[learning rate: 0.0089016]
	Learning Rate: 0.00890157
	LOSS [training: 1.724272807934416 | validation: 2.6260155400425385]
	TIME [epoch: 9.06 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6219470366825148		[learning rate: 0.0088743]
	Learning Rate: 0.00887428
	LOSS [training: 1.6219470366825148 | validation: 2.3601598769065113]
	TIME [epoch: 9.06 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5533032371700441		[learning rate: 0.0088471]
	Learning Rate: 0.00884708
	LOSS [training: 1.5533032371700441 | validation: 2.3049985630033154]
	TIME [epoch: 9.08 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6550387903132187		[learning rate: 0.00882]
	Learning Rate: 0.00881996
	LOSS [training: 1.6550387903132187 | validation: 2.52696388852297]
	TIME [epoch: 9.06 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.746650022413982		[learning rate: 0.0087929]
	Learning Rate: 0.00879292
	LOSS [training: 1.746650022413982 | validation: 2.1739222427277065]
	TIME [epoch: 9.06 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4799799543589187		[learning rate: 0.008766]
	Learning Rate: 0.00876597
	LOSS [training: 1.4799799543589187 | validation: 2.2074872926558116]
	TIME [epoch: 9.06 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4495918667609702		[learning rate: 0.0087391]
	Learning Rate: 0.0087391
	LOSS [training: 1.4495918667609702 | validation: 2.0676009747259876]
	TIME [epoch: 9.08 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4215990244435794		[learning rate: 0.0087123]
	Learning Rate: 0.00871231
	LOSS [training: 1.4215990244435794 | validation: 2.2154608892833574]
	TIME [epoch: 9.07 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4245553130221356		[learning rate: 0.0086856]
	Learning Rate: 0.0086856
	LOSS [training: 2.4245553130221356 | validation: 3.2886357997845317]
	TIME [epoch: 9.06 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.86364616654791		[learning rate: 0.008659]
	Learning Rate: 0.00865898
	LOSS [training: 2.86364616654791 | validation: 3.067701382144859]
	TIME [epoch: 9.07 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.804924705115927		[learning rate: 0.0086324]
	Learning Rate: 0.00863244
	LOSS [training: 2.804924705115927 | validation: 3.2444884927956723]
	TIME [epoch: 9.06 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0316606989988615		[learning rate: 0.008606]
	Learning Rate: 0.00860597
	LOSS [training: 3.0316606989988615 | validation: 3.4481846492794057]
	TIME [epoch: 9.07 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8431106340684265		[learning rate: 0.0085796]
	Learning Rate: 0.00857959
	LOSS [training: 2.8431106340684265 | validation: 3.6661607502502878]
	TIME [epoch: 9.07 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2484014039681854		[learning rate: 0.0085533]
	Learning Rate: 0.00855329
	LOSS [training: 3.2484014039681854 | validation: 3.6571747004405126]
	TIME [epoch: 9.06 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8270398192761435		[learning rate: 0.0085271]
	Learning Rate: 0.00852707
	LOSS [training: 3.8270398192761435 | validation: 5.166715883735266]
	TIME [epoch: 9.06 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.69945983293296		[learning rate: 0.0085009]
	Learning Rate: 0.00850093
	LOSS [training: 4.69945983293296 | validation: 4.095419116182124]
	TIME [epoch: 9.08 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.230528534596485		[learning rate: 0.0084749]
	Learning Rate: 0.00847488
	LOSS [training: 4.230528534596485 | validation: 4.860001949091908]
	TIME [epoch: 9.08 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.857574061087428		[learning rate: 0.0084489]
	Learning Rate: 0.0084489
	LOSS [training: 4.857574061087428 | validation: 5.388942683020069]
	TIME [epoch: 9.06 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.10169220044642		[learning rate: 0.008423]
	Learning Rate: 0.008423
	LOSS [training: 5.10169220044642 | validation: 5.64399124188607]
	TIME [epoch: 9.06 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.757266796501709		[learning rate: 0.0083972]
	Learning Rate: 0.00839718
	LOSS [training: 5.757266796501709 | validation: 6.3535451071707145]
	TIME [epoch: 9.06 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.488963469325472		[learning rate: 0.0083714]
	Learning Rate: 0.00837144
	LOSS [training: 6.488963469325472 | validation: 7.083942288058145]
	TIME [epoch: 9.08 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.972096525371869		[learning rate: 0.0083458]
	Learning Rate: 0.00834577
	LOSS [training: 6.972096525371869 | validation: 7.0614646177873475]
	TIME [epoch: 9.06 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.805638859061898		[learning rate: 0.0083202]
	Learning Rate: 0.00832019
	LOSS [training: 5.805638859061898 | validation: 4.594293539775483]
	TIME [epoch: 9.06 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3607283233979595		[learning rate: 0.0082947]
	Learning Rate: 0.00829469
	LOSS [training: 3.3607283233979595 | validation: 3.0418646740652138]
	TIME [epoch: 9.07 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.552056959739301		[learning rate: 0.0082693]
	Learning Rate: 0.00826926
	LOSS [training: 2.552056959739301 | validation: 3.6824370159967073]
	TIME [epoch: 9.06 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.177663838450873		[learning rate: 0.0082439]
	Learning Rate: 0.00824391
	LOSS [training: 2.177663838450873 | validation: 2.621604503839193]
	TIME [epoch: 9.08 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.21321499336274		[learning rate: 0.0082186]
	Learning Rate: 0.00821864
	LOSS [training: 2.21321499336274 | validation: 3.0806796622177313]
	TIME [epoch: 9.06 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.265902628321738		[learning rate: 0.0081934]
	Learning Rate: 0.00819345
	LOSS [training: 2.265902628321738 | validation: 3.154354230206128]
	TIME [epoch: 9.06 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9195892917021986		[learning rate: 0.0081683]
	Learning Rate: 0.00816833
	LOSS [training: 2.9195892917021986 | validation: 4.464209045553344]
	TIME [epoch: 9.06 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.105402520343717		[learning rate: 0.0081433]
	Learning Rate: 0.00814329
	LOSS [training: 3.105402520343717 | validation: 3.292071926395364]
	TIME [epoch: 9.07 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5541136759786953		[learning rate: 0.0081183]
	Learning Rate: 0.00811833
	LOSS [training: 2.5541136759786953 | validation: 3.0562252593033996]
	TIME [epoch: 9.06 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3791205850188195		[learning rate: 0.0080934]
	Learning Rate: 0.00809344
	LOSS [training: 2.3791205850188195 | validation: 3.116560679952988]
	TIME [epoch: 9.06 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.477131700493204		[learning rate: 0.0080686]
	Learning Rate: 0.00806863
	LOSS [training: 2.477131700493204 | validation: 3.0251592481459033]
	TIME [epoch: 9.05 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2545321642685194		[learning rate: 0.0080439]
	Learning Rate: 0.0080439
	LOSS [training: 2.2545321642685194 | validation: 2.7014893984196835]
	TIME [epoch: 9.06 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.091555375256948		[learning rate: 0.0080192]
	Learning Rate: 0.00801924
	LOSS [training: 2.091555375256948 | validation: 2.555314531485042]
	TIME [epoch: 9.08 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8291357470383134		[learning rate: 0.0079947]
	Learning Rate: 0.00799466
	LOSS [training: 1.8291357470383134 | validation: 2.25696283337071]
	TIME [epoch: 9.06 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5214750763647173		[learning rate: 0.0079702]
	Learning Rate: 0.00797015
	LOSS [training: 1.5214750763647173 | validation: 2.043122628053611]
	TIME [epoch: 9.05 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4485985978094882		[learning rate: 0.0079457]
	Learning Rate: 0.00794572
	LOSS [training: 1.4485985978094882 | validation: 2.1933991537934787]
	TIME [epoch: 9.06 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5198240392251248		[learning rate: 0.0079214]
	Learning Rate: 0.00792136
	LOSS [training: 1.5198240392251248 | validation: 2.2709326927791995]
	TIME [epoch: 9.07 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9628255271057227		[learning rate: 0.0078971]
	Learning Rate: 0.00789708
	LOSS [training: 1.9628255271057227 | validation: 2.9677801579189422]
	TIME [epoch: 9.07 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1214243884311688		[learning rate: 0.0078729]
	Learning Rate: 0.00787287
	LOSS [training: 2.1214243884311688 | validation: 2.915038569837602]
	TIME [epoch: 9.06 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8533024701008365		[learning rate: 0.0078487]
	Learning Rate: 0.00784874
	LOSS [training: 1.8533024701008365 | validation: 2.494875625733787]
	TIME [epoch: 9.06 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6171187785661516		[learning rate: 0.0078247]
	Learning Rate: 0.00782468
	LOSS [training: 1.6171187785661516 | validation: 2.5876723996645157]
	TIME [epoch: 9.06 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.843629890173802		[learning rate: 0.0078007]
	Learning Rate: 0.0078007
	LOSS [training: 1.843629890173802 | validation: 2.636509281589217]
	TIME [epoch: 9.08 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.760227215672996		[learning rate: 0.0077768]
	Learning Rate: 0.00777678
	LOSS [training: 1.760227215672996 | validation: 2.696540657134412]
	TIME [epoch: 9.06 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5085387745566696		[learning rate: 0.0077529]
	Learning Rate: 0.00775294
	LOSS [training: 1.5085387745566696 | validation: 1.9624586748369612]
	TIME [epoch: 9.06 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2333721591684053		[learning rate: 0.0077292]
	Learning Rate: 0.00772918
	LOSS [training: 1.2333721591684053 | validation: 1.8068586994744469]
	TIME [epoch: 9.06 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3457837833783113		[learning rate: 0.0077055]
	Learning Rate: 0.00770548
	LOSS [training: 1.3457837833783113 | validation: 1.8628877580839722]
	TIME [epoch: 9.06 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4090424513779516		[learning rate: 0.0076819]
	Learning Rate: 0.00768186
	LOSS [training: 1.4090424513779516 | validation: 4.404485830812109]
	TIME [epoch: 9.08 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.156799280962067		[learning rate: 0.0076583]
	Learning Rate: 0.00765832
	LOSS [training: 4.156799280962067 | validation: 4.685148355236346]
	TIME [epoch: 9.06 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.270627232246284		[learning rate: 0.0076348]
	Learning Rate: 0.00763484
	LOSS [training: 4.270627232246284 | validation: 6.37886291491403]
	TIME [epoch: 9.06 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.1815533811461085		[learning rate: 0.0076114]
	Learning Rate: 0.00761144
	LOSS [training: 6.1815533811461085 | validation: 6.516362210610513]
	TIME [epoch: 9.06 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.518340285276517		[learning rate: 0.0075881]
	Learning Rate: 0.0075881
	LOSS [training: 6.518340285276517 | validation: 5.136117059333759]
	TIME [epoch: 9.08 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.277485547328913		[learning rate: 0.0075648]
	Learning Rate: 0.00756484
	LOSS [training: 6.277485547328913 | validation: 6.783062997026583]
	TIME [epoch: 9.06 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.672446445893509		[learning rate: 0.0075417]
	Learning Rate: 0.00754165
	LOSS [training: 6.672446445893509 | validation: 6.760242158058912]
	TIME [epoch: 9.06 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.647122834547728		[learning rate: 0.0075185]
	Learning Rate: 0.00751854
	LOSS [training: 6.647122834547728 | validation: 6.823187008641092]
	TIME [epoch: 9.06 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.809274520250538		[learning rate: 0.0074955]
	Learning Rate: 0.00749549
	LOSS [training: 6.809274520250538 | validation: 6.798909309094244]
	TIME [epoch: 9.06 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.843093141463458		[learning rate: 0.0074725]
	Learning Rate: 0.00747251
	LOSS [training: 6.843093141463458 | validation: 7.075073486033024]
	TIME [epoch: 9.07 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.422681323977609		[learning rate: 0.0074496]
	Learning Rate: 0.00744961
	LOSS [training: 5.422681323977609 | validation: 3.953634274801356]
	TIME [epoch: 9.06 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.898586593751108		[learning rate: 0.0074268]
	Learning Rate: 0.00742677
	LOSS [training: 4.898586593751108 | validation: 6.87858051773685]
	TIME [epoch: 9.05 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.165200811574902		[learning rate: 0.007404]
	Learning Rate: 0.007404
	LOSS [training: 6.165200811574902 | validation: 4.89726522726579]
	TIME [epoch: 9.06 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.478488287704634		[learning rate: 0.0073813]
	Learning Rate: 0.00738131
	LOSS [training: 5.478488287704634 | validation: 4.50377979350603]
	TIME [epoch: 9.07 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.071656052214257		[learning rate: 0.0073587]
	Learning Rate: 0.00735868
	LOSS [training: 5.071656052214257 | validation: 4.055992833738271]
	TIME [epoch: 9.07 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6253508639058536		[learning rate: 0.0073361]
	Learning Rate: 0.00733612
	LOSS [training: 3.6253508639058536 | validation: 3.1562777364316954]
	TIME [epoch: 9.05 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.204473198339231		[learning rate: 0.0073136]
	Learning Rate: 0.00731364
	LOSS [training: 4.204473198339231 | validation: 5.755346024572937]
	TIME [epoch: 9.06 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.1987080701559405		[learning rate: 0.0072912]
	Learning Rate: 0.00729122
	LOSS [training: 5.1987080701559405 | validation: 5.633509712189639]
	TIME [epoch: 9.06 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.036371387825851		[learning rate: 0.0072689]
	Learning Rate: 0.00726887
	LOSS [training: 5.036371387825851 | validation: 5.271923288077227]
	TIME [epoch: 9.08 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.521068729375072		[learning rate: 0.0072466]
	Learning Rate: 0.00724658
	LOSS [training: 4.521068729375072 | validation: 5.078984973352933]
	TIME [epoch: 9.06 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.304753005392188		[learning rate: 0.0072244]
	Learning Rate: 0.00722437
	LOSS [training: 4.304753005392188 | validation: 4.81050262515651]
	TIME [epoch: 9.06 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.284699127997536		[learning rate: 0.0072022]
	Learning Rate: 0.00720222
	LOSS [training: 4.284699127997536 | validation: 4.974214479838972]
	TIME [epoch: 9.06 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.614968164023238		[learning rate: 0.0071801]
	Learning Rate: 0.00718015
	LOSS [training: 4.614968164023238 | validation: 5.28620428303747]
	TIME [epoch: 9.06 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.585026717112923		[learning rate: 0.0071581]
	Learning Rate: 0.00715814
	LOSS [training: 4.585026717112923 | validation: 5.206610409040831]
	TIME [epoch: 9.08 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.655767342440843		[learning rate: 0.0071362]
	Learning Rate: 0.00713619
	LOSS [training: 4.655767342440843 | validation: 5.2253376197213495]
	TIME [epoch: 9.06 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.462249645387676		[learning rate: 0.0071143]
	Learning Rate: 0.00711432
	LOSS [training: 4.462249645387676 | validation: 4.796807849883605]
	TIME [epoch: 9.06 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.60328791635262		[learning rate: 0.0070925]
	Learning Rate: 0.00709251
	LOSS [training: 4.60328791635262 | validation: 4.992103191672364]
	TIME [epoch: 9.06 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.153538587842847		[learning rate: 0.0070708]
	Learning Rate: 0.00707077
	LOSS [training: 4.153538587842847 | validation: 4.453569679333113]
	TIME [epoch: 9.07 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7502435638816882		[learning rate: 0.0070491]
	Learning Rate: 0.00704909
	LOSS [training: 3.7502435638816882 | validation: 3.9116092413149066]
	TIME [epoch: 9.06 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5432125119237847		[learning rate: 0.0070275]
	Learning Rate: 0.00702749
	LOSS [training: 3.5432125119237847 | validation: 3.6140308333781386]
	TIME [epoch: 9.06 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2924187218568073		[learning rate: 0.0070059]
	Learning Rate: 0.00700594
	LOSS [training: 3.2924187218568073 | validation: 3.4857003602337704]
	TIME [epoch: 9.06 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.77028023510053		[learning rate: 0.0069845]
	Learning Rate: 0.00698447
	LOSS [training: 3.77028023510053 | validation: 4.888140210663195]
	TIME [epoch: 9.06 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.559091713101045		[learning rate: 0.0069631]
	Learning Rate: 0.00696306
	LOSS [training: 5.559091713101045 | validation: 4.52668910894562]
	TIME [epoch: 9.07 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.018865362527295		[learning rate: 0.0069417]
	Learning Rate: 0.00694171
	LOSS [training: 5.018865362527295 | validation: 4.838168389042885]
	TIME [epoch: 9.06 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.341252326547359		[learning rate: 0.0069204]
	Learning Rate: 0.00692043
	LOSS [training: 3.341252326547359 | validation: 3.1870317629339526]
	TIME [epoch: 9.06 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2166873022197335		[learning rate: 0.0068992]
	Learning Rate: 0.00689922
	LOSS [training: 3.2166873022197335 | validation: 6.003537345468232]
	TIME [epoch: 9.05 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9441263489199705		[learning rate: 0.0068781]
	Learning Rate: 0.00687807
	LOSS [training: 3.9441263489199705 | validation: 3.3857445091435565]
	TIME [epoch: 9.08 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.037370604166361		[learning rate: 0.006857]
	Learning Rate: 0.00685699
	LOSS [training: 3.037370604166361 | validation: 5.140886400085535]
	TIME [epoch: 9.06 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.109352254032207		[learning rate: 0.006836]
	Learning Rate: 0.00683597
	LOSS [training: 4.109352254032207 | validation: 2.8668953539426267]
	TIME [epoch: 9.06 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.53183342314538		[learning rate: 0.006815]
	Learning Rate: 0.00681501
	LOSS [training: 2.53183342314538 | validation: 3.0372546807853364]
	TIME [epoch: 9.06 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1181492566420537		[learning rate: 0.0067941]
	Learning Rate: 0.00679412
	LOSS [training: 3.1181492566420537 | validation: 3.3241008602080147]
	TIME [epoch: 9.05 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9685648966408165		[learning rate: 0.0067733]
	Learning Rate: 0.00677329
	LOSS [training: 2.9685648966408165 | validation: 2.595527906458366]
	TIME [epoch: 9.07 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.048574490049115		[learning rate: 0.0067525]
	Learning Rate: 0.00675253
	LOSS [training: 3.048574490049115 | validation: 3.827314403845949]
	TIME [epoch: 9.05 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.469293110821769		[learning rate: 0.0067318]
	Learning Rate: 0.00673183
	LOSS [training: 3.469293110821769 | validation: 5.29540114412259]
	TIME [epoch: 9.06 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/10] avg loss: nan		[learning rate: 0.0067112]
ERROR:
nan encountered in epoch 629 (training loss).
