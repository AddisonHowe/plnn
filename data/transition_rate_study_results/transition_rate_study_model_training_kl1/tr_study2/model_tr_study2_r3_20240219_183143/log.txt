Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r3', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 19527277

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.682161627647485		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.845795441762418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.263978534704952 | validation: 9.197904985337438]
	TIME [epoch: 53.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.010056622510199		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.624760107509305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.817408365009752 | validation: 5.97166373546067]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.251594702905754		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8602275842024163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.055911143554085 | validation: 2.4063991002783265]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.055290224874434		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3554118912357214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7053510580550775 | validation: 2.596882132583253]
	TIME [epoch: 8.2 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.881938521497581		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.015076369005638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4485074452516096 | validation: 1.4566796467668257]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7300125362113594		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7622778879892416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7461452121003007 | validation: 1.2991759446474453]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4039465656367387		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6119931916250372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5079698786308877 | validation: 1.1371596224338338]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4365365889581303		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.531739120638933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4841378547985316 | validation: 2.186401232204238]
	TIME [epoch: 8.19 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5685278173766544		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5574397105232634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5629837639499589 | validation: 1.600390751224741]
	TIME [epoch: 8.22 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.409832634995624		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0847908007710543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2473117178833388 | validation: 2.7195148654448857]
	TIME [epoch: 8.21 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5184199159391887		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4743519606632838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4963859383012363 | validation: 1.488446684600941]
	TIME [epoch: 8.2 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.227009709805686		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0828427870891286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1549262484474072 | validation: 1.6743472889954092]
	TIME [epoch: 8.2 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1460777544291334		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1350531955567924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1405654749929628 | validation: 0.8167058985094935]
	TIME [epoch: 8.26 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1071951708074148		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0551140279730995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0811545993902572 | validation: 0.6654465821775328]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9189111163117071		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8977318213419464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9083214688268267 | validation: 0.6889135978242245]
	TIME [epoch: 8.21 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8033741897437647		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7949153200645285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7991447549041467 | validation: 0.6229964353793261]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8231525605081309		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7955130600666848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8093328102874079 | validation: 0.4070045748893466]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8188684742028209		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7829476898021275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8009080820024741 | validation: 0.6307390296664829]
	TIME [epoch: 8.2 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6020507116143443		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6255069988294089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6137788552218766 | validation: 1.4853045931247981]
	TIME [epoch: 8.19 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7930434766788055		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5802313963503132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6866374365145594 | validation: 0.6543882023735296]
	TIME [epoch: 8.23 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6260241975177137		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5812299142433122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6036270558805129 | validation: 0.4298117291443557]
	TIME [epoch: 8.22 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5398438189793567		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.739852191176663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6398480050780099 | validation: 0.45210695992015015]
	TIME [epoch: 8.19 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6313177172011368		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46220145320215433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5467595852016457 | validation: 0.3261429709752402]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48864944527727366		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5396690686166388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5141592569469562 | validation: 0.4064250527251343]
	TIME [epoch: 8.2 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5239824161505492		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5024261195995999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5132042678750743 | validation: 0.3916341150090912]
	TIME [epoch: 8.22 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.580400004361241		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43441740458136646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5074087044713036 | validation: 0.46905517429932586]
	TIME [epoch: 8.2 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.660065887811971		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45734564370456826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5587057657582697 | validation: 0.5966876652203965]
	TIME [epoch: 8.19 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5308709317841266		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.647233221210012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5890520764970694 | validation: 0.26844176744324927]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44501759063039203		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6774817290419738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5612496598361829 | validation: 0.4762339574390246]
	TIME [epoch: 8.23 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4723897786141797		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4299727992819136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4511812889480466 | validation: 0.4529979175957377]
	TIME [epoch: 8.18 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5560271971091132		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.506535360147068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5312812786280907 | validation: 0.32215932487914334]
	TIME [epoch: 8.18 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48131006895798845		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3391150376291574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41021255329357287 | validation: 0.45830990842800917]
	TIME [epoch: 8.19 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5414936163225348		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3806786166075574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4610861164650462 | validation: 0.3790697652365471]
	TIME [epoch: 8.22 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37437013564527927		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.538044259749323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4562071976973012 | validation: 0.4313587858103097]
	TIME [epoch: 8.19 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38195147576727		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4040082150931405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3929798454302053 | validation: 0.3843141170708565]
	TIME [epoch: 8.19 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3888441342099091		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6250025014161711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5069233178130401 | validation: 0.21203065622952683]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4223968539095774		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40133035520576826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41186360455767285 | validation: 0.417680961550277]
	TIME [epoch: 8.23 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3286356639580126		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30401894598934165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3163273049736771 | validation: 0.5945417540611435]
	TIME [epoch: 8.18 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34488070720453834		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.33217666093822906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3385286840713837 | validation: 0.24789168157968713]
	TIME [epoch: 8.18 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3371425761095492		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3827390095739817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35994079284176556 | validation: 0.7117385732626919]
	TIME [epoch: 8.19 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47949619880670424		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4753336047277806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4774149017672424 | validation: 0.3570620602513638]
	TIME [epoch: 8.21 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.534154669714253		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5951225651556948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5646386174349738 | validation: 0.491916320441628]
	TIME [epoch: 8.22 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3655025863894788		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.35330381082585643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35940319860766756 | validation: 0.24325277884205201]
	TIME [epoch: 8.2 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3034652518211414		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.35146690940402625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32746608061258387 | validation: 0.26860841589006235]
	TIME [epoch: 8.2 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3241904575747843		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3315132806776149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32785186912619957 | validation: 0.2782478922630463]
	TIME [epoch: 8.25 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5373647066002412		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.35414735360208566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4457560301011633 | validation: 0.3298946760463664]
	TIME [epoch: 8.2 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.376568923369112		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2444681875977852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31051855548344853 | validation: 0.1573766996727328]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38521259604238667		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2869342190737775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33607340755808207 | validation: 0.179736822537792]
	TIME [epoch: 8.19 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3910489092507438		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3696720176941611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38036046347245245 | validation: 0.25800369583773053]
	TIME [epoch: 8.19 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28874552003400766		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3190535145646439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3038995172993258 | validation: 0.2638556409912356]
	TIME [epoch: 8.22 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2791830807623504		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.37967218793471574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32942763434853306 | validation: 0.19141078661730845]
	TIME [epoch: 8.2 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27442983970643736		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32246833350274084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29844908660458913 | validation: 0.514285178344201]
	TIME [epoch: 8.19 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29386589033733285		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3577573327440674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3258116115407001 | validation: 0.48467235800509423]
	TIME [epoch: 8.2 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32566965779369306		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30992172870313667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31779569324841483 | validation: 0.13387556754640675]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28539155411176786		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2664582693354423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2759249117236051 | validation: 0.18280233536403934]
	TIME [epoch: 8.19 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26417985858071047		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3543001649275969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30924001175415367 | validation: 0.17001382957659172]
	TIME [epoch: 8.18 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25315942588609996		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2596001838493464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2563798048677232 | validation: 0.1355306455010176]
	TIME [epoch: 8.17 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22483205103152737		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2584839876232485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24165801932738798 | validation: 0.26834018604923143]
	TIME [epoch: 8.2 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23931281328246898		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25381911511682825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24656596419964857 | validation: 0.25318711759247114]
	TIME [epoch: 8.22 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24412793590479084		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2273588700151878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23574340295998933 | validation: 0.18620321411198912]
	TIME [epoch: 8.19 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16780314094351761		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26386422234344387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2158336816434807 | validation: 0.09090190924866184]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1602641444661099		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14674203939188854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1535030919289992 | validation: 0.3258308717025803]
	TIME [epoch: 8.23 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20400973312251156		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2038161826996927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2039129579111021 | validation: 0.12364585620160463]
	TIME [epoch: 8.26 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18640806403056215		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2302241823437344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2083161231871482 | validation: 0.33245235246036187]
	TIME [epoch: 8.21 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19454670449398354		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2075052071277849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20102595581088423 | validation: 0.17084153296093516]
	TIME [epoch: 8.21 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20765579302430295		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24767435954574082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22766507628502186 | validation: 0.12571875469782892]
	TIME [epoch: 8.21 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18690681395909942		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18364643432073724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18527662413991836 | validation: 0.36368120459330694]
	TIME [epoch: 8.25 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47157657840405187		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24781608639187583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35969633239796395 | validation: 0.24048479311351212]
	TIME [epoch: 8.23 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24299870640079257		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5341444930294031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3885715997150978 | validation: 0.16543693983856209]
	TIME [epoch: 8.21 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1837779266265362		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21321860777607632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1984982672013063 | validation: 0.09834726308619529]
	TIME [epoch: 8.24 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15938830941669782		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2680780903678213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21373319989225964 | validation: 0.3086176421920102]
	TIME [epoch: 8.22 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20696896217690197		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3190851419169445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2630270520469232 | validation: 0.4560470764784671]
	TIME [epoch: 8.23 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40963665393393967		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22704260009225058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31833962701309515 | validation: 0.12222542990815012]
	TIME [epoch: 8.21 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15480185127677054		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20625843180685166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18053014154181116 | validation: 0.25676038627513753]
	TIME [epoch: 8.2 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21211948537731734		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1576063152853524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18486290033133485 | validation: 0.0702069379065863]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13795410146852555		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5717551412531596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35485462136084256 | validation: 0.6086431318666433]
	TIME [epoch: 8.24 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29017473568811303		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18575453420546081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2379646349467869 | validation: 1.5542411934339093]
	TIME [epoch: 8.21 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4228333618961438		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16204914675501605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29244125432557994 | validation: 0.14809969659811614]
	TIME [epoch: 8.24 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21026503619583617		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16190812597024223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1860865810830392 | validation: 0.0583275852243562]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32981288675783565		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15513478897674918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24247383786729243 | validation: 0.16298518123293543]
	TIME [epoch: 8.21 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20975707337210422		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19185532009446704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20080619673328562 | validation: 0.20536726522815313]
	TIME [epoch: 8.2 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15856153472558598		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18704293946324282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1728022370944144 | validation: 0.23301832055192695]
	TIME [epoch: 8.19 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26513031842109985		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08937910479513772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17725471160811876 | validation: 0.0903366002793522]
	TIME [epoch: 8.2 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.268746104071946		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1746910386033082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22171857133762712 | validation: 0.14073008395307657]
	TIME [epoch: 8.21 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18719355962314674		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1856043735143343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1863989665687405 | validation: 0.2385810193948115]
	TIME [epoch: 8.23 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2538631644857872		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27645305311649104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2651581088011391 | validation: 0.3208121706966309]
	TIME [epoch: 8.2 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2063358126205695		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14972463853209964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17803022557633458 | validation: 0.11028934645637017]
	TIME [epoch: 8.21 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13398540610636864		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18732824219880997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1606568241525893 | validation: 0.3248574990129616]
	TIME [epoch: 8.23 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2696595789221844		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1742804969445218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22197003793335307 | validation: 0.17665117567823077]
	TIME [epoch: 8.22 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14200393008601833		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1423322101033481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14216807009468324 | validation: 0.12898258182282318]
	TIME [epoch: 8.2 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.170209920748609		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18922947556241151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1797196981555103 | validation: 0.14762747463369758]
	TIME [epoch: 8.19 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16532976157403936		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15254351949859646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1589366405363179 | validation: 0.1865327815087844]
	TIME [epoch: 8.19 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12432179110507333		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15266354841467616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13849266975987476 | validation: 0.06074841931701227]
	TIME [epoch: 8.21 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1532926613942474		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13599486360036578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14464376249730662 | validation: 0.09002005386320254]
	TIME [epoch: 8.22 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10447414659038197		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15444633141532033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12946023900285114 | validation: 0.12115533620333446]
	TIME [epoch: 8.2 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2098901224034803		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24832511895723441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22910762068035737 | validation: 0.12787047718537856]
	TIME [epoch: 8.22 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18216136714153236		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17573333618299872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17894735166226555 | validation: 0.12047505636616046]
	TIME [epoch: 8.19 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15192879448164556		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1683294488951954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16012912168842047 | validation: 0.10864147770332933]
	TIME [epoch: 8.21 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1440237994734495		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20984741144256533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1769356054580074 | validation: 0.15389083042439053]
	TIME [epoch: 8.18 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17791073442799052		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1650065532232428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17145864382561676 | validation: 0.0815719974846558]
	TIME [epoch: 8.19 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12448391858114446		[learning rate: 0.0099891]
		[batch 20/20] avg loss: 0.14128879740980324		[learning rate: 0.009977]
	Learning Rate: 0.009977
	LOSS [training: 0.13288635799547385 | validation: 0.14479329664594084]
	TIME [epoch: 8.19 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13583649994100958		[learning rate: 0.0099649]
		[batch 20/20] avg loss: 0.19681853895337242		[learning rate: 0.0099528]
	Learning Rate: 0.00995285
	LOSS [training: 0.16632751944719099 | validation: 0.16450330656158035]
	TIME [epoch: 8.23 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26090285336911945		[learning rate: 0.0099408]
		[batch 20/20] avg loss: 0.15045160941643465		[learning rate: 0.0099288]
	Learning Rate: 0.00992875
	LOSS [training: 0.20567723139277705 | validation: 0.14107572828791753]
	TIME [epoch: 8.2 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3097798574404069		[learning rate: 0.0099167]
		[batch 20/20] avg loss: 0.1539414785313901		[learning rate: 0.0099047]
	Learning Rate: 0.00990472
	LOSS [training: 0.23186066798589855 | validation: 0.10651354067265148]
	TIME [epoch: 8.19 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.115836559171446		[learning rate: 0.0098927]
		[batch 20/20] avg loss: 0.11238181611732186		[learning rate: 0.0098807]
	Learning Rate: 0.00988074
	LOSS [training: 0.11410918764438391 | validation: 0.1243754418480659]
	TIME [epoch: 8.23 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10979186681858907		[learning rate: 0.0098688]
		[batch 20/20] avg loss: 0.18180593874400405		[learning rate: 0.0098568]
	Learning Rate: 0.00985682
	LOSS [training: 0.1457989027812966 | validation: 0.06125280645737032]
	TIME [epoch: 8.18 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15522319439692886		[learning rate: 0.0098449]
		[batch 20/20] avg loss: 0.17000300382037023		[learning rate: 0.009833]
	Learning Rate: 0.00983296
	LOSS [training: 0.16261309910864957 | validation: 0.05741229742658514]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10605458857273928		[learning rate: 0.009821]
		[batch 20/20] avg loss: 0.17864833730305926		[learning rate: 0.0098092]
	Learning Rate: 0.00980915
	LOSS [training: 0.14235146293789924 | validation: 0.14365223829654472]
	TIME [epoch: 8.18 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16669279753065885		[learning rate: 0.0097973]
		[batch 20/20] avg loss: 0.11835972684076537		[learning rate: 0.0097854]
	Learning Rate: 0.00978541
	LOSS [training: 0.14252626218571213 | validation: 0.10474083517449628]
	TIME [epoch: 8.18 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13457086013804226		[learning rate: 0.0097736]
		[batch 20/20] avg loss: 0.19617767769810188		[learning rate: 0.0097617]
	Learning Rate: 0.00976172
	LOSS [training: 0.16537426891807208 | validation: 0.1481694792042392]
	TIME [epoch: 8.19 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13755304004151864		[learning rate: 0.0097499]
		[batch 20/20] avg loss: 0.10126757896964131		[learning rate: 0.0097381]
	Learning Rate: 0.00973809
	LOSS [training: 0.11941030950557999 | validation: 0.08963536574356228]
	TIME [epoch: 8.22 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1402093616657622		[learning rate: 0.0097263]
		[batch 20/20] avg loss: 0.13022967841379088		[learning rate: 0.0097145]
	Learning Rate: 0.00971451
	LOSS [training: 0.13521952003977658 | validation: 0.08985957114566612]
	TIME [epoch: 8.19 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13222004697144218		[learning rate: 0.0097027]
		[batch 20/20] avg loss: 0.1877815707752147		[learning rate: 0.009691]
	Learning Rate: 0.009691
	LOSS [training: 0.16000080887332846 | validation: 0.2526918890486075]
	TIME [epoch: 8.19 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17738300627513973		[learning rate: 0.0096793]
		[batch 20/20] avg loss: 0.3231600732299842		[learning rate: 0.0096675]
	Learning Rate: 0.00966754
	LOSS [training: 0.2502715397525619 | validation: 0.11698660373971388]
	TIME [epoch: 8.22 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13015211976774313		[learning rate: 0.0096558]
		[batch 20/20] avg loss: 0.1429366780623255		[learning rate: 0.0096441]
	Learning Rate: 0.00964413
	LOSS [training: 0.13654439891503434 | validation: 0.08062894419430992]
	TIME [epoch: 8.18 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.159263819762601		[learning rate: 0.0096325]
		[batch 20/20] avg loss: 0.15015866851108803		[learning rate: 0.0096208]
	Learning Rate: 0.00962078
	LOSS [training: 0.15471124413684456 | validation: 0.06891219071077484]
	TIME [epoch: 8.2 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16055315655004193		[learning rate: 0.0096091]
		[batch 20/20] avg loss: 0.17251445592819253		[learning rate: 0.0095975]
	Learning Rate: 0.00959749
	LOSS [training: 0.16653380623911723 | validation: 0.28775071663100277]
	TIME [epoch: 8.18 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1819061863161582		[learning rate: 0.0095859]
		[batch 20/20] avg loss: 0.11472361213102791		[learning rate: 0.0095743]
	Learning Rate: 0.00957426
	LOSS [training: 0.1483148992235931 | validation: 0.09739036246776711]
	TIME [epoch: 8.18 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1324908315153135		[learning rate: 0.0095627]
		[batch 20/20] avg loss: 0.13096926507752057		[learning rate: 0.0095511]
	Learning Rate: 0.00955108
	LOSS [training: 0.13173004829641705 | validation: 0.16506860738249235]
	TIME [epoch: 8.2 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43499320711075845		[learning rate: 0.0095395]
		[batch 20/20] avg loss: 0.4963360397188971		[learning rate: 0.009528]
	Learning Rate: 0.00952796
	LOSS [training: 0.4656646234148278 | validation: 0.16081331659741122]
	TIME [epoch: 8.21 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29659036099711644		[learning rate: 0.0095164]
		[batch 20/20] avg loss: 0.1364773286360336		[learning rate: 0.0095049]
	Learning Rate: 0.0095049
	LOSS [training: 0.216533844816575 | validation: 0.11063448906713855]
	TIME [epoch: 8.19 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12252328522302622		[learning rate: 0.0094934]
		[batch 20/20] avg loss: 0.25904202736372606		[learning rate: 0.0094819]
	Learning Rate: 0.00948189
	LOSS [training: 0.1907826562933761 | validation: 0.1595852150989328]
	TIME [epoch: 8.19 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18725110611921822		[learning rate: 0.0094704]
		[batch 20/20] avg loss: 0.14324268798617099		[learning rate: 0.0094589]
	Learning Rate: 0.00945893
	LOSS [training: 0.16524689705269463 | validation: 0.10464030028601648]
	TIME [epoch: 8.22 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1592575948808755		[learning rate: 0.0094475]
		[batch 20/20] avg loss: 0.207057837276875		[learning rate: 0.009436]
	Learning Rate: 0.00943603
	LOSS [training: 0.1831577160788753 | validation: 0.17690370059898225]
	TIME [epoch: 8.2 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12489977819142098		[learning rate: 0.0094246]
		[batch 20/20] avg loss: 0.11156126526121009		[learning rate: 0.0094132]
	Learning Rate: 0.00941319
	LOSS [training: 0.11823052172631554 | validation: 0.13134328445534846]
	TIME [epoch: 8.19 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14170209960992206		[learning rate: 0.0094018]
		[batch 20/20] avg loss: 0.13515264984606962		[learning rate: 0.0093904]
	Learning Rate: 0.0093904
	LOSS [training: 0.13842737472799585 | validation: 0.07161467184071416]
	TIME [epoch: 8.18 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11722247031327337		[learning rate: 0.009379]
		[batch 20/20] avg loss: 0.19852975873993337		[learning rate: 0.0093677]
	Learning Rate: 0.00936767
	LOSS [training: 0.15787611452660336 | validation: 0.2340547963355908]
	TIME [epoch: 8.2 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26048990300122454		[learning rate: 0.0093563]
		[batch 20/20] avg loss: 0.224187630768185		[learning rate: 0.009345]
	Learning Rate: 0.00934499
	LOSS [training: 0.24233876688470465 | validation: 0.2123796070606741]
	TIME [epoch: 8.18 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13359777381920418		[learning rate: 0.0093337]
		[batch 20/20] avg loss: 0.10736942633837647		[learning rate: 0.0093224]
	Learning Rate: 0.00932237
	LOSS [training: 0.1204836000787903 | validation: 0.11503960697690477]
	TIME [epoch: 8.21 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16583403913923642		[learning rate: 0.0093111]
		[batch 20/20] avg loss: 0.2103567075181112		[learning rate: 0.0092998]
	Learning Rate: 0.0092998
	LOSS [training: 0.18809537332867382 | validation: 0.22128581302529596]
	TIME [epoch: 8.22 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19923378897642735		[learning rate: 0.0092885]
		[batch 20/20] avg loss: 0.30933914015021186		[learning rate: 0.0092773]
	Learning Rate: 0.00927729
	LOSS [training: 0.25428646456331966 | validation: 0.11450827777456722]
	TIME [epoch: 8.2 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12369274189679424		[learning rate: 0.0092661]
		[batch 20/20] avg loss: 0.48856612824449747		[learning rate: 0.0092548]
	Learning Rate: 0.00925483
	LOSS [training: 0.3061294350706459 | validation: 0.7624865808598308]
	TIME [epoch: 8.18 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39744763367765623		[learning rate: 0.0092436]
		[batch 20/20] avg loss: 0.5127668979373086		[learning rate: 0.0092324]
	Learning Rate: 0.00923242
	LOSS [training: 0.45510726580748245 | validation: 0.2635708168827705]
	TIME [epoch: 8.21 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26872257875259686		[learning rate: 0.0092212]
		[batch 20/20] avg loss: 0.7560555714636263		[learning rate: 0.0092101]
	Learning Rate: 0.00921007
	LOSS [training: 0.5123890751081116 | validation: 0.40043535764947663]
	TIME [epoch: 8.21 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2156882201748691		[learning rate: 0.0091989]
		[batch 20/20] avg loss: 0.1443127626784914		[learning rate: 0.0091878]
	Learning Rate: 0.00918778
	LOSS [training: 0.1800004914266803 | validation: 0.0732479967232632]
	TIME [epoch: 8.19 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12354885805801248		[learning rate: 0.0091767]
		[batch 20/20] avg loss: 0.26424485625633415		[learning rate: 0.0091655]
	Learning Rate: 0.00916554
	LOSS [training: 0.1938968571571733 | validation: 0.09500189877135531]
	TIME [epoch: 8.2 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11853436449860741		[learning rate: 0.0091544]
		[batch 20/20] avg loss: 0.11522826697244559		[learning rate: 0.0091433]
	Learning Rate: 0.00914335
	LOSS [training: 0.11688131573552647 | validation: 0.2294750818622647]
	TIME [epoch: 8.2 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.223433645205301		[learning rate: 0.0091323]
		[batch 20/20] avg loss: 0.18422429737976245		[learning rate: 0.0091212]
	Learning Rate: 0.00912121
	LOSS [training: 0.20382897129253172 | validation: 0.129626665281508]
	TIME [epoch: 8.25 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10453341166884525		[learning rate: 0.0091102]
		[batch 20/20] avg loss: 0.089600471013256		[learning rate: 0.0090991]
	Learning Rate: 0.00909913
	LOSS [training: 0.09706694134105064 | validation: 0.053460741296408695]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12528133050566959		[learning rate: 0.0090881]
		[batch 20/20] avg loss: 0.12956347681133504		[learning rate: 0.0090771]
	Learning Rate: 0.0090771
	LOSS [training: 0.12742240365850233 | validation: 0.19512309176600223]
	TIME [epoch: 8.19 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11573999111891306		[learning rate: 0.0090661]
		[batch 20/20] avg loss: 0.1238917519469018		[learning rate: 0.0090551]
	Learning Rate: 0.00905513
	LOSS [training: 0.11981587153290743 | validation: 0.08406758115360259]
	TIME [epoch: 8.18 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14802374511166566		[learning rate: 0.0090442]
		[batch 20/20] avg loss: 0.15798059908729556		[learning rate: 0.0090332]
	Learning Rate: 0.00903321
	LOSS [training: 0.1530021720994806 | validation: 0.08635394456678244]
	TIME [epoch: 8.21 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14028747183838833		[learning rate: 0.0090223]
		[batch 20/20] avg loss: 0.17037690429629618		[learning rate: 0.0090113]
	Learning Rate: 0.00901134
	LOSS [training: 0.15533218806734223 | validation: 0.09964447376690115]
	TIME [epoch: 8.2 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11609306936298217		[learning rate: 0.0090004]
		[batch 20/20] avg loss: 0.1520640304057475		[learning rate: 0.0089895]
	Learning Rate: 0.00898953
	LOSS [training: 0.13407854988436482 | validation: 0.10379210449597406]
	TIME [epoch: 8.2 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13165386664297696		[learning rate: 0.0089786]
		[batch 20/20] avg loss: 0.10034445211552087		[learning rate: 0.0089678]
	Learning Rate: 0.00896776
	LOSS [training: 0.11599915937924891 | validation: 0.047394794620279276]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1281287832978318		[learning rate: 0.0089569]
		[batch 20/20] avg loss: 0.17885242744830931		[learning rate: 0.0089461]
	Learning Rate: 0.00894605
	LOSS [training: 0.15349060537307052 | validation: 0.10639487995554296]
	TIME [epoch: 8.21 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14834331337015125		[learning rate: 0.0089352]
		[batch 20/20] avg loss: 0.10192861369208431		[learning rate: 0.0089244]
	Learning Rate: 0.0089244
	LOSS [training: 0.1251359635311178 | validation: 0.10685330396780336]
	TIME [epoch: 8.23 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18445564554077085		[learning rate: 0.0089136]
		[batch 20/20] avg loss: 0.14842048552129		[learning rate: 0.0089028]
	Learning Rate: 0.00890279
	LOSS [training: 0.1664380655310304 | validation: 0.12200530688465902]
	TIME [epoch: 8.18 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21278069917564885		[learning rate: 0.008892]
		[batch 20/20] avg loss: 0.09839186549388622		[learning rate: 0.0088812]
	Learning Rate: 0.00888124
	LOSS [training: 0.15558628233476754 | validation: 0.11495135441265744]
	TIME [epoch: 8.18 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1254110441365084		[learning rate: 0.0088705]
		[batch 20/20] avg loss: 0.0962376189259045		[learning rate: 0.0088597]
	Learning Rate: 0.00885974
	LOSS [training: 0.11082433153120644 | validation: 0.07817833667566745]
	TIME [epoch: 8.17 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11102686009999178		[learning rate: 0.008849]
		[batch 20/20] avg loss: 0.16151343226030074		[learning rate: 0.0088383]
	Learning Rate: 0.00883829
	LOSS [training: 0.13627014618014624 | validation: 0.13443296796244739]
	TIME [epoch: 8.22 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16799787210225153		[learning rate: 0.0088276]
		[batch 20/20] avg loss: 0.14757038430882885		[learning rate: 0.0088169]
	Learning Rate: 0.0088169
	LOSS [training: 0.1577841282055402 | validation: 0.11664670801602457]
	TIME [epoch: 8.19 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14409487330486687		[learning rate: 0.0088062]
		[batch 20/20] avg loss: 0.14937936656716339		[learning rate: 0.0087956]
	Learning Rate: 0.00879555
	LOSS [training: 0.14673711993601513 | validation: 0.13853401116604822]
	TIME [epoch: 8.19 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15155720345042856		[learning rate: 0.0087849]
		[batch 20/20] avg loss: 0.09089864524217134		[learning rate: 0.0087743]
	Learning Rate: 0.00877426
	LOSS [training: 0.12122792434629996 | validation: 0.06412566473027949]
	TIME [epoch: 8.19 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12663170366664728		[learning rate: 0.0087636]
		[batch 20/20] avg loss: 0.12224665538549154		[learning rate: 0.008753]
	Learning Rate: 0.00875302
	LOSS [training: 0.12443917952606938 | validation: 0.06361361403189868]
	TIME [epoch: 8.24 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07344340118063493		[learning rate: 0.0087424]
		[batch 20/20] avg loss: 0.14729965614015333		[learning rate: 0.0087318]
	Learning Rate: 0.00873183
	LOSS [training: 0.11037152866039415 | validation: 0.21984092880920947]
	TIME [epoch: 8.19 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15222862667408893		[learning rate: 0.0087213]
		[batch 20/20] avg loss: 0.09817072882408298		[learning rate: 0.0087107]
	Learning Rate: 0.00871069
	LOSS [training: 0.12519967774908594 | validation: 0.07365602697124427]
	TIME [epoch: 8.18 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09331358647013502		[learning rate: 0.0087001]
		[batch 20/20] avg loss: 0.1546913326359005		[learning rate: 0.0086896]
	Learning Rate: 0.0086896
	LOSS [training: 0.12400245955301777 | validation: 0.12368955239503773]
	TIME [epoch: 8.18 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09897766655060286		[learning rate: 0.0086791]
		[batch 20/20] avg loss: 0.13214805002339008		[learning rate: 0.0086686]
	Learning Rate: 0.00866857
	LOSS [training: 0.11556285828699649 | validation: 0.041749064237785155]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10522689417620446		[learning rate: 0.0086581]
		[batch 20/20] avg loss: 0.16280799670937138		[learning rate: 0.0086476]
	Learning Rate: 0.00864758
	LOSS [training: 0.1340174454427879 | validation: 0.23633155324957653]
	TIME [epoch: 8.21 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13288375297247318		[learning rate: 0.0086371]
		[batch 20/20] avg loss: 0.15234961564280963		[learning rate: 0.0086266]
	Learning Rate: 0.00862665
	LOSS [training: 0.14261668430764143 | validation: 0.1165920087234892]
	TIME [epoch: 8.18 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18047120699195984		[learning rate: 0.0086162]
		[batch 20/20] avg loss: 0.13165248757196502		[learning rate: 0.0086058]
	Learning Rate: 0.00860576
	LOSS [training: 0.1560618472819624 | validation: 0.09572365670556854]
	TIME [epoch: 8.21 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14885750589783647		[learning rate: 0.0085953]
		[batch 20/20] avg loss: 0.24778155714262123		[learning rate: 0.0085849]
	Learning Rate: 0.00858493
	LOSS [training: 0.19831953152022883 | validation: 0.1421596597993678]
	TIME [epoch: 8.17 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17493593701855498		[learning rate: 0.0085745]
		[batch 20/20] avg loss: 0.25594984876940446		[learning rate: 0.0085641]
	Learning Rate: 0.00856415
	LOSS [training: 0.21544289289397972 | validation: 0.19277906089965993]
	TIME [epoch: 8.2 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17407530002159138		[learning rate: 0.0085538]
		[batch 20/20] avg loss: 0.14176716205527615		[learning rate: 0.0085434]
	Learning Rate: 0.00854342
	LOSS [training: 0.15792123103843375 | validation: 0.0835451810855818]
	TIME [epoch: 8.18 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22317117976543083		[learning rate: 0.0085331]
		[batch 20/20] avg loss: 0.10156315159147596		[learning rate: 0.0085227]
	Learning Rate: 0.00852273
	LOSS [training: 0.16236716567845338 | validation: 0.059954386931675374]
	TIME [epoch: 8.18 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13955583494583845		[learning rate: 0.0085124]
		[batch 20/20] avg loss: 0.10935639320597026		[learning rate: 0.0085021]
	Learning Rate: 0.0085021
	LOSS [training: 0.12445611407590433 | validation: 0.3000288609360352]
	TIME [epoch: 8.2 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1344432247125048		[learning rate: 0.0084918]
		[batch 20/20] avg loss: 0.1144476765158573		[learning rate: 0.0084815]
	Learning Rate: 0.00848152
	LOSS [training: 0.12444545061418104 | validation: 0.11179945376717948]
	TIME [epoch: 8.19 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12150813719711158		[learning rate: 0.0084712]
		[batch 20/20] avg loss: 0.16327217100443625		[learning rate: 0.008461]
	Learning Rate: 0.00846099
	LOSS [training: 0.14239015410077396 | validation: 0.074525512833179]
	TIME [epoch: 8.2 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11226255152537257		[learning rate: 0.0084507]
		[batch 20/20] avg loss: 0.20840547861030742		[learning rate: 0.0084405]
	Learning Rate: 0.0084405
	LOSS [training: 0.16033401506784 | validation: 0.061558373948723974]
	TIME [epoch: 8.22 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12690603655747315		[learning rate: 0.0084303]
		[batch 20/20] avg loss: 0.1253343900153112		[learning rate: 0.0084201]
	Learning Rate: 0.00842007
	LOSS [training: 0.12612021328639217 | validation: 0.14688858472617153]
	TIME [epoch: 8.18 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13780372287614112		[learning rate: 0.0084099]
		[batch 20/20] avg loss: 0.12711006267898778		[learning rate: 0.0083997]
	Learning Rate: 0.00839969
	LOSS [training: 0.13245689277756442 | validation: 0.042065016404858255]
	TIME [epoch: 8.17 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09928768050767217		[learning rate: 0.0083895]
		[batch 20/20] avg loss: 0.16152564333051306		[learning rate: 0.0083794]
	Learning Rate: 0.00837935
	LOSS [training: 0.13040666191909264 | validation: 0.21806411291929523]
	TIME [epoch: 8.2 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09963479277837671		[learning rate: 0.0083692]
		[batch 20/20] avg loss: 0.11337218758973595		[learning rate: 0.0083591]
	Learning Rate: 0.00835907
	LOSS [training: 0.10650349018405633 | validation: 0.2004398189575563]
	TIME [epoch: 8.19 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13238616683462245		[learning rate: 0.0083489]
		[batch 20/20] avg loss: 0.09269618542075578		[learning rate: 0.0083388]
	Learning Rate: 0.00833883
	LOSS [training: 0.1125411761276891 | validation: 0.06252041428101643]
	TIME [epoch: 8.2 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08280853397204677		[learning rate: 0.0083287]
		[batch 20/20] avg loss: 0.08507050291898298		[learning rate: 0.0083186]
	Learning Rate: 0.00831864
	LOSS [training: 0.0839395184455149 | validation: 0.04666207058333745]
	TIME [epoch: 8.18 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11692319353748097		[learning rate: 0.0083086]
		[batch 20/20] avg loss: 0.08576375424414072		[learning rate: 0.0082985]
	Learning Rate: 0.00829851
	LOSS [training: 0.10134347389081086 | validation: 0.043234441213233395]
	TIME [epoch: 8.21 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08314262623168198		[learning rate: 0.0082885]
		[batch 20/20] avg loss: 0.12882335819603305		[learning rate: 0.0082784]
	Learning Rate: 0.00827842
	LOSS [training: 0.10598299221385751 | validation: 0.04280412858924518]
	TIME [epoch: 8.22 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12710612216716669		[learning rate: 0.0082684]
		[batch 20/20] avg loss: 0.0936867852914914		[learning rate: 0.0082584]
	Learning Rate: 0.00825838
	LOSS [training: 0.110396453729329 | validation: 0.08734068728338064]
	TIME [epoch: 8.18 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0965532590898617		[learning rate: 0.0082484]
		[batch 20/20] avg loss: 0.13302990470097903		[learning rate: 0.0082384]
	Learning Rate: 0.00823839
	LOSS [training: 0.11479158189542035 | validation: 0.07753201248898153]
	TIME [epoch: 8.18 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10561128996151348		[learning rate: 0.0082284]
		[batch 20/20] avg loss: 0.09725633738521469		[learning rate: 0.0082184]
	Learning Rate: 0.00821844
	LOSS [training: 0.10143381367336408 | validation: 0.0730507006919287]
	TIME [epoch: 8.18 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12108267408952214		[learning rate: 0.0082085]
		[batch 20/20] avg loss: 0.09598860544810842		[learning rate: 0.0081985]
	Learning Rate: 0.00819855
	LOSS [training: 0.10853563976881526 | validation: 0.1075935834811359]
	TIME [epoch: 8.23 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12053085210121402		[learning rate: 0.0081886]
		[batch 20/20] avg loss: 0.11210639579777999		[learning rate: 0.0081787]
	Learning Rate: 0.0081787
	LOSS [training: 0.11631862394949702 | validation: 0.1082125468190514]
	TIME [epoch: 8.19 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12550494427865228		[learning rate: 0.0081688]
		[batch 20/20] avg loss: 0.08899869788032366		[learning rate: 0.0081589]
	Learning Rate: 0.0081589
	LOSS [training: 0.10725182107948794 | validation: 0.03297821794881029]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11036402848554305		[learning rate: 0.008149]
		[batch 20/20] avg loss: 0.08072887048034068		[learning rate: 0.0081391]
	Learning Rate: 0.00813915
	LOSS [training: 0.09554644948294186 | validation: 0.08439850919146298]
	TIME [epoch: 8.21 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12488896331188906		[learning rate: 0.0081293]
		[batch 20/20] avg loss: 0.1313292193826073		[learning rate: 0.0081194]
	Learning Rate: 0.00811944
	LOSS [training: 0.12810909134724816 | validation: 0.10083959512658887]
	TIME [epoch: 8.21 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10966939110620084		[learning rate: 0.0081096]
		[batch 20/20] avg loss: 0.11352886821883637		[learning rate: 0.0080998]
	Learning Rate: 0.00809979
	LOSS [training: 0.11159912966251861 | validation: 0.10533874678552226]
	TIME [epoch: 8.18 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1492795791620101		[learning rate: 0.00809]
		[batch 20/20] avg loss: 0.09538236521750937		[learning rate: 0.0080802]
	Learning Rate: 0.00808018
	LOSS [training: 0.12233097218975975 | validation: 0.09075976922444251]
	TIME [epoch: 8.17 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10543492939138346		[learning rate: 0.0080704]
		[batch 20/20] avg loss: 0.11842774491281971		[learning rate: 0.0080606]
	Learning Rate: 0.00806062
	LOSS [training: 0.11193133715210157 | validation: 0.13704826214061075]
	TIME [epoch: 8.19 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14203602117826347		[learning rate: 0.0080509]
		[batch 20/20] avg loss: 0.1731561214823383		[learning rate: 0.0080411]
	Learning Rate: 0.00804111
	LOSS [training: 0.15759607133030087 | validation: 0.15025115922384746]
	TIME [epoch: 8.19 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19415308419050162		[learning rate: 0.0080314]
		[batch 20/20] avg loss: 0.1345307305977021		[learning rate: 0.0080216]
	Learning Rate: 0.00802164
	LOSS [training: 0.16434190739410182 | validation: 0.08018736170452281]
	TIME [epoch: 8.2 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08873022981818826		[learning rate: 0.0080119]
		[batch 20/20] avg loss: 0.07744838782204554		[learning rate: 0.0080022]
	Learning Rate: 0.00800222
	LOSS [training: 0.08308930882011692 | validation: 0.06321124826278199]
	TIME [epoch: 8.19 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07898364426253672		[learning rate: 0.0079925]
		[batch 20/20] avg loss: 0.10235217587802548		[learning rate: 0.0079828]
	Learning Rate: 0.00798285
	LOSS [training: 0.0906679100702811 | validation: 0.11635085941403898]
	TIME [epoch: 8.21 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1954047439616884		[learning rate: 0.0079732]
		[batch 20/20] avg loss: 0.16712099184002696		[learning rate: 0.0079635]
	Learning Rate: 0.00796352
	LOSS [training: 0.18126286790085766 | validation: 0.12964261391956]
	TIME [epoch: 8.17 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1189729769947856		[learning rate: 0.0079539]
		[batch 20/20] avg loss: 0.09660545600670559		[learning rate: 0.0079442]
	Learning Rate: 0.00794424
	LOSS [training: 0.1077892165007456 | validation: 0.05622934654901861]
	TIME [epoch: 8.2 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12247398938689011		[learning rate: 0.0079346]
		[batch 20/20] avg loss: 0.12635716338900962		[learning rate: 0.007925]
	Learning Rate: 0.00792501
	LOSS [training: 0.12441557638794987 | validation: 0.08217057715478016]
	TIME [epoch: 8.18 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11378424675820942		[learning rate: 0.0079154]
		[batch 20/20] avg loss: 0.09115605108034727		[learning rate: 0.0079058]
	Learning Rate: 0.00790583
	LOSS [training: 0.10247014891927833 | validation: 0.09272395930122222]
	TIME [epoch: 8.2 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1292570444348507		[learning rate: 0.0078963]
		[batch 20/20] avg loss: 0.12510914373772528		[learning rate: 0.0078867]
	Learning Rate: 0.00788669
	LOSS [training: 0.12718309408628797 | validation: 0.046528073103067134]
	TIME [epoch: 8.18 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09476875696822654		[learning rate: 0.0078771]
		[batch 20/20] avg loss: 0.09499392750123103		[learning rate: 0.0078676]
	Learning Rate: 0.0078676
	LOSS [training: 0.09488134223472877 | validation: 0.04542149205347347]
	TIME [epoch: 8.2 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0848506470850114		[learning rate: 0.0078581]
		[batch 20/20] avg loss: 0.0741026898254967		[learning rate: 0.0078486]
	Learning Rate: 0.00784855
	LOSS [training: 0.07947666845525403 | validation: 0.1467585630451462]
	TIME [epoch: 8.21 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11803301656688955		[learning rate: 0.007839]
		[batch 20/20] avg loss: 0.10825880038357032		[learning rate: 0.0078296]
	Learning Rate: 0.00782955
	LOSS [training: 0.11314590847522994 | validation: 0.15604490224230305]
	TIME [epoch: 8.19 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11664669468804179		[learning rate: 0.0078201]
		[batch 20/20] avg loss: 0.11145101758455504		[learning rate: 0.0078106]
	Learning Rate: 0.0078106
	LOSS [training: 0.11404885613629841 | validation: 0.09882207297736238]
	TIME [epoch: 8.18 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11997492515205725		[learning rate: 0.0078011]
		[batch 20/20] avg loss: 0.0805710242727559		[learning rate: 0.0077917]
	Learning Rate: 0.00779169
	LOSS [training: 0.10027297471240659 | validation: 0.036452057972360626]
	TIME [epoch: 8.18 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11652203497058773		[learning rate: 0.0077823]
		[batch 20/20] avg loss: 0.1395746345010335		[learning rate: 0.0077728]
	Learning Rate: 0.00777283
	LOSS [training: 0.12804833473581057 | validation: 0.15665486814666044]
	TIME [epoch: 8.21 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12372027188457224		[learning rate: 0.0077634]
		[batch 20/20] avg loss: 0.09355182303951962		[learning rate: 0.007754]
	Learning Rate: 0.00775401
	LOSS [training: 0.10863604746204594 | validation: 0.13151902844457122]
	TIME [epoch: 8.19 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1650894222870259		[learning rate: 0.0077446]
		[batch 20/20] avg loss: 0.09849599171768325		[learning rate: 0.0077352]
	Learning Rate: 0.00773524
	LOSS [training: 0.13179270700235454 | validation: 0.057602447838604326]
	TIME [epoch: 8.2 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10257181025458531		[learning rate: 0.0077259]
		[batch 20/20] avg loss: 0.0929387750820685		[learning rate: 0.0077165]
	Learning Rate: 0.00771651
	LOSS [training: 0.0977552926683269 | validation: 0.16170560846273604]
	TIME [epoch: 8.19 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1363950598540638		[learning rate: 0.0077072]
		[batch 20/20] avg loss: 0.07155287058140526		[learning rate: 0.0076978]
	Learning Rate: 0.00769783
	LOSS [training: 0.10397396521773453 | validation: 0.0446768657546964]
	TIME [epoch: 8.21 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16583909654942655		[learning rate: 0.0076885]
		[batch 20/20] avg loss: 0.12447207389730153		[learning rate: 0.0076792]
	Learning Rate: 0.0076792
	LOSS [training: 0.14515558522336403 | validation: 0.5560991231358136]
	TIME [epoch: 8.22 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19709101210314137		[learning rate: 0.0076699]
		[batch 20/20] avg loss: 0.13049921843289067		[learning rate: 0.0076606]
	Learning Rate: 0.00766061
	LOSS [training: 0.163795115268016 | validation: 0.0512651314958632]
	TIME [epoch: 8.18 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0834974332777795		[learning rate: 0.0076513]
		[batch 20/20] avg loss: 0.1569495310254451		[learning rate: 0.0076421]
	Learning Rate: 0.00764206
	LOSS [training: 0.12022348215161231 | validation: 0.12145901492187404]
	TIME [epoch: 8.18 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12057640239823694		[learning rate: 0.0076328]
		[batch 20/20] avg loss: 0.10089112790611518		[learning rate: 0.0076236]
	Learning Rate: 0.00762356
	LOSS [training: 0.11073376515217606 | validation: 0.08923697630286866]
	TIME [epoch: 8.18 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1412023714018106		[learning rate: 0.0076143]
		[batch 20/20] avg loss: 0.21114640484491876		[learning rate: 0.0076051]
	Learning Rate: 0.00760511
	LOSS [training: 0.17617438812336467 | validation: 0.4569411041051885]
	TIME [epoch: 8.21 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1988676368441396		[learning rate: 0.0075959]
		[batch 20/20] avg loss: 0.13846594700729467		[learning rate: 0.0075867]
	Learning Rate: 0.00758669
	LOSS [training: 0.16866679192571715 | validation: 0.07792679556668108]
	TIME [epoch: 8.18 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11625050161549164		[learning rate: 0.0075775]
		[batch 20/20] avg loss: 0.11423324060606213		[learning rate: 0.0075683]
	Learning Rate: 0.00756833
	LOSS [training: 0.11524187111077686 | validation: 0.05916655005778367]
	TIME [epoch: 8.18 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08070652859216686		[learning rate: 0.0075592]
		[batch 20/20] avg loss: 0.1242786166552771		[learning rate: 0.00755]
	Learning Rate: 0.00755001
	LOSS [training: 0.10249257262372198 | validation: 0.09459562446040903]
	TIME [epoch: 8.2 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11740829712628338		[learning rate: 0.0075409]
		[batch 20/20] avg loss: 0.13523729383260125		[learning rate: 0.0075317]
	Learning Rate: 0.00753173
	LOSS [training: 0.1263227954794423 | validation: 0.05426961391266536]
	TIME [epoch: 8.23 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08712906522546562		[learning rate: 0.0075226]
		[batch 20/20] avg loss: 0.09405011275058567		[learning rate: 0.0075135]
	Learning Rate: 0.0075135
	LOSS [training: 0.09058958898802565 | validation: 0.06888043372704317]
	TIME [epoch: 8.2 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16973245003047094		[learning rate: 0.0075044]
		[batch 20/20] avg loss: 0.07698552774661124		[learning rate: 0.0074953]
	Learning Rate: 0.00749531
	LOSS [training: 0.12335898888854109 | validation: 0.05879262933617816]
	TIME [epoch: 8.22 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08349151947229581		[learning rate: 0.0074862]
		[batch 20/20] avg loss: 0.12002532746607956		[learning rate: 0.0074772]
	Learning Rate: 0.00747716
	LOSS [training: 0.10175842346918769 | validation: 0.10570676691974824]
	TIME [epoch: 8.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15330619593605463		[learning rate: 0.0074681]
		[batch 20/20] avg loss: 0.2211290233318403		[learning rate: 0.0074591]
	Learning Rate: 0.00745906
	LOSS [training: 0.18721760963394748 | validation: 0.08637499814309263]
	TIME [epoch: 8.19 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13360249299305918		[learning rate: 0.00745]
		[batch 20/20] avg loss: 0.13156700244308767		[learning rate: 0.007441]
	Learning Rate: 0.007441
	LOSS [training: 0.1325847477180734 | validation: 0.07626107759357457]
	TIME [epoch: 8.21 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37659905548709915		[learning rate: 0.007432]
		[batch 20/20] avg loss: 0.2221983727751955		[learning rate: 0.007423]
	Learning Rate: 0.00742299
	LOSS [training: 0.29939871413114727 | validation: 0.13233580869823033]
	TIME [epoch: 8.19 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12881480105298856		[learning rate: 0.007414]
		[batch 20/20] avg loss: 0.06933429600666036		[learning rate: 0.007405]
	Learning Rate: 0.00740502
	LOSS [training: 0.09907454852982445 | validation: 0.06405352390050781]
	TIME [epoch: 8.19 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11876525232935953		[learning rate: 0.0073961]
		[batch 20/20] avg loss: 0.18127298937704925		[learning rate: 0.0073871]
	Learning Rate: 0.0073871
	LOSS [training: 0.15001912085320437 | validation: 0.2343174255192789]
	TIME [epoch: 8.2 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19067562223192913		[learning rate: 0.0073781]
		[batch 20/20] avg loss: 0.1479425530433361		[learning rate: 0.0073692]
	Learning Rate: 0.00736921
	LOSS [training: 0.16930908763763264 | validation: 0.0912427491922357]
	TIME [epoch: 8.23 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1036052874518519		[learning rate: 0.0073603]
		[batch 20/20] avg loss: 0.08992192330319031		[learning rate: 0.0073514]
	Learning Rate: 0.00735137
	LOSS [training: 0.0967636053775211 | validation: 0.07567510329571932]
	TIME [epoch: 8.2 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07744202027686511		[learning rate: 0.0073425]
		[batch 20/20] avg loss: 0.08146733332271866		[learning rate: 0.0073336]
	Learning Rate: 0.00733358
	LOSS [training: 0.07945467679979187 | validation: 0.13357841692230943]
	TIME [epoch: 8.19 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13312998870503132		[learning rate: 0.0073247]
		[batch 20/20] avg loss: 0.11097935900774943		[learning rate: 0.0073158]
	Learning Rate: 0.00731582
	LOSS [training: 0.12205467385639039 | validation: 0.06940876410228268]
	TIME [epoch: 8.22 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10164575562455594		[learning rate: 0.007307]
		[batch 20/20] avg loss: 0.0952719612885732		[learning rate: 0.0072981]
	Learning Rate: 0.00729811
	LOSS [training: 0.09845885845656456 | validation: 0.09683596159997182]
	TIME [epoch: 8.21 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10737275320334612		[learning rate: 0.0072893]
		[batch 20/20] avg loss: 0.15104246047030503		[learning rate: 0.0072804]
	Learning Rate: 0.00728044
	LOSS [training: 0.12920760683682558 | validation: 0.17137036010542644]
	TIME [epoch: 8.19 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14620739317079176		[learning rate: 0.0072716]
		[batch 20/20] avg loss: 0.11692936006006631		[learning rate: 0.0072628]
	Learning Rate: 0.00726282
	LOSS [training: 0.13156837661542906 | validation: 0.07582129792257812]
	TIME [epoch: 8.2 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11281612493095139		[learning rate: 0.007254]
		[batch 20/20] avg loss: 0.09223523422793456		[learning rate: 0.0072452]
	Learning Rate: 0.00724524
	LOSS [training: 0.10252567957944296 | validation: 0.0762006463315871]
	TIME [epoch: 8.2 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09733170468293098		[learning rate: 0.0072365]
		[batch 20/20] avg loss: 0.11404365052439942		[learning rate: 0.0072277]
	Learning Rate: 0.0072277
	LOSS [training: 0.1056876776036652 | validation: 0.09713816162279351]
	TIME [epoch: 8.2 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1556778658714201		[learning rate: 0.0072189]
		[batch 20/20] avg loss: 0.1475610905169323		[learning rate: 0.0072102]
	Learning Rate: 0.0072102
	LOSS [training: 0.15161947819417618 | validation: 0.08910798440457673]
	TIME [epoch: 8.23 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14883607173004748		[learning rate: 0.0072015]
		[batch 20/20] avg loss: 0.22301331866172242		[learning rate: 0.0071927]
	Learning Rate: 0.00719275
	LOSS [training: 0.18592469519588498 | validation: 0.3483127782373955]
	TIME [epoch: 8.22 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36122222521380537		[learning rate: 0.007184]
		[batch 20/20] avg loss: 0.3178587267738321		[learning rate: 0.0071753]
	Learning Rate: 0.00717533
	LOSS [training: 0.3395404759938187 | validation: 0.2397466981628572]
	TIME [epoch: 8.19 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17425091842182205		[learning rate: 0.0071666]
		[batch 20/20] avg loss: 0.07156946514789661		[learning rate: 0.007158]
	Learning Rate: 0.00715796
	LOSS [training: 0.12291019178485936 | validation: 0.027630653069043703]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09832623631698142		[learning rate: 0.0071493]
		[batch 20/20] avg loss: 0.10065896095855642		[learning rate: 0.0071406]
	Learning Rate: 0.00714064
	LOSS [training: 0.09949259863776891 | validation: 0.08630231112228443]
	TIME [epoch: 8.46 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07344142477819639		[learning rate: 0.007132]
		[batch 20/20] avg loss: 0.12591247550181803		[learning rate: 0.0071233]
	Learning Rate: 0.00712335
	LOSS [training: 0.09967695014000721 | validation: 0.09480163567354447]
	TIME [epoch: 8.23 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11723705951869431		[learning rate: 0.0071147]
		[batch 20/20] avg loss: 0.14927944993791725		[learning rate: 0.0071061]
	Learning Rate: 0.0071061
	LOSS [training: 0.13325825472830577 | validation: 0.14257721543622212]
	TIME [epoch: 8.21 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1651499816676763		[learning rate: 0.0070975]
		[batch 20/20] avg loss: 0.09150536239022863		[learning rate: 0.0070889]
	Learning Rate: 0.0070889
	LOSS [training: 0.12832767202895246 | validation: 0.07248442102655245]
	TIME [epoch: 8.21 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08283548387574606		[learning rate: 0.0070803]
		[batch 20/20] avg loss: 0.07880999543670618		[learning rate: 0.0070717]
	Learning Rate: 0.00707174
	LOSS [training: 0.08082273965622612 | validation: 0.17968854619054794]
	TIME [epoch: 8.24 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09560919760928607		[learning rate: 0.0070632]
		[batch 20/20] avg loss: 0.17820883497966603		[learning rate: 0.0070546]
	Learning Rate: 0.00705462
	LOSS [training: 0.13690901629447605 | validation: 0.07667156721844526]
	TIME [epoch: 8.23 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08584651217305428		[learning rate: 0.0070461]
		[batch 20/20] avg loss: 0.1173132083761363		[learning rate: 0.0070375]
	Learning Rate: 0.00703754
	LOSS [training: 0.10157986027459528 | validation: 0.05367233001309249]
	TIME [epoch: 8.2 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2168049915236498		[learning rate: 0.007029]
		[batch 20/20] avg loss: 0.11770048613801314		[learning rate: 0.0070205]
	Learning Rate: 0.00702051
	LOSS [training: 0.16725273883083147 | validation: 0.06812983349529807]
	TIME [epoch: 8.2 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10897423174530729		[learning rate: 0.007012]
		[batch 20/20] avg loss: 0.13495492622735045		[learning rate: 0.0070035]
	Learning Rate: 0.00700351
	LOSS [training: 0.12196457898632887 | validation: 0.06466275945744474]
	TIME [epoch: 8.21 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11930763322537181		[learning rate: 0.006995]
		[batch 20/20] avg loss: 0.08394114674236208		[learning rate: 0.0069866]
	Learning Rate: 0.00698656
	LOSS [training: 0.10162438998386696 | validation: 0.04001789103381628]
	TIME [epoch: 8.25 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10189172149544548		[learning rate: 0.0069781]
		[batch 20/20] avg loss: 0.19790546526604572		[learning rate: 0.0069696]
	Learning Rate: 0.00696964
	LOSS [training: 0.14989859338074557 | validation: 0.12132197987263008]
	TIME [epoch: 8.21 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14430121911449167		[learning rate: 0.0069612]
		[batch 20/20] avg loss: 0.3418237907866332		[learning rate: 0.0069528]
	Learning Rate: 0.00695277
	LOSS [training: 0.24306250495056245 | validation: 0.34853536777173]
	TIME [epoch: 8.21 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3150591522157047		[learning rate: 0.0069443]
		[batch 20/20] avg loss: 0.16714239267221553		[learning rate: 0.0069359]
	Learning Rate: 0.00693594
	LOSS [training: 0.24110077244396017 | validation: 0.1077686359624772]
	TIME [epoch: 8.25 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1511344662580038		[learning rate: 0.0069275]
		[batch 20/20] avg loss: 0.10362990585543927		[learning rate: 0.0069191]
	Learning Rate: 0.00691915
	LOSS [training: 0.12738218605672155 | validation: 0.07697361840741876]
	TIME [epoch: 8.22 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1223149836805478		[learning rate: 0.0069108]
		[batch 20/20] avg loss: 0.11934843823373675		[learning rate: 0.0069024]
	Learning Rate: 0.0069024
	LOSS [training: 0.12083171095714226 | validation: 0.0861986021468635]
	TIME [epoch: 8.21 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08970451119749952		[learning rate: 0.006894]
		[batch 20/20] avg loss: 0.09994541693853612		[learning rate: 0.0068857]
	Learning Rate: 0.00688569
	LOSS [training: 0.09482496406801781 | validation: 0.042435895842034915]
	TIME [epoch: 8.2 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08419140635878641		[learning rate: 0.0068773]
		[batch 20/20] avg loss: 0.09709753801147267		[learning rate: 0.006869]
	Learning Rate: 0.00686902
	LOSS [training: 0.09064447218512955 | validation: 0.09675148631629389]
	TIME [epoch: 8.2 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09589583728873292		[learning rate: 0.0068607]
		[batch 20/20] avg loss: 0.08922284193138363		[learning rate: 0.0068524]
	Learning Rate: 0.00685239
	LOSS [training: 0.09255933961005827 | validation: 0.060359913222464805]
	TIME [epoch: 8.22 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07740772756201918		[learning rate: 0.0068441]
		[batch 20/20] avg loss: 0.1455446988223649		[learning rate: 0.0068358]
	Learning Rate: 0.0068358
	LOSS [training: 0.11147621319219203 | validation: 0.11968150385922309]
	TIME [epoch: 8.25 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13404401608938185		[learning rate: 0.0068275]
		[batch 20/20] avg loss: 0.1021886582023325		[learning rate: 0.0068193]
	Learning Rate: 0.00681925
	LOSS [training: 0.11811633714585716 | validation: 0.10584307278773039]
	TIME [epoch: 8.21 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09538996680968478		[learning rate: 0.006811]
		[batch 20/20] avg loss: 0.11593377928600983		[learning rate: 0.0068027]
	Learning Rate: 0.00680275
	LOSS [training: 0.10566187304784731 | validation: 0.06440497596269269]
	TIME [epoch: 8.21 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06322651022739592		[learning rate: 0.0067945]
		[batch 20/20] avg loss: 0.10235148679779653		[learning rate: 0.0067863]
	Learning Rate: 0.00678628
	LOSS [training: 0.08278899851259623 | validation: 0.07716689379453436]
	TIME [epoch: 8.23 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11665522837772187		[learning rate: 0.0067781]
		[batch 20/20] avg loss: 0.11619878282369675		[learning rate: 0.0067698]
	Learning Rate: 0.00676985
	LOSS [training: 0.11642700560070932 | validation: 0.060902308567292375]
	TIME [epoch: 8.24 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07295778209749768		[learning rate: 0.0067616]
		[batch 20/20] avg loss: 0.09303477854826578		[learning rate: 0.0067535]
	Learning Rate: 0.00675346
	LOSS [training: 0.08299628032288173 | validation: 0.2416036722665252]
	TIME [epoch: 8.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13212415430336447		[learning rate: 0.0067453]
		[batch 20/20] avg loss: 0.08779950393873694		[learning rate: 0.0067371]
	Learning Rate: 0.00673711
	LOSS [training: 0.10996182912105068 | validation: 0.0467567357263151]
	TIME [epoch: 8.2 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08823593880987748		[learning rate: 0.006729]
		[batch 20/20] avg loss: 0.12183756423915948		[learning rate: 0.0067208]
	Learning Rate: 0.0067208
	LOSS [training: 0.10503675152451848 | validation: 0.10257143644983631]
	TIME [epoch: 8.23 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07724155062421731		[learning rate: 0.0067127]
		[batch 20/20] avg loss: 0.08615666561623486		[learning rate: 0.0067045]
	Learning Rate: 0.00670453
	LOSS [training: 0.08169910812022606 | validation: 0.050423367532133216]
	TIME [epoch: 8.2 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12432840177724698		[learning rate: 0.0066964]
		[batch 20/20] avg loss: 0.13827906014941488		[learning rate: 0.0066883]
	Learning Rate: 0.0066883
	LOSS [training: 0.13130373096333095 | validation: 0.06284646830683838]
	TIME [epoch: 8.23 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09303178265236003		[learning rate: 0.0066802]
		[batch 20/20] avg loss: 0.09245816951862515		[learning rate: 0.0066721]
	Learning Rate: 0.00667211
	LOSS [training: 0.0927449760854926 | validation: 0.14331274392549362]
	TIME [epoch: 8.24 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13321795332761016		[learning rate: 0.006664]
		[batch 20/20] avg loss: 0.13634330925851776		[learning rate: 0.006656]
	Learning Rate: 0.00665596
	LOSS [training: 0.134780631293064 | validation: 0.14957641021841925]
	TIME [epoch: 8.21 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12070051154689239		[learning rate: 0.0066479]
		[batch 20/20] avg loss: 0.06804383892932662		[learning rate: 0.0066398]
	Learning Rate: 0.00663984
	LOSS [training: 0.0943721752381095 | validation: 0.028761370729044968]
	TIME [epoch: 8.21 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06641712861994506		[learning rate: 0.0066318]
		[batch 20/20] avg loss: 0.11448970839195623		[learning rate: 0.0066238]
	Learning Rate: 0.00662377
	LOSS [training: 0.09045341850595064 | validation: 0.08632204489624508]
	TIME [epoch: 8.23 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09487263195930648		[learning rate: 0.0066157]
		[batch 20/20] avg loss: 0.08084425109915526		[learning rate: 0.0066077]
	Learning Rate: 0.00660774
	LOSS [training: 0.08785844152923086 | validation: 0.04850110502805971]
	TIME [epoch: 8.21 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0693161533553843		[learning rate: 0.0065997]
		[batch 20/20] avg loss: 0.11860944836283768		[learning rate: 0.0065917]
	Learning Rate: 0.00659174
	LOSS [training: 0.09396280085911099 | validation: 0.08856225472624497]
	TIME [epoch: 8.2 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09410014802460842		[learning rate: 0.0065838]
		[batch 20/20] avg loss: 0.061123618335237606		[learning rate: 0.0065758]
	Learning Rate: 0.00657578
	LOSS [training: 0.07761188317992303 | validation: 0.1365115968791699]
	TIME [epoch: 8.23 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1648105101186684		[learning rate: 0.0065678]
		[batch 20/20] avg loss: 0.1258572951594011		[learning rate: 0.0065599]
	Learning Rate: 0.00655986
	LOSS [training: 0.14533390263903478 | validation: 0.14152184029420803]
	TIME [epoch: 8.23 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09243059100770774		[learning rate: 0.0065519]
		[batch 20/20] avg loss: 0.0725094701251815		[learning rate: 0.006544]
	Learning Rate: 0.00654398
	LOSS [training: 0.08247003056644464 | validation: 0.031158750883422234]
	TIME [epoch: 8.22 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09390810760598597		[learning rate: 0.0065361]
		[batch 20/20] avg loss: 0.0733858344561718		[learning rate: 0.0065281]
	Learning Rate: 0.00652814
	LOSS [training: 0.08364697103107888 | validation: 0.1282381756783734]
	TIME [epoch: 8.24 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10797995323685924		[learning rate: 0.0065202]
		[batch 20/20] avg loss: 0.07324259664027336		[learning rate: 0.0065123]
	Learning Rate: 0.00651234
	LOSS [training: 0.0906112749385663 | validation: 0.03686493338866972]
	TIME [epoch: 8.22 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14556775025278074		[learning rate: 0.0065044]
		[batch 20/20] avg loss: 0.11174225237197144		[learning rate: 0.0064966]
	Learning Rate: 0.00649657
	LOSS [training: 0.12865500131237612 | validation: 0.042967667563097674]
	TIME [epoch: 8.19 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10664146576426048		[learning rate: 0.0064887]
		[batch 20/20] avg loss: 0.09954552072102275		[learning rate: 0.0064808]
	Learning Rate: 0.00648084
	LOSS [training: 0.10309349324264161 | validation: 0.06224867281610566]
	TIME [epoch: 8.22 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08042592686317036		[learning rate: 0.006473]
		[batch 20/20] avg loss: 0.07923949467181075		[learning rate: 0.0064652]
	Learning Rate: 0.00646516
	LOSS [training: 0.07983271076749056 | validation: 0.10475452066392432]
	TIME [epoch: 8.21 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09683180170982773		[learning rate: 0.0064573]
		[batch 20/20] avg loss: 0.07157154687754641		[learning rate: 0.0064495]
	Learning Rate: 0.0064495
	LOSS [training: 0.08420167429368706 | validation: 0.03917278334949418]
	TIME [epoch: 8.22 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0823560771398097		[learning rate: 0.0064417]
		[batch 20/20] avg loss: 0.0670865287597827		[learning rate: 0.0064339]
	Learning Rate: 0.00643389
	LOSS [training: 0.0747213029497962 | validation: 0.05914984375357269]
	TIME [epoch: 8.21 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1003267587648676		[learning rate: 0.0064261]
		[batch 20/20] avg loss: 0.0937055660536796		[learning rate: 0.0064183]
	Learning Rate: 0.00641832
	LOSS [training: 0.0970161624092736 | validation: 0.07270881084041325]
	TIME [epoch: 8.23 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07637694030336606		[learning rate: 0.0064105]
		[batch 20/20] avg loss: 0.07985610944124365		[learning rate: 0.0064028]
	Learning Rate: 0.00640278
	LOSS [training: 0.07811652487230483 | validation: 0.02303011467190984]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06420272979812353		[learning rate: 0.006395]
		[batch 20/20] avg loss: 0.1576376836263798		[learning rate: 0.0063873]
	Learning Rate: 0.00638728
	LOSS [training: 0.11092020671225167 | validation: 0.13004727762718304]
	TIME [epoch: 8.18 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1932764618088902		[learning rate: 0.0063795]
		[batch 20/20] avg loss: 0.1472848933832706		[learning rate: 0.0063718]
	Learning Rate: 0.00637182
	LOSS [training: 0.1702806775960804 | validation: 0.09948240453471865]
	TIME [epoch: 8.19 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13496663363319436		[learning rate: 0.0063641]
		[batch 20/20] avg loss: 0.26737651535002616		[learning rate: 0.0063564]
	Learning Rate: 0.00635639
	LOSS [training: 0.20117157449161027 | validation: 0.273619189260061]
	TIME [epoch: 8.18 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23690481806537753		[learning rate: 0.0063487]
		[batch 20/20] avg loss: 0.36003073836524224		[learning rate: 0.006341]
	Learning Rate: 0.006341
	LOSS [training: 0.2984677782153099 | validation: 0.19787713266794205]
	TIME [epoch: 8.24 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2313299864785582		[learning rate: 0.0063333]
		[batch 20/20] avg loss: 0.13745340606837111		[learning rate: 0.0063257]
	Learning Rate: 0.00632565
	LOSS [training: 0.18439169627346463 | validation: 0.09048839051407631]
	TIME [epoch: 8.19 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11244198045697992		[learning rate: 0.006318]
		[batch 20/20] avg loss: 0.09649752665442798		[learning rate: 0.0063103]
	Learning Rate: 0.00631034
	LOSS [training: 0.10446975355570394 | validation: 0.06968716880349451]
	TIME [epoch: 8.19 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11036220449978054		[learning rate: 0.0063027]
		[batch 20/20] avg loss: 0.08334189226615864		[learning rate: 0.0062951]
	Learning Rate: 0.00629506
	LOSS [training: 0.0968520483829696 | validation: 0.09065993997467572]
	TIME [epoch: 8.21 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09998126813365613		[learning rate: 0.0062874]
		[batch 20/20] avg loss: 0.09494997650240443		[learning rate: 0.0062798]
	Learning Rate: 0.00627982
	LOSS [training: 0.0974656223180303 | validation: 0.0694537606732041]
	TIME [epoch: 8.23 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09726344890744062		[learning rate: 0.0062722]
		[batch 20/20] avg loss: 0.08939684138721435		[learning rate: 0.0062646]
	Learning Rate: 0.00626462
	LOSS [training: 0.09333014514732749 | validation: 0.05693204780372542]
	TIME [epoch: 8.19 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06196434887155997		[learning rate: 0.006257]
		[batch 20/20] avg loss: 0.1704290646923618		[learning rate: 0.0062495]
	Learning Rate: 0.00624945
	LOSS [training: 0.11619670678196088 | validation: 0.12762110022824036]
	TIME [epoch: 8.18 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32704525217367475		[learning rate: 0.0062419]
		[batch 20/20] avg loss: 0.26110995412165466		[learning rate: 0.0062343]
	Learning Rate: 0.00623433
	LOSS [training: 0.29407760314766473 | validation: 0.15904386777732893]
	TIME [epoch: 8.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16139660079662432		[learning rate: 0.0062268]
		[batch 20/20] avg loss: 0.1328076642987032		[learning rate: 0.0062192]
	Learning Rate: 0.00621923
	LOSS [training: 0.14710213254766374 | validation: 0.08279394479406933]
	TIME [epoch: 8.21 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10762440975560268		[learning rate: 0.0062117]
		[batch 20/20] avg loss: 0.07881968514758242		[learning rate: 0.0062042]
	Learning Rate: 0.00620418
	LOSS [training: 0.09322204745159256 | validation: 0.12302013061337963]
	TIME [epoch: 8.2 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19191031433004221		[learning rate: 0.0061967]
		[batch 20/20] avg loss: 0.10027592393596955		[learning rate: 0.0061892]
	Learning Rate: 0.00618916
	LOSS [training: 0.1460931191330059 | validation: 0.05035139475305645]
	TIME [epoch: 8.19 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0983557819950629		[learning rate: 0.0061817]
		[batch 20/20] avg loss: 0.10232832107498449		[learning rate: 0.0061742]
	Learning Rate: 0.00617417
	LOSS [training: 0.1003420515350237 | validation: 0.09908053819781518]
	TIME [epoch: 8.22 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09220877163485881		[learning rate: 0.0061667]
		[batch 20/20] avg loss: 0.06453446315793056		[learning rate: 0.0061592]
	Learning Rate: 0.00615923
	LOSS [training: 0.07837161739639467 | validation: 0.10196090885652095]
	TIME [epoch: 8.18 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09909999023654942		[learning rate: 0.0061518]
		[batch 20/20] avg loss: 0.06789814578864625		[learning rate: 0.0061443]
	Learning Rate: 0.00614432
	LOSS [training: 0.08349906801259785 | validation: 0.02862584503691757]
	TIME [epoch: 8.21 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08058694696927006		[learning rate: 0.0061369]
		[batch 20/20] avg loss: 0.2042622104540462		[learning rate: 0.0061294]
	Learning Rate: 0.00612944
	LOSS [training: 0.14242457871165812 | validation: 0.1310467114630106]
	TIME [epoch: 8.19 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10096322839247288		[learning rate: 0.006122]
		[batch 20/20] avg loss: 0.11515109352944645		[learning rate: 0.0061146]
	Learning Rate: 0.0061146
	LOSS [training: 0.10805716096095967 | validation: 0.05957165329415071]
	TIME [epoch: 8.19 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0752649818434791		[learning rate: 0.0061072]
		[batch 20/20] avg loss: 0.08226283424043886		[learning rate: 0.0060998]
	Learning Rate: 0.0060998
	LOSS [training: 0.07876390804195899 | validation: 0.07185328207523499]
	TIME [epoch: 8.21 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07090503692444024		[learning rate: 0.0060924]
		[batch 20/20] avg loss: 0.09569171699077905		[learning rate: 0.006085]
	Learning Rate: 0.00608504
	LOSS [training: 0.08329837695760965 | validation: 0.058185043079239385]
	TIME [epoch: 8.22 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06761655900564376		[learning rate: 0.0060777]
		[batch 20/20] avg loss: 0.0849924015931494		[learning rate: 0.0060703]
	Learning Rate: 0.00607031
	LOSS [training: 0.07630448029939658 | validation: 0.050811541301813756]
	TIME [epoch: 8.2 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08447328338686982		[learning rate: 0.006063]
		[batch 20/20] avg loss: 0.11840514421173298		[learning rate: 0.0060556]
	Learning Rate: 0.00605561
	LOSS [training: 0.1014392137993014 | validation: 0.049146461295892924]
	TIME [epoch: 8.21 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08274365806230707		[learning rate: 0.0060483]
		[batch 20/20] avg loss: 0.08614424379737731		[learning rate: 0.006041]
	Learning Rate: 0.00604095
	LOSS [training: 0.0844439509298422 | validation: 0.05058853115735572]
	TIME [epoch: 8.21 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06405805176492899		[learning rate: 0.0060336]
		[batch 20/20] avg loss: 0.11663879236892052		[learning rate: 0.0060263]
	Learning Rate: 0.00602633
	LOSS [training: 0.09034842206692474 | validation: 0.045502926050843155]
	TIME [epoch: 8.18 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07434091473608259		[learning rate: 0.006019]
		[batch 20/20] avg loss: 0.0803431395145967		[learning rate: 0.0060117]
	Learning Rate: 0.00601174
	LOSS [training: 0.07734202712533964 | validation: 0.10054523611055982]
	TIME [epoch: 8.22 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1144120928544109		[learning rate: 0.0060045]
		[batch 20/20] avg loss: 0.16447219338001745		[learning rate: 0.0059972]
	Learning Rate: 0.00599718
	LOSS [training: 0.1394421431172142 | validation: 0.06522429303583978]
	TIME [epoch: 8.19 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07685238870987521		[learning rate: 0.0059899]
		[batch 20/20] avg loss: 0.06678031446268927		[learning rate: 0.0059827]
	Learning Rate: 0.00598267
	LOSS [training: 0.07181635158628223 | validation: 0.05669906719168908]
	TIME [epoch: 8.21 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06421927783904724		[learning rate: 0.0059754]
		[batch 20/20] avg loss: 0.10582670638823163		[learning rate: 0.0059682]
	Learning Rate: 0.00596818
	LOSS [training: 0.08502299211363944 | validation: 0.12150091697091028]
	TIME [epoch: 8.2 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09724280880874085		[learning rate: 0.005961]
		[batch 20/20] avg loss: 0.08580763230768781		[learning rate: 0.0059537]
	Learning Rate: 0.00595374
	LOSS [training: 0.09152522055821435 | validation: 0.038824073536190426]
	TIME [epoch: 8.22 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08281606364614447		[learning rate: 0.0059465]
		[batch 20/20] avg loss: 0.07883052584408515		[learning rate: 0.0059393]
	Learning Rate: 0.00593932
	LOSS [training: 0.08082329474511481 | validation: 0.09143781726571909]
	TIME [epoch: 8.23 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0662460142099163		[learning rate: 0.0059321]
		[batch 20/20] avg loss: 0.0636590768511788		[learning rate: 0.0059249]
	Learning Rate: 0.00592494
	LOSS [training: 0.06495254553054754 | validation: 0.03368932202303745]
	TIME [epoch: 8.18 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05971358433901495		[learning rate: 0.0059178]
		[batch 20/20] avg loss: 0.06655123390481238		[learning rate: 0.0059106]
	Learning Rate: 0.0059106
	LOSS [training: 0.06313240912191365 | validation: 0.05542111272549508]
	TIME [epoch: 8.19 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10211905429608273		[learning rate: 0.0059034]
		[batch 20/20] avg loss: 0.06212680557695518		[learning rate: 0.0058963]
	Learning Rate: 0.00589629
	LOSS [training: 0.08212292993651896 | validation: 0.050710176169101825]
	TIME [epoch: 8.18 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07077855593606366		[learning rate: 0.0058892]
		[batch 20/20] avg loss: 0.06561971021315685		[learning rate: 0.005882]
	Learning Rate: 0.00588202
	LOSS [training: 0.06819913307461024 | validation: 0.09358098810261009]
	TIME [epoch: 8.21 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060539591149844175		[learning rate: 0.0058749]
		[batch 20/20] avg loss: 0.062439037038163336		[learning rate: 0.0058678]
	Learning Rate: 0.00586778
	LOSS [training: 0.06148931409400375 | validation: 0.13498399677266482]
	TIME [epoch: 8.2 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06929334420653346		[learning rate: 0.0058607]
		[batch 20/20] avg loss: 0.06377392225408053		[learning rate: 0.0058536]
	Learning Rate: 0.00585357
	LOSS [training: 0.066533633230307 | validation: 0.06495775227642057]
	TIME [epoch: 8.2 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07111532094671998		[learning rate: 0.0058465]
		[batch 20/20] avg loss: 0.10397926053919829		[learning rate: 0.0058394]
	Learning Rate: 0.0058394
	LOSS [training: 0.08754729074295912 | validation: 0.290982886518171]
	TIME [epoch: 8.19 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18466337044632616		[learning rate: 0.0058323]
		[batch 20/20] avg loss: 0.09717029412709495		[learning rate: 0.0058253]
	Learning Rate: 0.00582527
	LOSS [training: 0.14091683228671062 | validation: 0.09980632683146831]
	TIME [epoch: 8.22 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08294835700050467		[learning rate: 0.0058182]
		[batch 20/20] avg loss: 0.07776879936875267		[learning rate: 0.0058112]
	Learning Rate: 0.00581116
	LOSS [training: 0.08035857818462867 | validation: 0.12023437194480927]
	TIME [epoch: 8.24 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10630724388611272		[learning rate: 0.0058041]
		[batch 20/20] avg loss: 0.08141820775831804		[learning rate: 0.0057971]
	Learning Rate: 0.0057971
	LOSS [training: 0.09386272582221539 | validation: 0.044249550399594675]
	TIME [epoch: 8.18 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12440399527714763		[learning rate: 0.0057901]
		[batch 20/20] avg loss: 0.0793711061868467		[learning rate: 0.0057831]
	Learning Rate: 0.00578306
	LOSS [training: 0.10188755073199715 | validation: 0.07500175753244914]
	TIME [epoch: 8.19 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11043299762399135		[learning rate: 0.0057761]
		[batch 20/20] avg loss: 0.0520524684712582		[learning rate: 0.0057691]
	Learning Rate: 0.00576906
	LOSS [training: 0.08124273304762478 | validation: 0.03457940378388806]
	TIME [epoch: 8.2 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060828639071026605		[learning rate: 0.0057621]
		[batch 20/20] avg loss: 0.08995695482835782		[learning rate: 0.0057551]
	Learning Rate: 0.0057551
	LOSS [training: 0.07539279694969221 | validation: 0.06845438574023796]
	TIME [epoch: 8.19 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10548541118825464		[learning rate: 0.0057481]
		[batch 20/20] avg loss: 0.09425155102381451		[learning rate: 0.0057412]
	Learning Rate: 0.00574116
	LOSS [training: 0.09986848110603458 | validation: 0.13275477481325024]
	TIME [epoch: 8.2 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1109148068919962		[learning rate: 0.0057342]
		[batch 20/20] avg loss: 0.09317425401293719		[learning rate: 0.0057273]
	Learning Rate: 0.00572727
	LOSS [training: 0.10204453045246668 | validation: 0.031254464873802276]
	TIME [epoch: 8.2 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061208938130513556		[learning rate: 0.0057203]
		[batch 20/20] avg loss: 0.06327679338145069		[learning rate: 0.0057134]
	Learning Rate: 0.0057134
	LOSS [training: 0.06224286575598211 | validation: 0.04152409794844571]
	TIME [epoch: 8.19 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0631506809784517		[learning rate: 0.0057065]
		[batch 20/20] avg loss: 0.06960253154670235		[learning rate: 0.0056996]
	Learning Rate: 0.00569957
	LOSS [training: 0.06637660626257699 | validation: 0.054214916541269885]
	TIME [epoch: 8.22 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10646218557744103		[learning rate: 0.0056927]
		[batch 20/20] avg loss: 0.10097435390535361		[learning rate: 0.0056858]
	Learning Rate: 0.00568577
	LOSS [training: 0.1037182697413973 | validation: 0.057991905333614424]
	TIME [epoch: 8.23 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09069549547856318		[learning rate: 0.0056789]
		[batch 20/20] avg loss: 0.06556297124606056		[learning rate: 0.005672]
	Learning Rate: 0.00567201
	LOSS [training: 0.07812923336231185 | validation: 0.0666971674917075]
	TIME [epoch: 8.18 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0697323306749153		[learning rate: 0.0056651]
		[batch 20/20] avg loss: 0.07079050899913972		[learning rate: 0.0056583]
	Learning Rate: 0.00565828
	LOSS [training: 0.07026141983702751 | validation: 0.18722527879098794]
	TIME [epoch: 8.18 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10198125522032837		[learning rate: 0.0056514]
		[batch 20/20] avg loss: 0.062025552205962155		[learning rate: 0.0056446]
	Learning Rate: 0.00564458
	LOSS [training: 0.08200340371314527 | validation: 0.04478160484161482]
	TIME [epoch: 8.21 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06576860266432093		[learning rate: 0.0056377]
		[batch 20/20] avg loss: 0.08718085060402914		[learning rate: 0.0056309]
	Learning Rate: 0.00563092
	LOSS [training: 0.07647472663417502 | validation: 0.020842113606950274]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05351878079683803		[learning rate: 0.0056241]
		[batch 20/20] avg loss: 0.13199853424734836		[learning rate: 0.0056173]
	Learning Rate: 0.00561728
	LOSS [training: 0.0927586575220932 | validation: 0.056084559472658974]
	TIME [epoch: 8.2 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09530124357322779		[learning rate: 0.0056105]
		[batch 20/20] avg loss: 0.07191489320723302		[learning rate: 0.0056037]
	Learning Rate: 0.00560368
	LOSS [training: 0.0836080683902304 | validation: 0.046228393462378566]
	TIME [epoch: 8.18 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0640169365849798		[learning rate: 0.0055969]
		[batch 20/20] avg loss: 0.055918974765399664		[learning rate: 0.0055901]
	Learning Rate: 0.00559012
	LOSS [training: 0.059967955675189734 | validation: 0.07677706169589023]
	TIME [epoch: 8.18 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07415336932163591		[learning rate: 0.0055833]
		[batch 20/20] avg loss: 0.07731164401774696		[learning rate: 0.0055766]
	Learning Rate: 0.00557659
	LOSS [training: 0.07573250666969143 | validation: 0.0694753534087269]
	TIME [epoch: 8.23 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06583162269809123		[learning rate: 0.0055698]
		[batch 20/20] avg loss: 0.07600612875485524		[learning rate: 0.0055631]
	Learning Rate: 0.00556309
	LOSS [training: 0.07091887572647324 | validation: 0.020342008920450853]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05029510787281871		[learning rate: 0.0055563]
		[batch 20/20] avg loss: 0.07651369067380845		[learning rate: 0.0055496]
	Learning Rate: 0.00554962
	LOSS [training: 0.06340439927331358 | validation: 0.03283707302625652]
	TIME [epoch: 8.17 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046247465187108046		[learning rate: 0.0055429]
		[batch 20/20] avg loss: 0.059173152183670294		[learning rate: 0.0055362]
	Learning Rate: 0.00553618
	LOSS [training: 0.05271030868538919 | validation: 0.045615444911446354]
	TIME [epoch: 8.17 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06937620103780062		[learning rate: 0.0055295]
		[batch 20/20] avg loss: 0.09318953716237846		[learning rate: 0.0055228]
	Learning Rate: 0.00552278
	LOSS [training: 0.08128286910008954 | validation: 0.05680481867111539]
	TIME [epoch: 8.23 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07837216284898066		[learning rate: 0.0055161]
		[batch 20/20] avg loss: 0.08313453276465127		[learning rate: 0.0055094]
	Learning Rate: 0.00550941
	LOSS [training: 0.08075334780681596 | validation: 0.045690716283766905]
	TIME [epoch: 8.18 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04685543014204314		[learning rate: 0.0055027]
		[batch 20/20] avg loss: 0.058253374576762364		[learning rate: 0.0054961]
	Learning Rate: 0.00549607
	LOSS [training: 0.052554402359402744 | validation: 0.045638084288040184]
	TIME [epoch: 8.18 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07255527211647805		[learning rate: 0.0054894]
		[batch 20/20] avg loss: 0.10580697064189293		[learning rate: 0.0054828]
	Learning Rate: 0.00548277
	LOSS [training: 0.0891811213791855 | validation: 0.07978228024547761]
	TIME [epoch: 8.2 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07634085874813522		[learning rate: 0.0054761]
		[batch 20/20] avg loss: 0.07205535970760288		[learning rate: 0.0054695]
	Learning Rate: 0.0054695
	LOSS [training: 0.07419810922786906 | validation: 0.07798755521129823]
	TIME [epoch: 8.22 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09670152251390034		[learning rate: 0.0054629]
		[batch 20/20] avg loss: 0.07823546306932115		[learning rate: 0.0054563]
	Learning Rate: 0.00545626
	LOSS [training: 0.08746849279161076 | validation: 0.04139358571110596]
	TIME [epoch: 8.18 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07360768300146411		[learning rate: 0.0054496]
		[batch 20/20] avg loss: 0.10085457748212039		[learning rate: 0.005443]
	Learning Rate: 0.00544305
	LOSS [training: 0.08723113024179226 | validation: 0.09533048257739582]
	TIME [epoch: 8.18 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17084983982380336		[learning rate: 0.0054365]
		[batch 20/20] avg loss: 0.10830723792893245		[learning rate: 0.0054299]
	Learning Rate: 0.00542987
	LOSS [training: 0.1395785388763679 | validation: 0.02102720536720215]
	TIME [epoch: 8.18 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05187678916132763		[learning rate: 0.0054233]
		[batch 20/20] avg loss: 0.06175753568623845		[learning rate: 0.0054167]
	Learning Rate: 0.00541673
	LOSS [training: 0.05681716242378303 | validation: 0.015743811333245806]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06998578651836417		[learning rate: 0.0054102]
		[batch 20/20] avg loss: 0.08453301753689821		[learning rate: 0.0054036]
	Learning Rate: 0.00540361
	LOSS [training: 0.07725940202763118 | validation: 0.07943599430385533]
	TIME [epoch: 8.21 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09553723100191504		[learning rate: 0.0053971]
		[batch 20/20] avg loss: 0.082588109307781		[learning rate: 0.0053905]
	Learning Rate: 0.00539053
	LOSS [training: 0.08906267015484802 | validation: 0.03633447252640636]
	TIME [epoch: 8.18 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06308240306595143		[learning rate: 0.005384]
		[batch 20/20] avg loss: 0.05783548020776704		[learning rate: 0.0053775]
	Learning Rate: 0.00537748
	LOSS [training: 0.06045894163685924 | validation: 0.045504562347799954]
	TIME [epoch: 8.2 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07401496395004405		[learning rate: 0.005371]
		[batch 20/20] avg loss: 0.07853321727480474		[learning rate: 0.0053645]
	Learning Rate: 0.00536446
	LOSS [training: 0.07627409061242439 | validation: 0.10939574496060435]
	TIME [epoch: 8.19 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0756350202777779		[learning rate: 0.005358]
		[batch 20/20] avg loss: 0.06570898476205037		[learning rate: 0.0053515]
	Learning Rate: 0.00535148
	LOSS [training: 0.07067200251991414 | validation: 0.07632226313832997]
	TIME [epoch: 8.19 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05925288124111694		[learning rate: 0.005345]
		[batch 20/20] avg loss: 0.05885457590058214		[learning rate: 0.0053385]
	Learning Rate: 0.00533852
	LOSS [training: 0.05905372857084954 | validation: 0.04490695572644903]
	TIME [epoch: 8.18 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05754448950141163		[learning rate: 0.0053321]
		[batch 20/20] avg loss: 0.07905984719344519		[learning rate: 0.0053256]
	Learning Rate: 0.0053256
	LOSS [training: 0.0683021683474284 | validation: 0.09012090888720317]
	TIME [epoch: 8.17 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08633123425120025		[learning rate: 0.0053191]
		[batch 20/20] avg loss: 0.04445000638977619		[learning rate: 0.0053127]
	Learning Rate: 0.00531271
	LOSS [training: 0.06539062032048823 | validation: 0.06319975912107623]
	TIME [epoch: 8.2 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15968631512139392		[learning rate: 0.0053063]
		[batch 20/20] avg loss: 0.10737806164180178		[learning rate: 0.0052998]
	Learning Rate: 0.00529984
	LOSS [training: 0.1335321883815978 | validation: 0.049271215039715335]
	TIME [epoch: 8.18 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09079704948995374		[learning rate: 0.0052934]
		[batch 20/20] avg loss: 0.11115590683400875		[learning rate: 0.005287]
	Learning Rate: 0.00528701
	LOSS [training: 0.10097647816198123 | validation: 0.05684073621289144]
	TIME [epoch: 8.2 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08362453461123759		[learning rate: 0.0052806]
		[batch 20/20] avg loss: 0.16785515835732373		[learning rate: 0.0052742]
	Learning Rate: 0.00527422
	LOSS [training: 0.12573984648428066 | validation: 0.09854433232953118]
	TIME [epoch: 8.2 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1293726925409554		[learning rate: 0.0052678]
		[batch 20/20] avg loss: 0.0953049182041251		[learning rate: 0.0052614]
	Learning Rate: 0.00526145
	LOSS [training: 0.11233880537254026 | validation: 0.07381157841663724]
	TIME [epoch: 8.2 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10664601643463774		[learning rate: 0.0052551]
		[batch 20/20] avg loss: 0.1302970594859187		[learning rate: 0.0052487]
	Learning Rate: 0.00524871
	LOSS [training: 0.1184715379602782 | validation: 0.1063837108257334]
	TIME [epoch: 8.18 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1052801432048441		[learning rate: 0.0052424]
		[batch 20/20] avg loss: 0.11190524679211526		[learning rate: 0.005236]
	Learning Rate: 0.005236
	LOSS [training: 0.1085926949984797 | validation: 0.04499485545093401]
	TIME [epoch: 8.2 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07029016813974025		[learning rate: 0.0052297]
		[batch 20/20] avg loss: 0.06581988910049569		[learning rate: 0.0052233]
	Learning Rate: 0.00522333
	LOSS [training: 0.06805502862011797 | validation: 0.07059548964432501]
	TIME [epoch: 8.19 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06500085286228488		[learning rate: 0.005217]
		[batch 20/20] avg loss: 0.12386878545279978		[learning rate: 0.0052107]
	Learning Rate: 0.00521068
	LOSS [training: 0.09443481915754233 | validation: 0.04501145598277974]
	TIME [epoch: 8.2 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056532637590860954		[learning rate: 0.0052044]
		[batch 20/20] avg loss: 0.0674203926165832		[learning rate: 0.0051981]
	Learning Rate: 0.00519807
	LOSS [training: 0.06197651510372206 | validation: 0.03414004591234547]
	TIME [epoch: 8.19 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10099408954089122		[learning rate: 0.0051918]
		[batch 20/20] avg loss: 0.10511990641522888		[learning rate: 0.0051855]
	Learning Rate: 0.00518549
	LOSS [training: 0.10305699797806005 | validation: 0.05764827572032552]
	TIME [epoch: 8.2 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1069707612149641		[learning rate: 0.0051792]
		[batch 20/20] avg loss: 0.20619936940048142		[learning rate: 0.0051729]
	Learning Rate: 0.00517293
	LOSS [training: 0.1565850653077228 | validation: 0.12108002771133933]
	TIME [epoch: 8.2 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2012669231352187		[learning rate: 0.0051667]
		[batch 20/20] avg loss: 0.16690638013228337		[learning rate: 0.0051604]
	Learning Rate: 0.00516041
	LOSS [training: 0.18408665163375101 | validation: 0.17482310238181265]
	TIME [epoch: 8.22 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16967925125824418		[learning rate: 0.0051542]
		[batch 20/20] avg loss: 0.0991440615102134		[learning rate: 0.0051479]
	Learning Rate: 0.00514792
	LOSS [training: 0.13441165638422875 | validation: 0.1230529889130649]
	TIME [epoch: 8.18 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11267016053022927		[learning rate: 0.0051417]
		[batch 20/20] avg loss: 0.33114895210100415		[learning rate: 0.0051355]
	Learning Rate: 0.00513546
	LOSS [training: 0.22190955631561665 | validation: 0.1428024784157471]
	TIME [epoch: 8.18 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14723399198739628		[learning rate: 0.0051292]
		[batch 20/20] avg loss: 0.39789555963139994		[learning rate: 0.005123]
	Learning Rate: 0.00512302
	LOSS [training: 0.27256477580939814 | validation: 0.2389377394602814]
	TIME [epoch: 8.21 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17249003527175238		[learning rate: 0.0051168]
		[batch 20/20] avg loss: 0.1524079616471328		[learning rate: 0.0051106]
	Learning Rate: 0.00511062
	LOSS [training: 0.16244899845944255 | validation: 0.13812895126485542]
	TIME [epoch: 8.21 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16702607016857263		[learning rate: 0.0051044]
		[batch 20/20] avg loss: 0.12207254748608312		[learning rate: 0.0050982]
	Learning Rate: 0.00509825
	LOSS [training: 0.14454930882732786 | validation: 0.05614785457151808]
	TIME [epoch: 8.18 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08954563165342902		[learning rate: 0.0050921]
		[batch 20/20] avg loss: 0.7853990960192531		[learning rate: 0.0050859]
	Learning Rate: 0.00508591
	LOSS [training: 0.437472363836341 | validation: 0.33918862744483985]
	TIME [epoch: 8.19 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2836228033692244		[learning rate: 0.0050797]
		[batch 20/20] avg loss: 0.5842102354706864		[learning rate: 0.0050736]
	Learning Rate: 0.00507359
	LOSS [training: 0.43391651941995546 | validation: 0.39253619520521255]
	TIME [epoch: 8.22 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22537316410886704		[learning rate: 0.0050674]
		[batch 20/20] avg loss: 0.16333345091555898		[learning rate: 0.0050613]
	Learning Rate: 0.00506131
	LOSS [training: 0.19435330751221297 | validation: 0.08597031513868966]
	TIME [epoch: 8.22 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11705793535188953		[learning rate: 0.0050552]
		[batch 20/20] avg loss: 0.10653372097229401		[learning rate: 0.0050491]
	Learning Rate: 0.00504906
	LOSS [training: 0.11179582816209174 | validation: 0.09997007653656857]
	TIME [epoch: 8.18 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09717356908422138		[learning rate: 0.0050429]
		[batch 20/20] avg loss: 0.07140874426506645		[learning rate: 0.0050368]
	Learning Rate: 0.00503684
	LOSS [training: 0.08429115667464392 | validation: 0.08719811643037488]
	TIME [epoch: 8.18 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08016297451734382		[learning rate: 0.0050307]
		[batch 20/20] avg loss: 0.04186962184866463		[learning rate: 0.0050246]
	Learning Rate: 0.00502464
	LOSS [training: 0.06101629818300425 | validation: 0.041129543949112585]
	TIME [epoch: 8.18 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06802166297934262		[learning rate: 0.0050186]
		[batch 20/20] avg loss: 0.07783489144265777		[learning rate: 0.0050125]
	Learning Rate: 0.00501248
	LOSS [training: 0.07292827721100019 | validation: 0.10818281012817535]
	TIME [epoch: 8.22 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05699578485272613		[learning rate: 0.0050064]
		[batch 20/20] avg loss: 0.05295518805833466		[learning rate: 0.0050003]
	Learning Rate: 0.00500034
	LOSS [training: 0.054975486455530395 | validation: 0.03457988553649474]
	TIME [epoch: 8.19 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09277625887969346		[learning rate: 0.0049943]
		[batch 20/20] avg loss: 0.08498512422927793		[learning rate: 0.0049882]
	Learning Rate: 0.00498824
	LOSS [training: 0.0888806915544857 | validation: 0.05853078878606052]
	TIME [epoch: 8.18 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05322167157366716		[learning rate: 0.0049822]
		[batch 20/20] avg loss: 0.06388993853460198		[learning rate: 0.0049762]
	Learning Rate: 0.00497616
	LOSS [training: 0.05855580505413457 | validation: 0.017265525799419063]
	TIME [epoch: 8.22 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0730253327435609		[learning rate: 0.0049701]
		[batch 20/20] avg loss: 0.06269249018109535		[learning rate: 0.0049641]
	Learning Rate: 0.00496412
	LOSS [training: 0.06785891146232811 | validation: 0.029911814974264497]
	TIME [epoch: 8.21 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05095425253617545		[learning rate: 0.0049581]
		[batch 20/20] avg loss: 0.11613718875394233		[learning rate: 0.0049521]
	Learning Rate: 0.0049521
	LOSS [training: 0.08354572064505889 | validation: 0.13417801472740196]
	TIME [epoch: 8.19 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13222183296832649		[learning rate: 0.0049461]
		[batch 20/20] avg loss: 0.07009926082041251		[learning rate: 0.0049401]
	Learning Rate: 0.00494011
	LOSS [training: 0.10116054689436951 | validation: 0.045638032924487135]
	TIME [epoch: 8.18 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062219444464968056		[learning rate: 0.0049341]
		[batch 20/20] avg loss: 0.07863878198608622		[learning rate: 0.0049282]
	Learning Rate: 0.00492815
	LOSS [training: 0.07042911322552714 | validation: 0.057143781669027144]
	TIME [epoch: 8.2 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05837005953245385		[learning rate: 0.0049222]
		[batch 20/20] avg loss: 0.05660017181153578		[learning rate: 0.0049162]
	Learning Rate: 0.00491622
	LOSS [training: 0.057485115671994824 | validation: 0.03186005933629783]
	TIME [epoch: 8.19 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10966396084391619		[learning rate: 0.0049103]
		[batch 20/20] avg loss: 0.07691573172480318		[learning rate: 0.0049043]
	Learning Rate: 0.00490432
	LOSS [training: 0.09328984628435968 | validation: 0.1245760821765737]
	TIME [epoch: 8.21 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08396336904021383		[learning rate: 0.0048984]
		[batch 20/20] avg loss: 0.05188781451627307		[learning rate: 0.0048924]
	Learning Rate: 0.00489245
	LOSS [training: 0.06792559177824345 | validation: 0.036597795267391854]
	TIME [epoch: 8.2 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07250517215284343		[learning rate: 0.0048865]
		[batch 20/20] avg loss: 0.07288377475022628		[learning rate: 0.0048806]
	Learning Rate: 0.00488061
	LOSS [training: 0.07269447345153487 | validation: 0.0444741615567806]
	TIME [epoch: 8.2 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06330800073677333		[learning rate: 0.0048747]
		[batch 20/20] avg loss: 0.07091667206815597		[learning rate: 0.0048688]
	Learning Rate: 0.00486879
	LOSS [training: 0.06711233640246464 | validation: 0.041422563519727014]
	TIME [epoch: 8.18 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05616706947800859		[learning rate: 0.0048629]
		[batch 20/20] avg loss: 0.09435242558809166		[learning rate: 0.004857]
	Learning Rate: 0.004857
	LOSS [training: 0.07525974753305013 | validation: 0.05709164912602817]
	TIME [epoch: 8.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07750852821453734		[learning rate: 0.0048511]
		[batch 20/20] avg loss: 0.05550542552643006		[learning rate: 0.0048452]
	Learning Rate: 0.00484525
	LOSS [training: 0.06650697687048371 | validation: 0.024587432961369556]
	TIME [epoch: 8.19 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06692733784981865		[learning rate: 0.0048394]
		[batch 20/20] avg loss: 0.08421638436820686		[learning rate: 0.0048335]
	Learning Rate: 0.00483352
	LOSS [training: 0.07557186110901278 | validation: 0.059527844049347765]
	TIME [epoch: 8.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06839230192934437		[learning rate: 0.0048277]
		[batch 20/20] avg loss: 0.13154654819085204		[learning rate: 0.0048218]
	Learning Rate: 0.00482181
	LOSS [training: 0.0999694250600982 | validation: 0.0824153006981633]
	TIME [epoch: 8.19 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08514102024023591		[learning rate: 0.004816]
		[batch 20/20] avg loss: 0.06450546739615906		[learning rate: 0.0048101]
	Learning Rate: 0.00481014
	LOSS [training: 0.07482324381819747 | validation: 0.04928336006278634]
	TIME [epoch: 8.19 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06945305599370152		[learning rate: 0.0048043]
		[batch 20/20] avg loss: 0.072836727615976		[learning rate: 0.0047985]
	Learning Rate: 0.0047985
	LOSS [training: 0.07114489180483877 | validation: 0.07446394380865415]
	TIME [epoch: 8.23 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09273397077699355		[learning rate: 0.0047927]
		[batch 20/20] avg loss: 0.07217689314468993		[learning rate: 0.0047869]
	Learning Rate: 0.00478688
	LOSS [training: 0.08245543196084175 | validation: 0.03655264364257402]
	TIME [epoch: 8.18 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0665745600484184		[learning rate: 0.0047811]
		[batch 20/20] avg loss: 0.056801180382918495		[learning rate: 0.0047753]
	Learning Rate: 0.00477529
	LOSS [training: 0.06168787021566845 | validation: 0.07252175935676224]
	TIME [epoch: 8.17 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.065937788788275		[learning rate: 0.0047695]
		[batch 20/20] avg loss: 0.0649776917638757		[learning rate: 0.0047637]
	Learning Rate: 0.00476373
	LOSS [training: 0.06545774027607534 | validation: 0.050536249123605714]
	TIME [epoch: 8.17 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0905736619373973		[learning rate: 0.004758]
		[batch 20/20] avg loss: 0.057081341324381654		[learning rate: 0.0047522]
	Learning Rate: 0.0047522
	LOSS [training: 0.07382750163088948 | validation: 0.053607433987634026]
	TIME [epoch: 8.23 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0616951854508503		[learning rate: 0.0047464]
		[batch 20/20] avg loss: 0.06834270747362767		[learning rate: 0.0047407]
	Learning Rate: 0.0047407
	LOSS [training: 0.06501894646223898 | validation: 0.07582514833012714]
	TIME [epoch: 8.19 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0803547614975062		[learning rate: 0.004735]
		[batch 20/20] avg loss: 0.11042952603122172		[learning rate: 0.0047292]
	Learning Rate: 0.00472922
	LOSS [training: 0.09539214376436396 | validation: 0.09209783153823752]
	TIME [epoch: 8.19 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12972276595522045		[learning rate: 0.0047235]
		[batch 20/20] avg loss: 0.09560676886761335		[learning rate: 0.0047178]
	Learning Rate: 0.00471777
	LOSS [training: 0.11266476741141691 | validation: 0.03601691682244954]
	TIME [epoch: 8.2 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09230462299199897		[learning rate: 0.0047121]
		[batch 20/20] avg loss: 0.09626359057279928		[learning rate: 0.0047064]
	Learning Rate: 0.00470635
	LOSS [training: 0.09428410678239912 | validation: 0.08137922880378584]
	TIME [epoch: 8.21 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08382211648728129		[learning rate: 0.0047006]
		[batch 20/20] avg loss: 0.08032748257433345		[learning rate: 0.004695]
	Learning Rate: 0.00469496
	LOSS [training: 0.08207479953080736 | validation: 0.13892938958665405]
	TIME [epoch: 8.18 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0933084540259108		[learning rate: 0.0046893]
		[batch 20/20] avg loss: 0.07183158683606623		[learning rate: 0.0046836]
	Learning Rate: 0.00468359
	LOSS [training: 0.08257002043098852 | validation: 0.049012299406419726]
	TIME [epoch: 8.17 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06364999018768584		[learning rate: 0.0046779]
		[batch 20/20] avg loss: 0.058292414546000926		[learning rate: 0.0046723]
	Learning Rate: 0.00467225
	LOSS [training: 0.06097120236684339 | validation: 0.03924739543107597]
	TIME [epoch: 8.19 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06450713701331758		[learning rate: 0.0046666]
		[batch 20/20] avg loss: 0.05622711379741215		[learning rate: 0.0046609]
	Learning Rate: 0.00466094
	LOSS [training: 0.06036712540536486 | validation: 0.041770859420868396]
	TIME [epoch: 8.19 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06593843986855576		[learning rate: 0.0046553]
		[batch 20/20] avg loss: 0.04507907626467603		[learning rate: 0.0046497]
	Learning Rate: 0.00464966
	LOSS [training: 0.055508758066615885 | validation: 0.027183174909094318]
	TIME [epoch: 8.2 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05717456860442456		[learning rate: 0.004644]
		[batch 20/20] avg loss: 0.05126469150398719		[learning rate: 0.0046384]
	Learning Rate: 0.0046384
	LOSS [training: 0.05421963005420587 | validation: 0.035391330598978664]
	TIME [epoch: 8.18 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05976814821808914		[learning rate: 0.0046328]
		[batch 20/20] avg loss: 0.05718338237225188		[learning rate: 0.0046272]
	Learning Rate: 0.00462717
	LOSS [training: 0.0584757652951705 | validation: 0.0706488162103714]
	TIME [epoch: 8.21 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05147171816586049		[learning rate: 0.0046216]
		[batch 20/20] avg loss: 0.061990594320134		[learning rate: 0.004616]
	Learning Rate: 0.00461597
	LOSS [training: 0.05673115624299724 | validation: 0.04537270893476325]
	TIME [epoch: 8.18 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051502452765695636		[learning rate: 0.0046104]
		[batch 20/20] avg loss: 0.06553320437234952		[learning rate: 0.0046048]
	Learning Rate: 0.0046048
	LOSS [training: 0.058517828569022566 | validation: 0.05574293484527682]
	TIME [epoch: 8.19 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06881885145055644		[learning rate: 0.0045992]
		[batch 20/20] avg loss: 0.05378883639370981		[learning rate: 0.0045936]
	Learning Rate: 0.00459365
	LOSS [training: 0.06130384392213314 | validation: 0.038585382661022766]
	TIME [epoch: 8.18 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05276021594317119		[learning rate: 0.0045881]
		[batch 20/20] avg loss: 0.048745168074237594		[learning rate: 0.0045825]
	Learning Rate: 0.00458253
	LOSS [training: 0.05075269200870439 | validation: 0.02119912596419563]
	TIME [epoch: 8.19 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07128249907957362		[learning rate: 0.004577]
		[batch 20/20] avg loss: 0.06656715779795334		[learning rate: 0.0045714]
	Learning Rate: 0.00457144
	LOSS [training: 0.06892482843876349 | validation: 0.04884220110967029]
	TIME [epoch: 8.18 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06777531739891789		[learning rate: 0.0045659]
		[batch 20/20] avg loss: 0.05937936530266338		[learning rate: 0.0045604]
	Learning Rate: 0.00456037
	LOSS [training: 0.06357734135079063 | validation: 0.0489596080504083]
	TIME [epoch: 8.18 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08650901204191917		[learning rate: 0.0045548]
		[batch 20/20] avg loss: 0.10041103997206902		[learning rate: 0.0045493]
	Learning Rate: 0.00454933
	LOSS [training: 0.0934600260069941 | validation: 0.029233809577095576]
	TIME [epoch: 8.21 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06780113030429818		[learning rate: 0.0045438]
		[batch 20/20] avg loss: 0.062131924276472794		[learning rate: 0.0045383]
	Learning Rate: 0.00453832
	LOSS [training: 0.06496652729038548 | validation: 0.02178973360973884]
	TIME [epoch: 8.2 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05788419391997255		[learning rate: 0.0045328]
		[batch 20/20] avg loss: 0.07650682786991081		[learning rate: 0.0045273]
	Learning Rate: 0.00452733
	LOSS [training: 0.06719551089494168 | validation: 0.04807595870097358]
	TIME [epoch: 8.17 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08771731869801074		[learning rate: 0.0045218]
		[batch 20/20] avg loss: 0.07204567764731572		[learning rate: 0.0045164]
	Learning Rate: 0.00451637
	LOSS [training: 0.07988149817266323 | validation: 0.023282152967529206]
	TIME [epoch: 8.17 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04838953232378239		[learning rate: 0.0045109]
		[batch 20/20] avg loss: 0.0673097902646689		[learning rate: 0.0045054]
	Learning Rate: 0.00450544
	LOSS [training: 0.057849661294225654 | validation: 0.04645000584127981]
	TIME [epoch: 8.19 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06498464313215319		[learning rate: 0.0045]
		[batch 20/20] avg loss: 0.07943136524051043		[learning rate: 0.0044945]
	Learning Rate: 0.00449453
	LOSS [training: 0.0722080041863318 | validation: 0.031872641314104354]
	TIME [epoch: 8.18 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04909039178493488		[learning rate: 0.0044891]
		[batch 20/20] avg loss: 0.06017377994098498		[learning rate: 0.0044836]
	Learning Rate: 0.00448365
	LOSS [training: 0.054632085862959937 | validation: 0.019329181268434508]
	TIME [epoch: 8.19 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0804167373141704		[learning rate: 0.0044782]
		[batch 20/20] avg loss: 0.06418841449438259		[learning rate: 0.0044728]
	Learning Rate: 0.00447279
	LOSS [training: 0.07230257590427648 | validation: 0.023401740680628755]
	TIME [epoch: 8.18 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05065388791067756		[learning rate: 0.0044674]
		[batch 20/20] avg loss: 0.0615249590663704		[learning rate: 0.004462]
	Learning Rate: 0.00446197
	LOSS [training: 0.05608942348852397 | validation: 0.01814777578905418]
	TIME [epoch: 8.19 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06339291825426788		[learning rate: 0.0044566]
		[batch 20/20] avg loss: 0.0495800924144595		[learning rate: 0.0044512]
	Learning Rate: 0.00445116
	LOSS [training: 0.05648650533436369 | validation: 0.06793868988186318]
	TIME [epoch: 8.22 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05279181580278368		[learning rate: 0.0044458]
		[batch 20/20] avg loss: 0.04269366146113537		[learning rate: 0.0044404]
	Learning Rate: 0.00444039
	LOSS [training: 0.04774273863195953 | validation: 0.08009488284549184]
	TIME [epoch: 8.17 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1281700346230075		[learning rate: 0.004435]
		[batch 20/20] avg loss: 0.04681175919441032		[learning rate: 0.0044296]
	Learning Rate: 0.00442964
	LOSS [training: 0.0874908969087089 | validation: 0.05297234751216562]
	TIME [epoch: 8.18 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060433452475143444		[learning rate: 0.0044243]
		[batch 20/20] avg loss: 0.054508401730740785		[learning rate: 0.0044189]
	Learning Rate: 0.00441892
	LOSS [training: 0.05747092710294212 | validation: 0.020096662326736726]
	TIME [epoch: 8.17 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07714080130692007		[learning rate: 0.0044136]
		[batch 20/20] avg loss: 0.05484641595086148		[learning rate: 0.0044082]
	Learning Rate: 0.00440822
	LOSS [training: 0.06599360862889078 | validation: 0.03561637123110602]
	TIME [epoch: 8.21 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060224099756718996		[learning rate: 0.0044029]
		[batch 20/20] avg loss: 0.12662699841091618		[learning rate: 0.0043975]
	Learning Rate: 0.00439755
	LOSS [training: 0.09342554908381759 | validation: 0.10267014056687188]
	TIME [epoch: 8.19 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.071543887770752		[learning rate: 0.0043922]
		[batch 20/20] avg loss: 0.06945361657381369		[learning rate: 0.0043869]
	Learning Rate: 0.0043869
	LOSS [training: 0.07049875217228282 | validation: 0.04919109030067487]
	TIME [epoch: 8.18 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07878484781753733		[learning rate: 0.0043816]
		[batch 20/20] avg loss: 0.050551445524918304		[learning rate: 0.0043763]
	Learning Rate: 0.00437628
	LOSS [training: 0.06466814667122782 | validation: 0.04141317623063943]
	TIME [epoch: 8.18 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057552469418050554		[learning rate: 0.004371]
		[batch 20/20] avg loss: 0.05435132179397843		[learning rate: 0.0043657]
	Learning Rate: 0.00436569
	LOSS [training: 0.05595189560601449 | validation: 0.03274102348531906]
	TIME [epoch: 8.23 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05841387312082319		[learning rate: 0.0043604]
		[batch 20/20] avg loss: 0.04951749151271426		[learning rate: 0.0043551]
	Learning Rate: 0.00435512
	LOSS [training: 0.05396568231676872 | validation: 0.029078232150725036]
	TIME [epoch: 8.18 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07688923030879848		[learning rate: 0.0043498]
		[batch 20/20] avg loss: 0.07224448580911749		[learning rate: 0.0043446]
	Learning Rate: 0.00434458
	LOSS [training: 0.07456685805895798 | validation: 0.033864335633239914]
	TIME [epoch: 8.17 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059039860430977434		[learning rate: 0.0043393]
		[batch 20/20] avg loss: 0.09072728825112533		[learning rate: 0.0043341]
	Learning Rate: 0.00433406
	LOSS [training: 0.07488357434105138 | validation: 0.07279889429661347]
	TIME [epoch: 8.17 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08815467709909838		[learning rate: 0.0043288]
		[batch 20/20] avg loss: 0.0967304356861335		[learning rate: 0.0043236]
	Learning Rate: 0.00432357
	LOSS [training: 0.09244255639261593 | validation: 0.07456497461290135]
	TIME [epoch: 8.18 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08392054876390362		[learning rate: 0.0043183]
		[batch 20/20] avg loss: 0.0633728972944593		[learning rate: 0.0043131]
	Learning Rate: 0.0043131
	LOSS [training: 0.07364672302918147 | validation: 0.025968604279534326]
	TIME [epoch: 8.21 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0491669826521659		[learning rate: 0.0043079]
		[batch 20/20] avg loss: 0.06536680829417157		[learning rate: 0.0043027]
	Learning Rate: 0.00430266
	LOSS [training: 0.05726689547316873 | validation: 0.08930017571386972]
	TIME [epoch: 8.18 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08987806430911126		[learning rate: 0.0042974]
		[batch 20/20] avg loss: 0.06980559243580468		[learning rate: 0.0042922]
	Learning Rate: 0.00429224
	LOSS [training: 0.07984182837245798 | validation: 0.02734914548910376]
	TIME [epoch: 8.18 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05206781730645234		[learning rate: 0.004287]
		[batch 20/20] avg loss: 0.04907017224793938		[learning rate: 0.0042819]
	Learning Rate: 0.00428185
	LOSS [training: 0.050568994777195855 | validation: 0.02499066901926088]
	TIME [epoch: 8.21 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033442526182943536		[learning rate: 0.0042767]
		[batch 20/20] avg loss: 0.05458240404419157		[learning rate: 0.0042715]
	Learning Rate: 0.00427149
	LOSS [training: 0.04401246511356755 | validation: 0.0531000890822775]
	TIME [epoch: 8.19 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06270874781136523		[learning rate: 0.0042663]
		[batch 20/20] avg loss: 0.05771473195209202		[learning rate: 0.0042611]
	Learning Rate: 0.00426114
	LOSS [training: 0.060211739881728635 | validation: 0.02206585335506014]
	TIME [epoch: 8.17 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06191006369776355		[learning rate: 0.004256]
		[batch 20/20] avg loss: 0.06063621048224278		[learning rate: 0.0042508]
	Learning Rate: 0.00425083
	LOSS [training: 0.06127313709000316 | validation: 0.03645279688709067]
	TIME [epoch: 8.17 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07392521532167287		[learning rate: 0.0042457]
		[batch 20/20] avg loss: 0.08929840361652337		[learning rate: 0.0042405]
	Learning Rate: 0.00424054
	LOSS [training: 0.0816118094690981 | validation: 0.06298984431345818]
	TIME [epoch: 8.19 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06633339040456177		[learning rate: 0.0042354]
		[batch 20/20] avg loss: 0.060109005385262385		[learning rate: 0.0042303]
	Learning Rate: 0.00423027
	LOSS [training: 0.06322119789491207 | validation: 0.15524888659105515]
	TIME [epoch: 8.18 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12513681131250545		[learning rate: 0.0042251]
		[batch 20/20] avg loss: 0.1270451416915083		[learning rate: 0.00422]
	Learning Rate: 0.00422003
	LOSS [training: 0.12609097650200687 | validation: 0.05261739782106349]
	TIME [epoch: 8.2 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1096328647002793		[learning rate: 0.0042149]
		[batch 20/20] avg loss: 0.08894707746689537		[learning rate: 0.0042098]
	Learning Rate: 0.00420982
	LOSS [training: 0.09928997108358734 | validation: 0.03747773869684816]
	TIME [epoch: 8.21 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07397144997685838		[learning rate: 0.0042047]
		[batch 20/20] avg loss: 0.07998157075026838		[learning rate: 0.0041996]
	Learning Rate: 0.00419962
	LOSS [training: 0.07697651036356337 | validation: 0.047087167660440396]
	TIME [epoch: 8.17 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05768613833291336		[learning rate: 0.0041945]
		[batch 20/20] avg loss: 0.08731666943832024		[learning rate: 0.0041895]
	Learning Rate: 0.00418946
	LOSS [training: 0.0725014038856168 | validation: 0.03684747625824031]
	TIME [epoch: 8.17 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06253797550705117		[learning rate: 0.0041844]
		[batch 20/20] avg loss: 0.07983255188063804		[learning rate: 0.0041793]
	Learning Rate: 0.00417932
	LOSS [training: 0.0711852636938446 | validation: 0.060327322934603925]
	TIME [epoch: 8.19 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04117619665909926		[learning rate: 0.0041743]
		[batch 20/20] avg loss: 0.04726585784357281		[learning rate: 0.0041692]
	Learning Rate: 0.0041692
	LOSS [training: 0.04422102725133604 | validation: 0.07856623504539503]
	TIME [epoch: 8.18 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05500657139085652		[learning rate: 0.0041641]
		[batch 20/20] avg loss: 0.04492088931830391		[learning rate: 0.0041591]
	Learning Rate: 0.00415911
	LOSS [training: 0.04996373035458021 | validation: 0.038804162422233855]
	TIME [epoch: 8.19 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08182434273194947		[learning rate: 0.0041541]
		[batch 20/20] avg loss: 0.03973833478719677		[learning rate: 0.004149]
	Learning Rate: 0.00414904
	LOSS [training: 0.060781338759573124 | validation: 0.04827028391420685]
	TIME [epoch: 8.18 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044852206645581906		[learning rate: 0.004144]
		[batch 20/20] avg loss: 0.08585542557805459		[learning rate: 0.004139]
	Learning Rate: 0.00413899
	LOSS [training: 0.06535381611181824 | validation: 0.10208040575464845]
	TIME [epoch: 8.19 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11329849463670665		[learning rate: 0.004134]
		[batch 20/20] avg loss: 0.05179508948505168		[learning rate: 0.004129]
	Learning Rate: 0.00412897
	LOSS [training: 0.08254679206087918 | validation: 0.023522743397477605]
	TIME [epoch: 8.19 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04716143809037882		[learning rate: 0.004124]
		[batch 20/20] avg loss: 0.06336999528451033		[learning rate: 0.004119]
	Learning Rate: 0.00411898
	LOSS [training: 0.055265716687444566 | validation: 0.04763591438954879]
	TIME [epoch: 8.22 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060809453932478275		[learning rate: 0.004114]
		[batch 20/20] avg loss: 0.048547844107273745		[learning rate: 0.004109]
	Learning Rate: 0.00410901
	LOSS [training: 0.05467864901987601 | validation: 0.03461126529314202]
	TIME [epoch: 8.18 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05256592384799361		[learning rate: 0.004104]
		[batch 20/20] avg loss: 0.052024042832543325		[learning rate: 0.0040991]
	Learning Rate: 0.00409906
	LOSS [training: 0.052294983340268476 | validation: 0.022146867271873743]
	TIME [epoch: 8.17 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056810360997636346		[learning rate: 0.0040941]
		[batch 20/20] avg loss: 0.057693012290581346		[learning rate: 0.0040891]
	Learning Rate: 0.00408914
	LOSS [training: 0.057251686644108846 | validation: 0.03947461171599692]
	TIME [epoch: 8.2 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05027163400397557		[learning rate: 0.0040842]
		[batch 20/20] avg loss: 0.05072853903797793		[learning rate: 0.0040792]
	Learning Rate: 0.00407924
	LOSS [training: 0.05050008652097675 | validation: 0.0346259008387881]
	TIME [epoch: 8.18 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06604404627763596		[learning rate: 0.0040743]
		[batch 20/20] avg loss: 0.06351298487253801		[learning rate: 0.0040694]
	Learning Rate: 0.00406936
	LOSS [training: 0.06477851557508699 | validation: 0.0349466670314478]
	TIME [epoch: 8.2 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05433827354569602		[learning rate: 0.0040644]
		[batch 20/20] avg loss: 0.0700675610894127		[learning rate: 0.0040595]
	Learning Rate: 0.00405951
	LOSS [training: 0.062202917317554375 | validation: 0.050462741199890755]
	TIME [epoch: 8.18 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04680410000572828		[learning rate: 0.0040546]
		[batch 20/20] avg loss: 0.052894381617341925		[learning rate: 0.0040497]
	Learning Rate: 0.00404968
	LOSS [training: 0.0498492408115351 | validation: 0.038630640625308195]
	TIME [epoch: 8.2 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06443999011517751		[learning rate: 0.0040448]
		[batch 20/20] avg loss: 0.05442007195214872		[learning rate: 0.0040399]
	Learning Rate: 0.00403988
	LOSS [training: 0.05943003103366312 | validation: 0.03003717676417589]
	TIME [epoch: 8.22 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059980030088016036		[learning rate: 0.004035]
		[batch 20/20] avg loss: 0.07315322632621538		[learning rate: 0.0040301]
	Learning Rate: 0.0040301
	LOSS [training: 0.06656662820711572 | validation: 0.040057322008616225]
	TIME [epoch: 8.18 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06309715146041618		[learning rate: 0.0040252]
		[batch 20/20] avg loss: 0.041928525757200075		[learning rate: 0.0040203]
	Learning Rate: 0.00402034
	LOSS [training: 0.052512838608808135 | validation: 0.033855857549207585]
	TIME [epoch: 8.18 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056983957672968634		[learning rate: 0.0040155]
		[batch 20/20] avg loss: 0.05528904715750181		[learning rate: 0.0040106]
	Learning Rate: 0.00401061
	LOSS [training: 0.056136502415235226 | validation: 0.019120746685345004]
	TIME [epoch: 8.18 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03477781820055505		[learning rate: 0.0040058]
		[batch 20/20] avg loss: 0.09653318398349599		[learning rate: 0.0040009]
	Learning Rate: 0.0040009
	LOSS [training: 0.06565550109202553 | validation: 0.11767123943468907]
	TIME [epoch: 8.22 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11114322747716465		[learning rate: 0.0039961]
		[batch 20/20] avg loss: 0.07200591902116768		[learning rate: 0.0039912]
	Learning Rate: 0.00399122
	LOSS [training: 0.09157457324916615 | validation: 0.07388459000109768]
	TIME [epoch: 8.18 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06758363665618623		[learning rate: 0.0039864]
		[batch 20/20] avg loss: 0.07430523226415597		[learning rate: 0.0039816]
	Learning Rate: 0.00398155
	LOSS [training: 0.0709444344601711 | validation: 0.04622214217617496]
	TIME [epoch: 8.19 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06368011232752227		[learning rate: 0.0039767]
		[batch 20/20] avg loss: 0.08488200393713721		[learning rate: 0.0039719]
	Learning Rate: 0.00397192
	LOSS [training: 0.07428105813232974 | validation: 0.0907775309005645]
	TIME [epoch: 8.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10196624946730644		[learning rate: 0.0039671]
		[batch 20/20] avg loss: 0.08460192755833174		[learning rate: 0.0039623]
	Learning Rate: 0.0039623
	LOSS [training: 0.09328408851281908 | validation: 0.050275361048631804]
	TIME [epoch: 8.23 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0838408499372101		[learning rate: 0.0039575]
		[batch 20/20] avg loss: 0.10445373587479645		[learning rate: 0.0039527]
	Learning Rate: 0.00395271
	LOSS [training: 0.0941472929060033 | validation: 0.09472710350892104]
	TIME [epoch: 8.18 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1141190793498538		[learning rate: 0.0039479]
		[batch 20/20] avg loss: 0.10136152614091735		[learning rate: 0.0039431]
	Learning Rate: 0.00394314
	LOSS [training: 0.10774030274538557 | validation: 0.04320240918870301]
	TIME [epoch: 8.18 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08244609620529712		[learning rate: 0.0039384]
		[batch 20/20] avg loss: 0.11716911824866783		[learning rate: 0.0039336]
	Learning Rate: 0.00393359
	LOSS [training: 0.09980760722698248 | validation: 0.05358271622142412]
	TIME [epoch: 8.18 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07595841194100142		[learning rate: 0.0039288]
		[batch 20/20] avg loss: 0.09754673959204438		[learning rate: 0.0039241]
	Learning Rate: 0.00392407
	LOSS [training: 0.0867525757665229 | validation: 0.05801008735938079]
	TIME [epoch: 8.2 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10179845575389754		[learning rate: 0.0039193]
		[batch 20/20] avg loss: 0.05954355408198234		[learning rate: 0.0039146]
	Learning Rate: 0.00391457
	LOSS [training: 0.08067100491793992 | validation: 0.07206026370390065]
	TIME [epoch: 8.21 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0813119394837298		[learning rate: 0.0039098]
		[batch 20/20] avg loss: 0.0361164843770849		[learning rate: 0.0039051]
	Learning Rate: 0.00390509
	LOSS [training: 0.05871421193040736 | validation: 0.012166315217492762]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04044585294032842		[learning rate: 0.0039004]
		[batch 20/20] avg loss: 0.0447452452987201		[learning rate: 0.0038956]
	Learning Rate: 0.00389564
	LOSS [training: 0.04259554911952425 | validation: 0.04803253324669193]
	TIME [epoch: 8.2 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11258654757831202		[learning rate: 0.0038909]
		[batch 20/20] avg loss: 0.19044695837127384		[learning rate: 0.0038862]
	Learning Rate: 0.00388621
	LOSS [training: 0.15151675297479292 | validation: 0.08658687593113659]
	TIME [epoch: 8.17 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06759448870167402		[learning rate: 0.0038815]
		[batch 20/20] avg loss: 0.09673278948107975		[learning rate: 0.0038768]
	Learning Rate: 0.0038768
	LOSS [training: 0.08216363909137689 | validation: 0.07141468845345232]
	TIME [epoch: 8.18 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0679049895248552		[learning rate: 0.0038721]
		[batch 20/20] avg loss: 0.0816841012917759		[learning rate: 0.0038674]
	Learning Rate: 0.00386742
	LOSS [training: 0.07479454540831555 | validation: 0.05196023450525747]
	TIME [epoch: 8.17 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08384277114654698		[learning rate: 0.0038627]
		[batch 20/20] avg loss: 0.08754858182017225		[learning rate: 0.0038581]
	Learning Rate: 0.00385805
	LOSS [training: 0.0856956764833596 | validation: 0.04635558116701844]
	TIME [epoch: 8.16 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06514039022032829		[learning rate: 0.0038534]
		[batch 20/20] avg loss: 0.08908559705234367		[learning rate: 0.0038487]
	Learning Rate: 0.00384871
	LOSS [training: 0.07711299363633597 | validation: 0.03289079149402312]
	TIME [epoch: 8.16 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0565018836479023		[learning rate: 0.0038441]
		[batch 20/20] avg loss: 0.08001761385210561		[learning rate: 0.0038394]
	Learning Rate: 0.0038394
	LOSS [training: 0.06825974875000394 | validation: 0.1106993190253577]
	TIME [epoch: 8.21 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10905198715246747		[learning rate: 0.0038347]
		[batch 20/20] avg loss: 0.0906745515329312		[learning rate: 0.0038301]
	Learning Rate: 0.0038301
	LOSS [training: 0.09986326934269935 | validation: 0.07435895106310716]
	TIME [epoch: 8.18 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06389826434695267		[learning rate: 0.0038255]
		[batch 20/20] avg loss: 0.061012159767098076		[learning rate: 0.0038208]
	Learning Rate: 0.00382083
	LOSS [training: 0.06245521205702538 | validation: 0.12193701102750296]
	TIME [epoch: 8.17 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11180830591434192		[learning rate: 0.0038162]
		[batch 20/20] avg loss: 0.21221622212076724		[learning rate: 0.0038116]
	Learning Rate: 0.00381158
	LOSS [training: 0.16201226401755459 | validation: 0.174637293491938]
	TIME [epoch: 8.19 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24842875257179528		[learning rate: 0.003807]
		[batch 20/20] avg loss: 0.2569562647721868		[learning rate: 0.0038024]
	Learning Rate: 0.00380235
	LOSS [training: 0.25269250867199106 | validation: 0.3236762469130713]
	TIME [epoch: 8.18 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2299937480477198		[learning rate: 0.0037977]
		[batch 20/20] avg loss: 0.14416090921736574		[learning rate: 0.0037931]
	Learning Rate: 0.00379315
	LOSS [training: 0.1870773286325428 | validation: 0.12215383401178838]
	TIME [epoch: 8.19 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11079602851989281		[learning rate: 0.0037886]
		[batch 20/20] avg loss: 0.09125467376934977		[learning rate: 0.003784]
	Learning Rate: 0.00378397
	LOSS [training: 0.10102535114462127 | validation: 0.05166366640440612]
	TIME [epoch: 8.16 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05945187349156652		[learning rate: 0.0037794]
		[batch 20/20] avg loss: 0.07114154560709071		[learning rate: 0.0037748]
	Learning Rate: 0.00377481
	LOSS [training: 0.06529670954932862 | validation: 0.03453886218174457]
	TIME [epoch: 8.16 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07037751268393558		[learning rate: 0.0037702]
		[batch 20/20] avg loss: 0.05563305791079638		[learning rate: 0.0037657]
	Learning Rate: 0.00376567
	LOSS [training: 0.06300528529736599 | validation: 0.028483447106609404]
	TIME [epoch: 8.18 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05421853341213734		[learning rate: 0.0037611]
		[batch 20/20] avg loss: 0.07762561455336645		[learning rate: 0.0037566]
	Learning Rate: 0.00375655
	LOSS [training: 0.06592207398275189 | validation: 0.032630961750246726]
	TIME [epoch: 8.19 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047897321106829926		[learning rate: 0.003752]
		[batch 20/20] avg loss: 0.06615589877305092		[learning rate: 0.0037475]
	Learning Rate: 0.00374746
	LOSS [training: 0.05702660993994042 | validation: 0.09196851107817386]
	TIME [epoch: 8.18 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05502250007024312		[learning rate: 0.0037429]
		[batch 20/20] avg loss: 0.08028106117343807		[learning rate: 0.0037384]
	Learning Rate: 0.00373839
	LOSS [training: 0.06765178062184057 | validation: 0.09216851261498334]
	TIME [epoch: 8.19 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05553599048620047		[learning rate: 0.0037339]
		[batch 20/20] avg loss: 0.036025170762595086		[learning rate: 0.0037293]
	Learning Rate: 0.00372934
	LOSS [training: 0.045780580624397785 | validation: 0.03410617344018187]
	TIME [epoch: 8.19 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04310954565699969		[learning rate: 0.0037248]
		[batch 20/20] avg loss: 0.050709973111973616		[learning rate: 0.0037203]
	Learning Rate: 0.00372031
	LOSS [training: 0.04690975938448667 | validation: 0.03784872548360441]
	TIME [epoch: 8.17 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04151389724054498		[learning rate: 0.0037158]
		[batch 20/20] avg loss: 0.045657189230096804		[learning rate: 0.0037113]
	Learning Rate: 0.0037113
	LOSS [training: 0.04358554323532089 | validation: 0.04083124569734038]
	TIME [epoch: 8.19 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049678974654056095		[learning rate: 0.0037068]
		[batch 20/20] avg loss: 0.07366349279287451		[learning rate: 0.0037023]
	Learning Rate: 0.00370232
	LOSS [training: 0.0616712337234653 | validation: 0.06817594443467072]
	TIME [epoch: 8.16 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07006000166992493		[learning rate: 0.0036978]
		[batch 20/20] avg loss: 0.050221683216693645		[learning rate: 0.0036934]
	Learning Rate: 0.00369336
	LOSS [training: 0.06014084244330927 | validation: 0.03621095160790326]
	TIME [epoch: 8.17 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05774921709796847		[learning rate: 0.0036889]
		[batch 20/20] avg loss: 0.07776896203479718		[learning rate: 0.0036844]
	Learning Rate: 0.00368441
	LOSS [training: 0.06775908956638282 | validation: 0.09662923796750683]
	TIME [epoch: 8.19 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08322050050010657		[learning rate: 0.00368]
		[batch 20/20] avg loss: 0.05228884266758449		[learning rate: 0.0036755]
	Learning Rate: 0.00367549
	LOSS [training: 0.06775467158384554 | validation: 0.08834873817049779]
	TIME [epoch: 8.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05194712413120145		[learning rate: 0.003671]
		[batch 20/20] avg loss: 0.03868600944680282		[learning rate: 0.0036666]
	Learning Rate: 0.0036666
	LOSS [training: 0.04531656678900213 | validation: 0.032896950315278305]
	TIME [epoch: 8.18 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060354120405930335		[learning rate: 0.0036622]
		[batch 20/20] avg loss: 0.05079496957362546		[learning rate: 0.0036577]
	Learning Rate: 0.00365772
	LOSS [training: 0.05557454498977791 | validation: 0.02225423830914759]
	TIME [epoch: 8.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0480913239369909		[learning rate: 0.0036533]
		[batch 20/20] avg loss: 0.05460464104558791		[learning rate: 0.0036489]
	Learning Rate: 0.00364887
	LOSS [training: 0.0513479824912894 | validation: 0.023609027680765036]
	TIME [epoch: 8.17 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03950702705546107		[learning rate: 0.0036444]
		[batch 20/20] avg loss: 0.04292251125223804		[learning rate: 0.00364]
	Learning Rate: 0.00364003
	LOSS [training: 0.04121476915384956 | validation: 0.01752637480012738]
	TIME [epoch: 8.18 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03569346855769477		[learning rate: 0.0036356]
		[batch 20/20] avg loss: 0.038848848046208036		[learning rate: 0.0036312]
	Learning Rate: 0.00363122
	LOSS [training: 0.03727115830195141 | validation: 0.017303407409841876]
	TIME [epoch: 8.18 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035869380480257965		[learning rate: 0.0036268]
		[batch 20/20] avg loss: 0.043324699464195635		[learning rate: 0.0036224]
	Learning Rate: 0.00362243
	LOSS [training: 0.03959703997222679 | validation: 0.023878240775500856]
	TIME [epoch: 8.17 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05798831983671138		[learning rate: 0.003618]
		[batch 20/20] avg loss: 0.05389888363721072		[learning rate: 0.0036137]
	Learning Rate: 0.00361366
	LOSS [training: 0.05594360173696107 | validation: 0.035769452726533875]
	TIME [epoch: 8.18 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07011012643854919		[learning rate: 0.0036093]
		[batch 20/20] avg loss: 0.10106750400977413		[learning rate: 0.0036049]
	Learning Rate: 0.00360491
	LOSS [training: 0.08558881522416165 | validation: 0.07379932019691851]
	TIME [epoch: 8.18 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08312690239969663		[learning rate: 0.0036005]
		[batch 20/20] avg loss: 0.04955006748550317		[learning rate: 0.0035962]
	Learning Rate: 0.00359619
	LOSS [training: 0.0663384849425999 | validation: 0.0676982178244085]
	TIME [epoch: 8.2 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04777516200064551		[learning rate: 0.0035918]
		[batch 20/20] avg loss: 0.05202167910316735		[learning rate: 0.0035875]
	Learning Rate: 0.00358748
	LOSS [training: 0.04989842055190643 | validation: 0.026217197016958113]
	TIME [epoch: 8.18 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05351438098292118		[learning rate: 0.0035831]
		[batch 20/20] avg loss: 0.07340409794414995		[learning rate: 0.0035788]
	Learning Rate: 0.0035788
	LOSS [training: 0.06345923946353557 | validation: 0.0550058981440671]
	TIME [epoch: 8.21 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051399129636354325		[learning rate: 0.0035745]
		[batch 20/20] avg loss: 0.05141955235270198		[learning rate: 0.0035701]
	Learning Rate: 0.00357013
	LOSS [training: 0.05140934099452815 | validation: 0.044809242681538186]
	TIME [epoch: 8.17 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06118491611856525		[learning rate: 0.0035658]
		[batch 20/20] avg loss: 0.04711455457136962		[learning rate: 0.0035615]
	Learning Rate: 0.00356149
	LOSS [training: 0.05414973534496744 | validation: 0.09202752821382294]
	TIME [epoch: 8.19 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06100975708198366		[learning rate: 0.0035572]
		[batch 20/20] avg loss: 0.04384445503493832		[learning rate: 0.0035529]
	Learning Rate: 0.00355287
	LOSS [training: 0.052427106058461 | validation: 0.0198345516008634]
	TIME [epoch: 8.17 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05360422018448528		[learning rate: 0.0035486]
		[batch 20/20] avg loss: 0.034340041871357696		[learning rate: 0.0035443]
	Learning Rate: 0.00354427
	LOSS [training: 0.04397213102792149 | validation: 0.04441315317797885]
	TIME [epoch: 8.16 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046099548814827356		[learning rate: 0.00354]
		[batch 20/20] avg loss: 0.04286218859367813		[learning rate: 0.0035357]
	Learning Rate: 0.00353569
	LOSS [training: 0.044480868704252746 | validation: 0.01892442096860005]
	TIME [epoch: 8.17 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03982716976090279		[learning rate: 0.0035314]
		[batch 20/20] avg loss: 0.04424480112511457		[learning rate: 0.0035271]
	Learning Rate: 0.00352713
	LOSS [training: 0.04203598544300867 | validation: 0.025031273667029567]
	TIME [epoch: 8.17 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0526338856269077		[learning rate: 0.0035229]
		[batch 20/20] avg loss: 0.061308511207490035		[learning rate: 0.0035186]
	Learning Rate: 0.00351859
	LOSS [training: 0.05697119841719885 | validation: 0.034577863105476773]
	TIME [epoch: 8.21 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05307399540880775		[learning rate: 0.0035143]
		[batch 20/20] avg loss: 0.06400728198212627		[learning rate: 0.0035101]
	Learning Rate: 0.00351007
	LOSS [training: 0.058540638695467016 | validation: 0.06807963473850526]
	TIME [epoch: 8.17 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0678329757393776		[learning rate: 0.0035058]
		[batch 20/20] avg loss: 0.039401774352720645		[learning rate: 0.0035016]
	Learning Rate: 0.00350157
	LOSS [training: 0.05361737504604912 | validation: 0.027455014353968982]
	TIME [epoch: 8.18 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03874983589521806		[learning rate: 0.0034973]
		[batch 20/20] avg loss: 0.0579305685793266		[learning rate: 0.0034931]
	Learning Rate: 0.0034931
	LOSS [training: 0.048340202237272326 | validation: 0.06991683763141372]
	TIME [epoch: 8.2 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04760047215979783		[learning rate: 0.0034889]
		[batch 20/20] avg loss: 0.042572611771382184		[learning rate: 0.0034846]
	Learning Rate: 0.00348464
	LOSS [training: 0.04508654196559001 | validation: 0.026980974628401922]
	TIME [epoch: 8.19 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034265401261006787		[learning rate: 0.0034804]
		[batch 20/20] avg loss: 0.03434732378504505		[learning rate: 0.0034762]
	Learning Rate: 0.0034762
	LOSS [training: 0.03430636252302592 | validation: 0.03134656574477249]
	TIME [epoch: 8.17 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047302652727126124		[learning rate: 0.003472]
		[batch 20/20] avg loss: 0.037354713410593696		[learning rate: 0.0034678]
	Learning Rate: 0.00346779
	LOSS [training: 0.042328683068859924 | validation: 0.036277957584646095]
	TIME [epoch: 8.17 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04413394861813615		[learning rate: 0.0034636]
		[batch 20/20] avg loss: 0.04754864440537522		[learning rate: 0.0034594]
	Learning Rate: 0.00345939
	LOSS [training: 0.04584129651175569 | validation: 0.027236927449697957]
	TIME [epoch: 8.17 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04028662585765899		[learning rate: 0.0034552]
		[batch 20/20] avg loss: 0.04034719018580966		[learning rate: 0.003451]
	Learning Rate: 0.00345102
	LOSS [training: 0.040316908021734335 | validation: 0.035332030624908674]
	TIME [epoch: 8.17 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04560411386691274		[learning rate: 0.0034468]
		[batch 20/20] avg loss: 0.04628673886221781		[learning rate: 0.0034427]
	Learning Rate: 0.00344266
	LOSS [training: 0.04594542636456527 | validation: 0.04864912078225133]
	TIME [epoch: 8.21 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053581024811501655		[learning rate: 0.0034385]
		[batch 20/20] avg loss: 0.048519326099580574		[learning rate: 0.0034343]
	Learning Rate: 0.00343433
	LOSS [training: 0.051050175455541115 | validation: 0.028515053756614284]
	TIME [epoch: 8.17 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03335123814683004		[learning rate: 0.0034302]
		[batch 20/20] avg loss: 0.04677381821004034		[learning rate: 0.003426]
	Learning Rate: 0.00342602
	LOSS [training: 0.04006252817843518 | validation: 0.05318135243435798]
	TIME [epoch: 8.17 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038236070170395806		[learning rate: 0.0034219]
		[batch 20/20] avg loss: 0.032132366223074436		[learning rate: 0.0034177]
	Learning Rate: 0.00341772
	LOSS [training: 0.03518421819673512 | validation: 0.017805899277866355]
	TIME [epoch: 8.19 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03288659234922834		[learning rate: 0.0034136]
		[batch 20/20] avg loss: 0.053361878329510726		[learning rate: 0.0034094]
	Learning Rate: 0.00340945
	LOSS [training: 0.043124235339369536 | validation: 0.043799702997026284]
	TIME [epoch: 8.21 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047276286872942455		[learning rate: 0.0034053]
		[batch 20/20] avg loss: 0.05672701484119229		[learning rate: 0.0034012]
	Learning Rate: 0.0034012
	LOSS [training: 0.052001650857067386 | validation: 0.02246030558375832]
	TIME [epoch: 8.17 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0394408767944492		[learning rate: 0.0033971]
		[batch 20/20] avg loss: 0.05326661913623756		[learning rate: 0.003393]
	Learning Rate: 0.00339296
	LOSS [training: 0.046353747965343375 | validation: 0.06654205625354172]
	TIME [epoch: 8.17 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13627157401699153		[learning rate: 0.0033889]
		[batch 20/20] avg loss: 0.08385398194639922		[learning rate: 0.0033847]
	Learning Rate: 0.00338475
	LOSS [training: 0.11006277798169536 | validation: 0.07207318419005034]
	TIME [epoch: 8.17 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11639672189348596		[learning rate: 0.0033806]
		[batch 20/20] avg loss: 0.08439019380325279		[learning rate: 0.0033766]
	Learning Rate: 0.00337655
	LOSS [training: 0.1003934578483694 | validation: 0.16645127125063133]
	TIME [epoch: 8.19 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19003766504290084		[learning rate: 0.0033725]
		[batch 20/20] avg loss: 0.12368169397086441		[learning rate: 0.0033684]
	Learning Rate: 0.00336838
	LOSS [training: 0.15685967950688265 | validation: 0.08982979808750591]
	TIME [epoch: 8.21 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11986055356977376		[learning rate: 0.0033643]
		[batch 20/20] avg loss: 0.182996715815721		[learning rate: 0.0033602]
	Learning Rate: 0.00336023
	LOSS [training: 0.15142863469274734 | validation: 0.09733869031972073]
	TIME [epoch: 8.17 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09780651821737119		[learning rate: 0.0033562]
		[batch 20/20] avg loss: 0.07754554885954841		[learning rate: 0.0033521]
	Learning Rate: 0.00335209
	LOSS [training: 0.0876760335384598 | validation: 0.12228999248347606]
	TIME [epoch: 8.17 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09308824201709916		[learning rate: 0.003348]
		[batch 20/20] avg loss: 0.07227925092352411		[learning rate: 0.003344]
	Learning Rate: 0.00334398
	LOSS [training: 0.08268374647031165 | validation: 0.027986041113124756]
	TIME [epoch: 8.21 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05021081591343821		[learning rate: 0.0033399]
		[batch 20/20] avg loss: 0.03555793965215501		[learning rate: 0.0033359]
	Learning Rate: 0.00333588
	LOSS [training: 0.04288437778279661 | validation: 0.026966877835361246]
	TIME [epoch: 8.2 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056468716475768135		[learning rate: 0.0033318]
		[batch 20/20] avg loss: 0.04822348159727634		[learning rate: 0.0033278]
	Learning Rate: 0.00332781
	LOSS [training: 0.052346099036522244 | validation: 0.033193559274333734]
	TIME [epoch: 8.17 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04093613881963431		[learning rate: 0.0033238]
		[batch 20/20] avg loss: 0.05820141082073692		[learning rate: 0.0033197]
	Learning Rate: 0.00331975
	LOSS [training: 0.04956877482018561 | validation: 0.040522251369012496]
	TIME [epoch: 8.17 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051347696102567494		[learning rate: 0.0033157]
		[batch 20/20] avg loss: 0.07727240955969247		[learning rate: 0.0033117]
	Learning Rate: 0.00331171
	LOSS [training: 0.06431005283112998 | validation: 0.00618232663598773]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04311702025453081		[learning rate: 0.0033077]
		[batch 20/20] avg loss: 0.05486062205079514		[learning rate: 0.0033037]
	Learning Rate: 0.0033037
	LOSS [training: 0.04898882115266298 | validation: 0.045316218656317506]
	TIME [epoch: 8.21 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06432306467285867		[learning rate: 0.0032997]
		[batch 20/20] avg loss: 0.06328203544239579		[learning rate: 0.0032957]
	Learning Rate: 0.0032957
	LOSS [training: 0.06380255005762724 | validation: 0.04426490869341296]
	TIME [epoch: 8.19 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06546563605659288		[learning rate: 0.0032917]
		[batch 20/20] avg loss: 0.038167457001046304		[learning rate: 0.0032877]
	Learning Rate: 0.00328772
	LOSS [training: 0.05181654652881958 | validation: 0.021843312555713916]
	TIME [epoch: 8.18 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03968103009748716		[learning rate: 0.0032837]
		[batch 20/20] avg loss: 0.053936607800411526		[learning rate: 0.0032798]
	Learning Rate: 0.00327976
	LOSS [training: 0.04680881894894935 | validation: 0.025542403919389032]
	TIME [epoch: 8.18 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0708798275432985		[learning rate: 0.0032758]
		[batch 20/20] avg loss: 0.07351175903673828		[learning rate: 0.0032718]
	Learning Rate: 0.00327182
	LOSS [training: 0.07219579329001838 | validation: 0.06029298028913486]
	TIME [epoch: 8.21 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05459174854961947		[learning rate: 0.0032679]
		[batch 20/20] avg loss: 0.06128608805942772		[learning rate: 0.0032639]
	Learning Rate: 0.0032639
	LOSS [training: 0.0579389183045236 | validation: 0.027614216104840154]
	TIME [epoch: 8.19 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04526001739655159		[learning rate: 0.0032599]
		[batch 20/20] avg loss: 0.03584100827105991		[learning rate: 0.003256]
	Learning Rate: 0.003256
	LOSS [training: 0.040550512833805744 | validation: 0.02947975125520843]
	TIME [epoch: 8.16 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0523402617207853		[learning rate: 0.0032521]
		[batch 20/20] avg loss: 0.06586019295638493		[learning rate: 0.0032481]
	Learning Rate: 0.00324812
	LOSS [training: 0.05910022733858511 | validation: 0.017425892220231325]
	TIME [epoch: 8.17 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04377156034774307		[learning rate: 0.0032442]
		[batch 20/20] avg loss: 0.04054529368934693		[learning rate: 0.0032403]
	Learning Rate: 0.00324025
	LOSS [training: 0.042158427018545 | validation: 0.024224867905253623]
	TIME [epoch: 8.17 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04253185601648969		[learning rate: 0.0032363]
		[batch 20/20] avg loss: 0.04193566890712882		[learning rate: 0.0032324]
	Learning Rate: 0.00323241
	LOSS [training: 0.04223376246180926 | validation: 0.055091252226618176]
	TIME [epoch: 8.19 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06233003815723455		[learning rate: 0.0032285]
		[batch 20/20] avg loss: 0.054982554566720396		[learning rate: 0.0032246]
	Learning Rate: 0.00322458
	LOSS [training: 0.058656296361977465 | validation: 0.03710922422195746]
	TIME [epoch: 8.17 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038416010590559366		[learning rate: 0.0032207]
		[batch 20/20] avg loss: 0.0584515939194458		[learning rate: 0.0032168]
	Learning Rate: 0.00321678
	LOSS [training: 0.04843380225500258 | validation: 0.013286127607473004]
	TIME [epoch: 8.17 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03346811912468305		[learning rate: 0.0032129]
		[batch 20/20] avg loss: 0.03766798605620985		[learning rate: 0.003209]
	Learning Rate: 0.00320899
	LOSS [training: 0.03556805259044645 | validation: 0.02729237881237294]
	TIME [epoch: 8.17 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05942487950959693		[learning rate: 0.0032051]
		[batch 20/20] avg loss: 0.027397961630002677		[learning rate: 0.0032012]
	Learning Rate: 0.00320122
	LOSS [training: 0.04341142056979981 | validation: 0.02592818836687215]
	TIME [epoch: 8.17 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0552568943191203		[learning rate: 0.0031973]
		[batch 20/20] avg loss: 0.05089200692909497		[learning rate: 0.0031935]
	Learning Rate: 0.00319347
	LOSS [training: 0.053074450624107636 | validation: 0.01556133796983822]
	TIME [epoch: 8.19 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03892787615963615		[learning rate: 0.0031896]
		[batch 20/20] avg loss: 0.04748563892063454		[learning rate: 0.0031857]
	Learning Rate: 0.00318574
	LOSS [training: 0.04320675754013533 | validation: 0.03344453641030644]
	TIME [epoch: 8.17 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045070084474281444		[learning rate: 0.0031819]
		[batch 20/20] avg loss: 0.04653556173527056		[learning rate: 0.003178]
	Learning Rate: 0.00317803
	LOSS [training: 0.045802823104776 | validation: 0.013303133344827061]
	TIME [epoch: 8.17 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04927623880576417		[learning rate: 0.0031742]
		[batch 20/20] avg loss: 0.06910595027922771		[learning rate: 0.0031703]
	Learning Rate: 0.00317034
	LOSS [training: 0.05919109454249594 | validation: 0.028003833999303646]
	TIME [epoch: 8.19 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03978480668833518		[learning rate: 0.0031665]
		[batch 20/20] avg loss: 0.05809003423898211		[learning rate: 0.0031627]
	Learning Rate: 0.00316266
	LOSS [training: 0.04893742046365865 | validation: 0.019697426738454323]
	TIME [epoch: 8.21 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05121664796880663		[learning rate: 0.0031588]
		[batch 20/20] avg loss: 0.04745839297373823		[learning rate: 0.003155]
	Learning Rate: 0.003155
	LOSS [training: 0.04933752047127242 | validation: 0.03475459177915048]
	TIME [epoch: 8.18 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037489509286095186		[learning rate: 0.0031512]
		[batch 20/20] avg loss: 0.040529039072017915		[learning rate: 0.0031474]
	Learning Rate: 0.00314737
	LOSS [training: 0.039009274179056544 | validation: 0.0204441757344576]
	TIME [epoch: 8.17 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03344826029612486		[learning rate: 0.0031436]
		[batch 20/20] avg loss: 0.04220209693450583		[learning rate: 0.0031397]
	Learning Rate: 0.00313975
	LOSS [training: 0.03782517861531534 | validation: 0.05870564599198618]
	TIME [epoch: 8.21 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07795864198495384		[learning rate: 0.0031359]
		[batch 20/20] avg loss: 0.061026883227909536		[learning rate: 0.0031321]
	Learning Rate: 0.00313215
	LOSS [training: 0.06949276260643168 | validation: 0.07354166644118784]
	TIME [epoch: 8.18 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06455133267189427		[learning rate: 0.0031284]
		[batch 20/20] avg loss: 0.0844089485915879		[learning rate: 0.0031246]
	Learning Rate: 0.00312456
	LOSS [training: 0.0744801406317411 | validation: 0.0861168259418933]
	TIME [epoch: 8.18 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08090745446375387		[learning rate: 0.0031208]
		[batch 20/20] avg loss: 0.06464246018193134		[learning rate: 0.003117]
	Learning Rate: 0.003117
	LOSS [training: 0.0727749573228426 | validation: 0.050838788041380256]
	TIME [epoch: 8.17 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031554283059570934		[learning rate: 0.0031132]
		[batch 20/20] avg loss: 0.0495143839785335		[learning rate: 0.0031095]
	Learning Rate: 0.00310945
	LOSS [training: 0.040534333519052224 | validation: 0.01666259735897515]
	TIME [epoch: 8.17 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047799615801295076		[learning rate: 0.0031057]
		[batch 20/20] avg loss: 0.09541007494907935		[learning rate: 0.0031019]
	Learning Rate: 0.00310193
	LOSS [training: 0.07160484537518721 | validation: 0.11662305211350525]
	TIME [epoch: 8.17 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08958240181776907		[learning rate: 0.0030982]
		[batch 20/20] avg loss: 0.06469538068406251		[learning rate: 0.0030944]
	Learning Rate: 0.00309442
	LOSS [training: 0.07713889125091578 | validation: 0.0645348035110808]
	TIME [epoch: 8.2 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04888619171249448		[learning rate: 0.0030907]
		[batch 20/20] avg loss: 0.03148108794863894		[learning rate: 0.0030869]
	Learning Rate: 0.00308693
	LOSS [training: 0.04018363983056672 | validation: 0.04189925272136786]
	TIME [epoch: 8.17 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08114881109997671		[learning rate: 0.0030832]
		[batch 20/20] avg loss: 0.047532650700665126		[learning rate: 0.0030795]
	Learning Rate: 0.00307945
	LOSS [training: 0.06434073090032091 | validation: 0.03364534826232628]
	TIME [epoch: 8.19 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042300382221108254		[learning rate: 0.0030757]
		[batch 20/20] avg loss: 0.05248098323392072		[learning rate: 0.003072]
	Learning Rate: 0.003072
	LOSS [training: 0.04739068272751449 | validation: 0.029865760143152228]
	TIME [epoch: 8.17 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04884325592125228		[learning rate: 0.0030683]
		[batch 20/20] avg loss: 0.04373925841682038		[learning rate: 0.0030646]
	Learning Rate: 0.00306456
	LOSS [training: 0.04629125716903633 | validation: 0.0301279361006273]
	TIME [epoch: 8.2 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04696540338586534		[learning rate: 0.0030609]
		[batch 20/20] avg loss: 0.028193527162691377		[learning rate: 0.0030571]
	Learning Rate: 0.00305714
	LOSS [training: 0.03757946527427836 | validation: 0.011553085543297294]
	TIME [epoch: 8.18 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022855569826021417		[learning rate: 0.0030534]
		[batch 20/20] avg loss: 0.04071798258649047		[learning rate: 0.0030497]
	Learning Rate: 0.00304974
	LOSS [training: 0.03178677620625595 | validation: 0.02338673085728222]
	TIME [epoch: 8.2 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0334255199406982		[learning rate: 0.003046]
		[batch 20/20] avg loss: 0.09275765288426616		[learning rate: 0.0030424]
	Learning Rate: 0.00304236
	LOSS [training: 0.06309158641248219 | validation: 0.018962863599837794]
	TIME [epoch: 8.17 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05059734093285145		[learning rate: 0.0030387]
		[batch 20/20] avg loss: 0.06730887004830506		[learning rate: 0.003035]
	Learning Rate: 0.00303499
	LOSS [training: 0.05895310549057825 | validation: 0.03232385219312918]
	TIME [epoch: 8.17 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10438741539261502		[learning rate: 0.0030313]
		[batch 20/20] avg loss: 0.07947479991678119		[learning rate: 0.0030276]
	Learning Rate: 0.00302765
	LOSS [training: 0.09193110765469811 | validation: 0.05340921290411049]
	TIME [epoch: 8.19 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05384552389411999		[learning rate: 0.003024]
		[batch 20/20] avg loss: 0.054792440042699786		[learning rate: 0.0030203]
	Learning Rate: 0.00302032
	LOSS [training: 0.054318981968409896 | validation: 0.06668884101703162]
	TIME [epoch: 8.17 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05243284935564295		[learning rate: 0.0030167]
		[batch 20/20] avg loss: 0.039520608529436764		[learning rate: 0.003013]
	Learning Rate: 0.00301301
	LOSS [training: 0.04597672894253986 | validation: 0.030324194370130744]
	TIME [epoch: 8.18 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032092947186820694		[learning rate: 0.0030094]
		[batch 20/20] avg loss: 0.03392335274244905		[learning rate: 0.0030057]
	Learning Rate: 0.00300571
	LOSS [training: 0.03300814996463487 | validation: 0.00883961078861555]
	TIME [epoch: 8.18 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03881729052781587		[learning rate: 0.0030021]
		[batch 20/20] avg loss: 0.04266982729415451		[learning rate: 0.0029984]
	Learning Rate: 0.00299844
	LOSS [training: 0.04074355891098519 | validation: 0.02933359559740759]
	TIME [epoch: 8.19 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03106228769225383		[learning rate: 0.0029948]
		[batch 20/20] avg loss: 0.038217933541446265		[learning rate: 0.0029912]
	Learning Rate: 0.00299118
	LOSS [training: 0.034640110616850045 | validation: 0.02833017326786217]
	TIME [epoch: 8.2 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05707753706317721		[learning rate: 0.0029876]
		[batch 20/20] avg loss: 0.09583307901976687		[learning rate: 0.0029839]
	Learning Rate: 0.00298394
	LOSS [training: 0.07645530804147205 | validation: 0.08261589794288388]
	TIME [epoch: 8.18 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06843479941562068		[learning rate: 0.0029803]
		[batch 20/20] avg loss: 0.061536527756353245		[learning rate: 0.0029767]
	Learning Rate: 0.00297671
	LOSS [training: 0.06498566358598698 | validation: 0.026635765234821945]
	TIME [epoch: 8.16 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04830866232469724		[learning rate: 0.0029731]
		[batch 20/20] avg loss: 0.0621248453433738		[learning rate: 0.0029695]
	Learning Rate: 0.00296951
	LOSS [training: 0.055216753834035515 | validation: 0.023555404217620757]
	TIME [epoch: 8.17 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042558355722185345		[learning rate: 0.0029659]
		[batch 20/20] avg loss: 0.03620573132403429		[learning rate: 0.0029623]
	Learning Rate: 0.00296232
	LOSS [training: 0.039382043523109825 | validation: 0.06680409438755724]
	TIME [epoch: 8.19 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0439383447730085		[learning rate: 0.0029587]
		[batch 20/20] avg loss: 0.10073728559637271		[learning rate: 0.0029551]
	Learning Rate: 0.00295515
	LOSS [training: 0.07233781518469062 | validation: 0.04089932269735199]
	TIME [epoch: 8.19 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06035765177122567		[learning rate: 0.0029516]
		[batch 20/20] avg loss: 0.06300732005262594		[learning rate: 0.002948]
	Learning Rate: 0.00294799
	LOSS [training: 0.0616824859119258 | validation: 0.03127698907936696]
	TIME [epoch: 8.18 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05898704534429433		[learning rate: 0.0029444]
		[batch 20/20] avg loss: 0.035025466833827575		[learning rate: 0.0029409]
	Learning Rate: 0.00294086
	LOSS [training: 0.04700625608906096 | validation: 0.044862113887032574]
	TIME [epoch: 8.17 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06897923447171048		[learning rate: 0.0029373]
		[batch 20/20] avg loss: 0.05654866728302268		[learning rate: 0.0029337]
	Learning Rate: 0.00293374
	LOSS [training: 0.06276395087736657 | validation: 0.07531055456737316]
	TIME [epoch: 8.22 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046054351163583376		[learning rate: 0.0029302]
		[batch 20/20] avg loss: 0.06218018649596259		[learning rate: 0.0029266]
	Learning Rate: 0.00292663
	LOSS [training: 0.05411726882977299 | validation: 0.026215164830566857]
	TIME [epoch: 8.19 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031004244208157016		[learning rate: 0.0029231]
		[batch 20/20] avg loss: 0.040172300136573136		[learning rate: 0.0029195]
	Learning Rate: 0.00291955
	LOSS [training: 0.03558827217236508 | validation: 0.032277085332211214]
	TIME [epoch: 8.16 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045499903919102774		[learning rate: 0.002916]
		[batch 20/20] avg loss: 0.04307857340945565		[learning rate: 0.0029125]
	Learning Rate: 0.00291248
	LOSS [training: 0.04428923866427921 | validation: 0.017927902060358842]
	TIME [epoch: 8.17 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042792870036500646		[learning rate: 0.002909]
		[batch 20/20] avg loss: 0.028977668629799412		[learning rate: 0.0029054]
	Learning Rate: 0.00290543
	LOSS [training: 0.03588526933315003 | validation: 0.009752342116410816]
	TIME [epoch: 8.18 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03626362683137857		[learning rate: 0.0029019]
		[batch 20/20] avg loss: 0.07788807694030277		[learning rate: 0.0028984]
	Learning Rate: 0.0028984
	LOSS [training: 0.05707585188584067 | validation: 0.09523930183231938]
	TIME [epoch: 8.18 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08195477394711366		[learning rate: 0.0028949]
		[batch 20/20] avg loss: 0.05450296338647329		[learning rate: 0.0028914]
	Learning Rate: 0.00289138
	LOSS [training: 0.06822886866679348 | validation: 0.030745811485194383]
	TIME [epoch: 8.16 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05221819609409253		[learning rate: 0.0028879]
		[batch 20/20] avg loss: 0.0523219280545674		[learning rate: 0.0028844]
	Learning Rate: 0.00288438
	LOSS [training: 0.05227006207432996 | validation: -0.0026132556447240147]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039039462191104816		[learning rate: 0.0028809]
		[batch 20/20] avg loss: 0.03355358340972894		[learning rate: 0.0028774]
	Learning Rate: 0.0028774
	LOSS [training: 0.03629652280041688 | validation: 0.01421052460493418]
	TIME [epoch: 8.18 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05109281308846616		[learning rate: 0.0028739]
		[batch 20/20] avg loss: 0.0402821721026559		[learning rate: 0.0028704]
	Learning Rate: 0.00287043
	LOSS [training: 0.045687492595561034 | validation: 0.056681738451374135]
	TIME [epoch: 8.19 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08816068111868272		[learning rate: 0.002867]
		[batch 20/20] avg loss: 0.05638259886873679		[learning rate: 0.0028635]
	Learning Rate: 0.00286348
	LOSS [training: 0.07227163999370975 | validation: 0.04867063753982839]
	TIME [epoch: 8.2 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1130222377887008		[learning rate: 0.00286]
		[batch 20/20] avg loss: 0.09157998526120874		[learning rate: 0.0028566]
	Learning Rate: 0.00285655
	LOSS [training: 0.10230111152495476 | validation: 0.06064886208453077]
	TIME [epoch: 8.18 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08446280456409844		[learning rate: 0.0028531]
		[batch 20/20] avg loss: 0.0742997310340512		[learning rate: 0.0028496]
	Learning Rate: 0.00284964
	LOSS [training: 0.07938126779907481 | validation: 0.049146802947295815]
	TIME [epoch: 8.16 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06351406967287485		[learning rate: 0.0028462]
		[batch 20/20] avg loss: 0.05159528845763979		[learning rate: 0.0028427]
	Learning Rate: 0.00284274
	LOSS [training: 0.05755467906525731 | validation: 0.025991561571790507]
	TIME [epoch: 8.18 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03859933637869113		[learning rate: 0.0028393]
		[batch 20/20] avg loss: 0.03111863358759156		[learning rate: 0.0028359]
	Learning Rate: 0.00283586
	LOSS [training: 0.03485898498314134 | validation: 0.023942416578710768]
	TIME [epoch: 8.17 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048471877573981595		[learning rate: 0.0028324]
		[batch 20/20] avg loss: 0.03173871713530666		[learning rate: 0.002829]
	Learning Rate: 0.00282899
	LOSS [training: 0.040105297354644125 | validation: 0.05565647366027895]
	TIME [epoch: 8.16 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04867673285474316		[learning rate: 0.0028256]
		[batch 20/20] avg loss: 0.03285996141905809		[learning rate: 0.0028221]
	Learning Rate: 0.00282214
	LOSS [training: 0.040768347136900626 | validation: 0.017550975846453976]
	TIME [epoch: 8.19 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02247189119377436		[learning rate: 0.0028187]
		[batch 20/20] avg loss: 0.03452480519283767		[learning rate: 0.0028153]
	Learning Rate: 0.00281531
	LOSS [training: 0.02849834819330601 | validation: 0.02565782198013394]
	TIME [epoch: 8.17 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028245981196269677		[learning rate: 0.0028119]
		[batch 20/20] avg loss: 0.027956445033734588		[learning rate: 0.0028085]
	Learning Rate: 0.00280849
	LOSS [training: 0.02810121311500214 | validation: 0.021215033745437595]
	TIME [epoch: 8.2 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037984650682678894		[learning rate: 0.0028051]
		[batch 20/20] avg loss: 0.03375066284572786		[learning rate: 0.0028017]
	Learning Rate: 0.0028017
	LOSS [training: 0.035867656764203386 | validation: 0.03479654557988486]
	TIME [epoch: 8.19 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058263126858695745		[learning rate: 0.0027983]
		[batch 20/20] avg loss: 0.043218337155660966		[learning rate: 0.0027949]
	Learning Rate: 0.00279491
	LOSS [training: 0.05074073200717836 | validation: 0.015544682939155795]
	TIME [epoch: 8.18 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030339506833684853		[learning rate: 0.0027915]
		[batch 20/20] avg loss: 0.03544274406050782		[learning rate: 0.0027881]
	Learning Rate: 0.00278815
	LOSS [training: 0.03289112544709634 | validation: 0.007560462728394586]
	TIME [epoch: 8.16 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037886666468243		[learning rate: 0.0027848]
		[batch 20/20] avg loss: 0.03320332494519678		[learning rate: 0.0027814]
	Learning Rate: 0.0027814
	LOSS [training: 0.03554499570671988 | validation: 0.02698411071317169]
	TIME [epoch: 8.19 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042110536162627596		[learning rate: 0.002778]
		[batch 20/20] avg loss: 0.023359033881412937		[learning rate: 0.0027747]
	Learning Rate: 0.00277466
	LOSS [training: 0.03273478502202026 | validation: 0.030736498906627656]
	TIME [epoch: 8.17 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04158543461369808		[learning rate: 0.0027713]
		[batch 20/20] avg loss: 0.034411306817058215		[learning rate: 0.0027679]
	Learning Rate: 0.00276795
	LOSS [training: 0.03799837071537814 | validation: 0.03173393568033413]
	TIME [epoch: 8.16 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03736008084228417		[learning rate: 0.0027646]
		[batch 20/20] avg loss: 0.023368187300960092		[learning rate: 0.0027612]
	Learning Rate: 0.00276125
	LOSS [training: 0.030364134071622133 | validation: 0.017798047278458958]
	TIME [epoch: 8.18 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029404402257897096		[learning rate: 0.0027579]
		[batch 20/20] avg loss: 0.03245214975171464		[learning rate: 0.0027546]
	Learning Rate: 0.00275456
	LOSS [training: 0.030928276004805865 | validation: 0.03652058509166766]
	TIME [epoch: 8.17 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07630002245838347		[learning rate: 0.0027512]
		[batch 20/20] avg loss: 0.04757924204585737		[learning rate: 0.0027479]
	Learning Rate: 0.00274789
	LOSS [training: 0.06193963225212044 | validation: 0.022722496713064075]
	TIME [epoch: 8.2 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03585225543470609		[learning rate: 0.0027446]
		[batch 20/20] avg loss: 0.026725516959446204		[learning rate: 0.0027412]
	Learning Rate: 0.00274124
	LOSS [training: 0.03128888619707615 | validation: 0.01519591006483553]
	TIME [epoch: 8.19 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04032027829849029		[learning rate: 0.0027379]
		[batch 20/20] avg loss: 0.043331681753050154		[learning rate: 0.0027346]
	Learning Rate: 0.00273461
	LOSS [training: 0.041825980025770225 | validation: 0.09322257333667568]
	TIME [epoch: 8.19 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04663722820358211		[learning rate: 0.0027313]
		[batch 20/20] avg loss: 0.04616842697295857		[learning rate: 0.002728]
	Learning Rate: 0.00272799
	LOSS [training: 0.04640282758827034 | validation: 0.03164256537148654]
	TIME [epoch: 8.16 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05547178026528975		[learning rate: 0.0027247]
		[batch 20/20] avg loss: 0.07421769808092486		[learning rate: 0.0027214]
	Learning Rate: 0.00272138
	LOSS [training: 0.0648447391731073 | validation: 0.01655594538144258]
	TIME [epoch: 8.19 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05879730309852338		[learning rate: 0.0027181]
		[batch 20/20] avg loss: 0.04852528741537256		[learning rate: 0.0027148]
	Learning Rate: 0.00271479
	LOSS [training: 0.053661295256947975 | validation: 0.048666282710626965]
	TIME [epoch: 8.16 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045746535447674284		[learning rate: 0.0027115]
		[batch 20/20] avg loss: 0.04384380119085081		[learning rate: 0.0027082]
	Learning Rate: 0.00270822
	LOSS [training: 0.04479516831926254 | validation: 0.02420259129001067]
	TIME [epoch: 8.16 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052417268294825514		[learning rate: 0.0027049]
		[batch 20/20] avg loss: 0.04942627180938976		[learning rate: 0.0027017]
	Learning Rate: 0.00270167
	LOSS [training: 0.05092177005210764 | validation: 0.06000733209154572]
	TIME [epoch: 8.16 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05584727818303235		[learning rate: 0.0026984]
		[batch 20/20] avg loss: 0.04195863218915535		[learning rate: 0.0026951]
	Learning Rate: 0.00269513
	LOSS [training: 0.04890295518609385 | validation: 0.07398530237315415]
	TIME [epoch: 8.19 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05740509354341468		[learning rate: 0.0026919]
		[batch 20/20] avg loss: 0.03838778799096517		[learning rate: 0.0026886]
	Learning Rate: 0.0026886
	LOSS [training: 0.047896440767189914 | validation: 0.06465266660335559]
	TIME [epoch: 8.17 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04736146943657933		[learning rate: 0.0026853]
		[batch 20/20] avg loss: 0.04503353621919021		[learning rate: 0.0026821]
	Learning Rate: 0.00268209
	LOSS [training: 0.04619750282788476 | validation: 0.025945794685383553]
	TIME [epoch: 8.19 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05799501016550879		[learning rate: 0.0026788]
		[batch 20/20] avg loss: 0.03649332241676292		[learning rate: 0.0026756]
	Learning Rate: 0.0026756
	LOSS [training: 0.04724416629113586 | validation: 0.0262540857238094]
	TIME [epoch: 8.17 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04018748141533533		[learning rate: 0.0026724]
		[batch 20/20] avg loss: 0.03642184793573719		[learning rate: 0.0026691]
	Learning Rate: 0.00266912
	LOSS [training: 0.03830466467553626 | validation: 0.014034444062988687]
	TIME [epoch: 8.17 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034523890842702645		[learning rate: 0.0026659]
		[batch 20/20] avg loss: 0.0301304158787468		[learning rate: 0.0026627]
	Learning Rate: 0.00266266
	LOSS [training: 0.032327153360724725 | validation: 0.02163890275781416]
	TIME [epoch: 8.23 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03996136361059004		[learning rate: 0.0026594]
		[batch 20/20] avg loss: 0.06268615559414363		[learning rate: 0.0026562]
	Learning Rate: 0.00265621
	LOSS [training: 0.051323759602366834 | validation: 0.06702693605723897]
	TIME [epoch: 8.17 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08938200134987566		[learning rate: 0.002653]
		[batch 20/20] avg loss: 0.05917666408358789		[learning rate: 0.0026498]
	Learning Rate: 0.00264978
	LOSS [training: 0.0742793327167318 | validation: 0.02490269503461255]
	TIME [epoch: 8.17 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05324537379708714		[learning rate: 0.0026466]
		[batch 20/20] avg loss: 0.06043650293393852		[learning rate: 0.0026434]
	Learning Rate: 0.00264337
	LOSS [training: 0.05684093836551283 | validation: 0.018734188076572806]
	TIME [epoch: 8.17 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04774864806069135		[learning rate: 0.0026402]
		[batch 20/20] avg loss: 0.06405622947076746		[learning rate: 0.002637]
	Learning Rate: 0.00263697
	LOSS [training: 0.0559024387657294 | validation: 0.013207052295392796]
	TIME [epoch: 8.19 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03341827416888551		[learning rate: 0.0026338]
		[batch 20/20] avg loss: 0.03668734777272191		[learning rate: 0.0026306]
	Learning Rate: 0.00263059
	LOSS [training: 0.035052810970803705 | validation: 0.02714445446518529]
	TIME [epoch: 8.18 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052824294038005135		[learning rate: 0.0026274]
		[batch 20/20] avg loss: 0.044156796819148796		[learning rate: 0.0026242]
	Learning Rate: 0.00262422
	LOSS [training: 0.048490545428576975 | validation: 0.027290389557851978]
	TIME [epoch: 8.19 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02670959150401181		[learning rate: 0.002621]
		[batch 20/20] avg loss: 0.03411182268211497		[learning rate: 0.0026179]
	Learning Rate: 0.00261787
	LOSS [training: 0.030410707093063382 | validation: 0.03192602235892694]
	TIME [epoch: 8.17 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03787446456446915		[learning rate: 0.0026147]
		[batch 20/20] avg loss: 0.04732696161441194		[learning rate: 0.0026115]
	Learning Rate: 0.00261153
	LOSS [training: 0.04260071308944055 | validation: 0.03234759034277442]
	TIME [epoch: 8.18 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057343773541875795		[learning rate: 0.0026084]
		[batch 20/20] avg loss: 0.05721381904540892		[learning rate: 0.0026052]
	Learning Rate: 0.00260521
	LOSS [training: 0.05727879629364237 | validation: 0.048930147413175994]
	TIME [epoch: 8.23 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07090663952463308		[learning rate: 0.0026021]
		[batch 20/20] avg loss: 0.15701382537444047		[learning rate: 0.0025989]
	Learning Rate: 0.0025989
	LOSS [training: 0.11396023244953676 | validation: 0.05846098175829549]
	TIME [epoch: 8.17 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055815014840960644		[learning rate: 0.0025958]
		[batch 20/20] avg loss: 0.056364133193267496		[learning rate: 0.0025926]
	Learning Rate: 0.00259261
	LOSS [training: 0.05608957401711406 | validation: 0.027989978591080267]
	TIME [epoch: 8.17 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05118228513068905		[learning rate: 0.0025895]
		[batch 20/20] avg loss: 0.048484655460874514		[learning rate: 0.0025863]
	Learning Rate: 0.00258633
	LOSS [training: 0.049833470295781786 | validation: 0.01582766266265217]
	TIME [epoch: 8.16 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03589588827117105		[learning rate: 0.0025832]
		[batch 20/20] avg loss: 0.025823330093435137		[learning rate: 0.0025801]
	Learning Rate: 0.00258007
	LOSS [training: 0.03085960918230309 | validation: 0.010996872859978208]
	TIME [epoch: 8.19 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0519292443310187		[learning rate: 0.0025769]
		[batch 20/20] avg loss: 0.047427076659961806		[learning rate: 0.0025738]
	Learning Rate: 0.00257382
	LOSS [training: 0.04967816049549025 | validation: 0.03126831872459758]
	TIME [epoch: 8.17 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.087230643520477		[learning rate: 0.0025707]
		[batch 20/20] avg loss: 0.042566877029257134		[learning rate: 0.0025676]
	Learning Rate: 0.00256759
	LOSS [training: 0.06489876027486706 | validation: 0.022775587454085252]
	TIME [epoch: 8.17 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03445250343187209		[learning rate: 0.0025645]
		[batch 20/20] avg loss: 0.031323635171517324		[learning rate: 0.0025614]
	Learning Rate: 0.00256138
	LOSS [training: 0.0328880693016947 | validation: 0.029447493946299304]
	TIME [epoch: 8.17 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03039016690366996		[learning rate: 0.0025583]
		[batch 20/20] avg loss: 0.038626001483279895		[learning rate: 0.0025552]
	Learning Rate: 0.00255518
	LOSS [training: 0.03450808419347492 | validation: 0.005015161116905354]
	TIME [epoch: 8.18 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02440337750707777		[learning rate: 0.0025521]
		[batch 20/20] avg loss: 0.0935441692950745		[learning rate: 0.002549]
	Learning Rate: 0.00254899
	LOSS [training: 0.05897377340107614 | validation: 0.08286441723169898]
	TIME [epoch: 8.21 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08269736079400324		[learning rate: 0.0025459]
		[batch 20/20] avg loss: 0.02608431252531824		[learning rate: 0.0025428]
	Learning Rate: 0.00254282
	LOSS [training: 0.05439083665966075 | validation: 0.022011392067764174]
	TIME [epoch: 8.17 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030642698324178947		[learning rate: 0.0025397]
		[batch 20/20] avg loss: 0.02925910541426921		[learning rate: 0.0025367]
	Learning Rate: 0.00253667
	LOSS [training: 0.029950901869224073 | validation: 0.028093370675899045]
	TIME [epoch: 8.17 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025739422302865626		[learning rate: 0.0025336]
		[batch 20/20] avg loss: 0.02718412124829634		[learning rate: 0.0025305]
	Learning Rate: 0.00253052
	LOSS [training: 0.026461771775580983 | validation: 0.007688959248016375]
	TIME [epoch: 8.21 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029502889937390874		[learning rate: 0.0025275]
		[batch 20/20] avg loss: 0.05013289017568998		[learning rate: 0.0025244]
	Learning Rate: 0.0025244
	LOSS [training: 0.039817890056540434 | validation: 0.024002742145322085]
	TIME [epoch: 8.19 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036875700178584374		[learning rate: 0.0025213]
		[batch 20/20] avg loss: 0.03181501140245536		[learning rate: 0.0025183]
	Learning Rate: 0.00251829
	LOSS [training: 0.03434535579051987 | validation: 0.017208141430035986]
	TIME [epoch: 8.17 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04224802608610987		[learning rate: 0.0025152]
		[batch 20/20] avg loss: 0.024438491785521887		[learning rate: 0.0025122]
	Learning Rate: 0.00251219
	LOSS [training: 0.03334325893581587 | validation: 0.018119138659707115]
	TIME [epoch: 8.17 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049559386028190176		[learning rate: 0.0025091]
		[batch 20/20] avg loss: 0.039239371632868746		[learning rate: 0.0025061]
	Learning Rate: 0.00250611
	LOSS [training: 0.04439937883052946 | validation: 0.033957450752861926]
	TIME [epoch: 8.17 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03114763265408212		[learning rate: 0.0025031]
		[batch 20/20] avg loss: 0.0364111929311749		[learning rate: 0.0025]
	Learning Rate: 0.00250004
	LOSS [training: 0.033779412792628506 | validation: 0.03773642108854756]
	TIME [epoch: 8.2 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052020550825837564		[learning rate: 0.002497]
		[batch 20/20] avg loss: 0.028074758371316365		[learning rate: 0.002494]
	Learning Rate: 0.00249399
	LOSS [training: 0.04004765459857697 | validation: 0.01764430249282653]
	TIME [epoch: 8.18 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021991824854484615		[learning rate: 0.002491]
		[batch 20/20] avg loss: 0.031403307276526966		[learning rate: 0.002488]
	Learning Rate: 0.00248795
	LOSS [training: 0.02669756606550579 | validation: 0.024635189576146774]
	TIME [epoch: 8.17 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04669545897765468		[learning rate: 0.0024849]
		[batch 20/20] avg loss: 0.029515740958337235		[learning rate: 0.0024819]
	Learning Rate: 0.00248193
	LOSS [training: 0.038105599967995965 | validation: 0.028736899648565194]
	TIME [epoch: 8.19 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04240596690352681		[learning rate: 0.0024789]
		[batch 20/20] avg loss: 0.03955795894539433		[learning rate: 0.0024759]
	Learning Rate: 0.00247592
	LOSS [training: 0.040981962924460574 | validation: 0.035520001449334246]
	TIME [epoch: 8.19 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040446309717796246		[learning rate: 0.0024729]
		[batch 20/20] avg loss: 0.046861141328450755		[learning rate: 0.0024699]
	Learning Rate: 0.00246993
	LOSS [training: 0.04365372552312349 | validation: 0.025639930234075556]
	TIME [epoch: 8.19 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03523790092642301		[learning rate: 0.0024669]
		[batch 20/20] avg loss: 0.03040882033913046		[learning rate: 0.0024639]
	Learning Rate: 0.00246395
	LOSS [training: 0.03282336063277673 | validation: 0.0339893703840047]
	TIME [epoch: 8.17 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028124945624337077		[learning rate: 0.002461]
		[batch 20/20] avg loss: 0.03484255238782403		[learning rate: 0.002458]
	Learning Rate: 0.00245798
	LOSS [training: 0.031483749006080565 | validation: 0.023351777077584266]
	TIME [epoch: 8.17 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03201442759990359		[learning rate: 0.002455]
		[batch 20/20] avg loss: 0.033960686340994264		[learning rate: 0.002452]
	Learning Rate: 0.00245203
	LOSS [training: 0.03298755697044893 | validation: 0.05546585460545041]
	TIME [epoch: 8.18 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029925428417558596		[learning rate: 0.0024491]
		[batch 20/20] avg loss: 0.06681826787975192		[learning rate: 0.0024461]
	Learning Rate: 0.0024461
	LOSS [training: 0.048371848148655264 | validation: 0.07391260195033872]
	TIME [epoch: 8.2 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08475719301590166		[learning rate: 0.0024431]
		[batch 20/20] avg loss: 0.0770240623414252		[learning rate: 0.0024402]
	Learning Rate: 0.00244018
	LOSS [training: 0.08089062767866342 | validation: 0.04331655972559982]
	TIME [epoch: 8.18 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04834988272437381		[learning rate: 0.0024372]
		[batch 20/20] avg loss: 0.04871899453420088		[learning rate: 0.0024343]
	Learning Rate: 0.00243427
	LOSS [training: 0.04853443862928734 | validation: 0.019453930370362353]
	TIME [epoch: 8.18 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026020516097282436		[learning rate: 0.0024313]
		[batch 20/20] avg loss: 0.028340261190188544		[learning rate: 0.0024284]
	Learning Rate: 0.00242837
	LOSS [training: 0.027180388643735492 | validation: 0.0050716147259227485]
	TIME [epoch: 8.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040665319522092916		[learning rate: 0.0024254]
		[batch 20/20] avg loss: 0.05473814854218758		[learning rate: 0.0024225]
	Learning Rate: 0.0024225
	LOSS [training: 0.04770173403214025 | validation: 0.012389107370720824]
	TIME [epoch: 8.18 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041424385036754344		[learning rate: 0.0024196]
		[batch 20/20] avg loss: 0.03275250277381582		[learning rate: 0.0024166]
	Learning Rate: 0.00241663
	LOSS [training: 0.03708844390528509 | validation: 0.009278203338088023]
	TIME [epoch: 8.19 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017729175367879414		[learning rate: 0.0024137]
		[batch 20/20] avg loss: 0.032238949928957754		[learning rate: 0.0024108]
	Learning Rate: 0.00241078
	LOSS [training: 0.024984062648418577 | validation: 0.007131483050737126]
	TIME [epoch: 8.17 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03430694690709361		[learning rate: 0.0024079]
		[batch 20/20] avg loss: 0.026281578496340052		[learning rate: 0.0024049]
	Learning Rate: 0.00240495
	LOSS [training: 0.030294262701716833 | validation: 0.008180421169498887]
	TIME [epoch: 8.17 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028457290476434027		[learning rate: 0.002402]
		[batch 20/20] avg loss: 0.02683977345222204		[learning rate: 0.0023991]
	Learning Rate: 0.00239912
	LOSS [training: 0.02764853196432803 | validation: 0.027946481314023566]
	TIME [epoch: 8.16 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042588623013274624		[learning rate: 0.0023962]
		[batch 20/20] avg loss: 0.03613423199477035		[learning rate: 0.0023933]
	Learning Rate: 0.00239332
	LOSS [training: 0.03936142750402249 | validation: 0.00904744494213553]
	TIME [epoch: 8.21 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030389024621788115		[learning rate: 0.0023904]
		[batch 20/20] avg loss: 0.03562616458865886		[learning rate: 0.0023875]
	Learning Rate: 0.00238752
	LOSS [training: 0.0330075946052235 | validation: 0.004039057115960637]
	TIME [epoch: 8.18 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03701843078455925		[learning rate: 0.0023846]
		[batch 20/20] avg loss: 0.043009178651590686		[learning rate: 0.0023817]
	Learning Rate: 0.00238174
	LOSS [training: 0.040013804718074975 | validation: 0.033693556707046164]
	TIME [epoch: 8.17 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050230384779415896		[learning rate: 0.0023789]
		[batch 20/20] avg loss: 0.03477845324725869		[learning rate: 0.002376]
	Learning Rate: 0.00237598
	LOSS [training: 0.04250441901333729 | validation: 0.017124963443348375]
	TIME [epoch: 8.18 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04040272488985149		[learning rate: 0.0023731]
		[batch 20/20] avg loss: 0.12424958369363834		[learning rate: 0.0023702]
	Learning Rate: 0.00237022
	LOSS [training: 0.08232615429174492 | validation: 0.10102876681663352]
	TIME [epoch: 8.21 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07454282463928943		[learning rate: 0.0023674]
		[batch 20/20] avg loss: 0.059107102573949244		[learning rate: 0.0023645]
	Learning Rate: 0.00236449
	LOSS [training: 0.06682496360661935 | validation: 0.037253601799443044]
	TIME [epoch: 8.2 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0811235412304459		[learning rate: 0.0023616]
		[batch 20/20] avg loss: 0.11875712388042448		[learning rate: 0.0023588]
	Learning Rate: 0.00235876
	LOSS [training: 0.09994033255543518 | validation: 0.06785670742597157]
	TIME [epoch: 8.16 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08834667533976114		[learning rate: 0.0023559]
		[batch 20/20] avg loss: 0.06972675352484069		[learning rate: 0.0023531]
	Learning Rate: 0.00235305
	LOSS [training: 0.07903671443230091 | validation: 0.022698271428854663]
	TIME [epoch: 8.17 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04935790793065429		[learning rate: 0.0023502]
		[batch 20/20] avg loss: 0.06927655370945981		[learning rate: 0.0023474]
	Learning Rate: 0.00234736
	LOSS [training: 0.059317230820057056 | validation: 0.0357220018170524]
	TIME [epoch: 8.18 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04740652609208694		[learning rate: 0.0023445]
		[batch 20/20] avg loss: 0.026723107801455193		[learning rate: 0.0023417]
	Learning Rate: 0.00234167
	LOSS [training: 0.037064816946771065 | validation: 0.019772825322190535]
	TIME [epoch: 8.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03752100873956598		[learning rate: 0.0023388]
		[batch 20/20] avg loss: 0.020746896361700257		[learning rate: 0.002336]
	Learning Rate: 0.002336
	LOSS [training: 0.02913395255063312 | validation: 0.016251140191293725]
	TIME [epoch: 8.18 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025185438570746		[learning rate: 0.0023332]
		[batch 20/20] avg loss: 0.03368199978959251		[learning rate: 0.0023303]
	Learning Rate: 0.00233035
	LOSS [training: 0.029433719180169255 | validation: 0.004040468300024852]
	TIME [epoch: 8.17 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05053230698230752		[learning rate: 0.0023275]
		[batch 20/20] avg loss: 0.033452733557877964		[learning rate: 0.0023247]
	Learning Rate: 0.00232471
	LOSS [training: 0.041992520270092745 | validation: 0.03398746506766572]
	TIME [epoch: 8.21 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031105643957727143		[learning rate: 0.0023219]
		[batch 20/20] avg loss: 0.03611808971055584		[learning rate: 0.0023191]
	Learning Rate: 0.00231908
	LOSS [training: 0.033611866834141496 | validation: 0.02370773510369435]
	TIME [epoch: 8.19 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0461946824446256		[learning rate: 0.0023163]
		[batch 20/20] avg loss: 0.02346407687493655		[learning rate: 0.0023135]
	Learning Rate: 0.00231347
	LOSS [training: 0.03482937965978107 | validation: 0.025126732011290724]
	TIME [epoch: 8.18 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057461492066887776		[learning rate: 0.0023107]
		[batch 20/20] avg loss: 0.03920253998756709		[learning rate: 0.0023079]
	Learning Rate: 0.00230787
	LOSS [training: 0.04833201602722743 | validation: 0.07502863988845418]
	TIME [epoch: 8.16 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06574718699431602		[learning rate: 0.0023051]
		[batch 20/20] avg loss: 0.05016998857175414		[learning rate: 0.0023023]
	Learning Rate: 0.00230228
	LOSS [training: 0.05795858778303509 | validation: 0.020720691076616626]
	TIME [epoch: 8.18 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03448564405662228		[learning rate: 0.0022995]
		[batch 20/20] avg loss: 0.026459253211277713		[learning rate: 0.0022967]
	Learning Rate: 0.00229671
	LOSS [training: 0.030472448633949996 | validation: 0.03636019272846801]
	TIME [epoch: 8.18 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05098795077654137		[learning rate: 0.0022939]
		[batch 20/20] avg loss: 0.03575714643230209		[learning rate: 0.0022911]
	Learning Rate: 0.00229115
	LOSS [training: 0.04337254860442173 | validation: 0.011494184878097915]
	TIME [epoch: 8.19 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032941292308784		[learning rate: 0.0022884]
		[batch 20/20] avg loss: 0.02977413919933548		[learning rate: 0.0022856]
	Learning Rate: 0.0022856
	LOSS [training: 0.03135771575405974 | validation: 0.003171077478671935]
	TIME [epoch: 8.18 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03060750057155289		[learning rate: 0.0022828]
		[batch 20/20] avg loss: 0.05245161829767762		[learning rate: 0.0022801]
	Learning Rate: 0.00228007
	LOSS [training: 0.04152955943461526 | validation: 0.017335885434332135]
	TIME [epoch: 8.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044550670028734564		[learning rate: 0.0022773]
		[batch 20/20] avg loss: 0.030736803264705022		[learning rate: 0.0022745]
	Learning Rate: 0.00227455
	LOSS [training: 0.037643736646719786 | validation: 0.01827513229020903]
	TIME [epoch: 8.17 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04754939791088876		[learning rate: 0.0022718]
		[batch 20/20] avg loss: 0.03659575331337909		[learning rate: 0.002269]
	Learning Rate: 0.00226904
	LOSS [training: 0.04207257561213392 | validation: 0.010783280145574229]
	TIME [epoch: 8.19 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029343124530175558		[learning rate: 0.0022663]
		[batch 20/20] avg loss: 0.02478211596370513		[learning rate: 0.0022635]
	Learning Rate: 0.00226355
	LOSS [training: 0.027062620246940346 | validation: 0.031349364233412724]
	TIME [epoch: 8.17 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028942874500086063		[learning rate: 0.0022608]
		[batch 20/20] avg loss: 0.03504011930748093		[learning rate: 0.0022581]
	Learning Rate: 0.00225807
	LOSS [training: 0.0319914969037835 | validation: 0.024230141901479835]
	TIME [epoch: 8.17 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030711731155498856		[learning rate: 0.0022553]
		[batch 20/20] avg loss: 0.02881515659638313		[learning rate: 0.0022526]
	Learning Rate: 0.0022526
	LOSS [training: 0.029763443875940994 | validation: 0.006771057682477733]
	TIME [epoch: 8.18 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029839749140567355		[learning rate: 0.0022499]
		[batch 20/20] avg loss: 0.04136594878422773		[learning rate: 0.0022471]
	Learning Rate: 0.00224715
	LOSS [training: 0.03560284896239754 | validation: 0.02888143675144618]
	TIME [epoch: 8.18 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030530231472791		[learning rate: 0.0022444]
		[batch 20/20] avg loss: 0.016642926260547967		[learning rate: 0.0022417]
	Learning Rate: 0.00224171
	LOSS [training: 0.023586578866669487 | validation: 0.014528142393220839]
	TIME [epoch: 8.2 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03747914663923439		[learning rate: 0.002239]
		[batch 20/20] avg loss: 0.031062128572361965		[learning rate: 0.0022363]
	Learning Rate: 0.00223628
	LOSS [training: 0.034270637605798175 | validation: 0.029970896338118516]
	TIME [epoch: 8.17 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03666363640244523		[learning rate: 0.0022336]
		[batch 20/20] avg loss: 0.036453347700118044		[learning rate: 0.0022309]
	Learning Rate: 0.00223087
	LOSS [training: 0.03655849205128163 | validation: 0.02026790102628271]
	TIME [epoch: 8.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03250648726406176		[learning rate: 0.0022282]
		[batch 20/20] avg loss: 0.02869299940755395		[learning rate: 0.0022255]
	Learning Rate: 0.00222547
	LOSS [training: 0.03059974333580785 | validation: 0.022650457554737276]
	TIME [epoch: 8.16 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038846853213997334		[learning rate: 0.0022228]
		[batch 20/20] avg loss: 0.029276690102802132		[learning rate: 0.0022201]
	Learning Rate: 0.00222008
	LOSS [training: 0.03406177165839974 | validation: 0.025304927352950292]
	TIME [epoch: 8.19 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03632420860475287		[learning rate: 0.0022174]
		[batch 20/20] avg loss: 0.03917640735559508		[learning rate: 0.0022147]
	Learning Rate: 0.0022147
	LOSS [training: 0.03775030798017397 | validation: 0.0231913837690354]
	TIME [epoch: 8.16 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039584832247091875		[learning rate: 0.002212]
		[batch 20/20] avg loss: 0.03572970100419609		[learning rate: 0.0022093]
	Learning Rate: 0.00220934
	LOSS [training: 0.03765726662564398 | validation: 0.0456412654474462]
	TIME [epoch: 8.17 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038493992694578746		[learning rate: 0.0022067]
		[batch 20/20] avg loss: 0.033108060916350504		[learning rate: 0.002204]
	Learning Rate: 0.00220399
	LOSS [training: 0.035801026805464625 | validation: 0.022875679829853283]
	TIME [epoch: 8.19 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04127614892911331		[learning rate: 0.0022013]
		[batch 20/20] avg loss: 0.0419501732259086		[learning rate: 0.0021987]
	Learning Rate: 0.00219866
	LOSS [training: 0.041613161077510966 | validation: 0.018927980586190805]
	TIME [epoch: 8.17 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029499376570376306		[learning rate: 0.002196]
		[batch 20/20] avg loss: 0.024016659306335477		[learning rate: 0.0021933]
	Learning Rate: 0.00219334
	LOSS [training: 0.026758017938355888 | validation: 0.015072846674039391]
	TIME [epoch: 8.2 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030396655469790628		[learning rate: 0.0021907]
		[batch 20/20] avg loss: 0.025977780738391226		[learning rate: 0.002188]
	Learning Rate: 0.00218803
	LOSS [training: 0.028187218104090927 | validation: 0.009672382074482067]
	TIME [epoch: 8.19 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023464216018913252		[learning rate: 0.0021854]
		[batch 20/20] avg loss: 0.032406804211555465		[learning rate: 0.0021827]
	Learning Rate: 0.00218273
	LOSS [training: 0.027935510115234364 | validation: 0.02746554428874399]
	TIME [epoch: 8.18 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029169728026356718		[learning rate: 0.0021801]
		[batch 20/20] avg loss: 0.03453877721979336		[learning rate: 0.0021774]
	Learning Rate: 0.00217745
	LOSS [training: 0.031854252623075036 | validation: 0.08238583666054686]
	TIME [epoch: 8.16 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051336012242261544		[learning rate: 0.0021748]
		[batch 20/20] avg loss: 0.03236877516859375		[learning rate: 0.0021722]
	Learning Rate: 0.00217217
	LOSS [training: 0.04185239370542763 | validation: 0.0004261946796304625]
	TIME [epoch: 8.19 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030699202634609124		[learning rate: 0.0021695]
		[batch 20/20] avg loss: 0.02014546916308925		[learning rate: 0.0021669]
	Learning Rate: 0.00216692
	LOSS [training: 0.02542233589884919 | validation: 0.007724840630760239]
	TIME [epoch: 8.17 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03506074903860847		[learning rate: 0.0021643]
		[batch 20/20] avg loss: 0.02038697987851573		[learning rate: 0.0021617]
	Learning Rate: 0.00216167
	LOSS [training: 0.027723864458562102 | validation: 0.006096625225143144]
	TIME [epoch: 8.19 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048468056522472526		[learning rate: 0.0021591]
		[batch 20/20] avg loss: 0.04027926348207641		[learning rate: 0.0021564]
	Learning Rate: 0.00215644
	LOSS [training: 0.04437366000227448 | validation: 0.007892970393071574]
	TIME [epoch: 8.17 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026870036820882515		[learning rate: 0.0021538]
		[batch 20/20] avg loss: 0.025664462168703578		[learning rate: 0.0021512]
	Learning Rate: 0.00215122
	LOSS [training: 0.026267249494793043 | validation: 0.011192555125980767]
	TIME [epoch: 8.19 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017523374782283985		[learning rate: 0.0021486]
		[batch 20/20] avg loss: 0.0446848896752805		[learning rate: 0.002146]
	Learning Rate: 0.00214601
	LOSS [training: 0.031104132228782243 | validation: 0.015231366167356845]
	TIME [epoch: 8.21 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03340579562919014		[learning rate: 0.0021434]
		[batch 20/20] avg loss: 0.019177443714713284		[learning rate: 0.0021408]
	Learning Rate: 0.00214081
	LOSS [training: 0.02629161967195171 | validation: 0.007483893710141692]
	TIME [epoch: 8.17 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043554826504284		[learning rate: 0.0021382]
		[batch 20/20] avg loss: 0.053629717017364055		[learning rate: 0.0021356]
	Learning Rate: 0.00213563
	LOSS [training: 0.04859227176082403 | validation: 0.09322277161617848]
	TIME [epoch: 8.17 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05582149211628603		[learning rate: 0.002133]
		[batch 20/20] avg loss: 0.06026542223611418		[learning rate: 0.0021305]
	Learning Rate: 0.00213046
	LOSS [training: 0.05804345717620013 | validation: 0.030301999496936217]
	TIME [epoch: 8.16 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03658892330257678		[learning rate: 0.0021279]
		[batch 20/20] avg loss: 0.03707220189298553		[learning rate: 0.0021253]
	Learning Rate: 0.0021253
	LOSS [training: 0.036830562597781155 | validation: 0.03321163677539854]
	TIME [epoch: 8.19 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06080754898438682		[learning rate: 0.0021227]
		[batch 20/20] avg loss: 0.04209781000522293		[learning rate: 0.0021202]
	Learning Rate: 0.00212016
	LOSS [training: 0.05145267949480488 | validation: 0.054555849994253475]
	TIME [epoch: 8.18 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04996049504565458		[learning rate: 0.0021176]
		[batch 20/20] avg loss: 0.0501006889391846		[learning rate: 0.002115]
	Learning Rate: 0.00211503
	LOSS [training: 0.05003059199241959 | validation: 0.013585747823674388]
	TIME [epoch: 8.18 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029897968049000957		[learning rate: 0.0021125]
		[batch 20/20] avg loss: 0.027147133246605348		[learning rate: 0.0021099]
	Learning Rate: 0.00210991
	LOSS [training: 0.028522550647803145 | validation: 0.013383249183966626]
	TIME [epoch: 8.17 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04752853171260186		[learning rate: 0.0021074]
		[batch 20/20] avg loss: 0.019363427833029315		[learning rate: 0.0021048]
	Learning Rate: 0.0021048
	LOSS [training: 0.03344597977281558 | validation: 0.011192687409822844]
	TIME [epoch: 8.2 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03590707233294122		[learning rate: 0.0021022]
		[batch 20/20] avg loss: 0.036666526637184324		[learning rate: 0.0020997]
	Learning Rate: 0.0020997
	LOSS [training: 0.036286799485062776 | validation: 0.05153393873040135]
	TIME [epoch: 8.21 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05510657257615574		[learning rate: 0.0020972]
		[batch 20/20] avg loss: 0.03260294683786374		[learning rate: 0.0020946]
	Learning Rate: 0.00209462
	LOSS [training: 0.043854759707009736 | validation: 0.01392688369079034]
	TIME [epoch: 8.16 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028037935209633928		[learning rate: 0.0020921]
		[batch 20/20] avg loss: 0.020886114140507703		[learning rate: 0.0020895]
	Learning Rate: 0.00208955
	LOSS [training: 0.02446202467507082 | validation: 0.010275131829748672]
	TIME [epoch: 8.16 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04062300116506088		[learning rate: 0.002087]
		[batch 20/20] avg loss: 0.030072575803897588		[learning rate: 0.0020845]
	Learning Rate: 0.00208449
	LOSS [training: 0.03534778848447924 | validation: 0.011879718872662863]
	TIME [epoch: 8.16 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032344730043147016		[learning rate: 0.002082]
		[batch 20/20] avg loss: 0.041668440745812434		[learning rate: 0.0020794]
	Learning Rate: 0.00207944
	LOSS [training: 0.03700658539447972 | validation: 0.023471909177565375]
	TIME [epoch: 8.21 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03440425736049565		[learning rate: 0.0020769]
		[batch 20/20] avg loss: 0.03349454648755164		[learning rate: 0.0020744]
	Learning Rate: 0.00207441
	LOSS [training: 0.03394940192402364 | validation: 0.015894913473692596]
	TIME [epoch: 8.17 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03592392905870074		[learning rate: 0.0020719]
		[batch 20/20] avg loss: 0.026787081895353355		[learning rate: 0.0020694]
	Learning Rate: 0.00206939
	LOSS [training: 0.03135550547702705 | validation: 0.014417772946069086]
	TIME [epoch: 8.17 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019753785240508595		[learning rate: 0.0020669]
		[batch 20/20] avg loss: 0.028660895570443996		[learning rate: 0.0020644]
	Learning Rate: 0.00206438
	LOSS [training: 0.024207340405476296 | validation: 0.014681333727760117]
	TIME [epoch: 8.19 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01732308090854897		[learning rate: 0.0020619]
		[batch 20/20] avg loss: 0.05273040381768064		[learning rate: 0.0020594]
	Learning Rate: 0.00205938
	LOSS [training: 0.0350267423631148 | validation: 0.02564050089225836]
	TIME [epoch: 8.21 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03451638547490563		[learning rate: 0.0020569]
		[batch 20/20] avg loss: 0.025210597421134066		[learning rate: 0.0020544]
	Learning Rate: 0.0020544
	LOSS [training: 0.029863491448019846 | validation: 0.008406418163137246]
	TIME [epoch: 8.17 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02252324528133578		[learning rate: 0.0020519]
		[batch 20/20] avg loss: 0.027372056467841542		[learning rate: 0.0020494]
	Learning Rate: 0.00204942
	LOSS [training: 0.02494765087458866 | validation: 0.05024827166377568]
	TIME [epoch: 8.16 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038512575892120135		[learning rate: 0.0020469]
		[batch 20/20] avg loss: 0.01751072522797209		[learning rate: 0.0020445]
	Learning Rate: 0.00204446
	LOSS [training: 0.02801165056004612 | validation: 0.0034570155444091496]
	TIME [epoch: 8.16 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015114998266026872		[learning rate: 0.002042]
		[batch 20/20] avg loss: 0.02431285209141401		[learning rate: 0.0020395]
	Learning Rate: 0.00203951
	LOSS [training: 0.01971392517872044 | validation: 0.005337497467491476]
	TIME [epoch: 8.17 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02863438523368131		[learning rate: 0.002037]
		[batch 20/20] avg loss: 0.012534005441435219		[learning rate: 0.0020346]
	Learning Rate: 0.00203457
	LOSS [training: 0.020584195337558266 | validation: 0.0016585707754610348]
	TIME [epoch: 8.21 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019215665303602004		[learning rate: 0.0020321]
		[batch 20/20] avg loss: 0.011639495133444297		[learning rate: 0.0020296]
	Learning Rate: 0.00202965
	LOSS [training: 0.015427580218523149 | validation: 0.009144192180624037]
	TIME [epoch: 8.17 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020553307746543598		[learning rate: 0.0020272]
		[batch 20/20] avg loss: 0.03157401695216256		[learning rate: 0.0020247]
	Learning Rate: 0.00202474
	LOSS [training: 0.026063662349353083 | validation: 0.017273668531166488]
	TIME [epoch: 8.17 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025516607903894754		[learning rate: 0.0020223]
		[batch 20/20] avg loss: 0.02287709684643613		[learning rate: 0.0020198]
	Learning Rate: 0.00201983
	LOSS [training: 0.024196852375165452 | validation: 0.013950904290866387]
	TIME [epoch: 8.2 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025206749754734546		[learning rate: 0.0020174]
		[batch 20/20] avg loss: 0.025629512011787044		[learning rate: 0.0020149]
	Learning Rate: 0.00201494
	LOSS [training: 0.02541813088326079 | validation: 0.021875114092785912]
	TIME [epoch: 8.19 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015829260633082883		[learning rate: 0.0020125]
		[batch 20/20] avg loss: 0.040670385326251335		[learning rate: 0.0020101]
	Learning Rate: 0.00201007
	LOSS [training: 0.028249822979667112 | validation: 0.016339688935023265]
	TIME [epoch: 8.17 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022665499376522295		[learning rate: 0.0020076]
		[batch 20/20] avg loss: 0.02607969445684668		[learning rate: 0.0020052]
	Learning Rate: 0.0020052
	LOSS [training: 0.02437259691668449 | validation: 0.01847384396706725]
	TIME [epoch: 8.16 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02798938598840341		[learning rate: 0.0020028]
		[batch 20/20] avg loss: 0.025801001167898425		[learning rate: 0.0020003]
	Learning Rate: 0.00200035
	LOSS [training: 0.026895193578150917 | validation: 0.001696395955347174]
	TIME [epoch: 8.18 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03736735484976551		[learning rate: 0.0019979]
		[batch 20/20] avg loss: 0.036228978004341786		[learning rate: 0.0019955]
	Learning Rate: 0.0019955
	LOSS [training: 0.03679816642705365 | validation: 0.024653187734658705]
	TIME [epoch: 8.2 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027962211468734892		[learning rate: 0.0019931]
		[batch 20/20] avg loss: 0.01454474367346143		[learning rate: 0.0019907]
	Learning Rate: 0.00199067
	LOSS [training: 0.02125347757109816 | validation: 0.009031733427730549]
	TIME [epoch: 8.17 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017754964844972555		[learning rate: 0.0019883]
		[batch 20/20] avg loss: 0.0207189464806237		[learning rate: 0.0019859]
	Learning Rate: 0.00198585
	LOSS [training: 0.01923695566279813 | validation: -0.0006508488464069191]
	TIME [epoch: 8.17 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017736280801411687		[learning rate: 0.0019834]
		[batch 20/20] avg loss: 0.01952529654930702		[learning rate: 0.001981]
	Learning Rate: 0.00198105
	LOSS [training: 0.018630788675359358 | validation: 0.0030443541790185014]
	TIME [epoch: 8.2 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02165635099062586		[learning rate: 0.0019786]
		[batch 20/20] avg loss: 0.02718091590742234		[learning rate: 0.0019763]
	Learning Rate: 0.00197625
	LOSS [training: 0.024418633449024098 | validation: 0.013159780630226367]
	TIME [epoch: 8.17 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021508229903532634		[learning rate: 0.0019739]
		[batch 20/20] avg loss: 0.024162793759704693		[learning rate: 0.0019715]
	Learning Rate: 0.00197147
	LOSS [training: 0.022835511831618666 | validation: 0.013470307164107367]
	TIME [epoch: 8.19 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018006913185769295		[learning rate: 0.0019691]
		[batch 20/20] avg loss: 0.030399113374089393		[learning rate: 0.0019667]
	Learning Rate: 0.00196669
	LOSS [training: 0.024203013279929346 | validation: 0.021805307500788403]
	TIME [epoch: 8.17 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03657645383856798		[learning rate: 0.0019643]
		[batch 20/20] avg loss: 0.030778070591180123		[learning rate: 0.0019619]
	Learning Rate: 0.00196193
	LOSS [training: 0.03367726221487405 | validation: 0.008846197321450672]
	TIME [epoch: 8.16 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02849963818484199		[learning rate: 0.0019596]
		[batch 20/20] avg loss: 0.024091019010180986		[learning rate: 0.0019572]
	Learning Rate: 0.00195718
	LOSS [training: 0.026295328597511484 | validation: 0.004338425145577589]
	TIME [epoch: 8.16 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025568564334140183		[learning rate: 0.0019548]
		[batch 20/20] avg loss: 0.026438475380794628		[learning rate: 0.0019524]
	Learning Rate: 0.00195245
	LOSS [training: 0.0260035198574674 | validation: 0.01093693940205861]
	TIME [epoch: 8.19 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020448973201770707		[learning rate: 0.0019501]
		[batch 20/20] avg loss: 0.021503321661371054		[learning rate: 0.0019477]
	Learning Rate: 0.00194772
	LOSS [training: 0.020976147431570884 | validation: 0.01663353049140098]
	TIME [epoch: 8.19 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01869578592425182		[learning rate: 0.0019454]
		[batch 20/20] avg loss: 0.028848696814983553		[learning rate: 0.001943]
	Learning Rate: 0.001943
	LOSS [training: 0.023772241369617687 | validation: 0.007281152769450746]
	TIME [epoch: 8.18 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024303105153586527		[learning rate: 0.0019407]
		[batch 20/20] avg loss: 0.01940744058897856		[learning rate: 0.0019383]
	Learning Rate: 0.0019383
	LOSS [training: 0.02185527287128254 | validation: 0.037757385449877334]
	TIME [epoch: 8.17 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03597795085374314		[learning rate: 0.001936]
		[batch 20/20] avg loss: 0.032865462820577875		[learning rate: 0.0019336]
	Learning Rate: 0.00193361
	LOSS [training: 0.034421706837160504 | validation: 0.003941428094436601]
	TIME [epoch: 8.17 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024742209140261064		[learning rate: 0.0019313]
		[batch 20/20] avg loss: 0.021522522512656565		[learning rate: 0.0019289]
	Learning Rate: 0.00192893
	LOSS [training: 0.023132365826458813 | validation: 0.007246352229227988]
	TIME [epoch: 8.23 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0359902223658095		[learning rate: 0.0019266]
		[batch 20/20] avg loss: 0.03067941226375718		[learning rate: 0.0019243]
	Learning Rate: 0.00192426
	LOSS [training: 0.03333481731478333 | validation: 0.007438348926283901]
	TIME [epoch: 8.16 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023448202597292616		[learning rate: 0.0019219]
		[batch 20/20] avg loss: 0.01985090943699316		[learning rate: 0.0019196]
	Learning Rate: 0.0019196
	LOSS [training: 0.02164955601714289 | validation: -0.004454702200365618]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_781.pth
	Model improved!!!
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01617762279344174		[learning rate: 0.0019173]
		[batch 20/20] avg loss: 0.016964523734151825		[learning rate: 0.001915]
	Learning Rate: 0.00191495
	LOSS [training: 0.01657107326379678 | validation: -0.006837116888965039]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_782.pth
	Model improved!!!
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03055663980705934		[learning rate: 0.0019126]
		[batch 20/20] avg loss: 0.027309118064701727		[learning rate: 0.0019103]
	Learning Rate: 0.00191032
	LOSS [training: 0.028932878935880535 | validation: 0.014816008872005303]
	TIME [epoch: 8.22 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03230580420748301		[learning rate: 0.001908]
		[batch 20/20] avg loss: 0.059960956837153155		[learning rate: 0.0019057]
	Learning Rate: 0.00190569
	LOSS [training: 0.046133380522318086 | validation: 0.02590544543763692]
	TIME [epoch: 8.19 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03629980851151111		[learning rate: 0.0019034]
		[batch 20/20] avg loss: 0.03033469263535643		[learning rate: 0.0019011]
	Learning Rate: 0.00190108
	LOSS [training: 0.033317250573433765 | validation: 0.017848881723769756]
	TIME [epoch: 8.21 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02890592772166688		[learning rate: 0.0018988]
		[batch 20/20] avg loss: 0.06919924325208457		[learning rate: 0.0018965]
	Learning Rate: 0.00189648
	LOSS [training: 0.04905258548687572 | validation: 0.028708120035589976]
	TIME [epoch: 8.19 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043510463265242584		[learning rate: 0.0018942]
		[batch 20/20] avg loss: 0.03053622411233241		[learning rate: 0.0018919]
	Learning Rate: 0.00189188
	LOSS [training: 0.03702334368878749 | validation: 0.0038121901939837544]
	TIME [epoch: 8.22 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01844525102956989		[learning rate: 0.0018896]
		[batch 20/20] avg loss: 0.02389069894233206		[learning rate: 0.0018873]
	Learning Rate: 0.00188731
	LOSS [training: 0.021167974985950973 | validation: 0.012090438549439409]
	TIME [epoch: 8.23 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030495226080763244		[learning rate: 0.001885]
		[batch 20/20] avg loss: 0.023533590889549597		[learning rate: 0.0018827]
	Learning Rate: 0.00188274
	LOSS [training: 0.027014408485156422 | validation: 0.017215577756905173]
	TIME [epoch: 8.19 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017945606145833758		[learning rate: 0.0018805]
		[batch 20/20] avg loss: 0.017849333243708555		[learning rate: 0.0018782]
	Learning Rate: 0.00187818
	LOSS [training: 0.017897469694771156 | validation: 0.02083635893656929]
	TIME [epoch: 8.19 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019057319844758464		[learning rate: 0.0018759]
		[batch 20/20] avg loss: 0.02496179100300177		[learning rate: 0.0018736]
	Learning Rate: 0.00187363
	LOSS [training: 0.02200955542388012 | validation: 0.006168494120260085]
	TIME [epoch: 8.19 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020419459226320442		[learning rate: 0.0018714]
		[batch 20/20] avg loss: 0.020470626179168356		[learning rate: 0.0018691]
	Learning Rate: 0.0018691
	LOSS [training: 0.0204450427027444 | validation: -0.0007213109924458635]
	TIME [epoch: 8.21 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029175569918675214		[learning rate: 0.0018668]
		[batch 20/20] avg loss: 0.023327118822069164		[learning rate: 0.0018646]
	Learning Rate: 0.00186457
	LOSS [training: 0.026251344370372187 | validation: 0.0296518734098845]
	TIME [epoch: 8.19 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02826325011842324		[learning rate: 0.0018623]
		[batch 20/20] avg loss: 0.03274466669904876		[learning rate: 0.0018601]
	Learning Rate: 0.00186006
	LOSS [training: 0.030503958408736005 | validation: 0.03193163008688783]
	TIME [epoch: 8.21 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0300686828742446		[learning rate: 0.0018578]
		[batch 20/20] avg loss: 0.02462344765163888		[learning rate: 0.0018556]
	Learning Rate: 0.00185555
	LOSS [training: 0.027346065262941744 | validation: 0.019692400022534823]
	TIME [epoch: 8.19 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02714466168739122		[learning rate: 0.0018533]
		[batch 20/20] avg loss: 0.0328978534033381		[learning rate: 0.0018511]
	Learning Rate: 0.00185106
	LOSS [training: 0.030021257545364655 | validation: 0.039845050488730105]
	TIME [epoch: 8.21 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028385978725846583		[learning rate: 0.0018488]
		[batch 20/20] avg loss: 0.02829877771810112		[learning rate: 0.0018466]
	Learning Rate: 0.00184658
	LOSS [training: 0.028342378221973847 | validation: 0.02338126274996753]
	TIME [epoch: 8.21 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05481395790970488		[learning rate: 0.0018443]
		[batch 20/20] avg loss: 0.04227305148273928		[learning rate: 0.0018421]
	Learning Rate: 0.00184211
	LOSS [training: 0.04854350469622208 | validation: 0.02203102476862667]
	TIME [epoch: 8.22 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042198724634225145		[learning rate: 0.0018399]
		[batch 20/20] avg loss: 0.0668831566970296		[learning rate: 0.0018377]
	Learning Rate: 0.00183765
	LOSS [training: 0.05454094066562737 | validation: 0.03330807388555198]
	TIME [epoch: 8.19 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06383605348487131		[learning rate: 0.0018354]
		[batch 20/20] avg loss: 0.06401897289967809		[learning rate: 0.0018332]
	Learning Rate: 0.0018332
	LOSS [training: 0.06392751319227472 | validation: 0.04748888498969647]
	TIME [epoch: 8.19 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029015601991042212		[learning rate: 0.001831]
		[batch 20/20] avg loss: 0.02493501844770805		[learning rate: 0.0018288]
	Learning Rate: 0.00182876
	LOSS [training: 0.02697531021937513 | validation: 0.01657679804755339]
	TIME [epoch: 8.22 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052632179962251044		[learning rate: 0.0018265]
		[batch 20/20] avg loss: 0.03572421073844166		[learning rate: 0.0018243]
	Learning Rate: 0.00182434
	LOSS [training: 0.04417819535034634 | validation: 0.024487494452451034]
	TIME [epoch: 8.18 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03170856538387208		[learning rate: 0.0018221]
		[batch 20/20] avg loss: 0.023242245030458474		[learning rate: 0.0018199]
	Learning Rate: 0.00181992
	LOSS [training: 0.027475405207165276 | validation: 0.015110003648198967]
	TIME [epoch: 8.21 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01877250456042059		[learning rate: 0.0018177]
		[batch 20/20] avg loss: 0.02355550565232458		[learning rate: 0.0018155]
	Learning Rate: 0.00181552
	LOSS [training: 0.021164005106372587 | validation: 0.027419079929731235]
	TIME [epoch: 8.19 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041853854679442234		[learning rate: 0.0018133]
		[batch 20/20] avg loss: 0.021096604003316276		[learning rate: 0.0018111]
	Learning Rate: 0.00181112
	LOSS [training: 0.03147522934137926 | validation: 0.006191816123579475]
	TIME [epoch: 8.22 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021210033355144563		[learning rate: 0.0018089]
		[batch 20/20] avg loss: 0.03102859025713927		[learning rate: 0.0018067]
	Learning Rate: 0.00180674
	LOSS [training: 0.026119311806141916 | validation: 0.017601941825385572]
	TIME [epoch: 8.22 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02541553701018111		[learning rate: 0.0018045]
		[batch 20/20] avg loss: 0.02453012645349358		[learning rate: 0.0018024]
	Learning Rate: 0.00180236
	LOSS [training: 0.024972831731837345 | validation: 0.0024690511595741333]
	TIME [epoch: 8.21 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01886336867823194		[learning rate: 0.0018002]
		[batch 20/20] avg loss: 0.03260006848879399		[learning rate: 0.001798]
	Learning Rate: 0.001798
	LOSS [training: 0.025731718583512964 | validation: 0.014459822485296519]
	TIME [epoch: 8.19 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025207714498616536		[learning rate: 0.0017958]
		[batch 20/20] avg loss: 0.021399964966406716		[learning rate: 0.0017936]
	Learning Rate: 0.00179365
	LOSS [training: 0.023303839732511624 | validation: 0.02459780942199953]
	TIME [epoch: 8.19 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027936678833874563		[learning rate: 0.0017915]
		[batch 20/20] avg loss: 0.026895877553820946		[learning rate: 0.0017893]
	Learning Rate: 0.0017893
	LOSS [training: 0.027416278193847753 | validation: 0.018325938146796335]
	TIME [epoch: 8.2 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022365278774907033		[learning rate: 0.0017871]
		[batch 20/20] avg loss: 0.025002933124058806		[learning rate: 0.001785]
	Learning Rate: 0.00178497
	LOSS [training: 0.02368410594948292 | validation: 0.009164194916351738]
	TIME [epoch: 8.18 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02341745358226195		[learning rate: 0.0017828]
		[batch 20/20] avg loss: 0.01915776029657671		[learning rate: 0.0017807]
	Learning Rate: 0.00178065
	LOSS [training: 0.021287606939419338 | validation: 0.0034484398898235297]
	TIME [epoch: 8.21 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018803565721144843		[learning rate: 0.0017785]
		[batch 20/20] avg loss: 0.037763651724238276		[learning rate: 0.0017763]
	Learning Rate: 0.00177634
	LOSS [training: 0.028283608722691554 | validation: 0.053841767312447976]
	TIME [epoch: 8.19 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041485132905610414		[learning rate: 0.0017742]
		[batch 20/20] avg loss: 0.04880051814792123		[learning rate: 0.001772]
	Learning Rate: 0.00177204
	LOSS [training: 0.04514282552676583 | validation: 0.025597407259802113]
	TIME [epoch: 8.21 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03274976562867222		[learning rate: 0.0017699]
		[batch 20/20] avg loss: 0.03896865901467408		[learning rate: 0.0017678]
	Learning Rate: 0.00176775
	LOSS [training: 0.03585921232167315 | validation: 0.02874275107162331]
	TIME [epoch: 8.22 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03131130635932947		[learning rate: 0.0017656]
		[batch 20/20] avg loss: 0.032127701305328896		[learning rate: 0.0017635]
	Learning Rate: 0.00176347
	LOSS [training: 0.03171950383232918 | validation: 0.019578539210466032]
	TIME [epoch: 8.18 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03257366916474076		[learning rate: 0.0017613]
		[batch 20/20] avg loss: 0.01814776945612401		[learning rate: 0.0017592]
	Learning Rate: 0.0017592
	LOSS [training: 0.025360719310432385 | validation: 0.01663028782303623]
	TIME [epoch: 8.18 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017001197018244537		[learning rate: 0.0017571]
		[batch 20/20] avg loss: 0.02153832667899312		[learning rate: 0.0017549]
	Learning Rate: 0.00175494
	LOSS [training: 0.019269761848618824 | validation: 0.0048982237547100065]
	TIME [epoch: 8.21 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017959739890300707		[learning rate: 0.0017528]
		[batch 20/20] avg loss: 0.022223336835842312		[learning rate: 0.0017507]
	Learning Rate: 0.0017507
	LOSS [training: 0.02009153836307151 | validation: -0.0006535579539490577]
	TIME [epoch: 8.19 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019064190407823048		[learning rate: 0.0017486]
		[batch 20/20] avg loss: 0.018455321743818993		[learning rate: 0.0017465]
	Learning Rate: 0.00174646
	LOSS [training: 0.01875975607582102 | validation: 0.018112326036507295]
	TIME [epoch: 8.2 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02196996764349403		[learning rate: 0.0017443]
		[batch 20/20] avg loss: 0.015808239040211745		[learning rate: 0.0017422]
	Learning Rate: 0.00174223
	LOSS [training: 0.01888910334185289 | validation: 0.005100307404329854]
	TIME [epoch: 8.19 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025396516635431222		[learning rate: 0.0017401]
		[batch 20/20] avg loss: 0.02453214490716618		[learning rate: 0.001738]
	Learning Rate: 0.00173801
	LOSS [training: 0.024964330771298702 | validation: 0.018009591107275718]
	TIME [epoch: 8.19 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02419073988204993		[learning rate: 0.0017359]
		[batch 20/20] avg loss: 0.019823993179857318		[learning rate: 0.0017338]
	Learning Rate: 0.0017338
	LOSS [training: 0.022007366530953622 | validation: 0.0051080399926834545]
	TIME [epoch: 8.22 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020233243863113853		[learning rate: 0.0017317]
		[batch 20/20] avg loss: 0.0277501134809618		[learning rate: 0.0017296]
	Learning Rate: 0.00172961
	LOSS [training: 0.023991678672037827 | validation: 0.015840088854152706]
	TIME [epoch: 8.21 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0229013497255851		[learning rate: 0.0017275]
		[batch 20/20] avg loss: 0.041369404843808064		[learning rate: 0.0017254]
	Learning Rate: 0.00172542
	LOSS [training: 0.03213537728469658 | validation: 0.04275098278877616]
	TIME [epoch: 8.18 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026593841759181926		[learning rate: 0.0017233]
		[batch 20/20] avg loss: 0.023171565121284543		[learning rate: 0.0017212]
	Learning Rate: 0.00172124
	LOSS [training: 0.02488270344023323 | validation: -0.0009093481003322199]
	TIME [epoch: 8.18 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017251711208781217		[learning rate: 0.0017192]
		[batch 20/20] avg loss: 0.026902005802661715		[learning rate: 0.0017171]
	Learning Rate: 0.00171708
	LOSS [training: 0.022076858505721466 | validation: 0.007244025306446829]
	TIME [epoch: 8.2 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03246367850608934		[learning rate: 0.001715]
		[batch 20/20] avg loss: 0.026557712966089803		[learning rate: 0.0017129]
	Learning Rate: 0.00171292
	LOSS [training: 0.029510695736089564 | validation: 0.01606269767770583]
	TIME [epoch: 8.18 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02421719889985168		[learning rate: 0.0017108]
		[batch 20/20] avg loss: 0.02295717544920999		[learning rate: 0.0017088]
	Learning Rate: 0.00170877
	LOSS [training: 0.02358718717453084 | validation: 0.025035938237594292]
	TIME [epoch: 8.2 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05101056240170234		[learning rate: 0.0017067]
		[batch 20/20] avg loss: 0.0249067156618395		[learning rate: 0.0017046]
	Learning Rate: 0.00170464
	LOSS [training: 0.037958639031770915 | validation: 0.002823460834036854]
	TIME [epoch: 8.19 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02147983135851568		[learning rate: 0.0017026]
		[batch 20/20] avg loss: 0.019652841998154463		[learning rate: 0.0017005]
	Learning Rate: 0.00170051
	LOSS [training: 0.02056633667833507 | validation: 0.012711505030956386]
	TIME [epoch: 8.18 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023949124238366933		[learning rate: 0.0016984]
		[batch 20/20] avg loss: 0.021666625234319652		[learning rate: 0.0016964]
	Learning Rate: 0.00169639
	LOSS [training: 0.02280787473634329 | validation: 0.03279489052146224]
	TIME [epoch: 8.23 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033767311457943434		[learning rate: 0.0016943]
		[batch 20/20] avg loss: 0.021086756090001102		[learning rate: 0.0016923]
	Learning Rate: 0.00169229
	LOSS [training: 0.027427033773972265 | validation: 0.005232093620970316]
	TIME [epoch: 8.19 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015331915074761912		[learning rate: 0.0016902]
		[batch 20/20] avg loss: 0.01369421658128761		[learning rate: 0.0016882]
	Learning Rate: 0.00168819
	LOSS [training: 0.01451306582802476 | validation: 0.0008640083269975948]
	TIME [epoch: 8.18 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020198247171011083		[learning rate: 0.0016861]
		[batch 20/20] avg loss: 0.014481088037725381		[learning rate: 0.0016841]
	Learning Rate: 0.0016841
	LOSS [training: 0.017339667604368225 | validation: 0.006672040254855885]
	TIME [epoch: 8.18 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02154906743795587		[learning rate: 0.0016821]
		[batch 20/20] avg loss: 0.01754270854355606		[learning rate: 0.00168]
	Learning Rate: 0.00168003
	LOSS [training: 0.019545887990755968 | validation: -0.006364152784328544]
	TIME [epoch: 8.2 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014188399617193672		[learning rate: 0.001678]
		[batch 20/20] avg loss: 0.024953006967773448		[learning rate: 0.001676]
	Learning Rate: 0.00167596
	LOSS [training: 0.01957070329248356 | validation: 0.004025149357170197]
	TIME [epoch: 8.2 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028419254992426875		[learning rate: 0.0016739]
		[batch 20/20] avg loss: 0.019254523445010212		[learning rate: 0.0016719]
	Learning Rate: 0.0016719
	LOSS [training: 0.023836889218718543 | validation: 0.014547652712767777]
	TIME [epoch: 8.19 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016386559858156305		[learning rate: 0.0016699]
		[batch 20/20] avg loss: 0.03335648224533933		[learning rate: 0.0016679]
	Learning Rate: 0.00166785
	LOSS [training: 0.024871521051747822 | validation: 0.006972889729227652]
	TIME [epoch: 8.19 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025022469749852783		[learning rate: 0.0016658]
		[batch 20/20] avg loss: 0.02936628397064487		[learning rate: 0.0016638]
	Learning Rate: 0.00166382
	LOSS [training: 0.02719437686024883 | validation: 0.003613613735440801]
	TIME [epoch: 8.22 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021263587907512926		[learning rate: 0.0016618]
		[batch 20/20] avg loss: 0.022185428390396313		[learning rate: 0.0016598]
	Learning Rate: 0.00165979
	LOSS [training: 0.021724508148954623 | validation: 0.007201223768780094]
	TIME [epoch: 8.2 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0224425064287296		[learning rate: 0.0016578]
		[batch 20/20] avg loss: 0.018564588130597293		[learning rate: 0.0016558]
	Learning Rate: 0.00165577
	LOSS [training: 0.020503547279663446 | validation: 3.498853382337039e-05]
	TIME [epoch: 8.18 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012585695897268382		[learning rate: 0.0016538]
		[batch 20/20] avg loss: 0.020429850846253993		[learning rate: 0.0016518]
	Learning Rate: 0.00165176
	LOSS [training: 0.01650777337176119 | validation: 0.0035661264476136378]
	TIME [epoch: 8.18 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01933347168552601		[learning rate: 0.0016498]
		[batch 20/20] avg loss: 0.02400885260898212		[learning rate: 0.0016478]
	Learning Rate: 0.00164776
	LOSS [training: 0.021671162147254064 | validation: 0.006880050425469203]
	TIME [epoch: 8.18 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020514089372077		[learning rate: 0.0016458]
		[batch 20/20] avg loss: 0.024083692034825695		[learning rate: 0.0016438]
	Learning Rate: 0.00164377
	LOSS [training: 0.022298890703451348 | validation: 0.012486862611886924]
	TIME [epoch: 8.23 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029910879942686826		[learning rate: 0.0016418]
		[batch 20/20] avg loss: 0.026425758791018265		[learning rate: 0.0016398]
	Learning Rate: 0.00163979
	LOSS [training: 0.02816831936685254 | validation: 0.006807279010633966]
	TIME [epoch: 8.19 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01691476176593947		[learning rate: 0.0016378]
		[batch 20/20] avg loss: 0.02463428826077399		[learning rate: 0.0016358]
	Learning Rate: 0.00163583
	LOSS [training: 0.020774525013356727 | validation: 0.005662843097677197]
	TIME [epoch: 8.19 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02077284868447034		[learning rate: 0.0016338]
		[batch 20/20] avg loss: 0.03141583786656308		[learning rate: 0.0016319]
	Learning Rate: 0.00163186
	LOSS [training: 0.026094343275516706 | validation: 0.003299086596998354]
	TIME [epoch: 8.2 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02785410936272164		[learning rate: 0.0016299]
		[batch 20/20] avg loss: 0.025908924888558244		[learning rate: 0.0016279]
	Learning Rate: 0.00162791
	LOSS [training: 0.026881517125639943 | validation: 0.009497588482490457]
	TIME [epoch: 8.23 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030312915979047644		[learning rate: 0.0016259]
		[batch 20/20] avg loss: 0.024591211175777766		[learning rate: 0.001624]
	Learning Rate: 0.00162397
	LOSS [training: 0.027452063577412707 | validation: 0.0020207355165412257]
	TIME [epoch: 8.19 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01153602029042695		[learning rate: 0.001622]
		[batch 20/20] avg loss: 0.023545571763596736		[learning rate: 0.00162]
	Learning Rate: 0.00162004
	LOSS [training: 0.017540796027011845 | validation: 0.02628082132550521]
	TIME [epoch: 8.18 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02398248811259841		[learning rate: 0.0016181]
		[batch 20/20] avg loss: 0.024409576460146264		[learning rate: 0.0016161]
	Learning Rate: 0.00161612
	LOSS [training: 0.024196032286372335 | validation: 0.012158124963846245]
	TIME [epoch: 8.18 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020754507728307378		[learning rate: 0.0016142]
		[batch 20/20] avg loss: 0.01311097207311617		[learning rate: 0.0016122]
	Learning Rate: 0.00161221
	LOSS [training: 0.016932739900711775 | validation: 0.00515403864730939]
	TIME [epoch: 8.18 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007796425424740934		[learning rate: 0.0016103]
		[batch 20/20] avg loss: 0.024920969008076117		[learning rate: 0.0016083]
	Learning Rate: 0.0016083
	LOSS [training: 0.016358697216408525 | validation: 0.003344946468538671]
	TIME [epoch: 8.21 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030379350488485925		[learning rate: 0.0016064]
		[batch 20/20] avg loss: 0.0374779433273542		[learning rate: 0.0016044]
	Learning Rate: 0.00160441
	LOSS [training: 0.033928646907920065 | validation: 0.004245374934431041]
	TIME [epoch: 8.21 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02657244054224799		[learning rate: 0.0016025]
		[batch 20/20] avg loss: 0.027364309371087546		[learning rate: 0.0016005]
	Learning Rate: 0.00160053
	LOSS [training: 0.02696837495666777 | validation: 0.010139898220205789]
	TIME [epoch: 8.19 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0343819141214488		[learning rate: 0.0015986]
		[batch 20/20] avg loss: 0.01860224014448504		[learning rate: 0.0015967]
	Learning Rate: 0.00159665
	LOSS [training: 0.026492077132966925 | validation: -0.004914042712933566]
	TIME [epoch: 8.19 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020653380277214122		[learning rate: 0.0015947]
		[batch 20/20] avg loss: 0.018190185458320143		[learning rate: 0.0015928]
	Learning Rate: 0.00159279
	LOSS [training: 0.019421782867767138 | validation: 0.003790010966352726]
	TIME [epoch: 8.21 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024011389834974067		[learning rate: 0.0015909]
		[batch 20/20] avg loss: 0.027193035063397437		[learning rate: 0.0015889]
	Learning Rate: 0.00158893
	LOSS [training: 0.025602212449185747 | validation: 0.01664119944832755]
	TIME [epoch: 8.23 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023536769273791898		[learning rate: 0.001587]
		[batch 20/20] avg loss: 0.022762212123517315		[learning rate: 0.0015851]
	Learning Rate: 0.00158509
	LOSS [training: 0.023149490698654608 | validation: 0.012809462559164934]
	TIME [epoch: 8.19 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028025054344380906		[learning rate: 0.0015832]
		[batch 20/20] avg loss: 0.029505743899976178		[learning rate: 0.0015812]
	Learning Rate: 0.00158125
	LOSS [training: 0.028765399122178535 | validation: 0.005958478475052838]
	TIME [epoch: 8.19 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013339041719971426		[learning rate: 0.0015793]
		[batch 20/20] avg loss: 0.012932888201383686		[learning rate: 0.0015774]
	Learning Rate: 0.00157742
	LOSS [training: 0.013135964960677555 | validation: -0.0008717607142053612]
	TIME [epoch: 8.2 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01432622064194757		[learning rate: 0.0015755]
		[batch 20/20] avg loss: 0.011689104247214744		[learning rate: 0.0015736]
	Learning Rate: 0.0015736
	LOSS [training: 0.013007662444581156 | validation: 0.005063920214374027]
	TIME [epoch: 8.2 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016877733896838498		[learning rate: 0.0015717]
		[batch 20/20] avg loss: 0.02773970850278617		[learning rate: 0.0015698]
	Learning Rate: 0.00156979
	LOSS [training: 0.02230872119981233 | validation: 0.002160651179505366]
	TIME [epoch: 8.21 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018712065308207067		[learning rate: 0.0015679]
		[batch 20/20] avg loss: 0.01382824786577542		[learning rate: 0.001566]
	Learning Rate: 0.00156599
	LOSS [training: 0.016270156586991245 | validation: 0.005446070579381917]
	TIME [epoch: 8.2 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02255790664918878		[learning rate: 0.0015641]
		[batch 20/20] avg loss: 0.010777524048124406		[learning rate: 0.0015622]
	Learning Rate: 0.0015622
	LOSS [training: 0.016667715348656596 | validation: 0.001615697491395255]
	TIME [epoch: 8.18 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01282653784301897		[learning rate: 0.0015603]
		[batch 20/20] avg loss: 0.015917422948814118		[learning rate: 0.0015584]
	Learning Rate: 0.00155842
	LOSS [training: 0.014371980395916542 | validation: -0.0022729810761201942]
	TIME [epoch: 8.24 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023105176537483937		[learning rate: 0.0015565]
		[batch 20/20] avg loss: 0.02276870398223895		[learning rate: 0.0015546]
	Learning Rate: 0.00155465
	LOSS [training: 0.022936940259861444 | validation: 0.027003711942784447]
	TIME [epoch: 8.2 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02103768470438576		[learning rate: 0.0015528]
		[batch 20/20] avg loss: 0.02093288189688026		[learning rate: 0.0015509]
	Learning Rate: 0.00155088
	LOSS [training: 0.02098528330063301 | validation: 0.015602082962938553]
	TIME [epoch: 8.18 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031309308956160034		[learning rate: 0.001549]
		[batch 20/20] avg loss: 0.02039256644108598		[learning rate: 0.0015471]
	Learning Rate: 0.00154713
	LOSS [training: 0.025850937698623004 | validation: 0.023982998142133945]
	TIME [epoch: 8.18 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034552741027599254		[learning rate: 0.0015453]
		[batch 20/20] avg loss: 0.05758065683160102		[learning rate: 0.0015434]
	Learning Rate: 0.00154338
	LOSS [training: 0.04606669892960013 | validation: 0.02189322547239239]
	TIME [epoch: 8.23 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025539794304648377		[learning rate: 0.0015415]
		[batch 20/20] avg loss: 0.024884260264223284		[learning rate: 0.0015396]
	Learning Rate: 0.00153965
	LOSS [training: 0.025212027284435827 | validation: 0.006163759652207076]
	TIME [epoch: 8.2 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01819793936354355		[learning rate: 0.0015378]
		[batch 20/20] avg loss: 0.02440012881247653		[learning rate: 0.0015359]
	Learning Rate: 0.00153592
	LOSS [training: 0.021299034088010045 | validation: 0.01966859316784817]
	TIME [epoch: 8.19 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028394672413545256		[learning rate: 0.0015341]
		[batch 20/20] avg loss: 0.039848623230508494		[learning rate: 0.0015322]
	Learning Rate: 0.0015322
	LOSS [training: 0.03412164782202688 | validation: 0.015165722892637252]
	TIME [epoch: 8.2 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017636139454048964		[learning rate: 0.0015303]
		[batch 20/20] avg loss: 0.02427521465566403		[learning rate: 0.0015285]
	Learning Rate: 0.00152849
	LOSS [training: 0.020955677054856493 | validation: 0.010749097220932431]
	TIME [epoch: 8.22 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017731424100723012		[learning rate: 0.0015266]
		[batch 20/20] avg loss: 0.017199460017781894		[learning rate: 0.0015248]
	Learning Rate: 0.00152479
	LOSS [training: 0.017465442059252455 | validation: 0.0031059958617129244]
	TIME [epoch: 8.21 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012179845057265552		[learning rate: 0.0015229]
		[batch 20/20] avg loss: 0.026880817843487705		[learning rate: 0.0015211]
	Learning Rate: 0.0015211
	LOSS [training: 0.019530331450376626 | validation: 0.012055487438094085]
	TIME [epoch: 8.17 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015220526086685546		[learning rate: 0.0015193]
		[batch 20/20] avg loss: 0.02345788416107488		[learning rate: 0.0015174]
	Learning Rate: 0.00151742
	LOSS [training: 0.019339205123880217 | validation: 0.028469442754429634]
	TIME [epoch: 8.18 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01667388986855373		[learning rate: 0.0015156]
		[batch 20/20] avg loss: 0.015300893535758725		[learning rate: 0.0015137]
	Learning Rate: 0.00151374
	LOSS [training: 0.015987391702156226 | validation: 0.008898860292432004]
	TIME [epoch: 8.18 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024609234839077607		[learning rate: 0.0015119]
		[batch 20/20] avg loss: 0.021556747669950756		[learning rate: 0.0015101]
	Learning Rate: 0.00151008
	LOSS [training: 0.02308299125451418 | validation: 0.00999220676316264]
	TIME [epoch: 8.23 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020364260365106808		[learning rate: 0.0015083]
		[batch 20/20] avg loss: 0.021044982118740032		[learning rate: 0.0015064]
	Learning Rate: 0.00150642
	LOSS [training: 0.020704621241923415 | validation: 0.01412531529007327]
	TIME [epoch: 8.2 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01593939992902614		[learning rate: 0.0015046]
		[batch 20/20] avg loss: 0.02002430052364995		[learning rate: 0.0015028]
	Learning Rate: 0.00150278
	LOSS [training: 0.017981850226338043 | validation: 0.018226517807155954]
	TIME [epoch: 8.19 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02427796409831473		[learning rate: 0.001501]
		[batch 20/20] avg loss: 0.01849168375399336		[learning rate: 0.0014991]
	Learning Rate: 0.00149914
	LOSS [training: 0.021384823926154047 | validation: 0.008984419097218126]
	TIME [epoch: 8.23 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016838780706337996		[learning rate: 0.0014973]
		[batch 20/20] avg loss: 0.049857356860150845		[learning rate: 0.0014955]
	Learning Rate: 0.00149551
	LOSS [training: 0.03334806878324442 | validation: 0.057419293064158786]
	TIME [epoch: 8.18 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08052591914371358		[learning rate: 0.0014937]
		[batch 20/20] avg loss: 0.07134319653796957		[learning rate: 0.0014919]
	Learning Rate: 0.00149189
	LOSS [training: 0.07593455784084159 | validation: 0.032717256970930246]
	TIME [epoch: 8.21 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049213922193269685		[learning rate: 0.0014901]
		[batch 20/20] avg loss: 0.05248610283377777		[learning rate: 0.0014883]
	Learning Rate: 0.00148828
	LOSS [training: 0.050850012513523724 | validation: 0.024604400948658556]
	TIME [epoch: 8.18 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03458825547086042		[learning rate: 0.0014865]
		[batch 20/20] avg loss: 0.019403799341354798		[learning rate: 0.0014847]
	Learning Rate: 0.00148468
	LOSS [training: 0.026996027406107605 | validation: 0.00506468234585709]
	TIME [epoch: 8.2 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016637974254214597		[learning rate: 0.0014829]
		[batch 20/20] avg loss: 0.020911125383721484		[learning rate: 0.0014811]
	Learning Rate: 0.00148108
	LOSS [training: 0.01877454981896804 | validation: 0.0031583347735081313]
	TIME [epoch: 8.18 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022223331470769214		[learning rate: 0.0014793]
		[batch 20/20] avg loss: 0.02514701904213919		[learning rate: 0.0014775]
	Learning Rate: 0.0014775
	LOSS [training: 0.023685175256454204 | validation: 0.026864552925563986]
	TIME [epoch: 8.21 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016801272482834785		[learning rate: 0.0014757]
		[batch 20/20] avg loss: 0.016986625120906484		[learning rate: 0.0014739]
	Learning Rate: 0.00147392
	LOSS [training: 0.016893948801870635 | validation: 0.009448049457795926]
	TIME [epoch: 8.22 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02244261183433673		[learning rate: 0.0014721]
		[batch 20/20] avg loss: 0.021227025430341277		[learning rate: 0.0014704]
	Learning Rate: 0.00147035
	LOSS [training: 0.021834818632339003 | validation: -0.004389496554516384]
	TIME [epoch: 8.18 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015424087395830682		[learning rate: 0.0014686]
		[batch 20/20] avg loss: 0.013984717447621076		[learning rate: 0.0014668]
	Learning Rate: 0.00146679
	LOSS [training: 0.014704402421725878 | validation: -0.0055636354627253215]
	TIME [epoch: 8.18 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0159703116594158		[learning rate: 0.001465]
		[batch 20/20] avg loss: 0.013596428980925218		[learning rate: 0.0014632]
	Learning Rate: 0.00146324
	LOSS [training: 0.014783370320170509 | validation: -0.003508598861354334]
	TIME [epoch: 8.2 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011897428290303338		[learning rate: 0.0014615]
		[batch 20/20] avg loss: 0.021396182820108862		[learning rate: 0.0014597]
	Learning Rate: 0.0014597
	LOSS [training: 0.0166468055552061 | validation: 0.0163585121603618]
	TIME [epoch: 8.2 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03841228392658595		[learning rate: 0.0014579]
		[batch 20/20] avg loss: 0.01754136355647407		[learning rate: 0.0014562]
	Learning Rate: 0.00145616
	LOSS [training: 0.02797682374153001 | validation: 0.00426104457013784]
	TIME [epoch: 8.2 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019679778395685025		[learning rate: 0.0014544]
		[batch 20/20] avg loss: 0.028489134452456665		[learning rate: 0.0014526]
	Learning Rate: 0.00145264
	LOSS [training: 0.024084456424070845 | validation: 0.00607720597244601]
	TIME [epoch: 8.18 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02836060716788826		[learning rate: 0.0014509]
		[batch 20/20] avg loss: 0.024324646809517483		[learning rate: 0.0014491]
	Learning Rate: 0.00144912
	LOSS [training: 0.026342626988702877 | validation: 0.002962217483012422]
	TIME [epoch: 8.18 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021672802834851717		[learning rate: 0.0014474]
		[batch 20/20] avg loss: 0.023942801983550338		[learning rate: 0.0014456]
	Learning Rate: 0.00144562
	LOSS [training: 0.02280780240920103 | validation: 0.015260549856663521]
	TIME [epoch: 8.25 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023051856762049464		[learning rate: 0.0014439]
		[batch 20/20] avg loss: 0.0341596774472228		[learning rate: 0.0014421]
	Learning Rate: 0.00144212
	LOSS [training: 0.02860576710463613 | validation: -0.0023058775100084955]
	TIME [epoch: 8.18 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022021265815306532		[learning rate: 0.0014404]
		[batch 20/20] avg loss: 0.017010605664852844		[learning rate: 0.0014386]
	Learning Rate: 0.00143862
	LOSS [training: 0.019515935740079694 | validation: -0.0026070221787710514]
	TIME [epoch: 8.18 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010040975445551931		[learning rate: 0.0014369]
		[batch 20/20] avg loss: 0.011870696196671318		[learning rate: 0.0014351]
	Learning Rate: 0.00143514
	LOSS [training: 0.010955835821111624 | validation: -0.00286741060361384]
	TIME [epoch: 8.17 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0117886863309078		[learning rate: 0.0014334]
		[batch 20/20] avg loss: 0.029900166293820474		[learning rate: 0.0014317]
	Learning Rate: 0.00143167
	LOSS [training: 0.02084442631236414 | validation: -0.00876820789871086]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_902.pth
	Model improved!!!
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017501232766353604		[learning rate: 0.0014299]
		[batch 20/20] avg loss: 0.023682396365853033		[learning rate: 0.0014282]
	Learning Rate: 0.0014282
	LOSS [training: 0.02059181456610332 | validation: 0.009471879299631085]
	TIME [epoch: 8.19 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020698806943734663		[learning rate: 0.0014265]
		[batch 20/20] avg loss: 0.015168224984069861		[learning rate: 0.0014247]
	Learning Rate: 0.00142474
	LOSS [training: 0.017933515963902268 | validation: 0.005868100045315434]
	TIME [epoch: 8.19 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009112443778327378		[learning rate: 0.001423]
		[batch 20/20] avg loss: 0.019688596443835325		[learning rate: 0.0014213]
	Learning Rate: 0.00142129
	LOSS [training: 0.014400520111081352 | validation: -0.006699891898774828]
	TIME [epoch: 8.17 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012866239626482524		[learning rate: 0.0014196]
		[batch 20/20] avg loss: 0.012659364150355113		[learning rate: 0.0014179]
	Learning Rate: 0.00141785
	LOSS [training: 0.012762801888418818 | validation: 0.008075207535914652]
	TIME [epoch: 8.18 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014776301763817853		[learning rate: 0.0014161]
		[batch 20/20] avg loss: 0.023158416059958654		[learning rate: 0.0014144]
	Learning Rate: 0.00141442
	LOSS [training: 0.018967358911888253 | validation: 0.0032691610828267815]
	TIME [epoch: 8.23 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007736119840124694		[learning rate: 0.0014127]
		[batch 20/20] avg loss: 0.021017253310941487		[learning rate: 0.001411]
	Learning Rate: 0.001411
	LOSS [training: 0.014376686575533088 | validation: -0.012092469973752969]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_908.pth
	Model improved!!!
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011415839427436606		[learning rate: 0.0014093]
		[batch 20/20] avg loss: 0.008108368836536048		[learning rate: 0.0014076]
	Learning Rate: 0.00140758
	LOSS [training: 0.009762104131986325 | validation: -0.008775004527541538]
	TIME [epoch: 8.17 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020297914686627238		[learning rate: 0.0014059]
		[batch 20/20] avg loss: 0.010011018417851954		[learning rate: 0.0014042]
	Learning Rate: 0.00140417
	LOSS [training: 0.015154466552239596 | validation: 0.004221581177520791]
	TIME [epoch: 8.16 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014911992566255866		[learning rate: 0.0014025]
		[batch 20/20] avg loss: 0.01680984430198171		[learning rate: 0.0014008]
	Learning Rate: 0.00140078
	LOSS [training: 0.01586091843411879 | validation: 0.0005706715607283323]
	TIME [epoch: 8.2 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015584328199133269		[learning rate: 0.0013991]
		[batch 20/20] avg loss: 0.01907894370975497		[learning rate: 0.0013974]
	Learning Rate: 0.00139738
	LOSS [training: 0.01733163595444412 | validation: 0.0034159229452676823]
	TIME [epoch: 8.17 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014530825646001586		[learning rate: 0.0013957]
		[batch 20/20] avg loss: 0.018905438915814504		[learning rate: 0.001394]
	Learning Rate: 0.001394
	LOSS [training: 0.016718132280908043 | validation: 0.00047486849477928225]
	TIME [epoch: 8.17 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014815039000827729		[learning rate: 0.0013923]
		[batch 20/20] avg loss: 0.017529293363478266		[learning rate: 0.0013906]
	Learning Rate: 0.00139063
	LOSS [training: 0.016172166182153 | validation: 0.021673048856301884]
	TIME [epoch: 8.18 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020730499385911146		[learning rate: 0.0013889]
		[batch 20/20] avg loss: 0.02000276190815705		[learning rate: 0.0013873]
	Learning Rate: 0.00138726
	LOSS [training: 0.020366630647034097 | validation: 0.01980012719915878]
	TIME [epoch: 8.22 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020041229030213072		[learning rate: 0.0013856]
		[batch 20/20] avg loss: 0.01032895050370235		[learning rate: 0.0013839]
	Learning Rate: 0.0013839
	LOSS [training: 0.015185089766957713 | validation: -0.0015793384145653955]
	TIME [epoch: 8.17 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027914975676953446		[learning rate: 0.0013822]
		[batch 20/20] avg loss: 0.018318336436228357		[learning rate: 0.0013806]
	Learning Rate: 0.00138055
	LOSS [training: 0.0231166560565909 | validation: 0.0012058806347522376]
	TIME [epoch: 8.16 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012432112448933552		[learning rate: 0.0013789]
		[batch 20/20] avg loss: 0.01672587169950789		[learning rate: 0.0013772]
	Learning Rate: 0.00137721
	LOSS [training: 0.014578992074220717 | validation: 0.013522769034036732]
	TIME [epoch: 8.16 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018091735727110335		[learning rate: 0.0013755]
		[batch 20/20] avg loss: 0.011279066164245666		[learning rate: 0.0013739]
	Learning Rate: 0.00137388
	LOSS [training: 0.014685400945677999 | validation: 0.006824059008544973]
	TIME [epoch: 8.19 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012288983226576582		[learning rate: 0.0013722]
		[batch 20/20] avg loss: 0.024491663441792433		[learning rate: 0.0013705]
	Learning Rate: 0.00137055
	LOSS [training: 0.018390323334184507 | validation: -0.008395400422292655]
	TIME [epoch: 8.21 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010856064342467126		[learning rate: 0.0013689]
		[batch 20/20] avg loss: 0.018827873617999903		[learning rate: 0.0013672]
	Learning Rate: 0.00136723
	LOSS [training: 0.014841968980233516 | validation: 0.002058258542256518]
	TIME [epoch: 8.18 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016566328913714935		[learning rate: 0.0013656]
		[batch 20/20] avg loss: 0.010607776633139365		[learning rate: 0.0013639]
	Learning Rate: 0.00136392
	LOSS [training: 0.01358705277342715 | validation: -0.007730782822559329]
	TIME [epoch: 8.18 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008921553962594937		[learning rate: 0.0013623]
		[batch 20/20] avg loss: 0.01844224614227763		[learning rate: 0.0013606]
	Learning Rate: 0.00136062
	LOSS [training: 0.01368190005243628 | validation: 0.013421608912412406]
	TIME [epoch: 8.22 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014452388596843918		[learning rate: 0.001359]
		[batch 20/20] avg loss: 0.01658669716982756		[learning rate: 0.0013573]
	Learning Rate: 0.00135733
	LOSS [training: 0.015519542883335744 | validation: 0.0003739975627240961]
	TIME [epoch: 8.19 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018980571630179962		[learning rate: 0.0013557]
		[batch 20/20] avg loss: 0.01892092524911953		[learning rate: 0.001354]
	Learning Rate: 0.00135404
	LOSS [training: 0.018950748439649744 | validation: 0.01283466621032588]
	TIME [epoch: 8.17 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021444311453741878		[learning rate: 0.0013524]
		[batch 20/20] avg loss: 0.03190963387717565		[learning rate: 0.0013508]
	Learning Rate: 0.00135076
	LOSS [training: 0.026676972665458758 | validation: 0.002978540696453274]
	TIME [epoch: 8.17 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014307877240523223		[learning rate: 0.0013491]
		[batch 20/20] avg loss: 0.017026329954853994		[learning rate: 0.0013475]
	Learning Rate: 0.00134749
	LOSS [training: 0.015667103597688612 | validation: 0.023749976752243888]
	TIME [epoch: 8.18 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01750737678652212		[learning rate: 0.0013459]
		[batch 20/20] avg loss: 0.012619681733925803		[learning rate: 0.0013442]
	Learning Rate: 0.00134423
	LOSS [training: 0.015063529260223963 | validation: 0.0010124270264279696]
	TIME [epoch: 8.19 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01791679611422526		[learning rate: 0.0013426]
		[batch 20/20] avg loss: 0.016179992582712886		[learning rate: 0.001341]
	Learning Rate: 0.00134098
	LOSS [training: 0.017048394348469074 | validation: 0.016062560439275377]
	TIME [epoch: 8.2 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02654956355404482		[learning rate: 0.0013394]
		[batch 20/20] avg loss: 0.012859919408631473		[learning rate: 0.0013377]
	Learning Rate: 0.00133773
	LOSS [training: 0.01970474148133815 | validation: 0.009594898316158628]
	TIME [epoch: 8.18 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0219182320257294		[learning rate: 0.0013361]
		[batch 20/20] avg loss: 0.02563241482482547		[learning rate: 0.0013345]
	Learning Rate: 0.00133449
	LOSS [training: 0.02377532342527744 | validation: 0.0005905862214855486]
	TIME [epoch: 8.2 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014041624865521438		[learning rate: 0.0013329]
		[batch 20/20] avg loss: 0.01647502351312202		[learning rate: 0.0013313]
	Learning Rate: 0.00133126
	LOSS [training: 0.015258324189321732 | validation: 0.0040776493086582995]
	TIME [epoch: 8.19 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017201182862747993		[learning rate: 0.0013296]
		[batch 20/20] avg loss: 0.012993585333600743		[learning rate: 0.001328]
	Learning Rate: 0.00132804
	LOSS [training: 0.015097384098174372 | validation: 0.007005922566951981]
	TIME [epoch: 8.19 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015019663241315585		[learning rate: 0.0013264]
		[batch 20/20] avg loss: 0.016959056383795024		[learning rate: 0.0013248]
	Learning Rate: 0.00132482
	LOSS [training: 0.015989359812555302 | validation: -0.0066230693507843335]
	TIME [epoch: 8.17 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020464199003970306		[learning rate: 0.0013232]
		[batch 20/20] avg loss: 0.012340128696132586		[learning rate: 0.0013216]
	Learning Rate: 0.00132162
	LOSS [training: 0.016402163850051436 | validation: -0.0023531092804905266]
	TIME [epoch: 8.17 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013196439186510145		[learning rate: 0.00132]
		[batch 20/20] avg loss: 0.006158188501858983		[learning rate: 0.0013184]
	Learning Rate: 0.00131842
	LOSS [training: 0.009677313844184564 | validation: -0.003982790534045811]
	TIME [epoch: 8.18 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01396595400816852		[learning rate: 0.0013168]
		[batch 20/20] avg loss: 0.015430235100998535		[learning rate: 0.0013152]
	Learning Rate: 0.00131522
	LOSS [training: 0.01469809455458353 | validation: 0.0008526764813107764]
	TIME [epoch: 8.2 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01791002419990657		[learning rate: 0.0013136]
		[batch 20/20] avg loss: 0.009333119414909295		[learning rate: 0.001312]
	Learning Rate: 0.00131204
	LOSS [training: 0.013621571807407935 | validation: 0.00956182362645527]
	TIME [epoch: 8.19 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021190691900619334		[learning rate: 0.0013105]
		[batch 20/20] avg loss: 0.01944621321167376		[learning rate: 0.0013089]
	Learning Rate: 0.00130886
	LOSS [training: 0.02031845255614655 | validation: 0.016869852336247288]
	TIME [epoch: 8.17 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020542911637674806		[learning rate: 0.0013073]
		[batch 20/20] avg loss: 0.023259673071628812		[learning rate: 0.0013057]
	Learning Rate: 0.0013057
	LOSS [training: 0.02190129235465181 | validation: 0.008567291093563642]
	TIME [epoch: 8.2 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029333447249883203		[learning rate: 0.0013041]
		[batch 20/20] avg loss: 0.029368156899853948		[learning rate: 0.0013025]
	Learning Rate: 0.00130254
	LOSS [training: 0.029350802074868582 | validation: 0.0065344798554904816]
	TIME [epoch: 8.18 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027809843941516272		[learning rate: 0.001301]
		[batch 20/20] avg loss: 0.023310715761225998		[learning rate: 0.0012994]
	Learning Rate: 0.00129938
	LOSS [training: 0.02556027985137114 | validation: 0.0028336633294022105]
	TIME [epoch: 8.19 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011598159270060134		[learning rate: 0.0012978]
		[batch 20/20] avg loss: 0.018955642392655295		[learning rate: 0.0012962]
	Learning Rate: 0.00129624
	LOSS [training: 0.015276900831357715 | validation: -0.007598330804455904]
	TIME [epoch: 8.17 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015622327814866641		[learning rate: 0.0012947]
		[batch 20/20] avg loss: 0.023637017609360433		[learning rate: 0.0012931]
	Learning Rate: 0.0012931
	LOSS [training: 0.019629672712113536 | validation: 0.009829505517417987]
	TIME [epoch: 8.19 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022132206125172506		[learning rate: 0.0012915]
		[batch 20/20] avg loss: 0.007108916374696947		[learning rate: 0.00129]
	Learning Rate: 0.00128997
	LOSS [training: 0.014620561249934726 | validation: 0.003976635039067442]
	TIME [epoch: 8.18 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030186678315641396		[learning rate: 0.0012884]
		[batch 20/20] avg loss: 0.015473859061867472		[learning rate: 0.0012868]
	Learning Rate: 0.00128685
	LOSS [training: 0.022830268688754433 | validation: -0.0009744159976378618]
	TIME [epoch: 8.2 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01781815043696987		[learning rate: 0.0012853]
		[batch 20/20] avg loss: 0.010529804367727899		[learning rate: 0.0012837]
	Learning Rate: 0.00128373
	LOSS [training: 0.014173977402348884 | validation: 0.0014490963966626972]
	TIME [epoch: 8.2 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014726029105119354		[learning rate: 0.0012822]
		[batch 20/20] avg loss: 0.028833275852747208		[learning rate: 0.0012806]
	Learning Rate: 0.00128062
	LOSS [training: 0.021779652478933282 | validation: -0.003188739448346247]
	TIME [epoch: 8.19 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01642438739749959		[learning rate: 0.0012791]
		[batch 20/20] avg loss: 0.022101472946048775		[learning rate: 0.0012775]
	Learning Rate: 0.00127752
	LOSS [training: 0.019262930171774183 | validation: -0.0032425683551636953]
	TIME [epoch: 8.17 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005491156953955723		[learning rate: 0.001276]
		[batch 20/20] avg loss: 0.025806549228247905		[learning rate: 0.0012744]
	Learning Rate: 0.00127443
	LOSS [training: 0.015648853091101814 | validation: 0.013330618131427168]
	TIME [epoch: 8.16 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02527201502135636		[learning rate: 0.0012729]
		[batch 20/20] avg loss: 0.011466271484858523		[learning rate: 0.0012713]
	Learning Rate: 0.00127134
	LOSS [training: 0.018369143253107435 | validation: 0.0037042278947653174]
	TIME [epoch: 8.19 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012697433185513033		[learning rate: 0.0012698]
		[batch 20/20] avg loss: 0.01492586382000639		[learning rate: 0.0012683]
	Learning Rate: 0.00126827
	LOSS [training: 0.01381164850275971 | validation: 0.02525129618963169]
	TIME [epoch: 8.18 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012934940810364257		[learning rate: 0.0012667]
		[batch 20/20] avg loss: 0.015972923040951156		[learning rate: 0.0012652]
	Learning Rate: 0.0012652
	LOSS [training: 0.014453931925657703 | validation: 0.0011106295624216016]
	TIME [epoch: 8.17 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015818049532728053		[learning rate: 0.0012637]
		[batch 20/20] avg loss: 0.02838514755232171		[learning rate: 0.0012621]
	Learning Rate: 0.00126213
	LOSS [training: 0.02210159854252488 | validation: 0.017589176397400097]
	TIME [epoch: 8.18 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01840492073871639		[learning rate: 0.0012606]
		[batch 20/20] avg loss: 0.010618346277415822		[learning rate: 0.0012591]
	Learning Rate: 0.00125908
	LOSS [training: 0.01451163350806611 | validation: 0.0037025087105141564]
	TIME [epoch: 8.22 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014351049428125276		[learning rate: 0.0012576]
		[batch 20/20] avg loss: 0.02972016761769792		[learning rate: 0.001256]
	Learning Rate: 0.00125603
	LOSS [training: 0.022035608522911594 | validation: 0.011257256140638336]
	TIME [epoch: 8.18 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01655462387029059		[learning rate: 0.0012545]
		[batch 20/20] avg loss: 0.012017174368554966		[learning rate: 0.001253]
	Learning Rate: 0.00125299
	LOSS [training: 0.014285899119422778 | validation: 0.003269981244339701]
	TIME [epoch: 8.17 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027308231965239384		[learning rate: 0.0012515]
		[batch 20/20] avg loss: 0.01303629218779522		[learning rate: 0.00125]
	Learning Rate: 0.00124996
	LOSS [training: 0.020172262076517306 | validation: 0.0008164569150931724]
	TIME [epoch: 8.17 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012352565073966391		[learning rate: 0.0012484]
		[batch 20/20] avg loss: 0.015754466011573612		[learning rate: 0.0012469]
	Learning Rate: 0.00124693
	LOSS [training: 0.014053515542770002 | validation: -0.007889691592627085]
	TIME [epoch: 8.17 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0228628579140261		[learning rate: 0.0012454]
		[batch 20/20] avg loss: 0.014694808713133556		[learning rate: 0.0012439]
	Learning Rate: 0.00124391
	LOSS [training: 0.018778833313579824 | validation: -0.0018022642166352566]
	TIME [epoch: 8.21 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02289153376544471		[learning rate: 0.0012424]
		[batch 20/20] avg loss: 0.016965842400756295		[learning rate: 0.0012409]
	Learning Rate: 0.0012409
	LOSS [training: 0.019928688083100506 | validation: 0.012806942391927579]
	TIME [epoch: 8.17 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013328421686463195		[learning rate: 0.0012394]
		[batch 20/20] avg loss: 0.03959403991637648		[learning rate: 0.0012379]
	Learning Rate: 0.0012379
	LOSS [training: 0.02646123080141984 | validation: -0.005699138712971221]
	TIME [epoch: 8.17 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02027876783183341		[learning rate: 0.0012364]
		[batch 20/20] avg loss: 0.022245923231726642		[learning rate: 0.0012349]
	Learning Rate: 0.0012349
	LOSS [training: 0.021262345531780028 | validation: 0.020862650167802872]
	TIME [epoch: 8.21 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01837729349501107		[learning rate: 0.0012334]
		[batch 20/20] avg loss: 0.011417402063313879		[learning rate: 0.0012319]
	Learning Rate: 0.00123191
	LOSS [training: 0.014897347779162472 | validation: -0.00025219675761546027]
	TIME [epoch: 8.2 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015412324151036338		[learning rate: 0.0012304]
		[batch 20/20] avg loss: 0.017477989748775354		[learning rate: 0.0012289]
	Learning Rate: 0.00122893
	LOSS [training: 0.016445156949905844 | validation: 0.016649439548235275]
	TIME [epoch: 8.17 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02972573380430437		[learning rate: 0.0012274]
		[batch 20/20] avg loss: 0.016618415939536495		[learning rate: 0.001226]
	Learning Rate: 0.00122595
	LOSS [training: 0.023172074871920433 | validation: -0.001957383686070729]
	TIME [epoch: 8.17 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01615742753950692		[learning rate: 0.0012245]
		[batch 20/20] avg loss: 0.0037165887815997305		[learning rate: 0.001223]
	Learning Rate: 0.00122298
	LOSS [training: 0.009937008160553326 | validation: -0.0017985314055899656]
	TIME [epoch: 8.17 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01724649682579516		[learning rate: 0.0012215]
		[batch 20/20] avg loss: 0.004271655907589231		[learning rate: 0.00122]
	Learning Rate: 0.00122002
	LOSS [training: 0.010759076366692195 | validation: -0.004665820157297394]
	TIME [epoch: 8.2 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013066391349228329		[learning rate: 0.0012185]
		[batch 20/20] avg loss: 0.011669322909647567		[learning rate: 0.0012171]
	Learning Rate: 0.00121707
	LOSS [training: 0.012367857129437948 | validation: 0.001840990362531432]
	TIME [epoch: 8.19 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014390214418447515		[learning rate: 0.0012156]
		[batch 20/20] avg loss: 0.015034984965107415		[learning rate: 0.0012141]
	Learning Rate: 0.00121412
	LOSS [training: 0.014712599691777462 | validation: -0.004673783101016614]
	TIME [epoch: 8.17 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008883042740809288		[learning rate: 0.0012127]
		[batch 20/20] avg loss: 0.015518987167717069		[learning rate: 0.0012112]
	Learning Rate: 0.00121119
	LOSS [training: 0.012201014954263182 | validation: 0.008042645419617436]
	TIME [epoch: 8.2 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02966048052243711		[learning rate: 0.0012097]
		[batch 20/20] avg loss: 0.008422288526975992		[learning rate: 0.0012083]
	Learning Rate: 0.00120825
	LOSS [training: 0.019041384524706544 | validation: 0.01567458322638102]
	TIME [epoch: 8.18 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025000506268273753		[learning rate: 0.0012068]
		[batch 20/20] avg loss: 0.011747002267670543		[learning rate: 0.0012053]
	Learning Rate: 0.00120533
	LOSS [training: 0.01837375426797215 | validation: -0.006056124168340225]
	TIME [epoch: 8.19 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011221201963008128		[learning rate: 0.0012039]
		[batch 20/20] avg loss: 0.020298813639368588		[learning rate: 0.0012024]
	Learning Rate: 0.00120241
	LOSS [training: 0.01576000780118836 | validation: -0.007659956585100812]
	TIME [epoch: 8.18 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013661789815219563		[learning rate: 0.001201]
		[batch 20/20] avg loss: 0.010431911150288346		[learning rate: 0.0011995]
	Learning Rate: 0.0011995
	LOSS [training: 0.012046850482753954 | validation: -0.005175714279336948]
	TIME [epoch: 8.18 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01982807106119911		[learning rate: 0.001198]
		[batch 20/20] avg loss: 0.012167434944410306		[learning rate: 0.0011966]
	Learning Rate: 0.0011966
	LOSS [training: 0.01599775300280471 | validation: -0.00420076002313209]
	TIME [epoch: 8.18 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01257904411649039		[learning rate: 0.0011951]
		[batch 20/20] avg loss: 0.013274253100234482		[learning rate: 0.0011937]
	Learning Rate: 0.0011937
	LOSS [training: 0.012926648608362437 | validation: 0.0225276959131831]
	TIME [epoch: 8.19 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027595283117863058		[learning rate: 0.0011923]
		[batch 20/20] avg loss: 0.015562126078991753		[learning rate: 0.0011908]
	Learning Rate: 0.00119081
	LOSS [training: 0.021578704598427402 | validation: -0.008479141793953739]
	TIME [epoch: 8.21 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0195295905524006		[learning rate: 0.0011894]
		[batch 20/20] avg loss: 0.017134960556253322		[learning rate: 0.0011879]
	Learning Rate: 0.00118793
	LOSS [training: 0.01833227555432696 | validation: -0.008582240852687514]
	TIME [epoch: 8.17 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015750620162044187		[learning rate: 0.0011865]
		[batch 20/20] avg loss: 0.014689415799971373		[learning rate: 0.0011851]
	Learning Rate: 0.00118505
	LOSS [training: 0.01522001798100778 | validation: -0.005480873053164207]
	TIME [epoch: 8.16 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009300946362530258		[learning rate: 0.0011836]
		[batch 20/20] avg loss: 0.012890223558873645		[learning rate: 0.0011822]
	Learning Rate: 0.00118218
	LOSS [training: 0.01109558496070195 | validation: 0.001655225461414999]
	TIME [epoch: 8.16 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02488848547363196		[learning rate: 0.0011807]
		[batch 20/20] avg loss: 0.02034306583969266		[learning rate: 0.0011793]
	Learning Rate: 0.00117932
	LOSS [training: 0.02261577565666231 | validation: 0.011521642316402502]
	TIME [epoch: 8.19 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020560035745814005		[learning rate: 0.0011779]
		[batch 20/20] avg loss: 0.010323920658861042		[learning rate: 0.0011765]
	Learning Rate: 0.00117646
	LOSS [training: 0.015441978202337522 | validation: -0.004348772636850228]
	TIME [epoch: 8.21 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009721262353830673		[learning rate: 0.001175]
		[batch 20/20] avg loss: 0.027815460058641283		[learning rate: 0.0011736]
	Learning Rate: 0.00117362
	LOSS [training: 0.018768361206235977 | validation: 0.025796883362891224]
	TIME [epoch: 8.19 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016664225745896433		[learning rate: 0.0011722]
		[batch 20/20] avg loss: 0.01925885074229312		[learning rate: 0.0011708]
	Learning Rate: 0.00117078
	LOSS [training: 0.017961538244094778 | validation: 0.0012192568814329832]
	TIME [epoch: 8.2 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015845697949799956		[learning rate: 0.0011694]
		[batch 20/20] avg loss: 0.01685951180686399		[learning rate: 0.0011679]
	Learning Rate: 0.00116794
	LOSS [training: 0.016352604878331973 | validation: -0.0023310068054745526]
	TIME [epoch: 8.21 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01040412295393689		[learning rate: 0.0011665]
		[batch 20/20] avg loss: 0.020949857436794027		[learning rate: 0.0011651]
	Learning Rate: 0.00116511
	LOSS [training: 0.015676990195365458 | validation: 0.004396917024263278]
	TIME [epoch: 8.18 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009033671865114721		[learning rate: 0.0011637]
		[batch 20/20] avg loss: 0.009965329561144293		[learning rate: 0.0011623]
	Learning Rate: 0.00116229
	LOSS [training: 0.009499500713129504 | validation: -0.003765307884816443]
	TIME [epoch: 8.18 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007481141103431366		[learning rate: 0.0011609]
		[batch 20/20] avg loss: 0.014399672089766506		[learning rate: 0.0011595]
	Learning Rate: 0.00115948
	LOSS [training: 0.010940406596598937 | validation: -0.006007879505582698]
	TIME [epoch: 8.19 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008497131049274817		[learning rate: 0.0011581]
		[batch 20/20] avg loss: 0.024469915839789545		[learning rate: 0.0011567]
	Learning Rate: 0.00115667
	LOSS [training: 0.016483523444532182 | validation: 0.006473606767063381]
	TIME [epoch: 8.19 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03490140695964354		[learning rate: 0.0011553]
		[batch 20/20] avg loss: 0.01400217528109143		[learning rate: 0.0011539]
	Learning Rate: 0.00115387
	LOSS [training: 0.024451791120367487 | validation: 0.0036721280679007447]
	TIME [epoch: 8.2 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018110683441362994		[learning rate: 0.0011525]
		[batch 20/20] avg loss: 0.015294488643280068		[learning rate: 0.0011511]
	Learning Rate: 0.00115108
	LOSS [training: 0.01670258604232153 | validation: 0.0017227866247693825]
	TIME [epoch: 8.19 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003961552026324446		[learning rate: 0.0011497]
		[batch 20/20] avg loss: 0.012793949072152117		[learning rate: 0.0011483]
	Learning Rate: 0.00114829
	LOSS [training: 0.008377750549238281 | validation: 0.0026062185980098716]
	TIME [epoch: 8.19 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02145547162227319		[learning rate: 0.0011469]
		[batch 20/20] avg loss: 0.014018226490206903		[learning rate: 0.0011455]
	Learning Rate: 0.00114551
	LOSS [training: 0.017736849056240046 | validation: 0.0051035890890732865]
	TIME [epoch: 8.16 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033848771254069855		[learning rate: 0.0011441]
		[batch 20/20] avg loss: 0.023086731503929096		[learning rate: 0.0011427]
	Learning Rate: 0.00114274
	LOSS [training: 0.028467751378999477 | validation: 0.02200399218461542]
	TIME [epoch: 8.18 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02491866941938859		[learning rate: 0.0011414]
		[batch 20/20] avg loss: 0.023716054991850078		[learning rate: 0.00114]
	Learning Rate: 0.00113997
	LOSS [training: 0.02431736220561933 | validation: 0.018500821406511755]
	TIME [epoch: 8.17 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04690799593861136		[learning rate: 0.0011386]
		[batch 20/20] avg loss: 0.0475393671353457		[learning rate: 0.0011372]
	Learning Rate: 0.00113721
	LOSS [training: 0.04722368153697855 | validation: 0.027571093910462436]
	TIME [epoch: 8.16 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030816453533364048		[learning rate: 0.0011358]
		[batch 20/20] avg loss: 0.04091515100499689		[learning rate: 0.0011345]
	Learning Rate: 0.00113446
	LOSS [training: 0.03586580226918047 | validation: 0.017486630506308234]
	TIME [epoch: 8.17 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02852120680461363		[learning rate: 0.0011331]
		[batch 20/20] avg loss: 0.03205391011750736		[learning rate: 0.0011317]
	Learning Rate: 0.00113171
	LOSS [training: 0.030287558461060494 | validation: 0.016268791842289413]
	TIME [epoch: 8.18 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03251287671706713		[learning rate: 0.0011303]
		[batch 20/20] avg loss: 0.024501096618249334		[learning rate: 0.001129]
	Learning Rate: 0.00112897
	LOSS [training: 0.02850698666765823 | validation: 0.007786674766318352]
	TIME [epoch: 8.2 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012296251370242936		[learning rate: 0.0011276]
		[batch 20/20] avg loss: 0.014274010117475162		[learning rate: 0.0011262]
	Learning Rate: 0.00112624
	LOSS [training: 0.013285130743859046 | validation: 0.001606082346569583]
	TIME [epoch: 8.18 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011870257061138603		[learning rate: 0.0011249]
		[batch 20/20] avg loss: 0.017283653448205073		[learning rate: 0.0011235]
	Learning Rate: 0.00112352
	LOSS [training: 0.014576955254671838 | validation: 0.014567819913129713]
	TIME [epoch: 8.17 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017581998761930992		[learning rate: 0.0011222]
		[batch 20/20] avg loss: 0.015804457059228862		[learning rate: 0.0011208]
	Learning Rate: 0.0011208
	LOSS [training: 0.01669322791057993 | validation: 0.016606333735827344]
	TIME [epoch: 8.17 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011185391660184823		[learning rate: 0.0011194]
		[batch 20/20] avg loss: 0.01671350002670252		[learning rate: 0.0011181]
	Learning Rate: 0.00111808
	LOSS [training: 0.013949445843443672 | validation: 0.000253598101340893]
	TIME [epoch: 8.22 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01751968486245249		[learning rate: 0.0011167]
		[batch 20/20] avg loss: 0.016035044933823905		[learning rate: 0.0011154]
	Learning Rate: 0.00111538
	LOSS [training: 0.0167773648981382 | validation: 0.005322226256752879]
	TIME [epoch: 8.2 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014656308092332796		[learning rate: 0.001114]
		[batch 20/20] avg loss: 0.011966355633731326		[learning rate: 0.0011127]
	Learning Rate: 0.00111268
	LOSS [training: 0.013311331863032062 | validation: 0.0036380985066416373]
	TIME [epoch: 8.16 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009077139739364096		[learning rate: 0.0011113]
		[batch 20/20] avg loss: 0.013890711473805046		[learning rate: 0.00111]
	Learning Rate: 0.00110998
	LOSS [training: 0.01148392560658457 | validation: 0.00479414650530162]
	TIME [epoch: 8.16 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012434634028978278		[learning rate: 0.0011086]
		[batch 20/20] avg loss: 0.010035292341572598		[learning rate: 0.0011073]
	Learning Rate: 0.00110729
	LOSS [training: 0.01123496318527544 | validation: -0.0015802073742003696]
	TIME [epoch: 8.18 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013443546003990971		[learning rate: 0.001106]
		[batch 20/20] avg loss: 0.01082925105375348		[learning rate: 0.0011046]
	Learning Rate: 0.00110461
	LOSS [training: 0.012136398528872225 | validation: -0.010877570570195677]
	TIME [epoch: 8.17 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012868917210801318		[learning rate: 0.0011033]
		[batch 20/20] avg loss: 0.012076437804585668		[learning rate: 0.0011019]
	Learning Rate: 0.00110194
	LOSS [training: 0.012472677507693496 | validation: 0.0005832516385172266]
	TIME [epoch: 8.16 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024205285144582502		[learning rate: 0.0011006]
		[batch 20/20] avg loss: 0.00939304693680363		[learning rate: 0.0010993]
	Learning Rate: 0.00109927
	LOSS [training: 0.016799166040693068 | validation: 3.617105409028281e-05]
	TIME [epoch: 8.21 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009204662214318973		[learning rate: 0.0010979]
		[batch 20/20] avg loss: 0.012156300778091534		[learning rate: 0.0010966]
	Learning Rate: 0.00109661
	LOSS [training: 0.010680481496205253 | validation: 0.00556341298704211]
	TIME [epoch: 8.18 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010144624585643604		[learning rate: 0.0010953]
		[batch 20/20] avg loss: 0.01815294387372027		[learning rate: 0.001094]
	Learning Rate: 0.00109396
	LOSS [training: 0.014148784229681938 | validation: 0.004577364449790162]
	TIME [epoch: 8.2 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021072685627244434		[learning rate: 0.0010926]
		[batch 20/20] avg loss: 0.01777489441948435		[learning rate: 0.0010913]
	Learning Rate: 0.00109131
	LOSS [training: 0.019423790023364394 | validation: -0.004369437904825599]
	TIME [epoch: 8.17 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01463559898461437		[learning rate: 0.00109]
		[batch 20/20] avg loss: 0.007451743219326602		[learning rate: 0.0010887]
	Learning Rate: 0.00108867
	LOSS [training: 0.011043671101970486 | validation: 0.0006256476342582156]
	TIME [epoch: 8.21 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009502066970461584		[learning rate: 0.0010873]
		[batch 20/20] avg loss: 0.009865389223743701		[learning rate: 0.001086]
	Learning Rate: 0.00108603
	LOSS [training: 0.009683728097102641 | validation: -0.005109664042021079]
	TIME [epoch: 8.16 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012932393391656504		[learning rate: 0.0010847]
		[batch 20/20] avg loss: 0.009627738104390106		[learning rate: 0.0010834]
	Learning Rate: 0.0010834
	LOSS [training: 0.011280065748023305 | validation: 0.008560259155140948]
	TIME [epoch: 8.19 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030044522586351163		[learning rate: 0.0010821]
		[batch 20/20] avg loss: 0.009939283162578976		[learning rate: 0.0010808]
	Learning Rate: 0.00108078
	LOSS [training: 0.01999190287446507 | validation: -0.0023944581498106655]
	TIME [epoch: 8.17 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007452371742268718		[learning rate: 0.0010795]
		[batch 20/20] avg loss: 0.016061947771113107		[learning rate: 0.0010782]
	Learning Rate: 0.00107816
	LOSS [training: 0.011757159756690913 | validation: -0.00670734777714844]
	TIME [epoch: 8.16 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01044605407708767		[learning rate: 0.0010769]
		[batch 20/20] avg loss: 0.007161720080706332		[learning rate: 0.0010756]
	Learning Rate: 0.00107555
	LOSS [training: 0.008803887078897 | validation: -0.009774510532974198]
	TIME [epoch: 8.16 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01197454281019934		[learning rate: 0.0010742]
		[batch 20/20] avg loss: 0.0146976174196941		[learning rate: 0.0010729]
	Learning Rate: 0.00107295
	LOSS [training: 0.013336080114946722 | validation: 0.005318720044857044]
	TIME [epoch: 8.19 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011104897509796617		[learning rate: 0.0010716]
		[batch 20/20] avg loss: 0.015367193573642723		[learning rate: 0.0010704]
	Learning Rate: 0.00107035
	LOSS [training: 0.013236045541719667 | validation: 0.003572398160069698]
	TIME [epoch: 8.2 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009603920299053464		[learning rate: 0.0010691]
		[batch 20/20] avg loss: 0.021788529013095144		[learning rate: 0.0010678]
	Learning Rate: 0.00106776
	LOSS [training: 0.015696224656074297 | validation: 0.006824959690581669]
	TIME [epoch: 8.17 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02589474324729828		[learning rate: 0.0010665]
		[batch 20/20] avg loss: 0.02639751532467552		[learning rate: 0.0010652]
	Learning Rate: 0.00106518
	LOSS [training: 0.026146129285986898 | validation: 0.0009937308482211482]
	TIME [epoch: 8.18 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011413566034542317		[learning rate: 0.0010639]
		[batch 20/20] avg loss: 0.016139663137282673		[learning rate: 0.0010626]
	Learning Rate: 0.0010626
	LOSS [training: 0.013776614585912495 | validation: 0.005236632392162055]
	TIME [epoch: 8.2 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008548959249896012		[learning rate: 0.0010613]
		[batch 20/20] avg loss: 0.013228156238842197		[learning rate: 0.00106]
	Learning Rate: 0.00106002
	LOSS [training: 0.010888557744369105 | validation: -0.0028546869702486873]
	TIME [epoch: 8.19 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008812121028176644		[learning rate: 0.0010587]
		[batch 20/20] avg loss: 0.0171347015254947		[learning rate: 0.0010575]
	Learning Rate: 0.00105746
	LOSS [training: 0.01297341127683567 | validation: 0.01566918944647024]
	TIME [epoch: 8.17 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017992546341819588		[learning rate: 0.0010562]
		[batch 20/20] avg loss: 0.015321416859852102		[learning rate: 0.0010549]
	Learning Rate: 0.0010549
	LOSS [training: 0.016656981600835844 | validation: -0.009601350081212868]
	TIME [epoch: 8.16 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01535114478817779		[learning rate: 0.0010536]
		[batch 20/20] avg loss: 0.01578760349800056		[learning rate: 0.0010523]
	Learning Rate: 0.00105234
	LOSS [training: 0.015569374143089176 | validation: 0.0017339469795766194]
	TIME [epoch: 8.17 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015549922753112183		[learning rate: 0.0010511]
		[batch 20/20] avg loss: -0.0004261076786920101		[learning rate: 0.0010498]
	Learning Rate: 0.0010498
	LOSS [training: 0.007561907537210086 | validation: 0.0013498535870328215]
	TIME [epoch: 8.2 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008335434227839039		[learning rate: 0.0010485]
		[batch 20/20] avg loss: 0.013306057430906737		[learning rate: 0.0010473]
	Learning Rate: 0.00104726
	LOSS [training: 0.010820745829372888 | validation: -0.003508302600428707]
	TIME [epoch: 8.2 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01033607705811829		[learning rate: 0.001046]
		[batch 20/20] avg loss: 0.009250646397075847		[learning rate: 0.0010447]
	Learning Rate: 0.00104472
	LOSS [training: 0.009793361727597068 | validation: -0.0016048650662109118]
	TIME [epoch: 8.18 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009242843934256863		[learning rate: 0.0010435]
		[batch 20/20] avg loss: 0.007891948870899201		[learning rate: 0.0010422]
	Learning Rate: 0.00104219
	LOSS [training: 0.008567396402578033 | validation: -0.0020210706380818103]
	TIME [epoch: 8.18 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021473238927533726		[learning rate: 0.0010409]
		[batch 20/20] avg loss: 0.01910799974117697		[learning rate: 0.0010397]
	Learning Rate: 0.00103967
	LOSS [training: 0.02029061933435535 | validation: 0.0060103824444603295]
	TIME [epoch: 8.18 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013120969153039513		[learning rate: 0.0010384]
		[batch 20/20] avg loss: 0.016635473246651076		[learning rate: 0.0010372]
	Learning Rate: 0.00103715
	LOSS [training: 0.014878221199845295 | validation: -0.0014179956175034085]
	TIME [epoch: 8.22 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012645066404903576		[learning rate: 0.0010359]
		[batch 20/20] avg loss: 0.018135165166544405		[learning rate: 0.0010346]
	Learning Rate: 0.00103464
	LOSS [training: 0.01539011578572399 | validation: 0.004530151951269126]
	TIME [epoch: 8.17 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018472800000565427		[learning rate: 0.0010334]
		[batch 20/20] avg loss: 0.013578916815460801		[learning rate: 0.0010321]
	Learning Rate: 0.00103214
	LOSS [training: 0.016025858408013117 | validation: -0.0031749880028664407]
	TIME [epoch: 8.16 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00866858667185751		[learning rate: 0.0010309]
		[batch 20/20] avg loss: 0.008784403087089184		[learning rate: 0.0010296]
	Learning Rate: 0.00102964
	LOSS [training: 0.008726494879473346 | validation: -0.004140425104130314]
	TIME [epoch: 8.17 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014434796661156258		[learning rate: 0.0010284]
		[batch 20/20] avg loss: 0.023361788833749814		[learning rate: 0.0010271]
	Learning Rate: 0.00102714
	LOSS [training: 0.018898292747453038 | validation: 0.011466741065049088]
	TIME [epoch: 8.19 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018743371998099313		[learning rate: 0.0010259]
		[batch 20/20] avg loss: 0.019217251507150118		[learning rate: 0.0010247]
	Learning Rate: 0.00102466
	LOSS [training: 0.018980311752624716 | validation: 0.001400639121217787]
	TIME [epoch: 8.2 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009949677167162376		[learning rate: 0.0010234]
		[batch 20/20] avg loss: 0.019907735706568227		[learning rate: 0.0010222]
	Learning Rate: 0.00102218
	LOSS [training: 0.014928706436865299 | validation: 0.0013180641365439965]
	TIME [epoch: 8.17 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013999149056086044		[learning rate: 0.0010209]
		[batch 20/20] avg loss: 0.009871016271929045		[learning rate: 0.0010197]
	Learning Rate: 0.0010197
	LOSS [training: 0.011935082664007546 | validation: -0.006184652745993804]
	TIME [epoch: 8.17 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011849822650915567		[learning rate: 0.0010185]
		[batch 20/20] avg loss: 0.01333648001029659		[learning rate: 0.0010172]
	Learning Rate: 0.00101723
	LOSS [training: 0.012593151330606078 | validation: 0.010131033727742456]
	TIME [epoch: 8.19 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020840013979436828		[learning rate: 0.001016]
		[batch 20/20] avg loss: 0.010933818168727844		[learning rate: 0.0010148]
	Learning Rate: 0.00101477
	LOSS [training: 0.01588691607408233 | validation: 0.0032960037556428495]
	TIME [epoch: 8.22 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01888444012782738		[learning rate: 0.0010135]
		[batch 20/20] avg loss: 0.018133996850938562		[learning rate: 0.0010123]
	Learning Rate: 0.00101232
	LOSS [training: 0.01850921848938297 | validation: 0.03225970712732058]
	TIME [epoch: 8.17 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041226108910918005		[learning rate: 0.0010111]
		[batch 20/20] avg loss: 0.013383409794560023		[learning rate: 0.0010099]
	Learning Rate: 0.00100986
	LOSS [training: 0.027304759352739016 | validation: 0.008994704342861459]
	TIME [epoch: 8.17 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017015205092977298		[learning rate: 0.0010086]
		[batch 20/20] avg loss: 0.01932632030818103		[learning rate: 0.0010074]
	Learning Rate: 0.00100742
	LOSS [training: 0.018170762700579164 | validation: 0.0034743809279111637]
	TIME [epoch: 8.17 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02383667286044457		[learning rate: 0.0010062]
		[batch 20/20] avg loss: 0.01847190542248962		[learning rate: 0.001005]
	Learning Rate: 0.00100498
	LOSS [training: 0.021154289141467098 | validation: 0.0071103557717392294]
	TIME [epoch: 8.19 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014070151229127387		[learning rate: 0.0010038]
		[batch 20/20] avg loss: 0.01828370048287731		[learning rate: 0.0010025]
	Learning Rate: 0.00100255
	LOSS [training: 0.01617692585600235 | validation: 0.008308925769621967]
	TIME [epoch: 8.19 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017816937070401693		[learning rate: 0.0010013]
		[batch 20/20] avg loss: 0.02067466347583445		[learning rate: 0.0010001]
	Learning Rate: 0.00100012
	LOSS [training: 0.019245800273118074 | validation: 0.0002591694310059467]
	TIME [epoch: 8.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009165273791096998		[learning rate: 0.00099891]
		[batch 20/20] avg loss: 0.01430988503547736		[learning rate: 0.0009977]
	Learning Rate: 0.0009977
	LOSS [training: 0.011737579413287179 | validation: -0.0024606477051832155]
	TIME [epoch: 8.18 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01478268251642117		[learning rate: 0.00099649]
		[batch 20/20] avg loss: 0.008500001372059179		[learning rate: 0.00099528]
	Learning Rate: 0.000995285
	LOSS [training: 0.011641341944240172 | validation: 0.002767446950847403]
	TIME [epoch: 8.19 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009153977745240622		[learning rate: 0.00099408]
		[batch 20/20] avg loss: 0.015812527568935447		[learning rate: 0.00099288]
	Learning Rate: 0.000992875
	LOSS [training: 0.012483252657088035 | validation: -0.0007991805576926785]
	TIME [epoch: 8.2 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01398237594517657		[learning rate: 0.00099167]
		[batch 20/20] avg loss: 0.02788113478085702		[learning rate: 0.00099047]
	Learning Rate: 0.000990472
	LOSS [training: 0.020931755363016794 | validation: 0.020000665737207147]
	TIME [epoch: 8.21 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018441113517936803		[learning rate: 0.00098927]
		[batch 20/20] avg loss: 0.008675017493300729		[learning rate: 0.00098807]
	Learning Rate: 0.000988074
	LOSS [training: 0.01355806550561876 | validation: -0.002234593022574714]
	TIME [epoch: 8.17 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008731070509637071		[learning rate: 0.00098688]
		[batch 20/20] avg loss: 0.009873545120571276		[learning rate: 0.00098568]
	Learning Rate: 0.000985682
	LOSS [training: 0.009302307815104173 | validation: 0.015720292350232853]
	TIME [epoch: 8.17 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022149040606762517		[learning rate: 0.00098449]
		[batch 20/20] avg loss: 0.0058105444479946795		[learning rate: 0.0009833]
	Learning Rate: 0.000983296
	LOSS [training: 0.0139797925273786 | validation: -0.006073919748344084]
	TIME [epoch: 8.19 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006428937185240587		[learning rate: 0.0009821]
		[batch 20/20] avg loss: 0.01676500400913839		[learning rate: 0.00098092]
	Learning Rate: 0.000980916
	LOSS [training: 0.01159697059718949 | validation: -0.002136702605599251]
	TIME [epoch: 8.17 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019222535915655578		[learning rate: 0.00097973]
		[batch 20/20] avg loss: 0.015268317129948772		[learning rate: 0.00097854]
	Learning Rate: 0.000978541
	LOSS [training: 0.017245426522802178 | validation: -0.0031752836142202256]
	TIME [epoch: 8.2 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019986264004490484		[learning rate: 0.00097736]
		[batch 20/20] avg loss: 0.01474870334422731		[learning rate: 0.00097617]
	Learning Rate: 0.000976172
	LOSS [training: 0.017367483674358895 | validation: -0.0037317631351057277]
	TIME [epoch: 8.19 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007386486116711999		[learning rate: 0.00097499]
		[batch 20/20] avg loss: 0.009498297724908488		[learning rate: 0.00097381]
	Learning Rate: 0.000973809
	LOSS [training: 0.008442391920810243 | validation: -0.003825742321175293]
	TIME [epoch: 8.2 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011897156883709303		[learning rate: 0.00097263]
		[batch 20/20] avg loss: 0.011596002683195179		[learning rate: 0.00097145]
	Learning Rate: 0.000971451
	LOSS [training: 0.011746579783452237 | validation: -0.008225763461232036]
	TIME [epoch: 8.18 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009464724446249016		[learning rate: 0.00097027]
		[batch 20/20] avg loss: 0.016828937099694852		[learning rate: 0.0009691]
	Learning Rate: 0.0009691
	LOSS [training: 0.013146830772971935 | validation: 0.0047021008103762425]
	TIME [epoch: 8.2 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018789963461636774		[learning rate: 0.00096793]
		[batch 20/20] avg loss: 0.02218086956963581		[learning rate: 0.00096675]
	Learning Rate: 0.000966754
	LOSS [training: 0.02048541651563629 | validation: -0.00677206817298388]
	TIME [epoch: 8.18 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008542709127469243		[learning rate: 0.00096558]
		[batch 20/20] avg loss: 0.008059140890282026		[learning rate: 0.00096441]
	Learning Rate: 0.000964413
	LOSS [training: 0.008300925008875634 | validation: -0.007285643941470546]
	TIME [epoch: 8.16 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015986855872732655		[learning rate: 0.00096325]
		[batch 20/20] avg loss: 0.008928822537181186		[learning rate: 0.00096208]
	Learning Rate: 0.000962079
	LOSS [training: 0.012457839204956921 | validation: -0.002681411591253748]
	TIME [epoch: 8.19 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012484350493178479		[learning rate: 0.00096091]
		[batch 20/20] avg loss: 0.009745139421456408		[learning rate: 0.00095975]
	Learning Rate: 0.00095975
	LOSS [training: 0.011114744957317443 | validation: -0.0017144503216914673]
	TIME [epoch: 8.16 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01831989174138867		[learning rate: 0.00095859]
		[batch 20/20] avg loss: 0.00632403043199947		[learning rate: 0.00095743]
	Learning Rate: 0.000957426
	LOSS [training: 0.012321961086694071 | validation: 0.002050098280802557]
	TIME [epoch: 8.17 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01864236932044542		[learning rate: 0.00095627]
		[batch 20/20] avg loss: 0.025309808330996286		[learning rate: 0.00095511]
	Learning Rate: 0.000955108
	LOSS [training: 0.02197608882572085 | validation: 0.002379440159209003]
	TIME [epoch: 8.2 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015288192213081475		[learning rate: 0.00095395]
		[batch 20/20] avg loss: 0.011504593171000674		[learning rate: 0.0009528]
	Learning Rate: 0.000952796
	LOSS [training: 0.013396392692041074 | validation: 0.0030138333629273727]
	TIME [epoch: 8.2 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014611729517577294		[learning rate: 0.00095164]
		[batch 20/20] avg loss: 0.006428421177059616		[learning rate: 0.00095049]
	Learning Rate: 0.00095049
	LOSS [training: 0.010520075347318456 | validation: -0.0009423720526020119]
	TIME [epoch: 8.18 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008136937761202655		[learning rate: 0.00094934]
		[batch 20/20] avg loss: 0.007524228759612314		[learning rate: 0.00094819]
	Learning Rate: 0.000948189
	LOSS [training: 0.007830583260407482 | validation: -0.005891941868525332]
	TIME [epoch: 8.17 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0049262436471399695		[learning rate: 0.00094704]
		[batch 20/20] avg loss: 0.009659475906712112		[learning rate: 0.00094589]
	Learning Rate: 0.000945893
	LOSS [training: 0.007292859776926041 | validation: -0.0127481184927542]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_1073.pth
	Model improved!!!
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010951957660158294		[learning rate: 0.00094475]
		[batch 20/20] avg loss: 0.010601567001331361		[learning rate: 0.0009436]
	Learning Rate: 0.000943603
	LOSS [training: 0.010776762330744826 | validation: 0.006127264417576101]
	TIME [epoch: 8.17 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0067194543081520565		[learning rate: 0.00094246]
		[batch 20/20] avg loss: 0.005705756418607831		[learning rate: 0.00094132]
	Learning Rate: 0.000941319
	LOSS [training: 0.006212605363379944 | validation: -0.001184169232718988]
	TIME [epoch: 8.19 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008750273913611627		[learning rate: 0.00094018]
		[batch 20/20] avg loss: 0.010567627357882008		[learning rate: 0.00093904]
	Learning Rate: 0.00093904
	LOSS [training: 0.009658950635746818 | validation: -0.0003071027080988075]
	TIME [epoch: 8.2 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01708383669093925		[learning rate: 0.0009379]
		[batch 20/20] avg loss: 0.010589524393495928		[learning rate: 0.00093677]
	Learning Rate: 0.000936767
	LOSS [training: 0.013836680542217589 | validation: 0.0010278429367299642]
	TIME [epoch: 8.19 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007856765991621608		[learning rate: 0.00093563]
		[batch 20/20] avg loss: 0.011321668748933778		[learning rate: 0.0009345]
	Learning Rate: 0.000934499
	LOSS [training: 0.009589217370277691 | validation: 0.004608114165365404]
	TIME [epoch: 8.18 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011917411354982138		[learning rate: 0.00093337]
		[batch 20/20] avg loss: 0.016330470821030445		[learning rate: 0.00093224]
	Learning Rate: 0.000932237
	LOSS [training: 0.014123941088006292 | validation: 0.004613639695129858]
	TIME [epoch: 8.19 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01127033706717726		[learning rate: 0.00093111]
		[batch 20/20] avg loss: 0.0043071372604044965		[learning rate: 0.00092998]
	Learning Rate: 0.00092998
	LOSS [training: 0.007788737163790878 | validation: 0.004731215069929458]
	TIME [epoch: 8.19 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006977585952831755		[learning rate: 0.00092885]
		[batch 20/20] avg loss: 0.007348823356473931		[learning rate: 0.00092773]
	Learning Rate: 0.000927729
	LOSS [training: 0.0071632046546528435 | validation: -0.0015281118967814635]
	TIME [epoch: 8.19 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015415968815351028		[learning rate: 0.00092661]
		[batch 20/20] avg loss: 0.012127620272069017		[learning rate: 0.00092548]
	Learning Rate: 0.000925483
	LOSS [training: 0.013771794543710022 | validation: 0.005044843050210462]
	TIME [epoch: 8.17 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019296956677769213		[learning rate: 0.00092436]
		[batch 20/20] avg loss: 0.012655163373013267		[learning rate: 0.00092324]
	Learning Rate: 0.000923243
	LOSS [training: 0.01597606002539124 | validation: -0.006068827212167586]
	TIME [epoch: 8.18 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014578693752205593		[learning rate: 0.00092212]
		[batch 20/20] avg loss: 0.019717049528466143		[learning rate: 0.00092101]
	Learning Rate: 0.000921008
	LOSS [training: 0.01714787164033587 | validation: -0.001661505661940955]
	TIME [epoch: 8.17 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008516494468385628		[learning rate: 0.00091989]
		[batch 20/20] avg loss: 0.008407689409047377		[learning rate: 0.00091878]
	Learning Rate: 0.000918778
	LOSS [training: 0.008462091938716502 | validation: 0.004117626867662456]
	TIME [epoch: 8.17 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010074255875019918		[learning rate: 0.00091767]
		[batch 20/20] avg loss: 0.012037575209451059		[learning rate: 0.00091655]
	Learning Rate: 0.000916554
	LOSS [training: 0.011055915542235489 | validation: -0.0015654982140781875]
	TIME [epoch: 8.2 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012736683502945854		[learning rate: 0.00091544]
		[batch 20/20] avg loss: 0.0245954089897773		[learning rate: 0.00091433]
	Learning Rate: 0.000914335
	LOSS [training: 0.018666046246361576 | validation: 0.0037546888203992728]
	TIME [epoch: 8.2 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01007993331624166		[learning rate: 0.00091323]
		[batch 20/20] avg loss: 0.013322416458385333		[learning rate: 0.00091212]
	Learning Rate: 0.000912121
	LOSS [training: 0.011701174887313497 | validation: 0.0010860811788894066]
	TIME [epoch: 8.2 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019041433229182343		[learning rate: 0.00091102]
		[batch 20/20] avg loss: 0.02978533686707161		[learning rate: 0.00090991]
	Learning Rate: 0.000909913
	LOSS [training: 0.024413385048126974 | validation: 0.010818516782133632]
	TIME [epoch: 8.18 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028225261383460126		[learning rate: 0.00090881]
		[batch 20/20] avg loss: 0.02495934568613417		[learning rate: 0.00090771]
	Learning Rate: 0.00090771
	LOSS [training: 0.026592303534797152 | validation: 0.022902401421746207]
	TIME [epoch: 8.17 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03640417833670545		[learning rate: 0.00090661]
		[batch 20/20] avg loss: 0.03131273575319727		[learning rate: 0.00090551]
	Learning Rate: 0.000905513
	LOSS [training: 0.03385845704495136 | validation: 0.016317122478011335]
	TIME [epoch: 8.2 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032849279674113135		[learning rate: 0.00090442]
		[batch 20/20] avg loss: 0.034113682836987395		[learning rate: 0.00090332]
	Learning Rate: 0.000903321
	LOSS [training: 0.03348148125555027 | validation: 0.017368936431829646]
	TIME [epoch: 8.2 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016007426752383515		[learning rate: 0.00090223]
		[batch 20/20] avg loss: 0.012413621500028112		[learning rate: 0.00090113]
	Learning Rate: 0.000901134
	LOSS [training: 0.014210524126205817 | validation: -0.0023070439525413908]
	TIME [epoch: 8.16 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013025651908786429		[learning rate: 0.00090004]
		[batch 20/20] avg loss: 0.012136574380450627		[learning rate: 0.00089895]
	Learning Rate: 0.000898953
	LOSS [training: 0.012581113144618527 | validation: -0.003833500135743571]
	TIME [epoch: 8.16 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015582759699169686		[learning rate: 0.00089786]
		[batch 20/20] avg loss: 0.015814157025362006		[learning rate: 0.00089678]
	Learning Rate: 0.000896777
	LOSS [training: 0.015698458362265844 | validation: 0.010094008163210029]
	TIME [epoch: 8.17 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02886694909141731		[learning rate: 0.00089569]
		[batch 20/20] avg loss: 0.03738737717549503		[learning rate: 0.00089461]
	Learning Rate: 0.000894605
	LOSS [training: 0.033127163133456164 | validation: 0.03533313624431015]
	TIME [epoch: 8.16 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03516838307152341		[learning rate: 0.00089352]
		[batch 20/20] avg loss: 0.030291577876603144		[learning rate: 0.00089244]
	Learning Rate: 0.00089244
	LOSS [training: 0.032729980474063274 | validation: 0.0031853133505543333]
	TIME [epoch: 8.2 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022314056362776058		[learning rate: 0.00089136]
		[batch 20/20] avg loss: 0.016854886532974074		[learning rate: 0.00089028]
	Learning Rate: 0.000890279
	LOSS [training: 0.019584471447875066 | validation: 0.0042993882399175905]
	TIME [epoch: 8.2 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013419325554101493		[learning rate: 0.0008892]
		[batch 20/20] avg loss: 0.015187848393197891		[learning rate: 0.00088812]
	Learning Rate: 0.000888124
	LOSS [training: 0.01430358697364969 | validation: 0.0071243928656030235]
	TIME [epoch: 8.2 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014708298165913495		[learning rate: 0.00088705]
		[batch 20/20] avg loss: 0.013612613907185076		[learning rate: 0.00088597]
	Learning Rate: 0.000885974
	LOSS [training: 0.014160456036549288 | validation: -0.008264663387536305]
	TIME [epoch: 8.18 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007184405482164179		[learning rate: 0.0008849]
		[batch 20/20] avg loss: 0.0036351206690579717		[learning rate: 0.00088383]
	Learning Rate: 0.000883829
	LOSS [training: 0.005409763075611076 | validation: 0.0018891810865074704]
	TIME [epoch: 8.2 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01371575647793775		[learning rate: 0.00088276]
		[batch 20/20] avg loss: 0.0015090295146858257		[learning rate: 0.00088169]
	Learning Rate: 0.00088169
	LOSS [training: 0.007612392996311786 | validation: 0.0006021550742959072]
	TIME [epoch: 8.18 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008131525125957098		[learning rate: 0.00088062]
		[batch 20/20] avg loss: 0.012851771285545658		[learning rate: 0.00087956]
	Learning Rate: 0.000879555
	LOSS [training: 0.010491648205751377 | validation: -0.003496018858896644]
	TIME [epoch: 8.2 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007607560857141889		[learning rate: 0.00087849]
		[batch 20/20] avg loss: 0.009361300814264563		[learning rate: 0.00087743]
	Learning Rate: 0.000877426
	LOSS [training: 0.008484430835703225 | validation: -0.004153210271550424]
	TIME [epoch: 8.18 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014956945323552313		[learning rate: 0.00087636]
		[batch 20/20] avg loss: 0.006811225036425783		[learning rate: 0.0008753]
	Learning Rate: 0.000875302
	LOSS [training: 0.01088408517998905 | validation: 0.002143251006806471]
	TIME [epoch: 8.18 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015131045093191828		[learning rate: 0.00087424]
		[batch 20/20] avg loss: 0.009937595144925336		[learning rate: 0.00087318]
	Learning Rate: 0.000873183
	LOSS [training: 0.012534320119058584 | validation: -0.006772194549159983]
	TIME [epoch: 8.19 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011064506712350354		[learning rate: 0.00087213]
		[batch 20/20] avg loss: 0.0030705059326861677		[learning rate: 0.00087107]
	Learning Rate: 0.000871069
	LOSS [training: 0.007067506322518261 | validation: -0.005136495720415184]
	TIME [epoch: 8.2 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01115710641751608		[learning rate: 0.00087001]
		[batch 20/20] avg loss: 0.0073824673292333595		[learning rate: 0.00086896]
	Learning Rate: 0.00086896
	LOSS [training: 0.009269786873374718 | validation: 0.0020859028538972985]
	TIME [epoch: 8.19 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01614975871424678		[learning rate: 0.00086791]
		[batch 20/20] avg loss: 0.022757288155928812		[learning rate: 0.00086686]
	Learning Rate: 0.000866857
	LOSS [training: 0.019453523435087793 | validation: 0.008843449951925599]
	TIME [epoch: 8.17 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00716705764515541		[learning rate: 0.00086581]
		[batch 20/20] avg loss: 0.01735726577305336		[learning rate: 0.00086476]
	Learning Rate: 0.000864758
	LOSS [training: 0.012262161709104387 | validation: 0.0049935804808753335]
	TIME [epoch: 8.2 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012349883597403497		[learning rate: 0.00086371]
		[batch 20/20] avg loss: 0.010671351684171158		[learning rate: 0.00086266]
	Learning Rate: 0.000862665
	LOSS [training: 0.011510617640787327 | validation: 0.0006578328040992916]
	TIME [epoch: 8.2 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01124303610217894		[learning rate: 0.00086162]
		[batch 20/20] avg loss: 0.013013845357082502		[learning rate: 0.00086058]
	Learning Rate: 0.000860577
	LOSS [training: 0.01212844072963072 | validation: 0.0008514853452171794]
	TIME [epoch: 8.18 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01022248575388879		[learning rate: 0.00085953]
		[batch 20/20] avg loss: 0.010555360724851947		[learning rate: 0.00085849]
	Learning Rate: 0.000858493
	LOSS [training: 0.010388923239370369 | validation: 0.001134525087077739]
	TIME [epoch: 8.16 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038672966514727825		[learning rate: 0.00085745]
		[batch 20/20] avg loss: 0.012998783372921657		[learning rate: 0.00085641]
	Learning Rate: 0.000856415
	LOSS [training: 0.008433040012197222 | validation: -0.004023564752988748]
	TIME [epoch: 8.18 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009744408665298705		[learning rate: 0.00085538]
		[batch 20/20] avg loss: 0.016928040708750643		[learning rate: 0.00085434]
	Learning Rate: 0.000854342
	LOSS [training: 0.013336224687024672 | validation: -0.006823553182668314]
	TIME [epoch: 8.17 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012404247638690316		[learning rate: 0.00085331]
		[batch 20/20] avg loss: 0.013615528719813544		[learning rate: 0.00085227]
	Learning Rate: 0.000852273
	LOSS [training: 0.006187551977972256 | validation: 7.313052568793548e-05]
	TIME [epoch: 8.17 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011450179425565782		[learning rate: 0.00085124]
		[batch 20/20] avg loss: 0.01216215081326317		[learning rate: 0.00085021]
	Learning Rate: 0.00085021
	LOSS [training: 0.011806165119414475 | validation: 0.0190382671950432]
	TIME [epoch: 8.16 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028756273177193103		[learning rate: 0.00084918]
		[batch 20/20] avg loss: 0.01656531809416734		[learning rate: 0.00084815]
	Learning Rate: 0.000848152
	LOSS [training: 0.02266079563568022 | validation: -0.004855291856892709]
	TIME [epoch: 8.18 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004155212785008922		[learning rate: 0.00084712]
		[batch 20/20] avg loss: 0.014303649463020424		[learning rate: 0.0008461]
	Learning Rate: 0.000846099
	LOSS [training: 0.009229431124014673 | validation: 0.015624733107306946]
	TIME [epoch: 8.23 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020848123687549538		[learning rate: 0.00084507]
		[batch 20/20] avg loss: 0.01914893160483834		[learning rate: 0.00084405]
	Learning Rate: 0.000844051
	LOSS [training: 0.01999852764619394 | validation: 0.005661815199807874]
	TIME [epoch: 8.19 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007159669762067264		[learning rate: 0.00084303]
		[batch 20/20] avg loss: 0.009730072509914195		[learning rate: 0.00084201]
	Learning Rate: 0.000842007
	LOSS [training: 0.00844487113599073 | validation: -0.003106356709974248]
	TIME [epoch: 8.17 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033166648149117782		[learning rate: 0.00084099]
		[batch 20/20] avg loss: 0.010975763324236009		[learning rate: 0.00083997]
	Learning Rate: 0.000839969
	LOSS [training: 0.007146214069573893 | validation: -0.0021714625135816905]
	TIME [epoch: 8.17 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013361654438154538		[learning rate: 0.00083895]
		[batch 20/20] avg loss: 0.014405790819042507		[learning rate: 0.00083794]
	Learning Rate: 0.000837935
	LOSS [training: 0.013883722628598522 | validation: -0.002460153890140914]
	TIME [epoch: 8.21 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006786179948087059		[learning rate: 0.00083692]
		[batch 20/20] avg loss: 0.015446493057453598		[learning rate: 0.00083591]
	Learning Rate: 0.000835907
	LOSS [training: 0.01111633650277033 | validation: 0.0035877316471565573]
	TIME [epoch: 8.2 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01352991175332915		[learning rate: 0.00083489]
		[batch 20/20] avg loss: 0.009235684071103803		[learning rate: 0.00083388]
	Learning Rate: 0.000833883
	LOSS [training: 0.011382797912216474 | validation: -0.0005441145195092443]
	TIME [epoch: 8.17 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005981408343254566		[learning rate: 0.00083287]
		[batch 20/20] avg loss: 0.00941698218146674		[learning rate: 0.00083186]
	Learning Rate: 0.000831865
	LOSS [training: 0.007699195262360651 | validation: -0.002091047723469377]
	TIME [epoch: 8.16 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004442970325996154		[learning rate: 0.00083086]
		[batch 20/20] avg loss: 0.01668707489162335		[learning rate: 0.00082985]
	Learning Rate: 0.000829851
	LOSS [training: 0.010565022608809753 | validation: -1.1695799741136964e-05]
	TIME [epoch: 8.17 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006763439419888405		[learning rate: 0.00082885]
		[batch 20/20] avg loss: 0.01057473806193004		[learning rate: 0.00082784]
	Learning Rate: 0.000827842
	LOSS [training: 0.008669088740909223 | validation: -0.007623901653681677]
	TIME [epoch: 8.19 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0052970969934366995		[learning rate: 0.00082684]
		[batch 20/20] avg loss: 0.007205699648882446		[learning rate: 0.00082584]
	Learning Rate: 0.000825838
	LOSS [training: 0.006251398321159571 | validation: 0.002219325797387733]
	TIME [epoch: 8.17 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010003983354781124		[learning rate: 0.00082484]
		[batch 20/20] avg loss: 0.012191558660995857		[learning rate: 0.00082384]
	Learning Rate: 0.000823839
	LOSS [training: 0.01109777100788849 | validation: 0.008429131198603758]
	TIME [epoch: 8.2 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019080247955695295		[learning rate: 0.00082284]
		[batch 20/20] avg loss: 0.007811824510393408		[learning rate: 0.00082184]
	Learning Rate: 0.000821844
	LOSS [training: 0.013446036233044351 | validation: -0.005124706109918706]
	TIME [epoch: 8.18 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005088600510036702		[learning rate: 0.00082085]
		[batch 20/20] avg loss: 0.00795424933696831		[learning rate: 0.00081985]
	Learning Rate: 0.000819855
	LOSS [training: 0.006521424923502507 | validation: -0.009828847835726785]
	TIME [epoch: 8.19 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013928007985961239		[learning rate: 0.00081886]
		[batch 20/20] avg loss: 0.009763755791387527		[learning rate: 0.00081787]
	Learning Rate: 0.00081787
	LOSS [training: 0.011845881888674383 | validation: -0.005507325933532694]
	TIME [epoch: 8.18 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008490564318252004		[learning rate: 0.00081688]
		[batch 20/20] avg loss: 0.00938823963553001		[learning rate: 0.00081589]
	Learning Rate: 0.00081589
	LOSS [training: 0.008939401976891007 | validation: -0.003666796939239975]
	TIME [epoch: 8.2 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010065944555881988		[learning rate: 0.0008149]
		[batch 20/20] avg loss: 0.009020151906004734		[learning rate: 0.00081391]
	Learning Rate: 0.000813915
	LOSS [training: 0.009543048230943358 | validation: -0.0040872269884683465]
	TIME [epoch: 8.17 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005187623542613462		[learning rate: 0.00081293]
		[batch 20/20] avg loss: 0.01337824974156591		[learning rate: 0.00081194]
	Learning Rate: 0.000811944
	LOSS [training: 0.009282936642089689 | validation: -0.0012627493971475434]
	TIME [epoch: 8.18 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024896205744298684		[learning rate: 0.00081096]
		[batch 20/20] avg loss: 0.01140315831651659		[learning rate: 0.00080998]
	Learning Rate: 0.000809979
	LOSS [training: 0.006946389445473229 | validation: 0.0014909632467176312]
	TIME [epoch: 8.19 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013736780295586695		[learning rate: 0.000809]
		[batch 20/20] avg loss: 0.006301553840792717		[learning rate: 0.00080802]
	Learning Rate: 0.000808018
	LOSS [training: 0.0038376159351756938 | validation: -0.004505441401241933]
	TIME [epoch: 8.17 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018539289211913626		[learning rate: 0.00080704]
		[batch 20/20] avg loss: 0.007846132849672009		[learning rate: 0.00080606]
	Learning Rate: 0.000806062
	LOSS [training: 0.013192711030792817 | validation: -0.0031401606670202493]
	TIME [epoch: 8.17 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004950430799894438		[learning rate: 0.00080509]
		[batch 20/20] avg loss: 0.007411621188597768		[learning rate: 0.00080411]
	Learning Rate: 0.000804111
	LOSS [training: 0.006181025994246102 | validation: -0.0010461073464916603]
	TIME [epoch: 8.19 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020960273410590823		[learning rate: 0.00080314]
		[batch 20/20] avg loss: 0.0057556664514280395		[learning rate: 0.00080216]
	Learning Rate: 0.000802164
	LOSS [training: 0.013357969931009434 | validation: 0.005675510065838]
	TIME [epoch: 8.21 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005276323341080729		[learning rate: 0.00080119]
		[batch 20/20] avg loss: 0.012034570631212976		[learning rate: 0.00080022]
	Learning Rate: 0.000800222
	LOSS [training: 0.008655446986146852 | validation: 0.001826526084451271]
	TIME [epoch: 8.18 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010339328986712568		[learning rate: 0.00079925]
		[batch 20/20] avg loss: 0.019053143568909314		[learning rate: 0.00079828]
	Learning Rate: 0.000798285
	LOSS [training: 0.014696236277810942 | validation: 0.00668889620227483]
	TIME [epoch: 8.17 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02123223149258571		[learning rate: 0.00079732]
		[batch 20/20] avg loss: 0.012377408929357708		[learning rate: 0.00079635]
	Learning Rate: 0.000796352
	LOSS [training: 0.01680482021097171 | validation: 0.0037224086439151876]
	TIME [epoch: 8.19 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008047138190593234		[learning rate: 0.00079539]
		[batch 20/20] avg loss: 0.006972265457990899		[learning rate: 0.00079442]
	Learning Rate: 0.000794424
	LOSS [training: 0.0075097018242920675 | validation: -0.00983958635490017]
	TIME [epoch: 8.21 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010541560758666881		[learning rate: 0.00079346]
		[batch 20/20] avg loss: 0.013191399857937386		[learning rate: 0.0007925]
	Learning Rate: 0.000792501
	LOSS [training: 0.011866480308302134 | validation: 0.00365530539393669]
	TIME [epoch: 8.17 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015611367537108525		[learning rate: 0.00079154]
		[batch 20/20] avg loss: 0.017112035343259734		[learning rate: 0.00079058]
	Learning Rate: 0.000790583
	LOSS [training: 0.016361701440184128 | validation: -0.003738462512380734]
	TIME [epoch: 8.16 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011477945834875821		[learning rate: 0.00078963]
		[batch 20/20] avg loss: 0.005998555091406562		[learning rate: 0.00078867]
	Learning Rate: 0.000788669
	LOSS [training: 0.008738250463141193 | validation: 0.0007408886834082856]
	TIME [epoch: 8.16 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004185519460965159		[learning rate: 0.00078771]
		[batch 20/20] avg loss: 0.010192252876461007		[learning rate: 0.00078676]
	Learning Rate: 0.00078676
	LOSS [training: 0.007188886168713085 | validation: -0.0015104441291210742]
	TIME [epoch: 8.16 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019170615673437118		[learning rate: 0.00078581]
		[batch 20/20] avg loss: 0.014194165212605175		[learning rate: 0.00078486]
	Learning Rate: 0.000784855
	LOSS [training: 0.008055613389974443 | validation: -0.0016640501405435066]
	TIME [epoch: 8.19 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018540355478141125		[learning rate: 0.0007839]
		[batch 20/20] avg loss: 0.009598994488617865		[learning rate: 0.00078296]
	Learning Rate: 0.000782955
	LOSS [training: 0.014069674983379497 | validation: 0.002187194281514179]
	TIME [epoch: 8.16 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012699493496864538		[learning rate: 0.00078201]
		[batch 20/20] avg loss: 0.006115634311528404		[learning rate: 0.00078106]
	Learning Rate: 0.00078106
	LOSS [training: 0.009407563904196468 | validation: -0.001741529638850037]
	TIME [epoch: 8.19 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004544471968892934		[learning rate: 0.00078011]
		[batch 20/20] avg loss: 0.00672100957778793		[learning rate: 0.00077917]
	Learning Rate: 0.000779169
	LOSS [training: 0.005632740773340432 | validation: -0.0022062576115488438]
	TIME [epoch: 8.17 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003317146808888783		[learning rate: 0.00077823]
		[batch 20/20] avg loss: 0.004795907039069106		[learning rate: 0.00077728]
	Learning Rate: 0.000777283
	LOSS [training: 0.004056526923978945 | validation: 0.0050935683163542]
	TIME [epoch: 8.19 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007304474459935889		[learning rate: 0.00077634]
		[batch 20/20] avg loss: 0.009139941228847145		[learning rate: 0.0007754]
	Learning Rate: 0.000775401
	LOSS [training: 0.008222207844391517 | validation: -0.00047705769631881486]
	TIME [epoch: 8.18 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008444775279519853		[learning rate: 0.00077446]
		[batch 20/20] avg loss: 0.010317032324840452		[learning rate: 0.00077352]
	Learning Rate: 0.000773524
	LOSS [training: 0.009380903802180153 | validation: -0.007378722294493236]
	TIME [epoch: 8.2 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009023337959999699		[learning rate: 0.00077259]
		[batch 20/20] avg loss: 0.01975862334586852		[learning rate: 0.00077165]
	Learning Rate: 0.000771651
	LOSS [training: 0.014390980652934107 | validation: 0.001873332461592456]
	TIME [epoch: 8.17 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012160497887952107		[learning rate: 0.00077072]
		[batch 20/20] avg loss: 0.015588362450646301		[learning rate: 0.00076978]
	Learning Rate: 0.000769783
	LOSS [training: 0.013874430169299203 | validation: 0.004138201471042084]
	TIME [epoch: 8.16 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014587713745631451		[learning rate: 0.00076885]
		[batch 20/20] avg loss: 0.01400796982029458		[learning rate: 0.00076792]
	Learning Rate: 0.00076792
	LOSS [training: 0.014297841782963016 | validation: 0.004737472665012003]
	TIME [epoch: 8.19 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008881366889233059		[learning rate: 0.00076699]
		[batch 20/20] avg loss: 0.012805478251761426		[learning rate: 0.00076606]
	Learning Rate: 0.000766061
	LOSS [training: 0.010843422570497242 | validation: -0.009762774470744017]
	TIME [epoch: 8.17 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008931990337500856		[learning rate: 0.00076513]
		[batch 20/20] avg loss: 0.012219350354566727		[learning rate: 0.00076421]
	Learning Rate: 0.000764206
	LOSS [training: 0.01057567034603379 | validation: -0.0011991270009598272]
	TIME [epoch: 8.17 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016074469335681588		[learning rate: 0.00076328]
		[batch 20/20] avg loss: 0.007940643904485343		[learning rate: 0.00076236]
	Learning Rate: 0.000762356
	LOSS [training: 0.012007556620083465 | validation: -0.0010603295620638664]
	TIME [epoch: 8.17 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010215329409164765		[learning rate: 0.00076143]
		[batch 20/20] avg loss: 0.013385875672032452		[learning rate: 0.00076051]
	Learning Rate: 0.000760511
	LOSS [training: 0.011800602540598611 | validation: 0.002192772888489474]
	TIME [epoch: 8.19 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011175946258778803		[learning rate: 0.00075959]
		[batch 20/20] avg loss: 0.009434058498419759		[learning rate: 0.00075867]
	Learning Rate: 0.00075867
	LOSS [training: 0.010305002378599281 | validation: -0.0009040207670504502]
	TIME [epoch: 8.19 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006659203409809		[learning rate: 0.00075775]
		[batch 20/20] avg loss: 0.01163066284900856		[learning rate: 0.00075683]
	Learning Rate: 0.000756833
	LOSS [training: 0.00914493312940878 | validation: 0.00012727808867284314]
	TIME [epoch: 8.19 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007038900352327074		[learning rate: 0.00075592]
		[batch 20/20] avg loss: 0.01636776759084829		[learning rate: 0.000755]
	Learning Rate: 0.000755001
	LOSS [training: 0.011703333971587682 | validation: 0.01779764161729565]
	TIME [epoch: 8.17 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025919615070150566		[learning rate: 0.00075409]
		[batch 20/20] avg loss: 0.017659818469496647		[learning rate: 0.00075317]
	Learning Rate: 0.000753173
	LOSS [training: 0.021789716769823605 | validation: 0.019720291690491387]
	TIME [epoch: 8.18 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016195794191868264		[learning rate: 0.00075226]
		[batch 20/20] avg loss: 0.016342143082192638		[learning rate: 0.00075135]
	Learning Rate: 0.00075135
	LOSS [training: 0.01626896863703045 | validation: 0.003717816974915371]
	TIME [epoch: 8.21 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014104691402948599		[learning rate: 0.00075044]
		[batch 20/20] avg loss: 0.013409070014942922		[learning rate: 0.00074953]
	Learning Rate: 0.000749531
	LOSS [training: 0.013756880708945762 | validation: 0.011473507306864188]
	TIME [epoch: 8.19 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015269417701070603		[learning rate: 0.00074862]
		[batch 20/20] avg loss: 0.012888868491186195		[learning rate: 0.00074772]
	Learning Rate: 0.000747716
	LOSS [training: 0.0140791430961284 | validation: 0.012787039092271585]
	TIME [epoch: 8.16 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022917523858074336		[learning rate: 0.00074681]
		[batch 20/20] avg loss: 0.008702127243078506		[learning rate: 0.00074591]
	Learning Rate: 0.000745906
	LOSS [training: 0.01580982555057642 | validation: 0.0015410978203584823]
	TIME [epoch: 8.17 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007385481489862898		[learning rate: 0.000745]
		[batch 20/20] avg loss: 0.009044992745713338		[learning rate: 0.0007441]
	Learning Rate: 0.0007441
	LOSS [training: 0.008215237117788118 | validation: 0.009960802410919308]
	TIME [epoch: 8.19 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01140998869418694		[learning rate: 0.0007432]
		[batch 20/20] avg loss: 0.0031704635036423857		[learning rate: 0.0007423]
	Learning Rate: 0.000742299
	LOSS [training: 0.007290226098914662 | validation: 0.0015571563413843215]
	TIME [epoch: 8.17 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016661947634471083		[learning rate: 0.0007414]
		[batch 20/20] avg loss: 0.0070914122749539486		[learning rate: 0.0007405]
	Learning Rate: 0.000740502
	LOSS [training: 0.011876679954712515 | validation: -0.0036354761248249515]
	TIME [epoch: 8.17 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007093156346270481		[learning rate: 0.00073961]
		[batch 20/20] avg loss: 0.013446940037836346		[learning rate: 0.00073871]
	Learning Rate: 0.000738709
	LOSS [training: 0.010270048192053415 | validation: 0.00392700897142941]
	TIME [epoch: 8.17 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017234264550937604		[learning rate: 0.00073781]
		[batch 20/20] avg loss: 0.011030020705350998		[learning rate: 0.00073692]
	Learning Rate: 0.000736921
	LOSS [training: 0.0141321426281443 | validation: 0.0020955101765369783]
	TIME [epoch: 8.19 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017587607802390832		[learning rate: 0.00073603]
		[batch 20/20] avg loss: 0.029769920726201455		[learning rate: 0.00073514]
	Learning Rate: 0.000735137
	LOSS [training: 0.023678764264296142 | validation: 0.009201289258777742]
	TIME [epoch: 8.17 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021880708763959745		[learning rate: 0.00073425]
		[batch 20/20] avg loss: 0.02500788402232985		[learning rate: 0.00073336]
	Learning Rate: 0.000733358
	LOSS [training: 0.023444296393144798 | validation: 0.0027016732239063413]
	TIME [epoch: 8.17 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025371094971352974		[learning rate: 0.00073247]
		[batch 20/20] avg loss: 0.013042690129100592		[learning rate: 0.00073158]
	Learning Rate: 0.000731582
	LOSS [training: 0.019206892550226784 | validation: 0.006878412060600081]
	TIME [epoch: 8.17 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016410292446495063		[learning rate: 0.0007307]
		[batch 20/20] avg loss: 0.014584783542369306		[learning rate: 0.00072981]
	Learning Rate: 0.000729811
	LOSS [training: 0.015497537994432185 | validation: -0.0001416537128669309]
	TIME [epoch: 8.16 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017254102753000718		[learning rate: 0.00072893]
		[batch 20/20] avg loss: 0.013427160686343562		[learning rate: 0.00072804]
	Learning Rate: 0.000728044
	LOSS [training: 0.01534063171967214 | validation: 2.8263924884629546e-05]
	TIME [epoch: 8.2 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017614179256348683		[learning rate: 0.00072716]
		[batch 20/20] avg loss: 0.010365084340403466		[learning rate: 0.00072628]
	Learning Rate: 0.000726282
	LOSS [training: 0.013989631798376076 | validation: -0.0026124381403195536]
	TIME [epoch: 8.16 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01684583479807596		[learning rate: 0.0007254]
		[batch 20/20] avg loss: 0.020049389615590746		[learning rate: 0.00072452]
	Learning Rate: 0.000724524
	LOSS [training: 0.01844761220683335 | validation: -0.0006523082053500521]
	TIME [epoch: 8.17 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010790329781422938		[learning rate: 0.00072365]
		[batch 20/20] avg loss: 0.016003395425817985		[learning rate: 0.00072277]
	Learning Rate: 0.00072277
	LOSS [training: 0.01339686260362046 | validation: 0.012659722128728181]
	TIME [epoch: 8.2 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01602897316796658		[learning rate: 0.00072189]
		[batch 20/20] avg loss: 0.013169277644996674		[learning rate: 0.00072102]
	Learning Rate: 0.00072102
	LOSS [training: 0.014599125406481625 | validation: 0.0028460809255454896]
	TIME [epoch: 8.2 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012444172300226233		[learning rate: 0.00072015]
		[batch 20/20] avg loss: 0.004858516529915441		[learning rate: 0.00071927]
	Learning Rate: 0.000719275
	LOSS [training: 0.008651344415070835 | validation: -0.003768292971314718]
	TIME [epoch: 8.17 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020901508996500198		[learning rate: 0.0007184]
		[batch 20/20] avg loss: 0.007322586087752529		[learning rate: 0.00071753]
	Learning Rate: 0.000717533
	LOSS [training: 0.014112047542126364 | validation: -0.011316381450393053]
	TIME [epoch: 8.17 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025414013217697025		[learning rate: 0.00071666]
		[batch 20/20] avg loss: 0.014699839361832561		[learning rate: 0.0007158]
	Learning Rate: 0.000715796
	LOSS [training: 0.008620620341801133 | validation: 0.00020045233370951103]
	TIME [epoch: 8.21 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016772616737431533		[learning rate: 0.00071493]
		[batch 20/20] avg loss: 0.01127686068253905		[learning rate: 0.00071406]
	Learning Rate: 0.000714064
	LOSS [training: 0.014024738709985291 | validation: 0.008224714232899883]
	TIME [epoch: 8.17 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015568062270090107		[learning rate: 0.0007132]
		[batch 20/20] avg loss: 0.019345032745973927		[learning rate: 0.00071233]
	Learning Rate: 0.000712335
	LOSS [training: 0.017456547508032015 | validation: 0.0017152998348800683]
	TIME [epoch: 8.19 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005858770124022486		[learning rate: 0.00071147]
		[batch 20/20] avg loss: 0.008964674822962301		[learning rate: 0.00071061]
	Learning Rate: 0.00071061
	LOSS [training: 0.007411722473492392 | validation: -0.0035643598632023204]
	TIME [epoch: 8.16 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011795785040990148		[learning rate: 0.00070975]
		[batch 20/20] avg loss: 0.006453403744430748		[learning rate: 0.00070889]
	Learning Rate: 0.00070889
	LOSS [training: 0.009124594392710447 | validation: -0.009320293666431324]
	TIME [epoch: 8.17 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004670258148106209		[learning rate: 0.00070803]
		[batch 20/20] avg loss: 0.022057457480289137		[learning rate: 0.00070717]
	Learning Rate: 0.000707174
	LOSS [training: 0.013363857814197671 | validation: 0.027727585815431038]
	TIME [epoch: 8.16 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012674493738313869		[learning rate: 0.00070632]
		[batch 20/20] avg loss: 0.013578300586627517		[learning rate: 0.00070546]
	Learning Rate: 0.000705462
	LOSS [training: 0.013126397162470691 | validation: -0.003770668261221242]
	TIME [epoch: 8.19 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002519941012855111		[learning rate: 0.00070461]
		[batch 20/20] avg loss: 0.01281563849205324		[learning rate: 0.00070375]
	Learning Rate: 0.000703754
	LOSS [training: 0.006533816296669373 | validation: -0.003830283130452131]
	TIME [epoch: 8.16 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005425499435176535		[learning rate: 0.0007029]
		[batch 20/20] avg loss: 0.009957078609689876		[learning rate: 0.00070205]
	Learning Rate: 0.000702051
	LOSS [training: 0.007691289022433203 | validation: -0.005197620057352903]
	TIME [epoch: 8.19 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004019348928980237		[learning rate: 0.0007012]
		[batch 20/20] avg loss: 0.01185631187713693		[learning rate: 0.00070035]
	Learning Rate: 0.000700351
	LOSS [training: 0.007937830403058583 | validation: -0.0005762761323191649]
	TIME [epoch: 8.17 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032687775033661596		[learning rate: 0.0006995]
		[batch 20/20] avg loss: 0.0062370889543773116		[learning rate: 0.00069866]
	Learning Rate: 0.000698656
	LOSS [training: 0.004752933228871737 | validation: -0.003515518547984303]
	TIME [epoch: 8.19 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007826633352228479		[learning rate: 0.00069781]
		[batch 20/20] avg loss: 0.0008836934313955529		[learning rate: 0.00069696]
	Learning Rate: 0.000696964
	LOSS [training: 0.004355163391812015 | validation: -0.00979986719229334]
	TIME [epoch: 8.18 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011111628104601305		[learning rate: 0.00069612]
		[batch 20/20] avg loss: 0.012745482935611823		[learning rate: 0.00069528]
	Learning Rate: 0.000695277
	LOSS [training: 0.011928555520106562 | validation: -0.007459645110454575]
	TIME [epoch: 8.2 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010426071690049336		[learning rate: 0.00069444]
		[batch 20/20] avg loss: 0.02401321345504012		[learning rate: 0.00069359]
	Learning Rate: 0.000693594
	LOSS [training: 0.017219642572544727 | validation: 0.006388429813334579]
	TIME [epoch: 8.16 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009716705974848597		[learning rate: 0.00069275]
		[batch 20/20] avg loss: 0.021259966554520584		[learning rate: 0.00069191]
	Learning Rate: 0.000691915
	LOSS [training: 0.015488336264684585 | validation: 0.016775116789046817]
	TIME [epoch: 8.16 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015447473966597852		[learning rate: 0.00069108]
		[batch 20/20] avg loss: 0.004079016944043318		[learning rate: 0.00069024]
	Learning Rate: 0.00069024
	LOSS [training: 0.009763245455320585 | validation: -0.004709760990502604]
	TIME [epoch: 8.18 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006414478008738722		[learning rate: 0.0006894]
		[batch 20/20] avg loss: 0.010512726587460336		[learning rate: 0.00068857]
	Learning Rate: 0.000688569
	LOSS [training: 0.00846360229809953 | validation: -0.010986979054529104]
	TIME [epoch: 8.16 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00332565024468164		[learning rate: 0.00068773]
		[batch 20/20] avg loss: 0.0031347989888281685		[learning rate: 0.0006869]
	Learning Rate: 0.000686902
	LOSS [training: 0.0032302246167549035 | validation: -0.008716825138124953]
	TIME [epoch: 8.16 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007096395154758793		[learning rate: 0.00068607]
		[batch 20/20] avg loss: 0.003298788438494062		[learning rate: 0.00068524]
	Learning Rate: 0.000685239
	LOSS [training: 0.0012945744615090915 | validation: -0.0032954576655213025]
	TIME [epoch: 8.16 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004487376080525747		[learning rate: 0.00068441]
		[batch 20/20] avg loss: 0.008457038455935744		[learning rate: 0.00068358]
	Learning Rate: 0.00068358
	LOSS [training: 0.006472207268230746 | validation: -0.004105788522337081]
	TIME [epoch: 8.21 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006403861147694629		[learning rate: 0.00068275]
		[batch 20/20] avg loss: 0.01135653403134962		[learning rate: 0.00068193]
	Learning Rate: 0.000681925
	LOSS [training: 0.008880197589522127 | validation: -0.005814228829103575]
	TIME [epoch: 8.17 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00493870674422932		[learning rate: 0.0006811]
		[batch 20/20] avg loss: 0.007266425661775803		[learning rate: 0.00068027]
	Learning Rate: 0.000680275
	LOSS [training: 0.006102566203002561 | validation: -0.009326567911640188]
	TIME [epoch: 8.17 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008074196216958908		[learning rate: 0.00067945]
		[batch 20/20] avg loss: 0.011291074839428487		[learning rate: 0.00067863]
	Learning Rate: 0.000678628
	LOSS [training: 0.006049247230562187 | validation: -0.0102609648173098]
	TIME [epoch: 8.19 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034335909732958825		[learning rate: 0.00067781]
		[batch 20/20] avg loss: 0.008758253802462868		[learning rate: 0.00067698]
	Learning Rate: 0.000676985
	LOSS [training: 0.0060959223878793755 | validation: -0.015957542472166165]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_1211.pth
	Model improved!!!
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004769875152100275		[learning rate: 0.00067616]
		[batch 20/20] avg loss: 0.010328917107707714		[learning rate: 0.00067535]
	Learning Rate: 0.000675346
	LOSS [training: 0.0075493961299039946 | validation: 0.007349966961498266]
	TIME [epoch: 8.19 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01994726255082183		[learning rate: 0.00067453]
		[batch 20/20] avg loss: 0.012978377471764538		[learning rate: 0.00067371]
	Learning Rate: 0.000673711
	LOSS [training: 0.016462820011293185 | validation: -0.0004391433529367935]
	TIME [epoch: 8.15 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012751099233044641		[learning rate: 0.0006729]
		[batch 20/20] avg loss: 0.001896939538136844		[learning rate: 0.00067208]
	Learning Rate: 0.00067208
	LOSS [training: 0.007324019385590743 | validation: -0.005338023823747049]
	TIME [epoch: 8.16 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007911218098092492		[learning rate: 0.00067127]
		[batch 20/20] avg loss: 0.009556427888903698		[learning rate: 0.00067045]
	Learning Rate: 0.000670453
	LOSS [training: 0.008733822993498093 | validation: -0.005778009758319314]
	TIME [epoch: 8.16 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00814240437781497		[learning rate: 0.00066964]
		[batch 20/20] avg loss: 0.004856525469867645		[learning rate: 0.00066883]
	Learning Rate: 0.00066883
	LOSS [training: 0.006499464923841308 | validation: 0.0019883027747232187]
	TIME [epoch: 8.18 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014312371922183648		[learning rate: 0.00066802]
		[batch 20/20] avg loss: 0.0077917183563631365		[learning rate: 0.00066721]
	Learning Rate: 0.000667211
	LOSS [training: 0.011052045139273393 | validation: -0.006717956235144709]
	TIME [epoch: 8.16 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031743789899036797		[learning rate: 0.0006664]
		[batch 20/20] avg loss: 0.013897991853217755		[learning rate: 0.0006656]
	Learning Rate: 0.000665596
	LOSS [training: 0.008536185421560719 | validation: -0.0028426064182034373]
	TIME [epoch: 8.18 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016384147537409792		[learning rate: 0.00066479]
		[batch 20/20] avg loss: 0.006968783241393414		[learning rate: 0.00066398]
	Learning Rate: 0.000663984
	LOSS [training: 0.011676465389401602 | validation: 0.008310775317191177]
	TIME [epoch: 8.17 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012854248965963492		[learning rate: 0.00066318]
		[batch 20/20] avg loss: 0.005564997175496833		[learning rate: 0.00066238]
	Learning Rate: 0.000662377
	LOSS [training: 0.00920962307073016 | validation: 0.001894443249332094]
	TIME [epoch: 8.18 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010945403490379991		[learning rate: 0.00066157]
		[batch 20/20] avg loss: 0.01873376426771268		[learning rate: 0.00066077]
	Learning Rate: 0.000660773
	LOSS [training: 0.014839583879046334 | validation: 0.018497730233311326]
	TIME [epoch: 8.18 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007420827081485097		[learning rate: 0.00065997]
		[batch 20/20] avg loss: 0.007979640334387956		[learning rate: 0.00065917]
	Learning Rate: 0.000659174
	LOSS [training: 0.007700233707936527 | validation: -0.0023439532160661244]
	TIME [epoch: 8.19 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011186821275774481		[learning rate: 0.00065838]
		[batch 20/20] avg loss: 0.00922198430598441		[learning rate: 0.00065758]
	Learning Rate: 0.000657578
	LOSS [training: 0.010204402790879443 | validation: -0.0023032403661570194]
	TIME [epoch: 8.16 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012048929513487404		[learning rate: 0.00065678]
		[batch 20/20] avg loss: 0.010351435641229898		[learning rate: 0.00065599]
	Learning Rate: 0.000655986
	LOSS [training: 0.011200182577358652 | validation: -0.0037873139769288786]
	TIME [epoch: 8.16 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007297064098415532		[learning rate: 0.00065519]
		[batch 20/20] avg loss: 0.01175673903753983		[learning rate: 0.0006544]
	Learning Rate: 0.000654398
	LOSS [training: 0.00952690156797768 | validation: -0.007509320462847266]
	TIME [epoch: 8.18 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016146583264064769		[learning rate: 0.00065361]
		[batch 20/20] avg loss: 0.010671500586446272		[learning rate: 0.00065281]
	Learning Rate: 0.000652814
	LOSS [training: 0.006143079456426374 | validation: -0.009052872428212727]
	TIME [epoch: 8.16 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007972257233077179		[learning rate: 0.00065202]
		[batch 20/20] avg loss: 0.012380693655085585		[learning rate: 0.00065123]
	Learning Rate: 0.000651234
	LOSS [training: 0.010176475444081382 | validation: -0.006161768264363399]
	TIME [epoch: 8.17 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010260542350358514		[learning rate: 0.00065045]
		[batch 20/20] avg loss: 0.005772482050343908		[learning rate: 0.00064966]
	Learning Rate: 0.000649657
	LOSS [training: 0.00801651220035121 | validation: -0.0047748443659472875]
	TIME [epoch: 8.18 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0065728275323815285		[learning rate: 0.00064887]
		[batch 20/20] avg loss: 0.004018549233781847		[learning rate: 0.00064808]
	Learning Rate: 0.000648084
	LOSS [training: 0.0052956883830816886 | validation: -0.003396142705236764]
	TIME [epoch: 8.18 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0103429943462676		[learning rate: 0.0006473]
		[batch 20/20] avg loss: 0.0059641934427326114		[learning rate: 0.00064652]
	Learning Rate: 0.000646515
	LOSS [training: 0.008153593894500107 | validation: -0.00888582623162757]
	TIME [epoch: 8.17 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007685488838615017		[learning rate: 0.00064573]
		[batch 20/20] avg loss: 0.005310334113728503		[learning rate: 0.00064495]
	Learning Rate: 0.00064495
	LOSS [training: 0.006497911476171759 | validation: -0.004333133351138691]
	TIME [epoch: 8.19 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008982344587840843		[learning rate: 0.00064417]
		[batch 20/20] avg loss: 0.005059616666395771		[learning rate: 0.00064339]
	Learning Rate: 0.000643389
	LOSS [training: 0.007020980627118307 | validation: -0.008893763820011901]
	TIME [epoch: 8.17 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010729536806520607		[learning rate: 0.00064261]
		[batch 20/20] avg loss: 0.0073408161978069835		[learning rate: 0.00064183]
	Learning Rate: 0.000641832
	LOSS [training: 0.009035176502163796 | validation: -0.013437343653113906]
	TIME [epoch: 8.15 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003006913441546338		[learning rate: 0.00064105]
		[batch 20/20] avg loss: 0.00846836657366691		[learning rate: 0.00064028]
	Learning Rate: 0.000640278
	LOSS [training: 0.005737640007606624 | validation: -0.005708468246160449]
	TIME [epoch: 8.18 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008442353324802528		[learning rate: 0.0006395]
		[batch 20/20] avg loss: 0.007471621998113393		[learning rate: 0.00063873]
	Learning Rate: 0.000638728
	LOSS [training: 0.00795698766145796 | validation: -0.0039017196871034957]
	TIME [epoch: 8.16 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007537821543760201		[learning rate: 0.00063795]
		[batch 20/20] avg loss: 0.005071831187022747		[learning rate: 0.00063718]
	Learning Rate: 0.000637182
	LOSS [training: 0.0063048263653914735 | validation: -0.008686221852833595]
	TIME [epoch: 8.16 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005205410169475193		[learning rate: 0.00063641]
		[batch 20/20] avg loss: 0.005818332574654663		[learning rate: 0.00063564]
	Learning Rate: 0.000635639
	LOSS [training: 0.005511871372064929 | validation: -0.005165088693956895]
	TIME [epoch: 8.16 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014230168098968418		[learning rate: 0.00063487]
		[batch 20/20] avg loss: 0.003443803153255553		[learning rate: 0.0006341]
	Learning Rate: 0.0006341
	LOSS [training: 0.008836985626111984 | validation: -0.015380305876063708]
	TIME [epoch: 8.18 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014136085390098574		[learning rate: 0.00063333]
		[batch 20/20] avg loss: 0.013357659064680413		[learning rate: 0.00063257]
	Learning Rate: 0.000632565
	LOSS [training: 0.007385633801845136 | validation: -0.008583909388222755]
	TIME [epoch: 8.19 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002182300903988462		[learning rate: 0.0006318]
		[batch 20/20] avg loss: 0.006309166761316956		[learning rate: 0.00063103]
	Learning Rate: 0.000631034
	LOSS [training: 0.004245733832652709 | validation: -0.00021044310951471613]
	TIME [epoch: 8.16 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012093592061512093		[learning rate: 0.00063027]
		[batch 20/20] avg loss: 0.0066760965747469534		[learning rate: 0.00062951]
	Learning Rate: 0.000629506
	LOSS [training: 0.009384844318129523 | validation: -0.009576394480019849]
	TIME [epoch: 8.16 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004789217720412946		[learning rate: 0.00062874]
		[batch 20/20] avg loss: 0.006095709157555557		[learning rate: 0.00062798]
	Learning Rate: 0.000627982
	LOSS [training: 0.0054424634389842515 | validation: -0.001889066298048709]
	TIME [epoch: 8.17 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009037142910967995		[learning rate: 0.00062722]
		[batch 20/20] avg loss: 0.00959229683602599		[learning rate: 0.00062646]
	Learning Rate: 0.000626462
	LOSS [training: 0.009314719873496993 | validation: 0.003677542326698999]
	TIME [epoch: 8.23 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011813750865541737		[learning rate: 0.0006257]
		[batch 20/20] avg loss: 0.014449959794612291		[learning rate: 0.00062495]
	Learning Rate: 0.000624946
	LOSS [training: 0.013131855330077014 | validation: -0.00020391351985644718]
	TIME [epoch: 8.15 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013255144044196926		[learning rate: 0.00062419]
		[batch 20/20] avg loss: 0.0075971693304059795		[learning rate: 0.00062343]
	Learning Rate: 0.000623433
	LOSS [training: 0.010426156687301454 | validation: -0.007930561699895353]
	TIME [epoch: 8.16 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005967845699853246		[learning rate: 0.00062268]
		[batch 20/20] avg loss: 0.010178778580498333		[learning rate: 0.00062192]
	Learning Rate: 0.000621923
	LOSS [training: 0.008073312140175789 | validation: -0.010132281214827182]
	TIME [epoch: 8.16 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004916193739811109		[learning rate: 0.00062117]
		[batch 20/20] avg loss: 0.014071985532958683		[learning rate: 0.00062042]
	Learning Rate: 0.000620418
	LOSS [training: 0.009494089636384897 | validation: 0.003176730482690495]
	TIME [epoch: 8.18 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017226124692528322		[learning rate: 0.00061967]
		[batch 20/20] avg loss: 0.010310114016895282		[learning rate: 0.00061892]
	Learning Rate: 0.000618916
	LOSS [training: 0.013768119354711802 | validation: 0.00019104110676008632]
	TIME [epoch: 8.16 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01561944688186267		[learning rate: 0.00061817]
		[batch 20/20] avg loss: 0.008816680149344017		[learning rate: 0.00061742]
	Learning Rate: 0.000617418
	LOSS [training: 0.012218063515603343 | validation: 0.0011918482213760912]
	TIME [epoch: 8.18 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016505805386334428		[learning rate: 0.00061667]
		[batch 20/20] avg loss: 0.010929872149536068		[learning rate: 0.00061592]
	Learning Rate: 0.000615923
	LOSS [training: 0.013717838767935248 | validation: -0.004671910733146475]
	TIME [epoch: 8.17 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011165095425975106		[learning rate: 0.00061518]
		[batch 20/20] avg loss: 0.009659703887815218		[learning rate: 0.00061443]
	Learning Rate: 0.000614432
	LOSS [training: 0.010412399656895159 | validation: 0.00983221874027452]
	TIME [epoch: 8.18 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01574608488678998		[learning rate: 0.00061369]
		[batch 20/20] avg loss: 0.0180940301498295		[learning rate: 0.00061294]
	Learning Rate: 0.000612944
	LOSS [training: 0.016920057518309738 | validation: 0.0031171185053050815]
	TIME [epoch: 8.19 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016915065115679088		[learning rate: 0.0006122]
		[batch 20/20] avg loss: 0.014744260863410597		[learning rate: 0.00061146]
	Learning Rate: 0.000611461
	LOSS [training: 0.01582966298954484 | validation: 0.01774909498513711]
	TIME [epoch: 8.19 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019443315884218677		[learning rate: 0.00061072]
		[batch 20/20] avg loss: 0.01257445976260502		[learning rate: 0.00060998]
	Learning Rate: 0.00060998
	LOSS [training: 0.01600888782341185 | validation: -0.0025179035018294185]
	TIME [epoch: 8.16 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014882936330566551		[learning rate: 0.00060924]
		[batch 20/20] avg loss: 0.00859484789594834		[learning rate: 0.0006085]
	Learning Rate: 0.000608504
	LOSS [training: 0.011738892113257446 | validation: -0.00022487777304670744]
	TIME [epoch: 8.15 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007893350498765155		[learning rate: 0.00060777]
		[batch 20/20] avg loss: 0.0088654248467495		[learning rate: 0.00060703]
	Learning Rate: 0.00060703
	LOSS [training: 0.008379387672757328 | validation: -0.00820189610025385]
	TIME [epoch: 8.18 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00517800282878835		[learning rate: 0.0006063]
		[batch 20/20] avg loss: 0.003738627991184172		[learning rate: 0.00060556]
	Learning Rate: 0.000605561
	LOSS [training: 0.004458315409986261 | validation: -0.008859557040998537]
	TIME [epoch: 8.15 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006023299002179837		[learning rate: 0.00060483]
		[batch 20/20] avg loss: 0.007949040121703959		[learning rate: 0.00060409]
	Learning Rate: 0.000604095
	LOSS [training: 0.006986169561941898 | validation: 0.0007716210566337898]
	TIME [epoch: 8.16 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006376015604553653		[learning rate: 0.00060336]
		[batch 20/20] avg loss: 0.008052043384136372		[learning rate: 0.00060263]
	Learning Rate: 0.000602633
	LOSS [training: 0.007214029494345011 | validation: -0.00791365604356942]
	TIME [epoch: 8.15 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009334286657033829		[learning rate: 0.0006019]
		[batch 20/20] avg loss: 0.005538790874315277		[learning rate: 0.00060117]
	Learning Rate: 0.000601174
	LOSS [training: 0.007436538765674553 | validation: -0.008896158454291616]
	TIME [epoch: 8.17 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005669516132398197		[learning rate: 0.00060045]
		[batch 20/20] avg loss: 0.008057335378942192		[learning rate: 0.00059972]
	Learning Rate: 0.000599718
	LOSS [training: 0.006863425755670197 | validation: 0.0026194361241226524]
	TIME [epoch: 8.16 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008216412449455885		[learning rate: 0.00059899]
		[batch 20/20] avg loss: 0.003893232579355759		[learning rate: 0.00059827]
	Learning Rate: 0.000598267
	LOSS [training: 0.006054822514405821 | validation: -0.011214967490425477]
	TIME [epoch: 8.18 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006835161914769648		[learning rate: 0.00059754]
		[batch 20/20] avg loss: 0.007925781557028987		[learning rate: 0.00059682]
	Learning Rate: 0.000596818
	LOSS [training: 0.007380471735899318 | validation: -0.009063273762133272]
	TIME [epoch: 8.17 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010679567747274082		[learning rate: 0.0005961]
		[batch 20/20] avg loss: 0.014930223211579625		[learning rate: 0.00059537]
	Learning Rate: 0.000595373
	LOSS [training: 0.01280489547942685 | validation: 0.00017746286907222052]
	TIME [epoch: 8.16 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012266712918620016		[learning rate: 0.00059465]
		[batch 20/20] avg loss: 0.005467060461647079		[learning rate: 0.00059393]
	Learning Rate: 0.000593932
	LOSS [training: 0.008866886690133548 | validation: -0.002751571210073693]
	TIME [epoch: 8.19 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005569009485255427		[learning rate: 0.00059321]
		[batch 20/20] avg loss: 0.006435170781730067		[learning rate: 0.00059249]
	Learning Rate: 0.000592494
	LOSS [training: 0.006002090133492747 | validation: -0.012388834120933754]
	TIME [epoch: 8.19 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038327910022847413		[learning rate: 0.00059178]
		[batch 20/20] avg loss: 0.010525099634634282		[learning rate: 0.00059106]
	Learning Rate: 0.00059106
	LOSS [training: 0.007178945318459513 | validation: -0.0015546468275876514]
	TIME [epoch: 8.16 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007231017449476349		[learning rate: 0.00059034]
		[batch 20/20] avg loss: 0.008737234568261667		[learning rate: 0.00058963]
	Learning Rate: 0.000589629
	LOSS [training: 0.007984126008869007 | validation: -0.004881264297351014]
	TIME [epoch: 8.16 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006307079163422794		[learning rate: 0.00058892]
		[batch 20/20] avg loss: 0.009001079702689384		[learning rate: 0.0005882]
	Learning Rate: 0.000588202
	LOSS [training: 0.00765407943305609 | validation: -0.013010592500110038]
	TIME [epoch: 8.17 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005553921743486861		[learning rate: 0.00058749]
		[batch 20/20] avg loss: 0.005815179439698995		[learning rate: 0.00058678]
	Learning Rate: 0.000586778
	LOSS [training: 0.005684550591592928 | validation: -0.0073990359521082915]
	TIME [epoch: 8.16 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0044501263417538895		[learning rate: 0.00058607]
		[batch 20/20] avg loss: 0.008673110011074063		[learning rate: 0.00058536]
	Learning Rate: 0.000585357
	LOSS [training: 0.006561618176413977 | validation: -0.00419405729930037]
	TIME [epoch: 8.15 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007423018609535571		[learning rate: 0.00058465]
		[batch 20/20] avg loss: -0.0017088015540996687		[learning rate: 0.00058394]
	Learning Rate: 0.00058394
	LOSS [training: 0.0028571085277179515 | validation: -0.0019024861138161099]
	TIME [epoch: 8.18 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004248452590274314		[learning rate: 0.00058323]
		[batch 20/20] avg loss: 0.007098288175308283		[learning rate: 0.00058253]
	Learning Rate: 0.000582527
	LOSS [training: 0.005673370382791297 | validation: -0.004262295670918251]
	TIME [epoch: 8.17 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008842365824682305		[learning rate: 0.00058182]
		[batch 20/20] avg loss: 0.002035994299082004		[learning rate: 0.00058112]
	Learning Rate: 0.000581116
	LOSS [training: 0.005439180061882155 | validation: -0.006277721121551025]
	TIME [epoch: 8.19 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033868182530143315		[learning rate: 0.00058041]
		[batch 20/20] avg loss: 0.002874935420331893		[learning rate: 0.00057971]
	Learning Rate: 0.00057971
	LOSS [training: 0.0031308768366731118 | validation: -0.008590688163208699]
	TIME [epoch: 8.16 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005017084605206179		[learning rate: 0.00057901]
		[batch 20/20] avg loss: 0.010665974427100164		[learning rate: 0.00057831]
	Learning Rate: 0.000578306
	LOSS [training: 0.007841529516153172 | validation: -0.0053345924481910225]
	TIME [epoch: 8.2 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006524866619081431		[learning rate: 0.00057761]
		[batch 20/20] avg loss: 0.005387808488034904		[learning rate: 0.00057691]
	Learning Rate: 0.000576906
	LOSS [training: 0.005956337553558166 | validation: -0.007888568996561629]
	TIME [epoch: 8.15 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008429466158719416		[learning rate: 0.00057621]
		[batch 20/20] avg loss: 0.011351750413731768		[learning rate: 0.00057551]
	Learning Rate: 0.00057551
	LOSS [training: 0.00989060828622559 | validation: -0.00943453931469361]
	TIME [epoch: 8.18 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00398081012562445		[learning rate: 0.00057481]
		[batch 20/20] avg loss: 0.0061512388967646095		[learning rate: 0.00057412]
	Learning Rate: 0.000574117
	LOSS [training: 0.005066024511194529 | validation: -0.004394758682482823]
	TIME [epoch: 8.16 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007470731932567062		[learning rate: 0.00057342]
		[batch 20/20] avg loss: 0.004087843108922056		[learning rate: 0.00057273]
	Learning Rate: 0.000572727
	LOSS [training: 0.005779287520744559 | validation: -0.0038103657393160446]
	TIME [epoch: 8.15 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012094055266985268		[learning rate: 0.00057203]
		[batch 20/20] avg loss: 0.00477991967502774		[learning rate: 0.00057134]
	Learning Rate: 0.00057134
	LOSS [training: 0.008436987471006503 | validation: -0.01000816780998705]
	TIME [epoch: 8.16 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006891308939339383		[learning rate: 0.00057065]
		[batch 20/20] avg loss: 0.015520761163738789		[learning rate: 0.00056996]
	Learning Rate: 0.000569957
	LOSS [training: 0.011206035051539087 | validation: 0.012875612171645519]
	TIME [epoch: 8.17 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019330667588390138		[learning rate: 0.00056927]
		[batch 20/20] avg loss: 0.006004317826525292		[learning rate: 0.00056858]
	Learning Rate: 0.000568577
	LOSS [training: 0.012667492707457715 | validation: -0.010328106629206269]
	TIME [epoch: 8.2 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004863845788461492		[learning rate: 0.00056789]
		[batch 20/20] avg loss: 0.009390142438203646		[learning rate: 0.0005672]
	Learning Rate: 0.000567201
	LOSS [training: 0.007126994113332568 | validation: -0.014073531838862192]
	TIME [epoch: 8.16 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009296866099096749		[learning rate: 0.00056651]
		[batch 20/20] avg loss: 0.007095013534319894		[learning rate: 0.00056583]
	Learning Rate: 0.000565828
	LOSS [training: 0.008195939816708322 | validation: -0.0018951819994631046]
	TIME [epoch: 8.17 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003642628387032246		[learning rate: 0.00056514]
		[batch 20/20] avg loss: 0.006056154762704951		[learning rate: 0.00056446]
	Learning Rate: 0.000564458
	LOSS [training: 0.004849391574868599 | validation: -0.0002580339643802988]
	TIME [epoch: 8.17 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006097377574854405		[learning rate: 0.00056377]
		[batch 20/20] avg loss: 0.00902600747547094		[learning rate: 0.00056309]
	Learning Rate: 0.000563092
	LOSS [training: 0.007561692525162672 | validation: -0.006832242547597838]
	TIME [epoch: 8.21 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034190516448514762		[learning rate: 0.00056241]
		[batch 20/20] avg loss: 0.0064345298745892565		[learning rate: 0.00056173]
	Learning Rate: 0.000561728
	LOSS [training: 0.004926790759720367 | validation: -0.005747967144730304]
	TIME [epoch: 8.15 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009206852145203672		[learning rate: 0.00056105]
		[batch 20/20] avg loss: 0.006340531914241153		[learning rate: 0.00056037]
	Learning Rate: 0.000560369
	LOSS [training: 0.0077736920297224125 | validation: -0.0035997195336249844]
	TIME [epoch: 8.16 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004258013253809338		[learning rate: 0.00055969]
		[batch 20/20] avg loss: 0.01172078390857239		[learning rate: 0.00055901]
	Learning Rate: 0.000559012
	LOSS [training: 0.007989398581190865 | validation: -0.0015914625103423713]
	TIME [epoch: 8.16 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003944923847549661		[learning rate: 0.00055833]
		[batch 20/20] avg loss: 0.005206989555631972		[learning rate: 0.00055766]
	Learning Rate: 0.000557659
	LOSS [training: 0.004575956701590817 | validation: 0.008893642233986138]
	TIME [epoch: 8.18 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012786475383250284		[learning rate: 0.00055698]
		[batch 20/20] avg loss: 0.002138307756233643		[learning rate: 0.00055631]
	Learning Rate: 0.000556309
	LOSS [training: 0.0074623915697419625 | validation: -0.005283460227070185]
	TIME [epoch: 8.16 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008608434395569648		[learning rate: 0.00055563]
		[batch 20/20] avg loss: 0.022698202225078783		[learning rate: 0.00055496]
	Learning Rate: 0.000554962
	LOSS [training: 0.015653318310324217 | validation: 0.004430761143653921]
	TIME [epoch: 8.15 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006665744643377819		[learning rate: 0.00055429]
		[batch 20/20] avg loss: 0.003428555439100845		[learning rate: 0.00055362]
	Learning Rate: 0.000553618
	LOSS [training: 0.005047150041239332 | validation: -0.00791002346528835]
	TIME [epoch: 8.18 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008398816118663123		[learning rate: 0.00055295]
		[batch 20/20] avg loss: 0.0012987129635851258		[learning rate: 0.00055228]
	Learning Rate: 0.000552278
	LOSS [training: 0.004848764541124125 | validation: -0.0026518790927969335]
	TIME [epoch: 8.17 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0069849099780788055		[learning rate: 0.00055161]
		[batch 20/20] avg loss: 0.0045043747814142625		[learning rate: 0.00055094]
	Learning Rate: 0.000550941
	LOSS [training: 0.005744642379746533 | validation: -0.0029414405600528422]
	TIME [epoch: 8.19 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009443200056428435		[learning rate: 0.00055027]
		[batch 20/20] avg loss: 0.005182343326531563		[learning rate: 0.00054961]
	Learning Rate: 0.000549608
	LOSS [training: 0.007312771691480001 | validation: -0.00030664413174752827]
	TIME [epoch: 8.17 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008293353657023998		[learning rate: 0.00054894]
		[batch 20/20] avg loss: 0.008916452090385451		[learning rate: 0.00054828]
	Learning Rate: 0.000548277
	LOSS [training: 0.008604902873704724 | validation: -0.004136530625145308]
	TIME [epoch: 8.19 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007973265402263644		[learning rate: 0.00054761]
		[batch 20/20] avg loss: 0.009584915007302838		[learning rate: 0.00054695]
	Learning Rate: 0.00054695
	LOSS [training: 0.008779090204783242 | validation: -0.006893677914379455]
	TIME [epoch: 8.16 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008658742550670891		[learning rate: 0.00054629]
		[batch 20/20] avg loss: 0.003241390766954272		[learning rate: 0.00054563]
	Learning Rate: 0.000545626
	LOSS [training: 0.005950066658812582 | validation: -0.0043346433920756]
	TIME [epoch: 8.18 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010901432326761992		[learning rate: 0.00054496]
		[batch 20/20] avg loss: 0.002290565222560017		[learning rate: 0.0005443]
	Learning Rate: 0.000544305
	LOSS [training: 0.006595998774661003 | validation: -0.010524923210156592]
	TIME [epoch: 8.16 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006663531682156965		[learning rate: 0.00054365]
		[batch 20/20] avg loss: 0.0024915440767799874		[learning rate: 0.00054299]
	Learning Rate: 0.000542987
	LOSS [training: 0.004577537879468477 | validation: -0.002866089362876555]
	TIME [epoch: 8.15 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016638750323949788		[learning rate: 0.00054233]
		[batch 20/20] avg loss: 0.006030521975198271		[learning rate: 0.00054167]
	Learning Rate: 0.000541673
	LOSS [training: 0.0038471985037966234 | validation: -0.003625137093896698]
	TIME [epoch: 8.16 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011569869041939546		[learning rate: 0.00054102]
		[batch 20/20] avg loss: 0.01176192194878951		[learning rate: 0.00054036]
	Learning Rate: 0.000540361
	LOSS [training: 0.011665895495364526 | validation: -0.003937858018721104]
	TIME [epoch: 8.16 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006908981614848436		[learning rate: 0.00053971]
		[batch 20/20] avg loss: 0.012550094951117988		[learning rate: 0.00053905]
	Learning Rate: 0.000539053
	LOSS [training: 0.009729538282983213 | validation: -0.004313858026890157]
	TIME [epoch: 8.21 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007987931078866892		[learning rate: 0.0005384]
		[batch 20/20] avg loss: 0.004697987736543803		[learning rate: 0.00053775]
	Learning Rate: 0.000537748
	LOSS [training: 0.006342959407705348 | validation: 0.0031803017238728527]
	TIME [epoch: 8.16 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009117931076500644		[learning rate: 0.0005371]
		[batch 20/20] avg loss: 0.013450225982001482		[learning rate: 0.00053645]
	Learning Rate: 0.000536446
	LOSS [training: 0.011284078529251063 | validation: -0.0007145788070400051]
	TIME [epoch: 8.16 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013433880309464533		[learning rate: 0.0005358]
		[batch 20/20] avg loss: 0.009088971585197248		[learning rate: 0.00053515]
	Learning Rate: 0.000535148
	LOSS [training: 0.011261425947330891 | validation: -0.00654834520088212]
	TIME [epoch: 8.17 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013799745612925695		[learning rate: 0.0005345]
		[batch 20/20] avg loss: 0.005253410118572175		[learning rate: 0.00053385]
	Learning Rate: 0.000533852
	LOSS [training: 0.009526577865748936 | validation: -0.0035307692941839997]
	TIME [epoch: 8.22 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00415598673501709		[learning rate: 0.00053321]
		[batch 20/20] avg loss: 0.0037160456148963667		[learning rate: 0.00053256]
	Learning Rate: 0.00053256
	LOSS [training: 0.0039360161749567284 | validation: -0.007993483285375693]
	TIME [epoch: 8.16 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001947330183495171		[learning rate: 0.00053191]
		[batch 20/20] avg loss: 0.00904463274376301		[learning rate: 0.00053127]
	Learning Rate: 0.000531271
	LOSS [training: 0.0054959814636290905 | validation: -0.00772156270507898]
	TIME [epoch: 8.16 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002634119892005804		[learning rate: 0.00053063]
		[batch 20/20] avg loss: 0.007101253817512906		[learning rate: 0.00052998]
	Learning Rate: 0.000529985
	LOSS [training: 0.004867686854759355 | validation: -0.009574385758730018]
	TIME [epoch: 8.16 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006698026220965552		[learning rate: 0.00052934]
		[batch 20/20] avg loss: 0.002454288100596477		[learning rate: 0.0005287]
	Learning Rate: 0.000528702
	LOSS [training: 0.004576157160781014 | validation: -0.010455528775883575]
	TIME [epoch: 8.17 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005046227940545144		[learning rate: 0.00052806]
		[batch 20/20] avg loss: 0.0010709440417264581		[learning rate: 0.00052742]
	Learning Rate: 0.000527422
	LOSS [training: 0.0030585859911358014 | validation: -0.009464730753097202]
	TIME [epoch: 8.17 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012829132154132243		[learning rate: 0.00052678]
		[batch 20/20] avg loss: 0.009562881617007538		[learning rate: 0.00052614]
	Learning Rate: 0.000526145
	LOSS [training: 0.004139984200797156 | validation: -0.00610070033025004]
	TIME [epoch: 8.16 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009143915991316707		[learning rate: 0.00052551]
		[batch 20/20] avg loss: 0.00531355153714652		[learning rate: 0.00052487]
	Learning Rate: 0.000524871
	LOSS [training: 0.007228733764231614 | validation: -0.00250265699784848]
	TIME [epoch: 8.18 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009918348743150182		[learning rate: 0.00052424]
		[batch 20/20] avg loss: 0.009229079267548361		[learning rate: 0.0005236]
	Learning Rate: 0.0005236
	LOSS [training: 0.009573714005349272 | validation: -0.006921147583178011]
	TIME [epoch: 8.17 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007328370508103336		[learning rate: 0.00052297]
		[batch 20/20] avg loss: 0.0014457420478300432		[learning rate: 0.00052233]
	Learning Rate: 0.000522333
	LOSS [training: 0.00438705627796669 | validation: -0.002801163193833799]
	TIME [epoch: 8.19 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008449631156671206		[learning rate: 0.0005217]
		[batch 20/20] avg loss: 0.0027402673562956325		[learning rate: 0.00052107]
	Learning Rate: 0.000521068
	LOSS [training: 0.005594949256483419 | validation: -0.009157884381858751]
	TIME [epoch: 8.16 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004961494525492768		[learning rate: 0.00052044]
		[batch 20/20] avg loss: 0.005760667208009419		[learning rate: 0.00051981]
	Learning Rate: 0.000519807
	LOSS [training: 0.005361080866751093 | validation: -0.004005953812992163]
	TIME [epoch: 8.19 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029162161282774083		[learning rate: 0.00051918]
		[batch 20/20] avg loss: 0.003980885497922409		[learning rate: 0.00051855]
	Learning Rate: 0.000518549
	LOSS [training: 0.0005323346848225 | validation: 0.0019712222412417773]
	TIME [epoch: 8.17 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005189364924696904		[learning rate: 0.00051792]
		[batch 20/20] avg loss: 0.0010828794222220574		[learning rate: 0.00051729]
	Learning Rate: 0.000517293
	LOSS [training: 0.00313612217345948 | validation: -0.0062287767512335035]
	TIME [epoch: 8.19 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001958181629091399		[learning rate: 0.00051667]
		[batch 20/20] avg loss: 0.008389354059782698		[learning rate: 0.00051604]
	Learning Rate: 0.000516041
	LOSS [training: 0.005173767844437049 | validation: -0.0016677557027942526]
	TIME [epoch: 8.16 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0057441154204630005		[learning rate: 0.00051542]
		[batch 20/20] avg loss: 0.00826813553627303		[learning rate: 0.00051479]
	Learning Rate: 0.000514792
	LOSS [training: 0.007006125478368018 | validation: -0.008986074396755936]
	TIME [epoch: 8.15 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005504143556599445		[learning rate: 0.00051417]
		[batch 20/20] avg loss: 0.0037749862703848544		[learning rate: 0.00051355]
	Learning Rate: 0.000513545
	LOSS [training: 0.004639564913492149 | validation: -0.006798678693832721]
	TIME [epoch: 8.16 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006055397342126564		[learning rate: 0.00051292]
		[batch 20/20] avg loss: 0.01854972383843496		[learning rate: 0.0005123]
	Learning Rate: 0.000512302
	LOSS [training: 0.012302560590280762 | validation: -0.0017961886611220631]
	TIME [epoch: 8.15 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007034704443672858		[learning rate: 0.00051168]
		[batch 20/20] avg loss: 0.00831649752342139		[learning rate: 0.00051106]
	Learning Rate: 0.000511062
	LOSS [training: 0.007675600983547125 | validation: -0.01157900903775084]
	TIME [epoch: 8.18 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006576829927261839		[learning rate: 0.00051044]
		[batch 20/20] avg loss: 0.008903405637215884		[learning rate: 0.00050982]
	Learning Rate: 0.000509825
	LOSS [training: 0.00774011778223886 | validation: -0.011203732357471236]
	TIME [epoch: 8.15 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006145555154825477		[learning rate: 0.00050921]
		[batch 20/20] avg loss: 0.005549236789318353		[learning rate: 0.00050859]
	Learning Rate: 0.000508591
	LOSS [training: 0.005847395972071916 | validation: -0.0033856435726136638]
	TIME [epoch: 8.18 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007908970498370632		[learning rate: 0.00050797]
		[batch 20/20] avg loss: 0.018395919842947403		[learning rate: 0.00050736]
	Learning Rate: 0.00050736
	LOSS [training: 0.013152445170659017 | validation: 0.0015958926382809986]
	TIME [epoch: 8.17 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0062642493243137		[learning rate: 0.00050675]
		[batch 20/20] avg loss: 0.005712964075819093		[learning rate: 0.00050613]
	Learning Rate: 0.000506131
	LOSS [training: 0.005988606700066396 | validation: -0.01098926709261465]
	TIME [epoch: 8.18 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003275845113759433		[learning rate: 0.00050552]
		[batch 20/20] avg loss: 0.00562560195663033		[learning rate: 0.00050491]
	Learning Rate: 0.000504906
	LOSS [training: 0.00445072353519488 | validation: -0.008977564364790004]
	TIME [epoch: 8.17 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006100799703457961		[learning rate: 0.00050429]
		[batch 20/20] avg loss: 0.003438731689891458		[learning rate: 0.00050368]
	Learning Rate: 0.000503684
	LOSS [training: 0.0047697656966747114 | validation: -0.01076385369728007]
	TIME [epoch: 8.19 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012778464002602358		[learning rate: 0.00050307]
		[batch 20/20] avg loss: 0.006482342643269095		[learning rate: 0.00050246]
	Learning Rate: 0.000502464
	LOSS [training: 0.0026022481215044304 | validation: -0.0035358429301610996]
	TIME [epoch: 8.17 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003615958068168019		[learning rate: 0.00050186]
		[batch 20/20] avg loss: 0.008301653214106666		[learning rate: 0.00050125]
	Learning Rate: 0.000501248
	LOSS [training: 0.005958805641137343 | validation: 0.0005195718588339852]
	TIME [epoch: 8.16 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012797324037508532		[learning rate: 0.00050064]
		[batch 20/20] avg loss: 0.007914139981689858		[learning rate: 0.00050003]
	Learning Rate: 0.000500034
	LOSS [training: 0.010355732009599192 | validation: -0.008704730411122711]
	TIME [epoch: 8.18 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005846713882212429		[learning rate: 0.00049943]
		[batch 20/20] avg loss: 0.0056908118608759726		[learning rate: 0.00049882]
	Learning Rate: 0.000498824
	LOSS [training: 0.0057687628715442 | validation: -0.0057749230695948745]
	TIME [epoch: 8.15 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008038010407628567		[learning rate: 0.00049822]
		[batch 20/20] avg loss: 0.00903873463521489		[learning rate: 0.00049762]
	Learning Rate: 0.000497616
	LOSS [training: 0.004117466797226017 | validation: -0.013736911898862184]
	TIME [epoch: 8.16 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009992686968082209		[learning rate: 0.00049701]
		[batch 20/20] avg loss: 0.0058567564324556514		[learning rate: 0.00049641]
	Learning Rate: 0.000496412
	LOSS [training: 0.00792472170026893 | validation: -0.00308952022876431]
	TIME [epoch: 8.16 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012344456079636213		[learning rate: 0.00049581]
		[batch 20/20] avg loss: 0.0036704984198912918		[learning rate: 0.00049521]
	Learning Rate: 0.00049521
	LOSS [training: 0.00800747724976375 | validation: -0.0026281216881023093]
	TIME [epoch: 8.18 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00037880464482509957		[learning rate: 0.00049461]
		[batch 20/20] avg loss: 0.006913997939813945		[learning rate: 0.00049401]
	Learning Rate: 0.000494011
	LOSS [training: 0.003646401292319522 | validation: -0.003648882119593122]
	TIME [epoch: 8.16 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003959819257037924		[learning rate: 0.00049341]
		[batch 20/20] avg loss: 0.004175914101382546		[learning rate: 0.00049282]
	Learning Rate: 0.000492815
	LOSS [training: 0.004067866679210235 | validation: 0.008865851605785678]
	TIME [epoch: 8.15 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01112075621286284		[learning rate: 0.00049222]
		[batch 20/20] avg loss: 0.006541651730002445		[learning rate: 0.00049162]
	Learning Rate: 0.000491622
	LOSS [training: 0.008831203971432642 | validation: -0.0016079472654388306]
	TIME [epoch: 8.18 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032701116542803903		[learning rate: 0.00049103]
		[batch 20/20] avg loss: 0.014666203418731542		[learning rate: 0.00049043]
	Learning Rate: 0.000490432
	LOSS [training: 0.008968157536505966 | validation: -0.0027727191529988682]
	TIME [epoch: 8.18 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012671211589959841		[learning rate: 0.00048984]
		[batch 20/20] avg loss: 0.02109871981476661		[learning rate: 0.00048924]
	Learning Rate: 0.000489245
	LOSS [training: 0.016884965702363225 | validation: -0.003385468068063492]
	TIME [epoch: 8.16 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009745035370437546		[learning rate: 0.00048865]
		[batch 20/20] avg loss: 0.005129809756475328		[learning rate: 0.00048806]
	Learning Rate: 0.000488061
	LOSS [training: 0.007437422563456438 | validation: -0.011303460802588098]
	TIME [epoch: 8.17 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005057207474260102		[learning rate: 0.00048747]
		[batch 20/20] avg loss: 0.006165240164143019		[learning rate: 0.00048688]
	Learning Rate: 0.000486879
	LOSS [training: 0.005611223819201561 | validation: -0.00013181697269178637]
	TIME [epoch: 8.2 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013301597500966201		[learning rate: 0.00048629]
		[batch 20/20] avg loss: 0.008958053038856481		[learning rate: 0.0004857]
	Learning Rate: 0.0004857
	LOSS [training: 0.011129825269911341 | validation: 0.0014930146109517785]
	TIME [epoch: 8.16 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015231939608986455		[learning rate: 0.00048511]
		[batch 20/20] avg loss: 0.007304421378998037		[learning rate: 0.00048452]
	Learning Rate: 0.000484525
	LOSS [training: 0.011268180493992247 | validation: 0.0020965736039001075]
	TIME [epoch: 8.18 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015640403199483958		[learning rate: 0.00048394]
		[batch 20/20] avg loss: 0.010820707496828728		[learning rate: 0.00048335]
	Learning Rate: 0.000483352
	LOSS [training: 0.013230555348156346 | validation: 0.0030691095495876033]
	TIME [epoch: 8.16 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013277985565516029		[learning rate: 0.00048277]
		[batch 20/20] avg loss: 0.010586692875365351		[learning rate: 0.00048218]
	Learning Rate: 0.000482181
	LOSS [training: 0.01193233922044069 | validation: 0.004380064559592784]
	TIME [epoch: 8.15 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008227648927896042		[learning rate: 0.0004816]
		[batch 20/20] avg loss: 0.016675770317953503		[learning rate: 0.00048101]
	Learning Rate: 0.000481014
	LOSS [training: 0.012451709622924773 | validation: 0.00962060907777356]
	TIME [epoch: 8.15 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009729543463454426		[learning rate: 0.00048043]
		[batch 20/20] avg loss: 0.02167311074741785		[learning rate: 0.00047985]
	Learning Rate: 0.00047985
	LOSS [training: 0.01570132710543614 | validation: 0.007784067594937227]
	TIME [epoch: 8.19 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014058848045808852		[learning rate: 0.00047927]
		[batch 20/20] avg loss: 0.009862752406259575		[learning rate: 0.00047869]
	Learning Rate: 0.000478688
	LOSS [training: 0.011960800226034213 | validation: -0.002600188868220933]
	TIME [epoch: 8.18 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00524760266816135		[learning rate: 0.00047811]
		[batch 20/20] avg loss: 0.002164275641287521		[learning rate: 0.00047753]
	Learning Rate: 0.000477529
	LOSS [training: 0.0037059391547244356 | validation: -0.00817017418835576]
	TIME [epoch: 8.16 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008668153033594925		[learning rate: 0.00047695]
		[batch 20/20] avg loss: -0.0007854165845294095		[learning rate: 0.00047637]
	Learning Rate: 0.000476373
	LOSS [training: 0.0039413682245327585 | validation: -0.016463938924247346]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_1356.pth
	Model improved!!!
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002269318030286802		[learning rate: 0.0004758]
		[batch 20/20] avg loss: 0.007181694114814325		[learning rate: 0.00047522]
	Learning Rate: 0.00047522
	LOSS [training: 0.004725506072550564 | validation: -0.009480243046158823]
	TIME [epoch: 8.2 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009145733143157263		[learning rate: 0.00047464]
		[batch 20/20] avg loss: 0.004377464967835454		[learning rate: 0.00047407]
	Learning Rate: 0.00047407
	LOSS [training: 0.0017314458267598642 | validation: -0.0023395265804412708]
	TIME [epoch: 8.19 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007641629967639176		[learning rate: 0.0004735]
		[batch 20/20] avg loss: 0.010965288002400334		[learning rate: 0.00047292]
	Learning Rate: 0.000472922
	LOSS [training: 0.009303458985019754 | validation: -0.003058721801580252]
	TIME [epoch: 8.16 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01145450382408895		[learning rate: 0.00047235]
		[batch 20/20] avg loss: 0.016028605403738155		[learning rate: 0.00047178]
	Learning Rate: 0.000471777
	LOSS [training: 0.013741554613913553 | validation: -2.6355405357381817e-05]
	TIME [epoch: 8.16 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010892748262078764		[learning rate: 0.00047121]
		[batch 20/20] avg loss: 0.009386739274659573		[learning rate: 0.00047063]
	Learning Rate: 0.000470635
	LOSS [training: 0.010139743768369168 | validation: 0.00635583624171422]
	TIME [epoch: 8.16 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013520696833528804		[learning rate: 0.00047006]
		[batch 20/20] avg loss: 0.0032492058242999606		[learning rate: 0.0004695]
	Learning Rate: 0.000469496
	LOSS [training: 0.00838495132891438 | validation: -0.0013565971301720077]
	TIME [epoch: 8.18 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007739046909657305		[learning rate: 0.00046893]
		[batch 20/20] avg loss: 0.006913062719121432		[learning rate: 0.00046836]
	Learning Rate: 0.000468359
	LOSS [training: 0.0073260548143893696 | validation: -0.002112714177036103]
	TIME [epoch: 8.17 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00929434366559751		[learning rate: 0.00046779]
		[batch 20/20] avg loss: 0.006248392450426676		[learning rate: 0.00046723]
	Learning Rate: 0.000467225
	LOSS [training: 0.007771368058012094 | validation: -0.0027238262057071236]
	TIME [epoch: 8.17 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0042216640346826265		[learning rate: 0.00046666]
		[batch 20/20] avg loss: 0.006307194574887586		[learning rate: 0.00046609]
	Learning Rate: 0.000466094
	LOSS [training: 0.005264429304785106 | validation: -0.004187900608566064]
	TIME [epoch: 8.17 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015510450132411221		[learning rate: 0.00046553]
		[batch 20/20] avg loss: 0.0015667819147882438		[learning rate: 0.00046497]
	Learning Rate: 0.000464966
	LOSS [training: 0.008538616023599732 | validation: -0.011064672416606984]
	TIME [epoch: 8.18 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036518776112005706		[learning rate: 0.0004644]
		[batch 20/20] avg loss: 0.00533338966429635		[learning rate: 0.00046384]
	Learning Rate: 0.00046384
	LOSS [training: 0.00449263363774846 | validation: -0.0044924058271727536]
	TIME [epoch: 8.2 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004687020950914457		[learning rate: 0.00046328]
		[batch 20/20] avg loss: 0.0024740998134690495		[learning rate: 0.00046272]
	Learning Rate: 0.000462717
	LOSS [training: 0.003580560382191753 | validation: -0.012409898853125077]
	TIME [epoch: 8.17 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005924428744318403		[learning rate: 0.00046216]
		[batch 20/20] avg loss: 0.005133045300945448		[learning rate: 0.0004616]
	Learning Rate: 0.000461597
	LOSS [training: 0.005528737022631925 | validation: -0.0033997397336763247]
	TIME [epoch: 8.16 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008986726065571273		[learning rate: 0.00046104]
		[batch 20/20] avg loss: 0.006009116239435592		[learning rate: 0.00046048]
	Learning Rate: 0.00046048
	LOSS [training: 0.007497921152503431 | validation: -0.0007928739555747348]
	TIME [epoch: 8.16 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017660348746847431		[learning rate: 0.00045992]
		[batch 20/20] avg loss: 0.00866267873050303		[learning rate: 0.00045937]
	Learning Rate: 0.000459365
	LOSS [training: 0.005214356802593886 | validation: -0.0037245844848415224]
	TIME [epoch: 8.18 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007649112693274697		[learning rate: 0.00045881]
		[batch 20/20] avg loss: 0.003009768374819692		[learning rate: 0.00045825]
	Learning Rate: 0.000458253
	LOSS [training: 0.005329440534047196 | validation: -0.009807820951860188]
	TIME [epoch: 8.16 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0048995289597513455		[learning rate: 0.0004577]
		[batch 20/20] avg loss: 0.007794446919914352		[learning rate: 0.00045714]
	Learning Rate: 0.000457144
	LOSS [training: 0.006346987939832849 | validation: 0.00013426310047887004]
	TIME [epoch: 8.16 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007459699568271366		[learning rate: 0.00045659]
		[batch 20/20] avg loss: 0.012718606080906292		[learning rate: 0.00045604]
	Learning Rate: 0.000456037
	LOSS [training: 0.010089152824588828 | validation: -0.011806629657492249]
	TIME [epoch: 8.16 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007208357940534651		[learning rate: 0.00045548]
		[batch 20/20] avg loss: 0.006730145621571215		[learning rate: 0.00045493]
	Learning Rate: 0.000454933
	LOSS [training: 0.006969251781052933 | validation: -0.007508936369462034]
	TIME [epoch: 8.2 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033076138710976554		[learning rate: 0.00045438]
		[batch 20/20] avg loss: 0.007742405209651708		[learning rate: 0.00045383]
	Learning Rate: 0.000453832
	LOSS [training: 0.005525009540374682 | validation: -0.008133998638005097]
	TIME [epoch: 8.17 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012837718070296766		[learning rate: 0.00045328]
		[batch 20/20] avg loss: 0.003422619714066177		[learning rate: 0.00045273]
	Learning Rate: 0.000452733
	LOSS [training: 0.008130168892181473 | validation: -0.012448714495072014]
	TIME [epoch: 8.16 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002781183259603595		[learning rate: 0.00045218]
		[batch 20/20] avg loss: 0.0011509349818811462		[learning rate: 0.00045164]
	Learning Rate: 0.000451637
	LOSS [training: 0.001966059120742371 | validation: -0.010262980240485364]
	TIME [epoch: 8.17 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004302309712782662		[learning rate: 0.00045109]
		[batch 20/20] avg loss: 0.0025990456135633992		[learning rate: 0.00045054]
	Learning Rate: 0.000450544
	LOSS [training: 0.0034506776631730305 | validation: -0.006677614959351369]
	TIME [epoch: 8.18 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013097256181982236		[learning rate: 0.00045]
		[batch 20/20] avg loss: 0.014086675441582545		[learning rate: 0.00044945]
	Learning Rate: 0.000449453
	LOSS [training: 0.007698200529890382 | validation: -0.003294685314105411]
	TIME [epoch: 8.21 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007427156688641632		[learning rate: 0.00044891]
		[batch 20/20] avg loss: 0.006700053211286907		[learning rate: 0.00044836]
	Learning Rate: 0.000448365
	LOSS [training: 0.007063604949964271 | validation: -0.007599276837792554]
	TIME [epoch: 8.15 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008202188185771024		[learning rate: 0.00044782]
		[batch 20/20] avg loss: 0.003482954819825514		[learning rate: 0.00044728]
	Learning Rate: 0.000447279
	LOSS [training: 0.005842571502798269 | validation: 0.003207452265459322]
	TIME [epoch: 8.16 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01004387592211686		[learning rate: 0.00044674]
		[batch 20/20] avg loss: 0.003356360211494122		[learning rate: 0.0004462]
	Learning Rate: 0.000446197
	LOSS [training: 0.00670011806680549 | validation: -0.005242298381806026]
	TIME [epoch: 8.16 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005191343763448601		[learning rate: 0.00044566]
		[batch 20/20] avg loss: 0.0019601583307209333		[learning rate: 0.00044512]
	Learning Rate: 0.000445117
	LOSS [training: 0.003575751047084768 | validation: 0.0044918806475679925]
	TIME [epoch: 8.18 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008738951968704634		[learning rate: 0.00044458]
		[batch 20/20] avg loss: 0.00814750760101851		[learning rate: 0.00044404]
	Learning Rate: 0.000444039
	LOSS [training: 0.008443229784861572 | validation: -0.008369884139293612]
	TIME [epoch: 8.15 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006301637445021572		[learning rate: 0.0004435]
		[batch 20/20] avg loss: 0.006749565583665365		[learning rate: 0.00044296]
	Learning Rate: 0.000442964
	LOSS [training: 0.0065256015143434695 | validation: -0.00982145295226857]
	TIME [epoch: 8.16 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037688219761376476		[learning rate: 0.00044243]
		[batch 20/20] avg loss: 0.006883021512877553		[learning rate: 0.00044189]
	Learning Rate: 0.000441892
	LOSS [training: 0.0053259217445076005 | validation: -0.005746017718976345]
	TIME [epoch: 8.19 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004808967926954155		[learning rate: 0.00044136]
		[batch 20/20] avg loss: 0.007224002079647743		[learning rate: 0.00044082]
	Learning Rate: 0.000440822
	LOSS [training: 0.006016485003300948 | validation: 0.000601817965164864]
	TIME [epoch: 8.16 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007854278098041173		[learning rate: 0.00044029]
		[batch 20/20] avg loss: 0.006511643473571906		[learning rate: 0.00043975]
	Learning Rate: 0.000439755
	LOSS [training: 0.007182960785806538 | validation: 7.649836242583233e-05]
	TIME [epoch: 8.19 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008238009278766806		[learning rate: 0.00043922]
		[batch 20/20] avg loss: 0.006267440280648806		[learning rate: 0.00043869]
	Learning Rate: 0.00043869
	LOSS [training: 0.007252724779707806 | validation: -0.007345830456029923]
	TIME [epoch: 8.16 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005143414693968583		[learning rate: 0.00043816]
		[batch 20/20] avg loss: 0.006204417268252484		[learning rate: 0.00043763]
	Learning Rate: 0.000437628
	LOSS [training: 0.005673915981110533 | validation: -0.006153888304929396]
	TIME [epoch: 8.2 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005620001854306213		[learning rate: 0.0004371]
		[batch 20/20] avg loss: 0.00386509966207312		[learning rate: 0.00043657]
	Learning Rate: 0.000436569
	LOSS [training: 0.004742550758189666 | validation: -0.009750110517528578]
	TIME [epoch: 8.16 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029097283937312966		[learning rate: 0.00043604]
		[batch 20/20] avg loss: 0.009121653387080101		[learning rate: 0.00043551]
	Learning Rate: 0.000435512
	LOSS [training: 0.006015690890405699 | validation: -0.0013013038370150443]
	TIME [epoch: 8.18 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006387081477998204		[learning rate: 0.00043498]
		[batch 20/20] avg loss: -0.0007876407232561722		[learning rate: 0.00043446]
	Learning Rate: 0.000434458
	LOSS [training: 0.002799720377371016 | validation: -0.007360316118197866]
	TIME [epoch: 8.16 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004413658043131268		[learning rate: 0.00043393]
		[batch 20/20] avg loss: 0.0015359398681082555		[learning rate: 0.00043341]
	Learning Rate: 0.000433406
	LOSS [training: 0.0029747989556197616 | validation: -0.004622315128300307]
	TIME [epoch: 8.16 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021130281155794977		[learning rate: 0.00043288]
		[batch 20/20] avg loss: 0.003028272964262024		[learning rate: 0.00043236]
	Learning Rate: 0.000432357
	LOSS [training: 0.0025706505399207607 | validation: -0.00384021342406362]
	TIME [epoch: 8.15 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014661825213666128		[learning rate: 0.00043183]
		[batch 20/20] avg loss: 0.0022574193204937124		[learning rate: 0.00043131]
	Learning Rate: 0.00043131
	LOSS [training: 0.0018618009209301627 | validation: -0.005005371114958537]
	TIME [epoch: 8.17 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005328520243165059		[learning rate: 0.00043079]
		[batch 20/20] avg loss: -0.0010687071233234414		[learning rate: 0.00043027]
	Learning Rate: 0.000430266
	LOSS [training: 0.002129906559920809 | validation: -0.007995403823308827]
	TIME [epoch: 8.18 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001292490735889009		[learning rate: 0.00042974]
		[batch 20/20] avg loss: 0.007679920943493781		[learning rate: 0.00042922]
	Learning Rate: 0.000429224
	LOSS [training: 0.004486205839691395 | validation: -0.0058854480559063956]
	TIME [epoch: 8.19 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011564801855979026		[learning rate: 0.0004287]
		[batch 20/20] avg loss: 0.003142030960674419		[learning rate: 0.00042819]
	Learning Rate: 0.000428185
	LOSS [training: 0.0021492555731361606 | validation: -0.017281750178180073]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_1400.pth
	Model improved!!!
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007685504236937965		[learning rate: 0.00042767]
		[batch 20/20] avg loss: 0.003188541753624918		[learning rate: 0.00042715]
	Learning Rate: 0.000427149
	LOSS [training: 0.005437022995281442 | validation: -0.0023860116457624456]
	TIME [epoch: 8.18 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007021548419867759		[learning rate: 0.00042663]
		[batch 20/20] avg loss: 0.0013813656119722223		[learning rate: 0.00042611]
	Learning Rate: 0.000426115
	LOSS [training: 0.00420145701591999 | validation: -0.007195310752290692]
	TIME [epoch: 8.21 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01185608473524125		[learning rate: 0.0004256]
		[batch 20/20] avg loss: 0.002889075312057919		[learning rate: 0.00042508]
	Learning Rate: 0.000425083
	LOSS [training: 0.007372580023649583 | validation: -0.014584151894014935]
	TIME [epoch: 8.19 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008618207628835289		[learning rate: 0.00042457]
		[batch 20/20] avg loss: 0.006186941162532657		[learning rate: 0.00042405]
	Learning Rate: 0.000424054
	LOSS [training: 0.007402574395683972 | validation: -0.007260217403531855]
	TIME [epoch: 8.17 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032822566154683216		[learning rate: 0.00042354]
		[batch 20/20] avg loss: 0.0041295102547333744		[learning rate: 0.00042303]
	Learning Rate: 0.000423027
	LOSS [training: 0.003705883435100847 | validation: 0.007268687876075496]
	TIME [epoch: 8.17 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008212717475314293		[learning rate: 0.00042251]
		[batch 20/20] avg loss: 0.012294291786668939		[learning rate: 0.000422]
	Learning Rate: 0.000422003
	LOSS [training: 0.010253504630991618 | validation: -0.006908894245353167]
	TIME [epoch: 8.19 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003882006738605742		[learning rate: 0.00042149]
		[batch 20/20] avg loss: 0.00425589546262269		[learning rate: 0.00042098]
	Learning Rate: 0.000420982
	LOSS [training: 0.004068951100614215 | validation: -0.005332615584502928]
	TIME [epoch: 8.18 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004444311302404493		[learning rate: 0.00042047]
		[batch 20/20] avg loss: 0.005963851548241467		[learning rate: 0.00041996]
	Learning Rate: 0.000419963
	LOSS [training: 0.00520408142532298 | validation: -0.013535093826857249]
	TIME [epoch: 8.17 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004435411336116553		[learning rate: 0.00041945]
		[batch 20/20] avg loss: 0.006365767610156556		[learning rate: 0.00041895]
	Learning Rate: 0.000418946
	LOSS [training: 0.005400589473136553 | validation: -0.010210064595950887]
	TIME [epoch: 8.17 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0061510856502697804		[learning rate: 0.00041844]
		[batch 20/20] avg loss: 0.00536056204382547		[learning rate: 0.00041793]
	Learning Rate: 0.000417932
	LOSS [training: 0.005755823847047626 | validation: -0.010911323771965218]
	TIME [epoch: 8.21 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004390597508088		[learning rate: 0.00041743]
		[batch 20/20] avg loss: 0.0016071670086291612		[learning rate: 0.00041692]
	Learning Rate: 0.00041692
	LOSS [training: 0.0029988822583585815 | validation: -0.005256244598824816]
	TIME [epoch: 8.2 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021872184984074593		[learning rate: 0.00041641]
		[batch 20/20] avg loss: 0.005098013862399002		[learning rate: 0.00041591]
	Learning Rate: 0.000415911
	LOSS [training: 0.003642616180403231 | validation: -0.010793655335100075]
	TIME [epoch: 8.18 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024028281205430197		[learning rate: 0.00041541]
		[batch 20/20] avg loss: 0.005163084700690912		[learning rate: 0.0004149]
	Learning Rate: 0.000414904
	LOSS [training: 0.0013801282900739462 | validation: -0.013391927216068744]
	TIME [epoch: 8.18 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002037908105528932		[learning rate: 0.0004144]
		[batch 20/20] avg loss: 0.012166560520544583		[learning rate: 0.0004139]
	Learning Rate: 0.000413899
	LOSS [training: 0.005064326207507827 | validation: -0.0020557822669772345]
	TIME [epoch: 8.21 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.659007083391216e-05		[learning rate: 0.0004134]
		[batch 20/20] avg loss: 0.0027824834254952624		[learning rate: 0.0004129]
	Learning Rate: 0.000412897
	LOSS [training: 0.0014345367481645873 | validation: -0.009059883369799427]
	TIME [epoch: 8.2 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003358911674620981		[learning rate: 0.0004124]
		[batch 20/20] avg loss: -0.00013130458557047888		[learning rate: 0.0004119]
	Learning Rate: 0.000411898
	LOSS [training: 0.001613803544525251 | validation: -0.014146958279448327]
	TIME [epoch: 8.17 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005648474524243273		[learning rate: 0.0004114]
		[batch 20/20] avg loss: -0.0005046235449096961		[learning rate: 0.0004109]
	Learning Rate: 0.000410901
	LOSS [training: 0.002571925489666788 | validation: -0.0028119156139020996]
	TIME [epoch: 8.16 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005994860604705662		[learning rate: 0.0004104]
		[batch 20/20] avg loss: 0.008612814415751386		[learning rate: 0.00040991]
	Learning Rate: 0.000409906
	LOSS [training: 0.007303837510228524 | validation: -0.009101783706811362]
	TIME [epoch: 8.17 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034573310300465717		[learning rate: 0.00040941]
		[batch 20/20] avg loss: 0.004832644041683086		[learning rate: 0.00040891]
	Learning Rate: 0.000408914
	LOSS [training: 0.004144987535864829 | validation: -0.006654919554512283]
	TIME [epoch: 8.18 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006430643375770022		[learning rate: 0.00040842]
		[batch 20/20] avg loss: 0.004308958689180842		[learning rate: 0.00040792]
	Learning Rate: 0.000407924
	LOSS [training: 0.005369801032475432 | validation: -0.015246494438706647]
	TIME [epoch: 8.18 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010382398442335634		[learning rate: 0.00040743]
		[batch 20/20] avg loss: 0.0063706001164385625		[learning rate: 0.00040694]
	Learning Rate: 0.000406936
	LOSS [training: 0.0083764992793871 | validation: -0.005194841557922537]
	TIME [epoch: 8.17 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0070167748270487645		[learning rate: 0.00040644]
		[batch 20/20] avg loss: 0.005352872722728363		[learning rate: 0.00040595]
	Learning Rate: 0.000405951
	LOSS [training: 0.006184823774888564 | validation: -0.00691740483199471]
	TIME [epoch: 8.17 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008963386137655616		[learning rate: 0.00040546]
		[batch 20/20] avg loss: 0.007179618282894251		[learning rate: 0.00040497]
	Learning Rate: 0.000404968
	LOSS [training: 0.008071502210274934 | validation: -0.010847704132258546]
	TIME [epoch: 8.21 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009457633885233628		[learning rate: 0.00040448]
		[batch 20/20] avg loss: 0.0016686072668506864		[learning rate: 0.00040399]
	Learning Rate: 0.000403988
	LOSS [training: 0.005563120576042158 | validation: -0.006114566204513065]
	TIME [epoch: 8.21 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008404093697101744		[learning rate: 0.0004035]
		[batch 20/20] avg loss: 0.003100426229594319		[learning rate: 0.00040301]
	Learning Rate: 0.00040301
	LOSS [training: 0.005752259963348032 | validation: -0.012382284445021222]
	TIME [epoch: 8.19 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008514475210906668		[learning rate: 0.00040252]
		[batch 20/20] avg loss: 0.003345361838236552		[learning rate: 0.00040203]
	Learning Rate: 0.000402034
	LOSS [training: 0.00592991852457161 | validation: -0.008432153139566578]
	TIME [epoch: 8.21 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004310709780602202		[learning rate: 0.00040155]
		[batch 20/20] avg loss: 0.0006113102836811335		[learning rate: 0.00040106]
	Learning Rate: 0.000401061
	LOSS [training: 0.0024610100321416683 | validation: -0.0016767677365624018]
	TIME [epoch: 8.21 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005038373144245778		[learning rate: 0.00040058]
		[batch 20/20] avg loss: 0.00036443767293498373		[learning rate: 0.00040009]
	Learning Rate: 0.00040009
	LOSS [training: 0.0027014054085903808 | validation: -0.005743190508686328]
	TIME [epoch: 8.19 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00642774439623859		[learning rate: 0.00039961]
		[batch 20/20] avg loss: 0.0008090415842827277		[learning rate: 0.00039912]
	Learning Rate: 0.000399122
	LOSS [training: 0.0036183929902606584 | validation: -0.008778445291705347]
	TIME [epoch: 8.18 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0054885794093504445		[learning rate: 0.00039864]
		[batch 20/20] avg loss: -0.0009691861226679641		[learning rate: 0.00039816]
	Learning Rate: 0.000398155
	LOSS [training: 0.00225969664334124 | validation: -0.0032850851403267538]
	TIME [epoch: 8.17 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008829320698528337		[learning rate: 0.00039767]
		[batch 20/20] avg loss: 0.0026512279709270843		[learning rate: 0.00039719]
	Learning Rate: 0.000397192
	LOSS [training: 0.00574027433472771 | validation: -0.01124365686877933]
	TIME [epoch: 8.18 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028379417439994598		[learning rate: 0.00039671]
		[batch 20/20] avg loss: 0.005969214682134992		[learning rate: 0.00039623]
	Learning Rate: 0.00039623
	LOSS [training: 0.004403578213067225 | validation: -0.007278287254325571]
	TIME [epoch: 8.18 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004421685284608077		[learning rate: 0.00039575]
		[batch 20/20] avg loss: 0.005421477793195382		[learning rate: 0.00039527]
	Learning Rate: 0.000395271
	LOSS [training: 0.004921581538901729 | validation: -0.005103758339627554]
	TIME [epoch: 8.2 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006834024456446491		[learning rate: 0.00039479]
		[batch 20/20] avg loss: 0.0014120615033272484		[learning rate: 0.00039431]
	Learning Rate: 0.000394314
	LOSS [training: 0.00412304297988687 | validation: -0.003733318063506949]
	TIME [epoch: 8.18 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006383905739311864		[learning rate: 0.00039384]
		[batch 20/20] avg loss: 0.00660049663643089		[learning rate: 0.00039336]
	Learning Rate: 0.000393359
	LOSS [training: 0.006492201187871377 | validation: 0.0008304320053061986]
	TIME [epoch: 8.18 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007745662689527621		[learning rate: 0.00039288]
		[batch 20/20] avg loss: 0.011140471415648947		[learning rate: 0.00039241]
	Learning Rate: 0.000392407
	LOSS [training: 0.009443067052588282 | validation: -0.0065085372809885]
	TIME [epoch: 8.19 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008297371547615414		[learning rate: 0.00039193]
		[batch 20/20] avg loss: 0.00707538148801313		[learning rate: 0.00039146]
	Learning Rate: 0.000391457
	LOSS [training: 0.007686376517814274 | validation: -0.008947426861673442]
	TIME [epoch: 8.22 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0039392032082601345		[learning rate: 0.00039098]
		[batch 20/20] avg loss: 0.006731754623455303		[learning rate: 0.00039051]
	Learning Rate: 0.00039051
	LOSS [training: 0.005335478915857719 | validation: -0.005628989914632366]
	TIME [epoch: 8.18 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007134990132683552		[learning rate: 0.00039004]
		[batch 20/20] avg loss: 0.006341205498747322		[learning rate: 0.00038956]
	Learning Rate: 0.000389564
	LOSS [training: 0.006738097815715438 | validation: -0.0036566976486078267]
	TIME [epoch: 8.18 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007239592014074148		[learning rate: 0.00038909]
		[batch 20/20] avg loss: 0.00036908982872905464		[learning rate: 0.00038862]
	Learning Rate: 0.000388621
	LOSS [training: 0.0038043409214016015 | validation: -0.009362316754949855]
	TIME [epoch: 8.22 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007683410588868803		[learning rate: 0.00038815]
		[batch 20/20] avg loss: -0.001456333310050692		[learning rate: 0.00038768]
	Learning Rate: 0.00038768
	LOSS [training: 0.0031135386394090547 | validation: -0.009899814273472661]
	TIME [epoch: 8.18 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027961758755939425		[learning rate: 0.00038721]
		[batch 20/20] avg loss: 0.0028852670406599515		[learning rate: 0.00038674]
	Learning Rate: 0.000386742
	LOSS [training: 0.002840721458126947 | validation: -0.007110567728768895]
	TIME [epoch: 8.2 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022798680371740876		[learning rate: 0.00038627]
		[batch 20/20] avg loss: 0.00327678655140832		[learning rate: 0.00038581]
	Learning Rate: 0.000385805
	LOSS [training: 0.002778327294291204 | validation: -0.014167238887829108]
	TIME [epoch: 8.16 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006882173574306853		[learning rate: 0.00038534]
		[batch 20/20] avg loss: 0.004035602600446669		[learning rate: 0.00038487]
	Learning Rate: 0.000384872
	LOSS [training: 0.005458888087376762 | validation: -0.004018681766072746]
	TIME [epoch: 8.17 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0001398305585669132		[learning rate: 0.00038441]
		[batch 20/20] avg loss: 0.0031395082839935466		[learning rate: 0.00038394]
	Learning Rate: 0.00038394
	LOSS [training: 0.00163966942128023 | validation: -0.0022903714422118876]
	TIME [epoch: 8.17 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003604522492910303		[learning rate: 0.00038347]
		[batch 20/20] avg loss: 0.00047876974306533093		[learning rate: 0.00038301]
	Learning Rate: 0.00038301
	LOSS [training: 0.0020416461179878164 | validation: -0.007745954193170144]
	TIME [epoch: 8.19 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024385962314462757		[learning rate: 0.00038255]
		[batch 20/20] avg loss: 0.006151268657190666		[learning rate: 0.00038208]
	Learning Rate: 0.000382083
	LOSS [training: 0.0018563362128721952 | validation: -0.0003793884365495061]
	TIME [epoch: 8.17 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015314707949398785		[learning rate: 0.00038162]
		[batch 20/20] avg loss: 0.005112957312186953		[learning rate: 0.00038116]
	Learning Rate: 0.000381158
	LOSS [training: 0.0017907432586235367 | validation: -0.011406433877249678]
	TIME [epoch: 8.2 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037254954951622976		[learning rate: 0.0003807]
		[batch 20/20] avg loss: 0.0020053209259403896		[learning rate: 0.00038024]
	Learning Rate: 0.000380235
	LOSS [training: 0.0028654082105513436 | validation: -0.011480275619560514]
	TIME [epoch: 8.18 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008692785008923129		[learning rate: 0.00037977]
		[batch 20/20] avg loss: 0.006281322371369818		[learning rate: 0.00037931]
	Learning Rate: 0.000379315
	LOSS [training: 0.0027060219352387526 | validation: -0.009869941008923903]
	TIME [epoch: 8.2 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007944525196118175		[learning rate: 0.00037886]
		[batch 20/20] avg loss: 0.003571155616531691		[learning rate: 0.0003784]
	Learning Rate: 0.000378397
	LOSS [training: 0.001388351548459937 | validation: 0.0005709469692298718]
	TIME [epoch: 8.2 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006527941909814589		[learning rate: 0.00037794]
		[batch 20/20] avg loss: 0.0007253363740310093		[learning rate: 0.00037748]
	Learning Rate: 0.000377481
	LOSS [training: 0.0036266391419227982 | validation: -0.013246539814826335]
	TIME [epoch: 8.21 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005714797335255305		[learning rate: 0.00037702]
		[batch 20/20] avg loss: 0.00375729847829789		[learning rate: 0.00037657]
	Learning Rate: 0.000376567
	LOSS [training: 0.0047360479067765976 | validation: -0.005387430807080483]
	TIME [epoch: 8.18 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004141978926739237		[learning rate: 0.00037611]
		[batch 20/20] avg loss: 0.009788105856133647		[learning rate: 0.00037566]
	Learning Rate: 0.000375655
	LOSS [training: 0.006965042391436442 | validation: -0.0032370335816112278]
	TIME [epoch: 8.17 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007104977242664075		[learning rate: 0.0003752]
		[batch 20/20] avg loss: 0.003037424795713382		[learning rate: 0.00037475]
	Learning Rate: 0.000374746
	LOSS [training: 0.005071201019188729 | validation: -0.007621893816170422]
	TIME [epoch: 8.2 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004503248622485068		[learning rate: 0.00037429]
		[batch 20/20] avg loss: 0.003206773777103637		[learning rate: 0.00037384]
	Learning Rate: 0.000373839
	LOSS [training: 0.003855011199794353 | validation: -0.016585897179694803]
	TIME [epoch: 8.17 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004272517380166077		[learning rate: 0.00037339]
		[batch 20/20] avg loss: 0.005692618458035098		[learning rate: 0.00037293]
	Learning Rate: 0.000372934
	LOSS [training: 0.0007100505389345103 | validation: -0.006372101257519078]
	TIME [epoch: 8.16 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031869185603556455		[learning rate: 0.00037248]
		[batch 20/20] avg loss: 0.005218154518551838		[learning rate: 0.00037203]
	Learning Rate: 0.000372031
	LOSS [training: 0.004202536539453742 | validation: -0.007878221427503457]
	TIME [epoch: 8.17 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008529602547284212		[learning rate: 0.00037158]
		[batch 20/20] avg loss: 0.004147993105694302		[learning rate: 0.00037113]
	Learning Rate: 0.00037113
	LOSS [training: 0.006338797826489257 | validation: -0.011478386251028892]
	TIME [epoch: 8.19 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0047494207767637825		[learning rate: 0.00037068]
		[batch 20/20] avg loss: 0.009208119277083565		[learning rate: 0.00037023]
	Learning Rate: 0.000370232
	LOSS [training: 0.006978770026923674 | validation: -0.0007499107846698346]
	TIME [epoch: 8.18 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00023565352460411978		[learning rate: 0.00036978]
		[batch 20/20] avg loss: 0.002817148126123129		[learning rate: 0.00036934]
	Learning Rate: 0.000369336
	LOSS [training: 0.0015264008253636247 | validation: -0.005378333623643049]
	TIME [epoch: 8.18 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004039344500091096		[learning rate: 0.00036889]
		[batch 20/20] avg loss: 0.006161070648699507		[learning rate: 0.00036844]
	Learning Rate: 0.000368441
	LOSS [training: 0.005100207574395302 | validation: -0.0006969686387168461]
	TIME [epoch: 8.18 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00570251734500221		[learning rate: 0.000368]
		[batch 20/20] avg loss: 0.00670682996272209		[learning rate: 0.00036755]
	Learning Rate: 0.00036755
	LOSS [training: 0.00620467365386215 | validation: -0.004273887196063937]
	TIME [epoch: 8.21 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002915792897968169		[learning rate: 0.0003671]
		[batch 20/20] avg loss: 0.002996551600197387		[learning rate: 0.00036666]
	Learning Rate: 0.00036666
	LOSS [training: 0.0029561722490827782 | validation: -0.011253161577826621]
	TIME [epoch: 8.21 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032406903671265536		[learning rate: 0.00036622]
		[batch 20/20] avg loss: 0.004172264452792574		[learning rate: 0.00036577]
	Learning Rate: 0.000365772
	LOSS [training: 0.003706477409959564 | validation: -0.007290665188047596]
	TIME [epoch: 8.18 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008351335228635928		[learning rate: 0.00036533]
		[batch 20/20] avg loss: 0.002240329066802453		[learning rate: 0.00036489]
	Learning Rate: 0.000364887
	LOSS [training: 0.0052958321477191905 | validation: -0.013170370680762605]
	TIME [epoch: 8.2 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003636703975272755		[learning rate: 0.00036444]
		[batch 20/20] avg loss: 0.006085502124750708		[learning rate: 0.000364]
	Learning Rate: 0.000364003
	LOSS [training: 0.0032245862611389917 | validation: -0.010581037348936602]
	TIME [epoch: 8.2 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00307771479402192		[learning rate: 0.00036356]
		[batch 20/20] avg loss: 0.012019970341601122		[learning rate: 0.00036312]
	Learning Rate: 0.000363122
	LOSS [training: 0.007548842567811522 | validation: 0.0014198084916985498]
	TIME [epoch: 8.2 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014808049530508033		[learning rate: 0.00036268]
		[batch 20/20] avg loss: 0.0014044539069022854		[learning rate: 0.00036224]
	Learning Rate: 0.000362243
	LOSS [training: 0.0014426294299765441 | validation: -0.0035700598821123025]
	TIME [epoch: 8.18 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01075796191012534		[learning rate: 0.0003618]
		[batch 20/20] avg loss: 0.004136455699082898		[learning rate: 0.00036137]
	Learning Rate: 0.000361366
	LOSS [training: 0.007447208804604118 | validation: -0.0023656757356941218]
	TIME [epoch: 8.18 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0056284785456532675		[learning rate: 0.00036093]
		[batch 20/20] avg loss: 0.00573800016906844		[learning rate: 0.00036049]
	Learning Rate: 0.000360491
	LOSS [training: 0.005683239357360854 | validation: -0.009525404293568535]
	TIME [epoch: 8.18 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008964583334486177		[learning rate: 0.00036005]
		[batch 20/20] avg loss: 0.007899278499332942		[learning rate: 0.00035962]
	Learning Rate: 0.000359619
	LOSS [training: 0.008431930916909558 | validation: -0.0036067844667854714]
	TIME [epoch: 8.19 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00546830430391106		[learning rate: 0.00035918]
		[batch 20/20] avg loss: 0.0032821483071308042		[learning rate: 0.00035875]
	Learning Rate: 0.000358748
	LOSS [training: 0.004375226305520933 | validation: -2.262565436272836e-06]
	TIME [epoch: 8.22 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0060930680116611105		[learning rate: 0.00035831]
		[batch 20/20] avg loss: 0.008844237461817414		[learning rate: 0.00035788]
	Learning Rate: 0.00035788
	LOSS [training: 0.007468652736739262 | validation: -0.006048143328504582]
	TIME [epoch: 8.18 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004220139462008132		[learning rate: 0.00035745]
		[batch 20/20] avg loss: 0.0015253874975441218		[learning rate: 0.00035701]
	Learning Rate: 0.000357013
	LOSS [training: -0.0013473759822320056 | validation: -0.0021483529309267897]
	TIME [epoch: 8.18 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006894389955261236		[learning rate: 0.00035658]
		[batch 20/20] avg loss: 0.004119084874062118		[learning rate: 0.00035615]
	Learning Rate: 0.000356149
	LOSS [training: 0.002404261934794121 | validation: -0.012127624118426925]
	TIME [epoch: 8.18 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008706520858517707		[learning rate: 0.00035572]
		[batch 20/20] avg loss: 0.0015646743101285637		[learning rate: 0.00035529]
	Learning Rate: 0.000355287
	LOSS [training: 0.0051355975843231354 | validation: -0.011011169566356633]
	TIME [epoch: 8.24 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004268718681276293		[learning rate: 0.00035486]
		[batch 20/20] avg loss: 0.0040993196452789515		[learning rate: 0.00035443]
	Learning Rate: 0.000354427
	LOSS [training: 0.004184019163277622 | validation: -0.005546685178022516]
	TIME [epoch: 8.18 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004478019346271992		[learning rate: 0.000354]
		[batch 20/20] avg loss: 0.003836435439655677		[learning rate: 0.00035357]
	Learning Rate: 0.000353569
	LOSS [training: 0.004157227392963835 | validation: -0.005661571237523536]
	TIME [epoch: 8.18 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006851457390121748		[learning rate: 0.00035314]
		[batch 20/20] avg loss: 0.004203997470972198		[learning rate: 0.00035271]
	Learning Rate: 0.000352713
	LOSS [training: 0.005527727430546974 | validation: 0.0003950307239010808]
	TIME [epoch: 8.17 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005023727764494414		[learning rate: 0.00035229]
		[batch 20/20] avg loss: 0.001253156459032754		[learning rate: 0.00035186]
	Learning Rate: 0.000351859
	LOSS [training: 0.0031384421117635836 | validation: -0.0055888346533620094]
	TIME [epoch: 8.19 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003476162068828264		[learning rate: 0.00035143]
		[batch 20/20] avg loss: 0.003929271963772961		[learning rate: 0.00035101]
	Learning Rate: 0.000351007
	LOSS [training: 0.0037027170163006122 | validation: -0.0077528140124771975]
	TIME [epoch: 8.18 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018778640123417929		[learning rate: 0.00035058]
		[batch 20/20] avg loss: 0.009433325716989238		[learning rate: 0.00035016]
	Learning Rate: 0.000350157
	LOSS [training: 0.005655594864665515 | validation: -0.01716758445166712]
	TIME [epoch: 8.18 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036366849432401794		[learning rate: 0.00034973]
		[batch 20/20] avg loss: 0.008942895453752224		[learning rate: 0.00034931]
	Learning Rate: 0.00034931
	LOSS [training: 0.002653105255256023 | validation: -0.008545128591416845]
	TIME [epoch: 8.18 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005014423525767526		[learning rate: 0.00034889]
		[batch 20/20] avg loss: 0.003650977087099419		[learning rate: 0.00034846]
	Learning Rate: 0.000348464
	LOSS [training: 0.0043327003064334724 | validation: -0.0032091355244388206]
	TIME [epoch: 8.17 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007081996397564858		[learning rate: 0.00034804]
		[batch 20/20] avg loss: 0.0004119510045735477		[learning rate: 0.00034762]
	Learning Rate: 0.00034762
	LOSS [training: 0.0037469737010692025 | validation: -0.00482130367600855]
	TIME [epoch: 8.22 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004967417437286882		[learning rate: 0.0003472]
		[batch 20/20] avg loss: 0.0024827466926274355		[learning rate: 0.00034678]
	Learning Rate: 0.000346779
	LOSS [training: 0.0037250820649571597 | validation: -0.004595434694198885]
	TIME [epoch: 8.18 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008161136016788943		[learning rate: 0.00034636]
		[batch 20/20] avg loss: 0.0005821217810416349		[learning rate: 0.00034594]
	Learning Rate: 0.000345939
	LOSS [training: 0.004371628898915289 | validation: -0.005733489410390116]
	TIME [epoch: 8.19 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006405003815885024		[learning rate: 0.00034552]
		[batch 20/20] avg loss: 0.0024886271175662716		[learning rate: 0.0003451]
	Learning Rate: 0.000345102
	LOSS [training: 0.004446815466725648 | validation: -0.007492124797144365]
	TIME [epoch: 8.18 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024382092398742014		[learning rate: 0.00034468]
		[batch 20/20] avg loss: 0.007011580032634984		[learning rate: 0.00034427]
	Learning Rate: 0.000344267
	LOSS [training: 0.004724894636254594 | validation: -0.00817604577369659]
	TIME [epoch: 8.24 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003433643800087101		[learning rate: 0.00034385]
		[batch 20/20] avg loss: 0.00012972708231280238		[learning rate: 0.00034343]
	Learning Rate: 0.000343433
	LOSS [training: -0.00010681864884795386 | validation: -0.0036867936961441734]
	TIME [epoch: 8.17 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004028960773321702		[learning rate: 0.00034302]
		[batch 20/20] avg loss: 0.0057690670674101895		[learning rate: 0.0003426]
	Learning Rate: 0.000342602
	LOSS [training: 0.004899013920365946 | validation: -0.004548359037703353]
	TIME [epoch: 8.17 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009092710653417803		[learning rate: 0.00034219]
		[batch 20/20] avg loss: 0.0049065785661714205		[learning rate: 0.00034177]
	Learning Rate: 0.000341772
	LOSS [training: 0.0069996446097946126 | validation: -0.003987519040587356]
	TIME [epoch: 8.17 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031639979453279574		[learning rate: 0.00034136]
		[batch 20/20] avg loss: 0.004238438855218264		[learning rate: 0.00034094]
	Learning Rate: 0.000340945
	LOSS [training: 0.0037012184002731103 | validation: -0.004111716458988664]
	TIME [epoch: 8.17 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012901688431196383		[learning rate: 0.00034053]
		[batch 20/20] avg loss: 0.005750802818355448		[learning rate: 0.00034012]
	Learning Rate: 0.00034012
	LOSS [training: 0.003520485830737543 | validation: -0.0073378924542463735]
	TIME [epoch: 8.2 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008322010773157982		[learning rate: 0.00033971]
		[batch 20/20] avg loss: 0.002677760101747865		[learning rate: 0.0003393]
	Learning Rate: 0.000339296
	LOSS [training: 0.005499885437452923 | validation: -0.007248845096774501]
	TIME [epoch: 8.17 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005745855258881574		[learning rate: 0.00033889]
		[batch 20/20] avg loss: -0.002828979119910537		[learning rate: 0.00033847]
	Learning Rate: 0.000338475
	LOSS [training: 0.0014584380694855187 | validation: -0.006527170572472369]
	TIME [epoch: 8.21 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017647596991955676		[learning rate: 0.00033806]
		[batch 20/20] avg loss: 0.005099884644351062		[learning rate: 0.00033766]
	Learning Rate: 0.000337655
	LOSS [training: 0.0034323221717733146 | validation: -0.012485824799979275]
	TIME [epoch: 8.19 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00010497687953001656		[learning rate: 0.00033725]
		[batch 20/20] avg loss: 0.0016325642301785975		[learning rate: 0.00033684]
	Learning Rate: 0.000336838
	LOSS [training: 0.0007637936753242905 | validation: -0.0046975935624148785]
	TIME [epoch: 8.21 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028929362882873784		[learning rate: 0.00033643]
		[batch 20/20] avg loss: -0.0017362749975387594		[learning rate: 0.00033602]
	Learning Rate: 0.000336023
	LOSS [training: 0.0005783306453743097 | validation: -0.010175430542212512]
	TIME [epoch: 8.19 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004031585101288813		[learning rate: 0.00033562]
		[batch 20/20] avg loss: 0.00520053979946874		[learning rate: 0.00033521]
	Learning Rate: 0.000335209
	LOSS [training: 0.0046160624503787774 | validation: -0.007790214005894347]
	TIME [epoch: 8.23 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031245404966478785		[learning rate: 0.0003348]
		[batch 20/20] avg loss: 0.000985537434699837		[learning rate: 0.0003344]
	Learning Rate: 0.000334398
	LOSS [training: 0.002055038965673858 | validation: -0.004702121644476661]
	TIME [epoch: 8.18 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024729401796477523		[learning rate: 0.00033399]
		[batch 20/20] avg loss: 0.002806938437303866		[learning rate: 0.00033359]
	Learning Rate: 0.000333588
	LOSS [training: 0.0026399393084758087 | validation: -0.01423451418833154]
	TIME [epoch: 8.19 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002031805977085614		[learning rate: 0.00033318]
		[batch 20/20] avg loss: 0.0030619808089895964		[learning rate: 0.00033278]
	Learning Rate: 0.000332781
	LOSS [training: 0.0005150874159519917 | validation: -0.012112868020551252]
	TIME [epoch: 8.18 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003813130912568602		[learning rate: 0.00033238]
		[batch 20/20] avg loss: 0.0016511883490052878		[learning rate: 0.00033197]
	Learning Rate: 0.000331975
	LOSS [training: 0.0027321596307869453 | validation: -0.00634294606589612]
	TIME [epoch: 8.17 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004226050542127004		[learning rate: 0.00033157]
		[batch 20/20] avg loss: 0.0010642831707376606		[learning rate: 0.00033117]
	Learning Rate: 0.000331171
	LOSS [training: 0.0026451668564323326 | validation: -0.009686647174706476]
	TIME [epoch: 8.17 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008272655439676296		[learning rate: 0.00033077]
		[batch 20/20] avg loss: 0.003937045869791407		[learning rate: 0.00033037]
	Learning Rate: 0.00033037
	LOSS [training: 0.006104850654733851 | validation: 0.002120693016716111]
	TIME [epoch: 8.18 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007162113977520539		[learning rate: 0.00032997]
		[batch 20/20] avg loss: 0.008893659061575684		[learning rate: 0.00032957]
	Learning Rate: 0.00032957
	LOSS [training: 0.00802788651954811 | validation: 0.004300846671438785]
	TIME [epoch: 8.23 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017545606039468717		[learning rate: 0.00032917]
		[batch 20/20] avg loss: 0.004292034828726366		[learning rate: 0.00032877]
	Learning Rate: 0.000328772
	LOSS [training: 0.0030232977163366192 | validation: -0.006497697985496544]
	TIME [epoch: 8.19 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005477671692666388		[learning rate: 0.00032837]
		[batch 20/20] avg loss: -0.001245027173769027		[learning rate: 0.00032798]
	Learning Rate: 0.000327976
	LOSS [training: 0.0021163222594486808 | validation: -0.008142608418902424]
	TIME [epoch: 8.19 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034785211202127573		[learning rate: 0.00032758]
		[batch 20/20] avg loss: 0.0014573940491283647		[learning rate: 0.00032718]
	Learning Rate: 0.000327182
	LOSS [training: 0.002467957584670561 | validation: -0.012370422544768212]
	TIME [epoch: 8.2 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036913470397090973		[learning rate: 0.00032679]
		[batch 20/20] avg loss: 0.003524578880189298		[learning rate: 0.00032639]
	Learning Rate: 0.00032639
	LOSS [training: 0.0036079629599491984 | validation: -0.006923367701634641]
	TIME [epoch: 8.21 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0057871915431753325		[learning rate: 0.00032599]
		[batch 20/20] avg loss: 0.005815254958291906		[learning rate: 0.0003256]
	Learning Rate: 0.0003256
	LOSS [training: 0.00580122325073362 | validation: -0.0015117742874999748]
	TIME [epoch: 8.18 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004440085618899542		[learning rate: 0.00032521]
		[batch 20/20] avg loss: 0.006825836256396859		[learning rate: 0.00032481]
	Learning Rate: 0.000324812
	LOSS [training: 0.0056329609376482 | validation: -0.0038673202600086045]
	TIME [epoch: 8.17 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012007500615159834		[learning rate: 0.00032442]
		[batch 20/20] avg loss: 0.00575397053002076		[learning rate: 0.00032403]
	Learning Rate: 0.000324025
	LOSS [training: 0.008880735572590297 | validation: -0.005027090579046985]
	TIME [epoch: 8.18 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010473673773158597		[learning rate: 0.00032363]
		[batch 20/20] avg loss: 0.006080186370715027		[learning rate: 0.00032324]
	Learning Rate: 0.000323241
	LOSS [training: 0.00827693007193681 | validation: -0.010180997966904567]
	TIME [epoch: 8.18 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006423239654222234		[learning rate: 0.00032285]
		[batch 20/20] avg loss: 0.010695060252979217		[learning rate: 0.00032246]
	Learning Rate: 0.000322458
	LOSS [training: 0.008559149953600725 | validation: -0.0058144184552334785]
	TIME [epoch: 8.2 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0040698871955833086		[learning rate: 0.00032207]
		[batch 20/20] avg loss: -0.0005827775602358938		[learning rate: 0.00032168]
	Learning Rate: 0.000321678
	LOSS [training: 0.0017435548176737074 | validation: -0.014422869094465211]
	TIME [epoch: 8.17 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004778645882011415		[learning rate: 0.00032129]
		[batch 20/20] avg loss: 0.004442567398913149		[learning rate: 0.0003209]
	Learning Rate: 0.000320899
	LOSS [training: 0.004610606640462283 | validation: -0.010621433658634539]
	TIME [epoch: 8.21 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030776014211968683		[learning rate: 0.00032051]
		[batch 20/20] avg loss: 0.0034760800814189878		[learning rate: 0.00032012]
	Learning Rate: 0.000320122
	LOSS [training: 0.003276840751307928 | validation: -0.0048508306297125]
	TIME [epoch: 8.18 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013890711824835464		[learning rate: 0.00031973]
		[batch 20/20] avg loss: 2.2751597359839697e-05		[learning rate: 0.00031935]
	Learning Rate: 0.000319347
	LOSS [training: 0.006956731711097651 | validation: -0.006128215170133714]
	TIME [epoch: 8.21 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006326469690159639		[learning rate: 0.00031896]
		[batch 20/20] avg loss: 0.0016461345712591867		[learning rate: 0.00031857]
	Learning Rate: 0.000318574
	LOSS [training: 0.003986302130709412 | validation: 0.0029815990492400475]
	TIME [epoch: 8.18 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005754900696093532		[learning rate: 0.00031819]
		[batch 20/20] avg loss: 0.008282633521562718		[learning rate: 0.0003178]
	Learning Rate: 0.000317803
	LOSS [training: 0.007018767108828124 | validation: -0.0031532641740749942]
	TIME [epoch: 8.22 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007104734103202096		[learning rate: 0.00031742]
		[batch 20/20] avg loss: 0.007522687854719527		[learning rate: 0.00031703]
	Learning Rate: 0.000317034
	LOSS [training: 0.007313710978960812 | validation: -0.006550882196795522]
	TIME [epoch: 8.18 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008935620582863912		[learning rate: 0.00031665]
		[batch 20/20] avg loss: 0.0034459138623108886		[learning rate: 0.00031627]
	Learning Rate: 0.000316266
	LOSS [training: 0.0061907672225874 | validation: -0.005546382534594559]
	TIME [epoch: 8.19 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006435886014600667		[learning rate: 0.00031588]
		[batch 20/20] avg loss: 0.0014685681281567458		[learning rate: 0.0003155]
	Learning Rate: 0.0003155
	LOSS [training: 0.003952227071378707 | validation: -0.015733666718581583]
	TIME [epoch: 8.18 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004161124039440383		[learning rate: 0.00031512]
		[batch 20/20] avg loss: 0.003666171996274238		[learning rate: 0.00031474]
	Learning Rate: 0.000314737
	LOSS [training: 0.00391364801785731 | validation: -0.007749654152496191]
	TIME [epoch: 8.18 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002044161651182995		[learning rate: 0.00031436]
		[batch 20/20] avg loss: 0.007083535160062786		[learning rate: 0.00031397]
	Learning Rate: 0.000313975
	LOSS [training: 0.00456384840562289 | validation: -0.001665038646716677]
	TIME [epoch: 8.18 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004688139099794727		[learning rate: 0.00031359]
		[batch 20/20] avg loss: 0.007208310966199212		[learning rate: 0.00031321]
	Learning Rate: 0.000313215
	LOSS [training: 0.005948225032996969 | validation: 0.0024546688101706195]
	TIME [epoch: 8.18 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009764522185134837		[learning rate: 0.00031284]
		[batch 20/20] avg loss: 0.005278066193498396		[learning rate: 0.00031246]
	Learning Rate: 0.000312456
	LOSS [training: 0.007521294189316614 | validation: -0.0024769530236247433]
	TIME [epoch: 8.21 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004327416201796309		[learning rate: 0.00031208]
		[batch 20/20] avg loss: 0.0076638960583895145		[learning rate: 0.0003117]
	Learning Rate: 0.0003117
	LOSS [training: 0.005995656130092912 | validation: -0.004273083781656073]
	TIME [epoch: 8.21 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004999993624013073		[learning rate: 0.00031132]
		[batch 20/20] avg loss: 0.0035119256303644776		[learning rate: 0.00031095]
	Learning Rate: 0.000310945
	LOSS [training: 0.004255959627188776 | validation: -0.0037382207463811574]
	TIME [epoch: 8.18 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014140999732317147		[learning rate: 0.00031057]
		[batch 20/20] avg loss: 0.015353903371673546		[learning rate: 0.00031019]
	Learning Rate: 0.000310193
	LOSS [training: 0.00838400167245263 | validation: -0.008346507071255208]
	TIME [epoch: 8.19 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006691385297485403		[learning rate: 0.00030982]
		[batch 20/20] avg loss: 0.0021823387403749643		[learning rate: 0.00030944]
	Learning Rate: 0.000309442
	LOSS [training: 0.0044368620189301836 | validation: -0.011496735297706034]
	TIME [epoch: 8.22 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0079261761981206		[learning rate: 0.00030907]
		[batch 20/20] avg loss: 0.0049871320744201645		[learning rate: 0.00030869]
	Learning Rate: 0.000308693
	LOSS [training: 0.006456654136270381 | validation: -0.00902720011525391]
	TIME [epoch: 8.2 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0052971540293885525		[learning rate: 0.00030832]
		[batch 20/20] avg loss: 0.004195432429583782		[learning rate: 0.00030795]
	Learning Rate: 0.000307945
	LOSS [training: 0.0047462932294861674 | validation: -0.002663380634601209]
	TIME [epoch: 8.17 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005861904448074654		[learning rate: 0.00030757]
		[batch 20/20] avg loss: 0.008678060397027657		[learning rate: 0.0003072]
	Learning Rate: 0.0003072
	LOSS [training: 0.007269982422551155 | validation: -0.009040000135111161]
	TIME [epoch: 8.16 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006100780022096968		[learning rate: 0.00030683]
		[batch 20/20] avg loss: -0.00018129582016534985		[learning rate: 0.00030646]
	Learning Rate: 0.000306456
	LOSS [training: 0.002959742100965809 | validation: -0.009230142349800569]
	TIME [epoch: 8.17 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037799017783169417		[learning rate: 0.00030609]
		[batch 20/20] avg loss: 0.0066608577602424585		[learning rate: 0.00030571]
	Learning Rate: 0.000305714
	LOSS [training: 0.0052203797692797 | validation: -0.007841620748472099]
	TIME [epoch: 8.19 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033260481250046285		[learning rate: 0.00030534]
		[batch 20/20] avg loss: 0.0039303044213064775		[learning rate: 0.00030497]
	Learning Rate: 0.000304974
	LOSS [training: 0.003628176273155553 | validation: -0.013709295916296098]
	TIME [epoch: 8.17 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003760938368093855		[learning rate: 0.0003046]
		[batch 20/20] avg loss: 0.006362090968891907		[learning rate: 0.00030424]
	Learning Rate: 0.000304236
	LOSS [training: 0.005061514668492882 | validation: -0.010906929464885036]
	TIME [epoch: 8.18 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009635563812776645		[learning rate: 0.00030387]
		[batch 20/20] avg loss: -0.0002577218140831975		[learning rate: 0.0003035]
	Learning Rate: 0.000303499
	LOSS [training: 0.0003529172835972333 | validation: -0.005796037209866526]
	TIME [epoch: 8.2 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001304023362280931		[learning rate: 0.00030313]
		[batch 20/20] avg loss: -0.0012586747459496172		[learning rate: 0.00030276]
	Learning Rate: 0.000302765
	LOSS [training: 2.2674308165656926e-05 | validation: -0.007401737286097849]
	TIME [epoch: 8.21 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002907100702544221		[learning rate: 0.0003024]
		[batch 20/20] avg loss: 0.002632622191076708		[learning rate: 0.00030203]
	Learning Rate: 0.000302032
	LOSS [training: 0.0027698614468104643 | validation: -0.011130058395736105]
	TIME [epoch: 8.19 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002459981877861553		[learning rate: 0.00030167]
		[batch 20/20] avg loss: 0.005256168322571779		[learning rate: 0.0003013]
	Learning Rate: 0.000301301
	LOSS [training: 0.003858075100216666 | validation: -0.00348574185711999]
	TIME [epoch: 8.2 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020957270761125845		[learning rate: 0.00030094]
		[batch 20/20] avg loss: 0.006955656250857914		[learning rate: 0.00030057]
	Learning Rate: 0.000300571
	LOSS [training: 0.0045256916634852495 | validation: -0.006366934232800424]
	TIME [epoch: 8.2 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002303348879683368		[learning rate: 0.00030021]
		[batch 20/20] avg loss: 0.004977689698232875		[learning rate: 0.00029984]
	Learning Rate: 0.000299844
	LOSS [training: 0.0013371704092747531 | validation: -0.0127281627891552]
	TIME [epoch: 8.17 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028591836051634766		[learning rate: 0.00029948]
		[batch 20/20] avg loss: -0.0018500983035700136		[learning rate: 0.00029912]
	Learning Rate: 0.000299118
	LOSS [training: 0.0005045426507967313 | validation: -0.008683439983877616]
	TIME [epoch: 8.2 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029617118826993647		[learning rate: 0.00029876]
		[batch 20/20] avg loss: 0.0023867801630311197		[learning rate: 0.00029839]
	Learning Rate: 0.000298394
	LOSS [training: 0.0026742460228652417 | validation: -0.002027250750945795]
	TIME [epoch: 8.17 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007054840696866922		[learning rate: 0.00029803]
		[batch 20/20] avg loss: 0.010210483055067438		[learning rate: 0.00029767]
	Learning Rate: 0.000297671
	LOSS [training: 0.008632661875967182 | validation: -0.010132041927586377]
	TIME [epoch: 8.18 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014039069385025531		[learning rate: 0.00029731]
		[batch 20/20] avg loss: 0.006558349747520958		[learning rate: 0.00029695]
	Learning Rate: 0.000296951
	LOSS [training: 0.003981128343011756 | validation: -0.009808337238744825]
	TIME [epoch: 8.18 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021444793627740585		[learning rate: 0.00029659]
		[batch 20/20] avg loss: 0.008530002316199402		[learning rate: 0.00029623]
	Learning Rate: 0.000296232
	LOSS [training: 0.005337240839486729 | validation: -0.00437912280884875]
	TIME [epoch: 8.19 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010422399266364952		[learning rate: 0.00029587]
		[batch 20/20] avg loss: 0.00443027354951439		[learning rate: 0.00029551]
	Learning Rate: 0.000295515
	LOSS [training: 0.007426336407939669 | validation: -0.01297738667874114]
	TIME [epoch: 8.21 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009614718926236384		[learning rate: 0.00029516]
		[batch 20/20] avg loss: 0.0021148184386911963		[learning rate: 0.0002948]
	Learning Rate: 0.000294799
	LOSS [training: 0.00586476868246379 | validation: -0.008378760025738188]
	TIME [epoch: 8.18 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004135986880013396		[learning rate: 0.00029444]
		[batch 20/20] avg loss: 0.001423209683078836		[learning rate: 0.00029409]
	Learning Rate: 0.000294086
	LOSS [training: 0.002779598281546116 | validation: -0.005081707606945513]
	TIME [epoch: 8.18 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004817269304321669		[learning rate: 0.00029373]
		[batch 20/20] avg loss: 0.002074610737210975		[learning rate: 0.00029337]
	Learning Rate: 0.000293374
	LOSS [training: 0.003445940020766322 | validation: -0.013829826361184357]
	TIME [epoch: 8.2 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00010879333888570657		[learning rate: 0.00029302]
		[batch 20/20] avg loss: 0.0022563929163175378		[learning rate: 0.00029266]
	Learning Rate: 0.000292663
	LOSS [training: 0.0011825931276016222 | validation: -0.010141576680947055]
	TIME [epoch: 8.23 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011189862435360917		[learning rate: 0.00029231]
		[batch 20/20] avg loss: 0.0006801769312348137		[learning rate: 0.00029195]
	Learning Rate: 0.000291955
	LOSS [training: 0.0008995815873854528 | validation: -0.011758419112573719]
	TIME [epoch: 8.18 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: -4.922145072232865e-05		[learning rate: 0.0002916]
		[batch 20/20] avg loss: 0.006580606576906178		[learning rate: 0.00029125]
	Learning Rate: 0.000291248
	LOSS [training: 0.0032656925630919252 | validation: -0.005713755363308663]
	TIME [epoch: 8.18 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007230640573860424		[learning rate: 0.0002909]
		[batch 20/20] avg loss: 0.0017369244287941316		[learning rate: 0.00029054]
	Learning Rate: 0.000290543
	LOSS [training: 0.004483782501327277 | validation: -0.00626950120303092]
	TIME [epoch: 8.18 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012182607431261688		[learning rate: 0.00029019]
		[batch 20/20] avg loss: 0.005286053597089261		[learning rate: 0.00028984]
	Learning Rate: 0.00028984
	LOSS [training: 0.0032521571701077156 | validation: -0.009149899583121242]
	TIME [epoch: 8.19 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009163572816696848		[learning rate: 0.00028949]
		[batch 20/20] avg loss: 0.0013224154907457877		[learning rate: 0.00028914]
	Learning Rate: 0.000289138
	LOSS [training: 0.0011193863862077361 | validation: -0.00941888920233721]
	TIME [epoch: 8.18 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0052726579495566885		[learning rate: 0.00028879]
		[batch 20/20] avg loss: 0.0054628037397656		[learning rate: 0.00028844]
	Learning Rate: 0.000288438
	LOSS [training: 9.507289510445605e-05 | validation: -0.009687071946101182]
	TIME [epoch: 8.21 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004288374313203292		[learning rate: 0.00028809]
		[batch 20/20] avg loss: 0.00025418498200685215		[learning rate: 0.00028774]
	Learning Rate: 0.00028774
	LOSS [training: 0.002271279647605072 | validation: -0.012374344311598066]
	TIME [epoch: 8.19 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006141170158427957		[learning rate: 0.00028739]
		[batch 20/20] avg loss: 0.0012695916012941423		[learning rate: 0.00028704]
	Learning Rate: 0.000287043
	LOSS [training: 0.003705380879861049 | validation: -0.00939681311506392]
	TIME [epoch: 8.2 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031780518727370158		[learning rate: 0.0002867]
		[batch 20/20] avg loss: 0.006025660951765918		[learning rate: 0.00028635]
	Learning Rate: 0.000286348
	LOSS [training: 0.004601856412251468 | validation: -0.0033522756013053072]
	TIME [epoch: 8.18 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006882359495578969		[learning rate: 0.000286]
		[batch 20/20] avg loss: 0.007145460888943844		[learning rate: 0.00028566]
	Learning Rate: 0.000285655
	LOSS [training: 0.007013910192261405 | validation: 0.0060400696429818886]
	TIME [epoch: 8.22 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009340640340372646		[learning rate: 0.00028531]
		[batch 20/20] avg loss: 0.003752611113365064		[learning rate: 0.00028496]
	Learning Rate: 0.000284964
	LOSS [training: 0.0065466257268688545 | validation: -0.011321410597821116]
	TIME [epoch: 8.18 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019757284887504775		[learning rate: 0.00028462]
		[batch 20/20] avg loss: 0.0017821241563542433		[learning rate: 0.00028427]
	Learning Rate: 0.000284274
	LOSS [training: 0.0018789263225523604 | validation: -0.007419994569050249]
	TIME [epoch: 8.18 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00251220394645748		[learning rate: 0.00028393]
		[batch 20/20] avg loss: 0.000717885636567257		[learning rate: 0.00028359]
	Learning Rate: 0.000283586
	LOSS [training: 0.0016150447915123684 | validation: -0.011630988043953105]
	TIME [epoch: 8.19 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004548352680156599		[learning rate: 0.00028324]
		[batch 20/20] avg loss: 0.004528800260797776		[learning rate: 0.0002829]
	Learning Rate: 0.000282899
	LOSS [training: 0.004538576470477188 | validation: -0.0071859596124794825]
	TIME [epoch: 8.17 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022733325523036437		[learning rate: 0.00028256]
		[batch 20/20] avg loss: 0.003402327353378891		[learning rate: 0.00028221]
	Learning Rate: 0.000282214
	LOSS [training: 0.002837829952841267 | validation: -0.011918615559932396]
	TIME [epoch: 8.17 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034556000282038686		[learning rate: 0.00028187]
		[batch 20/20] avg loss: 0.00642299484229202		[learning rate: 0.00028153]
	Learning Rate: 0.000281531
	LOSS [training: 0.004939297435247944 | validation: -0.013510921485883457]
	TIME [epoch: 8.17 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008647740347522663		[learning rate: 0.00028119]
		[batch 20/20] avg loss: 0.005790698522743304		[learning rate: 0.00028085]
	Learning Rate: 0.000280849
	LOSS [training: 0.007219219435132984 | validation: -0.0036757991935603288]
	TIME [epoch: 8.2 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001763068696752894		[learning rate: 0.00028051]
		[batch 20/20] avg loss: 0.003311661829217872		[learning rate: 0.00028017]
	Learning Rate: 0.00028017
	LOSS [training: 0.002537365262985383 | validation: -0.013555799022924921]
	TIME [epoch: 8.18 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00037295133618503104		[learning rate: 0.00027983]
		[batch 20/20] avg loss: 0.0021406604072056732		[learning rate: 0.00027949]
	Learning Rate: 0.000279491
	LOSS [training: 0.0008838545355103211 | validation: -0.013029299402861424]
	TIME [epoch: 8.18 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006774467562049522		[learning rate: 0.00027915]
		[batch 20/20] avg loss: -0.0009091814755635933		[learning rate: 0.00027881]
	Learning Rate: 0.000278815
	LOSS [training: 0.002932643043242964 | validation: -0.008384881692475804]
	TIME [epoch: 8.21 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019759860587515335		[learning rate: 0.00027848]
		[batch 20/20] avg loss: 0.004428874896205157		[learning rate: 0.00027814]
	Learning Rate: 0.00027814
	LOSS [training: 0.0032024304774783455 | validation: -0.012946684145346547]
	TIME [epoch: 8.2 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005518088932452363		[learning rate: 0.0002778]
		[batch 20/20] avg loss: -0.0033649480143084784		[learning rate: 0.00027747]
	Learning Rate: 0.000277467
	LOSS [training: 0.0010765704590719421 | validation: -0.005266720978251843]
	TIME [epoch: 8.2 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007785128862034962		[learning rate: 0.00027713]
		[batch 20/20] avg loss: 0.008400678399646807		[learning rate: 0.00027679]
	Learning Rate: 0.000276795
	LOSS [training: 0.008092903630840883 | validation: -0.0004371395049452742]
	TIME [epoch: 8.2 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008698975156407896		[learning rate: 0.00027646]
		[batch 20/20] avg loss: 0.006905142781595104		[learning rate: 0.00027612]
	Learning Rate: 0.000276125
	LOSS [training: 0.0078020589690015 | validation: -0.009149397819540939]
	TIME [epoch: 8.21 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00043919028616323764		[learning rate: 0.00027579]
		[batch 20/20] avg loss: 0.001448864346811751		[learning rate: 0.00027546]
	Learning Rate: 0.000275456
	LOSS [training: 0.0005048370303242565 | validation: -0.016348849635066033]
	TIME [epoch: 8.18 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0069541577510602234		[learning rate: 0.00027512]
		[batch 20/20] avg loss: -0.0004898428642631292		[learning rate: 0.00027479]
	Learning Rate: 0.000274789
	LOSS [training: 0.0032321574433985477 | validation: -0.012617139342444171]
	TIME [epoch: 8.2 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004396600290101835		[learning rate: 0.00027446]
		[batch 20/20] avg loss: 0.00785967787401987		[learning rate: 0.00027412]
	Learning Rate: 0.000274124
	LOSS [training: 0.006128139082060851 | validation: -0.005812927653759174]
	TIME [epoch: 8.18 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005465794266662309		[learning rate: 0.00027379]
		[batch 20/20] avg loss: 0.006359070879408335		[learning rate: 0.00027346]
	Learning Rate: 0.000273461
	LOSS [training: 0.005912432573035323 | validation: -0.005941963615123889]
	TIME [epoch: 8.18 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006104145805458536		[learning rate: 0.00027313]
		[batch 20/20] avg loss: 0.0045695207959947194		[learning rate: 0.0002728]
	Learning Rate: 0.000272799
	LOSS [training: 0.005336833300726628 | validation: -0.012613599099115858]
	TIME [epoch: 8.18 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018349658325574985		[learning rate: 0.00027247]
		[batch 20/20] avg loss: 0.004053417477659509		[learning rate: 0.00027214]
	Learning Rate: 0.000272138
	LOSS [training: 0.0011092258225510053 | validation: -0.010872272138654597]
	TIME [epoch: 8.2 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027528738339688747		[learning rate: 0.00027181]
		[batch 20/20] avg loss: 0.005682091156329241		[learning rate: 0.00027148]
	Learning Rate: 0.000271479
	LOSS [training: 0.004217482495149057 | validation: -0.009513623510758035]
	TIME [epoch: 8.21 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004584364659168016		[learning rate: 0.00027115]
		[batch 20/20] avg loss: 0.0027216363378340393		[learning rate: 0.00027082]
	Learning Rate: 0.000270822
	LOSS [training: 0.0036530004985010273 | validation: -0.006767697609224952]
	TIME [epoch: 8.19 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020680073656311323		[learning rate: 0.00027049]
		[batch 20/20] avg loss: 0.005315765914226533		[learning rate: 0.00027017]
	Learning Rate: 0.000270167
	LOSS [training: 0.0036918866399288327 | validation: -0.004258888944180023]
	TIME [epoch: 8.19 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007494760860754977		[learning rate: 0.00026984]
		[batch 20/20] avg loss: 0.004465576066652419		[learning rate: 0.00026951]
	Learning Rate: 0.000269513
	LOSS [training: 0.0059801684637037 | validation: -0.0039891385682997]
	TIME [epoch: 8.19 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007889536761079473		[learning rate: 0.00026919]
		[batch 20/20] avg loss: 0.01074276678668047		[learning rate: 0.00026886]
	Learning Rate: 0.00026886
	LOSS [training: 0.005765860231394209 | validation: 0.0034197371315946024]
	TIME [epoch: 8.23 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00674791690792882		[learning rate: 0.00026853]
		[batch 20/20] avg loss: 0.0024404550124314667		[learning rate: 0.00026821]
	Learning Rate: 0.000268209
	LOSS [training: 0.004594185960180145 | validation: -0.00772281244122416]
	TIME [epoch: 8.17 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012532514106356595		[learning rate: 0.00026788]
		[batch 20/20] avg loss: 0.012075104702844303		[learning rate: 0.00026756]
	Learning Rate: 0.00026756
	LOSS [training: 0.0066641780567399805 | validation: -0.002103763613893002]
	TIME [epoch: 8.17 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017292177638639557		[learning rate: 0.00026724]
		[batch 20/20] avg loss: 0.002924311731414231		[learning rate: 0.00026691]
	Learning Rate: 0.000266912
	LOSS [training: 0.0023267647476390933 | validation: -0.00394316828403382]
	TIME [epoch: 8.17 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038183206561981675		[learning rate: 0.00026659]
		[batch 20/20] avg loss: 0.0012572508667116392		[learning rate: 0.00026627]
	Learning Rate: 0.000266266
	LOSS [training: 0.002537785761454903 | validation: -0.014331783554873733]
	TIME [epoch: 8.19 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007834560920059727		[learning rate: 0.00026594]
		[batch 20/20] avg loss: -7.032015430940711e-05		[learning rate: 0.00026562]
	Learning Rate: 0.000265621
	LOSS [training: 0.0038821203828751606 | validation: -0.011186539215040413]
	TIME [epoch: 8.18 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00036402942280899		[learning rate: 0.0002653]
		[batch 20/20] avg loss: 0.005585062500787235		[learning rate: 0.00026498]
	Learning Rate: 0.000264978
	LOSS [training: 0.002610516538989122 | validation: -0.013883295234127327]
	TIME [epoch: 8.2 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013868213878062015		[learning rate: 0.00026466]
		[batch 20/20] avg loss: 0.004212544697370312		[learning rate: 0.00026434]
	Learning Rate: 0.000264337
	LOSS [training: 0.0027996830425882566 | validation: -0.00967910599451877]
	TIME [epoch: 8.2 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004534085947262707		[learning rate: 0.00026402]
		[batch 20/20] avg loss: 0.004556112670787516		[learning rate: 0.0002637]
	Learning Rate: 0.000263697
	LOSS [training: 0.004545099309025112 | validation: -0.0015567149895363995]
	TIME [epoch: 8.2 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007474648976809065		[learning rate: 0.00026338]
		[batch 20/20] avg loss: -0.0030910845022468674		[learning rate: 0.00026306]
	Learning Rate: 0.000263059
	LOSS [training: 0.0021917822372810986 | validation: -0.008092887379116533]
	TIME [epoch: 8.19 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00033746073302005073		[learning rate: 0.00026274]
		[batch 20/20] avg loss: 0.00976415124361594		[learning rate: 0.00026242]
	Learning Rate: 0.000262422
	LOSS [training: 0.004713345255297945 | validation: 0.003189613920084373]
	TIME [epoch: 8.21 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0063091299852065285		[learning rate: 0.0002621]
		[batch 20/20] avg loss: 0.006580060356746636		[learning rate: 0.00026179]
	Learning Rate: 0.000261787
	LOSS [training: 0.006444595170976582 | validation: -0.010167359362448228]
	TIME [epoch: 8.19 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005101077483681409		[learning rate: 0.00026147]
		[batch 20/20] avg loss: 0.0011148185912595553		[learning rate: 0.00026115]
	Learning Rate: 0.000261153
	LOSS [training: 0.003107948037470483 | validation: -0.006749164241344563]
	TIME [epoch: 8.18 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004589906474085364		[learning rate: 0.00026084]
		[batch 20/20] avg loss: 0.004421577901243386		[learning rate: 0.00026052]
	Learning Rate: 0.000260521
	LOSS [training: 0.0045057421876643754 | validation: -0.009099733531575789]
	TIME [epoch: 8.19 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012892725896543988		[learning rate: 0.00026021]
		[batch 20/20] avg loss: 0.003676279646917072		[learning rate: 0.00025989]
	Learning Rate: 0.00025989
	LOSS [training: 0.0024827761182857357 | validation: -0.0030855689056223353]
	TIME [epoch: 8.18 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006012483159362437		[learning rate: 0.00025958]
		[batch 20/20] avg loss: 0.005177760150504729		[learning rate: 0.00025926]
	Learning Rate: 0.000259261
	LOSS [training: 0.005595121654933583 | validation: -0.0023053948864466644]
	TIME [epoch: 8.18 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006029473524215322		[learning rate: 0.00025895]
		[batch 20/20] avg loss: 0.0031150478852627014		[learning rate: 0.00025863]
	Learning Rate: 0.000258633
	LOSS [training: 0.004572260704739011 | validation: -0.005907639887135947]
	TIME [epoch: 8.17 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013486155815637386		[learning rate: 0.00025832]
		[batch 20/20] avg loss: 0.006731246086619796		[learning rate: 0.00025801]
	Learning Rate: 0.000258007
	LOSS [training: 0.004039930834091768 | validation: -0.009269095263038011]
	TIME [epoch: 8.2 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013684620586662927		[learning rate: 0.00025769]
		[batch 20/20] avg loss: 0.004835589114422807		[learning rate: 0.00025738]
	Learning Rate: 0.000257382
	LOSS [training: 0.0031020255865445495 | validation: -0.00537046935062168]
	TIME [epoch: 8.2 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005203155166891281		[learning rate: 0.00025707]
		[batch 20/20] avg loss: 0.0040575446582123125		[learning rate: 0.00025676]
	Learning Rate: 0.000256759
	LOSS [training: 0.004630349912551796 | validation: -0.003917349521127735]
	TIME [epoch: 8.19 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006453956940510708		[learning rate: 0.00025645]
		[batch 20/20] avg loss: 0.0010526495252398506		[learning rate: 0.00025614]
	Learning Rate: 0.000256138
	LOSS [training: 0.0037533032328752798 | validation: -0.0025970881858237206]
	TIME [epoch: 8.18 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009814462405091073		[learning rate: 0.00025583]
		[batch 20/20] avg loss: 0.004956934226464237		[learning rate: 0.00025552]
	Learning Rate: 0.000255518
	LOSS [training: 0.007385698315777655 | validation: -0.006649582552489705]
	TIME [epoch: 8.18 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005170732569397422		[learning rate: 0.00025521]
		[batch 20/20] avg loss: 0.0038956092989600863		[learning rate: 0.0002549]
	Learning Rate: 0.000254899
	LOSS [training: 0.002206341277949914 | validation: -0.00638469320819308]
	TIME [epoch: 8.23 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: -7.404266717158068e-05		[learning rate: 0.00025459]
		[batch 20/20] avg loss: 0.006372314371241408		[learning rate: 0.00025428]
	Learning Rate: 0.000254282
	LOSS [training: 0.0031491358520349135 | validation: -0.010496624839807473]
	TIME [epoch: 8.18 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000434201193819338		[learning rate: 0.00025397]
		[batch 20/20] avg loss: 0.0020276180840898225		[learning rate: 0.00025367]
	Learning Rate: 0.000253667
	LOSS [training: 0.0007967084451352418 | validation: -0.007917563645226389]
	TIME [epoch: 8.17 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002894399472897765		[learning rate: 0.00025336]
		[batch 20/20] avg loss: -0.004535095838411917		[learning rate: 0.00025305]
	Learning Rate: 0.000253052
	LOSS [training: -0.0008203481827570764 | validation: -0.017649859783155]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_1617.pth
	Model improved!!!
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012264879672442314		[learning rate: 0.00025275]
		[batch 20/20] avg loss: 0.0002600839922472331		[learning rate: 0.00025244]
	Learning Rate: 0.00025244
	LOSS [training: 0.0007432859797457325 | validation: -0.005630498335127036]
	TIME [epoch: 8.19 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001169518373972302		[learning rate: 0.00025213]
		[batch 20/20] avg loss: 0.0033633801756779054		[learning rate: 0.00025183]
	Learning Rate: 0.000251829
	LOSS [training: 0.0010969309008528013 | validation: -0.011476353622192553]
	TIME [epoch: 8.17 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.537922828377256e-05		[learning rate: 0.00025152]
		[batch 20/20] avg loss: 0.004306021996404996		[learning rate: 0.00025122]
	Learning Rate: 0.000251219
	LOSS [training: 0.002175700612344384 | validation: -0.011764138445637704]
	TIME [epoch: 8.17 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001879206482297935		[learning rate: 0.00025091]
		[batch 20/20] avg loss: 0.004735141981849712		[learning rate: 0.00025061]
	Learning Rate: 0.000250611
	LOSS [training: 0.0033071742320738237 | validation: -0.005434559366796017]
	TIME [epoch: 8.18 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020782249888492985		[learning rate: 0.00025031]
		[batch 20/20] avg loss: 0.0057699748572469095		[learning rate: 0.00025]
	Learning Rate: 0.000250004
	LOSS [training: 0.003924099923048104 | validation: -0.0064612867010676875]
	TIME [epoch: 8.19 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0046939589325299515		[learning rate: 0.0002497]
		[batch 20/20] avg loss: 0.005312814153472004		[learning rate: 0.0002494]
	Learning Rate: 0.000249399
	LOSS [training: 0.005003386543000978 | validation: -0.0010173795866931673]
	TIME [epoch: 8.18 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00360963528481231		[learning rate: 0.0002491]
		[batch 20/20] avg loss: 0.0024018432435335807		[learning rate: 0.0002488]
	Learning Rate: 0.000248795
	LOSS [training: 0.0030057392641729447 | validation: -0.006020298985700194]
	TIME [epoch: 8.2 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021228973588366673		[learning rate: 0.00024849]
		[batch 20/20] avg loss: 0.004118325018618551		[learning rate: 0.00024819]
	Learning Rate: 0.000248193
	LOSS [training: 0.0009977138298909415 | validation: -0.008485042731027403]
	TIME [epoch: 8.19 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013578717343365305		[learning rate: 0.00024789]
		[batch 20/20] avg loss: 0.003202469745887496		[learning rate: 0.00024759]
	Learning Rate: 0.000247592
	LOSS [training: 0.0022801707401120135 | validation: -0.008746283197384888]
	TIME [epoch: 8.17 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00031295296931778144		[learning rate: 0.00024729]
		[batch 20/20] avg loss: 0.00048697277528895067		[learning rate: 0.00024699]
	Learning Rate: 0.000246993
	LOSS [training: 0.00039996287230336595 | validation: -0.0018201736643988023]
	TIME [epoch: 8.19 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037236212828996593		[learning rate: 0.00024669]
		[batch 20/20] avg loss: 0.004287602717356971		[learning rate: 0.00024639]
	Learning Rate: 0.000246395
	LOSS [training: 0.0040056120001283144 | validation: -0.0056155315835975175]
	TIME [epoch: 8.18 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015740426302969864		[learning rate: 0.0002461]
		[batch 20/20] avg loss: 0.0010439199740019903		[learning rate: 0.0002458]
	Learning Rate: 0.000245798
	LOSS [training: 0.001308981302149488 | validation: -0.012028014438619042]
	TIME [epoch: 8.18 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002362989241150345		[learning rate: 0.0002455]
		[batch 20/20] avg loss: 0.004466193217418166		[learning rate: 0.0002452]
	Learning Rate: 0.000245203
	LOSS [training: 0.0034145912292842563 | validation: -0.011756746298241895]
	TIME [epoch: 8.18 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0061715964616811956		[learning rate: 0.00024491]
		[batch 20/20] avg loss: -0.0013611061506135175		[learning rate: 0.00024461]
	Learning Rate: 0.00024461
	LOSS [training: 0.0024052451555338387 | validation: -0.009884788681579899]
	TIME [epoch: 8.2 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004036689394188292		[learning rate: 0.00024431]
		[batch 20/20] avg loss: -0.0023803613609803044		[learning rate: 0.00024402]
	Learning Rate: 0.000244018
	LOSS [training: 0.0008281640166039942 | validation: -0.006554681024267516]
	TIME [epoch: 8.21 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003096955803731301		[learning rate: 0.00024372]
		[batch 20/20] avg loss: 0.0027689114990641023		[learning rate: 0.00024343]
	Learning Rate: 0.000243427
	LOSS [training: -0.0001640221523335995 | validation: -0.009232447844562862]
	TIME [epoch: 8.19 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00633113281169379		[learning rate: 0.00024313]
		[batch 20/20] avg loss: 0.0011664440680796157		[learning rate: 0.00024284]
	Learning Rate: 0.000242838
	LOSS [training: 0.0037487884398867037 | validation: -0.01135018908492655]
	TIME [epoch: 8.19 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023222949296608005		[learning rate: 0.00024254]
		[batch 20/20] avg loss: 0.0020637074518836487		[learning rate: 0.00024225]
	Learning Rate: 0.00024225
	LOSS [training: -0.00012929373888857605 | validation: -0.009537011739645582]
	TIME [epoch: 8.2 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003709147276783231		[learning rate: 0.00024196]
		[batch 20/20] avg loss: 0.0001057438904999387		[learning rate: 0.00024166]
	Learning Rate: 0.000241663
	LOSS [training: 0.0019074455836415854 | validation: -0.016708526459508148]
	TIME [epoch: 8.24 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035263291086153835		[learning rate: 0.00024137]
		[batch 20/20] avg loss: -0.000819372690461551		[learning rate: 0.00024108]
	Learning Rate: 0.000241078
	LOSS [training: 0.0013534782090769161 | validation: -0.0061887165939272905]
	TIME [epoch: 8.19 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002660602198061798		[learning rate: 0.00024079]
		[batch 20/20] avg loss: 0.0021516566495409463		[learning rate: 0.00024049]
	Learning Rate: 0.000240495
	LOSS [training: -0.0002544727742604258 | validation: -0.007211415636677219]
	TIME [epoch: 8.19 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006152237021315142		[learning rate: 0.0002402]
		[batch 20/20] avg loss: 0.00023300316527602376		[learning rate: 0.00023991]
	Learning Rate: 0.000239912
	LOSS [training: -0.0001911102684277452 | validation: -0.012158718677089475]
	TIME [epoch: 8.18 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033988564158136366		[learning rate: 0.00023962]
		[batch 20/20] avg loss: 0.006414133270537341		[learning rate: 0.00023933]
	Learning Rate: 0.000239332
	LOSS [training: 0.0015076384273618518 | validation: -0.006879239766715399]
	TIME [epoch: 8.19 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000542389954417487		[learning rate: 0.00023904]
		[batch 20/20] avg loss: -0.002571194799929145		[learning rate: 0.00023875]
	Learning Rate: 0.000238752
	LOSS [training: -0.0010144024227558286 | validation: -0.015413925796227796]
	TIME [epoch: 8.19 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006693613285841728		[learning rate: 0.00023846]
		[batch 20/20] avg loss: 0.00869992523533687		[learning rate: 0.00023817]
	Learning Rate: 0.000238174
	LOSS [training: 0.0010031559747475719 | validation: -0.010761379931712192]
	TIME [epoch: 8.19 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012599517167478792		[learning rate: 0.00023789]
		[batch 20/20] avg loss: 0.004223338197363048		[learning rate: 0.0002376]
	Learning Rate: 0.000237598
	LOSS [training: 0.002741644957055463 | validation: -0.007282275821829068]
	TIME [epoch: 8.18 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00035144185950416773		[learning rate: 0.00023731]
		[batch 20/20] avg loss: 0.0006753863489783425		[learning rate: 0.00023702]
	Learning Rate: 0.000237022
	LOSS [training: 0.00016197224473708737 | validation: -0.0068382079400915]
	TIME [epoch: 8.18 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016430704359618631		[learning rate: 0.00023674]
		[batch 20/20] avg loss: 0.005129463207088134		[learning rate: 0.00023645]
	Learning Rate: 0.000236449
	LOSS [training: 0.0033862668215249984 | validation: -0.015299497116260812]
	TIME [epoch: 8.21 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009966814490758185		[learning rate: 0.00023616]
		[batch 20/20] avg loss: 0.002277352642985706		[learning rate: 0.00023588]
	Learning Rate: 0.000235876
	LOSS [training: 0.001637017046030763 | validation: -0.0025034617574563444]
	TIME [epoch: 8.2 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038214730727031514		[learning rate: 0.00023559]
		[batch 20/20] avg loss: 0.0009470372367071156		[learning rate: 0.00023531]
	Learning Rate: 0.000235305
	LOSS [training: 0.0023842551547051333 | validation: -0.002457924680732977]
	TIME [epoch: 8.19 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007624318136106448		[learning rate: 0.00023502]
		[batch 20/20] avg loss: 0.0017339071250847312		[learning rate: 0.00023474]
	Learning Rate: 0.000234736
	LOSS [training: 0.0046791126305955905 | validation: -0.002903182833827989]
	TIME [epoch: 8.19 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024703295882870725		[learning rate: 0.00023445]
		[batch 20/20] avg loss: 0.002680283293987577		[learning rate: 0.00023417]
	Learning Rate: 0.000234167
	LOSS [training: 0.002575306441137324 | validation: -0.0031861869461206156]
	TIME [epoch: 8.23 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002553392575144725		[learning rate: 0.00023388]
		[batch 20/20] avg loss: 0.003407067321960497		[learning rate: 0.0002336]
	Learning Rate: 0.0002336
	LOSS [training: 0.0029802299485526106 | validation: -0.00424983006322888]
	TIME [epoch: 8.2 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005628341973290774		[learning rate: 0.00023332]
		[batch 20/20] avg loss: 0.0053682682103114495		[learning rate: 0.00023303]
	Learning Rate: 0.000233035
	LOSS [training: 0.005498305091801111 | validation: -0.00968786739178658]
	TIME [epoch: 8.17 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001326909438612087		[learning rate: 0.00023275]
		[batch 20/20] avg loss: 0.002118288685862942		[learning rate: 0.00023247]
	Learning Rate: 0.000232471
	LOSS [training: 0.001722599062237515 | validation: -0.010660112529339917]
	TIME [epoch: 8.18 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038698178416590205		[learning rate: 0.00023219]
		[batch 20/20] avg loss: 0.004982921902426216		[learning rate: 0.00023191]
	Learning Rate: 0.000231908
	LOSS [training: 0.004426369872042619 | validation: -0.00966624785742083]
	TIME [epoch: 8.19 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001624051613758586		[learning rate: 0.00023163]
		[batch 20/20] avg loss: 0.0053641774999144826		[learning rate: 0.00023135]
	Learning Rate: 0.000231347
	LOSS [training: 0.003494114556836534 | validation: -0.011802394388581147]
	TIME [epoch: 8.19 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014252378260718546		[learning rate: 0.00023107]
		[batch 20/20] avg loss: 0.0029039083646895097		[learning rate: 0.00023079]
	Learning Rate: 0.000230787
	LOSS [training: 0.002164573095380682 | validation: -0.0019521243842723043]
	TIME [epoch: 8.18 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018716169474451718		[learning rate: 0.00023051]
		[batch 20/20] avg loss: 0.003922426821703476		[learning rate: 0.00023023]
	Learning Rate: 0.000230228
	LOSS [training: 0.002897021884574324 | validation: -0.007768197850679993]
	TIME [epoch: 8.18 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004090979504479335		[learning rate: 0.00022995]
		[batch 20/20] avg loss: 0.003980881583664168		[learning rate: 0.00022967]
	Learning Rate: 0.000229671
	LOSS [training: 0.004035930544071751 | validation: -0.014288142390732748]
	TIME [epoch: 8.18 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026418288022071077		[learning rate: 0.00022939]
		[batch 20/20] avg loss: 0.001156189649473514		[learning rate: 0.00022911]
	Learning Rate: 0.000229115
	LOSS [training: 0.0018990092258403108 | validation: -0.009124283489362645]
	TIME [epoch: 8.23 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031383690552680655		[learning rate: 0.00022884]
		[batch 20/20] avg loss: 0.0028099412801089714		[learning rate: 0.00022856]
	Learning Rate: 0.00022856
	LOSS [training: 0.0029741551676885193 | validation: -0.013840875746585395]
	TIME [epoch: 8.2 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003046815357626755		[learning rate: 0.00022828]
		[batch 20/20] avg loss: 0.003737669610445855		[learning rate: 0.00022801]
	Learning Rate: 0.000228007
	LOSS [training: 0.003392242484036305 | validation: -0.006367374270577942]
	TIME [epoch: 8.19 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001263638275235395		[learning rate: 0.00022773]
		[batch 20/20] avg loss: 0.0021214658817671032		[learning rate: 0.00022745]
	Learning Rate: 0.000227455
	LOSS [training: 0.0016925520785012488 | validation: -0.010043445593933274]
	TIME [epoch: 8.2 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021239057326410647		[learning rate: 0.00022718]
		[batch 20/20] avg loss: -0.0020246654384003596		[learning rate: 0.0002269]
	Learning Rate: 0.000226904
	LOSS [training: 4.962014712035243e-05 | validation: -0.006532671376266477]
	TIME [epoch: 8.24 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005838801244613177		[learning rate: 0.00022663]
		[batch 20/20] avg loss: 0.007228370693289576		[learning rate: 0.00022635]
	Learning Rate: 0.000226355
	LOSS [training: 0.0006947847243381995 | validation: -0.00952211461510214]
	TIME [epoch: 8.2 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025251294169001915		[learning rate: 0.00022608]
		[batch 20/20] avg loss: 0.00599756923844062		[learning rate: 0.00022581]
	Learning Rate: 0.000225807
	LOSS [training: 0.0017362199107702145 | validation: -0.007579757627577673]
	TIME [epoch: 8.18 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003504455442312182		[learning rate: 0.00022553]
		[batch 20/20] avg loss: 0.0028816491133860775		[learning rate: 0.00022526]
	Learning Rate: 0.00022526
	LOSS [training: 0.0031930522778491296 | validation: -0.010181436910571029]
	TIME [epoch: 8.19 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008206765806782115		[learning rate: 0.00022499]
		[batch 20/20] avg loss: 0.002820768628628131		[learning rate: 0.00022471]
	Learning Rate: 0.000224715
	LOSS [training: 0.005513767217705122 | validation: -0.014638993021908641]
	TIME [epoch: 8.19 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003947193760599164		[learning rate: 0.00022444]
		[batch 20/20] avg loss: -0.0008357669992397631		[learning rate: 0.00022417]
	Learning Rate: 0.000224171
	LOSS [training: 0.0015557133806797007 | validation: -0.008022905180101543]
	TIME [epoch: 8.21 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005788499403977297		[learning rate: 0.0002239]
		[batch 20/20] avg loss: 0.0011007729948637102		[learning rate: 0.00022363]
	Learning Rate: 0.000223628
	LOSS [training: 0.0008398114676307202 | validation: -0.005578933816929481]
	TIME [epoch: 8.21 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005781580174172357		[learning rate: 0.00022336]
		[batch 20/20] avg loss: 0.005020192404087323		[learning rate: 0.00022309]
	Learning Rate: 0.000223087
	LOSS [training: 0.0054008862891298404 | validation: -0.00853747862681992]
	TIME [epoch: 8.19 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023343587610019875		[learning rate: 0.00022282]
		[batch 20/20] avg loss: 0.004777186127234565		[learning rate: 0.00022255]
	Learning Rate: 0.000222547
	LOSS [training: 0.0035557724441182756 | validation: -0.010648414280794176]
	TIME [epoch: 8.18 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004634653345933276		[learning rate: 0.00022228]
		[batch 20/20] avg loss: 0.0038705529859754686		[learning rate: 0.00022201]
	Learning Rate: 0.000222008
	LOSS [training: 0.002167009160284398 | validation: -0.015333370198183281]
	TIME [epoch: 8.2 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031421436364403865		[learning rate: 0.00022174]
		[batch 20/20] avg loss: -0.0013937194170649426		[learning rate: 0.00022147]
	Learning Rate: 0.00022147
	LOSS [training: 0.0008742121096877219 | validation: -0.006036159329134105]
	TIME [epoch: 8.18 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004078628455507363		[learning rate: 0.0002212]
		[batch 20/20] avg loss: 0.0014334851317614685		[learning rate: 0.00022093]
	Learning Rate: 0.000220934
	LOSS [training: -0.0013225716618729465 | validation: -0.01332302672450919]
	TIME [epoch: 8.18 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013464393071643166		[learning rate: 0.00022067]
		[batch 20/20] avg loss: 0.0032476772202580132		[learning rate: 0.0002204]
	Learning Rate: 0.000220399
	LOSS [training: 0.0022970582637111647 | validation: -0.003517523181483808]
	TIME [epoch: 8.18 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009195876884048514		[learning rate: 0.00022013]
		[batch 20/20] avg loss: -0.0010774315882458722		[learning rate: 0.00021987]
	Learning Rate: 0.000219866
	LOSS [training: 0.004059222647901321 | validation: -0.008159864622797126]
	TIME [epoch: 8.19 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014305605300755776		[learning rate: 0.0002196]
		[batch 20/20] avg loss: 0.0022205997768019944		[learning rate: 0.00021933]
	Learning Rate: 0.000219334
	LOSS [training: 0.0018255801534387858 | validation: -0.011283442510509445]
	TIME [epoch: 8.21 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004609434254426521		[learning rate: 0.00021907]
		[batch 20/20] avg loss: -0.0008281006798980629		[learning rate: 0.0002188]
	Learning Rate: 0.000218803
	LOSS [training: 0.0018906667872642291 | validation: -0.008496300502316746]
	TIME [epoch: 8.21 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008968507659611243		[learning rate: 0.00021854]
		[batch 20/20] avg loss: 0.0013952432709077416		[learning rate: 0.00021827]
	Learning Rate: 0.000218273
	LOSS [training: 0.005181875465259491 | validation: -0.00848241036910653]
	TIME [epoch: 8.19 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023203046760466505		[learning rate: 0.00021801]
		[batch 20/20] avg loss: 0.003485293715991913		[learning rate: 0.00021774]
	Learning Rate: 0.000217745
	LOSS [training: 0.002902799196019282 | validation: -0.011869021620380338]
	TIME [epoch: 8.19 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002762905401842822		[learning rate: 0.00021748]
		[batch 20/20] avg loss: -0.00017606868561837914		[learning rate: 0.00021722]
	Learning Rate: 0.000217217
	LOSS [training: 5.011092728295155e-05 | validation: -0.006594605847472305]
	TIME [epoch: 8.22 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00463448945731287		[learning rate: 0.00021695]
		[batch 20/20] avg loss: -0.0014590097625847924		[learning rate: 0.00021669]
	Learning Rate: 0.000216692
	LOSS [training: 0.0015877398473640384 | validation: -0.009288282675727033]
	TIME [epoch: 8.2 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024768866545054275		[learning rate: 0.00021643]
		[batch 20/20] avg loss: 0.0006644745546902945		[learning rate: 0.00021617]
	Learning Rate: 0.000216167
	LOSS [training: 0.001570680604597861 | validation: -0.008040126396310694]
	TIME [epoch: 8.17 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004005273328424744		[learning rate: 0.00021591]
		[batch 20/20] avg loss: 0.0025139657767530555		[learning rate: 0.00021564]
	Learning Rate: 0.000215644
	LOSS [training: 0.003259619552588899 | validation: -0.011712611105600703]
	TIME [epoch: 8.18 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003269393907130016		[learning rate: 0.00021538]
		[batch 20/20] avg loss: 0.0028481352794881556		[learning rate: 0.00021512]
	Learning Rate: 0.000215122
	LOSS [training: 0.0030587645933090857 | validation: -0.011359232986844058]
	TIME [epoch: 8.2 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007345165217734252		[learning rate: 0.00021486]
		[batch 20/20] avg loss: -0.0025723600152223066		[learning rate: 0.0002146]
	Learning Rate: 0.000214601
	LOSS [training: -0.0009189217467244408 | validation: -0.00021083072913183668]
	TIME [epoch: 8.18 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005785972728478193		[learning rate: 0.00021434]
		[batch 20/20] avg loss: 0.0015440868645026195		[learning rate: 0.00021408]
	Learning Rate: 0.000214081
	LOSS [training: 0.003665029796490406 | validation: -0.004039204424415907]
	TIME [epoch: 8.18 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016929403015588911		[learning rate: 0.00021382]
		[batch 20/20] avg loss: 0.0044594180241548775		[learning rate: 0.00021356]
	Learning Rate: 0.000213563
	LOSS [training: 0.0030761791628568847 | validation: -0.013659576110064475]
	TIME [epoch: 8.19 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005255783400233854		[learning rate: 0.0002133]
		[batch 20/20] avg loss: 0.007423047443157626		[learning rate: 0.00021305]
	Learning Rate: 0.000213046
	LOSS [training: 0.0010836320214618867 | validation: -0.00910084145576363]
	TIME [epoch: 8.21 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021207711301136573		[learning rate: 0.00021279]
		[batch 20/20] avg loss: -0.0018633350193802678		[learning rate: 0.00021253]
	Learning Rate: 0.00021253
	LOSS [training: 0.00012871805536669468 | validation: -0.008099475007858138]
	TIME [epoch: 8.21 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0052272397426330625		[learning rate: 0.00021227]
		[batch 20/20] avg loss: 0.00034413925079833477		[learning rate: 0.00021202]
	Learning Rate: 0.000212016
	LOSS [training: 0.002785689496715699 | validation: -0.007411917342080166]
	TIME [epoch: 8.19 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004186975547833182		[learning rate: 0.00021176]
		[batch 20/20] avg loss: -0.0017997598809667218		[learning rate: 0.0002115]
	Learning Rate: 0.000211503
	LOSS [training: 0.001193607833433231 | validation: -0.0047817240089282]
	TIME [epoch: 8.21 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013186364608298163		[learning rate: 0.00021125]
		[batch 20/20] avg loss: 0.005490275269122097		[learning rate: 0.00021099]
	Learning Rate: 0.000210991
	LOSS [training: 0.00208581940414614 | validation: -0.013093853948484776]
	TIME [epoch: 8.2 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007949394385425938		[learning rate: 0.00021074]
		[batch 20/20] avg loss: 0.0068656123794889595		[learning rate: 0.00021048]
	Learning Rate: 0.00021048
	LOSS [training: 0.007407503382457448 | validation: -0.011725319017074229]
	TIME [epoch: 8.2 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007914806237999106		[learning rate: 0.00021022]
		[batch 20/20] avg loss: 0.0010852946393145359		[learning rate: 0.00020997]
	Learning Rate: 0.00020997
	LOSS [training: 0.00014690700775731266 | validation: -0.011465317238087133]
	TIME [epoch: 8.19 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002055758726129419		[learning rate: 0.00020972]
		[batch 20/20] avg loss: 0.004703248756588888		[learning rate: 0.00020946]
	Learning Rate: 0.000209462
	LOSS [training: 0.0033795037413591536 | validation: -0.014322952427119515]
	TIME [epoch: 8.18 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026288874289359687		[learning rate: 0.00020921]
		[batch 20/20] avg loss: 0.0017517309357645307		[learning rate: 0.00020895]
	Learning Rate: 0.000208955
	LOSS [training: 0.00219030918235025 | validation: -0.013800362070844143]
	TIME [epoch: 8.18 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009656660548328222		[learning rate: 0.0002087]
		[batch 20/20] avg loss: 0.0027869608408960466		[learning rate: 0.00020845]
	Learning Rate: 0.000208449
	LOSS [training: 0.0018763134478644345 | validation: -0.013083621326120013]
	TIME [epoch: 8.2 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003452708762934161		[learning rate: 0.0002082]
		[batch 20/20] avg loss: 0.009035267404025993		[learning rate: 0.00020794]
	Learning Rate: 0.000207944
	LOSS [training: 0.004344998263866288 | validation: -0.01298493524980509]
	TIME [epoch: 8.21 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018664455850587974		[learning rate: 0.00020769]
		[batch 20/20] avg loss: -0.0007182139410552007		[learning rate: 0.00020744]
	Learning Rate: 0.000207441
	LOSS [training: -0.001292329763056999 | validation: -0.009130456880588314]
	TIME [epoch: 8.19 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004631526908941678		[learning rate: 0.00020719]
		[batch 20/20] avg loss: 0.00038472959378266106		[learning rate: 0.00020694]
	Learning Rate: 0.000206939
	LOSS [training: -3.921154855575327e-05 | validation: -0.014970315891913662]
	TIME [epoch: 8.18 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002241908369004841		[learning rate: 0.00020669]
		[batch 20/20] avg loss: 0.0023384134153200072		[learning rate: 0.00020644]
	Learning Rate: 0.000206438
	LOSS [training: 4.82525231575832e-05 | validation: -0.00670833737994776]
	TIME [epoch: 8.18 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00048182808723390235		[learning rate: 0.00020619]
		[batch 20/20] avg loss: 0.004543762875338393		[learning rate: 0.00020594]
	Learning Rate: 0.000205938
	LOSS [training: 0.002512795481286147 | validation: -0.012956112023388458]
	TIME [epoch: 8.24 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016908589900087611		[learning rate: 0.00020569]
		[batch 20/20] avg loss: 0.002020081909754271		[learning rate: 0.00020544]
	Learning Rate: 0.00020544
	LOSS [training: 0.0018554704498815167 | validation: -0.011065804942430672]
	TIME [epoch: 8.2 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005131242162435203		[learning rate: 0.00020519]
		[batch 20/20] avg loss: 0.00024490644386271235		[learning rate: 0.00020494]
	Learning Rate: 0.000204942
	LOSS [training: 0.002688074303148958 | validation: -0.005390901149606295]
	TIME [epoch: 8.18 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018380732653018872		[learning rate: 0.00020469]
		[batch 20/20] avg loss: 0.005459790188307122		[learning rate: 0.00020445]
	Learning Rate: 0.000204446
	LOSS [training: 0.0036489317268045053 | validation: -0.004111617934989832]
	TIME [epoch: 8.18 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001887076305549856		[learning rate: 0.0002042]
		[batch 20/20] avg loss: 0.004962714754547336		[learning rate: 0.00020395]
	Learning Rate: 0.000203951
	LOSS [training: 0.00153781922449874 | validation: -0.008141550086254244]
	TIME [epoch: 8.2 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00161606887909286		[learning rate: 0.0002037]
		[batch 20/20] avg loss: -0.0004360479694917827		[learning rate: 0.00020346]
	Learning Rate: 0.000203457
	LOSS [training: -0.0010260584242923212 | validation: -0.012741378104605922]
	TIME [epoch: 8.18 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018798665242158777		[learning rate: 0.00020321]
		[batch 20/20] avg loss: 0.0023016896322098925		[learning rate: 0.00020296]
	Learning Rate: 0.000202965
	LOSS [training: 0.0020907780782128856 | validation: -0.010749061701558149]
	TIME [epoch: 8.21 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004122433602395301		[learning rate: 0.00020272]
		[batch 20/20] avg loss: 0.006984411137052551		[learning rate: 0.00020247]
	Learning Rate: 0.000202474
	LOSS [training: 0.0014309887673286245 | validation: -0.01277218930408205]
	TIME [epoch: 8.18 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003912595754979317		[learning rate: 0.00020223]
		[batch 20/20] avg loss: -0.001733244901195059		[learning rate: 0.00020198]
	Learning Rate: 0.000201983
	LOSS [training: 0.0010896754268921297 | validation: -0.009556041798959858]
	TIME [epoch: 8.18 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005523933533279989		[learning rate: 0.00020174]
		[batch 20/20] avg loss: 0.0020272243865825613		[learning rate: 0.00020149]
	Learning Rate: 0.000201495
	LOSS [training: 0.003775578959931275 | validation: -0.008826730073048504]
	TIME [epoch: 8.2 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003718759484742061		[learning rate: 0.00020125]
		[batch 20/20] avg loss: 0.0026114669000214174		[learning rate: 0.00020101]
	Learning Rate: 0.000201007
	LOSS [training: 0.0031651131923817397 | validation: -0.012274795345451193]
	TIME [epoch: 8.22 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00110155830005005		[learning rate: 0.00020076]
		[batch 20/20] avg loss: 0.0028607523207642162		[learning rate: 0.00020052]
	Learning Rate: 0.00020052
	LOSS [training: 0.0008795970103570826 | validation: -0.01239293535785889]
	TIME [epoch: 8.18 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002286032410342447		[learning rate: 0.00020028]
		[batch 20/20] avg loss: 0.005036881651686516		[learning rate: 0.00020003]
	Learning Rate: 0.000200035
	LOSS [training: 0.0036614570310144817 | validation: -0.003645177170971354]
	TIME [epoch: 8.17 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005904847538276146		[learning rate: 0.00019979]
		[batch 20/20] avg loss: 0.007207786493486237		[learning rate: 0.00019955]
	Learning Rate: 0.00019955
	LOSS [training: 0.006556317015881194 | validation: -0.0083750600061443]
	TIME [epoch: 8.19 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005657299848217211		[learning rate: 0.00019931]
		[batch 20/20] avg loss: 0.003977013010840415		[learning rate: 0.00019907]
	Learning Rate: 0.000199067
	LOSS [training: 0.0022713714978310684 | validation: -0.012205637890348432]
	TIME [epoch: 8.17 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027423807566069117		[learning rate: 0.00019883]
		[batch 20/20] avg loss: 0.003824948483807866		[learning rate: 0.00019859]
	Learning Rate: 0.000198585
	LOSS [training: 0.003283664620207389 | validation: -0.014678938077937125]
	TIME [epoch: 8.17 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000308828754810421		[learning rate: 0.00019834]
		[batch 20/20] avg loss: 0.0011003910066151442		[learning rate: 0.0001981]
	Learning Rate: 0.000198105
	LOSS [training: 0.0003957811259023616 | validation: -0.008765166123097945]
	TIME [epoch: 8.18 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00577224944910879		[learning rate: 0.00019786]
		[batch 20/20] avg loss: 0.000802879564594353		[learning rate: 0.00019763]
	Learning Rate: 0.000197625
	LOSS [training: -0.002484684942257219 | validation: -0.01077017853969478]
	TIME [epoch: 8.21 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016932764113352702		[learning rate: 0.00019739]
		[batch 20/20] avg loss: 0.006070663608591384		[learning rate: 0.00019715]
	Learning Rate: 0.000197147
	LOSS [training: 0.0038819700099633273 | validation: -0.0030379820046115527]
	TIME [epoch: 8.2 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002411115684586777		[learning rate: 0.00019691]
		[batch 20/20] avg loss: 0.001215686181915451		[learning rate: 0.00019667]
	Learning Rate: 0.000196669
	LOSS [training: 0.0018134009332511137 | validation: -0.01121786815823342]
	TIME [epoch: 8.18 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032002347891722793		[learning rate: 0.00019643]
		[batch 20/20] avg loss: -0.006137718309922197		[learning rate: 0.00019619]
	Learning Rate: 0.000196193
	LOSS [training: -0.0014687417603749587 | validation: -0.013398919277071146]
	TIME [epoch: 8.18 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0044460199419024075		[learning rate: 0.00019596]
		[batch 20/20] avg loss: 0.0020073505520643562		[learning rate: 0.00019572]
	Learning Rate: 0.000195718
	LOSS [training: 0.0032266852469833816 | validation: -0.003961472176395445]
	TIME [epoch: 8.21 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013954913375337687		[learning rate: 0.00019548]
		[batch 20/20] avg loss: 0.0016652178208231827		[learning rate: 0.00019524]
	Learning Rate: 0.000195245
	LOSS [training: 0.0015303545791784756 | validation: -0.01341195125948442]
	TIME [epoch: 8.19 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00452699224806233		[learning rate: 0.00019501]
		[batch 20/20] avg loss: 0.005861054000657041		[learning rate: 0.00019477]
	Learning Rate: 0.000194772
	LOSS [training: 0.005194023124359686 | validation: -0.007957537273557235]
	TIME [epoch: 8.19 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002205939713984831		[learning rate: 0.00019454]
		[batch 20/20] avg loss: 0.004578989668790657		[learning rate: 0.0001943]
	Learning Rate: 0.0001943
	LOSS [training: 0.0033924646913877435 | validation: -0.010705359514639757]
	TIME [epoch: 8.18 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0047427185272983195		[learning rate: 0.00019407]
		[batch 20/20] avg loss: 0.0025181683669038297		[learning rate: 0.00019383]
	Learning Rate: 0.00019383
	LOSS [training: 0.0036304434471010755 | validation: -0.00851228614163076]
	TIME [epoch: 8.18 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024044608407363275		[learning rate: 0.0001936]
		[batch 20/20] avg loss: 0.006774502531613347		[learning rate: 0.00019336]
	Learning Rate: 0.000193361
	LOSS [training: 0.004589481686174837 | validation: -0.00781583559903281]
	TIME [epoch: 8.19 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0042560098115052335		[learning rate: 0.00019313]
		[batch 20/20] avg loss: 0.00583314214066337		[learning rate: 0.00019289]
	Learning Rate: 0.000192893
	LOSS [training: 0.005044575976084302 | validation: -0.0031705886162933994]
	TIME [epoch: 8.18 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006482312693898796		[learning rate: 0.00019266]
		[batch 20/20] avg loss: 0.0042274048420978755		[learning rate: 0.00019243]
	Learning Rate: 0.000192426
	LOSS [training: 0.0017895867863539979 | validation: -0.00651092507630246]
	TIME [epoch: 8.21 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002967387089309171		[learning rate: 0.00019219]
		[batch 20/20] avg loss: 0.004810398223366035		[learning rate: 0.00019196]
	Learning Rate: 0.00019196
	LOSS [training: 0.0009215055670284322 | validation: -0.012843876772552333]
	TIME [epoch: 8.19 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006916791109451053		[learning rate: 0.00019173]
		[batch 20/20] avg loss: 0.002352884927516952		[learning rate: 0.0001915]
	Learning Rate: 0.000191495
	LOSS [training: 0.004634838018484002 | validation: -0.0063695004833734525]
	TIME [epoch: 8.19 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029571607568424406		[learning rate: 0.00019126]
		[batch 20/20] avg loss: 0.006116154816231534		[learning rate: 0.00019103]
	Learning Rate: 0.000191032
	LOSS [training: 0.004536657786536988 | validation: -0.007744003105706138]
	TIME [epoch: 8.21 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004074287270595399		[learning rate: 0.0001908]
		[batch 20/20] avg loss: 0.0008817615041490751		[learning rate: 0.00019057]
	Learning Rate: 0.000190569
	LOSS [training: 0.0024780243873722373 | validation: -0.013300792946922664]
	TIME [epoch: 8.23 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004034256632013887		[learning rate: 0.00019034]
		[batch 20/20] avg loss: 0.002915308035072684		[learning rate: 0.00019011]
	Learning Rate: 0.000190108
	LOSS [training: 0.0034747823335432846 | validation: -0.003571361638699394]
	TIME [epoch: 8.18 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008605193676904759		[learning rate: 0.00018988]
		[batch 20/20] avg loss: 0.010355731963995035		[learning rate: 0.00018965]
	Learning Rate: 0.000189648
	LOSS [training: 0.009480462820449896 | validation: -0.006109509619146388]
	TIME [epoch: 8.18 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0040881802830544		[learning rate: 0.00018942]
		[batch 20/20] avg loss: 0.010540426300000847		[learning rate: 0.00018919]
	Learning Rate: 0.000189189
	LOSS [training: 0.007314303291527624 | validation: -0.0054370266887900694]
	TIME [epoch: 8.21 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002348835154738129		[learning rate: 0.00018896]
		[batch 20/20] avg loss: 0.01023182347676211		[learning rate: 0.00018873]
	Learning Rate: 0.000188731
	LOSS [training: 0.0062903293157501195 | validation: 0.0031205369410240388]
	TIME [epoch: 8.19 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027049688208243064		[learning rate: 0.0001885]
		[batch 20/20] avg loss: 0.014021926967508484		[learning rate: 0.00018827]
	Learning Rate: 0.000188274
	LOSS [training: 0.008363447894166395 | validation: -0.009802056425169672]
	TIME [epoch: 8.2 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013006193337255483		[learning rate: 0.00018805]
		[batch 20/20] avg loss: 0.00629292757125619		[learning rate: 0.00018782]
	Learning Rate: 0.000187818
	LOSS [training: 0.00249615411876532 | validation: -0.0016209885873707244]
	TIME [epoch: 8.22 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012480524471797415		[learning rate: 0.00018759]
		[batch 20/20] avg loss: 0.0033874190223276692		[learning rate: 0.00018736]
	Learning Rate: 0.000187363
	LOSS [training: 0.002317735734753705 | validation: -0.009052814420747887]
	TIME [epoch: 8.21 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006638608633418043		[learning rate: 0.00018714]
		[batch 20/20] avg loss: 0.003583219536967196		[learning rate: 0.00018691]
	Learning Rate: 0.00018691
	LOSS [training: 0.005110914085192621 | validation: -0.007647287691436261]
	TIME [epoch: 8.21 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0039170473198493995		[learning rate: 0.00018668]
		[batch 20/20] avg loss: 0.0030537999263896775		[learning rate: 0.00018646]
	Learning Rate: 0.000186457
	LOSS [training: 0.0034854236231195387 | validation: -0.013929361560879105]
	TIME [epoch: 8.19 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018855942584403383		[learning rate: 0.00018623]
		[batch 20/20] avg loss: 0.0006787347382673723		[learning rate: 0.00018601]
	Learning Rate: 0.000186006
	LOSS [training: 0.0012821644983538553 | validation: -0.009011053229235965]
	TIME [epoch: 8.2 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005702977141354175		[learning rate: 0.00018578]
		[batch 20/20] avg loss: 0.00023642256193996762		[learning rate: 0.00018556]
	Learning Rate: 0.000185555
	LOSS [training: 0.002969699851647071 | validation: -0.010978395250408229]
	TIME [epoch: 8.2 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015809946916900304		[learning rate: 0.00018533]
		[batch 20/20] avg loss: 0.0014234421124949784		[learning rate: 0.00018511]
	Learning Rate: 0.000185106
	LOSS [training: -7.877628959752592e-05 | validation: -0.011061108932208278]
	TIME [epoch: 8.2 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0044239623789005		[learning rate: 0.00018488]
		[batch 20/20] avg loss: 6.0674539849910884e-05		[learning rate: 0.00018466]
	Learning Rate: 0.000184658
	LOSS [training: 0.0022423184593752055 | validation: -0.003566802971733496]
	TIME [epoch: 8.18 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002806769964570525		[learning rate: 0.00018443]
		[batch 20/20] avg loss: 0.009653186261699824		[learning rate: 0.00018421]
	Learning Rate: 0.000184211
	LOSS [training: 0.0062299781131351754 | validation: -0.013187139914623267]
	TIME [epoch: 8.19 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007384622723175692		[learning rate: 0.00018399]
		[batch 20/20] avg loss: 0.0019688141614098067		[learning rate: 0.00018377]
	Learning Rate: 0.000183765
	LOSS [training: 0.0013536382168636877 | validation: -0.009177464951911907]
	TIME [epoch: 8.18 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003459726736494577		[learning rate: 0.00018354]
		[batch 20/20] avg loss: -5.375531671752039e-05		[learning rate: 0.00018332]
	Learning Rate: 0.00018332
	LOSS [training: 0.0017029857098885283 | validation: -0.011018015477687909]
	TIME [epoch: 8.2 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006892094419359665		[learning rate: 0.0001831]
		[batch 20/20] avg loss: -0.00026614704883406206		[learning rate: 0.00018288]
	Learning Rate: 0.000182876
	LOSS [training: 0.0033129736852628017 | validation: -0.015353054590774119]
	TIME [epoch: 8.19 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00368098950666456		[learning rate: 0.00018266]
		[batch 20/20] avg loss: 0.0023209817482514527		[learning rate: 0.00018243]
	Learning Rate: 0.000182434
	LOSS [training: 0.0030009856274580064 | validation: -0.00749227254360174]
	TIME [epoch: 8.2 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002197637892613101		[learning rate: 0.00018221]
		[batch 20/20] avg loss: -0.0007367243446999034		[learning rate: 0.00018199]
	Learning Rate: 0.000181992
	LOSS [training: 0.0007304567739565988 | validation: -0.005777885324539678]
	TIME [epoch: 8.21 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003989631033384249		[learning rate: 0.00018177]
		[batch 20/20] avg loss: 0.0028337892679463706		[learning rate: 0.00018155]
	Learning Rate: 0.000181552
	LOSS [training: 0.0034117101506653097 | validation: -0.00983412441263148]
	TIME [epoch: 8.19 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002739004597784479		[learning rate: 0.00018133]
		[batch 20/20] avg loss: -0.002181868912758155		[learning rate: 0.00018111]
	Learning Rate: 0.000181112
	LOSS [training: 0.0002785678425131621 | validation: -0.011933171517567683]
	TIME [epoch: 8.2 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014849045552354913		[learning rate: 0.00018089]
		[batch 20/20] avg loss: 0.0004814486288513023		[learning rate: 0.00018067]
	Learning Rate: 0.000180674
	LOSS [training: -0.0005017279631920944 | validation: -0.01677973763299364]
	TIME [epoch: 8.2 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00381467835295841		[learning rate: 0.00018045]
		[batch 20/20] avg loss: 0.0017545168532182736		[learning rate: 0.00018024]
	Learning Rate: 0.000180236
	LOSS [training: 0.002784597603088341 | validation: 0.0008670271647642487]
	TIME [epoch: 8.21 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.876234828953745e-06		[learning rate: 0.00018002]
		[batch 20/20] avg loss: 0.0016091362098062888		[learning rate: 0.0001798]
	Learning Rate: 0.0001798
	LOSS [training: 0.0008090062223176213 | validation: -0.00794712771227419]
	TIME [epoch: 8.19 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002244875511386287		[learning rate: 0.00017958]
		[batch 20/20] avg loss: 0.0037722986434857763		[learning rate: 0.00017936]
	Learning Rate: 0.000179365
	LOSS [training: 0.003008587077436031 | validation: -0.0085251003424832]
	TIME [epoch: 8.2 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002576580200570954		[learning rate: 0.00017915]
		[batch 20/20] avg loss: 0.003278841446089893		[learning rate: 0.00017893]
	Learning Rate: 0.00017893
	LOSS [training: 0.002927710823330424 | validation: -0.0011064680674702963]
	TIME [epoch: 8.2 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023506013276442957		[learning rate: 0.00017871]
		[batch 20/20] avg loss: 0.004090500010763205		[learning rate: 0.0001785]
	Learning Rate: 0.000178497
	LOSS [training: 0.0008699493415594546 | validation: -0.008778708311110607]
	TIME [epoch: 8.17 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007004585572397769		[learning rate: 0.00017828]
		[batch 20/20] avg loss: 0.0028220814649387925		[learning rate: 0.00017807]
	Learning Rate: 0.000178065
	LOSS [training: 0.004913333518668282 | validation: -0.012019021694366546]
	TIME [epoch: 8.18 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025971845545844563		[learning rate: 0.00017785]
		[batch 20/20] avg loss: 0.0021067594985332283		[learning rate: 0.00017763]
	Learning Rate: 0.000177634
	LOSS [training: 0.0023519720265588423 | validation: -0.010883712681251243]
	TIME [epoch: 8.18 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00686060986263755		[learning rate: 0.00017742]
		[batch 20/20] avg loss: 0.0037648476496135097		[learning rate: 0.0001772]
	Learning Rate: 0.000177204
	LOSS [training: 0.005312728756125531 | validation: -0.008907770695715158]
	TIME [epoch: 8.23 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004365681736453055		[learning rate: 0.00017699]
		[batch 20/20] avg loss: 0.004096877199489651		[learning rate: 0.00017678]
	Learning Rate: 0.000176775
	LOSS [training: 0.004231279467971353 | validation: -0.011825117286210949]
	TIME [epoch: 8.2 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010319695105179405		[learning rate: 0.00017656]
		[batch 20/20] avg loss: 0.004818646848563849		[learning rate: 0.00017635]
	Learning Rate: 0.000176347
	LOSS [training: 0.007569170976871627 | validation: -0.009222114842272464]
	TIME [epoch: 8.18 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038759700761229208		[learning rate: 0.00017613]
		[batch 20/20] avg loss: 0.005073246728219233		[learning rate: 0.00017592]
	Learning Rate: 0.00017592
	LOSS [training: 0.004474608402171077 | validation: -0.0075587228947074205]
	TIME [epoch: 8.19 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004065452902547752		[learning rate: 0.00017571]
		[batch 20/20] avg loss: 0.0004826615967175457		[learning rate: 0.00017549]
	Learning Rate: 0.000175494
	LOSS [training: 0.002274057249632649 | validation: -0.00251047084108204]
	TIME [epoch: 8.22 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004632270679245493		[learning rate: 0.00017528]
		[batch 20/20] avg loss: 0.0054675686667692925		[learning rate: 0.00017507]
	Learning Rate: 0.00017507
	LOSS [training: 0.005049919673007393 | validation: -0.010739200984067897]
	TIME [epoch: 8.21 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002397616637949643		[learning rate: 0.00017486]
		[batch 20/20] avg loss: 0.004490323807817559		[learning rate: 0.00017465]
	Learning Rate: 0.000174646
	LOSS [training: 0.0034439702228836024 | validation: -0.018727463911008414]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_1770.pth
	Model improved!!!
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029027710716984303		[learning rate: 0.00017443]
		[batch 20/20] avg loss: 0.0008679863451106987		[learning rate: 0.00017422]
	Learning Rate: 0.000174223
	LOSS [training: 0.0018853787084045647 | validation: -0.010197039690612105]
	TIME [epoch: 8.17 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005376054010333102		[learning rate: 0.00017401]
		[batch 20/20] avg loss: 0.004197907965126565		[learning rate: 0.0001738]
	Learning Rate: 0.000173801
	LOSS [training: 0.004786980987729833 | validation: -0.012031109488841589]
	TIME [epoch: 8.18 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015909004300052453		[learning rate: 0.00017359]
		[batch 20/20] avg loss: 0.003782165964641061		[learning rate: 0.00017338]
	Learning Rate: 0.00017338
	LOSS [training: 0.002686533197323153 | validation: -0.004950575095851345]
	TIME [epoch: 8.16 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002950026501710333		[learning rate: 0.00017317]
		[batch 20/20] avg loss: 0.007027811008810644		[learning rate: 0.00017296]
	Learning Rate: 0.000172961
	LOSS [training: 0.004988918755260489 | validation: -0.004860297238571077]
	TIME [epoch: 8.16 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002458048204540758		[learning rate: 0.00017275]
		[batch 20/20] avg loss: 0.003551320541667718		[learning rate: 0.00017254]
	Learning Rate: 0.000172542
	LOSS [training: 0.0030046843731042377 | validation: -0.011259765939484502]
	TIME [epoch: 8.16 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00038687233236904236		[learning rate: 0.00017233]
		[batch 20/20] avg loss: 0.005013094294651717		[learning rate: 0.00017212]
	Learning Rate: 0.000172124
	LOSS [training: 0.0026999833135103795 | validation: -0.015979745831689377]
	TIME [epoch: 8.18 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026488371753842655		[learning rate: 0.00017192]
		[batch 20/20] avg loss: -0.0008171716770470081		[learning rate: 0.00017171]
	Learning Rate: 0.000171708
	LOSS [training: 0.0009158327491686283 | validation: -0.012931341005227965]
	TIME [epoch: 8.2 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001294428100180876		[learning rate: 0.0001715]
		[batch 20/20] avg loss: 0.0027772321230925177		[learning rate: 0.00017129]
	Learning Rate: 0.000171292
	LOSS [training: 0.0020358301116366966 | validation: -0.016532015060864554]
	TIME [epoch: 8.17 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003306123894420339		[learning rate: 0.00017108]
		[batch 20/20] avg loss: 0.0005284752577364666		[learning rate: 0.00017088]
	Learning Rate: 0.000170877
	LOSS [training: -0.001388824318341936 | validation: -0.012017085248661389]
	TIME [epoch: 8.17 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015722748813289775		[learning rate: 0.00017067]
		[batch 20/20] avg loss: -0.0006131737667898828		[learning rate: 0.00017046]
	Learning Rate: 0.000170464
	LOSS [training: 0.00047955055726954727 | validation: -0.009369548760479025]
	TIME [epoch: 8.19 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016554280889653997		[learning rate: 0.00017026]
		[batch 20/20] avg loss: 0.0013421548224686155		[learning rate: 0.00017005]
	Learning Rate: 0.000170051
	LOSS [training: 0.0014987914557170073 | validation: -0.00687690929224869]
	TIME [epoch: 8.19 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023751179284136827		[learning rate: 0.00016984]
		[batch 20/20] avg loss: -0.00011072404588381886		[learning rate: 0.00016964]
	Learning Rate: 0.000169639
	LOSS [training: 0.0011321969412649315 | validation: -0.011888049989326785]
	TIME [epoch: 8.17 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007767649957286782		[learning rate: 0.00016943]
		[batch 20/20] avg loss: 0.004406442015440866		[learning rate: 0.00016923]
	Learning Rate: 0.000169229
	LOSS [training: 0.0025916035055847716 | validation: -0.008671527208209911]
	TIME [epoch: 8.17 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004398531741668249		[learning rate: 0.00016902]
		[batch 20/20] avg loss: 0.003320406299376132		[learning rate: 0.00016882]
	Learning Rate: 0.000168819
	LOSS [training: 0.00385946902052219 | validation: -0.003760845930607674]
	TIME [epoch: 8.17 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003746071591920713		[learning rate: 0.00016861]
		[batch 20/20] avg loss: 0.001697049233067103		[learning rate: 0.00016841]
	Learning Rate: 0.00016841
	LOSS [training: 0.0027215604124939084 | validation: -0.005552906975506324]
	TIME [epoch: 8.17 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004374592752879056		[learning rate: 0.00016821]
		[batch 20/20] avg loss: 0.0013778066678988693		[learning rate: 0.000168]
	Learning Rate: 0.000168003
	LOSS [training: 0.0028761997103889624 | validation: -0.008494138986223459]
	TIME [epoch: 8.18 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008251600374807688		[learning rate: 0.0001678]
		[batch 20/20] avg loss: 0.004706592183929108		[learning rate: 0.0001676]
	Learning Rate: 0.000167596
	LOSS [training: 0.0019407160732241694 | validation: -0.007913508484220845]
	TIME [epoch: 8.16 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019866959481023337		[learning rate: 0.00016739]
		[batch 20/20] avg loss: 0.003862272784259839		[learning rate: 0.00016719]
	Learning Rate: 0.00016719
	LOSS [training: 0.002924484366181086 | validation: -0.008177825104977086]
	TIME [epoch: 8.18 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018980381482057661		[learning rate: 0.00016699]
		[batch 20/20] avg loss: 0.004998583745647172		[learning rate: 0.00016679]
	Learning Rate: 0.000166785
	LOSS [training: 0.0034483109469264695 | validation: -0.008401865852055306]
	TIME [epoch: 8.19 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030908603895463617		[learning rate: 0.00016658]
		[batch 20/20] avg loss: 0.0003856075592958034		[learning rate: 0.00016638]
	Learning Rate: 0.000166382
	LOSS [training: 0.001738233974421083 | validation: -0.008600506944782845]
	TIME [epoch: 8.19 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002498547854587264		[learning rate: 0.00016618]
		[batch 20/20] avg loss: 0.005660681086650275		[learning rate: 0.00016598]
	Learning Rate: 0.000165979
	LOSS [training: 0.0015810666160315054 | validation: -0.008853409932041634]
	TIME [epoch: 8.18 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002575903818309605		[learning rate: 0.00016578]
		[batch 20/20] avg loss: 0.003441795237745439		[learning rate: 0.00016558]
	Learning Rate: 0.000165577
	LOSS [training: 0.003008849528027522 | validation: -0.00847370353038323]
	TIME [epoch: 8.21 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026956406877990734		[learning rate: 0.00016538]
		[batch 20/20] avg loss: 0.0026126644511057806		[learning rate: 0.00016518]
	Learning Rate: 0.000165176
	LOSS [training: 0.002654152569452427 | validation: -0.008140671707247717]
	TIME [epoch: 8.17 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.952037057563275e-06		[learning rate: 0.00016498]
		[batch 20/20] avg loss: 0.006128348944966998		[learning rate: 0.00016478]
	Learning Rate: 0.000164776
	LOSS [training: 0.003069150491012281 | validation: -0.00829019731322521]
	TIME [epoch: 8.18 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011055808971542334		[learning rate: 0.00016458]
		[batch 20/20] avg loss: 0.002728283114259842		[learning rate: 0.00016438]
	Learning Rate: 0.000164377
	LOSS [training: 0.0019169320057070376 | validation: -0.011945369129029663]
	TIME [epoch: 8.18 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011640353712787566		[learning rate: 0.00016418]
		[batch 20/20] avg loss: -0.0007654666680572293		[learning rate: 0.00016398]
	Learning Rate: 0.000163979
	LOSS [training: 0.00019928435161076317 | validation: -0.013598329349084962]
	TIME [epoch: 8.17 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009354379945950349		[learning rate: 0.00016378]
		[batch 20/20] avg loss: 0.0017490664666404854		[learning rate: 0.00016358]
	Learning Rate: 0.000163583
	LOSS [training: 0.0013422522306177603 | validation: -0.016191414510014806]
	TIME [epoch: 8.17 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008425637754029183		[learning rate: 0.00016338]
		[batch 20/20] avg loss: 0.0023256722597430096		[learning rate: 0.00016319]
	Learning Rate: 0.000163187
	LOSS [training: 0.0015841180175729642 | validation: -0.010609266533406119]
	TIME [epoch: 8.17 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013853269059743642		[learning rate: 0.00016299]
		[batch 20/20] avg loss: 0.0034108383066038064		[learning rate: 0.00016279]
	Learning Rate: 0.000162791
	LOSS [training: 0.0010127557003147213 | validation: -0.010941494845026182]
	TIME [epoch: 8.19 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021500415470392087		[learning rate: 0.00016259]
		[batch 20/20] avg loss: 0.002413670247941589		[learning rate: 0.0001624]
	Learning Rate: 0.000162397
	LOSS [training: 0.002281855897490399 | validation: -0.008973760757538474]
	TIME [epoch: 8.2 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004797779692655444		[learning rate: 0.0001622]
		[batch 20/20] avg loss: -0.003544491150408538		[learning rate: 0.000162]
	Learning Rate: 0.000162004
	LOSS [training: 0.0006266442711234525 | validation: -0.010621865177876718]
	TIME [epoch: 8.18 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003408730504841273		[learning rate: 0.00016181]
		[batch 20/20] avg loss: 0.0009503456695754712		[learning rate: 0.00016161]
	Learning Rate: 0.000161612
	LOSS [training: -0.0012291924176329005 | validation: -0.011516957427472965]
	TIME [epoch: 8.17 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009877765072155284		[learning rate: 0.00016142]
		[batch 20/20] avg loss: 0.0027673422955184135		[learning rate: 0.00016122]
	Learning Rate: 0.000161221
	LOSS [training: 0.0008897828941514428 | validation: -0.013796089891262468]
	TIME [epoch: 8.19 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018790276923694033		[learning rate: 0.00016103]
		[batch 20/20] avg loss: -0.0018250086051309334		[learning rate: 0.00016083]
	Learning Rate: 0.000160831
	LOSS [training: 2.7009543619234584e-05 | validation: -0.012480856489580257]
	TIME [epoch: 8.21 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003059457414349513		[learning rate: 0.00016064]
		[batch 20/20] avg loss: -0.002815910310802399		[learning rate: 0.00016044]
	Learning Rate: 0.000160441
	LOSS [training: 0.00012177355177355759 | validation: -0.011606675819021566]
	TIME [epoch: 8.17 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006203019331703765		[learning rate: 0.00016025]
		[batch 20/20] avg loss: 0.007652134462568556		[learning rate: 0.00016005]
	Learning Rate: 0.000160053
	LOSS [training: 0.0007245575654323956 | validation: -0.010436357886528198]
	TIME [epoch: 8.16 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00166251529815803		[learning rate: 0.00015986]
		[batch 20/20] avg loss: 0.0077083714434436805		[learning rate: 0.00015967]
	Learning Rate: 0.000159665
	LOSS [training: 0.0030229280726428247 | validation: -0.003171509894349469]
	TIME [epoch: 8.16 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029042913596756675		[learning rate: 0.00015947]
		[batch 20/20] avg loss: 0.008308930703836977		[learning rate: 0.00015928]
	Learning Rate: 0.000159279
	LOSS [training: 0.0056066110317563225 | validation: -0.006500718738856975]
	TIME [epoch: 8.18 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006934567587276818		[learning rate: 0.00015909]
		[batch 20/20] avg loss: -0.00037730243158175665		[learning rate: 0.00015889]
	Learning Rate: 0.000158893
	LOSS [training: 0.0032786325778475303 | validation: -0.012028806377803016]
	TIME [epoch: 8.16 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002995829469019598		[learning rate: 0.0001587]
		[batch 20/20] avg loss: 0.00031299157741557476		[learning rate: 0.00015851]
	Learning Rate: 0.000158509
	LOSS [training: 0.0016544105232175858 | validation: -0.00970394095660558]
	TIME [epoch: 8.16 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009565518200753964		[learning rate: 0.00015832]
		[batch 20/20] avg loss: 0.001651651185461718		[learning rate: 0.00015812]
	Learning Rate: 0.000158125
	LOSS [training: 0.0013041015027685573 | validation: -0.013929665108269904]
	TIME [epoch: 8.16 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012565766354783862		[learning rate: 0.00015793]
		[batch 20/20] avg loss: 0.000642428635917007		[learning rate: 0.00015774]
	Learning Rate: 0.000157742
	LOSS [training: -0.00030707399978068954 | validation: -0.010475683497682818]
	TIME [epoch: 8.19 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001385146901175199		[learning rate: 0.00015755]
		[batch 20/20] avg loss: 0.003954453985409184		[learning rate: 0.00015736]
	Learning Rate: 0.00015736
	LOSS [training: 0.002669800443292192 | validation: -0.016230859415966383]
	TIME [epoch: 8.2 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001132977643588345		[learning rate: 0.00015717]
		[batch 20/20] avg loss: 0.0020318512532105193		[learning rate: 0.00015698]
	Learning Rate: 0.000156979
	LOSS [training: 0.0015824144483994323 | validation: -0.010093368978793963]
	TIME [epoch: 8.17 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00140588387281369		[learning rate: 0.00015679]
		[batch 20/20] avg loss: -0.0036304253350370277		[learning rate: 0.0001566]
	Learning Rate: 0.000156599
	LOSS [training: -0.001112270731111669 | validation: -0.002103130784001996]
	TIME [epoch: 8.18 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023752749144192696		[learning rate: 0.00015641]
		[batch 20/20] avg loss: -0.0031151972211178373		[learning rate: 0.00015622]
	Learning Rate: 0.00015622
	LOSS [training: -0.00036996115334928377 | validation: -0.010977893969079196]
	TIME [epoch: 8.19 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013324150019000604		[learning rate: 0.00015603]
		[batch 20/20] avg loss: 0.0010168672764396202		[learning rate: 0.00015584]
	Learning Rate: 0.000155842
	LOSS [training: 0.00117464113916984 | validation: -0.007429786268031709]
	TIME [epoch: 8.21 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005760674870554644		[learning rate: 0.00015565]
		[batch 20/20] avg loss: -0.0016169622191573672		[learning rate: 0.00015546]
	Learning Rate: 0.000155465
	LOSS [training: 0.002071856325698639 | validation: -0.00723506957947205]
	TIME [epoch: 8.17 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004847444934265037		[learning rate: 0.00015528]
		[batch 20/20] avg loss: 0.006116478004399425		[learning rate: 0.00015509]
	Learning Rate: 0.000155088
	LOSS [training: 0.0054819614693322315 | validation: -0.0158294305365392]
	TIME [epoch: 8.16 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0055059524894301905		[learning rate: 0.0001549]
		[batch 20/20] avg loss: -0.0022481128736268714		[learning rate: 0.00015471]
	Learning Rate: 0.000154713
	LOSS [training: 0.0016289198079016596 | validation: -0.014354777397328319]
	TIME [epoch: 8.17 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002374389329752921		[learning rate: 0.00015453]
		[batch 20/20] avg loss: 0.0021647771605867493		[learning rate: 0.00015434]
	Learning Rate: 0.000154338
	LOSS [training: -0.0001048060845830862 | validation: -0.00896529603298646]
	TIME [epoch: 8.19 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004386169114515682		[learning rate: 0.00015415]
		[batch 20/20] avg loss: -9.903295992643567e-05		[learning rate: 0.00015396]
	Learning Rate: 0.000153965
	LOSS [training: 0.0021435680772946227 | validation: -0.009593495353917708]
	TIME [epoch: 8.16 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010984834876089787		[learning rate: 0.00015378]
		[batch 20/20] avg loss: 0.0007816984263290146		[learning rate: 0.00015359]
	Learning Rate: 0.000153592
	LOSS [training: 0.0009400909569689964 | validation: -0.011072128062553026]
	TIME [epoch: 8.17 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003168989971687397		[learning rate: 0.00015341]
		[batch 20/20] avg loss: -0.0017568240922740366		[learning rate: 0.00015322]
	Learning Rate: 0.00015322
	LOSS [training: 0.0007060829397066802 | validation: -0.007059698975466423]
	TIME [epoch: 8.17 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005822565728808411		[learning rate: 0.00015303]
		[batch 20/20] avg loss: 0.002349686367298289		[learning rate: 0.00015285]
	Learning Rate: 0.000152849
	LOSS [training: 0.000883714897208724 | validation: -0.006734357378698195]
	TIME [epoch: 8.22 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: -8.531501143759111e-05		[learning rate: 0.00015266]
		[batch 20/20] avg loss: -0.0009245163687596813		[learning rate: 0.00015248]
	Learning Rate: 0.000152479
	LOSS [training: -0.0005049156900986363 | validation: -0.01326132248433999]
	TIME [epoch: 8.18 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031694882117698415		[learning rate: 0.00015229]
		[batch 20/20] avg loss: 0.0019992526603620237		[learning rate: 0.00015211]
	Learning Rate: 0.00015211
	LOSS [training: 0.0025843704360659324 | validation: -0.008176128907969836]
	TIME [epoch: 8.18 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020122737876332583		[learning rate: 0.00015193]
		[batch 20/20] avg loss: 0.003725392536327178		[learning rate: 0.00015174]
	Learning Rate: 0.000151742
	LOSS [training: 0.002868833161980218 | validation: -0.013157214712405135]
	TIME [epoch: 8.17 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028296717856068875		[learning rate: 0.00015156]
		[batch 20/20] avg loss: 0.005178762690183472		[learning rate: 0.00015137]
	Learning Rate: 0.000151374
	LOSS [training: 0.00400421723789518 | validation: -0.007474316503907874]
	TIME [epoch: 8.21 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002089354673818656		[learning rate: 0.00015119]
		[batch 20/20] avg loss: 0.0015255524469722926		[learning rate: 0.00015101]
	Learning Rate: 0.000151008
	LOSS [training: -0.000281901113423182 | validation: -0.011239338410466211]
	TIME [epoch: 8.19 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00027654818451201894		[learning rate: 0.00015083]
		[batch 20/20] avg loss: 0.005516638688947235		[learning rate: 0.00015064]
	Learning Rate: 0.000150642
	LOSS [training: 0.002896593436729627 | validation: -0.0076371891017333]
	TIME [epoch: 8.17 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031191336320493942		[learning rate: 0.00015046]
		[batch 20/20] avg loss: -0.0011653270505898223		[learning rate: 0.00015028]
	Learning Rate: 0.000150278
	LOSS [training: 0.0009769032907297863 | validation: -0.008839513799555526]
	TIME [epoch: 8.17 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009062524970798501		[learning rate: 0.0001501]
		[batch 20/20] avg loss: 0.004378524680924088		[learning rate: 0.00014991]
	Learning Rate: 0.000149914
	LOSS [training: 0.001736136091922119 | validation: -0.010446619549878395]
	TIME [epoch: 8.17 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026525885018802093		[learning rate: 0.00014973]
		[batch 20/20] avg loss: 0.0010367112773092466		[learning rate: 0.00014955]
	Learning Rate: 0.000149551
	LOSS [training: 0.0018446498895947285 | validation: -0.015224226963209986]
	TIME [epoch: 8.19 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019848162004605712		[learning rate: 0.00014937]
		[batch 20/20] avg loss: -0.0002950336759305123		[learning rate: 0.00014919]
	Learning Rate: 0.000149189
	LOSS [training: 0.0008448912622650295 | validation: -0.011952034330324147]
	TIME [epoch: 8.17 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029382032011858305		[learning rate: 0.00014901]
		[batch 20/20] avg loss: -0.0002864883514900695		[learning rate: 0.00014883]
	Learning Rate: 0.000148828
	LOSS [training: -0.0016123457763379495 | validation: -0.010047176612086615]
	TIME [epoch: 8.16 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011797450236666596		[learning rate: 0.00014865]
		[batch 20/20] avg loss: 0.0005275436104682262		[learning rate: 0.00014847]
	Learning Rate: 0.000148468
	LOSS [training: 0.0008536443170674427 | validation: -0.0125143120445676]
	TIME [epoch: 8.16 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0052840246313239836		[learning rate: 0.00014829]
		[batch 20/20] avg loss: 0.002557525450065812		[learning rate: 0.00014811]
	Learning Rate: 0.000148108
	LOSS [training: -0.001363249590629086 | validation: -0.011725114993589204]
	TIME [epoch: 8.19 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015837639499968067		[learning rate: 0.00014793]
		[batch 20/20] avg loss: 0.0008079457046081678		[learning rate: 0.00014775]
	Learning Rate: 0.00014775
	LOSS [training: -0.0003879091226943193 | validation: -0.014811059627690172]
	TIME [epoch: 8.2 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028898649604911474		[learning rate: 0.00014757]
		[batch 20/20] avg loss: -0.0007533884875652781		[learning rate: 0.00014739]
	Learning Rate: 0.000147392
	LOSS [training: -0.0018216267240282126 | validation: -0.004174352582913575]
	TIME [epoch: 8.17 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005668338750381563		[learning rate: 0.00014721]
		[batch 20/20] avg loss: 0.000767488951186519		[learning rate: 0.00014704]
	Learning Rate: 0.000147035
	LOSS [training: 0.003217913850784041 | validation: -0.01179231434398463]
	TIME [epoch: 8.17 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002108908694720266		[learning rate: 0.00014686]
		[batch 20/20] avg loss: 0.00554318014535627		[learning rate: 0.00014668]
	Learning Rate: 0.000146679
	LOSS [training: 0.0017171357253180017 | validation: -0.008006793809132369]
	TIME [epoch: 8.2 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008867154339220531		[learning rate: 0.0001465]
		[batch 20/20] avg loss: 0.004260263701459671		[learning rate: 0.00014632]
	Learning Rate: 0.000146324
	LOSS [training: 0.0016867741337688088 | validation: -0.00624843648656734]
	TIME [epoch: 8.19 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011614610868746398		[learning rate: 0.00014615]
		[batch 20/20] avg loss: 0.0007791056368214079		[learning rate: 0.00014597]
	Learning Rate: 0.00014597
	LOSS [training: 0.000970283361848024 | validation: -0.00529824386149245]
	TIME [epoch: 8.16 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007127623383430369		[learning rate: 0.00014579]
		[batch 20/20] avg loss: 0.0014984039899656252		[learning rate: 0.00014562]
	Learning Rate: 0.000145616
	LOSS [training: 0.0011055831641543308 | validation: -0.0107050287259256]
	TIME [epoch: 8.17 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014483286046697093		[learning rate: 0.00014544]
		[batch 20/20] avg loss: 0.0014926884160911002		[learning rate: 0.00014526]
	Learning Rate: 0.000145264
	LOSS [training: 0.0014705085103804048 | validation: -0.013459118866325705]
	TIME [epoch: 8.16 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00040273616595942475		[learning rate: 0.00014509]
		[batch 20/20] avg loss: -0.0036282869996358226		[learning rate: 0.00014491]
	Learning Rate: 0.000144912
	LOSS [training: -0.001612775416838199 | validation: -0.013185389031126218]
	TIME [epoch: 8.18 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009383487065726008		[learning rate: 0.00014474]
		[batch 20/20] avg loss: 0.001428723787685878		[learning rate: 0.00014456]
	Learning Rate: 0.000144561
	LOSS [training: 0.0011835362471292396 | validation: -0.009279813255132469]
	TIME [epoch: 8.17 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007232965513982159		[learning rate: 0.00014439]
		[batch 20/20] avg loss: -0.006522571869697259		[learning rate: 0.00014421]
	Learning Rate: 0.000144212
	LOSS [training: 0.00035519682214245004 | validation: -0.010944929440921708]
	TIME [epoch: 8.19 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030232544278622512		[learning rate: 0.00014404]
		[batch 20/20] avg loss: -0.00047297184560299093		[learning rate: 0.00014386]
	Learning Rate: 0.000143862
	LOSS [training: 0.0012751412911296304 | validation: -0.01472023438031699]
	TIME [epoch: 8.19 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002965166964284725		[learning rate: 0.00014369]
		[batch 20/20] avg loss: 0.0005694456301820309		[learning rate: 0.00014351]
	Learning Rate: 0.000143514
	LOSS [training: 0.0001364644668767791 | validation: -0.01703883889280641]
	TIME [epoch: 8.17 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021637474181734266		[learning rate: 0.00014334]
		[batch 20/20] avg loss: 0.006608888629531184		[learning rate: 0.00014317]
	Learning Rate: 0.000143167
	LOSS [training: 0.0022225706056788787 | validation: -0.008804087060183432]
	TIME [epoch: 8.18 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006339520340868975		[learning rate: 0.00014299]
		[batch 20/20] avg loss: 0.0059478089656746155		[learning rate: 0.00014282]
	Learning Rate: 0.00014282
	LOSS [training: 0.006143664653271795 | validation: -0.009846658589646897]
	TIME [epoch: 8.18 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003993355268725412		[learning rate: 0.00014265]
		[batch 20/20] avg loss: 0.003605067335951951		[learning rate: 0.00014247]
	Learning Rate: 0.000142474
	LOSS [training: 0.002002201431412246 | validation: -0.009726541068695325]
	TIME [epoch: 8.18 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007899290293299855		[learning rate: 0.0001423]
		[batch 20/20] avg loss: -0.003636468023620579		[learning rate: 0.00014213]
	Learning Rate: 0.000142129
	LOSS [training: 0.002131411134839638 | validation: -0.010680703834149298]
	TIME [epoch: 8.16 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028614044823244795		[learning rate: 0.00014196]
		[batch 20/20] avg loss: 0.003927693036644105		[learning rate: 0.00014179]
	Learning Rate: 0.000141785
	LOSS [training: 0.003394548759484292 | validation: -0.009922502657073734]
	TIME [epoch: 8.19 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003775991470688587		[learning rate: 0.00014161]
		[batch 20/20] avg loss: 0.0029738764574852293		[learning rate: 0.00014144]
	Learning Rate: 0.000141442
	LOSS [training: -0.0004010575066016789 | validation: -0.012328092276632518]
	TIME [epoch: 8.17 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006029200403673974		[learning rate: 0.00014127]
		[batch 20/20] avg loss: -0.001980790352550349		[learning rate: 0.0001411]
	Learning Rate: 0.0001411
	LOSS [training: -0.0006889351560914757 | validation: -0.009253640077399006]
	TIME [epoch: 8.16 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002465077079165609		[learning rate: 0.00014093]
		[batch 20/20] avg loss: -0.0008321529374981281		[learning rate: 0.00014076]
	Learning Rate: 0.000140758
	LOSS [training: -0.0002928226147907837 | validation: -0.012305523848347481]
	TIME [epoch: 8.16 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006268218590525787		[learning rate: 0.00014059]
		[batch 20/20] avg loss: 0.0007462120635919183		[learning rate: 0.00014042]
	Learning Rate: 0.000140417
	LOSS [training: 0.0035072153270588524 | validation: -0.020490191853351065]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240219_183143/states/model_tr_study2_1860.pth
	Model improved!!!
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004793745247030557		[learning rate: 0.00014025]
		[batch 20/20] avg loss: 0.0014937769419036129		[learning rate: 0.00014008]
	Learning Rate: 0.000140078
	LOSS [training: 0.0031437610944670845 | validation: -0.009551110468854304]
	TIME [epoch: 8.19 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006420015693353991		[learning rate: 0.00013991]
		[batch 20/20] avg loss: 0.0018755322058088616		[learning rate: 0.00013974]
	Learning Rate: 0.000139738
	LOSS [training: 0.004147773949581426 | validation: -0.009766834287917147]
	TIME [epoch: 8.17 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007340077970629623		[learning rate: 0.00013957]
		[batch 20/20] avg loss: 0.0016248504639985062		[learning rate: 0.0001394]
	Learning Rate: 0.0001394
	LOSS [training: 0.0044824642173140646 | validation: -0.008079245557460804]
	TIME [epoch: 8.17 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007020772883181468		[learning rate: 0.00013923]
		[batch 20/20] avg loss: -0.0021095877238360306		[learning rate: 0.00013906]
	Learning Rate: 0.000139063
	LOSS [training: -0.001405832506077089 | validation: -0.0072467744191231]
	TIME [epoch: 8.17 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002468262391513509		[learning rate: 0.00013889]
		[batch 20/20] avg loss: 0.005768617769198013		[learning rate: 0.00013873]
	Learning Rate: 0.000138726
	LOSS [training: 0.0016501776888422526 | validation: -0.0034394581896955998]
	TIME [epoch: 8.19 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005383670287406146		[learning rate: 0.00013856]
		[batch 20/20] avg loss: -0.004694599364439478		[learning rate: 0.00013839]
	Learning Rate: 0.00013839
	LOSS [training: 0.00034453546148333395 | validation: -0.01160935671341264]
	TIME [epoch: 8.2 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002137386562474262		[learning rate: 0.00013822]
		[batch 20/20] avg loss: 0.0020272664426735656		[learning rate: 0.00013806]
	Learning Rate: 0.000138055
	LOSS [training: 0.0020823265025739133 | validation: -0.012314617931944834]
	TIME [epoch: 8.17 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000577309344044938		[learning rate: 0.00013789]
		[batch 20/20] avg loss: 0.0028234138904775063		[learning rate: 0.00013772]
	Learning Rate: 0.000137721
	LOSS [training: 0.0017003616172612227 | validation: -0.014860972374404346]
	TIME [epoch: 8.16 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026524360844479115		[learning rate: 0.00013755]
		[batch 20/20] avg loss: 0.0020411267140833527		[learning rate: 0.00013739]
	Learning Rate: 0.000137388
	LOSS [training: 0.002346781399265632 | validation: -0.014920414708349587]
	TIME [epoch: 8.17 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016785442965019648		[learning rate: 0.00013722]
		[batch 20/20] avg loss: 0.002379438698424468		[learning rate: 0.00013705]
	Learning Rate: 0.000137055
	LOSS [training: 0.00035044720096125175 | validation: -0.003829968700066421]
	TIME [epoch: 8.17 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008912513152693228		[learning rate: 0.00013689]
		[batch 20/20] avg loss: 0.01002005991299114		[learning rate: 0.00013672]
	Learning Rate: 0.000136723
	LOSS [training: 0.0045644042988609084 | validation: -0.007576767017838498]
	TIME [epoch: 8.16 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033479090862795916		[learning rate: 0.00013656]
		[batch 20/20] avg loss: 0.0020570639803941184		[learning rate: 0.00013639]
	Learning Rate: 0.000136392
	LOSS [training: 0.002702486533336855 | validation: -0.00826044645109326]
	TIME [epoch: 8.16 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005474039765636162		[learning rate: 0.00013623]
		[batch 20/20] avg loss: 0.0016242938785146997		[learning rate: 0.00013606]
	Learning Rate: 0.000136062
	LOSS [training: 0.0035491668220754306 | validation: -0.011233635923940472]
	TIME [epoch: 8.16 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002371138261623105		[learning rate: 0.0001359]
		[batch 20/20] avg loss: 0.004542608761478209		[learning rate: 0.00013573]
	Learning Rate: 0.000135733
	LOSS [training: 0.0034568735115506574 | validation: -0.01696995543509588]
	TIME [epoch: 8.19 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030318608455965398		[learning rate: 0.00013557]
		[batch 20/20] avg loss: 0.002843946714022856		[learning rate: 0.0001354]
	Learning Rate: 0.000135404
	LOSS [training: 0.002937903779809698 | validation: -0.008534256083593738]
	TIME [epoch: 8.2 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028805456308324097		[learning rate: 0.00013524]
		[batch 20/20] avg loss: -0.0036246497851974173		[learning rate: 0.00013508]
	Learning Rate: 0.000135076
	LOSS [training: -0.00037205207718250404 | validation: -0.0137909088403457]
	TIME [epoch: 8.17 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006936566261597031		[learning rate: 0.00013491]
		[batch 20/20] avg loss: -0.0027459995567349856		[learning rate: 0.00013475]
	Learning Rate: 0.000134749
	LOSS [training: 0.002095283352431023 | validation: -0.013042161159413375]
	TIME [epoch: 8.17 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007718705744280896		[learning rate: 0.00013459]
		[batch 20/20] avg loss: 0.0008014573418972258		[learning rate: 0.00013442]
	Learning Rate: 0.000134423
	LOSS [training: 1.4793383734568297e-05 | validation: -0.006427560624258539]
	TIME [epoch: 8.19 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004799239463175982		[learning rate: 0.00013426]
		[batch 20/20] avg loss: 0.0020622159869426214		[learning rate: 0.0001341]
	Learning Rate: 0.000134098
	LOSS [training: 0.00127106996663011 | validation: -0.010874025793916123]
	TIME [epoch: 8.2 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007471146972910012		[learning rate: 0.00013394]
		[batch 20/20] avg loss: 0.00022970984302612927		[learning rate: 0.00013377]
	Learning Rate: 0.000133773
	LOSS [training: 0.003850428407968072 | validation: -0.009099186477079944]
	TIME [epoch: 8.16 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004950594413279899		[learning rate: 0.00013361]
		[batch 20/20] avg loss: 0.004037962880987766		[learning rate: 0.00013345]
	Learning Rate: 0.000133449
	LOSS [training: 0.002266511161157878 | validation: -0.011806595360896633]
	TIME [epoch: 8.16 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017102510608009718		[learning rate: 0.00013329]
		[batch 20/20] avg loss: 0.0012777216155143332		[learning rate: 0.00013313]
	Learning Rate: 0.000133126
	LOSS [training: 0.0014939863381576526 | validation: -0.011366942012245472]
	TIME [epoch: 8.15 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004437048023904293		[learning rate: 0.00013296]
		[batch 20/20] avg loss: -0.0032682366787492624		[learning rate: 0.0001328]
	Learning Rate: 0.000132804
	LOSS [training: 0.0005844056725775157 | validation: -0.012833353522029939]
	TIME [epoch: 8.18 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003619230876621028		[learning rate: 0.00013264]
		[batch 20/20] avg loss: 0.007547832663150967		[learning rate: 0.00013248]
	Learning Rate: 0.000132482
	LOSS [training: 0.005583531769885999 | validation: -0.011162861029990192]
	TIME [epoch: 8.15 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009443822648659859		[learning rate: 0.00013232]
		[batch 20/20] avg loss: 0.004877598435443983		[learning rate: 0.00013216]
	Learning Rate: 0.000132162
	LOSS [training: 0.002910990350154985 | validation: -0.011181381280932495]
	TIME [epoch: 8.15 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010686890892923962		[learning rate: 0.000132]
		[batch 20/20] avg loss: 0.0022263038052862372		[learning rate: 0.00013184]
	Learning Rate: 0.000131842
	LOSS [training: 0.0005788073579969205 | validation: -0.009405844283756633]
	TIME [epoch: 8.16 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015437007235050023		[learning rate: 0.00013168]
		[batch 20/20] avg loss: 0.005603896202760813		[learning rate: 0.00013152]
	Learning Rate: 0.000131522
	LOSS [training: 0.003573798463132908 | validation: -0.01425622942885572]
	TIME [epoch: 8.19 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003103039416868309		[learning rate: 0.00013136]
		[batch 20/20] avg loss: 0.0037137635995201576		[learning rate: 0.0001312]
	Learning Rate: 0.000131204
	LOSS [training: 0.003408401508194233 | validation: -0.007922010706569692]
	TIME [epoch: 8.19 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021708189291466057		[learning rate: 0.00013105]
		[batch 20/20] avg loss: -0.0004740861477983957		[learning rate: 0.00013089]
	Learning Rate: 0.000130886
	LOSS [training: -0.0013224525384725005 | validation: -0.01578399840087149]
	TIME [epoch: 8.17 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007667469209882732		[learning rate: 0.00013073]
		[batch 20/20] avg loss: -0.003555581224502071		[learning rate: 0.00013057]
	Learning Rate: 0.00013057
	LOSS [training: -0.001394417151756899 | validation: -0.013228835843887274]
	TIME [epoch: 8.17 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024632211050104134		[learning rate: 0.00013041]
		[batch 20/20] avg loss: 0.0009039533054934134		[learning rate: 0.00013025]
	Learning Rate: 0.000130254
	LOSS [training: 0.0016835872052519136 | validation: -0.008016220256688826]
	TIME [epoch: 8.2 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0040076384296430425		[learning rate: 0.0001301]
		[batch 20/20] avg loss: -0.006088035630455418		[learning rate: 0.00012994]
	Learning Rate: 0.000129938
	LOSS [training: -0.0010401986004061879 | validation: -0.01687570262572443]
	TIME [epoch: 8.2 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005303460725827455		[learning rate: 0.00012978]
		[batch 20/20] avg loss: 0.00208377232880048		[learning rate: 0.00012962]
	Learning Rate: 0.000129624
	LOSS [training: 0.0007767131281088672 | validation: -0.010129485501568882]
	TIME [epoch: 8.16 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013090842313611295		[learning rate: 0.00012947]
		[batch 20/20] avg loss: 0.0033192049151478465		[learning rate: 0.00012931]
	Learning Rate: 0.00012931
	LOSS [training: 0.0010050603418933584 | validation: -0.00995308729367552]
	TIME [epoch: 8.17 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024650041986099902		[learning rate: 0.00012915]
		[batch 20/20] avg loss: -0.000999429757016347		[learning rate: 0.000129]
	Learning Rate: 0.000128997
	LOSS [training: 0.0007327872207968214 | validation: -0.007182906501374773]
	TIME [epoch: 8.16 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005745479987399236		[learning rate: 0.00012884]
		[batch 20/20] avg loss: 0.006688928008292408		[learning rate: 0.00012868]
	Learning Rate: 0.000128685
	LOSS [training: 0.006217203997845822 | validation: -0.009416819700280823]
	TIME [epoch: 8.19 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038135260868487576		[learning rate: 0.00012853]
		[batch 20/20] avg loss: 0.0006069017119728955		[learning rate: 0.00012837]
	Learning Rate: 0.000128373
	LOSS [training: 0.0022102138994108266 | validation: -0.007192497539457615]
	TIME [epoch: 8.17 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036621706569236303		[learning rate: 0.00012822]
		[batch 20/20] avg loss: 0.003142128916935094		[learning rate: 0.00012806]
	Learning Rate: 0.000128062
	LOSS [training: 0.003402149786929363 | validation: -0.011417156612877793]
	TIME [epoch: 8.17 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038372129166205976		[learning rate: 0.00012791]
		[batch 20/20] avg loss: -0.0035967978483399644		[learning rate: 0.00012775]
	Learning Rate: 0.000127752
	LOSS [training: 0.00012020753414031631 | validation: -0.007725626291050029]
	TIME [epoch: 8.16 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002391972110323528		[learning rate: 0.0001276]
		[batch 20/20] avg loss: 0.003236861441603347		[learning rate: 0.00012744]
	Learning Rate: 0.000127443
	LOSS [training: 0.002814416775963437 | validation: -0.008255365827918845]
	TIME [epoch: 8.18 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024353288485745696		[learning rate: 0.00012729]
		[batch 20/20] avg loss: 0.002718416342468495		[learning rate: 0.00012713]
	Learning Rate: 0.000127134
	LOSS [training: 0.00014154374694696283 | validation: -0.01058710341769216]
	TIME [epoch: 8.2 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003897247777979429		[learning rate: 0.00012698]
		[batch 20/20] avg loss: 1.6514308790857927e-06		[learning rate: 0.00012683]
	Learning Rate: 0.000126827
	LOSS [training: 0.0019494496044292569 | validation: -0.01141290494164824]
	TIME [epoch: 8.17 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004324859902335315		[learning rate: 0.00012667]
		[batch 20/20] avg loss: -0.0004525841352245527		[learning rate: 0.00012652]
	Learning Rate: 0.00012652
	LOSS [training: 0.0019361378835553813 | validation: -0.015587429337552403]
	TIME [epoch: 8.17 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015262889261651578		[learning rate: 0.00012637]
		[batch 20/20] avg loss: 0.0016649678359841903		[learning rate: 0.00012621]
	Learning Rate: 0.000126213
	LOSS [training: 6.933945490951627e-05 | validation: -0.0072205306888957375]
	TIME [epoch: 8.17 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002337951519887721		[learning rate: 0.00012606]
		[batch 20/20] avg loss: 0.0020342003490742256		[learning rate: 0.00012591]
	Learning Rate: 0.000125908
	LOSS [training: 0.000900202598542727 | validation: -0.010185781840178586]
	TIME [epoch: 8.23 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020360316651250256		[learning rate: 0.00012576]
		[batch 20/20] avg loss: 0.0002174529920739081		[learning rate: 0.0001256]
	Learning Rate: 0.000125603
	LOSS [training: 0.0011267423285994667 | validation: -0.012793571061926782]
	TIME [epoch: 8.17 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018950835097654162		[learning rate: 0.00012545]
		[batch 20/20] avg loss: 0.0065988546764436965		[learning rate: 0.0001253]
	Learning Rate: 0.000125299
	LOSS [training: 0.0023518855833391398 | validation: -0.009999972776442139]
	TIME [epoch: 8.17 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002177748136641533		[learning rate: 0.00012515]
		[batch 20/20] avg loss: 0.0026194498304271895		[learning rate: 0.000125]
	Learning Rate: 0.000124996
	LOSS [training: 0.0023985989835343608 | validation: -0.010553910929992177]
	TIME [epoch: 8.17 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001359042980365859		[learning rate: 0.00012484]
		[batch 20/20] avg loss: 0.002023848870279599		[learning rate: 0.00012469]
	Learning Rate: 0.000124693
	LOSS [training: 0.001691445925322729 | validation: -0.010779034424979508]
	TIME [epoch: 8.19 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00041635543683176817		[learning rate: 0.00012454]
		[batch 20/20] avg loss: -0.0007665287949714407		[learning rate: 0.00012439]
	Learning Rate: 0.000124391
	LOSS [training: -0.0005914421159016047 | validation: -0.010521643688786546]
	TIME [epoch: 8.17 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004463391273617644		[learning rate: 0.00012424]
		[batch 20/20] avg loss: -0.0021009072524255067		[learning rate: 0.00012409]
	Learning Rate: 0.00012409
	LOSS [training: 0.0011812420105960686 | validation: -0.00931588070384058]
	TIME [epoch: 8.16 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003165967922924305		[learning rate: 0.00012394]
		[batch 20/20] avg loss: 0.002935882894208974		[learning rate: 0.00012379]
	Learning Rate: 0.00012379
	LOSS [training: 0.0030509254085666396 | validation: -0.013921875483729623]
	TIME [epoch: 8.17 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035268478630066944		[learning rate: 0.00012364]
		[batch 20/20] avg loss: -0.0007454039933959854		[learning rate: 0.00012349]
	Learning Rate: 0.00012349
	LOSS [training: 0.0013907219348053544 | validation: -0.007817438914012619]
	TIME [epoch: 8.16 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008657593735315409		[learning rate: 0.00012334]
		[batch 20/20] avg loss: 0.0006323726802275756		[learning rate: 0.00012319]
	Learning Rate: 0.000123191
	LOSS [training: 0.0007490660268795583 | validation: -0.01666635929706072]
	TIME [epoch: 8.2 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002831233349321854		[learning rate: 0.00012304]
		[batch 20/20] avg loss: -0.0007253949154916391		[learning rate: 0.00012289]
	Learning Rate: 0.000122893
	LOSS [training: -0.001778314132406747 | validation: -0.013078738000960027]
	TIME [epoch: 8.19 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00138165403244269		[learning rate: 0.00012274]
		[batch 20/20] avg loss: 0.005979120615937706		[learning rate: 0.0001226]
	Learning Rate: 0.000122595
	LOSS [training: 0.002298733291747508 | validation: -0.014755042870639333]
	TIME [epoch: 8.18 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024922709753004294		[learning rate: 0.00012245]
		[batch 20/20] avg loss: 0.0034534511163594192		[learning rate: 0.0001223]
	Learning Rate: 0.000122298
	LOSS [training: 0.0029728610458299243 | validation: -0.012171025541444527]
	TIME [epoch: 8.18 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004661704678323465		[learning rate: 0.00012215]
		[batch 20/20] avg loss: -0.0012550837496193052		[learning rate: 0.000122]
	Learning Rate: 0.000122002
	LOSS [training: 0.0017033104643520802 | validation: -0.013863125914442815]
	TIME [epoch: 8.21 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005157085998598987		[learning rate: 0.00012185]
		[batch 20/20] avg loss: 0.000775925127250872		[learning rate: 0.00012171]
	Learning Rate: 0.000121707
	LOSS [training: 0.0006458168635553854 | validation: -0.00870817520143074]
	TIME [epoch: 8.2 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015723037984735778		[learning rate: 0.00012156]
		[batch 20/20] avg loss: -9.197621018919578e-05		[learning rate: 0.00012141]
	Learning Rate: 0.000121412
	LOSS [training: 0.0007401637941421909 | validation: -0.01033815286219724]
	TIME [epoch: 8.17 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006975408145531143		[learning rate: 0.00012127]
		[batch 20/20] avg loss: -0.0033824601768750336		[learning rate: 0.00012112]
	Learning Rate: 0.000121119
	LOSS [training: 0.001796473984328055 | validation: -0.014102384470411558]
	TIME [epoch: 8.17 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015958897343395586		[learning rate: 0.00012097]
		[batch 20/20] avg loss: -0.0012913364125732626		[learning rate: 0.00012083]
	Learning Rate: 0.000120825
	LOSS [training: 0.00015227666088314792 | validation: -0.007324116617568965]
	TIME [epoch: 8.17 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002116158954878258		[learning rate: 0.00012068]
		[batch 20/20] avg loss: 0.0016253565923680096		[learning rate: 0.00012053]
	Learning Rate: 0.000120533
	LOSS [training: 0.0009184862439279177 | validation: -0.011847707841790255]
	TIME [epoch: 8.19 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008961711765217582		[learning rate: 0.00012039]
		[batch 20/20] avg loss: 0.003593440944318236		[learning rate: 0.00012024]
	Learning Rate: 0.000120241
	LOSS [training: 0.0013486348838982385 | validation: -0.016809329773680853]
	TIME [epoch: 8.17 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004299746275051944		[learning rate: 0.0001201]
		[batch 20/20] avg loss: 0.00010096985606346427		[learning rate: 0.00011995]
	Learning Rate: 0.00011995
	LOSS [training: -0.0020993882094942394 | validation: -0.013598508616756065]
	TIME [epoch: 8.17 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008994109450249173		[learning rate: 0.0001198]
		[batch 20/20] avg loss: 0.005480903135410872		[learning rate: 0.00011966]
	Learning Rate: 0.00011966
	LOSS [training: 0.0022907460951929783 | validation: -0.007253936710082842]
	TIME [epoch: 8.2 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002679580512308549		[learning rate: 0.00011951]
		[batch 20/20] avg loss: 0.004315401468123063		[learning rate: 0.00011937]
	Learning Rate: 0.00011937
	LOSS [training: 0.0008179104779072567 | validation: -0.004680503522335408]
	TIME [epoch: 8.2 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035392345412528932		[learning rate: 0.00011923]
		[batch 20/20] avg loss: -0.0018599321244953016		[learning rate: 0.00011908]
	Learning Rate: 0.000119081
	LOSS [training: 0.0008396512083787956 | validation: -0.010827438186885908]
	TIME [epoch: 8.18 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004318668832813661		[learning rate: 0.00011894]
		[batch 20/20] avg loss: 0.0007683663703873702		[learning rate: 0.00011879]
	Learning Rate: 0.000118793
	LOSS [training: -0.0017751512312131456 | validation: -0.005189050866638145]
	TIME [epoch: 8.17 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0057472916550940735		[learning rate: 0.00011865]
		[batch 20/20] avg loss: 0.0015242807341903687		[learning rate: 0.00011851]
	Learning Rate: 0.000118505
	LOSS [training: 0.003635786194642221 | validation: -0.015619092643111503]
	TIME [epoch: 8.21 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030683680125734307		[learning rate: 0.00011836]
		[batch 20/20] avg loss: -0.005575484401700576		[learning rate: 0.00011822]
	Learning Rate: 0.000118218
	LOSS [training: -0.0012535581945635726 | validation: -0.014785347939681724]
	TIME [epoch: 8.19 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00038687361690223126		[learning rate: 0.00011808]
		[batch 20/20] avg loss: 0.004154891814705547		[learning rate: 0.00011793]
	Learning Rate: 0.000117932
	LOSS [training: 0.001884009098901658 | validation: -0.010022175234336671]
	TIME [epoch: 8.17 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003010559216310657		[learning rate: 0.00011779]
		[batch 20/20] avg loss: 0.0002821930613607367		[learning rate: 0.00011765]
	Learning Rate: 0.000117646
	LOSS [training: 0.0016463761388356972 | validation: -0.010138515411265224]
	TIME [epoch: 8.17 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012608293123700745		[learning rate: 0.0001175]
		[batch 20/20] avg loss: 0.005213888181494673		[learning rate: 0.00011736]
	Learning Rate: 0.000117362
	LOSS [training: 0.0032373587469323737 | validation: -0.005098125036675093]
	TIME [epoch: 8.17 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003638906070537777		[learning rate: 0.00011722]
		[batch 20/20] avg loss: -0.0011324422152410572		[learning rate: 0.00011708]
	Learning Rate: 0.000117078
	LOSS [training: 0.0012532319276483597 | validation: -0.010867281234093522]
	TIME [epoch: 8.15 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004396078331548351		[learning rate: 0.00011694]
		[batch 20/20] avg loss: -0.006320778796098154		[learning rate: 0.00011679]
	Learning Rate: 0.000116794
	LOSS [training: -0.000962350232274902 | validation: -0.0067310553225730565]
	TIME [epoch: 8.18 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003672243921602404		[learning rate: 0.00011665]
		[batch 20/20] avg loss: -0.0023559446348324353		[learning rate: 0.00011651]
	Learning Rate: 0.000116511
	LOSS [training: 0.0006581496433849841 | validation: -0.00595854906863391]
	TIME [epoch: 8.19 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005573173090741145		[learning rate: 0.00011637]
		[batch 20/20] avg loss: -0.0031960884566772916		[learning rate: 0.00011623]
	Learning Rate: 0.000116229
	LOSS [training: -0.001876702882875703 | validation: -0.008579168696865315]
	TIME [epoch: 8.19 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002318952048391017		[learning rate: 0.00011609]
		[batch 20/20] avg loss: 0.0017108767574265757		[learning rate: 0.00011595]
	Learning Rate: 0.000115948
	LOSS [training: 0.002014914402908796 | validation: -0.010770835552994776]
	TIME [epoch: 8.17 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005107153978716523		[learning rate: 0.00011581]
		[batch 20/20] avg loss: 0.001676055858006576		[learning rate: 0.00011567]
	Learning Rate: 0.000115667
	LOSS [training: 0.0010933856279391144 | validation: -0.012933735747588224]
	TIME [epoch: 8.19 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010477427012938832		[learning rate: 0.00011553]
		[batch 20/20] avg loss: 0.002489192974751344		[learning rate: 0.00011539]
	Learning Rate: 0.000115387
	LOSS [training: 0.0017684678380226137 | validation: -0.016074449273084913]
	TIME [epoch: 8.2 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004448863489309294		[learning rate: 0.00011525]
		[batch 20/20] avg loss: -0.0005774644015792219		[learning rate: 0.00011511]
	Learning Rate: 0.000115108
	LOSS [training: 0.0019356995438650365 | validation: -0.011461363321237369]
	TIME [epoch: 8.18 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003541475981570593		[learning rate: 0.00011497]
		[batch 20/20] avg loss: 0.0033751751649948405		[learning rate: 0.00011483]
	Learning Rate: 0.000114829
	LOSS [training: 0.003458325573282717 | validation: -0.010325204497953419]
	TIME [epoch: 8.17 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035518427018104886		[learning rate: 0.00011469]
		[batch 20/20] avg loss: 0.0011656541436483206		[learning rate: 0.00011455]
	Learning Rate: 0.000114551
	LOSS [training: -0.0011930942790810831 | validation: -0.008431095583697737]
	TIME [epoch: 8.17 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00041892947067911576		[learning rate: 0.00011441]
		[batch 20/20] avg loss: -0.0005884457924262403		[learning rate: 0.00011427]
	Learning Rate: 0.000114274
	LOSS [training: -0.000503687631552678 | validation: -0.009772009907119522]
	TIME [epoch: 8.19 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012953670485728026		[learning rate: 0.00011414]
		[batch 20/20] avg loss: 0.004648285198523368		[learning rate: 0.000114]
	Learning Rate: 0.000113997
	LOSS [training: 0.0029718261235480853 | validation: -0.011435532233529316]
	TIME [epoch: 8.16 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001802883627960277		[learning rate: 0.00011386]
		[batch 20/20] avg loss: 0.005095551386754266		[learning rate: 0.00011372]
	Learning Rate: 0.000113721
	LOSS [training: 0.0034492175073572717 | validation: -0.012600730569786516]
	TIME [epoch: 8.15 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010422529721419063		[learning rate: 0.00011358]
		[batch 20/20] avg loss: -0.0004370457709792659		[learning rate: 0.00011345]
	Learning Rate: 0.000113446
	LOSS [training: 0.00030260360058132006 | validation: -0.010620432249547541]
	TIME [epoch: 8.19 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006978503266760408		[learning rate: 0.00011331]
		[batch 20/20] avg loss: 0.004188889515510203		[learning rate: 0.00011317]
	Learning Rate: 0.000113171
	LOSS [training: 0.0017455195944170818 | validation: -0.011045942636283739]
	TIME [epoch: 8.18 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009728654153898326		[learning rate: 0.00011303]
		[batch 20/20] avg loss: -0.0018703259079651633		[learning rate: 0.0001129]
	Learning Rate: 0.000112897
	LOSS [training: -0.001421595661677498 | validation: -0.006273898437628874]
	TIME [epoch: 8.16 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011142722189357817		[learning rate: 0.00011276]
		[batch 20/20] avg loss: -0.001319922102820454		[learning rate: 0.00011262]
	Learning Rate: 0.000112624
	LOSS [training: -0.00010282494194233608 | validation: -0.010651999283398892]
	TIME [epoch: 8.16 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001869918005299303		[learning rate: 0.00011249]
		[batch 20/20] avg loss: 0.00034490295120684084		[learning rate: 0.00011235]
	Learning Rate: 0.000112352
	LOSS [training: -0.0007625075270462311 | validation: -0.012631651715157725]
	TIME [epoch: 8.2 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027662191464266623		[learning rate: 0.00011222]
		[batch 20/20] avg loss: 0.0025143590881735543		[learning rate: 0.00011208]
	Learning Rate: 0.00011208
	LOSS [training: -0.00012593002912655418 | validation: -0.01239704640241881]
	TIME [epoch: 8.18 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030234731216149746		[learning rate: 0.00011194]
		[batch 20/20] avg loss: -0.005033290472172933		[learning rate: 0.00011181]
	Learning Rate: 0.000111808
	LOSS [training: -0.001004908675278979 | validation: -0.012248334130498077]
	TIME [epoch: 8.18 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004487859552400961		[learning rate: 0.00011167]
		[batch 20/20] avg loss: -0.0018530800378615043		[learning rate: 0.00011154]
	Learning Rate: 0.000111538
	LOSS [training: -0.0031704697951312325 | validation: -0.01034261473160631]
	TIME [epoch: 8.16 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007594640872823449		[learning rate: 0.0001114]
		[batch 20/20] avg loss: 0.0026196378702160587		[learning rate: 0.00011127]
	Learning Rate: 0.000111268
	LOSS [training: 0.000930086891466857 | validation: -0.01486900982736463]
	TIME [epoch: 8.16 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003352815970346799		[learning rate: 0.00011113]
		[batch 20/20] avg loss: 0.004574391723901521		[learning rate: 0.000111]
	Learning Rate: 0.000110998
	LOSS [training: 0.00396360384712416 | validation: -0.01586607467075843]
	TIME [epoch: 8.17 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003316169402239505		[learning rate: 0.00011086]
		[batch 20/20] avg loss: 0.003558457169701528		[learning rate: 0.00011073]
	Learning Rate: 0.000110729
	LOSS [training: 0.00012114388373101189 | validation: -0.016797233134694507]
	TIME [epoch: 8.18 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025178625524099074		[learning rate: 0.0001106]
		[batch 20/20] avg loss: -0.0012731745578951374		[learning rate: 0.00011046]
	Learning Rate: 0.000110461
	LOSS [training: 0.0006223439972573849 | validation: -0.0097552341743599]
	TIME [epoch: 8.17 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016702257122045085		[learning rate: 0.00011033]
		[batch 20/20] avg loss: 0.0008917823052399191		[learning rate: 0.00011019]
	Learning Rate: 0.000110194
	LOSS [training: 0.0012810040087222138 | validation: -0.01854947956679315]
	TIME [epoch: 8.17 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012551581498140964		[learning rate: 0.00011006]
		[batch 20/20] avg loss: 0.0031597000844448657		[learning rate: 0.00010993]
	Learning Rate: 0.000109927
	LOSS [training: 0.0009522709673153848 | validation: -0.012270115567557307]
	TIME [epoch: 8.19 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009414660071740196		[learning rate: 0.00010979]
		[batch 20/20] avg loss: -0.0013124451893197526		[learning rate: 0.00010966]
	Learning Rate: 0.000109661
	LOSS [training: -0.00018548959107286643 | validation: -0.0051057648699333975]
	TIME [epoch: 8.19 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014970684521538671		[learning rate: 0.00010953]
		[batch 20/20] avg loss: 0.0010950895328291441		[learning rate: 0.0001094]
	Learning Rate: 0.000109396
	LOSS [training: -0.00020098945966236156 | validation: -0.012195701360369356]
	TIME [epoch: 8.17 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007704635717490033		[learning rate: 0.00010926]
		[batch 20/20] avg loss: -0.007789292748832178		[learning rate: 0.00010913]
	Learning Rate: 0.000109131
	LOSS [training: -0.0042798781602905915 | validation: -0.016120986848707348]
	TIME [epoch: 8.18 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000150246531706778		[learning rate: 0.000109]
		[batch 20/20] avg loss: -0.0007817075776355286		[learning rate: 0.00010887]
	Learning Rate: 0.000108867
	LOSS [training: -0.00031573052296437527 | validation: -0.008123013950518374]
	TIME [epoch: 8.16 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002836197297226599		[learning rate: 0.00010873]
		[batch 20/20] avg loss: 0.00022700954067359397		[learning rate: 0.0001086]
	Learning Rate: 0.000108603
	LOSS [training: -0.0013045938782765025 | validation: -0.004333952153279158]
	TIME [epoch: 8.16 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017360495349022123		[learning rate: 0.00010847]
		[batch 20/20] avg loss: 9.223597468908957e-05		[learning rate: 0.00010834]
	Learning Rate: 0.00010834
	LOSS [training: 0.0009141427547956508 | validation: -0.0104884884091502]
	TIME [epoch: 8.19 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038056202283660726		[learning rate: 0.00010821]
		[batch 20/20] avg loss: -0.0018264206929916274		[learning rate: 0.00010808]
	Learning Rate: 0.000108078
	LOSS [training: 0.0009895997676872222 | validation: -0.006773205680243769]
	TIME [epoch: 8.17 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011780752153113892		[learning rate: 0.00010795]
		[batch 20/20] avg loss: -0.0012290387465545723		[learning rate: 0.00010782]
	Learning Rate: 0.000107816
	LOSS [training: -0.0012035569809329808 | validation: -0.005476527280711291]
	TIME [epoch: 8.17 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018105896718498258		[learning rate: 0.00010769]
		[batch 20/20] avg loss: 0.0012826234732275035		[learning rate: 0.00010756]
	Learning Rate: 0.000107555
	LOSS [training: -0.0002639830993111611 | validation: -0.00842773396344327]
	TIME [epoch: 8.16 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006528851867299655		[learning rate: 0.00010742]
		[batch 20/20] avg loss: -0.00018214779908098518		[learning rate: 0.00010729]
	Learning Rate: 0.000107295
	LOSS [training: 0.003173352034109335 | validation: -0.007850471198664813]
	TIME [epoch: 8.18 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014848697438903196		[learning rate: 0.00010716]
		[batch 20/20] avg loss: 0.0025439605465711152		[learning rate: 0.00010704]
	Learning Rate: 0.000107035
	LOSS [training: 0.0020144151452307175 | validation: -0.009414602267971786]
	TIME [epoch: 8.16 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00408066309662099		[learning rate: 0.00010691]
		[batch 20/20] avg loss: 0.00021260879441028856		[learning rate: 0.00010678]
	Learning Rate: 0.000106776
	LOSS [training: 0.002146635945515639 | validation: -0.013510963705254654]
	TIME [epoch: 8.2 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019659044578306793		[learning rate: 0.00010665]
		[batch 20/20] avg loss: 0.003315046182100721		[learning rate: 0.00010652]
	Learning Rate: 0.000106518
	LOSS [training: 0.0026404753199657004 | validation: -0.00934710544924972]
	TIME [epoch: 8.17 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014083705225461903		[learning rate: 0.00010639]
		[batch 20/20] avg loss: 0.005260546233587402		[learning rate: 0.00010626]
	Learning Rate: 0.00010626
	LOSS [training: 0.001926087855520605 | validation: -0.012242542176371102]
	TIME [epoch: 8.18 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002896559682140966		[learning rate: 0.00010613]
		[batch 20/20] avg loss: 0.0006606836828980746		[learning rate: 0.000106]
	Learning Rate: 0.000106002
	LOSS [training: 0.0017786216825195205 | validation: -0.00780879631432657]
	TIME [epoch: 8.19 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001975683500118186		[learning rate: 0.00010587]
		[batch 20/20] avg loss: 0.0018799873580924455		[learning rate: 0.00010575]
	Learning Rate: 0.000105746
	LOSS [training: 0.0019278354291053159 | validation: -0.005000252261072206]
	TIME [epoch: 8.2 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: -9.267047157613412e-05		[learning rate: 0.00010562]
		[batch 20/20] avg loss: -0.003809690203041469		[learning rate: 0.00010549]
	Learning Rate: 0.00010549
	LOSS [training: -0.0019511803373088016 | validation: -0.008631511280754851]
	TIME [epoch: 8.17 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015159373180788245		[learning rate: 0.00010536]
		[batch 20/20] avg loss: 0.0009539066775665374		[learning rate: 0.00010523]
	Learning Rate: 0.000105234
	LOSS [training: 0.0012349219978226808 | validation: -0.005600878818474805]
	TIME [epoch: 8.16 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010790587134363139		[learning rate: 0.00010511]
		[batch 20/20] avg loss: 0.000835083167616731		[learning rate: 0.00010498]
	Learning Rate: 0.00010498
	LOSS [training: -0.0001219877729097916 | validation: -0.015448477906060226]
	TIME [epoch: 8.19 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004271982879153319		[learning rate: 0.00010485]
		[batch 20/20] avg loss: 0.0037466480129967734		[learning rate: 0.00010473]
	Learning Rate: 0.000104726
	LOSS [training: 0.004009315446075046 | validation: -0.015114648749944292]
	TIME [epoch: 8.17 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001365632204868632		[learning rate: 0.0001046]
		[batch 20/20] avg loss: 0.0004370190188863112		[learning rate: 0.00010447]
	Learning Rate: 0.000104472
	LOSS [training: -0.00046430659299116035 | validation: -0.006526707806750715]
	TIME [epoch: 8.16 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00020377196331654778		[learning rate: 0.00010435]
		[batch 20/20] avg loss: 0.0014971807436788874		[learning rate: 0.00010422]
	Learning Rate: 0.000104219
	LOSS [training: 0.0006467043901811698 | validation: -0.018818347723337004]
	TIME [epoch: 8.16 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004344856821619578		[learning rate: 0.00010409]
		[batch 20/20] avg loss: -0.0014740509324964123		[learning rate: 0.00010397]
	Learning Rate: 0.000103967
	LOSS [training: 0.0014354029445615834 | validation: -0.006248257130752655]
	TIME [epoch: 8.21 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002560496758889581		[learning rate: 0.00010384]
		[batch 20/20] avg loss: 0.005328176720722154		[learning rate: 0.00010372]
	Learning Rate: 0.000103715
	LOSS [training: 0.003944336739805867 | validation: -0.002862958203969296]
	TIME [epoch: 8.17 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006649804227977653		[learning rate: 0.00010359]
		[batch 20/20] avg loss: 0.0035882272408293754		[learning rate: 0.00010346]
	Learning Rate: 0.000103464
	LOSS [training: 0.005119015734403513 | validation: -0.009287253565545958]
	TIME [epoch: 8.17 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003105374490342962		[learning rate: 0.00010334]
		[batch 20/20] avg loss: 0.001812214669304675		[learning rate: 0.00010321]
	Learning Rate: 0.000103214
	LOSS [training: 0.002458794579823819 | validation: -0.007332382357861733]
	TIME [epoch: 8.18 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020534011329015616		[learning rate: 0.00010309]
		[batch 20/20] avg loss: -0.0011799078549160175		[learning rate: 0.00010296]
	Learning Rate: 0.000102964
	LOSS [training: 0.000436746638992772 | validation: -0.010143882226803111]
	TIME [epoch: 8.2 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015934862196471048		[learning rate: 0.00010284]
		[batch 20/20] avg loss: 0.0004312120406926736		[learning rate: 0.00010271]
	Learning Rate: 0.000102714
	LOSS [training: -0.0005811370894772154 | validation: -0.005149638985200318]
	TIME [epoch: 8.19 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006752218836299488		[learning rate: 0.00010259]
		[batch 20/20] avg loss: 0.0024559621995148394		[learning rate: 0.00010247]
	Learning Rate: 0.000102466
	LOSS [training: 0.004604090517907164 | validation: -0.013428652806290079]
	TIME [epoch: 8.16 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018875844679974592		[learning rate: 0.00010234]
		[batch 20/20] avg loss: 0.002915895940955284		[learning rate: 0.00010222]
	Learning Rate: 0.000102218
	LOSS [training: 0.0005141557364789122 | validation: -0.0065848275260136455]
	TIME [epoch: 8.17 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033963603377476447		[learning rate: 0.00010209]
		[batch 20/20] avg loss: -0.00047824295633359546		[learning rate: 0.00010197]
	Learning Rate: 0.00010197
	LOSS [training: 0.0014590586907070244 | validation: -0.009243773529787124]
	TIME [epoch: 8.16 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020170161594872974		[learning rate: 0.00010185]
		[batch 20/20] avg loss: 0.0003249771141265134		[learning rate: 0.00010172]
	Learning Rate: 0.000101723
	LOSS [training: -0.0008460195226803922 | validation: -0.01628212503521796]
	TIME [epoch: 8.19 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033990152513092356		[learning rate: 0.0001016]
		[batch 20/20] avg loss: -0.0006173587455688362		[learning rate: 0.00010148]
	Learning Rate: 0.000101477
	LOSS [training: 0.0013908282528701998 | validation: -0.01138822895209008]
	TIME [epoch: 8.17 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017861657755034205		[learning rate: 0.00010135]
		[batch 20/20] avg loss: 0.005846204637803965		[learning rate: 0.00010123]
	Learning Rate: 0.000101232
	LOSS [training: 0.0020300194311502727 | validation: -0.012509891702356463]
	TIME [epoch: 8.17 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026799865619519564		[learning rate: 0.00010111]
		[batch 20/20] avg loss: -0.0034701368205653166		[learning rate: 0.00010099]
	Learning Rate: 0.000100986
	LOSS [training: -0.0003950751293066802 | validation: -0.01015529111606811]
	TIME [epoch: 8.19 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002579600696532232		[learning rate: 0.00010086]
		[batch 20/20] avg loss: -0.00244178501282146		[learning rate: 0.00010074]
	Learning Rate: 0.000100742
	LOSS [training: 6.890784185538592e-05 | validation: -0.007323517916185804]
	TIME [epoch: 8.18 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005430494773413673		[learning rate: 0.00010062]
		[batch 20/20] avg loss: -0.0018663732319839448		[learning rate: 0.0001005]
	Learning Rate: 0.000100498
	LOSS [training: 0.0017820607707148648 | validation: -0.010305048995069318]
	TIME [epoch: 8.19 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022472330408065755		[learning rate: 0.00010038]
		[batch 20/20] avg loss: -0.0018425861879856245		[learning rate: 0.00010025]
	Learning Rate: 0.000100255
	LOSS [training: -0.0020449096143961 | validation: -0.014786935175782561]
	TIME [epoch: 8.17 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002267767586504598		[learning rate: 0.00010013]
		[batch 20/20] avg loss: -0.0016102507277066676		[learning rate: 0.00010001]
	Learning Rate: 0.000100012
	LOSS [training: 0.00032875842939896534 | validation: -0.008881024729386788]
	TIME [epoch: 8.2 sec]
Finished training in 16492.987 seconds.
