Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r2', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2998246816

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.282252154474987		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.568275090098926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.4252636222869555 | validation: 5.740705609174371]
	TIME [epoch: 53 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.331927020693522		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.864897448982765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.098412234838143 | validation: 4.083885524494317]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5038483006772125		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5156518795333787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.509750090105296 | validation: 2.883284281518511]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4299615498948053		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9121935025010264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1710775261979154 | validation: 1.6747774268423403]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7560408312874014		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.10176996267002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.928905396978711 | validation: 1.3679450082555484]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5200614497731184		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.581162933481448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5506121916272833 | validation: 1.0761931135572405]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2232090106298694		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5064304111455553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3648197108877123 | validation: 0.9442985893827447]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1258209737770974		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8642916289051928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9950563013411451 | validation: 0.658539425158154]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0911950862463082		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9562636566306155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0237293714384617 | validation: 0.6786171775065469]
	TIME [epoch: 8.2 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7824843126609252		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9377528282080345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8601185704344797 | validation: 0.4559480625210836]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7979423761519		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8391604982496302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8185514372007653 | validation: 0.7193322119062827]
	TIME [epoch: 8.22 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5966073475479792		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6283473498414832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6124773486947312 | validation: 0.5153975680273527]
	TIME [epoch: 8.21 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7042483840499931		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.561482700982161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.632865542516077 | validation: 1.9575504267339459]
	TIME [epoch: 8.21 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7587057492363238		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5699352107685655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6643204800024446 | validation: 0.5858012299433022]
	TIME [epoch: 8.19 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5631889456104147		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5802165112121117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5717027284112631 | validation: 0.5228299650487337]
	TIME [epoch: 8.21 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5596363059469576		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8602039509271606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7099201284370591 | validation: 0.5763069787555315]
	TIME [epoch: 8.19 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5562288937415648		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4645505785989375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5103897361702512 | validation: 0.771050883758699]
	TIME [epoch: 8.21 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6258822314167578		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5414770660531134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5836796487349357 | validation: 0.6358328766774552]
	TIME [epoch: 8.19 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5263255356960717		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5503828980371448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5383542168666083 | validation: 0.9529062792638388]
	TIME [epoch: 8.21 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5424001894917968		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5773102606022127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5598552250470049 | validation: 0.7257065911297753]
	TIME [epoch: 8.22 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4859379940934506		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5299855491284347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5079617716109426 | validation: 0.48236723385993197]
	TIME [epoch: 8.21 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4818716010727705		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4715202184351942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47669590975398235 | validation: 0.40770957719401857]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4863870897740168		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46764437119973223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47701573048687457 | validation: 0.44242956137202166]
	TIME [epoch: 8.21 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47557402275618854		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6186780761407461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5471260494484673 | validation: 0.7002074648345086]
	TIME [epoch: 8.19 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5945410510735601		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38516356621915726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48985230864635865 | validation: 0.3768011971580969]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4979161990174287		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5079213688367299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5029187839270792 | validation: 0.6487048596951707]
	TIME [epoch: 8.19 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47319988300419313		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5342069467218241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5037034148630088 | validation: 0.4483283988001733]
	TIME [epoch: 8.19 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5291049824679399		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4439982739075205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48655162818773035 | validation: 0.4309112716531882]
	TIME [epoch: 8.23 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45574718245726675		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.44719628553599594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4514717339966315 | validation: 0.5835151014355362]
	TIME [epoch: 8.22 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5887811906403377		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3555897552331121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47218547293672486 | validation: 0.4049133983367492]
	TIME [epoch: 8.18 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.442520207713964		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3905524096280943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41653630867102914 | validation: 0.27931413331708865]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41896309447568497		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3435466714863051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38125488298099497 | validation: 0.28072647181775423]
	TIME [epoch: 8.22 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4065289442763061		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.42509276663665796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.415810855456482 | validation: 0.3143566689152176]
	TIME [epoch: 8.21 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3154802318357502		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3447089374684093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33009458465207975 | validation: 0.2384611223082927]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36523043618873957		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3494485194684572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35733947782859843 | validation: 0.36603029438453766]
	TIME [epoch: 8.19 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5875514149403935		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28108651282142466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43431896388090907 | validation: 0.36640884224367837]
	TIME [epoch: 8.24 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5512878656808572		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3831300280976273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46720894688924225 | validation: 0.2428365942781664]
	TIME [epoch: 8.2 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.360579257896876		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3070251049333805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3338021814151283 | validation: 0.3380481023742719]
	TIME [epoch: 8.19 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3232975725955406		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22581098898254576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2745542807890432 | validation: 0.1560920944914461]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46262551993582524		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25599939500908303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3593124574724541 | validation: 0.36773341441842244]
	TIME [epoch: 8.22 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2605026117286869		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25698374370490834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25874317771679756 | validation: 0.1758018895474885]
	TIME [epoch: 8.19 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2483385605687572		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2986332112927971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27348588593077705 | validation: 0.2476079981797465]
	TIME [epoch: 8.22 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3319203729208744		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24417904226212767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.288049707591501 | validation: 1.0955101085379513]
	TIME [epoch: 8.19 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38668824147611247		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21721994357085364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30195409252348304 | validation: 0.17249872829192345]
	TIME [epoch: 8.21 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2614573683897252		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20876798360167972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23511267599570246 | validation: 0.17803642615323362]
	TIME [epoch: 8.25 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27228439028458523		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.35537489635166675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31382964331812596 | validation: 0.4843145077738977]
	TIME [epoch: 8.18 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2442214542331044		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5686821094111922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4064517818221483 | validation: 0.37963258492181484]
	TIME [epoch: 8.18 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6094146512542893		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28171092059663305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4455627859254611 | validation: 0.21429110863593773]
	TIME [epoch: 8.19 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4617962737586464		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1802472746261053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3210217741923758 | validation: 0.2445877469443895]
	TIME [epoch: 8.21 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1797259366969065		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14203162151129564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1608787791041011 | validation: 0.17621258926446698]
	TIME [epoch: 8.2 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23408847010057307		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24107860149045765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2375835357955154 | validation: 0.1571657210833803]
	TIME [epoch: 8.19 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3346804541018539		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14573279439331402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24020662424758393 | validation: 0.10445272644703495]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15836702857867754		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22842632714688965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1933966778627836 | validation: 0.4270769022286319]
	TIME [epoch: 8.23 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7876224525900818		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22262661444439402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.505124533517238 | validation: 0.11100457149976692]
	TIME [epoch: 8.22 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18603174185287308		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18839235426300838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18721204805794076 | validation: 0.7120118991248827]
	TIME [epoch: 8.18 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29933600307881975		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29762845677772437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.298482229928272 | validation: 0.29537727847986905]
	TIME [epoch: 8.18 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44620239463244527		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.34334715029955687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3947747724660012 | validation: 0.3200603657712446]
	TIME [epoch: 8.19 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23186759334039403		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32097669485049957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27642214409544674 | validation: 0.2011083145988465]
	TIME [epoch: 8.2 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15686965349800505		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.50174481092173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3293072322098676 | validation: 0.4620807075169822]
	TIME [epoch: 8.21 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2357640689227769		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1771272042425078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20644563658264237 | validation: 0.19261633530167255]
	TIME [epoch: 8.19 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1907779564275573		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23332274096437425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2120503486959658 | validation: 0.09507333324917858]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.197734153261941		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23328612701106494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21551014013650294 | validation: 0.15248859191535322]
	TIME [epoch: 8.22 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20732290696638792		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19276667713328463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20004479204983633 | validation: 0.28291437232810224]
	TIME [epoch: 8.21 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3323090409183567		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20653258134680558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26942081113258115 | validation: 0.3834738788153679]
	TIME [epoch: 8.18 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22467906473032814		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28226056010877937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2534698124195538 | validation: 0.23130360834229374]
	TIME [epoch: 8.19 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15494265154137923		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21956608106351766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18725436630244846 | validation: 0.1343308254872667]
	TIME [epoch: 8.19 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19562617960227602		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6706697457028887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9331479626525823 | validation: 3.4479212887004866]
	TIME [epoch: 8.2 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7207813319897562		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.966013398864225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8433973654269904 | validation: 4.468659704881032]
	TIME [epoch: 8.44 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1038838916782363		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9536503547768074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0287671232275217 | validation: 0.5237753216589202]
	TIME [epoch: 8.2 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6477925640498446		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8246769374625831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7362347507562138 | validation: 0.9273758503840464]
	TIME [epoch: 8.25 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.59313645745981		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4637801740072793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5284583157335447 | validation: 0.37815373975181643]
	TIME [epoch: 8.2 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42249575408061507		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4513959993578438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43694587671922935 | validation: 0.2996237874434876]
	TIME [epoch: 8.19 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1054592510807182		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6788914658851288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8921753584829235 | validation: 1.2020829047792745]
	TIME [epoch: 8.19 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6418015683623246		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3211198754508355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48146072190657996 | validation: 0.25350759288481917]
	TIME [epoch: 8.21 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26198414434008904		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7373293561754573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4996567502577731 | validation: 0.48160619872028804]
	TIME [epoch: 8.21 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31024613101776083		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32196127047772755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31610370074774413 | validation: 0.26567616044262776]
	TIME [epoch: 8.2 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.444434746035832		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3330675981762361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38875117210603405 | validation: 0.2755919737999273]
	TIME [epoch: 8.2 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33989501369880726		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30385702054013547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32187601711947134 | validation: 0.33235916959924]
	TIME [epoch: 8.25 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2928379380998134		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4128472862168218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3528426121583175 | validation: 0.3276493294959516]
	TIME [epoch: 8.19 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34202850165689636		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26933742333820004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3056829624975482 | validation: 0.21734160620940426]
	TIME [epoch: 8.18 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23427646041145236		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2828921305667341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25858429548909323 | validation: 0.5374295514029505]
	TIME [epoch: 8.19 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34318248325669687		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46999990542650727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40659119434160207 | validation: 0.22539966766904196]
	TIME [epoch: 8.22 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2659811269273205		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28559192622125246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2757865265742865 | validation: 0.3099662542428163]
	TIME [epoch: 8.19 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21029476631545618		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2597766911415921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23503572872852413 | validation: 0.22574144113865038]
	TIME [epoch: 8.21 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6195610735012872		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3016749471698232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4606180103355553 | validation: 2.8675940480654916]
	TIME [epoch: 8.2 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6070208998943167		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2278135486856265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9174172242899714 | validation: 2.764301381101705]
	TIME [epoch: 8.22 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1159728983789108		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4534280517500334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7847004750644722 | validation: 0.6060874228745801]
	TIME [epoch: 8.2 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44669701048601923		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2766604395597456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36167872502288234 | validation: 0.3095276484417488]
	TIME [epoch: 8.22 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.400521531046601		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23444614070866393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3174838358776324 | validation: 0.17950381570451157]
	TIME [epoch: 8.19 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29397934051654817		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2751172613034082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2845483009099782 | validation: 0.3584914640663126]
	TIME [epoch: 8.2 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3080680425187831		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2972798805342212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3026739615265021 | validation: 1.555691581785985]
	TIME [epoch: 8.19 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5342323209872195		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40934863514051106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4717904780638653 | validation: 0.4985908123346536]
	TIME [epoch: 8.18 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23943337513579133		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20011893009968373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21977615261773747 | validation: 0.17615206249423085]
	TIME [epoch: 8.21 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35630797539525555		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6923327878572599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5243203816262577 | validation: 0.6048139036296469]
	TIME [epoch: 8.22 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28303077000242094		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24496663428541562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2639987021439183 | validation: 0.2349786661147688]
	TIME [epoch: 8.19 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15968099747559308		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19368666240180007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17668382993869655 | validation: 0.0989414312099023]
	TIME [epoch: 8.22 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2111151921037128		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21761596358568017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2143655778446965 | validation: 0.2792568814004382]
	TIME [epoch: 8.18 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2391764632378231		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19568516347524584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21743081335653444 | validation: 0.23033949907423754]
	TIME [epoch: 8.2 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23650600627610321		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14423675776561246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19037138202085785 | validation: 0.19242399853874764]
	TIME [epoch: 8.18 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17208516594780804		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1598382940353059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16596172999155692 | validation: 0.3377721043186446]
	TIME [epoch: 8.18 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2630671148866026		[learning rate: 0.0099891]
		[batch 20/20] avg loss: 0.22056338679367973		[learning rate: 0.009977]
	Learning Rate: 0.009977
	LOSS [training: 0.24181525084014122 | validation: 0.13523545693268982]
	TIME [epoch: 8.18 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.134899694495696		[learning rate: 0.0099649]
		[batch 20/20] avg loss: 0.22064780192618638		[learning rate: 0.0099528]
	Learning Rate: 0.00995285
	LOSS [training: 0.1777737482109412 | validation: 0.4162632571119512]
	TIME [epoch: 8.23 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1870392924353798		[learning rate: 0.0099408]
		[batch 20/20] avg loss: 0.18169799018509886		[learning rate: 0.0099288]
	Learning Rate: 0.00992875
	LOSS [training: 0.1843686413102393 | validation: 0.2849923865849487]
	TIME [epoch: 8.19 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19008051153224648		[learning rate: 0.0099167]
		[batch 20/20] avg loss: 0.20939908315607236		[learning rate: 0.0099047]
	Learning Rate: 0.00990472
	LOSS [training: 0.19973979734415942 | validation: 0.1721008935648696]
	TIME [epoch: 8.18 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15413866169716536		[learning rate: 0.0098927]
		[batch 20/20] avg loss: 0.1889317124633541		[learning rate: 0.0098807]
	Learning Rate: 0.00988074
	LOSS [training: 0.1715351870802597 | validation: 0.17654961279132517]
	TIME [epoch: 8.21 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18628934959127869		[learning rate: 0.0098688]
		[batch 20/20] avg loss: 0.22844005686521304		[learning rate: 0.0098568]
	Learning Rate: 0.00985682
	LOSS [training: 0.20736470322824588 | validation: 0.08498556466694121]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24804371960198496		[learning rate: 0.0098449]
		[batch 20/20] avg loss: 0.276589904971159		[learning rate: 0.009833]
	Learning Rate: 0.00983296
	LOSS [training: 0.262316812286572 | validation: 0.12458564455799528]
	TIME [epoch: 8.18 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12794358819945856		[learning rate: 0.009821]
		[batch 20/20] avg loss: 0.13478249944165466		[learning rate: 0.0098092]
	Learning Rate: 0.00980915
	LOSS [training: 0.13136304382055664 | validation: 0.19991602890442908]
	TIME [epoch: 8.17 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25136124740575905		[learning rate: 0.0097973]
		[batch 20/20] avg loss: 0.20345322170356636		[learning rate: 0.0097854]
	Learning Rate: 0.00978541
	LOSS [training: 0.22740723455466272 | validation: 0.12663856233380408]
	TIME [epoch: 8.17 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12481930406858785		[learning rate: 0.0097736]
		[batch 20/20] avg loss: 0.18825081958129078		[learning rate: 0.0097617]
	Learning Rate: 0.00976172
	LOSS [training: 0.15653506182493931 | validation: 0.1421091337403453]
	TIME [epoch: 8.21 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1460281302924557		[learning rate: 0.0097499]
		[batch 20/20] avg loss: 0.41149090889957807		[learning rate: 0.0097381]
	Learning Rate: 0.00973809
	LOSS [training: 0.27875951959601686 | validation: 0.5202723822701188]
	TIME [epoch: 8.2 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4826490956858954		[learning rate: 0.0097263]
		[batch 20/20] avg loss: 0.29752493503114896		[learning rate: 0.0097145]
	Learning Rate: 0.00971451
	LOSS [training: 0.39008701535852225 | validation: 0.21527904540048282]
	TIME [epoch: 8.17 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21676346557885165		[learning rate: 0.0097027]
		[batch 20/20] avg loss: 0.15285541878727998		[learning rate: 0.009691]
	Learning Rate: 0.009691
	LOSS [training: 0.18480944218306583 | validation: 0.27998276244386966]
	TIME [epoch: 8.18 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20824725195488383		[learning rate: 0.0096793]
		[batch 20/20] avg loss: 0.16195766800890365		[learning rate: 0.0096675]
	Learning Rate: 0.00966754
	LOSS [training: 0.18510245998189373 | validation: 0.16924729626083593]
	TIME [epoch: 8.24 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1898728578256988		[learning rate: 0.0096558]
		[batch 20/20] avg loss: 0.23423851314553348		[learning rate: 0.0096441]
	Learning Rate: 0.00964413
	LOSS [training: 0.2120556854856161 | validation: 0.156371236362878]
	TIME [epoch: 8.18 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10784619269623778		[learning rate: 0.0096325]
		[batch 20/20] avg loss: 0.48113707679053297		[learning rate: 0.0096208]
	Learning Rate: 0.00962078
	LOSS [training: 0.29449163474338536 | validation: 0.2312604050227023]
	TIME [epoch: 8.17 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20258761116356755		[learning rate: 0.0096091]
		[batch 20/20] avg loss: 0.12853080612251455		[learning rate: 0.0095975]
	Learning Rate: 0.00959749
	LOSS [training: 0.16555920864304105 | validation: 0.14754080154882082]
	TIME [epoch: 8.17 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22624420892572314		[learning rate: 0.0095859]
		[batch 20/20] avg loss: 0.1639400036032486		[learning rate: 0.0095743]
	Learning Rate: 0.00957426
	LOSS [training: 0.19509210626448587 | validation: 0.11673034365806671]
	TIME [epoch: 8.2 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27378183716355664		[learning rate: 0.0095627]
		[batch 20/20] avg loss: 0.18692217341821532		[learning rate: 0.0095511]
	Learning Rate: 0.00955108
	LOSS [training: 0.23035200529088598 | validation: 0.412800983170282]
	TIME [epoch: 8.19 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23602694606751157		[learning rate: 0.0095395]
		[batch 20/20] avg loss: 0.12817548152773484		[learning rate: 0.009528]
	Learning Rate: 0.00952796
	LOSS [training: 0.1821012137976232 | validation: 0.19045668365650112]
	TIME [epoch: 8.19 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2610740035560281		[learning rate: 0.0095164]
		[batch 20/20] avg loss: 0.18242781661684027		[learning rate: 0.0095049]
	Learning Rate: 0.0095049
	LOSS [training: 0.22175091008643416 | validation: 0.09791768325122739]
	TIME [epoch: 8.18 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1354813646696106		[learning rate: 0.0094934]
		[batch 20/20] avg loss: 0.15481472769579283		[learning rate: 0.0094819]
	Learning Rate: 0.00948189
	LOSS [training: 0.1451480461827017 | validation: 0.09859990434430992]
	TIME [epoch: 8.2 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16566455912466396		[learning rate: 0.0094704]
		[batch 20/20] avg loss: 0.1891088344244912		[learning rate: 0.0094589]
	Learning Rate: 0.00945893
	LOSS [training: 0.17738669677457758 | validation: 0.16653488794007648]
	TIME [epoch: 8.22 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13757146847866047		[learning rate: 0.0094475]
		[batch 20/20] avg loss: 0.1557026453224614		[learning rate: 0.009436]
	Learning Rate: 0.00943603
	LOSS [training: 0.14663705690056092 | validation: 0.17739362415063264]
	TIME [epoch: 8.17 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15095864823155547		[learning rate: 0.0094246]
		[batch 20/20] avg loss: 0.34420883961761434		[learning rate: 0.0094132]
	Learning Rate: 0.00941319
	LOSS [training: 0.24758374392458493 | validation: 0.3671710290465806]
	TIME [epoch: 8.17 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22027976537164817		[learning rate: 0.0094018]
		[batch 20/20] avg loss: 0.15757277153082447		[learning rate: 0.0093904]
	Learning Rate: 0.0093904
	LOSS [training: 0.1889262684512363 | validation: 0.1722800968143244]
	TIME [epoch: 8.2 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13248392991559424		[learning rate: 0.009379]
		[batch 20/20] avg loss: 0.16364746916058842		[learning rate: 0.0093677]
	Learning Rate: 0.00936767
	LOSS [training: 0.14806569953809134 | validation: 0.1929494637275728]
	TIME [epoch: 8.2 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2817215253498335		[learning rate: 0.0093563]
		[batch 20/20] avg loss: 0.12862176188161503		[learning rate: 0.009345]
	Learning Rate: 0.00934499
	LOSS [training: 0.20517164361572426 | validation: 0.19974674876757134]
	TIME [epoch: 8.18 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15044570854575845		[learning rate: 0.0093337]
		[batch 20/20] avg loss: 0.1316648301698063		[learning rate: 0.0093224]
	Learning Rate: 0.00932237
	LOSS [training: 0.1410552693577824 | validation: 0.23820733306535902]
	TIME [epoch: 8.18 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1679539539839392		[learning rate: 0.0093111]
		[batch 20/20] avg loss: 0.16807396878769704		[learning rate: 0.0092998]
	Learning Rate: 0.0092998
	LOSS [training: 0.1680139613858181 | validation: 0.13749581288530985]
	TIME [epoch: 8.23 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13853019333722183		[learning rate: 0.0092885]
		[batch 20/20] avg loss: 0.15773488870862498		[learning rate: 0.0092773]
	Learning Rate: 0.00927729
	LOSS [training: 0.1481325410229234 | validation: 0.14664486183538847]
	TIME [epoch: 8.2 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14176727356634788		[learning rate: 0.0092661]
		[batch 20/20] avg loss: 0.13663453308967777		[learning rate: 0.0092548]
	Learning Rate: 0.00925483
	LOSS [training: 0.1392009033280128 | validation: 0.09822470124377407]
	TIME [epoch: 8.17 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16616873341060748		[learning rate: 0.0092436]
		[batch 20/20] avg loss: 0.13095462320259904		[learning rate: 0.0092324]
	Learning Rate: 0.00923242
	LOSS [training: 0.14856167830660322 | validation: 0.0826565290024146]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14201178172150122		[learning rate: 0.0092212]
		[batch 20/20] avg loss: 0.18680580067904534		[learning rate: 0.0092101]
	Learning Rate: 0.00921007
	LOSS [training: 0.16440879120027327 | validation: 0.21584448945120793]
	TIME [epoch: 8.2 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17011500841454566		[learning rate: 0.0091989]
		[batch 20/20] avg loss: 0.38985303636073365		[learning rate: 0.0091878]
	Learning Rate: 0.00918778
	LOSS [training: 0.27998402238763964 | validation: 0.8855350130457462]
	TIME [epoch: 8.17 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30154807621339563		[learning rate: 0.0091767]
		[batch 20/20] avg loss: 0.1334802296735333		[learning rate: 0.0091655]
	Learning Rate: 0.00916554
	LOSS [training: 0.2175141529434645 | validation: 0.22324212469479168]
	TIME [epoch: 8.16 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1912590281559802		[learning rate: 0.0091544]
		[batch 20/20] avg loss: 0.19338812273373027		[learning rate: 0.0091433]
	Learning Rate: 0.00914335
	LOSS [training: 0.19232357544485523 | validation: 0.2931420159342769]
	TIME [epoch: 8.16 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1647296670218952		[learning rate: 0.0091323]
		[batch 20/20] avg loss: 0.3249190778195354		[learning rate: 0.0091212]
	Learning Rate: 0.00912121
	LOSS [training: 0.24482437242071525 | validation: 0.10646456865261122]
	TIME [epoch: 8.23 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17958303314402174		[learning rate: 0.0091102]
		[batch 20/20] avg loss: 0.16632401585331674		[learning rate: 0.0090991]
	Learning Rate: 0.00909913
	LOSS [training: 0.17295352449866924 | validation: 0.21078970343521608]
	TIME [epoch: 8.16 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20884551533261392		[learning rate: 0.0090881]
		[batch 20/20] avg loss: 0.1938146465152155		[learning rate: 0.0090771]
	Learning Rate: 0.0090771
	LOSS [training: 0.20133008092391474 | validation: 0.23461261962671479]
	TIME [epoch: 8.16 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12987269379178218		[learning rate: 0.0090661]
		[batch 20/20] avg loss: 0.1116706911917118		[learning rate: 0.0090551]
	Learning Rate: 0.00905513
	LOSS [training: 0.12077169249174699 | validation: 0.29087460698124956]
	TIME [epoch: 8.15 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1566148744770359		[learning rate: 0.0090442]
		[batch 20/20] avg loss: 0.12836220475592755		[learning rate: 0.0090332]
	Learning Rate: 0.00903321
	LOSS [training: 0.1424885396164817 | validation: 0.1443976727874658]
	TIME [epoch: 8.17 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15908413158394336		[learning rate: 0.0090223]
		[batch 20/20] avg loss: 0.16778190265890142		[learning rate: 0.0090113]
	Learning Rate: 0.00901134
	LOSS [training: 0.1634330171214224 | validation: 0.11775851238426935]
	TIME [epoch: 8.18 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21094919609801402		[learning rate: 0.0090004]
		[batch 20/20] avg loss: 0.18229220376101163		[learning rate: 0.0089895]
	Learning Rate: 0.00898953
	LOSS [training: 0.1966206999295128 | validation: 0.4091115714905016]
	TIME [epoch: 8.18 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3094988424299721		[learning rate: 0.0089786]
		[batch 20/20] avg loss: 0.23210220844020818		[learning rate: 0.0089678]
	Learning Rate: 0.00896776
	LOSS [training: 0.27080052543509014 | validation: 0.13344884300092028]
	TIME [epoch: 8.16 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13513320562628453		[learning rate: 0.0089569]
		[batch 20/20] avg loss: 0.7110678127371022		[learning rate: 0.0089461]
	Learning Rate: 0.00894605
	LOSS [training: 0.42310050918169334 | validation: 0.17567138890668993]
	TIME [epoch: 8.16 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14504462363853737		[learning rate: 0.0089352]
		[batch 20/20] avg loss: 0.19815921737165337		[learning rate: 0.0089244]
	Learning Rate: 0.0089244
	LOSS [training: 0.17160192050509537 | validation: 0.17916610830258967]
	TIME [epoch: 8.21 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1675744935814451		[learning rate: 0.0089136]
		[batch 20/20] avg loss: 0.20909688500666954		[learning rate: 0.0089028]
	Learning Rate: 0.00890279
	LOSS [training: 0.18833568929405728 | validation: 0.0731525092785703]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13598635125343722		[learning rate: 0.008892]
		[batch 20/20] avg loss: 0.17694298147595294		[learning rate: 0.0088812]
	Learning Rate: 0.00888124
	LOSS [training: 0.15646466636469508 | validation: 0.10160769384175797]
	TIME [epoch: 8.16 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1622046343949019		[learning rate: 0.0088705]
		[batch 20/20] avg loss: 0.19076250595301397		[learning rate: 0.0088597]
	Learning Rate: 0.00885974
	LOSS [training: 0.17648357017395794 | validation: 0.2964354572805863]
	TIME [epoch: 8.16 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1948728931841211		[learning rate: 0.008849]
		[batch 20/20] avg loss: 0.16790406328244475		[learning rate: 0.0088383]
	Learning Rate: 0.00883829
	LOSS [training: 0.18138847823328294 | validation: 0.1558798111541244]
	TIME [epoch: 8.19 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15497444892706458		[learning rate: 0.0088276]
		[batch 20/20] avg loss: 0.1662750598128777		[learning rate: 0.0088169]
	Learning Rate: 0.0088169
	LOSS [training: 0.16062475436997117 | validation: 0.2162052721677971]
	TIME [epoch: 8.18 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27106053296249283		[learning rate: 0.0088062]
		[batch 20/20] avg loss: 0.4149562174168664		[learning rate: 0.0087956]
	Learning Rate: 0.00879555
	LOSS [training: 0.34300837518967964 | validation: 0.9937387352402585]
	TIME [epoch: 8.16 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3429594803600503		[learning rate: 0.0087849]
		[batch 20/20] avg loss: 0.19051431344601027		[learning rate: 0.0087743]
	Learning Rate: 0.00877426
	LOSS [training: 0.26673689690303026 | validation: 0.11872765206262098]
	TIME [epoch: 8.17 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16585601743354234		[learning rate: 0.0087636]
		[batch 20/20] avg loss: 0.14767721364374023		[learning rate: 0.008753]
	Learning Rate: 0.00875302
	LOSS [training: 0.15676661553864127 | validation: 0.2515851816195511]
	TIME [epoch: 8.22 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29732279910901627		[learning rate: 0.0087424]
		[batch 20/20] avg loss: 0.23751461580834166		[learning rate: 0.0087318]
	Learning Rate: 0.00873183
	LOSS [training: 0.267418707458679 | validation: 0.14545115666213185]
	TIME [epoch: 8.16 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1319705360839432		[learning rate: 0.0087213]
		[batch 20/20] avg loss: 0.1452234110247303		[learning rate: 0.0087107]
	Learning Rate: 0.00871069
	LOSS [training: 0.13859697355433676 | validation: 0.10167483447716724]
	TIME [epoch: 8.15 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3424088012252856		[learning rate: 0.0087001]
		[batch 20/20] avg loss: 0.1420847666260438		[learning rate: 0.0086896]
	Learning Rate: 0.0086896
	LOSS [training: 0.24224678392566465 | validation: 0.08760169386776573]
	TIME [epoch: 8.16 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18991167299048997		[learning rate: 0.0086791]
		[batch 20/20] avg loss: 0.19323552611242856		[learning rate: 0.0086686]
	Learning Rate: 0.00866857
	LOSS [training: 0.19157359955145928 | validation: 0.13240351442955317]
	TIME [epoch: 8.21 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2047816043988384		[learning rate: 0.0086581]
		[batch 20/20] avg loss: 0.13096777666808052		[learning rate: 0.0086476]
	Learning Rate: 0.00864758
	LOSS [training: 0.16787469053345944 | validation: 0.17245041950006332]
	TIME [epoch: 8.17 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17175606755957165		[learning rate: 0.0086371]
		[batch 20/20] avg loss: 0.13963291355910365		[learning rate: 0.0086266]
	Learning Rate: 0.00862665
	LOSS [training: 0.15569449055933765 | validation: 0.15049422244011484]
	TIME [epoch: 8.17 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13549855531095156		[learning rate: 0.0086162]
		[batch 20/20] avg loss: 0.1959823328566994		[learning rate: 0.0086058]
	Learning Rate: 0.00860576
	LOSS [training: 0.16574044408382546 | validation: 0.09204322049138892]
	TIME [epoch: 8.19 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1353624809941328		[learning rate: 0.0085953]
		[batch 20/20] avg loss: 0.5091033828576189		[learning rate: 0.0085849]
	Learning Rate: 0.00858493
	LOSS [training: 0.3222329319258759 | validation: 0.3648110893849986]
	TIME [epoch: 8.19 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21908087910200172		[learning rate: 0.0085745]
		[batch 20/20] avg loss: 0.15793823201844748		[learning rate: 0.0085641]
	Learning Rate: 0.00856415
	LOSS [training: 0.18850955556022458 | validation: 0.1210790949060055]
	TIME [epoch: 8.16 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.140842817538705		[learning rate: 0.0085538]
		[batch 20/20] avg loss: 0.14717345930960862		[learning rate: 0.0085434]
	Learning Rate: 0.00854342
	LOSS [training: 0.1440081384241568 | validation: 0.309035641659589]
	TIME [epoch: 8.16 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17697431744883002		[learning rate: 0.0085331]
		[batch 20/20] avg loss: 0.17879687668963104		[learning rate: 0.0085227]
	Learning Rate: 0.00852273
	LOSS [training: 0.17788559706923052 | validation: 0.09782573274943095]
	TIME [epoch: 8.16 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24752140464824768		[learning rate: 0.0085124]
		[batch 20/20] avg loss: 0.17014708493279415		[learning rate: 0.0085021]
	Learning Rate: 0.0085021
	LOSS [training: 0.20883424479052093 | validation: 0.12710847380504303]
	TIME [epoch: 8.21 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1339538785052357		[learning rate: 0.0084918]
		[batch 20/20] avg loss: 0.14876770934385689		[learning rate: 0.0084815]
	Learning Rate: 0.00848152
	LOSS [training: 0.1413607939245463 | validation: 0.1543638259222467]
	TIME [epoch: 8.17 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14261772523313376		[learning rate: 0.0084712]
		[batch 20/20] avg loss: 0.1079770925838491		[learning rate: 0.008461]
	Learning Rate: 0.00846099
	LOSS [training: 0.12529740890849142 | validation: 0.14189160530216113]
	TIME [epoch: 8.16 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40939527222637356		[learning rate: 0.0084507]
		[batch 20/20] avg loss: 0.186118350933201		[learning rate: 0.0084405]
	Learning Rate: 0.0084405
	LOSS [training: 0.2977568115797873 | validation: 0.16173859920976805]
	TIME [epoch: 8.18 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14423303963875647		[learning rate: 0.0084303]
		[batch 20/20] avg loss: 0.1084644110515783		[learning rate: 0.0084201]
	Learning Rate: 0.00842007
	LOSS [training: 0.12634872534516736 | validation: 0.1285118680606205]
	TIME [epoch: 8.2 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22604367336920053		[learning rate: 0.0084099]
		[batch 20/20] avg loss: 0.26523717019430193		[learning rate: 0.0083997]
	Learning Rate: 0.00839969
	LOSS [training: 0.24564042178175122 | validation: 0.25248222535548115]
	TIME [epoch: 8.16 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4719341580398888		[learning rate: 0.0083895]
		[batch 20/20] avg loss: 0.23068327679521353		[learning rate: 0.0083794]
	Learning Rate: 0.00837935
	LOSS [training: 0.35130871741755115 | validation: 0.26512243630124466]
	TIME [epoch: 8.16 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2272605475458105		[learning rate: 0.0083692]
		[batch 20/20] avg loss: 0.1991623731184397		[learning rate: 0.0083591]
	Learning Rate: 0.00835907
	LOSS [training: 0.21321146033212512 | validation: 0.2441162495365416]
	TIME [epoch: 8.15 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19588434208747502		[learning rate: 0.0083489]
		[batch 20/20] avg loss: 0.17766388914884246		[learning rate: 0.0083388]
	Learning Rate: 0.00833883
	LOSS [training: 0.18677411561815876 | validation: 0.3310388731095038]
	TIME [epoch: 8.2 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3010933754736129		[learning rate: 0.0083287]
		[batch 20/20] avg loss: 0.22685794110373236		[learning rate: 0.0083186]
	Learning Rate: 0.00831864
	LOSS [training: 0.26397565828867264 | validation: 0.1546347011691223]
	TIME [epoch: 8.16 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15062641397902837		[learning rate: 0.0083086]
		[batch 20/20] avg loss: 0.37773119017177953		[learning rate: 0.0082985]
	Learning Rate: 0.00829851
	LOSS [training: 0.26417880207540395 | validation: 0.1726480390923361]
	TIME [epoch: 8.17 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24358119028719938		[learning rate: 0.0082885]
		[batch 20/20] avg loss: 0.23194113402879105		[learning rate: 0.0082784]
	Learning Rate: 0.00827842
	LOSS [training: 0.2377611621579952 | validation: 0.1138921522604878]
	TIME [epoch: 8.2 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33417734958810497		[learning rate: 0.0082684]
		[batch 20/20] avg loss: 0.18605789448108434		[learning rate: 0.0082584]
	Learning Rate: 0.00825838
	LOSS [training: 0.2601176220345947 | validation: 0.3766645895027186]
	TIME [epoch: 8.2 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20674403138143246		[learning rate: 0.0082484]
		[batch 20/20] avg loss: 0.21249917889945835		[learning rate: 0.0082384]
	Learning Rate: 0.00823839
	LOSS [training: 0.20962160514044542 | validation: 0.123136613415241]
	TIME [epoch: 8.17 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2734545203779777		[learning rate: 0.0082284]
		[batch 20/20] avg loss: 0.29847501548016886		[learning rate: 0.0082184]
	Learning Rate: 0.00821844
	LOSS [training: 0.28596476792907327 | validation: 0.24877572646986904]
	TIME [epoch: 8.17 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2889886787735124		[learning rate: 0.0082085]
		[batch 20/20] avg loss: 0.39148758887683155		[learning rate: 0.0081985]
	Learning Rate: 0.00819855
	LOSS [training: 0.34023813382517204 | validation: 0.5427399354638289]
	TIME [epoch: 8.19 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9836222708230672		[learning rate: 0.0081886]
		[batch 20/20] avg loss: 0.8933659602311561		[learning rate: 0.0081787]
	Learning Rate: 0.0081787
	LOSS [training: 0.9384941155271118 | validation: 0.9935428795481163]
	TIME [epoch: 8.2 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.226445355481339		[learning rate: 0.0081688]
		[batch 20/20] avg loss: 0.9577522334655392		[learning rate: 0.0081589]
	Learning Rate: 0.0081589
	LOSS [training: 1.0920987944734393 | validation: 0.2537606560411248]
	TIME [epoch: 8.18 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7525118651713872		[learning rate: 0.008149]
		[batch 20/20] avg loss: 0.4245505050021651		[learning rate: 0.0081391]
	Learning Rate: 0.00813915
	LOSS [training: 0.588531185086776 | validation: 0.24302862747575074]
	TIME [epoch: 8.18 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3837558066550376		[learning rate: 0.0081293]
		[batch 20/20] avg loss: 0.24562775587674363		[learning rate: 0.0081194]
	Learning Rate: 0.00811944
	LOSS [training: 0.3146917812658907 | validation: 0.12915195370407503]
	TIME [epoch: 8.2 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2314225134438514		[learning rate: 0.0081096]
		[batch 20/20] avg loss: 0.2558920795872909		[learning rate: 0.0080998]
	Learning Rate: 0.00809979
	LOSS [training: 0.24365729651557116 | validation: 0.1568189316257522]
	TIME [epoch: 8.19 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24030298100229758		[learning rate: 0.00809]
		[batch 20/20] avg loss: 0.21260377866219216		[learning rate: 0.0080802]
	Learning Rate: 0.00808018
	LOSS [training: 0.22645337983224492 | validation: 0.562912774657654]
	TIME [epoch: 8.17 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41087286429162295		[learning rate: 0.0080704]
		[batch 20/20] avg loss: 0.25139346718031264		[learning rate: 0.0080606]
	Learning Rate: 0.00806062
	LOSS [training: 0.33113316573596785 | validation: 0.17634870187870527]
	TIME [epoch: 8.18 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21308237156900534		[learning rate: 0.0080509]
		[batch 20/20] avg loss: 0.28471814896950604		[learning rate: 0.0080411]
	Learning Rate: 0.00804111
	LOSS [training: 0.24890026026925569 | validation: 0.17470164032738944]
	TIME [epoch: 8.19 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17917647807471895		[learning rate: 0.0080314]
		[batch 20/20] avg loss: 0.22069227455330637		[learning rate: 0.0080216]
	Learning Rate: 0.00802164
	LOSS [training: 0.19993437631401267 | validation: 0.2675427679074017]
	TIME [epoch: 8.2 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28615718100887005		[learning rate: 0.0080119]
		[batch 20/20] avg loss: 0.4930515001210674		[learning rate: 0.0080022]
	Learning Rate: 0.00800222
	LOSS [training: 0.3896043405649687 | validation: 0.21672504857197283]
	TIME [epoch: 8.18 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43961950810044925		[learning rate: 0.0079925]
		[batch 20/20] avg loss: 0.4677083397418353		[learning rate: 0.0079828]
	Learning Rate: 0.00798285
	LOSS [training: 0.4536639239211423 | validation: 0.33482521061442483]
	TIME [epoch: 8.21 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1591378853841175		[learning rate: 0.0079732]
		[batch 20/20] avg loss: 0.07366016730291298		[learning rate: 0.0079635]
	Learning Rate: 0.00796352
	LOSS [training: 0.1163990263435152 | validation: 0.07045080724763467]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14050514991139357		[learning rate: 0.0079539]
		[batch 20/20] avg loss: 0.13043279069161762		[learning rate: 0.0079442]
	Learning Rate: 0.00794424
	LOSS [training: 0.1354689703015056 | validation: 0.08784345654329569]
	TIME [epoch: 8.18 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15482378413347467		[learning rate: 0.0079346]
		[batch 20/20] avg loss: 0.1148783704009474		[learning rate: 0.007925]
	Learning Rate: 0.00792501
	LOSS [training: 0.13485107726721104 | validation: 0.10947142627211397]
	TIME [epoch: 8.16 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14717455243984603		[learning rate: 0.0079154]
		[batch 20/20] avg loss: 0.13331109802071542		[learning rate: 0.0079058]
	Learning Rate: 0.00790583
	LOSS [training: 0.1402428252302807 | validation: 0.11320045485812803]
	TIME [epoch: 8.18 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1058688006621034		[learning rate: 0.0078963]
		[batch 20/20] avg loss: 0.1394801903091527		[learning rate: 0.0078867]
	Learning Rate: 0.00788669
	LOSS [training: 0.12267449548562807 | validation: 0.10640719640094214]
	TIME [epoch: 8.17 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12560196477068766		[learning rate: 0.0078771]
		[batch 20/20] avg loss: 0.196906384301901		[learning rate: 0.0078676]
	Learning Rate: 0.0078676
	LOSS [training: 0.16125417453629434 | validation: 0.12003529191442713]
	TIME [epoch: 8.19 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1797644153551404		[learning rate: 0.0078581]
		[batch 20/20] avg loss: 0.13922536028344212		[learning rate: 0.0078486]
	Learning Rate: 0.00784855
	LOSS [training: 0.15949488781929125 | validation: 0.13358104965560388]
	TIME [epoch: 8.17 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1576277519029226		[learning rate: 0.007839]
		[batch 20/20] avg loss: 0.14386132153132075		[learning rate: 0.0078296]
	Learning Rate: 0.00782955
	LOSS [training: 0.15074453671712168 | validation: 0.2528547315162952]
	TIME [epoch: 8.2 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14872405299312294		[learning rate: 0.0078201]
		[batch 20/20] avg loss: 0.14078684952351567		[learning rate: 0.0078106]
	Learning Rate: 0.0078106
	LOSS [training: 0.1447554512583193 | validation: 0.23387518582214295]
	TIME [epoch: 8.17 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14427054102207695		[learning rate: 0.0078011]
		[batch 20/20] avg loss: 0.13691054172309608		[learning rate: 0.0077917]
	Learning Rate: 0.00779169
	LOSS [training: 0.14059054137258656 | validation: 0.16147580044207194]
	TIME [epoch: 8.18 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15122702045359088		[learning rate: 0.0077823]
		[batch 20/20] avg loss: 0.14520234950203398		[learning rate: 0.0077728]
	Learning Rate: 0.00777283
	LOSS [training: 0.14821468497781243 | validation: 0.21875650511071126]
	TIME [epoch: 8.17 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12345638251355047		[learning rate: 0.0077634]
		[batch 20/20] avg loss: 0.11459653642173433		[learning rate: 0.007754]
	Learning Rate: 0.00775401
	LOSS [training: 0.11902645946764238 | validation: 0.06788610517655386]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15605065673657142		[learning rate: 0.0077446]
		[batch 20/20] avg loss: 0.14278076307975845		[learning rate: 0.0077352]
	Learning Rate: 0.00773524
	LOSS [training: 0.1494157099081649 | validation: 0.12964679876426669]
	TIME [epoch: 8.18 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1245905562112557		[learning rate: 0.0077259]
		[batch 20/20] avg loss: 0.1332331619981753		[learning rate: 0.0077165]
	Learning Rate: 0.00771651
	LOSS [training: 0.1289118591047155 | validation: 0.08089303713356327]
	TIME [epoch: 8.18 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14723381603219035		[learning rate: 0.0077072]
		[batch 20/20] avg loss: 0.11189013346217624		[learning rate: 0.0076978]
	Learning Rate: 0.00769783
	LOSS [training: 0.12956197474718334 | validation: 0.07404096646405424]
	TIME [epoch: 8.17 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15091728924409334		[learning rate: 0.0076885]
		[batch 20/20] avg loss: 0.1137317717283313		[learning rate: 0.0076792]
	Learning Rate: 0.0076792
	LOSS [training: 0.13232453048621232 | validation: 0.08903617736940317]
	TIME [epoch: 8.18 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17785393418074583		[learning rate: 0.0076699]
		[batch 20/20] avg loss: 0.15233613568094717		[learning rate: 0.0076606]
	Learning Rate: 0.00766061
	LOSS [training: 0.16509503493084648 | validation: 0.09861417561007561]
	TIME [epoch: 8.18 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18069424781082769		[learning rate: 0.0076513]
		[batch 20/20] avg loss: 0.7321143229179974		[learning rate: 0.0076421]
	Learning Rate: 0.00764206
	LOSS [training: 0.45640428536441247 | validation: 1.0236692274476584]
	TIME [epoch: 8.18 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32477263608481954		[learning rate: 0.0076328]
		[batch 20/20] avg loss: 0.23250065250789978		[learning rate: 0.0076236]
	Learning Rate: 0.00762356
	LOSS [training: 0.27863664429635965 | validation: 0.23965280186903545]
	TIME [epoch: 8.16 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20841374382795208		[learning rate: 0.0076143]
		[batch 20/20] avg loss: 0.12353733301808023		[learning rate: 0.0076051]
	Learning Rate: 0.00760511
	LOSS [training: 0.16597553842301618 | validation: 0.11246110455554643]
	TIME [epoch: 8.16 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10712828615908139		[learning rate: 0.0075959]
		[batch 20/20] avg loss: 0.08826537056691701		[learning rate: 0.0075867]
	Learning Rate: 0.00758669
	LOSS [training: 0.09769682836299921 | validation: 0.13179789889021992]
	TIME [epoch: 8.16 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11721637382106465		[learning rate: 0.0075775]
		[batch 20/20] avg loss: 0.11429240125992168		[learning rate: 0.0075683]
	Learning Rate: 0.00756833
	LOSS [training: 0.11575438754049314 | validation: 0.12608830748365052]
	TIME [epoch: 8.17 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1116994835995824		[learning rate: 0.0075592]
		[batch 20/20] avg loss: 0.08647824407871375		[learning rate: 0.00755]
	Learning Rate: 0.00755001
	LOSS [training: 0.09908886383914808 | validation: 0.07889542475898394]
	TIME [epoch: 8.2 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09023279427879068		[learning rate: 0.0075409]
		[batch 20/20] avg loss: 0.12967059832081065		[learning rate: 0.0075317]
	Learning Rate: 0.00753173
	LOSS [training: 0.10995169629980064 | validation: 0.15390085554099525]
	TIME [epoch: 8.17 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13939614137295017		[learning rate: 0.0075226]
		[batch 20/20] avg loss: 0.11920415518314324		[learning rate: 0.0075135]
	Learning Rate: 0.0075135
	LOSS [training: 0.1293001482780467 | validation: 0.21240358277422092]
	TIME [epoch: 8.18 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10339626931210533		[learning rate: 0.0075044]
		[batch 20/20] avg loss: 0.11926730997233528		[learning rate: 0.0074953]
	Learning Rate: 0.00749531
	LOSS [training: 0.1113317896422203 | validation: 0.13380179010802445]
	TIME [epoch: 8.19 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11621939696159356		[learning rate: 0.0074862]
		[batch 20/20] avg loss: 0.18824576527461923		[learning rate: 0.0074772]
	Learning Rate: 0.00747716
	LOSS [training: 0.1522325811181064 | validation: 0.5760478295053014]
	TIME [epoch: 8.21 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21454323261553326		[learning rate: 0.0074681]
		[batch 20/20] avg loss: 0.11189999939996938		[learning rate: 0.0074591]
	Learning Rate: 0.00745906
	LOSS [training: 0.16322161600775129 | validation: 0.11816251079578435]
	TIME [epoch: 8.16 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11359490250096127		[learning rate: 0.00745]
		[batch 20/20] avg loss: 0.13751097502145598		[learning rate: 0.007441]
	Learning Rate: 0.007441
	LOSS [training: 0.12555293876120863 | validation: 0.1712602862960295]
	TIME [epoch: 8.16 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13566920839593402		[learning rate: 0.007432]
		[batch 20/20] avg loss: 0.11021153901089487		[learning rate: 0.007423]
	Learning Rate: 0.00742299
	LOSS [training: 0.1229403737034144 | validation: 0.1295217593320046]
	TIME [epoch: 8.17 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14961764129502697		[learning rate: 0.007414]
		[batch 20/20] avg loss: 0.15770539031294678		[learning rate: 0.007405]
	Learning Rate: 0.00740502
	LOSS [training: 0.1536615158039869 | validation: 0.1350814891955439]
	TIME [epoch: 8.19 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14664355196778056		[learning rate: 0.0073961]
		[batch 20/20] avg loss: 0.2404199095676533		[learning rate: 0.0073871]
	Learning Rate: 0.0073871
	LOSS [training: 0.19353173076771696 | validation: 0.15398593792881343]
	TIME [epoch: 8.16 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11181605553038321		[learning rate: 0.0073781]
		[batch 20/20] avg loss: 0.12047505121472676		[learning rate: 0.0073692]
	Learning Rate: 0.00736921
	LOSS [training: 0.11614555337255501 | validation: 0.07440151833383499]
	TIME [epoch: 8.19 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11208807533447569		[learning rate: 0.0073603]
		[batch 20/20] avg loss: 0.14239430002118375		[learning rate: 0.0073514]
	Learning Rate: 0.00735137
	LOSS [training: 0.12724118767782971 | validation: 0.08715757998487378]
	TIME [epoch: 8.17 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09614422776378645		[learning rate: 0.0073425]
		[batch 20/20] avg loss: 0.1312878950213891		[learning rate: 0.0073336]
	Learning Rate: 0.00733358
	LOSS [training: 0.11371606139258776 | validation: 0.2736666580908653]
	TIME [epoch: 8.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15422248244707032		[learning rate: 0.0073247]
		[batch 20/20] avg loss: 0.1266913219491988		[learning rate: 0.0073158]
	Learning Rate: 0.00731582
	LOSS [training: 0.1404569021981346 | validation: 0.17185525065806953]
	TIME [epoch: 8.2 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08698139240998662		[learning rate: 0.007307]
		[batch 20/20] avg loss: 0.08508796557998655		[learning rate: 0.0072981]
	Learning Rate: 0.00729811
	LOSS [training: 0.08603467899498658 | validation: 0.20560978854349832]
	TIME [epoch: 8.16 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12683113323300402		[learning rate: 0.0072893]
		[batch 20/20] avg loss: 0.08684027490605321		[learning rate: 0.0072804]
	Learning Rate: 0.00728044
	LOSS [training: 0.10683570406952862 | validation: 0.10140215074699115]
	TIME [epoch: 8.17 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11699726664336596		[learning rate: 0.0072716]
		[batch 20/20] avg loss: 0.10190053625397873		[learning rate: 0.0072628]
	Learning Rate: 0.00726282
	LOSS [training: 0.10944890144867232 | validation: 0.100151266224743]
	TIME [epoch: 8.21 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12846371457436806		[learning rate: 0.007254]
		[batch 20/20] avg loss: 0.09266016399420378		[learning rate: 0.0072452]
	Learning Rate: 0.00724524
	LOSS [training: 0.1105619392842859 | validation: 0.2639618613983192]
	TIME [epoch: 8.18 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12981618361991237		[learning rate: 0.0072365]
		[batch 20/20] avg loss: 0.1049904150756458		[learning rate: 0.0072277]
	Learning Rate: 0.0072277
	LOSS [training: 0.11740329934777907 | validation: 0.18735963242653925]
	TIME [epoch: 8.17 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08512585544607756		[learning rate: 0.0072189]
		[batch 20/20] avg loss: 0.09135375703252155		[learning rate: 0.0072102]
	Learning Rate: 0.0072102
	LOSS [training: 0.08823980623929956 | validation: 0.17571415617006314]
	TIME [epoch: 8.18 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26797441848612424		[learning rate: 0.0072015]
		[batch 20/20] avg loss: 0.150937919732695		[learning rate: 0.0071927]
	Learning Rate: 0.00719275
	LOSS [training: 0.2094561691094096 | validation: 0.11226147326935332]
	TIME [epoch: 8.22 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12009077588431624		[learning rate: 0.007184]
		[batch 20/20] avg loss: 0.10871822746099373		[learning rate: 0.0071753]
	Learning Rate: 0.00717533
	LOSS [training: 0.11440450167265499 | validation: 0.12910343727571238]
	TIME [epoch: 8.17 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12992865706837375		[learning rate: 0.0071666]
		[batch 20/20] avg loss: 0.17968175326219793		[learning rate: 0.007158]
	Learning Rate: 0.00715796
	LOSS [training: 0.15480520516528584 | validation: 0.2649354271634615]
	TIME [epoch: 8.16 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2570796584723241		[learning rate: 0.0071493]
		[batch 20/20] avg loss: 0.3021248630206305		[learning rate: 0.0071406]
	Learning Rate: 0.00714064
	LOSS [training: 0.2796022607464773 | validation: 0.36935531986581077]
	TIME [epoch: 8.17 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28530975167458417		[learning rate: 0.007132]
		[batch 20/20] avg loss: 0.14671263079629335		[learning rate: 0.0071233]
	Learning Rate: 0.00712335
	LOSS [training: 0.21601119123543877 | validation: 0.1394484423386716]
	TIME [epoch: 8.21 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13977266709820363		[learning rate: 0.0071147]
		[batch 20/20] avg loss: 0.20733948205957603		[learning rate: 0.0071061]
	Learning Rate: 0.0071061
	LOSS [training: 0.1735560745788898 | validation: 0.08927555415805978]
	TIME [epoch: 8.18 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08355162154966819		[learning rate: 0.0070975]
		[batch 20/20] avg loss: 0.1985842528866241		[learning rate: 0.0070889]
	Learning Rate: 0.0070889
	LOSS [training: 0.14106793721814614 | validation: 0.1546239690378156]
	TIME [epoch: 8.17 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14160117910219913		[learning rate: 0.0070803]
		[batch 20/20] avg loss: 0.10128262566911253		[learning rate: 0.0070717]
	Learning Rate: 0.00707174
	LOSS [training: 0.12144190238565582 | validation: 0.1362974503840277]
	TIME [epoch: 8.19 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1151382675168934		[learning rate: 0.0070632]
		[batch 20/20] avg loss: 0.1317097574643658		[learning rate: 0.0070546]
	Learning Rate: 0.00705462
	LOSS [training: 0.12342401249062959 | validation: 0.06453167957927874]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060152691116083304		[learning rate: 0.0070461]
		[batch 20/20] avg loss: 0.13005346787099972		[learning rate: 0.0070375]
	Learning Rate: 0.00703754
	LOSS [training: 0.0951030794935415 | validation: 0.11652822599200924]
	TIME [epoch: 8.16 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14192440260201042		[learning rate: 0.007029]
		[batch 20/20] avg loss: 0.3378891370666534		[learning rate: 0.0070205]
	Learning Rate: 0.00702051
	LOSS [training: 0.23990676983433193 | validation: 0.39021060860856616]
	TIME [epoch: 8.15 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2595420072869362		[learning rate: 0.007012]
		[batch 20/20] avg loss: 0.4471408492863905		[learning rate: 0.0070035]
	Learning Rate: 0.00700351
	LOSS [training: 0.3533414282866633 | validation: 0.21755790340318087]
	TIME [epoch: 8.16 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2034559662402211		[learning rate: 0.006995]
		[batch 20/20] avg loss: 0.13639165428037786		[learning rate: 0.0069866]
	Learning Rate: 0.00698656
	LOSS [training: 0.16992381026029948 | validation: 0.0760037129457335]
	TIME [epoch: 8.2 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11388776910509238		[learning rate: 0.0069781]
		[batch 20/20] avg loss: 0.0876152373824094		[learning rate: 0.0069696]
	Learning Rate: 0.00696964
	LOSS [training: 0.10075150324375089 | validation: 0.1549458573617336]
	TIME [epoch: 8.17 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1506515053148083		[learning rate: 0.0069612]
		[batch 20/20] avg loss: 0.21538109922400653		[learning rate: 0.0069528]
	Learning Rate: 0.00695277
	LOSS [training: 0.1830163022694074 | validation: 0.3067451281054545]
	TIME [epoch: 8.16 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.227166881044417		[learning rate: 0.0069443]
		[batch 20/20] avg loss: 0.17118749243607959		[learning rate: 0.0069359]
	Learning Rate: 0.00693594
	LOSS [training: 0.19917718674024826 | validation: 0.3915557932102825]
	TIME [epoch: 8.18 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2525610946220095		[learning rate: 0.0069275]
		[batch 20/20] avg loss: 0.18882082967889458		[learning rate: 0.0069191]
	Learning Rate: 0.00691915
	LOSS [training: 0.22069096215045203 | validation: 0.10850081590823815]
	TIME [epoch: 8.2 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1575149281404232		[learning rate: 0.0069108]
		[batch 20/20] avg loss: 0.1779109462447374		[learning rate: 0.0069024]
	Learning Rate: 0.0069024
	LOSS [training: 0.16771293719258026 | validation: 0.20982854306433443]
	TIME [epoch: 8.15 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.254329379320459		[learning rate: 0.006894]
		[batch 20/20] avg loss: 0.2763833764068826		[learning rate: 0.0068857]
	Learning Rate: 0.00688569
	LOSS [training: 0.26535637786367083 | validation: 0.09971761964196793]
	TIME [epoch: 8.15 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09410549020804561		[learning rate: 0.0068773]
		[batch 20/20] avg loss: 0.12205837345458041		[learning rate: 0.006869]
	Learning Rate: 0.00686902
	LOSS [training: 0.10808193183131302 | validation: 0.11430630965799969]
	TIME [epoch: 8.15 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10734503656808299		[learning rate: 0.0068607]
		[batch 20/20] avg loss: 0.10434586492943787		[learning rate: 0.0068524]
	Learning Rate: 0.00685239
	LOSS [training: 0.10584545074876042 | validation: 0.1695168582204945]
	TIME [epoch: 8.17 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1115153178130961		[learning rate: 0.0068441]
		[batch 20/20] avg loss: 0.1449093646734387		[learning rate: 0.0068358]
	Learning Rate: 0.0068358
	LOSS [training: 0.1282123412432674 | validation: 0.19911670133221993]
	TIME [epoch: 8.17 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09680071407832389		[learning rate: 0.0068275]
		[batch 20/20] avg loss: 0.11538414534493954		[learning rate: 0.0068193]
	Learning Rate: 0.00681925
	LOSS [training: 0.10609242971163169 | validation: 0.08462020091508479]
	TIME [epoch: 8.18 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11058322243486973		[learning rate: 0.006811]
		[batch 20/20] avg loss: 0.16127191767147211		[learning rate: 0.0068027]
	Learning Rate: 0.00680275
	LOSS [training: 0.13592757005317097 | validation: 0.1592985286440719]
	TIME [epoch: 8.16 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10323142318832113		[learning rate: 0.0067945]
		[batch 20/20] avg loss: 0.4161165149044342		[learning rate: 0.0067863]
	Learning Rate: 0.00678628
	LOSS [training: 0.25967396904637763 | validation: 0.06213397445367293]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08929875040490581		[learning rate: 0.0067781]
		[batch 20/20] avg loss: 0.12104096048687887		[learning rate: 0.0067698]
	Learning Rate: 0.00676985
	LOSS [training: 0.10516985544589234 | validation: 0.12205384223781716]
	TIME [epoch: 8.19 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14285719324403245		[learning rate: 0.0067616]
		[batch 20/20] avg loss: 0.07658047220378128		[learning rate: 0.0067535]
	Learning Rate: 0.00675346
	LOSS [training: 0.10971883272390688 | validation: 0.1101140207088806]
	TIME [epoch: 8.16 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11772264436053954		[learning rate: 0.0067453]
		[batch 20/20] avg loss: 0.10295135828731376		[learning rate: 0.0067371]
	Learning Rate: 0.00673711
	LOSS [training: 0.11033700132392665 | validation: 0.09673981056808385]
	TIME [epoch: 8.16 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10033165591963397		[learning rate: 0.006729]
		[batch 20/20] avg loss: 0.07493957067175379		[learning rate: 0.0067208]
	Learning Rate: 0.0067208
	LOSS [training: 0.08763561329569389 | validation: 0.0972472705149944]
	TIME [epoch: 8.21 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1226233985161452		[learning rate: 0.0067127]
		[batch 20/20] avg loss: 0.0974871987438632		[learning rate: 0.0067045]
	Learning Rate: 0.00670453
	LOSS [training: 0.11005529863000421 | validation: 0.07290167193322024]
	TIME [epoch: 8.16 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07075921759649204		[learning rate: 0.0066964]
		[batch 20/20] avg loss: 0.1395274383902138		[learning rate: 0.0066883]
	Learning Rate: 0.0066883
	LOSS [training: 0.10514332799335291 | validation: 0.12559446012329994]
	TIME [epoch: 8.16 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11487770040654818		[learning rate: 0.0066802]
		[batch 20/20] avg loss: 0.09676236252466618		[learning rate: 0.0066721]
	Learning Rate: 0.00667211
	LOSS [training: 0.10582003146560719 | validation: 0.10317971170593876]
	TIME [epoch: 8.18 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11155330882874356		[learning rate: 0.006664]
		[batch 20/20] avg loss: 0.09293348695763358		[learning rate: 0.006656]
	Learning Rate: 0.00665596
	LOSS [training: 0.10224339789318856 | validation: 0.07835330670136742]
	TIME [epoch: 8.23 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08962153984189355		[learning rate: 0.0066479]
		[batch 20/20] avg loss: 0.5937393334445342		[learning rate: 0.0066398]
	Learning Rate: 0.00663984
	LOSS [training: 0.3416804366432139 | validation: 0.6211591363680311]
	TIME [epoch: 8.17 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25585426056292365		[learning rate: 0.0066318]
		[batch 20/20] avg loss: 0.1445741091634389		[learning rate: 0.0066238]
	Learning Rate: 0.00662377
	LOSS [training: 0.20021418486318127 | validation: 0.6485262121247019]
	TIME [epoch: 8.17 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20095788219572697		[learning rate: 0.0066157]
		[batch 20/20] avg loss: 0.11486954028533225		[learning rate: 0.0066077]
	Learning Rate: 0.00660774
	LOSS [training: 0.15791371124052955 | validation: 0.23909037793233312]
	TIME [epoch: 8.17 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2361304172598116		[learning rate: 0.0065997]
		[batch 20/20] avg loss: 0.13630345847607558		[learning rate: 0.0065917]
	Learning Rate: 0.00659174
	LOSS [training: 0.18621693786794358 | validation: 0.08000491971215178]
	TIME [epoch: 8.19 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08081136194842356		[learning rate: 0.0065838]
		[batch 20/20] avg loss: 0.08301343880044507		[learning rate: 0.0065758]
	Learning Rate: 0.00657578
	LOSS [training: 0.08191240037443433 | validation: 0.09043613354955779]
	TIME [epoch: 8.19 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14657185244533139		[learning rate: 0.0065678]
		[batch 20/20] avg loss: 0.16941880778336377		[learning rate: 0.0065599]
	Learning Rate: 0.00655986
	LOSS [training: 0.15799533011434758 | validation: 0.18756982211376896]
	TIME [epoch: 8.18 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1489757611495129		[learning rate: 0.0065519]
		[batch 20/20] avg loss: 0.06517061269073662		[learning rate: 0.006544]
	Learning Rate: 0.00654398
	LOSS [training: 0.10707318692012477 | validation: 0.14245967010588645]
	TIME [epoch: 8.18 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09185643271918563		[learning rate: 0.0065361]
		[batch 20/20] avg loss: 0.11498454376908307		[learning rate: 0.0065281]
	Learning Rate: 0.00652814
	LOSS [training: 0.10342048824413434 | validation: 0.11998589122313139]
	TIME [epoch: 8.21 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22547838417251972		[learning rate: 0.0065202]
		[batch 20/20] avg loss: 0.1043466104496101		[learning rate: 0.0065123]
	Learning Rate: 0.00651234
	LOSS [training: 0.16491249731106494 | validation: 0.12006285187119739]
	TIME [epoch: 8.21 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13798355185069638		[learning rate: 0.0065044]
		[batch 20/20] avg loss: 0.08876068123673482		[learning rate: 0.0064966]
	Learning Rate: 0.00649657
	LOSS [training: 0.1133721165437156 | validation: 0.06414634576904679]
	TIME [epoch: 8.17 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08998432549912341		[learning rate: 0.0064887]
		[batch 20/20] avg loss: 0.10563819556755556		[learning rate: 0.0064808]
	Learning Rate: 0.00648084
	LOSS [training: 0.09781126053333947 | validation: 0.1831571238192884]
	TIME [epoch: 8.17 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11032362422697392		[learning rate: 0.006473]
		[batch 20/20] avg loss: 0.10079653535044415		[learning rate: 0.0064652]
	Learning Rate: 0.00646516
	LOSS [training: 0.105560079788709 | validation: 0.04821804127416262]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0891172369113774		[learning rate: 0.0064573]
		[batch 20/20] avg loss: 0.08185195535186883		[learning rate: 0.0064495]
	Learning Rate: 0.0064495
	LOSS [training: 0.08548459613162313 | validation: 0.046367328441467376]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09822758168394417		[learning rate: 0.0064417]
		[batch 20/20] avg loss: 0.11698690574308986		[learning rate: 0.0064339]
	Learning Rate: 0.00643389
	LOSS [training: 0.10760724371351701 | validation: 0.0430771016279671]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08791046958903734		[learning rate: 0.0064261]
		[batch 20/20] avg loss: 0.1174967886212817		[learning rate: 0.0064183]
	Learning Rate: 0.00641832
	LOSS [training: 0.10270362910515954 | validation: 0.08620095902541289]
	TIME [epoch: 8.2 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09654427934979999		[learning rate: 0.0064105]
		[batch 20/20] avg loss: 0.08500722712305885		[learning rate: 0.0064028]
	Learning Rate: 0.00640278
	LOSS [training: 0.09077575323642942 | validation: 0.08128453340355388]
	TIME [epoch: 8.25 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12349911272747065		[learning rate: 0.006395]
		[batch 20/20] avg loss: 0.10270147003976016		[learning rate: 0.0063873]
	Learning Rate: 0.00638728
	LOSS [training: 0.11310029138361541 | validation: 0.16700365739149622]
	TIME [epoch: 8.2 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10834669232804313		[learning rate: 0.0063795]
		[batch 20/20] avg loss: 0.09380818213725457		[learning rate: 0.0063718]
	Learning Rate: 0.00637182
	LOSS [training: 0.10107743723264886 | validation: 0.0851567945632978]
	TIME [epoch: 8.19 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11848865866260316		[learning rate: 0.0063641]
		[batch 20/20] avg loss: 0.10111800063675476		[learning rate: 0.0063564]
	Learning Rate: 0.00635639
	LOSS [training: 0.10980332964967895 | validation: 0.08314496668611007]
	TIME [epoch: 8.18 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.088514195650823		[learning rate: 0.0063487]
		[batch 20/20] avg loss: 0.4847641483346443		[learning rate: 0.006341]
	Learning Rate: 0.006341
	LOSS [training: 0.28663917199273364 | validation: 0.16382010200393668]
	TIME [epoch: 8.22 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10378102634696273		[learning rate: 0.0063333]
		[batch 20/20] avg loss: 0.1279593216798597		[learning rate: 0.0063257]
	Learning Rate: 0.00632565
	LOSS [training: 0.11587017401341122 | validation: 0.10862479213946069]
	TIME [epoch: 8.2 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08030533252617647		[learning rate: 0.006318]
		[batch 20/20] avg loss: 0.12792707236410505		[learning rate: 0.0063103]
	Learning Rate: 0.00631034
	LOSS [training: 0.10411620244514075 | validation: 0.11656586644518746]
	TIME [epoch: 8.22 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10691859833711544		[learning rate: 0.0063027]
		[batch 20/20] avg loss: 0.10288474954837186		[learning rate: 0.0062951]
	Learning Rate: 0.00629506
	LOSS [training: 0.10490167394274366 | validation: 0.15591247045282952]
	TIME [epoch: 8.2 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09036936819653874		[learning rate: 0.0062874]
		[batch 20/20] avg loss: 0.1050040594182677		[learning rate: 0.0062798]
	Learning Rate: 0.00627982
	LOSS [training: 0.09768671380740324 | validation: 0.09764906382746001]
	TIME [epoch: 8.25 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15244056568551198		[learning rate: 0.0062722]
		[batch 20/20] avg loss: 0.12523601846684657		[learning rate: 0.0062646]
	Learning Rate: 0.00626462
	LOSS [training: 0.13883829207617931 | validation: 0.07085744553766501]
	TIME [epoch: 8.19 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09057557467574659		[learning rate: 0.006257]
		[batch 20/20] avg loss: 0.12221869798494822		[learning rate: 0.0062495]
	Learning Rate: 0.00624945
	LOSS [training: 0.10639713633034739 | validation: 0.09111679453901]
	TIME [epoch: 8.19 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12740322559563336		[learning rate: 0.0062419]
		[batch 20/20] avg loss: 0.0967336815096132		[learning rate: 0.0062343]
	Learning Rate: 0.00623433
	LOSS [training: 0.11206845355262329 | validation: 0.07857241498274896]
	TIME [epoch: 8.18 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0949910155783177		[learning rate: 0.0062268]
		[batch 20/20] avg loss: 0.12030321000555615		[learning rate: 0.0062192]
	Learning Rate: 0.00621923
	LOSS [training: 0.10764711279193692 | validation: 0.072963099769779]
	TIME [epoch: 8.24 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0764172881639326		[learning rate: 0.0062117]
		[batch 20/20] avg loss: 0.08135708426673526		[learning rate: 0.0062042]
	Learning Rate: 0.00620418
	LOSS [training: 0.07888718621533394 | validation: 0.08683566538327425]
	TIME [epoch: 8.2 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0776397085689389		[learning rate: 0.0061967]
		[batch 20/20] avg loss: 0.0914702560885857		[learning rate: 0.0061892]
	Learning Rate: 0.00618916
	LOSS [training: 0.0845549823287623 | validation: 0.08609935239270956]
	TIME [epoch: 8.19 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1145653856045512		[learning rate: 0.0061817]
		[batch 20/20] avg loss: 0.09830162340024216		[learning rate: 0.0061742]
	Learning Rate: 0.00617417
	LOSS [training: 0.1064335045023967 | validation: 0.10452677594961225]
	TIME [epoch: 8.22 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07659176022247541		[learning rate: 0.0061667]
		[batch 20/20] avg loss: 0.09153900469908935		[learning rate: 0.0061592]
	Learning Rate: 0.00615923
	LOSS [training: 0.08406538246078239 | validation: 0.07360214272823742]
	TIME [epoch: 8.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09602319869053058		[learning rate: 0.0061518]
		[batch 20/20] avg loss: 0.10419350696684127		[learning rate: 0.0061443]
	Learning Rate: 0.00614432
	LOSS [training: 0.10010835282868591 | validation: 0.17474028441519193]
	TIME [epoch: 8.19 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07871013842450457		[learning rate: 0.0061369]
		[batch 20/20] avg loss: 0.0821045135634699		[learning rate: 0.0061294]
	Learning Rate: 0.00612944
	LOSS [training: 0.08040732599398724 | validation: 0.07696419180744037]
	TIME [epoch: 8.18 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08022626413972009		[learning rate: 0.006122]
		[batch 20/20] avg loss: 0.07809161676817628		[learning rate: 0.0061146]
	Learning Rate: 0.0061146
	LOSS [training: 0.07915894045394817 | validation: 0.19370757973042452]
	TIME [epoch: 8.18 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1514449596270338		[learning rate: 0.0061072]
		[batch 20/20] avg loss: 0.1795747385777789		[learning rate: 0.0060998]
	Learning Rate: 0.0060998
	LOSS [training: 0.16550984910240635 | validation: 0.08272475503427645]
	TIME [epoch: 8.2 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15565106975949958		[learning rate: 0.0060924]
		[batch 20/20] avg loss: 0.08206269309755822		[learning rate: 0.006085]
	Learning Rate: 0.00608504
	LOSS [training: 0.1188568814285289 | validation: 0.10207342373745222]
	TIME [epoch: 8.21 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09248209977245322		[learning rate: 0.0060777]
		[batch 20/20] avg loss: 0.10005011448262158		[learning rate: 0.0060703]
	Learning Rate: 0.00607031
	LOSS [training: 0.09626610712753739 | validation: 0.11398675037907294]
	TIME [epoch: 8.19 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09326650094475428		[learning rate: 0.006063]
		[batch 20/20] avg loss: 0.11770181620026325		[learning rate: 0.0060556]
	Learning Rate: 0.00605561
	LOSS [training: 0.10548415857250874 | validation: 0.056529385526873345]
	TIME [epoch: 8.19 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0937951503575185		[learning rate: 0.0060483]
		[batch 20/20] avg loss: 0.10770292576914324		[learning rate: 0.006041]
	Learning Rate: 0.00604095
	LOSS [training: 0.10074903806333088 | validation: 0.11268379392611355]
	TIME [epoch: 8.23 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15835020312559495		[learning rate: 0.0060336]
		[batch 20/20] avg loss: 0.14098906930136199		[learning rate: 0.0060263]
	Learning Rate: 0.00602633
	LOSS [training: 0.14966963621347848 | validation: 0.047201689354360754]
	TIME [epoch: 8.2 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07781860022733514		[learning rate: 0.006019]
		[batch 20/20] avg loss: 0.1753475971710928		[learning rate: 0.0060117]
	Learning Rate: 0.00601174
	LOSS [training: 0.12658309869921397 | validation: 0.09337084366666447]
	TIME [epoch: 8.18 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059444838648501454		[learning rate: 0.0060045]
		[batch 20/20] avg loss: 0.08067109473256517		[learning rate: 0.0059972]
	Learning Rate: 0.00599718
	LOSS [training: 0.07005796669053332 | validation: 0.09850605066048249]
	TIME [epoch: 8.18 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.071102617691197		[learning rate: 0.0059899]
		[batch 20/20] avg loss: 0.07572422446185041		[learning rate: 0.0059827]
	Learning Rate: 0.00598267
	LOSS [training: 0.0734134210765237 | validation: 0.12480624732381526]
	TIME [epoch: 8.2 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09233279855100643		[learning rate: 0.0059754]
		[batch 20/20] avg loss: 0.07512392720474552		[learning rate: 0.0059682]
	Learning Rate: 0.00596818
	LOSS [training: 0.08372836287787597 | validation: 0.23268880332397995]
	TIME [epoch: 8.22 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10198415070129448		[learning rate: 0.005961]
		[batch 20/20] avg loss: 0.140870152868722		[learning rate: 0.0059537]
	Learning Rate: 0.00595374
	LOSS [training: 0.12142715178500825 | validation: 0.06969173617836291]
	TIME [epoch: 8.19 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09840032176855021		[learning rate: 0.0059465]
		[batch 20/20] avg loss: 0.10977296801490635		[learning rate: 0.0059393]
	Learning Rate: 0.00593932
	LOSS [training: 0.10408664489172828 | validation: 0.08524725257387991]
	TIME [epoch: 8.21 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.070132734375088		[learning rate: 0.0059321]
		[batch 20/20] avg loss: 0.09305261817920764		[learning rate: 0.0059249]
	Learning Rate: 0.00592494
	LOSS [training: 0.08159267627714781 | validation: 0.056194154216560065]
	TIME [epoch: 8.19 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0829539950476825		[learning rate: 0.0059178]
		[batch 20/20] avg loss: 0.07852838968081464		[learning rate: 0.0059106]
	Learning Rate: 0.0059106
	LOSS [training: 0.08074119236424859 | validation: 0.04674461462581116]
	TIME [epoch: 8.2 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06808964224707141		[learning rate: 0.0059034]
		[batch 20/20] avg loss: 0.07648184319865685		[learning rate: 0.0058963]
	Learning Rate: 0.00589629
	LOSS [training: 0.07228574272286413 | validation: 0.050692177866862934]
	TIME [epoch: 8.17 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11447918395858221		[learning rate: 0.0058892]
		[batch 20/20] avg loss: 0.11402000823026007		[learning rate: 0.005882]
	Learning Rate: 0.00588202
	LOSS [training: 0.11424959609442113 | validation: 0.08200543922053254]
	TIME [epoch: 8.18 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12552820373014417		[learning rate: 0.0058749]
		[batch 20/20] avg loss: 0.0906119069586411		[learning rate: 0.0058678]
	Learning Rate: 0.00586778
	LOSS [training: 0.10807005534439265 | validation: 0.19071501434158752]
	TIME [epoch: 8.18 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1086108844408912		[learning rate: 0.0058607]
		[batch 20/20] avg loss: 0.09097164868404724		[learning rate: 0.0058536]
	Learning Rate: 0.00585357
	LOSS [training: 0.09979126656246921 | validation: 0.09953675015365172]
	TIME [epoch: 8.23 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12103074664406432		[learning rate: 0.0058465]
		[batch 20/20] avg loss: 0.09403839906440412		[learning rate: 0.0058394]
	Learning Rate: 0.0058394
	LOSS [training: 0.10753457285423422 | validation: 0.08216858060730255]
	TIME [epoch: 8.18 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0806703481153722		[learning rate: 0.0058323]
		[batch 20/20] avg loss: 0.07399168433156497		[learning rate: 0.0058253]
	Learning Rate: 0.00582527
	LOSS [training: 0.07733101622346858 | validation: 0.1038289325739812]
	TIME [epoch: 8.18 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09337953335836421		[learning rate: 0.0058182]
		[batch 20/20] avg loss: 0.07731128134947525		[learning rate: 0.0058112]
	Learning Rate: 0.00581116
	LOSS [training: 0.08534540735391972 | validation: 0.09822479517684475]
	TIME [epoch: 8.21 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11665547012460395		[learning rate: 0.0058041]
		[batch 20/20] avg loss: 0.0930805310077071		[learning rate: 0.0057971]
	Learning Rate: 0.0057971
	LOSS [training: 0.1048680005661555 | validation: 0.09462850332643538]
	TIME [epoch: 8.22 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08020123649965734		[learning rate: 0.0057901]
		[batch 20/20] avg loss: 0.08003216669817281		[learning rate: 0.0057831]
	Learning Rate: 0.00578306
	LOSS [training: 0.08011670159891507 | validation: 0.06859285267261657]
	TIME [epoch: 8.18 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08392606016198065		[learning rate: 0.0057761]
		[batch 20/20] avg loss: 0.1007674046433477		[learning rate: 0.0057691]
	Learning Rate: 0.00576906
	LOSS [training: 0.0923467324026642 | validation: 0.06029479433705186]
	TIME [epoch: 8.18 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08356341401287509		[learning rate: 0.0057621]
		[batch 20/20] avg loss: 0.1040914894190584		[learning rate: 0.0057551]
	Learning Rate: 0.0057551
	LOSS [training: 0.09382745171596674 | validation: 0.05473881226251361]
	TIME [epoch: 8.18 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12176307449625742		[learning rate: 0.0057481]
		[batch 20/20] avg loss: 0.08407480135649885		[learning rate: 0.0057412]
	Learning Rate: 0.00574116
	LOSS [training: 0.10291893792637814 | validation: 0.11988274604678865]
	TIME [epoch: 8.2 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09836483159907433		[learning rate: 0.0057342]
		[batch 20/20] avg loss: 0.0846693541725372		[learning rate: 0.0057273]
	Learning Rate: 0.00572727
	LOSS [training: 0.09151709288580577 | validation: 0.09281511368454302]
	TIME [epoch: 8.2 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08614246472585713		[learning rate: 0.0057203]
		[batch 20/20] avg loss: 0.09338734562633773		[learning rate: 0.0057134]
	Learning Rate: 0.0057134
	LOSS [training: 0.08976490517609742 | validation: 0.08032200610688638]
	TIME [epoch: 8.18 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08361846481060582		[learning rate: 0.0057065]
		[batch 20/20] avg loss: 0.0743160866159128		[learning rate: 0.0056996]
	Learning Rate: 0.00569957
	LOSS [training: 0.07896727571325932 | validation: 0.06348715195792697]
	TIME [epoch: 8.18 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05464963229344895		[learning rate: 0.0056927]
		[batch 20/20] avg loss: 0.09927005517543223		[learning rate: 0.0056858]
	Learning Rate: 0.00568577
	LOSS [training: 0.0769598437344406 | validation: 0.09183689716349688]
	TIME [epoch: 8.25 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0972984739723084		[learning rate: 0.0056789]
		[batch 20/20] avg loss: 0.07669461979388262		[learning rate: 0.005672]
	Learning Rate: 0.00567201
	LOSS [training: 0.08699654688309551 | validation: 0.11085913571641419]
	TIME [epoch: 8.18 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09041149138866578		[learning rate: 0.0056651]
		[batch 20/20] avg loss: 0.07172548306422025		[learning rate: 0.0056583]
	Learning Rate: 0.00565828
	LOSS [training: 0.08106848722644303 | validation: 0.057446654716078654]
	TIME [epoch: 8.18 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06681883727771705		[learning rate: 0.0056514]
		[batch 20/20] avg loss: 0.14590668178246122		[learning rate: 0.0056446]
	Learning Rate: 0.00564458
	LOSS [training: 0.10636275953008915 | validation: 0.19975869621323164]
	TIME [epoch: 8.17 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13121379013978812		[learning rate: 0.0056377]
		[batch 20/20] avg loss: 0.08221044397919768		[learning rate: 0.0056309]
	Learning Rate: 0.00563092
	LOSS [training: 0.1067121170594929 | validation: 0.08494218889621236]
	TIME [epoch: 8.2 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0831992994556262		[learning rate: 0.0056241]
		[batch 20/20] avg loss: 0.06789560047480758		[learning rate: 0.0056173]
	Learning Rate: 0.00561728
	LOSS [training: 0.07554744996521691 | validation: 0.07172475151453586]
	TIME [epoch: 8.2 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11173122734207959		[learning rate: 0.0056105]
		[batch 20/20] avg loss: 0.08272367912912469		[learning rate: 0.0056037]
	Learning Rate: 0.00560368
	LOSS [training: 0.09722745323560217 | validation: 0.10950805596502441]
	TIME [epoch: 8.19 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09960028598818747		[learning rate: 0.0055969]
		[batch 20/20] avg loss: 0.09850926977466304		[learning rate: 0.0055901]
	Learning Rate: 0.00559012
	LOSS [training: 0.09905477788142526 | validation: 0.07587975887429779]
	TIME [epoch: 8.18 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08812765203239242		[learning rate: 0.0055833]
		[batch 20/20] avg loss: 0.08784467186099083		[learning rate: 0.0055766]
	Learning Rate: 0.00557659
	LOSS [training: 0.08798616194669162 | validation: 0.37017029858813605]
	TIME [epoch: 8.21 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13790753015244764		[learning rate: 0.0055698]
		[batch 20/20] avg loss: 0.0979597930582901		[learning rate: 0.0055631]
	Learning Rate: 0.00556309
	LOSS [training: 0.11793366160536889 | validation: 0.0607174480902407]
	TIME [epoch: 8.22 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08712083467549224		[learning rate: 0.0055563]
		[batch 20/20] avg loss: 0.07302050537460933		[learning rate: 0.0055496]
	Learning Rate: 0.00554962
	LOSS [training: 0.08007067002505079 | validation: 0.08239190896445794]
	TIME [epoch: 8.18 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08154717126040686		[learning rate: 0.0055429]
		[batch 20/20] avg loss: 0.05906674904670882		[learning rate: 0.0055362]
	Learning Rate: 0.00553618
	LOSS [training: 0.07030696015355785 | validation: 0.06113669042306651]
	TIME [epoch: 8.18 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15525246416711447		[learning rate: 0.0055295]
		[batch 20/20] avg loss: 0.15289264198593838		[learning rate: 0.0055228]
	Learning Rate: 0.00552278
	LOSS [training: 0.15407255307652637 | validation: 0.10457027090804391]
	TIME [epoch: 8.21 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10643930401770141		[learning rate: 0.0055161]
		[batch 20/20] avg loss: 0.12475954246237723		[learning rate: 0.0055094]
	Learning Rate: 0.00550941
	LOSS [training: 0.11559942324003933 | validation: 0.11746409368759751]
	TIME [epoch: 8.2 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1463052463471693		[learning rate: 0.0055027]
		[batch 20/20] avg loss: 0.06316753622709403		[learning rate: 0.0054961]
	Learning Rate: 0.00549607
	LOSS [training: 0.10473639128713164 | validation: 0.051824853991017494]
	TIME [epoch: 8.18 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07004702845161556		[learning rate: 0.0054894]
		[batch 20/20] avg loss: 0.07438768550174896		[learning rate: 0.0054828]
	Learning Rate: 0.00548277
	LOSS [training: 0.07221735697668226 | validation: 0.08760577665766174]
	TIME [epoch: 8.19 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08193567662470394		[learning rate: 0.0054761]
		[batch 20/20] avg loss: 0.09991072818952992		[learning rate: 0.0054695]
	Learning Rate: 0.0054695
	LOSS [training: 0.09092320240711693 | validation: 0.17236068264701998]
	TIME [epoch: 8.25 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10417138489812713		[learning rate: 0.0054629]
		[batch 20/20] avg loss: 0.1476167869833867		[learning rate: 0.0054563]
	Learning Rate: 0.00545626
	LOSS [training: 0.12589408594075693 | validation: 0.11424649022012401]
	TIME [epoch: 8.18 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09809559133859987		[learning rate: 0.0054496]
		[batch 20/20] avg loss: 0.07704371806025359		[learning rate: 0.005443]
	Learning Rate: 0.00544305
	LOSS [training: 0.08756965469942671 | validation: 0.057859917086558854]
	TIME [epoch: 8.18 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13140885329683405		[learning rate: 0.0054365]
		[batch 20/20] avg loss: 0.11487874206640167		[learning rate: 0.0054299]
	Learning Rate: 0.00542987
	LOSS [training: 0.12314379768161787 | validation: 0.09949305180247668]
	TIME [epoch: 8.18 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13345313959867455		[learning rate: 0.0054233]
		[batch 20/20] avg loss: 0.24828397530419935		[learning rate: 0.0054167]
	Learning Rate: 0.00541673
	LOSS [training: 0.1908685574514369 | validation: 0.055695705834673655]
	TIME [epoch: 8.22 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13324085761569524		[learning rate: 0.0054102]
		[batch 20/20] avg loss: 0.2538281203254119		[learning rate: 0.0054036]
	Learning Rate: 0.00540361
	LOSS [training: 0.19353448897055356 | validation: 0.2610002759114873]
	TIME [epoch: 8.2 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20416418519834903		[learning rate: 0.0053971]
		[batch 20/20] avg loss: 0.20603349900772283		[learning rate: 0.0053905]
	Learning Rate: 0.00539053
	LOSS [training: 0.20509884210303592 | validation: 0.06549549872373042]
	TIME [epoch: 8.18 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09978476445526356		[learning rate: 0.005384]
		[batch 20/20] avg loss: 0.09647112607055983		[learning rate: 0.0053775]
	Learning Rate: 0.00537748
	LOSS [training: 0.0981279452629117 | validation: 0.1369450868105685]
	TIME [epoch: 8.18 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45473285981225225		[learning rate: 0.005371]
		[batch 20/20] avg loss: 0.11968941048517726		[learning rate: 0.0053645]
	Learning Rate: 0.00536446
	LOSS [training: 0.28721113514871466 | validation: 0.0992473371665048]
	TIME [epoch: 8.24 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1641819083317189		[learning rate: 0.005358]
		[batch 20/20] avg loss: 0.1502967719398654		[learning rate: 0.0053515]
	Learning Rate: 0.00535148
	LOSS [training: 0.15723934013579213 | validation: 0.11457838049771865]
	TIME [epoch: 8.18 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09710435673874304		[learning rate: 0.005345]
		[batch 20/20] avg loss: 0.16613843986297333		[learning rate: 0.0053385]
	Learning Rate: 0.00533852
	LOSS [training: 0.13162139830085817 | validation: 0.3509688726659435]
	TIME [epoch: 8.17 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2697518737341193		[learning rate: 0.0053321]
		[batch 20/20] avg loss: 0.14027172427149198		[learning rate: 0.0053256]
	Learning Rate: 0.0053256
	LOSS [training: 0.2050117990028056 | validation: 0.14176369141715608]
	TIME [epoch: 8.17 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12600427439117762		[learning rate: 0.0053191]
		[batch 20/20] avg loss: 0.14054807860741897		[learning rate: 0.0053127]
	Learning Rate: 0.00531271
	LOSS [training: 0.1332761764992983 | validation: 0.08087691965051186]
	TIME [epoch: 8.2 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07495556001267711		[learning rate: 0.0053063]
		[batch 20/20] avg loss: 0.07782884623405503		[learning rate: 0.0052998]
	Learning Rate: 0.00529984
	LOSS [training: 0.07639220312336609 | validation: 0.0906452095435591]
	TIME [epoch: 8.21 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08744707829211515		[learning rate: 0.0052934]
		[batch 20/20] avg loss: 0.12831117398480887		[learning rate: 0.005287]
	Learning Rate: 0.00528701
	LOSS [training: 0.107879126138462 | validation: 0.08831146708179836]
	TIME [epoch: 8.19 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09504673358699813		[learning rate: 0.0052806]
		[batch 20/20] avg loss: 0.09745084789375313		[learning rate: 0.0052742]
	Learning Rate: 0.00527422
	LOSS [training: 0.09624879074037564 | validation: 0.1349500818605563]
	TIME [epoch: 8.18 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12679610470418945		[learning rate: 0.0052678]
		[batch 20/20] avg loss: 0.09177528943783383		[learning rate: 0.0052614]
	Learning Rate: 0.00526145
	LOSS [training: 0.10928569707101166 | validation: 0.11873582195798868]
	TIME [epoch: 8.25 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08572713203122997		[learning rate: 0.0052551]
		[batch 20/20] avg loss: 0.1664689128157731		[learning rate: 0.0052487]
	Learning Rate: 0.00524871
	LOSS [training: 0.12609802242350157 | validation: 0.08576806605898891]
	TIME [epoch: 8.18 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1800330670593187		[learning rate: 0.0052424]
		[batch 20/20] avg loss: 0.18507990091987486		[learning rate: 0.005236]
	Learning Rate: 0.005236
	LOSS [training: 0.1825564839895968 | validation: 0.13344980052423627]
	TIME [epoch: 8.18 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13884339623845665		[learning rate: 0.0052297]
		[batch 20/20] avg loss: 0.11353118375401165		[learning rate: 0.0052233]
	Learning Rate: 0.00522333
	LOSS [training: 0.12618728999623413 | validation: 0.05452876638540728]
	TIME [epoch: 8.18 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09018938852296957		[learning rate: 0.005217]
		[batch 20/20] avg loss: 0.06548801313143493		[learning rate: 0.0052107]
	Learning Rate: 0.00521068
	LOSS [training: 0.07783870082720225 | validation: 0.06259837489402806]
	TIME [epoch: 8.19 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07504223438188015		[learning rate: 0.0052044]
		[batch 20/20] avg loss: 0.08467024135032226		[learning rate: 0.0051981]
	Learning Rate: 0.00519807
	LOSS [training: 0.0798562378661012 | validation: 0.0781027762048689]
	TIME [epoch: 8.22 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09739579159805503		[learning rate: 0.0051918]
		[batch 20/20] avg loss: 0.06403890168354466		[learning rate: 0.0051855]
	Learning Rate: 0.00518549
	LOSS [training: 0.08071734664079983 | validation: 0.055733841732048005]
	TIME [epoch: 8.19 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07073211270595799		[learning rate: 0.0051792]
		[batch 20/20] avg loss: 0.09160412831395094		[learning rate: 0.0051729]
	Learning Rate: 0.00517293
	LOSS [training: 0.08116812050995446 | validation: 0.1208833480831823]
	TIME [epoch: 8.18 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0581185358801146		[learning rate: 0.0051667]
		[batch 20/20] avg loss: 0.08227511487304698		[learning rate: 0.0051604]
	Learning Rate: 0.00516041
	LOSS [training: 0.07019682537658077 | validation: 0.03892252752549902]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06585001534275185		[learning rate: 0.0051542]
		[batch 20/20] avg loss: 0.07509342040553288		[learning rate: 0.0051479]
	Learning Rate: 0.00514792
	LOSS [training: 0.07047171787414237 | validation: 0.06193648587581436]
	TIME [epoch: 8.2 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09388269429346854		[learning rate: 0.0051417]
		[batch 20/20] avg loss: 0.07864846874599571		[learning rate: 0.0051355]
	Learning Rate: 0.00513546
	LOSS [training: 0.08626558151973213 | validation: 0.07507159013409485]
	TIME [epoch: 8.16 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07335092562511854		[learning rate: 0.0051292]
		[batch 20/20] avg loss: 0.11100141227991175		[learning rate: 0.005123]
	Learning Rate: 0.00512302
	LOSS [training: 0.09217616895251515 | validation: 0.1067522926774113]
	TIME [epoch: 8.17 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08095543308416399		[learning rate: 0.0051168]
		[batch 20/20] avg loss: 0.3430022384130233		[learning rate: 0.0051106]
	Learning Rate: 0.00511062
	LOSS [training: 0.21197883574859366 | validation: 0.534794263148733]
	TIME [epoch: 8.18 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21685348398907975		[learning rate: 0.0051044]
		[batch 20/20] avg loss: 0.09709858613955957		[learning rate: 0.0050982]
	Learning Rate: 0.00509825
	LOSS [training: 0.15697603506431962 | validation: 0.1977360168594822]
	TIME [epoch: 8.2 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15415273758572934		[learning rate: 0.0050921]
		[batch 20/20] avg loss: 0.1320075034185865		[learning rate: 0.0050859]
	Learning Rate: 0.00508591
	LOSS [training: 0.1430801205021579 | validation: 0.21683947618171903]
	TIME [epoch: 8.17 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1272578511659418		[learning rate: 0.0050797]
		[batch 20/20] avg loss: 0.1110604516118269		[learning rate: 0.0050736]
	Learning Rate: 0.00507359
	LOSS [training: 0.1191591513888843 | validation: 0.29259527049545975]
	TIME [epoch: 8.17 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18760850180094701		[learning rate: 0.0050674]
		[batch 20/20] avg loss: 0.13753806614192907		[learning rate: 0.0050613]
	Learning Rate: 0.00506131
	LOSS [training: 0.16257328397143805 | validation: 0.10565238703087816]
	TIME [epoch: 8.22 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1084582331122796		[learning rate: 0.0050552]
		[batch 20/20] avg loss: 0.07275210382208677		[learning rate: 0.0050491]
	Learning Rate: 0.00504906
	LOSS [training: 0.09060516846718318 | validation: 0.04823840104141672]
	TIME [epoch: 8.2 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08174596569936925		[learning rate: 0.0050429]
		[batch 20/20] avg loss: 0.10652378480522184		[learning rate: 0.0050368]
	Learning Rate: 0.00503684
	LOSS [training: 0.09413487525229554 | validation: 0.1254800209269712]
	TIME [epoch: 8.17 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08473557980679632		[learning rate: 0.0050307]
		[batch 20/20] avg loss: 0.07417977343781554		[learning rate: 0.0050246]
	Learning Rate: 0.00502464
	LOSS [training: 0.07945767662230592 | validation: 0.05288553890948455]
	TIME [epoch: 8.17 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06898390236183978		[learning rate: 0.0050186]
		[batch 20/20] avg loss: 0.13409650867736136		[learning rate: 0.0050125]
	Learning Rate: 0.00501248
	LOSS [training: 0.10154020551960057 | validation: 0.07919743258240025]
	TIME [epoch: 8.19 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11510253874616776		[learning rate: 0.0050064]
		[batch 20/20] avg loss: 0.06932980811384641		[learning rate: 0.0050003]
	Learning Rate: 0.00500034
	LOSS [training: 0.09221617343000711 | validation: 0.08527966746751942]
	TIME [epoch: 8.21 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11417671829286236		[learning rate: 0.0049943]
		[batch 20/20] avg loss: 0.0861959322245053		[learning rate: 0.0049882]
	Learning Rate: 0.00498824
	LOSS [training: 0.10018632525868384 | validation: 0.16417022068732562]
	TIME [epoch: 8.18 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09291506307453987		[learning rate: 0.0049822]
		[batch 20/20] avg loss: 0.0762706783930831		[learning rate: 0.0049762]
	Learning Rate: 0.00497616
	LOSS [training: 0.08459287073381148 | validation: 0.08428137665035193]
	TIME [epoch: 8.18 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06268974567626891		[learning rate: 0.0049701]
		[batch 20/20] avg loss: 0.10506409279006945		[learning rate: 0.0049641]
	Learning Rate: 0.00496412
	LOSS [training: 0.08387691923316917 | validation: 0.057726150032459754]
	TIME [epoch: 8.22 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08871208229166985		[learning rate: 0.0049581]
		[batch 20/20] avg loss: 0.055491445414606155		[learning rate: 0.0049521]
	Learning Rate: 0.0049521
	LOSS [training: 0.072101763853138 | validation: 0.08042483025824687]
	TIME [epoch: 8.2 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04270638234608309		[learning rate: 0.0049461]
		[batch 20/20] avg loss: 0.06333373580940119		[learning rate: 0.0049401]
	Learning Rate: 0.00494011
	LOSS [training: 0.05302005907774214 | validation: 0.10708626608587093]
	TIME [epoch: 8.18 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0693807896472678		[learning rate: 0.0049341]
		[batch 20/20] avg loss: 0.17073887799528248		[learning rate: 0.0049282]
	Learning Rate: 0.00492815
	LOSS [training: 0.12005983382127516 | validation: 0.097300626584802]
	TIME [epoch: 8.18 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06449012249305716		[learning rate: 0.0049222]
		[batch 20/20] avg loss: 0.0561341649757345		[learning rate: 0.0049162]
	Learning Rate: 0.00491622
	LOSS [training: 0.06031214373439583 | validation: 0.10993707077612914]
	TIME [epoch: 8.2 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09507313883956187		[learning rate: 0.0049103]
		[batch 20/20] avg loss: 0.09074515828528697		[learning rate: 0.0049043]
	Learning Rate: 0.00490432
	LOSS [training: 0.09290914856242444 | validation: 0.048873935590590806]
	TIME [epoch: 8.21 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09336465461497581		[learning rate: 0.0048984]
		[batch 20/20] avg loss: 0.06250489320997203		[learning rate: 0.0048924]
	Learning Rate: 0.00489245
	LOSS [training: 0.07793477391247391 | validation: 0.04926427938539226]
	TIME [epoch: 8.18 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06612994679173588		[learning rate: 0.0048865]
		[batch 20/20] avg loss: 0.08297710500525231		[learning rate: 0.0048806]
	Learning Rate: 0.00488061
	LOSS [training: 0.0745535258984941 | validation: 0.11099907910823047]
	TIME [epoch: 8.22 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08111255884546294		[learning rate: 0.0048747]
		[batch 20/20] avg loss: 0.06767436746448546		[learning rate: 0.0048688]
	Learning Rate: 0.00486879
	LOSS [training: 0.0743934631549742 | validation: 0.04674959586208051]
	TIME [epoch: 8.18 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0477194818307989		[learning rate: 0.0048629]
		[batch 20/20] avg loss: 0.10298864200916587		[learning rate: 0.004857]
	Learning Rate: 0.004857
	LOSS [training: 0.0753540619199824 | validation: 0.06492122065737886]
	TIME [epoch: 8.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07548261498625573		[learning rate: 0.0048511]
		[batch 20/20] avg loss: 0.0602045200256184		[learning rate: 0.0048452]
	Learning Rate: 0.00484525
	LOSS [training: 0.06784356750593704 | validation: 0.02981586284013156]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10508758839872805		[learning rate: 0.0048394]
		[batch 20/20] avg loss: 0.07579826995742406		[learning rate: 0.0048335]
	Learning Rate: 0.00483352
	LOSS [training: 0.09044292917807605 | validation: 0.11351677545197414]
	TIME [epoch: 8.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08317981011938247		[learning rate: 0.0048277]
		[batch 20/20] avg loss: 0.07328882746256668		[learning rate: 0.0048218]
	Learning Rate: 0.00482181
	LOSS [training: 0.07823431879097456 | validation: 0.09477456152311278]
	TIME [epoch: 8.18 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05865099444004681		[learning rate: 0.004816]
		[batch 20/20] avg loss: 0.12774186587132524		[learning rate: 0.0048101]
	Learning Rate: 0.00481014
	LOSS [training: 0.09319643015568603 | validation: 0.10029693151714801]
	TIME [epoch: 8.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07403429293444293		[learning rate: 0.0048043]
		[batch 20/20] avg loss: 0.057926253705703735		[learning rate: 0.0047985]
	Learning Rate: 0.0047985
	LOSS [training: 0.06598027332007332 | validation: 0.04587113905146497]
	TIME [epoch: 8.19 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061113185490745556		[learning rate: 0.0047927]
		[batch 20/20] avg loss: 0.0585144133596178		[learning rate: 0.0047869]
	Learning Rate: 0.00478688
	LOSS [training: 0.05981379942518168 | validation: 0.06900183972779757]
	TIME [epoch: 8.19 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050685383193409594		[learning rate: 0.0047811]
		[batch 20/20] avg loss: 0.051213036731992824		[learning rate: 0.0047753]
	Learning Rate: 0.00477529
	LOSS [training: 0.050949209962701206 | validation: 0.09460034217419501]
	TIME [epoch: 8.17 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07515645768677856		[learning rate: 0.0047695]
		[batch 20/20] avg loss: 0.05454687831838784		[learning rate: 0.0047637]
	Learning Rate: 0.00476373
	LOSS [training: 0.0648516680025832 | validation: 0.03723448216048134]
	TIME [epoch: 8.19 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06841005212568828		[learning rate: 0.004758]
		[batch 20/20] avg loss: 0.10020909321065477		[learning rate: 0.0047522]
	Learning Rate: 0.0047522
	LOSS [training: 0.0843095726681715 | validation: 0.10185577463612243]
	TIME [epoch: 8.19 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0826303621162341		[learning rate: 0.0047464]
		[batch 20/20] avg loss: 0.0847337840333543		[learning rate: 0.0047407]
	Learning Rate: 0.0047407
	LOSS [training: 0.0836820730747942 | validation: 0.06779305035447172]
	TIME [epoch: 8.19 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07310018154184905		[learning rate: 0.004735]
		[batch 20/20] avg loss: 0.08245715559948415		[learning rate: 0.0047292]
	Learning Rate: 0.00472922
	LOSS [training: 0.07777866857066659 | validation: 0.033446130576685176]
	TIME [epoch: 8.17 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051888355999289905		[learning rate: 0.0047235]
		[batch 20/20] avg loss: 0.06714732453932629		[learning rate: 0.0047178]
	Learning Rate: 0.00471777
	LOSS [training: 0.059517840269308085 | validation: 0.03425763376614733]
	TIME [epoch: 8.2 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04911047239675318		[learning rate: 0.0047121]
		[batch 20/20] avg loss: 0.05100204422339309		[learning rate: 0.0047064]
	Learning Rate: 0.00470635
	LOSS [training: 0.05005625831007313 | validation: 0.05649925062585036]
	TIME [epoch: 8.21 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045994752094391536		[learning rate: 0.0047006]
		[batch 20/20] avg loss: 0.050771120551073634		[learning rate: 0.004695]
	Learning Rate: 0.00469496
	LOSS [training: 0.04838293632273258 | validation: 0.06041673205857206]
	TIME [epoch: 8.17 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06901837445925403		[learning rate: 0.0046893]
		[batch 20/20] avg loss: 0.08223019511636645		[learning rate: 0.0046836]
	Learning Rate: 0.00468359
	LOSS [training: 0.07562428478781023 | validation: 0.0645070996219526]
	TIME [epoch: 8.16 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05851674952278715		[learning rate: 0.0046779]
		[batch 20/20] avg loss: 0.054339172552290094		[learning rate: 0.0046723]
	Learning Rate: 0.00467225
	LOSS [training: 0.05642796103753861 | validation: 0.05276051318731899]
	TIME [epoch: 8.2 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06347625966971962		[learning rate: 0.0046666]
		[batch 20/20] avg loss: 0.07096721893100236		[learning rate: 0.0046609]
	Learning Rate: 0.00466094
	LOSS [training: 0.06722173930036097 | validation: 0.06893237863473944]
	TIME [epoch: 8.2 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05244051526908602		[learning rate: 0.0046553]
		[batch 20/20] avg loss: 0.052526959628088433		[learning rate: 0.0046497]
	Learning Rate: 0.00464966
	LOSS [training: 0.05248373744858722 | validation: 0.06822952129346166]
	TIME [epoch: 8.17 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07632441935297833		[learning rate: 0.004644]
		[batch 20/20] avg loss: 0.06760273839379315		[learning rate: 0.0046384]
	Learning Rate: 0.0046384
	LOSS [training: 0.07196357887338574 | validation: 0.1140909108094737]
	TIME [epoch: 8.17 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07670417049062792		[learning rate: 0.0046328]
		[batch 20/20] avg loss: 0.04970610483148571		[learning rate: 0.0046272]
	Learning Rate: 0.00462717
	LOSS [training: 0.06320513766105681 | validation: 0.061655511072277756]
	TIME [epoch: 8.22 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06542070319919112		[learning rate: 0.0046216]
		[batch 20/20] avg loss: 0.0546918484613861		[learning rate: 0.004616]
	Learning Rate: 0.00461597
	LOSS [training: 0.06005627583028862 | validation: 0.0984058506533736]
	TIME [epoch: 8.2 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07469248985082913		[learning rate: 0.0046104]
		[batch 20/20] avg loss: 0.0985440645415168		[learning rate: 0.0046048]
	Learning Rate: 0.0046048
	LOSS [training: 0.08661827719617296 | validation: 0.10282155524262342]
	TIME [epoch: 8.17 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06788306893635082		[learning rate: 0.0045992]
		[batch 20/20] avg loss: 0.1370209039825278		[learning rate: 0.0045936]
	Learning Rate: 0.00459365
	LOSS [training: 0.10245198645943931 | validation: 0.07079570784792986]
	TIME [epoch: 8.17 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06857014221829781		[learning rate: 0.0045881]
		[batch 20/20] avg loss: 0.10933520510145726		[learning rate: 0.0045825]
	Learning Rate: 0.00458253
	LOSS [training: 0.08895267365987754 | validation: 0.13885970251144703]
	TIME [epoch: 8.19 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10075625284737184		[learning rate: 0.004577]
		[batch 20/20] avg loss: 0.12387855716004456		[learning rate: 0.0045714]
	Learning Rate: 0.00457144
	LOSS [training: 0.11231740500370821 | validation: 0.12455460735880061]
	TIME [epoch: 8.2 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21463055052188515		[learning rate: 0.0045659]
		[batch 20/20] avg loss: 0.23118956074467256		[learning rate: 0.0045604]
	Learning Rate: 0.00456037
	LOSS [training: 0.22291005563327881 | validation: 0.07040981622005284]
	TIME [epoch: 8.18 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12927645630234225		[learning rate: 0.0045548]
		[batch 20/20] avg loss: 0.11823580536784077		[learning rate: 0.0045493]
	Learning Rate: 0.00454933
	LOSS [training: 0.12375613083509154 | validation: 0.10365002019710003]
	TIME [epoch: 8.18 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15021543210569743		[learning rate: 0.0045438]
		[batch 20/20] avg loss: 0.16912909784693547		[learning rate: 0.0045383]
	Learning Rate: 0.00453832
	LOSS [training: 0.15967226497631648 | validation: 0.19298891841247678]
	TIME [epoch: 8.23 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11785582508159624		[learning rate: 0.0045328]
		[batch 20/20] avg loss: 0.07383170963649154		[learning rate: 0.0045273]
	Learning Rate: 0.00452733
	LOSS [training: 0.0958437673590439 | validation: 0.05044697275627993]
	TIME [epoch: 8.18 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08860476746255817		[learning rate: 0.0045218]
		[batch 20/20] avg loss: 0.09236188456100723		[learning rate: 0.0045164]
	Learning Rate: 0.00451637
	LOSS [training: 0.0904833260117827 | validation: 0.07483585116254259]
	TIME [epoch: 8.17 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07419941254360979		[learning rate: 0.0045109]
		[batch 20/20] avg loss: 0.06273253383284297		[learning rate: 0.0045054]
	Learning Rate: 0.00450544
	LOSS [training: 0.06846597318822638 | validation: 0.05408402786551496]
	TIME [epoch: 8.17 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1188622060695362		[learning rate: 0.0045]
		[batch 20/20] avg loss: 0.09709985949281463		[learning rate: 0.0044945]
	Learning Rate: 0.00449453
	LOSS [training: 0.1079810327811754 | validation: 0.21738230415589252]
	TIME [epoch: 8.2 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09785440226870046		[learning rate: 0.0044891]
		[batch 20/20] avg loss: 0.11504500230329225		[learning rate: 0.0044836]
	Learning Rate: 0.00448365
	LOSS [training: 0.10644970228599634 | validation: 0.10870376939370485]
	TIME [epoch: 8.2 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11310423671848613		[learning rate: 0.0044782]
		[batch 20/20] avg loss: 0.0680938824146619		[learning rate: 0.0044728]
	Learning Rate: 0.00447279
	LOSS [training: 0.09059905956657402 | validation: 0.0669192708848759]
	TIME [epoch: 8.18 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0863534942954116		[learning rate: 0.0044674]
		[batch 20/20] avg loss: 0.1186029515470863		[learning rate: 0.004462]
	Learning Rate: 0.00446197
	LOSS [training: 0.10247822292124895 | validation: 0.08241984961647267]
	TIME [epoch: 8.18 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15822598451339756		[learning rate: 0.0044566]
		[batch 20/20] avg loss: 0.06543012134407164		[learning rate: 0.0044512]
	Learning Rate: 0.00445116
	LOSS [training: 0.11182805292873461 | validation: 0.03374933366015841]
	TIME [epoch: 8.22 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07963407513098772		[learning rate: 0.0044458]
		[batch 20/20] avg loss: 0.08788280963381223		[learning rate: 0.0044404]
	Learning Rate: 0.00444039
	LOSS [training: 0.08375844238239996 | validation: 0.05793261300956739]
	TIME [epoch: 8.2 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07667189776593679		[learning rate: 0.004435]
		[batch 20/20] avg loss: 0.04325941287738434		[learning rate: 0.0044296]
	Learning Rate: 0.00442964
	LOSS [training: 0.05996565532166058 | validation: 0.04709912562795994]
	TIME [epoch: 8.17 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06689212792617778		[learning rate: 0.0044243]
		[batch 20/20] avg loss: 0.051770374318588976		[learning rate: 0.0044189]
	Learning Rate: 0.00441892
	LOSS [training: 0.059331251122383376 | validation: 0.05659701226719624]
	TIME [epoch: 8.17 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08781764636397675		[learning rate: 0.0044136]
		[batch 20/20] avg loss: 0.06331216472829895		[learning rate: 0.0044082]
	Learning Rate: 0.00440822
	LOSS [training: 0.07556490554613786 | validation: 0.057685376925561305]
	TIME [epoch: 8.2 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05071366713804594		[learning rate: 0.0044029]
		[batch 20/20] avg loss: 0.038613863409457876		[learning rate: 0.0043975]
	Learning Rate: 0.00439755
	LOSS [training: 0.044663765273751904 | validation: 0.038399885305313165]
	TIME [epoch: 8.2 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05827867096766216		[learning rate: 0.0043922]
		[batch 20/20] avg loss: 0.055502557897866976		[learning rate: 0.0043869]
	Learning Rate: 0.0043869
	LOSS [training: 0.05689061443276457 | validation: 0.03297786562773788]
	TIME [epoch: 8.18 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07676752890691525		[learning rate: 0.0043816]
		[batch 20/20] avg loss: 0.06979601645500662		[learning rate: 0.0043763]
	Learning Rate: 0.00437628
	LOSS [training: 0.07328177268096095 | validation: 0.08553396446196483]
	TIME [epoch: 8.18 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04513688591154667		[learning rate: 0.004371]
		[batch 20/20] avg loss: 0.04274990763322368		[learning rate: 0.0043657]
	Learning Rate: 0.00436569
	LOSS [training: 0.04394339677238518 | validation: 0.03984167030785976]
	TIME [epoch: 8.22 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06929625907652279		[learning rate: 0.0043604]
		[batch 20/20] avg loss: 0.05951736288675384		[learning rate: 0.0043551]
	Learning Rate: 0.00435512
	LOSS [training: 0.06440681098163831 | validation: 0.03182309938917628]
	TIME [epoch: 8.2 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05855682597063215		[learning rate: 0.0043498]
		[batch 20/20] avg loss: 0.08388922034500126		[learning rate: 0.0043446]
	Learning Rate: 0.00434458
	LOSS [training: 0.07122302315781673 | validation: 0.03722966772500455]
	TIME [epoch: 8.17 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05535389234514713		[learning rate: 0.0043393]
		[batch 20/20] avg loss: 0.06828352536591262		[learning rate: 0.0043341]
	Learning Rate: 0.00433406
	LOSS [training: 0.06181870885552987 | validation: 0.035230354778309964]
	TIME [epoch: 8.17 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06263323043987683		[learning rate: 0.0043288]
		[batch 20/20] avg loss: 0.052621899480299925		[learning rate: 0.0043236]
	Learning Rate: 0.00432357
	LOSS [training: 0.057627564960088376 | validation: 0.049898251391529966]
	TIME [epoch: 8.19 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07570895353932552		[learning rate: 0.0043183]
		[batch 20/20] avg loss: 0.0643011580551874		[learning rate: 0.0043131]
	Learning Rate: 0.0043131
	LOSS [training: 0.07000505579725647 | validation: 0.03375406745080782]
	TIME [epoch: 8.2 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057229036787701595		[learning rate: 0.0043079]
		[batch 20/20] avg loss: 0.0625626813127945		[learning rate: 0.0043027]
	Learning Rate: 0.00430266
	LOSS [training: 0.05989585905024806 | validation: 0.04095003305457835]
	TIME [epoch: 8.18 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04096892466967354		[learning rate: 0.0042974]
		[batch 20/20] avg loss: 0.06391978463532397		[learning rate: 0.0042922]
	Learning Rate: 0.00429224
	LOSS [training: 0.05244435465249876 | validation: 0.04717174163486994]
	TIME [epoch: 8.18 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07676905175940227		[learning rate: 0.004287]
		[batch 20/20] avg loss: 0.04293489450487077		[learning rate: 0.0042819]
	Learning Rate: 0.00428185
	LOSS [training: 0.05985197313213651 | validation: 0.04998821596109444]
	TIME [epoch: 8.21 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06994250822486078		[learning rate: 0.0042767]
		[batch 20/20] avg loss: 0.05932371566610034		[learning rate: 0.0042715]
	Learning Rate: 0.00427149
	LOSS [training: 0.06463311194548058 | validation: 0.029957420232749304]
	TIME [epoch: 8.2 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04138024911029269		[learning rate: 0.0042663]
		[batch 20/20] avg loss: 0.07411663935611093		[learning rate: 0.0042611]
	Learning Rate: 0.00426114
	LOSS [training: 0.0577484442332018 | validation: 0.046907668014650405]
	TIME [epoch: 8.17 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0458030657503599		[learning rate: 0.004256]
		[batch 20/20] avg loss: 0.04849073250911242		[learning rate: 0.0042508]
	Learning Rate: 0.00425083
	LOSS [training: 0.04714689912973617 | validation: 0.04856629282953916]
	TIME [epoch: 8.17 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04588964397253677		[learning rate: 0.0042457]
		[batch 20/20] avg loss: 0.04976501808300815		[learning rate: 0.0042405]
	Learning Rate: 0.00424054
	LOSS [training: 0.04782733102777245 | validation: 0.1355176793136555]
	TIME [epoch: 8.2 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07971430346239444		[learning rate: 0.0042354]
		[batch 20/20] avg loss: 0.062314823344999125		[learning rate: 0.0042303]
	Learning Rate: 0.00423027
	LOSS [training: 0.07101456340369679 | validation: 0.08973299169458208]
	TIME [epoch: 8.2 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045625814109357433		[learning rate: 0.0042251]
		[batch 20/20] avg loss: 0.044426039165955354		[learning rate: 0.00422]
	Learning Rate: 0.00422003
	LOSS [training: 0.04502592663765639 | validation: 0.029691475398371334]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04652356945548861		[learning rate: 0.0042149]
		[batch 20/20] avg loss: 0.0369275561510624		[learning rate: 0.0042098]
	Learning Rate: 0.00420982
	LOSS [training: 0.041725562803275495 | validation: 0.02996514317631245]
	TIME [epoch: 8.19 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09156072766299109		[learning rate: 0.0042047]
		[batch 20/20] avg loss: 0.08655009788547485		[learning rate: 0.0041996]
	Learning Rate: 0.00419962
	LOSS [training: 0.08905541277423296 | validation: 0.08917832332579233]
	TIME [epoch: 8.19 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059908074396369394		[learning rate: 0.0041945]
		[batch 20/20] avg loss: 0.05662973057918491		[learning rate: 0.0041895]
	Learning Rate: 0.00418946
	LOSS [training: 0.05826890248777716 | validation: 0.07360063188686435]
	TIME [epoch: 8.19 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055959873626975944		[learning rate: 0.0041844]
		[batch 20/20] avg loss: 0.06189349114508573		[learning rate: 0.0041793]
	Learning Rate: 0.00417932
	LOSS [training: 0.05892668238603085 | validation: 0.09925180240035843]
	TIME [epoch: 8.17 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06093978373374007		[learning rate: 0.0041743]
		[batch 20/20] avg loss: 0.06436305505505795		[learning rate: 0.0041692]
	Learning Rate: 0.0041692
	LOSS [training: 0.06265141939439903 | validation: 0.07545734798885978]
	TIME [epoch: 8.17 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06767172416120751		[learning rate: 0.0041641]
		[batch 20/20] avg loss: 0.06897987827202957		[learning rate: 0.0041591]
	Learning Rate: 0.00415911
	LOSS [training: 0.06832580121661855 | validation: 0.03282454322412984]
	TIME [epoch: 8.18 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0447267701373889		[learning rate: 0.0041541]
		[batch 20/20] avg loss: 0.0440468655356059		[learning rate: 0.004149]
	Learning Rate: 0.00414904
	LOSS [training: 0.0443868178364974 | validation: 0.04817054944202948]
	TIME [epoch: 8.21 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05176386376937806		[learning rate: 0.004144]
		[batch 20/20] avg loss: 0.03560492493400826		[learning rate: 0.004139]
	Learning Rate: 0.00413899
	LOSS [training: 0.04368439435169316 | validation: 0.015529009134957887]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04969120177134466		[learning rate: 0.004134]
		[batch 20/20] avg loss: 0.06007738991248633		[learning rate: 0.004129]
	Learning Rate: 0.00412897
	LOSS [training: 0.054884295841915495 | validation: 0.05799498663723196]
	TIME [epoch: 8.18 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08296128989527893		[learning rate: 0.004124]
		[batch 20/20] avg loss: 0.10792310506532295		[learning rate: 0.004119]
	Learning Rate: 0.00411898
	LOSS [training: 0.09544219748030093 | validation: 0.09656213242811984]
	TIME [epoch: 8.21 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0948558578010294		[learning rate: 0.004114]
		[batch 20/20] avg loss: 0.06209229331351333		[learning rate: 0.004109]
	Learning Rate: 0.00410901
	LOSS [training: 0.07847407555727137 | validation: 0.044798111367514666]
	TIME [epoch: 8.2 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0782626633433198		[learning rate: 0.004104]
		[batch 20/20] avg loss: 0.05237718867959196		[learning rate: 0.0040991]
	Learning Rate: 0.00409906
	LOSS [training: 0.06531992601145588 | validation: 0.029523338690964705]
	TIME [epoch: 8.17 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035601572892545355		[learning rate: 0.0040941]
		[batch 20/20] avg loss: 0.06064140654497148		[learning rate: 0.0040891]
	Learning Rate: 0.00408914
	LOSS [training: 0.04812148971875842 | validation: 0.039320977974870795]
	TIME [epoch: 8.17 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049620674483310576		[learning rate: 0.0040842]
		[batch 20/20] avg loss: 0.06542343903513018		[learning rate: 0.0040792]
	Learning Rate: 0.00407924
	LOSS [training: 0.057522056759220376 | validation: 0.2144556878745149]
	TIME [epoch: 8.17 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12410708295998658		[learning rate: 0.0040743]
		[batch 20/20] avg loss: 0.06722011363649331		[learning rate: 0.0040694]
	Learning Rate: 0.00406936
	LOSS [training: 0.09566359829823996 | validation: 0.08166644028382325]
	TIME [epoch: 8.22 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22185049795713105		[learning rate: 0.0040644]
		[batch 20/20] avg loss: 0.11855239128738373		[learning rate: 0.0040595]
	Learning Rate: 0.00405951
	LOSS [training: 0.17020144462225742 | validation: 0.15380364177277017]
	TIME [epoch: 8.17 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06394969782407986		[learning rate: 0.0040546]
		[batch 20/20] avg loss: 0.056872956255998065		[learning rate: 0.0040497]
	Learning Rate: 0.00404968
	LOSS [training: 0.06041132704003895 | validation: 0.13131232894052225]
	TIME [epoch: 8.17 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06730818645437824		[learning rate: 0.0040448]
		[batch 20/20] avg loss: 0.08017389898977982		[learning rate: 0.0040399]
	Learning Rate: 0.00403988
	LOSS [training: 0.07374104272207903 | validation: 0.130893585656793]
	TIME [epoch: 8.19 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0841036468875206		[learning rate: 0.004035]
		[batch 20/20] avg loss: 0.08025416853409889		[learning rate: 0.0040301]
	Learning Rate: 0.0040301
	LOSS [training: 0.08217890771080974 | validation: 0.10872364648675327]
	TIME [epoch: 8.21 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08489488523632599		[learning rate: 0.0040252]
		[batch 20/20] avg loss: 0.03175446593850506		[learning rate: 0.0040203]
	Learning Rate: 0.00402034
	LOSS [training: 0.05832467558741553 | validation: 0.03330867866916966]
	TIME [epoch: 8.17 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06838989243526979		[learning rate: 0.0040155]
		[batch 20/20] avg loss: 0.06243873961685094		[learning rate: 0.0040106]
	Learning Rate: 0.00401061
	LOSS [training: 0.06541431602606036 | validation: 0.05548401951289462]
	TIME [epoch: 8.17 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04279308737645292		[learning rate: 0.0040058]
		[batch 20/20] avg loss: 0.040674373686269735		[learning rate: 0.0040009]
	Learning Rate: 0.0040009
	LOSS [training: 0.04173373053136133 | validation: 0.033500251387855784]
	TIME [epoch: 8.18 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0675764452619189		[learning rate: 0.0039961]
		[batch 20/20] avg loss: 0.05708288704518275		[learning rate: 0.0039912]
	Learning Rate: 0.00399122
	LOSS [training: 0.06232966615355083 | validation: 0.050482530930214826]
	TIME [epoch: 8.22 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06260887306561598		[learning rate: 0.0039864]
		[batch 20/20] avg loss: 0.08462722885966323		[learning rate: 0.0039816]
	Learning Rate: 0.00398155
	LOSS [training: 0.07361805096263961 | validation: 0.06032658140453453]
	TIME [epoch: 8.18 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07813295876874157		[learning rate: 0.0039767]
		[batch 20/20] avg loss: 0.07334273676126649		[learning rate: 0.0039719]
	Learning Rate: 0.00397192
	LOSS [training: 0.07573784776500402 | validation: 0.12155996740468054]
	TIME [epoch: 8.18 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07177920900657848		[learning rate: 0.0039671]
		[batch 20/20] avg loss: 0.04743861425135714		[learning rate: 0.0039623]
	Learning Rate: 0.0039623
	LOSS [training: 0.05960891162896781 | validation: 0.045031121078428535]
	TIME [epoch: 8.21 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04354065457821406		[learning rate: 0.0039575]
		[batch 20/20] avg loss: 0.0728755813582204		[learning rate: 0.0039527]
	Learning Rate: 0.00395271
	LOSS [training: 0.05820811796821722 | validation: 0.033309933285488216]
	TIME [epoch: 8.19 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048141629816814815		[learning rate: 0.0039479]
		[batch 20/20] avg loss: 0.058511968898711034		[learning rate: 0.0039431]
	Learning Rate: 0.00394314
	LOSS [training: 0.053326799357762925 | validation: 0.03530860870490243]
	TIME [epoch: 8.17 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07451086035034486		[learning rate: 0.0039384]
		[batch 20/20] avg loss: 0.08516045307166245		[learning rate: 0.0039336]
	Learning Rate: 0.00393359
	LOSS [training: 0.07983565671100366 | validation: 0.0667896699096138]
	TIME [epoch: 8.17 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11081738335071165		[learning rate: 0.0039288]
		[batch 20/20] avg loss: 0.10302241656896802		[learning rate: 0.0039241]
	Learning Rate: 0.00392407
	LOSS [training: 0.10691989995983982 | validation: 0.1017947816629291]
	TIME [epoch: 8.19 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08378153141626175		[learning rate: 0.0039193]
		[batch 20/20] avg loss: 0.08263449867850334		[learning rate: 0.0039146]
	Learning Rate: 0.00391457
	LOSS [training: 0.08320801504738257 | validation: 0.09270921145922707]
	TIME [epoch: 8.2 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06257939502522733		[learning rate: 0.0039098]
		[batch 20/20] avg loss: 0.06343618220865666		[learning rate: 0.0039051]
	Learning Rate: 0.00390509
	LOSS [training: 0.06300778861694199 | validation: 0.03655676334291555]
	TIME [epoch: 8.18 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049014127374939875		[learning rate: 0.0039004]
		[batch 20/20] avg loss: 0.04632559564134784		[learning rate: 0.0038956]
	Learning Rate: 0.00389564
	LOSS [training: 0.04766986150814386 | validation: 0.06440831631770035]
	TIME [epoch: 8.19 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058818854909368556		[learning rate: 0.0038909]
		[batch 20/20] avg loss: 0.06946239857131188		[learning rate: 0.0038862]
	Learning Rate: 0.00388621
	LOSS [training: 0.06414062674034021 | validation: 0.060465574095813344]
	TIME [epoch: 8.19 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04959311019960952		[learning rate: 0.0038815]
		[batch 20/20] avg loss: 0.06406721161370803		[learning rate: 0.0038768]
	Learning Rate: 0.0038768
	LOSS [training: 0.05683016090665878 | validation: 0.04950584340362723]
	TIME [epoch: 8.2 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07200737005073811		[learning rate: 0.0038721]
		[batch 20/20] avg loss: 0.07362414456594896		[learning rate: 0.0038674]
	Learning Rate: 0.00386742
	LOSS [training: 0.07281575730834353 | validation: 0.09838660513661221]
	TIME [epoch: 8.18 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06048688964189354		[learning rate: 0.0038627]
		[batch 20/20] avg loss: 0.06761681692465739		[learning rate: 0.0038581]
	Learning Rate: 0.00385805
	LOSS [training: 0.06405185328327546 | validation: 0.07625427167433899]
	TIME [epoch: 8.17 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05101512896807917		[learning rate: 0.0038534]
		[batch 20/20] avg loss: 0.04275680550640245		[learning rate: 0.0038487]
	Learning Rate: 0.00384871
	LOSS [training: 0.046885967237240804 | validation: 0.05610227906899286]
	TIME [epoch: 8.17 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039370742053506394		[learning rate: 0.0038441]
		[batch 20/20] avg loss: 0.0789339606014355		[learning rate: 0.0038394]
	Learning Rate: 0.0038394
	LOSS [training: 0.059152351327470945 | validation: 0.10206458511977896]
	TIME [epoch: 8.22 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10639592823677593		[learning rate: 0.0038347]
		[batch 20/20] avg loss: 0.0868148322317664		[learning rate: 0.0038301]
	Learning Rate: 0.0038301
	LOSS [training: 0.09660538023427115 | validation: 0.1700574600613309]
	TIME [epoch: 8.18 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1390849829678166		[learning rate: 0.0038255]
		[batch 20/20] avg loss: 0.06539584765625732		[learning rate: 0.0038208]
	Learning Rate: 0.00382083
	LOSS [training: 0.10224041531203694 | validation: 0.03914586698746725]
	TIME [epoch: 8.18 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04188540248482678		[learning rate: 0.0038162]
		[batch 20/20] avg loss: 0.04516747893674776		[learning rate: 0.0038116]
	Learning Rate: 0.00381158
	LOSS [training: 0.04352644071078727 | validation: 0.05328422964980419]
	TIME [epoch: 8.19 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061962461129331514		[learning rate: 0.003807]
		[batch 20/20] avg loss: 0.0994017730075448		[learning rate: 0.0038024]
	Learning Rate: 0.00380235
	LOSS [training: 0.08068211706843816 | validation: 0.11770320627262823]
	TIME [epoch: 8.23 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07154116073755475		[learning rate: 0.0037977]
		[batch 20/20] avg loss: 0.04478504834743901		[learning rate: 0.0037931]
	Learning Rate: 0.00379315
	LOSS [training: 0.05816310454249687 | validation: 0.04216717065067965]
	TIME [epoch: 8.18 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0486970276223178		[learning rate: 0.0037886]
		[batch 20/20] avg loss: 0.05549326983315315		[learning rate: 0.003784]
	Learning Rate: 0.00378397
	LOSS [training: 0.05209514872773546 | validation: 0.05239814660782693]
	TIME [epoch: 8.17 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07047990713765909		[learning rate: 0.0037794]
		[batch 20/20] avg loss: 0.03913408850730356		[learning rate: 0.0037748]
	Learning Rate: 0.00377481
	LOSS [training: 0.054806997822481315 | validation: 0.03016674878985899]
	TIME [epoch: 8.17 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03585250707212386		[learning rate: 0.0037702]
		[batch 20/20] avg loss: 0.06961269492552039		[learning rate: 0.0037657]
	Learning Rate: 0.00376567
	LOSS [training: 0.05273260099882212 | validation: 0.046194892867873205]
	TIME [epoch: 8.2 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054773574419118974		[learning rate: 0.0037611]
		[batch 20/20] avg loss: 0.03868676134488232		[learning rate: 0.0037566]
	Learning Rate: 0.00375655
	LOSS [training: 0.046730167882000635 | validation: 0.049300803574650075]
	TIME [epoch: 8.19 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08271626312814344		[learning rate: 0.003752]
		[batch 20/20] avg loss: 0.053460858820084056		[learning rate: 0.0037475]
	Learning Rate: 0.00374746
	LOSS [training: 0.06808856097411374 | validation: 0.039457024970471]
	TIME [epoch: 8.18 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037527538756086894		[learning rate: 0.0037429]
		[batch 20/20] avg loss: 0.05753263941570169		[learning rate: 0.0037384]
	Learning Rate: 0.00373839
	LOSS [training: 0.047530089085894296 | validation: 0.04539954553251367]
	TIME [epoch: 8.18 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05180179583834025		[learning rate: 0.0037339]
		[batch 20/20] avg loss: 0.06751721947190059		[learning rate: 0.0037293]
	Learning Rate: 0.00372934
	LOSS [training: 0.05965950765512042 | validation: 0.05338975241270807]
	TIME [epoch: 8.24 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04346386602940387		[learning rate: 0.0037248]
		[batch 20/20] avg loss: 0.08640680660417106		[learning rate: 0.0037203]
	Learning Rate: 0.00372031
	LOSS [training: 0.06493533631678747 | validation: 0.028410410145517876]
	TIME [epoch: 8.18 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04138586075003818		[learning rate: 0.0037158]
		[batch 20/20] avg loss: 0.08166273742166154		[learning rate: 0.0037113]
	Learning Rate: 0.0037113
	LOSS [training: 0.06152429908584985 | validation: 0.03195634945657628]
	TIME [epoch: 8.17 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043113148772416524		[learning rate: 0.0037068]
		[batch 20/20] avg loss: 0.05685693863373398		[learning rate: 0.0037023]
	Learning Rate: 0.00370232
	LOSS [training: 0.04998504370307525 | validation: 0.04257385288149229]
	TIME [epoch: 8.17 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05613720060332959		[learning rate: 0.0036978]
		[batch 20/20] avg loss: 0.06287775075492727		[learning rate: 0.0036934]
	Learning Rate: 0.00369336
	LOSS [training: 0.05950747567912845 | validation: 0.03736729401889559]
	TIME [epoch: 8.2 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05019499580526713		[learning rate: 0.0036889]
		[batch 20/20] avg loss: 0.04779869839985584		[learning rate: 0.0036844]
	Learning Rate: 0.00368441
	LOSS [training: 0.04899684710256148 | validation: 0.05376291307855137]
	TIME [epoch: 8.2 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05763356553951623		[learning rate: 0.00368]
		[batch 20/20] avg loss: 0.052299903377481215		[learning rate: 0.0036755]
	Learning Rate: 0.00367549
	LOSS [training: 0.05496673445849872 | validation: 0.03178156822646982]
	TIME [epoch: 8.18 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05569724588037731		[learning rate: 0.003671]
		[batch 20/20] avg loss: 0.06268099497727764		[learning rate: 0.0036666]
	Learning Rate: 0.0036666
	LOSS [training: 0.05918912042882747 | validation: 0.05194478768712504]
	TIME [epoch: 8.18 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06132030829545874		[learning rate: 0.0036622]
		[batch 20/20] avg loss: 0.04620908177051304		[learning rate: 0.0036577]
	Learning Rate: 0.00365772
	LOSS [training: 0.05376469503298588 | validation: 0.05603643822805802]
	TIME [epoch: 8.22 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07780192959906791		[learning rate: 0.0036533]
		[batch 20/20] avg loss: 0.04311137337708245		[learning rate: 0.0036489]
	Learning Rate: 0.00364887
	LOSS [training: 0.06045665148807518 | validation: 0.04870902767590433]
	TIME [epoch: 8.2 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03950747795377344		[learning rate: 0.0036444]
		[batch 20/20] avg loss: 0.057926234824478284		[learning rate: 0.00364]
	Learning Rate: 0.00364003
	LOSS [training: 0.04871685638912586 | validation: 0.035092011667329996]
	TIME [epoch: 8.17 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030923395128395325		[learning rate: 0.0036356]
		[batch 20/20] avg loss: 0.058281656858497545		[learning rate: 0.0036312]
	Learning Rate: 0.00363122
	LOSS [training: 0.04460252599344643 | validation: 0.027963962091401104]
	TIME [epoch: 8.18 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04190127191242614		[learning rate: 0.0036268]
		[batch 20/20] avg loss: 0.05050221216625663		[learning rate: 0.0036224]
	Learning Rate: 0.00362243
	LOSS [training: 0.046201742039341395 | validation: 0.026294077115460834]
	TIME [epoch: 8.22 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07255193479456297		[learning rate: 0.003618]
		[batch 20/20] avg loss: 0.05373588068768076		[learning rate: 0.0036137]
	Learning Rate: 0.00361366
	LOSS [training: 0.06314390774112186 | validation: 0.03919314228002032]
	TIME [epoch: 8.19 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04988030782986795		[learning rate: 0.0036093]
		[batch 20/20] avg loss: 0.05943937737282133		[learning rate: 0.0036049]
	Learning Rate: 0.00360491
	LOSS [training: 0.05465984260134464 | validation: 0.028835065710271127]
	TIME [epoch: 8.2 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047015109802334504		[learning rate: 0.0036005]
		[batch 20/20] avg loss: 0.053217731074446314		[learning rate: 0.0035962]
	Learning Rate: 0.00359619
	LOSS [training: 0.050116420438390395 | validation: 0.056978654287016844]
	TIME [epoch: 8.19 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07852366676156339		[learning rate: 0.0035918]
		[batch 20/20] avg loss: 0.04817053071821758		[learning rate: 0.0035875]
	Learning Rate: 0.00358748
	LOSS [training: 0.06334709873989049 | validation: 0.03924060603896157]
	TIME [epoch: 8.21 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050337807406908616		[learning rate: 0.0035831]
		[batch 20/20] avg loss: 0.06551189229602059		[learning rate: 0.0035788]
	Learning Rate: 0.0035788
	LOSS [training: 0.0579248498514646 | validation: 0.0651789499546007]
	TIME [epoch: 8.21 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05026260824776089		[learning rate: 0.0035745]
		[batch 20/20] avg loss: 0.044532312393412765		[learning rate: 0.0035701]
	Learning Rate: 0.00357013
	LOSS [training: 0.04739746032058682 | validation: 0.030683083037773232]
	TIME [epoch: 8.19 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04510132679185216		[learning rate: 0.0035658]
		[batch 20/20] avg loss: 0.057425572842425776		[learning rate: 0.0035615]
	Learning Rate: 0.00356149
	LOSS [training: 0.05126344981713897 | validation: 0.04262812280153307]
	TIME [epoch: 8.18 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0420811619915615		[learning rate: 0.0035572]
		[batch 20/20] avg loss: 0.04564571678143457		[learning rate: 0.0035529]
	Learning Rate: 0.00355287
	LOSS [training: 0.043863439386498025 | validation: 0.041210025177738675]
	TIME [epoch: 8.19 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08580411854844658		[learning rate: 0.0035486]
		[batch 20/20] avg loss: 0.06293282967515182		[learning rate: 0.0035443]
	Learning Rate: 0.00354427
	LOSS [training: 0.07436847411179917 | validation: 0.06309914442009874]
	TIME [epoch: 8.19 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056847211018804654		[learning rate: 0.00354]
		[batch 20/20] avg loss: 0.06795802633626348		[learning rate: 0.0035357]
	Learning Rate: 0.00353569
	LOSS [training: 0.062402618677534064 | validation: 0.16393927751779108]
	TIME [epoch: 8.18 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20549793254594784		[learning rate: 0.0035314]
		[batch 20/20] avg loss: 0.16677572894623421		[learning rate: 0.0035271]
	Learning Rate: 0.00352713
	LOSS [training: 0.18613683074609103 | validation: 0.10107869189147284]
	TIME [epoch: 8.18 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1059058980240889		[learning rate: 0.0035229]
		[batch 20/20] avg loss: 0.11370338854301432		[learning rate: 0.0035186]
	Learning Rate: 0.00351859
	LOSS [training: 0.10980464328355159 | validation: 0.06895298622224838]
	TIME [epoch: 8.2 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07678283216998635		[learning rate: 0.0035143]
		[batch 20/20] avg loss: 0.0851139134356567		[learning rate: 0.0035101]
	Learning Rate: 0.00351007
	LOSS [training: 0.08094837280282152 | validation: 0.030567893783622124]
	TIME [epoch: 8.21 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06001312153112633		[learning rate: 0.0035058]
		[batch 20/20] avg loss: 0.036704642727585605		[learning rate: 0.0035016]
	Learning Rate: 0.00350157
	LOSS [training: 0.048358882129355976 | validation: 0.097740985577991]
	TIME [epoch: 8.19 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062114504391558634		[learning rate: 0.0034973]
		[batch 20/20] avg loss: 0.03501767209634679		[learning rate: 0.0034931]
	Learning Rate: 0.0034931
	LOSS [training: 0.04856608824395271 | validation: 0.05031441644077571]
	TIME [epoch: 8.19 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04622352969768428		[learning rate: 0.0034889]
		[batch 20/20] avg loss: 0.04437283052011791		[learning rate: 0.0034846]
	Learning Rate: 0.00348464
	LOSS [training: 0.04529818010890109 | validation: 0.03466291433484505]
	TIME [epoch: 8.2 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03173440742189571		[learning rate: 0.0034804]
		[batch 20/20] avg loss: 0.05646946532578341		[learning rate: 0.0034762]
	Learning Rate: 0.0034762
	LOSS [training: 0.04410193637383956 | validation: 0.03480130647693752]
	TIME [epoch: 8.2 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04150892228550764		[learning rate: 0.003472]
		[batch 20/20] avg loss: 0.029675907847172785		[learning rate: 0.0034678]
	Learning Rate: 0.00346779
	LOSS [training: 0.03559241506634021 | validation: 0.023528469797577875]
	TIME [epoch: 8.18 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051621688470597796		[learning rate: 0.0034636]
		[batch 20/20] avg loss: 0.04394761174299396		[learning rate: 0.0034594]
	Learning Rate: 0.00345939
	LOSS [training: 0.047784650106795876 | validation: 0.06901017576570559]
	TIME [epoch: 8.18 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06994252099869784		[learning rate: 0.0034552]
		[batch 20/20] avg loss: 0.056030376367899294		[learning rate: 0.003451]
	Learning Rate: 0.00345102
	LOSS [training: 0.06298644868329858 | validation: 0.08419916220152464]
	TIME [epoch: 8.18 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07364924436824938		[learning rate: 0.0034468]
		[batch 20/20] avg loss: 0.05209694570958667		[learning rate: 0.0034427]
	Learning Rate: 0.00344266
	LOSS [training: 0.06287309503891803 | validation: 0.032383651309078734]
	TIME [epoch: 8.21 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061873486086016935		[learning rate: 0.0034385]
		[batch 20/20] avg loss: 0.03681896630745964		[learning rate: 0.0034343]
	Learning Rate: 0.00343433
	LOSS [training: 0.0493462261967383 | validation: 0.033465191758688215]
	TIME [epoch: 8.2 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030099790692828737		[learning rate: 0.0034302]
		[batch 20/20] avg loss: 0.049001199827307954		[learning rate: 0.003426]
	Learning Rate: 0.00342602
	LOSS [training: 0.039550495260068354 | validation: 0.0333227363278868]
	TIME [epoch: 8.18 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07645863175751257		[learning rate: 0.0034219]
		[batch 20/20] avg loss: 0.10739326327815282		[learning rate: 0.0034177]
	Learning Rate: 0.00341772
	LOSS [training: 0.09192594751783269 | validation: 0.08571603999634092]
	TIME [epoch: 8.19 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0461527059567663		[learning rate: 0.0034136]
		[batch 20/20] avg loss: 0.05143938602287305		[learning rate: 0.0034094]
	Learning Rate: 0.00340945
	LOSS [training: 0.048796045989819674 | validation: 0.08047724870285432]
	TIME [epoch: 8.24 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06639448134612173		[learning rate: 0.0034053]
		[batch 20/20] avg loss: 0.05670255593372088		[learning rate: 0.0034012]
	Learning Rate: 0.0034012
	LOSS [training: 0.0615485186399213 | validation: 0.05886544768773009]
	TIME [epoch: 8.17 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04898400642435876		[learning rate: 0.0033971]
		[batch 20/20] avg loss: 0.08012822263813502		[learning rate: 0.003393]
	Learning Rate: 0.00339296
	LOSS [training: 0.0645561145312469 | validation: 0.15028303888214886]
	TIME [epoch: 8.18 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06743048721237649		[learning rate: 0.0033889]
		[batch 20/20] avg loss: 0.05547858664351389		[learning rate: 0.0033847]
	Learning Rate: 0.00338475
	LOSS [training: 0.061454536927945204 | validation: 0.051517707156414606]
	TIME [epoch: 8.18 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04264782350209466		[learning rate: 0.0033806]
		[batch 20/20] avg loss: 0.031329636328093706		[learning rate: 0.0033766]
	Learning Rate: 0.00337655
	LOSS [training: 0.03698872991509419 | validation: 0.07622065051643205]
	TIME [epoch: 8.2 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0589435373051474		[learning rate: 0.0033725]
		[batch 20/20] avg loss: 0.04742026593747812		[learning rate: 0.0033684]
	Learning Rate: 0.00336838
	LOSS [training: 0.05318190162131274 | validation: 0.040564904267107425]
	TIME [epoch: 8.2 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023782317765325328		[learning rate: 0.0033643]
		[batch 20/20] avg loss: 0.045003048469396886		[learning rate: 0.0033602]
	Learning Rate: 0.00336023
	LOSS [training: 0.034392683117361114 | validation: 0.03861148528583067]
	TIME [epoch: 8.18 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039026447007730766		[learning rate: 0.0033562]
		[batch 20/20] avg loss: 0.04954691663915436		[learning rate: 0.0033521]
	Learning Rate: 0.00335209
	LOSS [training: 0.044286681823442565 | validation: 0.03507315872900625]
	TIME [epoch: 8.18 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048947430387940485		[learning rate: 0.003348]
		[batch 20/20] avg loss: 0.06303686621287433		[learning rate: 0.003344]
	Learning Rate: 0.00334398
	LOSS [training: 0.055992148300407406 | validation: 0.02889159396407222]
	TIME [epoch: 8.24 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0559975566202933		[learning rate: 0.0033399]
		[batch 20/20] avg loss: 0.028786512103780165		[learning rate: 0.0033359]
	Learning Rate: 0.00333588
	LOSS [training: 0.04239203436203674 | validation: 0.035428614996232746]
	TIME [epoch: 8.19 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03843813069097297		[learning rate: 0.0033318]
		[batch 20/20] avg loss: 0.05416666545053998		[learning rate: 0.0033278]
	Learning Rate: 0.00332781
	LOSS [training: 0.04630239807075647 | validation: 0.07229976379233666]
	TIME [epoch: 8.17 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056056998700315566		[learning rate: 0.0033238]
		[batch 20/20] avg loss: 0.10249978498888199		[learning rate: 0.0033197]
	Learning Rate: 0.00331975
	LOSS [training: 0.07927839184459877 | validation: 0.05509376277047274]
	TIME [epoch: 8.17 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0573563437928614		[learning rate: 0.0033157]
		[batch 20/20] avg loss: 0.051680173127688314		[learning rate: 0.0033117]
	Learning Rate: 0.00331171
	LOSS [training: 0.05451825846027486 | validation: 0.10518142408373819]
	TIME [epoch: 8.2 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08245639356980247		[learning rate: 0.0033077]
		[batch 20/20] avg loss: 0.060389017986514304		[learning rate: 0.0033037]
	Learning Rate: 0.0033037
	LOSS [training: 0.07142270577815837 | validation: 0.057189268094086275]
	TIME [epoch: 8.18 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0623785319489772		[learning rate: 0.0032997]
		[batch 20/20] avg loss: 0.07139506566151799		[learning rate: 0.0032957]
	Learning Rate: 0.0032957
	LOSS [training: 0.06688679880524759 | validation: 0.15648176150015275]
	TIME [epoch: 8.2 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1415037454807821		[learning rate: 0.0032917]
		[batch 20/20] avg loss: 0.07440684524547277		[learning rate: 0.0032877]
	Learning Rate: 0.00328772
	LOSS [training: 0.10795529536312745 | validation: 0.11131455965037546]
	TIME [epoch: 8.18 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1232542438498658		[learning rate: 0.0032837]
		[batch 20/20] avg loss: 0.09689290943083959		[learning rate: 0.0032798]
	Learning Rate: 0.00327976
	LOSS [training: 0.11007357664035271 | validation: 0.044447762584179436]
	TIME [epoch: 8.21 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05326950995111031		[learning rate: 0.0032758]
		[batch 20/20] avg loss: 0.08095648951804799		[learning rate: 0.0032718]
	Learning Rate: 0.00327182
	LOSS [training: 0.06711299973457915 | validation: 0.06371382107418293]
	TIME [epoch: 8.22 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057957801928866036		[learning rate: 0.0032679]
		[batch 20/20] avg loss: 0.041013616087813856		[learning rate: 0.0032639]
	Learning Rate: 0.0032639
	LOSS [training: 0.04948570900833995 | validation: 0.03312528797653705]
	TIME [epoch: 8.18 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059229548149587065		[learning rate: 0.0032599]
		[batch 20/20] avg loss: 0.07236526195510654		[learning rate: 0.003256]
	Learning Rate: 0.003256
	LOSS [training: 0.06579740505234681 | validation: 0.09392219810718691]
	TIME [epoch: 8.17 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07254531818931101		[learning rate: 0.0032521]
		[batch 20/20] avg loss: 0.06463193804167948		[learning rate: 0.0032481]
	Learning Rate: 0.00324812
	LOSS [training: 0.06858862811549525 | validation: 0.05041381582092738]
	TIME [epoch: 8.2 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06431467072252751		[learning rate: 0.0032442]
		[batch 20/20] avg loss: 0.058351311171918954		[learning rate: 0.0032403]
	Learning Rate: 0.00324025
	LOSS [training: 0.061332990947223234 | validation: 0.0465287992378187]
	TIME [epoch: 8.17 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04020689893280325		[learning rate: 0.0032363]
		[batch 20/20] avg loss: 0.058520084685607175		[learning rate: 0.0032324]
	Learning Rate: 0.00323241
	LOSS [training: 0.049363491809205205 | validation: 0.09578875047297197]
	TIME [epoch: 8.18 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059839995146688765		[learning rate: 0.0032285]
		[batch 20/20] avg loss: 0.06473463656165943		[learning rate: 0.0032246]
	Learning Rate: 0.00322458
	LOSS [training: 0.062287315854174095 | validation: 0.055505819347933916]
	TIME [epoch: 8.17 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06196093447519864		[learning rate: 0.0032207]
		[batch 20/20] avg loss: 0.045563011278943105		[learning rate: 0.0032168]
	Learning Rate: 0.00321678
	LOSS [training: 0.053761972877070886 | validation: 0.04673312981393362]
	TIME [epoch: 8.2 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03501420590171077		[learning rate: 0.0032129]
		[batch 20/20] avg loss: 0.06432548553945305		[learning rate: 0.003209]
	Learning Rate: 0.00320899
	LOSS [training: 0.04966984572058191 | validation: 0.08595553166012906]
	TIME [epoch: 8.18 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07243138829486838		[learning rate: 0.0032051]
		[batch 20/20] avg loss: 0.07871435046180288		[learning rate: 0.0032012]
	Learning Rate: 0.00320122
	LOSS [training: 0.0755728693783356 | validation: 0.09025207717795841]
	TIME [epoch: 8.17 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058744141348951076		[learning rate: 0.0031973]
		[batch 20/20] avg loss: 0.06727261366273896		[learning rate: 0.0031935]
	Learning Rate: 0.00319347
	LOSS [training: 0.06300837750584501 | validation: 0.06843866218197012]
	TIME [epoch: 8.17 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037010414127624035		[learning rate: 0.0031896]
		[batch 20/20] avg loss: 0.04734051926479026		[learning rate: 0.0031857]
	Learning Rate: 0.00318574
	LOSS [training: 0.04217546669620714 | validation: 0.1107159838103502]
	TIME [epoch: 8.19 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05688593479031827		[learning rate: 0.0031819]
		[batch 20/20] avg loss: 0.05914972906761533		[learning rate: 0.003178]
	Learning Rate: 0.00317803
	LOSS [training: 0.0580178319289668 | validation: 0.04588756298077514]
	TIME [epoch: 8.18 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05019191685261339		[learning rate: 0.0031742]
		[batch 20/20] avg loss: 0.054407518318800904		[learning rate: 0.0031703]
	Learning Rate: 0.00317034
	LOSS [training: 0.05229971758570714 | validation: 0.07386416044746043]
	TIME [epoch: 8.17 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06833177893020245		[learning rate: 0.0031665]
		[batch 20/20] avg loss: 0.036415047104205275		[learning rate: 0.0031627]
	Learning Rate: 0.00316266
	LOSS [training: 0.05237341301720387 | validation: 0.05367801678832697]
	TIME [epoch: 8.2 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035270469002046265		[learning rate: 0.0031588]
		[batch 20/20] avg loss: 0.0462372495627554		[learning rate: 0.003155]
	Learning Rate: 0.003155
	LOSS [training: 0.040753859282400835 | validation: 0.0445630398307668]
	TIME [epoch: 8.2 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04127044366224557		[learning rate: 0.0031512]
		[batch 20/20] avg loss: 0.04747388560373144		[learning rate: 0.0031474]
	Learning Rate: 0.00314737
	LOSS [training: 0.0443721646329885 | validation: 0.04060345976503256]
	TIME [epoch: 8.18 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04215062426415818		[learning rate: 0.0031436]
		[batch 20/20] avg loss: 0.0323006135845199		[learning rate: 0.0031397]
	Learning Rate: 0.00313975
	LOSS [training: 0.037225618924339024 | validation: 0.04240739039172903]
	TIME [epoch: 8.21 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03750744587503628		[learning rate: 0.0031359]
		[batch 20/20] avg loss: 0.029325241001308615		[learning rate: 0.0031321]
	Learning Rate: 0.00313215
	LOSS [training: 0.03341634343817245 | validation: 0.05072913233242589]
	TIME [epoch: 8.17 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030369534321613413		[learning rate: 0.0031284]
		[batch 20/20] avg loss: 0.05233729488906034		[learning rate: 0.0031246]
	Learning Rate: 0.00312456
	LOSS [training: 0.04135341460533686 | validation: 0.024820080634232126]
	TIME [epoch: 8.19 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05928190919919524		[learning rate: 0.0031208]
		[batch 20/20] avg loss: 0.0629857475385065		[learning rate: 0.003117]
	Learning Rate: 0.003117
	LOSS [training: 0.061133828368850875 | validation: 0.05768124822389171]
	TIME [epoch: 8.17 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054720527365809866		[learning rate: 0.0031132]
		[batch 20/20] avg loss: 0.03311725578092549		[learning rate: 0.0031095]
	Learning Rate: 0.00310945
	LOSS [training: 0.043918891573367674 | validation: 0.07425852161673048]
	TIME [epoch: 8.18 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03660842556976164		[learning rate: 0.0031057]
		[batch 20/20] avg loss: 0.03599469196737338		[learning rate: 0.0031019]
	Learning Rate: 0.00310193
	LOSS [training: 0.03630155876856751 | validation: 0.04219834697308283]
	TIME [epoch: 8.17 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07950634616430606		[learning rate: 0.0030982]
		[batch 20/20] avg loss: 0.048340349737397585		[learning rate: 0.0030944]
	Learning Rate: 0.00309442
	LOSS [training: 0.06392334795085183 | validation: 0.03364872145577483]
	TIME [epoch: 8.2 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040082520198562316		[learning rate: 0.0030907]
		[batch 20/20] avg loss: 0.04178369763069201		[learning rate: 0.0030869]
	Learning Rate: 0.00308693
	LOSS [training: 0.04093310891462717 | validation: 0.045337623257824616]
	TIME [epoch: 8.18 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04348830083133541		[learning rate: 0.0030832]
		[batch 20/20] avg loss: 0.08194356776114274		[learning rate: 0.0030795]
	Learning Rate: 0.00307945
	LOSS [training: 0.06271593429623908 | validation: 0.18954208540305167]
	TIME [epoch: 8.18 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07779271781437666		[learning rate: 0.0030757]
		[batch 20/20] avg loss: 0.03209571505019302		[learning rate: 0.003072]
	Learning Rate: 0.003072
	LOSS [training: 0.05494421643228483 | validation: 0.04018405402539209]
	TIME [epoch: 8.19 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06473886847392352		[learning rate: 0.0030683]
		[batch 20/20] avg loss: 0.02914672857006953		[learning rate: 0.0030646]
	Learning Rate: 0.00306456
	LOSS [training: 0.04694279852199652 | validation: 0.04094014137623503]
	TIME [epoch: 8.21 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05010321426321472		[learning rate: 0.0030609]
		[batch 20/20] avg loss: 0.04312586486409407		[learning rate: 0.0030571]
	Learning Rate: 0.00305714
	LOSS [training: 0.0466145395636544 | validation: 0.08527714078704962]
	TIME [epoch: 8.18 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036808180604921685		[learning rate: 0.0030534]
		[batch 20/20] avg loss: 0.04869870402796191		[learning rate: 0.0030497]
	Learning Rate: 0.00304974
	LOSS [training: 0.042753442316441806 | validation: 0.055555594364603236]
	TIME [epoch: 8.21 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047505289822495425		[learning rate: 0.003046]
		[batch 20/20] avg loss: 0.05843988318404817		[learning rate: 0.0030424]
	Learning Rate: 0.00304236
	LOSS [training: 0.0529725865032718 | validation: 0.12479792871445997]
	TIME [epoch: 8.18 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05925090275479211		[learning rate: 0.0030387]
		[batch 20/20] avg loss: 0.03709798304581444		[learning rate: 0.003035]
	Learning Rate: 0.00303499
	LOSS [training: 0.048174442900303274 | validation: 0.03858173048659383]
	TIME [epoch: 8.2 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05439644248020566		[learning rate: 0.0030313]
		[batch 20/20] avg loss: 0.030761302253491905		[learning rate: 0.0030276]
	Learning Rate: 0.00302765
	LOSS [training: 0.04257887236684878 | validation: 0.02803211703032145]
	TIME [epoch: 8.18 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02605596901878668		[learning rate: 0.003024]
		[batch 20/20] avg loss: 0.06625557832625915		[learning rate: 0.0030203]
	Learning Rate: 0.00302032
	LOSS [training: 0.04615577367252292 | validation: 0.03965682172283412]
	TIME [epoch: 8.17 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03927978566819675		[learning rate: 0.0030167]
		[batch 20/20] avg loss: 0.04420513316959844		[learning rate: 0.003013]
	Learning Rate: 0.00301301
	LOSS [training: 0.04174245941889761 | validation: 0.06335809479804604]
	TIME [epoch: 8.19 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08134264353346929		[learning rate: 0.0030094]
		[batch 20/20] avg loss: 0.04803391098852652		[learning rate: 0.0030057]
	Learning Rate: 0.00300571
	LOSS [training: 0.06468827726099789 | validation: 0.08684458587362655]
	TIME [epoch: 8.2 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1110359629214263		[learning rate: 0.0030021]
		[batch 20/20] avg loss: 0.09228505633279839		[learning rate: 0.0029984]
	Learning Rate: 0.00299844
	LOSS [training: 0.10166050962711234 | validation: 0.04756237306345411]
	TIME [epoch: 8.19 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05307051261012395		[learning rate: 0.0029948]
		[batch 20/20] avg loss: 0.055283771306556026		[learning rate: 0.0029912]
	Learning Rate: 0.00299118
	LOSS [training: 0.05417714195833999 | validation: 0.05218614258158212]
	TIME [epoch: 8.18 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03266043215966509		[learning rate: 0.0029876]
		[batch 20/20] avg loss: 0.06498374896805442		[learning rate: 0.0029839]
	Learning Rate: 0.00298394
	LOSS [training: 0.048822090563859756 | validation: 0.05204706512987879]
	TIME [epoch: 8.22 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07799663910192456		[learning rate: 0.0029803]
		[batch 20/20] avg loss: 0.03866158929118839		[learning rate: 0.0029767]
	Learning Rate: 0.00297671
	LOSS [training: 0.05832911419655649 | validation: 0.03145866116787121]
	TIME [epoch: 8.18 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034728697101875906		[learning rate: 0.0029731]
		[batch 20/20] avg loss: 0.04374468572685468		[learning rate: 0.0029695]
	Learning Rate: 0.00296951
	LOSS [training: 0.03923669141436529 | validation: 0.06490213671515284]
	TIME [epoch: 8.19 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04221610627558564		[learning rate: 0.0029659]
		[batch 20/20] avg loss: 0.03906611240859839		[learning rate: 0.0029623]
	Learning Rate: 0.00296232
	LOSS [training: 0.040641109342092016 | validation: 0.03587976228045023]
	TIME [epoch: 8.17 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03537283496754177		[learning rate: 0.0029587]
		[batch 20/20] avg loss: 0.0437857306649915		[learning rate: 0.0029551]
	Learning Rate: 0.00295515
	LOSS [training: 0.03957928281626663 | validation: 0.0399595288853773]
	TIME [epoch: 8.18 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061148848915856945		[learning rate: 0.0029516]
		[batch 20/20] avg loss: 0.08779694390727769		[learning rate: 0.002948]
	Learning Rate: 0.00294799
	LOSS [training: 0.07447289641156732 | validation: 0.14588599542853195]
	TIME [epoch: 8.19 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06607818707613311		[learning rate: 0.0029444]
		[batch 20/20] avg loss: 0.040575169827792176		[learning rate: 0.0029409]
	Learning Rate: 0.00294086
	LOSS [training: 0.053326678451962636 | validation: 0.02317717200872025]
	TIME [epoch: 8.2 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0356834638041211		[learning rate: 0.0029373]
		[batch 20/20] avg loss: 0.054307611347168536		[learning rate: 0.0029337]
	Learning Rate: 0.00293374
	LOSS [training: 0.044995537575644826 | validation: 0.03102890485646479]
	TIME [epoch: 8.18 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043275347977455914		[learning rate: 0.0029302]
		[batch 20/20] avg loss: 0.049631741449612396		[learning rate: 0.0029266]
	Learning Rate: 0.00292663
	LOSS [training: 0.04645354471353415 | validation: 0.03394178893781326]
	TIME [epoch: 8.22 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034255442385337384		[learning rate: 0.0029231]
		[batch 20/20] avg loss: 0.059504261731045796		[learning rate: 0.0029195]
	Learning Rate: 0.00291955
	LOSS [training: 0.0468798520581916 | validation: 0.04359536294169897]
	TIME [epoch: 8.17 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04092392584958349		[learning rate: 0.002916]
		[batch 20/20] avg loss: 0.0365029933619478		[learning rate: 0.0029125]
	Learning Rate: 0.00291248
	LOSS [training: 0.03871345960576565 | validation: 0.02076436536358448]
	TIME [epoch: 8.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03924123209951204		[learning rate: 0.002909]
		[batch 20/20] avg loss: 0.04386180794277296		[learning rate: 0.0029054]
	Learning Rate: 0.00290543
	LOSS [training: 0.0415515200211425 | validation: 0.03401715760437832]
	TIME [epoch: 8.17 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05889842983816061		[learning rate: 0.0029019]
		[batch 20/20] avg loss: 0.04095105278153079		[learning rate: 0.0028984]
	Learning Rate: 0.0028984
	LOSS [training: 0.0499247413098457 | validation: 0.04102883382095125]
	TIME [epoch: 8.18 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04611633141186096		[learning rate: 0.0028949]
		[batch 20/20] avg loss: 0.04639539330375267		[learning rate: 0.0028914]
	Learning Rate: 0.00289138
	LOSS [training: 0.046255862357806814 | validation: 0.05783911273734889]
	TIME [epoch: 8.17 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04970659673156941		[learning rate: 0.0028879]
		[batch 20/20] avg loss: 0.049997813276879494		[learning rate: 0.0028844]
	Learning Rate: 0.00288438
	LOSS [training: 0.04985220500422444 | validation: 0.07292245173528268]
	TIME [epoch: 8.21 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03618357532528682		[learning rate: 0.0028809]
		[batch 20/20] avg loss: 0.04356097086572248		[learning rate: 0.0028774]
	Learning Rate: 0.0028774
	LOSS [training: 0.03987227309550465 | validation: 0.0580082860949214]
	TIME [epoch: 8.2 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034800133018355786		[learning rate: 0.0028739]
		[batch 20/20] avg loss: 0.024364854454891165		[learning rate: 0.0028704]
	Learning Rate: 0.00287043
	LOSS [training: 0.029582493736623465 | validation: 0.06297122346286008]
	TIME [epoch: 8.19 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042288377936953676		[learning rate: 0.002867]
		[batch 20/20] avg loss: 0.07735461268025816		[learning rate: 0.0028635]
	Learning Rate: 0.00286348
	LOSS [training: 0.05982149530860592 | validation: 0.07767442952698578]
	TIME [epoch: 8.18 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057883792062860276		[learning rate: 0.00286]
		[batch 20/20] avg loss: 0.059573367179235934		[learning rate: 0.0028566]
	Learning Rate: 0.00285655
	LOSS [training: 0.05872857962104812 | validation: 0.04296240256782834]
	TIME [epoch: 8.25 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07149175837324634		[learning rate: 0.0028531]
		[batch 20/20] avg loss: 0.07131429576638768		[learning rate: 0.0028496]
	Learning Rate: 0.00284964
	LOSS [training: 0.071403027069817 | validation: 0.06989459642983248]
	TIME [epoch: 8.17 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03894716081614607		[learning rate: 0.0028462]
		[batch 20/20] avg loss: 0.04236501211220422		[learning rate: 0.0028427]
	Learning Rate: 0.00284274
	LOSS [training: 0.040656086464175144 | validation: 0.0906904154790666]
	TIME [epoch: 8.18 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046391163162515334		[learning rate: 0.0028393]
		[batch 20/20] avg loss: 0.02950903370383336		[learning rate: 0.0028359]
	Learning Rate: 0.00283586
	LOSS [training: 0.037950098433174345 | validation: 0.04106245181395738]
	TIME [epoch: 8.18 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03953142739359611		[learning rate: 0.0028324]
		[batch 20/20] avg loss: 0.03270139901915464		[learning rate: 0.002829]
	Learning Rate: 0.00282899
	LOSS [training: 0.03611641320637537 | validation: 0.07378349051081043]
	TIME [epoch: 8.2 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050788523593601656		[learning rate: 0.0028256]
		[batch 20/20] avg loss: 0.029717548255924143		[learning rate: 0.0028221]
	Learning Rate: 0.00282214
	LOSS [training: 0.04025303592476289 | validation: 0.05000852238023726]
	TIME [epoch: 8.18 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031960636746381006		[learning rate: 0.0028187]
		[batch 20/20] avg loss: 0.03892168758234195		[learning rate: 0.0028153]
	Learning Rate: 0.00281531
	LOSS [training: 0.03544116216436148 | validation: 0.022494410642601468]
	TIME [epoch: 8.19 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031799018801934635		[learning rate: 0.0028119]
		[batch 20/20] avg loss: 0.03955862976954062		[learning rate: 0.0028085]
	Learning Rate: 0.00280849
	LOSS [training: 0.03567882428573763 | validation: 0.03602659415149627]
	TIME [epoch: 8.18 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0441824545959501		[learning rate: 0.0028051]
		[batch 20/20] avg loss: 0.02532936046184193		[learning rate: 0.0028017]
	Learning Rate: 0.0028017
	LOSS [training: 0.03475590752889601 | validation: 0.025428201729183704]
	TIME [epoch: 8.21 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04148038911318493		[learning rate: 0.0027983]
		[batch 20/20] avg loss: 0.040932479422639154		[learning rate: 0.0027949]
	Learning Rate: 0.00279491
	LOSS [training: 0.04120643426791204 | validation: 0.06344113988624911]
	TIME [epoch: 8.22 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04920617411293858		[learning rate: 0.0027915]
		[batch 20/20] avg loss: 0.037826515313883806		[learning rate: 0.0027881]
	Learning Rate: 0.00278815
	LOSS [training: 0.04351634471341118 | validation: 0.022524470606215322]
	TIME [epoch: 8.17 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029727313324140154		[learning rate: 0.0027848]
		[batch 20/20] avg loss: 0.04905264661174211		[learning rate: 0.0027814]
	Learning Rate: 0.0027814
	LOSS [training: 0.039389979967941136 | validation: 0.0408105471139863]
	TIME [epoch: 8.18 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02301753176560505		[learning rate: 0.002778]
		[batch 20/20] avg loss: 0.05718349824395373		[learning rate: 0.0027747]
	Learning Rate: 0.00277466
	LOSS [training: 0.040100515004779395 | validation: 0.15243176493849625]
	TIME [epoch: 8.19 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07355668804260994		[learning rate: 0.0027713]
		[batch 20/20] avg loss: 0.05854477691619817		[learning rate: 0.0027679]
	Learning Rate: 0.00276795
	LOSS [training: 0.06605073247940406 | validation: 0.06934109863363842]
	TIME [epoch: 8.18 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05637667391283903		[learning rate: 0.0027646]
		[batch 20/20] avg loss: 0.04460778531147421		[learning rate: 0.0027612]
	Learning Rate: 0.00276125
	LOSS [training: 0.050492229612156624 | validation: 0.047696841768221976]
	TIME [epoch: 8.18 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04284037120535159		[learning rate: 0.0027579]
		[batch 20/20] avg loss: 0.05332278878103773		[learning rate: 0.0027546]
	Learning Rate: 0.00275456
	LOSS [training: 0.048081579993194665 | validation: 0.043760422658711766]
	TIME [epoch: 8.2 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0451507609647407		[learning rate: 0.0027512]
		[batch 20/20] avg loss: 0.06190967842905136		[learning rate: 0.0027479]
	Learning Rate: 0.00274789
	LOSS [training: 0.05353021969689603 | validation: 0.05363208649043444]
	TIME [epoch: 8.2 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0453184077363933		[learning rate: 0.0027446]
		[batch 20/20] avg loss: 0.06943425578718426		[learning rate: 0.0027412]
	Learning Rate: 0.00274124
	LOSS [training: 0.05737633176178877 | validation: 0.05988449743928698]
	TIME [epoch: 8.19 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035964620913938664		[learning rate: 0.0027379]
		[batch 20/20] avg loss: 0.0359348611827527		[learning rate: 0.0027346]
	Learning Rate: 0.00273461
	LOSS [training: 0.03594974104834568 | validation: 0.04326489377272635]
	TIME [epoch: 8.22 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04509662574348744		[learning rate: 0.0027313]
		[batch 20/20] avg loss: 0.05459660692993369		[learning rate: 0.002728]
	Learning Rate: 0.00272799
	LOSS [training: 0.049846616336710566 | validation: 0.04147356731992394]
	TIME [epoch: 8.18 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03709362540433009		[learning rate: 0.0027247]
		[batch 20/20] avg loss: 0.023512453467864253		[learning rate: 0.0027214]
	Learning Rate: 0.00272138
	LOSS [training: 0.030303039436097162 | validation: 0.051714096299320465]
	TIME [epoch: 8.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04402220792512325		[learning rate: 0.0027181]
		[batch 20/20] avg loss: 0.04340934405963874		[learning rate: 0.0027148]
	Learning Rate: 0.00271479
	LOSS [training: 0.04371577599238099 | validation: 0.022770214039013303]
	TIME [epoch: 8.18 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039825878793476696		[learning rate: 0.0027115]
		[batch 20/20] avg loss: 0.03767976380637974		[learning rate: 0.0027082]
	Learning Rate: 0.00270822
	LOSS [training: 0.03875282129992822 | validation: 0.014365188185209373]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_639.pth
	Model improved!!!
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03307678798335566		[learning rate: 0.0027049]
		[batch 20/20] avg loss: 0.022494714556581558		[learning rate: 0.0027017]
	Learning Rate: 0.00270167
	LOSS [training: 0.02778575126996861 | validation: 0.043191169279545895]
	TIME [epoch: 8.17 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051139676024286394		[learning rate: 0.0026984]
		[batch 20/20] avg loss: 0.049008029253813055		[learning rate: 0.0026951]
	Learning Rate: 0.00269513
	LOSS [training: 0.05007385263904972 | validation: 0.03711232887110786]
	TIME [epoch: 8.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04891314556859001		[learning rate: 0.0026919]
		[batch 20/20] avg loss: 0.03871911153189392		[learning rate: 0.0026886]
	Learning Rate: 0.0026886
	LOSS [training: 0.04381612855024196 | validation: 0.06147290768844622]
	TIME [epoch: 8.17 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04066674571596869		[learning rate: 0.0026853]
		[batch 20/20] avg loss: 0.03634799317002267		[learning rate: 0.0026821]
	Learning Rate: 0.00268209
	LOSS [training: 0.03850736944299568 | validation: 0.02216980138983926]
	TIME [epoch: 8.18 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036649494492311044		[learning rate: 0.0026788]
		[batch 20/20] avg loss: 0.056357049535858794		[learning rate: 0.0026756]
	Learning Rate: 0.0026756
	LOSS [training: 0.04650327201408492 | validation: 0.05602297342115042]
	TIME [epoch: 8.18 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042803916914761424		[learning rate: 0.0026724]
		[batch 20/20] avg loss: 0.03442927296710914		[learning rate: 0.0026691]
	Learning Rate: 0.00266912
	LOSS [training: 0.038616594940935285 | validation: 0.0403156818906306]
	TIME [epoch: 8.19 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05164945061467272		[learning rate: 0.0026659]
		[batch 20/20] avg loss: 0.05715701218068259		[learning rate: 0.0026627]
	Learning Rate: 0.00266266
	LOSS [training: 0.05440323139767765 | validation: 0.08911207209926453]
	TIME [epoch: 8.2 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05485938649567233		[learning rate: 0.0026594]
		[batch 20/20] avg loss: 0.06164713436407654		[learning rate: 0.0026562]
	Learning Rate: 0.00265621
	LOSS [training: 0.05825326042987442 | validation: 0.0748137919520206]
	TIME [epoch: 8.19 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060453417052968375		[learning rate: 0.002653]
		[batch 20/20] avg loss: 0.02402879298456177		[learning rate: 0.0026498]
	Learning Rate: 0.00264978
	LOSS [training: 0.042241105018765074 | validation: 0.02795977124052513]
	TIME [epoch: 8.17 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060835798444007584		[learning rate: 0.0026466]
		[batch 20/20] avg loss: 0.07260390086006616		[learning rate: 0.0026434]
	Learning Rate: 0.00264337
	LOSS [training: 0.06671984965203687 | validation: 0.028952165634058967]
	TIME [epoch: 8.19 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029711585112858385		[learning rate: 0.0026402]
		[batch 20/20] avg loss: 0.03839937573448561		[learning rate: 0.002637]
	Learning Rate: 0.00263697
	LOSS [training: 0.034055480423672 | validation: 0.023969788821873683]
	TIME [epoch: 8.17 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041545414483529795		[learning rate: 0.0026338]
		[batch 20/20] avg loss: 0.028112412347120575		[learning rate: 0.0026306]
	Learning Rate: 0.00263059
	LOSS [training: 0.034828913415325194 | validation: 0.047551504420931534]
	TIME [epoch: 8.17 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030458964249179647		[learning rate: 0.0026274]
		[batch 20/20] avg loss: 0.03391441223715874		[learning rate: 0.0026242]
	Learning Rate: 0.00262422
	LOSS [training: 0.0321866882431692 | validation: 0.026394555349802454]
	TIME [epoch: 8.19 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03235293534146919		[learning rate: 0.002621]
		[batch 20/20] avg loss: 0.04661234617801921		[learning rate: 0.0026179]
	Learning Rate: 0.00261787
	LOSS [training: 0.0394826407597442 | validation: 0.0399122288714296]
	TIME [epoch: 8.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0371770656152343		[learning rate: 0.0026147]
		[batch 20/20] avg loss: 0.05174937030486252		[learning rate: 0.0026115]
	Learning Rate: 0.00261153
	LOSS [training: 0.044463217960048415 | validation: 0.031063041944072847]
	TIME [epoch: 8.18 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033893926006758265		[learning rate: 0.0026084]
		[batch 20/20] avg loss: 0.03551781930789188		[learning rate: 0.0026052]
	Learning Rate: 0.00260521
	LOSS [training: 0.03470587265732507 | validation: 0.030173398933462357]
	TIME [epoch: 8.19 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035254074861763365		[learning rate: 0.0026021]
		[batch 20/20] avg loss: 0.041648397001549146		[learning rate: 0.0025989]
	Learning Rate: 0.0025989
	LOSS [training: 0.03845123593165625 | validation: 0.014164657297359842]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_656.pth
	Model improved!!!
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02719140979779606		[learning rate: 0.0025958]
		[batch 20/20] avg loss: 0.036927641469644334		[learning rate: 0.0025926]
	Learning Rate: 0.00259261
	LOSS [training: 0.0320595256337202 | validation: 0.026817828445128748]
	TIME [epoch: 8.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038309553857294926		[learning rate: 0.0025895]
		[batch 20/20] avg loss: 0.04202899808721618		[learning rate: 0.0025863]
	Learning Rate: 0.00258633
	LOSS [training: 0.04016927597225555 | validation: 0.021944973054200425]
	TIME [epoch: 8.17 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030003602080727505		[learning rate: 0.0025832]
		[batch 20/20] avg loss: 0.06371736752903304		[learning rate: 0.0025801]
	Learning Rate: 0.00258007
	LOSS [training: 0.046860484804880254 | validation: 0.06321296807991783]
	TIME [epoch: 8.17 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06958390409376405		[learning rate: 0.0025769]
		[batch 20/20] avg loss: 0.05265924208883738		[learning rate: 0.0025738]
	Learning Rate: 0.00257382
	LOSS [training: 0.06112157309130071 | validation: 0.03974438627028009]
	TIME [epoch: 8.16 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0533571455472774		[learning rate: 0.0025707]
		[batch 20/20] avg loss: 0.04306075808268548		[learning rate: 0.0025676]
	Learning Rate: 0.00256759
	LOSS [training: 0.04820895181498145 | validation: 0.03760802590787297]
	TIME [epoch: 8.19 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040489557089279316		[learning rate: 0.0025645]
		[batch 20/20] avg loss: 0.02962481026406132		[learning rate: 0.0025614]
	Learning Rate: 0.00256138
	LOSS [training: 0.03505718367667032 | validation: 0.031201193509818335]
	TIME [epoch: 8.17 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04196881430538456		[learning rate: 0.0025583]
		[batch 20/20] avg loss: 0.023723250742324654		[learning rate: 0.0025552]
	Learning Rate: 0.00255518
	LOSS [training: 0.03284603252385461 | validation: 0.039594021012410545]
	TIME [epoch: 8.17 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0476424174771054		[learning rate: 0.0025521]
		[batch 20/20] avg loss: 0.045186240598394344		[learning rate: 0.002549]
	Learning Rate: 0.00254899
	LOSS [training: 0.04641432903774988 | validation: 0.0511674757342699]
	TIME [epoch: 8.19 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034601237993552615		[learning rate: 0.0025459]
		[batch 20/20] avg loss: 0.04065086110917516		[learning rate: 0.0025428]
	Learning Rate: 0.00254282
	LOSS [training: 0.03762604955136388 | validation: 0.018045645668430355]
	TIME [epoch: 8.2 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03261027946067557		[learning rate: 0.0025397]
		[batch 20/20] avg loss: 0.04276809110786978		[learning rate: 0.0025367]
	Learning Rate: 0.00253667
	LOSS [training: 0.037689185284272675 | validation: 0.05711927075362404]
	TIME [epoch: 8.18 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033403756960837784		[learning rate: 0.0025336]
		[batch 20/20] avg loss: 0.04583731502297253		[learning rate: 0.0025305]
	Learning Rate: 0.00253052
	LOSS [training: 0.03962053599190515 | validation: 0.03950930056673999]
	TIME [epoch: 8.21 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045557741567092795		[learning rate: 0.0025275]
		[batch 20/20] avg loss: 0.040321747222847634		[learning rate: 0.0025244]
	Learning Rate: 0.0025244
	LOSS [training: 0.042939744394970225 | validation: 0.0784563878849533]
	TIME [epoch: 8.17 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04828318928788834		[learning rate: 0.0025213]
		[batch 20/20] avg loss: 0.035331669770183885		[learning rate: 0.0025183]
	Learning Rate: 0.00251829
	LOSS [training: 0.04180742952903611 | validation: 0.03319435902273886]
	TIME [epoch: 8.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03647647711374607		[learning rate: 0.0025152]
		[batch 20/20] avg loss: 0.03228218841075256		[learning rate: 0.0025122]
	Learning Rate: 0.00251219
	LOSS [training: 0.03437933276224931 | validation: 0.06221420790409567]
	TIME [epoch: 8.18 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04510792363806137		[learning rate: 0.0025091]
		[batch 20/20] avg loss: 0.05091800350909361		[learning rate: 0.0025061]
	Learning Rate: 0.00250611
	LOSS [training: 0.0480129635735775 | validation: 0.03320209906900126]
	TIME [epoch: 8.17 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04053905547524274		[learning rate: 0.0025031]
		[batch 20/20] avg loss: 0.04666480764092541		[learning rate: 0.0025]
	Learning Rate: 0.00250004
	LOSS [training: 0.043601931558084076 | validation: 0.0546526533335855]
	TIME [epoch: 8.19 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059001850992657034		[learning rate: 0.002497]
		[batch 20/20] avg loss: 0.052653891616058233		[learning rate: 0.002494]
	Learning Rate: 0.00249399
	LOSS [training: 0.055827871304357644 | validation: 0.04176214987383521]
	TIME [epoch: 8.21 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04348239742776479		[learning rate: 0.002491]
		[batch 20/20] avg loss: 0.03311440447699237		[learning rate: 0.002488]
	Learning Rate: 0.00248795
	LOSS [training: 0.03829840095237858 | validation: 0.02976328953036439]
	TIME [epoch: 8.18 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0298732168508844		[learning rate: 0.0024849]
		[batch 20/20] avg loss: 0.04661886848955898		[learning rate: 0.0024819]
	Learning Rate: 0.00248193
	LOSS [training: 0.03824604267022169 | validation: 0.05150690778881874]
	TIME [epoch: 8.17 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05252697240260492		[learning rate: 0.0024789]
		[batch 20/20] avg loss: 0.04474490918232825		[learning rate: 0.0024759]
	Learning Rate: 0.00247592
	LOSS [training: 0.04863594079246659 | validation: 0.05109204792575812]
	TIME [epoch: 8.21 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057647492384104125		[learning rate: 0.0024729]
		[batch 20/20] avg loss: 0.037780697968080444		[learning rate: 0.0024699]
	Learning Rate: 0.00246993
	LOSS [training: 0.047714095176092285 | validation: 0.026089936061623166]
	TIME [epoch: 8.19 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07093149650831294		[learning rate: 0.0024669]
		[batch 20/20] avg loss: 0.08713267067377187		[learning rate: 0.0024639]
	Learning Rate: 0.00246395
	LOSS [training: 0.0790320835910424 | validation: 0.09108423253985022]
	TIME [epoch: 8.19 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0757883389110559		[learning rate: 0.002461]
		[batch 20/20] avg loss: 0.04452969275155327		[learning rate: 0.002458]
	Learning Rate: 0.00245798
	LOSS [training: 0.060159015831304584 | validation: 0.03963341265367982]
	TIME [epoch: 8.17 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04829054308349645		[learning rate: 0.002455]
		[batch 20/20] avg loss: 0.04811221657802742		[learning rate: 0.002452]
	Learning Rate: 0.00245203
	LOSS [training: 0.048201379830761934 | validation: 0.044180001512506256]
	TIME [epoch: 8.19 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07877641156181127		[learning rate: 0.0024491]
		[batch 20/20] avg loss: 0.04214528949989386		[learning rate: 0.0024461]
	Learning Rate: 0.0024461
	LOSS [training: 0.06046085053085255 | validation: 0.03027894653113496]
	TIME [epoch: 8.19 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043490009696111456		[learning rate: 0.0024431]
		[batch 20/20] avg loss: 0.05938200045261903		[learning rate: 0.0024402]
	Learning Rate: 0.00244018
	LOSS [training: 0.05143600507436524 | validation: 0.04154974297531566]
	TIME [epoch: 8.19 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09213501153066954		[learning rate: 0.0024372]
		[batch 20/20] avg loss: 0.07724013021938425		[learning rate: 0.0024343]
	Learning Rate: 0.00243427
	LOSS [training: 0.0846875708750269 | validation: 0.052840986860731305]
	TIME [epoch: 8.19 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05519505628045971		[learning rate: 0.0024313]
		[batch 20/20] avg loss: 0.028184683743613863		[learning rate: 0.0024284]
	Learning Rate: 0.00242837
	LOSS [training: 0.04168987001203679 | validation: 0.039505801869277914]
	TIME [epoch: 8.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033958495631419845		[learning rate: 0.0024254]
		[batch 20/20] avg loss: 0.01870260989026231		[learning rate: 0.0024225]
	Learning Rate: 0.0024225
	LOSS [training: 0.026330552760841074 | validation: 0.029639980778792662]
	TIME [epoch: 8.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0237143616252895		[learning rate: 0.0024196]
		[batch 20/20] avg loss: 0.030791228923063223		[learning rate: 0.0024166]
	Learning Rate: 0.00241663
	LOSS [training: 0.027252795274176372 | validation: 0.06273839609244296]
	TIME [epoch: 8.19 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06173080316309263		[learning rate: 0.0024137]
		[batch 20/20] avg loss: 0.03410321713468775		[learning rate: 0.0024108]
	Learning Rate: 0.00241078
	LOSS [training: 0.047917010148890195 | validation: 0.03983486549356021]
	TIME [epoch: 8.17 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055354199860384354		[learning rate: 0.0024079]
		[batch 20/20] avg loss: 0.03416985498300842		[learning rate: 0.0024049]
	Learning Rate: 0.00240495
	LOSS [training: 0.04476202742169638 | validation: 0.01714605964663914]
	TIME [epoch: 8.17 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01970001938605786		[learning rate: 0.002402]
		[batch 20/20] avg loss: 0.02670727075483963		[learning rate: 0.0023991]
	Learning Rate: 0.00239912
	LOSS [training: 0.02320364507044875 | validation: 0.06836581631405286]
	TIME [epoch: 8.17 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07854469441788686		[learning rate: 0.0023962]
		[batch 20/20] avg loss: 0.040534846502378694		[learning rate: 0.0023933]
	Learning Rate: 0.00239332
	LOSS [training: 0.05953977046013277 | validation: 0.012215822528286468]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_690.pth
	Model improved!!!
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027740869283063418		[learning rate: 0.0023904]
		[batch 20/20] avg loss: 0.033596870131357606		[learning rate: 0.0023875]
	Learning Rate: 0.00238752
	LOSS [training: 0.030668869707210517 | validation: 0.04130182064803706]
	TIME [epoch: 8.19 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024423682140076636		[learning rate: 0.0023846]
		[batch 20/20] avg loss: 0.027659960915835148		[learning rate: 0.0023817]
	Learning Rate: 0.00238174
	LOSS [training: 0.026041821527955887 | validation: 0.008454857537142035]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_692.pth
	Model improved!!!
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013632296350760667		[learning rate: 0.0023789]
		[batch 20/20] avg loss: 0.033256804081256364		[learning rate: 0.002376]
	Learning Rate: 0.00237598
	LOSS [training: 0.023444550216008518 | validation: 0.03404590532776588]
	TIME [epoch: 8.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04186630002647027		[learning rate: 0.0023731]
		[batch 20/20] avg loss: 0.026106635032246573		[learning rate: 0.0023702]
	Learning Rate: 0.00237022
	LOSS [training: 0.033986467529358425 | validation: 0.02170812383488014]
	TIME [epoch: 8.26 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030210938582936726		[learning rate: 0.0023674]
		[batch 20/20] avg loss: 0.03670821973323338		[learning rate: 0.0023645]
	Learning Rate: 0.00236449
	LOSS [training: 0.03345957915808505 | validation: 0.011196540174312583]
	TIME [epoch: 8.19 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028590492376588804		[learning rate: 0.0023616]
		[batch 20/20] avg loss: 0.024677583848497976		[learning rate: 0.0023588]
	Learning Rate: 0.00235876
	LOSS [training: 0.02663403811254339 | validation: 0.03159859616452327]
	TIME [epoch: 8.18 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034146553856844335		[learning rate: 0.0023559]
		[batch 20/20] avg loss: 0.02030550372631946		[learning rate: 0.0023531]
	Learning Rate: 0.00235305
	LOSS [training: 0.027226028791581907 | validation: 0.03324374683278254]
	TIME [epoch: 8.19 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03740291305804615		[learning rate: 0.0023502]
		[batch 20/20] avg loss: 0.023818703090537918		[learning rate: 0.0023474]
	Learning Rate: 0.00234736
	LOSS [training: 0.030610808074292045 | validation: 0.030289204197073982]
	TIME [epoch: 8.23 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05087080250729472		[learning rate: 0.0023445]
		[batch 20/20] avg loss: 0.024551289636616257		[learning rate: 0.0023417]
	Learning Rate: 0.00234167
	LOSS [training: 0.03771104607195548 | validation: 0.03709539762777585]
	TIME [epoch: 8.21 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052449700845428715		[learning rate: 0.0023388]
		[batch 20/20] avg loss: 0.030066902169563776		[learning rate: 0.002336]
	Learning Rate: 0.002336
	LOSS [training: 0.04125830150749624 | validation: 0.032274734002650324]
	TIME [epoch: 8.19 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07021621362944788		[learning rate: 0.0023332]
		[batch 20/20] avg loss: 0.09445764585006518		[learning rate: 0.0023303]
	Learning Rate: 0.00233035
	LOSS [training: 0.08233692973975652 | validation: 0.066709961057867]
	TIME [epoch: 8.19 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06809100666440436		[learning rate: 0.0023275]
		[batch 20/20] avg loss: 0.03648305372671847		[learning rate: 0.0023247]
	Learning Rate: 0.00232471
	LOSS [training: 0.05228703019556141 | validation: 0.025172664775635876]
	TIME [epoch: 8.26 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03140432104454326		[learning rate: 0.0023219]
		[batch 20/20] avg loss: 0.03532452049681094		[learning rate: 0.0023191]
	Learning Rate: 0.00231908
	LOSS [training: 0.0333644207706771 | validation: 0.01929031394183736]
	TIME [epoch: 8.19 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03078122088299009		[learning rate: 0.0023163]
		[batch 20/20] avg loss: 0.029401211523328297		[learning rate: 0.0023135]
	Learning Rate: 0.00231347
	LOSS [training: 0.030091216203159188 | validation: 0.06546924674125895]
	TIME [epoch: 8.19 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0521330545128754		[learning rate: 0.0023107]
		[batch 20/20] avg loss: 0.0345689054371451		[learning rate: 0.0023079]
	Learning Rate: 0.00230787
	LOSS [training: 0.04335097997501026 | validation: 0.027413837455882654]
	TIME [epoch: 8.19 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028665265667558254		[learning rate: 0.0023051]
		[batch 20/20] avg loss: 0.06291797088526363		[learning rate: 0.0023023]
	Learning Rate: 0.00230228
	LOSS [training: 0.04579161827641094 | validation: 0.04167069022713756]
	TIME [epoch: 8.22 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030886427364832537		[learning rate: 0.0022995]
		[batch 20/20] avg loss: 0.029947702090111133		[learning rate: 0.0022967]
	Learning Rate: 0.00229671
	LOSS [training: 0.030417064727471833 | validation: 0.040025112271946245]
	TIME [epoch: 8.21 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045370771654121705		[learning rate: 0.0022939]
		[batch 20/20] avg loss: 0.02535842393471178		[learning rate: 0.0022911]
	Learning Rate: 0.00229115
	LOSS [training: 0.03536459779441674 | validation: 0.04092150901158351]
	TIME [epoch: 8.2 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044611261467575436		[learning rate: 0.0022884]
		[batch 20/20] avg loss: 0.04898963716794938		[learning rate: 0.0022856]
	Learning Rate: 0.0022856
	LOSS [training: 0.04680044931776242 | validation: 0.028733774826111638]
	TIME [epoch: 8.19 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03432210241740578		[learning rate: 0.0022828]
		[batch 20/20] avg loss: 0.025569437086185514		[learning rate: 0.0022801]
	Learning Rate: 0.00228007
	LOSS [training: 0.029945769751795654 | validation: 0.03116893123671092]
	TIME [epoch: 8.26 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03251472741102836		[learning rate: 0.0022773]
		[batch 20/20] avg loss: 0.029405545070152445		[learning rate: 0.0022745]
	Learning Rate: 0.00227455
	LOSS [training: 0.030960136240590398 | validation: 0.01771713475420758]
	TIME [epoch: 8.18 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027943826046577823		[learning rate: 0.0022718]
		[batch 20/20] avg loss: 0.03962289008203662		[learning rate: 0.002269]
	Learning Rate: 0.00226904
	LOSS [training: 0.03378335806430723 | validation: 0.05302424376554811]
	TIME [epoch: 8.19 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02828476743494839		[learning rate: 0.0022663]
		[batch 20/20] avg loss: 0.025644947665657246		[learning rate: 0.0022635]
	Learning Rate: 0.00226355
	LOSS [training: 0.026964857550302812 | validation: 0.018777576195441403]
	TIME [epoch: 8.19 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030894103397667683		[learning rate: 0.0022608]
		[batch 20/20] avg loss: 0.023003405945324087		[learning rate: 0.0022581]
	Learning Rate: 0.00225807
	LOSS [training: 0.02694875467149589 | validation: 0.03578985724172223]
	TIME [epoch: 8.21 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03301382869068536		[learning rate: 0.0022553]
		[batch 20/20] avg loss: 0.045620537795518425		[learning rate: 0.0022526]
	Learning Rate: 0.0022526
	LOSS [training: 0.039317183243101884 | validation: 0.08333606935285184]
	TIME [epoch: 8.2 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02920051090612084		[learning rate: 0.0022499]
		[batch 20/20] avg loss: 0.017030901693755634		[learning rate: 0.0022471]
	Learning Rate: 0.00224715
	LOSS [training: 0.023115706299938234 | validation: 0.013786348240488988]
	TIME [epoch: 8.21 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036280305944115854		[learning rate: 0.0022444]
		[batch 20/20] avg loss: 0.033641406817293946		[learning rate: 0.0022417]
	Learning Rate: 0.00224171
	LOSS [training: 0.034960856380704904 | validation: 0.0675068974369311]
	TIME [epoch: 8.19 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04576101037040581		[learning rate: 0.002239]
		[batch 20/20] avg loss: 0.039334096772376495		[learning rate: 0.0022363]
	Learning Rate: 0.00223628
	LOSS [training: 0.042547553571391165 | validation: 0.04066085288847823]
	TIME [epoch: 8.22 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021131211293520375		[learning rate: 0.0022336]
		[batch 20/20] avg loss: 0.016757182684914166		[learning rate: 0.0022309]
	Learning Rate: 0.00223087
	LOSS [training: 0.018944196989217267 | validation: 0.02097578920864253]
	TIME [epoch: 8.23 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01982322778423467		[learning rate: 0.0022282]
		[batch 20/20] avg loss: 0.02446317342439565		[learning rate: 0.0022255]
	Learning Rate: 0.00222547
	LOSS [training: 0.022143200604315155 | validation: 0.029062160544036132]
	TIME [epoch: 8.19 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02446412417098499		[learning rate: 0.0022228]
		[batch 20/20] avg loss: 0.026536776226597288		[learning rate: 0.0022201]
	Learning Rate: 0.00222008
	LOSS [training: 0.02550045019879114 | validation: 0.030635102304572958]
	TIME [epoch: 8.19 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021978536130308746		[learning rate: 0.0022174]
		[batch 20/20] avg loss: 0.019439229437505434		[learning rate: 0.0022147]
	Learning Rate: 0.0022147
	LOSS [training: 0.02070888278390709 | validation: 0.025226486653661694]
	TIME [epoch: 8.21 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06209915037460436		[learning rate: 0.002212]
		[batch 20/20] avg loss: 0.03411814572931339		[learning rate: 0.0022093]
	Learning Rate: 0.00220934
	LOSS [training: 0.04810864805195887 | validation: 0.034887819067865845]
	TIME [epoch: 8.19 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03646686421705671		[learning rate: 0.0022067]
		[batch 20/20] avg loss: 0.02918423788914744		[learning rate: 0.002204]
	Learning Rate: 0.00220399
	LOSS [training: 0.03282555105310207 | validation: 0.024443699071568285]
	TIME [epoch: 8.21 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023432357472870678		[learning rate: 0.0022013]
		[batch 20/20] avg loss: 0.028066789054318635		[learning rate: 0.0021987]
	Learning Rate: 0.00219866
	LOSS [training: 0.025749573263594654 | validation: 0.020484797593066734]
	TIME [epoch: 8.19 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023012149227355394		[learning rate: 0.002196]
		[batch 20/20] avg loss: 0.030297306266637798		[learning rate: 0.0021933]
	Learning Rate: 0.00219334
	LOSS [training: 0.026654727746996593 | validation: 0.03848201419241189]
	TIME [epoch: 8.22 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02708326078032721		[learning rate: 0.0021907]
		[batch 20/20] avg loss: 0.0570788485179314		[learning rate: 0.002188]
	Learning Rate: 0.00218803
	LOSS [training: 0.042081054649129306 | validation: 0.022949906978268264]
	TIME [epoch: 8.23 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024398970706642997		[learning rate: 0.0021854]
		[batch 20/20] avg loss: 0.03355832636768604		[learning rate: 0.0021827]
	Learning Rate: 0.00218273
	LOSS [training: 0.02897864853716452 | validation: 0.02766187554659076]
	TIME [epoch: 8.2 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02637314863316162		[learning rate: 0.0021801]
		[batch 20/20] avg loss: 0.030669843717421753		[learning rate: 0.0021774]
	Learning Rate: 0.00217745
	LOSS [training: 0.028521496175291688 | validation: 0.023143003553754263]
	TIME [epoch: 8.18 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022469744362802142		[learning rate: 0.0021748]
		[batch 20/20] avg loss: 0.01778792889418583		[learning rate: 0.0021722]
	Learning Rate: 0.00217217
	LOSS [training: 0.020128836628493983 | validation: 0.012893852105302984]
	TIME [epoch: 8.21 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0197164236803246		[learning rate: 0.0021695]
		[batch 20/20] avg loss: 0.024704930267085486		[learning rate: 0.0021669]
	Learning Rate: 0.00216692
	LOSS [training: 0.022210676973705042 | validation: 0.014428319858794026]
	TIME [epoch: 8.19 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024567611870230262		[learning rate: 0.0021643]
		[batch 20/20] avg loss: 0.02222195255458138		[learning rate: 0.0021617]
	Learning Rate: 0.00216167
	LOSS [training: 0.023394782212405825 | validation: 0.02286220936483266]
	TIME [epoch: 8.21 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028027740794237054		[learning rate: 0.0021591]
		[batch 20/20] avg loss: 0.025893559655464006		[learning rate: 0.0021564]
	Learning Rate: 0.00215644
	LOSS [training: 0.026960650224850523 | validation: 0.011266449073119343]
	TIME [epoch: 8.19 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019214113622566142		[learning rate: 0.0021538]
		[batch 20/20] avg loss: 0.014198158694297297		[learning rate: 0.0021512]
	Learning Rate: 0.00215122
	LOSS [training: 0.01670613615843172 | validation: 0.024763455870538666]
	TIME [epoch: 8.22 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04100359708039277		[learning rate: 0.0021486]
		[batch 20/20] avg loss: 0.030810671184403726		[learning rate: 0.002146]
	Learning Rate: 0.00214601
	LOSS [training: 0.03590713413239825 | validation: 0.06373183286512875]
	TIME [epoch: 8.23 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034101768442091555		[learning rate: 0.0021434]
		[batch 20/20] avg loss: 0.03452940156000529		[learning rate: 0.0021408]
	Learning Rate: 0.00214081
	LOSS [training: 0.03431558500104843 | validation: 0.04202335367650858]
	TIME [epoch: 8.19 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028466449131562394		[learning rate: 0.0021382]
		[batch 20/20] avg loss: 0.031112829609706495		[learning rate: 0.0021356]
	Learning Rate: 0.00213563
	LOSS [training: 0.02978963937063444 | validation: 0.06130140490175862]
	TIME [epoch: 8.19 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03336489966107997		[learning rate: 0.002133]
		[batch 20/20] avg loss: 0.022911023090950087		[learning rate: 0.0021305]
	Learning Rate: 0.00213046
	LOSS [training: 0.02813796137601503 | validation: 0.025833291117840457]
	TIME [epoch: 8.21 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04001715303951188		[learning rate: 0.0021279]
		[batch 20/20] avg loss: 0.040573929265162734		[learning rate: 0.0021253]
	Learning Rate: 0.0021253
	LOSS [training: 0.0402955411523373 | validation: 0.027284965551225143]
	TIME [epoch: 8.19 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017897325925178578		[learning rate: 0.0021227]
		[batch 20/20] avg loss: 0.038240112885355455		[learning rate: 0.0021202]
	Learning Rate: 0.00212016
	LOSS [training: 0.028068719405267017 | validation: 0.017561476327271175]
	TIME [epoch: 8.2 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019143419279767974		[learning rate: 0.0021176]
		[batch 20/20] avg loss: 0.030236071230082616		[learning rate: 0.002115]
	Learning Rate: 0.00211503
	LOSS [training: 0.024689745254925295 | validation: 0.03338291347757966]
	TIME [epoch: 8.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02515338218525285		[learning rate: 0.0021125]
		[batch 20/20] avg loss: 0.02488605058874014		[learning rate: 0.0021099]
	Learning Rate: 0.00210991
	LOSS [training: 0.025019716386996495 | validation: 0.01568987134111573]
	TIME [epoch: 8.21 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02294527768536883		[learning rate: 0.0021074]
		[batch 20/20] avg loss: 0.025657765310149194		[learning rate: 0.0021048]
	Learning Rate: 0.0021048
	LOSS [training: 0.024301521497759014 | validation: 0.034358533464599994]
	TIME [epoch: 8.2 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0255432742206469		[learning rate: 0.0021022]
		[batch 20/20] avg loss: 0.06618195268040679		[learning rate: 0.0020997]
	Learning Rate: 0.0020997
	LOSS [training: 0.04586261345052685 | validation: 0.088459166786723]
	TIME [epoch: 8.23 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05841591308392767		[learning rate: 0.0020972]
		[batch 20/20] avg loss: 0.051918366829495396		[learning rate: 0.0020946]
	Learning Rate: 0.00209462
	LOSS [training: 0.05516713995671153 | validation: 0.043533242347467055]
	TIME [epoch: 8.18 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027588020438306764		[learning rate: 0.0020921]
		[batch 20/20] avg loss: 0.04422573079765653		[learning rate: 0.0020895]
	Learning Rate: 0.00208955
	LOSS [training: 0.03590687561798164 | validation: 0.032777664218617386]
	TIME [epoch: 8.21 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05083556367803769		[learning rate: 0.002087]
		[batch 20/20] avg loss: 0.019867084182458285		[learning rate: 0.0020845]
	Learning Rate: 0.00208449
	LOSS [training: 0.03535132393024799 | validation: 0.041796154653824254]
	TIME [epoch: 8.18 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02893767512908966		[learning rate: 0.002082]
		[batch 20/20] avg loss: 0.028238717610760307		[learning rate: 0.0020794]
	Learning Rate: 0.00207944
	LOSS [training: 0.028588196369924983 | validation: 0.025551468407690216]
	TIME [epoch: 8.21 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022087026986486284		[learning rate: 0.0020769]
		[batch 20/20] avg loss: 0.017247908482973834		[learning rate: 0.0020744]
	Learning Rate: 0.00207441
	LOSS [training: 0.019667467734730062 | validation: 0.0533011107667466]
	TIME [epoch: 8.19 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052016943156762994		[learning rate: 0.0020719]
		[batch 20/20] avg loss: 0.0281563027408204		[learning rate: 0.0020694]
	Learning Rate: 0.00206939
	LOSS [training: 0.04008662294879169 | validation: 0.02002712728239759]
	TIME [epoch: 8.23 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01856592738723153		[learning rate: 0.0020669]
		[batch 20/20] avg loss: 0.03396578341238507		[learning rate: 0.0020644]
	Learning Rate: 0.00206438
	LOSS [training: 0.026265855399808302 | validation: 0.05485715029124698]
	TIME [epoch: 8.23 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04468773317216667		[learning rate: 0.0020619]
		[batch 20/20] avg loss: 0.03661848837452091		[learning rate: 0.0020594]
	Learning Rate: 0.00205938
	LOSS [training: 0.04065311077334379 | validation: 0.023852256380870126]
	TIME [epoch: 8.19 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0354754149791873		[learning rate: 0.0020569]
		[batch 20/20] avg loss: 0.021190562376488656		[learning rate: 0.0020544]
	Learning Rate: 0.0020544
	LOSS [training: 0.028332988677837973 | validation: 0.016313075512951004]
	TIME [epoch: 8.18 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02950026965561005		[learning rate: 0.0020519]
		[batch 20/20] avg loss: 0.043914451830666626		[learning rate: 0.0020494]
	Learning Rate: 0.00204942
	LOSS [training: 0.03670736074313834 | validation: 0.03548058279926487]
	TIME [epoch: 8.21 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03889112828553716		[learning rate: 0.0020469]
		[batch 20/20] avg loss: 0.03223803815346522		[learning rate: 0.0020445]
	Learning Rate: 0.00204446
	LOSS [training: 0.03556458321950119 | validation: 0.035768035133039416]
	TIME [epoch: 8.18 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03066922216020813		[learning rate: 0.002042]
		[batch 20/20] avg loss: 0.057546922781903063		[learning rate: 0.0020395]
	Learning Rate: 0.00203951
	LOSS [training: 0.044108072471055595 | validation: 0.036835183204929525]
	TIME [epoch: 8.19 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031175729962125275		[learning rate: 0.002037]
		[batch 20/20] avg loss: 0.023639489056533063		[learning rate: 0.0020346]
	Learning Rate: 0.00203457
	LOSS [training: 0.02740760950932916 | validation: 0.022665160321012442]
	TIME [epoch: 8.21 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031134896847525073		[learning rate: 0.0020321]
		[batch 20/20] avg loss: 0.02355919032259818		[learning rate: 0.0020296]
	Learning Rate: 0.00202965
	LOSS [training: 0.027347043585061633 | validation: 0.0262351360359741]
	TIME [epoch: 8.21 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04110173257487278		[learning rate: 0.0020272]
		[batch 20/20] avg loss: 0.03784861995253509		[learning rate: 0.0020247]
	Learning Rate: 0.00202474
	LOSS [training: 0.039475176263703934 | validation: 0.059066864935907064]
	TIME [epoch: 8.19 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037925414137393455		[learning rate: 0.0020223]
		[batch 20/20] avg loss: 0.034004695956644466		[learning rate: 0.0020198]
	Learning Rate: 0.00201983
	LOSS [training: 0.035965055047018954 | validation: 0.030473512213622607]
	TIME [epoch: 8.22 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029633554528122254		[learning rate: 0.0020174]
		[batch 20/20] avg loss: 0.04474921388660336		[learning rate: 0.0020149]
	Learning Rate: 0.00201494
	LOSS [training: 0.0371913842073628 | validation: 0.03836632798685091]
	TIME [epoch: 8.18 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02528044600001488		[learning rate: 0.0020125]
		[batch 20/20] avg loss: 0.03230439707258359		[learning rate: 0.0020101]
	Learning Rate: 0.00201007
	LOSS [training: 0.028792421536299233 | validation: 0.03560320453306764]
	TIME [epoch: 8.21 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04625654027521344		[learning rate: 0.0020076]
		[batch 20/20] avg loss: 0.03304468186229091		[learning rate: 0.0020052]
	Learning Rate: 0.0020052
	LOSS [training: 0.03965061106875218 | validation: 0.022543867250266088]
	TIME [epoch: 8.18 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03914642660520702		[learning rate: 0.0020028]
		[batch 20/20] avg loss: 0.04676554572026507		[learning rate: 0.0020003]
	Learning Rate: 0.00200035
	LOSS [training: 0.04295598616273605 | validation: 0.045077211316266734]
	TIME [epoch: 8.2 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046995960838535134		[learning rate: 0.0019979]
		[batch 20/20] avg loss: 0.06353242916899972		[learning rate: 0.0019955]
	Learning Rate: 0.0019955
	LOSS [training: 0.055264195003767426 | validation: 0.02684535381629209]
	TIME [epoch: 8.2 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028765799208928383		[learning rate: 0.0019931]
		[batch 20/20] avg loss: 0.026036296342720915		[learning rate: 0.0019907]
	Learning Rate: 0.00199067
	LOSS [training: 0.027401047775824645 | validation: 0.015189928435048478]
	TIME [epoch: 8.21 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020458416123441896		[learning rate: 0.0019883]
		[batch 20/20] avg loss: 0.03992446347097223		[learning rate: 0.0019859]
	Learning Rate: 0.00198585
	LOSS [training: 0.030191439797207065 | validation: 0.04946764579807206]
	TIME [epoch: 8.19 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03087732352582549		[learning rate: 0.0019834]
		[batch 20/20] avg loss: 0.024901682942225063		[learning rate: 0.001981]
	Learning Rate: 0.00198105
	LOSS [training: 0.027889503234025274 | validation: 0.03449134232944706]
	TIME [epoch: 8.22 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03773493340274706		[learning rate: 0.0019786]
		[batch 20/20] avg loss: 0.027459712374039862		[learning rate: 0.0019763]
	Learning Rate: 0.00197625
	LOSS [training: 0.03259732288839346 | validation: 0.045999130401766626]
	TIME [epoch: 8.18 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024202524344405046		[learning rate: 0.0019739]
		[batch 20/20] avg loss: 0.04372395554383747		[learning rate: 0.0019715]
	Learning Rate: 0.00197147
	LOSS [training: 0.03396323994412126 | validation: 0.0211629996329405]
	TIME [epoch: 8.2 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025720812472256372		[learning rate: 0.0019691]
		[batch 20/20] avg loss: 0.02745447193674405		[learning rate: 0.0019667]
	Learning Rate: 0.00196669
	LOSS [training: 0.02658764220450021 | validation: 0.03147608439609818]
	TIME [epoch: 8.19 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02777660058380215		[learning rate: 0.0019643]
		[batch 20/20] avg loss: 0.021600865645753663		[learning rate: 0.0019619]
	Learning Rate: 0.00196193
	LOSS [training: 0.02468873311477791 | validation: 0.016965766566237855]
	TIME [epoch: 8.18 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021381920073297065		[learning rate: 0.0019596]
		[batch 20/20] avg loss: 0.08396730714128327		[learning rate: 0.0019572]
	Learning Rate: 0.00195718
	LOSS [training: 0.052674613607290165 | validation: 0.04553271622612398]
	TIME [epoch: 8.18 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05502974098104725		[learning rate: 0.0019548]
		[batch 20/20] avg loss: 0.044463592351075214		[learning rate: 0.0019524]
	Learning Rate: 0.00195245
	LOSS [training: 0.049746666666061236 | validation: 0.026237201750256997]
	TIME [epoch: 8.19 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03894281820094867		[learning rate: 0.0019501]
		[batch 20/20] avg loss: 0.04135431007099124		[learning rate: 0.0019477]
	Learning Rate: 0.00194772
	LOSS [training: 0.040148564135969955 | validation: 0.019611074773665634]
	TIME [epoch: 8.23 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02242803438486991		[learning rate: 0.0019454]
		[batch 20/20] avg loss: 0.023797536378873274		[learning rate: 0.001943]
	Learning Rate: 0.001943
	LOSS [training: 0.023112785381871587 | validation: 0.018859786563597977]
	TIME [epoch: 8.19 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020744254544987635		[learning rate: 0.0019407]
		[batch 20/20] avg loss: 0.023410916389492907		[learning rate: 0.0019383]
	Learning Rate: 0.0019383
	LOSS [training: 0.02207758546724027 | validation: 0.041306421438269014]
	TIME [epoch: 8.19 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03666910435534412		[learning rate: 0.001936]
		[batch 20/20] avg loss: 0.030490876402652255		[learning rate: 0.0019336]
	Learning Rate: 0.00193361
	LOSS [training: 0.033579990378998184 | validation: 0.032605419065282824]
	TIME [epoch: 8.21 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03939585208105016		[learning rate: 0.0019313]
		[batch 20/20] avg loss: 0.04569136676143552		[learning rate: 0.0019289]
	Learning Rate: 0.00192893
	LOSS [training: 0.04254360942124284 | validation: 0.031213719825247566]
	TIME [epoch: 8.23 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04790914744190753		[learning rate: 0.0019266]
		[batch 20/20] avg loss: 0.05769943891214261		[learning rate: 0.0019243]
	Learning Rate: 0.00192426
	LOSS [training: 0.05280429317702508 | validation: 0.06352446772232584]
	TIME [epoch: 8.18 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04610747463615716		[learning rate: 0.0019219]
		[batch 20/20] avg loss: 0.03838261769507355		[learning rate: 0.0019196]
	Learning Rate: 0.0019196
	LOSS [training: 0.042245046165615356 | validation: 0.03523283954071497]
	TIME [epoch: 8.19 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057449779879484686		[learning rate: 0.0019173]
		[batch 20/20] avg loss: 0.05260228430135859		[learning rate: 0.001915]
	Learning Rate: 0.00191495
	LOSS [training: 0.05502603209042163 | validation: 0.04432269952214105]
	TIME [epoch: 8.18 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05232209614975406		[learning rate: 0.0019126]
		[batch 20/20] avg loss: 0.052285518914355976		[learning rate: 0.0019103]
	Learning Rate: 0.00191032
	LOSS [training: 0.052303807532055026 | validation: 0.036395961447439756]
	TIME [epoch: 8.2 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04005977084682409		[learning rate: 0.001908]
		[batch 20/20] avg loss: 0.04301808098999031		[learning rate: 0.0019057]
	Learning Rate: 0.00190569
	LOSS [training: 0.0415389259184072 | validation: 0.027561034705128586]
	TIME [epoch: 8.18 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023925189832773308		[learning rate: 0.0019034]
		[batch 20/20] avg loss: 0.027367752069492046		[learning rate: 0.0019011]
	Learning Rate: 0.00190108
	LOSS [training: 0.025646470951132684 | validation: 0.009429696098977255]
	TIME [epoch: 8.21 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030823416436583057		[learning rate: 0.0018988]
		[batch 20/20] avg loss: 0.027187354340554533		[learning rate: 0.0018965]
	Learning Rate: 0.00189648
	LOSS [training: 0.029005385388568793 | validation: 0.02395952215870314]
	TIME [epoch: 8.19 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03802540674979955		[learning rate: 0.0018942]
		[batch 20/20] avg loss: 0.05815293518134026		[learning rate: 0.0018919]
	Learning Rate: 0.00189188
	LOSS [training: 0.048089170965569895 | validation: 0.03701028985294736]
	TIME [epoch: 8.21 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025321018075881312		[learning rate: 0.0018896]
		[batch 20/20] avg loss: 0.029427514406333295		[learning rate: 0.0018873]
	Learning Rate: 0.00188731
	LOSS [training: 0.027374266241107302 | validation: 0.015810316864554155]
	TIME [epoch: 8.24 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030761243191286584		[learning rate: 0.001885]
		[batch 20/20] avg loss: 0.0345883500411094		[learning rate: 0.0018827]
	Learning Rate: 0.00188274
	LOSS [training: 0.032674796616198 | validation: 0.0325239802925541]
	TIME [epoch: 8.18 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034327245930884195		[learning rate: 0.0018805]
		[batch 20/20] avg loss: 0.049099611111894724		[learning rate: 0.0018782]
	Learning Rate: 0.00187818
	LOSS [training: 0.04171342852138946 | validation: 0.019689645571085566]
	TIME [epoch: 8.18 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04035890942979161		[learning rate: 0.0018759]
		[batch 20/20] avg loss: 0.03414183638808415		[learning rate: 0.0018736]
	Learning Rate: 0.00187363
	LOSS [training: 0.037250372908937866 | validation: 0.059627066686253644]
	TIME [epoch: 8.2 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05747807332757743		[learning rate: 0.0018714]
		[batch 20/20] avg loss: 0.028794551903226467		[learning rate: 0.0018691]
	Learning Rate: 0.0018691
	LOSS [training: 0.04313631261540195 | validation: 0.018038758328355692]
	TIME [epoch: 8.18 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02063623707305507		[learning rate: 0.0018668]
		[batch 20/20] avg loss: 0.01855842278515241		[learning rate: 0.0018646]
	Learning Rate: 0.00186457
	LOSS [training: 0.01959732992910374 | validation: 0.023457628062483554]
	TIME [epoch: 8.18 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050185564674154935		[learning rate: 0.0018623]
		[batch 20/20] avg loss: 0.03379914210677712		[learning rate: 0.0018601]
	Learning Rate: 0.00186006
	LOSS [training: 0.04199235339046603 | validation: 0.01783030635294978]
	TIME [epoch: 8.2 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0152850669813837		[learning rate: 0.0018578]
		[batch 20/20] avg loss: 0.02669803475846807		[learning rate: 0.0018556]
	Learning Rate: 0.00185555
	LOSS [training: 0.020991550869925886 | validation: 0.02034037509515997]
	TIME [epoch: 8.21 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0284385660444904		[learning rate: 0.0018533]
		[batch 20/20] avg loss: 0.026199731006809023		[learning rate: 0.0018511]
	Learning Rate: 0.00185106
	LOSS [training: 0.027319148525649716 | validation: 0.028531746466710084]
	TIME [epoch: 8.19 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01181832331148183		[learning rate: 0.0018488]
		[batch 20/20] avg loss: 0.026949675917776982		[learning rate: 0.0018466]
	Learning Rate: 0.00184658
	LOSS [training: 0.01938399961462941 | validation: 0.03379951194796654]
	TIME [epoch: 8.2 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03553947910019724		[learning rate: 0.0018443]
		[batch 20/20] avg loss: 0.037236635937325346		[learning rate: 0.0018421]
	Learning Rate: 0.00184211
	LOSS [training: 0.0363880575187613 | validation: 0.030556243017145247]
	TIME [epoch: 8.21 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0470610984203666		[learning rate: 0.0018399]
		[batch 20/20] avg loss: 0.01848761033256742		[learning rate: 0.0018377]
	Learning Rate: 0.00183765
	LOSS [training: 0.032774354376467016 | validation: 0.017980337227510225]
	TIME [epoch: 8.2 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02528584072330019		[learning rate: 0.0018354]
		[batch 20/20] avg loss: 0.01852211159206412		[learning rate: 0.0018332]
	Learning Rate: 0.0018332
	LOSS [training: 0.021903976157682154 | validation: 0.024129603357959762]
	TIME [epoch: 8.18 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017421399118929048		[learning rate: 0.001831]
		[batch 20/20] avg loss: 0.027255622834261937		[learning rate: 0.0018288]
	Learning Rate: 0.00182876
	LOSS [training: 0.022338510976595494 | validation: 0.030927522512259543]
	TIME [epoch: 8.18 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02327628214791013		[learning rate: 0.0018265]
		[batch 20/20] avg loss: 0.02073277459620549		[learning rate: 0.0018243]
	Learning Rate: 0.00182434
	LOSS [training: 0.02200452837205781 | validation: 0.018046706987774025]
	TIME [epoch: 8.18 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032955048828616496		[learning rate: 0.0018221]
		[batch 20/20] avg loss: 0.026293569928616867		[learning rate: 0.0018199]
	Learning Rate: 0.00181992
	LOSS [training: 0.02962430937861668 | validation: 0.016897195239359408]
	TIME [epoch: 8.23 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03505680363121038		[learning rate: 0.0018177]
		[batch 20/20] avg loss: 0.02376367118420159		[learning rate: 0.0018155]
	Learning Rate: 0.00181552
	LOSS [training: 0.029410237407705986 | validation: 0.01748489676821406]
	TIME [epoch: 8.19 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024643538521405103		[learning rate: 0.0018133]
		[batch 20/20] avg loss: 0.02072228090657987		[learning rate: 0.0018111]
	Learning Rate: 0.00181112
	LOSS [training: 0.022682909713992484 | validation: 0.019471905080064234]
	TIME [epoch: 8.19 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014203874509784243		[learning rate: 0.0018089]
		[batch 20/20] avg loss: 0.02781305922850964		[learning rate: 0.0018067]
	Learning Rate: 0.00180674
	LOSS [training: 0.021008466869146942 | validation: 0.028400820153750776]
	TIME [epoch: 8.23 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04097726073821405		[learning rate: 0.0018045]
		[batch 20/20] avg loss: 0.02137371741466979		[learning rate: 0.0018024]
	Learning Rate: 0.00180236
	LOSS [training: 0.031175489076441916 | validation: 0.029326633869110868]
	TIME [epoch: 8.22 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024186781077162694		[learning rate: 0.0018002]
		[batch 20/20] avg loss: 0.02318184063400518		[learning rate: 0.001798]
	Learning Rate: 0.001798
	LOSS [training: 0.023684310855583937 | validation: 0.007731012450318572]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_808.pth
	Model improved!!!
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012584296706259537		[learning rate: 0.0017958]
		[batch 20/20] avg loss: 0.017097065886924185		[learning rate: 0.0017936]
	Learning Rate: 0.00179365
	LOSS [training: 0.014840681296591862 | validation: 0.01895855881678124]
	TIME [epoch: 8.16 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028820435942112414		[learning rate: 0.0017915]
		[batch 20/20] avg loss: 0.03201478869483464		[learning rate: 0.0017893]
	Learning Rate: 0.0017893
	LOSS [training: 0.03041761231847352 | validation: 0.023089489162600965]
	TIME [epoch: 8.16 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024227000222224813		[learning rate: 0.0017871]
		[batch 20/20] avg loss: 0.024406595242636525		[learning rate: 0.001785]
	Learning Rate: 0.00178497
	LOSS [training: 0.024316797732430666 | validation: 0.009930652455768303]
	TIME [epoch: 8.19 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04515435318599066		[learning rate: 0.0017828]
		[batch 20/20] avg loss: 0.022393870356478003		[learning rate: 0.0017807]
	Learning Rate: 0.00178065
	LOSS [training: 0.033774111771234336 | validation: 0.010202199162907046]
	TIME [epoch: 8.19 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03173552285864205		[learning rate: 0.0017785]
		[batch 20/20] avg loss: 0.012230161420405889		[learning rate: 0.0017763]
	Learning Rate: 0.00177634
	LOSS [training: 0.021982842139523964 | validation: 0.01225634251624148]
	TIME [epoch: 8.17 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014559863512894256		[learning rate: 0.0017742]
		[batch 20/20] avg loss: 0.022872818906474877		[learning rate: 0.001772]
	Learning Rate: 0.00177204
	LOSS [training: 0.018716341209684566 | validation: 0.05113004700953999]
	TIME [epoch: 8.17 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03097339429761235		[learning rate: 0.0017699]
		[batch 20/20] avg loss: 0.023894131357881418		[learning rate: 0.0017678]
	Learning Rate: 0.00176775
	LOSS [training: 0.02743376282774689 | validation: 0.021618049375589195]
	TIME [epoch: 8.22 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03332996659846752		[learning rate: 0.0017656]
		[batch 20/20] avg loss: 0.035753840565780695		[learning rate: 0.0017635]
	Learning Rate: 0.00176347
	LOSS [training: 0.03454190358212411 | validation: 0.041940619030337384]
	TIME [epoch: 8.18 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01751000968002218		[learning rate: 0.0017613]
		[batch 20/20] avg loss: 0.01564956546625274		[learning rate: 0.0017592]
	Learning Rate: 0.0017592
	LOSS [training: 0.01657978757313746 | validation: 0.03163482087861546]
	TIME [epoch: 8.16 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020513263369646848		[learning rate: 0.0017571]
		[batch 20/20] avg loss: 0.035987277176818946		[learning rate: 0.0017549]
	Learning Rate: 0.00175494
	LOSS [training: 0.028250270273232892 | validation: 0.06464187787928533]
	TIME [epoch: 8.17 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05208278427722177		[learning rate: 0.0017528]
		[batch 20/20] avg loss: 0.01885674995782456		[learning rate: 0.0017507]
	Learning Rate: 0.0017507
	LOSS [training: 0.035469767117523164 | validation: 0.04862319268588937]
	TIME [epoch: 8.19 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03533833608220761		[learning rate: 0.0017486]
		[batch 20/20] avg loss: 0.0301495856546344		[learning rate: 0.0017465]
	Learning Rate: 0.00174646
	LOSS [training: 0.032743960868421004 | validation: 0.022630434608702815]
	TIME [epoch: 8.2 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028771889295172038		[learning rate: 0.0017443]
		[batch 20/20] avg loss: 0.02528042794874482		[learning rate: 0.0017422]
	Learning Rate: 0.00174223
	LOSS [training: 0.02702615862195843 | validation: 0.01166262910342671]
	TIME [epoch: 8.17 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02873343655026908		[learning rate: 0.0017401]
		[batch 20/20] avg loss: 0.04061386833710796		[learning rate: 0.001738]
	Learning Rate: 0.00173801
	LOSS [training: 0.034673652443688516 | validation: 0.041341189424773585]
	TIME [epoch: 8.18 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04423206846858888		[learning rate: 0.0017359]
		[batch 20/20] avg loss: 0.033142960133621155		[learning rate: 0.0017338]
	Learning Rate: 0.0017338
	LOSS [training: 0.03868751430110502 | validation: 0.04320172305167526]
	TIME [epoch: 8.21 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026212684884005744		[learning rate: 0.0017317]
		[batch 20/20] avg loss: 0.020094484266348404		[learning rate: 0.0017296]
	Learning Rate: 0.00172961
	LOSS [training: 0.023153584575177074 | validation: 0.029120186374886205]
	TIME [epoch: 8.2 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024193304172229407		[learning rate: 0.0017275]
		[batch 20/20] avg loss: 0.019626281916076654		[learning rate: 0.0017254]
	Learning Rate: 0.00172542
	LOSS [training: 0.02190979304415303 | validation: 0.017561467327006884]
	TIME [epoch: 8.17 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023918909576236266		[learning rate: 0.0017233]
		[batch 20/20] avg loss: 0.024313177376110797		[learning rate: 0.0017212]
	Learning Rate: 0.00172124
	LOSS [training: 0.024116043476173528 | validation: 0.025594378678442317]
	TIME [epoch: 8.17 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01155052524588818		[learning rate: 0.0017192]
		[batch 20/20] avg loss: 0.02229981898310775		[learning rate: 0.0017171]
	Learning Rate: 0.00171708
	LOSS [training: 0.016925172114497965 | validation: 0.020570829606487692]
	TIME [epoch: 8.2 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023308477114809125		[learning rate: 0.001715]
		[batch 20/20] avg loss: 0.03382930816561454		[learning rate: 0.0017129]
	Learning Rate: 0.00171292
	LOSS [training: 0.028568892640211833 | validation: 0.01589530242046966]
	TIME [epoch: 8.18 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01715660679909865		[learning rate: 0.0017108]
		[batch 20/20] avg loss: 0.02459264551267413		[learning rate: 0.0017088]
	Learning Rate: 0.00170877
	LOSS [training: 0.020874626155886387 | validation: 0.015240503400994051]
	TIME [epoch: 8.19 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01588376552584159		[learning rate: 0.0017067]
		[batch 20/20] avg loss: 0.03454287589137185		[learning rate: 0.0017046]
	Learning Rate: 0.00170464
	LOSS [training: 0.02521332070860673 | validation: 0.015737732800494956]
	TIME [epoch: 8.18 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038774095270425704		[learning rate: 0.0017026]
		[batch 20/20] avg loss: 0.02110846027674352		[learning rate: 0.0017005]
	Learning Rate: 0.00170051
	LOSS [training: 0.02994127777358462 | validation: 0.00727934670398902]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_831.pth
	Model improved!!!
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023526048109968615		[learning rate: 0.0016984]
		[batch 20/20] avg loss: 0.024118645258192524		[learning rate: 0.0016964]
	Learning Rate: 0.00169639
	LOSS [training: 0.023822346684080564 | validation: 0.011309878898766505]
	TIME [epoch: 8.21 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023597123913335082		[learning rate: 0.0016943]
		[batch 20/20] avg loss: 0.034921280252008406		[learning rate: 0.0016923]
	Learning Rate: 0.00169229
	LOSS [training: 0.02925920208267175 | validation: 0.024782019880337854]
	TIME [epoch: 8.17 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02235844061889658		[learning rate: 0.0016902]
		[batch 20/20] avg loss: 0.030116766605828637		[learning rate: 0.0016882]
	Learning Rate: 0.00168819
	LOSS [training: 0.02623760361236261 | validation: 0.013137677714697309]
	TIME [epoch: 8.17 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018382017880557797		[learning rate: 0.0016861]
		[batch 20/20] avg loss: 0.013023482806348912		[learning rate: 0.0016841]
	Learning Rate: 0.0016841
	LOSS [training: 0.015702750343453357 | validation: 0.007385398133965509]
	TIME [epoch: 8.18 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020789735265019718		[learning rate: 0.0016821]
		[batch 20/20] avg loss: 0.012760372375871843		[learning rate: 0.00168]
	Learning Rate: 0.00168003
	LOSS [training: 0.016775053820445782 | validation: 0.0355011183157712]
	TIME [epoch: 8.17 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016218480217214594		[learning rate: 0.001678]
		[batch 20/20] avg loss: 0.012042492047266722		[learning rate: 0.001676]
	Learning Rate: 0.00167596
	LOSS [training: 0.014130486132240655 | validation: 0.027803942840231983]
	TIME [epoch: 8.19 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025644531029816398		[learning rate: 0.0016739]
		[batch 20/20] avg loss: 0.03160560086432266		[learning rate: 0.0016719]
	Learning Rate: 0.0016719
	LOSS [training: 0.02862506594706953 | validation: 0.012083703686032255]
	TIME [epoch: 8.18 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016852726282121304		[learning rate: 0.0016699]
		[batch 20/20] avg loss: 0.031162504531153823		[learning rate: 0.0016679]
	Learning Rate: 0.00166785
	LOSS [training: 0.02400761540663756 | validation: 0.04403642069594746]
	TIME [epoch: 8.19 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045285842779280036		[learning rate: 0.0016658]
		[batch 20/20] avg loss: 0.06307321460501278		[learning rate: 0.0016638]
	Learning Rate: 0.00166382
	LOSS [training: 0.054179528692146414 | validation: 0.05528476558454621]
	TIME [epoch: 8.2 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031404828898386694		[learning rate: 0.0016618]
		[batch 20/20] avg loss: 0.0380066175762187		[learning rate: 0.0016598]
	Learning Rate: 0.00165979
	LOSS [training: 0.03470572323730269 | validation: 0.0549597716870031]
	TIME [epoch: 8.18 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06935002233306746		[learning rate: 0.0016578]
		[batch 20/20] avg loss: 0.05190198021177851		[learning rate: 0.0016558]
	Learning Rate: 0.00165577
	LOSS [training: 0.06062600127242299 | validation: 0.05295720539659388]
	TIME [epoch: 8.16 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04032152795045503		[learning rate: 0.0016538]
		[batch 20/20] avg loss: 0.02441613074906286		[learning rate: 0.0016518]
	Learning Rate: 0.00165176
	LOSS [training: 0.032368829349758946 | validation: 0.048479433705767135]
	TIME [epoch: 8.19 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08413118855451775		[learning rate: 0.0016498]
		[batch 20/20] avg loss: 0.06963903671787454		[learning rate: 0.0016478]
	Learning Rate: 0.00164776
	LOSS [training: 0.07688511263619614 | validation: 0.10626856520495088]
	TIME [epoch: 8.16 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07646974171250395		[learning rate: 0.0016458]
		[batch 20/20] avg loss: 0.047151842688737955		[learning rate: 0.0016438]
	Learning Rate: 0.00164377
	LOSS [training: 0.06181079220062095 | validation: 0.043324621046979796]
	TIME [epoch: 8.19 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040050834693012405		[learning rate: 0.0016418]
		[batch 20/20] avg loss: 0.0395128666215856		[learning rate: 0.0016398]
	Learning Rate: 0.00163979
	LOSS [training: 0.039781850657298994 | validation: 0.03458341340202121]
	TIME [epoch: 8.16 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029411769079323424		[learning rate: 0.0016378]
		[batch 20/20] avg loss: 0.02422810779093922		[learning rate: 0.0016358]
	Learning Rate: 0.00163583
	LOSS [training: 0.026819938435131325 | validation: 0.01876136673491754]
	TIME [epoch: 8.2 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018432194888961632		[learning rate: 0.0016338]
		[batch 20/20] avg loss: 0.04612140152790339		[learning rate: 0.0016319]
	Learning Rate: 0.00163186
	LOSS [training: 0.0322767982084325 | validation: 0.046749730263450015]
	TIME [epoch: 8.19 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05778905370204244		[learning rate: 0.0016299]
		[batch 20/20] avg loss: 0.038929787094459545		[learning rate: 0.0016279]
	Learning Rate: 0.00162791
	LOSS [training: 0.048359420398250974 | validation: 0.0579852984692725]
	TIME [epoch: 8.19 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03194746540543368		[learning rate: 0.0016259]
		[batch 20/20] avg loss: 0.026597356562151498		[learning rate: 0.001624]
	Learning Rate: 0.00162397
	LOSS [training: 0.02927241098379259 | validation: 0.017389201826071045]
	TIME [epoch: 8.16 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023955282998216303		[learning rate: 0.001622]
		[batch 20/20] avg loss: 0.03604663095610954		[learning rate: 0.00162]
	Learning Rate: 0.00162004
	LOSS [training: 0.03000095697716293 | validation: 0.03735841692235805]
	TIME [epoch: 8.19 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026950616004956997		[learning rate: 0.0016181]
		[batch 20/20] avg loss: 0.02599475901186159		[learning rate: 0.0016161]
	Learning Rate: 0.00161612
	LOSS [training: 0.026472687508409292 | validation: 0.02677960786080264]
	TIME [epoch: 8.16 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04519939869807423		[learning rate: 0.0016142]
		[batch 20/20] avg loss: 0.04525233414140392		[learning rate: 0.0016122]
	Learning Rate: 0.00161221
	LOSS [training: 0.04522586641973907 | validation: 0.05377715317128859]
	TIME [epoch: 8.16 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07027593914402343		[learning rate: 0.0016103]
		[batch 20/20] avg loss: 0.059456041806060546		[learning rate: 0.0016083]
	Learning Rate: 0.0016083
	LOSS [training: 0.06486599047504199 | validation: 0.029928688105061975]
	TIME [epoch: 8.16 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0359325883555369		[learning rate: 0.0016064]
		[batch 20/20] avg loss: 0.06652015806198051		[learning rate: 0.0016044]
	Learning Rate: 0.00160441
	LOSS [training: 0.05122637320875871 | validation: 0.03510384341861145]
	TIME [epoch: 8.2 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02942026905173877		[learning rate: 0.0016025]
		[batch 20/20] avg loss: 0.03021381706004811		[learning rate: 0.0016005]
	Learning Rate: 0.00160053
	LOSS [training: 0.029817043055893437 | validation: 0.023214945499203117]
	TIME [epoch: 8.18 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020460642320365006		[learning rate: 0.0015986]
		[batch 20/20] avg loss: 0.027959193737376754		[learning rate: 0.0015967]
	Learning Rate: 0.00159665
	LOSS [training: 0.024209918028870882 | validation: 0.03564199055082805]
	TIME [epoch: 8.17 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020544621284277427		[learning rate: 0.0015947]
		[batch 20/20] avg loss: 0.02172595538141717		[learning rate: 0.0015928]
	Learning Rate: 0.00159279
	LOSS [training: 0.021135288332847297 | validation: 0.028988873315074867]
	TIME [epoch: 8.18 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029328607633338326		[learning rate: 0.0015909]
		[batch 20/20] avg loss: 0.0177192342230653		[learning rate: 0.0015889]
	Learning Rate: 0.00158893
	LOSS [training: 0.023523920928201818 | validation: 0.023541138576461042]
	TIME [epoch: 8.2 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029195017962975266		[learning rate: 0.001587]
		[batch 20/20] avg loss: 0.03012362690093585		[learning rate: 0.0015851]
	Learning Rate: 0.00158509
	LOSS [training: 0.02965932243195556 | validation: 0.02206852550250942]
	TIME [epoch: 8.18 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021243976184513055		[learning rate: 0.0015832]
		[batch 20/20] avg loss: 0.021011208527468385		[learning rate: 0.0015812]
	Learning Rate: 0.00158125
	LOSS [training: 0.021127592355990717 | validation: 0.012129281935745867]
	TIME [epoch: 8.16 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020925476147846588		[learning rate: 0.0015793]
		[batch 20/20] avg loss: 0.02305873696780336		[learning rate: 0.0015774]
	Learning Rate: 0.00157742
	LOSS [training: 0.021992106557824975 | validation: 0.030471236352901603]
	TIME [epoch: 8.16 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03483690916898878		[learning rate: 0.0015755]
		[batch 20/20] avg loss: 0.0256541739670783		[learning rate: 0.0015736]
	Learning Rate: 0.0015736
	LOSS [training: 0.03024554156803355 | validation: 0.019792691007251083]
	TIME [epoch: 8.17 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019955046244826143		[learning rate: 0.0015717]
		[batch 20/20] avg loss: 0.02124109532815884		[learning rate: 0.0015698]
	Learning Rate: 0.00156979
	LOSS [training: 0.02059807078649249 | validation: 0.014656734499498699]
	TIME [epoch: 8.21 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013170069406831329		[learning rate: 0.0015679]
		[batch 20/20] avg loss: 0.029102979279468776		[learning rate: 0.001566]
	Learning Rate: 0.00156599
	LOSS [training: 0.021136524343150054 | validation: 0.020872713643594335]
	TIME [epoch: 8.17 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025745009582170943		[learning rate: 0.0015641]
		[batch 20/20] avg loss: 0.03010464283435004		[learning rate: 0.0015622]
	Learning Rate: 0.0015622
	LOSS [training: 0.027924826208260495 | validation: 0.016922004749948538]
	TIME [epoch: 8.17 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02932032677462878		[learning rate: 0.0015603]
		[batch 20/20] avg loss: 0.0302192934956102		[learning rate: 0.0015584]
	Learning Rate: 0.00155842
	LOSS [training: 0.02976981013511949 | validation: 0.0219798451181003]
	TIME [epoch: 8.2 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02371258108920859		[learning rate: 0.0015565]
		[batch 20/20] avg loss: 0.017810336831278875		[learning rate: 0.0015546]
	Learning Rate: 0.00155465
	LOSS [training: 0.020761458960243732 | validation: 0.012106402777236418]
	TIME [epoch: 8.19 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024439241596177292		[learning rate: 0.0015528]
		[batch 20/20] avg loss: 0.027735417686442793		[learning rate: 0.0015509]
	Learning Rate: 0.00155088
	LOSS [training: 0.026087329641310043 | validation: 0.047984578839978184]
	TIME [epoch: 8.17 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02322185917785645		[learning rate: 0.001549]
		[batch 20/20] avg loss: 0.062229602551221654		[learning rate: 0.0015471]
	Learning Rate: 0.00154713
	LOSS [training: 0.04272573086453905 | validation: 0.07414316478121553]
	TIME [epoch: 8.16 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07595837400358625		[learning rate: 0.0015453]
		[batch 20/20] avg loss: 0.05723726263792739		[learning rate: 0.0015434]
	Learning Rate: 0.00154338
	LOSS [training: 0.06659781832075681 | validation: 0.04603576687266907]
	TIME [epoch: 8.18 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04803986981921281		[learning rate: 0.0015415]
		[batch 20/20] avg loss: 0.035749690574608015		[learning rate: 0.0015396]
	Learning Rate: 0.00153965
	LOSS [training: 0.04189478019691041 | validation: 0.0772875050988077]
	TIME [epoch: 8.19 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0686446561157892		[learning rate: 0.0015378]
		[batch 20/20] avg loss: 0.10980065334824551		[learning rate: 0.0015359]
	Learning Rate: 0.00153592
	LOSS [training: 0.08922265473201738 | validation: 0.13672475110265359]
	TIME [epoch: 8.17 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08054051701205929		[learning rate: 0.0015341]
		[batch 20/20] avg loss: 0.06446027691965331		[learning rate: 0.0015322]
	Learning Rate: 0.0015322
	LOSS [training: 0.0725003969658563 | validation: 0.05427373532667595]
	TIME [epoch: 8.18 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048759045283699155		[learning rate: 0.0015303]
		[batch 20/20] avg loss: 0.030694953371234967		[learning rate: 0.0015285]
	Learning Rate: 0.00152849
	LOSS [training: 0.03972699932746705 | validation: 0.03208734978696451]
	TIME [epoch: 8.21 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0409064523996555		[learning rate: 0.0015266]
		[batch 20/20] avg loss: 0.040211502908490224		[learning rate: 0.0015248]
	Learning Rate: 0.00152479
	LOSS [training: 0.04055897765407286 | validation: 0.01801606654458776]
	TIME [epoch: 8.19 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030146622650848075		[learning rate: 0.0015229]
		[batch 20/20] avg loss: 0.026142296030545336		[learning rate: 0.0015211]
	Learning Rate: 0.0015211
	LOSS [training: 0.028144459340696697 | validation: 0.03728427917403572]
	TIME [epoch: 8.16 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01982683413347857		[learning rate: 0.0015193]
		[batch 20/20] avg loss: 0.024722757447762034		[learning rate: 0.0015174]
	Learning Rate: 0.00151742
	LOSS [training: 0.022274795790620302 | validation: 0.013595289353628578]
	TIME [epoch: 8.17 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0232172751458571		[learning rate: 0.0015156]
		[batch 20/20] avg loss: 0.02328128971883977		[learning rate: 0.0015137]
	Learning Rate: 0.00151374
	LOSS [training: 0.023249282432348434 | validation: 0.019362743950591784]
	TIME [epoch: 8.16 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014198075509500812		[learning rate: 0.0015119]
		[batch 20/20] avg loss: 0.02916122030431407		[learning rate: 0.0015101]
	Learning Rate: 0.00151008
	LOSS [training: 0.02167964790690744 | validation: 0.03056037477318666]
	TIME [epoch: 8.21 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022475436808597156		[learning rate: 0.0015083]
		[batch 20/20] avg loss: 0.022749230724342574		[learning rate: 0.0015064]
	Learning Rate: 0.00150642
	LOSS [training: 0.022612333766469865 | validation: 0.00867006821814058]
	TIME [epoch: 8.17 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026055208608223017		[learning rate: 0.0015046]
		[batch 20/20] avg loss: 0.011970504490223865		[learning rate: 0.0015028]
	Learning Rate: 0.00150278
	LOSS [training: 0.01901285654922344 | validation: 0.005110871700686068]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_882.pth
	Model improved!!!
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021420687414204363		[learning rate: 0.001501]
		[batch 20/20] avg loss: 0.0241113377286599		[learning rate: 0.0014991]
	Learning Rate: 0.00149914
	LOSS [training: 0.022766012571432133 | validation: 0.012250588986505058]
	TIME [epoch: 8.2 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010050689606828405		[learning rate: 0.0014973]
		[batch 20/20] avg loss: 0.02721358016157837		[learning rate: 0.0014955]
	Learning Rate: 0.00149551
	LOSS [training: 0.018632134884203393 | validation: 0.015024923086926186]
	TIME [epoch: 8.18 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015202714377074494		[learning rate: 0.0014937]
		[batch 20/20] avg loss: 0.023035419954860945		[learning rate: 0.0014919]
	Learning Rate: 0.00149189
	LOSS [training: 0.01911906716596772 | validation: 0.024471321987281525]
	TIME [epoch: 8.17 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025528376159899148		[learning rate: 0.0014901]
		[batch 20/20] avg loss: 0.023446935938948914		[learning rate: 0.0014883]
	Learning Rate: 0.00148828
	LOSS [training: 0.02448765604942403 | validation: 0.012586751033150254]
	TIME [epoch: 8.17 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016722052021087768		[learning rate: 0.0014865]
		[batch 20/20] avg loss: 0.03002741704507207		[learning rate: 0.0014847]
	Learning Rate: 0.00148468
	LOSS [training: 0.02337473453307992 | validation: 0.015693788907746975]
	TIME [epoch: 8.18 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025358932880299896		[learning rate: 0.0014829]
		[batch 20/20] avg loss: 0.037331363348852856		[learning rate: 0.0014811]
	Learning Rate: 0.00148108
	LOSS [training: 0.03134514811457638 | validation: 0.03473719031088799]
	TIME [epoch: 8.19 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02773408442063236		[learning rate: 0.0014793]
		[batch 20/20] avg loss: 0.014534052420564991		[learning rate: 0.0014775]
	Learning Rate: 0.0014775
	LOSS [training: 0.02113406842059868 | validation: 0.021890477732251508]
	TIME [epoch: 8.17 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016196729168943904		[learning rate: 0.0014757]
		[batch 20/20] avg loss: 0.011546805105652503		[learning rate: 0.0014739]
	Learning Rate: 0.00147392
	LOSS [training: 0.013871767137298202 | validation: 0.009125361904347404]
	TIME [epoch: 8.2 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02925474867318601		[learning rate: 0.0014721]
		[batch 20/20] avg loss: 0.025096877692988538		[learning rate: 0.0014704]
	Learning Rate: 0.00147035
	LOSS [training: 0.02717581318308727 | validation: 0.03490460572157217]
	TIME [epoch: 8.16 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02388707036679113		[learning rate: 0.0014686]
		[batch 20/20] avg loss: 0.026875630271660296		[learning rate: 0.0014668]
	Learning Rate: 0.00146679
	LOSS [training: 0.02538135031922571 | validation: 0.018678619414897336]
	TIME [epoch: 8.19 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02840372264516321		[learning rate: 0.001465]
		[batch 20/20] avg loss: 0.023773582492338484		[learning rate: 0.0014632]
	Learning Rate: 0.00146324
	LOSS [training: 0.02608865256875085 | validation: 0.0474581058312113]
	TIME [epoch: 8.17 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03236196465432036		[learning rate: 0.0014615]
		[batch 20/20] avg loss: 0.029186304278165633		[learning rate: 0.0014597]
	Learning Rate: 0.0014597
	LOSS [training: 0.030774134466243004 | validation: 0.01814399252200786]
	TIME [epoch: 8.17 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020188272308244805		[learning rate: 0.0014579]
		[batch 20/20] avg loss: 0.007241368670049561		[learning rate: 0.0014562]
	Learning Rate: 0.00145616
	LOSS [training: 0.013714820489147184 | validation: 0.023385946331089723]
	TIME [epoch: 8.17 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016417777004864357		[learning rate: 0.0014544]
		[batch 20/20] avg loss: 0.02406995279276134		[learning rate: 0.0014526]
	Learning Rate: 0.00145264
	LOSS [training: 0.02024386489881285 | validation: 0.024902253736359497]
	TIME [epoch: 8.19 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017932851789214027		[learning rate: 0.0014509]
		[batch 20/20] avg loss: 0.01896933089475062		[learning rate: 0.0014491]
	Learning Rate: 0.00144912
	LOSS [training: 0.018451091341982327 | validation: 0.010488428714516287]
	TIME [epoch: 8.17 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029658976644939744		[learning rate: 0.0014474]
		[batch 20/20] avg loss: 0.03474202401328476		[learning rate: 0.0014456]
	Learning Rate: 0.00144562
	LOSS [training: 0.03220050032911226 | validation: 0.0202813185303609]
	TIME [epoch: 8.21 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029201480586857165		[learning rate: 0.0014439]
		[batch 20/20] avg loss: 0.042513276105267984		[learning rate: 0.0014421]
	Learning Rate: 0.00144212
	LOSS [training: 0.03585737834606258 | validation: 0.02427888764985931]
	TIME [epoch: 8.16 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02955039694946158		[learning rate: 0.0014404]
		[batch 20/20] avg loss: 0.022239195365363413		[learning rate: 0.0014386]
	Learning Rate: 0.00143862
	LOSS [training: 0.025894796157412497 | validation: 0.02707689238078711]
	TIME [epoch: 8.19 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01300600048463894		[learning rate: 0.0014369]
		[batch 20/20] avg loss: 0.029713365529552456		[learning rate: 0.0014351]
	Learning Rate: 0.00143514
	LOSS [training: 0.021359683007095696 | validation: 0.013312211455692894]
	TIME [epoch: 8.19 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021728165961596775		[learning rate: 0.0014334]
		[batch 20/20] avg loss: 0.026973083481089205		[learning rate: 0.0014317]
	Learning Rate: 0.00143167
	LOSS [training: 0.02435062472134299 | validation: 0.058964360538282856]
	TIME [epoch: 8.16 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022765930395489298		[learning rate: 0.0014299]
		[batch 20/20] avg loss: 0.024283965201572572		[learning rate: 0.0014282]
	Learning Rate: 0.0014282
	LOSS [training: 0.023524947798530933 | validation: 0.034109353901075626]
	TIME [epoch: 8.17 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028318461183397387		[learning rate: 0.0014265]
		[batch 20/20] avg loss: 0.027290310746973828		[learning rate: 0.0014247]
	Learning Rate: 0.00142474
	LOSS [training: 0.027804385965185606 | validation: 0.013523514466519783]
	TIME [epoch: 8.21 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016332708727425507		[learning rate: 0.001423]
		[batch 20/20] avg loss: 0.022026204103206114		[learning rate: 0.0014213]
	Learning Rate: 0.00142129
	LOSS [training: 0.01917945641531581 | validation: 0.02647778494539297]
	TIME [epoch: 8.17 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026329033007811198		[learning rate: 0.0014196]
		[batch 20/20] avg loss: 0.02643030212249146		[learning rate: 0.0014179]
	Learning Rate: 0.00141785
	LOSS [training: 0.02637966756515133 | validation: 0.012812550220616975]
	TIME [epoch: 8.17 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01923775245114561		[learning rate: 0.0014161]
		[batch 20/20] avg loss: 0.012056571397378367		[learning rate: 0.0014144]
	Learning Rate: 0.00141442
	LOSS [training: 0.01564716192426199 | validation: 0.009721168553727934]
	TIME [epoch: 8.2 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015575414620191885		[learning rate: 0.0014127]
		[batch 20/20] avg loss: 0.019237464179199647		[learning rate: 0.001411]
	Learning Rate: 0.001411
	LOSS [training: 0.017406439399695765 | validation: 0.02272354427016022]
	TIME [epoch: 8.18 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021793172726632528		[learning rate: 0.0014093]
		[batch 20/20] avg loss: 0.01622170679190872		[learning rate: 0.0014076]
	Learning Rate: 0.00140758
	LOSS [training: 0.019007439759270627 | validation: 0.006115783980489925]
	TIME [epoch: 8.16 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022785999177103198		[learning rate: 0.0014059]
		[batch 20/20] avg loss: 0.016295835615621125		[learning rate: 0.0014042]
	Learning Rate: 0.00140417
	LOSS [training: 0.019540917396362163 | validation: 0.014587473473169823]
	TIME [epoch: 8.16 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02443579985891889		[learning rate: 0.0014025]
		[batch 20/20] avg loss: 0.03256665516024258		[learning rate: 0.0014008]
	Learning Rate: 0.00140078
	LOSS [training: 0.02850122750958074 | validation: 0.009337417566937992]
	TIME [epoch: 8.17 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021088428578658357		[learning rate: 0.0013991]
		[batch 20/20] avg loss: 0.011580843936857065		[learning rate: 0.0013974]
	Learning Rate: 0.00139738
	LOSS [training: 0.01633463625775771 | validation: 0.002372019439950776]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_912.pth
	Model improved!!!
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016225219764289475		[learning rate: 0.0013957]
		[batch 20/20] avg loss: 0.019786877982704204		[learning rate: 0.001394]
	Learning Rate: 0.001394
	LOSS [training: 0.018006048873496838 | validation: 0.019920581416673338]
	TIME [epoch: 8.18 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01847108617929189		[learning rate: 0.0013923]
		[batch 20/20] avg loss: 0.024009490386898032		[learning rate: 0.0013906]
	Learning Rate: 0.00139063
	LOSS [training: 0.02124028828309496 | validation: 0.006198670440732198]
	TIME [epoch: 8.17 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01905447316717087		[learning rate: 0.0013889]
		[batch 20/20] avg loss: 0.015498885778829153		[learning rate: 0.0013873]
	Learning Rate: 0.00138726
	LOSS [training: 0.01727667947300001 | validation: 0.04456914319390724]
	TIME [epoch: 8.21 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024161667400497265		[learning rate: 0.0013856]
		[batch 20/20] avg loss: 0.02585316208367643		[learning rate: 0.0013839]
	Learning Rate: 0.0013839
	LOSS [training: 0.025007414742086842 | validation: 0.021557979216647337]
	TIME [epoch: 8.18 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012545842358180159		[learning rate: 0.0013822]
		[batch 20/20] avg loss: 0.019577914701215844		[learning rate: 0.0013806]
	Learning Rate: 0.00138055
	LOSS [training: 0.016061878529698002 | validation: 0.020904841486759754]
	TIME [epoch: 8.16 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02339260757588204		[learning rate: 0.0013789]
		[batch 20/20] avg loss: 0.011473541844168458		[learning rate: 0.0013772]
	Learning Rate: 0.00137721
	LOSS [training: 0.017433074710025254 | validation: 0.002351798491940718]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_918.pth
	Model improved!!!
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01254923051017077		[learning rate: 0.0013755]
		[batch 20/20] avg loss: 0.014447165705978976		[learning rate: 0.0013739]
	Learning Rate: 0.00137388
	LOSS [training: 0.013498198108074875 | validation: 0.010356895271633432]
	TIME [epoch: 8.19 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02112373674284163		[learning rate: 0.0013722]
		[batch 20/20] avg loss: 0.03417013395463515		[learning rate: 0.0013705]
	Learning Rate: 0.00137055
	LOSS [training: 0.027646935348738393 | validation: 0.012707039995619488]
	TIME [epoch: 8.2 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01844215334903939		[learning rate: 0.0013689]
		[batch 20/20] avg loss: 0.012626159765620601		[learning rate: 0.0013672]
	Learning Rate: 0.00136723
	LOSS [training: 0.015534156557329998 | validation: 0.01733732782081352]
	TIME [epoch: 8.18 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019073397165512845		[learning rate: 0.0013656]
		[batch 20/20] avg loss: 0.02410170286949883		[learning rate: 0.0013639]
	Learning Rate: 0.00136392
	LOSS [training: 0.021587550017505838 | validation: 0.0205562336310052]
	TIME [epoch: 8.17 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026456145374591605		[learning rate: 0.0013623]
		[batch 20/20] avg loss: 0.032873282030878324		[learning rate: 0.0013606]
	Learning Rate: 0.00136062
	LOSS [training: 0.029664713702734963 | validation: 0.010721724433875411]
	TIME [epoch: 8.21 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018497362680320522		[learning rate: 0.001359]
		[batch 20/20] avg loss: 0.03944496776748537		[learning rate: 0.0013573]
	Learning Rate: 0.00135733
	LOSS [training: 0.028971165223902952 | validation: 0.019957077761724704]
	TIME [epoch: 8.19 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019099486494735137		[learning rate: 0.0013557]
		[batch 20/20] avg loss: 0.028512812491484834		[learning rate: 0.001354]
	Learning Rate: 0.00135404
	LOSS [training: 0.023806149493109986 | validation: 0.02983669038888429]
	TIME [epoch: 8.17 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030997868536786738		[learning rate: 0.0013524]
		[batch 20/20] avg loss: 0.028588400900285905		[learning rate: 0.0013508]
	Learning Rate: 0.00135076
	LOSS [training: 0.029793134718536323 | validation: 0.028949973438052393]
	TIME [epoch: 8.17 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015798444213355856		[learning rate: 0.0013491]
		[batch 20/20] avg loss: 0.01042116986406877		[learning rate: 0.0013475]
	Learning Rate: 0.00134749
	LOSS [training: 0.013109807038712312 | validation: 0.0032903281061524227]
	TIME [epoch: 8.16 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018345383280504605		[learning rate: 0.0013459]
		[batch 20/20] avg loss: 0.01336640074291393		[learning rate: 0.0013442]
	Learning Rate: 0.00134423
	LOSS [training: 0.01585589201170927 | validation: 0.01051007896163545]
	TIME [epoch: 8.21 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02400543748291794		[learning rate: 0.0013426]
		[batch 20/20] avg loss: 0.006450074471568675		[learning rate: 0.001341]
	Learning Rate: 0.00134098
	LOSS [training: 0.015227755977243306 | validation: 0.006404702751146535]
	TIME [epoch: 8.18 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013059637845044566		[learning rate: 0.0013394]
		[batch 20/20] avg loss: 0.015691612343238882		[learning rate: 0.0013377]
	Learning Rate: 0.00133773
	LOSS [training: 0.014375625094141725 | validation: -0.0007311126850846968]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_930.pth
	Model improved!!!
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01635686215516719		[learning rate: 0.0013361]
		[batch 20/20] avg loss: 0.01043935473803785		[learning rate: 0.0013345]
	Learning Rate: 0.00133449
	LOSS [training: 0.01339810844660252 | validation: 0.01158101352635494]
	TIME [epoch: 8.2 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018996116145182363		[learning rate: 0.0013329]
		[batch 20/20] avg loss: 0.010970174591620058		[learning rate: 0.0013313]
	Learning Rate: 0.00133126
	LOSS [training: 0.014983145368401207 | validation: 0.019139029882622813]
	TIME [epoch: 8.21 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015761481238834966		[learning rate: 0.0013296]
		[batch 20/20] avg loss: 0.012101842217244457		[learning rate: 0.001328]
	Learning Rate: 0.00132804
	LOSS [training: 0.013931661728039706 | validation: 0.008330037980557582]
	TIME [epoch: 8.17 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011725969693877512		[learning rate: 0.0013264]
		[batch 20/20] avg loss: 0.013567656556179636		[learning rate: 0.0013248]
	Learning Rate: 0.00132482
	LOSS [training: 0.012646813125028574 | validation: 0.009225383816337673]
	TIME [epoch: 8.16 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019969879258097928		[learning rate: 0.0013232]
		[batch 20/20] avg loss: 0.03136102033025536		[learning rate: 0.0013216]
	Learning Rate: 0.00132162
	LOSS [training: 0.025665449794176648 | validation: 0.0016557141559724394]
	TIME [epoch: 8.17 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008062409371726979		[learning rate: 0.00132]
		[batch 20/20] avg loss: 0.02191795380180985		[learning rate: 0.0013184]
	Learning Rate: 0.00131842
	LOSS [training: 0.014990181586768417 | validation: 0.021859297277375025]
	TIME [epoch: 8.19 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016098078104047873		[learning rate: 0.0013168]
		[batch 20/20] avg loss: 0.011089599615422607		[learning rate: 0.0013152]
	Learning Rate: 0.00131522
	LOSS [training: 0.013593838859735238 | validation: 0.007739277232217983]
	TIME [epoch: 8.18 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040658141853349025		[learning rate: 0.0013136]
		[batch 20/20] avg loss: 0.02319964359514982		[learning rate: 0.001312]
	Learning Rate: 0.00131204
	LOSS [training: 0.031928892724249425 | validation: 0.014793095140740623]
	TIME [epoch: 8.17 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02317102337310818		[learning rate: 0.0013105]
		[batch 20/20] avg loss: 0.017890662483922164		[learning rate: 0.0013089]
	Learning Rate: 0.00130886
	LOSS [training: 0.02053084292851517 | validation: 0.00932664233399872]
	TIME [epoch: 8.17 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02339294728051968		[learning rate: 0.0013073]
		[batch 20/20] avg loss: 0.016783025702729902		[learning rate: 0.0013057]
	Learning Rate: 0.0013057
	LOSS [training: 0.02008798649162479 | validation: 0.023628855662291677]
	TIME [epoch: 8.21 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02470997941346989		[learning rate: 0.0013041]
		[batch 20/20] avg loss: 0.01028361064289509		[learning rate: 0.0013025]
	Learning Rate: 0.00130254
	LOSS [training: 0.01749679502818249 | validation: 0.009350141879609156]
	TIME [epoch: 8.19 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014526674201376627		[learning rate: 0.001301]
		[batch 20/20] avg loss: 0.005398828925144248		[learning rate: 0.0012994]
	Learning Rate: 0.00129938
	LOSS [training: 0.009962751563260437 | validation: 0.00024015142158201086]
	TIME [epoch: 8.16 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009234984459139848		[learning rate: 0.0012978]
		[batch 20/20] avg loss: 0.016403779023054982		[learning rate: 0.0012962]
	Learning Rate: 0.00129624
	LOSS [training: 0.012819381741097414 | validation: 0.014006065965870774]
	TIME [epoch: 8.17 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01677234941588058		[learning rate: 0.0012947]
		[batch 20/20] avg loss: 0.013541087408262653		[learning rate: 0.0012931]
	Learning Rate: 0.0012931
	LOSS [training: 0.015156718412071616 | validation: -0.0002932670659115881]
	TIME [epoch: 8.19 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011567532123224259		[learning rate: 0.0012915]
		[batch 20/20] avg loss: 0.01008378173771365		[learning rate: 0.00129]
	Learning Rate: 0.00128997
	LOSS [training: 0.010825656930468957 | validation: 0.006298264913093023]
	TIME [epoch: 8.19 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018089304644281223		[learning rate: 0.0012884]
		[batch 20/20] avg loss: 0.0220347909313636		[learning rate: 0.0012868]
	Learning Rate: 0.00128685
	LOSS [training: 0.020062047787822414 | validation: 0.020347728602659418]
	TIME [epoch: 8.17 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030213762074751776		[learning rate: 0.0012853]
		[batch 20/20] avg loss: 0.02303418105645568		[learning rate: 0.0012837]
	Learning Rate: 0.00128373
	LOSS [training: 0.026623971565603727 | validation: 0.0043144559840013405]
	TIME [epoch: 8.2 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014984965095722471		[learning rate: 0.0012822]
		[batch 20/20] avg loss: 0.020813951691845724		[learning rate: 0.0012806]
	Learning Rate: 0.00128062
	LOSS [training: 0.017899458393784096 | validation: 0.0320882918613007]
	TIME [epoch: 8.18 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021922890490191697		[learning rate: 0.0012791]
		[batch 20/20] avg loss: 0.01970166559606089		[learning rate: 0.0012775]
	Learning Rate: 0.00127752
	LOSS [training: 0.02081227804312629 | validation: 0.010084551893975625]
	TIME [epoch: 8.19 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015547866317119857		[learning rate: 0.001276]
		[batch 20/20] avg loss: 0.015665636097373706		[learning rate: 0.0012744]
	Learning Rate: 0.00127443
	LOSS [training: 0.015606751207246783 | validation: 0.012365066638814277]
	TIME [epoch: 8.16 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015647759904984872		[learning rate: 0.0012729]
		[batch 20/20] avg loss: 0.021325982648594782		[learning rate: 0.0012713]
	Learning Rate: 0.00127134
	LOSS [training: 0.01848687127678983 | validation: 0.005083709242348365]
	TIME [epoch: 8.17 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01684730403361168		[learning rate: 0.0012698]
		[batch 20/20] avg loss: 0.03899936010580953		[learning rate: 0.0012683]
	Learning Rate: 0.00126827
	LOSS [training: 0.027923332069710604 | validation: 0.01663173400104392]
	TIME [epoch: 8.19 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02047207173867849		[learning rate: 0.0012667]
		[batch 20/20] avg loss: 0.018695735859453968		[learning rate: 0.0012652]
	Learning Rate: 0.0012652
	LOSS [training: 0.01958390379906623 | validation: 0.020436098799585887]
	TIME [epoch: 8.19 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017180522263564806		[learning rate: 0.0012637]
		[batch 20/20] avg loss: 0.009406835173522778		[learning rate: 0.0012621]
	Learning Rate: 0.00126213
	LOSS [training: 0.013293678718543792 | validation: 0.017646893974360787]
	TIME [epoch: 8.17 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01678861123774849		[learning rate: 0.0012606]
		[batch 20/20] avg loss: 0.018732850866751354		[learning rate: 0.0012591]
	Learning Rate: 0.00125908
	LOSS [training: 0.01776073105224992 | validation: 0.01006124906090506]
	TIME [epoch: 8.2 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01796462395586958		[learning rate: 0.0012576]
		[batch 20/20] avg loss: 0.01681610883144668		[learning rate: 0.001256]
	Learning Rate: 0.00125603
	LOSS [training: 0.01739036639365813 | validation: 0.014517370603433272]
	TIME [epoch: 8.17 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019740031151348723		[learning rate: 0.0012545]
		[batch 20/20] avg loss: 0.012115312722503272		[learning rate: 0.001253]
	Learning Rate: 0.00125299
	LOSS [training: 0.015927671936925996 | validation: 0.01602776706102073]
	TIME [epoch: 8.19 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015954311293191702		[learning rate: 0.0012515]
		[batch 20/20] avg loss: 0.0339807109288783		[learning rate: 0.00125]
	Learning Rate: 0.00124996
	LOSS [training: 0.024967511111035005 | validation: 0.029633261058690838]
	TIME [epoch: 8.16 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028250351131028716		[learning rate: 0.0012484]
		[batch 20/20] avg loss: 0.020752163793424434		[learning rate: 0.0012469]
	Learning Rate: 0.00124693
	LOSS [training: 0.02450125746222658 | validation: 0.04145443596257564]
	TIME [epoch: 8.17 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021016671064276064		[learning rate: 0.0012454]
		[batch 20/20] avg loss: 0.025885088508431336		[learning rate: 0.0012439]
	Learning Rate: 0.00124391
	LOSS [training: 0.023450879786353702 | validation: 0.027985210826305014]
	TIME [epoch: 8.18 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01641486276051899		[learning rate: 0.0012424]
		[batch 20/20] avg loss: 0.016520091711151735		[learning rate: 0.0012409]
	Learning Rate: 0.0012409
	LOSS [training: 0.016467477235835363 | validation: 0.015163835712449836]
	TIME [epoch: 8.2 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01040599483312766		[learning rate: 0.0012394]
		[batch 20/20] avg loss: 0.017845987425171645		[learning rate: 0.0012379]
	Learning Rate: 0.0012379
	LOSS [training: 0.014125991129149654 | validation: 0.014153015373361881]
	TIME [epoch: 8.17 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016634083876918844		[learning rate: 0.0012364]
		[batch 20/20] avg loss: 0.01697494818081991		[learning rate: 0.0012349]
	Learning Rate: 0.0012349
	LOSS [training: 0.016804516028869378 | validation: 0.009462258276780803]
	TIME [epoch: 8.21 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016487425432730858		[learning rate: 0.0012334]
		[batch 20/20] avg loss: 0.019452500865262103		[learning rate: 0.0012319]
	Learning Rate: 0.00123191
	LOSS [training: 0.01796996314899648 | validation: 0.014690499438159744]
	TIME [epoch: 8.17 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02441403242748963		[learning rate: 0.0012304]
		[batch 20/20] avg loss: 0.01632667823489769		[learning rate: 0.0012289]
	Learning Rate: 0.00122893
	LOSS [training: 0.020370355331193662 | validation: 0.023974518602787768]
	TIME [epoch: 8.18 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019394714460828293		[learning rate: 0.0012274]
		[batch 20/20] avg loss: 0.020631166513448505		[learning rate: 0.001226]
	Learning Rate: 0.00122595
	LOSS [training: 0.0200129404871384 | validation: 0.013936009277026512]
	TIME [epoch: 8.16 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011411381988254015		[learning rate: 0.0012245]
		[batch 20/20] avg loss: 0.01354934784372879		[learning rate: 0.001223]
	Learning Rate: 0.00122298
	LOSS [training: 0.012480364915991403 | validation: 0.006864526730035571]
	TIME [epoch: 8.16 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01623087967156285		[learning rate: 0.0012215]
		[batch 20/20] avg loss: 0.01276044051254599		[learning rate: 0.00122]
	Learning Rate: 0.00122002
	LOSS [training: 0.014495660092054422 | validation: 0.011826264206443629]
	TIME [epoch: 8.18 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010045696240690976		[learning rate: 0.0012185]
		[batch 20/20] avg loss: 0.016260697613593313		[learning rate: 0.0012171]
	Learning Rate: 0.00121707
	LOSS [training: 0.013153196927142143 | validation: 0.036149072833994725]
	TIME [epoch: 8.2 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01550687604136694		[learning rate: 0.0012156]
		[batch 20/20] avg loss: 0.02775553287656069		[learning rate: 0.0012141]
	Learning Rate: 0.00121412
	LOSS [training: 0.021631204458963812 | validation: 0.022435818720615883]
	TIME [epoch: 8.17 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017632462058738287		[learning rate: 0.0012127]
		[batch 20/20] avg loss: 0.01894363449472361		[learning rate: 0.0012112]
	Learning Rate: 0.00121119
	LOSS [training: 0.018288048276730952 | validation: 0.03325165242013813]
	TIME [epoch: 8.19 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023122953939789276		[learning rate: 0.0012097]
		[batch 20/20] avg loss: 0.012866930839212722		[learning rate: 0.0012083]
	Learning Rate: 0.00120825
	LOSS [training: 0.017994942389500997 | validation: 0.019789355473904653]
	TIME [epoch: 8.18 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011111892023620912		[learning rate: 0.0012068]
		[batch 20/20] avg loss: 0.014572715092028895		[learning rate: 0.0012053]
	Learning Rate: 0.00120533
	LOSS [training: 0.012842303557824907 | validation: 0.008852940477992989]
	TIME [epoch: 8.19 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012703603239690003		[learning rate: 0.0012039]
		[batch 20/20] avg loss: 0.017356017542649867		[learning rate: 0.0012024]
	Learning Rate: 0.00120241
	LOSS [training: 0.015029810391169934 | validation: 0.022405161136603246]
	TIME [epoch: 8.18 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021823015374747564		[learning rate: 0.001201]
		[batch 20/20] avg loss: 0.010952206068525266		[learning rate: 0.0011995]
	Learning Rate: 0.0011995
	LOSS [training: 0.01638761072163642 | validation: 0.009647036096330726]
	TIME [epoch: 8.18 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02432942829488525		[learning rate: 0.001198]
		[batch 20/20] avg loss: 0.015930875368785742		[learning rate: 0.0011966]
	Learning Rate: 0.0011966
	LOSS [training: 0.020130151831835495 | validation: 0.006308158754227851]
	TIME [epoch: 8.17 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009182854044683065		[learning rate: 0.0011951]
		[batch 20/20] avg loss: 0.019913298150340427		[learning rate: 0.0011937]
	Learning Rate: 0.0011937
	LOSS [training: 0.014548076097511745 | validation: 0.01966994171850364]
	TIME [epoch: 8.19 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010124692171139473		[learning rate: 0.0011923]
		[batch 20/20] avg loss: 0.014930591314043212		[learning rate: 0.0011908]
	Learning Rate: 0.00119081
	LOSS [training: 0.012527641742591345 | validation: 0.009565780373849954]
	TIME [epoch: 8.21 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015420805460124708		[learning rate: 0.0011894]
		[batch 20/20] avg loss: 0.016163989142277117		[learning rate: 0.0011879]
	Learning Rate: 0.00118793
	LOSS [training: 0.01579239730120091 | validation: 0.013254239439328304]
	TIME [epoch: 8.16 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009300701450794075		[learning rate: 0.0011865]
		[batch 20/20] avg loss: 0.01437537872593136		[learning rate: 0.0011851]
	Learning Rate: 0.00118505
	LOSS [training: 0.011838040088362718 | validation: 0.010926382966949574]
	TIME [epoch: 8.16 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008581387078392524		[learning rate: 0.0011836]
		[batch 20/20] avg loss: 0.013458924367022581		[learning rate: 0.0011822]
	Learning Rate: 0.00118218
	LOSS [training: 0.011020155722707554 | validation: 0.018592938884346738]
	TIME [epoch: 8.18 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014111272427697438		[learning rate: 0.0011807]
		[batch 20/20] avg loss: 0.016794384735863657		[learning rate: 0.0011793]
	Learning Rate: 0.00117932
	LOSS [training: 0.015452828581780547 | validation: 0.008348870945090228]
	TIME [epoch: 8.17 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013602063492162308		[learning rate: 0.0011779]
		[batch 20/20] avg loss: 0.02047460240427089		[learning rate: 0.0011765]
	Learning Rate: 0.00117646
	LOSS [training: 0.0170383329482166 | validation: 0.007578492241083738]
	TIME [epoch: 8.2 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0062028858222182275		[learning rate: 0.001175]
		[batch 20/20] avg loss: 0.0282444305267268		[learning rate: 0.0011736]
	Learning Rate: 0.00117362
	LOSS [training: 0.017223658174472512 | validation: 0.06915977000342273]
	TIME [epoch: 8.19 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04352592588161425		[learning rate: 0.0011722]
		[batch 20/20] avg loss: 0.0233238688041779		[learning rate: 0.0011708]
	Learning Rate: 0.00117078
	LOSS [training: 0.033424897342896075 | validation: 0.021932076507702816]
	TIME [epoch: 8.22 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010673052499401764		[learning rate: 0.0011694]
		[batch 20/20] avg loss: 0.015404556984694898		[learning rate: 0.0011679]
	Learning Rate: 0.00116794
	LOSS [training: 0.01303880474204833 | validation: 0.0038701626677574495]
	TIME [epoch: 8.19 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013043245583779357		[learning rate: 0.0011665]
		[batch 20/20] avg loss: 0.013893366839657131		[learning rate: 0.0011651]
	Learning Rate: 0.00116511
	LOSS [training: 0.013468306211718243 | validation: 0.02594090782675627]
	TIME [epoch: 8.18 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01702169615117913		[learning rate: 0.0011637]
		[batch 20/20] avg loss: 0.013397266981218228		[learning rate: 0.0011623]
	Learning Rate: 0.00116229
	LOSS [training: 0.015209481566198677 | validation: 0.012041462402493291]
	TIME [epoch: 8.18 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015782834857823325		[learning rate: 0.0011609]
		[batch 20/20] avg loss: 0.01261686405716185		[learning rate: 0.0011595]
	Learning Rate: 0.00115948
	LOSS [training: 0.014199849457492583 | validation: 0.014341930307171801]
	TIME [epoch: 8.22 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015245688142850785		[learning rate: 0.0011581]
		[batch 20/20] avg loss: 0.029254315129236763		[learning rate: 0.0011567]
	Learning Rate: 0.00115667
	LOSS [training: 0.02225000163604377 | validation: 0.01870728489209213]
	TIME [epoch: 8.18 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014039040872973252		[learning rate: 0.0011553]
		[batch 20/20] avg loss: 0.014111256476442124		[learning rate: 0.0011539]
	Learning Rate: 0.00115387
	LOSS [training: 0.014075148674707692 | validation: 0.018408702986637472]
	TIME [epoch: 8.17 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01826732980800292		[learning rate: 0.0011525]
		[batch 20/20] avg loss: 0.015863102896788633		[learning rate: 0.0011511]
	Learning Rate: 0.00115108
	LOSS [training: 0.01706521635239578 | validation: 0.025143816728099275]
	TIME [epoch: 8.2 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02103924523896582		[learning rate: 0.0011497]
		[batch 20/20] avg loss: 0.015736227960188087		[learning rate: 0.0011483]
	Learning Rate: 0.00114829
	LOSS [training: 0.018387736599576952 | validation: 0.0424978534121047]
	TIME [epoch: 8.21 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024260319189809247		[learning rate: 0.0011469]
		[batch 20/20] avg loss: 0.02436465149085143		[learning rate: 0.0011455]
	Learning Rate: 0.00114551
	LOSS [training: 0.024312485340330335 | validation: 0.007915825839649799]
	TIME [epoch: 8.17 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013164095581067296		[learning rate: 0.0011441]
		[batch 20/20] avg loss: 0.017740699003291914		[learning rate: 0.0011427]
	Learning Rate: 0.00114274
	LOSS [training: 0.015452397292179601 | validation: 0.001680980071276064]
	TIME [epoch: 8.16 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01572953915703483		[learning rate: 0.0011414]
		[batch 20/20] avg loss: 0.01303545123598437		[learning rate: 0.00114]
	Learning Rate: 0.00113997
	LOSS [training: 0.0143824951965096 | validation: 0.031327313053585805]
	TIME [epoch: 8.16 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03843139866856726		[learning rate: 0.0011386]
		[batch 20/20] avg loss: 0.018837606585895717		[learning rate: 0.0011372]
	Learning Rate: 0.00113721
	LOSS [training: 0.028634502627231488 | validation: 0.016894485315226276]
	TIME [epoch: 8.18 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014921592857615204		[learning rate: 0.0011358]
		[batch 20/20] avg loss: 0.014608471138276801		[learning rate: 0.0011345]
	Learning Rate: 0.00113446
	LOSS [training: 0.014765031997946005 | validation: 0.01570560250569997]
	TIME [epoch: 8.17 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01549158751405938		[learning rate: 0.0011331]
		[batch 20/20] avg loss: 0.015918926286021158		[learning rate: 0.0011317]
	Learning Rate: 0.00113171
	LOSS [training: 0.015705256900040265 | validation: 0.006811183153833876]
	TIME [epoch: 8.16 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011898129781658206		[learning rate: 0.0011303]
		[batch 20/20] avg loss: 0.012775625541712207		[learning rate: 0.001129]
	Learning Rate: 0.00112897
	LOSS [training: 0.012336877661685206 | validation: 0.00321893290907207]
	TIME [epoch: 8.19 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010365624306428374		[learning rate: 0.0011276]
		[batch 20/20] avg loss: 0.016259269298422235		[learning rate: 0.0011262]
	Learning Rate: 0.00112624
	LOSS [training: 0.013312446802425304 | validation: 0.010977043328776002]
	TIME [epoch: 8.2 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017244689931863587		[learning rate: 0.0011249]
		[batch 20/20] avg loss: 0.013683852826535106		[learning rate: 0.0011235]
	Learning Rate: 0.00112352
	LOSS [training: 0.015464271379199345 | validation: 0.016869297940850583]
	TIME [epoch: 8.18 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010359815130785975		[learning rate: 0.0011222]
		[batch 20/20] avg loss: 0.010303001946789599		[learning rate: 0.0011208]
	Learning Rate: 0.0011208
	LOSS [training: 0.010331408538787789 | validation: 0.006921821235266098]
	TIME [epoch: 8.17 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007698713010678819		[learning rate: 0.0011194]
		[batch 20/20] avg loss: 0.018205351748228137		[learning rate: 0.0011181]
	Learning Rate: 0.00111808
	LOSS [training: 0.01295203237945348 | validation: 0.0030479633691634796]
	TIME [epoch: 8.19 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017126936804196684		[learning rate: 0.0011167]
		[batch 20/20] avg loss: 0.020687979347058873		[learning rate: 0.0011154]
	Learning Rate: 0.00111538
	LOSS [training: 0.018907458075627777 | validation: 0.006058225706530341]
	TIME [epoch: 8.21 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018335773048468004		[learning rate: 0.001114]
		[batch 20/20] avg loss: 0.016684672834918222		[learning rate: 0.0011127]
	Learning Rate: 0.00111268
	LOSS [training: 0.01751022294169311 | validation: 0.026809094825431055]
	TIME [epoch: 8.16 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015042114911486859		[learning rate: 0.0011113]
		[batch 20/20] avg loss: 0.009803747743879267		[learning rate: 0.00111]
	Learning Rate: 0.00110998
	LOSS [training: 0.012422931327683061 | validation: -0.0005862209676333529]
	TIME [epoch: 8.17 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011023230115833607		[learning rate: 0.0011086]
		[batch 20/20] avg loss: 0.018753394963805507		[learning rate: 0.0011073]
	Learning Rate: 0.00110729
	LOSS [training: 0.01488831253981956 | validation: 0.014875343769451793]
	TIME [epoch: 8.16 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009815496409197958		[learning rate: 0.001106]
		[batch 20/20] avg loss: 0.013674725989262976		[learning rate: 0.0011046]
	Learning Rate: 0.00110461
	LOSS [training: 0.01174511119923047 | validation: 0.012004379481300505]
	TIME [epoch: 8.18 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025484104964141135		[learning rate: 0.0011033]
		[batch 20/20] avg loss: 0.024190757588814574		[learning rate: 0.0011019]
	Learning Rate: 0.00110194
	LOSS [training: 0.024837431276477858 | validation: 0.01655711205294731]
	TIME [epoch: 8.17 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012447079027594997		[learning rate: 0.0011006]
		[batch 20/20] avg loss: 0.01420112543152929		[learning rate: 0.0010993]
	Learning Rate: 0.00109927
	LOSS [training: 0.013324102229562146 | validation: 0.022379808025360028]
	TIME [epoch: 8.2 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012119546972551213		[learning rate: 0.0010979]
		[batch 20/20] avg loss: 0.016138062519557968		[learning rate: 0.0010966]
	Learning Rate: 0.00109661
	LOSS [training: 0.014128804746054591 | validation: 0.008891875353756865]
	TIME [epoch: 8.17 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021227915657167695		[learning rate: 0.0010953]
		[batch 20/20] avg loss: 0.009627767484168694		[learning rate: 0.001094]
	Learning Rate: 0.00109396
	LOSS [training: 0.015427841570668194 | validation: 0.006646312350932135]
	TIME [epoch: 8.19 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008406791994152885		[learning rate: 0.0010926]
		[batch 20/20] avg loss: 0.02029903448433518		[learning rate: 0.0010913]
	Learning Rate: 0.00109131
	LOSS [training: 0.014352913239244033 | validation: 0.02435527053883864]
	TIME [epoch: 8.18 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014912987297172709		[learning rate: 0.00109]
		[batch 20/20] avg loss: 0.024818527933693043		[learning rate: 0.0010887]
	Learning Rate: 0.00108867
	LOSS [training: 0.019865757615432873 | validation: 0.014900109298258326]
	TIME [epoch: 8.21 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0175044317043448		[learning rate: 0.0010873]
		[batch 20/20] avg loss: 0.030174080973320576		[learning rate: 0.001086]
	Learning Rate: 0.00108603
	LOSS [training: 0.023839256338832687 | validation: 0.01694299781020048]
	TIME [epoch: 8.17 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006637933694806935		[learning rate: 0.0010847]
		[batch 20/20] avg loss: 0.013161605638726795		[learning rate: 0.0010834]
	Learning Rate: 0.0010834
	LOSS [training: 0.009899769666766863 | validation: 0.011038832990910782]
	TIME [epoch: 8.17 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011118135287073212		[learning rate: 0.0010821]
		[batch 20/20] avg loss: 0.014051208555936731		[learning rate: 0.0010808]
	Learning Rate: 0.00108078
	LOSS [training: 0.012584671921504972 | validation: 0.0059946051234519345]
	TIME [epoch: 8.19 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025563622166449872		[learning rate: 0.0010795]
		[batch 20/20] avg loss: 0.026148846321604264		[learning rate: 0.0010782]
	Learning Rate: 0.00107816
	LOSS [training: 0.02585623424402707 | validation: 0.015255979712857928]
	TIME [epoch: 8.17 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027407652364910072		[learning rate: 0.0010769]
		[batch 20/20] avg loss: 0.01986590651484337		[learning rate: 0.0010756]
	Learning Rate: 0.00107555
	LOSS [training: 0.023636779439876722 | validation: 0.008024341594784377]
	TIME [epoch: 8.17 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011378581191179914		[learning rate: 0.0010742]
		[batch 20/20] avg loss: 0.00980950502504071		[learning rate: 0.0010729]
	Learning Rate: 0.00107295
	LOSS [training: 0.01059404310811031 | validation: 0.015032979453098528]
	TIME [epoch: 8.19 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0185113661250548		[learning rate: 0.0010716]
		[batch 20/20] avg loss: 0.009592707163581012		[learning rate: 0.0010704]
	Learning Rate: 0.00107035
	LOSS [training: 0.014052036644317905 | validation: 0.006435054088794421]
	TIME [epoch: 8.2 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01766879479031774		[learning rate: 0.0010691]
		[batch 20/20] avg loss: 0.0159853128134932		[learning rate: 0.0010678]
	Learning Rate: 0.00106776
	LOSS [training: 0.016827053801905474 | validation: 0.012956199835338522]
	TIME [epoch: 8.18 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01838278333731887		[learning rate: 0.0010665]
		[batch 20/20] avg loss: 0.029751965581690666		[learning rate: 0.0010652]
	Learning Rate: 0.00106518
	LOSS [training: 0.02406737445950477 | validation: 0.021103557138707922]
	TIME [epoch: 8.18 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018451073532322677		[learning rate: 0.0010639]
		[batch 20/20] avg loss: 0.01280098127485933		[learning rate: 0.0010626]
	Learning Rate: 0.0010626
	LOSS [training: 0.015626027403591005 | validation: 0.008890945462871053]
	TIME [epoch: 8.21 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013098080893129619		[learning rate: 0.0010613]
		[batch 20/20] avg loss: 0.013998107377663433		[learning rate: 0.00106]
	Learning Rate: 0.00106002
	LOSS [training: 0.013548094135396527 | validation: 0.011707221319927857]
	TIME [epoch: 8.19 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014038351605086674		[learning rate: 0.0010587]
		[batch 20/20] avg loss: 0.0171050849925158		[learning rate: 0.0010575]
	Learning Rate: 0.00105746
	LOSS [training: 0.015571718298801232 | validation: 0.014981669858728786]
	TIME [epoch: 8.17 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025959699513771417		[learning rate: 0.0010562]
		[batch 20/20] avg loss: 0.011792149549312698		[learning rate: 0.0010549]
	Learning Rate: 0.0010549
	LOSS [training: 0.018875924531542056 | validation: 0.001680577945955935]
	TIME [epoch: 8.17 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012558346440013163		[learning rate: 0.0010536]
		[batch 20/20] avg loss: 0.018245239819212566		[learning rate: 0.0010523]
	Learning Rate: 0.00105234
	LOSS [training: 0.015401793129612868 | validation: 0.014588033288148469]
	TIME [epoch: 8.17 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030310159317375424		[learning rate: 0.0010511]
		[batch 20/20] avg loss: 0.008642530935744812		[learning rate: 0.0010498]
	Learning Rate: 0.0010498
	LOSS [training: 0.01947634512656012 | validation: 0.006648950363160411]
	TIME [epoch: 8.2 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010856404248611454		[learning rate: 0.0010485]
		[batch 20/20] avg loss: 0.010317094841507671		[learning rate: 0.0010473]
	Learning Rate: 0.00104726
	LOSS [training: 0.010586749545059565 | validation: 0.03006196082337924]
	TIME [epoch: 8.19 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027473748708516425		[learning rate: 0.001046]
		[batch 20/20] avg loss: 0.017929350011584834		[learning rate: 0.0010447]
	Learning Rate: 0.00104472
	LOSS [training: 0.02270154936005063 | validation: 0.0095425058506803]
	TIME [epoch: 8.18 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010403484700580664		[learning rate: 0.0010435]
		[batch 20/20] avg loss: 0.012162165691872964		[learning rate: 0.0010422]
	Learning Rate: 0.00104219
	LOSS [training: 0.011282825196226814 | validation: 0.010484971773971598]
	TIME [epoch: 8.18 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010045124376108196		[learning rate: 0.0010409]
		[batch 20/20] avg loss: 0.027195051228567572		[learning rate: 0.0010397]
	Learning Rate: 0.00103967
	LOSS [training: 0.018620087802337888 | validation: 0.005417893189228416]
	TIME [epoch: 8.22 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01695147682101318		[learning rate: 0.0010384]
		[batch 20/20] avg loss: 0.013009401865370354		[learning rate: 0.0010372]
	Learning Rate: 0.00103715
	LOSS [training: 0.014980439343191768 | validation: 0.01020558626822089]
	TIME [epoch: 8.2 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013059890557787135		[learning rate: 0.0010359]
		[batch 20/20] avg loss: 0.010544973935791282		[learning rate: 0.0010346]
	Learning Rate: 0.00103464
	LOSS [training: 0.011802432246789207 | validation: 0.025649138610000906]
	TIME [epoch: 8.17 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017427518292089952		[learning rate: 0.0010334]
		[batch 20/20] avg loss: 0.010808185044435899		[learning rate: 0.0010321]
	Learning Rate: 0.00103214
	LOSS [training: 0.014117851668262928 | validation: 0.012488986913096758]
	TIME [epoch: 8.17 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010315908161520452		[learning rate: 0.0010309]
		[batch 20/20] avg loss: 0.019295825179885394		[learning rate: 0.0010296]
	Learning Rate: 0.00102964
	LOSS [training: 0.014805866670702921 | validation: 0.020252971305301893]
	TIME [epoch: 8.19 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008439943949560939		[learning rate: 0.0010284]
		[batch 20/20] avg loss: 0.018267952736580755		[learning rate: 0.0010271]
	Learning Rate: 0.00102714
	LOSS [training: 0.013353948343070845 | validation: 0.012910756182350224]
	TIME [epoch: 8.17 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015578483928964496		[learning rate: 0.0010259]
		[batch 20/20] avg loss: 0.0242584130459612		[learning rate: 0.0010247]
	Learning Rate: 0.00102466
	LOSS [training: 0.019918448487462846 | validation: 0.011420000591271729]
	TIME [epoch: 8.2 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01218733328960705		[learning rate: 0.0010234]
		[batch 20/20] avg loss: 0.008889398567636306		[learning rate: 0.0010222]
	Learning Rate: 0.00102218
	LOSS [training: 0.010538365928621679 | validation: 0.009148525589889071]
	TIME [epoch: 8.18 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010827794255541468		[learning rate: 0.0010209]
		[batch 20/20] avg loss: 0.010836376119659728		[learning rate: 0.0010197]
	Learning Rate: 0.0010197
	LOSS [training: 0.010832085187600596 | validation: 0.013294685548414977]
	TIME [epoch: 8.19 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009812212039409874		[learning rate: 0.0010185]
		[batch 20/20] avg loss: 0.014914668770089828		[learning rate: 0.0010172]
	Learning Rate: 0.00101723
	LOSS [training: 0.012363440404749852 | validation: 0.009836501698942743]
	TIME [epoch: 8.2 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015509900200985059		[learning rate: 0.001016]
		[batch 20/20] avg loss: 0.022210300741337913		[learning rate: 0.0010148]
	Learning Rate: 0.00101477
	LOSS [training: 0.018860100471161486 | validation: 0.025536213948627498]
	TIME [epoch: 8.19 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024839155426147896		[learning rate: 0.0010135]
		[batch 20/20] avg loss: 0.01850135484951637		[learning rate: 0.0010123]
	Learning Rate: 0.00101232
	LOSS [training: 0.021670255137832134 | validation: 0.02157760500612457]
	TIME [epoch: 8.17 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033762024274751924		[learning rate: 0.0010111]
		[batch 20/20] avg loss: 0.01700886948355064		[learning rate: 0.0010099]
	Learning Rate: 0.00100986
	LOSS [training: 0.025385446879151275 | validation: 0.01982555001546392]
	TIME [epoch: 8.19 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03362102004149183		[learning rate: 0.0010086]
		[batch 20/20] avg loss: 0.017311498266946914		[learning rate: 0.0010074]
	Learning Rate: 0.00100742
	LOSS [training: 0.02546625915421938 | validation: 0.02819721594542377]
	TIME [epoch: 8.17 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020759048464529962		[learning rate: 0.0010062]
		[batch 20/20] avg loss: 0.009133679278238601		[learning rate: 0.001005]
	Learning Rate: 0.00100498
	LOSS [training: 0.01494636387138428 | validation: 0.0005060981220930136]
	TIME [epoch: 8.17 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010137828453137038		[learning rate: 0.0010038]
		[batch 20/20] avg loss: 0.02126246115543099		[learning rate: 0.0010025]
	Learning Rate: 0.00100255
	LOSS [training: 0.015700144804284017 | validation: 0.011301994957029283]
	TIME [epoch: 8.19 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019372052789975967		[learning rate: 0.0010013]
		[batch 20/20] avg loss: 0.01867944360696533		[learning rate: 0.0010001]
	Learning Rate: 0.00100012
	LOSS [training: 0.01902574819847065 | validation: 0.010370763840116856]
	TIME [epoch: 8.22 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021787446881777337		[learning rate: 0.00099891]
		[batch 20/20] avg loss: 0.014778883133535406		[learning rate: 0.0009977]
	Learning Rate: 0.0009977
	LOSS [training: 0.01828316500765637 | validation: 0.007058730951116012]
	TIME [epoch: 8.18 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011119673281364177		[learning rate: 0.00099649]
		[batch 20/20] avg loss: 0.02100845167048935		[learning rate: 0.00099528]
	Learning Rate: 0.000995285
	LOSS [training: 0.016064062475926757 | validation: 0.00825087130274125]
	TIME [epoch: 8.18 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014799446229514058		[learning rate: 0.00099408]
		[batch 20/20] avg loss: 0.01804546676564402		[learning rate: 0.00099288]
	Learning Rate: 0.000992875
	LOSS [training: 0.016422456497579042 | validation: 0.014135930956982233]
	TIME [epoch: 8.2 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02481963623931495		[learning rate: 0.00099167]
		[batch 20/20] avg loss: 0.018452554292964045		[learning rate: 0.00099047]
	Learning Rate: 0.000990472
	LOSS [training: 0.0216360952661395 | validation: 0.017119591971914472]
	TIME [epoch: 8.23 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011878098294609936		[learning rate: 0.00098927]
		[batch 20/20] avg loss: 0.021072383974127185		[learning rate: 0.00098807]
	Learning Rate: 0.000988074
	LOSS [training: 0.016475241134368563 | validation: 0.02838230597920048]
	TIME [epoch: 8.17 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01950613624138929		[learning rate: 0.00098688]
		[batch 20/20] avg loss: 0.021488425895056607		[learning rate: 0.00098568]
	Learning Rate: 0.000985682
	LOSS [training: 0.02049728106822295 | validation: 0.01659137983586812]
	TIME [epoch: 8.17 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02567220426390384		[learning rate: 0.00098449]
		[batch 20/20] avg loss: 0.02356805712543466		[learning rate: 0.0009833]
	Learning Rate: 0.000983296
	LOSS [training: 0.024620130694669253 | validation: 0.029852563491008913]
	TIME [epoch: 8.17 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02650324192262078		[learning rate: 0.0009821]
		[batch 20/20] avg loss: 0.01846443419007757		[learning rate: 0.00098092]
	Learning Rate: 0.000980916
	LOSS [training: 0.022483838056349174 | validation: 0.02075012176253504]
	TIME [epoch: 8.19 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011373415645070361		[learning rate: 0.00097973]
		[batch 20/20] avg loss: 0.016795697496258366		[learning rate: 0.00097854]
	Learning Rate: 0.000978541
	LOSS [training: 0.014084556570664366 | validation: 0.009485562893609092]
	TIME [epoch: 8.21 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012789739434755126		[learning rate: 0.00097736]
		[batch 20/20] avg loss: 0.009570404927414915		[learning rate: 0.00097617]
	Learning Rate: 0.000976172
	LOSS [training: 0.01118007218108502 | validation: 0.00595626607411492]
	TIME [epoch: 8.18 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01843575993284015		[learning rate: 0.00097499]
		[batch 20/20] avg loss: 0.013844869273527391		[learning rate: 0.00097381]
	Learning Rate: 0.000973809
	LOSS [training: 0.01614031460318377 | validation: 0.010416125880737516]
	TIME [epoch: 8.18 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015902065515080166		[learning rate: 0.00097263]
		[batch 20/20] avg loss: 0.010593304690415854		[learning rate: 0.00097145]
	Learning Rate: 0.000971451
	LOSS [training: 0.013247685102748008 | validation: 0.01705966663995405]
	TIME [epoch: 8.2 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034562553497213946		[learning rate: 0.00097027]
		[batch 20/20] avg loss: 0.028460920593839277		[learning rate: 0.0009691]
	Learning Rate: 0.0009691
	LOSS [training: 0.03151173704552661 | validation: 0.0198858193917422]
	TIME [epoch: 8.2 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0145295443025077		[learning rate: 0.00096793]
		[batch 20/20] avg loss: 0.009655694062271422		[learning rate: 0.00096675]
	Learning Rate: 0.000966754
	LOSS [training: 0.01209261918238956 | validation: 0.012883483667781385]
	TIME [epoch: 8.19 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01535979633778779		[learning rate: 0.00096558]
		[batch 20/20] avg loss: 0.008621415616190538		[learning rate: 0.00096441]
	Learning Rate: 0.000964413
	LOSS [training: 0.011990605976989163 | validation: 0.010394901337606592]
	TIME [epoch: 8.17 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011468938375570618		[learning rate: 0.00096325]
		[batch 20/20] avg loss: 0.022492175746298797		[learning rate: 0.00096208]
	Learning Rate: 0.000962079
	LOSS [training: 0.01698055706093471 | validation: 0.012900085295024058]
	TIME [epoch: 8.19 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011384748134065203		[learning rate: 0.00096091]
		[batch 20/20] avg loss: 0.0186136491914174		[learning rate: 0.00095975]
	Learning Rate: 0.00095975
	LOSS [training: 0.014999198662741306 | validation: 0.014435717699418373]
	TIME [epoch: 8.17 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01799159027686346		[learning rate: 0.00095859]
		[batch 20/20] avg loss: 0.010918165245840331		[learning rate: 0.00095743]
	Learning Rate: 0.000957426
	LOSS [training: 0.014454877761351898 | validation: 0.009939021207011369]
	TIME [epoch: 8.17 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0066647165186774285		[learning rate: 0.00095627]
		[batch 20/20] avg loss: 0.011449133129476366		[learning rate: 0.00095511]
	Learning Rate: 0.000955108
	LOSS [training: 0.009056924824076896 | validation: 0.006742624570170286]
	TIME [epoch: 8.2 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01040732122388518		[learning rate: 0.00095395]
		[batch 20/20] avg loss: 0.008804658112444047		[learning rate: 0.0009528]
	Learning Rate: 0.000952796
	LOSS [training: 0.009605989668164613 | validation: 0.005865054305066207]
	TIME [epoch: 8.2 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007964531583836497		[learning rate: 0.00095164]
		[batch 20/20] avg loss: 0.007976197839578314		[learning rate: 0.00095049]
	Learning Rate: 0.00095049
	LOSS [training: 0.007970364711707405 | validation: 0.010965031148821424]
	TIME [epoch: 8.18 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01430673516905695		[learning rate: 0.00094934]
		[batch 20/20] avg loss: 0.009923152882654319		[learning rate: 0.00094819]
	Learning Rate: 0.000948189
	LOSS [training: 0.012114944025855634 | validation: 0.009393087263954006]
	TIME [epoch: 8.18 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013084531708283054		[learning rate: 0.00094704]
		[batch 20/20] avg loss: 0.009516853342512743		[learning rate: 0.00094589]
	Learning Rate: 0.000945893
	LOSS [training: 0.011300692525397898 | validation: 0.004824515679354192]
	TIME [epoch: 8.21 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004216110979917528		[learning rate: 0.00094475]
		[batch 20/20] avg loss: 0.014266237302253262		[learning rate: 0.0009436]
	Learning Rate: 0.000943603
	LOSS [training: 0.009241174141085395 | validation: 0.010850581293555884]
	TIME [epoch: 8.18 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020288229422230547		[learning rate: 0.00094246]
		[batch 20/20] avg loss: 0.0189423763984217		[learning rate: 0.00094132]
	Learning Rate: 0.000941319
	LOSS [training: 0.01961530291032612 | validation: 0.005515175626398732]
	TIME [epoch: 8.17 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009202805489450749		[learning rate: 0.00094018]
		[batch 20/20] avg loss: 0.010589030695155405		[learning rate: 0.00093904]
	Learning Rate: 0.00093904
	LOSS [training: 0.009895918092303077 | validation: 0.005211895940798284]
	TIME [epoch: 8.21 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01301212755046953		[learning rate: 0.0009379]
		[batch 20/20] avg loss: 0.017128909336920954		[learning rate: 0.00093677]
	Learning Rate: 0.000936767
	LOSS [training: 0.015070518443695244 | validation: -0.0017286723495958225]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_1077.pth
	Model improved!!!
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010101614656105071		[learning rate: 0.00093563]
		[batch 20/20] avg loss: 0.005050529823671659		[learning rate: 0.0009345]
	Learning Rate: 0.000934499
	LOSS [training: 0.007576072239888364 | validation: 0.003729974451818497]
	TIME [epoch: 8.2 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010023644162859125		[learning rate: 0.00093337]
		[batch 20/20] avg loss: 0.019952883895401556		[learning rate: 0.00093224]
	Learning Rate: 0.000932237
	LOSS [training: 0.014988264029130342 | validation: 0.02656400019467712]
	TIME [epoch: 8.17 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019827598946795207		[learning rate: 0.00093111]
		[batch 20/20] avg loss: 0.010051552097415078		[learning rate: 0.00092998]
	Learning Rate: 0.00092998
	LOSS [training: 0.014939575522105139 | validation: -0.00046115179210752966]
	TIME [epoch: 8.19 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012779042348824862		[learning rate: 0.00092885]
		[batch 20/20] avg loss: 0.01904396231361914		[learning rate: 0.00092773]
	Learning Rate: 0.000927729
	LOSS [training: 0.015911502331222 | validation: 0.013335028182039431]
	TIME [epoch: 8.19 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015243609432862442		[learning rate: 0.00092661]
		[batch 20/20] avg loss: 0.016397623732484996		[learning rate: 0.00092548]
	Learning Rate: 0.000925483
	LOSS [training: 0.015820616582673717 | validation: 0.022645226771525053]
	TIME [epoch: 8.19 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011886330231128103		[learning rate: 0.00092436]
		[batch 20/20] avg loss: 0.011681444011455827		[learning rate: 0.00092324]
	Learning Rate: 0.000923243
	LOSS [training: 0.011783887121291968 | validation: 0.01359127896563872]
	TIME [epoch: 8.17 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009004171249709186		[learning rate: 0.00092212]
		[batch 20/20] avg loss: 0.012994156638045384		[learning rate: 0.00092101]
	Learning Rate: 0.000921008
	LOSS [training: 0.010999163943877285 | validation: 0.01316604141442198]
	TIME [epoch: 8.16 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029025205430610386		[learning rate: 0.00091989]
		[batch 20/20] avg loss: 0.02932291204759207		[learning rate: 0.00091878]
	Learning Rate: 0.000918778
	LOSS [training: 0.029174058739101234 | validation: 0.007620872672332747]
	TIME [epoch: 8.17 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018076693132827275		[learning rate: 0.00091767]
		[batch 20/20] avg loss: 0.008954269890022997		[learning rate: 0.00091655]
	Learning Rate: 0.000916554
	LOSS [training: 0.013515481511425134 | validation: 0.021430377582482563]
	TIME [epoch: 8.21 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01836388038475327		[learning rate: 0.00091544]
		[batch 20/20] avg loss: 0.01582584533678056		[learning rate: 0.00091433]
	Learning Rate: 0.000914335
	LOSS [training: 0.017094862860766915 | validation: 0.02832610643264582]
	TIME [epoch: 8.21 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012289455348527763		[learning rate: 0.00091323]
		[batch 20/20] avg loss: 0.01095242820651706		[learning rate: 0.00091212]
	Learning Rate: 0.000912121
	LOSS [training: 0.011620941777522414 | validation: 0.0024291574086427233]
	TIME [epoch: 8.18 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010873119154651298		[learning rate: 0.00091102]
		[batch 20/20] avg loss: 0.008764884315827432		[learning rate: 0.00090991]
	Learning Rate: 0.000909913
	LOSS [training: 0.009819001735239363 | validation: 0.015114288792465142]
	TIME [epoch: 8.17 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017635052951626755		[learning rate: 0.00090881]
		[batch 20/20] avg loss: 0.015311076861997919		[learning rate: 0.00090771]
	Learning Rate: 0.00090771
	LOSS [training: 0.016473064906812343 | validation: 0.005974433308957949]
	TIME [epoch: 8.19 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009675032647497892		[learning rate: 0.00090661]
		[batch 20/20] avg loss: 0.012999575451410608		[learning rate: 0.00090551]
	Learning Rate: 0.000905513
	LOSS [training: 0.01133730404945425 | validation: 0.01835439846855232]
	TIME [epoch: 8.2 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015981722942916436		[learning rate: 0.00090442]
		[batch 20/20] avg loss: 0.018063880074058843		[learning rate: 0.00090332]
	Learning Rate: 0.000903321
	LOSS [training: 0.017022801508487638 | validation: 0.019490954170377574]
	TIME [epoch: 8.18 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028530171481838118		[learning rate: 0.00090223]
		[batch 20/20] avg loss: 0.017441895709406983		[learning rate: 0.00090113]
	Learning Rate: 0.000901134
	LOSS [training: 0.022986033595622547 | validation: 0.02127005351278404]
	TIME [epoch: 8.16 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01658979701185592		[learning rate: 0.00090004]
		[batch 20/20] avg loss: 0.026848991253204457		[learning rate: 0.00089895]
	Learning Rate: 0.000898953
	LOSS [training: 0.02171939413253019 | validation: 0.03618392154530976]
	TIME [epoch: 8.18 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03228927432570847		[learning rate: 0.00089786]
		[batch 20/20] avg loss: 0.01815291874054257		[learning rate: 0.00089678]
	Learning Rate: 0.000896777
	LOSS [training: 0.02522109653312552 | validation: 0.008039679754686262]
	TIME [epoch: 8.18 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011558311513141057		[learning rate: 0.00089569]
		[batch 20/20] avg loss: 0.012322044251624252		[learning rate: 0.00089461]
	Learning Rate: 0.000894605
	LOSS [training: 0.011940177882382655 | validation: 0.0018606984985254239]
	TIME [epoch: 8.17 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006935945986832087		[learning rate: 0.00089352]
		[batch 20/20] avg loss: 0.01384906644226257		[learning rate: 0.00089244]
	Learning Rate: 0.00089244
	LOSS [training: 0.010392506214547327 | validation: 0.009149171834916512]
	TIME [epoch: 8.16 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007921885982419306		[learning rate: 0.00089136]
		[batch 20/20] avg loss: 0.015562715737254243		[learning rate: 0.00089028]
	Learning Rate: 0.000890279
	LOSS [training: 0.011742300859836774 | validation: 0.0046427643022765265]
	TIME [epoch: 8.21 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004897276160536363		[learning rate: 0.0008892]
		[batch 20/20] avg loss: 0.006771526764986942		[learning rate: 0.00088812]
	Learning Rate: 0.000888124
	LOSS [training: 0.005834401462761653 | validation: 0.008492735116214454]
	TIME [epoch: 8.23 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014076466885915287		[learning rate: 0.00088705]
		[batch 20/20] avg loss: 0.013411004568116659		[learning rate: 0.00088597]
	Learning Rate: 0.000885974
	LOSS [training: 0.013743735727015972 | validation: 0.007367255167060667]
	TIME [epoch: 8.18 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016529014480123914		[learning rate: 0.0008849]
		[batch 20/20] avg loss: 0.018636861042252845		[learning rate: 0.00088383]
	Learning Rate: 0.000883829
	LOSS [training: 0.01758293776118838 | validation: 0.002607995587498633]
	TIME [epoch: 8.17 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00852947594615055		[learning rate: 0.00088276]
		[batch 20/20] avg loss: 0.008371876877934111		[learning rate: 0.00088169]
	Learning Rate: 0.00088169
	LOSS [training: 0.008450676412042332 | validation: 0.023691912702919878]
	TIME [epoch: 8.17 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020157507031915038		[learning rate: 0.00088062]
		[batch 20/20] avg loss: 0.010571912227998106		[learning rate: 0.00087956]
	Learning Rate: 0.000879555
	LOSS [training: 0.015364709629956574 | validation: 0.007180270902235188]
	TIME [epoch: 8.23 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011440317005876648		[learning rate: 0.00087849]
		[batch 20/20] avg loss: 0.012962478940673788		[learning rate: 0.00087743]
	Learning Rate: 0.000877426
	LOSS [training: 0.012201397973275218 | validation: 0.005030880986733305]
	TIME [epoch: 8.18 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012589650836586221		[learning rate: 0.00087636]
		[batch 20/20] avg loss: 0.003294364853188094		[learning rate: 0.0008753]
	Learning Rate: 0.000875302
	LOSS [training: 0.007942007844887158 | validation: 0.007299205097025494]
	TIME [epoch: 8.17 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010922283589602377		[learning rate: 0.00087424]
		[batch 20/20] avg loss: 0.010146599809956556		[learning rate: 0.00087318]
	Learning Rate: 0.000873183
	LOSS [training: 0.010534441699779466 | validation: 0.01376459881496816]
	TIME [epoch: 8.17 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013182301018317391		[learning rate: 0.00087213]
		[batch 20/20] avg loss: 0.011799517857687535		[learning rate: 0.00087107]
	Learning Rate: 0.000871069
	LOSS [training: 0.012490909438002462 | validation: 0.005962140320939011]
	TIME [epoch: 8.22 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00854489778922378		[learning rate: 0.00087001]
		[batch 20/20] avg loss: 0.008419697770812917		[learning rate: 0.00086896]
	Learning Rate: 0.00086896
	LOSS [training: 0.00848229778001835 | validation: 0.001032565544601001]
	TIME [epoch: 8.18 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006118866570426248		[learning rate: 0.00086791]
		[batch 20/20] avg loss: 0.005806946753739673		[learning rate: 0.00086686]
	Learning Rate: 0.000866857
	LOSS [training: 0.005962906662082961 | validation: 0.01129836291566504]
	TIME [epoch: 8.17 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007562691618132096		[learning rate: 0.00086581]
		[batch 20/20] avg loss: 0.0067904790440194585		[learning rate: 0.00086476]
	Learning Rate: 0.000864758
	LOSS [training: 0.007176585331075777 | validation: 0.015467866155443861]
	TIME [epoch: 8.18 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010699451304515743		[learning rate: 0.00086371]
		[batch 20/20] avg loss: 0.012890978158186633		[learning rate: 0.00086266]
	Learning Rate: 0.000862665
	LOSS [training: 0.01179521473135119 | validation: 0.027164400753234372]
	TIME [epoch: 8.23 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015165209698442516		[learning rate: 0.00086162]
		[batch 20/20] avg loss: 0.011477988301118546		[learning rate: 0.00086058]
	Learning Rate: 0.000860577
	LOSS [training: 0.013321598999780535 | validation: 0.010847518647914278]
	TIME [epoch: 8.18 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009597778718842783		[learning rate: 0.00085953]
		[batch 20/20] avg loss: 0.008914886160984414		[learning rate: 0.00085849]
	Learning Rate: 0.000858493
	LOSS [training: 0.009256332439913598 | validation: 0.008100485633616504]
	TIME [epoch: 8.16 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008793083093888463		[learning rate: 0.00085745]
		[batch 20/20] avg loss: 0.011425250242145725		[learning rate: 0.00085641]
	Learning Rate: 0.000856415
	LOSS [training: 0.010109166668017094 | validation: 0.01532887810076931]
	TIME [epoch: 8.17 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013269238539417689		[learning rate: 0.00085538]
		[batch 20/20] avg loss: 0.006373584374301392		[learning rate: 0.00085434]
	Learning Rate: 0.000854342
	LOSS [training: 0.00982141145685954 | validation: 0.0029529128078046]
	TIME [epoch: 8.18 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013009206659464348		[learning rate: 0.00085331]
		[batch 20/20] avg loss: 0.01667531826772861		[learning rate: 0.00085227]
	Learning Rate: 0.000852273
	LOSS [training: 0.014842262463596479 | validation: 0.01670234262199685]
	TIME [epoch: 8.16 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015780419688878403		[learning rate: 0.00085124]
		[batch 20/20] avg loss: 0.00807811018561871		[learning rate: 0.00085021]
	Learning Rate: 0.00085021
	LOSS [training: 0.011929264937248555 | validation: 0.0091277061898046]
	TIME [epoch: 8.16 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015979387240560645		[learning rate: 0.00084918]
		[batch 20/20] avg loss: 0.009154416449162478		[learning rate: 0.00084815]
	Learning Rate: 0.000848152
	LOSS [training: 0.012566901844861561 | validation: -0.003497812072039665]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_1118.pth
	Model improved!!!
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004465071090214925		[learning rate: 0.00084712]
		[batch 20/20] avg loss: 0.015105398908671533		[learning rate: 0.0008461]
	Learning Rate: 0.000846099
	LOSS [training: 0.00978523499944323 | validation: 0.00420449363967383]
	TIME [epoch: 8.26 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028069664817333235		[learning rate: 0.00084507]
		[batch 20/20] avg loss: 0.011836740523988951		[learning rate: 0.00084405]
	Learning Rate: 0.000844051
	LOSS [training: 0.007321853502861136 | validation: 0.006407919344313464]
	TIME [epoch: 8.2 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008093462361592875		[learning rate: 0.00084303]
		[batch 20/20] avg loss: 0.006829647112222866		[learning rate: 0.00084201]
	Learning Rate: 0.000842007
	LOSS [training: 0.00746155473690787 | validation: 0.005484421902994994]
	TIME [epoch: 8.19 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011609637405939635		[learning rate: 0.00084099]
		[batch 20/20] avg loss: 0.02672978946958577		[learning rate: 0.00083997]
	Learning Rate: 0.000839969
	LOSS [training: 0.0191697134377627 | validation: 0.016705395764933163]
	TIME [epoch: 8.2 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01004358927206006		[learning rate: 0.00083895]
		[batch 20/20] avg loss: 0.01423334654049023		[learning rate: 0.00083794]
	Learning Rate: 0.000837935
	LOSS [training: 0.012138467906275145 | validation: 0.008994297777786276]
	TIME [epoch: 8.25 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00866104238236894		[learning rate: 0.00083692]
		[batch 20/20] avg loss: 0.010206129235299047		[learning rate: 0.00083591]
	Learning Rate: 0.000835907
	LOSS [training: 0.009433585808833992 | validation: 0.00885419950469583]
	TIME [epoch: 8.2 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004478618807648263		[learning rate: 0.00083489]
		[batch 20/20] avg loss: 0.005350573839858293		[learning rate: 0.00083388]
	Learning Rate: 0.000833883
	LOSS [training: 0.004914596323753279 | validation: 0.011599688460012105]
	TIME [epoch: 8.18 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013128655001413863		[learning rate: 0.00083287]
		[batch 20/20] avg loss: 0.011684259408693883		[learning rate: 0.00083186]
	Learning Rate: 0.000831865
	LOSS [training: 0.012406457205053872 | validation: 0.007290488446398561]
	TIME [epoch: 8.19 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006684126030025789		[learning rate: 0.00083086]
		[batch 20/20] avg loss: 0.007375134874762049		[learning rate: 0.00082985]
	Learning Rate: 0.000829851
	LOSS [training: 0.00702963045239392 | validation: 0.021514464135912962]
	TIME [epoch: 8.21 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012272125296024894		[learning rate: 0.00082885]
		[batch 20/20] avg loss: 0.005821663045795736		[learning rate: 0.00082784]
	Learning Rate: 0.000827842
	LOSS [training: 0.009046894170910313 | validation: 0.005721916199178889]
	TIME [epoch: 8.19 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032573416915950395		[learning rate: 0.00082684]
		[batch 20/20] avg loss: 0.007199435047659228		[learning rate: 0.00082584]
	Learning Rate: 0.000825838
	LOSS [training: 0.005228388369627133 | validation: 0.004925572865314985]
	TIME [epoch: 8.18 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013041519025956372		[learning rate: 0.00082484]
		[batch 20/20] avg loss: 0.0013486163039371749		[learning rate: 0.00082384]
	Learning Rate: 0.000823839
	LOSS [training: 0.007195067664946775 | validation: 0.006660959749820755]
	TIME [epoch: 8.22 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038797412192196324		[learning rate: 0.00082284]
		[batch 20/20] avg loss: 0.006797604879887243		[learning rate: 0.00082184]
	Learning Rate: 0.000821844
	LOSS [training: 0.005338673049553438 | validation: 0.0028818629782221303]
	TIME [epoch: 8.22 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001382475407126656		[learning rate: 0.00082085]
		[batch 20/20] avg loss: 0.017344973340928237		[learning rate: 0.00081985]
	Learning Rate: 0.000819855
	LOSS [training: 0.009363724374027445 | validation: 0.019968951417653487]
	TIME [epoch: 8.19 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0170972617417881		[learning rate: 0.00081886]
		[batch 20/20] avg loss: 0.01228845217225647		[learning rate: 0.00081787]
	Learning Rate: 0.00081787
	LOSS [training: 0.014692856957022288 | validation: 0.005804428486800317]
	TIME [epoch: 8.21 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00932672678462687		[learning rate: 0.00081688]
		[batch 20/20] avg loss: 0.006428968618224351		[learning rate: 0.00081589]
	Learning Rate: 0.00081589
	LOSS [training: 0.007877847701425612 | validation: 0.0065924413138365254]
	TIME [epoch: 8.22 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004974178487133153		[learning rate: 0.0008149]
		[batch 20/20] avg loss: 0.0038478674545741206		[learning rate: 0.00081391]
	Learning Rate: 0.000813915
	LOSS [training: 0.004411022970853637 | validation: 0.011769685582698614]
	TIME [epoch: 8.21 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029584700435360905		[learning rate: 0.00081293]
		[batch 20/20] avg loss: 0.014033991199581246		[learning rate: 0.00081194]
	Learning Rate: 0.000811944
	LOSS [training: 0.00849623062155867 | validation: 0.01911110033700786]
	TIME [epoch: 8.19 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008110894903896465		[learning rate: 0.00081096]
		[batch 20/20] avg loss: 0.01009176670921261		[learning rate: 0.00080998]
	Learning Rate: 0.000809979
	LOSS [training: 0.009101330806554538 | validation: 0.011308887105350442]
	TIME [epoch: 8.19 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008231869198519853		[learning rate: 0.000809]
		[batch 20/20] avg loss: 0.004606845377007786		[learning rate: 0.00080802]
	Learning Rate: 0.000808018
	LOSS [training: 0.0064193572877638185 | validation: 0.00908442968470533]
	TIME [epoch: 8.18 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009795188752326964		[learning rate: 0.00080704]
		[batch 20/20] avg loss: 0.002357000244042335		[learning rate: 0.00080606]
	Learning Rate: 0.000806062
	LOSS [training: 0.006076094498184651 | validation: 0.012935309422103632]
	TIME [epoch: 8.21 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014472129678442072		[learning rate: 0.00080509]
		[batch 20/20] avg loss: 0.002530848056321611		[learning rate: 0.00080411]
	Learning Rate: 0.000804111
	LOSS [training: 0.008501488867381845 | validation: -0.00340207862341879]
	TIME [epoch: 8.23 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013154586715692307		[learning rate: 0.00080314]
		[batch 20/20] avg loss: 0.010465991336069068		[learning rate: 0.00080216]
	Learning Rate: 0.000802164
	LOSS [training: 0.011810289025880688 | validation: 0.017801572512062776]
	TIME [epoch: 8.21 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004557326487860481		[learning rate: 0.00080119]
		[batch 20/20] avg loss: 0.008905946231583984		[learning rate: 0.00080022]
	Learning Rate: 0.000800222
	LOSS [training: 0.006731636359722233 | validation: 0.014286414630068897]
	TIME [epoch: 8.19 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005459078681808228		[learning rate: 0.00079925]
		[batch 20/20] avg loss: 0.0019215743648685808		[learning rate: 0.00079828]
	Learning Rate: 0.000798285
	LOSS [training: 0.0036903265233384054 | validation: 0.008651127901663443]
	TIME [epoch: 8.22 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005674410056262622		[learning rate: 0.00079732]
		[batch 20/20] avg loss: 0.008431157818060336		[learning rate: 0.00079635]
	Learning Rate: 0.000796352
	LOSS [training: 0.007052783937161478 | validation: 0.00414649093909525]
	TIME [epoch: 8.23 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004537005824484322		[learning rate: 0.00079539]
		[batch 20/20] avg loss: 0.00667428638472461		[learning rate: 0.00079442]
	Learning Rate: 0.000794424
	LOSS [training: 0.0056056461046044646 | validation: 0.003243483592432263]
	TIME [epoch: 8.2 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009642871898700455		[learning rate: 0.00079346]
		[batch 20/20] avg loss: 0.013791274859381294		[learning rate: 0.0007925]
	Learning Rate: 0.000792501
	LOSS [training: 0.011717073379040873 | validation: 0.0250481795902422]
	TIME [epoch: 8.19 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020666499370920383		[learning rate: 0.00079154]
		[batch 20/20] avg loss: 0.016541786682114076		[learning rate: 0.00079058]
	Learning Rate: 0.000790583
	LOSS [training: 0.01860414302651723 | validation: 0.022229572352300754]
	TIME [epoch: 8.21 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021440480139252878		[learning rate: 0.00078963]
		[batch 20/20] avg loss: 0.008503357088745882		[learning rate: 0.00078867]
	Learning Rate: 0.000788669
	LOSS [training: 0.01497191861399938 | validation: 0.008775753217051237]
	TIME [epoch: 8.19 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013157650401617343		[learning rate: 0.00078771]
		[batch 20/20] avg loss: 0.013076502186070071		[learning rate: 0.00078676]
	Learning Rate: 0.00078676
	LOSS [training: 0.01311707629384371 | validation: 0.0026222823688816232]
	TIME [epoch: 8.19 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01112676620939269		[learning rate: 0.00078581]
		[batch 20/20] avg loss: 0.0021546534550250383		[learning rate: 0.00078486]
	Learning Rate: 0.000784855
	LOSS [training: 0.006640709832208863 | validation: 0.0030965373696788872]
	TIME [epoch: 8.19 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00483636043826763		[learning rate: 0.0007839]
		[batch 20/20] avg loss: 0.011026500630612383		[learning rate: 0.00078296]
	Learning Rate: 0.000782955
	LOSS [training: 0.007931430534440006 | validation: 0.005423414755231612]
	TIME [epoch: 8.22 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013662761518207786		[learning rate: 0.00078201]
		[batch 20/20] avg loss: 0.017071322465810678		[learning rate: 0.00078106]
	Learning Rate: 0.00078106
	LOSS [training: 0.015367041992009233 | validation: 0.020026152474726173]
	TIME [epoch: 8.22 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011470785562554856		[learning rate: 0.00078011]
		[batch 20/20] avg loss: 0.017135245211938314		[learning rate: 0.00077917]
	Learning Rate: 0.000779169
	LOSS [training: 0.014303015387246588 | validation: 0.01337368219169132]
	TIME [epoch: 8.2 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00642653962594472		[learning rate: 0.00077823]
		[batch 20/20] avg loss: 0.008417406346688484		[learning rate: 0.00077728]
	Learning Rate: 0.000777283
	LOSS [training: 0.007421972986316602 | validation: 0.00813820032848845]
	TIME [epoch: 8.2 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008624529055082018		[learning rate: 0.00077634]
		[batch 20/20] avg loss: 0.009495098659417304		[learning rate: 0.0007754]
	Learning Rate: 0.000775401
	LOSS [training: 0.009059813857249662 | validation: 0.002034340865757354]
	TIME [epoch: 8.25 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00914864119608622		[learning rate: 0.00077446]
		[batch 20/20] avg loss: 0.013199193068193536		[learning rate: 0.00077352]
	Learning Rate: 0.000773524
	LOSS [training: 0.011173917132139877 | validation: 0.0058971492021300765]
	TIME [epoch: 8.21 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01055674536915792		[learning rate: 0.00077259]
		[batch 20/20] avg loss: 0.014515203888107934		[learning rate: 0.00077165]
	Learning Rate: 0.000771651
	LOSS [training: 0.012535974628632925 | validation: 0.01727586392283936]
	TIME [epoch: 8.19 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009671737411485899		[learning rate: 0.00077072]
		[batch 20/20] avg loss: 0.010502689332784589		[learning rate: 0.00076978]
	Learning Rate: 0.000769783
	LOSS [training: 0.010087213372135244 | validation: 0.010384679639516845]
	TIME [epoch: 8.18 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012615247182581849		[learning rate: 0.00076885]
		[batch 20/20] avg loss: 0.011100786079940405		[learning rate: 0.00076792]
	Learning Rate: 0.00076792
	LOSS [training: 0.01185801663126113 | validation: 0.010068197504822992]
	TIME [epoch: 8.2 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00617303191429814		[learning rate: 0.00076699]
		[batch 20/20] avg loss: 0.018814250258490203		[learning rate: 0.00076606]
	Learning Rate: 0.000766061
	LOSS [training: 0.012493641086394173 | validation: 0.005849048553744358]
	TIME [epoch: 8.18 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010454252210717418		[learning rate: 0.00076513]
		[batch 20/20] avg loss: 0.014167916945370961		[learning rate: 0.00076421]
	Learning Rate: 0.000764206
	LOSS [training: 0.01231108457804419 | validation: 0.0063425427583854565]
	TIME [epoch: 8.18 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012905462403755443		[learning rate: 0.00076328]
		[batch 20/20] avg loss: 0.010989632754360873		[learning rate: 0.00076236]
	Learning Rate: 0.000762356
	LOSS [training: 0.01194754757905816 | validation: 0.009528625503345762]
	TIME [epoch: 8.18 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012369331784369901		[learning rate: 0.00076143]
		[batch 20/20] avg loss: 0.01628036396044403		[learning rate: 0.00076051]
	Learning Rate: 0.000760511
	LOSS [training: 0.014324847872406964 | validation: 0.004925257591400772]
	TIME [epoch: 8.2 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017378453816171417		[learning rate: 0.00075959]
		[batch 20/20] avg loss: 0.009423202756041048		[learning rate: 0.00075867]
	Learning Rate: 0.00075867
	LOSS [training: 0.013400828286106234 | validation: 0.0020250750228194905]
	TIME [epoch: 8.21 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005129660705109588		[learning rate: 0.00075775]
		[batch 20/20] avg loss: 0.010342146821103015		[learning rate: 0.00075683]
	Learning Rate: 0.000756833
	LOSS [training: 0.007735903763106303 | validation: 0.019158237056742015]
	TIME [epoch: 8.19 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01835957882339704		[learning rate: 0.00075592]
		[batch 20/20] avg loss: 0.006969303556339392		[learning rate: 0.000755]
	Learning Rate: 0.000755001
	LOSS [training: 0.012664441189868217 | validation: 0.0010952756344587934]
	TIME [epoch: 8.19 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008672969698285128		[learning rate: 0.00075409]
		[batch 20/20] avg loss: 0.019089950057314668		[learning rate: 0.00075317]
	Learning Rate: 0.000753173
	LOSS [training: 0.013881459877799896 | validation: 0.011787306694901502]
	TIME [epoch: 8.21 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011766353121646879		[learning rate: 0.00075226]
		[batch 20/20] avg loss: 0.010663057164242583		[learning rate: 0.00075135]
	Learning Rate: 0.00075135
	LOSS [training: 0.01121470514294473 | validation: -0.0004799289644575671]
	TIME [epoch: 8.23 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006538821814760909		[learning rate: 0.00075044]
		[batch 20/20] avg loss: 0.01884577513483698		[learning rate: 0.00074953]
	Learning Rate: 0.000749531
	LOSS [training: 0.012692298474798946 | validation: 0.0016317383179368407]
	TIME [epoch: 8.19 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014632293574864189		[learning rate: 0.00074862]
		[batch 20/20] avg loss: 0.011599226425763956		[learning rate: 0.00074772]
	Learning Rate: 0.000747716
	LOSS [training: 0.013115760000314072 | validation: 0.005978588958647003]
	TIME [epoch: 8.18 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011815588656720078		[learning rate: 0.00074681]
		[batch 20/20] avg loss: 0.009174951607354924		[learning rate: 0.00074591]
	Learning Rate: 0.000745906
	LOSS [training: 0.010495270132037502 | validation: 0.01489297502604289]
	TIME [epoch: 8.2 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02288058710561704		[learning rate: 0.000745]
		[batch 20/20] avg loss: 0.010652171239481685		[learning rate: 0.0007441]
	Learning Rate: 0.0007441
	LOSS [training: 0.016766379172549365 | validation: 0.0018304456084599185]
	TIME [epoch: 8.18 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011676434817310278		[learning rate: 0.0007432]
		[batch 20/20] avg loss: 0.014697525805265312		[learning rate: 0.0007423]
	Learning Rate: 0.000742299
	LOSS [training: 0.013186980311287797 | validation: 0.008498793713599826]
	TIME [epoch: 8.18 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016627561675267397		[learning rate: 0.0007414]
		[batch 20/20] avg loss: 0.005435410679818269		[learning rate: 0.0007405]
	Learning Rate: 0.000740502
	LOSS [training: 0.011031486177542833 | validation: 0.006458523894381547]
	TIME [epoch: 8.18 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011794996717482718		[learning rate: 0.00073961]
		[batch 20/20] avg loss: 0.00749250286906947		[learning rate: 0.00073871]
	Learning Rate: 0.000738709
	LOSS [training: 0.009643749793276094 | validation: 0.0228891011563929]
	TIME [epoch: 8.21 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007460290225797178		[learning rate: 0.00073781]
		[batch 20/20] avg loss: 0.008033893044131298		[learning rate: 0.00073692]
	Learning Rate: 0.000736921
	LOSS [training: 0.007747091634964237 | validation: 0.008534424010748004]
	TIME [epoch: 8.18 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01374620130835468		[learning rate: 0.00073603]
		[batch 20/20] avg loss: 0.006577719808992112		[learning rate: 0.00073514]
	Learning Rate: 0.000735137
	LOSS [training: 0.010161960558673397 | validation: 0.006783934893376031]
	TIME [epoch: 8.18 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01047858699610111		[learning rate: 0.00073425]
		[batch 20/20] avg loss: 0.012292492880007822		[learning rate: 0.00073336]
	Learning Rate: 0.000733358
	LOSS [training: 0.011385539938054464 | validation: 0.01892700406487139]
	TIME [epoch: 8.18 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011447522232446522		[learning rate: 0.00073247]
		[batch 20/20] avg loss: 0.010453995193825312		[learning rate: 0.00073158]
	Learning Rate: 0.000731582
	LOSS [training: 0.010950758713135915 | validation: 0.010607964728271567]
	TIME [epoch: 8.21 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01092755085295911		[learning rate: 0.0007307]
		[batch 20/20] avg loss: 0.018874893820431362		[learning rate: 0.00072981]
	Learning Rate: 0.000729811
	LOSS [training: 0.014901222336695238 | validation: 0.007002151080659846]
	TIME [epoch: 8.19 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008103384142133114		[learning rate: 0.00072893]
		[batch 20/20] avg loss: 0.01146180239582738		[learning rate: 0.00072804]
	Learning Rate: 0.000728044
	LOSS [training: 0.009782593268980247 | validation: 0.008461898168778006]
	TIME [epoch: 8.19 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007540043535372157		[learning rate: 0.00072716]
		[batch 20/20] avg loss: 0.013055274664265985		[learning rate: 0.00072628]
	Learning Rate: 0.000726282
	LOSS [training: 0.010297659099819072 | validation: 0.0008395217783855446]
	TIME [epoch: 8.18 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011552993124279463		[learning rate: 0.0007254]
		[batch 20/20] avg loss: 0.007727276237290538		[learning rate: 0.00072452]
	Learning Rate: 0.000724524
	LOSS [training: 0.009640134680784998 | validation: 0.011378960978710273]
	TIME [epoch: 8.21 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011536958666615652		[learning rate: 0.00072365]
		[batch 20/20] avg loss: 0.014707632436158479		[learning rate: 0.00072277]
	Learning Rate: 0.00072277
	LOSS [training: 0.00793066415141002 | validation: 0.016538208772834805]
	TIME [epoch: 8.21 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013782341066535		[learning rate: 0.00072189]
		[batch 20/20] avg loss: 0.003751758447389035		[learning rate: 0.00072102]
	Learning Rate: 0.00072102
	LOSS [training: 0.008767049756962018 | validation: 0.008085433152174215]
	TIME [epoch: 8.19 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008725107254865944		[learning rate: 0.00072015]
		[batch 20/20] avg loss: 0.009056321371612261		[learning rate: 0.00071927]
	Learning Rate: 0.000719275
	LOSS [training: 0.008890714313239104 | validation: 0.004071035205562443]
	TIME [epoch: 8.19 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01612440034848616		[learning rate: 0.0007184]
		[batch 20/20] avg loss: 0.015001155608041697		[learning rate: 0.00071753]
	Learning Rate: 0.000717533
	LOSS [training: 0.015562777978263933 | validation: 0.011900112133234955]
	TIME [epoch: 8.22 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00684387785501295		[learning rate: 0.00071666]
		[batch 20/20] avg loss: 0.004278697812497346		[learning rate: 0.0007158]
	Learning Rate: 0.000715796
	LOSS [training: 0.005561287833755148 | validation: 0.005720658611890894]
	TIME [epoch: 8.22 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00448169052623373		[learning rate: 0.00071493]
		[batch 20/20] avg loss: 0.014305843791641116		[learning rate: 0.00071406]
	Learning Rate: 0.000714064
	LOSS [training: 0.009393767158937423 | validation: 0.009244870432477358]
	TIME [epoch: 8.18 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007090816679162356		[learning rate: 0.0007132]
		[batch 20/20] avg loss: 0.005527028851258278		[learning rate: 0.00071233]
	Learning Rate: 0.000712335
	LOSS [training: 0.006308922765210318 | validation: 0.010293943998003227]
	TIME [epoch: 8.18 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013618734482347503		[learning rate: 0.00071147]
		[batch 20/20] avg loss: 0.012913324444978705		[learning rate: 0.00071061]
	Learning Rate: 0.00071061
	LOSS [training: 0.013266029463663107 | validation: 0.009121571850418717]
	TIME [epoch: 8.19 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01133376700955915		[learning rate: 0.00070975]
		[batch 20/20] avg loss: 0.004004203683461597		[learning rate: 0.00070889]
	Learning Rate: 0.00070889
	LOSS [training: 0.007668985346510375 | validation: 0.008090910494812597]
	TIME [epoch: 8.19 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01044314530786149		[learning rate: 0.00070803]
		[batch 20/20] avg loss: 0.013985197456129524		[learning rate: 0.00070717]
	Learning Rate: 0.000707174
	LOSS [training: 0.012214171381995508 | validation: 0.0011761783832477239]
	TIME [epoch: 8.17 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011941887175145915		[learning rate: 0.00070632]
		[batch 20/20] avg loss: 0.009098824671939062		[learning rate: 0.00070546]
	Learning Rate: 0.000705462
	LOSS [training: 0.010520355923542491 | validation: 0.003584315207639019]
	TIME [epoch: 8.17 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009127342027326051		[learning rate: 0.00070461]
		[batch 20/20] avg loss: 0.00976829294839874		[learning rate: 0.00070375]
	Learning Rate: 0.000703754
	LOSS [training: 0.009447817487862394 | validation: 0.012996155063681857]
	TIME [epoch: 8.18 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012170653025543664		[learning rate: 0.0007029]
		[batch 20/20] avg loss: 0.009532345419829849		[learning rate: 0.00070205]
	Learning Rate: 0.000702051
	LOSS [training: 0.010851499222686757 | validation: 0.0012490009328266498]
	TIME [epoch: 8.23 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007707751649653555		[learning rate: 0.0007012]
		[batch 20/20] avg loss: 0.005816249991649002		[learning rate: 0.00070035]
	Learning Rate: 0.000700351
	LOSS [training: 0.00676200082065128 | validation: -0.0021651524859062775]
	TIME [epoch: 8.18 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011645125355480141		[learning rate: 0.0006995]
		[batch 20/20] avg loss: 0.014710560698381104		[learning rate: 0.00069866]
	Learning Rate: 0.000698656
	LOSS [training: 0.013177843026930619 | validation: 0.02294876975637391]
	TIME [epoch: 8.19 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025514634734640006		[learning rate: 0.00069781]
		[batch 20/20] avg loss: 0.014269855076405782		[learning rate: 0.00069696]
	Learning Rate: 0.000696964
	LOSS [training: 0.019892244905522893 | validation: 0.008626030259845924]
	TIME [epoch: 8.21 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01077595805948538		[learning rate: 0.00069612]
		[batch 20/20] avg loss: 0.013623568548935908		[learning rate: 0.00069528]
	Learning Rate: 0.000695277
	LOSS [training: 0.012199763304210645 | validation: 0.009232457038599436]
	TIME [epoch: 8.23 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015068885468273674		[learning rate: 0.00069444]
		[batch 20/20] avg loss: 0.01681208077865714		[learning rate: 0.00069359]
	Learning Rate: 0.000693594
	LOSS [training: 0.01594048312346541 | validation: 0.022078576482261983]
	TIME [epoch: 8.18 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025085022448680633		[learning rate: 0.00069275]
		[batch 20/20] avg loss: 0.01113137041698044		[learning rate: 0.00069191]
	Learning Rate: 0.000691915
	LOSS [training: 0.018108196432830535 | validation: 0.006731397869434375]
	TIME [epoch: 8.18 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012111691355988347		[learning rate: 0.00069108]
		[batch 20/20] avg loss: 0.009248222582728387		[learning rate: 0.00069024]
	Learning Rate: 0.00069024
	LOSS [training: 0.010679956969358365 | validation: 0.009338362310158194]
	TIME [epoch: 8.18 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00878667581668925		[learning rate: 0.0006894]
		[batch 20/20] avg loss: 0.010135615261652322		[learning rate: 0.00068857]
	Learning Rate: 0.000688569
	LOSS [training: 0.009461145539170785 | validation: 0.008425853123204966]
	TIME [epoch: 8.21 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006123164288314937		[learning rate: 0.00068773]
		[batch 20/20] avg loss: 0.014097469149760927		[learning rate: 0.0006869]
	Learning Rate: 0.000686902
	LOSS [training: 0.010110316719037933 | validation: 0.009970009393176257]
	TIME [epoch: 8.18 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004649088144490917		[learning rate: 0.00068607]
		[batch 20/20] avg loss: 0.01619055128549424		[learning rate: 0.00068524]
	Learning Rate: 0.000685239
	LOSS [training: 0.010419819714992576 | validation: 0.009476170211924947]
	TIME [epoch: 8.2 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005715611900227496		[learning rate: 0.00068441]
		[batch 20/20] avg loss: 0.012966824689508199		[learning rate: 0.00068358]
	Learning Rate: 0.00068358
	LOSS [training: 0.009341218294867849 | validation: 0.0027551489033850935]
	TIME [epoch: 8.2 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010406647698253505		[learning rate: 0.00068275]
		[batch 20/20] avg loss: 0.013966376886473588		[learning rate: 0.00068193]
	Learning Rate: 0.000681925
	LOSS [training: 0.012186512292363545 | validation: 0.002918903807886913]
	TIME [epoch: 8.21 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012134967100137383		[learning rate: 0.0006811]
		[batch 20/20] avg loss: 0.006798490023836694		[learning rate: 0.00068027]
	Learning Rate: 0.000680275
	LOSS [training: 0.009466728561987037 | validation: 0.0041994772034838306]
	TIME [epoch: 8.19 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007328250398359837		[learning rate: 0.00067945]
		[batch 20/20] avg loss: 0.008464029592239925		[learning rate: 0.00067863]
	Learning Rate: 0.000678628
	LOSS [training: 0.00789613999529988 | validation: 0.007586350287214788]
	TIME [epoch: 8.21 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00605395341490645		[learning rate: 0.00067781]
		[batch 20/20] avg loss: 0.01980739671995518		[learning rate: 0.00067698]
	Learning Rate: 0.000676985
	LOSS [training: 0.012930675067430817 | validation: 0.021762757721673527]
	TIME [epoch: 8.2 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0180492599306872		[learning rate: 0.00067616]
		[batch 20/20] avg loss: 0.02117679622362829		[learning rate: 0.00067535]
	Learning Rate: 0.000675346
	LOSS [training: 0.019613028077157747 | validation: 0.018697851041178028]
	TIME [epoch: 8.2 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017202037265758398		[learning rate: 0.00067453]
		[batch 20/20] avg loss: 0.011822306193356582		[learning rate: 0.00067371]
	Learning Rate: 0.000673711
	LOSS [training: 0.014512171729557492 | validation: 0.013803572250070726]
	TIME [epoch: 8.18 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01718062655690234		[learning rate: 0.0006729]
		[batch 20/20] avg loss: 0.011765160781716442		[learning rate: 0.00067208]
	Learning Rate: 0.00067208
	LOSS [training: 0.014472893669309395 | validation: 0.014134190359639181]
	TIME [epoch: 8.17 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011973225850248886		[learning rate: 0.00067127]
		[batch 20/20] avg loss: 0.010299645262958826		[learning rate: 0.00067045]
	Learning Rate: 0.000670453
	LOSS [training: 0.011136435556603855 | validation: 0.010665726410081497]
	TIME [epoch: 8.18 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012744480009301848		[learning rate: 0.00066964]
		[batch 20/20] avg loss: 0.016470618256203235		[learning rate: 0.00066883]
	Learning Rate: 0.00066883
	LOSS [training: 0.014607549132752543 | validation: 0.014294298292863217]
	TIME [epoch: 8.2 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029196764302952205		[learning rate: 0.00066802]
		[batch 20/20] avg loss: 0.028026895360822424		[learning rate: 0.00066721]
	Learning Rate: 0.000667211
	LOSS [training: 0.028611829831887314 | validation: 0.02477050375647712]
	TIME [epoch: 8.21 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03413655977081849		[learning rate: 0.0006664]
		[batch 20/20] avg loss: 0.03546427675259841		[learning rate: 0.0006656]
	Learning Rate: 0.000665596
	LOSS [training: 0.03480041826170845 | validation: 0.020343062263794585]
	TIME [epoch: 8.2 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021852264481527467		[learning rate: 0.00066479]
		[batch 20/20] avg loss: 0.010080598359114158		[learning rate: 0.00066398]
	Learning Rate: 0.000663984
	LOSS [training: 0.015966431420320816 | validation: 0.005894917552222283]
	TIME [epoch: 8.18 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014123790532813704		[learning rate: 0.00066318]
		[batch 20/20] avg loss: 0.012784171003869794		[learning rate: 0.00066238]
	Learning Rate: 0.000662377
	LOSS [training: 0.01345398076834175 | validation: 0.017201856586916114]
	TIME [epoch: 8.2 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018030998178977038		[learning rate: 0.00066157]
		[batch 20/20] avg loss: 0.008934320614930885		[learning rate: 0.00066077]
	Learning Rate: 0.000660773
	LOSS [training: 0.01348265939695396 | validation: 0.01270036594845448]
	TIME [epoch: 8.21 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010716025464208582		[learning rate: 0.00065997]
		[batch 20/20] avg loss: 0.013354080282863329		[learning rate: 0.00065917]
	Learning Rate: 0.000659174
	LOSS [training: 0.012035052873535956 | validation: 0.002700647371388542]
	TIME [epoch: 8.2 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01673333246168063		[learning rate: 0.00065838]
		[batch 20/20] avg loss: 0.026397607825265356		[learning rate: 0.00065758]
	Learning Rate: 0.000657578
	LOSS [training: 0.021565470143472994 | validation: 0.007918669582396115]
	TIME [epoch: 8.17 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015805962769650215		[learning rate: 0.00065678]
		[batch 20/20] avg loss: 0.01396518384306544		[learning rate: 0.00065599]
	Learning Rate: 0.000655986
	LOSS [training: 0.01488557330635783 | validation: 0.0121854584693687]
	TIME [epoch: 8.19 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022439025861059766		[learning rate: 0.00065519]
		[batch 20/20] avg loss: 0.013652780121644902		[learning rate: 0.0006544]
	Learning Rate: 0.000654398
	LOSS [training: 0.018045902991352336 | validation: 0.018173816110768008]
	TIME [epoch: 8.18 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006805622440902763		[learning rate: 0.00065361]
		[batch 20/20] avg loss: 0.011017178451654754		[learning rate: 0.00065281]
	Learning Rate: 0.000652814
	LOSS [training: 0.008911400446278757 | validation: 0.005838306646227108]
	TIME [epoch: 8.18 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010700594641844112		[learning rate: 0.00065202]
		[batch 20/20] avg loss: 0.01804055679635948		[learning rate: 0.00065123]
	Learning Rate: 0.000651234
	LOSS [training: 0.014370575719101794 | validation: 0.008409360808972689]
	TIME [epoch: 8.21 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01562900529647283		[learning rate: 0.00065045]
		[batch 20/20] avg loss: 0.01773713765606301		[learning rate: 0.00064966]
	Learning Rate: 0.000649657
	LOSS [training: 0.01668307147626792 | validation: 0.016652939045864443]
	TIME [epoch: 8.21 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017981511169125233		[learning rate: 0.00064887]
		[batch 20/20] avg loss: 0.024743179440940215		[learning rate: 0.00064808]
	Learning Rate: 0.000648084
	LOSS [training: 0.021362345305032723 | validation: 0.017554840472884407]
	TIME [epoch: 8.19 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01950744291499591		[learning rate: 0.0006473]
		[batch 20/20] avg loss: 0.02028368221995637		[learning rate: 0.00064652]
	Learning Rate: 0.000646515
	LOSS [training: 0.01989556256747614 | validation: 0.008919591078480758]
	TIME [epoch: 8.18 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014690010137611748		[learning rate: 0.00064573]
		[batch 20/20] avg loss: 0.008716514514667425		[learning rate: 0.00064495]
	Learning Rate: 0.00064495
	LOSS [training: 0.011703262326139588 | validation: 0.005318283131691113]
	TIME [epoch: 8.22 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012687420115762519		[learning rate: 0.00064417]
		[batch 20/20] avg loss: 0.005930713935128135		[learning rate: 0.00064339]
	Learning Rate: 0.000643389
	LOSS [training: 0.009309067025445327 | validation: 5.608481451244965e-05]
	TIME [epoch: 8.2 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00896032687157874		[learning rate: 0.00064261]
		[batch 20/20] avg loss: 0.010170162608332544		[learning rate: 0.00064183]
	Learning Rate: 0.000641832
	LOSS [training: 0.009565244739955642 | validation: 0.012717803581858981]
	TIME [epoch: 8.18 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011123880153628623		[learning rate: 0.00064105]
		[batch 20/20] avg loss: 0.008851808680131884		[learning rate: 0.00064028]
	Learning Rate: 0.000640278
	LOSS [training: 0.009987844416880252 | validation: 0.011655737373626083]
	TIME [epoch: 8.17 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007241195260831934		[learning rate: 0.0006395]
		[batch 20/20] avg loss: 0.01169705249199918		[learning rate: 0.00063873]
	Learning Rate: 0.000638728
	LOSS [training: 0.009469123876415558 | validation: 0.009948183035182282]
	TIME [epoch: 8.18 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0110513617516018		[learning rate: 0.00063795]
		[batch 20/20] avg loss: 0.008183167823899688		[learning rate: 0.00063718]
	Learning Rate: 0.000637182
	LOSS [training: 0.009617264787750745 | validation: 0.0022193485800740364]
	TIME [epoch: 8.2 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007952122294567269		[learning rate: 0.00063641]
		[batch 20/20] avg loss: 0.006958193544528335		[learning rate: 0.00063564]
	Learning Rate: 0.000635639
	LOSS [training: 0.007455157919547801 | validation: 0.006948942604658226]
	TIME [epoch: 8.18 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004555642435933361		[learning rate: 0.00063487]
		[batch 20/20] avg loss: 0.0022614814256821446		[learning rate: 0.0006341]
	Learning Rate: 0.0006341
	LOSS [training: 0.0034085619308077525 | validation: -0.0036717789176288346]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_1238.pth
	Model improved!!!
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009056949719229709		[learning rate: 0.00063333]
		[batch 20/20] avg loss: 0.009938468474486335		[learning rate: 0.00063257]
	Learning Rate: 0.000632565
	LOSS [training: 0.009497709096858023 | validation: -0.0026006593737386537]
	TIME [epoch: 8.18 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002985482035907331		[learning rate: 0.0006318]
		[batch 20/20] avg loss: 0.0034939351672130573		[learning rate: 0.00063103]
	Learning Rate: 0.000631034
	LOSS [training: 0.003239708601560194 | validation: 0.001407663530567415]
	TIME [epoch: 8.19 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006890696062974305		[learning rate: 0.00063027]
		[batch 20/20] avg loss: 0.010962975634859203		[learning rate: 0.00062951]
	Learning Rate: 0.000629506
	LOSS [training: 0.008926835848916754 | validation: 0.0039017104394173793]
	TIME [epoch: 8.17 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030707283218191		[learning rate: 0.00062874]
		[batch 20/20] avg loss: 0.010327026651701193		[learning rate: 0.00062798]
	Learning Rate: 0.000627982
	LOSS [training: 0.006698877486760147 | validation: 0.012357113453532784]
	TIME [epoch: 8.2 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00832847929805545		[learning rate: 0.00062722]
		[batch 20/20] avg loss: 0.012093991946483744		[learning rate: 0.00062646]
	Learning Rate: 0.000626462
	LOSS [training: 0.010211235622269598 | validation: 0.005628653403676317]
	TIME [epoch: 8.17 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005059420470275354		[learning rate: 0.0006257]
		[batch 20/20] avg loss: 0.012073238778317048		[learning rate: 0.00062495]
	Learning Rate: 0.000624946
	LOSS [training: 0.0085663296242962 | validation: 0.0016236491888295553]
	TIME [epoch: 8.18 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0061108961536600745		[learning rate: 0.00062419]
		[batch 20/20] avg loss: 0.02201640855287389		[learning rate: 0.00062343]
	Learning Rate: 0.000623433
	LOSS [training: 0.014063652353266985 | validation: 0.011178704521182038]
	TIME [epoch: 8.16 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010519298429289815		[learning rate: 0.00062268]
		[batch 20/20] avg loss: 0.011880560549205886		[learning rate: 0.00062192]
	Learning Rate: 0.000621923
	LOSS [training: 0.011199929489247851 | validation: 0.007893851949878133]
	TIME [epoch: 8.16 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010291180520867959		[learning rate: 0.00062117]
		[batch 20/20] avg loss: 0.004383552428262915		[learning rate: 0.00062042]
	Learning Rate: 0.000620418
	LOSS [training: 0.007337366474565436 | validation: 0.0048845370996868625]
	TIME [epoch: 8.15 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005939870656087235		[learning rate: 0.00061967]
		[batch 20/20] avg loss: 0.012047655891370733		[learning rate: 0.00061892]
	Learning Rate: 0.000618916
	LOSS [training: 0.008993763273728982 | validation: 0.010018103620563394]
	TIME [epoch: 8.21 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007528032914388109		[learning rate: 0.00061817]
		[batch 20/20] avg loss: 0.008066767398290497		[learning rate: 0.00061742]
	Learning Rate: 0.000617418
	LOSS [training: 0.007797400156339304 | validation: -0.004348921882193236]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_1249.pth
	Model improved!!!
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010019060438570114		[learning rate: 0.00061667]
		[batch 20/20] avg loss: 0.004486941494908499		[learning rate: 0.00061592]
	Learning Rate: 0.000615923
	LOSS [training: 0.002744423769382755 | validation: 0.0035311271335715963]
	TIME [epoch: 8.17 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012153246154882285		[learning rate: 0.00061518]
		[batch 20/20] avg loss: -0.001339914118931698		[learning rate: 0.00061443]
	Learning Rate: 0.000614432
	LOSS [training: 0.005406666017975294 | validation: 0.00015443595944755273]
	TIME [epoch: 8.17 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007832068603721416		[learning rate: 0.00061369]
		[batch 20/20] avg loss: 0.00889118904914705		[learning rate: 0.00061294]
	Learning Rate: 0.000612944
	LOSS [training: 0.008361628826434235 | validation: 0.01392809012408732]
	TIME [epoch: 8.22 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001135602928825977		[learning rate: 0.0006122]
		[batch 20/20] avg loss: 0.013593063993787504		[learning rate: 0.00061146]
	Learning Rate: 0.000611461
	LOSS [training: 0.007364333461306739 | validation: 0.00272251092574868]
	TIME [epoch: 8.17 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006557616917063818		[learning rate: 0.00061072]
		[batch 20/20] avg loss: 0.004942483318822811		[learning rate: 0.00060998]
	Learning Rate: 0.00060998
	LOSS [training: 0.0057500501179433144 | validation: 0.009316871508305239]
	TIME [epoch: 8.16 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010222035480259701		[learning rate: 0.00060924]
		[batch 20/20] avg loss: 0.011800583916026114		[learning rate: 0.0006085]
	Learning Rate: 0.000608504
	LOSS [training: 0.011011309698142905 | validation: 0.00031526997562462836]
	TIME [epoch: 8.16 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005301669610465239		[learning rate: 0.00060777]
		[batch 20/20] avg loss: 0.004589436806016707		[learning rate: 0.00060703]
	Learning Rate: 0.00060703
	LOSS [training: 0.004945553208240972 | validation: 0.0018374575443752708]
	TIME [epoch: 8.17 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028600994359309095		[learning rate: 0.0006063]
		[batch 20/20] avg loss: 0.008947316910377647		[learning rate: 0.00060556]
	Learning Rate: 0.000605561
	LOSS [training: 0.005903708173154278 | validation: -0.0006954774742220686]
	TIME [epoch: 8.17 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012288572105443056		[learning rate: 0.00060483]
		[batch 20/20] avg loss: 0.00376177238298618		[learning rate: 0.00060409]
	Learning Rate: 0.000604095
	LOSS [training: 0.00802517224421462 | validation: 0.005966779837919516]
	TIME [epoch: 8.15 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005983617932588034		[learning rate: 0.00060336]
		[batch 20/20] avg loss: 0.0029485828019049595		[learning rate: 0.00060263]
	Learning Rate: 0.000602633
	LOSS [training: 0.004466100367246496 | validation: -0.0005098292587726438]
	TIME [epoch: 8.16 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0072800109225171035		[learning rate: 0.0006019]
		[batch 20/20] avg loss: 0.0005311462526845661		[learning rate: 0.00060117]
	Learning Rate: 0.000601174
	LOSS [training: 0.003905578587600835 | validation: 0.0006505133475492925]
	TIME [epoch: 8.18 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006599532865460099		[learning rate: 0.00060045]
		[batch 20/20] avg loss: 0.0022794410017551543		[learning rate: 0.00059972]
	Learning Rate: 0.000599718
	LOSS [training: 0.004439486933607627 | validation: -0.00020972898548713695]
	TIME [epoch: 8.19 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003785316280022279		[learning rate: 0.00059899]
		[batch 20/20] avg loss: 0.0104977474341434		[learning rate: 0.00059827]
	Learning Rate: 0.000598267
	LOSS [training: 0.00714153185708284 | validation: 0.002836145280576426]
	TIME [epoch: 8.17 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008784487119305815		[learning rate: 0.00059754]
		[batch 20/20] avg loss: 0.006521726283877885		[learning rate: 0.00059682]
	Learning Rate: 0.000596818
	LOSS [training: 0.003700087497904233 | validation: 0.004074385241778982]
	TIME [epoch: 8.16 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016581239694680724		[learning rate: 0.0005961]
		[batch 20/20] avg loss: 0.009816964124164801		[learning rate: 0.00059537]
	Learning Rate: 0.000595373
	LOSS [training: 0.013199101909422764 | validation: -0.00021609390497887102]
	TIME [epoch: 8.18 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016381835229500742		[learning rate: 0.00059465]
		[batch 20/20] avg loss: 0.006502559857640841		[learning rate: 0.00059393]
	Learning Rate: 0.000593932
	LOSS [training: 0.004070371690295458 | validation: 0.0036691921640506072]
	TIME [epoch: 8.2 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00393907001259859		[learning rate: 0.00059321]
		[batch 20/20] avg loss: 0.0008937051406644004		[learning rate: 0.00059249]
	Learning Rate: 0.000592494
	LOSS [training: 0.002416387576631496 | validation: -0.000289180020907612]
	TIME [epoch: 8.16 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011939143876883624		[learning rate: 0.00059178]
		[batch 20/20] avg loss: 0.007703141822336274		[learning rate: 0.00059106]
	Learning Rate: 0.00059106
	LOSS [training: 0.009821142849609948 | validation: 0.007864166358800152]
	TIME [epoch: 8.15 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025172600335606997		[learning rate: 0.00059034]
		[batch 20/20] avg loss: 0.00730257663081924		[learning rate: 0.00058963]
	Learning Rate: 0.000589629
	LOSS [training: 0.004909918332189969 | validation: 0.004995737342826014]
	TIME [epoch: 8.18 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006252437281333313		[learning rate: 0.00058892]
		[batch 20/20] avg loss: 0.0022508947340968513		[learning rate: 0.0005882]
	Learning Rate: 0.000588202
	LOSS [training: 0.004251666007715083 | validation: 0.007828163733298423]
	TIME [epoch: 8.16 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008896766501335043		[learning rate: 0.00058749]
		[batch 20/20] avg loss: 0.00423188776134307		[learning rate: 0.00058678]
	Learning Rate: 0.000586778
	LOSS [training: 0.006564327131339058 | validation: 0.013746913399487078]
	TIME [epoch: 8.16 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00950565588973315		[learning rate: 0.00058607]
		[batch 20/20] avg loss: 0.006305051430339513		[learning rate: 0.00058536]
	Learning Rate: 0.000585357
	LOSS [training: 0.007905353660036332 | validation: 0.003663476724023305]
	TIME [epoch: 8.17 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005877913958249446		[learning rate: 0.00058465]
		[batch 20/20] avg loss: 0.006901757688735115		[learning rate: 0.00058394]
	Learning Rate: 0.00058394
	LOSS [training: 0.0063898358234922795 | validation: 0.0049163064329493045]
	TIME [epoch: 8.2 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025817606662680826		[learning rate: 0.00058323]
		[batch 20/20] avg loss: 0.0006213168636347219		[learning rate: 0.00058253]
	Learning Rate: 0.000582527
	LOSS [training: 0.0016015387649514017 | validation: 0.00018091197189728093]
	TIME [epoch: 8.17 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010132112825540277		[learning rate: 0.00058182]
		[batch 20/20] avg loss: 0.0012787858669471774		[learning rate: 0.00058112]
	Learning Rate: 0.000581116
	LOSS [training: 0.005705449346243725 | validation: 0.003423772543227602]
	TIME [epoch: 8.16 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009793427838665434		[learning rate: 0.00058041]
		[batch 20/20] avg loss: 0.004727595140889397		[learning rate: 0.00057971]
	Learning Rate: 0.00057971
	LOSS [training: 0.0072605114897774155 | validation: 0.015780169519594434]
	TIME [epoch: 8.19 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009465793837414135		[learning rate: 0.00057901]
		[batch 20/20] avg loss: 0.007849990375159441		[learning rate: 0.00057831]
	Learning Rate: 0.000578306
	LOSS [training: 0.008657892106286789 | validation: 0.0043692116364675545]
	TIME [epoch: 8.18 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004967190410278729		[learning rate: 0.00057761]
		[batch 20/20] avg loss: 0.00530610529958404		[learning rate: 0.00057691]
	Learning Rate: 0.000576906
	LOSS [training: 0.0051366478549313855 | validation: 0.0021003366021996764]
	TIME [epoch: 8.17 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004370374197964099		[learning rate: 0.00057621]
		[batch 20/20] avg loss: 0.005326122512965449		[learning rate: 0.00057551]
	Learning Rate: 0.00057551
	LOSS [training: 0.004848248355464773 | validation: 0.006650000520821548]
	TIME [epoch: 8.15 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00614739949046124		[learning rate: 0.00057481]
		[batch 20/20] avg loss: -0.001082749818628741		[learning rate: 0.00057412]
	Learning Rate: 0.000574117
	LOSS [training: 0.0025323248359162493 | validation: 0.0047036531390909336]
	TIME [epoch: 8.16 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006435652800610902		[learning rate: 0.00057342]
		[batch 20/20] avg loss: 0.004360790659894204		[learning rate: 0.00057273]
	Learning Rate: 0.000572727
	LOSS [training: 0.005398221730252554 | validation: 0.005584886166137053]
	TIME [epoch: 8.16 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007056604332199623		[learning rate: 0.00057203]
		[batch 20/20] avg loss: 0.004452170905546762		[learning rate: 0.00057134]
	Learning Rate: 0.00057134
	LOSS [training: 0.005754387618873193 | validation: 0.0025538670196327013]
	TIME [epoch: 8.18 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006672126937304306		[learning rate: 0.00057065]
		[batch 20/20] avg loss: 0.013308233275876626		[learning rate: 0.00056996]
	Learning Rate: 0.000569957
	LOSS [training: 0.009990180106590465 | validation: 0.01201259874938525]
	TIME [epoch: 8.18 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00865915499190471		[learning rate: 0.00056927]
		[batch 20/20] avg loss: 0.0025334247562691864		[learning rate: 0.00056858]
	Learning Rate: 0.000568577
	LOSS [training: 0.005596289874086948 | validation: 0.013728322029517862]
	TIME [epoch: 8.17 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008895878808739776		[learning rate: 0.00056789]
		[batch 20/20] avg loss: 0.007392574469235885		[learning rate: 0.0005672]
	Learning Rate: 0.000567201
	LOSS [training: 0.008144226638987829 | validation: -0.0035119076764492507]
	TIME [epoch: 8.17 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004143483303697007		[learning rate: 0.00056651]
		[batch 20/20] avg loss: 0.0010552965997381997		[learning rate: 0.00056583]
	Learning Rate: 0.000565828
	LOSS [training: 0.002599389951717603 | validation: 0.0033545652813431063]
	TIME [epoch: 8.19 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008490668874368716		[learning rate: 0.00056514]
		[batch 20/20] avg loss: 0.0028269755669992447		[learning rate: 0.00056446]
	Learning Rate: 0.000564458
	LOSS [training: 0.005658822220683979 | validation: 0.004051057242484859]
	TIME [epoch: 8.19 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009682024264205506		[learning rate: 0.00056377]
		[batch 20/20] avg loss: 0.005908837641855931		[learning rate: 0.00056309]
	Learning Rate: 0.000563092
	LOSS [training: 0.0077954309530307185 | validation: 0.008965523383211873]
	TIME [epoch: 8.15 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006036759725079383		[learning rate: 0.00056241]
		[batch 20/20] avg loss: 0.008710741043185541		[learning rate: 0.00056173]
	Learning Rate: 0.000561728
	LOSS [training: 0.007373750384132462 | validation: 0.008385970823467138]
	TIME [epoch: 8.16 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010254272113461655		[learning rate: 0.00056105]
		[batch 20/20] avg loss: 0.0001980663969078549		[learning rate: 0.00056037]
	Learning Rate: 0.000560369
	LOSS [training: 0.005226169255184755 | validation: 0.0062302658672943375]
	TIME [epoch: 8.18 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009552317232311192		[learning rate: 0.00055969]
		[batch 20/20] avg loss: -0.002718278462751395		[learning rate: 0.00055901]
	Learning Rate: 0.000559012
	LOSS [training: 0.003417019384779898 | validation: -0.002638525682889553]
	TIME [epoch: 8.16 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8909445333478045e-05		[learning rate: 0.00055833]
		[batch 20/20] avg loss: 0.005579178934157958		[learning rate: 0.00055766]
	Learning Rate: 0.000557659
	LOSS [training: 0.0028040441897457186 | validation: -0.003896600392965498]
	TIME [epoch: 8.15 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004977000692662333		[learning rate: 0.00055698]
		[batch 20/20] avg loss: 0.004943726967433023		[learning rate: 0.00055631]
	Learning Rate: 0.000556309
	LOSS [training: 0.0022230134490833957 | validation: 0.002499898862170468]
	TIME [epoch: 8.16 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0055998410414752255		[learning rate: 0.00055563]
		[batch 20/20] avg loss: 0.006943435945663365		[learning rate: 0.00055496]
	Learning Rate: 0.000554962
	LOSS [training: 0.0006717974520940692 | validation: -0.006087920336806064]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_1293.pth
	Model improved!!!
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004872792556197571		[learning rate: 0.00055429]
		[batch 20/20] avg loss: 0.004436784629045774		[learning rate: 0.00055362]
	Learning Rate: 0.000553618
	LOSS [training: 0.004654788592621671 | validation: 0.0015145870623970883]
	TIME [epoch: 8.18 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003425683100839611		[learning rate: 0.00055295]
		[batch 20/20] avg loss: 0.0036237473294986056		[learning rate: 0.00055228]
	Learning Rate: 0.000552278
	LOSS [training: 0.0035247152151691085 | validation: 0.00357065389419996]
	TIME [epoch: 8.17 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010127786640968774		[learning rate: 0.00055161]
		[batch 20/20] avg loss: 0.006523461782914064		[learning rate: 0.00055094]
	Learning Rate: 0.000550941
	LOSS [training: 0.008325624211941418 | validation: 0.012992414580065935]
	TIME [epoch: 8.16 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010259252973429212		[learning rate: 0.00055027]
		[batch 20/20] avg loss: 0.006889371716528672		[learning rate: 0.00054961]
	Learning Rate: 0.000549608
	LOSS [training: 0.008574312344978942 | validation: 0.013447958129241598]
	TIME [epoch: 8.21 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00904188133942513		[learning rate: 0.00054894]
		[batch 20/20] avg loss: 0.009677363661680934		[learning rate: 0.00054828]
	Learning Rate: 0.000548277
	LOSS [training: 0.009359622500553035 | validation: 0.012445521045976113]
	TIME [epoch: 8.17 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005680699420192897		[learning rate: 0.00054761]
		[batch 20/20] avg loss: 0.008457002246041654		[learning rate: 0.00054695]
	Learning Rate: 0.00054695
	LOSS [training: 0.0070688508331172755 | validation: 0.0010389849751875396]
	TIME [epoch: 8.16 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011436186908941556		[learning rate: 0.00054629]
		[batch 20/20] avg loss: 0.009259360600152344		[learning rate: 0.00054563]
	Learning Rate: 0.000545626
	LOSS [training: 0.00520148964552325 | validation: -0.0030870888465461795]
	TIME [epoch: 8.15 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032394237997639278		[learning rate: 0.00054496]
		[batch 20/20] avg loss: 0.001662220596861551		[learning rate: 0.0005443]
	Learning Rate: 0.000544305
	LOSS [training: 0.0024508221983127387 | validation: 0.003695225173242833]
	TIME [epoch: 8.18 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00020961922264552802		[learning rate: 0.00054365]
		[batch 20/20] avg loss: 0.0055776614230898264		[learning rate: 0.00054299]
	Learning Rate: 0.000542987
	LOSS [training: 0.0028936403228676764 | validation: 0.011648968391737539]
	TIME [epoch: 8.16 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002212222701154225		[learning rate: 0.00054233]
		[batch 20/20] avg loss: 0.011893108777372902		[learning rate: 0.00054167]
	Learning Rate: 0.000541673
	LOSS [training: 0.007052665739263562 | validation: 0.004491591332300705]
	TIME [epoch: 8.16 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007120339122887552		[learning rate: 0.00054102]
		[batch 20/20] avg loss: 0.003970990160449608		[learning rate: 0.00054036]
	Learning Rate: 0.000540361
	LOSS [training: 0.00554566464166858 | validation: 0.003450710038660679]
	TIME [epoch: 8.19 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027226496870964697		[learning rate: 0.00053971]
		[batch 20/20] avg loss: 0.0016563698494655107		[learning rate: 0.00053905]
	Learning Rate: 0.000539053
	LOSS [training: 0.00218950976828099 | validation: -0.0028396584696139173]
	TIME [epoch: 8.19 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015292431629160793		[learning rate: 0.0005384]
		[batch 20/20] avg loss: 0.006413999540108113		[learning rate: 0.00053775]
	Learning Rate: 0.000537748
	LOSS [training: 0.003971621351512097 | validation: 0.003474930143673058]
	TIME [epoch: 8.16 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005002089422498532		[learning rate: 0.0005371]
		[batch 20/20] avg loss: 0.006518270195438334		[learning rate: 0.00053645]
	Learning Rate: 0.000536446
	LOSS [training: 0.005760179808968433 | validation: 0.006638209403841383]
	TIME [epoch: 8.16 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007035888827189965		[learning rate: 0.0005358]
		[batch 20/20] avg loss: 0.012653029214893552		[learning rate: 0.00053515]
	Learning Rate: 0.000535148
	LOSS [training: 0.009844459021041758 | validation: 0.005517538801453754]
	TIME [epoch: 8.2 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004584180628757902		[learning rate: 0.0005345]
		[batch 20/20] avg loss: 0.0030935295726881766		[learning rate: 0.00053385]
	Learning Rate: 0.000533852
	LOSS [training: 0.0038388551007230397 | validation: 0.004295322440589286]
	TIME [epoch: 8.18 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008332214044381225		[learning rate: 0.00053321]
		[batch 20/20] avg loss: 0.0070393991740154965		[learning rate: 0.00053256]
	Learning Rate: 0.00053256
	LOSS [training: 0.00768580660919836 | validation: 0.015442819705455562]
	TIME [epoch: 8.16 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008841877924697922		[learning rate: 0.00053191]
		[batch 20/20] avg loss: 0.004018271600929594		[learning rate: 0.00053127]
	Learning Rate: 0.000531271
	LOSS [training: 0.006430074762813759 | validation: -0.0025097644213445433]
	TIME [epoch: 8.15 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004848614640978258		[learning rate: 0.00053063]
		[batch 20/20] avg loss: 0.007683636152356616		[learning rate: 0.00052998]
	Learning Rate: 0.000529985
	LOSS [training: 0.006266125396667437 | validation: 0.007539772008987446]
	TIME [epoch: 8.15 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007411346194778791		[learning rate: 0.00052934]
		[batch 20/20] avg loss: 0.002879318005679393		[learning rate: 0.0005287]
	Learning Rate: 0.000528702
	LOSS [training: 0.005145332100229092 | validation: -0.0032759193516137598]
	TIME [epoch: 8.17 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009161370450797145		[learning rate: 0.00052806]
		[batch 20/20] avg loss: 0.008389809595043747		[learning rate: 0.00052742]
	Learning Rate: 0.000527422
	LOSS [training: 0.00465297332006173 | validation: 0.009297884338990045]
	TIME [epoch: 8.15 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004303663757809435		[learning rate: 0.00052678]
		[batch 20/20] avg loss: 0.00421516127010735		[learning rate: 0.00052614]
	Learning Rate: 0.000526145
	LOSS [training: 0.004259412513958392 | validation: -0.0030706378992773706]
	TIME [epoch: 8.16 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006553575131960784		[learning rate: 0.00052551]
		[batch 20/20] avg loss: 0.006017715429902729		[learning rate: 0.00052487]
	Learning Rate: 0.000524871
	LOSS [training: 0.0033365364715494034 | validation: 0.004926283565537523]
	TIME [epoch: 8.18 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015594814063536078		[learning rate: 0.00052424]
		[batch 20/20] avg loss: 0.004937574269804071		[learning rate: 0.0005236]
	Learning Rate: 0.0005236
	LOSS [training: 0.003248527838078839 | validation: 0.009158766709093344]
	TIME [epoch: 8.19 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012206038640283917		[learning rate: 0.00052297]
		[batch 20/20] avg loss: 0.010773379054506206		[learning rate: 0.00052233]
	Learning Rate: 0.000522333
	LOSS [training: 0.011489708847395062 | validation: 0.008371382930857214]
	TIME [epoch: 8.16 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002263196045106512		[learning rate: 0.0005217]
		[batch 20/20] avg loss: 0.0058240932336730816		[learning rate: 0.00052107]
	Learning Rate: 0.000521068
	LOSS [training: 0.004043644639389796 | validation: 0.007397161656337856]
	TIME [epoch: 8.18 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012305238460377295		[learning rate: 0.00052044]
		[batch 20/20] avg loss: 0.006943696499539687		[learning rate: 0.00051981]
	Learning Rate: 0.000519807
	LOSS [training: 0.00962446747995849 | validation: 0.009825946071957994]
	TIME [epoch: 8.17 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0044670143986883255		[learning rate: 0.00051918]
		[batch 20/20] avg loss: 0.004539979732949751		[learning rate: 0.00051855]
	Learning Rate: 0.000518549
	LOSS [training: 0.004503497065819038 | validation: 0.002313414896472553]
	TIME [epoch: 8.21 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010277441597873074		[learning rate: 0.00051792]
		[batch 20/20] avg loss: 0.006627014972716147		[learning rate: 0.00051729]
	Learning Rate: 0.000517293
	LOSS [training: 0.00845222828529461 | validation: 0.006559696764009007]
	TIME [epoch: 8.16 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013446524002227015		[learning rate: 0.00051667]
		[batch 20/20] avg loss: 0.007666874323081054		[learning rate: 0.00051604]
	Learning Rate: 0.000516041
	LOSS [training: 0.010556699162654035 | validation: 0.00949475388133219]
	TIME [epoch: 8.15 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011698634005481087		[learning rate: 0.00051542]
		[batch 20/20] avg loss: 0.0036622960497935558		[learning rate: 0.00051479]
	Learning Rate: 0.000514792
	LOSS [training: 0.007680465027637322 | validation: 0.010177317223649758]
	TIME [epoch: 8.15 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010263826397692829		[learning rate: 0.00051417]
		[batch 20/20] avg loss: 0.0024728126752401355		[learning rate: 0.00051355]
	Learning Rate: 0.000513545
	LOSS [training: 0.001749597657504709 | validation: 0.0015995790592736705]
	TIME [epoch: 8.18 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010407986334896098		[learning rate: 0.00051292]
		[batch 20/20] avg loss: 0.012249126074730226		[learning rate: 0.0005123]
	Learning Rate: 0.000512302
	LOSS [training: 0.006644962354109919 | validation: 0.009102659302919425]
	TIME [epoch: 8.16 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0050139634655656596		[learning rate: 0.00051168]
		[batch 20/20] avg loss: 0.012644025818886318		[learning rate: 0.00051106]
	Learning Rate: 0.000511062
	LOSS [training: 0.00882899464222599 | validation: 0.005297136322099239]
	TIME [epoch: 8.15 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01221765331342178		[learning rate: 0.00051044]
		[batch 20/20] avg loss: 0.01133377527017935		[learning rate: 0.00050982]
	Learning Rate: 0.000509825
	LOSS [training: 0.011775714291800563 | validation: 0.010821425722454977]
	TIME [epoch: 8.17 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008143408783636345		[learning rate: 0.00050921]
		[batch 20/20] avg loss: 0.01011641909611525		[learning rate: 0.00050859]
	Learning Rate: 0.000508591
	LOSS [training: 0.009129913939875797 | validation: -0.0015310748193089474]
	TIME [epoch: 8.19 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005500176423168604		[learning rate: 0.00050797]
		[batch 20/20] avg loss: 0.01094810013309592		[learning rate: 0.00050736]
	Learning Rate: 0.00050736
	LOSS [training: 0.008224138278132263 | validation: 0.00920574241245586]
	TIME [epoch: 8.16 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013728249859938008		[learning rate: 0.00050675]
		[batch 20/20] avg loss: 0.011478404122716585		[learning rate: 0.00050613]
	Learning Rate: 0.000506131
	LOSS [training: 0.0126033269913273 | validation: 0.0017327948496149873]
	TIME [epoch: 8.16 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007205896276965865		[learning rate: 0.00050552]
		[batch 20/20] avg loss: -0.000517467543100957		[learning rate: 0.00050491]
	Learning Rate: 0.000504906
	LOSS [training: 0.0033442143669324548 | validation: 0.007491085857648333]
	TIME [epoch: 8.19 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006884535474792955		[learning rate: 0.00050429]
		[batch 20/20] avg loss: 0.002309976155261753		[learning rate: 0.00050368]
	Learning Rate: 0.000503684
	LOSS [training: 0.0045972558150273535 | validation: -0.0019149766952332248]
	TIME [epoch: 8.2 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005266555581802673		[learning rate: 0.00050307]
		[batch 20/20] avg loss: 0.0035910376199883563		[learning rate: 0.00050246]
	Learning Rate: 0.000502464
	LOSS [training: 0.004428796600895515 | validation: 0.011241138928932389]
	TIME [epoch: 8.16 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007877408306214714		[learning rate: 0.00050186]
		[batch 20/20] avg loss: 0.007851655485352377		[learning rate: 0.00050125]
	Learning Rate: 0.000501248
	LOSS [training: 0.007864531895783546 | validation: -0.0011590953672262574]
	TIME [epoch: 8.15 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009004631853076806		[learning rate: 0.00050064]
		[batch 20/20] avg loss: 0.004391970722563575		[learning rate: 0.00050003]
	Learning Rate: 0.000500034
	LOSS [training: 0.00669830128782019 | validation: 0.007535536477597795]
	TIME [epoch: 8.16 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033631765055367963		[learning rate: 0.00049943]
		[batch 20/20] avg loss: 0.008767051142232803		[learning rate: 0.00049882]
	Learning Rate: 0.000498824
	LOSS [training: 0.0060651138238848 | validation: 0.011924321921122218]
	TIME [epoch: 8.18 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004799234784939982		[learning rate: 0.00049822]
		[batch 20/20] avg loss: 0.011185802114177591		[learning rate: 0.00049762]
	Learning Rate: 0.000497616
	LOSS [training: 0.007992518449558787 | validation: -0.0007212549964838019]
	TIME [epoch: 8.16 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00525957190786173		[learning rate: 0.00049701]
		[batch 20/20] avg loss: 0.0027260590147450284		[learning rate: 0.00049641]
	Learning Rate: 0.000496412
	LOSS [training: 0.0039928154613033786 | validation: -0.002515649118397689]
	TIME [epoch: 8.16 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003835468477152746		[learning rate: 0.00049581]
		[batch 20/20] avg loss: 0.00636895651487074		[learning rate: 0.00049521]
	Learning Rate: 0.00049521
	LOSS [training: 0.005102212496011743 | validation: 0.0063430782744348605]
	TIME [epoch: 8.16 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011383018729643667		[learning rate: 0.00049461]
		[batch 20/20] avg loss: 0.007600220547959448		[learning rate: 0.00049401]
	Learning Rate: 0.000494011
	LOSS [training: 0.009491619638801557 | validation: 0.0025705147564621953]
	TIME [epoch: 8.18 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003976280763677391		[learning rate: 0.00049341]
		[batch 20/20] avg loss: 0.00431015772238392		[learning rate: 0.00049282]
	Learning Rate: 0.000492815
	LOSS [training: 0.004143219243030655 | validation: -0.006386643933209086]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_1342.pth
	Model improved!!!
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010498799152466648		[learning rate: 0.00049222]
		[batch 20/20] avg loss: 0.0037107833222617117		[learning rate: 0.00049162]
	Learning Rate: 0.000491622
	LOSS [training: 0.002380331618754188 | validation: -0.0012821061710162522]
	TIME [epoch: 8.18 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002065220111609005		[learning rate: 0.00049103]
		[batch 20/20] avg loss: 0.0042098181091682115		[learning rate: 0.00049043]
	Learning Rate: 0.000490432
	LOSS [training: 0.0031375191103886076 | validation: 0.010984407512584398]
	TIME [epoch: 8.16 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006489011664052549		[learning rate: 0.00048984]
		[batch 20/20] avg loss: 0.005362527241274769		[learning rate: 0.00048924]
	Learning Rate: 0.000489245
	LOSS [training: 0.005925769452663659 | validation: -0.007123155540281112]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_1345.pth
	Model improved!!!
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0042390672635577616		[learning rate: 0.00048865]
		[batch 20/20] avg loss: 0.006177432519721324		[learning rate: 0.00048806]
	Learning Rate: 0.000488061
	LOSS [training: 0.005208249891639543 | validation: 0.004076552106820487]
	TIME [epoch: 8.2 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00042628386734810465		[learning rate: 0.00048747]
		[batch 20/20] avg loss: 0.0038993414447023164		[learning rate: 0.00048688]
	Learning Rate: 0.000486879
	LOSS [training: 0.002162812656025211 | validation: -0.0018004100809621347]
	TIME [epoch: 8.17 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003053037629629934		[learning rate: 0.00048629]
		[batch 20/20] avg loss: 0.007565758952397361		[learning rate: 0.0004857]
	Learning Rate: 0.0004857
	LOSS [training: 0.005309398291013649 | validation: 0.013837150510819046]
	TIME [epoch: 8.16 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012432657703875902		[learning rate: 0.00048511]
		[batch 20/20] avg loss: 0.001349998191688883		[learning rate: 0.00048452]
	Learning Rate: 0.000484525
	LOSS [training: 0.0068913279477823925 | validation: 0.0029490543965469598]
	TIME [epoch: 8.18 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00516789338967828		[learning rate: 0.00048394]
		[batch 20/20] avg loss: 0.0045280328439806046		[learning rate: 0.00048335]
	Learning Rate: 0.000483352
	LOSS [training: 0.004847963116829442 | validation: 0.0008071795300214229]
	TIME [epoch: 8.16 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011386510336039218		[learning rate: 0.00048277]
		[batch 20/20] avg loss: 0.009165303760377582		[learning rate: 0.00048218]
	Learning Rate: 0.000482181
	LOSS [training: 0.010275907048208402 | validation: 0.0012779251839373387]
	TIME [epoch: 8.16 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01107425394396128		[learning rate: 0.0004816]
		[batch 20/20] avg loss: 0.00019575377907781952		[learning rate: 0.00048101]
	Learning Rate: 0.000481014
	LOSS [training: 0.005635003861519551 | validation: 0.006421049016928789]
	TIME [epoch: 8.17 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007665897822086129		[learning rate: 0.00048043]
		[batch 20/20] avg loss: 0.001282707483577126		[learning rate: 0.00047985]
	Learning Rate: 0.00047985
	LOSS [training: 0.004474302652831628 | validation: -0.0004048214595354511]
	TIME [epoch: 8.2 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008668354268009945		[learning rate: 0.00047927]
		[batch 20/20] avg loss: 0.002151439108346532		[learning rate: 0.00047869]
	Learning Rate: 0.000478688
	LOSS [training: 0.005409896688178238 | validation: 0.004002356516891836]
	TIME [epoch: 8.17 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005509320460767879		[learning rate: 0.00047811]
		[batch 20/20] avg loss: 0.004639223894154828		[learning rate: 0.00047753]
	Learning Rate: 0.000477529
	LOSS [training: 0.005074272177461354 | validation: 0.00417874353043]
	TIME [epoch: 8.17 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008901718177485593		[learning rate: 0.00047695]
		[batch 20/20] avg loss: 4.722367146896065e-05		[learning rate: 0.00047637]
	Learning Rate: 0.000476373
	LOSS [training: 0.004474470924477278 | validation: 0.006190235014537901]
	TIME [epoch: 8.18 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012739971883715517		[learning rate: 0.0004758]
		[batch 20/20] avg loss: 0.006597807879822911		[learning rate: 0.00047522]
	Learning Rate: 0.00047522
	LOSS [training: 0.0039359025340972315 | validation: 0.006036998013734446]
	TIME [epoch: 8.2 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005150977923120675		[learning rate: 0.00047464]
		[batch 20/20] avg loss: 0.009606217047574757		[learning rate: 0.00047407]
	Learning Rate: 0.00047407
	LOSS [training: 0.007378597485347716 | validation: 0.011980879288194251]
	TIME [epoch: 8.16 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011543540473242856		[learning rate: 0.0004735]
		[batch 20/20] avg loss: 0.010167342178756667		[learning rate: 0.00047292]
	Learning Rate: 0.000472922
	LOSS [training: 0.01085544132599976 | validation: 0.003992468377269243]
	TIME [epoch: 8.16 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016230997519746208		[learning rate: 0.00047235]
		[batch 20/20] avg loss: 0.005528846656498746		[learning rate: 0.00047178]
	Learning Rate: 0.000471777
	LOSS [training: 0.003575973204236682 | validation: -0.0013649089493883006]
	TIME [epoch: 8.15 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008625765963094676		[learning rate: 0.00047121]
		[batch 20/20] avg loss: 0.014802092013190942		[learning rate: 0.00047063]
	Learning Rate: 0.000470635
	LOSS [training: 0.01171392898814281 | validation: 0.006571492387110938]
	TIME [epoch: 8.17 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00908556571073665		[learning rate: 0.00047006]
		[batch 20/20] avg loss: 0.008787716877509077		[learning rate: 0.0004695]
	Learning Rate: 0.000469496
	LOSS [training: 0.008936641294122864 | validation: 0.0032368321734503424]
	TIME [epoch: 8.18 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005354215115995426		[learning rate: 0.00046893]
		[batch 20/20] avg loss: 0.008936462009312317		[learning rate: 0.00046836]
	Learning Rate: 0.000468359
	LOSS [training: 0.007145338562653872 | validation: -0.00048164151147215856]
	TIME [epoch: 8.18 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00030322688256281		[learning rate: 0.00046779]
		[batch 20/20] avg loss: 0.010669414429514072		[learning rate: 0.00046723]
	Learning Rate: 0.000467225
	LOSS [training: 0.005183093773475631 | validation: 0.000199363292675145]
	TIME [epoch: 8.16 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018327188434028988		[learning rate: 0.00046666]
		[batch 20/20] avg loss: 0.003376224640908711		[learning rate: 0.00046609]
	Learning Rate: 0.000466094
	LOSS [training: 0.002604471742155805 | validation: 0.003648891410524687]
	TIME [epoch: 8.17 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010783844030711814		[learning rate: 0.00046553]
		[batch 20/20] avg loss: 0.005708658524376332		[learning rate: 0.00046497]
	Learning Rate: 0.000464966
	LOSS [training: 0.0033935214637237565 | validation: -0.0001625224983898433]
	TIME [epoch: 8.21 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004143131000863888		[learning rate: 0.0004644]
		[batch 20/20] avg loss: 0.0022099306025134087		[learning rate: 0.00046384]
	Learning Rate: 0.00046384
	LOSS [training: 0.0031765308016886485 | validation: 0.00163865787426774]
	TIME [epoch: 8.18 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005498565464766525		[learning rate: 0.00046328]
		[batch 20/20] avg loss: 0.004993610837815336		[learning rate: 0.00046272]
	Learning Rate: 0.000462717
	LOSS [training: 0.005246088151290929 | validation: 0.004628919672643205]
	TIME [epoch: 8.16 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005647583487738221		[learning rate: 0.00046216]
		[batch 20/20] avg loss: 0.0022565159426777043		[learning rate: 0.0004616]
	Learning Rate: 0.000461597
	LOSS [training: 0.003952049715207963 | validation: 0.004160921634935849]
	TIME [epoch: 8.16 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004137345922717717		[learning rate: 0.00046104]
		[batch 20/20] avg loss: 0.0015681640095798865		[learning rate: 0.00046048]
	Learning Rate: 0.00046048
	LOSS [training: 0.0028527549661488023 | validation: 0.001459673236401676]
	TIME [epoch: 8.18 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004088496546953769		[learning rate: 0.00045992]
		[batch 20/20] avg loss: 0.0023251162509329696		[learning rate: 0.00045937]
	Learning Rate: 0.000459365
	LOSS [training: 0.0032068063989433696 | validation: 0.003495180052732767]
	TIME [epoch: 8.16 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00872460735975659		[learning rate: 0.00045881]
		[batch 20/20] avg loss: 0.003715348953594323		[learning rate: 0.00045825]
	Learning Rate: 0.000458253
	LOSS [training: 0.006219978156675457 | validation: 0.0012873229690080232]
	TIME [epoch: 8.16 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0067392555702271584		[learning rate: 0.0004577]
		[batch 20/20] avg loss: 0.0031041427680625917		[learning rate: 0.00045714]
	Learning Rate: 0.000457144
	LOSS [training: 0.004921699169144875 | validation: -0.004652341863852282]
	TIME [epoch: 8.16 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000805829378393507		[learning rate: 0.00045659]
		[batch 20/20] avg loss: 0.008866041872066566		[learning rate: 0.00045604]
	Learning Rate: 0.000456037
	LOSS [training: 0.0048359356252300366 | validation: 0.00027855713528462944]
	TIME [epoch: 8.21 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037614699873268266		[learning rate: 0.00045548]
		[batch 20/20] avg loss: 0.006059442961211437		[learning rate: 0.00045493]
	Learning Rate: 0.000454933
	LOSS [training: 0.004910456474269133 | validation: 0.0005537093293186291]
	TIME [epoch: 8.17 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004733030385227861		[learning rate: 0.00045438]
		[batch 20/20] avg loss: 0.010935147653403164		[learning rate: 0.00045383]
	Learning Rate: 0.000453832
	LOSS [training: 0.007834089019315513 | validation: 0.003912275836870021]
	TIME [epoch: 8.17 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00903827401790695		[learning rate: 0.00045328]
		[batch 20/20] avg loss: 0.0025937378456125136		[learning rate: 0.00045273]
	Learning Rate: 0.000452733
	LOSS [training: 0.005816005931759732 | validation: -0.005564353254481116]
	TIME [epoch: 8.17 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005856908356153229		[learning rate: 0.00045218]
		[batch 20/20] avg loss: -0.0005319240320341633		[learning rate: 0.00045164]
	Learning Rate: 0.000451637
	LOSS [training: 0.0026624921620595336 | validation: 0.0008057990748960445]
	TIME [epoch: 8.2 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009095372953553064		[learning rate: 0.00045109]
		[batch 20/20] avg loss: 0.009521718433618804		[learning rate: 0.00045054]
	Learning Rate: 0.000450544
	LOSS [training: 0.009308545693585935 | validation: 0.00834914248280695]
	TIME [epoch: 8.19 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00786569428243595		[learning rate: 0.00045]
		[batch 20/20] avg loss: 0.0059395812229332675		[learning rate: 0.00044945]
	Learning Rate: 0.000449453
	LOSS [training: 0.006902637752684608 | validation: 0.0004205841247565921]
	TIME [epoch: 8.15 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005489466786459062		[learning rate: 0.00044891]
		[batch 20/20] avg loss: 0.007641679840791652		[learning rate: 0.00044836]
	Learning Rate: 0.000448365
	LOSS [training: 0.006565573313625356 | validation: 0.0034197855645123417]
	TIME [epoch: 8.16 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036437414571772416		[learning rate: 0.00044782]
		[batch 20/20] avg loss: -0.002489169263689692		[learning rate: 0.00044728]
	Learning Rate: 0.000447279
	LOSS [training: 0.0005772860967437751 | validation: -0.004667048033041417]
	TIME [epoch: 8.18 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001241815376469032		[learning rate: 0.00044674]
		[batch 20/20] avg loss: 0.004644198164463663		[learning rate: 0.0004462]
	Learning Rate: 0.000446197
	LOSS [training: 0.0029430067704663476 | validation: 0.010765767886772195]
	TIME [epoch: 8.16 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0140961100002575		[learning rate: 0.00044566]
		[batch 20/20] avg loss: 0.016379822712531987		[learning rate: 0.00044512]
	Learning Rate: 0.000445117
	LOSS [training: 0.015237966356394742 | validation: 0.009091758043788955]
	TIME [epoch: 8.15 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004545250066420585		[learning rate: 0.00044458]
		[batch 20/20] avg loss: -0.0027430941532653524		[learning rate: 0.00044404]
	Learning Rate: 0.000444039
	LOSS [training: 0.0009010779565776156 | validation: 0.0012786354574995946]
	TIME [epoch: 8.16 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019704338635694505		[learning rate: 0.0004435]
		[batch 20/20] avg loss: 0.0048843493712645404		[learning rate: 0.00044296]
	Learning Rate: 0.000442964
	LOSS [training: 0.0034273916174169957 | validation: 0.0016389451223814125]
	TIME [epoch: 8.2 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016293604947818583		[learning rate: 0.00044243]
		[batch 20/20] avg loss: 0.0012632813079746746		[learning rate: 0.00044189]
	Learning Rate: 0.000441892
	LOSS [training: 0.0014463209013782662 | validation: 0.003859351396123812]
	TIME [epoch: 8.17 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006888772953908859		[learning rate: 0.00044136]
		[batch 20/20] avg loss: 0.009390492910690515		[learning rate: 0.00044082]
	Learning Rate: 0.000440822
	LOSS [training: 0.008139632932299686 | validation: 0.0013087419514197516]
	TIME [epoch: 8.16 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004553357045369763		[learning rate: 0.00044029]
		[batch 20/20] avg loss: 0.003921462747899073		[learning rate: 0.00043975]
	Learning Rate: 0.000439755
	LOSS [training: 0.004237409896634418 | validation: 0.0108349279505421]
	TIME [epoch: 8.16 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0146722490579514		[learning rate: 0.00043922]
		[batch 20/20] avg loss: 0.018272686206658915		[learning rate: 0.00043869]
	Learning Rate: 0.00043869
	LOSS [training: 0.016472467632305157 | validation: 0.005871531168488186]
	TIME [epoch: 8.21 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008389200730561503		[learning rate: 0.00043816]
		[batch 20/20] avg loss: 0.013644842198340478		[learning rate: 0.00043763]
	Learning Rate: 0.000437628
	LOSS [training: 0.011017021464450992 | validation: 0.005669489721785744]
	TIME [epoch: 8.17 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008842175789280681		[learning rate: 0.0004371]
		[batch 20/20] avg loss: 0.012503450651943474		[learning rate: 0.00043657]
	Learning Rate: 0.000436569
	LOSS [training: 0.010672813220612077 | validation: 0.01978284350015632]
	TIME [epoch: 8.16 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013690935164367318		[learning rate: 0.00043604]
		[batch 20/20] avg loss: 0.0047338719784380924		[learning rate: 0.00043551]
	Learning Rate: 0.000435512
	LOSS [training: 0.009212403571402704 | validation: 0.006079089191606178]
	TIME [epoch: 8.16 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007418558684488807		[learning rate: 0.00043498]
		[batch 20/20] avg loss: 0.008902125208723745		[learning rate: 0.00043446]
	Learning Rate: 0.000434458
	LOSS [training: 0.008160341946606278 | validation: 0.0016025760158848042]
	TIME [epoch: 8.18 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008078659597751172		[learning rate: 0.00043393]
		[batch 20/20] avg loss: 0.001755029851451813		[learning rate: 0.00043341]
	Learning Rate: 0.000433406
	LOSS [training: 0.004916844724601492 | validation: 0.009910747522161808]
	TIME [epoch: 8.16 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008224760604408203		[learning rate: 0.00043288]
		[batch 20/20] avg loss: 0.00047909568138746627		[learning rate: 0.00043236]
	Learning Rate: 0.000432357
	LOSS [training: 0.004351928142897835 | validation: 0.004368568336405427]
	TIME [epoch: 8.16 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025433709746269842		[learning rate: 0.00043183]
		[batch 20/20] avg loss: 0.008699064790306452		[learning rate: 0.00043131]
	Learning Rate: 0.00043131
	LOSS [training: 0.005621217882466718 | validation: -0.005209591824692459]
	TIME [epoch: 8.16 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004307088873161362		[learning rate: 0.00043079]
		[batch 20/20] avg loss: 0.007907570330628392		[learning rate: 0.00043027]
	Learning Rate: 0.000430266
	LOSS [training: 0.006107329601894878 | validation: 0.004608620832274036]
	TIME [epoch: 8.21 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005922781302756794		[learning rate: 0.00042974]
		[batch 20/20] avg loss: 0.0023231362290020052		[learning rate: 0.00042922]
	Learning Rate: 0.000429224
	LOSS [training: 0.004122958765879399 | validation: 0.0008789651144864435]
	TIME [epoch: 8.17 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006613084261388039		[learning rate: 0.0004287]
		[batch 20/20] avg loss: 0.008562208266267284		[learning rate: 0.00042819]
	Learning Rate: 0.000428185
	LOSS [training: 0.004611758346203044 | validation: 0.004531733363488256]
	TIME [epoch: 8.17 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009527485918033042		[learning rate: 0.00042767]
		[batch 20/20] avg loss: 0.004304438637179593		[learning rate: 0.00042715]
	Learning Rate: 0.000427149
	LOSS [training: 0.006915962277606319 | validation: 0.007312155365427355]
	TIME [epoch: 8.17 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010339730590904492		[learning rate: 0.00042663]
		[batch 20/20] avg loss: 0.007716921079062504		[learning rate: 0.00042611]
	Learning Rate: 0.000426115
	LOSS [training: 0.009028325834983498 | validation: 0.007609325424640395]
	TIME [epoch: 8.21 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004576205838869325		[learning rate: 0.0004256]
		[batch 20/20] avg loss: 0.0024240917578796494		[learning rate: 0.00042508]
	Learning Rate: 0.000425083
	LOSS [training: 0.003500148798374487 | validation: -0.0004433552056091587]
	TIME [epoch: 8.16 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008301589046602598		[learning rate: 0.00042457]
		[batch 20/20] avg loss: 0.005913254620256781		[learning rate: 0.00042405]
	Learning Rate: 0.000424054
	LOSS [training: 0.003371706762458521 | validation: -0.0015630927390146176]
	TIME [epoch: 8.16 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004620670295344481		[learning rate: 0.00042354]
		[batch 20/20] avg loss: 0.008774995479745836		[learning rate: 0.00042303]
	Learning Rate: 0.000423027
	LOSS [training: 0.006697832887545156 | validation: 0.0065047610332892155]
	TIME [epoch: 8.16 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005122618966902961		[learning rate: 0.00042251]
		[batch 20/20] avg loss: 0.004713595091091628		[learning rate: 0.000422]
	Learning Rate: 0.000422003
	LOSS [training: 0.004918107028997295 | validation: 0.006928638142779472]
	TIME [epoch: 8.18 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005140124920273074		[learning rate: 0.00042149]
		[batch 20/20] avg loss: 0.011507801616698619		[learning rate: 0.00042098]
	Learning Rate: 0.000420982
	LOSS [training: 0.008323963268485845 | validation: 0.006542083371718611]
	TIME [epoch: 8.17 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014174898872944617		[learning rate: 0.00042047]
		[batch 20/20] avg loss: 0.011333564215571844		[learning rate: 0.00041996]
	Learning Rate: 0.000419963
	LOSS [training: 0.0063755270514331535 | validation: -0.0001440217417469124]
	TIME [epoch: 8.16 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00732565491564859		[learning rate: 0.00041945]
		[batch 20/20] avg loss: 0.007218981072079618		[learning rate: 0.00041895]
	Learning Rate: 0.000418946
	LOSS [training: 0.007272317993864104 | validation: 0.01024156646254313]
	TIME [epoch: 8.19 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007938040893258384		[learning rate: 0.00041844]
		[batch 20/20] avg loss: 0.003679143590697703		[learning rate: 0.00041793]
	Learning Rate: 0.000417932
	LOSS [training: 0.005808592241978043 | validation: 0.0004216513726979128]
	TIME [epoch: 8.2 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005532995680867739		[learning rate: 0.00041743]
		[batch 20/20] avg loss: 0.005799695885204095		[learning rate: 0.00041692]
	Learning Rate: 0.00041692
	LOSS [training: 0.005666345783035917 | validation: 0.001824098445094845]
	TIME [epoch: 8.17 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007932861176377572		[learning rate: 0.00041641]
		[batch 20/20] avg loss: -0.0009654458804346303		[learning rate: 0.00041591]
	Learning Rate: 0.000415911
	LOSS [training: 0.0034837076479714716 | validation: 0.0001337942428436159]
	TIME [epoch: 8.17 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017814143981919697		[learning rate: 0.00041541]
		[batch 20/20] avg loss: 0.00255987209990365		[learning rate: 0.0004149]
	Learning Rate: 0.000414904
	LOSS [training: 0.0003892288508558402 | validation: 0.001707202468059302]
	TIME [epoch: 8.2 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034595628865472018		[learning rate: 0.0004144]
		[batch 20/20] avg loss: 0.005496845611176467		[learning rate: 0.0004139]
	Learning Rate: 0.000413899
	LOSS [training: 0.0044782042488618334 | validation: -0.00045128931673530347]
	TIME [epoch: 8.19 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009843172735047652		[learning rate: 0.0004134]
		[batch 20/20] avg loss: 0.0022041682259175363		[learning rate: 0.0004129]
	Learning Rate: 0.000412897
	LOSS [training: 0.006023670480482595 | validation: 0.007010810765593136]
	TIME [epoch: 8.16 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0123579332166892		[learning rate: 0.0004124]
		[batch 20/20] avg loss: 0.003282758408185289		[learning rate: 0.0004119]
	Learning Rate: 0.000411898
	LOSS [training: 0.007820345812437244 | validation: 0.0032081712837912723]
	TIME [epoch: 8.16 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008126611507717709		[learning rate: 0.0004114]
		[batch 20/20] avg loss: 0.012784699282728799		[learning rate: 0.0004109]
	Learning Rate: 0.000410901
	LOSS [training: 0.010455655395223255 | validation: 0.00495064492806528]
	TIME [epoch: 8.16 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008523151202867929		[learning rate: 0.0004104]
		[batch 20/20] avg loss: 0.012283740876762053		[learning rate: 0.00040991]
	Learning Rate: 0.000409906
	LOSS [training: 0.005715712878237629 | validation: -0.0030381691534517004]
	TIME [epoch: 8.18 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004503250436011271		[learning rate: 0.00040941]
		[batch 20/20] avg loss: 0.005164777072308855		[learning rate: 0.00040891]
	Learning Rate: 0.000408914
	LOSS [training: 0.004834013754160063 | validation: 0.004806710392389994]
	TIME [epoch: 8.16 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012158868007098621		[learning rate: 0.00040842]
		[batch 20/20] avg loss: 0.0017156411179725273		[learning rate: 0.00040792]
	Learning Rate: 0.000407924
	LOSS [training: 0.0014657639593411945 | validation: 0.0032580419147720404]
	TIME [epoch: 8.16 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006695227227739469		[learning rate: 0.00040743]
		[batch 20/20] avg loss: 0.002147767658986945		[learning rate: 0.00040694]
	Learning Rate: 0.000406936
	LOSS [training: 0.004421497443363206 | validation: 0.003781168703399817]
	TIME [epoch: 8.16 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00530291587074164		[learning rate: 0.00040644]
		[batch 20/20] avg loss: 0.0017489682712940383		[learning rate: 0.00040595]
	Learning Rate: 0.000405951
	LOSS [training: 0.0035259420710178403 | validation: 0.006413032384604493]
	TIME [epoch: 8.21 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009712663579260761		[learning rate: 0.00040546]
		[batch 20/20] avg loss: 0.004542189032095543		[learning rate: 0.00040497]
	Learning Rate: 0.000404968
	LOSS [training: 0.0027567276950108094 | validation: -0.002163124003743409]
	TIME [epoch: 8.17 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005136728815887377		[learning rate: 0.00040448]
		[batch 20/20] avg loss: -0.0013006609497008157		[learning rate: 0.00040399]
	Learning Rate: 0.000403988
	LOSS [training: -0.000393494034056039 | validation: 0.0010291443235974458]
	TIME [epoch: 8.16 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00470659728497526		[learning rate: 0.0004035]
		[batch 20/20] avg loss: 0.0021801310879047187		[learning rate: 0.00040301]
	Learning Rate: 0.00040301
	LOSS [training: 0.0034433641864399893 | validation: 0.0031062119492202877]
	TIME [epoch: 8.17 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006015757985044122		[learning rate: 0.00040252]
		[batch 20/20] avg loss: 0.005579496634610296		[learning rate: 0.00040203]
	Learning Rate: 0.000402034
	LOSS [training: 0.0057976273098272095 | validation: 0.006998386913567163]
	TIME [epoch: 8.22 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010622425929944768		[learning rate: 0.00040155]
		[batch 20/20] avg loss: 0.006530181869702022		[learning rate: 0.00040106]
	Learning Rate: 0.000401061
	LOSS [training: 0.008576303899823393 | validation: 0.005308688488633188]
	TIME [epoch: 8.17 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007663919594317623		[learning rate: 0.00040058]
		[batch 20/20] avg loss: 0.00248550601002642		[learning rate: 0.00040009]
	Learning Rate: 0.00040009
	LOSS [training: 0.005074712802172022 | validation: -0.002132284706451365]
	TIME [epoch: 8.16 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006223474005215233		[learning rate: 0.00039961]
		[batch 20/20] avg loss: 0.005207325105842829		[learning rate: 0.00039912]
	Learning Rate: 0.000399122
	LOSS [training: 0.0029148362531821763 | validation: 0.008755676846252107]
	TIME [epoch: 8.16 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012684534931498176		[learning rate: 0.00039864]
		[batch 20/20] avg loss: 0.0061591090045676495		[learning rate: 0.00039816]
	Learning Rate: 0.000398155
	LOSS [training: 0.009421821968032913 | validation: 0.005846132778892485]
	TIME [epoch: 8.17 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001828860067380672		[learning rate: 0.00039767]
		[batch 20/20] avg loss: -0.0014474893892151026		[learning rate: 0.00039719]
	Learning Rate: 0.000397192
	LOSS [training: -0.0016381747282978871 | validation: 0.001491703341158611]
	TIME [epoch: 8.17 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005794879980848999		[learning rate: 0.00039671]
		[batch 20/20] avg loss: 0.004891501114102642		[learning rate: 0.00039623]
	Learning Rate: 0.00039623
	LOSS [training: 0.002156006558008871 | validation: 0.0033285700869646953]
	TIME [epoch: 8.16 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0040366819598727875		[learning rate: 0.00039575]
		[batch 20/20] avg loss: 0.0011230674085177684		[learning rate: 0.00039527]
	Learning Rate: 0.000395271
	LOSS [training: 0.0025798746841952784 | validation: 0.006553350995428087]
	TIME [epoch: 8.16 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003206241224490344		[learning rate: 0.00039479]
		[batch 20/20] avg loss: 0.010203405358642684		[learning rate: 0.00039431]
	Learning Rate: 0.000394314
	LOSS [training: 0.006704823291566515 | validation: 0.0011700161109943783]
	TIME [epoch: 8.17 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016804814248495045		[learning rate: 0.00039384]
		[batch 20/20] avg loss: 0.0042198948288689865		[learning rate: 0.00039336]
	Learning Rate: 0.000393359
	LOSS [training: 0.0029501881268592456 | validation: -0.0011404779991311294]
	TIME [epoch: 8.18 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004479487994552422		[learning rate: 0.00039288]
		[batch 20/20] avg loss: 0.002503314139987313		[learning rate: 0.00039241]
	Learning Rate: 0.000392407
	LOSS [training: 0.0034914010672698675 | validation: 0.0049193858500207075]
	TIME [epoch: 8.19 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014550532696909304		[learning rate: 0.00039193]
		[batch 20/20] avg loss: 0.0035347768604665373		[learning rate: 0.00039146]
	Learning Rate: 0.000391457
	LOSS [training: 0.0024949150650787335 | validation: 0.010300915454324686]
	TIME [epoch: 8.16 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007513415435526112		[learning rate: 0.00039098]
		[batch 20/20] avg loss: 0.005511045991418196		[learning rate: 0.00039051]
	Learning Rate: 0.00039051
	LOSS [training: 0.0065122307134721545 | validation: -0.006116393399570701]
	TIME [epoch: 8.17 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006683842517520683		[learning rate: 0.00039004]
		[batch 20/20] avg loss: 0.0044818959202386445		[learning rate: 0.00038956]
	Learning Rate: 0.000389564
	LOSS [training: 0.005582869218879664 | validation: 2.9319252714365908e-05]
	TIME [epoch: 8.21 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004137897760637256		[learning rate: 0.00038909]
		[batch 20/20] avg loss: 0.0050806422306001985		[learning rate: 0.00038862]
	Learning Rate: 0.000388621
	LOSS [training: 0.004609269995618727 | validation: -0.000632080748038122]
	TIME [epoch: 8.18 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002700609908830328		[learning rate: 0.00038815]
		[batch 20/20] avg loss: 0.0019194689111898503		[learning rate: 0.00038768]
	Learning Rate: 0.00038768
	LOSS [training: 0.002310039410010089 | validation: 0.00044837499805169346]
	TIME [epoch: 8.16 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025711340743595954		[learning rate: 0.00038721]
		[batch 20/20] avg loss: 0.0013211074822092102		[learning rate: 0.00038674]
	Learning Rate: 0.000386742
	LOSS [training: 0.0019461207782844026 | validation: 0.0005791697250763727]
	TIME [epoch: 8.16 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007534576252548641		[learning rate: 0.00038627]
		[batch 20/20] avg loss: 0.0009012981176328772		[learning rate: 0.00038581]
	Learning Rate: 0.000385805
	LOSS [training: 0.00421793718509076 | validation: 0.002449107349962843]
	TIME [epoch: 8.19 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007701181139998102		[learning rate: 0.00038534]
		[batch 20/20] avg loss: 0.0009643546094224942		[learning rate: 0.00038487]
	Learning Rate: 0.000384872
	LOSS [training: 0.004332767874710298 | validation: 0.0068581927247250984]
	TIME [epoch: 8.16 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012611169346020693		[learning rate: 0.00038441]
		[batch 20/20] avg loss: 0.0020905009880400676		[learning rate: 0.00038394]
	Learning Rate: 0.00038394
	LOSS [training: 0.00735083516703038 | validation: -0.0020163250644623437]
	TIME [epoch: 8.16 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0063775217651576375		[learning rate: 0.00038347]
		[batch 20/20] avg loss: 0.00639200306033322		[learning rate: 0.00038301]
	Learning Rate: 0.00038301
	LOSS [training: 0.0063847624127454285 | validation: 0.00286348766289981]
	TIME [epoch: 8.15 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004091995422450093		[learning rate: 0.00038255]
		[batch 20/20] avg loss: 0.006645314861240809		[learning rate: 0.00038208]
	Learning Rate: 0.000382083
	LOSS [training: 0.00536865514184545 | validation: 0.002704910977148672]
	TIME [epoch: 8.21 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026872779670340757		[learning rate: 0.00038162]
		[batch 20/20] avg loss: 0.0010508575485588642		[learning rate: 0.00038116]
	Learning Rate: 0.000381158
	LOSS [training: 0.00186906775779647 | validation: -0.002276942518278853]
	TIME [epoch: 8.17 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023760323266355443		[learning rate: 0.0003807]
		[batch 20/20] avg loss: -5.77168946654643e-05		[learning rate: 0.00038024]
	Learning Rate: 0.000380235
	LOSS [training: 0.0011591577159850398 | validation: -0.000582893461194168]
	TIME [epoch: 8.17 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006051637131161287		[learning rate: 0.00037977]
		[batch 20/20] avg loss: 0.003963373214456133		[learning rate: 0.00037931]
	Learning Rate: 0.000379315
	LOSS [training: 0.005007505172808709 | validation: 0.0023452824129065004]
	TIME [epoch: 8.17 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013061853334592243		[learning rate: 0.00037886]
		[batch 20/20] avg loss: 0.004142343343884776		[learning rate: 0.0003784]
	Learning Rate: 0.000378397
	LOSS [training: 0.0014180790052127762 | validation: 0.0004915053419905766]
	TIME [epoch: 8.23 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004711161030390728		[learning rate: 0.00037794]
		[batch 20/20] avg loss: 0.01012034482177803		[learning rate: 0.00037748]
	Learning Rate: 0.000377481
	LOSS [training: 0.007415752926084379 | validation: 0.006732767355919931]
	TIME [epoch: 8.17 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004377781955608414		[learning rate: 0.00037702]
		[batch 20/20] avg loss: -0.0014239227383703213		[learning rate: 0.00037657]
	Learning Rate: 0.000376567
	LOSS [training: 0.0014769296086190465 | validation: 0.005178762684863704]
	TIME [epoch: 8.16 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037447856567964137		[learning rate: 0.00037611]
		[batch 20/20] avg loss: 0.0033061001912741946		[learning rate: 0.00037566]
	Learning Rate: 0.000375655
	LOSS [training: 0.0035254429240353055 | validation: -0.0070872793384239505]
	TIME [epoch: 8.16 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005138286351508444		[learning rate: 0.0003752]
		[batch 20/20] avg loss: 0.0009799302525273462		[learning rate: 0.00037475]
	Learning Rate: 0.000374746
	LOSS [training: 0.0030591083020178947 | validation: -0.0008177169726033642]
	TIME [epoch: 8.18 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004940521862280753		[learning rate: 0.00037429]
		[batch 20/20] avg loss: 0.0023719673318128645		[learning rate: 0.00037384]
	Learning Rate: 0.000373839
	LOSS [training: 0.003656244597046808 | validation: -0.006734471965514601]
	TIME [epoch: 8.16 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00015791327248196232		[learning rate: 0.00037339]
		[batch 20/20] avg loss: 0.00403825208189274		[learning rate: 0.00037293]
	Learning Rate: 0.000372934
	LOSS [training: 0.002098082677187351 | validation: -0.0007225069815372759]
	TIME [epoch: 8.16 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034869046295931834		[learning rate: 0.00037248]
		[batch 20/20] avg loss: 0.0064965067293872285		[learning rate: 0.00037203]
	Learning Rate: 0.000372031
	LOSS [training: 0.004991705679490206 | validation: 0.0069245392406682696]
	TIME [epoch: 8.16 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007182364959188764		[learning rate: 0.00037158]
		[batch 20/20] avg loss: 0.00037565739080599493		[learning rate: 0.00037113]
	Learning Rate: 0.00037113
	LOSS [training: 0.003779011174997379 | validation: 0.006214861357336362]
	TIME [epoch: 8.18 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025408925409567636		[learning rate: 0.00037068]
		[batch 20/20] avg loss: 0.00020885854847213574		[learning rate: 0.00037023]
	Learning Rate: 0.000370232
	LOSS [training: 0.0013748755447144496 | validation: 0.0003960203884201606]
	TIME [epoch: 8.16 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007862095408250266		[learning rate: 0.00036978]
		[batch 20/20] avg loss: 0.003781137582216108		[learning rate: 0.00036934]
	Learning Rate: 0.000369336
	LOSS [training: 0.005821616495233188 | validation: 0.004590347570415167]
	TIME [epoch: 8.15 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004632576843222391		[learning rate: 0.00036889]
		[batch 20/20] avg loss: 0.0022475589694321944		[learning rate: 0.00036844]
	Learning Rate: 0.000368441
	LOSS [training: 0.0034400679063272925 | validation: -0.0004801636477204411]
	TIME [epoch: 8.18 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0046436889244389195		[learning rate: 0.000368]
		[batch 20/20] avg loss: 0.0003481291055598545		[learning rate: 0.00036755]
	Learning Rate: 0.00036755
	LOSS [training: 0.0024959090149993877 | validation: 0.0035617157695570657]
	TIME [epoch: 8.19 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010092021729588915		[learning rate: 0.0003671]
		[batch 20/20] avg loss: 0.0033719794804276796		[learning rate: 0.00036666]
	Learning Rate: 0.00036666
	LOSS [training: 0.006732000605008298 | validation: 0.0004334293510147258]
	TIME [epoch: 8.17 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037687229412846136		[learning rate: 0.00036622]
		[batch 20/20] avg loss: 0.010845641187664106		[learning rate: 0.00036577]
	Learning Rate: 0.000365772
	LOSS [training: 0.007307182064474359 | validation: 0.00508445439173414]
	TIME [epoch: 8.16 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006929869415945843		[learning rate: 0.00036533]
		[batch 20/20] avg loss: 0.0021905770980173048		[learning rate: 0.00036489]
	Learning Rate: 0.000364887
	LOSS [training: 0.004560223256981574 | validation: -0.0028883019758782595]
	TIME [epoch: 8.2 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003191557134528756		[learning rate: 0.00036444]
		[batch 20/20] avg loss: -0.0015445814177820893		[learning rate: 0.000364]
	Learning Rate: 0.000364003
	LOSS [training: 0.000823487858373333 | validation: -0.0016998535852798054]
	TIME [epoch: 8.19 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005805138590981751		[learning rate: 0.00036356]
		[batch 20/20] avg loss: 0.003915674117239856		[learning rate: 0.00036312]
	Learning Rate: 0.000363122
	LOSS [training: 0.0016675801290708403 | validation: 0.003610756797543151]
	TIME [epoch: 8.16 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.232449918320584e-06		[learning rate: 0.00036268]
		[batch 20/20] avg loss: 0.009938801305440562		[learning rate: 0.00036224]
	Learning Rate: 0.000362243
	LOSS [training: 0.0049705168776794425 | validation: 0.007085647967442077]
	TIME [epoch: 8.16 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0046232868695144065		[learning rate: 0.0003618]
		[batch 20/20] avg loss: 0.005291903511678705		[learning rate: 0.00036137]
	Learning Rate: 0.000361366
	LOSS [training: 0.004957595190596556 | validation: 0.009794857754104722]
	TIME [epoch: 8.16 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005764716826028048		[learning rate: 0.00036093]
		[batch 20/20] avg loss: -0.003398298654264037		[learning rate: 0.00036049]
	Learning Rate: 0.000360491
	LOSS [training: 0.0011832090858820052 | validation: 0.0008654748217326205]
	TIME [epoch: 8.18 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011828758824704191		[learning rate: 0.00036005]
		[batch 20/20] avg loss: 0.0005518912354063913		[learning rate: 0.00035962]
	Learning Rate: 0.000359619
	LOSS [training: 0.000867383558938405 | validation: 0.00020792094394620142]
	TIME [epoch: 8.18 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034263012661895345		[learning rate: 0.00035918]
		[batch 20/20] avg loss: 0.0029815049861096343		[learning rate: 0.00035875]
	Learning Rate: 0.000358748
	LOSS [training: 0.003203903126149584 | validation: 0.0023856295987773795]
	TIME [epoch: 8.18 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008973927829345368		[learning rate: 0.00035831]
		[batch 20/20] avg loss: 0.0027054482920819494		[learning rate: 0.00035788]
	Learning Rate: 0.00035788
	LOSS [training: 0.00583968806071366 | validation: -0.0012442699579491168]
	TIME [epoch: 8.17 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001882599493761975		[learning rate: 0.00035745]
		[batch 20/20] avg loss: 0.005334568327987263		[learning rate: 0.00035701]
	Learning Rate: 0.000357013
	LOSS [training: 0.0036085839108746186 | validation: 0.008314393334823585]
	TIME [epoch: 8.19 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006345691892552537		[learning rate: 0.00035658]
		[batch 20/20] avg loss: 0.0013141312629174728		[learning rate: 0.00035615]
	Learning Rate: 0.000356149
	LOSS [training: 0.003829911577735005 | validation: 0.0025360701877004095]
	TIME [epoch: 8.19 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006812574077354704		[learning rate: 0.00035572]
		[batch 20/20] avg loss: 0.003007292341800047		[learning rate: 0.00035529]
	Learning Rate: 0.000355287
	LOSS [training: 0.0018442748747677588 | validation: -0.0016015640523384489]
	TIME [epoch: 8.17 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006473815157626352		[learning rate: 0.00035486]
		[batch 20/20] avg loss: 0.007468798738786838		[learning rate: 0.00035443]
	Learning Rate: 0.000354427
	LOSS [training: 0.004058090127274737 | validation: 0.002586845845440783]
	TIME [epoch: 8.16 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002953688280860662		[learning rate: 0.000354]
		[batch 20/20] avg loss: 0.004335990123731283		[learning rate: 0.00035357]
	Learning Rate: 0.000353569
	LOSS [training: 0.003644839202295973 | validation: 0.003127574838953483]
	TIME [epoch: 8.18 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027806433971940612		[learning rate: 0.00035314]
		[batch 20/20] avg loss: 0.0040866990216606785		[learning rate: 0.00035271]
	Learning Rate: 0.000352713
	LOSS [training: 0.0034336712094273705 | validation: 0.009983590283013707]
	TIME [epoch: 8.16 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028809208085301997		[learning rate: 0.00035229]
		[batch 20/20] avg loss: 0.0020257656140540642		[learning rate: 0.00035186]
	Learning Rate: 0.000351859
	LOSS [training: 0.0024533432112921316 | validation: -0.0036578615520425507]
	TIME [epoch: 8.15 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025778183380521665		[learning rate: 0.00035143]
		[batch 20/20] avg loss: 0.011184079652468745		[learning rate: 0.00035101]
	Learning Rate: 0.000351007
	LOSS [training: 0.006880948995260455 | validation: 0.013441780473866105]
	TIME [epoch: 8.16 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00460057871681706		[learning rate: 0.00035058]
		[batch 20/20] avg loss: 0.0045228914183468635		[learning rate: 0.00035016]
	Learning Rate: 0.000350157
	LOSS [training: 0.004561735067581962 | validation: -0.0022191186305668617]
	TIME [epoch: 8.18 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023660067776326487		[learning rate: 0.00034973]
		[batch 20/20] avg loss: 0.007403507856058486		[learning rate: 0.00034931]
	Learning Rate: 0.00034931
	LOSS [training: 0.004884757316845569 | validation: 0.0012628609029626413]
	TIME [epoch: 8.16 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005498002736552284		[learning rate: 0.00034889]
		[batch 20/20] avg loss: -0.0012818189135676483		[learning rate: 0.00034846]
	Learning Rate: 0.000348464
	LOSS [training: 0.0021080919114923164 | validation: 0.006660640750557404]
	TIME [epoch: 8.17 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016414104446739349		[learning rate: 0.00034804]
		[batch 20/20] avg loss: 0.0029731691230581274		[learning rate: 0.00034762]
	Learning Rate: 0.00034762
	LOSS [training: 0.002307289783866031 | validation: 0.0008717752795378629]
	TIME [epoch: 8.18 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004579645065086988		[learning rate: 0.0003472]
		[batch 20/20] avg loss: 0.004614066145756361		[learning rate: 0.00034678]
	Learning Rate: 0.000346779
	LOSS [training: 0.002078050819623831 | validation: -0.004490546101494429]
	TIME [epoch: 8.19 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017736456816279036		[learning rate: 0.00034636]
		[batch 20/20] avg loss: 0.0020833532227551204		[learning rate: 0.00034594]
	Learning Rate: 0.000345939
	LOSS [training: 0.0019284994521915125 | validation: 0.0024163539212965737]
	TIME [epoch: 8.16 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023744471334756836		[learning rate: 0.00034552]
		[batch 20/20] avg loss: -0.0008126647010040042		[learning rate: 0.0003451]
	Learning Rate: 0.000345102
	LOSS [training: 0.0007808912162358396 | validation: 0.009995323239926301]
	TIME [epoch: 8.19 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00845732270968658		[learning rate: 0.00034468]
		[batch 20/20] avg loss: 0.003308964420167046		[learning rate: 0.00034427]
	Learning Rate: 0.000344267
	LOSS [training: 0.005883143564926812 | validation: 0.0017880292380966362]
	TIME [epoch: 8.16 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020087931766078104		[learning rate: 0.00034385]
		[batch 20/20] avg loss: 0.0014825729049509774		[learning rate: 0.00034343]
	Learning Rate: 0.000343433
	LOSS [training: -0.00026311013582841643 | validation: 0.005737378449545699]
	TIME [epoch: 8.18 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004617652083390105		[learning rate: 0.00034302]
		[batch 20/20] avg loss: 0.006351767756468429		[learning rate: 0.0003426]
	Learning Rate: 0.000342602
	LOSS [training: 0.005484709919929266 | validation: 0.0015705193378006605]
	TIME [epoch: 8.16 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001574275171617195		[learning rate: 0.00034219]
		[batch 20/20] avg loss: 0.0040687713881299185		[learning rate: 0.00034177]
	Learning Rate: 0.000341772
	LOSS [training: 0.002821523279873557 | validation: -0.0025065003438703532]
	TIME [epoch: 8.16 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005802343884340957		[learning rate: 0.00034136]
		[batch 20/20] avg loss: 0.004820412950692531		[learning rate: 0.00034094]
	Learning Rate: 0.000340945
	LOSS [training: 0.0021200892811292175 | validation: 0.0037477374361998637]
	TIME [epoch: 8.16 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007711103979339923		[learning rate: 0.00034053]
		[batch 20/20] avg loss: 0.006109003041832128		[learning rate: 0.00034012]
	Learning Rate: 0.00034012
	LOSS [training: 0.006910053510586026 | validation: -0.0058519044471708505]
	TIME [epoch: 8.18 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021896715903888856		[learning rate: 0.00033971]
		[batch 20/20] avg loss: 0.003655471053516314		[learning rate: 0.0003393]
	Learning Rate: 0.000339296
	LOSS [training: 0.0029225713219526 | validation: 0.007434541912278687]
	TIME [epoch: 8.17 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005117937558085015		[learning rate: 0.00033889]
		[batch 20/20] avg loss: 0.004022991090804731		[learning rate: 0.00033847]
	Learning Rate: 0.000338475
	LOSS [training: 0.001755598667498115 | validation: -0.005058200798559885]
	TIME [epoch: 8.18 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004363218408557883		[learning rate: 0.00033806]
		[batch 20/20] avg loss: 0.004848665747934607		[learning rate: 0.00033766]
	Learning Rate: 0.000337655
	LOSS [training: 0.004605942078246244 | validation: 0.008927050751922835]
	TIME [epoch: 8.17 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005436203592923295		[learning rate: 0.00033725]
		[batch 20/20] avg loss: 0.0021109691040108146		[learning rate: 0.00033684]
	Learning Rate: 0.000336838
	LOSS [training: 0.003773586348467055 | validation: 0.0003466829514836324]
	TIME [epoch: 8.19 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002925357953850996		[learning rate: 0.00033643]
		[batch 20/20] avg loss: 0.0024019335764813776		[learning rate: 0.00033602]
	Learning Rate: 0.000336023
	LOSS [training: 0.002663645765166187 | validation: 0.0005303955415867527]
	TIME [epoch: 8.19 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.0261817247537e-05		[learning rate: 0.00033562]
		[batch 20/20] avg loss: 0.007504980020903093		[learning rate: 0.00033521]
	Learning Rate: 0.000335209
	LOSS [training: 0.003782620919075315 | validation: 0.010131961343167923]
	TIME [epoch: 8.18 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004807904393466201		[learning rate: 0.0003348]
		[batch 20/20] avg loss: 0.006591542027319033		[learning rate: 0.0003344]
	Learning Rate: 0.000334398
	LOSS [training: 0.0056997232103926185 | validation: 0.006523564545134182]
	TIME [epoch: 8.16 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010302758286345543		[learning rate: 0.00033399]
		[batch 20/20] avg loss: -0.0015768217205527264		[learning rate: 0.00033359]
	Learning Rate: 0.000333588
	LOSS [training: 0.004362968282896409 | validation: -0.002027249332707093]
	TIME [epoch: 8.17 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007186901434183447		[learning rate: 0.00033318]
		[batch 20/20] avg loss: 0.005177100343034786		[learning rate: 0.00033278]
	Learning Rate: 0.000332781
	LOSS [training: 0.0022292050998082204 | validation: -0.002471801504671652]
	TIME [epoch: 8.17 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004939616362893328		[learning rate: 0.00033238]
		[batch 20/20] avg loss: 0.008601044353387433		[learning rate: 0.00033197]
	Learning Rate: 0.000331975
	LOSS [training: 0.006770330358140379 | validation: 0.005189246899397243]
	TIME [epoch: 8.16 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00435061358738662		[learning rate: 0.00033157]
		[batch 20/20] avg loss: -0.0022705700806704026		[learning rate: 0.00033117]
	Learning Rate: 0.000331171
	LOSS [training: 0.0010400217533581089 | validation: 0.0032764235931202344]
	TIME [epoch: 8.15 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00487781871261906		[learning rate: 0.00033077]
		[batch 20/20] avg loss: 0.01080651047722267		[learning rate: 0.00033037]
	Learning Rate: 0.00033037
	LOSS [training: 0.007842164594920865 | validation: 0.002436181560525091]
	TIME [epoch: 8.19 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019999147462512034		[learning rate: 0.00032997]
		[batch 20/20] avg loss: 0.006571661068381718		[learning rate: 0.00032957]
	Learning Rate: 0.00032957
	LOSS [training: 0.00428578790731646 | validation: -0.0013442488210830637]
	TIME [epoch: 8.18 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017470595161894627		[learning rate: 0.00032917]
		[batch 20/20] avg loss: 0.005579095911561256		[learning rate: 0.00032877]
	Learning Rate: 0.000328772
	LOSS [training: 0.0036630777138753597 | validation: -0.001070658038115551]
	TIME [epoch: 8.17 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005848819358776182		[learning rate: 0.00032837]
		[batch 20/20] avg loss: -0.0002143054123006064		[learning rate: 0.00032798]
	Learning Rate: 0.000327976
	LOSS [training: 0.0028172569732377874 | validation: 0.002148939030597015]
	TIME [epoch: 8.16 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033305729784301463		[learning rate: 0.00032758]
		[batch 20/20] avg loss: 0.007790382609978251		[learning rate: 0.00032718]
	Learning Rate: 0.000327182
	LOSS [training: 0.005560477794204198 | validation: -0.00044164252755010875]
	TIME [epoch: 8.2 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017171336876556926		[learning rate: 0.00032679]
		[batch 20/20] avg loss: 0.0052032367801156595		[learning rate: 0.00032639]
	Learning Rate: 0.00032639
	LOSS [training: 0.0034601852338856766 | validation: -0.0010758601887682804]
	TIME [epoch: 8.18 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004984265215438439		[learning rate: 0.00032599]
		[batch 20/20] avg loss: 0.006237435992431985		[learning rate: 0.0003256]
	Learning Rate: 0.0003256
	LOSS [training: 0.0056108506039352106 | validation: 0.004782442255912085]
	TIME [epoch: 8.16 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012472903348628383		[learning rate: 0.00032521]
		[batch 20/20] avg loss: 0.00214918212673497		[learning rate: 0.00032481]
	Learning Rate: 0.000324812
	LOSS [training: 0.0016982362307989042 | validation: 0.00042916854582956986]
	TIME [epoch: 8.16 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004350473254504625		[learning rate: 0.00032442]
		[batch 20/20] avg loss: 0.006082587071546091		[learning rate: 0.00032403]
	Learning Rate: 0.000324025
	LOSS [training: 0.0052165301630253584 | validation: -0.0010104572643973727]
	TIME [epoch: 8.16 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008344854035969771		[learning rate: 0.00032363]
		[batch 20/20] avg loss: 0.0008157659795435965		[learning rate: 0.00032324]
	Learning Rate: 0.000323241
	LOSS [training: 0.004580310007756683 | validation: -0.0026574315012854255]
	TIME [epoch: 8.18 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6352707342444945e-06		[learning rate: 0.00032285]
		[batch 20/20] avg loss: 0.002721847233706466		[learning rate: 0.00032246]
	Learning Rate: 0.000322458
	LOSS [training: 0.0013627412522203548 | validation: -0.0029557758275880618]
	TIME [epoch: 8.16 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004438991376542798		[learning rate: 0.00032207]
		[batch 20/20] avg loss: 0.0004580396758502985		[learning rate: 0.00032168]
	Learning Rate: 0.000321678
	LOSS [training: 0.0024485155261965473 | validation: 0.0009369753485554411]
	TIME [epoch: 8.17 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001933222755978014		[learning rate: 0.00032129]
		[batch 20/20] avg loss: 0.0033665481846181467		[learning rate: 0.0003209]
	Learning Rate: 0.000320899
	LOSS [training: 0.0026498854702980803 | validation: -0.004847161450911541]
	TIME [epoch: 8.18 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027783533323310627		[learning rate: 0.00032051]
		[batch 20/20] avg loss: 0.003833909622363301		[learning rate: 0.00032012]
	Learning Rate: 0.000320122
	LOSS [training: 0.003306131477347181 | validation: -0.004365573279823787]
	TIME [epoch: 8.19 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004589167706294578		[learning rate: 0.00031973]
		[batch 20/20] avg loss: 0.007546809340123947		[learning rate: 0.00031935]
	Learning Rate: 0.000319347
	LOSS [training: 0.006067988523209264 | validation: -0.001743541840976377]
	TIME [epoch: 8.17 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006765807164038943		[learning rate: 0.00031896]
		[batch 20/20] avg loss: -0.0010382915003551293		[learning rate: 0.00031857]
	Learning Rate: 0.000318574
	LOSS [training: 0.0028637578318419067 | validation: 0.004556297506570106]
	TIME [epoch: 8.18 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007212764399011524		[learning rate: 0.00031819]
		[batch 20/20] avg loss: 0.001055405615202846		[learning rate: 0.0003178]
	Learning Rate: 0.000317803
	LOSS [training: 0.0041340850071071855 | validation: -0.003671535193681276]
	TIME [epoch: 8.18 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019357104487982431		[learning rate: 0.00031742]
		[batch 20/20] avg loss: 0.005717925011577469		[learning rate: 0.00031703]
	Learning Rate: 0.000317034
	LOSS [training: 0.0038268177301878565 | validation: 0.0005483843267829491]
	TIME [epoch: 8.18 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003927280650685193		[learning rate: 0.00031665]
		[batch 20/20] avg loss: 0.00583212388022472		[learning rate: 0.00031627]
	Learning Rate: 0.000316266
	LOSS [training: 0.004879702265454957 | validation: 0.0006570436385929918]
	TIME [epoch: 8.16 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006648456601288076		[learning rate: 0.00031588]
		[batch 20/20] avg loss: 0.006517525309600078		[learning rate: 0.0003155]
	Learning Rate: 0.0003155
	LOSS [training: 0.0065829909554440775 | validation: -0.0015439647387405645]
	TIME [epoch: 8.15 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020992310577896134		[learning rate: 0.00031512]
		[batch 20/20] avg loss: 0.006490141982482496		[learning rate: 0.00031474]
	Learning Rate: 0.000314737
	LOSS [training: 0.004294686520136055 | validation: 0.00233568084610083]
	TIME [epoch: 8.16 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014207775557195878		[learning rate: 0.00031436]
		[batch 20/20] avg loss: -0.001984519424199364		[learning rate: 0.00031397]
	Learning Rate: 0.000313975
	LOSS [training: -0.0002818709342398882 | validation: -0.000763858175084068]
	TIME [epoch: 8.18 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009803808576593794		[learning rate: 0.00031359]
		[batch 20/20] avg loss: -0.0007301722977103575		[learning rate: 0.00031321]
	Learning Rate: 0.000313215
	LOSS [training: 0.004536818139441718 | validation: -0.00098072852508412]
	TIME [epoch: 8.16 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012901791668760002		[learning rate: 0.00031284]
		[batch 20/20] avg loss: 0.0063114310201807435		[learning rate: 0.00031246]
	Learning Rate: 0.000312456
	LOSS [training: 0.0038008050935283712 | validation: 0.008492134041982794]
	TIME [epoch: 8.18 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002534208413773242		[learning rate: 0.00031208]
		[batch 20/20] avg loss: 0.009023068887674646		[learning rate: 0.0003117]
	Learning Rate: 0.0003117
	LOSS [training: 0.005778638650723943 | validation: 0.00010725603362919276]
	TIME [epoch: 8.17 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004248636854281538		[learning rate: 0.00031132]
		[batch 20/20] avg loss: 0.004161101280569367		[learning rate: 0.00031095]
	Learning Rate: 0.000310945
	LOSS [training: 0.004204869067425452 | validation: 0.002515009099254851]
	TIME [epoch: 8.19 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006884012027354208		[learning rate: 0.00031057]
		[batch 20/20] avg loss: 0.00027546198534739033		[learning rate: 0.00031019]
	Learning Rate: 0.000310193
	LOSS [training: 0.0035797370063508 | validation: 0.0020944102491498814]
	TIME [epoch: 8.17 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001986293953360466		[learning rate: 0.00030982]
		[batch 20/20] avg loss: 0.002274346243238944		[learning rate: 0.00030944]
	Learning Rate: 0.000309442
	LOSS [training: 0.0021303200982997054 | validation: 0.0017813370110589043]
	TIME [epoch: 8.2 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00118460323928803		[learning rate: 0.00030907]
		[batch 20/20] avg loss: 0.0037601547031975617		[learning rate: 0.00030869]
	Learning Rate: 0.000308693
	LOSS [training: 0.002472378971242796 | validation: 2.2006766484446855e-06]
	TIME [epoch: 8.15 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001066474513014002		[learning rate: 0.00030832]
		[batch 20/20] avg loss: 0.0047195763506093155		[learning rate: 0.00030795]
	Learning Rate: 0.000307945
	LOSS [training: 0.0028930254318116585 | validation: 0.0023041102617181826]
	TIME [epoch: 8.18 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00487449503639391		[learning rate: 0.00030757]
		[batch 20/20] avg loss: 0.002490235286033453		[learning rate: 0.0003072]
	Learning Rate: 0.0003072
	LOSS [training: 0.0036823651612136812 | validation: -0.007754778668150558]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_1537.pth
	Model improved!!!
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037573709266076226		[learning rate: 0.00030683]
		[batch 20/20] avg loss: 0.003187026123881159		[learning rate: 0.00030646]
	Learning Rate: 0.000306456
	LOSS [training: 0.0034721985252443915 | validation: 0.002649253619112485]
	TIME [epoch: 8.16 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029109202807472284		[learning rate: 0.00030609]
		[batch 20/20] avg loss: 0.0036358188251967966		[learning rate: 0.00030571]
	Learning Rate: 0.000305714
	LOSS [training: 0.0032733695529720123 | validation: -0.000328282105491125]
	TIME [epoch: 8.15 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008502405595913124		[learning rate: 0.00030534]
		[batch 20/20] avg loss: 0.007798759541642786		[learning rate: 0.00030497]
	Learning Rate: 0.000304974
	LOSS [training: 0.00432450005061705 | validation: 0.005789889912395647]
	TIME [epoch: 8.18 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010705165310802155		[learning rate: 0.0003046]
		[batch 20/20] avg loss: 0.003090949532006536		[learning rate: 0.00030424]
	Learning Rate: 0.000304236
	LOSS [training: 0.00101021650046316 | validation: -0.0006956443464409146]
	TIME [epoch: 8.18 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004370048685707501		[learning rate: 0.00030387]
		[batch 20/20] avg loss: 0.009258390379387063		[learning rate: 0.0003035]
	Learning Rate: 0.000303499
	LOSS [training: 0.006814219532547282 | validation: -0.009004745956071378]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_1542.pth
	Model improved!!!
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013228510872853934		[learning rate: 0.00030313]
		[batch 20/20] avg loss: 0.006076125176605875		[learning rate: 0.00030276]
	Learning Rate: 0.000302765
	LOSS [training: 0.0036994881319456336 | validation: 0.004599702053181028]
	TIME [epoch: 8.19 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007117558284440726		[learning rate: 0.0003024]
		[batch 20/20] avg loss: 0.00681952594501615		[learning rate: 0.00030203]
	Learning Rate: 0.000302032
	LOSS [training: 0.006968542114728439 | validation: 0.0007219167103944949]
	TIME [epoch: 8.21 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010277337665206688		[learning rate: 0.00030167]
		[batch 20/20] avg loss: 0.003928007482305972		[learning rate: 0.0003013]
	Learning Rate: 0.000301301
	LOSS [training: 0.002477870624413321 | validation: 0.0042694328255052615]
	TIME [epoch: 8.22 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005435088645871172		[learning rate: 0.00030094]
		[batch 20/20] avg loss: 0.003477600138160677		[learning rate: 0.00030057]
	Learning Rate: 0.000300571
	LOSS [training: 0.004456344392015923 | validation: 0.0032724118999350287]
	TIME [epoch: 8.17 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008210007895404483		[learning rate: 0.00030021]
		[batch 20/20] avg loss: 0.0002518881259196238		[learning rate: 0.00029984]
	Learning Rate: 0.000299844
	LOSS [training: 0.004230948010662055 | validation: 0.004282292651228952]
	TIME [epoch: 8.17 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005041314649233034		[learning rate: 0.00029948]
		[batch 20/20] avg loss: 0.0009471498810210675		[learning rate: 0.00029912]
	Learning Rate: 0.000299118
	LOSS [training: 0.002994232265127051 | validation: -0.0001550276324234612]
	TIME [epoch: 8.19 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007560156812219281		[learning rate: 0.00029876]
		[batch 20/20] avg loss: 0.0019437053842544325		[learning rate: 0.00029839]
	Learning Rate: 0.000298394
	LOSS [training: 0.004751931098236855 | validation: 0.0015018574331492763]
	TIME [epoch: 8.18 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003870885958421172		[learning rate: 0.00029803]
		[batch 20/20] avg loss: -0.0009195813990818862		[learning rate: 0.00029767]
	Learning Rate: 0.000297671
	LOSS [training: 0.0014756522796696425 | validation: -0.0013384376022122176]
	TIME [epoch: 8.17 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0053733600679872085		[learning rate: 0.00029731]
		[batch 20/20] avg loss: 0.005904902973074961		[learning rate: 0.00029695]
	Learning Rate: 0.000296951
	LOSS [training: 0.005639131520531086 | validation: -0.006606504475046894]
	TIME [epoch: 8.17 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002666462798210683		[learning rate: 0.00029659]
		[batch 20/20] avg loss: -0.0002896563597737416		[learning rate: 0.00029623]
	Learning Rate: 0.000296232
	LOSS [training: 0.0011884032192184704 | validation: 0.001131323877360541]
	TIME [epoch: 8.21 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00044614258790184863		[learning rate: 0.00029587]
		[batch 20/20] avg loss: 0.004684688053765101		[learning rate: 0.00029551]
	Learning Rate: 0.000295515
	LOSS [training: 0.002119272732931626 | validation: -0.0071061881337914495]
	TIME [epoch: 8.2 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019442880122783467		[learning rate: 0.00029516]
		[batch 20/20] avg loss: 0.0019472160961972434		[learning rate: 0.0002948]
	Learning Rate: 0.000294799
	LOSS [training: 0.0019457520542377946 | validation: -0.0006334112104107907]
	TIME [epoch: 8.18 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006943231395540881		[learning rate: 0.00029444]
		[batch 20/20] avg loss: 0.003440721122211441		[learning rate: 0.00029409]
	Learning Rate: 0.000294086
	LOSS [training: 0.00519197625887616 | validation: 0.0012816091587385427]
	TIME [epoch: 8.18 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025701678301417894		[learning rate: 0.00029373]
		[batch 20/20] avg loss: -0.0033584083064052426		[learning rate: 0.00029337]
	Learning Rate: 0.000293374
	LOSS [training: -0.0003941202381317264 | validation: 0.00036622459372186904]
	TIME [epoch: 8.23 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023449418986073965		[learning rate: 0.00029302]
		[batch 20/20] avg loss: 0.003184526253803105		[learning rate: 0.00029266]
	Learning Rate: 0.000292663
	LOSS [training: 0.0004197921775978542 | validation: -0.0016863409539367787]
	TIME [epoch: 8.19 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007555407244184354		[learning rate: 0.00029231]
		[batch 20/20] avg loss: -0.0023062545088851786		[learning rate: 0.00029195]
	Learning Rate: 0.000291955
	LOSS [training: -0.001530897616651807 | validation: -0.007725306242553842]
	TIME [epoch: 8.18 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0001043329224190823		[learning rate: 0.0002916]
		[batch 20/20] avg loss: 0.004440641311901525		[learning rate: 0.00029125]
	Learning Rate: 0.000291248
	LOSS [training: 0.0022724871171603037 | validation: -0.0020343045905804793]
	TIME [epoch: 8.17 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011554598008446802		[learning rate: 0.0002909]
		[batch 20/20] avg loss: 0.002394269244040592		[learning rate: 0.00029054]
	Learning Rate: 0.000290543
	LOSS [training: 0.0017748645224426357 | validation: -0.002031769187564519]
	TIME [epoch: 8.2 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000589825260219547		[learning rate: 0.00029019]
		[batch 20/20] avg loss: 0.0033721691224586825		[learning rate: 0.00028984]
	Learning Rate: 0.00028984
	LOSS [training: 0.001980997191339115 | validation: -0.0065985139216992545]
	TIME [epoch: 8.17 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006886685056962136		[learning rate: 0.00028949]
		[batch 20/20] avg loss: 0.0009703308306847609		[learning rate: 0.00028914]
	Learning Rate: 0.000289138
	LOSS [training: 0.00014083116249427363 | validation: 0.0003776059200804847]
	TIME [epoch: 8.19 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021990796232054717		[learning rate: 0.00028879]
		[batch 20/20] avg loss: 0.0001332133479102026		[learning rate: 0.00028844]
	Learning Rate: 0.000288438
	LOSS [training: -0.0010329331376476346 | validation: 0.0009378274989425251]
	TIME [epoch: 8.19 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000694165645744306		[learning rate: 0.00028809]
		[batch 20/20] avg loss: 0.0016718784195940445		[learning rate: 0.00028774]
	Learning Rate: 0.00028774
	LOSS [training: 0.0011830220326691752 | validation: -0.0057910433799573365]
	TIME [epoch: 8.21 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004437162162463376		[learning rate: 0.00028739]
		[batch 20/20] avg loss: 0.006996942224529549		[learning rate: 0.00028704]
	Learning Rate: 0.000287043
	LOSS [training: 0.003276613004141607 | validation: 0.004050895289193434]
	TIME [epoch: 8.18 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010094833540214287		[learning rate: 0.0002867]
		[batch 20/20] avg loss: 0.0046462592211713605		[learning rate: 0.00028635]
	Learning Rate: 0.000286348
	LOSS [training: 0.0018183879335749657 | validation: -0.0005380248962155696]
	TIME [epoch: 8.2 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004253920431589923		[learning rate: 0.000286]
		[batch 20/20] avg loss: -0.0008065107304565753		[learning rate: 0.00028566]
	Learning Rate: 0.000285655
	LOSS [training: 0.0017237048505666735 | validation: 0.008827652958865808]
	TIME [epoch: 8.18 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003559090175369353		[learning rate: 0.00028531]
		[batch 20/20] avg loss: 0.007395226384035317		[learning rate: 0.00028496]
	Learning Rate: 0.000284964
	LOSS [training: 0.005477158279702335 | validation: 0.0048743780864504305]
	TIME [epoch: 8.19 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001465708789896116		[learning rate: 0.00028462]
		[batch 20/20] avg loss: 0.005255971878735634		[learning rate: 0.00028427]
	Learning Rate: 0.000284274
	LOSS [training: 0.0033608403343158753 | validation: 0.006182966120629861]
	TIME [epoch: 8.17 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003021203632503136		[learning rate: 0.00028393]
		[batch 20/20] avg loss: 0.00359665583603067		[learning rate: 0.00028359]
	Learning Rate: 0.000283586
	LOSS [training: 0.003308929734266903 | validation: 0.009578961704311358]
	TIME [epoch: 8.17 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0001180446920765979		[learning rate: 0.00028324]
		[batch 20/20] avg loss: 0.003076618647409997		[learning rate: 0.0002829]
	Learning Rate: 0.000282899
	LOSS [training: 0.0014792869776666996 | validation: 0.005821543139888298]
	TIME [epoch: 8.17 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001087424568003728		[learning rate: 0.00028256]
		[batch 20/20] avg loss: 0.003598124394757308		[learning rate: 0.00028221]
	Learning Rate: 0.000282214
	LOSS [training: 0.002342774481380518 | validation: -0.0020705667995518824]
	TIME [epoch: 8.19 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011390416554998178		[learning rate: 0.00028187]
		[batch 20/20] avg loss: 0.0015139632661242512		[learning rate: 0.00028153]
	Learning Rate: 0.000281531
	LOSS [training: 0.0001874608053122166 | validation: 0.0020432878711659185]
	TIME [epoch: 8.17 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002185295999035095		[learning rate: 0.00028119]
		[batch 20/20] avg loss: 0.0071076287444806815		[learning rate: 0.00028085]
	Learning Rate: 0.000280849
	LOSS [training: 0.004646462371757888 | validation: -0.004358611474651528]
	TIME [epoch: 8.17 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004146817471499052		[learning rate: 0.00028051]
		[batch 20/20] avg loss: 0.007635649255773173		[learning rate: 0.00028017]
	Learning Rate: 0.00028017
	LOSS [training: 0.0017444158921370607 | validation: -0.0015617499694895913]
	TIME [epoch: 8.17 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002653941318556668		[learning rate: 0.00027983]
		[batch 20/20] avg loss: 0.010732478961662607		[learning rate: 0.00027949]
	Learning Rate: 0.000279491
	LOSS [training: 0.004039268821552969 | validation: 0.008304607894909917]
	TIME [epoch: 8.22 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0056497253375134655		[learning rate: 0.00027915]
		[batch 20/20] avg loss: 0.001131210556367259		[learning rate: 0.00027881]
	Learning Rate: 0.000278815
	LOSS [training: 0.003390467946940362 | validation: 0.007029969233967894]
	TIME [epoch: 8.19 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006392798910694622		[learning rate: 0.00027848]
		[batch 20/20] avg loss: 0.0032736234263846435		[learning rate: 0.00027814]
	Learning Rate: 0.00027814
	LOSS [training: 0.004833211168539632 | validation: 0.009502883651660193]
	TIME [epoch: 8.18 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00123183149317333		[learning rate: 0.0002778]
		[batch 20/20] avg loss: 0.0027381829496435964		[learning rate: 0.00027747]
	Learning Rate: 0.000277467
	LOSS [training: 0.001985007221408463 | validation: 0.0026173430223805187]
	TIME [epoch: 8.19 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019110274502618762		[learning rate: 0.00027713]
		[batch 20/20] avg loss: 0.001019337707938614		[learning rate: 0.00027679]
	Learning Rate: 0.000276795
	LOSS [training: 0.0014651825791002454 | validation: -5.640999826959892e-05]
	TIME [epoch: 8.25 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002291818280663201		[learning rate: 0.00027646]
		[batch 20/20] avg loss: 0.0029214963756625154		[learning rate: 0.00027612]
	Learning Rate: 0.000276125
	LOSS [training: 0.002606657328162859 | validation: 0.006095754747155297]
	TIME [epoch: 8.18 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00012462097445706082		[learning rate: 0.00027579]
		[batch 20/20] avg loss: 0.007357565041748719		[learning rate: 0.00027546]
	Learning Rate: 0.000275456
	LOSS [training: 0.00374109300810289 | validation: 0.0006417899975933766]
	TIME [epoch: 8.17 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006659306678736959		[learning rate: 0.00027512]
		[batch 20/20] avg loss: 0.002034918997475295		[learning rate: 0.00027479]
	Learning Rate: 0.000274789
	LOSS [training: 0.004347112838106127 | validation: -0.0029989514248101824]
	TIME [epoch: 8.17 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026979909423697802		[learning rate: 0.00027446]
		[batch 20/20] avg loss: 0.007175289722062524		[learning rate: 0.00027412]
	Learning Rate: 0.000274124
	LOSS [training: 0.004936640332216152 | validation: 0.0037537090018941004]
	TIME [epoch: 8.2 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003085481978063283		[learning rate: 0.00027379]
		[batch 20/20] avg loss: 0.002394274039886111		[learning rate: 0.00027346]
	Learning Rate: 0.000273461
	LOSS [training: 0.0027398780089746963 | validation: -0.004156943204214528]
	TIME [epoch: 8.18 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002690016500537012		[learning rate: 0.00027313]
		[batch 20/20] avg loss: 0.003164627147761369		[learning rate: 0.0002728]
	Learning Rate: 0.000272799
	LOSS [training: 0.00023730532361217844 | validation: -0.0036552008086563567]
	TIME [epoch: 8.18 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001903577564090175		[learning rate: 0.00027247]
		[batch 20/20] avg loss: 0.006518057336882738		[learning rate: 0.00027214]
	Learning Rate: 0.000272138
	LOSS [training: 0.002307239886396281 | validation: 0.006313537064702024]
	TIME [epoch: 8.21 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006412695963607463		[learning rate: 0.00027181]
		[batch 20/20] avg loss: -0.0009275906068682499		[learning rate: 0.00027148]
	Learning Rate: 0.000271479
	LOSS [training: 0.0027425526783696067 | validation: 0.00015856579152187565]
	TIME [epoch: 8.22 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003771694599747156		[learning rate: 0.00027115]
		[batch 20/20] avg loss: 0.0034399552893099815		[learning rate: 0.00027082]
	Learning Rate: 0.000270822
	LOSS [training: -0.00016586965521858712 | validation: 0.002541630355109452]
	TIME [epoch: 8.19 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004755555482283196		[learning rate: 0.00027049]
		[batch 20/20] avg loss: 0.0033837331116876373		[learning rate: 0.00027017]
	Learning Rate: 0.000270167
	LOSS [training: 0.004069644296985418 | validation: 0.005502577246658936]
	TIME [epoch: 8.19 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017641320297961		[learning rate: 0.00026984]
		[batch 20/20] avg loss: 0.008402775001165387		[learning rate: 0.00026951]
	Learning Rate: 0.000269513
	LOSS [training: 0.005083453515480743 | validation: 0.007832094626550401]
	TIME [epoch: 8.22 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026400520549298657		[learning rate: 0.00026919]
		[batch 20/20] avg loss: 0.004307049528572646		[learning rate: 0.00026886]
	Learning Rate: 0.00026886
	LOSS [training: 0.0034735507917512563 | validation: 0.0011552625270711124]
	TIME [epoch: 8.2 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010058254159464781		[learning rate: 0.00026853]
		[batch 20/20] avg loss: 0.004130194184471603		[learning rate: 0.00026821]
	Learning Rate: 0.000268209
	LOSS [training: 0.0025680098002090405 | validation: -0.0007034746031835784]
	TIME [epoch: 8.18 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005049923776630531		[learning rate: 0.00026788]
		[batch 20/20] avg loss: 0.002463681222083185		[learning rate: 0.00026756]
	Learning Rate: 0.00026756
	LOSS [training: 0.003756802499356857 | validation: -0.0008318628506538471]
	TIME [epoch: 8.18 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018383961677380946		[learning rate: 0.00026724]
		[batch 20/20] avg loss: 0.006771427259552989		[learning rate: 0.00026691]
	Learning Rate: 0.000266912
	LOSS [training: 0.004304911713645542 | validation: 0.0038348281003144054]
	TIME [epoch: 8.18 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015611696934047254		[learning rate: 0.00026659]
		[batch 20/20] avg loss: 0.0060313738662189625		[learning rate: 0.00026627]
	Learning Rate: 0.000266266
	LOSS [training: 0.003796271779811844 | validation: 0.0022933271991941546]
	TIME [epoch: 8.2 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008151651186070513		[learning rate: 0.00026594]
		[batch 20/20] avg loss: 0.0011857828615044646		[learning rate: 0.00026562]
	Learning Rate: 0.000265621
	LOSS [training: 0.004668717023787489 | validation: 0.007256808746337346]
	TIME [epoch: 8.2 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003281410004139452		[learning rate: 0.0002653]
		[batch 20/20] avg loss: 0.002935801391804993		[learning rate: 0.00026498]
	Learning Rate: 0.000264978
	LOSS [training: 0.0031086056979722225 | validation: 0.00011025352189032131]
	TIME [epoch: 8.21 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023276321511349735		[learning rate: 0.00026466]
		[batch 20/20] avg loss: 0.00509699790850397		[learning rate: 0.00026434]
	Learning Rate: 0.000264337
	LOSS [training: 0.0013846828786844977 | validation: 0.002922145049256819]
	TIME [epoch: 8.19 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032642761029968203		[learning rate: 0.00026402]
		[batch 20/20] avg loss: -0.004755210009788692		[learning rate: 0.0002637]
	Learning Rate: 0.000263697
	LOSS [training: -0.000745466953395936 | validation: 0.006710706313856913]
	TIME [epoch: 8.21 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005542291767911145		[learning rate: 0.00026338]
		[batch 20/20] avg loss: 0.0020836502769563505		[learning rate: 0.00026306]
	Learning Rate: 0.000263059
	LOSS [training: 0.0038129710224337476 | validation: 0.009670474947633083]
	TIME [epoch: 8.22 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024955501746598886		[learning rate: 0.00026274]
		[batch 20/20] avg loss: 0.003941646118077858		[learning rate: 0.00026242]
	Learning Rate: 0.000262422
	LOSS [training: 0.0032185981463688728 | validation: 0.002517117344460872]
	TIME [epoch: 8.2 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019826568505545627		[learning rate: 0.0002621]
		[batch 20/20] avg loss: 0.005493660165084591		[learning rate: 0.00026179]
	Learning Rate: 0.000261787
	LOSS [training: 0.0037381585078195756 | validation: 0.0007581897314690551]
	TIME [epoch: 8.18 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002348426586070423		[learning rate: 0.00026147]
		[batch 20/20] avg loss: -0.0013749936636871637		[learning rate: 0.00026115]
	Learning Rate: 0.000261153
	LOSS [training: 0.00048671646119162915 | validation: 0.0009302430825746401]
	TIME [epoch: 8.18 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007632114745962579		[learning rate: 0.00026084]
		[batch 20/20] avg loss: -0.004198234399111306		[learning rate: 0.00026052]
	Learning Rate: 0.000260521
	LOSS [training: 0.0017169401734256364 | validation: -0.000993225951244985]
	TIME [epoch: 8.21 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005087955233992958		[learning rate: 0.00026021]
		[batch 20/20] avg loss: 0.004702303484068955		[learning rate: 0.00025989]
	Learning Rate: 0.00025989
	LOSS [training: 0.0020967539803348296 | validation: 0.00834633422445912]
	TIME [epoch: 8.18 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013583823354308606		[learning rate: 0.00025958]
		[batch 20/20] avg loss: 0.0021872062447176556		[learning rate: 0.00025926]
	Learning Rate: 0.000259261
	LOSS [training: 0.0017727942900742585 | validation: 0.002087258421275483]
	TIME [epoch: 8.18 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038440549932989903		[learning rate: 0.00025895]
		[batch 20/20] avg loss: 0.003136730303078975		[learning rate: 0.00025863]
	Learning Rate: 0.000258633
	LOSS [training: 0.0034903926481889827 | validation: 4.12201105043624e-06]
	TIME [epoch: 8.18 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006485126957700657		[learning rate: 0.00025832]
		[batch 20/20] avg loss: -0.0017650168122260723		[learning rate: 0.00025801]
	Learning Rate: 0.000258007
	LOSS [training: 0.0023600550727372917 | validation: 0.002861257429144808]
	TIME [epoch: 8.22 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010528018036287576		[learning rate: 0.00025769]
		[batch 20/20] avg loss: 0.0035659242204559016		[learning rate: 0.00025738]
	Learning Rate: 0.000257382
	LOSS [training: 0.00230936301204233 | validation: -0.0004709582398024935]
	TIME [epoch: 8.21 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007727346064833907		[learning rate: 0.00025707]
		[batch 20/20] avg loss: -0.0015979643417437992		[learning rate: 0.00025676]
	Learning Rate: 0.000256759
	LOSS [training: 0.0030646908615450537 | validation: -0.0016456152583289381]
	TIME [epoch: 8.19 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019081752618508676		[learning rate: 0.00025645]
		[batch 20/20] avg loss: 0.00010187555817783201		[learning rate: 0.00025614]
	Learning Rate: 0.000256138
	LOSS [training: 0.0010050254100143498 | validation: -0.004542354352761575]
	TIME [epoch: 8.19 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004364824704095768		[learning rate: 0.00025583]
		[batch 20/20] avg loss: 0.0007557493936769284		[learning rate: 0.00025552]
	Learning Rate: 0.000255518
	LOSS [training: 0.002560287048886348 | validation: -0.005495152997384602]
	TIME [epoch: 8.23 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008617597116308209		[learning rate: 0.00025521]
		[batch 20/20] avg loss: 0.007094405736958411		[learning rate: 0.0002549]
	Learning Rate: 0.000254899
	LOSS [training: 0.003978082724294616 | validation: 0.0039781669371868646]
	TIME [epoch: 8.2 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00048434852027097605		[learning rate: 0.00025459]
		[batch 20/20] avg loss: 0.0021696343994979756		[learning rate: 0.00025428]
	Learning Rate: 0.000254282
	LOSS [training: 0.0008426429396134999 | validation: -0.00046220460237530233]
	TIME [epoch: 8.18 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00019290194018464147		[learning rate: 0.00025397]
		[batch 20/20] avg loss: -0.002257361261447605		[learning rate: 0.00025367]
	Learning Rate: 0.000253667
	LOSS [training: -0.001032229660631482 | validation: -0.002138204951047479]
	TIME [epoch: 8.18 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016348219575067658		[learning rate: 0.00025336]
		[batch 20/20] avg loss: 0.001415718926708097		[learning rate: 0.00025305]
	Learning Rate: 0.000253052
	LOSS [training: 0.0015252704421074316 | validation: 0.003966781171409349]
	TIME [epoch: 8.21 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0052406837529073635		[learning rate: 0.00025275]
		[batch 20/20] avg loss: -0.0019877102002468093		[learning rate: 0.00025244]
	Learning Rate: 0.00025244
	LOSS [training: 0.0016264867763302767 | validation: -0.0006818894835476162]
	TIME [epoch: 8.18 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016378558093830358		[learning rate: 0.00025213]
		[batch 20/20] avg loss: 0.000979926898944011		[learning rate: 0.00025183]
	Learning Rate: 0.000251829
	LOSS [training: -0.00032896445521951253 | validation: 0.001465804093641917]
	TIME [epoch: 8.19 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004534725070550555		[learning rate: 0.00025152]
		[batch 20/20] avg loss: 0.0015229779985442076		[learning rate: 0.00025122]
	Learning Rate: 0.000251219
	LOSS [training: 0.003028851534547381 | validation: -0.0007215941765090635]
	TIME [epoch: 8.21 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003618635248345261		[learning rate: 0.00025091]
		[batch 20/20] avg loss: 0.0011277592252631316		[learning rate: 0.00025061]
	Learning Rate: 0.000250611
	LOSS [training: 0.0023731972368041957 | validation: 0.0004999727658192405]
	TIME [epoch: 8.21 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003971958607127418		[learning rate: 0.00025031]
		[batch 20/20] avg loss: -0.00024835329828228406		[learning rate: 0.00025]
	Learning Rate: 0.000250004
	LOSS [training: 0.0018618026544225674 | validation: 0.007767144181476301]
	TIME [epoch: 8.18 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004295192984080711		[learning rate: 0.0002497]
		[batch 20/20] avg loss: 0.0014145978462006595		[learning rate: 0.0002494]
	Learning Rate: 0.000249399
	LOSS [training: 0.0028548954151406854 | validation: 0.0035166516268526613]
	TIME [epoch: 8.21 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012625615117343005		[learning rate: 0.0002491]
		[batch 20/20] avg loss: 0.0021749985569940243		[learning rate: 0.0002488]
	Learning Rate: 0.000248795
	LOSS [training: 0.00045621852262986196 | validation: 0.0069298367373513215]
	TIME [epoch: 8.2 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002301036916283341		[learning rate: 0.00024849]
		[batch 20/20] avg loss: -0.0001844463236693352		[learning rate: 0.00024819]
	Learning Rate: 0.000248193
	LOSS [training: 0.0010582952963070027 | validation: -0.006442712200071719]
	TIME [epoch: 8.2 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009272227459041566		[learning rate: 0.00024789]
		[batch 20/20] avg loss: 0.004697055636778934		[learning rate: 0.00024759]
	Learning Rate: 0.000247592
	LOSS [training: 0.0018849164454373887 | validation: -0.008841349153132349]
	TIME [epoch: 8.18 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029863032003946957		[learning rate: 0.00024729]
		[batch 20/20] avg loss: 0.00543564917313216		[learning rate: 0.00024699]
	Learning Rate: 0.000246993
	LOSS [training: 0.0012246729863687321 | validation: 0.010244662836208712]
	TIME [epoch: 8.18 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004117666727538565		[learning rate: 0.00024669]
		[batch 20/20] avg loss: 0.006588556899689927		[learning rate: 0.00024639]
	Learning Rate: 0.000246395
	LOSS [training: 0.005353111813614246 | validation: -0.00507350493821482]
	TIME [epoch: 8.18 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003927464137420941		[learning rate: 0.0002461]
		[batch 20/20] avg loss: 0.0006889331150328305		[learning rate: 0.0002458]
	Learning Rate: 0.000245798
	LOSS [training: 0.0023081986262268863 | validation: 0.0032524798025427744]
	TIME [epoch: 8.2 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031740526874847685		[learning rate: 0.0002455]
		[batch 20/20] avg loss: 0.00616097646742399		[learning rate: 0.0002452]
	Learning Rate: 0.000245203
	LOSS [training: 0.004667514577454379 | validation: -0.000830308388109385]
	TIME [epoch: 8.17 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004058496342951988		[learning rate: 0.00024491]
		[batch 20/20] avg loss: 5.9996213916405585e-05		[learning rate: 0.00024461]
	Learning Rate: 0.00024461
	LOSS [training: 0.0020592462784341965 | validation: -0.008817878619167696]
	TIME [epoch: 8.21 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002871274395267459		[learning rate: 0.00024431]
		[batch 20/20] avg loss: -0.0037811236343262508		[learning rate: 0.00024402]
	Learning Rate: 0.000244018
	LOSS [training: -0.0004549246195293956 | validation: 0.0036142393036213724]
	TIME [epoch: 8.19 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004352331905672326		[learning rate: 0.00024372]
		[batch 20/20] avg loss: 0.006329220770581967		[learning rate: 0.00024343]
	Learning Rate: 0.000243427
	LOSS [training: 0.0033822269805745993 | validation: -0.0029791350354781065]
	TIME [epoch: 8.21 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005518959039157293		[learning rate: 0.00024313]
		[batch 20/20] avg loss: 0.0013454847828995057		[learning rate: 0.00024284]
	Learning Rate: 0.000242838
	LOSS [training: 0.00039679443949188826 | validation: -0.00407404567880976]
	TIME [epoch: 8.19 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006885384709667925		[learning rate: 0.00024254]
		[batch 20/20] avg loss: 0.007179567426961751		[learning rate: 0.00024225]
	Learning Rate: 0.00024225
	LOSS [training: 0.00014709135864691405 | validation: -0.006802658670600367]
	TIME [epoch: 8.22 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004325063381322141		[learning rate: 0.00024196]
		[batch 20/20] avg loss: 0.0014175534831697492		[learning rate: 0.00024166]
	Learning Rate: 0.000241663
	LOSS [training: 0.002871308432245945 | validation: -0.000443262029655589]
	TIME [epoch: 8.18 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003635191825839414		[learning rate: 0.00024137]
		[batch 20/20] avg loss: 0.0048740755891580525		[learning rate: 0.00024108]
	Learning Rate: 0.000241078
	LOSS [training: 0.0026187973858709974 | validation: 0.003596149112605371]
	TIME [epoch: 8.21 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009361028490312447		[learning rate: 0.00024079]
		[batch 20/20] avg loss: -0.0035298455659902167		[learning rate: 0.00024049]
	Learning Rate: 0.000240495
	LOSS [training: -0.0012968713584794861 | validation: 0.0014781673576110875]
	TIME [epoch: 8.18 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029701626877664407		[learning rate: 0.0002402]
		[batch 20/20] avg loss: 0.007068448210582133		[learning rate: 0.00023991]
	Learning Rate: 0.000239912
	LOSS [training: 0.002049142761407846 | validation: 0.002048240991880038]
	TIME [epoch: 8.18 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028212397885011347		[learning rate: 0.00023962]
		[batch 20/20] avg loss: -0.0005295661671586443		[learning rate: 0.00023933]
	Learning Rate: 0.000239332
	LOSS [training: 0.001145836810671245 | validation: -0.005634108964128608]
	TIME [epoch: 8.18 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002582280751954459		[learning rate: 0.00023904]
		[batch 20/20] avg loss: 0.0005619403493765443		[learning rate: 0.00023875]
	Learning Rate: 0.000238752
	LOSS [training: 0.0015721105506655013 | validation: 0.0004252222044850312]
	TIME [epoch: 8.2 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005689645468816577		[learning rate: 0.00023846]
		[batch 20/20] avg loss: -0.0028910047870123823		[learning rate: 0.00023817]
	Learning Rate: 0.000238174
	LOSS [training: 0.0013993203409020968 | validation: 0.004857689311909169]
	TIME [epoch: 8.18 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014563410806047463		[learning rate: 0.00023789]
		[batch 20/20] avg loss: 0.0031538678271980022		[learning rate: 0.0002376]
	Learning Rate: 0.000237598
	LOSS [training: 0.0023051044539013745 | validation: -0.0025265565448627713]
	TIME [epoch: 8.18 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00022440426079685643		[learning rate: 0.00023731]
		[batch 20/20] avg loss: 0.0008020247393562503		[learning rate: 0.00023702]
	Learning Rate: 0.000237022
	LOSS [training: 0.0005132145000765534 | validation: 0.0021342221145808026]
	TIME [epoch: 8.18 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014091612613521154		[learning rate: 0.00023674]
		[batch 20/20] avg loss: 0.0084234422200213		[learning rate: 0.00023645]
	Learning Rate: 0.000236449
	LOSS [training: 0.004916301740686708 | validation: 0.010301143302072662]
	TIME [epoch: 8.24 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012552430989653507		[learning rate: 0.00023616]
		[batch 20/20] avg loss: 0.00752457124166315		[learning rate: 0.00023588]
	Learning Rate: 0.000235876
	LOSS [training: 0.01003850111565833 | validation: 0.002196826502068183]
	TIME [epoch: 8.19 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006514700820562734		[learning rate: 0.00023559]
		[batch 20/20] avg loss: 0.006876070132348084		[learning rate: 0.00023531]
	Learning Rate: 0.000235305
	LOSS [training: 0.00669538547645541 | validation: 0.0014039944418702615]
	TIME [epoch: 8.19 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005092494919058945		[learning rate: 0.00023502]
		[batch 20/20] avg loss: 0.011471733538498886		[learning rate: 0.00023474]
	Learning Rate: 0.000234736
	LOSS [training: 0.0031896193097199705 | validation: 0.0023549146899310563]
	TIME [epoch: 8.2 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00034720322759438234		[learning rate: 0.00023445]
		[batch 20/20] avg loss: -0.0019257137364151681		[learning rate: 0.00023417]
	Learning Rate: 0.000234167
	LOSS [training: -0.0007892552544103929 | validation: -0.0031727274507992115]
	TIME [epoch: 8.23 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022112170089994138		[learning rate: 0.00023388]
		[batch 20/20] avg loss: 0.0008486500425520686		[learning rate: 0.0002336]
	Learning Rate: 0.0002336
	LOSS [training: 0.0015299335257757413 | validation: -0.0019708398288067765]
	TIME [epoch: 8.18 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006619279690296996		[learning rate: 0.00023332]
		[batch 20/20] avg loss: -0.0036784117722255564		[learning rate: 0.00023303]
	Learning Rate: 0.000233035
	LOSS [training: 0.0014704339590357193 | validation: 0.0015308325196211198]
	TIME [epoch: 8.18 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031352209604809584		[learning rate: 0.00023275]
		[batch 20/20] avg loss: -0.0007248616324042913		[learning rate: 0.00023247]
	Learning Rate: 0.000232471
	LOSS [training: -0.001930041296442625 | validation: 0.0003217682315788976]
	TIME [epoch: 8.18 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002233915853161609		[learning rate: 0.00023219]
		[batch 20/20] avg loss: 0.003164713252987767		[learning rate: 0.00023191]
	Learning Rate: 0.000231908
	LOSS [training: 0.0026993145530746884 | validation: 0.0048270378264736805]
	TIME [epoch: 8.21 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002292747613442056		[learning rate: 0.00023163]
		[batch 20/20] avg loss: 0.00036305389939195594		[learning rate: 0.00023135]
	Learning Rate: 0.000231347
	LOSS [training: 0.0013279007564170062 | validation: 0.0024490723387790166]
	TIME [epoch: 8.18 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000325804912641427		[learning rate: 0.00023107]
		[batch 20/20] avg loss: 0.0043400657630567885		[learning rate: 0.00023079]
	Learning Rate: 0.000230787
	LOSS [training: 0.0023329353378491076 | validation: 0.00727861511098914]
	TIME [epoch: 8.18 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005986590224290081		[learning rate: 0.00023051]
		[batch 20/20] avg loss: 0.004132389773592606		[learning rate: 0.00023023]
	Learning Rate: 0.000230228
	LOSS [training: 0.005059489998941343 | validation: -0.0007648853135242334]
	TIME [epoch: 8.17 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002632143898156838		[learning rate: 0.00022995]
		[batch 20/20] avg loss: 0.001251390153222435		[learning rate: 0.00022967]
	Learning Rate: 0.000229671
	LOSS [training: 0.0004940878817033757 | validation: -0.002633064159450672]
	TIME [epoch: 8.21 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007899449073804628		[learning rate: 0.00022939]
		[batch 20/20] avg loss: -0.00021546886913567488		[learning rate: 0.00022911]
	Learning Rate: 0.000229115
	LOSS [training: 0.0038419901023344777 | validation: 0.002792369625302451]
	TIME [epoch: 8.21 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004266638545912624		[learning rate: 0.00022884]
		[batch 20/20] avg loss: -0.002572743931882214		[learning rate: 0.00022856]
	Learning Rate: 0.00022856
	LOSS [training: 0.0008469473070152056 | validation: 0.001284557314528301]
	TIME [epoch: 8.19 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00039155578248578247		[learning rate: 0.00022828]
		[batch 20/20] avg loss: 0.00838568866007616		[learning rate: 0.00022801]
	Learning Rate: 0.000228007
	LOSS [training: 0.003997066438795187 | validation: 0.00911000578362016]
	TIME [epoch: 8.19 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010231378821305943		[learning rate: 0.00022773]
		[batch 20/20] avg loss: 0.002379374772262165		[learning rate: 0.00022745]
	Learning Rate: 0.000227455
	LOSS [training: 0.0063053767967840525 | validation: 0.006016557798612024]
	TIME [epoch: 8.23 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00508838575239944		[learning rate: 0.00022718]
		[batch 20/20] avg loss: 0.005391582028934009		[learning rate: 0.0002269]
	Learning Rate: 0.000226904
	LOSS [training: 0.005239983890666724 | validation: 0.0061194706728660685]
	TIME [epoch: 8.21 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001341625084072529		[learning rate: 0.00022663]
		[batch 20/20] avg loss: 3.4734036472827345e-05		[learning rate: 0.00022635]
	Learning Rate: 0.000226355
	LOSS [training: 0.0006881795602726782 | validation: 0.0004292719728351078]
	TIME [epoch: 8.17 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002711553022622893		[learning rate: 0.00022608]
		[batch 20/20] avg loss: 0.002102425430510432		[learning rate: 0.00022581]
	Learning Rate: 0.000225807
	LOSS [training: 0.0024069892265666626 | validation: -0.005719936488493291]
	TIME [epoch: 8.2 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002981255445433697		[learning rate: 0.00022553]
		[batch 20/20] avg loss: 0.0007686253996049856		[learning rate: 0.00022526]
	Learning Rate: 0.00022526
	LOSS [training: 0.0005333754720741777 | validation: -0.0020571896238535574]
	TIME [epoch: 8.21 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024368795882866944		[learning rate: 0.00022499]
		[batch 20/20] avg loss: 0.001848053753581888		[learning rate: 0.00022471]
	Learning Rate: 0.000224715
	LOSS [training: 0.0021424666709342916 | validation: -0.0020034405933106168]
	TIME [epoch: 8.19 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033271906528720948		[learning rate: 0.00022444]
		[batch 20/20] avg loss: -0.0023804461406005035		[learning rate: 0.00022417]
	Learning Rate: 0.000224171
	LOSS [training: 0.00047337225613579596 | validation: 0.0024600602237825186]
	TIME [epoch: 8.2 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001695657778497179		[learning rate: 0.0002239]
		[batch 20/20] avg loss: 0.004799783157990144		[learning rate: 0.00022363]
	Learning Rate: 0.000223628
	LOSS [training: 0.003247720468243661 | validation: -0.003636545601782161]
	TIME [epoch: 8.2 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002733451688738972		[learning rate: 0.00022336]
		[batch 20/20] avg loss: 0.003120862791956721		[learning rate: 0.00022309]
	Learning Rate: 0.000223087
	LOSS [training: 0.0001937055516088749 | validation: -0.003104080148339808]
	TIME [epoch: 8.2 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0045693267488340576		[learning rate: 0.00022282]
		[batch 20/20] avg loss: 0.00048360616331945586		[learning rate: 0.00022255]
	Learning Rate: 0.000222547
	LOSS [training: 0.0025264664560767573 | validation: 0.001117746158257926]
	TIME [epoch: 8.18 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.000900338381474e-05		[learning rate: 0.00022228]
		[batch 20/20] avg loss: 0.0012362894773148198		[learning rate: 0.00022201]
	Learning Rate: 0.000222008
	LOSS [training: 0.0006581492403493171 | validation: -0.0012288311229210753]
	TIME [epoch: 8.18 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013008826428196987		[learning rate: 0.00022174]
		[batch 20/20] avg loss: -0.0018245234620797544		[learning rate: 0.00022147]
	Learning Rate: 0.00022147
	LOSS [training: -0.0015627030524497261 | validation: 0.0064591328450035596]
	TIME [epoch: 8.18 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017872219254851135		[learning rate: 0.0002212]
		[batch 20/20] avg loss: 0.004986094950141158		[learning rate: 0.00022093]
	Learning Rate: 0.000220934
	LOSS [training: 0.003386658437813136 | validation: -0.0002869422869595192]
	TIME [epoch: 8.2 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010568508278306998		[learning rate: 0.00022067]
		[batch 20/20] avg loss: 0.003267562305768795		[learning rate: 0.0002204]
	Learning Rate: 0.000220399
	LOSS [training: 0.0021622065667997476 | validation: 0.008617389169642498]
	TIME [epoch: 8.18 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008619807321642466		[learning rate: 0.00022013]
		[batch 20/20] avg loss: 0.00043328491185668633		[learning rate: 0.00021987]
	Learning Rate: 0.000219866
	LOSS [training: 0.004526546116749576 | validation: -0.00012058879502209528]
	TIME [epoch: 8.18 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001853825692814947		[learning rate: 0.0002196]
		[batch 20/20] avg loss: -0.00030041819163397247		[learning rate: 0.00021933]
	Learning Rate: 0.000219334
	LOSS [training: 0.0007767037505904872 | validation: -0.006709346810962977]
	TIME [epoch: 8.21 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00276929545321829		[learning rate: 0.00021907]
		[batch 20/20] avg loss: 0.002000284380067587		[learning rate: 0.0002188]
	Learning Rate: 0.000218803
	LOSS [training: 0.002384789916642938 | validation: -0.006892234653365905]
	TIME [epoch: 8.2 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007053284791820269		[learning rate: 0.00021854]
		[batch 20/20] avg loss: -0.0010465341160903977		[learning rate: 0.00021827]
	Learning Rate: 0.000218273
	LOSS [training: -0.0001706028184541856 | validation: -0.0077442815415545456]
	TIME [epoch: 8.18 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009354346216738929		[learning rate: 0.00021801]
		[batch 20/20] avg loss: -0.000313071697283386		[learning rate: 0.00021774]
	Learning Rate: 0.000217745
	LOSS [training: -0.0006242531594786393 | validation: -0.0014302392192667608]
	TIME [epoch: 8.2 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006316890241999926		[learning rate: 0.00021748]
		[batch 20/20] avg loss: 0.0040677175955002795		[learning rate: 0.00021722]
	Learning Rate: 0.000217217
	LOSS [training: -0.0011245863232498229 | validation: 0.0035154956235559924]
	TIME [epoch: 8.2 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004586243561323364		[learning rate: 0.00021695]
		[batch 20/20] avg loss: 0.0005799067380739671		[learning rate: 0.00021669]
	Learning Rate: 0.000216692
	LOSS [training: 0.002583075149698665 | validation: -0.0020613496404849967]
	TIME [epoch: 8.19 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00253022694784183		[learning rate: 0.00021643]
		[batch 20/20] avg loss: -0.0015637610644218057		[learning rate: 0.00021617]
	Learning Rate: 0.000216167
	LOSS [training: 0.00048323294171001227 | validation: -0.00013298532426175]
	TIME [epoch: 8.18 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001833745772398187		[learning rate: 0.00021591]
		[batch 20/20] avg loss: 0.0008721173659844642		[learning rate: 0.00021564]
	Learning Rate: 0.000215644
	LOSS [training: 0.0013529315691913255 | validation: -0.0008880067855091853]
	TIME [epoch: 8.17 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003928978711371371		[learning rate: 0.00021538]
		[batch 20/20] avg loss: 6.573256101353223e-05		[learning rate: 0.00021512]
	Learning Rate: 0.000215122
	LOSS [training: -0.00193162307517892 | validation: -0.001576391602465602]
	TIME [epoch: 8.18 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035101587659210434		[learning rate: 0.00021486]
		[batch 20/20] avg loss: 0.00437454868147788		[learning rate: 0.0002146]
	Learning Rate: 0.000214601
	LOSS [training: 0.0039423537236994615 | validation: -0.0002469140572192518]
	TIME [epoch: 8.19 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031663272048715226		[learning rate: 0.00021434]
		[batch 20/20] avg loss: -0.0016057642280020985		[learning rate: 0.00021408]
	Learning Rate: 0.000214081
	LOSS [training: 0.000780281488434712 | validation: -0.005194492122604872]
	TIME [epoch: 8.18 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001629643454996525		[learning rate: 0.00021382]
		[batch 20/20] avg loss: 0.00627374576438656		[learning rate: 0.00021356]
	Learning Rate: 0.000213563
	LOSS [training: 0.0023220511546950184 | validation: -0.003160107441563812]
	TIME [epoch: 8.2 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013290704077746078		[learning rate: 0.0002133]
		[batch 20/20] avg loss: 0.00567657341347561		[learning rate: 0.00021305]
	Learning Rate: 0.000213046
	LOSS [training: 0.002173751502850501 | validation: -0.002336122294166765]
	TIME [epoch: 8.18 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027991943865877966		[learning rate: 0.00021279]
		[batch 20/20] avg loss: -0.001732149325226292		[learning rate: 0.00021253]
	Learning Rate: 0.00021253
	LOSS [training: 0.0005335225306807524 | validation: -0.002429505098437493]
	TIME [epoch: 8.2 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004571706380320048		[learning rate: 0.00021227]
		[batch 20/20] avg loss: 0.000745224963315343		[learning rate: 0.00021202]
	Learning Rate: 0.000212016
	LOSS [training: 0.0026584656718176953 | validation: -0.002553115515259074]
	TIME [epoch: 8.19 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003735330020091461		[learning rate: 0.00021176]
		[batch 20/20] avg loss: -0.002099005816480393		[learning rate: 0.0002115]
	Learning Rate: 0.000211503
	LOSS [training: 0.0008181621018055338 | validation: 0.005294676474090584]
	TIME [epoch: 8.22 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001970309580728688		[learning rate: 0.00021125]
		[batch 20/20] avg loss: 0.0023199321676011645		[learning rate: 0.00021099]
	Learning Rate: 0.000210991
	LOSS [training: 0.002145120874164926 | validation: -0.004415211626191473]
	TIME [epoch: 8.17 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007315579577248366		[learning rate: 0.00021074]
		[batch 20/20] avg loss: 0.004584112892335637		[learning rate: 0.00021048]
	Learning Rate: 0.00021048
	LOSS [training: 0.0026578354250302366 | validation: 0.0005059637505503306]
	TIME [epoch: 8.18 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005389916969734837		[learning rate: 0.00021022]
		[batch 20/20] avg loss: 0.0002729845386892704		[learning rate: 0.00020997]
	Learning Rate: 0.00020997
	LOSS [training: 0.002831450754212053 | validation: 0.003547935906287715]
	TIME [epoch: 8.18 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00182634398618342		[learning rate: 0.00020972]
		[batch 20/20] avg loss: 0.004238841600368836		[learning rate: 0.00020946]
	Learning Rate: 0.000209462
	LOSS [training: 0.0030325927932761286 | validation: 0.004921986378313786]
	TIME [epoch: 8.17 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035150161206631164		[learning rate: 0.00020921]
		[batch 20/20] avg loss: -0.00415860643634053		[learning rate: 0.00020895]
	Learning Rate: 0.000208955
	LOSS [training: -0.00032179515783870705 | validation: -0.0031094718423935546]
	TIME [epoch: 8.17 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038632227080516576		[learning rate: 0.0002087]
		[batch 20/20] avg loss: 0.00039438979149918334		[learning rate: 0.00020845]
	Learning Rate: 0.000208449
	LOSS [training: -0.0017344164582762367 | validation: -0.0014546285259757026]
	TIME [epoch: 8.18 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007966443302482761		[learning rate: 0.0002082]
		[batch 20/20] avg loss: 0.003939910226829444		[learning rate: 0.00020794]
	Learning Rate: 0.000207944
	LOSS [training: 0.0023682772785388597 | validation: 0.008382835668217034]
	TIME [epoch: 8.22 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028733998617776605		[learning rate: 0.00020769]
		[batch 20/20] avg loss: 0.003813048083153592		[learning rate: 0.00020744]
	Learning Rate: 0.000207441
	LOSS [training: 0.003343223972465626 | validation: -0.0028689563615711933]
	TIME [epoch: 8.18 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026987028048385897		[learning rate: 0.00020719]
		[batch 20/20] avg loss: 0.002329845553762189		[learning rate: 0.00020694]
	Learning Rate: 0.000206939
	LOSS [training: 0.0025142741793003896 | validation: 0.0037616498692207715]
	TIME [epoch: 8.18 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0042489367889041296		[learning rate: 0.00020669]
		[batch 20/20] avg loss: 0.0021373960130158637		[learning rate: 0.00020644]
	Learning Rate: 0.000206438
	LOSS [training: 0.0031931664009599964 | validation: 0.0030055136469311935]
	TIME [epoch: 8.2 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003091357926540171		[learning rate: 0.00020619]
		[batch 20/20] avg loss: 0.003196430179483649		[learning rate: 0.00020594]
	Learning Rate: 0.000205938
	LOSS [training: 0.0031438940530119095 | validation: -0.002107390480483425]
	TIME [epoch: 8.22 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010185477904171365		[learning rate: 0.00020569]
		[batch 20/20] avg loss: 0.0015309606012376333		[learning rate: 0.00020544]
	Learning Rate: 0.00020544
	LOSS [training: 0.001274754195827385 | validation: 0.006395044429027687]
	TIME [epoch: 8.17 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007068923767402253		[learning rate: 0.00020519]
		[batch 20/20] avg loss: 7.337753452342491e-05		[learning rate: 0.00020494]
	Learning Rate: 0.000204942
	LOSS [training: 0.003571150650962839 | validation: -0.002248641590031297]
	TIME [epoch: 8.18 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001956187610735922		[learning rate: 0.00020469]
		[batch 20/20] avg loss: -0.0014363090785260127		[learning rate: 0.00020445]
	Learning Rate: 0.000204446
	LOSS [training: 0.00025993926610495463 | validation: 0.0015786083153917132]
	TIME [epoch: 8.17 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003270395370878984		[learning rate: 0.0002042]
		[batch 20/20] avg loss: 0.004416516534367224		[learning rate: 0.00020395]
	Learning Rate: 0.000203951
	LOSS [training: 0.0005730605817441196 | validation: -0.006414376196773322]
	TIME [epoch: 8.2 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002143377367122141		[learning rate: 0.0002037]
		[batch 20/20] avg loss: 0.001922624259581146		[learning rate: 0.00020346]
	Learning Rate: 0.000203457
	LOSS [training: 0.0020330008133516435 | validation: -0.004795358394191042]
	TIME [epoch: 8.19 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002787597195464226		[learning rate: 0.00020321]
		[batch 20/20] avg loss: -0.0039362640641672055		[learning rate: 0.00020296]
	Learning Rate: 0.000202965
	LOSS [training: -0.003361930629815717 | validation: 0.004960905138656748]
	TIME [epoch: 8.2 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021718388533022853		[learning rate: 0.00020272]
		[batch 20/20] avg loss: -0.0017726612597789054		[learning rate: 0.00020247]
	Learning Rate: 0.000202474
	LOSS [training: 0.00019958879676169007 | validation: -0.005114659818173441]
	TIME [epoch: 8.18 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025937651838454875		[learning rate: 0.00020223]
		[batch 20/20] avg loss: 0.0061403483000351		[learning rate: 0.00020198]
	Learning Rate: 0.000201983
	LOSS [training: 0.004367056741940293 | validation: 0.004333138918231366]
	TIME [epoch: 8.2 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014293379440195		[learning rate: 0.00020174]
		[batch 20/20] avg loss: 0.0049249056726608596		[learning rate: 0.00020149]
	Learning Rate: 0.000201495
	LOSS [training: 0.0031771218083401797 | validation: 0.006372132316559146]
	TIME [epoch: 8.2 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007477407756030695		[learning rate: 0.00020125]
		[batch 20/20] avg loss: -0.001863424811677776		[learning rate: 0.00020101]
	Learning Rate: 0.000201007
	LOSS [training: 0.0028069914721764585 | validation: 0.013932950993196866]
	TIME [epoch: 8.19 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004767456165004935		[learning rate: 0.00020076]
		[batch 20/20] avg loss: 0.003702266437022975		[learning rate: 0.00020052]
	Learning Rate: 0.00020052
	LOSS [training: 0.004234861301013955 | validation: -0.004640255096085006]
	TIME [epoch: 8.18 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005690611627506625		[learning rate: 0.00020028]
		[batch 20/20] avg loss: 0.004737413103566787		[learning rate: 0.00020003]
	Learning Rate: 0.000200035
	LOSS [training: -0.00047659926196991854 | validation: -0.0036747428044909083]
	TIME [epoch: 8.19 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003309955532295318		[learning rate: 0.00019979]
		[batch 20/20] avg loss: 0.004016342943300942		[learning rate: 0.00019955]
	Learning Rate: 0.00019955
	LOSS [training: 0.0036631492377981307 | validation: 0.000534514442535319]
	TIME [epoch: 8.17 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008479854277263828		[learning rate: 0.00019931]
		[batch 20/20] avg loss: 0.008110127149356293		[learning rate: 0.00019907]
	Learning Rate: 0.000199067
	LOSS [training: 0.0036310708608149555 | validation: 0.0011553361232368143]
	TIME [epoch: 8.17 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002464753228862032		[learning rate: 0.00019883]
		[batch 20/20] avg loss: 0.0010046441356368584		[learning rate: 0.00019859]
	Learning Rate: 0.000198585
	LOSS [training: 0.0017346986822494453 | validation: -0.0005168528445368943]
	TIME [epoch: 8.18 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025704283762640247		[learning rate: 0.00019834]
		[batch 20/20] avg loss: -0.0017877075503442204		[learning rate: 0.0001981]
	Learning Rate: 0.000198105
	LOSS [training: 0.0003913604129599021 | validation: -0.003963231280456854]
	TIME [epoch: 8.21 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009483408585995381		[learning rate: 0.00019786]
		[batch 20/20] avg loss: -0.0005991526146902187		[learning rate: 0.00019763]
	Learning Rate: 0.000197625
	LOSS [training: 0.004442127985652581 | validation: 0.003069074419742808]
	TIME [epoch: 8.19 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004010680750234663		[learning rate: 0.00019739]
		[batch 20/20] avg loss: 0.001998210013289474		[learning rate: 0.00019715]
	Learning Rate: 0.000197147
	LOSS [training: 0.0030044453817620687 | validation: -0.002371766742097843]
	TIME [epoch: 8.19 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037944120341804424		[learning rate: 0.00019691]
		[batch 20/20] avg loss: 0.010487186995036007		[learning rate: 0.00019667]
	Learning Rate: 0.000196669
	LOSS [training: 0.007140799514608225 | validation: 0.0068526326057158915]
	TIME [epoch: 8.18 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001546336043113384		[learning rate: 0.00019643]
		[batch 20/20] avg loss: 0.00195330980413756		[learning rate: 0.00019619]
	Learning Rate: 0.000196193
	LOSS [training: 0.0017498229236254722 | validation: -9.72107943666132e-05]
	TIME [epoch: 8.23 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031733681463279027		[learning rate: 0.00019596]
		[batch 20/20] avg loss: 0.00548968329602653		[learning rate: 0.00019572]
	Learning Rate: 0.000195718
	LOSS [training: 0.004331525721177216 | validation: 0.0034429579799122745]
	TIME [epoch: 8.18 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018607108244701868		[learning rate: 0.00019548]
		[batch 20/20] avg loss: -0.0009615549124104403		[learning rate: 0.00019524]
	Learning Rate: 0.000195245
	LOSS [training: 0.0004495779560298731 | validation: 2.6895468982525428e-05]
	TIME [epoch: 8.17 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000805607894753236		[learning rate: 0.00019501]
		[batch 20/20] avg loss: 0.0013260895237195934		[learning rate: 0.00019477]
	Learning Rate: 0.000194772
	LOSS [training: 0.001065848709236415 | validation: -0.0084608823265147]
	TIME [epoch: 8.17 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004199163690700904		[learning rate: 0.00019454]
		[batch 20/20] avg loss: 0.0006769977841910281		[learning rate: 0.0001943]
	Learning Rate: 0.0001943
	LOSS [training: 0.0024380807374459663 | validation: -0.0039854256387197775]
	TIME [epoch: 8.19 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025709895413026413		[learning rate: 0.00019407]
		[batch 20/20] avg loss: 0.0030386485360341417		[learning rate: 0.00019383]
	Learning Rate: 0.00019383
	LOSS [training: 0.0028048190386683915 | validation: -0.0020486537831264262]
	TIME [epoch: 8.17 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018242119147407262		[learning rate: 0.0001936]
		[batch 20/20] avg loss: 0.0023031302913104003		[learning rate: 0.00019336]
	Learning Rate: 0.000193361
	LOSS [training: 0.00023945918828483724 | validation: 0.002089191047590818]
	TIME [epoch: 8.17 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: -4.451692741744172e-05		[learning rate: 0.00019313]
		[batch 20/20] avg loss: 0.00545355315457734		[learning rate: 0.00019289]
	Learning Rate: 0.000192893
	LOSS [training: 0.0027045181135799496 | validation: -0.0007518310816550878]
	TIME [epoch: 8.19 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002250045100767084		[learning rate: 0.00019266]
		[batch 20/20] avg loss: 0.006196229551919525		[learning rate: 0.00019243]
	Learning Rate: 0.000192426
	LOSS [training: 0.004223137326343304 | validation: 0.004696423622672841]
	TIME [epoch: 8.21 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002030217652190895		[learning rate: 0.00019219]
		[batch 20/20] avg loss: 0.0023894152494894024		[learning rate: 0.00019196]
	Learning Rate: 0.00019196
	LOSS [training: 0.002209816450840149 | validation: 0.0013287165044326902]
	TIME [epoch: 8.18 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015599007400724293		[learning rate: 0.00019173]
		[batch 20/20] avg loss: 0.0021481515297625793		[learning rate: 0.0001915]
	Learning Rate: 0.000191495
	LOSS [training: 0.0018540261349175046 | validation: -0.0042331750211354316]
	TIME [epoch: 8.18 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000990756681708753		[learning rate: 0.00019126]
		[batch 20/20] avg loss: 0.005961469077387646		[learning rate: 0.00019103]
	Learning Rate: 0.000191032
	LOSS [training: 0.0034761128795481994 | validation: -0.0018060442736497096]
	TIME [epoch: 8.2 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0058495616979923224		[learning rate: 0.0001908]
		[batch 20/20] avg loss: 0.0009327128100138727		[learning rate: 0.00019057]
	Learning Rate: 0.000190569
	LOSS [training: -0.0024584244439892254 | validation: 0.00046329816928921384]
	TIME [epoch: 8.21 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025322808925414063		[learning rate: 0.00019034]
		[batch 20/20] avg loss: 0.004453163583165849		[learning rate: 0.00019011]
	Learning Rate: 0.000190108
	LOSS [training: 0.003492722237853628 | validation: -0.007416974070921565]
	TIME [epoch: 8.18 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003283708147149732		[learning rate: 0.00018988]
		[batch 20/20] avg loss: -0.0038208880743338053		[learning rate: 0.00018965]
	Learning Rate: 0.000189648
	LOSS [training: -0.00026858996359203636 | validation: -0.0014528425931404092]
	TIME [epoch: 8.17 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017570712302790218		[learning rate: 0.00018942]
		[batch 20/20] avg loss: -0.0013107464937423868		[learning rate: 0.00018919]
	Learning Rate: 0.000189189
	LOSS [training: -0.0015339088620107042 | validation: 7.327028259784319e-05]
	TIME [epoch: 8.17 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004477768902056115		[learning rate: 0.00018896]
		[batch 20/20] avg loss: 0.00536904962740004		[learning rate: 0.00018873]
	Learning Rate: 0.000188731
	LOSS [training: 0.00044564036267196285 | validation: -0.0039894951319379875]
	TIME [epoch: 8.2 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015854344162595645		[learning rate: 0.0001885]
		[batch 20/20] avg loss: 0.00335897372064654		[learning rate: 0.00018827]
	Learning Rate: 0.000188274
	LOSS [training: 0.0008867696521934877 | validation: -0.00023397281606796477]
	TIME [epoch: 8.21 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003414202072786451		[learning rate: 0.00018805]
		[batch 20/20] avg loss: 0.0030798527287079205		[learning rate: 0.00018782]
	Learning Rate: 0.000187818
	LOSS [training: 0.003247027400747186 | validation: -0.001431461447120465]
	TIME [epoch: 8.2 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002028766686765649		[learning rate: 0.00018759]
		[batch 20/20] avg loss: -0.004166404981744439		[learning rate: 0.00018736]
	Learning Rate: 0.000187363
	LOSS [training: -0.0030975858342550434 | validation: 0.005936355421394421]
	TIME [epoch: 8.18 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004216623245433349		[learning rate: 0.00018714]
		[batch 20/20] avg loss: 0.003112243293905157		[learning rate: 0.00018691]
	Learning Rate: 0.00018691
	LOSS [training: 0.0036644332696692526 | validation: -2.852997771624348e-05]
	TIME [epoch: 8.2 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0049945390642191926		[learning rate: 0.00018668]
		[batch 20/20] avg loss: -0.002215326073822976		[learning rate: 0.00018646]
	Learning Rate: 0.000186457
	LOSS [training: 0.0013896064951981085 | validation: 4.696197198574611e-05]
	TIME [epoch: 8.19 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025304502335725177		[learning rate: 0.00018623]
		[batch 20/20] avg loss: 0.0029286588436685346		[learning rate: 0.00018601]
	Learning Rate: 0.000186006
	LOSS [training: 0.00019910430504800847 | validation: 0.0008903585164442038]
	TIME [epoch: 8.2 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002776411646469351		[learning rate: 0.00018578]
		[batch 20/20] avg loss: 0.005737420222550421		[learning rate: 0.00018556]
	Learning Rate: 0.000185555
	LOSS [training: 0.001480504288040535 | validation: -0.001893036894651763]
	TIME [epoch: 8.16 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004017036935509153		[learning rate: 0.00018533]
		[batch 20/20] avg loss: 0.0064391083532498936		[learning rate: 0.00018511]
	Learning Rate: 0.000185106
	LOSS [training: 0.0012110357088703709 | validation: 0.002494945957906135]
	TIME [epoch: 8.19 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002222026817565766		[learning rate: 0.00018488]
		[batch 20/20] avg loss: 0.0006537782366460062		[learning rate: 0.00018466]
	Learning Rate: 0.000184658
	LOSS [training: -0.0007841242904598796 | validation: 0.004805985912359852]
	TIME [epoch: 8.17 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036079235271536834		[learning rate: 0.00018443]
		[batch 20/20] avg loss: -0.0025291864341848813		[learning rate: 0.00018421]
	Learning Rate: 0.000184211
	LOSS [training: 0.0005393685464844008 | validation: -0.001286089179499788]
	TIME [epoch: 8.18 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000632983098351371		[learning rate: 0.00018399]
		[batch 20/20] avg loss: -0.0019312055570747514		[learning rate: 0.00018377]
	Learning Rate: 0.000183765
	LOSS [training: -0.0012820943277130612 | validation: -0.004706674519695054]
	TIME [epoch: 8.17 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000539751581690906		[learning rate: 0.00018354]
		[batch 20/20] avg loss: -0.007659004039801444		[learning rate: 0.00018332]
	Learning Rate: 0.00018332
	LOSS [training: -0.003559626229055269 | validation: -0.011407539274200797]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240219_183143/states/model_tr_study2_1750.pth
	Model improved!!!
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014421547550073379		[learning rate: 0.0001831]
		[batch 20/20] avg loss: -0.0028002116932153128		[learning rate: 0.00018288]
	Learning Rate: 0.000182876
	LOSS [training: -0.002121183224111325 | validation: 0.0007921169669899563]
	TIME [epoch: 8.17 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015304545151692678		[learning rate: 0.00018266]
		[batch 20/20] avg loss: 0.0027554610931825553		[learning rate: 0.00018243]
	Learning Rate: 0.000182434
	LOSS [training: 0.002142957804175911 | validation: -0.007863577395648151]
	TIME [epoch: 8.21 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019985047533760516		[learning rate: 0.00018221]
		[batch 20/20] avg loss: 0.0002144413091417925		[learning rate: 0.00018199]
	Learning Rate: 0.000181992
	LOSS [training: 0.001106473031258922 | validation: 0.00685880861061409]
	TIME [epoch: 8.18 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.698213721772183e-05		[learning rate: 0.00018177]
		[batch 20/20] avg loss: 0.0010744654927352393		[learning rate: 0.00018155]
	Learning Rate: 0.000181552
	LOSS [training: 0.0005757238149764804 | validation: 0.0027408175542389286]
	TIME [epoch: 8.2 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026046856343327823		[learning rate: 0.00018133]
		[batch 20/20] avg loss: -0.0004899961848703431		[learning rate: 0.00018111]
	Learning Rate: 0.000181112
	LOSS [training: 0.0010573447247312197 | validation: -0.0022615007114903797]
	TIME [epoch: 8.18 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004674043468197994		[learning rate: 0.00018089]
		[batch 20/20] avg loss: -0.001776980844318083		[learning rate: 0.00018067]
	Learning Rate: 0.000180674
	LOSS [training: -0.0006547882487491421 | validation: -0.002158713732672112]
	TIME [epoch: 8.21 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003876843044623638		[learning rate: 0.00018045]
		[batch 20/20] avg loss: 0.0002882385340734467		[learning rate: 0.00018024]
	Learning Rate: 0.000180236
	LOSS [training: -0.0017943022552750962 | validation: -5.448711464786562e-05]
	TIME [epoch: 8.18 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001669079975092852		[learning rate: 0.00018002]
		[batch 20/20] avg loss: -0.00192755722225756		[learning rate: 0.0001798]
	Learning Rate: 0.0001798
	LOSS [training: -0.00012923862358235383 | validation: 0.0018940013661700365]
	TIME [epoch: 8.19 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003716658068585172		[learning rate: 0.00017958]
		[batch 20/20] avg loss: -0.0025130864319530734		[learning rate: 0.00017936]
	Learning Rate: 0.000179365
	LOSS [training: 0.0006017858183160494 | validation: -0.00472356814843092]
	TIME [epoch: 8.18 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010236552710701807		[learning rate: 0.00017915]
		[batch 20/20] avg loss: -0.0038090575866483885		[learning rate: 0.00017893]
	Learning Rate: 0.00017893
	LOSS [training: -0.002416356428859284 | validation: -0.002157408596876853]
	TIME [epoch: 8.17 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004600040368806106		[learning rate: 0.00017871]
		[batch 20/20] avg loss: -0.0028854813316625924		[learning rate: 0.0001785]
	Learning Rate: 0.000178497
	LOSS [training: 0.0008572795185717569 | validation: 0.0011884884272314188]
	TIME [epoch: 8.17 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009725295806462242		[learning rate: 0.00017828]
		[batch 20/20] avg loss: 0.0008583467384683794		[learning rate: 0.00017807]
	Learning Rate: 0.000178065
	LOSS [training: 0.0009154381595573019 | validation: -0.007674926380694228]
	TIME [epoch: 8.19 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012479817846792877		[learning rate: 0.00017785]
		[batch 20/20] avg loss: 0.00023280444506817286		[learning rate: 0.00017763]
	Learning Rate: 0.000177634
	LOSS [training: 0.0007403931148737303 | validation: -0.004861624298893691]
	TIME [epoch: 8.18 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003055903785277906		[learning rate: 0.00017742]
		[batch 20/20] avg loss: 0.0035030428676195496		[learning rate: 0.0001772]
	Learning Rate: 0.000177204
	LOSS [training: 0.00022356954117082198 | validation: -0.006138352568171657]
	TIME [epoch: 8.2 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010462831021560374		[learning rate: 0.00017699]
		[batch 20/20] avg loss: 0.0010813445137702223		[learning rate: 0.00017678]
	Learning Rate: 0.000176775
	LOSS [training: 1.7530705807092608e-05 | validation: -0.004739198454608812]
	TIME [epoch: 8.18 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004735270829788112		[learning rate: 0.00017656]
		[batch 20/20] avg loss: -0.0010380318159813402		[learning rate: 0.00017635]
	Learning Rate: 0.000176347
	LOSS [training: 0.0018486195069033855 | validation: -0.008315019985926065]
	TIME [epoch: 8.2 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00029810835770987973		[learning rate: 0.00017613]
		[batch 20/20] avg loss: -0.00550276114612327		[learning rate: 0.00017592]
	Learning Rate: 0.00017592
	LOSS [training: -0.0026023263942066954 | validation: -0.0015130040080023196]
	TIME [epoch: 8.19 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00024465585792103763		[learning rate: 0.00017571]
		[batch 20/20] avg loss: 0.004524931695886102		[learning rate: 0.00017549]
	Learning Rate: 0.000175494
	LOSS [training: 0.002140137918982532 | validation: -0.00450763017246144]
	TIME [epoch: 8.2 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0047832401891682455		[learning rate: 0.00017528]
		[batch 20/20] avg loss: 0.004790493300488524		[learning rate: 0.00017507]
	Learning Rate: 0.00017507
	LOSS [training: 3.6265556601388844e-06 | validation: 0.0009097057977626905]
	TIME [epoch: 8.17 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022657388974463674		[learning rate: 0.00017486]
		[batch 20/20] avg loss: 0.004434552816799132		[learning rate: 0.00017465]
	Learning Rate: 0.000174646
	LOSS [training: 0.003350145857122749 | validation: 0.0001128706782536769]
	TIME [epoch: 8.19 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002773945287461868		[learning rate: 0.00017443]
		[batch 20/20] avg loss: -0.001271391961563516		[learning rate: 0.00017422]
	Learning Rate: 0.000174223
	LOSS [training: -0.002022668624512692 | validation: -0.0029356389319923548]
	TIME [epoch: 8.17 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007649376214330832		[learning rate: 0.00017401]
		[batch 20/20] avg loss: -0.0003364809737729221		[learning rate: 0.0001738]
	Learning Rate: 0.000173801
	LOSS [training: 0.00021422832383008063 | validation: 0.0038753111963438502]
	TIME [epoch: 8.17 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005286347569542827		[learning rate: 0.00017359]
		[batch 20/20] avg loss: -0.0016745076693221229		[learning rate: 0.00017338]
	Learning Rate: 0.00017338
	LOSS [training: 0.0018059199501103524 | validation: -0.0010599834125145453]
	TIME [epoch: 8.17 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002993133202436755		[learning rate: 0.00017317]
		[batch 20/20] avg loss: 0.0022298113985002766		[learning rate: 0.00017296]
	Learning Rate: 0.000172961
	LOSS [training: 0.001264562359371976 | validation: -0.0045359074401030795]
	TIME [epoch: 8.2 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00213258244260358		[learning rate: 0.00017275]
		[batch 20/20] avg loss: 0.004319771772478817		[learning rate: 0.00017254]
	Learning Rate: 0.000172542
	LOSS [training: 0.0010935946649376187 | validation: -0.006544689056866087]
	TIME [epoch: 8.18 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00341077144934781		[learning rate: 0.00017233]
		[batch 20/20] avg loss: -0.002050686125619915		[learning rate: 0.00017212]
	Learning Rate: 0.000172124
	LOSS [training: 0.0006800426618639478 | validation: -0.003750269155561054]
	TIME [epoch: 8.2 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00011563455199279796		[learning rate: 0.00017192]
		[batch 20/20] avg loss: -0.0005176153030433958		[learning rate: 0.00017171]
	Learning Rate: 0.000171708
	LOSS [training: -0.0002009903755252988 | validation: -0.0004921538973219758]
	TIME [epoch: 8.18 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033038801845495145		[learning rate: 0.0001715]
		[batch 20/20] avg loss: -0.0016822970325534713		[learning rate: 0.00017129]
	Learning Rate: 0.000171292
	LOSS [training: 0.0008107915759980211 | validation: -0.009028882161886997]
	TIME [epoch: 8.2 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033132388179252245		[learning rate: 0.00017108]
		[batch 20/20] avg loss: 0.002390769254881473		[learning rate: 0.00017088]
	Learning Rate: 0.000170877
	LOSS [training: -0.0004612347815218752 | validation: -0.004897308458111728]
	TIME [epoch: 8.2 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001295198678447798		[learning rate: 0.00017067]
		[batch 20/20] avg loss: 0.0013074539835189593		[learning rate: 0.00017046]
	Learning Rate: 0.000170464
	LOSS [training: 0.0013013263309833787 | validation: -0.004544635783635867]
	TIME [epoch: 8.19 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030356636116558276		[learning rate: 0.00017026]
		[batch 20/20] avg loss: -2.456294124370687e-05		[learning rate: 0.00017005]
	Learning Rate: 0.000170051
	LOSS [training: 0.0015055503352060604 | validation: 0.006985169650113447]
	TIME [epoch: 8.17 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001217423951295309		[learning rate: 0.00016984]
		[batch 20/20] avg loss: 0.002183149419748447		[learning rate: 0.00016964]
	Learning Rate: 0.000169639
	LOSS [training: 0.0017002866855218783 | validation: -0.0009560821877939725]
	TIME [epoch: 8.18 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0053474537898085765		[learning rate: 0.00016943]
		[batch 20/20] avg loss: 0.004530737834736549		[learning rate: 0.00016923]
	Learning Rate: 0.000169229
	LOSS [training: -0.00040835797753601376 | validation: -0.004651243371636615]
	TIME [epoch: 8.18 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002401475405225538		[learning rate: 0.00016902]
		[batch 20/20] avg loss: -0.0006906116202279038		[learning rate: 0.00016882]
	Learning Rate: 0.000168819
	LOSS [training: 0.000855431892498817 | validation: -0.0019110237525273724]
	TIME [epoch: 8.17 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00024900945443975483		[learning rate: 0.00016861]
		[batch 20/20] avg loss: -0.00019167790184922953		[learning rate: 0.00016841]
	Learning Rate: 0.00016841
	LOSS [training: -0.00022034367814449205 | validation: -0.010476283129878961]
	TIME [epoch: 8.17 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014238380171262624		[learning rate: 0.00016821]
		[batch 20/20] avg loss: -0.002339792413099351		[learning rate: 0.000168]
	Learning Rate: 0.000168003
	LOSS [training: -0.0004579771979865444 | validation: -0.00449456294033803]
	TIME [epoch: 8.17 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026930218908327646		[learning rate: 0.0001678]
		[batch 20/20] avg loss: 0.0021996883955347625		[learning rate: 0.0001676]
	Learning Rate: 0.000167596
	LOSS [training: 0.0024463551431837633 | validation: -0.001835871750937094]
	TIME [epoch: 8.19 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00017571446338543304		[learning rate: 0.00016739]
		[batch 20/20] avg loss: 0.0019591176787053235		[learning rate: 0.00016719]
	Learning Rate: 0.00016719
	LOSS [training: 0.0008917016076599451 | validation: -0.0020267909557650002]
	TIME [epoch: 8.2 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005461010041220188		[learning rate: 0.00016699]
		[batch 20/20] avg loss: -0.005869132121127509		[learning rate: 0.00016679]
	Learning Rate: 0.000166785
	LOSS [training: -0.00020406103995366062 | validation: 0.004209607125672824]
	TIME [epoch: 8.18 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003797927958177183		[learning rate: 0.00016658]
		[batch 20/20] avg loss: 0.0030450880656860144		[learning rate: 0.00016638]
	Learning Rate: 0.000166382
	LOSS [training: 0.001332647634934148 | validation: -0.003087506702156221]
	TIME [epoch: 8.18 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008326960285502331		[learning rate: 0.00016618]
		[batch 20/20] avg loss: 0.0052504197613691145		[learning rate: 0.00016598]
	Learning Rate: 0.000165979
	LOSS [training: 0.003041557894959675 | validation: -0.0006568397219247386]
	TIME [epoch: 8.21 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002507352564147674		[learning rate: 0.00016578]
		[batch 20/20] avg loss: 0.0017676210866417168		[learning rate: 0.00016558]
	Learning Rate: 0.000165577
	LOSS [training: -0.00036986573875297864 | validation: 0.0018317455292412863]
	TIME [epoch: 8.2 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035819487399749044		[learning rate: 0.00016538]
		[batch 20/20] avg loss: 0.0027162684974981893		[learning rate: 0.00016518]
	Learning Rate: 0.000165176
	LOSS [training: 0.0031491086187365467 | validation: 0.001875400496535376]
	TIME [epoch: 8.17 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002363485968184277		[learning rate: 0.00016498]
		[batch 20/20] avg loss: 0.0033865129445153066		[learning rate: 0.00016478]
	Learning Rate: 0.000164776
	LOSS [training: 0.002874999456349792 | validation: 0.000739752547737738]
	TIME [epoch: 8.17 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004717396608656718		[learning rate: 0.00016458]
		[batch 20/20] avg loss: -0.007951833614390388		[learning rate: 0.00016438]
	Learning Rate: 0.000164377
	LOSS [training: -0.001617218502866835 | validation: 0.0034735264308325296]
	TIME [epoch: 8.19 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010817626057642543		[learning rate: 0.00016418]
		[batch 20/20] avg loss: 0.0014262456018037588		[learning rate: 0.00016398]
	Learning Rate: 0.000163979
	LOSS [training: 0.001254004103784007 | validation: -0.006809570667924909]
	TIME [epoch: 8.17 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010618808612328286		[learning rate: 0.00016378]
		[batch 20/20] avg loss: 0.003187907411090285		[learning rate: 0.00016358]
	Learning Rate: 0.000163583
	LOSS [training: 0.0021248941361615564 | validation: -0.003151425101100105]
	TIME [epoch: 8.17 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015481243836764232		[learning rate: 0.00016338]
		[batch 20/20] avg loss: 0.0005579516459380085		[learning rate: 0.00016319]
	Learning Rate: 0.000163187
	LOSS [training: 0.0010530380148072158 | validation: 0.00010928695264549645]
	TIME [epoch: 8.18 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001371049287354378		[learning rate: 0.00016299]
		[batch 20/20] avg loss: 0.004262303009831791		[learning rate: 0.00016279]
	Learning Rate: 0.000162791
	LOSS [training: 0.0014456268612387069 | validation: -0.005197893419302805]
	TIME [epoch: 8.22 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003454375714826813		[learning rate: 0.00016259]
		[batch 20/20] avg loss: 0.006815518975711601		[learning rate: 0.0001624]
	Learning Rate: 0.000162397
	LOSS [training: 0.003580478273597141 | validation: -0.0043083782910674105]
	TIME [epoch: 8.19 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008591875647669618		[learning rate: 0.0001622]
		[batch 20/20] avg loss: 0.0005592524568392413		[learning rate: 0.000162]
	Learning Rate: 0.000162004
	LOSS [training: -0.0001499675539638603 | validation: -0.005705693764815383]
	TIME [epoch: 8.18 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000305026143863663		[learning rate: 0.00016181]
		[batch 20/20] avg loss: 0.00371899291081396		[learning rate: 0.00016161]
	Learning Rate: 0.000161612
	LOSS [training: 0.0017069833834751484 | validation: 0.005916896340513308]
	TIME [epoch: 8.18 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.944241693980029e-05		[learning rate: 0.00016142]
		[batch 20/20] avg loss: -0.0012213386541588688		[learning rate: 0.00016122]
	Learning Rate: 0.000161221
	LOSS [training: -0.0006009481186095343 | validation: 0.003166710506740824]
	TIME [epoch: 8.23 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029784283746165765		[learning rate: 0.00016103]
		[batch 20/20] avg loss: -0.0023648020522605588		[learning rate: 0.00016083]
	Learning Rate: 0.000160831
	LOSS [training: 0.0003068131611780085 | validation: -0.007042106262848952]
	TIME [epoch: 8.17 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002472500077829395		[learning rate: 0.00016064]
		[batch 20/20] avg loss: -0.0013380380481291042		[learning rate: 0.00016044]
	Learning Rate: 0.000160441
	LOSS [training: -0.0005453940201730822 | validation: -0.003457237006803107]
	TIME [epoch: 8.17 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026474268530867332		[learning rate: 0.00016025]
		[batch 20/20] avg loss: -0.0031231765999941347		[learning rate: 0.00016005]
	Learning Rate: 0.000160053
	LOSS [training: -0.00023787487345370084 | validation: -0.007515111218946004]
	TIME [epoch: 8.17 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016120627087580258		[learning rate: 0.00015986]
		[batch 20/20] avg loss: 0.000427193092545536		[learning rate: 0.00015967]
	Learning Rate: 0.000159665
	LOSS [training: -0.0005924348081062447 | validation: -0.0012217192755338384]
	TIME [epoch: 8.19 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00041067479360524866		[learning rate: 0.00015947]
		[batch 20/20] avg loss: -0.0026081998420253117		[learning rate: 0.00015928]
	Learning Rate: 0.000159279
	LOSS [training: -0.0010987625242100314 | validation: -0.00022477454939551743]
	TIME [epoch: 8.17 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016099003302288775		[learning rate: 0.00015909]
		[batch 20/20] avg loss: 0.0012850153810881608		[learning rate: 0.00015889]
	Learning Rate: 0.000158893
	LOSS [training: -0.0001624424745703586 | validation: -0.002297234212693556]
	TIME [epoch: 8.17 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019473458559892676		[learning rate: 0.0001587]
		[batch 20/20] avg loss: 0.0007624438018177945		[learning rate: 0.00015851]
	Learning Rate: 0.000158509
	LOSS [training: 0.0013548948289035315 | validation: -0.0001241697819528969]
	TIME [epoch: 8.17 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006111819548683503		[learning rate: 0.00015832]
		[batch 20/20] avg loss: -0.00010469183506921264		[learning rate: 0.00015812]
	Learning Rate: 0.000158125
	LOSS [training: 0.0030035638568071455 | validation: -0.002262972023354406]
	TIME [epoch: 8.19 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009618692944307981		[learning rate: 0.00015793]
		[batch 20/20] avg loss: -0.003635218767158308		[learning rate: 0.00015774]
	Learning Rate: 0.000157742
	LOSS [training: -0.001336674736363755 | validation: -0.005592785269141722]
	TIME [epoch: 8.19 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001758723752938308		[learning rate: 0.00015755]
		[batch 20/20] avg loss: -0.0015196862572979268		[learning rate: 0.00015736]
	Learning Rate: 0.00015736
	LOSS [training: 0.00011951874782019099 | validation: -0.003117400710097879]
	TIME [epoch: 8.18 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00021019654043908693		[learning rate: 0.00015717]
		[batch 20/20] avg loss: -0.0007087634393991317		[learning rate: 0.00015698]
	Learning Rate: 0.000156979
	LOSS [training: -0.0004594799899191093 | validation: -0.004245434816747385]
	TIME [epoch: 8.18 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002688794104069045		[learning rate: 0.00015679]
		[batch 20/20] avg loss: 0.0012290182215847119		[learning rate: 0.0001566]
	Learning Rate: 0.000156599
	LOSS [training: -0.0007298879412421667 | validation: 0.0018490500739387797]
	TIME [epoch: 8.2 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004062118274658889		[learning rate: 0.00015641]
		[batch 20/20] avg loss: -0.007308834886126707		[learning rate: 0.00015622]
	Learning Rate: 0.00015622
	LOSS [training: -0.0016233583057339081 | validation: 0.0023963793756978675]
	TIME [epoch: 8.21 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00018880287313281706		[learning rate: 0.00015603]
		[batch 20/20] avg loss: 0.0045619037995796325		[learning rate: 0.00015584]
	Learning Rate: 0.000155842
	LOSS [training: 0.0021865504632234078 | validation: 0.0017921859641837287]
	TIME [epoch: 8.17 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002473731930059166		[learning rate: 0.00015565]
		[batch 20/20] avg loss: 0.00014734037761529373		[learning rate: 0.00015546]
	Learning Rate: 0.000155465
	LOSS [training: 0.0013105361538372296 | validation: -0.0018537703763310681]
	TIME [epoch: 8.17 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002268671740649843		[learning rate: 0.00015528]
		[batch 20/20] avg loss: -0.0024800543847905464		[learning rate: 0.00015509]
	Learning Rate: 0.000155088
	LOSS [training: -0.002374363062720195 | validation: -0.005621475398459365]
	TIME [epoch: 8.19 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028705851084781255		[learning rate: 0.0001549]
		[batch 20/20] avg loss: 0.002568262115724742		[learning rate: 0.00015471]
	Learning Rate: 0.000154713
	LOSS [training: -0.00015116149637669254 | validation: 0.0035305343335231146]
	TIME [epoch: 8.17 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00123511105920331		[learning rate: 0.00015453]
		[batch 20/20] avg loss: -0.004236059634547222		[learning rate: 0.00015434]
	Learning Rate: 0.000154338
	LOSS [training: -0.0015004742876719563 | validation: -0.002942638309530816]
	TIME [epoch: 8.17 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00032740267753177197		[learning rate: 0.00015415]
		[batch 20/20] avg loss: 0.0027151631255625933		[learning rate: 0.00015396]
	Learning Rate: 0.000153965
	LOSS [training: 0.0011938802240154106 | validation: -0.000327662107045573]
	TIME [epoch: 8.17 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006920316887277357		[learning rate: 0.00015378]
		[batch 20/20] avg loss: 0.0037899891393461406		[learning rate: 0.00015359]
	Learning Rate: 0.000153592
	LOSS [training: 0.0022410104140369383 | validation: -0.0023609527825605506]
	TIME [epoch: 8.19 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00270760964039176		[learning rate: 0.00015341]
		[batch 20/20] avg loss: 0.0005242334529086914		[learning rate: 0.00015322]
	Learning Rate: 0.00015322
	LOSS [training: -0.0010916880937415342 | validation: -0.0007234950325159626]
	TIME [epoch: 8.2 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003311377059462415		[learning rate: 0.00015303]
		[batch 20/20] avg loss: 0.002886410919243391		[learning rate: 0.00015285]
	Learning Rate: 0.000152849
	LOSS [training: -0.00021248307010951159 | validation: 0.0003684001403562302]
	TIME [epoch: 8.19 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001067880014143103		[learning rate: 0.00015266]
		[batch 20/20] avg loss: 0.004573201775150973		[learning rate: 0.00015248]
	Learning Rate: 0.000152479
	LOSS [training: 0.002820540894647038 | validation: -0.0024466197938911006]
	TIME [epoch: 8.18 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00038664324752525484		[learning rate: 0.00015229]
		[batch 20/20] avg loss: -0.0006802972186162002		[learning rate: 0.00015211]
	Learning Rate: 0.00015211
	LOSS [training: -0.0005334702330707275 | validation: 0.0018657825267293863]
	TIME [epoch: 8.2 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013413950403517575		[learning rate: 0.00015193]
		[batch 20/20] avg loss: 0.004353540014335067		[learning rate: 0.00015174]
	Learning Rate: 0.000151742
	LOSS [training: 0.0015060724869916547 | validation: 0.000484459262647174]
	TIME [epoch: 8.2 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003654634004950178		[learning rate: 0.00015156]
		[batch 20/20] avg loss: -0.0027438632591684736		[learning rate: 0.00015137]
	Learning Rate: 0.000151374
	LOSS [training: -0.0015546633298317456 | validation: 0.0067848354154616194]
	TIME [epoch: 8.19 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002774996975083097		[learning rate: 0.00015119]
		[batch 20/20] avg loss: 0.00030692303344942196		[learning rate: 0.00015101]
	Learning Rate: 0.000151008
	LOSS [training: 0.0015409600042662602 | validation: -0.004634980112723525]
	TIME [epoch: 8.17 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004528652133714377		[learning rate: 0.00015083]
		[batch 20/20] avg loss: -0.0027168569950945425		[learning rate: 0.00015064]
	Learning Rate: 0.000150642
	LOSS [training: 0.0009058975693099173 | validation: 0.00020872374875573413]
	TIME [epoch: 8.19 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014530429278689905		[learning rate: 0.00015046]
		[batch 20/20] avg loss: -0.0031381026350888173		[learning rate: 0.00015028]
	Learning Rate: 0.000150278
	LOSS [training: -0.0022955727814789042 | validation: -0.005512551146367829]
	TIME [epoch: 8.18 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009064534453443492		[learning rate: 0.0001501]
		[batch 20/20] avg loss: -0.0009387219104185163		[learning rate: 0.00014991]
	Learning Rate: 0.000149914
	LOSS [training: -0.0009225876778814328 | validation: -0.0020467491129782384]
	TIME [epoch: 8.17 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004263437569426315		[learning rate: 0.00014973]
		[batch 20/20] avg loss: 0.002242565738292371		[learning rate: 0.00014955]
	Learning Rate: 0.000149551
	LOSS [training: -0.0010104359155669723 | validation: 0.0005143283975578139]
	TIME [epoch: 8.17 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028229604650910275		[learning rate: 0.00014937]
		[batch 20/20] avg loss: 0.0030996764074579983		[learning rate: 0.00014919]
	Learning Rate: 0.000149189
	LOSS [training: 0.0001383579711834854 | validation: -0.00922207961795924]
	TIME [epoch: 8.19 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006448167759070771		[learning rate: 0.00014901]
		[batch 20/20] avg loss: -0.0005297954346644774		[learning rate: 0.00014883]
	Learning Rate: 0.000148828
	LOSS [training: 5.751067062129997e-05 | validation: -0.00633788148609688]
	TIME [epoch: 8.17 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034996313152088794		[learning rate: 0.00014865]
		[batch 20/20] avg loss: 0.001767379686398857		[learning rate: 0.00014847]
	Learning Rate: 0.000148468
	LOSS [training: 0.0026335055008038684 | validation: -0.006517138014710926]
	TIME [epoch: 8.19 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004663248021571557		[learning rate: 0.00014829]
		[batch 20/20] avg loss: -0.0019128287244553504		[learning rate: 0.00014811]
	Learning Rate: 0.000148108
	LOSS [training: -0.0011895767633062532 | validation: -0.007373825186458443]
	TIME [epoch: 8.19 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017360984274780386		[learning rate: 0.00014793]
		[batch 20/20] avg loss: -0.00040790237819955406		[learning rate: 0.00014775]
	Learning Rate: 0.00014775
	LOSS [training: -0.001072000402838796 | validation: -0.004858771137921967]
	TIME [epoch: 8.19 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014787281019563777		[learning rate: 0.00014757]
		[batch 20/20] avg loss: -0.0017114569999410647		[learning rate: 0.00014739]
	Learning Rate: 0.000147392
	LOSS [training: -0.00011636444899234344 | validation: -0.006816512419090769]
	TIME [epoch: 8.18 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023466436869199097		[learning rate: 0.00014721]
		[batch 20/20] avg loss: -0.003815775431697665		[learning rate: 0.00014704]
	Learning Rate: 0.000147035
	LOSS [training: -0.0007345658723888777 | validation: -0.003699289757818305]
	TIME [epoch: 8.19 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015064587059686386		[learning rate: 0.00014686]
		[batch 20/20] avg loss: 0.0028595512725093464		[learning rate: 0.00014668]
	Learning Rate: 0.000146679
	LOSS [training: 0.0021830049892389925 | validation: 0.0005603464473811319]
	TIME [epoch: 8.18 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001808715425080256		[learning rate: 0.0001465]
		[batch 20/20] avg loss: -0.002859714709439587		[learning rate: 0.00014632]
	Learning Rate: 0.000146324
	LOSS [training: -0.0005254996421796658 | validation: 0.0033861091375576476]
	TIME [epoch: 8.18 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006727114875684809		[learning rate: 0.00014615]
		[batch 20/20] avg loss: -0.002220576704247311		[learning rate: 0.00014597]
	Learning Rate: 0.00014597
	LOSS [training: -0.001446644095907896 | validation: 0.002093003013502042]
	TIME [epoch: 8.16 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011175166404347463		[learning rate: 0.00014579]
		[batch 20/20] avg loss: 0.0008445473715859902		[learning rate: 0.00014562]
	Learning Rate: 0.000145616
	LOSS [training: -0.00013648463442437813 | validation: 0.000807804307108011]
	TIME [epoch: 8.17 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007049958003619911		[learning rate: 0.00014544]
		[batch 20/20] avg loss: -0.00025100053801514895		[learning rate: 0.00014526]
	Learning Rate: 0.000145264
	LOSS [training: -0.0004779981691885699 | validation: -0.0012096276610263114]
	TIME [epoch: 8.17 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003455015820065757		[learning rate: 0.00014509]
		[batch 20/20] avg loss: 0.0016530250649392571		[learning rate: 0.00014491]
	Learning Rate: 0.000144912
	LOSS [training: -0.0009009953775632502 | validation: -0.0043868277570964275]
	TIME [epoch: 8.18 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002147287984407046		[learning rate: 0.00014474]
		[batch 20/20] avg loss: -0.00017256679059840158		[learning rate: 0.00014456]
	Learning Rate: 0.000144561
	LOSS [training: 0.0009873605969043225 | validation: 0.00014269055757262436]
	TIME [epoch: 8.17 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026620220744043967		[learning rate: 0.00014439]
		[batch 20/20] avg loss: -0.0010781081970496656		[learning rate: 0.00014421]
	Learning Rate: 0.000144212
	LOSS [training: 0.0007919569386773656 | validation: -0.006007118523961933]
	TIME [epoch: 8.19 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003077907433660752		[learning rate: 0.00014404]
		[batch 20/20] avg loss: 0.0027329885705744414		[learning rate: 0.00014386]
	Learning Rate: 0.000143862
	LOSS [training: -0.00017245943154315545 | validation: -0.0024861425778849216]
	TIME [epoch: 8.17 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029203044001665603		[learning rate: 0.00014369]
		[batch 20/20] avg loss: -0.0027159886127920835		[learning rate: 0.00014351]
	Learning Rate: 0.000143514
	LOSS [training: -0.0028181465064793217 | validation: 0.0015179297016700885]
	TIME [epoch: 8.2 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004256777448689224		[learning rate: 0.00014334]
		[batch 20/20] avg loss: 0.0026117977160743837		[learning rate: 0.00014317]
	Learning Rate: 0.000143167
	LOSS [training: -0.0008224898663074206 | validation: -0.002217895610012551]
	TIME [epoch: 8.19 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006175513853280077		[learning rate: 0.00014299]
		[batch 20/20] avg loss: -0.003244864700831164		[learning rate: 0.00014282]
	Learning Rate: 0.00014282
	LOSS [training: 0.0014653245762244575 | validation: -0.0072152222465458994]
	TIME [epoch: 8.19 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029608364299288795		[learning rate: 0.00014265]
		[batch 20/20] avg loss: -0.0005889697354878234		[learning rate: 0.00014247]
	Learning Rate: 0.000142474
	LOSS [training: 0.0011859333472205278 | validation: -0.005448010379815869]
	TIME [epoch: 8.16 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017208183311777342		[learning rate: 0.0001423]
		[batch 20/20] avg loss: 0.0033661643394057804		[learning rate: 0.00014213]
	Learning Rate: 0.000142129
	LOSS [training: 0.0008226730041140231 | validation: -0.007824367631166078]
	TIME [epoch: 8.18 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00012902821247018766		[learning rate: 0.00014196]
		[batch 20/20] avg loss: -0.00028990186262626265		[learning rate: 0.00014179]
	Learning Rate: 0.000141785
	LOSS [training: -8.043682507803745e-05 | validation: -0.007965185419286666]
	TIME [epoch: 8.17 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013431587047866241		[learning rate: 0.00014161]
		[batch 20/20] avg loss: 0.005835491612712774		[learning rate: 0.00014144]
	Learning Rate: 0.000141442
	LOSS [training: 0.003589325158749699 | validation: 8.616442105916183e-05]
	TIME [epoch: 8.16 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004969334347265774		[learning rate: 0.00014127]
		[batch 20/20] avg loss: -0.0013806966609084506		[learning rate: 0.0001411]
	Learning Rate: 0.0001411
	LOSS [training: -0.003175015504087113 | validation: -0.006344169058335626]
	TIME [epoch: 8.16 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002932699410677469		[learning rate: 0.00014093]
		[batch 20/20] avg loss: 0.002552524194344264		[learning rate: 0.00014076]
	Learning Rate: 0.000140758
	LOSS [training: -0.00019008760816660274 | validation: 0.005800130319762544]
	TIME [epoch: 8.19 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025064203929019806		[learning rate: 0.00014059]
		[batch 20/20] avg loss: 0.0028868636967571645		[learning rate: 0.00014042]
	Learning Rate: 0.000140417
	LOSS [training: 0.00019022165192759195 | validation: 0.00038351918185291394]
	TIME [epoch: 8.17 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00016884620037120463		[learning rate: 0.00014025]
		[batch 20/20] avg loss: 0.0008538763999655712		[learning rate: 0.00014008]
	Learning Rate: 0.000140078
	LOSS [training: 0.00034251509979718326 | validation: -0.005243624830497385]
	TIME [epoch: 8.17 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00033325117715974175		[learning rate: 0.00013991]
		[batch 20/20] avg loss: -0.0007984783549965197		[learning rate: 0.00013974]
	Learning Rate: 0.000139738
	LOSS [training: -0.0005658647660781307 | validation: -0.004616520480328797]
	TIME [epoch: 8.18 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002283940090117532		[learning rate: 0.00013957]
		[batch 20/20] avg loss: 0.00113932729191973		[learning rate: 0.0001394]
	Learning Rate: 0.0001394
	LOSS [training: -0.0005723063990989007 | validation: -0.0006476328069466125]
	TIME [epoch: 8.19 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007509598151192759		[learning rate: 0.00013923]
		[batch 20/20] avg loss: -0.004653140128027903		[learning rate: 0.00013906]
	Learning Rate: 0.000139063
	LOSS [training: -0.002702049971573589 | validation: 0.0023730325293853313]
	TIME [epoch: 8.18 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022630660220694238		[learning rate: 0.00013889]
		[batch 20/20] avg loss: -0.003322120140296759		[learning rate: 0.00013873]
	Learning Rate: 0.000138726
	LOSS [training: -0.0027925930811830905 | validation: -0.00027098910863519276]
	TIME [epoch: 8.19 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013357441941976892		[learning rate: 0.00013856]
		[batch 20/20] avg loss: 0.00027421149754373094		[learning rate: 0.00013839]
	Learning Rate: 0.00013839
	LOSS [training: 0.0008049778458707098 | validation: -0.004968771532320107]
	TIME [epoch: 8.17 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0072322913177907495		[learning rate: 0.00013822]
		[batch 20/20] avg loss: -0.001804928808123183		[learning rate: 0.00013806]
	Learning Rate: 0.000138055
	LOSS [training: 0.0027136812548337833 | validation: -0.007474677624015033]
	TIME [epoch: 8.17 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013876742256042516		[learning rate: 0.00013789]
		[batch 20/20] avg loss: 0.0008125626844239682		[learning rate: 0.00013772]
	Learning Rate: 0.000137721
	LOSS [training: -0.00028755577059014177 | validation: -0.002775955604584983]
	TIME [epoch: 8.18 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029438379639875186		[learning rate: 0.00013755]
		[batch 20/20] avg loss: -0.00633078463523328		[learning rate: 0.00013739]
	Learning Rate: 0.000137388
	LOSS [training: -0.0016934733356228804 | validation: -0.0012204629737196234]
	TIME [epoch: 8.16 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.04466058445597e-05		[learning rate: 0.00013722]
		[batch 20/20] avg loss: -5.760205278663618e-05		[learning rate: 0.00013705]
	Learning Rate: 0.000137055
	LOSS [training: 1.642227652896184e-05 | validation: -0.004929136465854547]
	TIME [epoch: 8.16 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003339599908594816		[learning rate: 0.00013689]
		[batch 20/20] avg loss: -0.00231699104133474		[learning rate: 0.00013672]
	Learning Rate: 0.000136723
	LOSS [training: -0.002828295474964778 | validation: -0.0038972813614744505]
	TIME [epoch: 8.16 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011184967665026314		[learning rate: 0.00013656]
		[batch 20/20] avg loss: -0.004485316244226272		[learning rate: 0.00013639]
	Learning Rate: 0.000136392
	LOSS [training: -0.0028019065053644515 | validation: -0.0046069883997826196]
	TIME [epoch: 8.19 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00221136185552137		[learning rate: 0.00013623]
		[batch 20/20] avg loss: 0.0015925817188302805		[learning rate: 0.00013606]
	Learning Rate: 0.000136062
	LOSS [training: -0.00030939006834554503 | validation: -0.0008634258988743349]
	TIME [epoch: 8.16 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005613841963566303		[learning rate: 0.0001359]
		[batch 20/20] avg loss: -0.004094550603975047		[learning rate: 0.00013573]
	Learning Rate: 0.000135733
	LOSS [training: 0.0007596456797956284 | validation: -2.6276045845918306e-05]
	TIME [epoch: 8.18 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006696476378181999		[learning rate: 0.00013557]
		[batch 20/20] avg loss: -0.004381433271536396		[learning rate: 0.0001354]
	Learning Rate: 0.000135404
	LOSS [training: 0.0011575215533228015 | validation: -0.002068876888126471]
	TIME [epoch: 8.18 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004230199340165037		[learning rate: 0.00013524]
		[batch 20/20] avg loss: -0.001120633655026754		[learning rate: 0.00013508]
	Learning Rate: 0.000135076
	LOSS [training: 0.0015547828425691412 | validation: -0.002946061391162144]
	TIME [epoch: 8.19 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013437755539104004		[learning rate: 0.00013491]
		[batch 20/20] avg loss: -0.008923803685654036		[learning rate: 0.00013475]
	Learning Rate: 0.000134749
	LOSS [training: -0.0037900140658718175 | validation: -0.0032119016396580853]
	TIME [epoch: 8.17 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009017675560784435		[learning rate: 0.00013459]
		[batch 20/20] avg loss: 0.0022124387587535515		[learning rate: 0.00013442]
	Learning Rate: 0.000134423
	LOSS [training: 0.000655335601337554 | validation: -0.009825848420353417]
	TIME [epoch: 8.2 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038236353085676286		[learning rate: 0.00013426]
		[batch 20/20] avg loss: 0.0038780314272640116		[learning rate: 0.0001341]
	Learning Rate: 0.000134098
	LOSS [training: 2.7198059348191878e-05 | validation: -0.004770809349252117]
	TIME [epoch: 8.17 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009517591765914683		[learning rate: 0.00013394]
		[batch 20/20] avg loss: 0.0008274149679164445		[learning rate: 0.00013377]
	Learning Rate: 0.000133773
	LOSS [training: -6.217210433751207e-05 | validation: -0.003638659609498328]
	TIME [epoch: 8.18 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017065283332288051		[learning rate: 0.00013361]
		[batch 20/20] avg loss: -0.006042827346140152		[learning rate: 0.00013345]
	Learning Rate: 0.000133449
	LOSS [training: -0.002168149506455673 | validation: -0.002799409113202139]
	TIME [epoch: 8.16 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013366187585293044		[learning rate: 0.00013329]
		[batch 20/20] avg loss: -0.003571726707201636		[learning rate: 0.00013313]
	Learning Rate: 0.000133126
	LOSS [training: -0.0024541727328654707 | validation: -0.00474494255686342]
	TIME [epoch: 8.16 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037648082850277034		[learning rate: 0.00013296]
		[batch 20/20] avg loss: -0.00102816577886519		[learning rate: 0.0001328]
	Learning Rate: 0.000132804
	LOSS [training: -0.002396487031946447 | validation: -0.0043297165830916825]
	TIME [epoch: 8.17 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002108134015136611		[learning rate: 0.00013264]
		[batch 20/20] avg loss: -0.0006880056070409394		[learning rate: 0.00013248]
	Learning Rate: 0.000132482
	LOSS [training: 0.000710064204047836 | validation: -0.0015902002405021125]
	TIME [epoch: 8.18 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013263709038507311		[learning rate: 0.00013232]
		[batch 20/20] avg loss: -0.0028611703421416892		[learning rate: 0.00013216]
	Learning Rate: 0.000132162
	LOSS [training: -0.0007673997191454793 | validation: -0.006137973592755827]
	TIME [epoch: 8.16 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032805733367234787		[learning rate: 0.000132]
		[batch 20/20] avg loss: -0.0039488924396506924		[learning rate: 0.00013184]
	Learning Rate: 0.000131842
	LOSS [training: -0.00033415955146360725 | validation: -0.00306486372368929]
	TIME [epoch: 8.17 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002294229084220549		[learning rate: 0.00013168]
		[batch 20/20] avg loss: -0.0004647199071315814		[learning rate: 0.00013152]
	Learning Rate: 0.000131522
	LOSS [training: -0.00011764849935476324 | validation: -0.007816426754710119]
	TIME [epoch: 8.19 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003552502206460819		[learning rate: 0.00013136]
		[batch 20/20] avg loss: 0.002063008797451905		[learning rate: 0.0001312]
	Learning Rate: 0.000131204
	LOSS [training: -0.0007447467045044572 | validation: -0.0012037818462178281]
	TIME [epoch: 8.19 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006256571607788161		[learning rate: 0.00013105]
		[batch 20/20] avg loss: 0.00291454305862839		[learning rate: 0.00013089]
	Learning Rate: 0.000130886
	LOSS [training: 0.0017701001097036027 | validation: -0.005507442243537801]
	TIME [epoch: 8.17 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00033016785496573793		[learning rate: 0.00013073]
		[batch 20/20] avg loss: 0.0012722201118104328		[learning rate: 0.00013057]
	Learning Rate: 0.00013057
	LOSS [training: 0.0004710261284223475 | validation: 0.006886726402720098]
	TIME [epoch: 8.19 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00331528612496447		[learning rate: 0.00013041]
		[batch 20/20] avg loss: -0.003578746698082805		[learning rate: 0.00013025]
	Learning Rate: 0.000130254
	LOSS [training: -0.00013173028655916733 | validation: -0.003733110611539674]
	TIME [epoch: 8.19 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005958082868108376		[learning rate: 0.0001301]
		[batch 20/20] avg loss: 0.002863854306602144		[learning rate: 0.00012994]
	Learning Rate: 0.000129938
	LOSS [training: 0.0017298312967064908 | validation: 0.0006772128538918721]
	TIME [epoch: 8.18 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00039343722783843194		[learning rate: 0.00012978]
		[batch 20/20] avg loss: -0.00038181224567727744		[learning rate: 0.00012962]
	Learning Rate: 0.000129624
	LOSS [training: -0.00038762473675785474 | validation: -0.0084458859643806]
	TIME [epoch: 8.16 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031295631389581316		[learning rate: 0.00012947]
		[batch 20/20] avg loss: -0.003376315519033212		[learning rate: 0.00012931]
	Learning Rate: 0.00012931
	LOSS [training: -0.00012337619003754009 | validation: -0.009228140229009326]
	TIME [epoch: 8.16 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029227948094022434		[learning rate: 0.00012915]
		[batch 20/20] avg loss: -0.003438229995810251		[learning rate: 0.000129]
	Learning Rate: 0.000128997
	LOSS [training: -0.003180512402606247 | validation: -0.006578335263512101]
	TIME [epoch: 8.16 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00036410039786224306		[learning rate: 0.00012884]
		[batch 20/20] avg loss: -0.0035059762710054946		[learning rate: 0.00012868]
	Learning Rate: 0.000128685
	LOSS [training: -0.0019350383344338689 | validation: -0.005917600248640828]
	TIME [epoch: 8.18 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00025231990392282286		[learning rate: 0.00012853]
		[batch 20/20] avg loss: -0.0004351368784263138		[learning rate: 0.00012837]
	Learning Rate: 0.000128373
	LOSS [training: -9.140848725174542e-05 | validation: -0.009313227011732284]
	TIME [epoch: 8.16 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015743537975122072		[learning rate: 0.00012822]
		[batch 20/20] avg loss: -0.0019132620710174907		[learning rate: 0.00012806]
	Learning Rate: 0.000128062
	LOSS [training: -0.0001694541367526419 | validation: -0.0013202736737810568]
	TIME [epoch: 8.16 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001072282619340354		[learning rate: 0.00012791]
		[batch 20/20] avg loss: 0.0024399157731975463		[learning rate: 0.00012775]
	Learning Rate: 0.000127752
	LOSS [training: 0.0006838165769285959 | validation: -0.002483820267361257]
	TIME [epoch: 8.16 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00018298348166334715		[learning rate: 0.0001276]
		[batch 20/20] avg loss: -0.0007427165700386702		[learning rate: 0.00012744]
	Learning Rate: 0.000127443
	LOSS [training: -0.0004628500258510086 | validation: -0.0029890179952015237]
	TIME [epoch: 8.2 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016706059191728604		[learning rate: 0.00012729]
		[batch 20/20] avg loss: -0.00012581643557778204		[learning rate: 0.00012713]
	Learning Rate: 0.000127134
	LOSS [training: 0.0007723947417975392 | validation: -0.002729877200301874]
	TIME [epoch: 8.18 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036768714484776976		[learning rate: 0.00012698]
		[batch 20/20] avg loss: 0.002101226260439383		[learning rate: 0.00012683]
	Learning Rate: 0.000126827
	LOSS [training: -0.0007878225940191573 | validation: -0.003418551168412319]
	TIME [epoch: 8.16 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001073943045256064		[learning rate: 0.00012667]
		[batch 20/20] avg loss: 0.0015562263127737806		[learning rate: 0.00012652]
	Learning Rate: 0.00012652
	LOSS [training: 0.00024114163375885825 | validation: 0.0034348184874561515]
	TIME [epoch: 8.17 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000994824817783038		[learning rate: 0.00012637]
		[batch 20/20] avg loss: -0.0017522479264925543		[learning rate: 0.00012621]
	Learning Rate: 0.000126213
	LOSS [training: -0.0003787115543547584 | validation: -0.006249019189644589]
	TIME [epoch: 8.21 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007282044081391016		[learning rate: 0.00012606]
		[batch 20/20] avg loss: 0.0002385221158035314		[learning rate: 0.00012591]
	Learning Rate: 0.000125908
	LOSS [training: -0.0002448411461677854 | validation: -0.0011782638899698029]
	TIME [epoch: 8.18 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021643714238739767		[learning rate: 0.00012576]
		[batch 20/20] avg loss: -0.0013365076564585364		[learning rate: 0.0001256]
	Learning Rate: 0.000125603
	LOSS [training: -0.0017504395401662567 | validation: -0.00841393946853989]
	TIME [epoch: 8.16 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005823466985357407		[learning rate: 0.00012545]
		[batch 20/20] avg loss: -0.005029906327691286		[learning rate: 0.0001253]
	Learning Rate: 0.000125299
	LOSS [training: -0.002223779814577773 | validation: -0.008080049971274934]
	TIME [epoch: 8.16 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003490771371054377		[learning rate: 0.00012515]
		[batch 20/20] avg loss: -0.004153312092929846		[learning rate: 0.000125]
	Learning Rate: 0.000124996
	LOSS [training: -0.002251194615017642 | validation: -0.003740664018178582]
	TIME [epoch: 8.18 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00035160948290139835		[learning rate: 0.00012484]
		[batch 20/20] avg loss: -0.004342606507080686		[learning rate: 0.00012469]
	Learning Rate: 0.000124693
	LOSS [training: -0.0019954985120896438 | validation: -0.004965549930626597]
	TIME [epoch: 8.16 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025435096097045635		[learning rate: 0.00012454]
		[batch 20/20] avg loss: 0.0010998494990093989		[learning rate: 0.00012439]
	Learning Rate: 0.000124391
	LOSS [training: -0.0007218300553475823 | validation: -0.001222733957092827]
	TIME [epoch: 8.16 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026482353221650642		[learning rate: 0.00012424]
		[batch 20/20] avg loss: -0.00145940133450329		[learning rate: 0.00012409]
	Learning Rate: 0.00012409
	LOSS [training: 0.0005944169938308874 | validation: -0.0009306968777767439]
	TIME [epoch: 8.16 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028177519962862716		[learning rate: 0.00012394]
		[batch 20/20] avg loss: 0.0006318792126553631		[learning rate: 0.00012379]
	Learning Rate: 0.00012379
	LOSS [training: -0.0010929363918154547 | validation: -0.006534822746805394]
	TIME [epoch: 8.18 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023348187679680553		[learning rate: 0.00012364]
		[batch 20/20] avg loss: 0.00037623493673088516		[learning rate: 0.00012349]
	Learning Rate: 0.00012349
	LOSS [training: 0.0013555268523494698 | validation: -0.002101818669591415]
	TIME [epoch: 8.17 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027632563073109856		[learning rate: 0.00012334]
		[batch 20/20] avg loss: -0.00022314093476508457		[learning rate: 0.00012319]
	Learning Rate: 0.000123191
	LOSS [training: 0.0012700576862729508 | validation: 0.002472400351000044]
	TIME [epoch: 8.19 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002176899727499621		[learning rate: 0.00012304]
		[batch 20/20] avg loss: 0.004325970544132271		[learning rate: 0.00012289]
	Learning Rate: 0.000122893
	LOSS [training: 0.0032514351358159457 | validation: 0.004409130216558573]
	TIME [epoch: 8.17 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021840765288176156		[learning rate: 0.00012274]
		[batch 20/20] avg loss: 0.004432117333843566		[learning rate: 0.0001226]
	Learning Rate: 0.000122595
	LOSS [training: 0.0011240204025129753 | validation: 0.0018468698116848464]
	TIME [epoch: 8.18 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020295257678344426		[learning rate: 0.00012245]
		[batch 20/20] avg loss: 0.00579089987515748		[learning rate: 0.0001223]
	Learning Rate: 0.000122298
	LOSS [training: 0.001880687053661519 | validation: 0.007533029960185311]
	TIME [epoch: 8.17 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004643360466106698		[learning rate: 0.00012215]
		[batch 20/20] avg loss: -0.0009485874841723224		[learning rate: 0.000122]
	Learning Rate: 0.000122002
	LOSS [training: 0.0018473864909671887 | validation: -0.0030874924477912722]
	TIME [epoch: 8.2 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008839115832147163		[learning rate: 0.00012185]
		[batch 20/20] avg loss: 0.005082711235419599		[learning rate: 0.00012171]
	Learning Rate: 0.000121707
	LOSS [training: 0.0020993998261024417 | validation: 0.00480505378882835]
	TIME [epoch: 8.16 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004562333654421026		[learning rate: 0.00012156]
		[batch 20/20] avg loss: -0.00034237643542239175		[learning rate: 0.00012141]
	Learning Rate: 0.000121412
	LOSS [training: -0.0024523550449217085 | validation: 0.0030608917986464567]
	TIME [epoch: 8.18 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004108321938805445		[learning rate: 0.00012127]
		[batch 20/20] avg loss: 0.0009903139080245046		[learning rate: 0.00012112]
	Learning Rate: 0.000121119
	LOSS [training: 0.0025493179234149744 | validation: -0.008306442433970223]
	TIME [epoch: 8.16 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002332589273127174		[learning rate: 0.00012097]
		[batch 20/20] avg loss: 0.0013663255700796192		[learning rate: 0.00012083]
	Learning Rate: 0.000120825
	LOSS [training: -0.00048313185152377747 | validation: -0.002819039404364372]
	TIME [epoch: 8.15 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009636366268826316		[learning rate: 0.00012068]
		[batch 20/20] avg loss: -0.001494816178213947		[learning rate: 0.00012053]
	Learning Rate: 0.000120533
	LOSS [training: -0.0012292264025482893 | validation: -0.002782452562109011]
	TIME [epoch: 8.16 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006497235655455844		[learning rate: 0.00012039]
		[batch 20/20] avg loss: -0.0008893978173313746		[learning rate: 0.00012024]
	Learning Rate: 0.000120241
	LOSS [training: -0.00011983712589289509 | validation: -0.0036944218824985038]
	TIME [epoch: 8.18 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: -1.178527605564144e-05		[learning rate: 0.0001201]
		[batch 20/20] avg loss: -0.0017334795898523321		[learning rate: 0.00011995]
	Learning Rate: 0.00011995
	LOSS [training: -0.0008726324329539866 | validation: -0.0034510495534083104]
	TIME [epoch: 8.17 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028359464323205455		[learning rate: 0.0001198]
		[batch 20/20] avg loss: 0.0001911221390435686		[learning rate: 0.00011966]
	Learning Rate: 0.00011966
	LOSS [training: -0.0013224121466384885 | validation: -0.001746470511894617]
	TIME [epoch: 8.18 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017152722420406834		[learning rate: 0.00011951]
		[batch 20/20] avg loss: 0.0027454889787405664		[learning rate: 0.00011937]
	Learning Rate: 0.00011937
	LOSS [training: 0.0005151083683499416 | validation: -0.004586524372004012]
	TIME [epoch: 8.17 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003201117271535739		[learning rate: 0.00011923]
		[batch 20/20] avg loss: -0.0024684819268172694		[learning rate: 0.00011908]
	Learning Rate: 0.000119081
	LOSS [training: 0.0003663176723592347 | validation: -0.0038362920391208323]
	TIME [epoch: 8.19 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005915684475381397		[learning rate: 0.00011894]
		[batch 20/20] avg loss: -0.003288857285846193		[learning rate: 0.00011879]
	Learning Rate: 0.000118793
	LOSS [training: -0.004602270880613794 | validation: -0.0015660619220350476]
	TIME [epoch: 8.18 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018668920154105324		[learning rate: 0.00011865]
		[batch 20/20] avg loss: -0.0016155952773439554		[learning rate: 0.00011851]
	Learning Rate: 0.000118505
	LOSS [training: -0.001741243646377244 | validation: -0.006592461808719343]
	TIME [epoch: 8.18 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030976144447548404		[learning rate: 0.00011836]
		[batch 20/20] avg loss: 0.0025208477584414478		[learning rate: 0.00011822]
	Learning Rate: 0.000118218
	LOSS [training: -0.00028838334315669617 | validation: -0.0018392268801061095]
	TIME [epoch: 8.16 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014534049317090647		[learning rate: 0.00011808]
		[batch 20/20] avg loss: -0.0016503227055604704		[learning rate: 0.00011793]
	Learning Rate: 0.000117932
	LOSS [training: -0.0015518638186347677 | validation: -0.007372042752982067]
	TIME [epoch: 8.18 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008420674341543278		[learning rate: 0.00011779]
		[batch 20/20] avg loss: -0.0047414538971396756		[learning rate: 0.00011765]
	Learning Rate: 0.000117646
	LOSS [training: -0.002791760665647002 | validation: -0.006728631984998625]
	TIME [epoch: 8.16 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003262689631014817		[learning rate: 0.0001175]
		[batch 20/20] avg loss: -0.0014441367617733828		[learning rate: 0.00011736]
	Learning Rate: 0.000117362
	LOSS [training: 0.0009092764346207167 | validation: -0.0064876415464189905]
	TIME [epoch: 8.16 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003824850418201027		[learning rate: 0.00011722]
		[batch 20/20] avg loss: -0.0012441879267832872		[learning rate: 0.00011708]
	Learning Rate: 0.000117078
	LOSS [training: -0.0025345191724921575 | validation: -0.005018799368354039]
	TIME [epoch: 8.16 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.677767377166884e-05		[learning rate: 0.00011694]
		[batch 20/20] avg loss: 0.005296378413107049		[learning rate: 0.00011679]
	Learning Rate: 0.000116794
	LOSS [training: 0.0026865780434393582 | validation: -0.005651658392953282]
	TIME [epoch: 8.19 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025318052925005986		[learning rate: 0.00011665]
		[batch 20/20] avg loss: -0.0048448938080202		[learning rate: 0.00011651]
	Learning Rate: 0.000116511
	LOSS [training: -0.0011565442577597999 | validation: -0.005323258439601675]
	TIME [epoch: 8.19 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005351523750382207		[learning rate: 0.00011637]
		[batch 20/20] avg loss: 0.002935893509727152		[learning rate: 0.00011623]
	Learning Rate: 0.000116229
	LOSS [training: 0.0012003705673444658 | validation: 0.00010473768107881155]
	TIME [epoch: 8.16 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004633091211925214		[learning rate: 0.00011609]
		[batch 20/20] avg loss: -0.006183000360724548		[learning rate: 0.00011595]
	Learning Rate: 0.000115948
	LOSS [training: -0.0007749545743996675 | validation: -0.00907806731798946]
	TIME [epoch: 8.17 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012640931429817681		[learning rate: 0.00011581]
		[batch 20/20] avg loss: -0.001169614947498043		[learning rate: 0.00011567]
	Learning Rate: 0.000115667
	LOSS [training: -0.0012168540452399059 | validation: 0.003661263209309478]
	TIME [epoch: 8.2 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018324289149229032		[learning rate: 0.00011553]
		[batch 20/20] avg loss: 0.0020915419917042874		[learning rate: 0.00011539]
	Learning Rate: 0.000115387
	LOSS [training: 0.00012955653839069196 | validation: -0.0017499740231996038]
	TIME [epoch: 8.19 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005497860031442373		[learning rate: 0.00011525]
		[batch 20/20] avg loss: 0.0010946291600063911		[learning rate: 0.00011511]
	Learning Rate: 0.000115108
	LOSS [training: -0.002201615435717991 | validation: -0.00679794790372103]
	TIME [epoch: 8.16 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005800802136494799		[learning rate: 0.00011497]
		[batch 20/20] avg loss: -0.000167666357015651		[learning rate: 0.00011483]
	Learning Rate: 0.000114829
	LOSS [training: -0.0029842342467552244 | validation: 0.0013281002467186017]
	TIME [epoch: 8.16 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007713478486150805		[learning rate: 0.00011469]
		[batch 20/20] avg loss: -0.0020281373033136287		[learning rate: 0.00011455]
	Learning Rate: 0.000114551
	LOSS [training: -0.0006283947273492743 | validation: -0.003935449665863899]
	TIME [epoch: 8.17 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003017422697881013		[learning rate: 0.00011441]
		[batch 20/20] avg loss: -0.0030738010362254464		[learning rate: 0.00011427]
	Learning Rate: 0.000114274
	LOSS [training: -2.8189169172216536e-05 | validation: -0.0023066475553488834]
	TIME [epoch: 8.17 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00026344085406663955		[learning rate: 0.00011414]
		[batch 20/20] avg loss: -0.0014959241641817154		[learning rate: 0.000114]
	Learning Rate: 0.000113997
	LOSS [training: -0.000616241655057538 | validation: -0.0037879646673058386]
	TIME [epoch: 8.16 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006993125383096181		[learning rate: 0.00011386]
		[batch 20/20] avg loss: 0.0034079269600539933		[learning rate: 0.00011372]
	Learning Rate: 0.000113721
	LOSS [training: 0.0020536197491818056 | validation: 0.0027485497011545405]
	TIME [epoch: 8.18 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00022047521915094097		[learning rate: 0.00011358]
		[batch 20/20] avg loss: -7.032909783736408e-05		[learning rate: 0.00011345]
	Learning Rate: 0.000113446
	LOSS [training: 7.507306065678845e-05 | validation: 0.0005093086677678922]
	TIME [epoch: 8.2 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009226087449665267		[learning rate: 0.00011331]
		[batch 20/20] avg loss: 0.004344897220064464		[learning rate: 0.00011317]
	Learning Rate: 0.000113171
	LOSS [training: 0.0017111442375489685 | validation: 0.0002024941289151148]
	TIME [epoch: 8.18 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00024200246814184683		[learning rate: 0.00011303]
		[batch 20/20] avg loss: 0.001570259294128492		[learning rate: 0.0001129]
	Learning Rate: 0.000112897
	LOSS [training: 0.0006641284129933225 | validation: 0.00159738458949977]
	TIME [epoch: 8.17 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004322523862949369		[learning rate: 0.00011276]
		[batch 20/20] avg loss: -0.0038879960546012128		[learning rate: 0.00011262]
	Learning Rate: 0.000112624
	LOSS [training: 0.0002172639041740783 | validation: 0.0011178927129310151]
	TIME [epoch: 8.18 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003306626258799907		[learning rate: 0.00011249]
		[batch 20/20] avg loss: -0.0015659899419601334		[learning rate: 0.00011235]
	Learning Rate: 0.000112352
	LOSS [training: -0.0006176636580400715 | validation: -0.003958246055353019]
	TIME [epoch: 8.18 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006055078593385875		[learning rate: 0.00011222]
		[batch 20/20] avg loss: -0.0010967650332939422		[learning rate: 0.00011208]
	Learning Rate: 0.00011208
	LOSS [training: 0.002479156780045966 | validation: -0.0030305351037219507]
	TIME [epoch: 8.18 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: -9.577956232456036e-05		[learning rate: 0.00011194]
		[batch 20/20] avg loss: 0.0024746221805176808		[learning rate: 0.00011181]
	Learning Rate: 0.000111808
	LOSS [training: 0.00118942130909656 | validation: -0.0017523364765151654]
	TIME [epoch: 8.16 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017831155899068105		[learning rate: 0.00011167]
		[batch 20/20] avg loss: -0.00220222755481509		[learning rate: 0.00011154]
	Learning Rate: 0.000111538
	LOSS [training: -0.00020955598245413939 | validation: -0.003090582014767133]
	TIME [epoch: 8.16 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015521011784267941		[learning rate: 0.0001114]
		[batch 20/20] avg loss: -0.0023144276073887543		[learning rate: 0.00011127]
	Learning Rate: 0.000111268
	LOSS [training: -0.0003811632144809802 | validation: -0.0029760444674975842]
	TIME [epoch: 8.17 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021050751896090586		[learning rate: 0.00011113]
		[batch 20/20] avg loss: -0.00664963096765213		[learning rate: 0.000111]
	Learning Rate: 0.000110998
	LOSS [training: -0.004377353078630594 | validation: -0.001717906719679448]
	TIME [epoch: 8.18 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003541953059664475		[learning rate: 0.00011086]
		[batch 20/20] avg loss: -0.004344833459464316		[learning rate: 0.00011073]
	Learning Rate: 0.000110729
	LOSS [training: -0.003943393259564396 | validation: -0.004900941418723241]
	TIME [epoch: 8.16 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003415595162297484		[learning rate: 0.0001106]
		[batch 20/20] avg loss: -0.0014672577885424631		[learning rate: 0.00011046]
	Learning Rate: 0.000110461
	LOSS [training: -0.0024414264754199734 | validation: -0.0034800036212205045]
	TIME [epoch: 8.16 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036917496065641724		[learning rate: 0.00011033]
		[batch 20/20] avg loss: -0.0010122227631619001		[learning rate: 0.00011019]
	Learning Rate: 0.000110194
	LOSS [training: -0.002351986184863036 | validation: -0.010205272252665116]
	TIME [epoch: 8.2 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026202907560750133		[learning rate: 0.00011006]
		[batch 20/20] avg loss: 0.001539133260959519		[learning rate: 0.00010993]
	Learning Rate: 0.000109927
	LOSS [training: -0.0005405787475577468 | validation: -0.0016613742450190224]
	TIME [epoch: 8.2 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003660878601974723		[learning rate: 0.00010979]
		[batch 20/20] avg loss: -0.0016110713343202806		[learning rate: 0.00010966]
	Learning Rate: 0.000109661
	LOSS [training: 0.0010249036338272211 | validation: -0.005448906925918175]
	TIME [epoch: 8.17 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003490579003294989		[learning rate: 0.00010953]
		[batch 20/20] avg loss: -0.0002466242059450849		[learning rate: 0.0001094]
	Learning Rate: 0.000109396
	LOSS [training: -0.00029784105313729206 | validation: -0.0014759722570898293]
	TIME [epoch: 8.17 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023086611957868547		[learning rate: 0.00010926]
		[batch 20/20] avg loss: -0.0022771927890267154		[learning rate: 0.00010913]
	Learning Rate: 0.000109131
	LOSS [training: 1.573420338006968e-05 | validation: -3.72123309069427e-05]
	TIME [epoch: 8.2 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012708177428703616		[learning rate: 0.000109]
		[batch 20/20] avg loss: 0.0028482577844892664		[learning rate: 0.00010887]
	Learning Rate: 0.000108867
	LOSS [training: 0.0007887200208094525 | validation: -0.004530920054851774]
	TIME [epoch: 8.18 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030980158261208826		[learning rate: 0.00010873]
		[batch 20/20] avg loss: -0.0023733687027967646		[learning rate: 0.0001086]
	Learning Rate: 0.000108603
	LOSS [training: 0.0003623235616620591 | validation: -0.0017239660294510702]
	TIME [epoch: 8.16 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006709508585357774		[learning rate: 0.00010847]
		[batch 20/20] avg loss: 0.002931602143111671		[learning rate: 0.00010834]
	Learning Rate: 0.00010834
	LOSS [training: -0.0018889532211230514 | validation: -0.0021629165405227368]
	TIME [epoch: 8.16 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022461483913017266		[learning rate: 0.00010821]
		[batch 20/20] avg loss: -0.0006195962324218346		[learning rate: 0.00010808]
	Learning Rate: 0.000108078
	LOSS [training: 0.0008132760794399459 | validation: -0.004396154078175734]
	TIME [epoch: 8.16 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014209326186212584		[learning rate: 0.00010795]
		[batch 20/20] avg loss: -0.00029959027339315737		[learning rate: 0.00010782]
	Learning Rate: 0.000107816
	LOSS [training: 0.0005606711726140504 | validation: -0.00019131681839853364]
	TIME [epoch: 8.18 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00045017280876665075		[learning rate: 0.00010769]
		[batch 20/20] avg loss: -0.0007868926365636525		[learning rate: 0.00010756]
	Learning Rate: 0.000107555
	LOSS [training: -0.0006185327226651514 | validation: -0.0031113054983040785]
	TIME [epoch: 8.17 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036671378621704774		[learning rate: 0.00010742]
		[batch 20/20] avg loss: -0.003825386393494421		[learning rate: 0.00010729]
	Learning Rate: 0.000107295
	LOSS [training: -7.912426566197142e-05 | validation: -0.0022606137401995565]
	TIME [epoch: 8.16 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011568642521934203		[learning rate: 0.00010716]
		[batch 20/20] avg loss: 0.0038843082394592527		[learning rate: 0.00010704]
	Learning Rate: 0.000107035
	LOSS [training: -0.0038421671412374745 | validation: -0.004895401101414793]
	TIME [epoch: 8.18 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001925242750736354		[learning rate: 0.00010691]
		[batch 20/20] avg loss: -0.0008985229030963309		[learning rate: 0.00010678]
	Learning Rate: 0.000106776
	LOSS [training: -0.0014118828269163424 | validation: -0.0011073845775274485]
	TIME [epoch: 8.2 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006586355563970839		[learning rate: 0.00010665]
		[batch 20/20] avg loss: 0.0016794931084301342		[learning rate: 0.00010652]
	Learning Rate: 0.000106518
	LOSS [training: -0.0024534312277703523 | validation: -0.004966991082136569]
	TIME [epoch: 8.17 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002014509871982477		[learning rate: 0.00010639]
		[batch 20/20] avg loss: -0.0025438126604187803		[learning rate: 0.00010626]
	Learning Rate: 0.00010626
	LOSS [training: -0.001372631823808514 | validation: -0.0067637238070804]
	TIME [epoch: 8.17 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001364530582775865		[learning rate: 0.00010613]
		[batch 20/20] avg loss: 0.004504825810588409		[learning rate: 0.000106]
	Learning Rate: 0.000106002
	LOSS [training: 0.0015701476139062722 | validation: -0.0019158552226545974]
	TIME [epoch: 8.2 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004541171723489305		[learning rate: 0.00010587]
		[batch 20/20] avg loss: 0.0043850788849859965		[learning rate: 0.00010575]
	Learning Rate: 0.000105746
	LOSS [training: 0.0024195980286674636 | validation: -0.0010760685131780794]
	TIME [epoch: 8.18 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010334556607549744		[learning rate: 0.00010562]
		[batch 20/20] avg loss: 0.00149739755744033		[learning rate: 0.00010549]
	Learning Rate: 0.00010549
	LOSS [training: 0.001265426609097652 | validation: 0.0037663560579708094]
	TIME [epoch: 8.17 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028447157327494534		[learning rate: 0.00010536]
		[batch 20/20] avg loss: -0.0006076203656155524		[learning rate: 0.00010523]
	Learning Rate: 0.000105234
	LOSS [training: -0.0017261680491825033 | validation: -0.0006731764533887445]
	TIME [epoch: 8.16 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001708791717168293		[learning rate: 0.00010511]
		[batch 20/20] avg loss: -0.0007295904357131104		[learning rate: 0.00010498]
	Learning Rate: 0.00010498
	LOSS [training: 0.0004896006407275913 | validation: -0.006637440095089952]
	TIME [epoch: 8.15 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005969928932909407		[learning rate: 0.00010485]
		[batch 20/20] avg loss: 0.005671177276613125		[learning rate: 0.00010473]
	Learning Rate: 0.000104726
	LOSS [training: -0.00014937582814814133 | validation: -0.0005314876464426028]
	TIME [epoch: 8.18 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002312461761133091		[learning rate: 0.0001046]
		[batch 20/20] avg loss: 0.001601575441641192		[learning rate: 0.00010447]
	Learning Rate: 0.000104472
	LOSS [training: -0.0003554431597459497 | validation: 0.002535193700818712]
	TIME [epoch: 8.16 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012428932044572928		[learning rate: 0.00010435]
		[batch 20/20] avg loss: 0.0010018099665281906		[learning rate: 0.00010422]
	Learning Rate: 0.000104219
	LOSS [training: -0.00012054161896455124 | validation: 0.0029531619855032182]
	TIME [epoch: 8.18 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021498689173613445		[learning rate: 0.00010409]
		[batch 20/20] avg loss: -0.004416286711028143		[learning rate: 0.00010397]
	Learning Rate: 0.000103967
	LOSS [training: -0.001133208896833399 | validation: 0.0035314538218288206]
	TIME [epoch: 8.17 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004255467732198208		[learning rate: 0.00010384]
		[batch 20/20] avg loss: 0.001263681623961613		[learning rate: 0.00010372]
	Learning Rate: 0.000103715
	LOSS [training: 0.0008446141985907166 | validation: -0.004347542139804764]
	TIME [epoch: 8.19 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010294376465983896		[learning rate: 0.00010359]
		[batch 20/20] avg loss: -4.337485080319549e-05		[learning rate: 0.00010346]
	Learning Rate: 0.000103464
	LOSS [training: -0.0005364062487007925 | validation: -0.0010345040148950135]
	TIME [epoch: 8.17 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024433484641492		[learning rate: 0.00010334]
		[batch 20/20] avg loss: -0.0024083639236675675		[learning rate: 0.00010321]
	Learning Rate: 0.000103214
	LOSS [training: -0.002425856193908384 | validation: 0.0007024870157439382]
	TIME [epoch: 8.2 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002083923275388302		[learning rate: 0.00010309]
		[batch 20/20] avg loss: -0.0031946238426475487		[learning rate: 0.00010296]
	Learning Rate: 0.000102964
	LOSS [training: -0.0014931157575543592 | validation: -0.001535497929838824]
	TIME [epoch: 8.17 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004677440800952871		[learning rate: 0.00010284]
		[batch 20/20] avg loss: 0.00463779876488456		[learning rate: 0.00010271]
	Learning Rate: 0.000102714
	LOSS [training: -1.9821018034155666e-05 | validation: -0.0020378748153259785]
	TIME [epoch: 8.19 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029136365167715256		[learning rate: 0.00010259]
		[batch 20/20] avg loss: -0.00482963967158335		[learning rate: 0.00010247]
	Learning Rate: 0.000102466
	LOSS [training: -0.003871638094177438 | validation: -0.0017332293358413979]
	TIME [epoch: 8.17 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009424934154558428		[learning rate: 0.00010234]
		[batch 20/20] avg loss: -0.0024584578831285874		[learning rate: 0.00010222]
	Learning Rate: 0.000102218
	LOSS [training: -0.001700475649292215 | validation: -0.0004870296074203969]
	TIME [epoch: 8.16 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004486384155264721		[learning rate: 0.00010209]
		[batch 20/20] avg loss: -7.302470305358671e-05		[learning rate: 0.00010197]
	Learning Rate: 0.00010197
	LOSS [training: -0.002279704429159154 | validation: -0.003959108202259702]
	TIME [epoch: 8.17 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009415868271467825		[learning rate: 0.00010185]
		[batch 20/20] avg loss: 0.0024738369191125336		[learning rate: 0.00010172]
	Learning Rate: 0.000101723
	LOSS [training: 0.0007661250459828755 | validation: -0.0023248896559667402]
	TIME [epoch: 8.19 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002726266561290743		[learning rate: 0.0001016]
		[batch 20/20] avg loss: -0.005083663285082555		[learning rate: 0.00010148]
	Learning Rate: 0.000101477
	LOSS [training: -0.002678144970605815 | validation: -0.009699332181907409]
	TIME [epoch: 8.17 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00032479558211793573		[learning rate: 0.00010135]
		[batch 20/20] avg loss: -0.00157329193216773		[learning rate: 0.00010123]
	Learning Rate: 0.000101232
	LOSS [training: -0.0009490437571428329 | validation: -0.0013813403644594904]
	TIME [epoch: 8.17 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025044526930289696		[learning rate: 0.00010111]
		[batch 20/20] avg loss: 0.006411211990793457		[learning rate: 0.00010099]
	Learning Rate: 0.000100986
	LOSS [training: 0.001953379648882244 | validation: -0.00217353755289227]
	TIME [epoch: 8.19 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003707806986864069		[learning rate: 0.00010086]
		[batch 20/20] avg loss: 0.002396508628935999		[learning rate: 0.00010074]
	Learning Rate: 0.000100742
	LOSS [training: 0.0010128639651247964 | validation: -0.0024606056407859085]
	TIME [epoch: 8.19 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033260701608079273		[learning rate: 0.00010062]
		[batch 20/20] avg loss: -0.00335779780251964		[learning rate: 0.0001005]
	Learning Rate: 0.000100498
	LOSS [training: -1.5863820855856842e-05 | validation: 0.0001287536553422926]
	TIME [epoch: 8.19 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005736499448767884		[learning rate: 0.00010038]
		[batch 20/20] avg loss: 0.002373142089510867		[learning rate: 0.00010025]
	Learning Rate: 0.000100255
	LOSS [training: -0.0016816786796285082 | validation: 0.002818244692842261]
	TIME [epoch: 8.19 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00025970665882931474		[learning rate: 0.00010013]
		[batch 20/20] avg loss: -0.003106602755110893		[learning rate: 0.00010001]
	Learning Rate: 0.000100012
	LOSS [training: -0.0014234480481407894 | validation: -0.006946082459287512]
	TIME [epoch: 8.21 sec]
Finished training in 16497.142 seconds.
