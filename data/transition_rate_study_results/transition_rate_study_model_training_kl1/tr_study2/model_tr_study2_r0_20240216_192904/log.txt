Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r0', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3597856974

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.847072574136734		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.645887686997064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.746480130566899 | validation: 4.359135986774637]
	TIME [epoch: 48.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8969628491286854		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2631933988272857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5800781239779864 | validation: 4.090708979487298]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.693087746554318		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8435295718669464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.268308659210633 | validation: 1.8373185650421344]
	TIME [epoch: 9.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8392735515628575		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1994194638016884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0193465076822728 | validation: 2.1763213965012476]
	TIME [epoch: 8.86 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.900865385568234		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.568701704462934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7347835450155837 | validation: 1.6785249665663828]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5748448984628005		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.429816481963724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5023306902132623 | validation: 1.6847768945057353]
	TIME [epoch: 8.83 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5329552945271385		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2202641797792213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3766097371531796 | validation: 1.3624374637183478]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1581293437830817		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2253442811303412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1917368124567116 | validation: 0.8567838397797217]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9701555586231999		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7110599878513837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8406077732372917 | validation: 1.6459041677204995]
	TIME [epoch: 8.88 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9844681133954423		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6460676647072567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8152678890513494 | validation: 0.6677286927371671]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6166332458231423		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6007874201777164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6087103330004294 | validation: 0.8030827044832177]
	TIME [epoch: 8.88 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6533761495906562		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.565343272635274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6093597111129649 | validation: 0.9841258140015131]
	TIME [epoch: 8.89 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.642177749862343		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47738132528975274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5597795375760478 | validation: 1.1149813034939795]
	TIME [epoch: 8.87 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6171724825777378		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5878586707342599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6025155766559989 | validation: 0.42873149101792507]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5296974049852208		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.499615980354075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5146566926696479 | validation: 1.0594507743120063]
	TIME [epoch: 8.85 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5690487683360964		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49061159361200407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5298301809740503 | validation: 0.6151851031234106]
	TIME [epoch: 8.85 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.561707236686736		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5531481688443003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5574277027655181 | validation: 0.5124731942828726]
	TIME [epoch: 8.87 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4846381176648177		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5865449115338813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5355915145993497 | validation: 0.5797471777098995]
	TIME [epoch: 8.85 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49929317471174733		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5464727449926088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.522882959852178 | validation: 0.37001859297588247]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41112207603186046		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5794337562300151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49527791613093786 | validation: 0.49246343221392996]
	TIME [epoch: 8.84 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48093854559041266		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.516727010755827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4988327781731197 | validation: 0.46580436889500865]
	TIME [epoch: 8.86 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6051181086163435		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5131820441210515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5591500763686974 | validation: 0.38251828681481576]
	TIME [epoch: 8.86 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.483669113214216		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5752157816278248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5294424474210203 | validation: 0.4146460809673188]
	TIME [epoch: 8.84 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.474561374602103		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4901547308628672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.482358052732485 | validation: 0.3072659437837323]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7900534433233928		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5008649885400962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6454592159317445 | validation: 0.3634165806630078]
	TIME [epoch: 8.85 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41156221465317255		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47215930893758334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4418607617953779 | validation: 0.37097596187351556]
	TIME [epoch: 8.87 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4340518254589494		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5066295659725277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47034069571573867 | validation: 0.5096824578272066]
	TIME [epoch: 8.85 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4439227879860317		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4238127291408572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43386775856344445 | validation: 0.3652303249685092]
	TIME [epoch: 8.84 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4274434414290044		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40970107844528725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41857225993714586 | validation: 0.3155097487699824]
	TIME [epoch: 8.85 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5098759029771187		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4857002096613699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4977880563192443 | validation: 0.9311659387251281]
	TIME [epoch: 8.85 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43267940069008404		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5042813690733311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4684803848817075 | validation: 0.3795978406281237]
	TIME [epoch: 8.87 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4337878607157456		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5646300393831077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49920895004942656 | validation: 0.3735798475540538]
	TIME [epoch: 8.85 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3986016915673979		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3938824121061606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39624205183677924 | validation: 0.7565545767533063]
	TIME [epoch: 8.85 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4172251955736425		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3753775323179924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39630136394581755 | validation: 0.31448785115794803]
	TIME [epoch: 8.85 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4001636544950024		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3566521153903971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37840788494269967 | validation: 0.5434769220538833]
	TIME [epoch: 8.85 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.420910584071009		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3677592446780975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39433491437455326 | validation: 0.27956875278088966]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32270169061208115		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32514144851526827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3239215695636747 | validation: 0.2928133926980863]
	TIME [epoch: 8.85 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3542675940433225		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3376233877660111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34594549090466675 | validation: 0.2768141054382318]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29040693008990903		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3251024049468705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30775466751838976 | validation: 0.3231229992324529]
	TIME [epoch: 8.85 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3620776170995865		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4173236023568829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38970060972823467 | validation: 0.19069770177298714]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6807827276890264		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36835162778651737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5245671777377718 | validation: 0.41584321713641065]
	TIME [epoch: 8.87 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2723509157132928		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24279892688655846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25757492129992565 | validation: 0.23573710252728491]
	TIME [epoch: 8.86 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3725363295914303		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3870409485907119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37978863909107113 | validation: 0.2848326947795755]
	TIME [epoch: 8.86 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29740371607971905		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.31799808306403293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.307700899571876 | validation: 0.18187091210007586]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27609873488337405		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2780609602260064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2770798475546902 | validation: 0.24324782914436313]
	TIME [epoch: 8.88 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3350661656407298		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2452412586305257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2901537121356278 | validation: 0.17708595698094198]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18831635776371117		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9286654369755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5584908973696057 | validation: 3.048214596426048]
	TIME [epoch: 8.86 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8865615189895195		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45424187126330046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1704016951264098 | validation: 0.21834986456317063]
	TIME [epoch: 8.86 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34526769807086766		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32186419816995504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3335659481204113 | validation: 0.30973539033804476]
	TIME [epoch: 8.86 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43487785717389915		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.44227950175982167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43857867946686035 | validation: 0.13583726334467883]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24420214801455847		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2789947396477183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2615984438311384 | validation: 0.3313932778336359]
	TIME [epoch: 8.85 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9730039980670704		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7321432764409517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3525736372540105 | validation: 3.746505733791576]
	TIME [epoch: 8.85 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0941531634067485		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5315352403465514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3128442018766506 | validation: 0.3979208304185892]
	TIME [epoch: 8.85 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38156899712296843		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.172564302824719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2770666499738437 | validation: 0.6479678628041794]
	TIME [epoch: 8.88 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35161735563330615		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23908441273481898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29535088418406247 | validation: 0.256700081594796]
	TIME [epoch: 8.86 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5875424932081146		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.39779460946374795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49266855133593124 | validation: 0.23085284771466635]
	TIME [epoch: 8.86 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4604115754901339		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6663739107288101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5633927431094721 | validation: 0.9643048837647326]
	TIME [epoch: 8.86 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5177697426954501		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.44534797854520736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48155886062032877 | validation: 0.30877679735926944]
	TIME [epoch: 8.85 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42094048719206933		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32945772330166595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3751991052468676 | validation: 0.39952563265447827]
	TIME [epoch: 8.88 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2869972720905546		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2777767063767066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28238698923363054 | validation: 0.184913917536149]
	TIME [epoch: 8.86 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24177158032748447		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40644566724836323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3241086237879239 | validation: 0.6071113032356874]
	TIME [epoch: 8.86 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2830090106889132		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18698846610934272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23499873839912788 | validation: 0.16734588604761919]
	TIME [epoch: 8.86 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1801091169406351		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45525567108784226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3176823940142387 | validation: 0.25567311048220326]
	TIME [epoch: 8.85 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3125314244953542		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16752761734304317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2400295209191987 | validation: 0.10604710397699907]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2615131983204742		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3153290184251781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2884211083728261 | validation: 0.1599759830481445]
	TIME [epoch: 8.85 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23688889694655205		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19176417018495662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21432653356575432 | validation: 0.10477200723664931]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22186926000822887		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16079051941638645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1913298897123077 | validation: 0.21451926192868814]
	TIME [epoch: 8.85 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24368402800103328		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40232248661485553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32300325730794444 | validation: 0.7904103196299235]
	TIME [epoch: 8.87 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29444304297487245		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26755158529286305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28099731413386764 | validation: 0.23752418151180282]
	TIME [epoch: 8.85 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21647875643760167		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15527491353222808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18587683498491486 | validation: 0.3530753494129299]
	TIME [epoch: 8.88 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23514455710178234		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19770279596737234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21642367653457736 | validation: 0.4938779734550881]
	TIME [epoch: 8.85 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5483554509294051		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2038046008524222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37608002589091366 | validation: 0.0893494348494069]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14866888367532427		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17348354376981948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16107621372257186 | validation: 0.21828563782541405]
	TIME [epoch: 8.87 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18044249730673206		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20116857397329774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19080553564001487 | validation: 0.13076296570973975]
	TIME [epoch: 8.84 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19881432210753805		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2646495366489382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2317319293782381 | validation: 0.627946880590571]
	TIME [epoch: 8.84 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18776431043368774		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.33780007507911547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2627821927564016 | validation: 0.15824330354632413]
	TIME [epoch: 8.84 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1366120433839259		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18946607995040032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16303906166716314 | validation: 0.11689695572107098]
	TIME [epoch: 8.91 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16504434378695593		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1627834941435133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16391391896523466 | validation: 0.13382876978156028]
	TIME [epoch: 8.87 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1375697015783966		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.415289325010966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2764295132946813 | validation: 0.15392707292015184]
	TIME [epoch: 8.85 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15276505703834065		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5084561717569973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33061061439766903 | validation: 0.17552334048880988]
	TIME [epoch: 8.84 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15545313077645515		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17214511050330164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1637991206398784 | validation: 0.05198299345211098]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16127524305663418		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13934099415248108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15030811860455764 | validation: 0.0740769615619614]
	TIME [epoch: 8.87 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13906903132929951		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18821323012068342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16364113072499145 | validation: 0.1776316457381971]
	TIME [epoch: 8.85 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23887166950266564		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16467561830756294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20177364390511426 | validation: 0.05844704700131767]
	TIME [epoch: 8.85 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23774139942343742		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10543824193344302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17158982067844022 | validation: 0.06966973058960305]
	TIME [epoch: 8.85 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24045945295599722		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19112979433687707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21579462364643706 | validation: 0.10746667249344347]
	TIME [epoch: 8.84 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17066551430844493		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1555292239131208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16309736911078287 | validation: 0.060500046047134284]
	TIME [epoch: 8.86 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21352434298337325		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24412247248034596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2288234077318596 | validation: 0.7943036239401937]
	TIME [epoch: 8.85 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3747286074435784		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19816823682074314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28644842213216076 | validation: 0.10370053687056048]
	TIME [epoch: 8.84 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14830021178576858		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20352654395391906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1759133778698438 | validation: 0.218266350384252]
	TIME [epoch: 8.85 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1261072795943931		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29590211079394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21100469519416656 | validation: 0.12037722053263987]
	TIME [epoch: 8.84 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16696519475695554		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.127488272104541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14722673343074827 | validation: 0.13657084133150266]
	TIME [epoch: 8.86 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13323069324068768		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17968021236036189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15645545280052475 | validation: 0.06071250151370675]
	TIME [epoch: 8.84 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12421404878318412		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14421812861682062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13421608870000237 | validation: 0.057996637306368504]
	TIME [epoch: 8.85 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15627453828217225		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15464266717043618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1554586027263042 | validation: 0.05769688530515826]
	TIME [epoch: 8.85 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.332881421656717		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18719356304150664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2600374923491119 | validation: 0.14581928198160185]
	TIME [epoch: 8.84 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17620892705407104		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19165581098827014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1839323690211706 | validation: 0.10792756445126679]
	TIME [epoch: 8.86 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1826239708482434		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3259768590763742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2543004149623088 | validation: 0.06969664124360686]
	TIME [epoch: 8.84 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11026698483195937		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20033765514073187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15530231998634564 | validation: 0.11109453247899696]
	TIME [epoch: 8.84 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15258830185242944		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11566476525555672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1341265335539931 | validation: 0.11265048734412021]
	TIME [epoch: 8.84 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12933082859996395		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12926691205137125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12929887032566761 | validation: 0.10768647794457008]
	TIME [epoch: 8.86 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1698368030009445		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16389655211196993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16686667755645723 | validation: 0.14551387020964937]
	TIME [epoch: 8.85 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2364709716790636		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15087433115403345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1936726514165485 | validation: 0.1515934846219607]
	TIME [epoch: 8.84 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16082297941366094		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15381811900117706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15732054920741897 | validation: 0.10609361308314866]
	TIME [epoch: 8.84 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14258548082818623		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12569019270335308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13413783676576968 | validation: 0.27035731939666496]
	TIME [epoch: 8.84 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29515083654081575		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1222711405380827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20871098853944922 | validation: 0.16506201601072623]
	TIME [epoch: 8.87 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15060476748396462		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1471356550205344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14887021125224947 | validation: 0.10630290590641273]
	TIME [epoch: 8.85 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14901940221169005		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13403708680093396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14152824450631202 | validation: 0.10587377379235804]
	TIME [epoch: 8.84 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13955155717626472		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23162174413235664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18558665065431068 | validation: 0.19956673895518046]
	TIME [epoch: 8.84 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1718493008263746		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16407606129729582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16796268106183523 | validation: 0.15250190202288302]
	TIME [epoch: 8.85 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10326301290624054		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09880482322325577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10103391806474815 | validation: 0.1502500282031838]
	TIME [epoch: 8.87 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1825871227085331		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14996048610124801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16627380440489062 | validation: 0.18603218125726864]
	TIME [epoch: 8.85 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12759594424006204		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12913085770622798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12836340097314503 | validation: 0.2081718486268651]
	TIME [epoch: 8.85 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1208038275731892		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24488836545091036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1828460965120498 | validation: 0.2682335539244032]
	TIME [epoch: 8.85 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16527732855036897		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1612428487150246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16326008863269675 | validation: 0.06791763856963044]
	TIME [epoch: 8.86 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14306956977832844		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1251475308338461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13410855030608723 | validation: 0.09144404093412768]
	TIME [epoch: 8.86 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12966278748731885		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11839893174490541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12403085961611211 | validation: 0.04256271009895182]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13514417898285		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09236751763761426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11375584831023211 | validation: 0.13618999336158433]
	TIME [epoch: 8.85 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12129254562004368		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3010280332336107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21116028942682724 | validation: 0.12246270065574327]
	TIME [epoch: 8.84 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11131389875655		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1231131193766735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11721350906661174 | validation: 0.06945827945196262]
	TIME [epoch: 8.86 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11557640729325003		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16014248648871687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13785944689098345 | validation: 0.12291880498815112]
	TIME [epoch: 8.85 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09813868656714306		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1470403355352404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12258951105119172 | validation: 0.2234611015289975]
	TIME [epoch: 8.85 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21853657860202316		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15882459749672156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18868058804937235 | validation: 0.25212702728844094]
	TIME [epoch: 8.84 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14370671145344022		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1904863458657362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1670965286595882 | validation: 0.2252080507157087]
	TIME [epoch: 8.85 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09181911551544117		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09942529742786282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09562220647165201 | validation: 0.06405560005043838]
	TIME [epoch: 8.87 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09717696176298328		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18796369543717012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14257032860007665 | validation: 0.21490284732681883]
	TIME [epoch: 16.3 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3298286724104006		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1515166732362458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24067267282332322 | validation: 0.325578505124863]
	TIME [epoch: 8.85 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10631655073561871		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15674077935227917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13152866504394894 | validation: 0.12601662213714918]
	TIME [epoch: 8.85 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11364652858288422		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11909298634289367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11636975746288894 | validation: 0.05085818247596321]
	TIME [epoch: 8.84 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1996752324910293		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13777323436640568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16872423342871748 | validation: 0.19270935305645231]
	TIME [epoch: 8.87 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11384849087039173		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18575208384490932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1498002873576505 | validation: 0.13801025733331357]
	TIME [epoch: 8.84 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17283010316302033		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16903794245840403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17093402281071218 | validation: 0.08544987886526806]
	TIME [epoch: 8.85 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08992533043792553		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13711002140770295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11351767592281425 | validation: 0.0989686077813065]
	TIME [epoch: 8.85 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1604711284327469		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10659879275917776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13353496059596232 | validation: 0.13095361187135252]
	TIME [epoch: 8.87 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13015804618391497		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11060823457519145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1203831403795532 | validation: 0.12098334809616237]
	TIME [epoch: 8.85 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16904996608874584		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2868784363808595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22796420123480265 | validation: 0.18251106035636597]
	TIME [epoch: 8.85 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19847983782563916		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11216060370138507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1553202207635121 | validation: 0.06924259042734421]
	TIME [epoch: 8.85 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2069630988681957		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09924339682843294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15310324784831433 | validation: 0.1329140699481019]
	TIME [epoch: 8.85 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1239103105293344		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15611167196159728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14001099124546584 | validation: 0.1316241134129225]
	TIME [epoch: 8.87 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2732817550710996		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14371314756670595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20849745131890277 | validation: 0.25701638411703137]
	TIME [epoch: 8.86 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20092971753132044		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16652274104327586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18372622928729818 | validation: 0.1764324274626768]
	TIME [epoch: 8.85 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30887152363773424		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25691471907242386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28289312135507905 | validation: 0.033931777889391876]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0944330160797178		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11688528167094556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10565914887533165 | validation: 0.05927955060850079]
	TIME [epoch: 8.85 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.311630345758656		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26924043324685176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2904353895027539 | validation: 0.5390079543002393]
	TIME [epoch: 8.87 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3436220292240585		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2154823446874302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27955218695574435 | validation: 0.06792151958711214]
	TIME [epoch: 8.86 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2187642037726533		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19366569517962548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2062149494761394 | validation: 0.1481035023295562]
	TIME [epoch: 8.85 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09857306148370024		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13283316478297091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11570311313333555 | validation: 0.07408791845907188]
	TIME [epoch: 8.85 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1096707443043545		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11058188242948205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11012631336691832 | validation: 0.009509295223056189]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1256208388750289		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20289179295965415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16425631591734152 | validation: 0.05843378522583814]
	TIME [epoch: 8.87 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15816173515097146		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08539875402262395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1217802445867977 | validation: 0.19205656906204027]
	TIME [epoch: 8.85 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11397655255262173		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09561410928504413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10479533091883293 | validation: 0.12399969549969622]
	TIME [epoch: 8.85 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34087414067341226		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29492962707238296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31790188387289764 | validation: 0.245772248369454]
	TIME [epoch: 8.84 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2988859609641836		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13317927495842186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21603261796130271 | validation: 0.055374801948695]
	TIME [epoch: 8.87 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09741866732007064		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1228111826227009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11011492497138575 | validation: 0.11365582981063557]
	TIME [epoch: 8.85 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09770755273704564		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10548308439258444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10159531856481505 | validation: 0.16012942214348386]
	TIME [epoch: 8.84 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13423187189495406		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2583871308169061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1963095013559301 | validation: 0.17474104888467662]
	TIME [epoch: 8.84 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1947594585114643		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1816014113220777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.188180434916771 | validation: 0.18345774878415136]
	TIME [epoch: 8.84 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2245595486566664		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3031571806189343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2638583646378003 | validation: 0.7657309360988545]
	TIME [epoch: 8.87 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5806014320586634		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16455556029365023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3725784961761568 | validation: 0.12152662496307122]
	TIME [epoch: 8.85 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10357441302684438		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18057943351857353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14207692327270896 | validation: 0.46709399494357284]
	TIME [epoch: 8.85 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13256605793413426		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4286061238191907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2805860908766624 | validation: 0.47227195711450454]
	TIME [epoch: 8.85 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2621688239236314		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.249524634732906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25584672932826874 | validation: 0.10721652853031297]
	TIME [epoch: 8.84 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1595726307089329		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08520023166904911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.122386431188991 | validation: 0.158994729711986]
	TIME [epoch: 8.86 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0988148450943493		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09939887320763457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09910685915099196 | validation: 0.04958721605449094]
	TIME [epoch: 8.87 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08350451004640932		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13738929649680695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11044690327160818 | validation: 0.3087426204971994]
	TIME [epoch: 8.87 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21951666682301735		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13792620367238626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17872143524770176 | validation: 0.04208327927440862]
	TIME [epoch: 8.87 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09079695719883707		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10407239763836443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09743467741860076 | validation: 0.16364706694785142]
	TIME [epoch: 8.9 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13838071119691114		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.102785236834666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12058297401578855 | validation: 0.11974141915901912]
	TIME [epoch: 8.87 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12359865900844676		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07585877581151051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09972871740997864 | validation: 0.1280281139312033]
	TIME [epoch: 8.85 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12173170054011032		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09535642544308312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10854406299159672 | validation: 0.057155906705433177]
	TIME [epoch: 8.87 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12481237340809251		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1212414455044096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12302690945625103 | validation: 0.1280550238394949]
	TIME [epoch: 8.87 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09428943445827893		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18592100303519962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14010521874673929 | validation: 0.08698061145003635]
	TIME [epoch: 11.3 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10339453279034831		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11659936866680705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10999695072857767 | validation: 0.12888712713466835]
	TIME [epoch: 8.85 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26853370707155894		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11746375226014275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1929987296658508 | validation: 0.07638879446807431]
	TIME [epoch: 8.86 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1278454273319822		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12446356331453527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1261544953232588 | validation: 0.08685693846112902]
	TIME [epoch: 13.4 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10259227961977173		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10511909390286589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1038556867613188 | validation: 0.23565238866182436]
	TIME [epoch: 8.85 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2222121071552318		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15349304273827316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18785257494675245 | validation: 0.19273721521843973]
	TIME [epoch: 8.87 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17227566493092106		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15713430757020994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1647049862505655 | validation: 0.10095800853096691]
	TIME [epoch: 8.85 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14864021964399587		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11684552371865313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1327428716813245 | validation: 0.04931469231503567]
	TIME [epoch: 8.85 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12024620108912436		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13142639502644993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12583629805778712 | validation: 0.1970778929771781]
	TIME [epoch: 8.84 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14745493495416223		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18367903090569165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16556698292992694 | validation: 0.1198984206154798]
	TIME [epoch: 8.85 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22402496820969722		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6467647982687199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4353948832392085 | validation: 0.05818845436972627]
	TIME [epoch: 8.86 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13611401584162103		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1455861901732692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1408501030074451 | validation: 0.14214301899048337]
	TIME [epoch: 8.84 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11760009352996478		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09861084376889837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10810546864943157 | validation: 0.0606666513932433]
	TIME [epoch: 8.84 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12405408569924244		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1556010462379875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13982756596861495 | validation: 0.199338496778995]
	TIME [epoch: 11.8 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21287869751565208		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1752321459309067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1940554217232794 | validation: 0.049592512052334795]
	TIME [epoch: 8.87 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10723090578147765		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08366839079104006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09544964828625886 | validation: 0.09055350063598595]
	TIME [epoch: 8.85 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09865303604529972		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11589705390476253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10727504497503111 | validation: 0.13664874257223297]
	TIME [epoch: 8.92 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1159221037229354		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1438266228263198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12987436327462762 | validation: 0.0854786515436178]
	TIME [epoch: 8.84 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1623870370147833		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10937940123045072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13588321912261697 | validation: 0.06765617859547926]
	TIME [epoch: 8.85 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15920534451451648		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09774994183672742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1284776431756219 | validation: 0.15717387410965633]
	TIME [epoch: 8.87 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12714874964915307		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09673109078159939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11193992021537622 | validation: 0.20103183213770073]
	TIME [epoch: 8.86 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18929168970888013		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3312879983274512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2602898440181657 | validation: 0.25370755512689985]
	TIME [epoch: 8.85 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19359586045832194		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22034743865909062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2069716495587063 | validation: 0.15549975035813435]
	TIME [epoch: 8.85 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20251853087267344		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21252643876629512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20752248481948424 | validation: 0.06303275420497655]
	TIME [epoch: 8.84 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10743712418226678		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14533465417426056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12638588917826368 | validation: 0.10728850127180597]
	TIME [epoch: 8.88 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12100736933943348		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17899060610946393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1499989877244487 | validation: 0.051491174870162956]
	TIME [epoch: 8.83 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09270939725213985		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15850668149348832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1256080393728141 | validation: 0.4483893402686741]
	TIME [epoch: 8.84 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23128096131054504		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12243787565339084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17685941848196793 | validation: 0.0716607226329103]
	TIME [epoch: 8.84 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11259641876661378		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14582864195636047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12921253036148714 | validation: 0.06642859981999746]
	TIME [epoch: 8.86 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08908771777341537		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10719820073052726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09814295925197132 | validation: 0.1354099605632842]
	TIME [epoch: 8.85 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12719996081301235		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11524405428016712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12122200754658971 | validation: 0.07933205723461126]
	TIME [epoch: 8.84 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11264708595858877		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22591758368314213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16928233482086544 | validation: 0.7028132634168681]
	TIME [epoch: 8.84 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2446925061728103		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15272469516306003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19870860066793516 | validation: 0.09565081577952331]
	TIME [epoch: 8.84 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13196398560947764		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09194323181436084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11195360871191926 | validation: 0.028533067260697324]
	TIME [epoch: 8.88 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08814219951493181		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12300085258173363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10557152604833273 | validation: 0.060142715580583546]
	TIME [epoch: 8.85 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10875357298288876		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12073932516675985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1147464490748243 | validation: 0.07516512423907136]
	TIME [epoch: 8.84 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11405490873102128		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1727968125590327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.143425860645027 | validation: 0.1922208985696653]
	TIME [epoch: 8.84 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12839623157194083		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18375215239184067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1560741919818907 | validation: 0.20292653807386418]
	TIME [epoch: 8.84 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13813143684921356		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3070411947063498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22258631577778165 | validation: 0.10561317616686958]
	TIME [epoch: 8.86 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15406228449082623		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2038710305471883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17896665751900726 | validation: 0.11047760664805563]
	TIME [epoch: 8.84 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13143281737640214		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11128668048182441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12135974892911328 | validation: 0.12647886092832045]
	TIME [epoch: 8.83 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13972352724529827		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17623361476716848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15797857100623333 | validation: 0.14881350083272188]
	TIME [epoch: 8.84 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20779252032484616		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19024121344253375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19901686688368997 | validation: 0.17051632510965836]
	TIME [epoch: 8.85 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15138999016665794		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1373681803704613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1443790852685596 | validation: 0.0866569277837103]
	TIME [epoch: 8.86 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07259937053727913		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13001239439955212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10130588246841561 | validation: 0.22117936404919758]
	TIME [epoch: 8.84 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08939138723043569		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11407383341021274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10173261032032421 | validation: 0.08448205102055914]
	TIME [epoch: 8.84 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1044250869344467		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14196708109855125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12319608401649898 | validation: 0.03185206320320087]
	TIME [epoch: 8.83 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11894314129085462		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15141687075575852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13518000602330654 | validation: 0.09421391459780704]
	TIME [epoch: 8.87 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07064864813017666		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07979056278190763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07521960545604214 | validation: 0.08131077944484182]
	TIME [epoch: 8.86 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12682686503257548		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.130162592185553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12849472860906425 | validation: 0.08929756780498009]
	TIME [epoch: 8.84 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1714974480037104		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1240906397310942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1477940438674023 | validation: 0.045460593153666046]
	TIME [epoch: 8.85 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10399610053176753		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.096330131628568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10016311608016777 | validation: 0.10673821475377583]
	TIME [epoch: 8.84 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11347137376589382		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1653788616041158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1394251176850048 | validation: 0.10299798089489447]
	TIME [epoch: 8.86 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10659220754273269		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1279868005502786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11728950404650566 | validation: 0.09073691878078899]
	TIME [epoch: 8.83 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10346223114583544		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12494154183928093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11420188649255816 | validation: 0.0911322462679383]
	TIME [epoch: 8.84 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14870315252915364		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06291129074031382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10580722163473373 | validation: 0.03702990039689143]
	TIME [epoch: 8.84 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09870902421667065		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10503352201937197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10187127311802131 | validation: 0.14320791589697823]
	TIME [epoch: 8.85 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10347384677637825		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08477465377669462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09412425027653645 | validation: 0.07794706166530059]
	TIME [epoch: 8.87 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09763404476848064		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10564953061388298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10164178769118179 | validation: 0.04878464473040056]
	TIME [epoch: 8.84 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1103247081953023		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09987052430743301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10509761625136764 | validation: 0.33787199636036064]
	TIME [epoch: 8.84 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11873147565959415		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10280403760682555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11076775663320985 | validation: 0.13321610000866663]
	TIME [epoch: 8.84 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20514468178722836		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09769335044444837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15141901611583836 | validation: 0.07261968888643286]
	TIME [epoch: 8.86 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11841047803732563		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10554323095840201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11197685449786383 | validation: 0.1274757161382944]
	TIME [epoch: 8.86 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12412160214847279		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14041031795698783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1322659600527303 | validation: 0.10108984890504297]
	TIME [epoch: 8.84 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10595973085960349		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11338424014504703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10967198550232526 | validation: 0.14742450447156455]
	TIME [epoch: 8.83 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16804647070202366		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3330472065491389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25054683862558125 | validation: 1.4748246714419717]
	TIME [epoch: 8.84 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35537964761067486		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13220844028029835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2437940439454866 | validation: 0.22214795710526428]
	TIME [epoch: 8.86 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12790090870563725		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11582251589221604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12186171229892662 | validation: 0.12032075046217458]
	TIME [epoch: 8.85 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09468473458548504		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11467140476119775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1046780696733414 | validation: 0.22816137775582238]
	TIME [epoch: 8.84 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20829871875771766		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.081289396187444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14479405747258084 | validation: 0.042022860640780216]
	TIME [epoch: 8.85 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1083591619055286		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08093393585111992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09464654887832427 | validation: 0.10874727889208012]
	TIME [epoch: 8.84 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11334729548468644		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10438879441384634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10886804494926638 | validation: 0.17084027187650058]
	TIME [epoch: 8.86 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24663618203494794		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13350589460185985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19007103831840388 | validation: 0.06187528573428475]
	TIME [epoch: 8.85 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06580989176487295		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09515753348200302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.080483712623438 | validation: 0.031083083627548573]
	TIME [epoch: 8.85 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08683588231183256		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0892085094472773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08802219587955493 | validation: 0.10015111630241451]
	TIME [epoch: 8.85 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1180834181085438		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09086136991476199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1044723940116529 | validation: 0.09334280864099258]
	TIME [epoch: 8.86 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11152174495268437		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09206017100504406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10179095797886421 | validation: 0.11853902034383092]
	TIME [epoch: 8.87 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07769531279404707		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09140737023065326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08455134151235019 | validation: 0.10804724718066822]
	TIME [epoch: 8.85 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12470947861868242		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08503352074072004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10487149967970122 | validation: 0.05030447079114809]
	TIME [epoch: 8.84 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08973964755359945		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07991651466858052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08482808111109 | validation: 0.06084312541906613]
	TIME [epoch: 8.84 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11010666837647146		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1905317897676731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15031922907207226 | validation: 0.05015403809294363]
	TIME [epoch: 8.86 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1229233782690983		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16390575607018998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14341456716964412 | validation: 0.262296145229414]
	TIME [epoch: 8.84 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13209656062553007		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08436304739272166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10822980400912585 | validation: 0.08292641497418429]
	TIME [epoch: 8.83 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13378637258903728		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07560360192863798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10469498725883764 | validation: 0.2375518162343144]
	TIME [epoch: 8.84 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10298323721729503		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09193471449417137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09745897585573318 | validation: 0.09506108798308209]
	TIME [epoch: 8.85 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11479988629272156		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16968204208664533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14224096418968343 | validation: 0.2765038200030638]
	TIME [epoch: 8.87 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17255581958194313		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10348239486930584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1380191072256245 | validation: 0.11857358380772065]
	TIME [epoch: 8.84 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09772109041463467		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09261051112387582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09516580076925525 | validation: 0.037800761037248325]
	TIME [epoch: 8.83 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09644581209716502		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13064435932859297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.113545085712879 | validation: 0.12697964559444885]
	TIME [epoch: 8.84 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10337975744411718		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09634900211089062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09986437977750388 | validation: 0.09252744436398559]
	TIME [epoch: 8.84 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08011655061565948		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16419442831590123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12215548946578034 | validation: 0.0523630822240763]
	TIME [epoch: 8.85 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11297470766077175		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07606147780096115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09451809273086643 | validation: 0.058492614436914836]
	TIME [epoch: 8.85 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19500726541651442		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.154204052386204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17460565890135923 | validation: 0.4520630708937079]
	TIME [epoch: 8.84 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2139397434112255		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09050053340245975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15222013840684262 | validation: 0.12732685249703857]
	TIME [epoch: 8.84 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0895223525572728		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10673528338931566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09812881797329423 | validation: 0.08139222321952286]
	TIME [epoch: 8.85 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08007186709553822		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10159144119535454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09083165414544636 | validation: 0.21210504142389375]
	TIME [epoch: 8.86 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13025939804859393		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14739566095124718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13882752949992055 | validation: 0.5338921947548856]
	TIME [epoch: 8.84 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22488724392830511		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12062806276066351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17275765334448429 | validation: 0.12547882522485]
	TIME [epoch: 8.84 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5291515776082386		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47191783644836427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5005347070283015 | validation: 0.915303407718324]
	TIME [epoch: 8.84 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8210646821354939		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6374851845744548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7292749333549744 | validation: 0.2392669580426175]
	TIME [epoch: 8.86 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17908756008646085		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1518710301109043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1654792950986826 | validation: 0.09632644079606782]
	TIME [epoch: 8.84 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13736600012016512		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14922979537255568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14329789774636037 | validation: 0.17839080373221325]
	TIME [epoch: 8.83 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12988285914747277		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12843586566228632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12915936240487955 | validation: 0.07630528320899516]
	TIME [epoch: 8.83 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10227776423955277		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1640805726115425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1331791684255476 | validation: 0.13827210189543335]
	TIME [epoch: 8.84 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11496734303013505		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10376379300189224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10936556801601363 | validation: 0.09886840233143313]
	TIME [epoch: 8.86 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13975303866565408		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13535098179611146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13755201023088276 | validation: 0.08202862692621829]
	TIME [epoch: 8.84 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.153882539418772		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09549824450191949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12469039196034577 | validation: 0.04331748255099031]
	TIME [epoch: 8.84 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12236661868021222		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13519736636016833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1287819925201903 | validation: 0.09942116673076906]
	TIME [epoch: 8.83 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08603023193807505		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10351369445260299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09477196319533901 | validation: 0.10600876118139524]
	TIME [epoch: 8.84 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11610267192081883		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10121123801322214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1086569549670205 | validation: 0.11278875956707073]
	TIME [epoch: 8.87 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10135163097897608		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1116332524667035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10649244172283978 | validation: 0.07593651590244188]
	TIME [epoch: 8.84 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08071344623525373		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08973270189961875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08522307406743623 | validation: 0.04909715499584531]
	TIME [epoch: 8.83 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07195113821036205		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13453633865585787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10324373843310994 | validation: 0.050778338088000835]
	TIME [epoch: 8.84 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09321421113465246		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08301570202820273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0881149565814276 | validation: 0.06309289955317654]
	TIME [epoch: 8.86 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1568222462597076		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08830298889336526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12256261757653646 | validation: 0.08216045093484299]
	TIME [epoch: 8.84 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10777061267972196		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07212623608403129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08994842438187664 | validation: 0.05139788302020312]
	TIME [epoch: 8.84 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08532651919305803		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11724839245679153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10128745582492478 | validation: 0.14686527538464894]
	TIME [epoch: 8.84 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11752520355927558		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0924592482705597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10499222591491764 | validation: 0.150936277820055]
	TIME [epoch: 8.83 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09738694161673855		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15928361278505612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12833527720089732 | validation: 0.08685051820584076]
	TIME [epoch: 8.86 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1301830409372134		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12593067782211545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12805685937966443 | validation: 0.04818183984227312]
	TIME [epoch: 8.84 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09619169117127026		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5381246848523666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31715818801181844 | validation: 0.1103889139808619]
	TIME [epoch: 8.83 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11999684359256543		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07508840033088045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09754262196172295 | validation: 0.08298384663388733]
	TIME [epoch: 8.84 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09892214370521266		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12404342079300268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11148278224910771 | validation: 0.1348147062378505]
	TIME [epoch: 8.84 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1015968162705948		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0880482866815975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09482255147609617 | validation: 0.03428561719462329]
	TIME [epoch: 8.86 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0801817129200488		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14683983831927674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11351077561966276 | validation: 0.10561138028923894]
	TIME [epoch: 8.84 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09388158564850596		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16665834509339367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1302699653709498 | validation: 0.17565458043456153]
	TIME [epoch: 8.83 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13150968428456783		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07317309625556122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10234139027006453 | validation: 0.05498945239470164]
	TIME [epoch: 8.83 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09339924302055096		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07284090499615249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0831200740083517 | validation: 0.029835155837427227]
	TIME [epoch: 8.83 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11304896378924693		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18234559513423704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.147697279461742 | validation: 0.05937608789942566]
	TIME [epoch: 8.86 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12861624205695563		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13269337198137832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13065480701916696 | validation: 0.07500738124504072]
	TIME [epoch: 8.84 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09386117395650409		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0771213822929733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08549127812473868 | validation: 0.09627435868399034]
	TIME [epoch: 8.83 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1310305135745133		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07415116336589596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10259083847020464 | validation: 0.055479663823801206]
	TIME [epoch: 8.83 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14712080758062424		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06548465653947515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1063027320600497 | validation: 0.05983400462682311]
	TIME [epoch: 8.86 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07966299080082233		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0912868798026524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08547493530173737 | validation: 0.040484383535244674]
	TIME [epoch: 8.84 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07259815041854663		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.05944507338022763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06602161189938711 | validation: 0.06003610838163795]
	TIME [epoch: 8.84 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10636672811746087		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09534239080924804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10085455946335446 | validation: 0.14082799512950298]
	TIME [epoch: 8.84 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10522738765340098		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08629211984727729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09575975375033915 | validation: 0.11128355480176988]
	TIME [epoch: 8.84 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08872012347365918		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07439968836164326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08155990591765122 | validation: 0.09957122801884105]
	TIME [epoch: 8.85 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0833737432443743		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0826385735211605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0830061583827674 | validation: 0.14476840791258777]
	TIME [epoch: 8.85 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08085239588168401		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07748203781508524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07916721684838463 | validation: 0.19317018108910872]
	TIME [epoch: 8.83 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1010762871853346		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09103770321643063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09605699520088261 | validation: 0.0664255072447797]
	TIME [epoch: 8.84 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07516890854965268		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06397694936571933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.069572928957686 | validation: 0.10323370916036154]
	TIME [epoch: 8.84 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09506932773858397		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08289247207661105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08898089990759749 | validation: 0.0846284749882765]
	TIME [epoch: 8.86 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.091469172112486		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07643748371587474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08395332791418038 | validation: 0.02550715982188766]
	TIME [epoch: 8.84 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07630244260586157		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07322354140646088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0747629920061612 | validation: 0.11899103113080949]
	TIME [epoch: 8.83 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07903770359613828		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12948093862246188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1042593211093001 | validation: 0.07547625385538298]
	TIME [epoch: 8.84 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0838526885143355		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10084381894651966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09234825373042757 | validation: 0.06288725279894618]
	TIME [epoch: 8.86 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06447070876574199		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12718271711911014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09582671294242605 | validation: 0.13634627492411358]
	TIME [epoch: 8.85 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07437281259811732		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07583725864358026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07510503562084878 | validation: 0.13140629306266696]
	TIME [epoch: 8.84 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10534215789274559		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08515253825747346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09524734807510954 | validation: 0.07282187588271283]
	TIME [epoch: 8.84 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07511811591907766		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14026371666047965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10769091628977864 | validation: 0.0660509234423662]
	TIME [epoch: 8.83 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06848797639378673		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07821371762777543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07335084701078108 | validation: 0.0642050276026954]
	TIME [epoch: 8.85 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08943331906938097		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10715709693094103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09829520800016099 | validation: 0.02846667604419657]
	TIME [epoch: 8.84 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09599829205423487		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11691865684770257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10645847445096868 | validation: 0.10283697610518258]
	TIME [epoch: 8.83 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07401242568894781		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.059132559906692216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06657249279782003 | validation: 0.11285851495726824]
	TIME [epoch: 8.83 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06472142866459354		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0905360335185746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07762873109158407 | validation: 0.012974862960553021]
	TIME [epoch: 8.83 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1267058297106371		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09059429106094924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10865006038579317 | validation: 0.059587577895721194]
	TIME [epoch: 8.86 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11770930070668117		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08243085001913894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10007007536291004 | validation: 0.06193182172436347]
	TIME [epoch: 8.84 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08591338479983734		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10237079117100542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09414208798542137 | validation: 0.04284541386404844]
	TIME [epoch: 8.85 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0706014867430776		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15213643138771452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11136895906539604 | validation: 0.07481814971882572]
	TIME [epoch: 8.83 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06318874197450808		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07669774433787925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06994324315619367 | validation: 0.04767060788940833]
	TIME [epoch: 8.84 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0857285804854391		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10045542942023448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0930920049528368 | validation: 0.22280199289979297]
	TIME [epoch: 8.85 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14356317938426494		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.05035899465333689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0969610870188009 | validation: 0.16496828939680222]
	TIME [epoch: 8.84 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12602291499403545		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.052975134442842894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08949902471843918 | validation: 0.16821614697636436]
	TIME [epoch: 8.83 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11977693946054917		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07302409807709281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09640051876882098 | validation: 0.048002597387100956]
	TIME [epoch: 8.83 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17639085983450556		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15046383207331587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1634273459539107 | validation: 0.667980083352384]
	TIME [epoch: 8.85 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3016938840426418		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07618633320490766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1889401086237747 | validation: 0.14654256433111426]
	TIME [epoch: 8.84 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09972711541713751		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12730462683751465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11351587112732606 | validation: 0.08042783979387007]
	TIME [epoch: 8.83 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11072731208681982		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07538657984952453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09305694596817218 | validation: 0.06285454965580185]
	TIME [epoch: 8.84 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0799359880815442		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09161141436856617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08577370122505518 | validation: 0.14837209307715307]
	TIME [epoch: 8.82 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12407720504402517		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10185516684517097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11296618594459806 | validation: 0.13229778072994242]
	TIME [epoch: 8.86 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09775732385288662		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1012415434615043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09949943365719545 | validation: 0.0802794921857505]
	TIME [epoch: 8.85 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1256743445341632		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08264141442475417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10415787947945872 | validation: 0.10279135331147672]
	TIME [epoch: 8.85 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15551484937074092		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10873766269353966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13212625603214032 | validation: 0.06878951957786668]
	TIME [epoch: 8.84 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10064548788537528		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09035335384536475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09549942086537001 | validation: 0.024962804120218174]
	TIME [epoch: 8.84 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07800522626275705		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06204891231646581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07002706928961142 | validation: 0.06717703296052999]
	TIME [epoch: 8.86 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09723631985947881		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0924411709104845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09483874538498163 | validation: 0.11203099635294991]
	TIME [epoch: 8.84 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0836938585693254		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1107596438500525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09722675120968893 | validation: 0.07656647133680528]
	TIME [epoch: 8.83 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14263113019262788		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21821514366614028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1804231369293841 | validation: 0.22357428367966506]
	TIME [epoch: 8.83 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17453327200584062		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11740859589477563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1459709339503081 | validation: 0.09094439566185958]
	TIME [epoch: 8.84 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18918941605121314		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3138851698410443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25153729294612875 | validation: 0.21191927264175586]
	TIME [epoch: 8.86 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1723375543854727		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12109425628182682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14671590533364975 | validation: 0.06350977465679615]
	TIME [epoch: 8.84 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0949182678777669		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10523586945611874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10007706866694284 | validation: 0.12303232265044854]
	TIME [epoch: 8.83 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10262715181618773		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10115642654152465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10189178917885618 | validation: 0.15087795270580592]
	TIME [epoch: 8.83 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07886598750403706		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07999049999452566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07942824374928137 | validation: 0.06951401508244363]
	TIME [epoch: 8.85 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06909145696291613		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10210220578461197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08559683137376406 | validation: 0.09926957284333365]
	TIME [epoch: 8.84 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08455274329237296		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08127233279500101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08291253804368696 | validation: 0.13609375612740285]
	TIME [epoch: 8.84 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11890526265468944		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11048000597359069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11469263431414005 | validation: 0.06269879772782994]
	TIME [epoch: 8.84 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0747846646480161		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08850092458468392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08164279461635 | validation: 0.12279913662388063]
	TIME [epoch: 8.83 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10125676553414813		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1246876573418351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1129722114379916 | validation: 0.0658674395702428]
	TIME [epoch: 8.85 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13968814047972963		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09744299218665325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11856556633319144 | validation: 0.05016968368633807]
	TIME [epoch: 8.85 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0766180749194294		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08206711354831796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07934259423387367 | validation: 0.06666330065251663]
	TIME [epoch: 8.84 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2021269214050614		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10292751023650451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15252721582078296 | validation: 0.06498587897527515]
	TIME [epoch: 8.84 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11533854016190867		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08138909928528636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09836381972359752 | validation: 0.16897037202288895]
	TIME [epoch: 8.84 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09748833911756438		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08745726135673879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09247280023715158 | validation: 0.07005083039663355]
	TIME [epoch: 8.86 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08017045236441436		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07943617593142788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0798033141479211 | validation: 0.07851626592899753]
	TIME [epoch: 8.84 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1471436966513291		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08078925891341548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11396647778237226 | validation: 0.05963482033074155]
	TIME [epoch: 8.85 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10767574160130951		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1254384596686343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11655710063497193 | validation: 0.06626791439612312]
	TIME [epoch: 8.84 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07848177124961109		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06608773411941246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07228475268451176 | validation: 0.061512485423266844]
	TIME [epoch: 8.87 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12362869280258197		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10631963499988306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11497416390123252 | validation: 0.08352068241584375]
	TIME [epoch: 8.84 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09798943094306586		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08177848406132668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08988395750219627 | validation: 0.02568540285729793]
	TIME [epoch: 8.83 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11384960166174503		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06449107626734052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08917033896454277 | validation: 0.015227793185167444]
	TIME [epoch: 8.84 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06568299599552054		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09412275595044718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07990287597298387 | validation: 0.0749477570291176]
	TIME [epoch: 8.84 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1272354127082156		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06385326903977234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09554434087399397 | validation: 0.07616237030232031]
	TIME [epoch: 8.87 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057907202187000496		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08035242155755282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06912981187227665 | validation: 0.041030150151326814]
	TIME [epoch: 8.85 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06152549697283678		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07430997505091781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06791773601187728 | validation: 0.15810641532196937]
	TIME [epoch: 8.84 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14560364890743774		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07827358258565542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11193861574654655 | validation: 0.044326929000346904]
	TIME [epoch: 8.83 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07496592216612077		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19477976194298757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1348728420545542 | validation: 0.2042259128410548]
	TIME [epoch: 8.85 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10508689878648214		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07598047046663534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09053368462655872 | validation: 0.10323817181285853]
	TIME [epoch: 8.87 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08419387814008003		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10732764954895306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09576076384451652 | validation: 0.21304653375476687]
	TIME [epoch: 8.83 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08443283863657022		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06338636175846102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07390960019751563 | validation: 0.07999454514020589]
	TIME [epoch: 8.85 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06753806841422713		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14608796905586935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10681301873504823 | validation: 0.17963983663346414]
	TIME [epoch: 8.84 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13317571359051966		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09782488167427703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11550029763239833 | validation: 0.04317540699662339]
	TIME [epoch: 8.85 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0841651215610014		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07835447840231118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08125979998165629 | validation: 0.04738408197713764]
	TIME [epoch: 8.87 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08106626282455451		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09158386923988521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08632506603221986 | validation: 0.31149594579920664]
	TIME [epoch: 8.84 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09403500660288053		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.130666116278991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11235056144093578 | validation: 0.05990857435923719]
	TIME [epoch: 8.84 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08724061040449352		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07305500516781899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08014780778615624 | validation: 0.10660312740875849]
	TIME [epoch: 8.84 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08519435357625302		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06995019694896226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07757227526260764 | validation: 0.07292203334220235]
	TIME [epoch: 8.86 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11565955516203626		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10146087683816041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10856021600009835 | validation: 0.21526851186527357]
	TIME [epoch: 8.84 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11131501127195449		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06282651322595749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08707076224895599 | validation: 0.04218196546320224]
	TIME [epoch: 8.84 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09737440662026728		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07502607115481696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08620023888754212 | validation: 0.05194354238338704]
	TIME [epoch: 8.83 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06291819872891444		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0796716067778188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07129490275336663 | validation: 0.11930046953347262]
	TIME [epoch: 8.85 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09271290700789972		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09534062288836428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09402676494813199 | validation: 0.05395144443589571]
	TIME [epoch: 8.87 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05810602565470846		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11212431641675616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08511517103573232 | validation: 0.07367031788750007]
	TIME [epoch: 8.85 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07655516355366383		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11201407018567038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0942846168696671 | validation: 0.18132822397016535]
	TIME [epoch: 8.84 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09783138808474204		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08205829690630054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0899448424955213 | validation: 0.06672754718929014]
	TIME [epoch: 8.84 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08036482085558369		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.05587663464256509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06812072774907442 | validation: 0.10152272893355838]
	TIME [epoch: 8.84 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09944681882152301		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12465846633093287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11205264257622796 | validation: 0.07133804114787076]
	TIME [epoch: 8.86 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07496720660446395		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13908349534562675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10702535097504536 | validation: 0.03467007032627238]
	TIME [epoch: 8.85 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11064984367115302		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07763148182938018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09414066275026661 | validation: 0.07198033826972491]
	TIME [epoch: 8.84 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07749102600856372		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08680351147402057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08214726874129216 | validation: 0.03386637802119661]
	TIME [epoch: 8.83 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11116423468217718		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21105634002912868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16111028735565294 | validation: 0.08214747390302045]
	TIME [epoch: 8.85 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09361630578224563		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08038817813112212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08700224195668389 | validation: 0.1201174245121922]
	TIME [epoch: 8.85 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12355811213851868		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11451938828112848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1190387502098236 | validation: 0.059886727811927075]
	TIME [epoch: 8.83 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09094613451023194		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10790818400584927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09942715925804062 | validation: 0.43117763377138374]
	TIME [epoch: 8.84 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14481303509844068		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08406826950304157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11444065230074112 | validation: 0.15826787709033202]
	TIME [epoch: 8.83 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12467503210369428		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06543518083766796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09505510647068112 | validation: 0.07122767616344532]
	TIME [epoch: 8.86 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08002959229793476		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07520850143865682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07761904686829577 | validation: 0.12458687363405721]
	TIME [epoch: 9.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08419190801744246		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06165268354781507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07292229578262877 | validation: 0.24689200528114907]
	TIME [epoch: 8.84 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15339450125800408		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19922481537256173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17630965831528292 | validation: 0.17362598229554144]
	TIME [epoch: 8.85 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11363800754049173		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14875829009338273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13119814881693723 | validation: 0.08853012789803846]
	TIME [epoch: 8.85 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09721131588739887		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0855948766203963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0914030962538976 | validation: 0.1434694069942115]
	TIME [epoch: 8.87 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07819607376698283		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07044676535865164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07432141956281722 | validation: 0.03527390383369828]
	TIME [epoch: 8.83 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10670754946717445		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09635004991240771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10152879968979107 | validation: 0.05313264965215822]
	TIME [epoch: 8.84 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09699206005807105		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07834064389121569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08766635197464337 | validation: 0.12669027434788765]
	TIME [epoch: 8.84 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07771556576133098		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07743993685851236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07757775130992167 | validation: 0.11464162169046022]
	TIME [epoch: 8.84 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07797895163614924		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08880926089544228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08339410626579577 | validation: 0.0691237068354967]
	TIME [epoch: 8.86 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055770848708738704		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15098490521915786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10337787696394826 | validation: 0.14380129548989373]
	TIME [epoch: 8.84 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08145218043437834		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06137121521770165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07141169782603998 | validation: 0.030746337234720374]
	TIME [epoch: 8.84 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07680425517470228		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07792304440150245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07736364978810237 | validation: 0.026037686699644565]
	TIME [epoch: 8.83 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06710801970079895		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10019797381019872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08365299675549884 | validation: 0.08875614573054899]
	TIME [epoch: 8.85 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09174744094385825		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07626361658597199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08400552876491511 | validation: 0.039114219296740575]
	TIME [epoch: 8.85 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06800203247282074		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08028450371294885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07414326809288478 | validation: 0.09054013217872424]
	TIME [epoch: 8.84 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08323972470352757		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07079736918244456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07701854694298607 | validation: 0.056506597830873265]
	TIME [epoch: 8.85 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08037537671189743		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.05359010802295518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0669827423674263 | validation: 0.023497731489800583]
	TIME [epoch: 8.84 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06968901637982136		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10471540443040128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08720221040511134 | validation: 0.07452799616479656]
	TIME [epoch: 8.86 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1341621385887231		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12270990543758997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12843602201315654 | validation: 0.08907894959897603]
	TIME [epoch: 8.84 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08237836564285476		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07425519372178607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07831677968232043 | validation: 0.03218662316994595]
	TIME [epoch: 8.83 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06332948819367486		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09607880067456423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07970414443411956 | validation: 0.02861279970048786]
	TIME [epoch: 8.84 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05347060151533574		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06296049886586766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05821555019060169 | validation: 0.04731967164545467]
	TIME [epoch: 8.84 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10137125594627197		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.05923386248869742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08030255921748468 | validation: 0.0705604114831666]
	TIME [epoch: 8.86 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07079656305838096		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0705932350791035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07069489906874224 | validation: 0.026773088818475312]
	TIME [epoch: 8.84 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11595971153936653		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06886705929632497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09241338541784576 | validation: 0.1069788535573863]
	TIME [epoch: 8.84 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13897051828296225		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08508447431119234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11202749629707731 | validation: 0.029942710712269553]
	TIME [epoch: 8.84 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05807118915711327		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06486167218565804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06146643067138565 | validation: 0.10231387090607812]
	TIME [epoch: 8.84 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07331641173162592		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06357832066332435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06844736619747513 | validation: 0.03888537606331262]
	TIME [epoch: 8.87 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1019201281624899		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0718284002702542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08687426421637202 | validation: 0.03330452036033872]
	TIME [epoch: 8.85 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0999830369883343		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08809126650259586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09403715174546506 | validation: 0.050390160429002295]
	TIME [epoch: 8.83 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05157023544403734		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06005549916223128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055812867303134304 | validation: 0.12289731654320785]
	TIME [epoch: 8.82 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054773160750794456		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07113037134147852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06295176604613649 | validation: 0.07344992670169999]
	TIME [epoch: 8.85 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07768925164026777		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0927075298245039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08519839073238583 | validation: 0.05282877725112595]
	TIME [epoch: 8.84 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07128206762518394		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06813927434499581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06971067098508987 | validation: 0.05944331577789286]
	TIME [epoch: 8.83 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11138685957782886		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07312925553140616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09225805755461752 | validation: 0.4744899687585093]
	TIME [epoch: 8.83 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19924906139281007		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08327745591586182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14126325865433595 | validation: 0.09943329793046056]
	TIME [epoch: 8.83 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09053182206853902		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09989964502454748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09521573354654325 | validation: 0.06160948438309277]
	TIME [epoch: 8.85 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0830811235205724		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.051917440505538634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06749928201305552 | validation: 0.21090824072016776]
	TIME [epoch: 8.84 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1177488244903053		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07548265535861039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09661573992445785 | validation: 0.07660956512367642]
	TIME [epoch: 8.82 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07751334018360327		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.05305211169322184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06528272593841256 | validation: 0.07916536146824421]
	TIME [epoch: 8.84 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10260931555783781		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1033207384669322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10296502701238501 | validation: 0.156400472116685]
	TIME [epoch: 8.83 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06964541632204826		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09268010198144291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0811627591517456 | validation: 0.05573352057981069]
	TIME [epoch: 8.86 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09944612219825441		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08050930845585798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08997771532705617 | validation: 0.07707435572153787]
	TIME [epoch: 8.84 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04563092380260285		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07850645857787958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06206869119024121 | validation: 0.01353803469570037]
	TIME [epoch: 8.83 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06445034822613172		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13574874130169096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10009954476391134 | validation: 0.10131213806316751]
	TIME [epoch: 8.82 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08278927360211821		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09939203235314079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09109065297762947 | validation: 0.01225059357693082]
	TIME [epoch: 8.85 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07632276792363685		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.055015974488485685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06566937120606127 | validation: 0.01185781920310066]
	TIME [epoch: 8.85 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053729152317869025		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06815485701087544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06094200466437222 | validation: 0.050668409260340615]
	TIME [epoch: 8.83 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10094729574404475		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0648655233065941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08290640952531943 | validation: 0.06920137010284487]
	TIME [epoch: 8.83 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06846832789564274		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0770660098861686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07276716889090566 | validation: 0.06139003198440872]
	TIME [epoch: 8.83 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13429449746297176		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08007753043914431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10718601395105802 | validation: 0.025963831002347115]
	TIME [epoch: 8.86 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11623557380497636		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1015287517114295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10888216275820291 | validation: 0.00828988934777972]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_461.pth
	Model improved!!!
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05378025655651003		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08972845375298705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07175435515474853 | validation: 0.028836827255055524]
	TIME [epoch: 8.82 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07075218559790492		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06754555600184678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06914887079987585 | validation: 0.09796213431164094]
	TIME [epoch: 8.83 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07827035629575574		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0679880985136753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0731292274047155 | validation: 0.07745990645001952]
	TIME [epoch: 8.83 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06777901262955316		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07411874397242905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07094887830099111 | validation: 0.07344459161556252]
	TIME [epoch: 8.85 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06546777102308454		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13147800025775058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09847288564041755 | validation: 0.14819847800331046]
	TIME [epoch: 8.83 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07870819250445694		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06726937052140161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07298878151292928 | validation: 0.039227143940690386]
	TIME [epoch: 8.83 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05053156353564333		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0966589647618896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07359526414876649 | validation: 0.1933754083513138]
	TIME [epoch: 8.83 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0844885017099386		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11399932191637292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09924391181315576 | validation: 0.0703091453324028]
	TIME [epoch: 8.83 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0722902141355802		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1368572372448071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10457372569019366 | validation: 0.1493362034460048]
	TIME [epoch: 8.86 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08369414986243696		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08169139471242189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08269277228742943 | validation: 0.044321311748411904]
	TIME [epoch: 8.82 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04911554563673605		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06860483273095927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058860189183847646 | validation: 0.023278144705180158]
	TIME [epoch: 8.83 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06890516961315238		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.05862804345609968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06376660653462603 | validation: 0.046386612258352354]
	TIME [epoch: 8.83 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05954001932689842		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15082589466124374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10518295699407107 | validation: 0.20495225191721977]
	TIME [epoch: 8.85 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11436430747321336		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08259753864431874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09848092305876606 | validation: 0.051424433536271484]
	TIME [epoch: 8.83 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15529990604962732		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09883395170914547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12706692887938636 | validation: 0.10533916091138136]
	TIME [epoch: 8.84 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07925812073018491		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08643107691425908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.082844598822222 | validation: 0.06011144212421658]
	TIME [epoch: 8.82 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0763448693618567		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14130253428410072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10882370182297871 | validation: 0.09948869495014412]
	TIME [epoch: 8.82 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14133231685181602		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08691649446458628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11412440565820112 | validation: 0.15882208984911345]
	TIME [epoch: 8.86 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10387409016241902		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09417964585960861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09902686801101382 | validation: 0.15802003961535557]
	TIME [epoch: 8.83 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14212626048297053		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08114085228917793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11163355638607424 | validation: 0.027799299463787445]
	TIME [epoch: 8.83 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053329204792697205		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07732998830529889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06532959654899806 | validation: 0.1418469789897095]
	TIME [epoch: 8.82 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08002392965732236		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.05939136074463711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06970764520097972 | validation: 0.04958601194608549]
	TIME [epoch: 8.83 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06948916430339087		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07668046922701172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07308481676520129 | validation: 0.05186027193875091]
	TIME [epoch: 8.85 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08585872924176934		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09287105631561082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08936489277869006 | validation: 0.08912885195605083]
	TIME [epoch: 8.83 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07079748234624758		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06350399657644086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06715073946134423 | validation: 0.044227732887550594]
	TIME [epoch: 8.83 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057413941103119114		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17248307148622008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11494850629466957 | validation: 0.027276666750078213]
	TIME [epoch: 8.83 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06453065690307266		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09755637725563758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08104351707935513 | validation: 0.10668360002825208]
	TIME [epoch: 8.83 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05535468419310796		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.05743531327036531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056394998731736624 | validation: 0.11730304538946282]
	TIME [epoch: 8.86 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09236136450096512		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07711749349940825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08473942900018668 | validation: 0.06492225740137644]
	TIME [epoch: 8.83 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05430852238824373		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08898425555851884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0716463889733813 | validation: 0.08840357993943314]
	TIME [epoch: 9.34 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11547676425187921		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.061629756699840374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08855326047585978 | validation: 0.06366809178435548]
	TIME [epoch: 8.81 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08918034812066858		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1795420162640257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1343611821923471 | validation: 0.1636578572291767]
	TIME [epoch: 8.86 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12039251067043008		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16022146434162846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14030698750602927 | validation: 0.07564083546496499]
	TIME [epoch: 8.84 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07358037652387295		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07219003385708579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07288520519047936 | validation: 0.052720939347314845]
	TIME [epoch: 8.83 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07466698846219692		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07742457317253647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0760457808173667 | validation: 0.08189572327996004]
	TIME [epoch: 8.83 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10686882553778868		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10527523944337669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10607203249058268 | validation: 0.005693664274724258]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0435418047749109		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08811064714907953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06582622596199522 | validation: 0.04188953386828006]
	TIME [epoch: 8.86 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06629987250460932		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06719802052046527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0667489465125373 | validation: 0.0439735810359969]
	TIME [epoch: 8.83 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07062554976981278		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09465067762915608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08263811369948444 | validation: 0.09966886782296817]
	TIME [epoch: 8.83 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1286478134990773		[learning rate: 0.0099862]
		[batch 20/20] avg loss: 0.08696320959355787		[learning rate: 0.0099709]
	Learning Rate: 0.00997088
	LOSS [training: 0.10780551154631761 | validation: 0.058199137085087047]
	TIME [epoch: 8.84 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07452510253695492		[learning rate: 0.0099556]
		[batch 20/20] avg loss: 0.0705448578206734		[learning rate: 0.0099403]
	Learning Rate: 0.00994031
	LOSS [training: 0.07253498017881417 | validation: 0.06109460705020699]
	TIME [epoch: 8.83 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06556337482577282		[learning rate: 0.0099251]
		[batch 20/20] avg loss: 0.056795762914713145		[learning rate: 0.0099098]
	Learning Rate: 0.00990984
	LOSS [training: 0.061179568870242985 | validation: 0.0716402437148301]
	TIME [epoch: 8.86 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07637703142003369		[learning rate: 0.0098946]
		[batch 20/20] avg loss: 0.06385403176977314		[learning rate: 0.0098795]
	Learning Rate: 0.00987946
	LOSS [training: 0.0701155315949034 | validation: 0.03554968039005646]
	TIME [epoch: 8.84 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051619214502684405		[learning rate: 0.0098643]
		[batch 20/20] avg loss: 0.07413282117572506		[learning rate: 0.0098492]
	Learning Rate: 0.00984918
	LOSS [training: 0.06287601783920474 | validation: 0.03431331094872446]
	TIME [epoch: 8.83 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06365815753995198		[learning rate: 0.0098341]
		[batch 20/20] avg loss: 0.06550991038044783		[learning rate: 0.009819]
	Learning Rate: 0.00981899
	LOSS [training: 0.06458403396019989 | validation: 0.06694832174750813]
	TIME [epoch: 8.83 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06995357630696183		[learning rate: 0.0098039]
		[batch 20/20] avg loss: 0.11071049005316509		[learning rate: 0.0097889]
	Learning Rate: 0.00978889
	LOSS [training: 0.09033203318006347 | validation: 0.07687521794828053]
	TIME [epoch: 8.85 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09340209949783725		[learning rate: 0.0097739]
		[batch 20/20] avg loss: 0.06838902232338928		[learning rate: 0.0097589]
	Learning Rate: 0.00975888
	LOSS [training: 0.08089556091061327 | validation: 0.0496524326673904]
	TIME [epoch: 8.83 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04698644727714279		[learning rate: 0.0097439]
		[batch 20/20] avg loss: 0.08586695754690372		[learning rate: 0.009729]
	Learning Rate: 0.00972897
	LOSS [training: 0.06642670241202327 | validation: 0.04031704073720232]
	TIME [epoch: 8.83 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05697372293223559		[learning rate: 0.009714]
		[batch 20/20] avg loss: 0.05746728134110535		[learning rate: 0.0096991]
	Learning Rate: 0.00969914
	LOSS [training: 0.05722050213667046 | validation: 0.06316975450959927]
	TIME [epoch: 8.83 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10387481885816323		[learning rate: 0.0096843]
		[batch 20/20] avg loss: 0.16718610357530075		[learning rate: 0.0096694]
	Learning Rate: 0.00966941
	LOSS [training: 0.13553046121673198 | validation: 0.15102961111287444]
	TIME [epoch: 8.83 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10290484061281886		[learning rate: 0.0096546]
		[batch 20/20] avg loss: 0.052825491738019724		[learning rate: 0.0096398]
	Learning Rate: 0.00963977
	LOSS [training: 0.07786516617541932 | validation: 0.12555672750802516]
	TIME [epoch: 8.86 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09789884301629459		[learning rate: 0.009625]
		[batch 20/20] avg loss: 0.08021849438516242		[learning rate: 0.0096102]
	Learning Rate: 0.00961022
	LOSS [training: 0.08905866870072848 | validation: 0.131737708603211]
	TIME [epoch: 8.84 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13617059481902702		[learning rate: 0.0095955]
		[batch 20/20] avg loss: 0.06349173066529416		[learning rate: 0.0095808]
	Learning Rate: 0.00958076
	LOSS [training: 0.0998311627421606 | validation: 0.11247456691502496]
	TIME [epoch: 8.83 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12308656712560284		[learning rate: 0.0095661]
		[batch 20/20] avg loss: 0.07836073870822897		[learning rate: 0.0095514]
	Learning Rate: 0.00955139
	LOSS [training: 0.10072365291691592 | validation: 0.048447168588672394]
	TIME [epoch: 8.83 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07872104215408385		[learning rate: 0.0095367]
		[batch 20/20] avg loss: 0.07039028856636069		[learning rate: 0.0095221]
	Learning Rate: 0.00952211
	LOSS [training: 0.07455566536022226 | validation: 0.05184800758225029]
	TIME [epoch: 8.84 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049398596999668504		[learning rate: 0.0095075]
		[batch 20/20] avg loss: 0.06462321934426933		[learning rate: 0.0094929]
	Learning Rate: 0.00949292
	LOSS [training: 0.05701090817196893 | validation: 0.020568090563373692]
	TIME [epoch: 8.86 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07724265317150218		[learning rate: 0.0094784]
		[batch 20/20] avg loss: 0.062183471191583815		[learning rate: 0.0094638]
	Learning Rate: 0.00946382
	LOSS [training: 0.06971306218154301 | validation: 0.07938902027375586]
	TIME [epoch: 16 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0667393625304802		[learning rate: 0.0094493]
		[batch 20/20] avg loss: 0.08799481184404145		[learning rate: 0.0094348]
	Learning Rate: 0.00943481
	LOSS [training: 0.0773670871872608 | validation: 0.0525903739643747]
	TIME [epoch: 8.83 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09819887157778621		[learning rate: 0.0094203]
		[batch 20/20] avg loss: 0.0711553806677946		[learning rate: 0.0094059]
	Learning Rate: 0.00940589
	LOSS [training: 0.08467712612279041 | validation: 0.05853049779428758]
	TIME [epoch: 9.65 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07295500687752171		[learning rate: 0.0093915]
		[batch 20/20] avg loss: 0.07192866077649583		[learning rate: 0.0093771]
	Learning Rate: 0.00937706
	LOSS [training: 0.07244183382700878 | validation: 0.05962420844337798]
	TIME [epoch: 8.84 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05601152920842084		[learning rate: 0.0093627]
		[batch 20/20] avg loss: 0.08206936430370713		[learning rate: 0.0093483]
	Learning Rate: 0.00934831
	LOSS [training: 0.069040446756064 | validation: 0.015503388755329854]
	TIME [epoch: 8.86 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06327024324578041		[learning rate: 0.009334]
		[batch 20/20] avg loss: 0.05628811684511098		[learning rate: 0.0093197]
	Learning Rate: 0.00931966
	LOSS [training: 0.05977918004544569 | validation: 0.04817221328622699]
	TIME [epoch: 8.83 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08101122885509471		[learning rate: 0.0093054]
		[batch 20/20] avg loss: 0.08909597128386261		[learning rate: 0.0092911]
	Learning Rate: 0.00929109
	LOSS [training: 0.08505360006947864 | validation: 0.1108850714933414]
	TIME [epoch: 8.84 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13485178962337713		[learning rate: 0.0092768]
		[batch 20/20] avg loss: 0.06894832283488286		[learning rate: 0.0092626]
	Learning Rate: 0.00926261
	LOSS [training: 0.10190005622912998 | validation: 0.04219504403258831]
	TIME [epoch: 8.84 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08111393895265956		[learning rate: 0.0092484]
		[batch 20/20] avg loss: 0.06815270453505476		[learning rate: 0.0092342]
	Learning Rate: 0.00923422
	LOSS [training: 0.07463332174385715 | validation: 0.16527124304553764]
	TIME [epoch: 8.86 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08716329584277253		[learning rate: 0.0092201]
		[batch 20/20] avg loss: 0.06220606174491862		[learning rate: 0.0092059]
	Learning Rate: 0.00920591
	LOSS [training: 0.07468467879384558 | validation: 0.06140858936999108]
	TIME [epoch: 8.84 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07008018934039861		[learning rate: 0.0091918]
		[batch 20/20] avg loss: 0.20056907437074703		[learning rate: 0.0091777]
	Learning Rate: 0.00917769
	LOSS [training: 0.13532463185557284 | validation: 0.20395103356172703]
	TIME [epoch: 8.83 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12328546151422955		[learning rate: 0.0091636]
		[batch 20/20] avg loss: 0.08178339343772481		[learning rate: 0.0091496]
	Learning Rate: 0.00914956
	LOSS [training: 0.10253442747597716 | validation: 0.23765498169264318]
	TIME [epoch: 8.83 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06765684387358645		[learning rate: 0.0091355]
		[batch 20/20] avg loss: 0.05150564358634262		[learning rate: 0.0091215]
	Learning Rate: 0.00912151
	LOSS [training: 0.059581243729964525 | validation: 0.035471442593468794]
	TIME [epoch: 8.84 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07505566767391125		[learning rate: 0.0091075]
		[batch 20/20] avg loss: 0.1380534264977111		[learning rate: 0.0090935]
	Learning Rate: 0.00909355
	LOSS [training: 0.10655454708581116 | validation: 0.17648433276867287]
	TIME [epoch: 8.86 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1397589684216973		[learning rate: 0.0090796]
		[batch 20/20] avg loss: 0.09550289384576685		[learning rate: 0.0090657]
	Learning Rate: 0.00906567
	LOSS [training: 0.11763093113373209 | validation: 0.08432051307410432]
	TIME [epoch: 8.84 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07839545122788308		[learning rate: 0.0090518]
		[batch 20/20] avg loss: 0.11006380144958727		[learning rate: 0.0090379]
	Learning Rate: 0.00903788
	LOSS [training: 0.09422962633873519 | validation: 0.15761394044883575]
	TIME [epoch: 8.83 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1158555322794009		[learning rate: 0.009024]
		[batch 20/20] avg loss: 0.07343824777381219		[learning rate: 0.0090102]
	Learning Rate: 0.00901018
	LOSS [training: 0.09464689002660656 | validation: 0.12094529245984059]
	TIME [epoch: 8.84 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07116442356730905		[learning rate: 0.0089964]
		[batch 20/20] avg loss: 0.0560923853359334		[learning rate: 0.0089826]
	Learning Rate: 0.00898256
	LOSS [training: 0.06362840445162123 | validation: 0.010462696360517394]
	TIME [epoch: 8.83 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05184819686063482		[learning rate: 0.0089688]
		[batch 20/20] avg loss: 0.053326034146464994		[learning rate: 0.008955]
	Learning Rate: 0.00895502
	LOSS [training: 0.05258711550354991 | validation: 0.030755091840145826]
	TIME [epoch: 8.86 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07683412253806286		[learning rate: 0.0089413]
		[batch 20/20] avg loss: 0.08804932813877372		[learning rate: 0.0089276]
	Learning Rate: 0.00892757
	LOSS [training: 0.0824417253384183 | validation: 0.03620878383690025]
	TIME [epoch: 8.83 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05092356531908521		[learning rate: 0.0089139]
		[batch 20/20] avg loss: 0.12345856082275494		[learning rate: 0.0089002]
	Learning Rate: 0.0089002
	LOSS [training: 0.08719106307092009 | validation: 0.07319222497417255]
	TIME [epoch: 8.83 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06752180357065116		[learning rate: 0.0088866]
		[batch 20/20] avg loss: 0.07447705312937157		[learning rate: 0.0088729]
	Learning Rate: 0.00887292
	LOSS [training: 0.07099942835001136 | validation: 0.09427180683793288]
	TIME [epoch: 8.83 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053652968308271154		[learning rate: 0.0088593]
		[batch 20/20] avg loss: 0.06904798503058547		[learning rate: 0.0088457]
	Learning Rate: 0.00884572
	LOSS [training: 0.06135047666942831 | validation: 0.04568394911806613]
	TIME [epoch: 8.85 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04721122854125598		[learning rate: 0.0088322]
		[batch 20/20] avg loss: 0.05361424100118149		[learning rate: 0.0088186]
	Learning Rate: 0.00881861
	LOSS [training: 0.050412734771218735 | validation: 0.11792355987626499]
	TIME [epoch: 8.85 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0821947676775742		[learning rate: 0.0088051]
		[batch 20/20] avg loss: 0.07761593472858333		[learning rate: 0.0087916]
	Learning Rate: 0.00879157
	LOSS [training: 0.07990535120307878 | validation: 0.13249730217943592]
	TIME [epoch: 8.83 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06064281298083688		[learning rate: 0.0087781]
		[batch 20/20] avg loss: 0.06445538910946781		[learning rate: 0.0087646]
	Learning Rate: 0.00876462
	LOSS [training: 0.06254910104515235 | validation: 0.04582916529843379]
	TIME [epoch: 8.84 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056818568920678145		[learning rate: 0.0087512]
		[batch 20/20] avg loss: 0.14385593486329742		[learning rate: 0.0087378]
	Learning Rate: 0.00873776
	LOSS [training: 0.10033725189198779 | validation: 0.07567562997124261]
	TIME [epoch: 8.83 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08535838485614378		[learning rate: 0.0087244]
		[batch 20/20] avg loss: 0.05935935746791532		[learning rate: 0.008711]
	Learning Rate: 0.00871097
	LOSS [training: 0.07235887116202953 | validation: 0.01565493458368227]
	TIME [epoch: 8.86 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06670643778631985		[learning rate: 0.0086976]
		[batch 20/20] avg loss: 0.07194738773764532		[learning rate: 0.0086843]
	Learning Rate: 0.00868427
	LOSS [training: 0.06932691276198258 | validation: 0.05720569421381623]
	TIME [epoch: 8.84 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041955391270536484		[learning rate: 0.0086709]
		[batch 20/20] avg loss: 0.05622430199259041		[learning rate: 0.0086576]
	Learning Rate: 0.00865765
	LOSS [training: 0.04908984663156345 | validation: 0.03711323915978719]
	TIME [epoch: 8.84 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05782274797713819		[learning rate: 0.0086444]
		[batch 20/20] avg loss: 0.04504482702639938		[learning rate: 0.0086311]
	Learning Rate: 0.00863111
	LOSS [training: 0.05143378750176879 | validation: 0.09166425865913783]
	TIME [epoch: 8.84 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07460155908766522		[learning rate: 0.0086179]
		[batch 20/20] avg loss: 0.07415562726486954		[learning rate: 0.0086047]
	Learning Rate: 0.00860465
	LOSS [training: 0.07437859317626737 | validation: 0.15802613110380553]
	TIME [epoch: 8.84 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10863620062560568		[learning rate: 0.0085915]
		[batch 20/20] avg loss: 0.07735590525846829		[learning rate: 0.0085783]
	Learning Rate: 0.00857828
	LOSS [training: 0.092996052942037 | validation: 0.10697813022457545]
	TIME [epoch: 8.86 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07022440166803166		[learning rate: 0.0085651]
		[batch 20/20] avg loss: 0.06705896560704161		[learning rate: 0.008552]
	Learning Rate: 0.00855198
	LOSS [training: 0.06864168363753664 | validation: 0.026833815632210507]
	TIME [epoch: 8.84 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05874141255260442		[learning rate: 0.0085389]
		[batch 20/20] avg loss: 0.05713734177528448		[learning rate: 0.0085258]
	Learning Rate: 0.00852576
	LOSS [training: 0.05793937716394445 | validation: 0.07366174055994688]
	TIME [epoch: 8.84 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05215318439533807		[learning rate: 0.0085127]
		[batch 20/20] avg loss: 0.09263808922973922		[learning rate: 0.0084996]
	Learning Rate: 0.00849963
	LOSS [training: 0.07239563681253865 | validation: 0.07075046947822844]
	TIME [epoch: 8.84 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10702841950621886		[learning rate: 0.0084866]
		[batch 20/20] avg loss: 0.054203068828024906		[learning rate: 0.0084736]
	Learning Rate: 0.00847358
	LOSS [training: 0.08061574416712189 | validation: 0.07271193793892886]
	TIME [epoch: 8.84 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05354919695480005		[learning rate: 0.0084606]
		[batch 20/20] avg loss: 0.0648015355372089		[learning rate: 0.0084476]
	Learning Rate: 0.0084476
	LOSS [training: 0.05917536624600448 | validation: 0.07527756072287747]
	TIME [epoch: 8.86 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05017631634420118		[learning rate: 0.0084346]
		[batch 20/20] avg loss: 0.04457276500349948		[learning rate: 0.0084217]
	Learning Rate: 0.0084217
	LOSS [training: 0.04737454067385033 | validation: 0.05151349962191423]
	TIME [epoch: 8.83 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05196648137666974		[learning rate: 0.0084088]
		[batch 20/20] avg loss: 0.06189840546832169		[learning rate: 0.0083959]
	Learning Rate: 0.00839589
	LOSS [training: 0.05693244342249572 | validation: 0.06962512381258498]
	TIME [epoch: 8.84 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06611305636681178		[learning rate: 0.008383]
		[batch 20/20] avg loss: 0.05492499997814275		[learning rate: 0.0083702]
	Learning Rate: 0.00837015
	LOSS [training: 0.060519028172477264 | validation: 0.09885209716613691]
	TIME [epoch: 8.84 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0719360297001695		[learning rate: 0.0083573]
		[batch 20/20] avg loss: 0.13117028373222153		[learning rate: 0.0083445]
	Learning Rate: 0.00834449
	LOSS [training: 0.1015531567161955 | validation: 0.06855751229222096]
	TIME [epoch: 8.86 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07702412353220105		[learning rate: 0.0083317]
		[batch 20/20] avg loss: 0.07233444097832421		[learning rate: 0.0083189]
	Learning Rate: 0.00831891
	LOSS [training: 0.07467928225526263 | validation: 0.03710805066776441]
	TIME [epoch: 8.84 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055085987209399036		[learning rate: 0.0083062]
		[batch 20/20] avg loss: 0.06106747402375754		[learning rate: 0.0082934]
	Learning Rate: 0.00829341
	LOSS [training: 0.05807673061657829 | validation: 0.05441278889676601]
	TIME [epoch: 8.84 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08234248608249799		[learning rate: 0.0082807]
		[batch 20/20] avg loss: 0.09245600984421566		[learning rate: 0.008268]
	Learning Rate: 0.00826799
	LOSS [training: 0.08739924796335682 | validation: 0.12823789887773163]
	TIME [epoch: 8.84 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12270877314801729		[learning rate: 0.0082553]
		[batch 20/20] avg loss: 0.0643897061198543		[learning rate: 0.0082426]
	Learning Rate: 0.00824265
	LOSS [training: 0.09354923963393583 | validation: 0.06679732679045172]
	TIME [epoch: 8.84 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07537779571981208		[learning rate: 0.00823]
		[batch 20/20] avg loss: 0.08548403252127533		[learning rate: 0.0082174]
	Learning Rate: 0.00821738
	LOSS [training: 0.0804309141205437 | validation: 0.09054455465378877]
	TIME [epoch: 8.86 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06542955818052601		[learning rate: 0.0082048]
		[batch 20/20] avg loss: 0.06058590989456205		[learning rate: 0.0081922]
	Learning Rate: 0.00819219
	LOSS [training: 0.06300773403754403 | validation: 0.10896304608861458]
	TIME [epoch: 8.84 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08390872646674263		[learning rate: 0.0081796]
		[batch 20/20] avg loss: 0.0712146532488545		[learning rate: 0.0081671]
	Learning Rate: 0.00816708
	LOSS [training: 0.07756168985779857 | validation: 0.07502938926394759]
	TIME [epoch: 8.84 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07761900486168516		[learning rate: 0.0081545]
		[batch 20/20] avg loss: 0.11276484696166626		[learning rate: 0.008142]
	Learning Rate: 0.00814204
	LOSS [training: 0.09519192591167572 | validation: 0.047728709943994495]
	TIME [epoch: 8.84 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07076544284857512		[learning rate: 0.0081296]
		[batch 20/20] avg loss: 0.07772788775705783		[learning rate: 0.0081171]
	Learning Rate: 0.00811708
	LOSS [training: 0.07424666530281647 | validation: 0.0831787572078011]
	TIME [epoch: 8.85 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07641396962674499		[learning rate: 0.0081046]
		[batch 20/20] avg loss: 0.06874409883793672		[learning rate: 0.0080922]
	Learning Rate: 0.0080922
	LOSS [training: 0.07257903423234086 | validation: 0.025408092850510437]
	TIME [epoch: 8.86 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043456666306254346		[learning rate: 0.0080798]
		[batch 20/20] avg loss: 0.14960897316237817		[learning rate: 0.0080674]
	Learning Rate: 0.00806739
	LOSS [training: 0.09653281973431625 | validation: 0.12921063477131517]
	TIME [epoch: 8.84 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11132864194737793		[learning rate: 0.008055]
		[batch 20/20] avg loss: 0.18619630118013822		[learning rate: 0.0080427]
	Learning Rate: 0.00804267
	LOSS [training: 0.14876247156375805 | validation: 0.37970028711439036]
	TIME [epoch: 8.84 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11813611449490999		[learning rate: 0.0080303]
		[batch 20/20] avg loss: 0.06378508244654643		[learning rate: 0.008018]
	Learning Rate: 0.00801801
	LOSS [training: 0.09096059847072821 | validation: 0.009430242357548546]
	TIME [epoch: 8.84 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06104827831238353		[learning rate: 0.0080057]
		[batch 20/20] avg loss: 0.060170109169333005		[learning rate: 0.0079934]
	Learning Rate: 0.00799343
	LOSS [training: 0.060609193740858255 | validation: 0.1027936188942969]
	TIME [epoch: 8.84 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08078495312859812		[learning rate: 0.0079812]
		[batch 20/20] avg loss: 0.04942391171055238		[learning rate: 0.0079689]
	Learning Rate: 0.00796893
	LOSS [training: 0.06510443241957527 | validation: 0.05872039565531012]
	TIME [epoch: 8.86 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10210435143861411		[learning rate: 0.0079567]
		[batch 20/20] avg loss: 0.06524230402020895		[learning rate: 0.0079445]
	Learning Rate: 0.0079445
	LOSS [training: 0.08367332772941152 | validation: 0.03302769404664338]
	TIME [epoch: 8.84 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039202752750946326		[learning rate: 0.0079323]
		[batch 20/20] avg loss: 0.05404553555610486		[learning rate: 0.0079201]
	Learning Rate: 0.00792015
	LOSS [training: 0.046624144153525594 | validation: 0.0628588761107609]
	TIME [epoch: 8.84 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07604131834907979		[learning rate: 0.007908]
		[batch 20/20] avg loss: 0.10594079859342251		[learning rate: 0.0078959]
	Learning Rate: 0.00789587
	LOSS [training: 0.09099105847125116 | validation: 0.08511617618538056]
	TIME [epoch: 8.85 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06609488568769674		[learning rate: 0.0078838]
		[batch 20/20] avg loss: 0.05813054782616169		[learning rate: 0.0078717]
	Learning Rate: 0.00787167
	LOSS [training: 0.06211271675692921 | validation: 0.04483188057918713]
	TIME [epoch: 8.86 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06474373205705443		[learning rate: 0.0078596]
		[batch 20/20] avg loss: 0.040003837985769		[learning rate: 0.0078475]
	Learning Rate: 0.00784754
	LOSS [training: 0.05237378502141171 | validation: 0.02727478919991064]
	TIME [epoch: 8.84 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04526780885761744		[learning rate: 0.0078355]
		[batch 20/20] avg loss: 0.054371478400161787		[learning rate: 0.0078235]
	Learning Rate: 0.00782348
	LOSS [training: 0.04981964362888962 | validation: 0.10074406644488744]
	TIME [epoch: 8.84 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062158135787282985		[learning rate: 0.0078115]
		[batch 20/20] avg loss: 0.058612917002013945		[learning rate: 0.0077995]
	Learning Rate: 0.0077995
	LOSS [training: 0.06038552639464847 | validation: 0.029700438266670633]
	TIME [epoch: 8.83 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05858103642563585		[learning rate: 0.0077875]
		[batch 20/20] avg loss: 0.03440201262735779		[learning rate: 0.0077756]
	Learning Rate: 0.00777559
	LOSS [training: 0.04649152452649682 | validation: 0.026150310418683372]
	TIME [epoch: 8.84 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05091849240203017		[learning rate: 0.0077637]
		[batch 20/20] avg loss: 0.06531696876062262		[learning rate: 0.0077518]
	Learning Rate: 0.00775175
	LOSS [training: 0.058117730581326396 | validation: 0.059348909862207366]
	TIME [epoch: 8.86 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05012158189537667		[learning rate: 0.0077399]
		[batch 20/20] avg loss: 0.037796843356033075		[learning rate: 0.007728]
	Learning Rate: 0.00772799
	LOSS [training: 0.04395921262570486 | validation: 0.005431219306175077]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046802017511698664		[learning rate: 0.0077161]
		[batch 20/20] avg loss: 0.06064256257773955		[learning rate: 0.0077043]
	Learning Rate: 0.0077043
	LOSS [training: 0.05372229004471911 | validation: 0.14114323810802437]
	TIME [epoch: 8.83 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11105228785258565		[learning rate: 0.0076925]
		[batch 20/20] avg loss: 0.07182800388986252		[learning rate: 0.0076807]
	Learning Rate: 0.00768069
	LOSS [training: 0.0914401458712241 | validation: 0.07215787468591515]
	TIME [epoch: 8.83 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04329676583560625		[learning rate: 0.0076689]
		[batch 20/20] avg loss: 0.03879603672120445		[learning rate: 0.0076571]
	Learning Rate: 0.00765714
	LOSS [training: 0.04104640127840536 | validation: 0.10211937757753989]
	TIME [epoch: 8.84 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06542016891091781		[learning rate: 0.0076454]
		[batch 20/20] avg loss: 0.04957169257498661		[learning rate: 0.0076337]
	Learning Rate: 0.00763367
	LOSS [training: 0.05749593074295221 | validation: 0.023461471587747567]
	TIME [epoch: 8.85 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053731886892861104		[learning rate: 0.007622]
		[batch 20/20] avg loss: 0.050604791209466114		[learning rate: 0.0076103]
	Learning Rate: 0.00761027
	LOSS [training: 0.0521683390511636 | validation: 0.05350462919462965]
	TIME [epoch: 8.83 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07506039274379901		[learning rate: 0.0075986]
		[batch 20/20] avg loss: 0.045601769515075476		[learning rate: 0.0075869]
	Learning Rate: 0.00758694
	LOSS [training: 0.06033108112943725 | validation: 0.055232575824064656]
	TIME [epoch: 8.83 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058277304673562846		[learning rate: 0.0075753]
		[batch 20/20] avg loss: 0.06397753935102203		[learning rate: 0.0075637]
	Learning Rate: 0.00756368
	LOSS [training: 0.061127422012292446 | validation: 0.11410246265411517]
	TIME [epoch: 8.83 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0697777916389726		[learning rate: 0.0075521]
		[batch 20/20] avg loss: 0.028528765588062686		[learning rate: 0.0075405]
	Learning Rate: 0.0075405
	LOSS [training: 0.049153278613517655 | validation: 0.026966292904917304]
	TIME [epoch: 8.86 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06153408975036834		[learning rate: 0.0075289]
		[batch 20/20] avg loss: 0.08112840251532819		[learning rate: 0.0075174]
	Learning Rate: 0.00751738
	LOSS [training: 0.07133124613284825 | validation: 0.09446139410021637]
	TIME [epoch: 8.83 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08926867382184427		[learning rate: 0.0075059]
		[batch 20/20] avg loss: 0.07935295850047		[learning rate: 0.0074943]
	Learning Rate: 0.00749434
	LOSS [training: 0.08431081616115713 | validation: 0.02639190591485617]
	TIME [epoch: 8.83 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07202876682026531		[learning rate: 0.0074828]
		[batch 20/20] avg loss: 0.08030096056264385		[learning rate: 0.0074714]
	Learning Rate: 0.00747137
	LOSS [training: 0.07616486369145459 | validation: 0.04519305143582581]
	TIME [epoch: 8.83 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03975497776327737		[learning rate: 0.0074599]
		[batch 20/20] avg loss: 0.07331598638135978		[learning rate: 0.0074485]
	Learning Rate: 0.00744846
	LOSS [training: 0.056535482072318566 | validation: 0.10106165249607793]
	TIME [epoch: 8.82 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.075492403236544		[learning rate: 0.007437]
		[batch 20/20] avg loss: 0.053562810895068924		[learning rate: 0.0074256]
	Learning Rate: 0.00742563
	LOSS [training: 0.06452760706580647 | validation: 0.05272076072686644]
	TIME [epoch: 8.86 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06600443310358216		[learning rate: 0.0074142]
		[batch 20/20] avg loss: 0.07975664294350242		[learning rate: 0.0074029]
	Learning Rate: 0.00740287
	LOSS [training: 0.07288053802354229 | validation: 0.054084063694095005]
	TIME [epoch: 8.83 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04969094512568389		[learning rate: 0.0073915]
		[batch 20/20] avg loss: 0.05584313524091476		[learning rate: 0.0073802]
	Learning Rate: 0.00738017
	LOSS [training: 0.05276704018329932 | validation: 0.019301135801074613]
	TIME [epoch: 8.83 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039985402992662025		[learning rate: 0.0073689]
		[batch 20/20] avg loss: 0.06412768708619952		[learning rate: 0.0073576]
	Learning Rate: 0.00735755
	LOSS [training: 0.05205654503943078 | validation: 0.0690080770514596]
	TIME [epoch: 8.83 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05596093493082752		[learning rate: 0.0073463]
		[batch 20/20] avg loss: 0.08191150717319569		[learning rate: 0.007335]
	Learning Rate: 0.007335
	LOSS [training: 0.06893622105201161 | validation: 0.06088801715010722]
	TIME [epoch: 8.84 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043136931125556374		[learning rate: 0.0073237]
		[batch 20/20] avg loss: 0.0391828806455578		[learning rate: 0.0073125]
	Learning Rate: 0.00731251
	LOSS [training: 0.04115990588555709 | validation: 0.02284494536997957]
	TIME [epoch: 8.85 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08553007497599516		[learning rate: 0.0073013]
		[batch 20/20] avg loss: 0.050119880647798896		[learning rate: 0.0072901]
	Learning Rate: 0.0072901
	LOSS [training: 0.06782497781189703 | validation: 0.02845824551505521]
	TIME [epoch: 8.84 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06580215772586151		[learning rate: 0.0072789]
		[batch 20/20] avg loss: 0.06995054716674906		[learning rate: 0.0072678]
	Learning Rate: 0.00726775
	LOSS [training: 0.06787635244630529 | validation: 0.026581795304851365]
	TIME [epoch: 8.84 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05681262746603477		[learning rate: 0.0072566]
		[batch 20/20] avg loss: 0.039385830927032774		[learning rate: 0.0072455]
	Learning Rate: 0.00724547
	LOSS [training: 0.04809922919653377 | validation: 0.08303612001186106]
	TIME [epoch: 8.83 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05673543831555261		[learning rate: 0.0072344]
		[batch 20/20] avg loss: 0.07901788248576318		[learning rate: 0.0072233]
	Learning Rate: 0.00722326
	LOSS [training: 0.0678766604006579 | validation: 0.09725502599668964]
	TIME [epoch: 8.83 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06654396247348389		[learning rate: 0.0072122]
		[batch 20/20] avg loss: 0.04377912989521114		[learning rate: 0.0072011]
	Learning Rate: 0.00720112
	LOSS [training: 0.0551615461843475 | validation: 0.04669439350512702]
	TIME [epoch: 8.86 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05572120273613675		[learning rate: 0.0071901]
		[batch 20/20] avg loss: 0.05999252435701187		[learning rate: 0.007179]
	Learning Rate: 0.00717904
	LOSS [training: 0.05785686354657432 | validation: 0.03551487398818922]
	TIME [epoch: 8.83 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03897301698525936		[learning rate: 0.007168]
		[batch 20/20] avg loss: 0.045848192070917		[learning rate: 0.007157]
	Learning Rate: 0.00715704
	LOSS [training: 0.042410604528088175 | validation: 0.048936136618733944]
	TIME [epoch: 8.83 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05137532766720173		[learning rate: 0.0071461]
		[batch 20/20] avg loss: 0.05603196308911046		[learning rate: 0.0071351]
	Learning Rate: 0.0071351
	LOSS [training: 0.0537036453781561 | validation: 0.05198256399385702]
	TIME [epoch: 8.83 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05057573209977249		[learning rate: 0.0071242]
		[batch 20/20] avg loss: 0.0630113429337654		[learning rate: 0.0071132]
	Learning Rate: 0.00711323
	LOSS [training: 0.056793537516768945 | validation: 0.030793277306999486]
	TIME [epoch: 8.85 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04478882554257003		[learning rate: 0.0071023]
		[batch 20/20] avg loss: 0.07441694617892791		[learning rate: 0.0070914]
	Learning Rate: 0.00709142
	LOSS [training: 0.059602885860748966 | validation: 0.0558511351370979]
	TIME [epoch: 8.84 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037351229772026144		[learning rate: 0.0070805]
		[batch 20/20] avg loss: 0.0921596732484093		[learning rate: 0.0070697]
	Learning Rate: 0.00706968
	LOSS [training: 0.06475545151021772 | validation: 0.06807151997311063]
	TIME [epoch: 8.84 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07585940706823932		[learning rate: 0.0070588]
		[batch 20/20] avg loss: 0.04248479657222419		[learning rate: 0.007048]
	Learning Rate: 0.00704801
	LOSS [training: 0.05917210182023175 | validation: 0.06162972803199425]
	TIME [epoch: 8.84 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05156318972186613		[learning rate: 0.0070372]
		[batch 20/20] avg loss: 0.04977619195564282		[learning rate: 0.0070264]
	Learning Rate: 0.00702641
	LOSS [training: 0.05066969083875448 | validation: 0.023043064918539514]
	TIME [epoch: 8.84 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05755390247174917		[learning rate: 0.0070156]
		[batch 20/20] avg loss: 0.05921161765138311		[learning rate: 0.0070049]
	Learning Rate: 0.00700487
	LOSS [training: 0.05838276006156614 | validation: 0.042830774331464774]
	TIME [epoch: 11.9 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05124347646244372		[learning rate: 0.0069941]
		[batch 20/20] avg loss: 0.06927550699318327		[learning rate: 0.0069834]
	Learning Rate: 0.0069834
	LOSS [training: 0.060259491727813495 | validation: 0.06349454789320766]
	TIME [epoch: 8.84 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0654904779252817		[learning rate: 0.0069727]
		[batch 20/20] avg loss: 0.07513276340220824		[learning rate: 0.006962]
	Learning Rate: 0.00696199
	LOSS [training: 0.07031162066374498 | validation: 0.15672397153120887]
	TIME [epoch: 8.84 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08999617188923624		[learning rate: 0.0069513]
		[batch 20/20] avg loss: 0.05385011084956608		[learning rate: 0.0069406]
	Learning Rate: 0.00694065
	LOSS [training: 0.07192314136940117 | validation: 0.10325596330489281]
	TIME [epoch: 8.83 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20679570831606903		[learning rate: 0.00693]
		[batch 20/20] avg loss: 0.04996357803361296		[learning rate: 0.0069194]
	Learning Rate: 0.00691937
	LOSS [training: 0.128379643174841 | validation: 0.04465782450494234]
	TIME [epoch: 8.83 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05593887659668341		[learning rate: 0.0069088]
		[batch 20/20] avg loss: 0.06377269306002928		[learning rate: 0.0068982]
	Learning Rate: 0.00689816
	LOSS [training: 0.05985578482835635 | validation: 0.1156896621777161]
	TIME [epoch: 8.85 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05546981559665577		[learning rate: 0.0068876]
		[batch 20/20] avg loss: 0.040942529829281156		[learning rate: 0.006877]
	Learning Rate: 0.00687702
	LOSS [training: 0.048206172712968454 | validation: 0.03497132649948865]
	TIME [epoch: 8.83 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04232139390439961		[learning rate: 0.0068665]
		[batch 20/20] avg loss: 0.06743108051267238		[learning rate: 0.0068559]
	Learning Rate: 0.00685593
	LOSS [training: 0.054876237208536 | validation: 0.0269683812806912]
	TIME [epoch: 8.82 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04529592074264871		[learning rate: 0.0068454]
		[batch 20/20] avg loss: 0.04382542509309403		[learning rate: 0.0068349]
	Learning Rate: 0.00683492
	LOSS [training: 0.04456067291787137 | validation: 0.04809619656304944]
	TIME [epoch: 8.83 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03833880295787659		[learning rate: 0.0068244]
		[batch 20/20] avg loss: 0.04817080026821916		[learning rate: 0.006814]
	Learning Rate: 0.00681397
	LOSS [training: 0.043254801613047865 | validation: 0.03247637895533299]
	TIME [epoch: 8.84 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05203010748474222		[learning rate: 0.0068035]
		[batch 20/20] avg loss: 0.05591909359037585		[learning rate: 0.0067931]
	Learning Rate: 0.00679308
	LOSS [training: 0.053974600537559037 | validation: 0.0586436535354333]
	TIME [epoch: 8.85 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0469243300404002		[learning rate: 0.0067827]
		[batch 20/20] avg loss: 0.03305277738919905		[learning rate: 0.0067723]
	Learning Rate: 0.00677225
	LOSS [training: 0.039988553714799625 | validation: 0.08679479648094063]
	TIME [epoch: 8.84 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08601594598929906		[learning rate: 0.0067619]
		[batch 20/20] avg loss: 0.035640433134852344		[learning rate: 0.0067515]
	Learning Rate: 0.0067515
	LOSS [training: 0.06082818956207571 | validation: 0.04936326285795253]
	TIME [epoch: 8.83 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07865734576866468		[learning rate: 0.0067411]
		[batch 20/20] avg loss: 0.07421570105659471		[learning rate: 0.0067308]
	Learning Rate: 0.0067308
	LOSS [training: 0.07643652341262971 | validation: 0.029440568613108505]
	TIME [epoch: 8.83 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03478832048903697		[learning rate: 0.0067205]
		[batch 20/20] avg loss: 0.05648699299639297		[learning rate: 0.0067102]
	Learning Rate: 0.00671017
	LOSS [training: 0.04563765674271497 | validation: 0.056081532773313486]
	TIME [epoch: 8.87 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05021055054470228		[learning rate: 0.0066999]
		[batch 20/20] avg loss: 0.04829795611240614		[learning rate: 0.0066896]
	Learning Rate: 0.0066896
	LOSS [training: 0.04925425332855421 | validation: 0.03204440125850183]
	TIME [epoch: 8.84 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036833096687494765		[learning rate: 0.0066793]
		[batch 20/20] avg loss: 0.09599278543127629		[learning rate: 0.0066691]
	Learning Rate: 0.00666909
	LOSS [training: 0.06641294105938553 | validation: 0.02979165667072954]
	TIME [epoch: 8.83 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029129639925272666		[learning rate: 0.0066589]
		[batch 20/20] avg loss: 0.04042578047610728		[learning rate: 0.0066486]
	Learning Rate: 0.00664865
	LOSS [training: 0.03477771020068997 | validation: 0.02361952663690713]
	TIME [epoch: 8.84 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05860944493448369		[learning rate: 0.0066384]
		[batch 20/20] avg loss: 0.04721564060316191		[learning rate: 0.0066283]
	Learning Rate: 0.00662827
	LOSS [training: 0.052912542768822804 | validation: 0.0731081138550545]
	TIME [epoch: 8.84 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04003953051357149		[learning rate: 0.0066181]
		[batch 20/20] avg loss: 0.05911775425624929		[learning rate: 0.0066079]
	Learning Rate: 0.00660795
	LOSS [training: 0.04957864238491039 | validation: 0.022472027638714587]
	TIME [epoch: 8.86 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05896738548072966		[learning rate: 0.0065978]
		[batch 20/20] avg loss: 0.042795252014698935		[learning rate: 0.0065877]
	Learning Rate: 0.00658769
	LOSS [training: 0.0508813187477143 | validation: 0.01676035661917607]
	TIME [epoch: 8.83 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04047631848682233		[learning rate: 0.0065776]
		[batch 20/20] avg loss: 0.05312279239705501		[learning rate: 0.0065675]
	Learning Rate: 0.0065675
	LOSS [training: 0.046799555441938666 | validation: 0.04571085985345787]
	TIME [epoch: 8.84 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04438383020275848		[learning rate: 0.0065574]
		[batch 20/20] avg loss: 0.05701939685161389		[learning rate: 0.0065474]
	Learning Rate: 0.00654737
	LOSS [training: 0.050701613527186176 | validation: 0.030564654692985797]
	TIME [epoch: 8.83 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052470353232744606		[learning rate: 0.0065373]
		[batch 20/20] avg loss: 0.056466589154826274		[learning rate: 0.0065273]
	Learning Rate: 0.0065273
	LOSS [training: 0.05446847119378545 | validation: 0.03856221058412872]
	TIME [epoch: 8.84 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0401095284914748		[learning rate: 0.0065173]
		[batch 20/20] avg loss: 0.063033076551067		[learning rate: 0.0065073]
	Learning Rate: 0.00650729
	LOSS [training: 0.0515713025212709 | validation: 0.05162941031681488]
	TIME [epoch: 8.85 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04215039752658397		[learning rate: 0.0064973]
		[batch 20/20] avg loss: 0.050577756181744725		[learning rate: 0.0064873]
	Learning Rate: 0.00648734
	LOSS [training: 0.04636407685416435 | validation: 0.04694708133455577]
	TIME [epoch: 8.82 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04444777647008107		[learning rate: 0.0064774]
		[batch 20/20] avg loss: 0.08444468273165431		[learning rate: 0.0064675]
	Learning Rate: 0.00646745
	LOSS [training: 0.06444622960086768 | validation: 0.04260132012638758]
	TIME [epoch: 8.83 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05298180177320664		[learning rate: 0.0064575]
		[batch 20/20] avg loss: 0.04911208580225056		[learning rate: 0.0064476]
	Learning Rate: 0.00644763
	LOSS [training: 0.0510469437877286 | validation: 0.00888371767918393]
	TIME [epoch: 8.84 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043665855576781515		[learning rate: 0.0064377]
		[batch 20/20] avg loss: 0.051809088137816116		[learning rate: 0.0064279]
	Learning Rate: 0.00642786
	LOSS [training: 0.04773747185729881 | validation: 0.08537751376573532]
	TIME [epoch: 8.86 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09114688661571188		[learning rate: 0.006418]
		[batch 20/20] avg loss: 0.05762872810577725		[learning rate: 0.0064082]
	Learning Rate: 0.00640816
	LOSS [training: 0.07438780736074457 | validation: 0.20965185274938047]
	TIME [epoch: 8.84 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07860537196348363		[learning rate: 0.0063983]
		[batch 20/20] avg loss: 0.05072835960846804		[learning rate: 0.0063885]
	Learning Rate: 0.00638852
	LOSS [training: 0.06466686578597583 | validation: 0.028647082754404186]
	TIME [epoch: 8.83 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04599678225294859		[learning rate: 0.0063787]
		[batch 20/20] avg loss: 0.025223379655407847		[learning rate: 0.0063689]
	Learning Rate: 0.00636893
	LOSS [training: 0.03561008095417822 | validation: 0.021399763260594756]
	TIME [epoch: 8.84 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08529398323531541		[learning rate: 0.0063592]
		[batch 20/20] avg loss: 0.04578838140291617		[learning rate: 0.0063494]
	Learning Rate: 0.00634941
	LOSS [training: 0.06554118231911579 | validation: 0.018611935425643118]
	TIME [epoch: 8.83 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024963175736493332		[learning rate: 0.0063397]
		[batch 20/20] avg loss: 0.04527778697948216		[learning rate: 0.0063299]
	Learning Rate: 0.00632995
	LOSS [training: 0.035120481357987746 | validation: 0.015286735470444731]
	TIME [epoch: 8.87 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04862651939644965		[learning rate: 0.0063202]
		[batch 20/20] avg loss: 0.08863859144129434		[learning rate: 0.0063105]
	Learning Rate: 0.00631054
	LOSS [training: 0.068632555418872 | validation: 0.1925774832183986]
	TIME [epoch: 8.84 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06999015862385036		[learning rate: 0.0063009]
		[batch 20/20] avg loss: 0.11311984271495834		[learning rate: 0.0062912]
	Learning Rate: 0.0062912
	LOSS [training: 0.09155500066940434 | validation: 0.18337223149192594]
	TIME [epoch: 8.84 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06576085646596024		[learning rate: 0.0062815]
		[batch 20/20] avg loss: 0.047283619929315804		[learning rate: 0.0062719]
	Learning Rate: 0.00627191
	LOSS [training: 0.05652223819763802 | validation: 0.07129249494040092]
	TIME [epoch: 8.83 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08936436305851134		[learning rate: 0.0062623]
		[batch 20/20] avg loss: 0.024351698147221424		[learning rate: 0.0062527]
	Learning Rate: 0.00625269
	LOSS [training: 0.056858030602866386 | validation: 0.11570321217981544]
	TIME [epoch: 8.84 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04141932996580672		[learning rate: 0.0062431]
		[batch 20/20] avg loss: 0.05070329788334875		[learning rate: 0.0062335]
	Learning Rate: 0.00623352
	LOSS [training: 0.04606131392457773 | validation: 0.023464482583804562]
	TIME [epoch: 8.86 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020973148924866878		[learning rate: 0.006224]
		[batch 20/20] avg loss: 0.03727808358364225		[learning rate: 0.0062144]
	Learning Rate: 0.00621441
	LOSS [training: 0.029125616254254565 | validation: 0.09986760911033193]
	TIME [epoch: 8.84 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04844732449826511		[learning rate: 0.0062049]
		[batch 20/20] avg loss: 0.03624871774429385		[learning rate: 0.0061954]
	Learning Rate: 0.00619536
	LOSS [training: 0.042348021121279486 | validation: 0.02616707626052718]
	TIME [epoch: 8.84 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04552645350294965		[learning rate: 0.0061859]
		[batch 20/20] avg loss: 0.04991978004237522		[learning rate: 0.0061764]
	Learning Rate: 0.00617637
	LOSS [training: 0.047723116772662444 | validation: 0.03095670065462971]
	TIME [epoch: 8.83 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04082411520092126		[learning rate: 0.0061669]
		[batch 20/20] avg loss: 0.04139570226327842		[learning rate: 0.0061574]
	Learning Rate: 0.00615744
	LOSS [training: 0.04110990873209985 | validation: 0.038281097451682525]
	TIME [epoch: 8.85 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0674344854905298		[learning rate: 0.006148]
		[batch 20/20] avg loss: 0.06276069491880754		[learning rate: 0.0061386]
	Learning Rate: 0.00613856
	LOSS [training: 0.06509759020466867 | validation: 0.1475377192705165]
	TIME [epoch: 8.87 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0888269967024915		[learning rate: 0.0061291]
		[batch 20/20] avg loss: 0.06359579594246564		[learning rate: 0.0061197]
	Learning Rate: 0.00611974
	LOSS [training: 0.07621139632247857 | validation: 0.08747059504491358]
	TIME [epoch: 8.83 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04892290105468105		[learning rate: 0.0061104]
		[batch 20/20] avg loss: 0.06618763749673381		[learning rate: 0.006101]
	Learning Rate: 0.00610099
	LOSS [training: 0.05755526927570744 | validation: 0.02569218944948287]
	TIME [epoch: 8.84 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04637033790462357		[learning rate: 0.0060916]
		[batch 20/20] avg loss: 0.048698919857145105		[learning rate: 0.0060823]
	Learning Rate: 0.00608228
	LOSS [training: 0.04753462888088433 | validation: 0.030959762552645473]
	TIME [epoch: 8.84 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05038233949707298		[learning rate: 0.006073]
		[batch 20/20] avg loss: 0.03129286662846036		[learning rate: 0.0060636]
	Learning Rate: 0.00606364
	LOSS [training: 0.04083760306276667 | validation: 0.003681520792201107]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_663.pth
	Model improved!!!
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03125984268698219		[learning rate: 0.0060543]
		[batch 20/20] avg loss: 0.023051551653618507		[learning rate: 0.0060451]
	Learning Rate: 0.00604505
	LOSS [training: 0.027155697170300353 | validation: 0.05610472476489076]
	TIME [epoch: 8.84 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0490947366247294		[learning rate: 0.0060358]
		[batch 20/20] avg loss: 0.07660560047478693		[learning rate: 0.0060265]
	Learning Rate: 0.00602652
	LOSS [training: 0.06285016854975817 | validation: 0.07329523572964418]
	TIME [epoch: 8.84 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05675631386832473		[learning rate: 0.0060173]
		[batch 20/20] avg loss: 0.029259169504728122		[learning rate: 0.006008]
	Learning Rate: 0.00600805
	LOSS [training: 0.04300774168652642 | validation: 0.04722722078492084]
	TIME [epoch: 8.83 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05752885756046229		[learning rate: 0.0059988]
		[batch 20/20] avg loss: 0.031620941718425034		[learning rate: 0.0059896]
	Learning Rate: 0.00598963
	LOSS [training: 0.04457489963944366 | validation: 0.025238909456387758]
	TIME [epoch: 8.82 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01909436494925659		[learning rate: 0.0059804]
		[batch 20/20] avg loss: 0.022545999137610417		[learning rate: 0.0059713]
	Learning Rate: 0.00597127
	LOSS [training: 0.020820182043433506 | validation: 0.019674813993702415]
	TIME [epoch: 8.85 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052129799733440875		[learning rate: 0.0059621]
		[batch 20/20] avg loss: 0.054955611204807076		[learning rate: 0.005953]
	Learning Rate: 0.00595297
	LOSS [training: 0.053542705469123986 | validation: 0.018141320527842942]
	TIME [epoch: 8.84 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03136502284133831		[learning rate: 0.0059438]
		[batch 20/20] avg loss: 0.022292904464711723		[learning rate: 0.0059347]
	Learning Rate: 0.00593472
	LOSS [training: 0.026828963653025017 | validation: 0.04609061181759393]
	TIME [epoch: 8.83 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04820350928517856		[learning rate: 0.0059256]
		[batch 20/20] avg loss: 0.03660269001444283		[learning rate: 0.0059165]
	Learning Rate: 0.00591652
	LOSS [training: 0.04240309964981069 | validation: 0.019535671051359366]
	TIME [epoch: 8.82 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04970739247933465		[learning rate: 0.0059074]
		[batch 20/20] avg loss: 0.041505764236607935		[learning rate: 0.0058984]
	Learning Rate: 0.00589839
	LOSS [training: 0.045606578357971286 | validation: 0.024274842262085736]
	TIME [epoch: 8.83 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03413204483758236		[learning rate: 0.0058893]
		[batch 20/20] avg loss: 0.03199170968514631		[learning rate: 0.0058803]
	Learning Rate: 0.00588031
	LOSS [training: 0.033061877261364336 | validation: 0.039929128280916215]
	TIME [epoch: 8.85 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031918564353679485		[learning rate: 0.0058713]
		[batch 20/20] avg loss: 0.036765544588576816		[learning rate: 0.0058623]
	Learning Rate: 0.00586228
	LOSS [training: 0.03434205447112815 | validation: 0.027672490641267575]
	TIME [epoch: 8.83 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031443603728823113		[learning rate: 0.0058533]
		[batch 20/20] avg loss: 0.045249978543276725		[learning rate: 0.0058443]
	Learning Rate: 0.00584431
	LOSS [training: 0.03834679113604992 | validation: 0.10328443671148108]
	TIME [epoch: 8.83 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0651406479326436		[learning rate: 0.0058353]
		[batch 20/20] avg loss: 0.029330329809456813		[learning rate: 0.0058264]
	Learning Rate: 0.0058264
	LOSS [training: 0.04723548887105021 | validation: 0.12600185319190987]
	TIME [epoch: 8.82 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06818795532073615		[learning rate: 0.0058175]
		[batch 20/20] avg loss: 0.02341535868220759		[learning rate: 0.0058085]
	Learning Rate: 0.00580854
	LOSS [training: 0.04580165700147187 | validation: 0.026173916938409817]
	TIME [epoch: 8.85 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04539699720045901		[learning rate: 0.0057996]
		[batch 20/20] avg loss: 0.021626114529063923		[learning rate: 0.0057907]
	Learning Rate: 0.00579073
	LOSS [training: 0.03351155586476146 | validation: 0.04171339414136999]
	TIME [epoch: 8.84 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05278861350922244		[learning rate: 0.0057818]
		[batch 20/20] avg loss: 0.03294105032107451		[learning rate: 0.005773]
	Learning Rate: 0.00577298
	LOSS [training: 0.04286483191514846 | validation: 0.028326449078481617]
	TIME [epoch: 8.83 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025884351876424027		[learning rate: 0.0057641]
		[batch 20/20] avg loss: 0.029113998637273562		[learning rate: 0.0057553]
	Learning Rate: 0.00575528
	LOSS [training: 0.027499175256848786 | validation: 0.009471664260463622]
	TIME [epoch: 8.83 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03623641697426762		[learning rate: 0.0057465]
		[batch 20/20] avg loss: 0.059774037060500516		[learning rate: 0.0057376]
	Learning Rate: 0.00573764
	LOSS [training: 0.048005227017384064 | validation: 0.029692989361255178]
	TIME [epoch: 8.83 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02160806710189722		[learning rate: 0.0057288]
		[batch 20/20] avg loss: 0.035119915814457885		[learning rate: 0.0057201]
	Learning Rate: 0.00572005
	LOSS [training: 0.02836399145817755 | validation: 0.006516790177289336]
	TIME [epoch: 8.86 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03060412412563977		[learning rate: 0.0057113]
		[batch 20/20] avg loss: 0.032005841377972785		[learning rate: 0.0057025]
	Learning Rate: 0.00570252
	LOSS [training: 0.03130498275180628 | validation: 0.011421657413184743]
	TIME [epoch: 8.82 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045417626862567506		[learning rate: 0.0056938]
		[batch 20/20] avg loss: 0.030412414023191302		[learning rate: 0.005685]
	Learning Rate: 0.00568504
	LOSS [training: 0.037915020442879414 | validation: 0.015437388944036155]
	TIME [epoch: 8.83 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03834970163986552		[learning rate: 0.0056763]
		[batch 20/20] avg loss: 0.038359193850015266		[learning rate: 0.0056676]
	Learning Rate: 0.00566761
	LOSS [training: 0.0383544477449404 | validation: 0.022425898892309365]
	TIME [epoch: 8.83 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05303555684039689		[learning rate: 0.0056589]
		[batch 20/20] avg loss: 0.026484780276421604		[learning rate: 0.0056502]
	Learning Rate: 0.00565024
	LOSS [training: 0.03976016855840925 | validation: 0.02471250001238349]
	TIME [epoch: 8.83 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04873628473604791		[learning rate: 0.0056416]
		[batch 20/20] avg loss: 0.0687251364333299		[learning rate: 0.0056329]
	Learning Rate: 0.00563292
	LOSS [training: 0.05873071058468889 | validation: 0.03638289264737839]
	TIME [epoch: 8.84 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05905081583116008		[learning rate: 0.0056243]
		[batch 20/20] avg loss: 0.033111109357220905		[learning rate: 0.0056156]
	Learning Rate: 0.00561565
	LOSS [training: 0.046080962594190485 | validation: 0.02557629176005728]
	TIME [epoch: 8.83 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027234560739616276		[learning rate: 0.005607]
		[batch 20/20] avg loss: 0.025728212559021198		[learning rate: 0.0055984]
	Learning Rate: 0.00559843
	LOSS [training: 0.02648138664931874 | validation: 0.01221107360228046]
	TIME [epoch: 8.82 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04341424072758055		[learning rate: 0.0055898]
		[batch 20/20] avg loss: 0.04153562256292913		[learning rate: 0.0055813]
	Learning Rate: 0.00558127
	LOSS [training: 0.042474931645254836 | validation: 0.014502652522771385]
	TIME [epoch: 8.83 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044441854332630225		[learning rate: 0.0055727]
		[batch 20/20] avg loss: 0.06179937657754229		[learning rate: 0.0055642]
	Learning Rate: 0.00556416
	LOSS [training: 0.05312061545508626 | validation: 0.060895505298714035]
	TIME [epoch: 8.83 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05492310146362613		[learning rate: 0.0055556]
		[batch 20/20] avg loss: 0.036058227030928924		[learning rate: 0.0055471]
	Learning Rate: 0.00554711
	LOSS [training: 0.04549066424727753 | validation: 0.0275602500891411]
	TIME [epoch: 8.85 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035597856410977714		[learning rate: 0.0055386]
		[batch 20/20] avg loss: 0.021170224256099397		[learning rate: 0.0055301]
	Learning Rate: 0.0055301
	LOSS [training: 0.028384040333538557 | validation: 0.00766748983160067]
	TIME [epoch: 8.83 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029100845961793763		[learning rate: 0.0055216]
		[batch 20/20] avg loss: 0.030201269130716312		[learning rate: 0.0055132]
	Learning Rate: 0.00551315
	LOSS [training: 0.02965105754625504 | validation: 0.062408967103943155]
	TIME [epoch: 8.83 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04207941905286955		[learning rate: 0.0055047]
		[batch 20/20] avg loss: 0.06488690032429072		[learning rate: 0.0054963]
	Learning Rate: 0.00549625
	LOSS [training: 0.05348315968858014 | validation: 0.051493562420471026]
	TIME [epoch: 8.83 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0662326457820391		[learning rate: 0.0054878]
		[batch 20/20] avg loss: 0.08068997103720069		[learning rate: 0.0054794]
	Learning Rate: 0.0054794
	LOSS [training: 0.0734613084096199 | validation: 0.04763518936439654]
	TIME [epoch: 8.86 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04291576590109276		[learning rate: 0.005471]
		[batch 20/20] avg loss: 0.04886554674793146		[learning rate: 0.0054626]
	Learning Rate: 0.00546261
	LOSS [training: 0.045890656324512114 | validation: 0.028263993316598124]
	TIME [epoch: 8.84 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054562515313627956		[learning rate: 0.0054542]
		[batch 20/20] avg loss: 0.06742073397178705		[learning rate: 0.0054459]
	Learning Rate: 0.00544586
	LOSS [training: 0.060991624642707506 | validation: 0.013829794467809318]
	TIME [epoch: 8.83 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02670088661227869		[learning rate: 0.0054375]
		[batch 20/20] avg loss: 0.04259018137385505		[learning rate: 0.0054292]
	Learning Rate: 0.00542917
	LOSS [training: 0.03464553399306687 | validation: 0.04409837156985256]
	TIME [epoch: 8.83 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03454555599325554		[learning rate: 0.0054208]
		[batch 20/20] avg loss: 0.03502685435836149		[learning rate: 0.0054125]
	Learning Rate: 0.00541253
	LOSS [training: 0.03478620517580851 | validation: 0.03033302919553666]
	TIME [epoch: 8.84 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05175021743995084		[learning rate: 0.0054042]
		[batch 20/20] avg loss: 0.05423518177742316		[learning rate: 0.0053959]
	Learning Rate: 0.00539593
	LOSS [training: 0.052992699608687 | validation: 0.01806761455078375]
	TIME [epoch: 8.85 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09939477571093092		[learning rate: 0.0053877]
		[batch 20/20] avg loss: 0.06035889009970743		[learning rate: 0.0053794]
	Learning Rate: 0.00537939
	LOSS [training: 0.07987683290531919 | validation: 0.05307866347773309]
	TIME [epoch: 8.84 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038554409535137904		[learning rate: 0.0053711]
		[batch 20/20] avg loss: 0.045867744227619925		[learning rate: 0.0053629]
	Learning Rate: 0.0053629
	LOSS [training: 0.042211076881378914 | validation: 0.016587610103565555]
	TIME [epoch: 8.84 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037023563516917435		[learning rate: 0.0053547]
		[batch 20/20] avg loss: 0.0723827442410326		[learning rate: 0.0053465]
	Learning Rate: 0.00534646
	LOSS [training: 0.054703153878974994 | validation: 0.048580284812983245]
	TIME [epoch: 8.83 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09400882167813665		[learning rate: 0.0053383]
		[batch 20/20] avg loss: 0.04115917059940848		[learning rate: 0.0053301]
	Learning Rate: 0.00533008
	LOSS [training: 0.06758399613877258 | validation: 0.04862040063938822]
	TIME [epoch: 8.83 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04397435850165145		[learning rate: 0.0053219]
		[batch 20/20] avg loss: 0.022545638820911013		[learning rate: 0.0053137]
	Learning Rate: 0.00531374
	LOSS [training: 0.03325999866128124 | validation: 0.029740828071803928]
	TIME [epoch: 8.84 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019244919156954757		[learning rate: 0.0053056]
		[batch 20/20] avg loss: 0.042935374012337234		[learning rate: 0.0052974]
	Learning Rate: 0.00529745
	LOSS [training: 0.03109014658464599 | validation: 0.027780017183688437]
	TIME [epoch: 8.83 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03592669486077361		[learning rate: 0.0052893]
		[batch 20/20] avg loss: 0.03039369826137322		[learning rate: 0.0052812]
	Learning Rate: 0.00528121
	LOSS [training: 0.03316019656107341 | validation: 0.02788411867710113]
	TIME [epoch: 8.83 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03376803416550776		[learning rate: 0.0052731]
		[batch 20/20] avg loss: 0.060194398682793956		[learning rate: 0.005265]
	Learning Rate: 0.00526502
	LOSS [training: 0.04698121642415086 | validation: 0.05390656372369053]
	TIME [epoch: 8.83 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031255968663199844		[learning rate: 0.0052569]
		[batch 20/20] avg loss: 0.03464812235938365		[learning rate: 0.0052489]
	Learning Rate: 0.00524888
	LOSS [training: 0.03295204551129175 | validation: 0.0319592857135449]
	TIME [epoch: 8.85 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041180064816021975		[learning rate: 0.0052408]
		[batch 20/20] avg loss: 0.028602023103889436		[learning rate: 0.0052328]
	Learning Rate: 0.00523279
	LOSS [training: 0.0348910439599557 | validation: 0.024684925996485002]
	TIME [epoch: 8.85 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030561530029042862		[learning rate: 0.0052248]
		[batch 20/20] avg loss: 0.03994691586427143		[learning rate: 0.0052167]
	Learning Rate: 0.00521675
	LOSS [training: 0.03525422294665715 | validation: 0.09982146619233734]
	TIME [epoch: 8.83 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03533154759233131		[learning rate: 0.0052087]
		[batch 20/20] avg loss: 0.036397662228306156		[learning rate: 0.0052008]
	Learning Rate: 0.00520076
	LOSS [training: 0.03586460491031873 | validation: 0.023731194540106908]
	TIME [epoch: 8.83 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04142193773498258		[learning rate: 0.0051928]
		[batch 20/20] avg loss: 0.04910164985522297		[learning rate: 0.0051848]
	Learning Rate: 0.00518482
	LOSS [training: 0.04526179379510277 | validation: 0.0989263206444793]
	TIME [epoch: 8.84 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055288823819095924		[learning rate: 0.0051769]
		[batch 20/20] avg loss: 0.03419331243640081		[learning rate: 0.0051689]
	Learning Rate: 0.00516892
	LOSS [training: 0.04474106812774837 | validation: 0.014431558031163078]
	TIME [epoch: 8.86 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01592330189752171		[learning rate: 0.005161]
		[batch 20/20] avg loss: 0.018255617599466797		[learning rate: 0.0051531]
	Learning Rate: 0.00515308
	LOSS [training: 0.017089459748494254 | validation: 0.01102426211392119]
	TIME [epoch: 8.84 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053796091166146455		[learning rate: 0.0051452]
		[batch 20/20] avg loss: 0.024463153856243446		[learning rate: 0.0051373]
	Learning Rate: 0.00513728
	LOSS [training: 0.03912962251119495 | validation: 0.01155634952576531]
	TIME [epoch: 8.84 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02787219001078763		[learning rate: 0.0051294]
		[batch 20/20] avg loss: 0.04268175763943203		[learning rate: 0.0051215]
	Learning Rate: 0.00512153
	LOSS [training: 0.035276973825109824 | validation: 0.08780180133253976]
	TIME [epoch: 8.83 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051798404595275314		[learning rate: 0.0051137]
		[batch 20/20] avg loss: 0.03900764316890804		[learning rate: 0.0051058]
	Learning Rate: 0.00510583
	LOSS [training: 0.04540302388209168 | validation: 0.017029566264514698]
	TIME [epoch: 8.84 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026999417467601584		[learning rate: 0.005098]
		[batch 20/20] avg loss: 0.029523264664880013		[learning rate: 0.0050902]
	Learning Rate: 0.00509018
	LOSS [training: 0.028261341066240793 | validation: 0.11038276887924113]
	TIME [epoch: 8.86 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041425305100762935		[learning rate: 0.0050824]
		[batch 20/20] avg loss: 0.029703268938915655		[learning rate: 0.0050746]
	Learning Rate: 0.00507458
	LOSS [training: 0.03556428701983928 | validation: 0.057787091027683465]
	TIME [epoch: 8.82 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035194584164331015		[learning rate: 0.0050668]
		[batch 20/20] avg loss: 0.02119743745142132		[learning rate: 0.005059]
	Learning Rate: 0.00505902
	LOSS [training: 0.02819601080787617 | validation: 0.009392112862577738]
	TIME [epoch: 8.84 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02867804425046879		[learning rate: 0.0050513]
		[batch 20/20] avg loss: 0.03683547800447497		[learning rate: 0.0050435]
	Learning Rate: 0.00504352
	LOSS [training: 0.03275676112747188 | validation: 0.0652227435357612]
	TIME [epoch: 8.84 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031693590951990265		[learning rate: 0.0050358]
		[batch 20/20] avg loss: 0.04963271255238228		[learning rate: 0.0050281]
	Learning Rate: 0.00502805
	LOSS [training: 0.040663151752186266 | validation: 0.03570102105480295]
	TIME [epoch: 8.84 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04647979676034325		[learning rate: 0.0050203]
		[batch 20/20] avg loss: 0.029015878837961112		[learning rate: 0.0050126]
	Learning Rate: 0.00501264
	LOSS [training: 0.03774783779915218 | validation: 0.015564298631516541]
	TIME [epoch: 8.85 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022945596001658675		[learning rate: 0.005005]
		[batch 20/20] avg loss: 0.022859240317459745		[learning rate: 0.0049973]
	Learning Rate: 0.00499728
	LOSS [training: 0.02290241815955921 | validation: 0.023618736934669267]
	TIME [epoch: 8.84 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04298438928088484		[learning rate: 0.0049896]
		[batch 20/20] avg loss: 0.047015074845244656		[learning rate: 0.004982]
	Learning Rate: 0.00498196
	LOSS [training: 0.04499973206306475 | validation: 0.019013296569830734]
	TIME [epoch: 8.85 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030514925960210167		[learning rate: 0.0049743]
		[batch 20/20] avg loss: 0.029109221663062502		[learning rate: 0.0049667]
	Learning Rate: 0.00496669
	LOSS [training: 0.029812073811636336 | validation: 0.04184195068759026]
	TIME [epoch: 8.83 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.080653568982016		[learning rate: 0.0049591]
		[batch 20/20] avg loss: 0.02064992019002824		[learning rate: 0.0049515]
	Learning Rate: 0.00495146
	LOSS [training: 0.05065174458602212 | validation: 0.036420826514830605]
	TIME [epoch: 8.85 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04704504415218035		[learning rate: 0.0049439]
		[batch 20/20] avg loss: 0.025789926792551272		[learning rate: 0.0049363]
	Learning Rate: 0.00493628
	LOSS [training: 0.036417485472365806 | validation: 0.02039138987081011]
	TIME [epoch: 8.84 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041339133141800606		[learning rate: 0.0049287]
		[batch 20/20] avg loss: 0.027108544146726764		[learning rate: 0.0049211]
	Learning Rate: 0.00492115
	LOSS [training: 0.03422383864426369 | validation: 0.09107531261167326]
	TIME [epoch: 8.83 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032989580824009664		[learning rate: 0.0049136]
		[batch 20/20] avg loss: 0.022716101256395442		[learning rate: 0.0049061]
	Learning Rate: 0.00490607
	LOSS [training: 0.027852841040202553 | validation: 0.05907270984396748]
	TIME [epoch: 8.83 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026497903203161595		[learning rate: 0.0048985]
		[batch 20/20] avg loss: 0.02212919315324224		[learning rate: 0.004891]
	Learning Rate: 0.00489103
	LOSS [training: 0.024313548178201923 | validation: 0.012677275663970049]
	TIME [epoch: 8.83 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019137231813560182		[learning rate: 0.0048835]
		[batch 20/20] avg loss: 0.04081930088620527		[learning rate: 0.004876]
	Learning Rate: 0.00487603
	LOSS [training: 0.029978266349882727 | validation: 0.06842662748581595]
	TIME [epoch: 8.86 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0597064411806306		[learning rate: 0.0048686]
		[batch 20/20] avg loss: 0.08101107883100986		[learning rate: 0.0048611]
	Learning Rate: 0.00486109
	LOSS [training: 0.07035876000582023 | validation: 0.06945783543925844]
	TIME [epoch: 8.83 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06133566109580245		[learning rate: 0.0048536]
		[batch 20/20] avg loss: 0.02860641721573735		[learning rate: 0.0048462]
	Learning Rate: 0.00484618
	LOSS [training: 0.044971039155769894 | validation: 0.019105521992564767]
	TIME [epoch: 8.84 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05614218276349624		[learning rate: 0.0048388]
		[batch 20/20] avg loss: 0.04806590789380881		[learning rate: 0.0048313]
	Learning Rate: 0.00483133
	LOSS [training: 0.052104045328652523 | validation: 0.036421504730843476]
	TIME [epoch: 8.82 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03756630154135893		[learning rate: 0.0048239]
		[batch 20/20] avg loss: 0.03370674621355096		[learning rate: 0.0048165]
	Learning Rate: 0.00481652
	LOSS [training: 0.035636523877454945 | validation: 0.02832023253807569]
	TIME [epoch: 8.83 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04257713951501434		[learning rate: 0.0048091]
		[batch 20/20] avg loss: 0.0342934402883525		[learning rate: 0.0048018]
	Learning Rate: 0.00480176
	LOSS [training: 0.03843528990168343 | validation: 0.06050951261867704]
	TIME [epoch: 8.85 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049033673857826185		[learning rate: 0.0047944]
		[batch 20/20] avg loss: 0.03330537402699642		[learning rate: 0.004787]
	Learning Rate: 0.00478704
	LOSS [training: 0.041169523942411294 | validation: -0.0015026862102340117]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_740.pth
	Model improved!!!
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02803258368338853		[learning rate: 0.0047797]
		[batch 20/20] avg loss: 0.05376302323985679		[learning rate: 0.0047724]
	Learning Rate: 0.00477236
	LOSS [training: 0.04089780346162265 | validation: 0.03291559272896328]
	TIME [epoch: 8.84 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020967300609453933		[learning rate: 0.004765]
		[batch 20/20] avg loss: 0.03999470525853694		[learning rate: 0.0047577]
	Learning Rate: 0.00475773
	LOSS [training: 0.030481002933995437 | validation: 0.05410471731454708]
	TIME [epoch: 8.83 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03351415884642351		[learning rate: 0.0047504]
		[batch 20/20] avg loss: 0.018265092340056112		[learning rate: 0.0047431]
	Learning Rate: 0.00474315
	LOSS [training: 0.025889625593239807 | validation: 0.04746096406310604]
	TIME [epoch: 8.82 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024337428881251982		[learning rate: 0.0047359]
		[batch 20/20] avg loss: 0.021877018342671785		[learning rate: 0.0047286]
	Learning Rate: 0.00472861
	LOSS [training: 0.023107223611961887 | validation: 0.04030732266910553]
	TIME [epoch: 8.86 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053200004023319404		[learning rate: 0.0047214]
		[batch 20/20] avg loss: 0.05897984247971788		[learning rate: 0.0047141]
	Learning Rate: 0.00471411
	LOSS [training: 0.056089923251518646 | validation: 0.05231316102264702]
	TIME [epoch: 8.82 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04027509700692418		[learning rate: 0.0047069]
		[batch 20/20] avg loss: 0.053380422802035275		[learning rate: 0.0046997]
	Learning Rate: 0.00469966
	LOSS [training: 0.046827759904479724 | validation: 0.032923271245999655]
	TIME [epoch: 8.82 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04785428250667119		[learning rate: 0.0046925]
		[batch 20/20] avg loss: 0.04638770863076532		[learning rate: 0.0046853]
	Learning Rate: 0.00468526
	LOSS [training: 0.04712099556871825 | validation: 0.05789684503213222]
	TIME [epoch: 8.81 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06555795329785438		[learning rate: 0.0046781]
		[batch 20/20] avg loss: 0.0729514939252863		[learning rate: 0.0046709]
	Learning Rate: 0.00467089
	LOSS [training: 0.06925472361157035 | validation: 0.09894135180327718]
	TIME [epoch: 8.84 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08718489728514774		[learning rate: 0.0046637]
		[batch 20/20] avg loss: 0.07793063348867842		[learning rate: 0.0046566]
	Learning Rate: 0.00465658
	LOSS [training: 0.08255776538691308 | validation: 0.08449430637074512]
	TIME [epoch: 8.82 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04648550280168123		[learning rate: 0.0046494]
		[batch 20/20] avg loss: 0.043876802385838236		[learning rate: 0.0046423]
	Learning Rate: 0.0046423
	LOSS [training: 0.04518115259375974 | validation: 0.04907231510181667]
	TIME [epoch: 8.83 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10914791557765904		[learning rate: 0.0046352]
		[batch 20/20] avg loss: 0.07347801759659894		[learning rate: 0.0046281]
	Learning Rate: 0.00462807
	LOSS [training: 0.09131296658712898 | validation: 0.04767558395555197]
	TIME [epoch: 8.83 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0585569500150125		[learning rate: 0.004621]
		[batch 20/20] avg loss: 0.11088371737488537		[learning rate: 0.0046139]
	Learning Rate: 0.00461388
	LOSS [training: 0.08472033369494891 | validation: 0.10211660586955468]
	TIME [epoch: 8.82 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13512039804227993		[learning rate: 0.0046068]
		[batch 20/20] avg loss: 0.09626567296711953		[learning rate: 0.0045997]
	Learning Rate: 0.00459974
	LOSS [training: 0.11569303550469974 | validation: 0.04267655937630663]
	TIME [epoch: 8.84 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06084405896326848		[learning rate: 0.0045927]
		[batch 20/20] avg loss: 0.12826840972152642		[learning rate: 0.0045856]
	Learning Rate: 0.00458564
	LOSS [training: 0.09455623434239745 | validation: 0.20525353226793913]
	TIME [epoch: 8.83 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12834071753759127		[learning rate: 0.0045786]
		[batch 20/20] avg loss: 0.13366877277011102		[learning rate: 0.0045716]
	Learning Rate: 0.00457158
	LOSS [training: 0.13100474515385116 | validation: 0.17387583428637723]
	TIME [epoch: 8.83 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10130247414903193		[learning rate: 0.0045646]
		[batch 20/20] avg loss: 0.1321216313129584		[learning rate: 0.0045576]
	Learning Rate: 0.00455757
	LOSS [training: 0.11671205273099516 | validation: 0.21889039815763683]
	TIME [epoch: 8.84 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17305084987452476		[learning rate: 0.0045506]
		[batch 20/20] avg loss: 0.14322784036729713		[learning rate: 0.0045436]
	Learning Rate: 0.0045436
	LOSS [training: 0.15813934512091093 | validation: 0.10286130801361355]
	TIME [epoch: 8.83 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08383588123706116		[learning rate: 0.0045366]
		[batch 20/20] avg loss: 0.06234158149951029		[learning rate: 0.0045297]
	Learning Rate: 0.00452967
	LOSS [training: 0.07308873136828573 | validation: 0.16312480839147692]
	TIME [epoch: 8.85 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08595746471157031		[learning rate: 0.0045227]
		[batch 20/20] avg loss: 0.041960110792200744		[learning rate: 0.0045158]
	Learning Rate: 0.00451579
	LOSS [training: 0.06395878775188552 | validation: 0.050360211524368564]
	TIME [epoch: 8.83 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04824951732260672		[learning rate: 0.0045089]
		[batch 20/20] avg loss: 0.05397987841193493		[learning rate: 0.0045019]
	Learning Rate: 0.00450194
	LOSS [training: 0.05111469786727082 | validation: 0.02429941237641167]
	TIME [epoch: 8.82 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02024037187716769		[learning rate: 0.004495]
		[batch 20/20] avg loss: 0.05736356371399506		[learning rate: 0.0044881]
	Learning Rate: 0.00448814
	LOSS [training: 0.03880196779558137 | validation: 0.01739001034991656]
	TIME [epoch: 8.83 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03545754472239467		[learning rate: 0.0044813]
		[batch 20/20] avg loss: 0.042088656220833236		[learning rate: 0.0044744]
	Learning Rate: 0.00447438
	LOSS [training: 0.03877310047161395 | validation: 0.0020206382844852495]
	TIME [epoch: 8.85 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031214478041561057		[learning rate: 0.0044675]
		[batch 20/20] avg loss: 0.019857960468076596		[learning rate: 0.0044607]
	Learning Rate: 0.00446067
	LOSS [training: 0.02553621925481883 | validation: 0.047266723757008564]
	TIME [epoch: 8.85 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045786914034811915		[learning rate: 0.0044538]
		[batch 20/20] avg loss: 0.06226117814137817		[learning rate: 0.004447]
	Learning Rate: 0.00444699
	LOSS [training: 0.05402404608809504 | validation: 0.022438746887429695]
	TIME [epoch: 8.83 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057604747281806944		[learning rate: 0.0044402]
		[batch 20/20] avg loss: 0.11226809016731125		[learning rate: 0.0044334]
	Learning Rate: 0.00443336
	LOSS [training: 0.0849364187245591 | validation: 0.23521681784898266]
	TIME [epoch: 8.83 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09495531213380966		[learning rate: 0.0044266]
		[batch 20/20] avg loss: 0.0484271584613164		[learning rate: 0.0044198]
	Learning Rate: 0.00441977
	LOSS [training: 0.07169123529756304 | validation: 0.030413948601309148]
	TIME [epoch: 8.83 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030074367246543436		[learning rate: 0.004413]
		[batch 20/20] avg loss: 0.04553195940179612		[learning rate: 0.0044062]
	Learning Rate: 0.00440622
	LOSS [training: 0.037803163324169775 | validation: 0.005187770925401732]
	TIME [epoch: 8.85 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052865287340222736		[learning rate: 0.0043995]
		[batch 20/20] avg loss: 0.02841692270311382		[learning rate: 0.0043927]
	Learning Rate: 0.00439272
	LOSS [training: 0.040641105021668275 | validation: 0.021661781252470334]
	TIME [epoch: 9.1 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03046274287161771		[learning rate: 0.004386]
		[batch 20/20] avg loss: 0.04924078187476291		[learning rate: 0.0043793]
	Learning Rate: 0.00437925
	LOSS [training: 0.03985176237319031 | validation: 0.05930119189933733]
	TIME [epoch: 8.84 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02713236658028968		[learning rate: 0.0043725]
		[batch 20/20] avg loss: 0.02330464795602038		[learning rate: 0.0043658]
	Learning Rate: 0.00436583
	LOSS [training: 0.025218507268155027 | validation: 0.020873455460929773]
	TIME [epoch: 8.83 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024547604661657174		[learning rate: 0.0043591]
		[batch 20/20] avg loss: 0.031212190342313883		[learning rate: 0.0043524]
	Learning Rate: 0.00435245
	LOSS [training: 0.02787989750198553 | validation: 0.08911943118203959]
	TIME [epoch: 8.83 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05329866763870318		[learning rate: 0.0043458]
		[batch 20/20] avg loss: 0.02093701278365392		[learning rate: 0.0043391]
	Learning Rate: 0.0043391
	LOSS [training: 0.037117840211178546 | validation: 0.02394803988546159]
	TIME [epoch: 9.29 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03689036284386724		[learning rate: 0.0043324]
		[batch 20/20] avg loss: 0.015348360424024595		[learning rate: 0.0043258]
	Learning Rate: 0.0043258
	LOSS [training: 0.026119361633945914 | validation: 0.03804149359555252]
	TIME [epoch: 8.83 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023846515955293888		[learning rate: 0.0043192]
		[batch 20/20] avg loss: 0.007574491566256595		[learning rate: 0.0043125]
	Learning Rate: 0.00431254
	LOSS [training: 0.015710503760775242 | validation: 0.011351328837677184]
	TIME [epoch: 9.93 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022206157954817875		[learning rate: 0.0043059]
		[batch 20/20] avg loss: 0.035257495616224564		[learning rate: 0.0042993]
	Learning Rate: 0.00429932
	LOSS [training: 0.028731826785521226 | validation: 0.03666280246287695]
	TIME [epoch: 8.83 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027119116723500803		[learning rate: 0.0042927]
		[batch 20/20] avg loss: 0.022978952729057072		[learning rate: 0.0042861]
	Learning Rate: 0.00428614
	LOSS [training: 0.02504903472627894 | validation: 0.014528704882179465]
	TIME [epoch: 8.83 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024665539223759327		[learning rate: 0.0042796]
		[batch 20/20] avg loss: 0.008981464977038205		[learning rate: 0.004273]
	Learning Rate: 0.004273
	LOSS [training: 0.01682350210039877 | validation: 0.010833460530368861]
	TIME [epoch: 8.85 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020319709003344188		[learning rate: 0.0042664]
		[batch 20/20] avg loss: 0.024999361976612938		[learning rate: 0.0042599]
	Learning Rate: 0.00425991
	LOSS [training: 0.02265953548997857 | validation: 0.02217444935473445]
	TIME [epoch: 8.83 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017858154040587396		[learning rate: 0.0042534]
		[batch 20/20] avg loss: 0.04122005869320235		[learning rate: 0.0042468]
	Learning Rate: 0.00424685
	LOSS [training: 0.029539106366894875 | validation: 0.014659977188664543]
	TIME [epoch: 8.83 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016014501890244365		[learning rate: 0.0042403]
		[batch 20/20] avg loss: 0.03907410184219219		[learning rate: 0.0042338]
	Learning Rate: 0.00423383
	LOSS [training: 0.027544301866218278 | validation: 0.030715045423390232]
	TIME [epoch: 8.83 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030655398755154217		[learning rate: 0.0042273]
		[batch 20/20] avg loss: 0.016946496126442577		[learning rate: 0.0042209]
	Learning Rate: 0.00422085
	LOSS [training: 0.023800947440798404 | validation: 0.04799447059511462]
	TIME [epoch: 8.85 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045093429752828346		[learning rate: 0.0042144]
		[batch 20/20] avg loss: 0.03927887140717614		[learning rate: 0.0042079]
	Learning Rate: 0.00420791
	LOSS [training: 0.04218615058000224 | validation: 0.016811091037734052]
	TIME [epoch: 8.83 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009539213622693091		[learning rate: 0.0042015]
		[batch 20/20] avg loss: 0.024164456850273706		[learning rate: 0.004195]
	Learning Rate: 0.00419501
	LOSS [training: 0.0168518352364834 | validation: 0.015846469095009908]
	TIME [epoch: 8.83 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01388072345059553		[learning rate: 0.0041886]
		[batch 20/20] avg loss: 0.03035307961600837		[learning rate: 0.0041822]
	Learning Rate: 0.00418215
	LOSS [training: 0.02211690153330195 | validation: -0.005933979018374379]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240216_192904/states/model_tr_study2_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01127104245655743		[learning rate: 0.0041757]
		[batch 20/20] avg loss: 0.026628784410759172		[learning rate: 0.0041693]
	Learning Rate: 0.00416933
	LOSS [training: 0.0189499134336583 | validation: 0.02815589681476504]
	TIME [epoch: 8.84 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026585855366532535		[learning rate: 0.0041629]
		[batch 20/20] avg loss: 0.018012311376666073		[learning rate: 0.0041566]
	Learning Rate: 0.00415655
	LOSS [training: 0.022299083371599304 | validation: 0.013248828106892437]
	TIME [epoch: 8.86 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028572357259168308		[learning rate: 0.0041502]
		[batch 20/20] avg loss: 0.036042259564306615		[learning rate: 0.0041438]
	Learning Rate: 0.00414381
	LOSS [training: 0.03230730841173747 | validation: 0.02544979437967635]
	TIME [epoch: 8.83 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026659973874676463		[learning rate: 0.0041375]
		[batch 20/20] avg loss: 0.035041604435940996		[learning rate: 0.0041311]
	Learning Rate: 0.00413111
	LOSS [training: 0.030850789155308735 | validation: 0.01919062827726712]
	TIME [epoch: 8.83 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012669457515481358		[learning rate: 0.0041248]
		[batch 20/20] avg loss: 0.01721491187676868		[learning rate: 0.0041184]
	Learning Rate: 0.00411845
	LOSS [training: 0.014942184696125021 | validation: 0.01948622999232516]
	TIME [epoch: 8.83 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026193682972224068		[learning rate: 0.0041121]
		[batch 20/20] avg loss: 0.015226524796236391		[learning rate: 0.0041058]
	Learning Rate: 0.00410582
	LOSS [training: 0.02071010388423023 | validation: 0.004195618508075379]
	TIME [epoch: 8.83 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04380293200694027		[learning rate: 0.0040995]
		[batch 20/20] avg loss: 0.024610251216034267		[learning rate: 0.0040932]
	Learning Rate: 0.00409323
	LOSS [training: 0.034206591611487265 | validation: 0.03308957573017111]
	TIME [epoch: 8.85 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033493389982014454		[learning rate: 0.004087]
		[batch 20/20] avg loss: 0.02339149859540045		[learning rate: 0.0040807]
	Learning Rate: 0.00408069
	LOSS [training: 0.028442444288707448 | validation: 0.013206973608643962]
	TIME [epoch: 8.83 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01624266759809888		[learning rate: 0.0040744]
		[batch 20/20] avg loss: 0.021960978715632497		[learning rate: 0.0040682]
	Learning Rate: 0.00406818
	LOSS [training: 0.01910182315686569 | validation: 0.01180176720143465]
	TIME [epoch: 8.84 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009972540398677386		[learning rate: 0.0040619]
		[batch 20/20] avg loss: 0.026225535917233124		[learning rate: 0.0040557]
	Learning Rate: 0.00405571
	LOSS [training: 0.01809903815795526 | validation: 0.013499329401541405]
	TIME [epoch: 8.83 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01585463236318801		[learning rate: 0.0040495]
		[batch 20/20] avg loss: 0.020085894147594503		[learning rate: 0.0040433]
	Learning Rate: 0.00404328
	LOSS [training: 0.017970263255391257 | validation: 0.028481700658223388]
	TIME [epoch: 8.84 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023223254547350716		[learning rate: 0.0040371]
		[batch 20/20] avg loss: 0.031734946279716424		[learning rate: 0.0040309]
	Learning Rate: 0.00403088
	LOSS [training: 0.027479100413533568 | validation: 0.02010711826650472]
	TIME [epoch: 8.84 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03069062768794767		[learning rate: 0.0040247]
		[batch 20/20] avg loss: 0.03091178207293115		[learning rate: 0.0040185]
	Learning Rate: 0.00401852
	LOSS [training: 0.030801204880439408 | validation: 0.06871014428783405]
	TIME [epoch: 8.81 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032628171545082484		[learning rate: 0.0040124]
		[batch 20/20] avg loss: 0.02007171966981101		[learning rate: 0.0040062]
	Learning Rate: 0.00400621
	LOSS [training: 0.026349945607446752 | validation: 0.020363484297144803]
	TIME [epoch: 8.83 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011186307814758493		[learning rate: 0.0040001]
		[batch 20/20] avg loss: 0.023342201349848747		[learning rate: 0.0039939]
	Learning Rate: 0.00399393
	LOSS [training: 0.017264254582303618 | validation: 0.004751677627455353]
	TIME [epoch: 8.83 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01729792794183082		[learning rate: 0.0039878]
		[batch 20/20] avg loss: 0.014738986322467354		[learning rate: 0.0039817]
	Learning Rate: 0.00398168
	LOSS [training: 0.016018457132149083 | validation: 0.013486273857470286]
	TIME [epoch: 8.85 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030182739132257226		[learning rate: 0.0039756]
		[batch 20/20] avg loss: 0.02136537327471782		[learning rate: 0.0039695]
	Learning Rate: 0.00396948
	LOSS [training: 0.025774056203487522 | validation: 0.015575447276687297]
	TIME [epoch: 8.84 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021352659408700948		[learning rate: 0.0039634]
		[batch 20/20] avg loss: 0.027218634073444402		[learning rate: 0.0039573]
	Learning Rate: 0.00395731
	LOSS [training: 0.024285646741072676 | validation: 0.05408397212031374]
	TIME [epoch: 8.83 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02319583610332871		[learning rate: 0.0039512]
		[batch 20/20] avg loss: 0.013355535261404431		[learning rate: 0.0039452]
	Learning Rate: 0.00394518
	LOSS [training: 0.018275685682366573 | validation: 0.0052618349746521515]
	TIME [epoch: 8.83 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020037113347690472		[learning rate: 0.0039391]
		[batch 20/20] avg loss: 0.027735695339693354		[learning rate: 0.0039331]
	Learning Rate: 0.00393308
	LOSS [training: 0.023886404343691915 | validation: 0.059120350132305224]
	TIME [epoch: 8.83 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033074040687881265		[learning rate: 0.0039271]
		[batch 20/20] avg loss: 0.032963631366525054		[learning rate: 0.003921]
	Learning Rate: 0.00392103
	LOSS [training: 0.033018836027203166 | validation: 0.002041118480357822]
	TIME [epoch: 8.85 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024352128083309468		[learning rate: 0.003915]
		[batch 20/20] avg loss: 0.03815005023997358		[learning rate: 0.003909]
	Learning Rate: 0.00390901
	LOSS [training: 0.03125108916164153 | validation: 0.022953825890878406]
	TIME [epoch: 8.82 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029106220461968722		[learning rate: 0.003903]
		[batch 20/20] avg loss: nan		[learning rate: 0.003897]
ERROR:
nan encountered in epoch 806 (training loss).
