Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r2', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 184796211

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 10/20] avg loss: 8.388721869102898		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.799245843850743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.093983856476822 | validation: 7.623968218281684]
	TIME [epoch: 79.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 10/20] avg loss: 7.124401184463899		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.075774774581111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.600087979522504 | validation: 5.472605211288904]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.29322132924828		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.079307862445292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.686264595846786 | validation: 4.901995576386016]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.317001816506798		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.814236979952327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.065619398229562 | validation: 4.214682308460498]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.325749217610675		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.180875045782434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.253312131696554 | validation: 3.616463635418958]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.40319594434746		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.44237046777149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.422783206059474 | validation: 3.244701453825024]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_6.pth
	Model improved!!!
EPOCH 7/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.544415640986407		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5982452735239296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0713304572551676 | validation: 3.037594016663679]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_7.pth
	Model improved!!!
EPOCH 8/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.6430269051622823		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.243653799531392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.443340352346837 | validation: 1.6591708917952281]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.211773107059551		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1998722512183964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2058226791389743 | validation: 2.344737953535994]
	TIME [epoch: 8.22 sec]
EPOCH 10/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7661629704371797		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7768658494181888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.771514409927685 | validation: 1.4569182385623798]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_10.pth
	Model improved!!!
EPOCH 11/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7075754749684458		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6352353915764675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6714054332724566 | validation: 2.883397512021206]
	TIME [epoch: 8.19 sec]
EPOCH 12/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7914138048360269		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6263432277875307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7088785163117783 | validation: 1.6338236537988817]
	TIME [epoch: 8.18 sec]
EPOCH 13/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8133402470027362		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9101778636344093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8617590553185728 | validation: 1.407327987219484]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_13.pth
	Model improved!!!
EPOCH 14/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4932814793938438		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5991084121008448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5461949457473443 | validation: 1.544380328649895]
	TIME [epoch: 8.19 sec]
EPOCH 15/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5363307732311116		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5559634244875808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5461470988593462 | validation: 1.4898932444658897]
	TIME [epoch: 8.18 sec]
EPOCH 16/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5407442281299106		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4730503625825924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5068972953562514 | validation: 1.6902813158308687]
	TIME [epoch: 8.19 sec]
EPOCH 17/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7002035654963883		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8690909029353335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.784647234215861 | validation: 2.14948664376995]
	TIME [epoch: 8.2 sec]
EPOCH 18/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5769582872901853		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5142837667812552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5456210270357205 | validation: 1.8735920632376653]
	TIME [epoch: 8.19 sec]
EPOCH 19/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.477399143258988		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4540898537119022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.465744498485445 | validation: 1.3097830130350891]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_19.pth
	Model improved!!!
EPOCH 20/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8137962988548932		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3338112744996817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5738037866772874 | validation: 1.624146681392409]
	TIME [epoch: 8.18 sec]
EPOCH 21/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4952002198744254		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4958907769866028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4955454984305143 | validation: 1.5650412307604615]
	TIME [epoch: 8.19 sec]
EPOCH 22/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5445491932871602		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5363584002470396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5404537967671 | validation: 1.2306789294707523]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_22.pth
	Model improved!!!
EPOCH 23/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6222101884287494		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.439162655117547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5306864217731486 | validation: 1.302863891198171]
	TIME [epoch: 8.19 sec]
EPOCH 24/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5072955083748005		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5226901954523129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.514992851913557 | validation: 1.4073250641506632]
	TIME [epoch: 8.18 sec]
EPOCH 25/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4212632933306957		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4140259626771903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4176446280039428 | validation: 1.614775520172423]
	TIME [epoch: 8.19 sec]
EPOCH 26/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4737554465394422		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4207317881514654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4472436173454541 | validation: 1.362133423560938]
	TIME [epoch: 8.21 sec]
EPOCH 27/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.453575118948325		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.504753220891437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4791641699198812 | validation: 2.527228933725784]
	TIME [epoch: 8.19 sec]
EPOCH 28/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4495553639864966		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4982214213144807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4738883926504887 | validation: 1.2141348363924653]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_28.pth
	Model improved!!!
EPOCH 29/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5316949241764835		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2316676750243927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8816812996004384 | validation: 1.510054596529275]
	TIME [epoch: 8.19 sec]
EPOCH 30/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7208348833610327		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3646522125087617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5427435479348972 | validation: 1.5961627253083708]
	TIME [epoch: 8.21 sec]
EPOCH 31/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3688301874851632		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3522684764578525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3605493319715078 | validation: 1.880935371884359]
	TIME [epoch: 8.2 sec]
EPOCH 32/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.491754913041252		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.548006320111444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.519880616576348 | validation: 1.4588880328887177]
	TIME [epoch: 8.19 sec]
EPOCH 33/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6946684984937808		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.590831818527489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6427501585106346 | validation: 1.2852476583795516]
	TIME [epoch: 8.19 sec]
EPOCH 34/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.533202557332741		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2611072282913605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.397154892812051 | validation: 1.216554160377589]
	TIME [epoch: 8.18 sec]
EPOCH 35/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4525744159484835		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5667444144593072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.509659415203895 | validation: 1.4268966766128148]
	TIME [epoch: 8.22 sec]
EPOCH 36/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3849828453083126		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.337507195306268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3612450203072903 | validation: 1.3530522431793992]
	TIME [epoch: 8.2 sec]
EPOCH 37/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3752302550929683		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4976078869300122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.43641907101149 | validation: 1.0910620704270475]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_37.pth
	Model improved!!!
EPOCH 38/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3138107476505627		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4624425252142816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.388126636432422 | validation: 2.147380010034891]
	TIME [epoch: 8.21 sec]
EPOCH 39/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5451768536050257		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6516359087908312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5984063811979288 | validation: 1.1191407012179724]
	TIME [epoch: 8.23 sec]
EPOCH 40/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3008954822078171		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.312645577649745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3067705299287808 | validation: 1.7391435175592314]
	TIME [epoch: 8.22 sec]
EPOCH 41/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4375606410240898		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5013365299489005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4694485854864952 | validation: 1.3135224497204596]
	TIME [epoch: 8.21 sec]
EPOCH 42/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5077921965942616		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4362810302121374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4720366134031997 | validation: 1.3530078337479599]
	TIME [epoch: 8.2 sec]
EPOCH 43/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4179332493416363		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.411357017218203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4146451332799195 | validation: 1.74522679063599]
	TIME [epoch: 8.22 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5253830689797834		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3632078739153508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.444295471447567 | validation: 1.287244033227639]
	TIME [epoch: 8.22 sec]
EPOCH 45/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.334354695268587		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3582174971958492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3462860962322178 | validation: 1.448690638463592]
	TIME [epoch: 8.21 sec]
EPOCH 46/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3282466925479615		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3860460343441616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3571463634460614 | validation: 1.243689417017434]
	TIME [epoch: 8.2 sec]
EPOCH 47/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3129482993994894		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5478537120742466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4304010057368677 | validation: 1.3469600641825035]
	TIME [epoch: 8.21 sec]
EPOCH 48/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3317455638797113		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4259826670307763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3788641154552435 | validation: 1.4248414246082766]
	TIME [epoch: 8.24 sec]
EPOCH 49/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5447560808382934		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4732781374309725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5090171091346327 | validation: 1.3360099752365124]
	TIME [epoch: 8.21 sec]
EPOCH 50/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4339279710316089		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5735363521129693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.503732161572289 | validation: 1.1834212693998558]
	TIME [epoch: 8.2 sec]
EPOCH 51/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4258103258779724		[learning rate: 0.0099782]
		[batch 20/20] avg loss: 1.280422871353148		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 1.3531165986155602 | validation: 1.8281694632006185]
	TIME [epoch: 8.21 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6270214985023599		[learning rate: 0.00993]
		[batch 20/20] avg loss: 1.3979769920086047		[learning rate: 0.0099059]
	Learning Rate: 0.00990592
	LOSS [training: 1.5124992452554824 | validation: 1.3724867178564968]
	TIME [epoch: 8.23 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.557893732088442		[learning rate: 0.0098819]
		[batch 20/20] avg loss: 1.5031748939247194		[learning rate: 0.009858]
	Learning Rate: 0.00985801
	LOSS [training: 1.5305343130065805 | validation: 1.2598265845495957]
	TIME [epoch: 8.21 sec]
EPOCH 54/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3478476015278102		[learning rate: 0.0098341]
		[batch 20/20] avg loss: 1.3619521432853638		[learning rate: 0.0098103]
	Learning Rate: 0.00981034
	LOSS [training: 1.3548998724065873 | validation: 1.6371101727477624]
	TIME [epoch: 8.2 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.327003563762513		[learning rate: 0.0097866]
		[batch 20/20] avg loss: 1.5525176454233474		[learning rate: 0.0097629]
	Learning Rate: 0.0097629
	LOSS [training: 1.4397606045929303 | validation: 1.7725913062578953]
	TIME [epoch: 8.2 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.554919380987251		[learning rate: 0.0097393]
		[batch 20/20] avg loss: 2.145462871540576		[learning rate: 0.0097157]
	Learning Rate: 0.00971569
	LOSS [training: 1.8501911262639137 | validation: 1.3564006827223338]
	TIME [epoch: 8.21 sec]
EPOCH 57/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3936821278220024		[learning rate: 0.0096922]
		[batch 20/20] avg loss: 1.4241394957013345		[learning rate: 0.0096687]
	Learning Rate: 0.00966871
	LOSS [training: 1.4089108117616684 | validation: 1.1076592889645605]
	TIME [epoch: 8.22 sec]
EPOCH 58/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2739667665523347		[learning rate: 0.0096453]
		[batch 20/20] avg loss: 1.30533474927534		[learning rate: 0.009622]
	Learning Rate: 0.00962195
	LOSS [training: 1.2896507579138372 | validation: 1.2554726539604424]
	TIME [epoch: 8.21 sec]
EPOCH 59/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3192707281621714		[learning rate: 0.0095987]
		[batch 20/20] avg loss: 1.661621615582029		[learning rate: 0.0095754]
	Learning Rate: 0.00957542
	LOSS [training: 1.4904461718721005 | validation: 1.3068604423092656]
	TIME [epoch: 8.2 sec]
EPOCH 60/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3218498013453777		[learning rate: 0.0095522]
		[batch 20/20] avg loss: 1.4741979543909751		[learning rate: 0.0095291]
	Learning Rate: 0.00952912
	LOSS [training: 1.3980238778681764 | validation: 1.4310250656049517]
	TIME [epoch: 8.2 sec]
EPOCH 61/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4342993647860558		[learning rate: 0.009506]
		[batch 20/20] avg loss: 1.337579447171738		[learning rate: 0.009483]
	Learning Rate: 0.00948303
	LOSS [training: 1.3859394059788968 | validation: 1.0436039931891612]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_61.pth
	Model improved!!!
EPOCH 62/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3260867014744409		[learning rate: 0.0094601]
		[batch 20/20] avg loss: 1.6089305420357765		[learning rate: 0.0094372]
	Learning Rate: 0.00943718
	LOSS [training: 1.4675086217551085 | validation: 1.1084136036801042]
	TIME [epoch: 8.2 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3269820477995797		[learning rate: 0.0094143]
		[batch 20/20] avg loss: 1.5073084125559597		[learning rate: 0.0093915]
	Learning Rate: 0.00939154
	LOSS [training: 1.4171452301777694 | validation: 1.4003338682967676]
	TIME [epoch: 8.2 sec]
EPOCH 64/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.472150961421253		[learning rate: 0.0093688]
		[batch 20/20] avg loss: 1.2659647383342492		[learning rate: 0.0093461]
	Learning Rate: 0.00934612
	LOSS [training: 1.3690578498777508 | validation: 1.5230013456216613]
	TIME [epoch: 8.19 sec]
EPOCH 65/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3460259641404233		[learning rate: 0.0093235]
		[batch 20/20] avg loss: 1.4104315235882412		[learning rate: 0.0093009]
	Learning Rate: 0.00930093
	LOSS [training: 1.3782287438643324 | validation: 1.1662693168218774]
	TIME [epoch: 8.22 sec]
EPOCH 66/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4112608883024005		[learning rate: 0.0092784]
		[batch 20/20] avg loss: 1.308673815665703		[learning rate: 0.009256]
	Learning Rate: 0.00925595
	LOSS [training: 1.3599673519840518 | validation: 1.3939159241995454]
	TIME [epoch: 8.2 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.34402647981118		[learning rate: 0.0092335]
		[batch 20/20] avg loss: 1.4143442714665526		[learning rate: 0.0092112]
	Learning Rate: 0.00921119
	LOSS [training: 1.3791853756388663 | validation: 1.477860469401902]
	TIME [epoch: 8.19 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4816550300573788		[learning rate: 0.0091889]
		[batch 20/20] avg loss: 1.2746083540249091		[learning rate: 0.0091666]
	Learning Rate: 0.00916665
	LOSS [training: 1.3781316920411437 | validation: 1.3913126866818861]
	TIME [epoch: 8.19 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6496549086890504		[learning rate: 0.0091445]
		[batch 20/20] avg loss: 1.4112847525651735		[learning rate: 0.0091223]
	Learning Rate: 0.00912232
	LOSS [training: 1.5304698306271118 | validation: 1.1054618234578009]
	TIME [epoch: 8.2 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4122732853550528		[learning rate: 0.0091002]
		[batch 20/20] avg loss: 1.422228279071736		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 1.4172507822133944 | validation: 1.4299497603980955]
	TIME [epoch: 8.22 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4256653377727049		[learning rate: 0.0090562]
		[batch 20/20] avg loss: 1.3403977128163482		[learning rate: 0.0090343]
	Learning Rate: 0.0090343
	LOSS [training: 1.3830315252945264 | validation: 1.1091002382152302]
	TIME [epoch: 8.19 sec]
EPOCH 72/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3750540821805544		[learning rate: 0.0090124]
		[batch 20/20] avg loss: 1.2874541089234743		[learning rate: 0.0089906]
	Learning Rate: 0.00899062
	LOSS [training: 1.3312540955520142 | validation: 1.3918869933673834]
	TIME [epoch: 8.2 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4595210762113322		[learning rate: 0.0089689]
		[batch 20/20] avg loss: 1.6129631483453477		[learning rate: 0.0089471]
	Learning Rate: 0.00894714
	LOSS [training: 1.5362421122783398 | validation: 1.1682848456496753]
	TIME [epoch: 8.19 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5065681769268837		[learning rate: 0.0089255]
		[batch 20/20] avg loss: 1.5543121238399131		[learning rate: 0.0089039]
	Learning Rate: 0.00890387
	LOSS [training: 1.5304401503833984 | validation: 1.2722036848593374]
	TIME [epoch: 8.21 sec]
EPOCH 75/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.232664709984818		[learning rate: 0.0088823]
		[batch 20/20] avg loss: 1.5407704369871702		[learning rate: 0.0088608]
	Learning Rate: 0.00886082
	LOSS [training: 1.3867175734859942 | validation: 1.5184448170293634]
	TIME [epoch: 8.21 sec]
EPOCH 76/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4339102389013865		[learning rate: 0.0088394]
		[batch 20/20] avg loss: 1.3227016108307084		[learning rate: 0.008818]
	Learning Rate: 0.00881797
	LOSS [training: 1.378305924866047 | validation: 1.7233297717690297]
	TIME [epoch: 8.19 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4234992906422959		[learning rate: 0.0087966]
		[batch 20/20] avg loss: 1.32855062797874		[learning rate: 0.0087753]
	Learning Rate: 0.00877532
	LOSS [training: 1.3760249593105176 | validation: 1.0757488236965322]
	TIME [epoch: 8.19 sec]
EPOCH 78/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3466751778911838		[learning rate: 0.0087541]
		[batch 20/20] avg loss: 1.212572588584648		[learning rate: 0.0087329]
	Learning Rate: 0.00873289
	LOSS [training: 1.2796238832379159 | validation: 1.291471836724643]
	TIME [epoch: 8.2 sec]
EPOCH 79/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4180101152073046		[learning rate: 0.0087117]
		[batch 20/20] avg loss: 1.3389689522897779		[learning rate: 0.0086907]
	Learning Rate: 0.00869066
	LOSS [training: 1.378489533748541 | validation: 1.0797225496294063]
	TIME [epoch: 8.22 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1980833595752824		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 1.2904991063226963		[learning rate: 0.0086486]
	Learning Rate: 0.00864863
	LOSS [training: 1.2442912329489892 | validation: 1.1565280249749121]
	TIME [epoch: 8.2 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2701368640454045		[learning rate: 0.0086277]
		[batch 20/20] avg loss: 1.239269042639913		[learning rate: 0.0086068]
	Learning Rate: 0.00860681
	LOSS [training: 1.2547029533426588 | validation: 1.115774791971953]
	TIME [epoch: 8.19 sec]
EPOCH 82/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4459735452226785		[learning rate: 0.008586]
		[batch 20/20] avg loss: 1.2988373933065007		[learning rate: 0.0085652]
	Learning Rate: 0.00856519
	LOSS [training: 1.3724054692645895 | validation: 1.0398705769855736]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_82.pth
	Model improved!!!
EPOCH 83/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2498421575342864		[learning rate: 0.0085445]
		[batch 20/20] avg loss: 1.1916480392640925		[learning rate: 0.0085238]
	Learning Rate: 0.00852377
	LOSS [training: 1.2207450983991897 | validation: 1.0476606872128047]
	TIME [epoch: 8.22 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.37845480668256		[learning rate: 0.0085031]
		[batch 20/20] avg loss: 1.1675714449240844		[learning rate: 0.0084825]
	Learning Rate: 0.00848255
	LOSS [training: 1.2730131258033224 | validation: 1.4983635567593385]
	TIME [epoch: 8.19 sec]
EPOCH 85/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2270850141576701		[learning rate: 0.008462]
		[batch 20/20] avg loss: 1.226283465556184		[learning rate: 0.0084415]
	Learning Rate: 0.00844153
	LOSS [training: 1.2266842398569269 | validation: 1.193101724900043]
	TIME [epoch: 8.18 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1446967765328402		[learning rate: 0.0084211]
		[batch 20/20] avg loss: 1.2338710909523125		[learning rate: 0.0084007]
	Learning Rate: 0.00840071
	LOSS [training: 1.1892839337425762 | validation: 0.9840025135150111]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_86.pth
	Model improved!!!
EPOCH 87/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2837267042427185		[learning rate: 0.0083804]
		[batch 20/20] avg loss: 1.2310304264132248		[learning rate: 0.0083601]
	Learning Rate: 0.00836008
	LOSS [training: 1.2573785653279714 | validation: 1.7857084544106896]
	TIME [epoch: 8.22 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.355859110136219		[learning rate: 0.0083398]
		[batch 20/20] avg loss: 1.2417904570081133		[learning rate: 0.0083197]
	Learning Rate: 0.00831965
	LOSS [training: 1.2988247835721662 | validation: 1.6007738575912944]
	TIME [epoch: 8.21 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4308383937388545		[learning rate: 0.0082995]
		[batch 20/20] avg loss: 1.1379173935526798		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 1.284377893645767 | validation: 1.2778329183808523]
	TIME [epoch: 8.2 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3439808859573668		[learning rate: 0.0082594]
		[batch 20/20] avg loss: 1.2182752722081842		[learning rate: 0.0082394]
	Learning Rate: 0.00823938
	LOSS [training: 1.2811280790827753 | validation: 1.7731856268700539]
	TIME [epoch: 8.19 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5442237435450576		[learning rate: 0.0082194]
		[batch 20/20] avg loss: 1.3874920673840851		[learning rate: 0.0081995]
	Learning Rate: 0.00819954
	LOSS [training: 1.4658579054645713 | validation: 1.337116417925503]
	TIME [epoch: 8.19 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.224991501904201		[learning rate: 0.0081797]
		[batch 20/20] avg loss: 1.2509869472693635		[learning rate: 0.0081599]
	Learning Rate: 0.00815989
	LOSS [training: 1.2379892245867823 | validation: 1.1794762015958822]
	TIME [epoch: 8.23 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4933924890660264		[learning rate: 0.0081401]
		[batch 20/20] avg loss: 1.3363285187505347		[learning rate: 0.0081204]
	Learning Rate: 0.00812043
	LOSS [training: 1.4148605039082807 | validation: 0.9746957405723973]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_93.pth
	Model improved!!!
EPOCH 94/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2935764767750375		[learning rate: 0.0081008]
		[batch 20/20] avg loss: 1.323164165250589		[learning rate: 0.0080812]
	Learning Rate: 0.00808116
	LOSS [training: 1.3083703210128126 | validation: 1.1392199660322297]
	TIME [epoch: 8.2 sec]
EPOCH 95/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1532027911679523		[learning rate: 0.0080616]
		[batch 20/20] avg loss: 1.2706601242241622		[learning rate: 0.0080421]
	Learning Rate: 0.00804208
	LOSS [training: 1.2119314576960571 | validation: 1.1351036342062162]
	TIME [epoch: 8.19 sec]
EPOCH 96/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3579112470060632		[learning rate: 0.0080226]
		[batch 20/20] avg loss: 1.3043288407855345		[learning rate: 0.0080032]
	Learning Rate: 0.00800319
	LOSS [training: 1.3311200438957986 | validation: 1.0539252055358301]
	TIME [epoch: 8.21 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3372294776095872		[learning rate: 0.0079838]
		[batch 20/20] avg loss: 1.2727615430857628		[learning rate: 0.0079645]
	Learning Rate: 0.00796449
	LOSS [training: 1.3049955103476754 | validation: 1.176394411555117]
	TIME [epoch: 8.21 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.314444008101346		[learning rate: 0.0079452]
		[batch 20/20] avg loss: 1.2894021795358885		[learning rate: 0.007926]
	Learning Rate: 0.00792597
	LOSS [training: 1.3019230938186173 | validation: 0.9476646108596358]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_98.pth
	Model improved!!!
EPOCH 99/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3390456989418908		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 1.2679793890003161		[learning rate: 0.0078876]
	Learning Rate: 0.00788765
	LOSS [training: 1.3035125439711033 | validation: 1.2257835766463097]
	TIME [epoch: 8.2 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1991270334994109		[learning rate: 0.0078685]
		[batch 20/20] avg loss: 1.2477738224855783		[learning rate: 0.0078495]
	Learning Rate: 0.0078495
	LOSS [training: 1.2234504279924945 | validation: 1.1810469648196087]
	TIME [epoch: 8.2 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3007397198310051		[learning rate: 0.0078305]
		[batch 20/20] avg loss: 1.2907136431890058		[learning rate: 0.0078115]
	Learning Rate: 0.00781154
	LOSS [training: 1.2957266815100053 | validation: 0.9688613460723022]
	TIME [epoch: 8.22 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4239584995789363		[learning rate: 0.0077926]
		[batch 20/20] avg loss: 1.3021115075944771		[learning rate: 0.0077738]
	Learning Rate: 0.00777377
	LOSS [training: 1.3630350035867065 | validation: 0.9890101908058444]
	TIME [epoch: 8.19 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2872225096958212		[learning rate: 0.0077549]
		[batch 20/20] avg loss: 1.211462602889849		[learning rate: 0.0077362]
	Learning Rate: 0.00773618
	LOSS [training: 1.2493425562928353 | validation: 3.7908143386487585]
	TIME [epoch: 8.19 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.022993878917511		[learning rate: 0.0077174]
		[batch 20/20] avg loss: 1.2233290381615975		[learning rate: 0.0076988]
	Learning Rate: 0.00769876
	LOSS [training: 1.623161458539554 | validation: 2.26915547122486]
	TIME [epoch: 8.19 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.2810950704498354		[learning rate: 0.0076801]
		[batch 20/20] avg loss: 1.7176829885388538		[learning rate: 0.0076615]
	Learning Rate: 0.00766153
	LOSS [training: 1.9993890294943444 | validation: 1.744048366715679]
	TIME [epoch: 8.22 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3592466392332812		[learning rate: 0.007643]
		[batch 20/20] avg loss: 1.4247319991953145		[learning rate: 0.0076245]
	Learning Rate: 0.00762449
	LOSS [training: 1.391989319214298 | validation: 1.3430109898259621]
	TIME [epoch: 8.19 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3622285912804213		[learning rate: 0.007606]
		[batch 20/20] avg loss: 1.3744818831314025		[learning rate: 0.0075876]
	Learning Rate: 0.00758761
	LOSS [training: 1.3683552372059118 | validation: 1.2337799665377618]
	TIME [epoch: 8.19 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1948380788682724		[learning rate: 0.0075692]
		[batch 20/20] avg loss: 1.638546594251461		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 1.4166923365598665 | validation: 2.383931228489]
	TIME [epoch: 8.19 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3271628230894923		[learning rate: 0.0075326]
		[batch 20/20] avg loss: 1.1890769891845732		[learning rate: 0.0075144]
	Learning Rate: 0.00751441
	LOSS [training: 1.2581199061370325 | validation: 0.999541468136518]
	TIME [epoch: 8.21 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2294569415754357		[learning rate: 0.0074962]
		[batch 20/20] avg loss: 1.274295138889547		[learning rate: 0.0074781]
	Learning Rate: 0.00747807
	LOSS [training: 1.2518760402324915 | validation: 1.407254399793839]
	TIME [epoch: 8.2 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3114983173642272		[learning rate: 0.00746]
		[batch 20/20] avg loss: 1.116577183121793		[learning rate: 0.0074419]
	Learning Rate: 0.00744191
	LOSS [training: 1.21403775024301 | validation: 1.2316262136620213]
	TIME [epoch: 8.19 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.118860581638508		[learning rate: 0.0074239]
		[batch 20/20] avg loss: 1.0415578395112899		[learning rate: 0.0074059]
	Learning Rate: 0.00740592
	LOSS [training: 1.080209210574899 | validation: 1.0586906853350238]
	TIME [epoch: 8.18 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2403411530626463		[learning rate: 0.007388]
		[batch 20/20] avg loss: 1.3689443744690528		[learning rate: 0.0073701]
	Learning Rate: 0.00737011
	LOSS [training: 1.3046427637658495 | validation: 1.0500215344644812]
	TIME [epoch: 8.18 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.023014157109389		[learning rate: 0.0073523]
		[batch 20/20] avg loss: 1.1616871416878096		[learning rate: 0.0073345]
	Learning Rate: 0.00733446
	LOSS [training: 1.0923506493985993 | validation: 1.7082148861965294]
	TIME [epoch: 8.21 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1349877514898827		[learning rate: 0.0073167]
		[batch 20/20] avg loss: 1.2199547857713413		[learning rate: 0.007299]
	Learning Rate: 0.007299
	LOSS [training: 1.1774712686306117 | validation: 1.356960691697802]
	TIME [epoch: 8.19 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0418300302488461		[learning rate: 0.0072813]
		[batch 20/20] avg loss: 1.1878779534221033		[learning rate: 0.0072637]
	Learning Rate: 0.0072637
	LOSS [training: 1.1148539918354747 | validation: 0.745272619094655]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_116.pth
	Model improved!!!
EPOCH 117/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1260551675959407		[learning rate: 0.0072461]
		[batch 20/20] avg loss: 0.999621283435825		[learning rate: 0.0072286]
	Learning Rate: 0.00722857
	LOSS [training: 1.062838225515883 | validation: 1.0423748386038767]
	TIME [epoch: 8.19 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3633053750382875		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 1.2258471588294833		[learning rate: 0.0071936]
	Learning Rate: 0.00719362
	LOSS [training: 1.2945762669338854 | validation: 0.982414802594031]
	TIME [epoch: 8.21 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0977937767983243		[learning rate: 0.0071762]
		[batch 20/20] avg loss: 1.3806750101640162		[learning rate: 0.0071588]
	Learning Rate: 0.00715883
	LOSS [training: 1.2392343934811703 | validation: 1.0080725957133505]
	TIME [epoch: 8.19 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0184785790544015		[learning rate: 0.0071415]
		[batch 20/20] avg loss: 1.1757961175673626		[learning rate: 0.0071242]
	Learning Rate: 0.00712421
	LOSS [training: 1.097137348310882 | validation: 1.6535603300578967]
	TIME [epoch: 8.19 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0956301515802815		[learning rate: 0.007107]
		[batch 20/20] avg loss: 1.266377033804199		[learning rate: 0.0070898]
	Learning Rate: 0.00708976
	LOSS [training: 1.1810035926922402 | validation: 1.1300056051617187]
	TIME [epoch: 8.18 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.074424664926939		[learning rate: 0.0070726]
		[batch 20/20] avg loss: 1.0593129292737697		[learning rate: 0.0070555]
	Learning Rate: 0.00705548
	LOSS [training: 1.0668687971003545 | validation: 0.9321800214042614]
	TIME [epoch: 8.19 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0033807308316747		[learning rate: 0.0070384]
		[batch 20/20] avg loss: 1.0847710834200313		[learning rate: 0.0070214]
	Learning Rate: 0.00702136
	LOSS [training: 1.044075907125853 | validation: 0.8353912423817293]
	TIME [epoch: 8.21 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1043603954772268		[learning rate: 0.0070044]
		[batch 20/20] avg loss: 1.100346041739953		[learning rate: 0.0069874]
	Learning Rate: 0.0069874
	LOSS [training: 1.1023532186085896 | validation: 0.8512332369613042]
	TIME [epoch: 8.19 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0906810901248534		[learning rate: 0.0069705]
		[batch 20/20] avg loss: 1.2227376361768374		[learning rate: 0.0069536]
	Learning Rate: 0.00695361
	LOSS [training: 1.1567093631508452 | validation: 0.9754642906734325]
	TIME [epoch: 8.19 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.082178815323471		[learning rate: 0.0069368]
		[batch 20/20] avg loss: 1.0470361064535831		[learning rate: 0.00692]
	Learning Rate: 0.00691999
	LOSS [training: 1.064607460888527 | validation: 2.1955917889729593]
	TIME [epoch: 8.19 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4416368886595003		[learning rate: 0.0069032]
		[batch 20/20] avg loss: 1.2157308200799264		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 1.3286838543697133 | validation: 1.2944010149995924]
	TIME [epoch: 8.22 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.056537719461646		[learning rate: 0.0068699]
		[batch 20/20] avg loss: 1.061759774044047		[learning rate: 0.0068532]
	Learning Rate: 0.00685322
	LOSS [training: 1.0591487467528464 | validation: 2.0102559893537424]
	TIME [epoch: 8.2 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3547750709684396		[learning rate: 0.0068366]
		[batch 20/20] avg loss: 1.1417486769512952		[learning rate: 0.0068201]
	Learning Rate: 0.00682008
	LOSS [training: 1.2482618739598668 | validation: 0.997842492412613]
	TIME [epoch: 8.19 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.084297244758692		[learning rate: 0.0068036]
		[batch 20/20] avg loss: 1.0616313777300614		[learning rate: 0.0067871]
	Learning Rate: 0.0067871
	LOSS [training: 1.072964311244377 | validation: 0.8263716604921492]
	TIME [epoch: 8.18 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1036857955325616		[learning rate: 0.0067707]
		[batch 20/20] avg loss: 1.041609445326736		[learning rate: 0.0067543]
	Learning Rate: 0.00675428
	LOSS [training: 1.0726476204296485 | validation: 1.7822569190796422]
	TIME [epoch: 8.2 sec]
EPOCH 132/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3218140882294356		[learning rate: 0.0067379]
		[batch 20/20] avg loss: 1.206029839170252		[learning rate: 0.0067216]
	Learning Rate: 0.00672162
	LOSS [training: 1.2639219636998436 | validation: 1.48944835651992]
	TIME [epoch: 8.21 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.126503984740177		[learning rate: 0.0067053]
		[batch 20/20] avg loss: 1.3045626640621044		[learning rate: 0.0066891]
	Learning Rate: 0.00668911
	LOSS [training: 1.2155333244011406 | validation: 0.86527515988944]
	TIME [epoch: 8.18 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.028034465791706		[learning rate: 0.0066729]
		[batch 20/20] avg loss: 1.1238335660800154		[learning rate: 0.0066568]
	Learning Rate: 0.00665676
	LOSS [training: 1.0759340159358608 | validation: 1.3609197974650298]
	TIME [epoch: 8.19 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.387662350874269		[learning rate: 0.0066406]
		[batch 20/20] avg loss: 1.1626947008780788		[learning rate: 0.0066246]
	Learning Rate: 0.00662457
	LOSS [training: 1.2751785258761736 | validation: 1.0530024700343734]
	TIME [epoch: 8.18 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1030713497654956		[learning rate: 0.0066085]
		[batch 20/20] avg loss: 1.075998806237656		[learning rate: 0.0065925]
	Learning Rate: 0.00659254
	LOSS [training: 1.0895350780015756 | validation: 0.8822279258608098]
	TIME [epoch: 8.21 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1935670616356755		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 1.261889674092675		[learning rate: 0.0065607]
	Learning Rate: 0.00656066
	LOSS [training: 1.2277283678641753 | validation: 0.9681730530613419]
	TIME [epoch: 8.19 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0536632502075616		[learning rate: 0.0065448]
		[batch 20/20] avg loss: 1.1866923140161467		[learning rate: 0.0065289]
	Learning Rate: 0.00652893
	LOSS [training: 1.1201777821118541 | validation: 0.9601965635737792]
	TIME [epoch: 8.19 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0689596790620421		[learning rate: 0.0065131]
		[batch 20/20] avg loss: 1.1566432352134592		[learning rate: 0.0064974]
	Learning Rate: 0.00649736
	LOSS [training: 1.1128014571377507 | validation: 0.9176333783312004]
	TIME [epoch: 8.19 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0353358383422813		[learning rate: 0.0064816]
		[batch 20/20] avg loss: 1.1203299293283357		[learning rate: 0.0064659]
	Learning Rate: 0.00646594
	LOSS [training: 1.0778328838353086 | validation: 1.1163731629508284]
	TIME [epoch: 8.21 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1054488373172515		[learning rate: 0.0064503]
		[batch 20/20] avg loss: 1.100536655569252		[learning rate: 0.0064347]
	Learning Rate: 0.00643467
	LOSS [training: 1.1029927464432518 | validation: 0.8468179572872592]
	TIME [epoch: 8.2 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2112750909816805		[learning rate: 0.0064191]
		[batch 20/20] avg loss: 1.1575458426126932		[learning rate: 0.0064036]
	Learning Rate: 0.00640355
	LOSS [training: 1.184410466797187 | validation: 0.8113696320016652]
	TIME [epoch: 8.19 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9490227233677739		[learning rate: 0.0063881]
		[batch 20/20] avg loss: 1.0166537944796072		[learning rate: 0.0063726]
	Learning Rate: 0.00637259
	LOSS [training: 0.9828382589236906 | validation: 1.0093943344897478]
	TIME [epoch: 8.19 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1597268443744533		[learning rate: 0.0063572]
		[batch 20/20] avg loss: 0.9990685536511211		[learning rate: 0.0063418]
	Learning Rate: 0.00634177
	LOSS [training: 1.0793976990127874 | validation: 0.9328517977721708]
	TIME [epoch: 8.19 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0625859600736918		[learning rate: 0.0063264]
		[batch 20/20] avg loss: 0.9945125226153954		[learning rate: 0.0063111]
	Learning Rate: 0.0063111
	LOSS [training: 1.0285492413445438 | validation: 1.0219712088986337]
	TIME [epoch: 8.22 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0382036837022697		[learning rate: 0.0062958]
		[batch 20/20] avg loss: 0.967833428831568		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 1.003018556266919 | validation: 0.8486164051669334]
	TIME [epoch: 8.19 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9019656321735023		[learning rate: 0.0062654]
		[batch 20/20] avg loss: 1.0018107510798857		[learning rate: 0.0062502]
	Learning Rate: 0.00625021
	LOSS [training: 0.9518881916266941 | validation: 1.0128094357554867]
	TIME [epoch: 8.19 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.960608109683986		[learning rate: 0.0062351]
		[batch 20/20] avg loss: 1.4543523615021214		[learning rate: 0.00622]
	Learning Rate: 0.00621999
	LOSS [training: 1.2074802355930534 | validation: 0.8858937192095222]
	TIME [epoch: 8.19 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9589255192026599		[learning rate: 0.0062049]
		[batch 20/20] avg loss: 1.3312504811881816		[learning rate: 0.0061899]
	Learning Rate: 0.00618991
	LOSS [training: 1.145088000195421 | validation: 0.9216281723816708]
	TIME [epoch: 8.21 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9924361635994188		[learning rate: 0.0061749]
		[batch 20/20] avg loss: 0.9800168476361755		[learning rate: 0.00616]
	Learning Rate: 0.00615997
	LOSS [training: 0.9862265056177971 | validation: 0.9037054563401483]
	TIME [epoch: 8.2 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9189408760843636		[learning rate: 0.0061451]
		[batch 20/20] avg loss: 1.2177600792755117		[learning rate: 0.0061302]
	Learning Rate: 0.00613019
	LOSS [training: 1.0683504776799377 | validation: 0.7253416426545118]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_151.pth
	Model improved!!!
EPOCH 152/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1882215732105534		[learning rate: 0.0061153]
		[batch 20/20] avg loss: 0.9534491747298686		[learning rate: 0.0061005]
	Learning Rate: 0.00610054
	LOSS [training: 1.0708353739702108 | validation: 1.2197413474712604]
	TIME [epoch: 8.19 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9303682112133511		[learning rate: 0.0060858]
		[batch 20/20] avg loss: 0.8966902415368769		[learning rate: 0.006071]
	Learning Rate: 0.00607104
	LOSS [training: 0.913529226375114 | validation: 0.650326650341974]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_153.pth
	Model improved!!!
EPOCH 154/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9541911473019701		[learning rate: 0.0060563]
		[batch 20/20] avg loss: 0.8805049511803882		[learning rate: 0.0060417]
	Learning Rate: 0.00604168
	LOSS [training: 0.9173480492411791 | validation: 0.8969141711343892]
	TIME [epoch: 8.22 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9060638727120678		[learning rate: 0.0060271]
		[batch 20/20] avg loss: 0.856203674934026		[learning rate: 0.0060125]
	Learning Rate: 0.00601247
	LOSS [training: 0.8811337738230467 | validation: 0.5851065427408283]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_155.pth
	Model improved!!!
EPOCH 156/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8907556433736292		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 1.043222842014142		[learning rate: 0.0059834]
	Learning Rate: 0.00598339
	LOSS [training: 0.9669892426938855 | validation: 0.7283642018990937]
	TIME [epoch: 8.19 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9209687845521838		[learning rate: 0.0059689]
		[batch 20/20] avg loss: 0.9526022556408064		[learning rate: 0.0059545]
	Learning Rate: 0.00595446
	LOSS [training: 0.936785520096495 | validation: 0.8023866289897292]
	TIME [epoch: 8.18 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8942474451335164		[learning rate: 0.00594]
		[batch 20/20] avg loss: 0.9481574226642682		[learning rate: 0.0059257]
	Learning Rate: 0.00592566
	LOSS [training: 0.9212024338988922 | validation: 0.6478307117880994]
	TIME [epoch: 8.21 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9415060918024256		[learning rate: 0.0059113]
		[batch 20/20] avg loss: 1.1013176254616417		[learning rate: 0.005897]
	Learning Rate: 0.00589701
	LOSS [training: 1.0214118586320338 | validation: 0.7100276655149496]
	TIME [epoch: 8.18 sec]
EPOCH 160/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.010947485668702		[learning rate: 0.0058827]
		[batch 20/20] avg loss: 1.1754931726093532		[learning rate: 0.0058685]
	Learning Rate: 0.00586849
	LOSS [training: 1.0932203291390274 | validation: 1.281632919866869]
	TIME [epoch: 8.19 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9725662669599344		[learning rate: 0.0058543]
		[batch 20/20] avg loss: 0.9137878011154296		[learning rate: 0.0058401]
	Learning Rate: 0.00584011
	LOSS [training: 0.9431770340376822 | validation: 0.6664178532986942]
	TIME [epoch: 8.18 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8894962936526225		[learning rate: 0.005826]
		[batch 20/20] avg loss: 0.911780911819846		[learning rate: 0.0058119]
	Learning Rate: 0.00581187
	LOSS [training: 0.9006386027362344 | validation: 0.8835553551493665]
	TIME [epoch: 8.2 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8944047774960812		[learning rate: 0.0057978]
		[batch 20/20] avg loss: 1.2639465800337537		[learning rate: 0.0057838]
	Learning Rate: 0.00578376
	LOSS [training: 1.0791756787649176 | validation: 0.7811097333608276]
	TIME [epoch: 8.2 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8530452185641078		[learning rate: 0.0057698]
		[batch 20/20] avg loss: 0.827962144613324		[learning rate: 0.0057558]
	Learning Rate: 0.00575579
	LOSS [training: 0.8405036815887158 | validation: 1.2832293644957018]
	TIME [epoch: 8.19 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.931931592756378		[learning rate: 0.0057419]
		[batch 20/20] avg loss: 1.3274691682782025		[learning rate: 0.005728]
	Learning Rate: 0.00572796
	LOSS [training: 1.12970038051729 | validation: 1.4745609287562027]
	TIME [epoch: 8.19 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9714879674599779		[learning rate: 0.0057141]
		[batch 20/20] avg loss: 0.8944278823148043		[learning rate: 0.0057003]
	Learning Rate: 0.00570026
	LOSS [training: 0.932957924887391 | validation: 0.9362949373394225]
	TIME [epoch: 8.19 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0417960488390965		[learning rate: 0.0056865]
		[batch 20/20] avg loss: 0.9011725092237196		[learning rate: 0.0056727]
	Learning Rate: 0.0056727
	LOSS [training: 0.971484279031408 | validation: 1.1301559251703348]
	TIME [epoch: 8.22 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8813009769437068		[learning rate: 0.005659]
		[batch 20/20] avg loss: 0.8855754579469325		[learning rate: 0.0056453]
	Learning Rate: 0.00564526
	LOSS [training: 0.8834382174453195 | validation: 1.025119176302786]
	TIME [epoch: 8.19 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8366632880363774		[learning rate: 0.0056316]
		[batch 20/20] avg loss: 1.0403645003532684		[learning rate: 0.005618]
	Learning Rate: 0.00561796
	LOSS [training: 0.9385138941948229 | validation: 0.5696930996620396]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_169.pth
	Model improved!!!
EPOCH 170/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8953094509148543		[learning rate: 0.0056044]
		[batch 20/20] avg loss: 1.309875045184439		[learning rate: 0.0055908]
	Learning Rate: 0.0055908
	LOSS [training: 1.1025922480496466 | validation: 1.0138496811799431]
	TIME [epoch: 8.19 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.27316920354093		[learning rate: 0.0055773]
		[batch 20/20] avg loss: 0.7940915950346673		[learning rate: 0.0055638]
	Learning Rate: 0.00556376
	LOSS [training: 1.0336303992877989 | validation: 0.6457223573477449]
	TIME [epoch: 8.21 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9740538321711363		[learning rate: 0.0055503]
		[batch 20/20] avg loss: 1.0904526326509774		[learning rate: 0.0055369]
	Learning Rate: 0.00553685
	LOSS [training: 1.032253232411057 | validation: 1.2344891529516226]
	TIME [epoch: 8.21 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0530274426339614		[learning rate: 0.0055235]
		[batch 20/20] avg loss: 1.0140448571384244		[learning rate: 0.0055101]
	Learning Rate: 0.00551008
	LOSS [training: 1.0335361498861928 | validation: 1.4960002352111532]
	TIME [epoch: 8.19 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0083357247111882		[learning rate: 0.0054967]
		[batch 20/20] avg loss: 0.8588207221820578		[learning rate: 0.0054834]
	Learning Rate: 0.00548343
	LOSS [training: 0.933578223446623 | validation: 1.041672255393382]
	TIME [epoch: 8.19 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0810808558146434		[learning rate: 0.0054702]
		[batch 20/20] avg loss: 0.8107929164828187		[learning rate: 0.0054569]
	Learning Rate: 0.00545692
	LOSS [training: 0.9459368861487307 | validation: 0.5270414957232817]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_175.pth
	Model improved!!!
EPOCH 176/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9783455109029701		[learning rate: 0.0054437]
		[batch 20/20] avg loss: 0.9625783389509474		[learning rate: 0.0054305]
	Learning Rate: 0.00543053
	LOSS [training: 0.9704619249269587 | validation: 1.1031716561967204]
	TIME [epoch: 8.22 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1114439697691847		[learning rate: 0.0054174]
		[batch 20/20] avg loss: 0.8204897374807223		[learning rate: 0.0054043]
	Learning Rate: 0.00540427
	LOSS [training: 0.9659668536249534 | validation: 0.8519044289987878]
	TIME [epoch: 8.18 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.021493986327278		[learning rate: 0.0053912]
		[batch 20/20] avg loss: 0.9720982912159535		[learning rate: 0.0053781]
	Learning Rate: 0.00537813
	LOSS [training: 0.9967961387716159 | validation: 1.1133768065964567]
	TIME [epoch: 8.2 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9624663164939374		[learning rate: 0.0053651]
		[batch 20/20] avg loss: 0.853911661877871		[learning rate: 0.0053521]
	Learning Rate: 0.00535213
	LOSS [training: 0.9081889891859042 | validation: 1.2188424037730776]
	TIME [epoch: 8.19 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9710160354242194		[learning rate: 0.0053392]
		[batch 20/20] avg loss: 1.3576203010882053		[learning rate: 0.0053262]
	Learning Rate: 0.00532624
	LOSS [training: 1.1643181682562123 | validation: 2.0092485804028923]
	TIME [epoch: 8.22 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0631062580988286		[learning rate: 0.0053133]
		[batch 20/20] avg loss: 0.8637781264842367		[learning rate: 0.0053005]
	Learning Rate: 0.00530049
	LOSS [training: 0.9634421922915326 | validation: 0.6963738571605853]
	TIME [epoch: 8.19 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9035371807369181		[learning rate: 0.0052877]
		[batch 20/20] avg loss: 1.4766180111923226		[learning rate: 0.0052749]
	Learning Rate: 0.00527485
	LOSS [training: 1.1900775959646206 | validation: 0.8378863827736004]
	TIME [epoch: 8.2 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8977299951938512		[learning rate: 0.0052621]
		[batch 20/20] avg loss: 0.9194251598866972		[learning rate: 0.0052493]
	Learning Rate: 0.00524935
	LOSS [training: 0.9085775775402741 | validation: 0.5810243206653477]
	TIME [epoch: 8.19 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8346927460417215		[learning rate: 0.0052366]
		[batch 20/20] avg loss: 1.160268493906095		[learning rate: 0.005224]
	Learning Rate: 0.00522396
	LOSS [training: 0.9974806199739084 | validation: 2.047951326471688]
	TIME [epoch: 8.22 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2234375734078784		[learning rate: 0.0052113]
		[batch 20/20] avg loss: 1.1570468417566373		[learning rate: 0.0051987]
	Learning Rate: 0.0051987
	LOSS [training: 1.1902422075822578 | validation: 0.9056921944927915]
	TIME [epoch: 8.21 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.06068416637575		[learning rate: 0.0051861]
		[batch 20/20] avg loss: 1.0568199951619974		[learning rate: 0.0051736]
	Learning Rate: 0.00517356
	LOSS [training: 1.058752080768874 | validation: 1.2935830436695883]
	TIME [epoch: 8.19 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0682643600936905		[learning rate: 0.005161]
		[batch 20/20] avg loss: 1.1447421902222525		[learning rate: 0.0051485]
	Learning Rate: 0.00514854
	LOSS [training: 1.1065032751579715 | validation: 3.205429937064194]
	TIME [epoch: 8.19 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3356068136470822		[learning rate: 0.0051361]
		[batch 20/20] avg loss: 0.949306553760127		[learning rate: 0.0051236]
	Learning Rate: 0.00512364
	LOSS [training: 1.1424566837036045 | validation: 0.6370865027936942]
	TIME [epoch: 8.19 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9174514085183689		[learning rate: 0.0051112]
		[batch 20/20] avg loss: 0.8222653349333762		[learning rate: 0.0050989]
	Learning Rate: 0.00509887
	LOSS [training: 0.8698583717258724 | validation: 1.4470841292699668]
	TIME [epoch: 8.22 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1160854401397793		[learning rate: 0.0050865]
		[batch 20/20] avg loss: 1.0414311757722299		[learning rate: 0.0050742]
	Learning Rate: 0.00507421
	LOSS [training: 1.0787583079560046 | validation: 1.2068241602125207]
	TIME [epoch: 8.19 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.869801491070971		[learning rate: 0.0050619]
		[batch 20/20] avg loss: 0.8596631144520881		[learning rate: 0.0050497]
	Learning Rate: 0.00504967
	LOSS [training: 0.8647323027615297 | validation: 0.9694659016307499]
	TIME [epoch: 8.19 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2247367968986511		[learning rate: 0.0050374]
		[batch 20/20] avg loss: 1.3697022858698604		[learning rate: 0.0050253]
	Learning Rate: 0.00502525
	LOSS [training: 1.2972195413842555 | validation: 0.6520578053153843]
	TIME [epoch: 8.19 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.200103475653688		[learning rate: 0.0050131]
		[batch 20/20] avg loss: 0.8745440602976572		[learning rate: 0.005001]
	Learning Rate: 0.00500095
	LOSS [training: 1.0373237679756726 | validation: 0.7219865965941538]
	TIME [epoch: 8.21 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.922398232890731		[learning rate: 0.0049888]
		[batch 20/20] avg loss: 1.1935610506941932		[learning rate: 0.0049768]
	Learning Rate: 0.00497677
	LOSS [training: 1.0579796417924623 | validation: 0.6785335969869647]
	TIME [epoch: 8.2 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0356245461537037		[learning rate: 0.0049647]
		[batch 20/20] avg loss: 0.9632817777237463		[learning rate: 0.0049527]
	Learning Rate: 0.0049527
	LOSS [training: 0.9994531619387251 | validation: 0.7924718149303155]
	TIME [epoch: 8.19 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.813444675046959		[learning rate: 0.0049407]
		[batch 20/20] avg loss: 0.9077526531784141		[learning rate: 0.0049288]
	Learning Rate: 0.00492875
	LOSS [training: 0.8605986641126867 | validation: 1.845628693829712]
	TIME [epoch: 8.19 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0684368772290849		[learning rate: 0.0049168]
		[batch 20/20] avg loss: 1.157918482382374		[learning rate: 0.0049049]
	Learning Rate: 0.00490492
	LOSS [training: 1.1131776798057296 | validation: 1.2031398333255823]
	TIME [epoch: 8.2 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8395910473137569		[learning rate: 0.004893]
		[batch 20/20] avg loss: 0.7970204677886591		[learning rate: 0.0048812]
	Learning Rate: 0.0048812
	LOSS [training: 0.818305757551208 | validation: 0.775738052461101]
	TIME [epoch: 8.23 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8861221468687248		[learning rate: 0.0048694]
		[batch 20/20] avg loss: 1.1137662835366988		[learning rate: 0.0048576]
	Learning Rate: 0.00485759
	LOSS [training: 0.9999442152027121 | validation: 0.9381210504018073]
	TIME [epoch: 8.19 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1299790503940128		[learning rate: 0.0048458]
		[batch 20/20] avg loss: 1.2076998185006595		[learning rate: 0.0048341]
	Learning Rate: 0.0048341
	LOSS [training: 1.1688394344473363 | validation: 0.6462397844898393]
	TIME [epoch: 8.2 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7736911047897734		[learning rate: 0.0048224]
		[batch 20/20] avg loss: 0.9118316370692281		[learning rate: 0.0048107]
	Learning Rate: 0.00481072
	LOSS [training: 0.8427613709295008 | validation: 0.546804932467239]
	TIME [epoch: 8.19 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8921305198176779		[learning rate: 0.0047991]
		[batch 20/20] avg loss: 1.230278080082146		[learning rate: 0.0047875]
	Learning Rate: 0.00478746
	LOSS [training: 1.0612042999499118 | validation: 0.7717760252906487]
	TIME [epoch: 8.22 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8344170180505863		[learning rate: 0.0047759]
		[batch 20/20] avg loss: 0.9493473922127549		[learning rate: 0.0047643]
	Learning Rate: 0.00476431
	LOSS [training: 0.8918822051316706 | validation: 1.5767828355469589]
	TIME [epoch: 8.2 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1486531794453325		[learning rate: 0.0047528]
		[batch 20/20] avg loss: 0.9373096003471433		[learning rate: 0.0047413]
	Learning Rate: 0.00474127
	LOSS [training: 1.0429813898962377 | validation: 0.9510291751811212]
	TIME [epoch: 8.2 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1150439217291017		[learning rate: 0.0047298]
		[batch 20/20] avg loss: 0.8176983004272147		[learning rate: 0.0047183]
	Learning Rate: 0.00471834
	LOSS [training: 0.9663711110781582 | validation: 0.538681869455884]
	TIME [epoch: 8.19 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8796607404765332		[learning rate: 0.0047069]
		[batch 20/20] avg loss: 0.8687568443891186		[learning rate: 0.0046955]
	Learning Rate: 0.00469553
	LOSS [training: 0.8742087924328258 | validation: 1.1639829845027236]
	TIME [epoch: 8.2 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6745986874652578		[learning rate: 0.0046842]
		[batch 20/20] avg loss: 0.6243209314022085		[learning rate: 0.0046728]
	Learning Rate: 0.00467282
	LOSS [training: 0.6494598094337332 | validation: 0.8946011490054372]
	TIME [epoch: 8.23 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8777969934636916		[learning rate: 0.0046615]
		[batch 20/20] avg loss: 0.6801658847971106		[learning rate: 0.0046502]
	Learning Rate: 0.00465022
	LOSS [training: 0.7789814391304011 | validation: 0.5011643366085052]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_208.pth
	Model improved!!!
EPOCH 209/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6340379173238497		[learning rate: 0.004639]
		[batch 20/20] avg loss: 1.6029538200803095		[learning rate: 0.0046277]
	Learning Rate: 0.00462773
	LOSS [training: 1.1184958687020798 | validation: 0.45638173653235675]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_209.pth
	Model improved!!!
EPOCH 210/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9116745423125548		[learning rate: 0.0046165]
		[batch 20/20] avg loss: 1.145750436783063		[learning rate: 0.0046054]
	Learning Rate: 0.00460536
	LOSS [training: 1.028712489547809 | validation: 1.2524913815548484]
	TIME [epoch: 8.19 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8185299279180114		[learning rate: 0.0045942]
		[batch 20/20] avg loss: 0.6354100142891869		[learning rate: 0.0045831]
	Learning Rate: 0.00458308
	LOSS [training: 0.7269699711035993 | validation: 1.3298233747814314]
	TIME [epoch: 8.22 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2143069943622382		[learning rate: 0.004572]
		[batch 20/20] avg loss: 0.9067745635165727		[learning rate: 0.0045609]
	Learning Rate: 0.00456092
	LOSS [training: 1.0605407789394057 | validation: 0.7078483459714475]
	TIME [epoch: 8.2 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6464887242845814		[learning rate: 0.0045499]
		[batch 20/20] avg loss: 0.9007731859099867		[learning rate: 0.0045389]
	Learning Rate: 0.00453887
	LOSS [training: 0.7736309550972839 | validation: 0.5564686857633834]
	TIME [epoch: 8.19 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6936927487571023		[learning rate: 0.0045279]
		[batch 20/20] avg loss: 0.7224184379515846		[learning rate: 0.0045169]
	Learning Rate: 0.00451692
	LOSS [training: 0.7080555933543434 | validation: 0.3888978258895057]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_214.pth
	Model improved!!!
EPOCH 215/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9367716980084779		[learning rate: 0.004506]
		[batch 20/20] avg loss: 1.3422879051297956		[learning rate: 0.0044951]
	Learning Rate: 0.00449507
	LOSS [training: 1.1395298015691369 | validation: 0.6861368953590571]
	TIME [epoch: 8.21 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8505502512350773		[learning rate: 0.0044842]
		[batch 20/20] avg loss: 0.859300894177925		[learning rate: 0.0044733]
	Learning Rate: 0.00447334
	LOSS [training: 0.8549255727065012 | validation: 1.1380363719109137]
	TIME [epoch: 8.2 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2216341182240538		[learning rate: 0.0044625]
		[batch 20/20] avg loss: 1.0949154908153693		[learning rate: 0.0044517]
	Learning Rate: 0.0044517
	LOSS [training: 1.1582748045197113 | validation: 0.8860402175654326]
	TIME [epoch: 8.19 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8768338229066137		[learning rate: 0.0044409]
		[batch 20/20] avg loss: 1.1828619120782198		[learning rate: 0.0044302]
	Learning Rate: 0.00443018
	LOSS [training: 1.029847867492417 | validation: 0.45083898005416456]
	TIME [epoch: 8.19 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5076611842652443		[learning rate: 0.0044195]
		[batch 20/20] avg loss: 1.1699047086591572		[learning rate: 0.0044088]
	Learning Rate: 0.00440875
	LOSS [training: 1.3387829464622008 | validation: 0.4256612081131741]
	TIME [epoch: 8.19 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2688005810010283		[learning rate: 0.0043981]
		[batch 20/20] avg loss: 1.0098643322408605		[learning rate: 0.0043874]
	Learning Rate: 0.00438743
	LOSS [training: 1.1393324566209446 | validation: 0.5596743215197711]
	TIME [epoch: 8.22 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2478884360280507		[learning rate: 0.0043768]
		[batch 20/20] avg loss: 0.8229858247784403		[learning rate: 0.0043662]
	Learning Rate: 0.00436622
	LOSS [training: 1.0354371304032457 | validation: 1.0271798736752527]
	TIME [epoch: 8.19 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0626258179931867		[learning rate: 0.0043556]
		[batch 20/20] avg loss: 1.0432393101245463		[learning rate: 0.0043451]
	Learning Rate: 0.0043451
	LOSS [training: 1.0529325640588665 | validation: 0.8384163608981394]
	TIME [epoch: 8.2 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9055197473994318		[learning rate: 0.0043346]
		[batch 20/20] avg loss: 1.1716097797754548		[learning rate: 0.0043241]
	Learning Rate: 0.00432409
	LOSS [training: 1.0385647635874435 | validation: 1.1168928213244729]
	TIME [epoch: 8.19 sec]
EPOCH 224/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8881172519352006		[learning rate: 0.0043136]
		[batch 20/20] avg loss: 0.9191356466801066		[learning rate: 0.0043032]
	Learning Rate: 0.00430318
	LOSS [training: 0.9036264493076536 | validation: 0.8451473235065914]
	TIME [epoch: 8.22 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9823755716548046		[learning rate: 0.0042928]
		[batch 20/20] avg loss: 1.0101015975721226		[learning rate: 0.0042824]
	Learning Rate: 0.00428237
	LOSS [training: 0.9962385846134636 | validation: 0.8511778679929597]
	TIME [epoch: 8.19 sec]
EPOCH 226/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.771863364204198		[learning rate: 0.004272]
		[batch 20/20] avg loss: 0.9550394599117629		[learning rate: 0.0042617]
	Learning Rate: 0.00426166
	LOSS [training: 0.8634514120579804 | validation: 1.6539391815391422]
	TIME [epoch: 8.2 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9069361983006032		[learning rate: 0.0042513]
		[batch 20/20] avg loss: 0.7495365981791771		[learning rate: 0.0042411]
	Learning Rate: 0.00424105
	LOSS [training: 0.8282363982398901 | validation: 0.6131579264704393]
	TIME [epoch: 8.2 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6985853047610237		[learning rate: 0.0042308]
		[batch 20/20] avg loss: 0.8607295088962628		[learning rate: 0.0042205]
	Learning Rate: 0.00422054
	LOSS [training: 0.779657406828643 | validation: 0.534646037052426]
	TIME [epoch: 8.2 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1072936758389265		[learning rate: 0.0042103]
		[batch 20/20] avg loss: 0.9056895920197823		[learning rate: 0.0042001]
	Learning Rate: 0.00420013
	LOSS [training: 1.0064916339293544 | validation: 0.4908542698075785]
	TIME [epoch: 8.22 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8822715038881519		[learning rate: 0.00419]
		[batch 20/20] avg loss: 0.9237295399929574		[learning rate: 0.0041798]
	Learning Rate: 0.00417982
	LOSS [training: 0.9030005219405546 | validation: 0.6729554801094881]
	TIME [epoch: 8.19 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0491422754529078		[learning rate: 0.0041697]
		[batch 20/20] avg loss: 0.7381363742511109		[learning rate: 0.0041596]
	Learning Rate: 0.00415961
	LOSS [training: 0.8936393248520094 | validation: 1.1080549766097691]
	TIME [epoch: 8.19 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6841708709337182		[learning rate: 0.0041495]
		[batch 20/20] avg loss: 0.707303474382182		[learning rate: 0.0041395]
	Learning Rate: 0.0041395
	LOSS [training: 0.6957371726579502 | validation: 1.223375713317428]
	TIME [epoch: 8.19 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7243378768911931		[learning rate: 0.0041295]
		[batch 20/20] avg loss: 0.6066735480508849		[learning rate: 0.0041195]
	Learning Rate: 0.00411948
	LOSS [training: 0.6655057124710391 | validation: 0.5606566958571293]
	TIME [epoch: 8.22 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.591196211891748		[learning rate: 0.0041095]
		[batch 20/20] avg loss: 0.5313143679104941		[learning rate: 0.0040996]
	Learning Rate: 0.00409956
	LOSS [training: 0.5612552899011212 | validation: 0.8663429021251676]
	TIME [epoch: 8.2 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7077489245232783		[learning rate: 0.0040896]
		[batch 20/20] avg loss: 0.7590236975820199		[learning rate: 0.0040797]
	Learning Rate: 0.00407973
	LOSS [training: 0.7333863110526491 | validation: 1.0978410969572534]
	TIME [epoch: 8.2 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9436406356803412		[learning rate: 0.0040699]
		[batch 20/20] avg loss: 0.9669771848644022		[learning rate: 0.00406]
	Learning Rate: 0.00406
	LOSS [training: 0.9553089102723717 | validation: 0.6102199415812446]
	TIME [epoch: 8.19 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7544025286707332		[learning rate: 0.0040502]
		[batch 20/20] avg loss: 0.8873967197081873		[learning rate: 0.0040404]
	Learning Rate: 0.00404037
	LOSS [training: 0.8208996241894603 | validation: 0.7359485163512747]
	TIME [epoch: 8.21 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8208528125163388		[learning rate: 0.0040306]
		[batch 20/20] avg loss: 0.6429577222169887		[learning rate: 0.0040208]
	Learning Rate: 0.00402083
	LOSS [training: 0.731905267366664 | validation: 0.302955088790506]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_238.pth
	Model improved!!!
EPOCH 239/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5064743317942804		[learning rate: 0.0040111]
		[batch 20/20] avg loss: 0.8310405460706644		[learning rate: 0.0040014]
	Learning Rate: 0.00400139
	LOSS [training: 0.6687574389324725 | validation: 0.4022904111714952]
	TIME [epoch: 8.19 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5842856437061952		[learning rate: 0.0039917]
		[batch 20/20] avg loss: 0.7935845553585041		[learning rate: 0.003982]
	Learning Rate: 0.00398204
	LOSS [training: 0.6889350995323495 | validation: 0.4592214904350942]
	TIME [epoch: 8.19 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1177935460713133		[learning rate: 0.0039724]
		[batch 20/20] avg loss: 0.7355890344241549		[learning rate: 0.0039628]
	Learning Rate: 0.00396278
	LOSS [training: 0.9266912902477342 | validation: 0.9166272878593551]
	TIME [epoch: 8.19 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6624356764730273		[learning rate: 0.0039532]
		[batch 20/20] avg loss: 0.5524195084402802		[learning rate: 0.0039436]
	Learning Rate: 0.00394362
	LOSS [training: 0.6074275924566537 | validation: 0.5978087006542843]
	TIME [epoch: 8.22 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6854156804045274		[learning rate: 0.0039341]
		[batch 20/20] avg loss: 0.7541493042650369		[learning rate: 0.0039245]
	Learning Rate: 0.00392455
	LOSS [training: 0.7197824923347821 | validation: 0.5110201709057159]
	TIME [epoch: 8.2 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8289659533179392		[learning rate: 0.003915]
		[batch 20/20] avg loss: 1.0161196153691987		[learning rate: 0.0039056]
	Learning Rate: 0.00390557
	LOSS [training: 0.922542784343569 | validation: 1.1036654570013205]
	TIME [epoch: 8.2 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7694049560959836		[learning rate: 0.0038961]
		[batch 20/20] avg loss: 0.9380045259423371		[learning rate: 0.0038867]
	Learning Rate: 0.00388668
	LOSS [training: 0.85370474101916 | validation: 0.5068307053867342]
	TIME [epoch: 8.2 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7626859717119553		[learning rate: 0.0038773]
		[batch 20/20] avg loss: 0.63072415604488		[learning rate: 0.0038679]
	Learning Rate: 0.00386789
	LOSS [training: 0.6967050638784175 | validation: 0.7063316391039187]
	TIME [epoch: 8.24 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8543317647571993		[learning rate: 0.0038585]
		[batch 20/20] avg loss: 0.6766761622734161		[learning rate: 0.0038492]
	Learning Rate: 0.00384918
	LOSS [training: 0.7655039635153076 | validation: 2.2243766049508964]
	TIME [epoch: 8.21 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.004241610389389		[learning rate: 0.0038399]
		[batch 20/20] avg loss: 0.8996665276188007		[learning rate: 0.0038306]
	Learning Rate: 0.00383057
	LOSS [training: 0.9519540690040948 | validation: 0.35206729263004716]
	TIME [epoch: 8.2 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8080549724684174		[learning rate: 0.0038213]
		[batch 20/20] avg loss: 0.7190149660827874		[learning rate: 0.003812]
	Learning Rate: 0.00381204
	LOSS [training: 0.7635349692756025 | validation: 1.3811957433149045]
	TIME [epoch: 8.2 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7194985794195782		[learning rate: 0.0038028]
		[batch 20/20] avg loss: 0.7406648881153493		[learning rate: 0.0037936]
	Learning Rate: 0.00379361
	LOSS [training: 0.7300817337674637 | validation: 1.1326016998422508]
	TIME [epoch: 8.2 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6464034454722501		[learning rate: 0.0037844]
		[batch 20/20] avg loss: 0.9957757302347561		[learning rate: 0.0037753]
	Learning Rate: 0.00377526
	LOSS [training: 0.8210895878535031 | validation: 0.42679073026195585]
	TIME [epoch: 8.22 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7679787068922418		[learning rate: 0.0037661]
		[batch 20/20] avg loss: 1.061151821882153		[learning rate: 0.003757]
	Learning Rate: 0.00375701
	LOSS [training: 0.9145652643871974 | validation: 1.742675138781751]
	TIME [epoch: 8.19 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2668180456221072		[learning rate: 0.0037479]
		[batch 20/20] avg loss: 0.6765581495021069		[learning rate: 0.0037388]
	Learning Rate: 0.00373884
	LOSS [training: 0.971688097562107 | validation: 0.5031083742991298]
	TIME [epoch: 8.19 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7402227414785828		[learning rate: 0.0037298]
		[batch 20/20] avg loss: 0.6886722045159568		[learning rate: 0.0037208]
	Learning Rate: 0.00372076
	LOSS [training: 0.7144474729972699 | validation: 0.3114885050214068]
	TIME [epoch: 8.21 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5326103746285457		[learning rate: 0.0037118]
		[batch 20/20] avg loss: 0.691795181374955		[learning rate: 0.0037028]
	Learning Rate: 0.00370277
	LOSS [training: 0.6122027780017503 | validation: 0.6980138439858288]
	TIME [epoch: 8.22 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9071334521864085		[learning rate: 0.0036938]
		[batch 20/20] avg loss: 0.8736628052611755		[learning rate: 0.0036849]
	Learning Rate: 0.00368486
	LOSS [training: 0.8903981287237921 | validation: 1.9884040830322276]
	TIME [epoch: 8.18 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9020138805858686		[learning rate: 0.0036759]
		[batch 20/20] avg loss: 0.5808369555303168		[learning rate: 0.003667]
	Learning Rate: 0.00366704
	LOSS [training: 0.7414254180580928 | validation: 0.7275879478008744]
	TIME [epoch: 8.18 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5527790281712645		[learning rate: 0.0036582]
		[batch 20/20] avg loss: 0.7582085298507919		[learning rate: 0.0036493]
	Learning Rate: 0.00364931
	LOSS [training: 0.6554937790110283 | validation: 1.6394697276879275]
	TIME [epoch: 8.18 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8379116363411985		[learning rate: 0.0036405]
		[batch 20/20] avg loss: 0.571487644709356		[learning rate: 0.0036317]
	Learning Rate: 0.00363166
	LOSS [training: 0.7046996405252773 | validation: 0.4432197656837433]
	TIME [epoch: 8.18 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5867432670557804		[learning rate: 0.0036229]
		[batch 20/20] avg loss: 0.8043278359576524		[learning rate: 0.0036141]
	Learning Rate: 0.0036141
	LOSS [training: 0.6955355515067162 | validation: 0.6822944049063787]
	TIME [epoch: 8.2 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7111654646728968		[learning rate: 0.0036053]
		[batch 20/20] avg loss: 0.7834066105049008		[learning rate: 0.0035966]
	Learning Rate: 0.00359662
	LOSS [training: 0.7472860375888988 | validation: 1.0284725282147484]
	TIME [epoch: 8.18 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8775530553563622		[learning rate: 0.0035879]
		[batch 20/20] avg loss: 0.7673872650889686		[learning rate: 0.0035792]
	Learning Rate: 0.00357923
	LOSS [training: 0.8224701602226654 | validation: 0.4880162105782803]
	TIME [epoch: 8.18 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4752169468610816		[learning rate: 0.0035706]
		[batch 20/20] avg loss: 0.7661803431054913		[learning rate: 0.0035619]
	Learning Rate: 0.00356192
	LOSS [training: 0.6206986449832865 | validation: 0.4248168285631647]
	TIME [epoch: 8.18 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6252269416946963		[learning rate: 0.0035533]
		[batch 20/20] avg loss: 0.6145894291935592		[learning rate: 0.0035447]
	Learning Rate: 0.0035447
	LOSS [training: 0.6199081854441278 | validation: 0.8707840515730482]
	TIME [epoch: 8.21 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6746673426826474		[learning rate: 0.0035361]
		[batch 20/20] avg loss: 1.037960248491205		[learning rate: 0.0035276]
	Learning Rate: 0.00352755
	LOSS [training: 0.856313795586926 | validation: 1.0021483554405615]
	TIME [epoch: 8.18 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7397334529487246		[learning rate: 0.003519]
		[batch 20/20] avg loss: 0.9071119492476403		[learning rate: 0.0035105]
	Learning Rate: 0.0035105
	LOSS [training: 0.8234227010981824 | validation: 0.7980618170954717]
	TIME [epoch: 8.18 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6663584086059193		[learning rate: 0.003502]
		[batch 20/20] avg loss: 1.0187199496717922		[learning rate: 0.0034935]
	Learning Rate: 0.00349352
	LOSS [training: 0.8425391791388559 | validation: 0.9998746652253159]
	TIME [epoch: 8.18 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8345250402377253		[learning rate: 0.0034851]
		[batch 20/20] avg loss: 0.721273233448877		[learning rate: 0.0034766]
	Learning Rate: 0.00347663
	LOSS [training: 0.777899136843301 | validation: 0.943383170126575]
	TIME [epoch: 8.2 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6461921001898208		[learning rate: 0.0034682]
		[batch 20/20] avg loss: 0.6575269830694518		[learning rate: 0.0034598]
	Learning Rate: 0.00345981
	LOSS [training: 0.6518595416296362 | validation: 0.9029373986938722]
	TIME [epoch: 8.19 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5777005270993013		[learning rate: 0.0034514]
		[batch 20/20] avg loss: 1.0140893709642247		[learning rate: 0.0034431]
	Learning Rate: 0.00344308
	LOSS [training: 0.795894949031763 | validation: 1.5392814248018953]
	TIME [epoch: 8.18 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7863679937464119		[learning rate: 0.0034347]
		[batch 20/20] avg loss: 0.738201000118413		[learning rate: 0.0034264]
	Learning Rate: 0.00342643
	LOSS [training: 0.7622844969324124 | validation: 1.1725164553680338]
	TIME [epoch: 8.18 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6887150650356626		[learning rate: 0.0034181]
		[batch 20/20] avg loss: 0.6988009230335843		[learning rate: 0.0034099]
	Learning Rate: 0.00340986
	LOSS [training: 0.6937579940346235 | validation: 0.4165897940836567]
	TIME [epoch: 8.18 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5836843231453069		[learning rate: 0.0034016]
		[batch 20/20] avg loss: 0.5864594225501304		[learning rate: 0.0033934]
	Learning Rate: 0.00339337
	LOSS [training: 0.5850718728477187 | validation: 1.1754296166812452]
	TIME [epoch: 8.21 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7991364894329614		[learning rate: 0.0033852]
		[batch 20/20] avg loss: 0.5239456315964405		[learning rate: 0.003377]
	Learning Rate: 0.00337696
	LOSS [training: 0.6615410605147011 | validation: 0.36413579198765206]
	TIME [epoch: 8.18 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5912744484938085		[learning rate: 0.0033688]
		[batch 20/20] avg loss: 0.603480307655792		[learning rate: 0.0033606]
	Learning Rate: 0.00336063
	LOSS [training: 0.5973773780748002 | validation: 0.5322903546544904]
	TIME [epoch: 8.18 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8752087727229825		[learning rate: 0.0033525]
		[batch 20/20] avg loss: 0.7559475321501942		[learning rate: 0.0033444]
	Learning Rate: 0.00334438
	LOSS [training: 0.8155781524365884 | validation: 0.5494047955789298]
	TIME [epoch: 8.18 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6478128060240926		[learning rate: 0.0033363]
		[batch 20/20] avg loss: 0.6330852928374499		[learning rate: 0.0033282]
	Learning Rate: 0.00332821
	LOSS [training: 0.6404490494307712 | validation: 1.0340416180715333]
	TIME [epoch: 8.2 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.611528938065514		[learning rate: 0.0033202]
		[batch 20/20] avg loss: 0.7003426343884736		[learning rate: 0.0033121]
	Learning Rate: 0.00331211
	LOSS [training: 0.6559357862269938 | validation: 1.0009191709455973]
	TIME [epoch: 8.18 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.579210916858327		[learning rate: 0.0033041]
		[batch 20/20] avg loss: 0.5782710011184836		[learning rate: 0.0032961]
	Learning Rate: 0.0032961
	LOSS [training: 0.5787409589884054 | validation: 0.23909841970780585]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_279.pth
	Model improved!!!
EPOCH 280/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7545134709908664		[learning rate: 0.0032881]
		[batch 20/20] avg loss: 0.9484326467191568		[learning rate: 0.0032802]
	Learning Rate: 0.00328016
	LOSS [training: 0.8514730588550117 | validation: 0.7378968574658835]
	TIME [epoch: 8.18 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8873326863920742		[learning rate: 0.0032722]
		[batch 20/20] avg loss: 0.803888278571361		[learning rate: 0.0032643]
	Learning Rate: 0.0032643
	LOSS [training: 0.8456104824817177 | validation: 0.3888397703046459]
	TIME [epoch: 8.17 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.582599357651773		[learning rate: 0.0032564]
		[batch 20/20] avg loss: 0.63224616881394		[learning rate: 0.0032485]
	Learning Rate: 0.00324851
	LOSS [training: 0.6074227632328564 | validation: 0.9663098439778612]
	TIME [epoch: 8.2 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5727815881240449		[learning rate: 0.0032406]
		[batch 20/20] avg loss: 0.734612150387774		[learning rate: 0.0032328]
	Learning Rate: 0.0032328
	LOSS [training: 0.6536968692559096 | validation: 0.45343960040251685]
	TIME [epoch: 8.17 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6686123266137278		[learning rate: 0.003225]
		[batch 20/20] avg loss: 0.545720254107898		[learning rate: 0.0032172]
	Learning Rate: 0.00321717
	LOSS [training: 0.6071662903608128 | validation: 0.30027970865238357]
	TIME [epoch: 8.17 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6760748834082728		[learning rate: 0.0032094]
		[batch 20/20] avg loss: 1.1294359585552665		[learning rate: 0.0032016]
	Learning Rate: 0.00320161
	LOSS [training: 0.9027554209817694 | validation: 1.24300744698119]
	TIME [epoch: 8.17 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9648770343795962		[learning rate: 0.0031939]
		[batch 20/20] avg loss: 0.5727115625757703		[learning rate: 0.0031861]
	Learning Rate: 0.00318613
	LOSS [training: 0.7687942984776833 | validation: 0.8426967085338841]
	TIME [epoch: 8.2 sec]
EPOCH 287/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8270310149799591		[learning rate: 0.0031784]
		[batch 20/20] avg loss: 0.7457867292616869		[learning rate: 0.0031707]
	Learning Rate: 0.00317072
	LOSS [training: 0.7864088721208227 | validation: 0.447630889531766]
	TIME [epoch: 8.18 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6513535002358116		[learning rate: 0.003163]
		[batch 20/20] avg loss: 0.6973893826848003		[learning rate: 0.0031554]
	Learning Rate: 0.00315539
	LOSS [training: 0.674371441460306 | validation: 0.45708070692561903]
	TIME [epoch: 8.16 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7593062719174845		[learning rate: 0.0031477]
		[batch 20/20] avg loss: 1.1620122479149435		[learning rate: 0.0031401]
	Learning Rate: 0.00314013
	LOSS [training: 0.9606592599162139 | validation: 0.4738706883035208]
	TIME [epoch: 8.18 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8382401309153866		[learning rate: 0.0031325]
		[batch 20/20] avg loss: 1.0401016009881296		[learning rate: 0.0031249]
	Learning Rate: 0.00312494
	LOSS [training: 0.9391708659517581 | validation: 1.7416998209712498]
	TIME [epoch: 8.18 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9351387274154878		[learning rate: 0.0031174]
		[batch 20/20] avg loss: 1.016924686842367		[learning rate: 0.0031098]
	Learning Rate: 0.00310983
	LOSS [training: 0.9760317071289275 | validation: 0.4903188926289299]
	TIME [epoch: 8.19 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7009869943812832		[learning rate: 0.0031023]
		[batch 20/20] avg loss: 0.5789046516931239		[learning rate: 0.0030948]
	Learning Rate: 0.00309479
	LOSS [training: 0.6399458230372035 | validation: 0.6652201674788866]
	TIME [epoch: 8.17 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.725408677709963		[learning rate: 0.0030873]
		[batch 20/20] avg loss: 0.5675001777155655		[learning rate: 0.0030798]
	Learning Rate: 0.00307983
	LOSS [training: 0.6464544277127643 | validation: 0.5247132402767155]
	TIME [epoch: 8.18 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9276789346305104		[learning rate: 0.0030724]
		[batch 20/20] avg loss: 1.05968294882899		[learning rate: 0.0030649]
	Learning Rate: 0.00306493
	LOSS [training: 0.9936809417297503 | validation: 0.5023735612967204]
	TIME [epoch: 8.17 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9568020336132456		[learning rate: 0.0030575]
		[batch 20/20] avg loss: 0.5603528988722228		[learning rate: 0.0030501]
	Learning Rate: 0.00305011
	LOSS [training: 0.7585774662427343 | validation: 0.47775993880631623]
	TIME [epoch: 8.2 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.933494700701037		[learning rate: 0.0030427]
		[batch 20/20] avg loss: 0.6167326378383778		[learning rate: 0.0030354]
	Learning Rate: 0.00303536
	LOSS [training: 0.7751136692697076 | validation: 0.41222853904503665]
	TIME [epoch: 8.18 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5243372417590833		[learning rate: 0.003028]
		[batch 20/20] avg loss: 0.6652315799359939		[learning rate: 0.0030207]
	Learning Rate: 0.00302068
	LOSS [training: 0.5947844108475387 | validation: 0.6142618459855141]
	TIME [epoch: 8.19 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5724155500377812		[learning rate: 0.0030134]
		[batch 20/20] avg loss: 0.5123515586479193		[learning rate: 0.0030061]
	Learning Rate: 0.00300608
	LOSS [training: 0.5423835543428502 | validation: 0.8767004539851722]
	TIME [epoch: 8.18 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5560803515976782		[learning rate: 0.0029988]
		[batch 20/20] avg loss: 0.5232257796702434		[learning rate: 0.0029915]
	Learning Rate: 0.00299154
	LOSS [training: 0.5396530656339609 | validation: 0.9611843319032187]
	TIME [epoch: 8.2 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6039578860303241		[learning rate: 0.0029843]
		[batch 20/20] avg loss: 0.5320541409451491		[learning rate: 0.0029771]
	Learning Rate: 0.00297707
	LOSS [training: 0.5680060134877365 | validation: 0.686802188916933]
	TIME [epoch: 8.19 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0350537761465073		[learning rate: 0.0029699]
		[batch 20/20] avg loss: 0.6852644042540205		[learning rate: 0.0029627]
	Learning Rate: 0.00296268
	LOSS [training: 0.8601590902002638 | validation: 0.47123095043876495]
	TIME [epoch: 8.18 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8112412147660611		[learning rate: 0.0029555]
		[batch 20/20] avg loss: 0.5481161957310243		[learning rate: 0.0029483]
	Learning Rate: 0.00294835
	LOSS [training: 0.6796787052485427 | validation: 0.6990135924336809]
	TIME [epoch: 8.18 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9617207341856785		[learning rate: 0.0029412]
		[batch 20/20] avg loss: 0.8957822292103262		[learning rate: 0.0029341]
	Learning Rate: 0.00293409
	LOSS [training: 0.9287514816980021 | validation: 0.8924335408774757]
	TIME [epoch: 8.17 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.673917426379966		[learning rate: 0.002927]
		[batch 20/20] avg loss: 0.7239042370157991		[learning rate: 0.0029199]
	Learning Rate: 0.0029199
	LOSS [training: 0.6989108316978825 | validation: 1.003150071452989]
	TIME [epoch: 8.21 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5185612493739336		[learning rate: 0.0029128]
		[batch 20/20] avg loss: 0.6905815598104106		[learning rate: 0.0029058]
	Learning Rate: 0.00290578
	LOSS [training: 0.6045714045921721 | validation: 0.9065811994240454]
	TIME [epoch: 8.18 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8487439650762312		[learning rate: 0.0028987]
		[batch 20/20] avg loss: 0.8291276117836658		[learning rate: 0.0028917]
	Learning Rate: 0.00289173
	LOSS [training: 0.8389357884299485 | validation: 0.8286520059116591]
	TIME [epoch: 8.18 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5173988390835891		[learning rate: 0.0028847]
		[batch 20/20] avg loss: 0.5691783395927726		[learning rate: 0.0028777]
	Learning Rate: 0.00287775
	LOSS [training: 0.5432885893381807 | validation: 0.4484975196055072]
	TIME [epoch: 8.17 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7895773549992722		[learning rate: 0.0028708]
		[batch 20/20] avg loss: 0.8870894551164554		[learning rate: 0.0028638]
	Learning Rate: 0.00286383
	LOSS [training: 0.8383334050578639 | validation: 0.844583181696127]
	TIME [epoch: 8.19 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8645365291706831		[learning rate: 0.0028569]
		[batch 20/20] avg loss: 0.6520689933415461		[learning rate: 0.00285]
	Learning Rate: 0.00284998
	LOSS [training: 0.7583027612561146 | validation: 0.6267035551375184]
	TIME [epoch: 8.19 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8385639601545123		[learning rate: 0.0028431]
		[batch 20/20] avg loss: 0.9098709129999172		[learning rate: 0.0028362]
	Learning Rate: 0.0028362
	LOSS [training: 0.8742174365772147 | validation: 1.0973652183449403]
	TIME [epoch: 8.18 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8806704105049654		[learning rate: 0.0028293]
		[batch 20/20] avg loss: 0.5462160747873928		[learning rate: 0.0028225]
	Learning Rate: 0.00282248
	LOSS [training: 0.7134432426461792 | validation: 1.2961071889317446]
	TIME [epoch: 8.18 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9902587144276426		[learning rate: 0.0028157]
		[batch 20/20] avg loss: 0.9203923757160244		[learning rate: 0.0028088]
	Learning Rate: 0.00280884
	LOSS [training: 0.9553255450718335 | validation: 0.8982890720224789]
	TIME [epoch: 8.18 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7867720045180511		[learning rate: 0.002802]
		[batch 20/20] avg loss: 0.7085667680270749		[learning rate: 0.0027953]
	Learning Rate: 0.00279525
	LOSS [training: 0.7476693862725629 | validation: 1.0101427451959526]
	TIME [epoch: 8.2 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8211601419887499		[learning rate: 0.0027885]
		[batch 20/20] avg loss: 0.8759114593462343		[learning rate: 0.0027817]
	Learning Rate: 0.00278174
	LOSS [training: 0.848535800667492 | validation: 0.436376413669709]
	TIME [epoch: 8.17 sec]
EPOCH 315/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6722883912969706		[learning rate: 0.002775]
		[batch 20/20] avg loss: 0.543096853301774		[learning rate: 0.0027683]
	Learning Rate: 0.00276828
	LOSS [training: 0.6076926222993723 | validation: 0.35735385610882586]
	TIME [epoch: 8.18 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5687357956814186		[learning rate: 0.0027616]
		[batch 20/20] avg loss: 0.54281580070564		[learning rate: 0.0027549]
	Learning Rate: 0.0027549
	LOSS [training: 0.5557757981935294 | validation: 1.6205206414559403]
	TIME [epoch: 8.18 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5997283677649066		[learning rate: 0.0027482]
		[batch 20/20] avg loss: 0.5893001598827262		[learning rate: 0.0027416]
	Learning Rate: 0.00274157
	LOSS [training: 0.5945142638238164 | validation: 0.4404514420293245]
	TIME [epoch: 8.21 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5467770734888956		[learning rate: 0.0027349]
		[batch 20/20] avg loss: 0.5218215871116464		[learning rate: 0.0027283]
	Learning Rate: 0.00272832
	LOSS [training: 0.534299330300271 | validation: 0.3726529046548962]
	TIME [epoch: 8.18 sec]
EPOCH 319/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5927544800075065		[learning rate: 0.0027217]
		[batch 20/20] avg loss: 0.5832224087425304		[learning rate: 0.0027151]
	Learning Rate: 0.00271512
	LOSS [training: 0.5879884443750185 | validation: 0.8820535101308355]
	TIME [epoch: 8.19 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5094079203302057		[learning rate: 0.0027086]
		[batch 20/20] avg loss: 0.49815822592928505		[learning rate: 0.002702]
	Learning Rate: 0.00270199
	LOSS [training: 0.5037830731297455 | validation: 1.7455084395757126]
	TIME [epoch: 8.18 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6757265773890407		[learning rate: 0.0026955]
		[batch 20/20] avg loss: 0.745774233413546		[learning rate: 0.0026889]
	Learning Rate: 0.00268893
	LOSS [training: 0.7107504054012932 | validation: 1.0706894326821592]
	TIME [epoch: 8.19 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8801648512197338		[learning rate: 0.0026824]
		[batch 20/20] avg loss: 0.8065333505652867		[learning rate: 0.0026759]
	Learning Rate: 0.00267592
	LOSS [training: 0.8433491008925105 | validation: 0.554890682953977]
	TIME [epoch: 8.21 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7006446729837051		[learning rate: 0.0026694]
		[batch 20/20] avg loss: 0.7323981503933032		[learning rate: 0.002663]
	Learning Rate: 0.00266298
	LOSS [training: 0.7165214116885041 | validation: 0.3733086306192596]
	TIME [epoch: 8.18 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6176406684998791		[learning rate: 0.0026565]
		[batch 20/20] avg loss: 0.8240131193219298		[learning rate: 0.0026501]
	Learning Rate: 0.00265011
	LOSS [training: 0.7208268939109045 | validation: 1.1284567002793866]
	TIME [epoch: 8.19 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5825027825896701		[learning rate: 0.0026437]
		[batch 20/20] avg loss: 0.84478892426833		[learning rate: 0.0026373]
	Learning Rate: 0.00263729
	LOSS [training: 0.7136458534290002 | validation: 1.0772018344746592]
	TIME [epoch: 8.18 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.711882814754824		[learning rate: 0.0026309]
		[batch 20/20] avg loss: 0.6908591306828372		[learning rate: 0.0026245]
	Learning Rate: 0.00262454
	LOSS [training: 0.7013709727188308 | validation: 0.3301069641693667]
	TIME [epoch: 8.21 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4604716448084316		[learning rate: 0.0026182]
		[batch 20/20] avg loss: 0.5444585830244246		[learning rate: 0.0026118]
	Learning Rate: 0.00261184
	LOSS [training: 0.502465113916428 | validation: 0.42381170745552865]
	TIME [epoch: 8.19 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5297421948924482		[learning rate: 0.0026055]
		[batch 20/20] avg loss: 0.5480155875532002		[learning rate: 0.0025992]
	Learning Rate: 0.00259921
	LOSS [training: 0.5388788912228242 | validation: 0.5906235073435226]
	TIME [epoch: 8.19 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6312335771996457		[learning rate: 0.0025929]
		[batch 20/20] avg loss: 0.8276495255315733		[learning rate: 0.0025866]
	Learning Rate: 0.00258664
	LOSS [training: 0.7294415513656094 | validation: 0.4159060682580338]
	TIME [epoch: 8.18 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6356706919625189		[learning rate: 0.0025804]
		[batch 20/20] avg loss: 0.5502218527098266		[learning rate: 0.0025741]
	Learning Rate: 0.00257414
	LOSS [training: 0.5929462723361729 | validation: 0.45257593145251995]
	TIME [epoch: 8.19 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6012037889311316		[learning rate: 0.0025679]
		[batch 20/20] avg loss: 0.7412468976972717		[learning rate: 0.0025617]
	Learning Rate: 0.00256169
	LOSS [training: 0.6712253433142016 | validation: 1.1026462141314957]
	TIME [epoch: 8.18 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7637356889168221		[learning rate: 0.0025555]
		[batch 20/20] avg loss: 0.806697140840086		[learning rate: 0.0025493]
	Learning Rate: 0.0025493
	LOSS [training: 0.7852164148784541 | validation: 0.7154382724402251]
	TIME [epoch: 8.18 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6945685862920146		[learning rate: 0.0025431]
		[batch 20/20] avg loss: 0.832164632384611		[learning rate: 0.002537]
	Learning Rate: 0.00253697
	LOSS [training: 0.7633666093383128 | validation: 0.950665447265221]
	TIME [epoch: 8.18 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7237960970487909		[learning rate: 0.0025308]
		[batch 20/20] avg loss: 0.6753176272765666		[learning rate: 0.0025247]
	Learning Rate: 0.0025247
	LOSS [training: 0.6995568621626788 | validation: 1.2995508867032366]
	TIME [epoch: 8.18 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7285618130460838		[learning rate: 0.0025186]
		[batch 20/20] avg loss: 0.5915643535219341		[learning rate: 0.0025125]
	Learning Rate: 0.0025125
	LOSS [training: 0.6600630832840089 | validation: 0.38931008225442815]
	TIME [epoch: 8.21 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7257483964700995		[learning rate: 0.0025064]
		[batch 20/20] avg loss: 0.5560922295593398		[learning rate: 0.0025003]
	Learning Rate: 0.00250035
	LOSS [training: 0.6409203130147196 | validation: 0.7634657712064165]
	TIME [epoch: 8.17 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46464639327952834		[learning rate: 0.0024943]
		[batch 20/20] avg loss: 0.37455940092037154		[learning rate: 0.0024883]
	Learning Rate: 0.00248825
	LOSS [training: 0.41960289709994986 | validation: 0.3443479135394061]
	TIME [epoch: 8.18 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3499372514464595		[learning rate: 0.0024822]
		[batch 20/20] avg loss: 0.4201905611498013		[learning rate: 0.0024762]
	Learning Rate: 0.00247622
	LOSS [training: 0.3850639062981304 | validation: 1.010429662835875]
	TIME [epoch: 8.16 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6707711047861804		[learning rate: 0.0024702]
		[batch 20/20] avg loss: 0.7112705859003523		[learning rate: 0.0024642]
	Learning Rate: 0.00246425
	LOSS [training: 0.6910208453432662 | validation: 0.7958051345431986]
	TIME [epoch: 8.19 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6460321824483599		[learning rate: 0.0024583]
		[batch 20/20] avg loss: 0.6475352067568222		[learning rate: 0.0024523]
	Learning Rate: 0.00245233
	LOSS [training: 0.6467836946025909 | validation: 0.45993810131567514]
	TIME [epoch: 8.18 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3902285830355992		[learning rate: 0.0024464]
		[batch 20/20] avg loss: 0.4134861779911173		[learning rate: 0.0024405]
	Learning Rate: 0.00244047
	LOSS [training: 0.4018573805133582 | validation: 0.862877925319693]
	TIME [epoch: 8.17 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7819985566644075		[learning rate: 0.0024346]
		[batch 20/20] avg loss: 0.7945462521223854		[learning rate: 0.0024287]
	Learning Rate: 0.00242867
	LOSS [training: 0.7882724043933964 | validation: 0.15594520221751945]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_342.pth
	Model improved!!!
EPOCH 343/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4351466549208233		[learning rate: 0.0024228]
		[batch 20/20] avg loss: 0.5385016905105512		[learning rate: 0.0024169]
	Learning Rate: 0.00241693
	LOSS [training: 0.48682417271568734 | validation: 1.0652660698730627]
	TIME [epoch: 8.17 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.858950168538105		[learning rate: 0.0024111]
		[batch 20/20] avg loss: 0.8215459926666833		[learning rate: 0.0024052]
	Learning Rate: 0.00240524
	LOSS [training: 0.8402480806023942 | validation: 0.7640560966099788]
	TIME [epoch: 8.19 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6813662235257942		[learning rate: 0.0023994]
		[batch 20/20] avg loss: 0.6325003622078709		[learning rate: 0.0023936]
	Learning Rate: 0.00239361
	LOSS [training: 0.6569332928668324 | validation: 0.6110045002498897]
	TIME [epoch: 8.16 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5549545033629284		[learning rate: 0.0023878]
		[batch 20/20] avg loss: 0.6938381641364024		[learning rate: 0.002382]
	Learning Rate: 0.00238203
	LOSS [training: 0.6243963337496654 | validation: 0.30880941937046014]
	TIME [epoch: 8.17 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.729414387252195		[learning rate: 0.0023763]
		[batch 20/20] avg loss: 0.6918569536847105		[learning rate: 0.0023705]
	Learning Rate: 0.00237051
	LOSS [training: 0.7106356704684529 | validation: 0.3811901765602693]
	TIME [epoch: 8.16 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6056645894446365		[learning rate: 0.0023648]
		[batch 20/20] avg loss: 0.5962853705619601		[learning rate: 0.002359]
	Learning Rate: 0.00235905
	LOSS [training: 0.6009749800032984 | validation: 0.22091236133279507]
	TIME [epoch: 8.19 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48460925862594584		[learning rate: 0.0023533]
		[batch 20/20] avg loss: 0.5706859208412812		[learning rate: 0.0023476]
	Learning Rate: 0.00234764
	LOSS [training: 0.5276475897336134 | validation: 0.9818252150876068]
	TIME [epoch: 8.16 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5926478671048423		[learning rate: 0.002342]
		[batch 20/20] avg loss: 0.3760009774088432		[learning rate: 0.0023363]
	Learning Rate: 0.00233629
	LOSS [training: 0.4843244222568428 | validation: 0.3154658646709887]
	TIME [epoch: 8.16 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3835191989621093		[learning rate: 0.0023306]
		[batch 20/20] avg loss: 0.7160720501944757		[learning rate: 0.002325]
	Learning Rate: 0.00232499
	LOSS [training: 0.5497956245782925 | validation: 0.20281817431991273]
	TIME [epoch: 8.16 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3673199817751116		[learning rate: 0.0023194]
		[batch 20/20] avg loss: 0.4209854506921837		[learning rate: 0.0023137]
	Learning Rate: 0.00231375
	LOSS [training: 0.3941527162336476 | validation: 0.9869367442517898]
	TIME [epoch: 8.18 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7291292630198103		[learning rate: 0.0023081]
		[batch 20/20] avg loss: 0.46229674983723557		[learning rate: 0.0023026]
	Learning Rate: 0.00230256
	LOSS [training: 0.595713006428523 | validation: 0.36958229740748927]
	TIME [epoch: 8.18 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3561263529466586		[learning rate: 0.002297]
		[batch 20/20] avg loss: 0.3925986445523368		[learning rate: 0.0022914]
	Learning Rate: 0.00229142
	LOSS [training: 0.3743624987494977 | validation: 0.296740013066384]
	TIME [epoch: 8.17 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3706705129899887		[learning rate: 0.0022859]
		[batch 20/20] avg loss: 0.5538178879875009		[learning rate: 0.0022803]
	Learning Rate: 0.00228034
	LOSS [training: 0.4622442004887447 | validation: 0.6858157141225555]
	TIME [epoch: 8.16 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6822585780740291		[learning rate: 0.0022748]
		[batch 20/20] avg loss: 0.5182422886455262		[learning rate: 0.0022693]
	Learning Rate: 0.00226931
	LOSS [training: 0.6002504333597778 | validation: 1.118411187122142]
	TIME [epoch: 8.16 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.513538686603376		[learning rate: 0.0022638]
		[batch 20/20] avg loss: 0.6599372953645328		[learning rate: 0.0022583]
	Learning Rate: 0.00225834
	LOSS [training: 0.5867379909839545 | validation: 0.4388975348246521]
	TIME [epoch: 8.19 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7272242965144776		[learning rate: 0.0022529]
		[batch 20/20] avg loss: 0.4206742614717678		[learning rate: 0.0022474]
	Learning Rate: 0.00224742
	LOSS [training: 0.5739492789931228 | validation: 0.5604315920594863]
	TIME [epoch: 8.16 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45197088476110264		[learning rate: 0.002242]
		[batch 20/20] avg loss: 0.6034772369053643		[learning rate: 0.0022366]
	Learning Rate: 0.00223655
	LOSS [training: 0.5277240608332334 | validation: 0.5187137875449019]
	TIME [epoch: 8.17 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.434538970693095		[learning rate: 0.0022311]
		[batch 20/20] avg loss: 0.4929571152018668		[learning rate: 0.0022257]
	Learning Rate: 0.00222574
	LOSS [training: 0.46374804294748095 | validation: 0.27048914193111256]
	TIME [epoch: 8.16 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.49530015147442114		[learning rate: 0.0022203]
		[batch 20/20] avg loss: 0.6109643061772723		[learning rate: 0.002215]
	Learning Rate: 0.00221497
	LOSS [training: 0.5531322288258467 | validation: 1.1203340224644873]
	TIME [epoch: 8.18 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6217462495488487		[learning rate: 0.0022096]
		[batch 20/20] avg loss: 0.5440128550896386		[learning rate: 0.0022043]
	Learning Rate: 0.00220426
	LOSS [training: 0.5828795523192436 | validation: 0.5336228939232339]
	TIME [epoch: 8.17 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45976846276252814		[learning rate: 0.0021989]
		[batch 20/20] avg loss: 0.6656891712615721		[learning rate: 0.0021936]
	Learning Rate: 0.0021936
	LOSS [training: 0.5627288170120501 | validation: 0.318608367555363]
	TIME [epoch: 8.17 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5293463473377009		[learning rate: 0.0021883]
		[batch 20/20] avg loss: 0.6206140073410321		[learning rate: 0.002183]
	Learning Rate: 0.00218299
	LOSS [training: 0.5749801773393666 | validation: 0.4049200471839678]
	TIME [epoch: 8.15 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4084477916421645		[learning rate: 0.0021777]
		[batch 20/20] avg loss: 0.4480029099009334		[learning rate: 0.0021724]
	Learning Rate: 0.00217244
	LOSS [training: 0.428225350771549 | validation: 0.6324013260268055]
	TIME [epoch: 8.16 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5239723848625256		[learning rate: 0.0021672]
		[batch 20/20] avg loss: 0.5482150326529673		[learning rate: 0.0021619]
	Learning Rate: 0.00216193
	LOSS [training: 0.5360937087577464 | validation: 0.8045357393692727]
	TIME [epoch: 8.19 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6880458689212282		[learning rate: 0.0021567]
		[batch 20/20] avg loss: 0.5268583810750439		[learning rate: 0.0021515]
	Learning Rate: 0.00215148
	LOSS [training: 0.607452124998136 | validation: 0.27379759295288153]
	TIME [epoch: 8.16 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.499151233192695		[learning rate: 0.0021463]
		[batch 20/20] avg loss: 0.7502099748826491		[learning rate: 0.0021411]
	Learning Rate: 0.00214107
	LOSS [training: 0.6246806040376721 | validation: 0.8535368789696031]
	TIME [epoch: 8.16 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4590512585451158		[learning rate: 0.0021359]
		[batch 20/20] avg loss: 0.46432281814310883		[learning rate: 0.0021307]
	Learning Rate: 0.00213072
	LOSS [training: 0.4616870383441124 | validation: 0.2657581130063891]
	TIME [epoch: 8.16 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5326605180222417		[learning rate: 0.0021256]
		[batch 20/20] avg loss: 0.5168597300287316		[learning rate: 0.0021204]
	Learning Rate: 0.00212042
	LOSS [training: 0.5247601240254867 | validation: 0.23575364672727117]
	TIME [epoch: 8.18 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3885696339978816		[learning rate: 0.0021153]
		[batch 20/20] avg loss: 0.430350530616279		[learning rate: 0.0021102]
	Learning Rate: 0.00211016
	LOSS [training: 0.4094600823070803 | validation: 0.3246120196050474]
	TIME [epoch: 8.17 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4153441330311461		[learning rate: 0.0021051]
		[batch 20/20] avg loss: 0.36669470011633687		[learning rate: 0.0021]
	Learning Rate: 0.00209996
	LOSS [training: 0.39101941657374145 | validation: 0.4396955499082786]
	TIME [epoch: 8.16 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6413819675993387		[learning rate: 0.0020949]
		[batch 20/20] avg loss: 0.5336097827458677		[learning rate: 0.0020898]
	Learning Rate: 0.0020898
	LOSS [training: 0.5874958751726033 | validation: 0.2654212521628483]
	TIME [epoch: 8.16 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5158598884456747		[learning rate: 0.0020847]
		[batch 20/20] avg loss: 0.5633492079060551		[learning rate: 0.0020797]
	Learning Rate: 0.0020797
	LOSS [training: 0.5396045481758649 | validation: 1.1719001626913086]
	TIME [epoch: 8.16 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6291662037494465		[learning rate: 0.0020747]
		[batch 20/20] avg loss: 0.5542981490572161		[learning rate: 0.0020696]
	Learning Rate: 0.00206964
	LOSS [training: 0.5917321764033312 | validation: 0.2663706901742298]
	TIME [epoch: 8.18 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6059714353862995		[learning rate: 0.0020646]
		[batch 20/20] avg loss: 0.5074505606194296		[learning rate: 0.0020596]
	Learning Rate: 0.00205963
	LOSS [training: 0.5567109980028644 | validation: 0.8228962892580083]
	TIME [epoch: 8.16 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6456772167915255		[learning rate: 0.0020546]
		[batch 20/20] avg loss: 0.3706245932723572		[learning rate: 0.0020497]
	Learning Rate: 0.00204967
	LOSS [training: 0.5081509050319414 | validation: 0.658003741257758]
	TIME [epoch: 8.15 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6826923583605852		[learning rate: 0.0020447]
		[batch 20/20] avg loss: 0.475873204410442		[learning rate: 0.0020398]
	Learning Rate: 0.00203976
	LOSS [training: 0.5792827813855135 | validation: 0.3470484463998385]
	TIME [epoch: 8.15 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45906027079603584		[learning rate: 0.0020348]
		[batch 20/20] avg loss: 0.5870242932570531		[learning rate: 0.0020299]
	Learning Rate: 0.0020299
	LOSS [training: 0.5230422820265446 | validation: 0.44680931718612443]
	TIME [epoch: 8.18 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37926758011459916		[learning rate: 0.002025]
		[batch 20/20] avg loss: 0.3391591030599861		[learning rate: 0.0020201]
	Learning Rate: 0.00202008
	LOSS [training: 0.35921334158729257 | validation: 0.28700644979950357]
	TIME [epoch: 8.16 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3338563837642462		[learning rate: 0.0020152]
		[batch 20/20] avg loss: 0.4879990219997441		[learning rate: 0.0020103]
	Learning Rate: 0.00201031
	LOSS [training: 0.41092770288199515 | validation: 0.24213338740942658]
	TIME [epoch: 8.16 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5666618366475825		[learning rate: 0.0020054]
		[batch 20/20] avg loss: 0.4158542645262318		[learning rate: 0.0020006]
	Learning Rate: 0.00200059
	LOSS [training: 0.49125805058690714 | validation: 0.8043735396485434]
	TIME [epoch: 8.16 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4228101744889193		[learning rate: 0.0019957]
		[batch 20/20] avg loss: 0.49880890841122943		[learning rate: 0.0019909]
	Learning Rate: 0.00199091
	LOSS [training: 0.46080954145007436 | validation: 0.4353346420059648]
	TIME [epoch: 8.16 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5400222892955531		[learning rate: 0.0019861]
		[batch 20/20] avg loss: 0.535294083317773		[learning rate: 0.0019813]
	Learning Rate: 0.00198129
	LOSS [training: 0.537658186306663 | validation: 0.328138481338376]
	TIME [epoch: 8.18 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34135610644339553		[learning rate: 0.0019765]
		[batch 20/20] avg loss: 0.5327941506239917		[learning rate: 0.0019717]
	Learning Rate: 0.00197171
	LOSS [training: 0.43707512853369357 | validation: 0.5644182281663376]
	TIME [epoch: 8.16 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40282323588868785		[learning rate: 0.0019669]
		[batch 20/20] avg loss: 0.32163753915108567		[learning rate: 0.0019622]
	Learning Rate: 0.00196217
	LOSS [training: 0.3622303875198868 | validation: 0.3532254646895744]
	TIME [epoch: 8.15 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37821173283975806		[learning rate: 0.0019574]
		[batch 20/20] avg loss: 0.43562716324723605		[learning rate: 0.0019527]
	Learning Rate: 0.00195268
	LOSS [training: 0.40691944804349706 | validation: 0.2599685021483481]
	TIME [epoch: 8.15 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3738193691834602		[learning rate: 0.001948]
		[batch 20/20] avg loss: 0.5185165886717092		[learning rate: 0.0019432]
	Learning Rate: 0.00194324
	LOSS [training: 0.44616797892758475 | validation: 0.6677831199632885]
	TIME [epoch: 8.19 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5509841185503452		[learning rate: 0.0019385]
		[batch 20/20] avg loss: 0.36892700915507587		[learning rate: 0.0019338]
	Learning Rate: 0.00193384
	LOSS [training: 0.4599555638527105 | validation: 0.19046598868116213]
	TIME [epoch: 8.17 sec]
EPOCH 390/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3868050405722556		[learning rate: 0.0019292]
		[batch 20/20] avg loss: 0.392962150581393		[learning rate: 0.0019245]
	Learning Rate: 0.00192449
	LOSS [training: 0.38988359557682434 | validation: 0.9081127420975152]
	TIME [epoch: 8.16 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.41115972013376495		[learning rate: 0.0019198]
		[batch 20/20] avg loss: 0.26989771464174916		[learning rate: 0.0019152]
	Learning Rate: 0.00191518
	LOSS [training: 0.34052871738775703 | validation: 0.982978845789109]
	TIME [epoch: 8.15 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36708670641981317		[learning rate: 0.0019105]
		[batch 20/20] avg loss: 0.6951804824352191		[learning rate: 0.0019059]
	Learning Rate: 0.00190592
	LOSS [training: 0.531133594427516 | validation: 0.2906061738586056]
	TIME [epoch: 8.17 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39221911136151816		[learning rate: 0.0019013]
		[batch 20/20] avg loss: 0.5449393700743408		[learning rate: 0.0018967]
	Learning Rate: 0.00189671
	LOSS [training: 0.4685792407179295 | validation: 0.1982798504495747]
	TIME [epoch: 8.16 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3575657019483739		[learning rate: 0.0018921]
		[batch 20/20] avg loss: 0.4982268824727815		[learning rate: 0.0018875]
	Learning Rate: 0.00188753
	LOSS [training: 0.42789629221057773 | validation: 0.3440676520589945]
	TIME [epoch: 8.16 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40917571723503954		[learning rate: 0.001883]
		[batch 20/20] avg loss: 0.3367171212742125		[learning rate: 0.0018784]
	Learning Rate: 0.00187841
	LOSS [training: 0.37294641925462607 | validation: 0.37143236363335175]
	TIME [epoch: 8.15 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5018281758612531		[learning rate: 0.0018739]
		[batch 20/20] avg loss: 0.4653005430676149		[learning rate: 0.0018693]
	Learning Rate: 0.00186932
	LOSS [training: 0.48356435946443393 | validation: 0.34711274114861423]
	TIME [epoch: 8.16 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4106207047329682		[learning rate: 0.0018648]
		[batch 20/20] avg loss: 0.373088747026289		[learning rate: 0.0018603]
	Learning Rate: 0.00186028
	LOSS [training: 0.39185472587962866 | validation: 0.15374447314922568]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_397.pth
	Model improved!!!
EPOCH 398/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5745346673831107		[learning rate: 0.0018558]
		[batch 20/20] avg loss: 0.3817484533269899		[learning rate: 0.0018513]
	Learning Rate: 0.00185129
	LOSS [training: 0.4781415603550503 | validation: 0.19343453187821313]
	TIME [epoch: 8.16 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.41221744917837927		[learning rate: 0.0018468]
		[batch 20/20] avg loss: 0.3686824438688221		[learning rate: 0.0018423]
	Learning Rate: 0.00184233
	LOSS [training: 0.3904499465236006 | validation: 0.7276478016187251]
	TIME [epoch: 8.17 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.43601582559312463		[learning rate: 0.0018379]
		[batch 20/20] avg loss: 0.4198824418254089		[learning rate: 0.0018334]
	Learning Rate: 0.00183343
	LOSS [training: 0.4279491337092668 | validation: 0.5268770944626202]
	TIME [epoch: 8.16 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4030103046056953		[learning rate: 0.001829]
		[batch 20/20] avg loss: 0.4809260508651951		[learning rate: 0.0018246]
	Learning Rate: 0.00182456
	LOSS [training: 0.44196817773544517 | validation: 0.4007464090611238]
	TIME [epoch: 8.19 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.49759750754228244		[learning rate: 0.0018201]
		[batch 20/20] avg loss: 0.42343134279754224		[learning rate: 0.0018157]
	Learning Rate: 0.00181574
	LOSS [training: 0.46051442516991237 | validation: 0.25486479933458317]
	TIME [epoch: 8.18 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39892776809650793		[learning rate: 0.0018113]
		[batch 20/20] avg loss: 0.42004422651635565		[learning rate: 0.001807]
	Learning Rate: 0.00180696
	LOSS [training: 0.40948599730643176 | validation: 0.5969357786177452]
	TIME [epoch: 8.16 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4726003044484214		[learning rate: 0.0018026]
		[batch 20/20] avg loss: 0.4037074851718095		[learning rate: 0.0017982]
	Learning Rate: 0.00179822
	LOSS [training: 0.4381538948101154 | validation: 0.3344108064277558]
	TIME [epoch: 8.16 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3517285869725771		[learning rate: 0.0017939]
		[batch 20/20] avg loss: 0.37256236314969826		[learning rate: 0.0017895]
	Learning Rate: 0.00178952
	LOSS [training: 0.36214547506113764 | validation: 0.2888785184280976]
	TIME [epoch: 8.16 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3882268768645659		[learning rate: 0.0017852]
		[batch 20/20] avg loss: 0.3994786385942385		[learning rate: 0.0017809]
	Learning Rate: 0.00178087
	LOSS [training: 0.39385275772940226 | validation: 0.36533784714889445]
	TIME [epoch: 8.2 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4398265836751939		[learning rate: 0.0017766]
		[batch 20/20] avg loss: 0.6210875086456913		[learning rate: 0.0017723]
	Learning Rate: 0.00177226
	LOSS [training: 0.5304570461604425 | validation: 0.5149158695928787]
	TIME [epoch: 8.16 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3704542335835885		[learning rate: 0.001768]
		[batch 20/20] avg loss: 0.3392990973667035		[learning rate: 0.0017637]
	Learning Rate: 0.00176369
	LOSS [training: 0.35487666547514596 | validation: 0.5027152391720564]
	TIME [epoch: 8.16 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38872751949596507		[learning rate: 0.0017594]
		[batch 20/20] avg loss: 0.3677462783486104		[learning rate: 0.0017552]
	Learning Rate: 0.00175516
	LOSS [training: 0.37823689892228773 | validation: 0.5156519168004244]
	TIME [epoch: 8.16 sec]
EPOCH 410/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.41694100869961553		[learning rate: 0.0017509]
		[batch 20/20] avg loss: 0.3466529456138978		[learning rate: 0.0017467]
	Learning Rate: 0.00174667
	LOSS [training: 0.38179697715675665 | validation: 0.17687134789955436]
	TIME [epoch: 8.19 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29863857780538927		[learning rate: 0.0017424]
		[batch 20/20] avg loss: 0.3529909828927401		[learning rate: 0.0017382]
	Learning Rate: 0.00173822
	LOSS [training: 0.32581478034906464 | validation: 1.1167948169441526]
	TIME [epoch: 8.17 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5338718228618679		[learning rate: 0.001734]
		[batch 20/20] avg loss: 0.34401300296283		[learning rate: 0.0017298]
	Learning Rate: 0.00172982
	LOSS [training: 0.438942412912349 | validation: 0.15758555676147123]
	TIME [epoch: 8.17 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3618003786359169		[learning rate: 0.0017256]
		[batch 20/20] avg loss: 0.37343607650825367		[learning rate: 0.0017215]
	Learning Rate: 0.00172145
	LOSS [training: 0.36761822757208534 | validation: 0.28606565025362113]
	TIME [epoch: 8.17 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36464918448239836		[learning rate: 0.0017173]
		[batch 20/20] avg loss: 0.5064371859034216		[learning rate: 0.0017131]
	Learning Rate: 0.00171313
	LOSS [training: 0.43554318519291 | validation: 0.29903930194266387]
	TIME [epoch: 8.17 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3465126031798104		[learning rate: 0.001709]
		[batch 20/20] avg loss: 0.3289168502690606		[learning rate: 0.0017048]
	Learning Rate: 0.00170484
	LOSS [training: 0.3377147267244355 | validation: 0.1744994366435454]
	TIME [epoch: 8.19 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.35118281232228893		[learning rate: 0.0017007]
		[batch 20/20] avg loss: 0.5606454049637176		[learning rate: 0.0016966]
	Learning Rate: 0.0016966
	LOSS [training: 0.4559141086430034 | validation: 0.2700008086624302]
	TIME [epoch: 8.18 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5740983209623923		[learning rate: 0.0016925]
		[batch 20/20] avg loss: 0.39116502240449225		[learning rate: 0.0016884]
	Learning Rate: 0.00168839
	LOSS [training: 0.48263167168344234 | validation: 0.22450004008589683]
	TIME [epoch: 8.16 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3264323226320923		[learning rate: 0.0016843]
		[batch 20/20] avg loss: 0.3266501339025829		[learning rate: 0.0016802]
	Learning Rate: 0.00168023
	LOSS [training: 0.32654122826733767 | validation: 0.6377460860798123]
	TIME [epoch: 8.17 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33392322869288893		[learning rate: 0.0016762]
		[batch 20/20] avg loss: 0.5067382648140153		[learning rate: 0.0016721]
	Learning Rate: 0.0016721
	LOSS [training: 0.4203307467534521 | validation: 0.7854009138764186]
	TIME [epoch: 8.19 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36022514542041467		[learning rate: 0.0016681]
		[batch 20/20] avg loss: 0.3478390804697929		[learning rate: 0.001664]
	Learning Rate: 0.00166402
	LOSS [training: 0.3540321129451038 | validation: 0.4187770983943425]
	TIME [epoch: 8.16 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30312707022180835		[learning rate: 0.00166]
		[batch 20/20] avg loss: 0.5545234955410969		[learning rate: 0.001656]
	Learning Rate: 0.00165597
	LOSS [training: 0.4288252828814526 | validation: 0.5448810144807664]
	TIME [epoch: 8.15 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4211627550259541		[learning rate: 0.001652]
		[batch 20/20] avg loss: 0.2963389105216901		[learning rate: 0.001648]
	Learning Rate: 0.00164796
	LOSS [training: 0.3587508327738222 | validation: 0.2550032995043461]
	TIME [epoch: 8.15 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2851203008895439		[learning rate: 0.001644]
		[batch 20/20] avg loss: 0.4292063340369466		[learning rate: 0.00164]
	Learning Rate: 0.00163999
	LOSS [training: 0.35716331746324526 | validation: 0.26696509639403854]
	TIME [epoch: 8.18 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39362086332290136		[learning rate: 0.001636]
		[batch 20/20] avg loss: 0.4010219763847768		[learning rate: 0.0016321]
	Learning Rate: 0.00163206
	LOSS [training: 0.3973214198538391 | validation: 0.28809737359053006]
	TIME [epoch: 8.18 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39295374466387506		[learning rate: 0.0016281]
		[batch 20/20] avg loss: 0.5212400844084524		[learning rate: 0.0016242]
	Learning Rate: 0.00162417
	LOSS [training: 0.45709691453616375 | validation: 1.0690785651710275]
	TIME [epoch: 8.16 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6602508225304653		[learning rate: 0.0016202]
		[batch 20/20] avg loss: 0.2963595163076738		[learning rate: 0.0016163]
	Learning Rate: 0.00161632
	LOSS [training: 0.47830516941906964 | validation: 0.4512809222471737]
	TIME [epoch: 8.17 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3773641449077821		[learning rate: 0.0016124]
		[batch 20/20] avg loss: 0.4017837457600922		[learning rate: 0.0016085]
	Learning Rate: 0.0016085
	LOSS [training: 0.38957394533393713 | validation: 0.2493747008566786]
	TIME [epoch: 8.18 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2697840394546758		[learning rate: 0.0016046]
		[batch 20/20] avg loss: 0.4181406794759333		[learning rate: 0.0016007]
	Learning Rate: 0.00160072
	LOSS [training: 0.3439623594653045 | validation: 1.0171845484521502]
	TIME [epoch: 8.2 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3801656076027665		[learning rate: 0.0015968]
		[batch 20/20] avg loss: 0.30602550211712604		[learning rate: 0.001593]
	Learning Rate: 0.00159298
	LOSS [training: 0.34309555485994625 | validation: 0.2224500924470386]
	TIME [epoch: 8.17 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34048391982820586		[learning rate: 0.0015891]
		[batch 20/20] avg loss: 0.30245228763128995		[learning rate: 0.0015853]
	Learning Rate: 0.00158528
	LOSS [training: 0.3214681037297479 | validation: 0.25779279930571614]
	TIME [epoch: 8.17 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3177005875821339		[learning rate: 0.0015814]
		[batch 20/20] avg loss: 0.3878707173214922		[learning rate: 0.0015776]
	Learning Rate: 0.00157761
	LOSS [training: 0.35278565245181304 | validation: 0.18198299399630866]
	TIME [epoch: 8.17 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2682937975972284		[learning rate: 0.0015738]
		[batch 20/20] avg loss: 0.42456301393617063		[learning rate: 0.00157]
	Learning Rate: 0.00156998
	LOSS [training: 0.34642840576669953 | validation: 0.21187301244033194]
	TIME [epoch: 8.18 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34717169392488334		[learning rate: 0.0015662]
		[batch 20/20] avg loss: 0.29583373212093605		[learning rate: 0.0015624]
	Learning Rate: 0.00156239
	LOSS [training: 0.32150271302290967 | validation: 0.4667789050861431]
	TIME [epoch: 8.18 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5881494699742448		[learning rate: 0.0015586]
		[batch 20/20] avg loss: 0.3364145573222852		[learning rate: 0.0015548]
	Learning Rate: 0.00155483
	LOSS [training: 0.462282013648265 | validation: 0.33367961594766304]
	TIME [epoch: 8.17 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26745510846308324		[learning rate: 0.0015511]
		[batch 20/20] avg loss: 0.3894793968462182		[learning rate: 0.0015473]
	Learning Rate: 0.00154732
	LOSS [training: 0.32846725265465077 | validation: 0.4110434777328684]
	TIME [epoch: 8.16 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3462248243056032		[learning rate: 0.0015436]
		[batch 20/20] avg loss: 0.343431424532966		[learning rate: 0.0015398]
	Learning Rate: 0.00153983
	LOSS [training: 0.3448281244192847 | validation: 0.1452383207441413]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_436.pth
	Model improved!!!
EPOCH 437/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28003489113853536		[learning rate: 0.0015361]
		[batch 20/20] avg loss: 0.3040222031092128		[learning rate: 0.0015324]
	Learning Rate: 0.00153239
	LOSS [training: 0.29202854712387405 | validation: 0.23387207643146546]
	TIME [epoch: 8.18 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3932569074233453		[learning rate: 0.0015287]
		[batch 20/20] avg loss: 0.33643310247476566		[learning rate: 0.001525]
	Learning Rate: 0.00152498
	LOSS [training: 0.3648450049490555 | validation: 0.1776604879368041]
	TIME [epoch: 8.16 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33030499575044153		[learning rate: 0.0015213]
		[batch 20/20] avg loss: 0.3446817192174099		[learning rate: 0.0015176]
	Learning Rate: 0.0015176
	LOSS [training: 0.3374933574839257 | validation: 0.4056975756941704]
	TIME [epoch: 8.15 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32747819824854224		[learning rate: 0.0015139]
		[batch 20/20] avg loss: 0.3292850738790271		[learning rate: 0.0015103]
	Learning Rate: 0.00151026
	LOSS [training: 0.3283816360637846 | validation: 0.381000541674098]
	TIME [epoch: 8.15 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3619796670495171		[learning rate: 0.0015066]
		[batch 20/20] avg loss: 0.4260046466518295		[learning rate: 0.001503]
	Learning Rate: 0.00150296
	LOSS [training: 0.3939921568506734 | validation: 0.2814509084075461]
	TIME [epoch: 8.18 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38037933720335376		[learning rate: 0.0014993]
		[batch 20/20] avg loss: 0.30711862339808166		[learning rate: 0.0014957]
	Learning Rate: 0.00149569
	LOSS [training: 0.34374898030071777 | validation: 0.5187133724858177]
	TIME [epoch: 8.16 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3131955303658249		[learning rate: 0.0014921]
		[batch 20/20] avg loss: 0.45578337697048166		[learning rate: 0.0014885]
	Learning Rate: 0.00148846
	LOSS [training: 0.38448945366815324 | validation: 0.2359500597833023]
	TIME [epoch: 8.15 sec]
EPOCH 444/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33638481232847106		[learning rate: 0.0014849]
		[batch 20/20] avg loss: 0.3385587548783792		[learning rate: 0.0014813]
	Learning Rate: 0.00148126
	LOSS [training: 0.3374717836034251 | validation: 0.14636116913279726]
	TIME [epoch: 8.15 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3147025025812251		[learning rate: 0.0014777]
		[batch 20/20] avg loss: 0.3175897296413875		[learning rate: 0.0014741]
	Learning Rate: 0.0014741
	LOSS [training: 0.3161461161113063 | validation: 0.5295075208915669]
	TIME [epoch: 8.16 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4013232990660035		[learning rate: 0.0014705]
		[batch 20/20] avg loss: 0.36977082191925825		[learning rate: 0.001467]
	Learning Rate: 0.00146697
	LOSS [training: 0.385547060492631 | validation: 0.2683319073391272]
	TIME [epoch: 8.18 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33000546241949424		[learning rate: 0.0014634]
		[batch 20/20] avg loss: 0.25967002624196306		[learning rate: 0.0014599]
	Learning Rate: 0.00145988
	LOSS [training: 0.2948377443307286 | validation: 0.14780967609956816]
	TIME [epoch: 8.16 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.366142394705132		[learning rate: 0.0014563]
		[batch 20/20] avg loss: 0.28465707343596414		[learning rate: 0.0014528]
	Learning Rate: 0.00145282
	LOSS [training: 0.32539973407054806 | validation: 0.13951612040347866]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_448.pth
	Model improved!!!
EPOCH 449/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28187752514034836		[learning rate: 0.0014493]
		[batch 20/20] avg loss: 0.39449313877357917		[learning rate: 0.0014458]
	Learning Rate: 0.00144579
	LOSS [training: 0.33818533195696376 | validation: 0.24315363761102238]
	TIME [epoch: 8.16 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2737220091766674		[learning rate: 0.0014423]
		[batch 20/20] avg loss: 0.2804168789752016		[learning rate: 0.0014388]
	Learning Rate: 0.0014388
	LOSS [training: 0.2770694440759346 | validation: 0.24079035304322993]
	TIME [epoch: 8.18 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23454540387752706		[learning rate: 0.0014353]
		[batch 20/20] avg loss: 0.24820617283184845		[learning rate: 0.0014318]
	Learning Rate: 0.00143184
	LOSS [training: 0.24137578835468773 | validation: 0.2982312676010733]
	TIME [epoch: 8.15 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24643096674297765		[learning rate: 0.0014284]
		[batch 20/20] avg loss: 0.3585660640612062		[learning rate: 0.0014249]
	Learning Rate: 0.00142492
	LOSS [training: 0.30249851540209194 | validation: 0.26314144066246037]
	TIME [epoch: 8.16 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2785855908148671		[learning rate: 0.0014215]
		[batch 20/20] avg loss: 0.36530585095665674		[learning rate: 0.001418]
	Learning Rate: 0.00141803
	LOSS [training: 0.32194572088576195 | validation: 0.1538658826206919]
	TIME [epoch: 8.16 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.346857301954787		[learning rate: 0.0014146]
		[batch 20/20] avg loss: 0.46630425459242836		[learning rate: 0.0014112]
	Learning Rate: 0.00141117
	LOSS [training: 0.4065807782736077 | validation: 0.2784497231277451]
	TIME [epoch: 8.17 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22063965284051745		[learning rate: 0.0014078]
		[batch 20/20] avg loss: 0.36431992505968386		[learning rate: 0.0014043]
	Learning Rate: 0.00140434
	LOSS [training: 0.2924797889501006 | validation: 0.14262479354999136]
	TIME [epoch: 8.17 sec]
EPOCH 456/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.41393907661638146		[learning rate: 0.0014009]
		[batch 20/20] avg loss: 0.316732835183753		[learning rate: 0.0013976]
	Learning Rate: 0.00139755
	LOSS [training: 0.36533595590006723 | validation: 0.1811322626025337]
	TIME [epoch: 8.16 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2916351553771096		[learning rate: 0.0013942]
		[batch 20/20] avg loss: 0.25224947661035885		[learning rate: 0.0013908]
	Learning Rate: 0.0013908
	LOSS [training: 0.27194231599373425 | validation: 0.18518642759702603]
	TIME [epoch: 8.16 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34434816241568067		[learning rate: 0.0013874]
		[batch 20/20] avg loss: 0.39410699284243106		[learning rate: 0.0013841]
	Learning Rate: 0.00138407
	LOSS [training: 0.36922757762905584 | validation: 0.408274075041209]
	TIME [epoch: 8.17 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29105413089109167		[learning rate: 0.0013807]
		[batch 20/20] avg loss: 0.3372136223582288		[learning rate: 0.0013774]
	Learning Rate: 0.00137738
	LOSS [training: 0.3141338766246603 | validation: 0.3627960297056505]
	TIME [epoch: 8.19 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23731256379635718		[learning rate: 0.001374]
		[batch 20/20] avg loss: 0.43721937773797787		[learning rate: 0.0013707]
	Learning Rate: 0.00137072
	LOSS [training: 0.3372659707671675 | validation: 0.17810069464224798]
	TIME [epoch: 8.17 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27262816375867044		[learning rate: 0.0013674]
		[batch 20/20] avg loss: 0.21922844190286686		[learning rate: 0.0013641]
	Learning Rate: 0.00136409
	LOSS [training: 0.24592830283076866 | validation: 0.4467246492967608]
	TIME [epoch: 8.16 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31634849985665786		[learning rate: 0.0013608]
		[batch 20/20] avg loss: 0.2532829042006759		[learning rate: 0.0013575]
	Learning Rate: 0.00135749
	LOSS [training: 0.28481570202866696 | validation: 0.11116026777325204]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_462.pth
	Model improved!!!
EPOCH 463/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1896895099396731		[learning rate: 0.0013542]
		[batch 20/20] avg loss: 0.29848394272899037		[learning rate: 0.0013509]
	Learning Rate: 0.00135093
	LOSS [training: 0.24408672633433168 | validation: 0.12845303035420622]
	TIME [epoch: 8.18 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27854992142488905		[learning rate: 0.0013477]
		[batch 20/20] avg loss: 0.27291007262993483		[learning rate: 0.0013444]
	Learning Rate: 0.00134439
	LOSS [training: 0.27572999702741197 | validation: 0.2797872159214611]
	TIME [epoch: 8.17 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33223161822149233		[learning rate: 0.0013411]
		[batch 20/20] avg loss: 0.20888042206890148		[learning rate: 0.0013379]
	Learning Rate: 0.00133789
	LOSS [training: 0.270556020145197 | validation: 0.600398435567383]
	TIME [epoch: 8.14 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31125867321990097		[learning rate: 0.0013347]
		[batch 20/20] avg loss: 0.20710316370627346		[learning rate: 0.0013314]
	Learning Rate: 0.00133142
	LOSS [training: 0.2591809184630872 | validation: 0.1311815527846572]
	TIME [epoch: 8.15 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32091790237012197		[learning rate: 0.0013282]
		[batch 20/20] avg loss: 0.26704359476295203		[learning rate: 0.001325]
	Learning Rate: 0.00132498
	LOSS [training: 0.293980748566537 | validation: 0.13543315952266227]
	TIME [epoch: 8.16 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25621795519006063		[learning rate: 0.0013218]
		[batch 20/20] avg loss: 0.2265928152522338		[learning rate: 0.0013186]
	Learning Rate: 0.00131858
	LOSS [training: 0.2414053852211472 | validation: 0.15918227300783297]
	TIME [epoch: 8.18 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2677703370974076		[learning rate: 0.0013154]
		[batch 20/20] avg loss: 0.26647110945732044		[learning rate: 0.0013122]
	Learning Rate: 0.0013122
	LOSS [training: 0.26712072327736397 | validation: 0.2548317509313719]
	TIME [epoch: 8.17 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2320731121943569		[learning rate: 0.001309]
		[batch 20/20] avg loss: 0.364519912078355		[learning rate: 0.0013059]
	Learning Rate: 0.00130585
	LOSS [training: 0.29829651213635594 | validation: 0.3553478861605393]
	TIME [epoch: 8.15 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3211186417039331		[learning rate: 0.0013027]
		[batch 20/20] avg loss: 0.3869187079845214		[learning rate: 0.0012995]
	Learning Rate: 0.00129954
	LOSS [training: 0.3540186748442272 | validation: 0.1580810723830886]
	TIME [epoch: 8.16 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21702100675270075		[learning rate: 0.0012964]
		[batch 20/20] avg loss: 0.32946324690836487		[learning rate: 0.0012933]
	Learning Rate: 0.00129326
	LOSS [training: 0.2732421268305328 | validation: 0.39153069868784124]
	TIME [epoch: 8.18 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3182134841157194		[learning rate: 0.0012901]
		[batch 20/20] avg loss: 0.3393131518655344		[learning rate: 0.001287]
	Learning Rate: 0.001287
	LOSS [training: 0.3287633179906269 | validation: 0.8038794784513008]
	TIME [epoch: 8.17 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2702050882168463		[learning rate: 0.0012839]
		[batch 20/20] avg loss: 0.29528797869450624		[learning rate: 0.0012808]
	Learning Rate: 0.00128078
	LOSS [training: 0.28274653345567624 | validation: 0.5393781391327468]
	TIME [epoch: 8.16 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3254307702906514		[learning rate: 0.0012777]
		[batch 20/20] avg loss: 0.23243329420516962		[learning rate: 0.0012746]
	Learning Rate: 0.00127458
	LOSS [training: 0.2789320322479105 | validation: 0.4507689965598496]
	TIME [epoch: 8.15 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30599115558015716		[learning rate: 0.0012715]
		[batch 20/20] avg loss: 0.2078977643692498		[learning rate: 0.0012684]
	Learning Rate: 0.00126842
	LOSS [training: 0.25694445997470344 | validation: 1.076497893659427]
	TIME [epoch: 8.16 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3692655899113559		[learning rate: 0.0012653]
		[batch 20/20] avg loss: 0.2294204783683642		[learning rate: 0.0012623]
	Learning Rate: 0.00126229
	LOSS [training: 0.2993430341398601 | validation: 0.4449675786185736]
	TIME [epoch: 8.19 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2803202331032575		[learning rate: 0.0012592]
		[batch 20/20] avg loss: 0.27888348574616256		[learning rate: 0.0012562]
	Learning Rate: 0.00125618
	LOSS [training: 0.27960185942471 | validation: 1.090245089006063]
	TIME [epoch: 8.16 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3970356424570708		[learning rate: 0.0012531]
		[batch 20/20] avg loss: 0.2315576772801579		[learning rate: 0.0012501]
	Learning Rate: 0.00125011
	LOSS [training: 0.31429665986861427 | validation: 0.4269176503855982]
	TIME [epoch: 8.16 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2523633279536482		[learning rate: 0.0012471]
		[batch 20/20] avg loss: 0.2889256437362808		[learning rate: 0.0012441]
	Learning Rate: 0.00124406
	LOSS [training: 0.2706444858449645 | validation: 0.21078217945337235]
	TIME [epoch: 8.16 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2743937199158034		[learning rate: 0.0012411]
		[batch 20/20] avg loss: 0.24519116653071316		[learning rate: 0.001238]
	Learning Rate: 0.00123805
	LOSS [training: 0.2597924432232584 | validation: 0.12344794891025652]
	TIME [epoch: 8.18 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2789263372392595		[learning rate: 0.001235]
		[batch 20/20] avg loss: 0.261732322481719		[learning rate: 0.0012321]
	Learning Rate: 0.00123206
	LOSS [training: 0.2703293298604893 | validation: 0.3480059848338698]
	TIME [epoch: 8.16 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34710455033408805		[learning rate: 0.0012291]
		[batch 20/20] avg loss: 0.3350055796840673		[learning rate: 0.0012261]
	Learning Rate: 0.0012261
	LOSS [training: 0.3410550650090777 | validation: 0.3674571798786259]
	TIME [epoch: 8.16 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3801887040447476		[learning rate: 0.0012231]
		[batch 20/20] avg loss: 0.2473411835307094		[learning rate: 0.0012202]
	Learning Rate: 0.00122017
	LOSS [training: 0.31376494378772857 | validation: 0.12940678295312555]
	TIME [epoch: 8.16 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2427834897427832		[learning rate: 0.0012172]
		[batch 20/20] avg loss: 0.2926431641126114		[learning rate: 0.0012143]
	Learning Rate: 0.00121427
	LOSS [training: 0.26771332692769734 | validation: 0.19336053101793477]
	TIME [epoch: 8.18 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1990128760066151		[learning rate: 0.0012113]
		[batch 20/20] avg loss: 0.26257016811601275		[learning rate: 0.0012084]
	Learning Rate: 0.0012084
	LOSS [training: 0.23079152206131398 | validation: 0.32829853788126534]
	TIME [epoch: 8.17 sec]
EPOCH 487/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2425300077778883		[learning rate: 0.0012055]
		[batch 20/20] avg loss: 0.21917126500133072		[learning rate: 0.0012026]
	Learning Rate: 0.00120256
	LOSS [training: 0.2308506363896095 | validation: 0.361958677131216]
	TIME [epoch: 8.16 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3832341078306072		[learning rate: 0.0011996]
		[batch 20/20] avg loss: 0.23846609745710942		[learning rate: 0.0011967]
	Learning Rate: 0.00119674
	LOSS [training: 0.3108501026438583 | validation: 0.3452600432033284]
	TIME [epoch: 8.16 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27205018362718614		[learning rate: 0.0011938]
		[batch 20/20] avg loss: 0.25252410758381216		[learning rate: 0.001191]
	Learning Rate: 0.00119095
	LOSS [training: 0.26228714560549904 | validation: 0.23285257240955287]
	TIME [epoch: 8.16 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2569853175372335		[learning rate: 0.0011881]
		[batch 20/20] avg loss: 0.21773959848568375		[learning rate: 0.0011852]
	Learning Rate: 0.00118519
	LOSS [training: 0.23736245801145867 | validation: 0.10431804025184868]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_490.pth
	Model improved!!!
EPOCH 491/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25430188417400246		[learning rate: 0.0011823]
		[batch 20/20] avg loss: 0.2995624198637125		[learning rate: 0.0011795]
	Learning Rate: 0.00117946
	LOSS [training: 0.27693215201885746 | validation: 0.24280443165272791]
	TIME [epoch: 8.17 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2016677475546386		[learning rate: 0.0011766]
		[batch 20/20] avg loss: 0.34997450736144425		[learning rate: 0.0011738]
	Learning Rate: 0.00117376
	LOSS [training: 0.27582112745804144 | validation: 0.4816368558650358]
	TIME [epoch: 8.16 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25718965464581994		[learning rate: 0.0011709]
		[batch 20/20] avg loss: 0.27408363494555876		[learning rate: 0.0011681]
	Learning Rate: 0.00116808
	LOSS [training: 0.2656366447956893 | validation: 0.1758829091148204]
	TIME [epoch: 8.16 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23113619044418737		[learning rate: 0.0011653]
		[batch 20/20] avg loss: 0.26548241042767085		[learning rate: 0.0011624]
	Learning Rate: 0.00116243
	LOSS [training: 0.24830930043592905 | validation: 0.12777420658381192]
	TIME [epoch: 8.19 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2902104913650639		[learning rate: 0.0011596]
		[batch 20/20] avg loss: 0.29421412029985		[learning rate: 0.0011568]
	Learning Rate: 0.00115681
	LOSS [training: 0.29221230583245694 | validation: 0.12902161304068904]
	TIME [epoch: 8.16 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.43814060822436013		[learning rate: 0.001154]
		[batch 20/20] avg loss: 0.28394788349885897		[learning rate: 0.0011512]
	Learning Rate: 0.00115122
	LOSS [training: 0.3610442458616095 | validation: 0.22844890478785354]
	TIME [epoch: 8.16 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30703880721698834		[learning rate: 0.0011484]
		[batch 20/20] avg loss: 0.22866572909461072		[learning rate: 0.0011457]
	Learning Rate: 0.00114565
	LOSS [training: 0.26785226815579954 | validation: 0.22781204519865195]
	TIME [epoch: 8.15 sec]
EPOCH 498/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2448808560558708		[learning rate: 0.0011429]
		[batch 20/20] avg loss: 0.23617046535759217		[learning rate: 0.0011401]
	Learning Rate: 0.00114011
	LOSS [training: 0.2405256607067315 | validation: 0.3177043956785042]
	TIME [epoch: 8.18 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19855729565118432		[learning rate: 0.0011374]
		[batch 20/20] avg loss: 0.28960478881723545		[learning rate: 0.0011346]
	Learning Rate: 0.0011346
	LOSS [training: 0.2440810422342099 | validation: 0.17514584846859196]
	TIME [epoch: 8.17 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17904565687991839		[learning rate: 0.0011319]
		[batch 20/20] avg loss: 0.2089387782717232		[learning rate: 0.0011291]
	Learning Rate: 0.00112911
	LOSS [training: 0.1939922175758208 | validation: 0.18114431853108365]
	TIME [epoch: 8.17 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2655410506718366		[learning rate: 0.0011264]
		[batch 20/20] avg loss: 0.25902676182430845		[learning rate: 0.0011237]
	Learning Rate: 0.00112365
	LOSS [training: 0.2622839062480725 | validation: 0.13532323277895303]
	TIME [epoch: 8.16 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22112659866428866		[learning rate: 0.0011209]
		[batch 20/20] avg loss: 0.3787967358219322		[learning rate: 0.0011182]
	Learning Rate: 0.00111822
	LOSS [training: 0.2999616672431106 | validation: 0.1555144049650797]
	TIME [epoch: 8.16 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18716336889171678		[learning rate: 0.0011155]
		[batch 20/20] avg loss: 0.24567312027607707		[learning rate: 0.0011128]
	Learning Rate: 0.00111281
	LOSS [training: 0.2164182445838969 | validation: 0.1746927751771853]
	TIME [epoch: 8.19 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3275743137261242		[learning rate: 0.0011101]
		[batch 20/20] avg loss: 0.2566401754956999		[learning rate: 0.0011074]
	Learning Rate: 0.00110743
	LOSS [training: 0.29210724461091203 | validation: 0.15415072023369586]
	TIME [epoch: 8.16 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23801093240081905		[learning rate: 0.0011047]
		[batch 20/20] avg loss: 0.29180802254424654		[learning rate: 0.0011021]
	Learning Rate: 0.00110207
	LOSS [training: 0.2649094774725328 | validation: 0.15963695952385704]
	TIME [epoch: 8.15 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2610584762727987		[learning rate: 0.0010994]
		[batch 20/20] avg loss: 0.22068802105228222		[learning rate: 0.0010967]
	Learning Rate: 0.00109674
	LOSS [training: 0.24087324866254045 | validation: 0.16356064545167748]
	TIME [epoch: 8.16 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20874839827726516		[learning rate: 0.0010941]
		[batch 20/20] avg loss: 0.25170369502738615		[learning rate: 0.0010914]
	Learning Rate: 0.00109144
	LOSS [training: 0.2302260466523256 | validation: 0.301813881860713]
	TIME [epoch: 8.18 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2133624578049437		[learning rate: 0.0010888]
		[batch 20/20] avg loss: 0.2821363750311333		[learning rate: 0.0010862]
	Learning Rate: 0.00108616
	LOSS [training: 0.24774941641803858 | validation: 0.138864132625745]
	TIME [epoch: 8.17 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2294844758639128		[learning rate: 0.0010835]
		[batch 20/20] avg loss: 0.33427612211945945		[learning rate: 0.0010809]
	Learning Rate: 0.00108091
	LOSS [training: 0.28188029899168615 | validation: 0.11139832452082783]
	TIME [epoch: 8.16 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20697691977604432		[learning rate: 0.0010783]
		[batch 20/20] avg loss: 0.22654614061205933		[learning rate: 0.0010757]
	Learning Rate: 0.00107568
	LOSS [training: 0.2167615301940518 | validation: 0.11419217615889524]
	TIME [epoch: 8.15 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18020577850714223		[learning rate: 0.0010731]
		[batch 20/20] avg loss: 0.2845872936335948		[learning rate: 0.0010705]
	Learning Rate: 0.00107048
	LOSS [training: 0.2323965360703685 | validation: 0.23168628773672492]
	TIME [epoch: 8.16 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23237675066621852		[learning rate: 0.0010679]
		[batch 20/20] avg loss: 0.20872680771980218		[learning rate: 0.0010653]
	Learning Rate: 0.0010653
	LOSS [training: 0.22055177919301036 | validation: 0.2279308025301188]
	TIME [epoch: 8.18 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17903617377977749		[learning rate: 0.0010627]
		[batch 20/20] avg loss: 0.23440850997784063		[learning rate: 0.0010602]
	Learning Rate: 0.00106015
	LOSS [training: 0.20672234187880906 | validation: 0.4234261131700821]
	TIME [epoch: 8.16 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28768437303017186		[learning rate: 0.0010576]
		[batch 20/20] avg loss: 0.26178709248839227		[learning rate: 0.001055]
	Learning Rate: 0.00105503
	LOSS [training: 0.2747357327592821 | validation: 0.3781226460681661]
	TIME [epoch: 8.15 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29296303060640877		[learning rate: 0.0010525]
		[batch 20/20] avg loss: 0.3847053299599469		[learning rate: 0.0010499]
	Learning Rate: 0.00104992
	LOSS [training: 0.33883418028317785 | validation: 0.3910183485675706]
	TIME [epoch: 8.16 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26420612770186025		[learning rate: 0.0010474]
		[batch 20/20] avg loss: 0.28360701591067494		[learning rate: 0.0010448]
	Learning Rate: 0.00104485
	LOSS [training: 0.2739065718062676 | validation: 0.2225722447742596]
	TIME [epoch: 8.18 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26632063365183567		[learning rate: 0.0010423]
		[batch 20/20] avg loss: 0.2808164100138574		[learning rate: 0.0010398]
	Learning Rate: 0.00103979
	LOSS [training: 0.2735685218328465 | validation: 0.195561709007533]
	TIME [epoch: 8.16 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20164074478902022		[learning rate: 0.0010373]
		[batch 20/20] avg loss: 0.22139116891529173		[learning rate: 0.0010348]
	Learning Rate: 0.00103477
	LOSS [training: 0.211515956852156 | validation: 0.12978285228668537]
	TIME [epoch: 8.16 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25250650726943824		[learning rate: 0.0010323]
		[batch 20/20] avg loss: 0.27197548025397544		[learning rate: 0.0010298]
	Learning Rate: 0.00102976
	LOSS [training: 0.26224099376170684 | validation: 0.24009150959679426]
	TIME [epoch: 8.16 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1739070508353955		[learning rate: 0.0010273]
		[batch 20/20] avg loss: 0.1903049216252684		[learning rate: 0.0010248]
	Learning Rate: 0.00102478
	LOSS [training: 0.18210598623033197 | validation: 0.19446701935769395]
	TIME [epoch: 8.16 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2295974924146369		[learning rate: 0.0010223]
		[batch 20/20] avg loss: 0.19911589371787783		[learning rate: 0.0010198]
	Learning Rate: 0.00101983
	LOSS [training: 0.21435669306625735 | validation: 0.16856083169257416]
	TIME [epoch: 8.19 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22601385777320124		[learning rate: 0.0010174]
		[batch 20/20] avg loss: 0.31269809475779076		[learning rate: 0.0010149]
	Learning Rate: 0.00101489
	LOSS [training: 0.2693559762654959 | validation: 0.13064447885980313]
	TIME [epoch: 8.16 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2594545254886214		[learning rate: 0.0010124]
		[batch 20/20] avg loss: 0.2087958749001199		[learning rate: 0.00101]
	Learning Rate: 0.00100999
	LOSS [training: 0.23412520019437064 | validation: 0.12236215605745891]
	TIME [epoch: 8.16 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27932923988729996		[learning rate: 0.0010075]
		[batch 20/20] avg loss: 0.20554922793903171		[learning rate: 0.0010051]
	Learning Rate: 0.0010051
	LOSS [training: 0.24243923391316585 | validation: 0.2126947694277223]
	TIME [epoch: 8.17 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23677211024975256		[learning rate: 0.0010027]
		[batch 20/20] avg loss: 0.17825993865005235		[learning rate: 0.0010002]
	Learning Rate: 0.00100024
	LOSS [training: 0.20751602444990244 | validation: 0.16488832460124644]
	TIME [epoch: 8.19 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15715489067412242		[learning rate: 0.00099782]
		[batch 20/20] avg loss: 0.3067612551237004		[learning rate: 0.00099541]
	Learning Rate: 0.000995405
	LOSS [training: 0.2319580728989114 | validation: 0.11771988215911545]
	TIME [epoch: 8.17 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19769365737642233		[learning rate: 0.000993]
		[batch 20/20] avg loss: 0.19218477325391148		[learning rate: 0.00099059]
	Learning Rate: 0.000990592
	LOSS [training: 0.19493921531516692 | validation: 0.6592829510178855]
	TIME [epoch: 8.16 sec]
EPOCH 528/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2618973865420781		[learning rate: 0.00098819]
		[batch 20/20] avg loss: 0.24609445127379498		[learning rate: 0.0009858]
	Learning Rate: 0.000985801
	LOSS [training: 0.2539959189079365 | validation: 0.34586855825450324]
	TIME [epoch: 8.16 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18672235895687878		[learning rate: 0.00098341]
		[batch 20/20] avg loss: 0.2364889210903446		[learning rate: 0.00098103]
	Learning Rate: 0.000981034
	LOSS [training: 0.21160564002361165 | validation: 0.21337861899689406]
	TIME [epoch: 8.18 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33625842791513477		[learning rate: 0.00097866]
		[batch 20/20] avg loss: 0.21165958166767393		[learning rate: 0.00097629]
	Learning Rate: 0.00097629
	LOSS [training: 0.27395900479140434 | validation: 0.15581132601056058]
	TIME [epoch: 8.17 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21284061872530918		[learning rate: 0.00097393]
		[batch 20/20] avg loss: 0.2305281978124573		[learning rate: 0.00097157]
	Learning Rate: 0.000971569
	LOSS [training: 0.2216844082688832 | validation: 0.2919016739148673]
	TIME [epoch: 8.16 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19210622514737957		[learning rate: 0.00096922]
		[batch 20/20] avg loss: 0.19506917748475688		[learning rate: 0.00096687]
	Learning Rate: 0.000966871
	LOSS [training: 0.19358770131606823 | validation: 0.26774714211887773]
	TIME [epoch: 8.16 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18882345447852034		[learning rate: 0.00096453]
		[batch 20/20] avg loss: 0.20160822991658578		[learning rate: 0.00096219]
	Learning Rate: 0.000962195
	LOSS [training: 0.19521584219755306 | validation: 0.30013971090235064]
	TIME [epoch: 8.16 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24658180728937545		[learning rate: 0.00095987]
		[batch 20/20] avg loss: 0.2380699353004681		[learning rate: 0.00095754]
	Learning Rate: 0.000957542
	LOSS [training: 0.24232587129492175 | validation: 0.3734262612010558]
	TIME [epoch: 8.18 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2533681170879972		[learning rate: 0.00095522]
		[batch 20/20] avg loss: 0.26419932294191456		[learning rate: 0.00095291]
	Learning Rate: 0.000952912
	LOSS [training: 0.2587837200149559 | validation: 0.27229306400231673]
	TIME [epoch: 8.17 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21373819626505952		[learning rate: 0.0009506]
		[batch 20/20] avg loss: 0.15573102823362092		[learning rate: 0.0009483]
	Learning Rate: 0.000948304
	LOSS [training: 0.18473461224934024 | validation: 0.11157392055977247]
	TIME [epoch: 8.16 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18848010065923465		[learning rate: 0.00094601]
		[batch 20/20] avg loss: 0.2351727132875155		[learning rate: 0.00094372]
	Learning Rate: 0.000943718
	LOSS [training: 0.21182640697337507 | validation: 0.119239270562518]
	TIME [epoch: 8.17 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20703254494093465		[learning rate: 0.00094143]
		[batch 20/20] avg loss: 0.2703559557437943		[learning rate: 0.00093915]
	Learning Rate: 0.000939154
	LOSS [training: 0.2386942503423645 | validation: 0.21025604880315787]
	TIME [epoch: 8.18 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18633179977109382		[learning rate: 0.00093688]
		[batch 20/20] avg loss: 0.20462239300934754		[learning rate: 0.00093461]
	Learning Rate: 0.000934613
	LOSS [training: 0.19547709639022068 | validation: 0.12716140257054456]
	TIME [epoch: 8.17 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17913988275921566		[learning rate: 0.00093235]
		[batch 20/20] avg loss: 0.17407438604912776		[learning rate: 0.00093009]
	Learning Rate: 0.000930093
	LOSS [training: 0.1766071344041717 | validation: 0.09476340175916168]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_540.pth
	Model improved!!!
EPOCH 541/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16272763875662216		[learning rate: 0.00092784]
		[batch 20/20] avg loss: 0.20830453934356732		[learning rate: 0.0009256]
	Learning Rate: 0.000925595
	LOSS [training: 0.18551608905009476 | validation: 0.17904360746625203]
	TIME [epoch: 8.17 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19136804918905337		[learning rate: 0.00092335]
		[batch 20/20] avg loss: 0.17953077147974467		[learning rate: 0.00092112]
	Learning Rate: 0.000921119
	LOSS [training: 0.18544941033439905 | validation: 0.2648321370597814]
	TIME [epoch: 8.17 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18749774667538524		[learning rate: 0.00091889]
		[batch 20/20] avg loss: 0.16053039180667		[learning rate: 0.00091666]
	Learning Rate: 0.000916665
	LOSS [training: 0.17401406924102764 | validation: 0.10943635277947884]
	TIME [epoch: 8.19 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2063648472156335		[learning rate: 0.00091445]
		[batch 20/20] avg loss: 0.23080410691160624		[learning rate: 0.00091223]
	Learning Rate: 0.000912232
	LOSS [training: 0.2185844770636199 | validation: 0.26726455061377596]
	TIME [epoch: 8.16 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2767075297883923		[learning rate: 0.00091002]
		[batch 20/20] avg loss: 0.2104194948569686		[learning rate: 0.00090782]
	Learning Rate: 0.000907821
	LOSS [training: 0.24356351232268048 | validation: 0.10598069747323682]
	TIME [epoch: 8.16 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1835360059735444		[learning rate: 0.00090562]
		[batch 20/20] avg loss: 0.2004369099162558		[learning rate: 0.00090343]
	Learning Rate: 0.00090343
	LOSS [training: 0.1919864579449001 | validation: 0.35090888641206497]
	TIME [epoch: 8.16 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2197488438166752		[learning rate: 0.00090124]
		[batch 20/20] avg loss: 0.1678108912122125		[learning rate: 0.00089906]
	Learning Rate: 0.000899062
	LOSS [training: 0.19377986751444387 | validation: 0.45916273312578126]
	TIME [epoch: 8.19 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29836088078806516		[learning rate: 0.00089689]
		[batch 20/20] avg loss: 0.19497623459958072		[learning rate: 0.00089471]
	Learning Rate: 0.000894714
	LOSS [training: 0.24666855769382293 | validation: 0.10759561897025911]
	TIME [epoch: 8.16 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.190780115298013		[learning rate: 0.00089255]
		[batch 20/20] avg loss: 0.2506412832527335		[learning rate: 0.00089039]
	Learning Rate: 0.000890387
	LOSS [training: 0.22071069927537326 | validation: 0.12052529406297058]
	TIME [epoch: 8.15 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21247699259121555		[learning rate: 0.00088823]
		[batch 20/20] avg loss: 0.19404215023978436		[learning rate: 0.00088608]
	Learning Rate: 0.000886081
	LOSS [training: 0.20325957141549994 | validation: 0.12364915117097827]
	TIME [epoch: 8.16 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25195343693050487		[learning rate: 0.00088394]
		[batch 20/20] avg loss: 0.19846000763908428		[learning rate: 0.0008818]
	Learning Rate: 0.000881797
	LOSS [training: 0.22520672228479457 | validation: 0.21412208611145025]
	TIME [epoch: 8.18 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20809568438860557		[learning rate: 0.00087966]
		[batch 20/20] avg loss: 0.19367539744190915		[learning rate: 0.00087753]
	Learning Rate: 0.000877532
	LOSS [training: 0.20088554091525737 | validation: 0.11768914017837971]
	TIME [epoch: 8.17 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19699034139260674		[learning rate: 0.00087541]
		[batch 20/20] avg loss: 0.18865705714283168		[learning rate: 0.00087329]
	Learning Rate: 0.000873289
	LOSS [training: 0.19282369926771925 | validation: 0.08323106495905754]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_553.pth
	Model improved!!!
EPOCH 554/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16305983907704324		[learning rate: 0.00087117]
		[batch 20/20] avg loss: 0.1934547073186645		[learning rate: 0.00086907]
	Learning Rate: 0.000869066
	LOSS [training: 0.17825727319785384 | validation: 0.14924593226035915]
	TIME [epoch: 8.2 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2030741922899552		[learning rate: 0.00086696]
		[batch 20/20] avg loss: 0.1890539027944969		[learning rate: 0.00086486]
	Learning Rate: 0.000864863
	LOSS [training: 0.19606404754222606 | validation: 0.1155531048200723]
	TIME [epoch: 8.19 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1366685079257856		[learning rate: 0.00086277]
		[batch 20/20] avg loss: 0.16731904080836654		[learning rate: 0.00086068]
	Learning Rate: 0.000860681
	LOSS [training: 0.1519937743670761 | validation: 0.12430494367622964]
	TIME [epoch: 8.21 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2246857530567362		[learning rate: 0.0008586]
		[batch 20/20] avg loss: 0.1774430976348972		[learning rate: 0.00085652]
	Learning Rate: 0.000856519
	LOSS [training: 0.2010644253458167 | validation: 0.19351035841687098]
	TIME [epoch: 8.19 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.251922721065338		[learning rate: 0.00085445]
		[batch 20/20] avg loss: 0.17317655501668508		[learning rate: 0.00085238]
	Learning Rate: 0.000852377
	LOSS [training: 0.21254963804101154 | validation: 0.08682993707480123]
	TIME [epoch: 8.18 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18448354815905998		[learning rate: 0.00085031]
		[batch 20/20] avg loss: 0.17093039582469435		[learning rate: 0.00084825]
	Learning Rate: 0.000848255
	LOSS [training: 0.17770697199187718 | validation: 0.21952697326802953]
	TIME [epoch: 8.18 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2091230512497079		[learning rate: 0.0008462]
		[batch 20/20] avg loss: 0.1821309809283418		[learning rate: 0.00084415]
	Learning Rate: 0.000844153
	LOSS [training: 0.19562701608902483 | validation: 0.38198767935354416]
	TIME [epoch: 8.21 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20177165645832762		[learning rate: 0.00084211]
		[batch 20/20] avg loss: 0.25714406575621773		[learning rate: 0.00084007]
	Learning Rate: 0.000840071
	LOSS [training: 0.22945786110727268 | validation: 0.1102786723020034]
	TIME [epoch: 8.19 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1488941372187887		[learning rate: 0.00083804]
		[batch 20/20] avg loss: 0.19189763922677813		[learning rate: 0.00083601]
	Learning Rate: 0.000836008
	LOSS [training: 0.17039588822278343 | validation: 0.6902589203101067]
	TIME [epoch: 8.19 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29038761989352		[learning rate: 0.00083398]
		[batch 20/20] avg loss: 0.15711472654981407		[learning rate: 0.00083197]
	Learning Rate: 0.000831965
	LOSS [training: 0.22375117322166704 | validation: 0.09807537541458011]
	TIME [epoch: 8.19 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17949168293738513		[learning rate: 0.00082995]
		[batch 20/20] avg loss: 0.15840466134111691		[learning rate: 0.00082794]
	Learning Rate: 0.000827942
	LOSS [training: 0.168948172139251 | validation: 0.22028752314109962]
	TIME [epoch: 8.2 sec]
EPOCH 565/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.172699939804964		[learning rate: 0.00082594]
		[batch 20/20] avg loss: 0.17736854857385986		[learning rate: 0.00082394]
	Learning Rate: 0.000823938
	LOSS [training: 0.17503424418941194 | validation: 0.2010413174432053]
	TIME [epoch: 8.2 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18466178648945278		[learning rate: 0.00082194]
		[batch 20/20] avg loss: 0.1966203128579514		[learning rate: 0.00081995]
	Learning Rate: 0.000819954
	LOSS [training: 0.19064104967370207 | validation: 0.20235455852377218]
	TIME [epoch: 8.19 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16308537934678205		[learning rate: 0.00081797]
		[batch 20/20] avg loss: 0.17296302161409763		[learning rate: 0.00081599]
	Learning Rate: 0.000815989
	LOSS [training: 0.16802420048043984 | validation: 0.4256666983938663]
	TIME [epoch: 8.19 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21610454671839233		[learning rate: 0.00081401]
		[batch 20/20] avg loss: 0.15991105868705963		[learning rate: 0.00081204]
	Learning Rate: 0.000812043
	LOSS [training: 0.188007802702726 | validation: 0.23647397104312648]
	TIME [epoch: 8.19 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16656418822738786		[learning rate: 0.00081008]
		[batch 20/20] avg loss: 0.15661929565474203		[learning rate: 0.00080812]
	Learning Rate: 0.000808116
	LOSS [training: 0.16159174194106493 | validation: 0.290441520784465]
	TIME [epoch: 8.21 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1706311659101284		[learning rate: 0.00080616]
		[batch 20/20] avg loss: 0.19642203321754664		[learning rate: 0.00080421]
	Learning Rate: 0.000804208
	LOSS [training: 0.1835265995638375 | validation: 0.4610257050555381]
	TIME [epoch: 8.19 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23672383745624		[learning rate: 0.00080226]
		[batch 20/20] avg loss: 0.25744653762501124		[learning rate: 0.00080032]
	Learning Rate: 0.000800319
	LOSS [training: 0.24708518754062564 | validation: 0.09786633533907785]
	TIME [epoch: 8.19 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14878787934293508		[learning rate: 0.00079838]
		[batch 20/20] avg loss: 0.14645196194327917		[learning rate: 0.00079645]
	Learning Rate: 0.000796449
	LOSS [training: 0.1476199206431071 | validation: 0.09149009473619363]
	TIME [epoch: 8.18 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13861538772493265		[learning rate: 0.00079452]
		[batch 20/20] avg loss: 0.22276603643198883		[learning rate: 0.0007926]
	Learning Rate: 0.000792597
	LOSS [training: 0.18069071207846074 | validation: 0.353891223101828]
	TIME [epoch: 8.21 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21384912739766326		[learning rate: 0.00079068]
		[batch 20/20] avg loss: 0.2657552787307008		[learning rate: 0.00078876]
	Learning Rate: 0.000788765
	LOSS [training: 0.23980220306418207 | validation: 0.08790257232478992]
	TIME [epoch: 8.2 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18832793993525004		[learning rate: 0.00078686]
		[batch 20/20] avg loss: 0.19466592837884583		[learning rate: 0.00078495]
	Learning Rate: 0.00078495
	LOSS [training: 0.19149693415704794 | validation: 0.11870327588937109]
	TIME [epoch: 8.19 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15972022429822008		[learning rate: 0.00078305]
		[batch 20/20] avg loss: 0.16751184001988212		[learning rate: 0.00078115]
	Learning Rate: 0.000781154
	LOSS [training: 0.16361603215905107 | validation: 0.10460575492587229]
	TIME [epoch: 8.18 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16558424600376187		[learning rate: 0.00077926]
		[batch 20/20] avg loss: 0.19051138810793938		[learning rate: 0.00077738]
	Learning Rate: 0.000777377
	LOSS [training: 0.17804781705585065 | validation: 0.10005973292131515]
	TIME [epoch: 8.19 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18543865019129718		[learning rate: 0.00077549]
		[batch 20/20] avg loss: 0.16728301677005852		[learning rate: 0.00077362]
	Learning Rate: 0.000773618
	LOSS [training: 0.17636083348067783 | validation: 0.08806178969678341]
	TIME [epoch: 8.21 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23987417684490125		[learning rate: 0.00077174]
		[batch 20/20] avg loss: 0.17715584159726414		[learning rate: 0.00076988]
	Learning Rate: 0.000769877
	LOSS [training: 0.20851500922108274 | validation: 0.4515642106368074]
	TIME [epoch: 8.19 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27132345715802		[learning rate: 0.00076801]
		[batch 20/20] avg loss: 0.18170475239476894		[learning rate: 0.00076615]
	Learning Rate: 0.000766154
	LOSS [training: 0.2265141047763945 | validation: 0.3111990113559482]
	TIME [epoch: 8.19 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2056794735830056		[learning rate: 0.0007643]
		[batch 20/20] avg loss: 0.15194311443440428		[learning rate: 0.00076245]
	Learning Rate: 0.000762448
	LOSS [training: 0.1788112940087049 | validation: 0.11904944574672732]
	TIME [epoch: 8.19 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2326745874343908		[learning rate: 0.0007606]
		[batch 20/20] avg loss: 0.15451860734786924		[learning rate: 0.00075876]
	Learning Rate: 0.000758761
	LOSS [training: 0.19359659739113005 | validation: 0.1105494392669903]
	TIME [epoch: 8.21 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1507862698669562		[learning rate: 0.00075692]
		[batch 20/20] avg loss: 0.1591314418860464		[learning rate: 0.00075509]
	Learning Rate: 0.000755092
	LOSS [training: 0.15495885587650124 | validation: 0.16788022070860875]
	TIME [epoch: 8.19 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14106691362393703		[learning rate: 0.00075326]
		[batch 20/20] avg loss: 0.15858054458087092		[learning rate: 0.00075144]
	Learning Rate: 0.000751441
	LOSS [training: 0.149823729102404 | validation: 0.10025992503466186]
	TIME [epoch: 8.19 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1336746503804887		[learning rate: 0.00074962]
		[batch 20/20] avg loss: 0.2086151794246452		[learning rate: 0.00074781]
	Learning Rate: 0.000747807
	LOSS [training: 0.1711449149025669 | validation: 0.09030264520162717]
	TIME [epoch: 8.19 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15019436507155487		[learning rate: 0.000746]
		[batch 20/20] avg loss: 0.17082698296165658		[learning rate: 0.00074419]
	Learning Rate: 0.000744191
	LOSS [training: 0.16051067401660574 | validation: 0.26356114734267067]
	TIME [epoch: 8.2 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16571329859341163		[learning rate: 0.00074239]
		[batch 20/20] avg loss: 0.161390883251817		[learning rate: 0.00074059]
	Learning Rate: 0.000740592
	LOSS [training: 0.16355209092261433 | validation: 0.3502786551661707]
	TIME [epoch: 8.22 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1450256651285105		[learning rate: 0.0007388]
		[batch 20/20] avg loss: 0.1759613432012621		[learning rate: 0.00073701]
	Learning Rate: 0.000737011
	LOSS [training: 0.16049350416488628 | validation: 0.12992146808215288]
	TIME [epoch: 8.2 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17214234949869817		[learning rate: 0.00073523]
		[batch 20/20] avg loss: 0.13946707231712804		[learning rate: 0.00073345]
	Learning Rate: 0.000733446
	LOSS [training: 0.1558047109079131 | validation: 0.3428195806083392]
	TIME [epoch: 8.19 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15545779752053254		[learning rate: 0.00073167]
		[batch 20/20] avg loss: 0.1877540672649489		[learning rate: 0.0007299]
	Learning Rate: 0.0007299
	LOSS [training: 0.17160593239274072 | validation: 0.1378480272742348]
	TIME [epoch: 8.19 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1613999897913912		[learning rate: 0.00072813]
		[batch 20/20] avg loss: 0.1933066397542606		[learning rate: 0.00072637]
	Learning Rate: 0.00072637
	LOSS [training: 0.1773533147728259 | validation: 0.10506843484574055]
	TIME [epoch: 8.22 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1526435949657609		[learning rate: 0.00072461]
		[batch 20/20] avg loss: 0.1479742332509097		[learning rate: 0.00072286]
	Learning Rate: 0.000722857
	LOSS [training: 0.15030891410833533 | validation: 0.12442005595566304]
	TIME [epoch: 8.19 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15036020476647288		[learning rate: 0.00072111]
		[batch 20/20] avg loss: 0.12440227390632386		[learning rate: 0.00071936]
	Learning Rate: 0.000719362
	LOSS [training: 0.13738123933639837 | validation: 0.14781062901149894]
	TIME [epoch: 8.19 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20897370383903696		[learning rate: 0.00071762]
		[batch 20/20] avg loss: 0.20084499228125816		[learning rate: 0.00071588]
	Learning Rate: 0.000715883
	LOSS [training: 0.20490934806014754 | validation: 0.1464818665317612]
	TIME [epoch: 8.18 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17711433608164112		[learning rate: 0.00071415]
		[batch 20/20] avg loss: 0.17238855260271757		[learning rate: 0.00071242]
	Learning Rate: 0.000712421
	LOSS [training: 0.17475144434217932 | validation: 0.21720613317508314]
	TIME [epoch: 8.21 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2365134522126766		[learning rate: 0.0007107]
		[batch 20/20] avg loss: 0.20054089078189877		[learning rate: 0.00070898]
	Learning Rate: 0.000708976
	LOSS [training: 0.2185271714972877 | validation: 0.13516775936825615]
	TIME [epoch: 8.2 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23892622851648926		[learning rate: 0.00070726]
		[batch 20/20] avg loss: 0.2006984206497861		[learning rate: 0.00070555]
	Learning Rate: 0.000705548
	LOSS [training: 0.21981232458313768 | validation: 0.09054985450686392]
	TIME [epoch: 8.2 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18646332385464096		[learning rate: 0.00070384]
		[batch 20/20] avg loss: 0.1308326914808073		[learning rate: 0.00070214]
	Learning Rate: 0.000702136
	LOSS [training: 0.1586480076677241 | validation: 0.11066301841073461]
	TIME [epoch: 8.19 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21239539090681125		[learning rate: 0.00070044]
		[batch 20/20] avg loss: 0.17834681061996244		[learning rate: 0.00069874]
	Learning Rate: 0.00069874
	LOSS [training: 0.19537110076338687 | validation: 0.12703348114711074]
	TIME [epoch: 8.2 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17947661460173775		[learning rate: 0.00069705]
		[batch 20/20] avg loss: 0.17569981730613432		[learning rate: 0.00069536]
	Learning Rate: 0.000695361
	LOSS [training: 0.17758821595393603 | validation: 0.1366819419751612]
	TIME [epoch: 8.21 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14862522880043927		[learning rate: 0.00069368]
		[batch 20/20] avg loss: 0.14277563368025215		[learning rate: 0.000692]
	Learning Rate: 0.000691999
	LOSS [training: 0.14570043124034568 | validation: 0.23029326821532664]
	TIME [epoch: 8.19 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1428474765962649		[learning rate: 0.00069032]
		[batch 20/20] avg loss: 0.13966037632368258		[learning rate: 0.00068865]
	Learning Rate: 0.000688652
	LOSS [training: 0.14125392645997376 | validation: 0.10044545037832242]
	TIME [epoch: 8.18 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1449484825029553		[learning rate: 0.00068699]
		[batch 20/20] avg loss: 0.14221987473457107		[learning rate: 0.00068532]
	Learning Rate: 0.000685322
	LOSS [training: 0.1435841786187632 | validation: 0.2627405123819493]
	TIME [epoch: 8.18 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14612053097622685		[learning rate: 0.00068366]
		[batch 20/20] avg loss: 0.1438067146202122		[learning rate: 0.00068201]
	Learning Rate: 0.000682008
	LOSS [training: 0.14496362279821956 | validation: 0.2603601980440974]
	TIME [epoch: 8.21 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2130839890518114		[learning rate: 0.00068036]
		[batch 20/20] avg loss: 0.1214785268589776		[learning rate: 0.00067871]
	Learning Rate: 0.00067871
	LOSS [training: 0.16728125795539456 | validation: 0.09266130729721983]
	TIME [epoch: 8.19 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21974128018992797		[learning rate: 0.00067707]
		[batch 20/20] avg loss: 0.1696473810893817		[learning rate: 0.00067543]
	Learning Rate: 0.000675428
	LOSS [training: 0.19469433063965486 | validation: 0.12013583717063403]
	TIME [epoch: 8.19 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15643941216873886		[learning rate: 0.00067379]
		[batch 20/20] avg loss: 0.1733941445661632		[learning rate: 0.00067216]
	Learning Rate: 0.000672162
	LOSS [training: 0.16491677836745103 | validation: 0.13719933918869243]
	TIME [epoch: 8.18 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20997084848299447		[learning rate: 0.00067053]
		[batch 20/20] avg loss: 0.16069735971302382		[learning rate: 0.00066891]
	Learning Rate: 0.000668911
	LOSS [training: 0.18533410409800913 | validation: 0.14699073362662748]
	TIME [epoch: 8.19 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16410787547907824		[learning rate: 0.00066729]
		[batch 20/20] avg loss: 0.18598855936750663		[learning rate: 0.00066568]
	Learning Rate: 0.000665676
	LOSS [training: 0.1750482174232924 | validation: 0.11311229777492826]
	TIME [epoch: 8.22 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13562246172920153		[learning rate: 0.00066406]
		[batch 20/20] avg loss: 0.16557157368965686		[learning rate: 0.00066246]
	Learning Rate: 0.000662457
	LOSS [training: 0.15059701770942918 | validation: 0.1398564332994824]
	TIME [epoch: 8.2 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14212238429941756		[learning rate: 0.00066085]
		[batch 20/20] avg loss: 0.19869865629335143		[learning rate: 0.00065925]
	Learning Rate: 0.000659254
	LOSS [training: 0.1704105202963845 | validation: 0.3486044785824168]
	TIME [epoch: 8.2 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18972407369138772		[learning rate: 0.00065766]
		[batch 20/20] avg loss: 0.1533460147891928		[learning rate: 0.00065607]
	Learning Rate: 0.000656066
	LOSS [training: 0.17153504424029029 | validation: 0.17583233918533725]
	TIME [epoch: 8.18 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19796344848815073		[learning rate: 0.00065448]
		[batch 20/20] avg loss: 0.2046732696944928		[learning rate: 0.00065289]
	Learning Rate: 0.000652893
	LOSS [training: 0.20131835909132176 | validation: 0.07937339583479605]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_613.pth
	Model improved!!!
EPOCH 614/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12494955147999354		[learning rate: 0.00065131]
		[batch 20/20] avg loss: 0.13900919474463685		[learning rate: 0.00064974]
	Learning Rate: 0.000649736
	LOSS [training: 0.1319793731123152 | validation: 0.1197853169031325]
	TIME [epoch: 8.2 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12931220141382382		[learning rate: 0.00064816]
		[batch 20/20] avg loss: 0.1562819768072565		[learning rate: 0.00064659]
	Learning Rate: 0.000646594
	LOSS [training: 0.14279708911054018 | validation: 0.15953260147009923]
	TIME [epoch: 8.2 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1721576074767383		[learning rate: 0.00064503]
		[batch 20/20] avg loss: 0.13459602948871943		[learning rate: 0.00064347]
	Learning Rate: 0.000643467
	LOSS [training: 0.15337681848272885 | validation: 0.12289555478730041]
	TIME [epoch: 8.17 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14230225318103012		[learning rate: 0.00064191]
		[batch 20/20] avg loss: 0.13177666879078026		[learning rate: 0.00064036]
	Learning Rate: 0.000640355
	LOSS [training: 0.1370394609859052 | validation: 0.1300161984137479]
	TIME [epoch: 8.21 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14993938996085426		[learning rate: 0.00063881]
		[batch 20/20] avg loss: 0.14168498593558315		[learning rate: 0.00063726]
	Learning Rate: 0.000637259
	LOSS [training: 0.14581218794821876 | validation: 0.28666248728070154]
	TIME [epoch: 8.18 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1284902104198702		[learning rate: 0.00063572]
		[batch 20/20] avg loss: 0.1846573540499484		[learning rate: 0.00063418]
	Learning Rate: 0.000634177
	LOSS [training: 0.1565737822349093 | validation: 0.15811497578182854]
	TIME [epoch: 8.18 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19599429049069192		[learning rate: 0.00063264]
		[batch 20/20] avg loss: 0.12793294048515785		[learning rate: 0.00063111]
	Learning Rate: 0.00063111
	LOSS [training: 0.16196361548792493 | validation: 0.1619478722763571]
	TIME [epoch: 8.18 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13962388039480744		[learning rate: 0.00062958]
		[batch 20/20] avg loss: 0.18317092831378856		[learning rate: 0.00062806]
	Learning Rate: 0.000628058
	LOSS [training: 0.161397404354298 | validation: 0.08664699951210889]
	TIME [epoch: 8.19 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15908300419756044		[learning rate: 0.00062654]
		[batch 20/20] avg loss: 0.18953920187672135		[learning rate: 0.00062502]
	Learning Rate: 0.000625021
	LOSS [training: 0.17431110303714087 | validation: 0.1349773716974641]
	TIME [epoch: 8.2 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1310215152089401		[learning rate: 0.00062351]
		[batch 20/20] avg loss: 0.13026342527395712		[learning rate: 0.000622]
	Learning Rate: 0.000621999
	LOSS [training: 0.1306424702414486 | validation: 0.13987913525637377]
	TIME [epoch: 8.18 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15138474591691864		[learning rate: 0.00062049]
		[batch 20/20] avg loss: 0.1269905401687384		[learning rate: 0.00061899]
	Learning Rate: 0.000618991
	LOSS [training: 0.1391876430428285 | validation: 0.32630036666537077]
	TIME [epoch: 8.18 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17996592854053137		[learning rate: 0.00061749]
		[batch 20/20] avg loss: 0.12633892561120902		[learning rate: 0.000616]
	Learning Rate: 0.000615997
	LOSS [training: 0.1531524270758702 | validation: 0.10165356613795724]
	TIME [epoch: 8.18 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16868255828004655		[learning rate: 0.00061451]
		[batch 20/20] avg loss: 0.11790643838261203		[learning rate: 0.00061302]
	Learning Rate: 0.000613019
	LOSS [training: 0.14329449833132926 | validation: 0.14020654878770003]
	TIME [epoch: 8.21 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1337040544015035		[learning rate: 0.00061153]
		[batch 20/20] avg loss: 0.13403668326200574		[learning rate: 0.00061005]
	Learning Rate: 0.000610054
	LOSS [training: 0.1338703688317546 | validation: 0.08991895080126511]
	TIME [epoch: 8.18 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13975474648366099		[learning rate: 0.00060858]
		[batch 20/20] avg loss: 0.17148553281004247		[learning rate: 0.0006071]
	Learning Rate: 0.000607104
	LOSS [training: 0.15562013964685173 | validation: 0.06800543695243537]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_628.pth
	Model improved!!!
EPOCH 629/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14312955604790828		[learning rate: 0.00060563]
		[batch 20/20] avg loss: 0.14589769434750818		[learning rate: 0.00060417]
	Learning Rate: 0.000604168
	LOSS [training: 0.14451362519770822 | validation: 0.08219950592915189]
	TIME [epoch: 8.18 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12035990154750605		[learning rate: 0.00060271]
		[batch 20/20] avg loss: 0.1852218693190824		[learning rate: 0.00060125]
	Learning Rate: 0.000601247
	LOSS [training: 0.1527908854332942 | validation: 0.08429192142231726]
	TIME [epoch: 8.2 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1310024461649551		[learning rate: 0.00059979]
		[batch 20/20] avg loss: 0.16723372235134942		[learning rate: 0.00059834]
	Learning Rate: 0.000598339
	LOSS [training: 0.14911808425815226 | validation: 0.06683151199160095]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_631.pth
	Model improved!!!
EPOCH 632/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1115037219739264		[learning rate: 0.00059689]
		[batch 20/20] avg loss: 0.15508844942779626		[learning rate: 0.00059545]
	Learning Rate: 0.000595446
	LOSS [training: 0.13329608570086132 | validation: 0.13020650849913512]
	TIME [epoch: 8.18 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14615368062794248		[learning rate: 0.000594]
		[batch 20/20] avg loss: 0.13238999075321062		[learning rate: 0.00059257]
	Learning Rate: 0.000592566
	LOSS [training: 0.13927183569057655 | validation: 0.08832144429437225]
	TIME [epoch: 8.17 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19689944211436872		[learning rate: 0.00059113]
		[batch 20/20] avg loss: 0.14225199429779217		[learning rate: 0.0005897]
	Learning Rate: 0.000589701
	LOSS [training: 0.16957571820608042 | validation: 0.18056880218971638]
	TIME [epoch: 8.19 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1366759466767931		[learning rate: 0.00058827]
		[batch 20/20] avg loss: 0.17112644374886607		[learning rate: 0.00058685]
	Learning Rate: 0.000586849
	LOSS [training: 0.15390119521282958 | validation: 0.6651062349255686]
	TIME [epoch: 8.19 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3033362334724864		[learning rate: 0.00058543]
		[batch 20/20] avg loss: 0.11763296884490113		[learning rate: 0.00058401]
	Learning Rate: 0.000584011
	LOSS [training: 0.21048460115869377 | validation: 0.1332741346527476]
	TIME [epoch: 8.18 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17497454557125616		[learning rate: 0.0005826]
		[batch 20/20] avg loss: 0.16331315646367592		[learning rate: 0.00058119]
	Learning Rate: 0.000581187
	LOSS [training: 0.16914385101746604 | validation: 0.1248129322146321]
	TIME [epoch: 8.18 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12308219961327735		[learning rate: 0.00057978]
		[batch 20/20] avg loss: 0.21548876740740713		[learning rate: 0.00057838]
	Learning Rate: 0.000578376
	LOSS [training: 0.16928548351034226 | validation: 0.2566063870356825]
	TIME [epoch: 8.19 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11447716803935737		[learning rate: 0.00057698]
		[batch 20/20] avg loss: 0.12709741501625121		[learning rate: 0.00057558]
	Learning Rate: 0.000575579
	LOSS [training: 0.12078729152780429 | validation: 0.1497135783930251]
	TIME [epoch: 8.2 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12929719625975467		[learning rate: 0.00057419]
		[batch 20/20] avg loss: 0.13851522555472176		[learning rate: 0.0005728]
	Learning Rate: 0.000572796
	LOSS [training: 0.1339062109072382 | validation: 0.05898747350098933]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_640.pth
	Model improved!!!
EPOCH 641/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1612095968584258		[learning rate: 0.00057141]
		[batch 20/20] avg loss: 0.10851608308504715		[learning rate: 0.00057003]
	Learning Rate: 0.000570026
	LOSS [training: 0.13486283997173645 | validation: 0.26840004640998716]
	TIME [epoch: 8.18 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20278706691918963		[learning rate: 0.00056865]
		[batch 20/20] avg loss: 0.17446638718068425		[learning rate: 0.00056727]
	Learning Rate: 0.00056727
	LOSS [training: 0.18862672704993694 | validation: 0.12534544445723825]
	TIME [epoch: 8.18 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11501601003759973		[learning rate: 0.0005659]
		[batch 20/20] avg loss: 0.1931918022694736		[learning rate: 0.00056453]
	Learning Rate: 0.000564526
	LOSS [training: 0.15410390615353664 | validation: 0.06633191602389864]
	TIME [epoch: 8.2 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1363745233179941		[learning rate: 0.00056316]
		[batch 20/20] avg loss: 0.1400907193943916		[learning rate: 0.0005618]
	Learning Rate: 0.000561796
	LOSS [training: 0.13823262135619283 | validation: 0.08223092899943005]
	TIME [epoch: 8.18 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14461859874507782		[learning rate: 0.00056044]
		[batch 20/20] avg loss: 0.16679264108981778		[learning rate: 0.00055908]
	Learning Rate: 0.00055908
	LOSS [training: 0.1557056199174478 | validation: 0.08630095180135629]
	TIME [epoch: 8.18 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10571089985047684		[learning rate: 0.00055773]
		[batch 20/20] avg loss: 0.17584053235819075		[learning rate: 0.00055638]
	Learning Rate: 0.000556376
	LOSS [training: 0.14077571610433376 | validation: 0.06259001523005436]
	TIME [epoch: 8.18 sec]
EPOCH 647/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12786024542331825		[learning rate: 0.00055503]
		[batch 20/20] avg loss: 0.1163064867305433		[learning rate: 0.00055369]
	Learning Rate: 0.000553685
	LOSS [training: 0.12208336607693075 | validation: 0.07398853089829815]
	TIME [epoch: 8.2 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12239879312695572		[learning rate: 0.00055235]
		[batch 20/20] avg loss: 0.1285690367378463		[learning rate: 0.00055101]
	Learning Rate: 0.000551008
	LOSS [training: 0.12548391493240102 | validation: 0.14914844146867742]
	TIME [epoch: 8.18 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11583794835300967		[learning rate: 0.00054967]
		[batch 20/20] avg loss: 0.14438781260061892		[learning rate: 0.00054834]
	Learning Rate: 0.000548343
	LOSS [training: 0.13011288047681432 | validation: 0.12389312027892706]
	TIME [epoch: 8.17 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12580380973858724		[learning rate: 0.00054702]
		[batch 20/20] avg loss: 0.12392754721069776		[learning rate: 0.00054569]
	Learning Rate: 0.000545692
	LOSS [training: 0.12486567847464251 | validation: 0.18639768482145436]
	TIME [epoch: 8.18 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12097615849585557		[learning rate: 0.00054437]
		[batch 20/20] avg loss: 0.15377739382856173		[learning rate: 0.00054305]
	Learning Rate: 0.000543053
	LOSS [training: 0.13737677616220864 | validation: 0.21736581322211407]
	TIME [epoch: 8.19 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17839263990236198		[learning rate: 0.00054174]
		[batch 20/20] avg loss: 0.1837892458285612		[learning rate: 0.00054043]
	Learning Rate: 0.000540427
	LOSS [training: 0.1810909428654616 | validation: 0.12947710138420282]
	TIME [epoch: 8.19 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.163320490508428		[learning rate: 0.00053912]
		[batch 20/20] avg loss: 0.1466373834465478		[learning rate: 0.00053781]
	Learning Rate: 0.000537813
	LOSS [training: 0.1549789369774879 | validation: 0.21042125958820035]
	TIME [epoch: 8.18 sec]
EPOCH 654/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1386838592054952		[learning rate: 0.00053651]
		[batch 20/20] avg loss: 0.13019220695772485		[learning rate: 0.00053521]
	Learning Rate: 0.000535213
	LOSS [training: 0.13443803308161 | validation: 0.07810895990060154]
	TIME [epoch: 8.17 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13040687600888193		[learning rate: 0.00053392]
		[batch 20/20] avg loss: 0.1384608795176507		[learning rate: 0.00053262]
	Learning Rate: 0.000532624
	LOSS [training: 0.13443387776326632 | validation: 0.12167951701215714]
	TIME [epoch: 8.18 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11932598431455514		[learning rate: 0.00053134]
		[batch 20/20] avg loss: 0.11523605666273748		[learning rate: 0.00053005]
	Learning Rate: 0.000530049
	LOSS [training: 0.1172810204886463 | validation: 0.056671614216506475]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_656.pth
	Model improved!!!
EPOCH 657/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13075073271860665		[learning rate: 0.00052877]
		[batch 20/20] avg loss: 0.11881570400239161		[learning rate: 0.00052749]
	Learning Rate: 0.000527485
	LOSS [training: 0.12478321836049915 | validation: 0.14915030425007303]
	TIME [epoch: 8.18 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1451906033354547		[learning rate: 0.00052621]
		[batch 20/20] avg loss: 0.14873458063410655		[learning rate: 0.00052493]
	Learning Rate: 0.000524935
	LOSS [training: 0.14696259198478062 | validation: 0.06955393387578748]
	TIME [epoch: 8.17 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20488049264023753		[learning rate: 0.00052366]
		[batch 20/20] avg loss: 0.14016773483475226		[learning rate: 0.0005224]
	Learning Rate: 0.000522396
	LOSS [training: 0.17252411373749488 | validation: 0.07599182951090055]
	TIME [epoch: 8.18 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13762329683367894		[learning rate: 0.00052113]
		[batch 20/20] avg loss: 0.1190996792246686		[learning rate: 0.00051987]
	Learning Rate: 0.00051987
	LOSS [training: 0.12836148802917377 | validation: 0.09883941551001817]
	TIME [epoch: 8.2 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1809752337772041		[learning rate: 0.00051861]
		[batch 20/20] avg loss: 0.09276994348721881		[learning rate: 0.00051736]
	Learning Rate: 0.000517356
	LOSS [training: 0.13687258863221147 | validation: 0.06792187860103498]
	TIME [epoch: 8.18 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12386128268046952		[learning rate: 0.0005161]
		[batch 20/20] avg loss: 0.15133626603896577		[learning rate: 0.00051485]
	Learning Rate: 0.000514854
	LOSS [training: 0.13759877435971765 | validation: 0.07095933710923978]
	TIME [epoch: 8.17 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13398277651230575		[learning rate: 0.00051361]
		[batch 20/20] avg loss: 0.16378649871825215		[learning rate: 0.00051236]
	Learning Rate: 0.000512364
	LOSS [training: 0.14888463761527895 | validation: 0.09510800162153278]
	TIME [epoch: 8.18 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12277967475739857		[learning rate: 0.00051112]
		[batch 20/20] avg loss: 0.11506795398493685		[learning rate: 0.00050989]
	Learning Rate: 0.000509887
	LOSS [training: 0.11892381437116772 | validation: 0.1342300305532561]
	TIME [epoch: 8.2 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13419203869365054		[learning rate: 0.00050865]
		[batch 20/20] avg loss: 0.14244830144939563		[learning rate: 0.00050742]
	Learning Rate: 0.000507421
	LOSS [training: 0.13832017007152314 | validation: 0.09505859925198572]
	TIME [epoch: 8.17 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09826292637515185		[learning rate: 0.00050619]
		[batch 20/20] avg loss: 0.10226972539107873		[learning rate: 0.00050497]
	Learning Rate: 0.000504967
	LOSS [training: 0.10026632588311528 | validation: 0.07060754641864812]
	TIME [epoch: 8.18 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10871954980721496		[learning rate: 0.00050374]
		[batch 20/20] avg loss: 0.12244762261577338		[learning rate: 0.00050253]
	Learning Rate: 0.000502525
	LOSS [training: 0.11558358621149419 | validation: 0.07877368105388623]
	TIME [epoch: 8.17 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12306475790670017		[learning rate: 0.00050131]
		[batch 20/20] avg loss: 0.1634273673891085		[learning rate: 0.0005001]
	Learning Rate: 0.000500095
	LOSS [training: 0.14324606264790435 | validation: 0.09904511100662368]
	TIME [epoch: 8.18 sec]
EPOCH 669/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11302032225956293		[learning rate: 0.00049888]
		[batch 20/20] avg loss: 0.10537733084453091		[learning rate: 0.00049768]
	Learning Rate: 0.000497677
	LOSS [training: 0.10919882655204693 | validation: 0.08358746121338147]
	TIME [epoch: 8.2 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1252451510211946		[learning rate: 0.00049647]
		[batch 20/20] avg loss: 0.19047770002150996		[learning rate: 0.00049527]
	Learning Rate: 0.00049527
	LOSS [training: 0.1578614255213523 | validation: 0.18545019736134982]
	TIME [epoch: 8.17 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1533602842216712		[learning rate: 0.00049407]
		[batch 20/20] avg loss: 0.12387493435574733		[learning rate: 0.00049288]
	Learning Rate: 0.000492875
	LOSS [training: 0.13861760928870925 | validation: 0.10218630661845254]
	TIME [epoch: 8.17 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1574090496179827		[learning rate: 0.00049168]
		[batch 20/20] avg loss: 0.15574780193669716		[learning rate: 0.00049049]
	Learning Rate: 0.000490492
	LOSS [training: 0.15657842577733994 | validation: 0.10878051252609847]
	TIME [epoch: 8.17 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11713777154807212		[learning rate: 0.0004893]
		[batch 20/20] avg loss: 0.09828942699448315		[learning rate: 0.00048812]
	Learning Rate: 0.00048812
	LOSS [training: 0.10771359927127763 | validation: 0.09223336555795157]
	TIME [epoch: 8.2 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09754782819010818		[learning rate: 0.00048694]
		[batch 20/20] avg loss: 0.16808670762244213		[learning rate: 0.00048576]
	Learning Rate: 0.000485759
	LOSS [training: 0.13281726790627515 | validation: 0.2729576246100134]
	TIME [epoch: 8.18 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13529929206539218		[learning rate: 0.00048458]
		[batch 20/20] avg loss: 0.13686329412124748		[learning rate: 0.00048341]
	Learning Rate: 0.00048341
	LOSS [training: 0.13608129309331984 | validation: 0.08882147636055798]
	TIME [epoch: 8.18 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1258588057418322		[learning rate: 0.00048224]
		[batch 20/20] avg loss: 0.13477359921347415		[learning rate: 0.00048107]
	Learning Rate: 0.000481072
	LOSS [training: 0.13031620247765316 | validation: 0.1299146213609006]
	TIME [epoch: 8.17 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10953028109854004		[learning rate: 0.00047991]
		[batch 20/20] avg loss: 0.1390436944868073		[learning rate: 0.00047875]
	Learning Rate: 0.000478746
	LOSS [training: 0.12428698779267364 | validation: 0.1467752606727083]
	TIME [epoch: 8.19 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16123040010997342		[learning rate: 0.00047759]
		[batch 20/20] avg loss: 0.1381558720810665		[learning rate: 0.00047643]
	Learning Rate: 0.000476431
	LOSS [training: 0.14969313609551999 | validation: 0.06933345556603164]
	TIME [epoch: 8.19 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1021192692099641		[learning rate: 0.00047528]
		[batch 20/20] avg loss: 0.10323815622781443		[learning rate: 0.00047413]
	Learning Rate: 0.000474127
	LOSS [training: 0.10267871271888926 | validation: 0.11458630248850502]
	TIME [epoch: 8.17 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13273029391070312		[learning rate: 0.00047298]
		[batch 20/20] avg loss: 0.12635549750364997		[learning rate: 0.00047183]
	Learning Rate: 0.000471834
	LOSS [training: 0.12954289570717653 | validation: 0.13048887555309174]
	TIME [epoch: 8.18 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15762066651766565		[learning rate: 0.00047069]
		[batch 20/20] avg loss: 0.13792586277105523		[learning rate: 0.00046955]
	Learning Rate: 0.000469553
	LOSS [training: 0.1477732646443604 | validation: 0.0772804453771091]
	TIME [epoch: 8.17 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11202287549644425		[learning rate: 0.00046842]
		[batch 20/20] avg loss: 0.11441144937117312		[learning rate: 0.00046728]
	Learning Rate: 0.000467282
	LOSS [training: 0.11321716243380867 | validation: 0.05326100056292341]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_682.pth
	Model improved!!!
EPOCH 683/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0923778009810089		[learning rate: 0.00046615]
		[batch 20/20] avg loss: 0.127466700085157		[learning rate: 0.00046502]
	Learning Rate: 0.000465022
	LOSS [training: 0.10992225053308295 | validation: 0.0587845334003589]
	TIME [epoch: 8.18 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1163202251304255		[learning rate: 0.0004639]
		[batch 20/20] avg loss: 0.09067115323394769		[learning rate: 0.00046277]
	Learning Rate: 0.000462773
	LOSS [training: 0.10349568918218659 | validation: 0.06252284681019667]
	TIME [epoch: 8.17 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09667669633919285		[learning rate: 0.00046165]
		[batch 20/20] avg loss: 0.1346711649171454		[learning rate: 0.00046054]
	Learning Rate: 0.000460536
	LOSS [training: 0.11567393062816908 | validation: 0.14593652706234567]
	TIME [epoch: 8.17 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15523999930190285		[learning rate: 0.00045942]
		[batch 20/20] avg loss: 0.11487963269069774		[learning rate: 0.00045831]
	Learning Rate: 0.000458309
	LOSS [training: 0.1350598159963003 | validation: 0.06912994814311052]
	TIME [epoch: 8.21 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11394274469273195		[learning rate: 0.0004572]
		[batch 20/20] avg loss: 0.11579738508301432		[learning rate: 0.00045609]
	Learning Rate: 0.000456092
	LOSS [training: 0.11487006488787314 | validation: 0.1414978766995615]
	TIME [epoch: 8.18 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10681316584984213		[learning rate: 0.00045499]
		[batch 20/20] avg loss: 0.13322850792626245		[learning rate: 0.00045389]
	Learning Rate: 0.000453887
	LOSS [training: 0.12002083688805232 | validation: 0.10975688740388176]
	TIME [epoch: 8.18 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12799873350553517		[learning rate: 0.00045279]
		[batch 20/20] avg loss: 0.12341456165163442		[learning rate: 0.00045169]
	Learning Rate: 0.000451692
	LOSS [training: 0.1257066475785848 | validation: 0.13398193027548833]
	TIME [epoch: 8.17 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11425413079514044		[learning rate: 0.0004506]
		[batch 20/20] avg loss: 0.16325512400462766		[learning rate: 0.00044951]
	Learning Rate: 0.000449507
	LOSS [training: 0.13875462739988403 | validation: 0.06053345130949832]
	TIME [epoch: 8.2 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10309549597411731		[learning rate: 0.00044842]
		[batch 20/20] avg loss: 0.15023768520379205		[learning rate: 0.00044733]
	Learning Rate: 0.000447334
	LOSS [training: 0.12666659058895466 | validation: 0.12822031838893438]
	TIME [epoch: 8.18 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1171702645408971		[learning rate: 0.00044625]
		[batch 20/20] avg loss: 0.13878141889065215		[learning rate: 0.00044517]
	Learning Rate: 0.00044517
	LOSS [training: 0.12797584171577464 | validation: 0.07907058335556076]
	TIME [epoch: 8.18 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12126476760775051		[learning rate: 0.00044409]
		[batch 20/20] avg loss: 0.11430222642727317		[learning rate: 0.00044302]
	Learning Rate: 0.000443018
	LOSS [training: 0.11778349701751185 | validation: 0.061987872784501616]
	TIME [epoch: 8.17 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13001913557740347		[learning rate: 0.00044195]
		[batch 20/20] avg loss: 0.1142165926848809		[learning rate: 0.00044088]
	Learning Rate: 0.000440875
	LOSS [training: 0.12211786413114219 | validation: 0.17274929555870278]
	TIME [epoch: 8.19 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15444522100478256		[learning rate: 0.00043981]
		[batch 20/20] avg loss: 0.10043701716902254		[learning rate: 0.00043874]
	Learning Rate: 0.000438743
	LOSS [training: 0.12744111908690253 | validation: 0.15559583853287445]
	TIME [epoch: 8.19 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13563575188614768		[learning rate: 0.00043768]
		[batch 20/20] avg loss: 0.11831784461700232		[learning rate: 0.00043662]
	Learning Rate: 0.000436622
	LOSS [training: 0.12697679825157498 | validation: 0.13002632655070195]
	TIME [epoch: 8.17 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1299467696099399		[learning rate: 0.00043556]
		[batch 20/20] avg loss: 0.13480911084366573		[learning rate: 0.00043451]
	Learning Rate: 0.00043451
	LOSS [training: 0.13237794022680283 | validation: 0.11027354361586521]
	TIME [epoch: 8.18 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.138727468697403		[learning rate: 0.00043346]
		[batch 20/20] avg loss: 0.1299243830881735		[learning rate: 0.00043241]
	Learning Rate: 0.000432409
	LOSS [training: 0.13432592589278822 | validation: 0.10700915917495119]
	TIME [epoch: 8.17 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11647647992354289		[learning rate: 0.00043136]
		[batch 20/20] avg loss: 0.10258261776854816		[learning rate: 0.00043032]
	Learning Rate: 0.000430318
	LOSS [training: 0.10952954884604552 | validation: 0.08382770922740525]
	TIME [epoch: 8.2 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11709518266668953		[learning rate: 0.00042928]
		[batch 20/20] avg loss: 0.1370121730665112		[learning rate: 0.00042824]
	Learning Rate: 0.000428237
	LOSS [training: 0.12705367786660032 | validation: 0.09248395135531287]
	TIME [epoch: 8.17 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0971677001755895		[learning rate: 0.0004272]
		[batch 20/20] avg loss: 0.1042237906009025		[learning rate: 0.00042617]
	Learning Rate: 0.000426166
	LOSS [training: 0.100695745388246 | validation: 0.18463716772029581]
	TIME [epoch: 8.17 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14359106676382288		[learning rate: 0.00042513]
		[batch 20/20] avg loss: 0.11808384917524077		[learning rate: 0.00042411]
	Learning Rate: 0.000424105
	LOSS [training: 0.1308374579695318 | validation: 0.16453890090978962]
	TIME [epoch: 8.17 sec]
EPOCH 703/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1338016220794494		[learning rate: 0.00042308]
		[batch 20/20] avg loss: 0.10248261463618771		[learning rate: 0.00042205]
	Learning Rate: 0.000422054
	LOSS [training: 0.11814211835781856 | validation: 0.152389873980718]
	TIME [epoch: 8.19 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12186916024649075		[learning rate: 0.00042103]
		[batch 20/20] avg loss: 0.10077317931083873		[learning rate: 0.00042001]
	Learning Rate: 0.000420013
	LOSS [training: 0.11132116977866477 | validation: 0.13036062900347772]
	TIME [epoch: 8.18 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1402361250654838		[learning rate: 0.000419]
		[batch 20/20] avg loss: 0.1125457138413325		[learning rate: 0.00041798]
	Learning Rate: 0.000417982
	LOSS [training: 0.12639091945340813 | validation: 0.05705133154965113]
	TIME [epoch: 8.17 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1310416219918848		[learning rate: 0.00041697]
		[batch 20/20] avg loss: 0.1181361413734486		[learning rate: 0.00041596]
	Learning Rate: 0.000415961
	LOSS [training: 0.12458888168266669 | validation: 0.06517943972617028]
	TIME [epoch: 8.17 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1012218898712693		[learning rate: 0.00041495]
		[batch 20/20] avg loss: 0.09591299130481076		[learning rate: 0.00041395]
	Learning Rate: 0.00041395
	LOSS [training: 0.09856744058804004 | validation: 0.051233710173249666]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_707.pth
	Model improved!!!
EPOCH 708/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09788245790488173		[learning rate: 0.00041295]
		[batch 20/20] avg loss: 0.08764411679722643		[learning rate: 0.00041195]
	Learning Rate: 0.000411948
	LOSS [training: 0.09276328735105409 | validation: 0.12267009126136959]
	TIME [epoch: 8.21 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10516169987314576		[learning rate: 0.00041095]
		[batch 20/20] avg loss: 0.10786188718389848		[learning rate: 0.00040996]
	Learning Rate: 0.000409956
	LOSS [training: 0.10651179352852211 | validation: 0.29480536730872725]
	TIME [epoch: 8.18 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1193812040197999		[learning rate: 0.00040896]
		[batch 20/20] avg loss: 0.1334559260509417		[learning rate: 0.00040797]
	Learning Rate: 0.000407973
	LOSS [training: 0.1264185650353708 | validation: 0.14434248930328783]
	TIME [epoch: 8.18 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10604193738664973		[learning rate: 0.00040699]
		[batch 20/20] avg loss: 0.12331135331971155		[learning rate: 0.000406]
	Learning Rate: 0.000406
	LOSS [training: 0.11467664535318065 | validation: 0.07224589843487891]
	TIME [epoch: 8.19 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09119922623307194		[learning rate: 0.00040502]
		[batch 20/20] avg loss: 0.10887253227303093		[learning rate: 0.00040404]
	Learning Rate: 0.000404037
	LOSS [training: 0.10003587925305146 | validation: 0.06176690702741858]
	TIME [epoch: 8.21 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09466833308160369		[learning rate: 0.00040306]
		[batch 20/20] avg loss: 0.1430540736659062		[learning rate: 0.00040208]
	Learning Rate: 0.000402083
	LOSS [training: 0.11886120337375494 | validation: 0.10777077183927047]
	TIME [epoch: 8.18 sec]
EPOCH 714/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12885481920556704		[learning rate: 0.00040111]
		[batch 20/20] avg loss: 0.14053653108466366		[learning rate: 0.00040014]
	Learning Rate: 0.000400139
	LOSS [training: 0.13469567514511532 | validation: 0.1678486234703202]
	TIME [epoch: 8.18 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12805009210022122		[learning rate: 0.00039917]
		[batch 20/20] avg loss: 0.08889612158206782		[learning rate: 0.0003982]
	Learning Rate: 0.000398204
	LOSS [training: 0.1084731068411445 | validation: 0.07638945768905413]
	TIME [epoch: 8.18 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10447522691484708		[learning rate: 0.00039724]
		[batch 20/20] avg loss: 0.11179993453467629		[learning rate: 0.00039628]
	Learning Rate: 0.000396278
	LOSS [training: 0.10813758072476168 | validation: 0.07681452961079113]
	TIME [epoch: 8.2 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11526897918925343		[learning rate: 0.00039532]
		[batch 20/20] avg loss: 0.15932337332018165		[learning rate: 0.00039436]
	Learning Rate: 0.000394362
	LOSS [training: 0.13729617625471752 | validation: 0.15710085928680712]
	TIME [epoch: 8.18 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09976402214120728		[learning rate: 0.00039341]
		[batch 20/20] avg loss: 0.17000385332799234		[learning rate: 0.00039245]
	Learning Rate: 0.000392455
	LOSS [training: 0.13488393773459983 | validation: 0.07256620669747169]
	TIME [epoch: 8.18 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11751216831088711		[learning rate: 0.0003915]
		[batch 20/20] avg loss: 0.10824315959873672		[learning rate: 0.00039056]
	Learning Rate: 0.000390557
	LOSS [training: 0.11287766395481191 | validation: 0.09068204564176903]
	TIME [epoch: 8.18 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12572434401306976		[learning rate: 0.00038961]
		[batch 20/20] avg loss: 0.08573090156697019		[learning rate: 0.00038867]
	Learning Rate: 0.000388668
	LOSS [training: 0.10572762279001997 | validation: 0.12620504346417438]
	TIME [epoch: 8.18 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11720364664391558		[learning rate: 0.00038773]
		[batch 20/20] avg loss: 0.1095726353007002		[learning rate: 0.00038679]
	Learning Rate: 0.000386789
	LOSS [training: 0.1133881409723079 | validation: 0.07997302954237483]
	TIME [epoch: 8.2 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10934915041998039		[learning rate: 0.00038585]
		[batch 20/20] avg loss: 0.10827523687779742		[learning rate: 0.00038492]
	Learning Rate: 0.000384918
	LOSS [training: 0.10881219364888887 | validation: 0.08791560556733216]
	TIME [epoch: 8.17 sec]
EPOCH 723/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1010494755499792		[learning rate: 0.00038399]
		[batch 20/20] avg loss: 0.10020330010812586		[learning rate: 0.00038306]
	Learning Rate: 0.000383057
	LOSS [training: 0.10062638782905253 | validation: 0.0845431364554532]
	TIME [epoch: 8.18 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11626524797185198		[learning rate: 0.00038213]
		[batch 20/20] avg loss: 0.09041586848936625		[learning rate: 0.0003812]
	Learning Rate: 0.000381204
	LOSS [training: 0.10334055823060913 | validation: 0.12277674279846232]
	TIME [epoch: 8.17 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13195501012854977		[learning rate: 0.00038028]
		[batch 20/20] avg loss: 0.11291931241276625		[learning rate: 0.00037936]
	Learning Rate: 0.000379361
	LOSS [training: 0.12243716127065798 | validation: 0.11004943486307618]
	TIME [epoch: 8.2 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14219071417478785		[learning rate: 0.00037844]
		[batch 20/20] avg loss: 0.11751678523156513		[learning rate: 0.00037753]
	Learning Rate: 0.000377526
	LOSS [training: 0.12985374970317648 | validation: 0.05875860025406553]
	TIME [epoch: 8.18 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14374252186015202		[learning rate: 0.00037661]
		[batch 20/20] avg loss: 0.13106527755583547		[learning rate: 0.0003757]
	Learning Rate: 0.000375701
	LOSS [training: 0.13740389970799372 | validation: 0.17933426494049662]
	TIME [epoch: 8.18 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12309338875060627		[learning rate: 0.00037479]
		[batch 20/20] avg loss: 0.1015584681810258		[learning rate: 0.00037388]
	Learning Rate: 0.000373884
	LOSS [training: 0.11232592846581604 | validation: 0.11493269293588532]
	TIME [epoch: 8.17 sec]
EPOCH 729/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09207294287794325		[learning rate: 0.00037298]
		[batch 20/20] avg loss: 0.10746084163384124		[learning rate: 0.00037208]
	Learning Rate: 0.000372076
	LOSS [training: 0.09976689225589226 | validation: 0.1625163144035826]
	TIME [epoch: 8.2 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1266428170493607		[learning rate: 0.00037118]
		[batch 20/20] avg loss: 0.12658624054542175		[learning rate: 0.00037028]
	Learning Rate: 0.000370277
	LOSS [training: 0.12661452879739124 | validation: 0.13702356726895695]
	TIME [epoch: 8.19 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08953778022256738		[learning rate: 0.00036938]
		[batch 20/20] avg loss: 0.10416465889458378		[learning rate: 0.00036849]
	Learning Rate: 0.000368486
	LOSS [training: 0.09685121955857558 | validation: 0.12276188491207875]
	TIME [epoch: 8.17 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11254538086156822		[learning rate: 0.00036759]
		[batch 20/20] avg loss: 0.08125277670408265		[learning rate: 0.0003667]
	Learning Rate: 0.000366704
	LOSS [training: 0.09689907878282546 | validation: 0.11485533884730786]
	TIME [epoch: 8.18 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12192612377377118		[learning rate: 0.00036582]
		[batch 20/20] avg loss: 0.12126119238312587		[learning rate: 0.00036493]
	Learning Rate: 0.000364931
	LOSS [training: 0.1215936580784485 | validation: 0.08546980195130886]
	TIME [epoch: 8.17 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11851312397374336		[learning rate: 0.00036405]
		[batch 20/20] avg loss: 0.08440715839750881		[learning rate: 0.00036317]
	Learning Rate: 0.000363166
	LOSS [training: 0.1014601411856261 | validation: 0.07178463137428462]
	TIME [epoch: 8.2 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16572026867754003		[learning rate: 0.00036229]
		[batch 20/20] avg loss: 0.12083972327424182		[learning rate: 0.00036141]
	Learning Rate: 0.00036141
	LOSS [training: 0.14327999597589092 | validation: 0.07141445013715907]
	TIME [epoch: 8.17 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10729001649721277		[learning rate: 0.00036053]
		[batch 20/20] avg loss: 0.10237704575683966		[learning rate: 0.00035966]
	Learning Rate: 0.000359662
	LOSS [training: 0.10483353112702623 | validation: 0.071524175548405]
	TIME [epoch: 8.17 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10393984519254089		[learning rate: 0.00035879]
		[batch 20/20] avg loss: 0.07878196207705575		[learning rate: 0.00035792]
	Learning Rate: 0.000357923
	LOSS [training: 0.0913609036347983 | validation: 0.08040108206739308]
	TIME [epoch: 8.17 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0954551541624696		[learning rate: 0.00035706]
		[batch 20/20] avg loss: 0.10507542919387429		[learning rate: 0.00035619]
	Learning Rate: 0.000356192
	LOSS [training: 0.10026529167817196 | validation: 0.09608109802101913]
	TIME [epoch: 8.19 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14524281214320095		[learning rate: 0.00035533]
		[batch 20/20] avg loss: 0.08516897100960762		[learning rate: 0.00035447]
	Learning Rate: 0.00035447
	LOSS [training: 0.11520589157640426 | validation: 0.06755836137662033]
	TIME [epoch: 8.2 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1073191662201105		[learning rate: 0.00035361]
		[batch 20/20] avg loss: 0.08731179055198365		[learning rate: 0.00035276]
	Learning Rate: 0.000352755
	LOSS [training: 0.09731547838604707 | validation: 0.10523126176410474]
	TIME [epoch: 8.18 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10119862464008696		[learning rate: 0.0003519]
		[batch 20/20] avg loss: 0.12393660494014638		[learning rate: 0.00035105]
	Learning Rate: 0.00035105
	LOSS [training: 0.11256761479011665 | validation: 0.057416084205182384]
	TIME [epoch: 8.18 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10156078552127927		[learning rate: 0.0003502]
		[batch 20/20] avg loss: 0.11757955799322244		[learning rate: 0.00034935]
	Learning Rate: 0.000349352
	LOSS [training: 0.10957017175725085 | validation: 0.06745035962109983]
	TIME [epoch: 8.17 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1353167868276438		[learning rate: 0.00034851]
		[batch 20/20] avg loss: 0.10258129732390764		[learning rate: 0.00034766]
	Learning Rate: 0.000347663
	LOSS [training: 0.1189490420757757 | validation: 0.10254564940468483]
	TIME [epoch: 8.21 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11823745802286019		[learning rate: 0.00034682]
		[batch 20/20] avg loss: 0.1148682128564035		[learning rate: 0.00034598]
	Learning Rate: 0.000345981
	LOSS [training: 0.11655283543963187 | validation: 0.06789414601085936]
	TIME [epoch: 8.18 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08511467857784501		[learning rate: 0.00034514]
		[batch 20/20] avg loss: 0.10550187580362531		[learning rate: 0.00034431]
	Learning Rate: 0.000344308
	LOSS [training: 0.09530827719073515 | validation: 0.058428319403052674]
	TIME [epoch: 8.18 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09359241235103856		[learning rate: 0.00034347]
		[batch 20/20] avg loss: 0.08597673232866557		[learning rate: 0.00034264]
	Learning Rate: 0.000342643
	LOSS [training: 0.08978457233985206 | validation: 0.07435421417942226]
	TIME [epoch: 8.17 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10051907571904253		[learning rate: 0.00034181]
		[batch 20/20] avg loss: 0.10761141432150259		[learning rate: 0.00034099]
	Learning Rate: 0.000340986
	LOSS [training: 0.10406524502027256 | validation: 0.05801673019348297]
	TIME [epoch: 8.2 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09207361858469985		[learning rate: 0.00034016]
		[batch 20/20] avg loss: 0.11273124562412071		[learning rate: 0.00033934]
	Learning Rate: 0.000339337
	LOSS [training: 0.10240243210441029 | validation: 0.11722829901728692]
	TIME [epoch: 8.18 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11550025437840224		[learning rate: 0.00033852]
		[batch 20/20] avg loss: 0.08708123286961492		[learning rate: 0.0003377]
	Learning Rate: 0.000337696
	LOSS [training: 0.10129074362400858 | validation: 0.052192055246756755]
	TIME [epoch: 8.17 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11480827072960662		[learning rate: 0.00033688]
		[batch 20/20] avg loss: 0.0720438494663844		[learning rate: 0.00033606]
	Learning Rate: 0.000336063
	LOSS [training: 0.0934260600979955 | validation: 0.06791795699884148]
	TIME [epoch: 8.18 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11997529531418683		[learning rate: 0.00033525]
		[batch 20/20] avg loss: 0.10404344729842745		[learning rate: 0.00033444]
	Learning Rate: 0.000334438
	LOSS [training: 0.11200937130630713 | validation: 0.07569930010819141]
	TIME [epoch: 8.18 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1314943617727328		[learning rate: 0.00033363]
		[batch 20/20] avg loss: 0.10031921485926658		[learning rate: 0.00033282]
	Learning Rate: 0.000332821
	LOSS [training: 0.11590678831599968 | validation: 0.05568626745293871]
	TIME [epoch: 8.2 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09127662081525674		[learning rate: 0.00033202]
		[batch 20/20] avg loss: 0.12136948743283131		[learning rate: 0.00033121]
	Learning Rate: 0.000331211
	LOSS [training: 0.10632305412404401 | validation: 0.07868639322117474]
	TIME [epoch: 8.17 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0977597964611787		[learning rate: 0.00033041]
		[batch 20/20] avg loss: 0.10822899042897885		[learning rate: 0.00032961]
	Learning Rate: 0.00032961
	LOSS [training: 0.10299439344507877 | validation: 0.06346186335705428]
	TIME [epoch: 8.18 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08481122372367182		[learning rate: 0.00032881]
		[batch 20/20] avg loss: 0.11470360105260724		[learning rate: 0.00032802]
	Learning Rate: 0.000328016
	LOSS [training: 0.09975741238813954 | validation: 0.0749329242763642]
	TIME [epoch: 8.17 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08008190682959535		[learning rate: 0.00032722]
		[batch 20/20] avg loss: 0.09693009870719879		[learning rate: 0.00032643]
	Learning Rate: 0.00032643
	LOSS [training: 0.08850600276839705 | validation: 0.10487029425892898]
	TIME [epoch: 8.2 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11701122893156188		[learning rate: 0.00032564]
		[batch 20/20] avg loss: 0.10126430596425455		[learning rate: 0.00032485]
	Learning Rate: 0.000324851
	LOSS [training: 0.10913776744790823 | validation: 0.1354616705529904]
	TIME [epoch: 8.17 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12907414911569834		[learning rate: 0.00032406]
		[batch 20/20] avg loss: 0.10099957680584919		[learning rate: 0.00032328]
	Learning Rate: 0.00032328
	LOSS [training: 0.11503686296077376 | validation: 0.07777674169417745]
	TIME [epoch: 8.18 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10356180186551171		[learning rate: 0.0003225]
		[batch 20/20] avg loss: 0.09750413754660997		[learning rate: 0.00032172]
	Learning Rate: 0.000321717
	LOSS [training: 0.10053296970606082 | validation: 0.0643028302609161]
	TIME [epoch: 8.17 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11926884237915056		[learning rate: 0.00032094]
		[batch 20/20] avg loss: 0.08927244357714711		[learning rate: 0.00032016]
	Learning Rate: 0.000320161
	LOSS [training: 0.10427064297814885 | validation: 0.06667720711190661]
	TIME [epoch: 8.19 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09418931187356146		[learning rate: 0.00031939]
		[batch 20/20] avg loss: 0.07847771960293215		[learning rate: 0.00031861]
	Learning Rate: 0.000318613
	LOSS [training: 0.08633351573824681 | validation: 0.13478682944181156]
	TIME [epoch: 8.2 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12869881879405337		[learning rate: 0.00031784]
		[batch 20/20] avg loss: 0.13254936162879902		[learning rate: 0.00031707]
	Learning Rate: 0.000317072
	LOSS [training: 0.1306240902114262 | validation: 0.07788361192083812]
	TIME [epoch: 8.19 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09005873649926242		[learning rate: 0.0003163]
		[batch 20/20] avg loss: 0.0863295138280123		[learning rate: 0.00031554]
	Learning Rate: 0.000315539
	LOSS [training: 0.08819412516363737 | validation: 0.07343775459425142]
	TIME [epoch: 8.17 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10271039246448295		[learning rate: 0.00031477]
		[batch 20/20] avg loss: 0.09023989029464514		[learning rate: 0.00031401]
	Learning Rate: 0.000314013
	LOSS [training: 0.09647514137956405 | validation: 0.07035080162236815]
	TIME [epoch: 8.17 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12162418733096247		[learning rate: 0.00031325]
		[batch 20/20] avg loss: 0.10532475834364467		[learning rate: 0.00031249]
	Learning Rate: 0.000312494
	LOSS [training: 0.11347447283730358 | validation: 0.08428350892188233]
	TIME [epoch: 8.2 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11150636126174465		[learning rate: 0.00031174]
		[batch 20/20] avg loss: 0.15361109963793818		[learning rate: 0.00031098]
	Learning Rate: 0.000310983
	LOSS [training: 0.1325587304498414 | validation: 0.06290944123252598]
	TIME [epoch: 8.17 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1116035769246867		[learning rate: 0.00031023]
		[batch 20/20] avg loss: 0.0924185514580683		[learning rate: 0.00030948]
	Learning Rate: 0.000309479
	LOSS [training: 0.10201106419137748 | validation: 0.06331792963786068]
	TIME [epoch: 8.17 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08363410924010026		[learning rate: 0.00030873]
		[batch 20/20] avg loss: 0.11918526251699901		[learning rate: 0.00030798]
	Learning Rate: 0.000307983
	LOSS [training: 0.10140968587854964 | validation: 0.1825376752665609]
	TIME [epoch: 8.17 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11373583487699726		[learning rate: 0.00030724]
		[batch 20/20] avg loss: 0.08704531427418871		[learning rate: 0.00030649]
	Learning Rate: 0.000306493
	LOSS [training: 0.10039057457559297 | validation: 0.06708077316655056]
	TIME [epoch: 8.19 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08846285248698248		[learning rate: 0.00030575]
		[batch 20/20] avg loss: 0.1262720228610455		[learning rate: 0.00030501]
	Learning Rate: 0.000305011
	LOSS [training: 0.10736743767401398 | validation: 0.05690530677313654]
	TIME [epoch: 8.18 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10532554684160736		[learning rate: 0.00030427]
		[batch 20/20] avg loss: 0.09850738636058186		[learning rate: 0.00030354]
	Learning Rate: 0.000303536
	LOSS [training: 0.10191646660109462 | validation: 0.0674118185976024]
	TIME [epoch: 8.17 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08583488578116064		[learning rate: 0.0003028]
		[batch 20/20] avg loss: 0.07486505543245144		[learning rate: 0.00030207]
	Learning Rate: 0.000302068
	LOSS [training: 0.08034997060680603 | validation: 0.1433180514991817]
	TIME [epoch: 8.18 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09724590780116338		[learning rate: 0.00030134]
		[batch 20/20] avg loss: 0.11488078902918349		[learning rate: 0.00030061]
	Learning Rate: 0.000300608
	LOSS [training: 0.10606334841517344 | validation: 0.06919191498736156]
	TIME [epoch: 8.17 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07856709778586232		[learning rate: 0.00029988]
		[batch 20/20] avg loss: 0.09273279363599057		[learning rate: 0.00029915]
	Learning Rate: 0.000299154
	LOSS [training: 0.08564994571092643 | validation: 0.10708420952664197]
	TIME [epoch: 8.2 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11522083391581987		[learning rate: 0.00029843]
		[batch 20/20] avg loss: 0.07956485990440262		[learning rate: 0.00029771]
	Learning Rate: 0.000297707
	LOSS [training: 0.09739284691011124 | validation: 0.10155008863496123]
	TIME [epoch: 8.17 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08627092953791957		[learning rate: 0.00029699]
		[batch 20/20] avg loss: 0.13145715163468738		[learning rate: 0.00029627]
	Learning Rate: 0.000296268
	LOSS [training: 0.10886404058630347 | validation: 0.07861831059529975]
	TIME [epoch: 8.17 sec]
EPOCH 777/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0909199602756545		[learning rate: 0.00029555]
		[batch 20/20] avg loss: 0.11823915695548118		[learning rate: 0.00029483]
	Learning Rate: 0.000294835
	LOSS [training: 0.10457955861556782 | validation: 0.0930170115338786]
	TIME [epoch: 8.17 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08098999155524562		[learning rate: 0.00029412]
		[batch 20/20] avg loss: 0.09658931237891667		[learning rate: 0.00029341]
	Learning Rate: 0.000293409
	LOSS [training: 0.08878965196708113 | validation: 0.05864103618217231]
	TIME [epoch: 8.2 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07744492232725067		[learning rate: 0.0002927]
		[batch 20/20] avg loss: 0.09535597212418052		[learning rate: 0.00029199]
	Learning Rate: 0.00029199
	LOSS [training: 0.08640044722571559 | validation: 0.054520744661629116]
	TIME [epoch: 8.17 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09342543800749474		[learning rate: 0.00029128]
		[batch 20/20] avg loss: 0.11724100394448347		[learning rate: 0.00029058]
	Learning Rate: 0.000290578
	LOSS [training: 0.1053332209759891 | validation: 0.08151986563974667]
	TIME [epoch: 8.17 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13173002301960346		[learning rate: 0.00028987]
		[batch 20/20] avg loss: 0.10129795419323528		[learning rate: 0.00028917]
	Learning Rate: 0.000289173
	LOSS [training: 0.11651398860641937 | validation: 0.11131917314915865]
	TIME [epoch: 8.17 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11633478365756089		[learning rate: 0.00028847]
		[batch 20/20] avg loss: 0.08202552781992274		[learning rate: 0.00028777]
	Learning Rate: 0.000287775
	LOSS [training: 0.09918015573874181 | validation: 0.15325199377777687]
	TIME [epoch: 8.18 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10506031443015455		[learning rate: 0.00028708]
		[batch 20/20] avg loss: 0.10834360196234802		[learning rate: 0.00028638]
	Learning Rate: 0.000286383
	LOSS [training: 0.10670195819625128 | validation: 0.08204147554598412]
	TIME [epoch: 8.19 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1099812737877025		[learning rate: 0.00028569]
		[batch 20/20] avg loss: 0.11221808984795811		[learning rate: 0.000285]
	Learning Rate: 0.000284998
	LOSS [training: 0.11109968181783032 | validation: 0.14149414372701136]
	TIME [epoch: 8.17 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11088896011130392		[learning rate: 0.00028431]
		[batch 20/20] avg loss: 0.08644883114920561		[learning rate: 0.00028362]
	Learning Rate: 0.00028362
	LOSS [training: 0.09866889563025476 | validation: 0.075825320276154]
	TIME [epoch: 8.17 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10180971399853952		[learning rate: 0.00028293]
		[batch 20/20] avg loss: 0.07720288162844256		[learning rate: 0.00028225]
	Learning Rate: 0.000282248
	LOSS [training: 0.08950629781349104 | validation: 0.10973301644311291]
	TIME [epoch: 8.17 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09865762482931235		[learning rate: 0.00028157]
		[batch 20/20] avg loss: 0.11620280059944244		[learning rate: 0.00028088]
	Learning Rate: 0.000280884
	LOSS [training: 0.1074302127143774 | validation: 0.1584954605761909]
	TIME [epoch: 8.19 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.106257032340952		[learning rate: 0.0002802]
		[batch 20/20] avg loss: 0.0917200214273732		[learning rate: 0.00027953]
	Learning Rate: 0.000279525
	LOSS [training: 0.09898852688416261 | validation: 0.08572918310869873]
	TIME [epoch: 8.17 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1132057203851635		[learning rate: 0.00027885]
		[batch 20/20] avg loss: 0.09036085059727977		[learning rate: 0.00027817]
	Learning Rate: 0.000278173
	LOSS [training: 0.10178328549122162 | validation: 0.07430582207037192]
	TIME [epoch: 8.17 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12058658031835215		[learning rate: 0.0002775]
		[batch 20/20] avg loss: 0.08619074547574096		[learning rate: 0.00027683]
	Learning Rate: 0.000276828
	LOSS [training: 0.10338866289704655 | validation: 0.07421981590537687]
	TIME [epoch: 8.17 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10332115409060427		[learning rate: 0.00027616]
		[batch 20/20] avg loss: 0.14128885243840417		[learning rate: 0.00027549]
	Learning Rate: 0.00027549
	LOSS [training: 0.12230500326450422 | validation: 0.24286332397099686]
	TIME [epoch: 8.19 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11828412752395023		[learning rate: 0.00027482]
		[batch 20/20] avg loss: 0.0785092913446685		[learning rate: 0.00027416]
	Learning Rate: 0.000274157
	LOSS [training: 0.09839670943430937 | validation: 0.07008056181622448]
	TIME [epoch: 8.18 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08896574848869246		[learning rate: 0.00027349]
		[batch 20/20] avg loss: 0.08002117053721493		[learning rate: 0.00027283]
	Learning Rate: 0.000272832
	LOSS [training: 0.08449345951295369 | validation: 0.05582087398044909]
	TIME [epoch: 8.17 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10125547586713249		[learning rate: 0.00027217]
		[batch 20/20] avg loss: 0.09681969522464595		[learning rate: 0.00027151]
	Learning Rate: 0.000271512
	LOSS [training: 0.09903758554588922 | validation: 0.05412774784590084]
	TIME [epoch: 8.17 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11823979250580345		[learning rate: 0.00027086]
		[batch 20/20] avg loss: 0.09094543649870386		[learning rate: 0.0002702]
	Learning Rate: 0.000270199
	LOSS [training: 0.10459261450225366 | validation: 0.06141167736475189]
	TIME [epoch: 8.17 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07938213477674334		[learning rate: 0.00026955]
		[batch 20/20] avg loss: 0.11518866133437675		[learning rate: 0.00026889]
	Learning Rate: 0.000268893
	LOSS [training: 0.09728539805556004 | validation: 0.09368036844132786]
	TIME [epoch: 8.2 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10442896296609701		[learning rate: 0.00026824]
		[batch 20/20] avg loss: 0.10266846531970633		[learning rate: 0.00026759]
	Learning Rate: 0.000267592
	LOSS [training: 0.10354871414290168 | validation: 0.15340356828307045]
	TIME [epoch: 8.16 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09247625575699969		[learning rate: 0.00026694]
		[batch 20/20] avg loss: 0.10004547533785657		[learning rate: 0.0002663]
	Learning Rate: 0.000266298
	LOSS [training: 0.09626086554742815 | validation: 0.09958478640327365]
	TIME [epoch: 8.17 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08032740888730781		[learning rate: 0.00026565]
		[batch 20/20] avg loss: 0.07913986561414997		[learning rate: 0.00026501]
	Learning Rate: 0.000265011
	LOSS [training: 0.07973363725072889 | validation: 0.06500857793850504]
	TIME [epoch: 8.17 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09039296884080911		[learning rate: 0.00026437]
		[batch 20/20] avg loss: 0.08701762194188578		[learning rate: 0.00026373]
	Learning Rate: 0.000263729
	LOSS [training: 0.08870529539134744 | validation: 0.05607383106277499]
	TIME [epoch: 8.19 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09170909132629843		[learning rate: 0.00026309]
		[batch 20/20] avg loss: 0.11491094214013331		[learning rate: 0.00026245]
	Learning Rate: 0.000262454
	LOSS [training: 0.10331001673321587 | validation: 0.12794104122185462]
	TIME [epoch: 8.18 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09413761562889808		[learning rate: 0.00026182]
		[batch 20/20] avg loss: 0.08404217064099616		[learning rate: 0.00026118]
	Learning Rate: 0.000261184
	LOSS [training: 0.08908989313494711 | validation: 0.052906706658416264]
	TIME [epoch: 8.17 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09984722217449757		[learning rate: 0.00026055]
		[batch 20/20] avg loss: 0.10040763285273754		[learning rate: 0.00025992]
	Learning Rate: 0.000259921
	LOSS [training: 0.10012742751361756 | validation: 0.05734016240167491]
	TIME [epoch: 8.17 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10243145953286262		[learning rate: 0.00025929]
		[batch 20/20] avg loss: 0.10194377643904318		[learning rate: 0.00025866]
	Learning Rate: 0.000258665
	LOSS [training: 0.10218761798595291 | validation: 0.28202418347945]
	TIME [epoch: 8.17 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13611723022751449		[learning rate: 0.00025804]
		[batch 20/20] avg loss: 0.0984932337607769		[learning rate: 0.00025741]
	Learning Rate: 0.000257414
	LOSS [training: 0.11730523199414569 | validation: 0.09503436186831386]
	TIME [epoch: 8.2 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09391588138919306		[learning rate: 0.00025679]
		[batch 20/20] avg loss: 0.09374397952407867		[learning rate: 0.00025617]
	Learning Rate: 0.000256169
	LOSS [training: 0.09382993045663586 | validation: 0.08567498939511296]
	TIME [epoch: 8.17 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08957467573155124		[learning rate: 0.00025555]
		[batch 20/20] avg loss: 0.12209674749861976		[learning rate: 0.00025493]
	Learning Rate: 0.00025493
	LOSS [training: 0.1058357116150855 | validation: 0.12360395722252254]
	TIME [epoch: 8.17 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1132748479568744		[learning rate: 0.00025431]
		[batch 20/20] avg loss: 0.08742571756831778		[learning rate: 0.0002537]
	Learning Rate: 0.000253697
	LOSS [training: 0.10035028276259608 | validation: 0.08174550982613864]
	TIME [epoch: 8.16 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10554214938369948		[learning rate: 0.00025308]
		[batch 20/20] avg loss: 0.09392507146982773		[learning rate: 0.00025247]
	Learning Rate: 0.00025247
	LOSS [training: 0.09973361042676361 | validation: 0.06623330916967]
	TIME [epoch: 8.19 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09247822197511617		[learning rate: 0.00025186]
		[batch 20/20] avg loss: 0.08296982676617035		[learning rate: 0.00025125]
	Learning Rate: 0.00025125
	LOSS [training: 0.08772402437064325 | validation: 0.09247168483880985]
	TIME [epoch: 8.16 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07759264510140781		[learning rate: 0.00025064]
		[batch 20/20] avg loss: 0.09645163769006529		[learning rate: 0.00025003]
	Learning Rate: 0.000250035
	LOSS [training: 0.08702214139573654 | validation: 0.11233049591748884]
	TIME [epoch: 8.17 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08554675119166302		[learning rate: 0.00024943]
		[batch 20/20] avg loss: 0.10389613508793034		[learning rate: 0.00024883]
	Learning Rate: 0.000248825
	LOSS [training: 0.09472144313979669 | validation: 0.07249065245177]
	TIME [epoch: 8.16 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09310267767382588		[learning rate: 0.00024822]
		[batch 20/20] avg loss: 0.09917557175389426		[learning rate: 0.00024762]
	Learning Rate: 0.000247622
	LOSS [training: 0.09613912471386006 | validation: 0.0810699498028273]
	TIME [epoch: 8.18 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07746835061186244		[learning rate: 0.00024702]
		[batch 20/20] avg loss: 0.09820803897858758		[learning rate: 0.00024642]
	Learning Rate: 0.000246425
	LOSS [training: 0.087838194795225 | validation: 0.07785688032713559]
	TIME [epoch: 8.2 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09749674135864564		[learning rate: 0.00024583]
		[batch 20/20] avg loss: 0.09323569908471653		[learning rate: 0.00024523]
	Learning Rate: 0.000245233
	LOSS [training: 0.09536622022168109 | validation: 0.0656490766298815]
	TIME [epoch: 8.17 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0921304142131287		[learning rate: 0.00024464]
		[batch 20/20] avg loss: 0.08077731905998753		[learning rate: 0.00024405]
	Learning Rate: 0.000244047
	LOSS [training: 0.08645386663655812 | validation: 0.06892148058142257]
	TIME [epoch: 8.17 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09152254278040939		[learning rate: 0.00024346]
		[batch 20/20] avg loss: 0.09012902038147337		[learning rate: 0.00024287]
	Learning Rate: 0.000242867
	LOSS [training: 0.09082578158094137 | validation: 0.0854959689895659]
	TIME [epoch: 8.16 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09811725848173339		[learning rate: 0.00024228]
		[batch 20/20] avg loss: 0.1128005882990496		[learning rate: 0.00024169]
	Learning Rate: 0.000241693
	LOSS [training: 0.10545892339039149 | validation: 0.04017611619176206]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240217_140923/states/model_tr_study2_818.pth
	Model improved!!!
EPOCH 819/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08663891874765081		[learning rate: 0.00024111]
		[batch 20/20] avg loss: 0.10475663667328021		[learning rate: 0.00024052]
	Learning Rate: 0.000240524
	LOSS [training: 0.0956977777104655 | validation: 0.06796534511901277]
	TIME [epoch: 8.16 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07710269916357097		[learning rate: 0.00023994]
		[batch 20/20] avg loss: 0.08795670793297974		[learning rate: 0.00023936]
	Learning Rate: 0.000239361
	LOSS [training: 0.08252970354827535 | validation: 0.057930833434879106]
	TIME [epoch: 8.16 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10474965113737589		[learning rate: 0.00023878]
		[batch 20/20] avg loss: 0.10717318639424153		[learning rate: 0.0002382]
	Learning Rate: 0.000238203
	LOSS [training: 0.10596141876580871 | validation: 0.09217993270649281]
	TIME [epoch: 8.16 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08597442364040639		[learning rate: 0.00023763]
		[batch 20/20] avg loss: 0.08400900314544346		[learning rate: 0.00023705]
	Learning Rate: 0.000237051
	LOSS [training: 0.08499171339292491 | validation: 0.07027694477326431]
	TIME [epoch: 8.19 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08694859493447271		[learning rate: 0.00023648]
		[batch 20/20] avg loss: 0.0825728995369833		[learning rate: 0.0002359]
	Learning Rate: 0.000235905
	LOSS [training: 0.08476074723572799 | validation: 0.059142861349411664]
	TIME [epoch: 8.17 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08501523888238458		[learning rate: 0.00023533]
		[batch 20/20] avg loss: 0.10107367150572706		[learning rate: 0.00023476]
	Learning Rate: 0.000234764
	LOSS [training: 0.09304445519405583 | validation: 0.05098430170405826]
	TIME [epoch: 8.16 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08157246908892182		[learning rate: 0.0002342]
		[batch 20/20] avg loss: 0.10664468444534696		[learning rate: 0.00023363]
	Learning Rate: 0.000233629
	LOSS [training: 0.09410857676713437 | validation: 0.10052617711560313]
	TIME [epoch: 8.17 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09727308505595478		[learning rate: 0.00023306]
		[batch 20/20] avg loss: 0.0783334339349635		[learning rate: 0.0002325]
	Learning Rate: 0.000232499
	LOSS [training: 0.08780325949545914 | validation: 0.0823257451876369]
	TIME [epoch: 8.18 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0834937041296936		[learning rate: 0.00023194]
		[batch 20/20] avg loss: 0.08252430628813376		[learning rate: 0.00023137]
	Learning Rate: 0.000231375
	LOSS [training: 0.08300900520891369 | validation: 0.05987651422140326]
	TIME [epoch: 8.18 sec]
EPOCH 828/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09590875607527544		[learning rate: 0.00023081]
		[batch 20/20] avg loss: 0.0893117178591128		[learning rate: 0.00023026]
	Learning Rate: 0.000230256
	LOSS [training: 0.09261023696719412 | validation: 0.12101199586382287]
	TIME [epoch: 8.16 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08928494269229477		[learning rate: 0.0002297]
		[batch 20/20] avg loss: 0.07899429318731266		[learning rate: 0.00022914]
	Learning Rate: 0.000229142
	LOSS [training: 0.08413961793980371 | validation: 0.066858397860387]
	TIME [epoch: 8.17 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08045769382721024		[learning rate: 0.00022859]
		[batch 20/20] avg loss: 0.09459337862049906		[learning rate: 0.00022803]
	Learning Rate: 0.000228034
	LOSS [training: 0.08752553622385464 | validation: 0.07110834069359605]
	TIME [epoch: 8.16 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0705926503195419		[learning rate: 0.00022748]
		[batch 20/20] avg loss: 0.09270178607637136		[learning rate: 0.00022693]
	Learning Rate: 0.000226931
	LOSS [training: 0.08164721819795664 | validation: 0.06915241574682308]
	TIME [epoch: 8.19 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07545644881814756		[learning rate: 0.00022638]
		[batch 20/20] avg loss: 0.09770707573784308		[learning rate: 0.00022583]
	Learning Rate: 0.000225834
	LOSS [training: 0.0865817622779953 | validation: 0.057705823458346704]
	TIME [epoch: 8.16 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09660769490613694		[learning rate: 0.00022529]
		[batch 20/20] avg loss: 0.10169867125972794		[learning rate: 0.00022474]
	Learning Rate: 0.000224742
	LOSS [training: 0.09915318308293244 | validation: 0.183198302999933]
	TIME [epoch: 8.16 sec]
EPOCH 834/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1270599101980955		[learning rate: 0.0002242]
		[batch 20/20] avg loss: 0.09431682929437434		[learning rate: 0.00022366]
	Learning Rate: 0.000223655
	LOSS [training: 0.11068836974623493 | validation: 0.060634891743255565]
	TIME [epoch: 8.16 sec]
EPOCH 835/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07630903760132692		[learning rate: 0.00022311]
		[batch 20/20] avg loss: 0.08395801844198751		[learning rate: 0.00022257]
	Learning Rate: 0.000222574
	LOSS [training: 0.08013352802165723 | validation: 0.06313417280669198]
	TIME [epoch: 8.18 sec]
EPOCH 836/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1054320526651302		[learning rate: 0.00022203]
		[batch 20/20] avg loss: 0.09213167891459889		[learning rate: 0.0002215]
	Learning Rate: 0.000221497
	LOSS [training: 0.09878186578986455 | validation: 0.057268459830666835]
	TIME [epoch: 8.17 sec]
EPOCH 837/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07340260396079375		[learning rate: 0.00022096]
		[batch 20/20] avg loss: 0.07962768217208488		[learning rate: 0.00022043]
	Learning Rate: 0.000220426
	LOSS [training: 0.07651514306643932 | validation: 0.056061458206145]
	TIME [epoch: 8.16 sec]
EPOCH 838/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0719903481685131		[learning rate: 0.00021989]
		[batch 20/20] avg loss: 0.0901046100782515		[learning rate: 0.00021936]
	Learning Rate: 0.00021936
	LOSS [training: 0.08104747912338231 | validation: 0.06602619602989718]
	TIME [epoch: 8.16 sec]
EPOCH 839/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07861126369868358		[learning rate: 0.00021883]
		[batch 20/20] avg loss: 0.07142019490301191		[learning rate: 0.0002183]
	Learning Rate: 0.000218299
	LOSS [training: 0.07501572930084774 | validation: 0.12721639788229444]
	TIME [epoch: 8.15 sec]
EPOCH 840/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10792088066317371		[learning rate: 0.00021777]
		[batch 20/20] avg loss: 0.08664244965800813		[learning rate: 0.00021724]
	Learning Rate: 0.000217244
	LOSS [training: 0.09728166516059095 | validation: 0.06663753632691594]
	TIME [epoch: 8.19 sec]
EPOCH 841/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11835455951225311		[learning rate: 0.00021672]
		[batch 20/20] avg loss: 0.12165063663856243		[learning rate: 0.00021619]
	Learning Rate: 0.000216193
	LOSS [training: 0.12000259807540778 | validation: 0.05989564998960269]
	TIME [epoch: 8.15 sec]
EPOCH 842/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08303559053495216		[learning rate: 0.00021567]
		[batch 20/20] avg loss: 0.13905948803688764		[learning rate: 0.00021515]
	Learning Rate: 0.000215148
	LOSS [training: 0.1110475392859199 | validation: 0.053262284952169316]
	TIME [epoch: 8.16 sec]
EPOCH 843/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08451376988652097		[learning rate: 0.00021463]
		[batch 20/20] avg loss: 0.09717441158242378		[learning rate: 0.00021411]
	Learning Rate: 0.000214107
	LOSS [training: 0.09084409073447237 | validation: 0.08140961879008514]
	TIME [epoch: 8.15 sec]
EPOCH 844/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07501164573249788		[learning rate: 0.00021359]
		[batch 20/20] avg loss: 0.08821663123531488		[learning rate: 0.00021307]
	Learning Rate: 0.000213072
	LOSS [training: 0.08161413848390636 | validation: 0.0552443528023213]
	TIME [epoch: 8.18 sec]
EPOCH 845/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07822583727982255		[learning rate: 0.00021256]
		[batch 20/20] avg loss: 0.10329338464211373		[learning rate: 0.00021204]
	Learning Rate: 0.000212042
	LOSS [training: 0.09075961096096814 | validation: 0.07429836616378487]
	TIME [epoch: 8.16 sec]
EPOCH 846/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07064688086326501		[learning rate: 0.00021153]
		[batch 20/20] avg loss: 0.08491554523861952		[learning rate: 0.00021102]
	Learning Rate: 0.000211016
	LOSS [training: 0.07778121305094227 | validation: 0.05037116438903609]
	TIME [epoch: 8.16 sec]
EPOCH 847/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07929778757200849		[learning rate: 0.00021051]
		[batch 20/20] avg loss: 0.1234168631276126		[learning rate: 0.00021]
	Learning Rate: 0.000209996
	LOSS [training: 0.10135732534981054 | validation: 0.07632491872797234]
	TIME [epoch: 8.16 sec]
EPOCH 848/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08048950298511051		[learning rate: 0.00020949]
		[batch 20/20] avg loss: 0.0963138897411185		[learning rate: 0.00020898]
	Learning Rate: 0.00020898
	LOSS [training: 0.08840169636311448 | validation: 0.06064993951274925]
	TIME [epoch: 8.16 sec]
EPOCH 849/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0686216858502054		[learning rate: 0.00020847]
		[batch 20/20] avg loss: 0.09461571422079894		[learning rate: 0.00020797]
	Learning Rate: 0.00020797
	LOSS [training: 0.08161870003550216 | validation: 0.06848728629132722]
	TIME [epoch: 8.18 sec]
EPOCH 850/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10290051692678714		[learning rate: 0.00020747]
		[batch 20/20] avg loss: 0.06717484126127395		[learning rate: 0.00020696]
	Learning Rate: 0.000206964
	LOSS [training: 0.08503767909403054 | validation: 0.08603710594030867]
	TIME [epoch: 8.16 sec]
EPOCH 851/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10556431133610637		[learning rate: 0.00020646]
		[batch 20/20] avg loss: 0.10467020846141455		[learning rate: 0.00020596]
	Learning Rate: 0.000205963
	LOSS [training: 0.10511725989876046 | validation: 0.09329610196800307]
	TIME [epoch: 8.16 sec]
EPOCH 852/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08513285276739542		[learning rate: 0.00020546]
		[batch 20/20] avg loss: 0.0691894115968879		[learning rate: 0.00020497]
	Learning Rate: 0.000204967
	LOSS [training: 0.07716113218214166 | validation: 0.05788975526644738]
	TIME [epoch: 8.16 sec]
EPOCH 853/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08706367184151052		[learning rate: 0.00020447]
		[batch 20/20] avg loss: 0.08018630701160523		[learning rate: 0.00020398]
	Learning Rate: 0.000203976
	LOSS [training: 0.08362498942655787 | validation: 0.11960222453759234]
	TIME [epoch: 8.18 sec]
EPOCH 854/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09201357956639619		[learning rate: 0.00020348]
		[batch 20/20] avg loss: 0.08358742833979324		[learning rate: 0.00020299]
	Learning Rate: 0.00020299
	LOSS [training: 0.08780050395309473 | validation: 0.06028433824315896]
	TIME [epoch: 8.16 sec]
EPOCH 855/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09611759342489054		[learning rate: 0.0002025]
		[batch 20/20] avg loss: 0.10304261616136032		[learning rate: 0.00020201]
	Learning Rate: 0.000202008
	LOSS [training: 0.09958010479312543 | validation: 0.12029484892082158]
	TIME [epoch: 8.16 sec]
EPOCH 856/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09626206888560229		[learning rate: 0.00020152]
		[batch 20/20] avg loss: 0.0926656744057952		[learning rate: 0.00020103]
	Learning Rate: 0.000201031
	LOSS [training: 0.09446387164569875 | validation: 0.0692885283882268]
	TIME [epoch: 8.16 sec]
EPOCH 857/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08974658887166115		[learning rate: 0.00020054]
		[batch 20/20] avg loss: 0.07398514023363338		[learning rate: 0.00020006]
	Learning Rate: 0.000200059
	LOSS [training: 0.08186586455264724 | validation: 0.05924077839656186]
	TIME [epoch: 8.16 sec]
EPOCH 858/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09800452012350869		[learning rate: 0.00019957]
		[batch 20/20] avg loss: 0.0965235849143431		[learning rate: 0.00019909]
	Learning Rate: 0.000199091
	LOSS [training: 0.09726405251892589 | validation: 0.10597843736780221]
	TIME [epoch: 8.17 sec]
EPOCH 859/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09330361487207064		[learning rate: 0.00019861]
		[batch 20/20] avg loss: 0.07088217543045738		[learning rate: 0.00019813]
	Learning Rate: 0.000198129
	LOSS [training: 0.08209289515126403 | validation: 0.06559699830175102]
	TIME [epoch: 8.15 sec]
EPOCH 860/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10761677761764617		[learning rate: 0.00019765]
		[batch 20/20] avg loss: 0.1093538152986147		[learning rate: 0.00019717]
	Learning Rate: 0.000197171
	LOSS [training: 0.1084852964581304 | validation: 0.08208478148206316]
	TIME [epoch: 8.15 sec]
EPOCH 861/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07442341689266307		[learning rate: 0.00019669]
		[batch 20/20] avg loss: 0.08281927036437825		[learning rate: 0.00019622]
	Learning Rate: 0.000196217
	LOSS [training: 0.07862134362852066 | validation: 0.05721536609368476]
	TIME [epoch: 8.15 sec]
EPOCH 862/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07615423698495678		[learning rate: 0.00019574]
		[batch 20/20] avg loss: 0.08176603975355626		[learning rate: 0.00019527]
	Learning Rate: 0.000195268
	LOSS [training: 0.0789601383692565 | validation: 0.07234151348793469]
	TIME [epoch: 8.18 sec]
EPOCH 863/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08867085248670191		[learning rate: 0.0001948]
		[batch 20/20] avg loss: 0.09331280093207062		[learning rate: 0.00019432]
	Learning Rate: 0.000194324
	LOSS [training: 0.09099182670938626 | validation: 0.1382516612713457]
	TIME [epoch: 8.15 sec]
EPOCH 864/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10558571912706718		[learning rate: 0.00019385]
		[batch 20/20] avg loss: 0.07583696908546309		[learning rate: 0.00019338]
	Learning Rate: 0.000193384
	LOSS [training: 0.09071134410626513 | validation: 0.08419908904919911]
	TIME [epoch: 8.16 sec]
EPOCH 865/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08034117931679959		[learning rate: 0.00019292]
		[batch 20/20] avg loss: 0.07004687783528332		[learning rate: 0.00019245]
	Learning Rate: 0.000192449
	LOSS [training: 0.07519402857604145 | validation: 0.07733080489209913]
	TIME [epoch: 8.16 sec]
EPOCH 866/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09849969446964998		[learning rate: 0.00019198]
		[batch 20/20] avg loss: 0.08931276961767996		[learning rate: 0.00019152]
	Learning Rate: 0.000191518
	LOSS [training: 0.09390623204366495 | validation: 0.06754752508217446]
	TIME [epoch: 8.17 sec]
EPOCH 867/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07824700708002173		[learning rate: 0.00019105]
		[batch 20/20] avg loss: 0.07104839730557812		[learning rate: 0.00019059]
	Learning Rate: 0.000190592
	LOSS [training: 0.0746477021927999 | validation: 0.08311360083534908]
	TIME [epoch: 8.17 sec]
EPOCH 868/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09836145887192856		[learning rate: 0.00019013]
		[batch 20/20] avg loss: 0.08484762810904004		[learning rate: 0.00018967]
	Learning Rate: 0.000189671
	LOSS [training: 0.09160454349048429 | validation: 0.059737692132915646]
	TIME [epoch: 8.16 sec]
EPOCH 869/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08795147660451796		[learning rate: 0.00018921]
		[batch 20/20] avg loss: 0.07218440220373698		[learning rate: 0.00018875]
	Learning Rate: 0.000188753
	LOSS [training: 0.08006793940412747 | validation: 0.06607477025063302]
	TIME [epoch: 8.16 sec]
EPOCH 870/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.104904040986241		[learning rate: 0.0001883]
		[batch 20/20] avg loss: 0.07561783339883141		[learning rate: 0.00018784]
	Learning Rate: 0.000187841
	LOSS [training: 0.0902609371925362 | validation: 0.09604897271588012]
	TIME [epoch: 8.16 sec]
EPOCH 871/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09374821444530408		[learning rate: 0.00018739]
		[batch 20/20] avg loss: 0.08623497894001056		[learning rate: 0.00018693]
	Learning Rate: 0.000186932
	LOSS [training: 0.08999159669265734 | validation: 0.08086228708333851]
	TIME [epoch: 8.18 sec]
EPOCH 872/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08545180620561846		[learning rate: 0.00018648]
		[batch 20/20] avg loss: 0.08593665525001516		[learning rate: 0.00018603]
	Learning Rate: 0.000186028
	LOSS [training: 0.0856942307278168 | validation: 0.05425435367735065]
	TIME [epoch: 8.15 sec]
EPOCH 873/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07515940545710474		[learning rate: 0.00018558]
		[batch 20/20] avg loss: 0.11647027231359104		[learning rate: 0.00018513]
	Learning Rate: 0.000185129
	LOSS [training: 0.09581483888534789 | validation: 0.06318732120774173]
	TIME [epoch: 8.16 sec]
EPOCH 874/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08054251843535512		[learning rate: 0.00018468]
		[batch 20/20] avg loss: 0.08427163801230787		[learning rate: 0.00018423]
	Learning Rate: 0.000184233
	LOSS [training: 0.0824070782238315 | validation: 0.07318138256096915]
	TIME [epoch: 8.15 sec]
EPOCH 875/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09373862798403845		[learning rate: 0.00018379]
		[batch 20/20] avg loss: 0.08929189126894964		[learning rate: 0.00018334]
	Learning Rate: 0.000183343
	LOSS [training: 0.09151525962649405 | validation: 0.04903087944019825]
	TIME [epoch: 8.17 sec]
EPOCH 876/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08718931171673694		[learning rate: 0.0001829]
		[batch 20/20] avg loss: 0.08736125396025676		[learning rate: 0.00018246]
	Learning Rate: 0.000182456
	LOSS [training: 0.08727528283849685 | validation: 0.04944862983211492]
	TIME [epoch: 8.17 sec]
EPOCH 877/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07381497405867067		[learning rate: 0.00018201]
		[batch 20/20] avg loss: 0.08548129350939672		[learning rate: 0.00018157]
	Learning Rate: 0.000181574
	LOSS [training: 0.07964813378403371 | validation: 0.06541488994486735]
	TIME [epoch: 8.16 sec]
EPOCH 878/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10400999605499472		[learning rate: 0.00018113]
		[batch 20/20] avg loss: 0.09359999566060612		[learning rate: 0.0001807]
	Learning Rate: 0.000180696
	LOSS [training: 0.09880499585780042 | validation: 0.09693226649657345]
	TIME [epoch: 8.16 sec]
EPOCH 879/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0834528582112191		[learning rate: 0.00018026]
		[batch 20/20] avg loss: 0.08994510605273136		[learning rate: 0.00017982]
	Learning Rate: 0.000179822
	LOSS [training: 0.08669898213197524 | validation: 0.07338897822914846]
	TIME [epoch: 8.16 sec]
EPOCH 880/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11580820858275158		[learning rate: 0.00017939]
		[batch 20/20] avg loss: 0.07963953262624122		[learning rate: 0.00017895]
	Learning Rate: 0.000178952
	LOSS [training: 0.09772387060449637 | validation: 0.05945549437118487]
	TIME [epoch: 8.19 sec]
EPOCH 881/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07393662055737013		[learning rate: 0.00017852]
		[batch 20/20] avg loss: 0.0842725459958187		[learning rate: 0.00017809]
	Learning Rate: 0.000178087
	LOSS [training: 0.0791045832765944 | validation: 0.07286273916447122]
	TIME [epoch: 8.15 sec]
EPOCH 882/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09816791360269203		[learning rate: 0.00017766]
		[batch 20/20] avg loss: 0.07810455259635501		[learning rate: 0.00017723]
	Learning Rate: 0.000177226
	LOSS [training: 0.08813623309952352 | validation: 0.049603041103238335]
	TIME [epoch: 8.16 sec]
EPOCH 883/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07465490055886528		[learning rate: 0.0001768]
		[batch 20/20] avg loss: 0.08798198113858073		[learning rate: 0.00017637]
	Learning Rate: 0.000176369
	LOSS [training: 0.081318440848723 | validation: 0.05887037806959326]
	TIME [epoch: 8.16 sec]
EPOCH 884/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06683800348530504		[learning rate: 0.00017594]
		[batch 20/20] avg loss: 0.08506406335193142		[learning rate: 0.00017552]
	Learning Rate: 0.000175516
	LOSS [training: 0.07595103341861822 | validation: 0.06592581280806999]
	TIME [epoch: 8.18 sec]
EPOCH 885/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.092066508707256		[learning rate: 0.00017509]
		[batch 20/20] avg loss: 0.07958069190369868		[learning rate: 0.00017467]
	Learning Rate: 0.000174667
	LOSS [training: 0.08582360030547734 | validation: 0.06636074076596445]
	TIME [epoch: 8.16 sec]
EPOCH 886/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08559069142619372		[learning rate: 0.00017424]
		[batch 20/20] avg loss: 0.08247758186717022		[learning rate: 0.00017382]
	Learning Rate: 0.000173822
	LOSS [training: 0.084034136646682 | validation: 0.09911146144131765]
	TIME [epoch: 8.16 sec]
EPOCH 887/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09401947879306147		[learning rate: 0.0001734]
		[batch 20/20] avg loss: 0.08400575824445726		[learning rate: 0.00017298]
	Learning Rate: 0.000172982
	LOSS [training: 0.08901261851875937 | validation: 0.053768326384036634]
	TIME [epoch: 8.16 sec]
EPOCH 888/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06864465916814812		[learning rate: 0.00017256]
		[batch 20/20] avg loss: 0.07535727019747844		[learning rate: 0.00017215]
	Learning Rate: 0.000172145
	LOSS [training: 0.07200096468281328 | validation: 0.06433104017752883]
	TIME [epoch: 8.17 sec]
EPOCH 889/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08595721113263759		[learning rate: 0.00017173]
		[batch 20/20] avg loss: 0.0790449994283457		[learning rate: 0.00017131]
	Learning Rate: 0.000171313
	LOSS [training: 0.08250110528049165 | validation: 0.06346613356920291]
	TIME [epoch: 8.18 sec]
EPOCH 890/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07113415516667598		[learning rate: 0.0001709]
		[batch 20/20] avg loss: 0.08485343545476554		[learning rate: 0.00017048]
	Learning Rate: 0.000170484
	LOSS [training: 0.07799379531072076 | validation: 0.05964312830390819]
	TIME [epoch: 8.16 sec]
EPOCH 891/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08396617587751223		[learning rate: 0.00017007]
		[batch 20/20] avg loss: 0.06746651079454392		[learning rate: 0.00016966]
	Learning Rate: 0.00016966
	LOSS [training: 0.07571634333602809 | validation: 0.09406611489582457]
	TIME [epoch: 8.16 sec]
EPOCH 892/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09810741724866284		[learning rate: 0.00016925]
		[batch 20/20] avg loss: 0.07062400195282148		[learning rate: 0.00016884]
	Learning Rate: 0.000168839
	LOSS [training: 0.08436570960074216 | validation: 0.10051652047532225]
	TIME [epoch: 8.16 sec]
EPOCH 893/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08934294661877565		[learning rate: 0.00016843]
		[batch 20/20] avg loss: 0.08632148583036015		[learning rate: 0.00016802]
	Learning Rate: 0.000168023
	LOSS [training: 0.08783221622456791 | validation: 0.04616092198974229]
	TIME [epoch: 8.19 sec]
EPOCH 894/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10574600548665532		[learning rate: 0.00016762]
		[batch 20/20] avg loss: 0.0694445766318012		[learning rate: 0.00016721]
	Learning Rate: 0.00016721
	LOSS [training: 0.08759529105922828 | validation: 0.09614628566766445]
	TIME [epoch: 8.16 sec]
EPOCH 895/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11549298155424541		[learning rate: 0.00016681]
		[batch 20/20] avg loss: 0.07095405313795325		[learning rate: 0.0001664]
	Learning Rate: 0.000166402
	LOSS [training: 0.09322351734609932 | validation: 0.07253730600480515]
	TIME [epoch: 8.16 sec]
EPOCH 896/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07287403522664646		[learning rate: 0.000166]
		[batch 20/20] avg loss: 0.07457671351192352		[learning rate: 0.0001656]
	Learning Rate: 0.000165597
	LOSS [training: 0.07372537436928497 | validation: 0.07910109089253628]
	TIME [epoch: 8.16 sec]
EPOCH 897/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08926670039305985		[learning rate: 0.0001652]
		[batch 20/20] avg loss: 0.07043407711878387		[learning rate: 0.0001648]
	Learning Rate: 0.000164796
	LOSS [training: 0.07985038875592185 | validation: 0.06448656827098385]
	TIME [epoch: 8.18 sec]
EPOCH 898/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07313717388230574		[learning rate: 0.0001644]
		[batch 20/20] avg loss: 0.08553404558122457		[learning rate: 0.000164]
	Learning Rate: 0.000163999
	LOSS [training: 0.07933560973176515 | validation: 0.07316750382546289]
	TIME [epoch: 8.17 sec]
EPOCH 899/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07137200411220952		[learning rate: 0.0001636]
		[batch 20/20] avg loss: 0.07094232782843453		[learning rate: 0.00016321]
	Learning Rate: 0.000163206
	LOSS [training: 0.07115716597032204 | validation: 0.06620543556840465]
	TIME [epoch: 8.16 sec]
EPOCH 900/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07078146388559123		[learning rate: 0.00016281]
		[batch 20/20] avg loss: 0.08623802827845153		[learning rate: 0.00016242]
	Learning Rate: 0.000162417
	LOSS [training: 0.07850974608202138 | validation: 0.09834171260360267]
	TIME [epoch: 8.16 sec]
EPOCH 901/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08017942399593857		[learning rate: 0.00016202]
		[batch 20/20] avg loss: 0.07756937685434293		[learning rate: 0.00016163]
	Learning Rate: 0.000161632
	LOSS [training: 0.07887440042514075 | validation: 0.19203180140937853]
	TIME [epoch: 8.16 sec]
EPOCH 902/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10921777705569373		[learning rate: 0.00016124]
		[batch 20/20] avg loss: 0.06803313972609255		[learning rate: 0.00016085]
	Learning Rate: 0.00016085
	LOSS [training: 0.08862545839089314 | validation: 0.06225920953689792]
	TIME [epoch: 8.19 sec]
EPOCH 903/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07326999033061393		[learning rate: 0.00016046]
		[batch 20/20] avg loss: 0.07620912676700395		[learning rate: 0.00016007]
	Learning Rate: 0.000160072
	LOSS [training: 0.07473955854880894 | validation: 0.06765322668078606]
	TIME [epoch: 8.16 sec]
EPOCH 904/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09180782287179508		[learning rate: 0.00015968]
		[batch 20/20] avg loss: 0.08057955238616134		[learning rate: 0.0001593]
	Learning Rate: 0.000159298
	LOSS [training: 0.08619368762897821 | validation: 0.07558238029619344]
	TIME [epoch: 8.16 sec]
EPOCH 905/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07091213813106198		[learning rate: 0.00015891]
		[batch 20/20] avg loss: 0.07822643688575195		[learning rate: 0.00015853]
	Learning Rate: 0.000158528
	LOSS [training: 0.07456928750840697 | validation: 0.05350463553803733]
	TIME [epoch: 8.16 sec]
EPOCH 906/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07438102703073698		[learning rate: 0.00015814]
		[batch 20/20] avg loss: 0.06979113087992088		[learning rate: 0.00015776]
	Learning Rate: 0.000157761
	LOSS [training: 0.07208607895532892 | validation: 0.0736942444240517]
	TIME [epoch: 8.18 sec]
EPOCH 907/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07308794554075834		[learning rate: 0.00015738]
		[batch 20/20] avg loss: 0.08902638634641397		[learning rate: 0.000157]
	Learning Rate: 0.000156998
	LOSS [training: 0.08105716594358614 | validation: 0.06786032736631549]
	TIME [epoch: 8.17 sec]
EPOCH 908/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07972032451274938		[learning rate: 0.00015662]
		[batch 20/20] avg loss: 0.06879344312951409		[learning rate: 0.00015624]
	Learning Rate: 0.000156239
	LOSS [training: 0.07425688382113173 | validation: 0.04563230830708573]
	TIME [epoch: 8.16 sec]
EPOCH 909/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07853293829868788		[learning rate: 0.00015586]
		[batch 20/20] avg loss: 0.06860261231024052		[learning rate: 0.00015548]
	Learning Rate: 0.000155483
	LOSS [training: 0.0735677753044642 | validation: 0.06161275582665186]
	TIME [epoch: 8.16 sec]
EPOCH 910/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09022786261574335		[learning rate: 0.00015511]
		[batch 20/20] avg loss: 0.07033174908877363		[learning rate: 0.00015473]
	Learning Rate: 0.000154732
	LOSS [training: 0.0802798058522585 | validation: 0.13603926771457545]
	TIME [epoch: 8.17 sec]
EPOCH 911/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09006404140185095		[learning rate: 0.00015436]
		[batch 20/20] avg loss: 0.07207911626513781		[learning rate: 0.00015398]
	Learning Rate: 0.000153983
	LOSS [training: 0.08107157883349436 | validation: 0.07817064072479876]
	TIME [epoch: 8.19 sec]
EPOCH 912/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07452235714012315		[learning rate: 0.00015361]
		[batch 20/20] avg loss: 0.07836588656069855		[learning rate: 0.00015324]
	Learning Rate: 0.000153239
	LOSS [training: 0.07644412185041086 | validation: 0.05467564471489597]
	TIME [epoch: 8.16 sec]
EPOCH 913/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06097335121532514		[learning rate: 0.00015287]
		[batch 20/20] avg loss: 0.10661133831278118		[learning rate: 0.0001525]
	Learning Rate: 0.000152498
	LOSS [training: 0.08379234476405317 | validation: 0.09672385042129479]
	TIME [epoch: 8.18 sec]
EPOCH 914/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0901890266641164		[learning rate: 0.00015213]
		[batch 20/20] avg loss: 0.06270087062636469		[learning rate: 0.00015176]
	Learning Rate: 0.00015176
	LOSS [training: 0.07644494864524053 | validation: 0.051717969781646986]
	TIME [epoch: 8.16 sec]
EPOCH 915/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0802636740367579		[learning rate: 0.00015139]
		[batch 20/20] avg loss: 0.07656716534109713		[learning rate: 0.00015103]
	Learning Rate: 0.000151026
	LOSS [training: 0.07841541968892753 | validation: 0.05745048854828335]
	TIME [epoch: 8.18 sec]
EPOCH 916/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08394985431026156		[learning rate: 0.00015066]
		[batch 20/20] avg loss: 0.10317182970277652		[learning rate: 0.0001503]
	Learning Rate: 0.000150296
	LOSS [training: 0.09356084200651904 | validation: 0.09047627373030127]
	TIME [epoch: 8.16 sec]
EPOCH 917/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07510097134939317		[learning rate: 0.00014993]
		[batch 20/20] avg loss: 0.11567633971727531		[learning rate: 0.00014957]
	Learning Rate: 0.000149569
	LOSS [training: 0.09538865553333425 | validation: 0.09894336429963344]
	TIME [epoch: 8.16 sec]
EPOCH 918/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07531825719580093		[learning rate: 0.00014921]
		[batch 20/20] avg loss: 0.07061297040294152		[learning rate: 0.00014885]
	Learning Rate: 0.000148846
	LOSS [training: 0.07296561379937122 | validation: 0.07136794237791674]
	TIME [epoch: 8.16 sec]
EPOCH 919/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07411215387177562		[learning rate: 0.00014849]
		[batch 20/20] avg loss: 0.07918450946041675		[learning rate: 0.00014813]
	Learning Rate: 0.000148126
	LOSS [training: 0.07664833166609619 | validation: 0.05419502240546256]
	TIME [epoch: 8.16 sec]
EPOCH 920/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0743058487358195		[learning rate: 0.00014777]
		[batch 20/20] avg loss: 0.07038690589029427		[learning rate: 0.00014741]
	Learning Rate: 0.00014741
	LOSS [training: 0.07234637731305688 | validation: 0.052204995503056684]
	TIME [epoch: 8.18 sec]
EPOCH 921/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07564584400399929		[learning rate: 0.00014705]
		[batch 20/20] avg loss: 0.08462475267924593		[learning rate: 0.0001467]
	Learning Rate: 0.000146697
	LOSS [training: 0.08013529834162261 | validation: 0.0916961660286156]
	TIME [epoch: 8.16 sec]
EPOCH 922/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07254382029053308		[learning rate: 0.00014634]
		[batch 20/20] avg loss: 0.07932343334368039		[learning rate: 0.00014599]
	Learning Rate: 0.000145988
	LOSS [training: 0.07593362681710673 | validation: 0.05540781081234253]
	TIME [epoch: 8.16 sec]
EPOCH 923/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07757592027290042		[learning rate: 0.00014563]
		[batch 20/20] avg loss: 0.0876756820260324		[learning rate: 0.00014528]
	Learning Rate: 0.000145282
	LOSS [training: 0.08262580114946641 | validation: 0.05203407541187638]
	TIME [epoch: 8.16 sec]
EPOCH 924/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0678201742618163		[learning rate: 0.00014493]
		[batch 20/20] avg loss: 0.07902115089225306		[learning rate: 0.00014458]
	Learning Rate: 0.000144579
	LOSS [training: 0.07342066257703467 | validation: 0.04435531035423889]
	TIME [epoch: 8.18 sec]
EPOCH 925/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08454738248432624		[learning rate: 0.00014423]
		[batch 20/20] avg loss: 0.0777328956665367		[learning rate: 0.00014388]
	Learning Rate: 0.00014388
	LOSS [training: 0.08114013907543147 | validation: 0.05533524404950987]
	TIME [epoch: 8.16 sec]
EPOCH 926/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06673739360120579		[learning rate: 0.00014353]
		[batch 20/20] avg loss: 0.08450836848378693		[learning rate: 0.00014318]
	Learning Rate: 0.000143184
	LOSS [training: 0.07562288104249634 | validation: 0.052101749339349704]
	TIME [epoch: 8.16 sec]
EPOCH 927/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06543122680078725		[learning rate: 0.00014284]
		[batch 20/20] avg loss: 0.08093069902742328		[learning rate: 0.00014249]
	Learning Rate: 0.000142492
	LOSS [training: 0.07318096291410528 | validation: 0.05531527402318369]
	TIME [epoch: 8.15 sec]
EPOCH 928/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06808372156058805		[learning rate: 0.00014215]
		[batch 20/20] avg loss: 0.07673615123546187		[learning rate: 0.0001418]
	Learning Rate: 0.000141803
	LOSS [training: 0.07240993639802495 | validation: 0.07518317052004804]
	TIME [epoch: 8.17 sec]
EPOCH 929/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07182621140856789		[learning rate: 0.00014146]
		[batch 20/20] avg loss: 0.09966944072362778		[learning rate: 0.00014112]
	Learning Rate: 0.000141117
	LOSS [training: 0.08574782606609782 | validation: 0.05842289344823803]
	TIME [epoch: 8.16 sec]
EPOCH 930/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07420285653813515		[learning rate: 0.00014078]
		[batch 20/20] avg loss: 0.08272275550808399		[learning rate: 0.00014043]
	Learning Rate: 0.000140434
	LOSS [training: 0.07846280602310958 | validation: 0.05163754017467466]
	TIME [epoch: 8.16 sec]
EPOCH 931/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06653880456594695		[learning rate: 0.00014009]
		[batch 20/20] avg loss: 0.06776645502259457		[learning rate: 0.00013976]
	Learning Rate: 0.000139755
	LOSS [training: 0.06715262979427077 | validation: 0.049441665023913055]
	TIME [epoch: 8.15 sec]
EPOCH 932/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06129965191498419		[learning rate: 0.00013942]
		[batch 20/20] avg loss: 0.08089536756346123		[learning rate: 0.00013908]
	Learning Rate: 0.00013908
	LOSS [training: 0.07109750973922271 | validation: 0.06161180602271106]
	TIME [epoch: 8.15 sec]
EPOCH 933/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07208291943484493		[learning rate: 0.00013874]
		[batch 20/20] avg loss: 0.07790231374302853		[learning rate: 0.00013841]
	Learning Rate: 0.000138407
	LOSS [training: 0.07499261658893673 | validation: 0.06987973838808655]
	TIME [epoch: 8.18 sec]
EPOCH 934/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07269419995752802		[learning rate: 0.00013807]
		[batch 20/20] avg loss: 0.0819331500750855		[learning rate: 0.00013774]
	Learning Rate: 0.000137738
	LOSS [training: 0.07731367501630675 | validation: 0.06569854914575757]
	TIME [epoch: 8.16 sec]
EPOCH 935/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07854251910809307		[learning rate: 0.0001374]
		[batch 20/20] avg loss: 0.07713528255623084		[learning rate: 0.00013707]
	Learning Rate: 0.000137072
	LOSS [training: 0.07783890083216197 | validation: 0.06917008116061897]
	TIME [epoch: 8.16 sec]
EPOCH 936/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06249373383391591		[learning rate: 0.00013674]
		[batch 20/20] avg loss: 0.08079819897987002		[learning rate: 0.00013641]
	Learning Rate: 0.000136409
	LOSS [training: 0.07164596640689296 | validation: 0.06159465690473219]
	TIME [epoch: 8.15 sec]
EPOCH 937/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07755948828001272		[learning rate: 0.00013608]
		[batch 20/20] avg loss: 0.08862925241453573		[learning rate: 0.00013575]
	Learning Rate: 0.000135749
	LOSS [training: 0.08309437034727424 | validation: 0.04599921397102906]
	TIME [epoch: 8.17 sec]
EPOCH 938/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06509824332704067		[learning rate: 0.00013542]
		[batch 20/20] avg loss: 0.07637853836181215		[learning rate: 0.00013509]
	Learning Rate: 0.000135093
	LOSS [training: 0.07073839084442639 | validation: 0.06590869977956387]
	TIME [epoch: 8.16 sec]
EPOCH 939/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08939373306639661		[learning rate: 0.00013477]
		[batch 20/20] avg loss: 0.07811956396314161		[learning rate: 0.00013444]
	Learning Rate: 0.000134439
	LOSS [training: 0.08375664851476912 | validation: 0.08497167638284706]
	TIME [epoch: 8.15 sec]
EPOCH 940/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07657090836451469		[learning rate: 0.00013411]
		[batch 20/20] avg loss: 0.07158327217318354		[learning rate: 0.00013379]
	Learning Rate: 0.000133789
	LOSS [training: 0.07407709026884911 | validation: 0.05528961147773745]
	TIME [epoch: 8.15 sec]
EPOCH 941/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07919404237701111		[learning rate: 0.00013347]
		[batch 20/20] avg loss: 0.0855767982447081		[learning rate: 0.00013314]
	Learning Rate: 0.000133142
	LOSS [training: 0.08238542031085962 | validation: 0.08760145850623573]
	TIME [epoch: 8.15 sec]
EPOCH 942/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0879015994461402		[learning rate: 0.00013282]
		[batch 20/20] avg loss: 0.07544151946716005		[learning rate: 0.0001325]
	Learning Rate: 0.000132498
	LOSS [training: 0.08167155945665013 | validation: 0.04463570694478441]
	TIME [epoch: 8.18 sec]
EPOCH 943/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07391305674234747		[learning rate: 0.00013218]
		[batch 20/20] avg loss: 0.0658411443328378		[learning rate: 0.00013186]
	Learning Rate: 0.000131858
	LOSS [training: 0.06987710053759263 | validation: 0.052796208203710976]
	TIME [epoch: 8.15 sec]
EPOCH 944/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07347195111778161		[learning rate: 0.00013154]
		[batch 20/20] avg loss: 0.07053231996364054		[learning rate: 0.00013122]
	Learning Rate: 0.00013122
	LOSS [training: 0.07200213554071107 | validation: 0.057806400282407716]
	TIME [epoch: 8.15 sec]
EPOCH 945/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07104181964075021		[learning rate: 0.0001309]
		[batch 20/20] avg loss: 0.07351063394438352		[learning rate: 0.00013059]
	Learning Rate: 0.000130585
	LOSS [training: 0.07227622679256687 | validation: 0.060186916258225455]
	TIME [epoch: 8.15 sec]
EPOCH 946/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07593009097839405		[learning rate: 0.00013027]
		[batch 20/20] avg loss: 0.08452390857220866		[learning rate: 0.00012995]
	Learning Rate: 0.000129954
	LOSS [training: 0.08022699977530137 | validation: 0.08745189396163042]
	TIME [epoch: 8.17 sec]
EPOCH 947/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08310879807513279		[learning rate: 0.00012964]
		[batch 20/20] avg loss: 0.07050506276690291		[learning rate: 0.00012933]
	Learning Rate: 0.000129326
	LOSS [training: 0.07680693042101784 | validation: 0.05310294689649204]
	TIME [epoch: 8.16 sec]
EPOCH 948/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06421788070596993		[learning rate: 0.00012901]
		[batch 20/20] avg loss: 0.08171940122072896		[learning rate: 0.0001287]
	Learning Rate: 0.0001287
	LOSS [training: 0.07296864096334944 | validation: 0.08660968418559954]
	TIME [epoch: 8.15 sec]
EPOCH 949/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08216563683700417		[learning rate: 0.00012839]
		[batch 20/20] avg loss: 0.07303213048956095		[learning rate: 0.00012808]
	Learning Rate: 0.000128078
	LOSS [training: 0.07759888366328255 | validation: 0.06393939824936841]
	TIME [epoch: 8.15 sec]
EPOCH 950/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07822616660813596		[learning rate: 0.00012777]
		[batch 20/20] avg loss: 0.07516181202716114		[learning rate: 0.00012746]
	Learning Rate: 0.000127458
	LOSS [training: 0.07669398931764856 | validation: 0.04631260799036736]
	TIME [epoch: 8.16 sec]
EPOCH 951/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08123361441860238		[learning rate: 0.00012715]
		[batch 20/20] avg loss: 0.07165600702079389		[learning rate: 0.00012684]
	Learning Rate: 0.000126842
	LOSS [training: 0.07644481071969812 | validation: 0.0620246945043362]
	TIME [epoch: 8.18 sec]
EPOCH 952/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06911669192456645		[learning rate: 0.00012653]
		[batch 20/20] avg loss: 0.07160875022574752		[learning rate: 0.00012623]
	Learning Rate: 0.000126229
	LOSS [training: 0.07036272107515698 | validation: 0.0425239032750593]
	TIME [epoch: 8.15 sec]
EPOCH 953/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06506046417740742		[learning rate: 0.00012592]
		[batch 20/20] avg loss: 0.08198023891185394		[learning rate: 0.00012562]
	Learning Rate: 0.000125618
	LOSS [training: 0.07352035154463067 | validation: 0.05214604023449166]
	TIME [epoch: 8.15 sec]
EPOCH 954/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06795924959508984		[learning rate: 0.00012531]
		[batch 20/20] avg loss: 0.07763382082218352		[learning rate: 0.00012501]
	Learning Rate: 0.000125011
	LOSS [training: 0.07279653520863667 | validation: 0.054280915209017774]
	TIME [epoch: 8.15 sec]
EPOCH 955/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0680355016230583		[learning rate: 0.00012471]
		[batch 20/20] avg loss: 0.07489960032657642		[learning rate: 0.00012441]
	Learning Rate: 0.000124406
	LOSS [training: 0.07146755097481736 | validation: 0.05465999586535589]
	TIME [epoch: 8.17 sec]
EPOCH 956/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07550676462836323		[learning rate: 0.00012411]
		[batch 20/20] avg loss: 0.07087056310545306		[learning rate: 0.0001238]
	Learning Rate: 0.000123805
	LOSS [training: 0.07318866386690814 | validation: 0.0652218830373424]
	TIME [epoch: 8.15 sec]
EPOCH 957/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06429751286269625		[learning rate: 0.0001235]
		[batch 20/20] avg loss: 0.08625882556451364		[learning rate: 0.00012321]
	Learning Rate: 0.000123206
	LOSS [training: 0.07527816921360495 | validation: 0.06683333662205529]
	TIME [epoch: 8.16 sec]
EPOCH 958/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06715162770953101		[learning rate: 0.00012291]
		[batch 20/20] avg loss: 0.077312247329866		[learning rate: 0.00012261]
	Learning Rate: 0.00012261
	LOSS [training: 0.07223193751969852 | validation: 0.08498927301463186]
	TIME [epoch: 8.15 sec]
EPOCH 959/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07736809723478533		[learning rate: 0.00012231]
		[batch 20/20] avg loss: 0.07319537421607074		[learning rate: 0.00012202]
	Learning Rate: 0.000122017
	LOSS [training: 0.07528173572542804 | validation: 0.06169984501220144]
	TIME [epoch: 8.16 sec]
EPOCH 960/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08274474218223393		[learning rate: 0.00012172]
		[batch 20/20] avg loss: 0.0774893282336006		[learning rate: 0.00012143]
	Learning Rate: 0.000121427
	LOSS [training: 0.08011703520791726 | validation: 0.06557304332911033]
	TIME [epoch: 8.17 sec]
EPOCH 961/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07197799455158387		[learning rate: 0.00012113]
		[batch 20/20] avg loss: 0.07497253950109274		[learning rate: 0.00012084]
	Learning Rate: 0.00012084
	LOSS [training: 0.07347526702633832 | validation: 0.07991399468042618]
	TIME [epoch: 8.15 sec]
EPOCH 962/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06942583342551775		[learning rate: 0.00012055]
		[batch 20/20] avg loss: 0.0777793987241199		[learning rate: 0.00012026]
	Learning Rate: 0.000120256
	LOSS [training: 0.07360261607481881 | validation: 0.058197950251067734]
	TIME [epoch: 8.15 sec]
EPOCH 963/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07040787785915766		[learning rate: 0.00011996]
		[batch 20/20] avg loss: 0.07704272414448411		[learning rate: 0.00011967]
	Learning Rate: 0.000119674
	LOSS [training: 0.07372530100182088 | validation: 0.06488933471404976]
	TIME [epoch: 8.16 sec]
EPOCH 964/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07273997167458798		[learning rate: 0.00011938]
		[batch 20/20] avg loss: 0.06466432498171307		[learning rate: 0.0001191]
	Learning Rate: 0.000119095
	LOSS [training: 0.06870214832815054 | validation: 0.056406306285565526]
	TIME [epoch: 8.17 sec]
EPOCH 965/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.061225390850054226		[learning rate: 0.00011881]
		[batch 20/20] avg loss: 0.08161683049793983		[learning rate: 0.00011852]
	Learning Rate: 0.000118519
	LOSS [training: 0.07142111067399703 | validation: 0.08278028985747123]
	TIME [epoch: 8.15 sec]
EPOCH 966/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09297326505280704		[learning rate: 0.00011823]
		[batch 20/20] avg loss: 0.0935993948298203		[learning rate: 0.00011795]
	Learning Rate: 0.000117946
	LOSS [training: 0.09328632994131367 | validation: 0.07293857856596855]
	TIME [epoch: 8.15 sec]
EPOCH 967/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08754360609028101		[learning rate: 0.00011766]
		[batch 20/20] avg loss: 0.0753462256805327		[learning rate: 0.00011738]
	Learning Rate: 0.000117376
	LOSS [training: 0.08144491588540687 | validation: 0.04635810683849468]
	TIME [epoch: 8.14 sec]
EPOCH 968/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10064898096508028		[learning rate: 0.00011709]
		[batch 20/20] avg loss: 0.08439107546019356		[learning rate: 0.00011681]
	Learning Rate: 0.000116808
	LOSS [training: 0.09252002821263691 | validation: 0.06521521093380002]
	TIME [epoch: 8.17 sec]
EPOCH 969/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07035990971162051		[learning rate: 0.00011653]
		[batch 20/20] avg loss: 0.09041483880495198		[learning rate: 0.00011624]
	Learning Rate: 0.000116243
	LOSS [training: 0.08038737425828625 | validation: 0.10660656265998837]
	TIME [epoch: 8.16 sec]
EPOCH 970/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06972896878249875		[learning rate: 0.00011596]
		[batch 20/20] avg loss: 0.07063626926081976		[learning rate: 0.00011568]
	Learning Rate: 0.000115681
	LOSS [training: 0.07018261902165926 | validation: 0.07338159025083538]
	TIME [epoch: 8.15 sec]
EPOCH 971/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.064591535769744		[learning rate: 0.0001154]
		[batch 20/20] avg loss: 0.10711762427311226		[learning rate: 0.00011512]
	Learning Rate: 0.000115122
	LOSS [training: 0.08585458002142814 | validation: 0.058793328558098296]
	TIME [epoch: 8.15 sec]
EPOCH 972/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08017873415615459		[learning rate: 0.00011484]
		[batch 20/20] avg loss: 0.059889246823087106		[learning rate: 0.00011457]
	Learning Rate: 0.000114565
	LOSS [training: 0.07003399048962085 | validation: 0.06743888560661185]
	TIME [epoch: 8.15 sec]
EPOCH 973/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07126119807338335		[learning rate: 0.00011429]
		[batch 20/20] avg loss: 0.06465570786845591		[learning rate: 0.00011401]
	Learning Rate: 0.000114011
	LOSS [training: 0.06795845297091965 | validation: 0.04735542652641324]
	TIME [epoch: 8.17 sec]
EPOCH 974/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06352378651878005		[learning rate: 0.00011374]
		[batch 20/20] avg loss: 0.07506630337666476		[learning rate: 0.00011346]
	Learning Rate: 0.00011346
	LOSS [training: 0.0692950449477224 | validation: 0.06954581370742488]
	TIME [epoch: 8.15 sec]
EPOCH 975/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06682324514751908		[learning rate: 0.00011319]
		[batch 20/20] avg loss: 0.06979004606821074		[learning rate: 0.00011291]
	Learning Rate: 0.000112911
	LOSS [training: 0.0683066456078649 | validation: 0.049762435536751]
	TIME [epoch: 8.15 sec]
EPOCH 976/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07204520983309742		[learning rate: 0.00011264]
		[batch 20/20] avg loss: 0.06656349145958482		[learning rate: 0.00011237]
	Learning Rate: 0.000112365
	LOSS [training: 0.06930435064634113 | validation: 0.05161644896885723]
	TIME [epoch: 8.15 sec]
EPOCH 977/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07677045218746803		[learning rate: 0.00011209]
		[batch 20/20] avg loss: 0.0715382061096297		[learning rate: 0.00011182]
	Learning Rate: 0.000111822
	LOSS [training: 0.07415432914854887 | validation: 0.05204725612731825]
	TIME [epoch: 8.16 sec]
EPOCH 978/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06809797162160612		[learning rate: 0.00011155]
		[batch 20/20] avg loss: 0.08113213153159167		[learning rate: 0.00011128]
	Learning Rate: 0.000111281
	LOSS [training: 0.07461505157659891 | validation: 0.05360065139514655]
	TIME [epoch: 8.15 sec]
EPOCH 979/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07819881609542342		[learning rate: 0.00011101]
		[batch 20/20] avg loss: 0.06937217852690171		[learning rate: 0.00011074]
	Learning Rate: 0.000110743
	LOSS [training: 0.07378549731116257 | validation: 0.05497571835457571]
	TIME [epoch: 8.14 sec]
EPOCH 980/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07383413251814834		[learning rate: 0.00011047]
		[batch 20/20] avg loss: 0.0678378457012778		[learning rate: 0.00011021]
	Learning Rate: 0.000110207
	LOSS [training: 0.07083598910971307 | validation: 0.0489747890976004]
	TIME [epoch: 8.15 sec]
EPOCH 981/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09269150540523183		[learning rate: 0.00010994]
		[batch 20/20] avg loss: 0.07274378530616168		[learning rate: 0.00010967]
	Learning Rate: 0.000109674
	LOSS [training: 0.08271764535569677 | validation: 0.06012735848691385]
	TIME [epoch: 8.16 sec]
EPOCH 982/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07342603020400122		[learning rate: 0.00010941]
		[batch 20/20] avg loss: 0.09560014526309478		[learning rate: 0.00010914]
	Learning Rate: 0.000109144
	LOSS [training: 0.08451308773354801 | validation: 0.11444714909384862]
	TIME [epoch: 8.18 sec]
EPOCH 983/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0840216458787438		[learning rate: 0.00010888]
		[batch 20/20] avg loss: 0.07938396585248289		[learning rate: 0.00010862]
	Learning Rate: 0.000108616
	LOSS [training: 0.08170280586561338 | validation: 0.09005885093252716]
	TIME [epoch: 8.15 sec]
EPOCH 984/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08552746039369137		[learning rate: 0.00010835]
		[batch 20/20] avg loss: 0.07010703191590602		[learning rate: 0.00010809]
	Learning Rate: 0.000108091
	LOSS [training: 0.0778172461547987 | validation: 0.04758852880686379]
	TIME [epoch: 8.14 sec]
EPOCH 985/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07077377164206027		[learning rate: 0.00010783]
		[batch 20/20] avg loss: 0.06297310681362964		[learning rate: 0.00010757]
	Learning Rate: 0.000107568
	LOSS [training: 0.06687343922784494 | validation: 0.061591355317735096]
	TIME [epoch: 8.15 sec]
EPOCH 986/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.060349713234793066		[learning rate: 0.00010731]
		[batch 20/20] avg loss: 0.07285740784548282		[learning rate: 0.00010705]
	Learning Rate: 0.000107048
	LOSS [training: 0.06660356054013794 | validation: 0.051282022981362384]
	TIME [epoch: 8.16 sec]
EPOCH 987/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06601177086237534		[learning rate: 0.00010679]
		[batch 20/20] avg loss: 0.11326199438349782		[learning rate: 0.00010653]
	Learning Rate: 0.00010653
	LOSS [training: 0.08963688262293659 | validation: 0.10718075808703255]
	TIME [epoch: 8.16 sec]
EPOCH 988/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08851272422216118		[learning rate: 0.00010627]
		[batch 20/20] avg loss: 0.06710603316166328		[learning rate: 0.00010602]
	Learning Rate: 0.000106015
	LOSS [training: 0.07780937869191223 | validation: 0.050920422172752854]
	TIME [epoch: 8.15 sec]
EPOCH 989/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06931723369509273		[learning rate: 0.00010576]
		[batch 20/20] avg loss: 0.06505803805526753		[learning rate: 0.0001055]
	Learning Rate: 0.000105503
	LOSS [training: 0.06718763587518012 | validation: 0.04841867615448549]
	TIME [epoch: 8.16 sec]
EPOCH 990/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08053511021495943		[learning rate: 0.00010525]
		[batch 20/20] avg loss: 0.07364022240013776		[learning rate: 0.00010499]
	Learning Rate: 0.000104992
	LOSS [training: 0.0770876663075486 | validation: 0.12581843209935958]
	TIME [epoch: 8.16 sec]
EPOCH 991/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08347097590608299		[learning rate: 0.00010474]
		[batch 20/20] avg loss: 0.07205387120557996		[learning rate: 0.00010448]
	Learning Rate: 0.000104485
	LOSS [training: 0.07776242355583146 | validation: 0.04693176888722058]
	TIME [epoch: 8.18 sec]
EPOCH 992/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07317255817247621		[learning rate: 0.00010423]
		[batch 20/20] avg loss: 0.07201778932611995		[learning rate: 0.00010398]
	Learning Rate: 0.000103979
	LOSS [training: 0.0725951737492981 | validation: 0.07279339898155919]
	TIME [epoch: 8.16 sec]
EPOCH 993/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08285528381552001		[learning rate: 0.00010373]
		[batch 20/20] avg loss: 0.07809954330939692		[learning rate: 0.00010348]
	Learning Rate: 0.000103477
	LOSS [training: 0.08047741356245845 | validation: 0.07411524302050007]
	TIME [epoch: 8.16 sec]
EPOCH 994/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0637422978517366		[learning rate: 0.00010323]
		[batch 20/20] avg loss: 0.0728934625226603		[learning rate: 0.00010298]
	Learning Rate: 0.000102976
	LOSS [training: 0.06831788018719845 | validation: 0.059887179354910045]
	TIME [epoch: 8.15 sec]
EPOCH 995/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06490407629772905		[learning rate: 0.00010273]
		[batch 20/20] avg loss: 0.08220805632723012		[learning rate: 0.00010248]
	Learning Rate: 0.000102478
	LOSS [training: 0.07355606631247959 | validation: 0.06750516106481658]
	TIME [epoch: 8.17 sec]
EPOCH 996/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08074789163301954		[learning rate: 0.00010223]
		[batch 20/20] avg loss: 0.059394694966534024		[learning rate: 0.00010198]
	Learning Rate: 0.000101983
	LOSS [training: 0.07007129329977677 | validation: 0.055956133416978665]
	TIME [epoch: 8.15 sec]
EPOCH 997/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08088868591661148		[learning rate: 0.00010174]
		[batch 20/20] avg loss: 0.07150977894473093		[learning rate: 0.00010149]
	Learning Rate: 0.000101489
	LOSS [training: 0.0761992324306712 | validation: 0.07055118771734543]
	TIME [epoch: 8.14 sec]
EPOCH 998/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06402554722246197		[learning rate: 0.00010124]
		[batch 20/20] avg loss: 0.07328681631212278		[learning rate: 0.000101]
	Learning Rate: 0.000100999
	LOSS [training: 0.06865618176729237 | validation: 0.058111916300161284]
	TIME [epoch: 8.16 sec]
EPOCH 999/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08329773324417265		[learning rate: 0.00010075]
		[batch 20/20] avg loss: 0.07030341253017097		[learning rate: 0.00010051]
	Learning Rate: 0.00010051
	LOSS [training: 0.07680057288717182 | validation: 0.0678204802754637]
	TIME [epoch: 8.16 sec]
EPOCH 1000/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.072550663014523		[learning rate: 0.00010027]
		[batch 20/20] avg loss: 0.08642785784661164		[learning rate: 0.00010002]
	Learning Rate: 0.000100024
	LOSS [training: 0.07948926043056734 | validation: 0.054779209171693224]
	TIME [epoch: 8.16 sec]
Finished training in 8282.558 seconds.
