Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r1', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 931036429

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 10/20] avg loss: 8.209068853569613		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.963560383014347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.086314618291979 | validation: 4.288570857291573]
	TIME [epoch: 70.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.622415031819564		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.034931482352423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.328673257085993 | validation: 3.200781468947767]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.7941048619978757		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1507738387537207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.472439350375798 | validation: 3.624292294790213]
	TIME [epoch: 8.84 sec]
EPOCH 4/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.154652746633585		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9973806305778554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0760166886057205 | validation: 2.3504766924043334]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.783388163690567		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6918164430316303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7376023033610988 | validation: 2.131144801868978]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.529578192321075		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2376873668084736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.883632779564774 | validation: 3.680817223696705]
	TIME [epoch: 8.85 sec]
EPOCH 7/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.8212049652433153		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7438796595089805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.782542312376147 | validation: 3.5819306001853843]
	TIME [epoch: 8.83 sec]
EPOCH 8/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.7474015574949213		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3965006536827542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5719511055888375 | validation: 2.0386189716015295]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.6481384961973133		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3258778109409226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.487008153569118 | validation: 1.4539814611202257]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_9.pth
	Model improved!!!
EPOCH 10/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.1144656879002555		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.602759310234607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3586124990674313 | validation: 4.622257881317221]
	TIME [epoch: 8.85 sec]
EPOCH 11/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.5891964170851094		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.255620424704349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.422408420894729 | validation: 2.435678071714194]
	TIME [epoch: 8.84 sec]
EPOCH 12/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.010360654241086		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8142456176325124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9123031359367992 | validation: 3.0604153904469844]
	TIME [epoch: 8.83 sec]
EPOCH 13/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.4099456108123487		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3970899472998974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4035177790561226 | validation: 2.9167510799653424]
	TIME [epoch: 8.83 sec]
EPOCH 14/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.2781937919859505		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1497245435799166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2139591677829333 | validation: 1.682081881219134]
	TIME [epoch: 8.82 sec]
EPOCH 15/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7682575166822665		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7688035131950373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7685305149386519 | validation: 1.4800664495943643]
	TIME [epoch: 8.84 sec]
EPOCH 16/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7895562618883194		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6527415440988258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7211489029935727 | validation: 1.9974080560021994]
	TIME [epoch: 8.84 sec]
EPOCH 17/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6019270865890167		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7970353666448324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6994812266169244 | validation: 1.6009525869147732]
	TIME [epoch: 8.83 sec]
EPOCH 18/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6837546023515064		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.680676114582773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6822153584671398 | validation: 1.6081731916953634]
	TIME [epoch: 8.83 sec]
EPOCH 19/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5645625582560703		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6444768927909377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6045197255235037 | validation: 1.6364238737434755]
	TIME [epoch: 8.83 sec]
EPOCH 20/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5137227280623786		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4515852602480268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.482653994155203 | validation: 1.8071659073690054]
	TIME [epoch: 8.84 sec]
EPOCH 21/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4815759576743546		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4946261010751742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4881010293747645 | validation: 1.4170815712508509]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_21.pth
	Model improved!!!
EPOCH 22/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.466053529175468		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4124846698977276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4392690995365975 | validation: 1.7773113985916695]
	TIME [epoch: 8.82 sec]
EPOCH 23/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4606621422751493		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5838557948750547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5222589685751018 | validation: 1.4264278028131205]
	TIME [epoch: 8.82 sec]
EPOCH 24/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4062132160975718		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6185395334578725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5123763747777221 | validation: 1.0825111373392906]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_24.pth
	Model improved!!!
EPOCH 25/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5228575329347795		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4232245078468586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4730410203908193 | validation: 1.2630035595314444]
	TIME [epoch: 8.83 sec]
EPOCH 26/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.359751613535582		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5003853098749176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.43006846170525 | validation: 1.668115259653502]
	TIME [epoch: 8.84 sec]
EPOCH 27/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4002486620252967		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4370871797132128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.418667920869255 | validation: 1.8893928637882291]
	TIME [epoch: 8.81 sec]
EPOCH 28/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8118409205571282		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5942697302227422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.703055325389935 | validation: 2.1239784990326647]
	TIME [epoch: 8.81 sec]
EPOCH 29/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.573011071590587		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4277277741006675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5003694228456272 | validation: 1.4072216660528536]
	TIME [epoch: 8.81 sec]
EPOCH 30/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4632150530732897		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4354519084626878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4493334807679883 | validation: 1.4795019634339763]
	TIME [epoch: 8.82 sec]
EPOCH 31/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.459156693311224		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5363282037038022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4977424485075133 | validation: 1.385413823697926]
	TIME [epoch: 8.85 sec]
EPOCH 32/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2932082812898769		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3697155067538354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.331461894021856 | validation: 1.3355331333140823]
	TIME [epoch: 8.82 sec]
EPOCH 33/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6002019194171744		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3446765004079462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4724392099125603 | validation: 1.0581909597790728]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_33.pth
	Model improved!!!
EPOCH 34/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3478370420336787		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4706942928619193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.409265667447799 | validation: 1.8250722917903226]
	TIME [epoch: 8.82 sec]
EPOCH 35/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.438972000716356		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.392323457659632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4156477291879939 | validation: 1.0920633246012712]
	TIME [epoch: 8.81 sec]
EPOCH 36/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3503693743564047		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3901368537714345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3702531140639198 | validation: 1.2017040917302593]
	TIME [epoch: 8.84 sec]
EPOCH 37/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.403393264789691		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4078575585654431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4056254116775668 | validation: 1.6740074935358762]
	TIME [epoch: 8.83 sec]
EPOCH 38/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3625072540112533		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.037309985889429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6999086199503413 | validation: 2.7061909202888437]
	TIME [epoch: 8.82 sec]
EPOCH 39/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.927725870562797		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9628318052614542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9452788379121262 | validation: 2.203167891657461]
	TIME [epoch: 8.82 sec]
EPOCH 40/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6537797486341828		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2414345226325525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4476071356333677 | validation: 1.4446629169145866]
	TIME [epoch: 8.82 sec]
EPOCH 41/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.39792496337023		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.305582958959604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3517539611649165 | validation: 1.0586240271219225]
	TIME [epoch: 8.85 sec]
EPOCH 42/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2088430696232595		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5033674031730837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3561052363981712 | validation: 1.4095529174617443]
	TIME [epoch: 8.82 sec]
EPOCH 43/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2914602991300381		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.300875281216318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.296167790173178 | validation: 1.5292679202444137]
	TIME [epoch: 8.81 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2571420865424252		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3406561854846664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2988991360135458 | validation: 1.1563949064646946]
	TIME [epoch: 8.82 sec]
EPOCH 45/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3445506683152533		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3529378555536336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3487442619344439 | validation: 1.273163635818205]
	TIME [epoch: 8.81 sec]
EPOCH 46/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1833984657909087		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4111903187113124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2972943922511109 | validation: 1.0332931539875156]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_46.pth
	Model improved!!!
EPOCH 47/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3003476809674421		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3420093218591236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3211785014132829 | validation: 1.243615259371961]
	TIME [epoch: 8.82 sec]
EPOCH 48/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3258044926494663		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3404663097253764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3331354011874212 | validation: 1.393429670821264]
	TIME [epoch: 8.81 sec]
EPOCH 49/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3011566668712693		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2205214307278527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.260839048799561 | validation: 1.0536786004536596]
	TIME [epoch: 8.82 sec]
EPOCH 50/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1379347599930132		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2081993181278488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.173067039060431 | validation: 0.9837328128300532]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_50.pth
	Model improved!!!
EPOCH 51/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2599188612616588		[learning rate: 0.0099782]
		[batch 20/20] avg loss: 1.2437492682892417		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 1.2518340647754502 | validation: 1.9156712591917218]
	TIME [epoch: 8.84 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3988488120545446		[learning rate: 0.00993]
		[batch 20/20] avg loss: 1.5746763721615893		[learning rate: 0.0099059]
	Learning Rate: 0.00990592
	LOSS [training: 1.4867625921080667 | validation: 2.316154631963858]
	TIME [epoch: 8.84 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.582741284700004		[learning rate: 0.0098819]
		[batch 20/20] avg loss: 1.3297454030036895		[learning rate: 0.009858]
	Learning Rate: 0.00985801
	LOSS [training: 1.4562433438518465 | validation: 1.3534490609751773]
	TIME [epoch: 8.82 sec]
EPOCH 54/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2678579839425026		[learning rate: 0.0098341]
		[batch 20/20] avg loss: 1.190554514215624		[learning rate: 0.0098103]
	Learning Rate: 0.00981034
	LOSS [training: 1.2292062490790634 | validation: 1.1819619282281255]
	TIME [epoch: 8.82 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1905760651028072		[learning rate: 0.0097866]
		[batch 20/20] avg loss: 1.2953740985813371		[learning rate: 0.0097629]
	Learning Rate: 0.0097629
	LOSS [training: 1.242975081842072 | validation: 1.1759419223365115]
	TIME [epoch: 8.81 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.223763916285346		[learning rate: 0.0097393]
		[batch 20/20] avg loss: 1.2403888749675622		[learning rate: 0.0097157]
	Learning Rate: 0.00971569
	LOSS [training: 1.232076395626454 | validation: 0.9867778505428193]
	TIME [epoch: 8.83 sec]
EPOCH 57/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.243297711955748		[learning rate: 0.0096922]
		[batch 20/20] avg loss: 1.2253010023520465		[learning rate: 0.0096687]
	Learning Rate: 0.00966871
	LOSS [training: 1.2342993571538972 | validation: 0.948689483172581]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_57.pth
	Model improved!!!
EPOCH 58/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.524591834930132		[learning rate: 0.0096453]
		[batch 20/20] avg loss: 1.3037345739946589		[learning rate: 0.009622]
	Learning Rate: 0.00962195
	LOSS [training: 1.4141632044623957 | validation: 1.509073781343688]
	TIME [epoch: 8.82 sec]
EPOCH 59/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2092514435281745		[learning rate: 0.0095987]
		[batch 20/20] avg loss: 1.2851992650273822		[learning rate: 0.0095754]
	Learning Rate: 0.00957542
	LOSS [training: 1.2472253542777785 | validation: 1.631814436201306]
	TIME [epoch: 8.82 sec]
EPOCH 60/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1974583531277		[learning rate: 0.0095522]
		[batch 20/20] avg loss: 1.2052619167744139		[learning rate: 0.0095291]
	Learning Rate: 0.00952912
	LOSS [training: 1.201360134951057 | validation: 1.1925853284602752]
	TIME [epoch: 8.82 sec]
EPOCH 61/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1324119853130141		[learning rate: 0.009506]
		[batch 20/20] avg loss: 1.1585242811421133		[learning rate: 0.009483]
	Learning Rate: 0.00948303
	LOSS [training: 1.1454681332275638 | validation: 1.1720355439215635]
	TIME [epoch: 8.82 sec]
EPOCH 62/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1907854497062746		[learning rate: 0.0094601]
		[batch 20/20] avg loss: 1.1466862996602079		[learning rate: 0.0094372]
	Learning Rate: 0.00943718
	LOSS [training: 1.1687358746832412 | validation: 1.2846575328335617]
	TIME [epoch: 8.84 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2003345518950475		[learning rate: 0.0094143]
		[batch 20/20] avg loss: 1.217969693644542		[learning rate: 0.0093915]
	Learning Rate: 0.00939154
	LOSS [training: 1.2091521227697948 | validation: 2.064258406713471]
	TIME [epoch: 8.82 sec]
EPOCH 64/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2425955872962529		[learning rate: 0.0093688]
		[batch 20/20] avg loss: 1.3102511519298035		[learning rate: 0.0093461]
	Learning Rate: 0.00934612
	LOSS [training: 1.276423369613028 | validation: 1.0686712755210908]
	TIME [epoch: 8.83 sec]
EPOCH 65/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1632691759542348		[learning rate: 0.0093235]
		[batch 20/20] avg loss: 1.1730324232019014		[learning rate: 0.0093009]
	Learning Rate: 0.00930093
	LOSS [training: 1.1681507995780684 | validation: 0.9011906807717301]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_65.pth
	Model improved!!!
EPOCH 66/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2201311388277674		[learning rate: 0.0092784]
		[batch 20/20] avg loss: 1.0746529499119144		[learning rate: 0.009256]
	Learning Rate: 0.00925595
	LOSS [training: 1.1473920443698407 | validation: 1.0691040238161045]
	TIME [epoch: 8.83 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1653482330622582		[learning rate: 0.0092335]
		[batch 20/20] avg loss: 1.091349591862033		[learning rate: 0.0092112]
	Learning Rate: 0.00921119
	LOSS [training: 1.1283489124621457 | validation: 0.9443059289951732]
	TIME [epoch: 8.84 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0647214402295122		[learning rate: 0.0091889]
		[batch 20/20] avg loss: 1.3398964379343568		[learning rate: 0.0091666]
	Learning Rate: 0.00916665
	LOSS [training: 1.2023089390819346 | validation: 1.492825083903875]
	TIME [epoch: 8.82 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1329804204157818		[learning rate: 0.0091445]
		[batch 20/20] avg loss: 1.121651282113858		[learning rate: 0.0091223]
	Learning Rate: 0.00912232
	LOSS [training: 1.12731585126482 | validation: 1.011023765076305]
	TIME [epoch: 8.82 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0746455268411155		[learning rate: 0.0091002]
		[batch 20/20] avg loss: 1.1261542888995955		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 1.1003999078703555 | validation: 0.9353591957507819]
	TIME [epoch: 8.82 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.063135393929163		[learning rate: 0.0090562]
		[batch 20/20] avg loss: 1.170395274558695		[learning rate: 0.0090343]
	Learning Rate: 0.0090343
	LOSS [training: 1.116765334243929 | validation: 1.0324776607936932]
	TIME [epoch: 8.82 sec]
EPOCH 72/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2425126710551073		[learning rate: 0.0090124]
		[batch 20/20] avg loss: 1.132596998238766		[learning rate: 0.0089906]
	Learning Rate: 0.00899062
	LOSS [training: 1.1875548346469367 | validation: 1.1616609631331882]
	TIME [epoch: 8.84 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.350023225216725		[learning rate: 0.0089689]
		[batch 20/20] avg loss: 1.1388880037382132		[learning rate: 0.0089471]
	Learning Rate: 0.00894714
	LOSS [training: 1.2444556144774692 | validation: 0.9963804876001398]
	TIME [epoch: 8.82 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1305661694887121		[learning rate: 0.0089255]
		[batch 20/20] avg loss: 1.1487362501718974		[learning rate: 0.0089039]
	Learning Rate: 0.00890387
	LOSS [training: 1.1396512098303049 | validation: 1.945208653351779]
	TIME [epoch: 8.82 sec]
EPOCH 75/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4155136460890927		[learning rate: 0.0088823]
		[batch 20/20] avg loss: 1.0915252485082418		[learning rate: 0.0088608]
	Learning Rate: 0.00886082
	LOSS [training: 1.2535194472986673 | validation: 0.9760026504700978]
	TIME [epoch: 8.82 sec]
EPOCH 76/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2681343906604152		[learning rate: 0.0088394]
		[batch 20/20] avg loss: 1.2051280232269992		[learning rate: 0.008818]
	Learning Rate: 0.00881797
	LOSS [training: 1.236631206943707 | validation: 1.0968899905979665]
	TIME [epoch: 8.82 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0707949834951438		[learning rate: 0.0087966]
		[batch 20/20] avg loss: 1.124517515193516		[learning rate: 0.0087753]
	Learning Rate: 0.00877532
	LOSS [training: 1.0976562493443298 | validation: 1.09884591461896]
	TIME [epoch: 8.85 sec]
EPOCH 78/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.061509805975432		[learning rate: 0.0087541]
		[batch 20/20] avg loss: 1.0937941088457763		[learning rate: 0.0087329]
	Learning Rate: 0.00873289
	LOSS [training: 1.077651957410604 | validation: 1.9020055362811534]
	TIME [epoch: 8.83 sec]
EPOCH 79/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4795341917286344		[learning rate: 0.0087117]
		[batch 20/20] avg loss: 1.3690368150131056		[learning rate: 0.0086907]
	Learning Rate: 0.00869066
	LOSS [training: 1.4242855033708701 | validation: 1.3417951335910732]
	TIME [epoch: 9.13 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.249979821930302		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 1.0605822828592915		[learning rate: 0.0086486]
	Learning Rate: 0.00864863
	LOSS [training: 1.1552810523947967 | validation: 1.0184223349065604]
	TIME [epoch: 8.84 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0742574729306946		[learning rate: 0.0086277]
		[batch 20/20] avg loss: 1.3017757881354757		[learning rate: 0.0086068]
	Learning Rate: 0.00860681
	LOSS [training: 1.1880166305330853 | validation: 0.8112054811903717]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_81.pth
	Model improved!!!
EPOCH 82/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1659127790053274		[learning rate: 0.008586]
		[batch 20/20] avg loss: 1.089116208129558		[learning rate: 0.0085652]
	Learning Rate: 0.00856519
	LOSS [training: 1.1275144935674426 | validation: 1.0136445554255138]
	TIME [epoch: 8.86 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0480665841830983		[learning rate: 0.0085445]
		[batch 20/20] avg loss: 1.4450304728255248		[learning rate: 0.0085238]
	Learning Rate: 0.00852377
	LOSS [training: 1.2465485285043116 | validation: 1.9317016202307855]
	TIME [epoch: 8.85 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3023084928873299		[learning rate: 0.0085031]
		[batch 20/20] avg loss: 1.0543952403910652		[learning rate: 0.0084825]
	Learning Rate: 0.00848255
	LOSS [training: 1.1783518666391974 | validation: 0.8231180676641837]
	TIME [epoch: 8.87 sec]
EPOCH 85/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0133544886320682		[learning rate: 0.008462]
		[batch 20/20] avg loss: 1.1992408087074151		[learning rate: 0.0084415]
	Learning Rate: 0.00844153
	LOSS [training: 1.1062976486697418 | validation: 1.0618629072864052]
	TIME [epoch: 8.84 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0972057265299247		[learning rate: 0.0084211]
		[batch 20/20] avg loss: 1.2823813520562353		[learning rate: 0.0084007]
	Learning Rate: 0.00840071
	LOSS [training: 1.1897935392930798 | validation: 1.055516269917049]
	TIME [epoch: 8.84 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1928129743566314		[learning rate: 0.0083804]
		[batch 20/20] avg loss: 1.1530113454868716		[learning rate: 0.0083601]
	Learning Rate: 0.00836008
	LOSS [training: 1.1729121599217514 | validation: 0.8753875324663101]
	TIME [epoch: 8.86 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0320462949846967		[learning rate: 0.0083398]
		[batch 20/20] avg loss: 1.111319695842279		[learning rate: 0.0083197]
	Learning Rate: 0.00831965
	LOSS [training: 1.071682995413488 | validation: 1.072486217141365]
	TIME [epoch: 8.83 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.033898339729595		[learning rate: 0.0082995]
		[batch 20/20] avg loss: 1.0669317012137967		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 1.050415020471696 | validation: 1.0302940880624205]
	TIME [epoch: 8.83 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9927992008519146		[learning rate: 0.0082594]
		[batch 20/20] avg loss: 1.008972897144457		[learning rate: 0.0082394]
	Learning Rate: 0.00823938
	LOSS [training: 1.0008860489981857 | validation: 0.9823897311246178]
	TIME [epoch: 8.84 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.028578637974249		[learning rate: 0.0082194]
		[batch 20/20] avg loss: 1.0628718226232292		[learning rate: 0.0081995]
	Learning Rate: 0.00819954
	LOSS [training: 1.0457252302987394 | validation: 0.9250363725527825]
	TIME [epoch: 8.84 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.060041679707048		[learning rate: 0.0081797]
		[batch 20/20] avg loss: 0.9894760545765957		[learning rate: 0.0081599]
	Learning Rate: 0.00815989
	LOSS [training: 1.024758867141822 | validation: 1.2171957283803232]
	TIME [epoch: 8.87 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5339184002342912		[learning rate: 0.0081401]
		[batch 20/20] avg loss: 1.047233277086265		[learning rate: 0.0081204]
	Learning Rate: 0.00812043
	LOSS [training: 1.2905758386602781 | validation: 0.8423524333823753]
	TIME [epoch: 8.85 sec]
EPOCH 94/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4493858585480495		[learning rate: 0.0081008]
		[batch 20/20] avg loss: 1.281631482282211		[learning rate: 0.0080812]
	Learning Rate: 0.00808116
	LOSS [training: 1.3655086704151302 | validation: 1.2589240887461732]
	TIME [epoch: 8.83 sec]
EPOCH 95/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0636873469604358		[learning rate: 0.0080616]
		[batch 20/20] avg loss: 1.0433730825812306		[learning rate: 0.0080421]
	Learning Rate: 0.00804208
	LOSS [training: 1.0535302147708332 | validation: 0.8366189493353301]
	TIME [epoch: 8.84 sec]
EPOCH 96/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0896945577376864		[learning rate: 0.0080226]
		[batch 20/20] avg loss: 1.3395572825052133		[learning rate: 0.0080032]
	Learning Rate: 0.00800319
	LOSS [training: 1.2146259201214502 | validation: 0.8440503347891293]
	TIME [epoch: 8.83 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3663831835365		[learning rate: 0.0079838]
		[batch 20/20] avg loss: 1.2208935619742964		[learning rate: 0.0079645]
	Learning Rate: 0.00796449
	LOSS [training: 1.2936383727553982 | validation: 0.9520735305894961]
	TIME [epoch: 8.86 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3853314504944538		[learning rate: 0.0079452]
		[batch 20/20] avg loss: 1.1412537752940037		[learning rate: 0.007926]
	Learning Rate: 0.00792597
	LOSS [training: 1.2632926128942288 | validation: 1.495128235105438]
	TIME [epoch: 8.84 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0552988164369128		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 0.9988420791343909		[learning rate: 0.0078876]
	Learning Rate: 0.00788765
	LOSS [training: 1.0270704477856518 | validation: 1.284462280258981]
	TIME [epoch: 8.83 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.260090552233674		[learning rate: 0.0078685]
		[batch 20/20] avg loss: 1.2813828835292829		[learning rate: 0.0078495]
	Learning Rate: 0.0078495
	LOSS [training: 1.270736717881478 | validation: 1.0164956927440565]
	TIME [epoch: 8.84 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9964218831774614		[learning rate: 0.0078305]
		[batch 20/20] avg loss: 1.0405408551517954		[learning rate: 0.0078115]
	Learning Rate: 0.00781154
	LOSS [training: 1.0184813691646286 | validation: 1.484020680719087]
	TIME [epoch: 8.83 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0781274602187445		[learning rate: 0.0077926]
		[batch 20/20] avg loss: 0.9834065951063717		[learning rate: 0.0077738]
	Learning Rate: 0.00777377
	LOSS [training: 1.030767027662558 | validation: 0.8907569814188492]
	TIME [epoch: 8.84 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.159174897423856		[learning rate: 0.0077549]
		[batch 20/20] avg loss: 1.1376054277175363		[learning rate: 0.0077362]
	Learning Rate: 0.00773618
	LOSS [training: 1.148390162570696 | validation: 2.001655355009025]
	TIME [epoch: 8.84 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4430213029713688		[learning rate: 0.0077174]
		[batch 20/20] avg loss: 1.1124074068070073		[learning rate: 0.0076988]
	Learning Rate: 0.00769876
	LOSS [training: 1.277714354889188 | validation: 2.0489253253264716]
	TIME [epoch: 8.83 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3913945136977584		[learning rate: 0.0076801]
		[batch 20/20] avg loss: 1.3112323396637149		[learning rate: 0.0076615]
	Learning Rate: 0.00766153
	LOSS [training: 1.351313426680736 | validation: 1.0843772349841212]
	TIME [epoch: 8.84 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9549366779129185		[learning rate: 0.007643]
		[batch 20/20] avg loss: 0.9493820291125242		[learning rate: 0.0076245]
	Learning Rate: 0.00762449
	LOSS [training: 0.9521593535127213 | validation: 1.1953518951705095]
	TIME [epoch: 8.83 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0363966298478808		[learning rate: 0.007606]
		[batch 20/20] avg loss: 1.2363976105349206		[learning rate: 0.0075876]
	Learning Rate: 0.00758761
	LOSS [training: 1.1363971201914005 | validation: 1.1449775263781374]
	TIME [epoch: 8.85 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.013490762541924		[learning rate: 0.0075692]
		[batch 20/20] avg loss: 1.2510782929638355		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 1.1322845277528801 | validation: 0.9818564143565636]
	TIME [epoch: 8.85 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2250756242526017		[learning rate: 0.0075326]
		[batch 20/20] avg loss: 1.1688114711933537		[learning rate: 0.0075144]
	Learning Rate: 0.00751441
	LOSS [training: 1.1969435477229777 | validation: 0.8856089159382917]
	TIME [epoch: 8.82 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0764611643844748		[learning rate: 0.0074962]
		[batch 20/20] avg loss: 0.976729828361821		[learning rate: 0.0074781]
	Learning Rate: 0.00747807
	LOSS [training: 1.0265954963731478 | validation: 0.9780109879111001]
	TIME [epoch: 8.83 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0476913424855394		[learning rate: 0.00746]
		[batch 20/20] avg loss: 1.0184000309401064		[learning rate: 0.0074419]
	Learning Rate: 0.00744191
	LOSS [training: 1.0330456867128228 | validation: 1.5046342315654628]
	TIME [epoch: 8.83 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2237982934182678		[learning rate: 0.0074239]
		[batch 20/20] avg loss: 0.9996645316028963		[learning rate: 0.0074059]
	Learning Rate: 0.00740592
	LOSS [training: 1.1117314125105822 | validation: 1.208407990320834]
	TIME [epoch: 8.84 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9811478348503236		[learning rate: 0.007388]
		[batch 20/20] avg loss: 0.9354988507558651		[learning rate: 0.0073701]
	Learning Rate: 0.00737011
	LOSS [training: 0.9583233428030946 | validation: 0.8942456883774242]
	TIME [epoch: 8.84 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0811874334760918		[learning rate: 0.0073523]
		[batch 20/20] avg loss: 0.9313001113846477		[learning rate: 0.0073345]
	Learning Rate: 0.00733446
	LOSS [training: 1.0062437724303697 | validation: 0.8249847282482652]
	TIME [epoch: 8.83 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9459971251521692		[learning rate: 0.0073167]
		[batch 20/20] avg loss: 0.9952948006408601		[learning rate: 0.007299]
	Learning Rate: 0.007299
	LOSS [training: 0.970645962896515 | validation: 0.8662490489366109]
	TIME [epoch: 8.82 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0733225894924083		[learning rate: 0.0072813]
		[batch 20/20] avg loss: 1.3727111369918608		[learning rate: 0.0072637]
	Learning Rate: 0.0072637
	LOSS [training: 1.2230168632421345 | validation: 1.6870839312910952]
	TIME [epoch: 8.82 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.342082198555627		[learning rate: 0.0072461]
		[batch 20/20] avg loss: 1.3003046566589236		[learning rate: 0.0072286]
	Learning Rate: 0.00722857
	LOSS [training: 1.3211934276072752 | validation: 1.4370825217244323]
	TIME [epoch: 8.83 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0192103123885725		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 0.8683638403675676		[learning rate: 0.0071936]
	Learning Rate: 0.00719362
	LOSS [training: 0.9437870763780701 | validation: 1.0799728970702718]
	TIME [epoch: 8.86 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8939653117894842		[learning rate: 0.0071762]
		[batch 20/20] avg loss: 0.9268585903526685		[learning rate: 0.0071588]
	Learning Rate: 0.00715883
	LOSS [training: 0.9104119510710763 | validation: 1.2713711170310962]
	TIME [epoch: 8.83 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.24238498165924		[learning rate: 0.0071415]
		[batch 20/20] avg loss: 0.9571670774738743		[learning rate: 0.0071242]
	Learning Rate: 0.00712421
	LOSS [training: 1.0997760295665573 | validation: 0.9645984686332092]
	TIME [epoch: 8.83 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8777720028217034		[learning rate: 0.007107]
		[batch 20/20] avg loss: 1.1160551850450586		[learning rate: 0.0070898]
	Learning Rate: 0.00708976
	LOSS [training: 0.9969135939333811 | validation: 1.3643757420025882]
	TIME [epoch: 8.82 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3805921272796173		[learning rate: 0.0070726]
		[batch 20/20] avg loss: 1.2495339616878138		[learning rate: 0.0070555]
	Learning Rate: 0.00705548
	LOSS [training: 1.3150630444837152 | validation: 0.7541938446156597]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_122.pth
	Model improved!!!
EPOCH 123/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9615426306035694		[learning rate: 0.0070384]
		[batch 20/20] avg loss: 1.013410895633561		[learning rate: 0.0070214]
	Learning Rate: 0.00702136
	LOSS [training: 0.9874767631185654 | validation: 1.2298212913958069]
	TIME [epoch: 8.86 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9618764727589829		[learning rate: 0.0070044]
		[batch 20/20] avg loss: 1.0528123944170171		[learning rate: 0.0069874]
	Learning Rate: 0.0069874
	LOSS [training: 1.007344433588 | validation: 0.7764502007891614]
	TIME [epoch: 8.83 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9945596775853767		[learning rate: 0.0069705]
		[batch 20/20] avg loss: 1.0607774556750473		[learning rate: 0.0069536]
	Learning Rate: 0.00695361
	LOSS [training: 1.0276685666302123 | validation: 0.8767588202408293]
	TIME [epoch: 8.83 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8999182692721746		[learning rate: 0.0069368]
		[batch 20/20] avg loss: 0.9073834167196966		[learning rate: 0.00692]
	Learning Rate: 0.00691999
	LOSS [training: 0.9036508429959355 | validation: 0.7512055843933743]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_126.pth
	Model improved!!!
EPOCH 127/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9265980845795427		[learning rate: 0.0069032]
		[batch 20/20] avg loss: 1.1911352139732485		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 1.0588666492763956 | validation: 1.1012574604645118]
	TIME [epoch: 8.84 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.94690492996988		[learning rate: 0.0068699]
		[batch 20/20] avg loss: 1.1014221993744147		[learning rate: 0.0068532]
	Learning Rate: 0.00685322
	LOSS [training: 1.0241635646721474 | validation: 0.7495116627280594]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_128.pth
	Model improved!!!
EPOCH 129/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8738622501967883		[learning rate: 0.0068366]
		[batch 20/20] avg loss: 0.8784408501069608		[learning rate: 0.0068201]
	Learning Rate: 0.00682008
	LOSS [training: 0.8761515501518747 | validation: 0.9631081691848828]
	TIME [epoch: 8.84 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8927768959727642		[learning rate: 0.0068036]
		[batch 20/20] avg loss: 0.8652899279231241		[learning rate: 0.0067871]
	Learning Rate: 0.0067871
	LOSS [training: 0.8790334119479442 | validation: 0.8232039122076359]
	TIME [epoch: 8.83 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8682443871613641		[learning rate: 0.0067707]
		[batch 20/20] avg loss: 0.8801957517979913		[learning rate: 0.0067543]
	Learning Rate: 0.00675428
	LOSS [training: 0.8742200694796776 | validation: 0.7685071159034371]
	TIME [epoch: 8.84 sec]
EPOCH 132/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8509648647173179		[learning rate: 0.0067379]
		[batch 20/20] avg loss: 1.1154257149808116		[learning rate: 0.0067216]
	Learning Rate: 0.00672162
	LOSS [training: 0.9831952898490645 | validation: 1.3369183896551262]
	TIME [epoch: 8.84 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9134844247408583		[learning rate: 0.0067053]
		[batch 20/20] avg loss: 0.8513796890261066		[learning rate: 0.0066891]
	Learning Rate: 0.00668911
	LOSS [training: 0.8824320568834827 | validation: 0.8736618450019962]
	TIME [epoch: 8.86 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8835952037071019		[learning rate: 0.0066729]
		[batch 20/20] avg loss: 1.2117624457712552		[learning rate: 0.0066568]
	Learning Rate: 0.00665676
	LOSS [training: 1.0476788247391784 | validation: 0.7361747680600138]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_134.pth
	Model improved!!!
EPOCH 135/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1896460170860848		[learning rate: 0.0066406]
		[batch 20/20] avg loss: 1.2151623437885222		[learning rate: 0.0066246]
	Learning Rate: 0.00662457
	LOSS [training: 1.2024041804373036 | validation: 0.8383649310502194]
	TIME [epoch: 8.84 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0237952924885616		[learning rate: 0.0066085]
		[batch 20/20] avg loss: 0.8935295857269183		[learning rate: 0.0065925]
	Learning Rate: 0.00659254
	LOSS [training: 0.9586624391077398 | validation: 1.9612650789779709]
	TIME [epoch: 8.83 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1198331576796396		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 0.8803003697364087		[learning rate: 0.0065607]
	Learning Rate: 0.00656066
	LOSS [training: 1.0000667637080245 | validation: 0.8173246929371984]
	TIME [epoch: 8.83 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8528709629130026		[learning rate: 0.0065448]
		[batch 20/20] avg loss: 0.8424951643691795		[learning rate: 0.0065289]
	Learning Rate: 0.00652893
	LOSS [training: 0.8476830636410909 | validation: 0.6706427453453601]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_138.pth
	Model improved!!!
EPOCH 139/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.853118131394836		[learning rate: 0.0065131]
		[batch 20/20] avg loss: 0.8434099961191068		[learning rate: 0.0064974]
	Learning Rate: 0.00649736
	LOSS [training: 0.8482640637569716 | validation: 0.7498460514049967]
	TIME [epoch: 8.83 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7985029494703666		[learning rate: 0.0064816]
		[batch 20/20] avg loss: 0.7852742267093554		[learning rate: 0.0064659]
	Learning Rate: 0.00646594
	LOSS [training: 0.7918885880898612 | validation: 0.7539787909783189]
	TIME [epoch: 8.82 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7644921076456181		[learning rate: 0.0064503]
		[batch 20/20] avg loss: 0.8300733209240411		[learning rate: 0.0064347]
	Learning Rate: 0.00643467
	LOSS [training: 0.7972827142848297 | validation: 0.9027379145076831]
	TIME [epoch: 8.81 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9161172640905187		[learning rate: 0.0064191]
		[batch 20/20] avg loss: 0.8015291158519104		[learning rate: 0.0064036]
	Learning Rate: 0.00640355
	LOSS [training: 0.8588231899712147 | validation: 0.9357500022452642]
	TIME [epoch: 8.82 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8610316880012334		[learning rate: 0.0063881]
		[batch 20/20] avg loss: 1.2514260757764653		[learning rate: 0.0063726]
	Learning Rate: 0.00637259
	LOSS [training: 1.0562288818888494 | validation: 1.1391273592340638]
	TIME [epoch: 8.82 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.309214425770487		[learning rate: 0.0063572]
		[batch 20/20] avg loss: 1.2348691767682785		[learning rate: 0.0063418]
	Learning Rate: 0.00634177
	LOSS [training: 1.2720418012693826 | validation: 0.5917134344975126]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_144.pth
	Model improved!!!
EPOCH 145/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1834126756649817		[learning rate: 0.0063264]
		[batch 20/20] avg loss: 0.8669901034282093		[learning rate: 0.0063111]
	Learning Rate: 0.0063111
	LOSS [training: 1.0252013895465955 | validation: 0.7999116761675353]
	TIME [epoch: 8.83 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8286180659895779		[learning rate: 0.0062958]
		[batch 20/20] avg loss: 0.7869244628040731		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 0.8077712643968255 | validation: 0.8744044467086199]
	TIME [epoch: 8.83 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0572483733841986		[learning rate: 0.0062654]
		[batch 20/20] avg loss: 1.121617952206623		[learning rate: 0.0062502]
	Learning Rate: 0.00625021
	LOSS [training: 1.0894331627954108 | validation: 0.6742355935040987]
	TIME [epoch: 8.83 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7695746068182435		[learning rate: 0.0062351]
		[batch 20/20] avg loss: 0.83764688709462		[learning rate: 0.00622]
	Learning Rate: 0.00621999
	LOSS [training: 0.803610746956432 | validation: 0.7831818141718849]
	TIME [epoch: 8.84 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8082779258473872		[learning rate: 0.0062049]
		[batch 20/20] avg loss: 0.956301523071336		[learning rate: 0.0061899]
	Learning Rate: 0.00618991
	LOSS [training: 0.8822897244593614 | validation: 1.221381431135226]
	TIME [epoch: 8.84 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9585087125632118		[learning rate: 0.0061749]
		[batch 20/20] avg loss: 0.8557066128286216		[learning rate: 0.00616]
	Learning Rate: 0.00615997
	LOSS [training: 0.9071076626959169 | validation: 1.1285953392797392]
	TIME [epoch: 8.82 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8563400853348428		[learning rate: 0.0061451]
		[batch 20/20] avg loss: 1.1694274503474422		[learning rate: 0.0061302]
	Learning Rate: 0.00613019
	LOSS [training: 1.0128837678411424 | validation: 1.7882075430576392]
	TIME [epoch: 8.82 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1930873996774483		[learning rate: 0.0061153]
		[batch 20/20] avg loss: 1.2495090073099315		[learning rate: 0.0061005]
	Learning Rate: 0.00610054
	LOSS [training: 1.22129820349369 | validation: 1.80551835551847]
	TIME [epoch: 8.82 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.302601287070756		[learning rate: 0.0060858]
		[batch 20/20] avg loss: 1.2370555217145132		[learning rate: 0.006071]
	Learning Rate: 0.00607104
	LOSS [training: 1.2698284043926347 | validation: 1.5186741523557938]
	TIME [epoch: 8.82 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2158782301347508		[learning rate: 0.0060563]
		[batch 20/20] avg loss: 1.2825950671669057		[learning rate: 0.0060417]
	Learning Rate: 0.00604168
	LOSS [training: 1.2492366486508282 | validation: 1.257814102814352]
	TIME [epoch: 8.84 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1610498535885558		[learning rate: 0.0060271]
		[batch 20/20] avg loss: 1.160956306628027		[learning rate: 0.0060125]
	Learning Rate: 0.00601247
	LOSS [training: 1.1610030801082916 | validation: 1.3199988272093406]
	TIME [epoch: 8.81 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1828926689026225		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 1.1705054836193955		[learning rate: 0.0059834]
	Learning Rate: 0.00598339
	LOSS [training: 1.1766990762610088 | validation: 1.5494889896127964]
	TIME [epoch: 8.82 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1302884464714391		[learning rate: 0.0059689]
		[batch 20/20] avg loss: 1.1172638251611224		[learning rate: 0.0059545]
	Learning Rate: 0.00595446
	LOSS [training: 1.123776135816281 | validation: 1.4553734066740258]
	TIME [epoch: 8.82 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1376498780869233		[learning rate: 0.00594]
		[batch 20/20] avg loss: 1.132831039602604		[learning rate: 0.0059257]
	Learning Rate: 0.00592566
	LOSS [training: 1.1352404588447635 | validation: 1.7087002018959594]
	TIME [epoch: 8.82 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.177699061399956		[learning rate: 0.0059113]
		[batch 20/20] avg loss: 1.1371051711114712		[learning rate: 0.005897]
	Learning Rate: 0.00589701
	LOSS [training: 1.1574021162557138 | validation: 1.6134594101973798]
	TIME [epoch: 8.87 sec]
EPOCH 160/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1984502764845477		[learning rate: 0.0058827]
		[batch 20/20] avg loss: 0.7584296541078147		[learning rate: 0.0058685]
	Learning Rate: 0.00586849
	LOSS [training: 0.9784399652961809 | validation: 0.7362979685569031]
	TIME [epoch: 8.83 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7388623394331418		[learning rate: 0.0058543]
		[batch 20/20] avg loss: 0.8933538288580987		[learning rate: 0.0058401]
	Learning Rate: 0.00584011
	LOSS [training: 0.8161080841456203 | validation: 1.5456707243716603]
	TIME [epoch: 8.83 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1668244353279762		[learning rate: 0.005826]
		[batch 20/20] avg loss: 1.1172386774083538		[learning rate: 0.0058119]
	Learning Rate: 0.00581187
	LOSS [training: 1.1420315563681656 | validation: 0.7635419801485583]
	TIME [epoch: 8.82 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.152562829696595		[learning rate: 0.0057978]
		[batch 20/20] avg loss: 1.1010609361706205		[learning rate: 0.0057838]
	Learning Rate: 0.00578376
	LOSS [training: 1.1268118829336076 | validation: 0.7000362452050485]
	TIME [epoch: 8.83 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1331598923855177		[learning rate: 0.0057698]
		[batch 20/20] avg loss: 1.0125002386609594		[learning rate: 0.0057558]
	Learning Rate: 0.00575579
	LOSS [training: 1.0728300655232386 | validation: 0.8368202550667093]
	TIME [epoch: 8.85 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.773329200048579		[learning rate: 0.0057419]
		[batch 20/20] avg loss: 0.749731849745564		[learning rate: 0.005728]
	Learning Rate: 0.00572796
	LOSS [training: 0.7615305248970714 | validation: 0.8771769963375189]
	TIME [epoch: 8.82 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.910063519416853		[learning rate: 0.0057141]
		[batch 20/20] avg loss: 1.2767139676203263		[learning rate: 0.0057003]
	Learning Rate: 0.00570026
	LOSS [training: 1.0933887435185898 | validation: 1.280234584408698]
	TIME [epoch: 8.83 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1146976296965518		[learning rate: 0.0056865]
		[batch 20/20] avg loss: 1.1609998157204182		[learning rate: 0.0056727]
	Learning Rate: 0.0056727
	LOSS [training: 1.137848722708485 | validation: 1.5215962049024017]
	TIME [epoch: 8.83 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1307750276146822		[learning rate: 0.005659]
		[batch 20/20] avg loss: 1.0991978716427195		[learning rate: 0.0056453]
	Learning Rate: 0.00564526
	LOSS [training: 1.114986449628701 | validation: 1.2675797046634933]
	TIME [epoch: 8.83 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8068731721043655		[learning rate: 0.0056316]
		[batch 20/20] avg loss: 1.0399648315392593		[learning rate: 0.005618]
	Learning Rate: 0.00561796
	LOSS [training: 0.9234190018218124 | validation: 0.5590241115262375]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_169.pth
	Model improved!!!
EPOCH 170/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2377586860696224		[learning rate: 0.0056044]
		[batch 20/20] avg loss: 1.1256020975050767		[learning rate: 0.0055908]
	Learning Rate: 0.0055908
	LOSS [training: 1.1816803917873493 | validation: 0.6876820447554763]
	TIME [epoch: 8.82 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1287821451334792		[learning rate: 0.0055773]
		[batch 20/20] avg loss: 1.0605083565977713		[learning rate: 0.0055638]
	Learning Rate: 0.00556376
	LOSS [training: 1.094645250865625 | validation: 0.6651954992613858]
	TIME [epoch: 8.82 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1387060951807002		[learning rate: 0.0055503]
		[batch 20/20] avg loss: 0.897100093506347		[learning rate: 0.0055369]
	Learning Rate: 0.00553685
	LOSS [training: 1.0179030943435234 | validation: 1.409595084369655]
	TIME [epoch: 8.83 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.207904076555527		[learning rate: 0.0055235]
		[batch 20/20] avg loss: 1.2333310385831948		[learning rate: 0.0055101]
	Learning Rate: 0.00551008
	LOSS [training: 1.2206175575693612 | validation: 1.3036473259693668]
	TIME [epoch: 8.83 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1059474498766857		[learning rate: 0.0054967]
		[batch 20/20] avg loss: 1.005554819439204		[learning rate: 0.0054834]
	Learning Rate: 0.00548343
	LOSS [training: 1.055751134657945 | validation: 1.140290109377948]
	TIME [epoch: 8.86 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9959502235878139		[learning rate: 0.0054702]
		[batch 20/20] avg loss: 0.7639357991347379		[learning rate: 0.0054569]
	Learning Rate: 0.00545692
	LOSS [training: 0.879943011361276 | validation: 0.8874217801015712]
	TIME [epoch: 8.82 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7630848678981363		[learning rate: 0.0054437]
		[batch 20/20] avg loss: 0.8145135657020874		[learning rate: 0.0054305]
	Learning Rate: 0.00543053
	LOSS [training: 0.7887992168001119 | validation: 0.552159850640249]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_176.pth
	Model improved!!!
EPOCH 177/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8248461050704327		[learning rate: 0.0054174]
		[batch 20/20] avg loss: 0.8635618289586244		[learning rate: 0.0054043]
	Learning Rate: 0.00540427
	LOSS [training: 0.8442039670145285 | validation: 0.5900799034575841]
	TIME [epoch: 8.82 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8417020647079706		[learning rate: 0.0053912]
		[batch 20/20] avg loss: 1.1577515874138826		[learning rate: 0.0053781]
	Learning Rate: 0.00537813
	LOSS [training: 0.9997268260609264 | validation: 1.1716138701551844]
	TIME [epoch: 8.82 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2855317369637185		[learning rate: 0.0053651]
		[batch 20/20] avg loss: 0.9300364077885126		[learning rate: 0.0053521]
	Learning Rate: 0.00535213
	LOSS [training: 1.1077840723761152 | validation: 1.1036761438573448]
	TIME [epoch: 8.84 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0406218355996237		[learning rate: 0.0053392]
		[batch 20/20] avg loss: 1.2395726593995648		[learning rate: 0.0053262]
	Learning Rate: 0.00532624
	LOSS [training: 1.1400972474995945 | validation: 0.5298825905063145]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_180.pth
	Model improved!!!
EPOCH 181/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8139659004332929		[learning rate: 0.0053133]
		[batch 20/20] avg loss: 0.7742907889949033		[learning rate: 0.0053005]
	Learning Rate: 0.00530049
	LOSS [training: 0.794128344714098 | validation: 0.802285295929823]
	TIME [epoch: 8.83 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8814094838241651		[learning rate: 0.0052877]
		[batch 20/20] avg loss: 0.8507595160314827		[learning rate: 0.0052749]
	Learning Rate: 0.00527485
	LOSS [training: 0.866084499927824 | validation: 0.5325115929382068]
	TIME [epoch: 8.82 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0810281593423545		[learning rate: 0.0052621]
		[batch 20/20] avg loss: 0.8005738519242003		[learning rate: 0.0052493]
	Learning Rate: 0.00524935
	LOSS [training: 0.9408010056332772 | validation: 1.016805898078056]
	TIME [epoch: 8.82 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8437196593820181		[learning rate: 0.0052366]
		[batch 20/20] avg loss: 0.8039434752163059		[learning rate: 0.005224]
	Learning Rate: 0.00522396
	LOSS [training: 0.823831567299162 | validation: 0.7320327071676296]
	TIME [epoch: 8.83 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1088594295888625		[learning rate: 0.0052113]
		[batch 20/20] avg loss: 1.2405089835016647		[learning rate: 0.0051987]
	Learning Rate: 0.0051987
	LOSS [training: 1.1746842065452636 | validation: 0.8187185203545689]
	TIME [epoch: 8.84 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9114290497679651		[learning rate: 0.0051861]
		[batch 20/20] avg loss: 0.8149807047244984		[learning rate: 0.0051736]
	Learning Rate: 0.00517356
	LOSS [training: 0.8632048772462317 | validation: 1.658018914514504]
	TIME [epoch: 8.83 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3146733310472598		[learning rate: 0.005161]
		[batch 20/20] avg loss: 1.0336876687583103		[learning rate: 0.0051485]
	Learning Rate: 0.00514854
	LOSS [training: 1.1741804999027852 | validation: 0.6162944771068913]
	TIME [epoch: 8.83 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1509112965268418		[learning rate: 0.0051361]
		[batch 20/20] avg loss: 0.7514027709798352		[learning rate: 0.0051236]
	Learning Rate: 0.00512364
	LOSS [training: 0.9511570337533384 | validation: 0.9681022489225946]
	TIME [epoch: 8.83 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8216780242287942		[learning rate: 0.0051112]
		[batch 20/20] avg loss: 0.718491450617505		[learning rate: 0.0050989]
	Learning Rate: 0.00509887
	LOSS [training: 0.7700847374231496 | validation: 1.497459859991785]
	TIME [epoch: 8.83 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4563995843338762		[learning rate: 0.0050865]
		[batch 20/20] avg loss: 0.9146901352758927		[learning rate: 0.0050742]
	Learning Rate: 0.00507421
	LOSS [training: 1.1855448598048846 | validation: 0.7465568624632662]
	TIME [epoch: 8.84 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7912877011549632		[learning rate: 0.0050619]
		[batch 20/20] avg loss: 0.8896170183241215		[learning rate: 0.0050497]
	Learning Rate: 0.00504967
	LOSS [training: 0.8404523597395421 | validation: 0.715814092922896]
	TIME [epoch: 8.82 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9042705934649433		[learning rate: 0.0050374]
		[batch 20/20] avg loss: 0.9962976171994955		[learning rate: 0.0050253]
	Learning Rate: 0.00502525
	LOSS [training: 0.9502841053322191 | validation: 0.5794009485269775]
	TIME [epoch: 8.82 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7272893351180175		[learning rate: 0.0050131]
		[batch 20/20] avg loss: 1.099418129618986		[learning rate: 0.005001]
	Learning Rate: 0.00500095
	LOSS [training: 0.9133537323685017 | validation: 0.8443431782598996]
	TIME [epoch: 8.83 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8794112532790926		[learning rate: 0.0049888]
		[batch 20/20] avg loss: 0.7733607436149594		[learning rate: 0.0049768]
	Learning Rate: 0.00497677
	LOSS [training: 0.8263859984470259 | validation: 0.6013439560178178]
	TIME [epoch: 8.83 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8506730667924952		[learning rate: 0.0049647]
		[batch 20/20] avg loss: 0.7920062489871722		[learning rate: 0.0049527]
	Learning Rate: 0.0049527
	LOSS [training: 0.8213396578898339 | validation: 0.9123762721603856]
	TIME [epoch: 8.85 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8321515508011638		[learning rate: 0.0049407]
		[batch 20/20] avg loss: 0.8664871225516212		[learning rate: 0.0049288]
	Learning Rate: 0.00492875
	LOSS [training: 0.8493193366763924 | validation: 0.8046999224191396]
	TIME [epoch: 8.83 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2356615859780538		[learning rate: 0.0049168]
		[batch 20/20] avg loss: 0.8574115879398247		[learning rate: 0.0049049]
	Learning Rate: 0.00490492
	LOSS [training: 1.0465365869589394 | validation: 0.6897382881808141]
	TIME [epoch: 8.82 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7801917488635034		[learning rate: 0.004893]
		[batch 20/20] avg loss: 0.8394340639814668		[learning rate: 0.0048812]
	Learning Rate: 0.0048812
	LOSS [training: 0.8098129064224852 | validation: 0.7659194361395955]
	TIME [epoch: 8.82 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9975749706469481		[learning rate: 0.0048694]
		[batch 20/20] avg loss: 0.8610781270092316		[learning rate: 0.0048576]
	Learning Rate: 0.00485759
	LOSS [training: 0.9293265488280898 | validation: 0.56063198891677]
	TIME [epoch: 8.83 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7898750829196775		[learning rate: 0.0048458]
		[batch 20/20] avg loss: 1.504501067866036		[learning rate: 0.0048341]
	Learning Rate: 0.0048341
	LOSS [training: 1.1471880753928567 | validation: 0.7198102433621117]
	TIME [epoch: 8.86 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7743272787136726		[learning rate: 0.0048224]
		[batch 20/20] avg loss: 0.7384765994621627		[learning rate: 0.0048107]
	Learning Rate: 0.00481072
	LOSS [training: 0.7564019390879176 | validation: 0.7526005535288883]
	TIME [epoch: 8.83 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7652897165109274		[learning rate: 0.0047991]
		[batch 20/20] avg loss: 1.3639377583831318		[learning rate: 0.0047875]
	Learning Rate: 0.00478746
	LOSS [training: 1.0646137374470297 | validation: 1.2192013708299299]
	TIME [epoch: 8.83 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8409608075365476		[learning rate: 0.0047759]
		[batch 20/20] avg loss: 0.7540806760264852		[learning rate: 0.0047643]
	Learning Rate: 0.00476431
	LOSS [training: 0.7975207417815164 | validation: 0.8416241023739606]
	TIME [epoch: 8.83 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7061312485411959		[learning rate: 0.0047528]
		[batch 20/20] avg loss: 0.7498845812975764		[learning rate: 0.0047413]
	Learning Rate: 0.00474127
	LOSS [training: 0.7280079149193861 | validation: 0.5156543335310276]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_204.pth
	Model improved!!!
EPOCH 205/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7310225607193944		[learning rate: 0.0047298]
		[batch 20/20] avg loss: 0.6659573531785844		[learning rate: 0.0047183]
	Learning Rate: 0.00471834
	LOSS [training: 0.6984899569489895 | validation: 1.0991702039047824]
	TIME [epoch: 8.85 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9025390880358991		[learning rate: 0.0047069]
		[batch 20/20] avg loss: 0.6854466182507398		[learning rate: 0.0046955]
	Learning Rate: 0.00469553
	LOSS [training: 0.7939928531433194 | validation: 0.6682487130534315]
	TIME [epoch: 8.82 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8109487025121107		[learning rate: 0.0046842]
		[batch 20/20] avg loss: 0.8826569149617702		[learning rate: 0.0046728]
	Learning Rate: 0.00467282
	LOSS [training: 0.8468028087369405 | validation: 0.9180247311791265]
	TIME [epoch: 8.82 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9538134416832886		[learning rate: 0.0046615]
		[batch 20/20] avg loss: 1.0481182279950554		[learning rate: 0.0046502]
	Learning Rate: 0.00465022
	LOSS [training: 1.0009658348391717 | validation: 0.6074247746096989]
	TIME [epoch: 8.82 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7468309548155083		[learning rate: 0.004639]
		[batch 20/20] avg loss: 1.050948144080077		[learning rate: 0.0046277]
	Learning Rate: 0.00462773
	LOSS [training: 0.8988895494477926 | validation: 1.1311801166646116]
	TIME [epoch: 8.81 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0772551889046045		[learning rate: 0.0046165]
		[batch 20/20] avg loss: 0.7708299484725525		[learning rate: 0.0046054]
	Learning Rate: 0.00460536
	LOSS [training: 0.9240425686885784 | validation: 0.7080322050745236]
	TIME [epoch: 8.85 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6631203589899923		[learning rate: 0.0045942]
		[batch 20/20] avg loss: 0.6968209596640209		[learning rate: 0.0045831]
	Learning Rate: 0.00458308
	LOSS [training: 0.6799706593270068 | validation: 0.6153200472628901]
	TIME [epoch: 8.81 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0270354197494667		[learning rate: 0.004572]
		[batch 20/20] avg loss: 0.6981860480200728		[learning rate: 0.0045609]
	Learning Rate: 0.00456092
	LOSS [training: 0.8626107338847697 | validation: 0.6528720657359602]
	TIME [epoch: 8.82 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.779546572716289		[learning rate: 0.0045499]
		[batch 20/20] avg loss: 0.8134379119526519		[learning rate: 0.0045389]
	Learning Rate: 0.00453887
	LOSS [training: 0.7964922423344704 | validation: 0.730310994178384]
	TIME [epoch: 8.82 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1826894683623834		[learning rate: 0.0045279]
		[batch 20/20] avg loss: 1.2386675459330452		[learning rate: 0.0045169]
	Learning Rate: 0.00451692
	LOSS [training: 1.2106785071477142 | validation: 0.9967771616922834]
	TIME [epoch: 8.82 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8691522398227496		[learning rate: 0.004506]
		[batch 20/20] avg loss: 1.0420158634466394		[learning rate: 0.0044951]
	Learning Rate: 0.00449507
	LOSS [training: 0.9555840516346944 | validation: 0.5831925249471189]
	TIME [epoch: 8.85 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7192638309466025		[learning rate: 0.0044842]
		[batch 20/20] avg loss: 0.9254903051271894		[learning rate: 0.0044733]
	Learning Rate: 0.00447334
	LOSS [training: 0.8223770680368959 | validation: 1.1477212421449874]
	TIME [epoch: 8.82 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9898061605231179		[learning rate: 0.0044625]
		[batch 20/20] avg loss: 0.7628919326883961		[learning rate: 0.0044517]
	Learning Rate: 0.0044517
	LOSS [training: 0.8763490466057569 | validation: 0.4298172061562604]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_217.pth
	Model improved!!!
EPOCH 218/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8938583169269355		[learning rate: 0.0044409]
		[batch 20/20] avg loss: 0.6440635684936186		[learning rate: 0.0044302]
	Learning Rate: 0.00443018
	LOSS [training: 0.768960942710277 | validation: 0.511253192797734]
	TIME [epoch: 8.82 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6922264071591023		[learning rate: 0.0044195]
		[batch 20/20] avg loss: 0.802943614292365		[learning rate: 0.0044088]
	Learning Rate: 0.00440875
	LOSS [training: 0.7475850107257336 | validation: 0.7886553386069157]
	TIME [epoch: 8.81 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9767941186798433		[learning rate: 0.0043981]
		[batch 20/20] avg loss: 0.7901810292731085		[learning rate: 0.0043874]
	Learning Rate: 0.00438743
	LOSS [training: 0.883487573976476 | validation: 0.5521087711106035]
	TIME [epoch: 8.83 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7559216153222247		[learning rate: 0.0043768]
		[batch 20/20] avg loss: 1.0314751948986576		[learning rate: 0.0043662]
	Learning Rate: 0.00436622
	LOSS [training: 0.8936984051104412 | validation: 0.4898153263447363]
	TIME [epoch: 8.83 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7547347585654829		[learning rate: 0.0043556]
		[batch 20/20] avg loss: 0.8265404210520156		[learning rate: 0.0043451]
	Learning Rate: 0.0043451
	LOSS [training: 0.7906375898087491 | validation: 1.097558395503384]
	TIME [epoch: 8.82 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1014048899297832		[learning rate: 0.0043346]
		[batch 20/20] avg loss: 0.6885721165659702		[learning rate: 0.0043241]
	Learning Rate: 0.00432409
	LOSS [training: 0.8949885032478766 | validation: 0.42059420633137423]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_223.pth
	Model improved!!!
EPOCH 224/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9151228455365821		[learning rate: 0.0043136]
		[batch 20/20] avg loss: 1.1469816636356227		[learning rate: 0.0043032]
	Learning Rate: 0.00430318
	LOSS [training: 1.0310522545861023 | validation: 1.507032990354622]
	TIME [epoch: 8.81 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9163601544912984		[learning rate: 0.0042928]
		[batch 20/20] avg loss: 0.8039207681927956		[learning rate: 0.0042824]
	Learning Rate: 0.00428237
	LOSS [training: 0.860140461342047 | validation: 1.292321194772295]
	TIME [epoch: 8.82 sec]
EPOCH 226/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8348834388694668		[learning rate: 0.004272]
		[batch 20/20] avg loss: 0.6355798912728079		[learning rate: 0.0042617]
	Learning Rate: 0.00426166
	LOSS [training: 0.7352316650711376 | validation: 0.8447811962134641]
	TIME [epoch: 8.83 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.206808483943625		[learning rate: 0.0042513]
		[batch 20/20] avg loss: 1.1786834143029021		[learning rate: 0.0042411]
	Learning Rate: 0.00424105
	LOSS [training: 1.1927459491232635 | validation: 1.674671685567126]
	TIME [epoch: 8.82 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1369261303658214		[learning rate: 0.0042308]
		[batch 20/20] avg loss: 1.1086714269572602		[learning rate: 0.0042205]
	Learning Rate: 0.00422054
	LOSS [training: 1.1227987786615408 | validation: 0.9958351019402263]
	TIME [epoch: 8.82 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9681818348214742		[learning rate: 0.0042103]
		[batch 20/20] avg loss: 1.0685067184894201		[learning rate: 0.0042001]
	Learning Rate: 0.00420013
	LOSS [training: 1.0183442766554471 | validation: 1.3084580117986544]
	TIME [epoch: 8.82 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9410843236254202		[learning rate: 0.00419]
		[batch 20/20] avg loss: 1.0189117895533255		[learning rate: 0.0041798]
	Learning Rate: 0.00417982
	LOSS [training: 0.979998056589373 | validation: 1.0228844819908265]
	TIME [epoch: 8.83 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6704928317096455		[learning rate: 0.0041697]
		[batch 20/20] avg loss: 0.5968079128514807		[learning rate: 0.0041596]
	Learning Rate: 0.00415961
	LOSS [training: 0.633650372280563 | validation: 0.5534752619743946]
	TIME [epoch: 8.82 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6374306833866078		[learning rate: 0.0041495]
		[batch 20/20] avg loss: 0.9109628738223755		[learning rate: 0.0041395]
	Learning Rate: 0.0041395
	LOSS [training: 0.7741967786044915 | validation: 0.48944597931651346]
	TIME [epoch: 8.81 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6667121415097809		[learning rate: 0.0041295]
		[batch 20/20] avg loss: 0.6800861778808903		[learning rate: 0.0041195]
	Learning Rate: 0.00411948
	LOSS [training: 0.6733991596953356 | validation: 0.641660425262884]
	TIME [epoch: 8.81 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7030330103045552		[learning rate: 0.0041095]
		[batch 20/20] avg loss: 0.9109328202389122		[learning rate: 0.0040996]
	Learning Rate: 0.00409956
	LOSS [training: 0.8069829152717338 | validation: 0.8101202045421827]
	TIME [epoch: 8.82 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0187244154770227		[learning rate: 0.0040896]
		[batch 20/20] avg loss: 0.9903248026780632		[learning rate: 0.0040797]
	Learning Rate: 0.00407973
	LOSS [training: 1.004524609077543 | validation: 0.5993664992415454]
	TIME [epoch: 8.82 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0309224726037653		[learning rate: 0.0040699]
		[batch 20/20] avg loss: 0.9405528548483174		[learning rate: 0.00406]
	Learning Rate: 0.00406
	LOSS [training: 0.9857376637260413 | validation: 0.9328171435004585]
	TIME [epoch: 8.84 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.701671994421688		[learning rate: 0.0040502]
		[batch 20/20] avg loss: 0.8445531712218098		[learning rate: 0.0040404]
	Learning Rate: 0.00404037
	LOSS [training: 0.7731125828217491 | validation: 0.5449144874660417]
	TIME [epoch: 8.81 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8889558123385768		[learning rate: 0.0040306]
		[batch 20/20] avg loss: 1.0242678171939583		[learning rate: 0.0040208]
	Learning Rate: 0.00402083
	LOSS [training: 0.9566118147662677 | validation: 0.5728407391618454]
	TIME [epoch: 8.8 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8574580044440703		[learning rate: 0.0040111]
		[batch 20/20] avg loss: 0.7539748179497228		[learning rate: 0.0040014]
	Learning Rate: 0.00400139
	LOSS [training: 0.8057164111968966 | validation: 0.6174148918986684]
	TIME [epoch: 8.82 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7146119775481884		[learning rate: 0.0039917]
		[batch 20/20] avg loss: 0.581949279660597		[learning rate: 0.003982]
	Learning Rate: 0.00398204
	LOSS [training: 0.6482806286043926 | validation: 0.6302043126433106]
	TIME [epoch: 8.82 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7082345582985279		[learning rate: 0.0039724]
		[batch 20/20] avg loss: 1.0616526231325962		[learning rate: 0.0039628]
	Learning Rate: 0.00396278
	LOSS [training: 0.884943590715562 | validation: 1.051592595199017]
	TIME [epoch: 8.85 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5777568261088145		[learning rate: 0.0039532]
		[batch 20/20] avg loss: 0.5612836316214974		[learning rate: 0.0039436]
	Learning Rate: 0.00394362
	LOSS [training: 0.569520228865156 | validation: 0.6893897750146646]
	TIME [epoch: 8.83 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6882134930361838		[learning rate: 0.0039341]
		[batch 20/20] avg loss: 0.6415389738552999		[learning rate: 0.0039245]
	Learning Rate: 0.00392455
	LOSS [training: 0.664876233445742 | validation: 0.6768014504602234]
	TIME [epoch: 8.81 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.038950681362212		[learning rate: 0.003915]
		[batch 20/20] avg loss: 0.7860151894807468		[learning rate: 0.0039056]
	Learning Rate: 0.00390557
	LOSS [training: 0.9124829354214794 | validation: 0.5726709675149886]
	TIME [epoch: 8.82 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6245994206963317		[learning rate: 0.0038961]
		[batch 20/20] avg loss: 0.5842427499914188		[learning rate: 0.0038867]
	Learning Rate: 0.00388668
	LOSS [training: 0.6044210853438752 | validation: 0.7891134715334813]
	TIME [epoch: 8.81 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6121070106814466		[learning rate: 0.0038773]
		[batch 20/20] avg loss: 0.6059872029411983		[learning rate: 0.0038679]
	Learning Rate: 0.00386789
	LOSS [training: 0.6090471068113225 | validation: 0.6656623896123296]
	TIME [epoch: 8.83 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6970737593067844		[learning rate: 0.0038585]
		[batch 20/20] avg loss: 0.8832015741101789		[learning rate: 0.0038492]
	Learning Rate: 0.00384918
	LOSS [training: 0.7901376667084816 | validation: 1.5994797843619133]
	TIME [epoch: 8.81 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.070266909272264		[learning rate: 0.0038399]
		[batch 20/20] avg loss: 0.931492588641573		[learning rate: 0.0038306]
	Learning Rate: 0.00383057
	LOSS [training: 1.0008797489569186 | validation: 1.5253413686290518]
	TIME [epoch: 8.81 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9175564143124564		[learning rate: 0.0038213]
		[batch 20/20] avg loss: 0.9278511850580099		[learning rate: 0.003812]
	Learning Rate: 0.00381204
	LOSS [training: 0.9227037996852332 | validation: 0.37277561060102]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_249.pth
	Model improved!!!
EPOCH 250/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7589614535341382		[learning rate: 0.0038028]
		[batch 20/20] avg loss: 0.866450316816937		[learning rate: 0.0037936]
	Learning Rate: 0.00379361
	LOSS [training: 0.8127058851755378 | validation: 0.9462846639671304]
	TIME [epoch: 8.81 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7124931722393664		[learning rate: 0.0037844]
		[batch 20/20] avg loss: 0.5119219105985919		[learning rate: 0.0037753]
	Learning Rate: 0.00377526
	LOSS [training: 0.6122075414189792 | validation: 0.7207022356514305]
	TIME [epoch: 8.83 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6927443193272496		[learning rate: 0.0037661]
		[batch 20/20] avg loss: 0.6950010039211085		[learning rate: 0.003757]
	Learning Rate: 0.00375701
	LOSS [training: 0.6938726616241788 | validation: 0.7369634133206611]
	TIME [epoch: 8.82 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.545888728346826		[learning rate: 0.0037479]
		[batch 20/20] avg loss: 0.9711831721926251		[learning rate: 0.0037388]
	Learning Rate: 0.00373884
	LOSS [training: 0.7585359502697255 | validation: 0.7082898754319423]
	TIME [epoch: 8.82 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7370423825477215		[learning rate: 0.0037298]
		[batch 20/20] avg loss: 0.7081322772177902		[learning rate: 0.0037208]
	Learning Rate: 0.00372076
	LOSS [training: 0.7225873298827558 | validation: 0.7640386046337251]
	TIME [epoch: 8.82 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6050386569609946		[learning rate: 0.0037118]
		[batch 20/20] avg loss: 0.5655991085900611		[learning rate: 0.0037028]
	Learning Rate: 0.00370277
	LOSS [training: 0.5853188827755278 | validation: 1.2336871038680637]
	TIME [epoch: 8.81 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6757092653210164		[learning rate: 0.0036938]
		[batch 20/20] avg loss: 0.7559530899046991		[learning rate: 0.0036849]
	Learning Rate: 0.00368486
	LOSS [training: 0.7158311776128576 | validation: 0.67245581703608]
	TIME [epoch: 8.84 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7211075864332501		[learning rate: 0.0036759]
		[batch 20/20] avg loss: 0.6126779078469036		[learning rate: 0.003667]
	Learning Rate: 0.00366704
	LOSS [training: 0.6668927471400768 | validation: 0.7820941167656865]
	TIME [epoch: 8.83 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8195039148485774		[learning rate: 0.0036582]
		[batch 20/20] avg loss: 0.9807052886350979		[learning rate: 0.0036493]
	Learning Rate: 0.00364931
	LOSS [training: 0.9001046017418377 | validation: 0.8438089407979427]
	TIME [epoch: 8.82 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5727194945883775		[learning rate: 0.0036405]
		[batch 20/20] avg loss: 0.7766567531737637		[learning rate: 0.0036317]
	Learning Rate: 0.00363166
	LOSS [training: 0.6746881238810707 | validation: 0.5985822372648607]
	TIME [epoch: 8.82 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6182012198970008		[learning rate: 0.0036229]
		[batch 20/20] avg loss: 0.5824728509662788		[learning rate: 0.0036141]
	Learning Rate: 0.0036141
	LOSS [training: 0.6003370354316397 | validation: 0.41679911934839897]
	TIME [epoch: 8.82 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5637751253141341		[learning rate: 0.0036053]
		[batch 20/20] avg loss: 0.846263030865719		[learning rate: 0.0035966]
	Learning Rate: 0.00359662
	LOSS [training: 0.7050190780899267 | validation: 0.9108930151924692]
	TIME [epoch: 8.84 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6233346237701233		[learning rate: 0.0035879]
		[batch 20/20] avg loss: 0.6538305113580345		[learning rate: 0.0035792]
	Learning Rate: 0.00357923
	LOSS [training: 0.6385825675640789 | validation: 0.7638570124282906]
	TIME [epoch: 8.84 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0412283098260944		[learning rate: 0.0035706]
		[batch 20/20] avg loss: 0.7296327485978575		[learning rate: 0.0035619]
	Learning Rate: 0.00356192
	LOSS [training: 0.8854305292119762 | validation: 0.671221079713263]
	TIME [epoch: 8.82 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.674953094777058		[learning rate: 0.0035533]
		[batch 20/20] avg loss: 0.7525645871381309		[learning rate: 0.0035447]
	Learning Rate: 0.0035447
	LOSS [training: 0.7137588409575946 | validation: 0.808375541721069]
	TIME [epoch: 8.82 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0197327660442046		[learning rate: 0.0035361]
		[batch 20/20] avg loss: 0.7728559332333536		[learning rate: 0.0035276]
	Learning Rate: 0.00352755
	LOSS [training: 0.8962943496387791 | validation: 0.5252484485966051]
	TIME [epoch: 8.8 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.578297039954115		[learning rate: 0.003519]
		[batch 20/20] avg loss: 0.6234813760836987		[learning rate: 0.0035105]
	Learning Rate: 0.0035105
	LOSS [training: 0.6008892080189068 | validation: 0.35857710961996764]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_266.pth
	Model improved!!!
EPOCH 267/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7480382158162436		[learning rate: 0.003502]
		[batch 20/20] avg loss: 0.520968815881159		[learning rate: 0.0034935]
	Learning Rate: 0.00349352
	LOSS [training: 0.6345035158487012 | validation: 0.8256862013747795]
	TIME [epoch: 8.84 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7743855531905163		[learning rate: 0.0034851]
		[batch 20/20] avg loss: 1.0243530317314877		[learning rate: 0.0034766]
	Learning Rate: 0.00347663
	LOSS [training: 0.8993692924610018 | validation: 1.1255163937867043]
	TIME [epoch: 8.82 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8255909950210736		[learning rate: 0.0034682]
		[batch 20/20] avg loss: 0.5847404248403175		[learning rate: 0.0034598]
	Learning Rate: 0.00345981
	LOSS [training: 0.7051657099306955 | validation: 1.014231646198756]
	TIME [epoch: 8.82 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7055416567374577		[learning rate: 0.0034514]
		[batch 20/20] avg loss: 0.5653258374518962		[learning rate: 0.0034431]
	Learning Rate: 0.00344308
	LOSS [training: 0.635433747094677 | validation: 0.3782053174215659]
	TIME [epoch: 8.81 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5898653453526383		[learning rate: 0.0034347]
		[batch 20/20] avg loss: 0.5767890245138745		[learning rate: 0.0034264]
	Learning Rate: 0.00342643
	LOSS [training: 0.5833271849332563 | validation: 0.46774054299963336]
	TIME [epoch: 8.82 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8416649657727454		[learning rate: 0.0034181]
		[batch 20/20] avg loss: 0.49951092383718637		[learning rate: 0.0034099]
	Learning Rate: 0.00340986
	LOSS [training: 0.6705879448049659 | validation: 0.4637706338712037]
	TIME [epoch: 8.83 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4781934587047284		[learning rate: 0.0034016]
		[batch 20/20] avg loss: 0.9222335185821212		[learning rate: 0.0033934]
	Learning Rate: 0.00339337
	LOSS [training: 0.7002134886434248 | validation: 0.5124400710300655]
	TIME [epoch: 8.81 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5803417722955752		[learning rate: 0.0033852]
		[batch 20/20] avg loss: 0.6522554204053785		[learning rate: 0.003377]
	Learning Rate: 0.00337696
	LOSS [training: 0.6162985963504769 | validation: 0.6641211561900794]
	TIME [epoch: 8.82 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5877383225700894		[learning rate: 0.0033688]
		[batch 20/20] avg loss: 0.46539327981002643		[learning rate: 0.0033606]
	Learning Rate: 0.00336063
	LOSS [training: 0.5265658011900578 | validation: 0.4466249943805699]
	TIME [epoch: 8.81 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48863876934109474		[learning rate: 0.0033525]
		[batch 20/20] avg loss: 0.8315410147522506		[learning rate: 0.0033444]
	Learning Rate: 0.00334438
	LOSS [training: 0.6600898920466726 | validation: 1.5083109758895956]
	TIME [epoch: 8.83 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9507680153453778		[learning rate: 0.0033363]
		[batch 20/20] avg loss: 0.9056906059960805		[learning rate: 0.0033282]
	Learning Rate: 0.00332821
	LOSS [training: 0.9282293106707291 | validation: 0.7831256273812448]
	TIME [epoch: 8.82 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.649547417336865		[learning rate: 0.0033202]
		[batch 20/20] avg loss: 0.9130888989146573		[learning rate: 0.0033121]
	Learning Rate: 0.00331211
	LOSS [training: 0.7813181581257611 | validation: 1.4650764347327234]
	TIME [epoch: 8.81 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9158201010830513		[learning rate: 0.0033041]
		[batch 20/20] avg loss: 0.7354336963900067		[learning rate: 0.0032961]
	Learning Rate: 0.0032961
	LOSS [training: 0.8256268987365291 | validation: 0.5445046380725824]
	TIME [epoch: 8.81 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7894266036362396		[learning rate: 0.0032881]
		[batch 20/20] avg loss: 0.8590684221054368		[learning rate: 0.0032802]
	Learning Rate: 0.00328016
	LOSS [training: 0.8242475128708382 | validation: 0.41667984042814027]
	TIME [epoch: 8.81 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7680206852561311		[learning rate: 0.0032722]
		[batch 20/20] avg loss: 0.44114608855281273		[learning rate: 0.0032643]
	Learning Rate: 0.0032643
	LOSS [training: 0.6045833869044719 | validation: 0.41239399586307823]
	TIME [epoch: 8.82 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4617219044910502		[learning rate: 0.0032564]
		[batch 20/20] avg loss: 0.7337618750350082		[learning rate: 0.0032485]
	Learning Rate: 0.00324851
	LOSS [training: 0.5977418897630293 | validation: 1.5715161853801862]
	TIME [epoch: 8.84 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9136508152628512		[learning rate: 0.0032406]
		[batch 20/20] avg loss: 0.5728265126284873		[learning rate: 0.0032328]
	Learning Rate: 0.0032328
	LOSS [training: 0.7432386639456692 | validation: 0.3295660085088777]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_283.pth
	Model improved!!!
EPOCH 284/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45423133176813285		[learning rate: 0.003225]
		[batch 20/20] avg loss: 0.4829053246827704		[learning rate: 0.0032172]
	Learning Rate: 0.00321717
	LOSS [training: 0.4685683282254517 | validation: 0.3740782713196736]
	TIME [epoch: 8.8 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5393718095533047		[learning rate: 0.0032094]
		[batch 20/20] avg loss: 0.6802008512072925		[learning rate: 0.0032016]
	Learning Rate: 0.00320161
	LOSS [training: 0.6097863303802985 | validation: 0.4439912831355095]
	TIME [epoch: 8.79 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48753890014705803		[learning rate: 0.0031939]
		[batch 20/20] avg loss: 0.8116584447706755		[learning rate: 0.0031861]
	Learning Rate: 0.00318613
	LOSS [training: 0.6495986724588667 | validation: 0.6233908159122931]
	TIME [epoch: 8.79 sec]
EPOCH 287/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8380167390294894		[learning rate: 0.0031784]
		[batch 20/20] avg loss: 0.6386054249674548		[learning rate: 0.0031707]
	Learning Rate: 0.00317072
	LOSS [training: 0.7383110819984721 | validation: 0.6321352170848263]
	TIME [epoch: 8.82 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5023180610791851		[learning rate: 0.003163]
		[batch 20/20] avg loss: 0.4671548135718059		[learning rate: 0.0031554]
	Learning Rate: 0.00315539
	LOSS [training: 0.48473643732549554 | validation: 0.4328209828486772]
	TIME [epoch: 8.8 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5115994136242695		[learning rate: 0.0031477]
		[batch 20/20] avg loss: 0.7201218793818311		[learning rate: 0.0031401]
	Learning Rate: 0.00314013
	LOSS [training: 0.6158606465030504 | validation: 0.33996632962736406]
	TIME [epoch: 8.79 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5907947496078465		[learning rate: 0.0031325]
		[batch 20/20] avg loss: 0.5047115056418888		[learning rate: 0.0031249]
	Learning Rate: 0.00312494
	LOSS [training: 0.5477531276248677 | validation: 0.43322458644535883]
	TIME [epoch: 8.79 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5952160330183554		[learning rate: 0.0031174]
		[batch 20/20] avg loss: 0.7437451474229824		[learning rate: 0.0031098]
	Learning Rate: 0.00310983
	LOSS [training: 0.6694805902206686 | validation: 0.5708497876239593]
	TIME [epoch: 8.81 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.823240153825847		[learning rate: 0.0031023]
		[batch 20/20] avg loss: 0.7789644101945601		[learning rate: 0.0030948]
	Learning Rate: 0.00309479
	LOSS [training: 0.8011022820102036 | validation: 0.37031695075568993]
	TIME [epoch: 8.81 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7651870163627169		[learning rate: 0.0030873]
		[batch 20/20] avg loss: 0.8084807550482209		[learning rate: 0.0030798]
	Learning Rate: 0.00307983
	LOSS [training: 0.786833885705469 | validation: 0.40592855838937314]
	TIME [epoch: 8.79 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4666901302706612		[learning rate: 0.0030724]
		[batch 20/20] avg loss: 0.5132023045318735		[learning rate: 0.0030649]
	Learning Rate: 0.00306493
	LOSS [training: 0.4899462174012674 | validation: 0.4986820185726131]
	TIME [epoch: 8.8 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40338773145968015		[learning rate: 0.0030575]
		[batch 20/20] avg loss: 0.4645705063462621		[learning rate: 0.0030501]
	Learning Rate: 0.00305011
	LOSS [training: 0.4339791189029711 | validation: 0.3216712855175574]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_295.pth
	Model improved!!!
EPOCH 296/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6034641515492156		[learning rate: 0.0030427]
		[batch 20/20] avg loss: 0.5892955077925873		[learning rate: 0.0030354]
	Learning Rate: 0.00303536
	LOSS [training: 0.5963798296709015 | validation: 0.5220802630020153]
	TIME [epoch: 8.83 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6161917689911406		[learning rate: 0.003028]
		[batch 20/20] avg loss: 0.7579747814092151		[learning rate: 0.0030207]
	Learning Rate: 0.00302068
	LOSS [training: 0.6870832752001779 | validation: 0.3298787902064724]
	TIME [epoch: 8.83 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5558476889350142		[learning rate: 0.0030134]
		[batch 20/20] avg loss: 0.6542669730877783		[learning rate: 0.0030061]
	Learning Rate: 0.00300608
	LOSS [training: 0.6050573310113962 | validation: 0.30284618003701214]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_298.pth
	Model improved!!!
EPOCH 299/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6315050661825051		[learning rate: 0.0029988]
		[batch 20/20] avg loss: 0.7047627068264646		[learning rate: 0.0029915]
	Learning Rate: 0.00299154
	LOSS [training: 0.6681338865044848 | validation: 0.9094991216869901]
	TIME [epoch: 8.81 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.42765126614652055		[learning rate: 0.0029843]
		[batch 20/20] avg loss: 0.6597469118393849		[learning rate: 0.0029771]
	Learning Rate: 0.00297707
	LOSS [training: 0.5436990889929529 | validation: 0.41787817080044726]
	TIME [epoch: 8.8 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4113909269646843		[learning rate: 0.0029699]
		[batch 20/20] avg loss: 0.49017289554719		[learning rate: 0.0029627]
	Learning Rate: 0.00296268
	LOSS [training: 0.4507819112559373 | validation: 0.4620444810783013]
	TIME [epoch: 8.82 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3998767368206974		[learning rate: 0.0029555]
		[batch 20/20] avg loss: 0.796302100763055		[learning rate: 0.0029483]
	Learning Rate: 0.00294835
	LOSS [training: 0.5980894187918763 | validation: 0.4940578998259989]
	TIME [epoch: 8.81 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7309752937029098		[learning rate: 0.0029412]
		[batch 20/20] avg loss: 0.6643918936083404		[learning rate: 0.0029341]
	Learning Rate: 0.00293409
	LOSS [training: 0.6976835936556252 | validation: 0.24675810232824782]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_303.pth
	Model improved!!!
EPOCH 304/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5556341615736919		[learning rate: 0.002927]
		[batch 20/20] avg loss: 0.5813570799278114		[learning rate: 0.0029199]
	Learning Rate: 0.0029199
	LOSS [training: 0.5684956207507519 | validation: 0.24538812633594448]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_304.pth
	Model improved!!!
EPOCH 305/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6843827120125094		[learning rate: 0.0029128]
		[batch 20/20] avg loss: 0.6185709370286446		[learning rate: 0.0029058]
	Learning Rate: 0.00290578
	LOSS [training: 0.6514768245205771 | validation: 0.3638950715262715]
	TIME [epoch: 8.81 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7125915874293592		[learning rate: 0.0028987]
		[batch 20/20] avg loss: 0.4597354586704676		[learning rate: 0.0028917]
	Learning Rate: 0.00289173
	LOSS [training: 0.5861635230499134 | validation: 1.0257749835663876]
	TIME [epoch: 8.81 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5340638762981825		[learning rate: 0.0028847]
		[batch 20/20] avg loss: 0.4042132017611113		[learning rate: 0.0028777]
	Learning Rate: 0.00287775
	LOSS [training: 0.46913853902964686 | validation: 0.4295806064444715]
	TIME [epoch: 8.8 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5191247906113713		[learning rate: 0.0028708]
		[batch 20/20] avg loss: 0.40289650785251896		[learning rate: 0.0028638]
	Learning Rate: 0.00286383
	LOSS [training: 0.4610106492319451 | validation: 0.32118379451648343]
	TIME [epoch: 8.82 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3658251027382346		[learning rate: 0.0028569]
		[batch 20/20] avg loss: 0.71100523680494		[learning rate: 0.00285]
	Learning Rate: 0.00284998
	LOSS [training: 0.5384151697715872 | validation: 0.8345083684313401]
	TIME [epoch: 8.81 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7172299027803796		[learning rate: 0.0028431]
		[batch 20/20] avg loss: 0.745074898661485		[learning rate: 0.0028362]
	Learning Rate: 0.0028362
	LOSS [training: 0.7311524007209322 | validation: 1.072850641369663]
	TIME [epoch: 8.81 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7169240824536206		[learning rate: 0.0028293]
		[batch 20/20] avg loss: 0.44936919524756247		[learning rate: 0.0028225]
	Learning Rate: 0.00282248
	LOSS [training: 0.5831466388505915 | validation: 0.33633441453432666]
	TIME [epoch: 8.82 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45902456942287645		[learning rate: 0.0028157]
		[batch 20/20] avg loss: 0.7718254856429978		[learning rate: 0.0028088]
	Learning Rate: 0.00280884
	LOSS [training: 0.6154250275329369 | validation: 0.467699244291626]
	TIME [epoch: 8.8 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.41814479580150554		[learning rate: 0.002802]
		[batch 20/20] avg loss: 0.3495719862101271		[learning rate: 0.0027953]
	Learning Rate: 0.00279525
	LOSS [training: 0.3838583910058162 | validation: 0.561638811211739]
	TIME [epoch: 8.8 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5884563538196401		[learning rate: 0.0027885]
		[batch 20/20] avg loss: 0.4868368670977395		[learning rate: 0.0027817]
	Learning Rate: 0.00278174
	LOSS [training: 0.5376466104586899 | validation: 0.26573136312033285]
	TIME [epoch: 8.79 sec]
EPOCH 315/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.647907060825423		[learning rate: 0.002775]
		[batch 20/20] avg loss: 0.6953254225457648		[learning rate: 0.0027683]
	Learning Rate: 0.00276828
	LOSS [training: 0.6716162416855939 | validation: 1.3864234728971168]
	TIME [epoch: 8.81 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7073890880278784		[learning rate: 0.0027616]
		[batch 20/20] avg loss: 0.44226059352864777		[learning rate: 0.0027549]
	Learning Rate: 0.0027549
	LOSS [training: 0.5748248407782629 | validation: 0.3397954816626188]
	TIME [epoch: 8.81 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4983805171761187		[learning rate: 0.0027482]
		[batch 20/20] avg loss: 0.5117996362651136		[learning rate: 0.0027416]
	Learning Rate: 0.00274157
	LOSS [training: 0.505090076720616 | validation: 1.0638343102801686]
	TIME [epoch: 8.79 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.554711752967836		[learning rate: 0.0027349]
		[batch 20/20] avg loss: 0.40927470734594473		[learning rate: 0.0027283]
	Learning Rate: 0.00272832
	LOSS [training: 0.4819932301568903 | validation: 0.909722833593733]
	TIME [epoch: 8.79 sec]
EPOCH 319/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4557956657880625		[learning rate: 0.0027217]
		[batch 20/20] avg loss: 0.4365133862276488		[learning rate: 0.0027151]
	Learning Rate: 0.00271512
	LOSS [training: 0.44615452600785577 | validation: 0.2415931561660113]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_319.pth
	Model improved!!!
EPOCH 320/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5864160999750744		[learning rate: 0.0027086]
		[batch 20/20] avg loss: 0.36291157907640736		[learning rate: 0.002702]
	Learning Rate: 0.00270199
	LOSS [training: 0.4746638395257408 | validation: 0.2683408477139151]
	TIME [epoch: 8.81 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.573858634395143		[learning rate: 0.0026955]
		[batch 20/20] avg loss: 0.5982549924918346		[learning rate: 0.0026889]
	Learning Rate: 0.00268893
	LOSS [training: 0.5860568134434887 | validation: 0.9112365793794998]
	TIME [epoch: 8.82 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3858232641203144		[learning rate: 0.0026824]
		[batch 20/20] avg loss: 0.49102849200440407		[learning rate: 0.0026759]
	Learning Rate: 0.00267592
	LOSS [training: 0.4384258780623592 | validation: 1.01922017681191]
	TIME [epoch: 8.81 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7986026178715784		[learning rate: 0.0026694]
		[batch 20/20] avg loss: 0.6562118507220486		[learning rate: 0.002663]
	Learning Rate: 0.00266298
	LOSS [training: 0.7274072342968136 | validation: 0.334191482021999]
	TIME [epoch: 8.8 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34875102249237144		[learning rate: 0.0026565]
		[batch 20/20] avg loss: 0.42601910642945084		[learning rate: 0.0026501]
	Learning Rate: 0.00265011
	LOSS [training: 0.38738506446091103 | validation: 0.33168148333985886]
	TIME [epoch: 8.79 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4502261110768714		[learning rate: 0.0026437]
		[batch 20/20] avg loss: 0.5865746603542257		[learning rate: 0.0026373]
	Learning Rate: 0.00263729
	LOSS [training: 0.5184003857155485 | validation: 1.1172486894766978]
	TIME [epoch: 8.8 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.587849832430706		[learning rate: 0.0026309]
		[batch 20/20] avg loss: 0.3445641655474246		[learning rate: 0.0026245]
	Learning Rate: 0.00262454
	LOSS [training: 0.46620699898906526 | validation: 0.3000386913024732]
	TIME [epoch: 8.82 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3666302687937105		[learning rate: 0.0026182]
		[batch 20/20] avg loss: 0.5325912408734823		[learning rate: 0.0026118]
	Learning Rate: 0.00261184
	LOSS [training: 0.4496107548335965 | validation: 0.8554287093413399]
	TIME [epoch: 8.79 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.473222254881193		[learning rate: 0.0026055]
		[batch 20/20] avg loss: 0.4171559973427734		[learning rate: 0.0025992]
	Learning Rate: 0.00259921
	LOSS [training: 0.44518912611198314 | validation: 0.8140991674410082]
	TIME [epoch: 8.79 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5151546112086203		[learning rate: 0.0025929]
		[batch 20/20] avg loss: 0.4715698741827257		[learning rate: 0.0025866]
	Learning Rate: 0.00258664
	LOSS [training: 0.493362242695673 | validation: 0.2980543164370518]
	TIME [epoch: 8.8 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3190448199492708		[learning rate: 0.0025804]
		[batch 20/20] avg loss: 0.4145322047662633		[learning rate: 0.0025741]
	Learning Rate: 0.00257414
	LOSS [training: 0.3667885123577671 | validation: 0.4726589572250013]
	TIME [epoch: 8.8 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3792954459710799		[learning rate: 0.0025679]
		[batch 20/20] avg loss: 0.45312090531722465		[learning rate: 0.0025617]
	Learning Rate: 0.00256169
	LOSS [training: 0.41620817564415225 | validation: 0.562855432340119]
	TIME [epoch: 8.82 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3940506503782291		[learning rate: 0.0025555]
		[batch 20/20] avg loss: 0.3649700964439943		[learning rate: 0.0025493]
	Learning Rate: 0.0025493
	LOSS [training: 0.3795103734111117 | validation: 0.32129723132131716]
	TIME [epoch: 8.8 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7766365077124971		[learning rate: 0.0025431]
		[batch 20/20] avg loss: 0.4281598189455232		[learning rate: 0.002537]
	Learning Rate: 0.00253697
	LOSS [training: 0.6023981633290103 | validation: 0.1624459522939672]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_333.pth
	Model improved!!!
EPOCH 334/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.42032576955126855		[learning rate: 0.0025308]
		[batch 20/20] avg loss: 0.3875515558828256		[learning rate: 0.0025247]
	Learning Rate: 0.0025247
	LOSS [training: 0.403938662717047 | validation: 0.7863655726092548]
	TIME [epoch: 8.81 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45992878682623706		[learning rate: 0.0025186]
		[batch 20/20] avg loss: 0.5214081972499567		[learning rate: 0.0025125]
	Learning Rate: 0.0025125
	LOSS [training: 0.4906684920380969 | validation: 0.400093825318272]
	TIME [epoch: 8.81 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31239594687296474		[learning rate: 0.0025064]
		[batch 20/20] avg loss: 0.3454883330666392		[learning rate: 0.0025003]
	Learning Rate: 0.00250035
	LOSS [training: 0.328942139969802 | validation: 0.3961053811875137]
	TIME [epoch: 8.83 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4492652471368688		[learning rate: 0.0024943]
		[batch 20/20] avg loss: 0.4297865248650063		[learning rate: 0.0024883]
	Learning Rate: 0.00248825
	LOSS [training: 0.4395258860009375 | validation: 0.24030020436764457]
	TIME [epoch: 8.81 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3909588627076251		[learning rate: 0.0024822]
		[batch 20/20] avg loss: 0.2964372142348534		[learning rate: 0.0024762]
	Learning Rate: 0.00247622
	LOSS [training: 0.3436980384712393 | validation: 0.2373334784325307]
	TIME [epoch: 8.79 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4648217343654838		[learning rate: 0.0024702]
		[batch 20/20] avg loss: 0.29965354716672754		[learning rate: 0.0024642]
	Learning Rate: 0.00246425
	LOSS [training: 0.38223764076610556 | validation: 0.31023399569751364]
	TIME [epoch: 8.8 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4745658058911119		[learning rate: 0.0024583]
		[batch 20/20] avg loss: 0.6219565387998597		[learning rate: 0.0024523]
	Learning Rate: 0.00245233
	LOSS [training: 0.5482611723454858 | validation: 0.676304040990099]
	TIME [epoch: 8.8 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4419877650295033		[learning rate: 0.0024464]
		[batch 20/20] avg loss: 0.5253702387099444		[learning rate: 0.0024405]
	Learning Rate: 0.00244047
	LOSS [training: 0.4836790018697238 | validation: 0.4310216293899092]
	TIME [epoch: 8.82 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44229469409204675		[learning rate: 0.0024346]
		[batch 20/20] avg loss: 0.6119278651709333		[learning rate: 0.0024287]
	Learning Rate: 0.00242867
	LOSS [training: 0.5271112796314898 | validation: 0.9471615870889305]
	TIME [epoch: 8.8 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5887219033537637		[learning rate: 0.0024228]
		[batch 20/20] avg loss: 0.50795400479101		[learning rate: 0.0024169]
	Learning Rate: 0.00241693
	LOSS [training: 0.548337954072387 | validation: 0.4709416119959703]
	TIME [epoch: 8.8 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3815134666949838		[learning rate: 0.0024111]
		[batch 20/20] avg loss: 0.4012053367361984		[learning rate: 0.0024052]
	Learning Rate: 0.00240524
	LOSS [training: 0.391359401715591 | validation: 0.4789684490401909]
	TIME [epoch: 8.8 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5572446718283323		[learning rate: 0.0023994]
		[batch 20/20] avg loss: 0.5357793626163808		[learning rate: 0.0023936]
	Learning Rate: 0.00239361
	LOSS [training: 0.5465120172223564 | validation: 0.7359359622240247]
	TIME [epoch: 8.8 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36369698875913625		[learning rate: 0.0023878]
		[batch 20/20] avg loss: 0.35275725246644407		[learning rate: 0.002382]
	Learning Rate: 0.00238203
	LOSS [training: 0.35822712061279016 | validation: 0.4615102129974842]
	TIME [epoch: 8.83 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4792398950296978		[learning rate: 0.0023763]
		[batch 20/20] avg loss: 0.2796392884728841		[learning rate: 0.0023705]
	Learning Rate: 0.00237051
	LOSS [training: 0.379439591751291 | validation: 0.23332861944213057]
	TIME [epoch: 8.8 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4990562576686594		[learning rate: 0.0023648]
		[batch 20/20] avg loss: 0.46842797575301437		[learning rate: 0.002359]
	Learning Rate: 0.00235905
	LOSS [training: 0.4837421167108369 | validation: 0.192683352597795]
	TIME [epoch: 8.8 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6443842441653628		[learning rate: 0.0023533]
		[batch 20/20] avg loss: 0.5349114725921311		[learning rate: 0.0023476]
	Learning Rate: 0.00234764
	LOSS [training: 0.5896478583787468 | validation: 0.4128066505157864]
	TIME [epoch: 8.81 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6102591574241198		[learning rate: 0.002342]
		[batch 20/20] avg loss: 0.5673146898073146		[learning rate: 0.0023363]
	Learning Rate: 0.00233629
	LOSS [training: 0.5887869236157172 | validation: 0.27642087749247735]
	TIME [epoch: 8.8 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.426547499693954		[learning rate: 0.0023306]
		[batch 20/20] avg loss: 0.33551467352231334		[learning rate: 0.002325]
	Learning Rate: 0.00232499
	LOSS [training: 0.3810310866081336 | validation: 0.23425958023299492]
	TIME [epoch: 8.83 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5957368247293258		[learning rate: 0.0023194]
		[batch 20/20] avg loss: 0.5669633087355913		[learning rate: 0.0023137]
	Learning Rate: 0.00231375
	LOSS [training: 0.5813500667324585 | validation: 0.19798162036215408]
	TIME [epoch: 8.81 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4716036709765457		[learning rate: 0.0023081]
		[batch 20/20] avg loss: 0.34728282174886227		[learning rate: 0.0023026]
	Learning Rate: 0.00230256
	LOSS [training: 0.409443246362704 | validation: 0.13581297999487316]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_353.pth
	Model improved!!!
EPOCH 354/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44996768153119604		[learning rate: 0.002297]
		[batch 20/20] avg loss: 0.39270949410379286		[learning rate: 0.0022914]
	Learning Rate: 0.00229142
	LOSS [training: 0.4213385878174945 | validation: 0.23176825427902606]
	TIME [epoch: 8.8 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38968325198498155		[learning rate: 0.0022859]
		[batch 20/20] avg loss: 0.28028237612361073		[learning rate: 0.0022803]
	Learning Rate: 0.00228034
	LOSS [training: 0.3349828140542962 | validation: 0.19257891974523067]
	TIME [epoch: 8.79 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5600196652677487		[learning rate: 0.0022748]
		[batch 20/20] avg loss: 0.3054733727459153		[learning rate: 0.0022693]
	Learning Rate: 0.00226931
	LOSS [training: 0.43274651900683203 | validation: 0.3826345809303615]
	TIME [epoch: 8.81 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5056368048834339		[learning rate: 0.0022638]
		[batch 20/20] avg loss: 0.47404341982937437		[learning rate: 0.0022583]
	Learning Rate: 0.00225834
	LOSS [training: 0.4898401123564041 | validation: 0.35455186686526513]
	TIME [epoch: 8.81 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38661590879472213		[learning rate: 0.0022529]
		[batch 20/20] avg loss: 0.6384030980429334		[learning rate: 0.0022474]
	Learning Rate: 0.00224742
	LOSS [training: 0.5125095034188277 | validation: 0.595587815868479]
	TIME [epoch: 8.8 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5539693840999367		[learning rate: 0.002242]
		[batch 20/20] avg loss: 0.45672988657915264		[learning rate: 0.0022366]
	Learning Rate: 0.00223655
	LOSS [training: 0.5053496353395446 | validation: 0.2709499923249126]
	TIME [epoch: 8.8 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46204614088753554		[learning rate: 0.0022311]
		[batch 20/20] avg loss: 0.3001361470967564		[learning rate: 0.0022257]
	Learning Rate: 0.00222574
	LOSS [training: 0.381091143992146 | validation: 0.246868380498281]
	TIME [epoch: 8.79 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3514558792797672		[learning rate: 0.0022203]
		[batch 20/20] avg loss: 0.3952386327092029		[learning rate: 0.002215]
	Learning Rate: 0.00221497
	LOSS [training: 0.37334725599448493 | validation: 0.4269181244639563]
	TIME [epoch: 8.82 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5121675178680712		[learning rate: 0.0022096]
		[batch 20/20] avg loss: 0.4166045900809234		[learning rate: 0.0022043]
	Learning Rate: 0.00220426
	LOSS [training: 0.46438605397449734 | validation: 0.8194489135792661]
	TIME [epoch: 8.81 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46542073101755876		[learning rate: 0.0021989]
		[batch 20/20] avg loss: 0.38610224684239264		[learning rate: 0.0021936]
	Learning Rate: 0.0021936
	LOSS [training: 0.4257614889299757 | validation: 0.18898366596626254]
	TIME [epoch: 8.8 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5238179675074985		[learning rate: 0.0021883]
		[batch 20/20] avg loss: 0.36293130413011154		[learning rate: 0.002183]
	Learning Rate: 0.00218299
	LOSS [training: 0.4433746358188051 | validation: 0.21782948447404574]
	TIME [epoch: 8.8 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.42565943865916445		[learning rate: 0.0021777]
		[batch 20/20] avg loss: 0.3433730821216846		[learning rate: 0.0021724]
	Learning Rate: 0.00217244
	LOSS [training: 0.3845162603904245 | validation: 0.46780568155221874]
	TIME [epoch: 8.8 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3643701282520295		[learning rate: 0.0021672]
		[batch 20/20] avg loss: 0.2931724558759826		[learning rate: 0.0021619]
	Learning Rate: 0.00216193
	LOSS [training: 0.32877129206400607 | validation: 0.2514743544255809]
	TIME [epoch: 8.83 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37539292544284586		[learning rate: 0.0021567]
		[batch 20/20] avg loss: 0.3867067430632015		[learning rate: 0.0021515]
	Learning Rate: 0.00215148
	LOSS [training: 0.38104983425302363 | validation: 0.32037997300218246]
	TIME [epoch: 8.79 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4904967253184309		[learning rate: 0.0021463]
		[batch 20/20] avg loss: 0.9064092596652472		[learning rate: 0.0021411]
	Learning Rate: 0.00214107
	LOSS [training: 0.698452992491839 | validation: 1.0784917771299252]
	TIME [epoch: 8.8 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5351762212718439		[learning rate: 0.0021359]
		[batch 20/20] avg loss: 0.48560486472711883		[learning rate: 0.0021307]
	Learning Rate: 0.00213072
	LOSS [training: 0.5103905429994814 | validation: 0.6879307080597177]
	TIME [epoch: 8.8 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.43093130233016286		[learning rate: 0.0021256]
		[batch 20/20] avg loss: 0.4920639626575859		[learning rate: 0.0021204]
	Learning Rate: 0.00212042
	LOSS [training: 0.4614976324938744 | validation: 0.41290520524097885]
	TIME [epoch: 8.8 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2585358701150266		[learning rate: 0.0021153]
		[batch 20/20] avg loss: 0.2675614552572455		[learning rate: 0.0021102]
	Learning Rate: 0.00211016
	LOSS [training: 0.2630486626861361 | validation: 0.328177885566594]
	TIME [epoch: 8.82 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33629544384779175		[learning rate: 0.0021051]
		[batch 20/20] avg loss: 0.27969616840901584		[learning rate: 0.0021]
	Learning Rate: 0.00209996
	LOSS [training: 0.3079958061284038 | validation: 0.20916782186636104]
	TIME [epoch: 8.81 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3310921069666312		[learning rate: 0.0020949]
		[batch 20/20] avg loss: 0.5185491443925736		[learning rate: 0.0020898]
	Learning Rate: 0.0020898
	LOSS [training: 0.42482062567960244 | validation: 0.3343907135637594]
	TIME [epoch: 8.8 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26102048834619085		[learning rate: 0.0020847]
		[batch 20/20] avg loss: 0.5781093307154075		[learning rate: 0.0020797]
	Learning Rate: 0.0020797
	LOSS [training: 0.4195649095307992 | validation: 0.7968278877699155]
	TIME [epoch: 8.79 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.47353993481831846		[learning rate: 0.0020747]
		[batch 20/20] avg loss: 0.3846230929580259		[learning rate: 0.0020696]
	Learning Rate: 0.00206964
	LOSS [training: 0.42908151388817206 | validation: 0.15797304448272828]
	TIME [epoch: 8.8 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48350950661413367		[learning rate: 0.0020646]
		[batch 20/20] avg loss: 0.264830941755123		[learning rate: 0.0020596]
	Learning Rate: 0.00205963
	LOSS [training: 0.3741702241846284 | validation: 0.1835368928605345]
	TIME [epoch: 8.82 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3218804077695019		[learning rate: 0.0020546]
		[batch 20/20] avg loss: 0.5453649907057155		[learning rate: 0.0020497]
	Learning Rate: 0.00204967
	LOSS [training: 0.43362269923760877 | validation: 0.2792110523467621]
	TIME [epoch: 8.82 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2539180763712786		[learning rate: 0.0020447]
		[batch 20/20] avg loss: 0.32079060749140076		[learning rate: 0.0020398]
	Learning Rate: 0.00203976
	LOSS [training: 0.2873543419313397 | validation: 0.23635165533385444]
	TIME [epoch: 8.81 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23395484004262718		[learning rate: 0.0020348]
		[batch 20/20] avg loss: 0.2606982094628702		[learning rate: 0.0020299]
	Learning Rate: 0.0020299
	LOSS [training: 0.24732652475274866 | validation: 1.2055996893806011]
	TIME [epoch: 8.8 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48801693492855575		[learning rate: 0.002025]
		[batch 20/20] avg loss: 0.5938592962801864		[learning rate: 0.0020201]
	Learning Rate: 0.00202008
	LOSS [training: 0.5409381156043711 | validation: 0.5664707338948931]
	TIME [epoch: 8.8 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5020426879914905		[learning rate: 0.0020152]
		[batch 20/20] avg loss: 0.46506549190516366		[learning rate: 0.0020103]
	Learning Rate: 0.00201031
	LOSS [training: 0.483554089948327 | validation: 0.24865424182104706]
	TIME [epoch: 8.8 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21533043762971818		[learning rate: 0.0020054]
		[batch 20/20] avg loss: 0.34938691373330794		[learning rate: 0.0020006]
	Learning Rate: 0.00200059
	LOSS [training: 0.282358675681513 | validation: 0.3885798601588497]
	TIME [epoch: 8.82 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3590170845323203		[learning rate: 0.0019957]
		[batch 20/20] avg loss: 0.49442180527577023		[learning rate: 0.0019909]
	Learning Rate: 0.00199091
	LOSS [training: 0.4267194449040453 | validation: 0.9711863750990468]
	TIME [epoch: 8.8 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5040097301716648		[learning rate: 0.0019861]
		[batch 20/20] avg loss: 0.3157123395422471		[learning rate: 0.0019813]
	Learning Rate: 0.00198129
	LOSS [training: 0.40986103485695596 | validation: 0.2777257785999822]
	TIME [epoch: 8.8 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46806819487463364		[learning rate: 0.0019765]
		[batch 20/20] avg loss: 0.39545704698412093		[learning rate: 0.0019717]
	Learning Rate: 0.00197171
	LOSS [training: 0.43176262092937734 | validation: 1.060358265385835]
	TIME [epoch: 8.8 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5909786652873334		[learning rate: 0.0019669]
		[batch 20/20] avg loss: 0.5625929055759331		[learning rate: 0.0019622]
	Learning Rate: 0.00196217
	LOSS [training: 0.5767857854316334 | validation: 0.16374755698528545]
	TIME [epoch: 8.8 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5102977317098109		[learning rate: 0.0019574]
		[batch 20/20] avg loss: 0.34386340960164796		[learning rate: 0.0019527]
	Learning Rate: 0.00195268
	LOSS [training: 0.42708057065572935 | validation: 0.23201890215680548]
	TIME [epoch: 8.82 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40047767044742405		[learning rate: 0.001948]
		[batch 20/20] avg loss: 0.40119779423090957		[learning rate: 0.0019432]
	Learning Rate: 0.00194324
	LOSS [training: 0.40083773233916686 | validation: 0.5662042961382211]
	TIME [epoch: 8.81 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4290113550951169		[learning rate: 0.0019385]
		[batch 20/20] avg loss: 0.5321889353553984		[learning rate: 0.0019338]
	Learning Rate: 0.00193384
	LOSS [training: 0.4806001452252576 | validation: 0.8758119995552235]
	TIME [epoch: 8.81 sec]
EPOCH 390/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4122665522381097		[learning rate: 0.0019292]
		[batch 20/20] avg loss: 0.38058603920265766		[learning rate: 0.0019245]
	Learning Rate: 0.00192449
	LOSS [training: 0.3964262957203837 | validation: 0.32336800728072823]
	TIME [epoch: 8.81 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2532898865059926		[learning rate: 0.0019198]
		[batch 20/20] avg loss: 0.3504760750919253		[learning rate: 0.0019152]
	Learning Rate: 0.00191518
	LOSS [training: 0.301882980798959 | validation: 0.515707770554278]
	TIME [epoch: 8.81 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39328940091784165		[learning rate: 0.0019105]
		[batch 20/20] avg loss: 0.4754948541145977		[learning rate: 0.0019059]
	Learning Rate: 0.00190592
	LOSS [training: 0.4343921275162196 | validation: 0.6230578729423208]
	TIME [epoch: 8.83 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2795823088582297		[learning rate: 0.0019013]
		[batch 20/20] avg loss: 0.46089511333609046		[learning rate: 0.0018967]
	Learning Rate: 0.00189671
	LOSS [training: 0.37023871109716006 | validation: 0.14762028405333738]
	TIME [epoch: 8.81 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46527053215833136		[learning rate: 0.0018921]
		[batch 20/20] avg loss: 0.34692790974211823		[learning rate: 0.0018875]
	Learning Rate: 0.00188753
	LOSS [training: 0.4060992209502248 | validation: 0.12481606296615044]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_394.pth
	Model improved!!!
EPOCH 395/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4831153172666198		[learning rate: 0.001883]
		[batch 20/20] avg loss: 0.4319175041422219		[learning rate: 0.0018784]
	Learning Rate: 0.00187841
	LOSS [training: 0.45751641070442084 | validation: 1.3734681108328353]
	TIME [epoch: 8.8 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6710361507182909		[learning rate: 0.0018739]
		[batch 20/20] avg loss: 0.3202804689430493		[learning rate: 0.0018693]
	Learning Rate: 0.00186932
	LOSS [training: 0.49565830983067005 | validation: 0.1495934957852125]
	TIME [epoch: 8.8 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5344540509287543		[learning rate: 0.0018648]
		[batch 20/20] avg loss: 0.344652134634891		[learning rate: 0.0018603]
	Learning Rate: 0.00186028
	LOSS [training: 0.43955309278182264 | validation: 0.12817447314835306]
	TIME [epoch: 8.82 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22610376907575777		[learning rate: 0.0018558]
		[batch 20/20] avg loss: 0.2620073154540348		[learning rate: 0.0018513]
	Learning Rate: 0.00185129
	LOSS [training: 0.24405554226489629 | validation: 0.15685958360410382]
	TIME [epoch: 8.81 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21765039010652315		[learning rate: 0.0018468]
		[batch 20/20] avg loss: 0.3446286410472036		[learning rate: 0.0018423]
	Learning Rate: 0.00184233
	LOSS [training: 0.28113951557686334 | validation: 0.2564278549881495]
	TIME [epoch: 8.79 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34378375823219076		[learning rate: 0.0018379]
		[batch 20/20] avg loss: 0.4849685444044532		[learning rate: 0.0018334]
	Learning Rate: 0.00183343
	LOSS [training: 0.4143761513183219 | validation: 0.2271123714271495]
	TIME [epoch: 8.8 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3900670967842663		[learning rate: 0.001829]
		[batch 20/20] avg loss: 0.5301996848448869		[learning rate: 0.0018246]
	Learning Rate: 0.00182456
	LOSS [training: 0.4601333908145765 | validation: 0.20783943847623498]
	TIME [epoch: 8.79 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5712778315723612		[learning rate: 0.0018201]
		[batch 20/20] avg loss: 0.4150044067159442		[learning rate: 0.0018157]
	Learning Rate: 0.00181574
	LOSS [training: 0.4931411191441527 | validation: 0.2141004846044567]
	TIME [epoch: 8.83 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27330314321413		[learning rate: 0.0018113]
		[batch 20/20] avg loss: 0.39693748266293094		[learning rate: 0.001807]
	Learning Rate: 0.00180696
	LOSS [training: 0.33512031293853045 | validation: 0.3761607080767489]
	TIME [epoch: 8.81 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48344405829041914		[learning rate: 0.0018026]
		[batch 20/20] avg loss: 0.356503545232858		[learning rate: 0.0017982]
	Learning Rate: 0.00179822
	LOSS [training: 0.4199738017616387 | validation: 0.10149408364659808]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_404.pth
	Model improved!!!
EPOCH 405/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38716099502509976		[learning rate: 0.0017939]
		[batch 20/20] avg loss: 0.31572689036885315		[learning rate: 0.0017895]
	Learning Rate: 0.00178952
	LOSS [training: 0.3514439426969765 | validation: 0.23414387693090377]
	TIME [epoch: 9.16 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30343265095138705		[learning rate: 0.0017852]
		[batch 20/20] avg loss: 0.3659528436166524		[learning rate: 0.0017809]
	Learning Rate: 0.00178087
	LOSS [training: 0.33469274728401976 | validation: 0.25408680509684417]
	TIME [epoch: 8.84 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24434203922770698		[learning rate: 0.0017766]
		[batch 20/20] avg loss: 0.42901911563924866		[learning rate: 0.0017723]
	Learning Rate: 0.00177226
	LOSS [training: 0.3366805774334778 | validation: 0.6914655140838913]
	TIME [epoch: 8.85 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39383958471468145		[learning rate: 0.001768]
		[batch 20/20] avg loss: 0.30420897065625263		[learning rate: 0.0017637]
	Learning Rate: 0.00176369
	LOSS [training: 0.349024277685467 | validation: 0.21152410053667273]
	TIME [epoch: 8.84 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2503036424708606		[learning rate: 0.0017594]
		[batch 20/20] avg loss: 0.34957929610837657		[learning rate: 0.0017552]
	Learning Rate: 0.00175516
	LOSS [training: 0.2999414692896186 | validation: 0.2224828358426538]
	TIME [epoch: 8.82 sec]
EPOCH 410/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4807394080597055		[learning rate: 0.0017509]
		[batch 20/20] avg loss: 0.3547742363959297		[learning rate: 0.0017467]
	Learning Rate: 0.00174667
	LOSS [training: 0.4177568222278175 | validation: 0.450022821467285]
	TIME [epoch: 8.83 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40724124290171504		[learning rate: 0.0017424]
		[batch 20/20] avg loss: 0.32617665652982214		[learning rate: 0.0017382]
	Learning Rate: 0.00173822
	LOSS [training: 0.36670894971576856 | validation: 0.22435580748006645]
	TIME [epoch: 8.83 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2906092910747491		[learning rate: 0.001734]
		[batch 20/20] avg loss: 0.5601486097864827		[learning rate: 0.0017298]
	Learning Rate: 0.00172982
	LOSS [training: 0.4253789504306159 | validation: 0.12058789371170796]
	TIME [epoch: 8.86 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2869467477781911		[learning rate: 0.0017256]
		[batch 20/20] avg loss: 0.47269489880056986		[learning rate: 0.0017215]
	Learning Rate: 0.00172145
	LOSS [training: 0.37982082328938044 | validation: 0.19839886927306674]
	TIME [epoch: 8.83 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4695609399613083		[learning rate: 0.0017173]
		[batch 20/20] avg loss: 0.4613891892085554		[learning rate: 0.0017131]
	Learning Rate: 0.00171313
	LOSS [training: 0.4654750645849319 | validation: 0.782461565322834]
	TIME [epoch: 8.83 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46517946891654516		[learning rate: 0.001709]
		[batch 20/20] avg loss: 0.41271969010179665		[learning rate: 0.0017048]
	Learning Rate: 0.00170484
	LOSS [training: 0.4389495795091709 | validation: 0.33358528084148625]
	TIME [epoch: 8.83 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3473849553268354		[learning rate: 0.0017007]
		[batch 20/20] avg loss: 0.38536626098952226		[learning rate: 0.0016966]
	Learning Rate: 0.0016966
	LOSS [training: 0.3663756081581788 | validation: 0.38215110091562815]
	TIME [epoch: 8.83 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.41890852953064756		[learning rate: 0.0016925]
		[batch 20/20] avg loss: 0.46576950225051805		[learning rate: 0.0016884]
	Learning Rate: 0.00168839
	LOSS [training: 0.44233901589058267 | validation: 0.28274966659517264]
	TIME [epoch: 8.86 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.431404371879775		[learning rate: 0.0016843]
		[batch 20/20] avg loss: 0.3374497333747029		[learning rate: 0.0016802]
	Learning Rate: 0.00168023
	LOSS [training: 0.3844270526272389 | validation: 0.5179765970082688]
	TIME [epoch: 8.84 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20694372895692997		[learning rate: 0.0016762]
		[batch 20/20] avg loss: 0.3402112202444261		[learning rate: 0.0016721]
	Learning Rate: 0.0016721
	LOSS [training: 0.27357747460067805 | validation: 0.24704935176853493]
	TIME [epoch: 8.83 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31984236563445917		[learning rate: 0.0016681]
		[batch 20/20] avg loss: 0.41049837861586597		[learning rate: 0.001664]
	Learning Rate: 0.00166402
	LOSS [training: 0.36517037212516257 | validation: 0.481708626078783]
	TIME [epoch: 8.84 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33361562337090145		[learning rate: 0.00166]
		[batch 20/20] avg loss: 0.2674791951443991		[learning rate: 0.001656]
	Learning Rate: 0.00165597
	LOSS [training: 0.30054740925765033 | validation: 0.29609012832077414]
	TIME [epoch: 8.83 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3108652960121478		[learning rate: 0.001652]
		[batch 20/20] avg loss: 0.26478694786079987		[learning rate: 0.001648]
	Learning Rate: 0.00164796
	LOSS [training: 0.28782612193647383 | validation: 0.16045535812726877]
	TIME [epoch: 8.85 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32341200184242047		[learning rate: 0.001644]
		[batch 20/20] avg loss: 0.28084396286804536		[learning rate: 0.00164]
	Learning Rate: 0.00163999
	LOSS [training: 0.3021279823552329 | validation: 0.17440475151626308]
	TIME [epoch: 8.84 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25300059967992633		[learning rate: 0.001636]
		[batch 20/20] avg loss: 0.49089369032135927		[learning rate: 0.0016321]
	Learning Rate: 0.00163206
	LOSS [training: 0.37194714500064274 | validation: 0.11312855092613902]
	TIME [epoch: 8.84 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.299739925375851		[learning rate: 0.0016281]
		[batch 20/20] avg loss: 0.2562044004726044		[learning rate: 0.0016242]
	Learning Rate: 0.00162417
	LOSS [training: 0.27797216292422766 | validation: 0.5036716883491743]
	TIME [epoch: 8.84 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38231484318044556		[learning rate: 0.0016202]
		[batch 20/20] avg loss: 0.3988937422014144		[learning rate: 0.0016163]
	Learning Rate: 0.00161632
	LOSS [training: 0.39060429269093 | validation: 0.3137227446876337]
	TIME [epoch: 8.84 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.681703037318091		[learning rate: 0.0016124]
		[batch 20/20] avg loss: 0.21158899794858366		[learning rate: 0.0016085]
	Learning Rate: 0.0016085
	LOSS [training: 0.44664601763333733 | validation: 0.19910224235119528]
	TIME [epoch: 8.85 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32635660191281934		[learning rate: 0.0016046]
		[batch 20/20] avg loss: 0.3206784521153482		[learning rate: 0.0016007]
	Learning Rate: 0.00160072
	LOSS [training: 0.32351752701408376 | validation: 0.15731081585748832]
	TIME [epoch: 8.85 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22285838987970985		[learning rate: 0.0015968]
		[batch 20/20] avg loss: 0.6635237240738936		[learning rate: 0.001593]
	Learning Rate: 0.00159298
	LOSS [training: 0.4431910569768017 | validation: 0.16128260217805562]
	TIME [epoch: 8.84 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21964175898393745		[learning rate: 0.0015891]
		[batch 20/20] avg loss: 0.29866312441923093		[learning rate: 0.0015853]
	Learning Rate: 0.00158528
	LOSS [training: 0.2591524417015842 | validation: 0.4813010278623443]
	TIME [epoch: 8.85 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36395264829051194		[learning rate: 0.0015814]
		[batch 20/20] avg loss: 0.31462336085093906		[learning rate: 0.0015776]
	Learning Rate: 0.00157761
	LOSS [training: 0.3392880045707255 | validation: 0.35665902795397264]
	TIME [epoch: 8.84 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4330100486757008		[learning rate: 0.0015738]
		[batch 20/20] avg loss: 0.3873629545978524		[learning rate: 0.00157]
	Learning Rate: 0.00156998
	LOSS [training: 0.41018650163677667 | validation: 0.22697060526121218]
	TIME [epoch: 8.86 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33301107035869665		[learning rate: 0.0015662]
		[batch 20/20] avg loss: 0.2786828682684982		[learning rate: 0.0015624]
	Learning Rate: 0.00156239
	LOSS [training: 0.30584696931359734 | validation: 0.3594822447762391]
	TIME [epoch: 8.86 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3552341672658597		[learning rate: 0.0015586]
		[batch 20/20] avg loss: 0.29174913103220124		[learning rate: 0.0015548]
	Learning Rate: 0.00155483
	LOSS [training: 0.32349164914903056 | validation: 0.14969341219952487]
	TIME [epoch: 8.84 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22206213309317985		[learning rate: 0.0015511]
		[batch 20/20] avg loss: 0.379154109857074		[learning rate: 0.0015473]
	Learning Rate: 0.00154732
	LOSS [training: 0.30060812147512694 | validation: 0.23536698687699903]
	TIME [epoch: 8.83 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3118037025824963		[learning rate: 0.0015436]
		[batch 20/20] avg loss: 0.19408242588403846		[learning rate: 0.0015398]
	Learning Rate: 0.00153983
	LOSS [training: 0.2529430642332674 | validation: 0.2385129866817522]
	TIME [epoch: 8.84 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27539092213150534		[learning rate: 0.0015361]
		[batch 20/20] avg loss: 0.3302199476331781		[learning rate: 0.0015324]
	Learning Rate: 0.00153239
	LOSS [training: 0.30280543488234174 | validation: 0.550363991944437]
	TIME [epoch: 8.84 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3235935959044426		[learning rate: 0.0015287]
		[batch 20/20] avg loss: 0.3806045422843368		[learning rate: 0.001525]
	Learning Rate: 0.00152498
	LOSS [training: 0.35209906909438976 | validation: 0.12312575692984093]
	TIME [epoch: 8.86 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3190544583722818		[learning rate: 0.0015213]
		[batch 20/20] avg loss: 0.26415767791173717		[learning rate: 0.0015176]
	Learning Rate: 0.0015176
	LOSS [training: 0.2916060681420094 | validation: 0.09415487085598855]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_439.pth
	Model improved!!!
EPOCH 440/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3028541237097628		[learning rate: 0.0015139]
		[batch 20/20] avg loss: 0.25889903295272804		[learning rate: 0.0015103]
	Learning Rate: 0.00151026
	LOSS [training: 0.28087657833124535 | validation: 0.149969507566446]
	TIME [epoch: 8.83 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2529000130599096		[learning rate: 0.0015066]
		[batch 20/20] avg loss: 0.26043431000263906		[learning rate: 0.001503]
	Learning Rate: 0.00150296
	LOSS [training: 0.2566671615312743 | validation: 0.09489985582416827]
	TIME [epoch: 8.83 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.35858243099865683		[learning rate: 0.0014993]
		[batch 20/20] avg loss: 0.3430567992257078		[learning rate: 0.0014957]
	Learning Rate: 0.00149569
	LOSS [training: 0.3508196151121823 | validation: 0.13890638991271237]
	TIME [epoch: 8.83 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30891133524159314		[learning rate: 0.0014921]
		[batch 20/20] avg loss: 0.3238035564432362		[learning rate: 0.0014885]
	Learning Rate: 0.00148846
	LOSS [training: 0.31635744584241476 | validation: 0.1885009236981396]
	TIME [epoch: 8.85 sec]
EPOCH 444/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30751474860540345		[learning rate: 0.0014849]
		[batch 20/20] avg loss: 0.28176906936852053		[learning rate: 0.0014813]
	Learning Rate: 0.00148126
	LOSS [training: 0.294641908986962 | validation: 0.1882715339150851]
	TIME [epoch: 8.85 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28874178300383857		[learning rate: 0.0014777]
		[batch 20/20] avg loss: 0.3020619037975656		[learning rate: 0.0014741]
	Learning Rate: 0.0014741
	LOSS [training: 0.2954018434007021 | validation: 0.20627871138668274]
	TIME [epoch: 8.83 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17679774524251446		[learning rate: 0.0014705]
		[batch 20/20] avg loss: 0.22545432434251378		[learning rate: 0.001467]
	Learning Rate: 0.00146697
	LOSS [training: 0.20112603479251412 | validation: 0.7401564209720788]
	TIME [epoch: 8.84 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30745845425662044		[learning rate: 0.0014634]
		[batch 20/20] avg loss: 0.3081770822131919		[learning rate: 0.0014599]
	Learning Rate: 0.00145988
	LOSS [training: 0.3078177682349061 | validation: 0.346500688902284]
	TIME [epoch: 8.84 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3157032873818315		[learning rate: 0.0014563]
		[batch 20/20] avg loss: 0.29728023455604047		[learning rate: 0.0014528]
	Learning Rate: 0.00145282
	LOSS [training: 0.3064917609689359 | validation: 0.44143924906408694]
	TIME [epoch: 8.84 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3364514387201595		[learning rate: 0.0014493]
		[batch 20/20] avg loss: 0.22593093031417522		[learning rate: 0.0014458]
	Learning Rate: 0.00144579
	LOSS [training: 0.2811911845171674 | validation: 0.1574048427818941]
	TIME [epoch: 8.83 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17624171929514393		[learning rate: 0.0014423]
		[batch 20/20] avg loss: 0.23761616349255532		[learning rate: 0.0014388]
	Learning Rate: 0.0014388
	LOSS [training: 0.20692894139384962 | validation: 0.4239572983655098]
	TIME [epoch: 8.83 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3313872764892852		[learning rate: 0.0014353]
		[batch 20/20] avg loss: 0.32642255599269515		[learning rate: 0.0014318]
	Learning Rate: 0.00143184
	LOSS [training: 0.32890491624099016 | validation: 0.08929632229957792]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_451.pth
	Model improved!!!
EPOCH 452/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23838608450987922		[learning rate: 0.0014284]
		[batch 20/20] avg loss: 0.2748611032434777		[learning rate: 0.0014249]
	Learning Rate: 0.00142492
	LOSS [training: 0.25662359387667844 | validation: 0.1569635483541717]
	TIME [epoch: 8.84 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28487591845323446		[learning rate: 0.0014215]
		[batch 20/20] avg loss: 0.29692233413651065		[learning rate: 0.001418]
	Learning Rate: 0.00141803
	LOSS [training: 0.2908991262948725 | validation: 0.21363965324360495]
	TIME [epoch: 8.85 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2903599426636127		[learning rate: 0.0014146]
		[batch 20/20] avg loss: 0.3193915386644205		[learning rate: 0.0014112]
	Learning Rate: 0.00141117
	LOSS [training: 0.3048757406640167 | validation: 0.21239065826078857]
	TIME [epoch: 8.83 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1875581402840119		[learning rate: 0.0014078]
		[batch 20/20] avg loss: 0.2623849437496597		[learning rate: 0.0014043]
	Learning Rate: 0.00140434
	LOSS [training: 0.2249715420168358 | validation: 0.15182728354696667]
	TIME [epoch: 8.83 sec]
EPOCH 456/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2337896322165566		[learning rate: 0.0014009]
		[batch 20/20] avg loss: 0.30725663546024506		[learning rate: 0.0013976]
	Learning Rate: 0.00139755
	LOSS [training: 0.2705231338384008 | validation: 0.15271193530937688]
	TIME [epoch: 8.83 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24558929928072665		[learning rate: 0.0013942]
		[batch 20/20] avg loss: 0.15532678098349134		[learning rate: 0.0013908]
	Learning Rate: 0.0013908
	LOSS [training: 0.200458040132109 | validation: 0.21602630340702347]
	TIME [epoch: 8.85 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19531904905863445		[learning rate: 0.0013874]
		[batch 20/20] avg loss: 0.2347161902781954		[learning rate: 0.0013841]
	Learning Rate: 0.00138407
	LOSS [training: 0.2150176196684149 | validation: 0.11776807262992282]
	TIME [epoch: 8.85 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2878302064161062		[learning rate: 0.0013807]
		[batch 20/20] avg loss: 0.2623198105590546		[learning rate: 0.0013774]
	Learning Rate: 0.00137738
	LOSS [training: 0.27507500848758043 | validation: 0.12670157182520164]
	TIME [epoch: 8.84 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15900978134321278		[learning rate: 0.001374]
		[batch 20/20] avg loss: 0.2905901593899801		[learning rate: 0.0013707]
	Learning Rate: 0.00137072
	LOSS [training: 0.22479997036659644 | validation: 0.3035081907304642]
	TIME [epoch: 8.82 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23609289561779404		[learning rate: 0.0013674]
		[batch 20/20] avg loss: 0.2186409358589032		[learning rate: 0.0013641]
	Learning Rate: 0.00136409
	LOSS [training: 0.2273669157383486 | validation: 0.16238184450530635]
	TIME [epoch: 8.83 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29897580439699023		[learning rate: 0.0013608]
		[batch 20/20] avg loss: 0.18846538031509055		[learning rate: 0.0013575]
	Learning Rate: 0.00135749
	LOSS [training: 0.24372059235604038 | validation: 0.4018802234298164]
	TIME [epoch: 8.83 sec]
EPOCH 463/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37363800620533205		[learning rate: 0.0013542]
		[batch 20/20] avg loss: 0.2327394275795863		[learning rate: 0.0013509]
	Learning Rate: 0.00135093
	LOSS [training: 0.30318871689245913 | validation: 0.17291658618929515]
	TIME [epoch: 8.85 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15564776265216282		[learning rate: 0.0013477]
		[batch 20/20] avg loss: 0.28170646828368723		[learning rate: 0.0013444]
	Learning Rate: 0.00134439
	LOSS [training: 0.218677115467925 | validation: 0.16040976277464508]
	TIME [epoch: 8.83 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25792231944768373		[learning rate: 0.0013411]
		[batch 20/20] avg loss: 0.2441266715774772		[learning rate: 0.0013379]
	Learning Rate: 0.00133789
	LOSS [training: 0.2510244955125805 | validation: 0.0933177009438826]
	TIME [epoch: 8.83 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26389899737191574		[learning rate: 0.0013347]
		[batch 20/20] avg loss: 0.19057321998903234		[learning rate: 0.0013314]
	Learning Rate: 0.00133142
	LOSS [training: 0.227236108680474 | validation: 0.6914023643430105]
	TIME [epoch: 8.84 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3798096811955759		[learning rate: 0.0013282]
		[batch 20/20] avg loss: 0.21979602004648174		[learning rate: 0.001325]
	Learning Rate: 0.00132498
	LOSS [training: 0.29980285062102885 | validation: 0.1215771273321143]
	TIME [epoch: 8.84 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16704317273412095		[learning rate: 0.0013218]
		[batch 20/20] avg loss: 0.31181191859445845		[learning rate: 0.0013186]
	Learning Rate: 0.00131858
	LOSS [training: 0.23942754566428973 | validation: 0.18596385755139522]
	TIME [epoch: 8.86 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18715115559303805		[learning rate: 0.0013154]
		[batch 20/20] avg loss: 0.25771089346897025		[learning rate: 0.0013122]
	Learning Rate: 0.0013122
	LOSS [training: 0.22243102453100422 | validation: 0.27496795617648273]
	TIME [epoch: 8.83 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2742929671257754		[learning rate: 0.001309]
		[batch 20/20] avg loss: 0.2507703947816227		[learning rate: 0.0013059]
	Learning Rate: 0.00130585
	LOSS [training: 0.26253168095369905 | validation: 0.1780750464094032]
	TIME [epoch: 8.84 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23291047747851232		[learning rate: 0.0013027]
		[batch 20/20] avg loss: 0.2266473281222011		[learning rate: 0.0012995]
	Learning Rate: 0.00129954
	LOSS [training: 0.22977890280035673 | validation: 0.16158910752550426]
	TIME [epoch: 8.85 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25536025739519463		[learning rate: 0.0012964]
		[batch 20/20] avg loss: 0.2300345246387135		[learning rate: 0.0012933]
	Learning Rate: 0.00129326
	LOSS [training: 0.24269739101695403 | validation: 0.18964435919872027]
	TIME [epoch: 8.84 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2894025481660121		[learning rate: 0.0012901]
		[batch 20/20] avg loss: 0.154304703613288		[learning rate: 0.001287]
	Learning Rate: 0.001287
	LOSS [training: 0.22185362588965002 | validation: 0.13733595942889876]
	TIME [epoch: 8.87 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2332790812598077		[learning rate: 0.0012839]
		[batch 20/20] avg loss: 0.27669895569316105		[learning rate: 0.0012808]
	Learning Rate: 0.00128078
	LOSS [training: 0.25498901847648436 | validation: 0.1405160335139662]
	TIME [epoch: 8.83 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2665645670430472		[learning rate: 0.0012777]
		[batch 20/20] avg loss: 0.22535091030969473		[learning rate: 0.0012746]
	Learning Rate: 0.00127458
	LOSS [training: 0.24595773867637094 | validation: 0.42254883461229453]
	TIME [epoch: 8.82 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2794201002867232		[learning rate: 0.0012715]
		[batch 20/20] avg loss: 0.20834844879921777		[learning rate: 0.0012684]
	Learning Rate: 0.00126842
	LOSS [training: 0.2438842745429705 | validation: 0.3354972090755704]
	TIME [epoch: 8.83 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2812534862841972		[learning rate: 0.0012653]
		[batch 20/20] avg loss: 0.24426479848364813		[learning rate: 0.0012623]
	Learning Rate: 0.00126229
	LOSS [training: 0.2627591423839227 | validation: 0.11254126786621982]
	TIME [epoch: 8.82 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18055876576016872		[learning rate: 0.0012592]
		[batch 20/20] avg loss: 0.3389460352746143		[learning rate: 0.0012562]
	Learning Rate: 0.00125618
	LOSS [training: 0.2597524005173915 | validation: 0.5824082186895274]
	TIME [epoch: 8.86 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25053978566161633		[learning rate: 0.0012531]
		[batch 20/20] avg loss: 0.24430047192885382		[learning rate: 0.0012501]
	Learning Rate: 0.00125011
	LOSS [training: 0.24742012879523512 | validation: 0.11483576235834445]
	TIME [epoch: 8.83 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16259272277020384		[learning rate: 0.0012471]
		[batch 20/20] avg loss: 0.2333070020852785		[learning rate: 0.0012441]
	Learning Rate: 0.00124406
	LOSS [training: 0.1979498624277412 | validation: 0.6755410538547144]
	TIME [epoch: 8.83 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21346839183322106		[learning rate: 0.0012411]
		[batch 20/20] avg loss: 0.3174965782510785		[learning rate: 0.001238]
	Learning Rate: 0.00123805
	LOSS [training: 0.2654824850421498 | validation: 0.257911011520294]
	TIME [epoch: 8.82 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17214275185911834		[learning rate: 0.001235]
		[batch 20/20] avg loss: 0.2211536567962277		[learning rate: 0.0012321]
	Learning Rate: 0.00123206
	LOSS [training: 0.19664820432767302 | validation: 0.13597624966008853]
	TIME [epoch: 8.82 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24590867440749659		[learning rate: 0.0012291]
		[batch 20/20] avg loss: 0.22078261599958263		[learning rate: 0.0012261]
	Learning Rate: 0.0012261
	LOSS [training: 0.23334564520353956 | validation: 0.15639687553525897]
	TIME [epoch: 8.85 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23003302224858113		[learning rate: 0.0012231]
		[batch 20/20] avg loss: 0.2326700530661451		[learning rate: 0.0012202]
	Learning Rate: 0.00122017
	LOSS [training: 0.2313515376573631 | validation: 0.4066678735844217]
	TIME [epoch: 8.83 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23361328864956912		[learning rate: 0.0012172]
		[batch 20/20] avg loss: 0.29901041204708895		[learning rate: 0.0012143]
	Learning Rate: 0.00121427
	LOSS [training: 0.26631185034832894 | validation: 0.19410788887145114]
	TIME [epoch: 8.83 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23341291339644124		[learning rate: 0.0012113]
		[batch 20/20] avg loss: 0.2666414538351212		[learning rate: 0.0012084]
	Learning Rate: 0.0012084
	LOSS [training: 0.2500271836157812 | validation: 0.2439103110563756]
	TIME [epoch: 8.83 sec]
EPOCH 487/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27191085679393573		[learning rate: 0.0012055]
		[batch 20/20] avg loss: 0.21025971492001383		[learning rate: 0.0012026]
	Learning Rate: 0.00120256
	LOSS [training: 0.24108528585697475 | validation: 0.09144091987872618]
	TIME [epoch: 8.82 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22783662608141939		[learning rate: 0.0011996]
		[batch 20/20] avg loss: 0.6178960872103937		[learning rate: 0.0011967]
	Learning Rate: 0.00119674
	LOSS [training: 0.42286635664590655 | validation: 0.42289493921482135]
	TIME [epoch: 8.84 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26516319755066975		[learning rate: 0.0011938]
		[batch 20/20] avg loss: 0.2558532908121556		[learning rate: 0.001191]
	Learning Rate: 0.00119095
	LOSS [training: 0.2605082441814127 | validation: 0.17064112070628928]
	TIME [epoch: 8.82 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27413989971207636		[learning rate: 0.0011881]
		[batch 20/20] avg loss: 0.18388229062084574		[learning rate: 0.0011852]
	Learning Rate: 0.00118519
	LOSS [training: 0.22901109516646107 | validation: 0.47155409926432557]
	TIME [epoch: 8.83 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2677355021691165		[learning rate: 0.0011823]
		[batch 20/20] avg loss: 0.16634427307415092		[learning rate: 0.0011795]
	Learning Rate: 0.00117946
	LOSS [training: 0.21703988762163368 | validation: 0.17525827478057934]
	TIME [epoch: 8.83 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16876738432650124		[learning rate: 0.0011766]
		[batch 20/20] avg loss: 0.19303263996919898		[learning rate: 0.0011738]
	Learning Rate: 0.00117376
	LOSS [training: 0.18090001214785012 | validation: 0.20368081816107073]
	TIME [epoch: 8.83 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23746189968417095		[learning rate: 0.0011709]
		[batch 20/20] avg loss: 0.2588785348501204		[learning rate: 0.0011681]
	Learning Rate: 0.00116808
	LOSS [training: 0.2481702172671457 | validation: 0.13125242836750312]
	TIME [epoch: 8.84 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15928533025196204		[learning rate: 0.0011653]
		[batch 20/20] avg loss: 0.1961844365341504		[learning rate: 0.0011624]
	Learning Rate: 0.00116243
	LOSS [training: 0.17773488339305624 | validation: 0.4174953627958982]
	TIME [epoch: 8.83 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.227954941343636		[learning rate: 0.0011596]
		[batch 20/20] avg loss: 0.32918130532424583		[learning rate: 0.0011568]
	Learning Rate: 0.00115681
	LOSS [training: 0.278568123333941 | validation: 0.26855508836696307]
	TIME [epoch: 8.82 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19664479622122003		[learning rate: 0.001154]
		[batch 20/20] avg loss: 0.23107152860888594		[learning rate: 0.0011512]
	Learning Rate: 0.00115122
	LOSS [training: 0.21385816241505298 | validation: 0.0902183488327136]
	TIME [epoch: 8.81 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2185151313654039		[learning rate: 0.0011484]
		[batch 20/20] avg loss: 0.30063269857383884		[learning rate: 0.0011457]
	Learning Rate: 0.00114565
	LOSS [training: 0.25957391496962134 | validation: 0.22262115973819083]
	TIME [epoch: 8.82 sec]
EPOCH 498/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18075025072030765		[learning rate: 0.0011429]
		[batch 20/20] avg loss: 0.17416219916723374		[learning rate: 0.0011401]
	Learning Rate: 0.00114011
	LOSS [training: 0.17745622494377067 | validation: 0.30943729463751735]
	TIME [epoch: 8.84 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20761206755457096		[learning rate: 0.0011374]
		[batch 20/20] avg loss: 0.2265317017160458		[learning rate: 0.0011346]
	Learning Rate: 0.0011346
	LOSS [training: 0.2170718846353084 | validation: 0.2808236828953987]
	TIME [epoch: 8.84 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1982963502615214		[learning rate: 0.0011319]
		[batch 20/20] avg loss: 0.2149640546940396		[learning rate: 0.0011291]
	Learning Rate: 0.00112911
	LOSS [training: 0.2066302024777805 | validation: 0.3824441094759522]
	TIME [epoch: 8.83 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1817358939284193		[learning rate: 0.0011264]
		[batch 20/20] avg loss: 0.20374294507209312		[learning rate: 0.0011237]
	Learning Rate: 0.00112365
	LOSS [training: 0.19273941950025622 | validation: 0.41947469821912176]
	TIME [epoch: 8.83 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19019437134176398		[learning rate: 0.0011209]
		[batch 20/20] avg loss: 0.18774504180045662		[learning rate: 0.0011182]
	Learning Rate: 0.00111822
	LOSS [training: 0.1889697065711103 | validation: 0.1897959072819978]
	TIME [epoch: 8.82 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18662891117264235		[learning rate: 0.0011155]
		[batch 20/20] avg loss: 0.18620865402183712		[learning rate: 0.0011128]
	Learning Rate: 0.00111281
	LOSS [training: 0.1864187825972397 | validation: 0.09165165309552965]
	TIME [epoch: 8.82 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20871838894508615		[learning rate: 0.0011101]
		[batch 20/20] avg loss: 0.30194429973149284		[learning rate: 0.0011074]
	Learning Rate: 0.00110743
	LOSS [training: 0.2553313443382895 | validation: 0.2506182028054103]
	TIME [epoch: 8.84 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18654178453460052		[learning rate: 0.0011047]
		[batch 20/20] avg loss: 0.242283593095111		[learning rate: 0.0011021]
	Learning Rate: 0.00110207
	LOSS [training: 0.21441268881485578 | validation: 0.0918284015234836]
	TIME [epoch: 8.82 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2913702440231264		[learning rate: 0.0010994]
		[batch 20/20] avg loss: 0.33887412342071926		[learning rate: 0.0010967]
	Learning Rate: 0.00109674
	LOSS [training: 0.3151221837219228 | validation: 0.10139975867214376]
	TIME [epoch: 8.82 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14674171935755292		[learning rate: 0.0010941]
		[batch 20/20] avg loss: 0.2507957076942707		[learning rate: 0.0010914]
	Learning Rate: 0.00109144
	LOSS [training: 0.1987687135259118 | validation: 0.5516111692990039]
	TIME [epoch: 8.83 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32002451424999323		[learning rate: 0.0010888]
		[batch 20/20] avg loss: 0.2162375578060088		[learning rate: 0.0010862]
	Learning Rate: 0.00108616
	LOSS [training: 0.26813103602800104 | validation: 0.290886449020714]
	TIME [epoch: 8.83 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28458175686161213		[learning rate: 0.0010835]
		[batch 20/20] avg loss: 0.2367766824926733		[learning rate: 0.0010809]
	Learning Rate: 0.00108091
	LOSS [training: 0.26067921967714275 | validation: 0.21145868000051046]
	TIME [epoch: 8.86 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20316506442646093		[learning rate: 0.0010783]
		[batch 20/20] avg loss: 0.18876026549708017		[learning rate: 0.0010757]
	Learning Rate: 0.00107568
	LOSS [training: 0.19596266496177048 | validation: 0.06674640183917462]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_510.pth
	Model improved!!!
EPOCH 511/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1280719914210599		[learning rate: 0.0010731]
		[batch 20/20] avg loss: 0.2986219820562998		[learning rate: 0.0010705]
	Learning Rate: 0.00107048
	LOSS [training: 0.21334698673867983 | validation: 0.4384152704550359]
	TIME [epoch: 8.84 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23661712147380828		[learning rate: 0.0010679]
		[batch 20/20] avg loss: 0.1827242430623827		[learning rate: 0.0010653]
	Learning Rate: 0.0010653
	LOSS [training: 0.20967068226809552 | validation: 0.2885656478046332]
	TIME [epoch: 8.84 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23106537802248842		[learning rate: 0.0010627]
		[batch 20/20] avg loss: 0.1812210484269169		[learning rate: 0.0010602]
	Learning Rate: 0.00106015
	LOSS [training: 0.2061432132247026 | validation: 0.18051766826791538]
	TIME [epoch: 8.84 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1653368962465857		[learning rate: 0.0010576]
		[batch 20/20] avg loss: 0.22827222283943333		[learning rate: 0.001055]
	Learning Rate: 0.00105503
	LOSS [training: 0.1968045595430095 | validation: 0.3179443038044225]
	TIME [epoch: 8.85 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21725763748479315		[learning rate: 0.0010525]
		[batch 20/20] avg loss: 0.16135323545928615		[learning rate: 0.0010499]
	Learning Rate: 0.00104992
	LOSS [training: 0.18930543647203965 | validation: 0.13180976086580848]
	TIME [epoch: 8.84 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2069694622479524		[learning rate: 0.0010474]
		[batch 20/20] avg loss: 0.20072292916173526		[learning rate: 0.0010448]
	Learning Rate: 0.00104485
	LOSS [training: 0.20384619570484386 | validation: 0.4468515310300132]
	TIME [epoch: 8.82 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26233734206655596		[learning rate: 0.0010423]
		[batch 20/20] avg loss: 0.2366194105373009		[learning rate: 0.0010398]
	Learning Rate: 0.00103979
	LOSS [training: 0.24947837630192843 | validation: 0.07028275058918915]
	TIME [epoch: 8.83 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1863164660244982		[learning rate: 0.0010373]
		[batch 20/20] avg loss: 0.18140350795874563		[learning rate: 0.0010348]
	Learning Rate: 0.00103477
	LOSS [training: 0.18385998699162193 | validation: 0.1412255430368361]
	TIME [epoch: 8.83 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3170022610691841		[learning rate: 0.0010323]
		[batch 20/20] avg loss: 0.11643980259967394		[learning rate: 0.0010298]
	Learning Rate: 0.00102976
	LOSS [training: 0.21672103183442898 | validation: 0.7525645120147625]
	TIME [epoch: 8.86 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2282921878555427		[learning rate: 0.0010273]
		[batch 20/20] avg loss: 0.30172888793720715		[learning rate: 0.0010248]
	Learning Rate: 0.00102478
	LOSS [training: 0.26501053789637496 | validation: 0.2552776821687255]
	TIME [epoch: 8.83 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13773244522623135		[learning rate: 0.0010223]
		[batch 20/20] avg loss: 0.23147058854861813		[learning rate: 0.0010198]
	Learning Rate: 0.00101983
	LOSS [training: 0.18460151688742474 | validation: 0.1015121137759647]
	TIME [epoch: 8.82 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15306506849196524		[learning rate: 0.0010174]
		[batch 20/20] avg loss: 0.19879114818751728		[learning rate: 0.0010149]
	Learning Rate: 0.00101489
	LOSS [training: 0.17592810833974126 | validation: 0.19361755301073386]
	TIME [epoch: 8.82 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21640830247355897		[learning rate: 0.0010124]
		[batch 20/20] avg loss: 0.16621938187151133		[learning rate: 0.00101]
	Learning Rate: 0.00100999
	LOSS [training: 0.19131384217253516 | validation: 0.08960956160824037]
	TIME [epoch: 8.83 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14950253652998766		[learning rate: 0.0010075]
		[batch 20/20] avg loss: 0.22557722234880603		[learning rate: 0.0010051]
	Learning Rate: 0.0010051
	LOSS [training: 0.18753987943939684 | validation: 0.07033651892108893]
	TIME [epoch: 8.85 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2550306582145588		[learning rate: 0.0010027]
		[batch 20/20] avg loss: 0.2547539614806507		[learning rate: 0.0010002]
	Learning Rate: 0.00100024
	LOSS [training: 0.2548923098476047 | validation: 0.06960122572464675]
	TIME [epoch: 8.84 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1381476262120665		[learning rate: 0.00099782]
		[batch 20/20] avg loss: 0.2727130299972091		[learning rate: 0.00099541]
	Learning Rate: 0.000995405
	LOSS [training: 0.20543032810463782 | validation: 0.2058454027073761]
	TIME [epoch: 8.83 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21054032816226959		[learning rate: 0.000993]
		[batch 20/20] avg loss: 0.18530013919292923		[learning rate: 0.00099059]
	Learning Rate: 0.000990592
	LOSS [training: 0.1979202336775994 | validation: 0.22106420349179345]
	TIME [epoch: 8.83 sec]
EPOCH 528/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2092062065479074		[learning rate: 0.00098819]
		[batch 20/20] avg loss: 0.19359093028510302		[learning rate: 0.0009858]
	Learning Rate: 0.000985801
	LOSS [training: 0.20139856841650516 | validation: 0.17766802311884367]
	TIME [epoch: 8.82 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14861547927913282		[learning rate: 0.00098341]
		[batch 20/20] avg loss: 0.18035145788037146		[learning rate: 0.00098103]
	Learning Rate: 0.000981034
	LOSS [training: 0.16448346857975213 | validation: 0.18569587154464026]
	TIME [epoch: 8.85 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15324112421886812		[learning rate: 0.00097866]
		[batch 20/20] avg loss: 0.2244523974179065		[learning rate: 0.00097629]
	Learning Rate: 0.00097629
	LOSS [training: 0.1888467608183873 | validation: 0.17683595359101892]
	TIME [epoch: 8.83 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18971311076126568		[learning rate: 0.00097393]
		[batch 20/20] avg loss: 0.1452093254687241		[learning rate: 0.00097157]
	Learning Rate: 0.000971569
	LOSS [training: 0.16746121811499484 | validation: 0.2720000769009204]
	TIME [epoch: 8.82 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14518304703012497		[learning rate: 0.00096922]
		[batch 20/20] avg loss: 0.1606052613161499		[learning rate: 0.00096687]
	Learning Rate: 0.000966871
	LOSS [training: 0.15289415417313748 | validation: 0.35897419634535904]
	TIME [epoch: 8.82 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2018629716458192		[learning rate: 0.00096453]
		[batch 20/20] avg loss: 0.20508639808654955		[learning rate: 0.00096219]
	Learning Rate: 0.000962195
	LOSS [training: 0.20347468486618436 | validation: 0.21526800625805226]
	TIME [epoch: 8.82 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17028495622709927		[learning rate: 0.00095987]
		[batch 20/20] avg loss: 0.20559129324662853		[learning rate: 0.00095754]
	Learning Rate: 0.000957542
	LOSS [training: 0.1879381247368639 | validation: 0.11279305622847383]
	TIME [epoch: 8.85 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1411026357069336		[learning rate: 0.00095522]
		[batch 20/20] avg loss: 0.24615719691249743		[learning rate: 0.00095291]
	Learning Rate: 0.000952912
	LOSS [training: 0.19362991630971554 | validation: 0.1503814738647112]
	TIME [epoch: 8.83 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11658927874447403		[learning rate: 0.0009506]
		[batch 20/20] avg loss: 0.19268322189338422		[learning rate: 0.0009483]
	Learning Rate: 0.000948304
	LOSS [training: 0.15463625031892914 | validation: 0.22882342344765677]
	TIME [epoch: 8.83 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1531720831023771		[learning rate: 0.00094601]
		[batch 20/20] avg loss: 0.1648700405502455		[learning rate: 0.00094372]
	Learning Rate: 0.000943718
	LOSS [training: 0.15902106182631134 | validation: 0.5104845966956625]
	TIME [epoch: 8.82 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20590318772188324		[learning rate: 0.00094143]
		[batch 20/20] avg loss: 0.21984372354367773		[learning rate: 0.00093915]
	Learning Rate: 0.000939154
	LOSS [training: 0.21287345563278048 | validation: 0.5835343921763425]
	TIME [epoch: 8.82 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2248239081843327		[learning rate: 0.00093688]
		[batch 20/20] avg loss: 0.1414639846731444		[learning rate: 0.00093461]
	Learning Rate: 0.000934613
	LOSS [training: 0.18314394642873855 | validation: 0.08553404235974063]
	TIME [epoch: 8.85 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13365194853790902		[learning rate: 0.00093235]
		[batch 20/20] avg loss: 0.17169863041858727		[learning rate: 0.00093009]
	Learning Rate: 0.000930093
	LOSS [training: 0.15267528947824818 | validation: 0.2911820205627119]
	TIME [epoch: 8.82 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17051094583123266		[learning rate: 0.00092784]
		[batch 20/20] avg loss: 0.19619158517028476		[learning rate: 0.0009256]
	Learning Rate: 0.000925595
	LOSS [training: 0.1833512655007587 | validation: 0.10792103576269348]
	TIME [epoch: 8.82 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20430861023516375		[learning rate: 0.00092335]
		[batch 20/20] avg loss: 0.15976565984150579		[learning rate: 0.00092112]
	Learning Rate: 0.000921119
	LOSS [training: 0.1820371350383348 | validation: 0.08705503459915107]
	TIME [epoch: 8.81 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1695426619043058		[learning rate: 0.00091889]
		[batch 20/20] avg loss: 0.18183469340047376		[learning rate: 0.00091666]
	Learning Rate: 0.000916665
	LOSS [training: 0.17568867765238977 | validation: 0.17150107420195293]
	TIME [epoch: 8.81 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14069067835479784		[learning rate: 0.00091445]
		[batch 20/20] avg loss: 0.1782746841322594		[learning rate: 0.00091223]
	Learning Rate: 0.000912232
	LOSS [training: 0.15948268124352863 | validation: 0.46516645788831934]
	TIME [epoch: 8.83 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24596517561403033		[learning rate: 0.00091002]
		[batch 20/20] avg loss: 0.20827204965754084		[learning rate: 0.00090782]
	Learning Rate: 0.000907821
	LOSS [training: 0.2271186126357856 | validation: 0.30616102455630567]
	TIME [epoch: 8.83 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16245664693929776		[learning rate: 0.00090562]
		[batch 20/20] avg loss: 0.14142639168705357		[learning rate: 0.00090343]
	Learning Rate: 0.00090343
	LOSS [training: 0.15194151931317565 | validation: 0.29693722394517935]
	TIME [epoch: 8.81 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2114274546482084		[learning rate: 0.00090124]
		[batch 20/20] avg loss: 0.1566435728477617		[learning rate: 0.00089906]
	Learning Rate: 0.000899062
	LOSS [training: 0.18403551374798505 | validation: 0.32655097359969654]
	TIME [epoch: 8.82 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16463957502837626		[learning rate: 0.00089689]
		[batch 20/20] avg loss: 0.14125496048967887		[learning rate: 0.00089471]
	Learning Rate: 0.000894714
	LOSS [training: 0.1529472677590276 | validation: 0.11730269742952289]
	TIME [epoch: 8.81 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17948503682461908		[learning rate: 0.00089255]
		[batch 20/20] avg loss: 0.10591944325157195		[learning rate: 0.00089039]
	Learning Rate: 0.000890387
	LOSS [training: 0.14270224003809556 | validation: 0.13128841344621311]
	TIME [epoch: 8.83 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12179198968233715		[learning rate: 0.00088823]
		[batch 20/20] avg loss: 0.17428648565576116		[learning rate: 0.00088608]
	Learning Rate: 0.000886081
	LOSS [training: 0.14803923766904917 | validation: 0.1457134122499244]
	TIME [epoch: 8.82 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2541384694446855		[learning rate: 0.00088394]
		[batch 20/20] avg loss: 0.13476935047579822		[learning rate: 0.0008818]
	Learning Rate: 0.000881797
	LOSS [training: 0.19445390996024184 | validation: 0.15247497968942253]
	TIME [epoch: 8.82 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14389336835151872		[learning rate: 0.00087966]
		[batch 20/20] avg loss: 0.1310525799085272		[learning rate: 0.00087753]
	Learning Rate: 0.000877532
	LOSS [training: 0.137472974130023 | validation: 0.1979879728630907]
	TIME [epoch: 8.82 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10750243565465896		[learning rate: 0.00087541]
		[batch 20/20] avg loss: 0.22140738637273297		[learning rate: 0.00087329]
	Learning Rate: 0.000873289
	LOSS [training: 0.16445491101369597 | validation: 0.0841361163640004]
	TIME [epoch: 8.82 sec]
EPOCH 554/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13431047320149747		[learning rate: 0.00087117]
		[batch 20/20] avg loss: 0.20045900532516042		[learning rate: 0.00086907]
	Learning Rate: 0.000869066
	LOSS [training: 0.16738473926332892 | validation: 0.1953774096511508]
	TIME [epoch: 8.82 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2163954748985622		[learning rate: 0.00086696]
		[batch 20/20] avg loss: 0.16628736706490432		[learning rate: 0.00086486]
	Learning Rate: 0.000864863
	LOSS [training: 0.19134142098173323 | validation: 0.2584837107515999]
	TIME [epoch: 8.83 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15109039012680192		[learning rate: 0.00086277]
		[batch 20/20] avg loss: 0.1536943706825787		[learning rate: 0.00086068]
	Learning Rate: 0.000860681
	LOSS [training: 0.15239238040469033 | validation: 0.1690003440775848]
	TIME [epoch: 8.81 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22282013899006467		[learning rate: 0.0008586]
		[batch 20/20] avg loss: 0.12561732355746838		[learning rate: 0.00085652]
	Learning Rate: 0.000856519
	LOSS [training: 0.17421873127376655 | validation: 0.08310495316461392]
	TIME [epoch: 8.81 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2054664485386672		[learning rate: 0.00085445]
		[batch 20/20] avg loss: 0.25803307540986004		[learning rate: 0.00085238]
	Learning Rate: 0.000852377
	LOSS [training: 0.2317497619742636 | validation: 0.06991137434147625]
	TIME [epoch: 8.81 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1574073426385707		[learning rate: 0.00085031]
		[batch 20/20] avg loss: 0.11930003310910027		[learning rate: 0.00084825]
	Learning Rate: 0.000848255
	LOSS [training: 0.13835368787383545 | validation: 0.05623674219661573]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_559.pth
	Model improved!!!
EPOCH 560/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2217038931562542		[learning rate: 0.0008462]
		[batch 20/20] avg loss: 0.1964950655151986		[learning rate: 0.00084415]
	Learning Rate: 0.000844153
	LOSS [training: 0.20909947933572642 | validation: 0.39559466745211874]
	TIME [epoch: 8.83 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21065388304458432		[learning rate: 0.00084211]
		[batch 20/20] avg loss: 0.1611190844751796		[learning rate: 0.00084007]
	Learning Rate: 0.000840071
	LOSS [training: 0.18588648375988198 | validation: 0.12390506221545254]
	TIME [epoch: 8.8 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15046339684958682		[learning rate: 0.00083804]
		[batch 20/20] avg loss: 0.17561966640985627		[learning rate: 0.00083601]
	Learning Rate: 0.000836008
	LOSS [training: 0.16304153162972157 | validation: 0.10106547486462704]
	TIME [epoch: 8.8 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18951500472959074		[learning rate: 0.00083398]
		[batch 20/20] avg loss: 0.30050910722425633		[learning rate: 0.00083197]
	Learning Rate: 0.000831965
	LOSS [training: 0.24501205597692355 | validation: 0.15339650343013225]
	TIME [epoch: 8.81 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1578024536623067		[learning rate: 0.00082995]
		[batch 20/20] avg loss: 0.21005250527724334		[learning rate: 0.00082794]
	Learning Rate: 0.000827942
	LOSS [training: 0.183927479469775 | validation: 0.06714239501118564]
	TIME [epoch: 8.82 sec]
EPOCH 565/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2707629258788356		[learning rate: 0.00082594]
		[batch 20/20] avg loss: 0.1845830626227064		[learning rate: 0.00082394]
	Learning Rate: 0.000823938
	LOSS [training: 0.22767299425077106 | validation: 0.2011664587490377]
	TIME [epoch: 8.83 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14816481701679662		[learning rate: 0.00082194]
		[batch 20/20] avg loss: 0.13744944324646785		[learning rate: 0.00081995]
	Learning Rate: 0.000819954
	LOSS [training: 0.14280713013163224 | validation: 0.1323242272961827]
	TIME [epoch: 8.83 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16686491129534195		[learning rate: 0.00081797]
		[batch 20/20] avg loss: 0.16417884585443315		[learning rate: 0.00081599]
	Learning Rate: 0.000815989
	LOSS [training: 0.16552187857488757 | validation: 0.14643709763699359]
	TIME [epoch: 8.82 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1741237440548539		[learning rate: 0.00081401]
		[batch 20/20] avg loss: 0.1707524578407583		[learning rate: 0.00081204]
	Learning Rate: 0.000812043
	LOSS [training: 0.1724381009478061 | validation: 0.06417904599022232]
	TIME [epoch: 8.82 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18133220814011092		[learning rate: 0.00081008]
		[batch 20/20] avg loss: 0.16884528039915944		[learning rate: 0.00080812]
	Learning Rate: 0.000808116
	LOSS [training: 0.17508874426963522 | validation: 0.30583178406597294]
	TIME [epoch: 8.81 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16802524198582183		[learning rate: 0.00080616]
		[batch 20/20] avg loss: 0.17738737121269232		[learning rate: 0.00080421]
	Learning Rate: 0.000804208
	LOSS [training: 0.17270630659925706 | validation: 0.31161072119033223]
	TIME [epoch: 8.83 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19901251949109422		[learning rate: 0.00080226]
		[batch 20/20] avg loss: 0.11279422924157652		[learning rate: 0.00080032]
	Learning Rate: 0.000800319
	LOSS [training: 0.1559033743663354 | validation: 0.1688635227151724]
	TIME [epoch: 8.8 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18506232318408655		[learning rate: 0.00079838]
		[batch 20/20] avg loss: 0.22119717480032194		[learning rate: 0.00079645]
	Learning Rate: 0.000796449
	LOSS [training: 0.2031297489922043 | validation: 0.3973268762523651]
	TIME [epoch: 8.81 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1912118707414641		[learning rate: 0.00079452]
		[batch 20/20] avg loss: 0.14573515655176736		[learning rate: 0.0007926]
	Learning Rate: 0.000792597
	LOSS [training: 0.16847351364661572 | validation: 0.12204847930410717]
	TIME [epoch: 8.82 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13512497674015259		[learning rate: 0.00079068]
		[batch 20/20] avg loss: 0.1565199096582917		[learning rate: 0.00078876]
	Learning Rate: 0.000788765
	LOSS [training: 0.14582244319922216 | validation: 0.14729701433208445]
	TIME [epoch: 8.81 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23163883223791518		[learning rate: 0.00078686]
		[batch 20/20] avg loss: 0.13985719211049458		[learning rate: 0.00078495]
	Learning Rate: 0.00078495
	LOSS [training: 0.18574801217420484 | validation: 0.19777445061521667]
	TIME [epoch: 8.84 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13781760976208055		[learning rate: 0.00078305]
		[batch 20/20] avg loss: 0.14591780326120352		[learning rate: 0.00078115]
	Learning Rate: 0.000781154
	LOSS [training: 0.14186770651164204 | validation: 0.11632364166973155]
	TIME [epoch: 8.82 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12845276955564033		[learning rate: 0.00077926]
		[batch 20/20] avg loss: 0.16847403602810926		[learning rate: 0.00077738]
	Learning Rate: 0.000777377
	LOSS [training: 0.1484634027918748 | validation: 0.26156452906562805]
	TIME [epoch: 8.81 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2432686628539705		[learning rate: 0.00077549]
		[batch 20/20] avg loss: 0.16755248819133184		[learning rate: 0.00077362]
	Learning Rate: 0.000773618
	LOSS [training: 0.20541057552265124 | validation: 0.15115197605463765]
	TIME [epoch: 8.81 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1413839497892619		[learning rate: 0.00077174]
		[batch 20/20] avg loss: 0.1261808611213314		[learning rate: 0.00076988]
	Learning Rate: 0.000769877
	LOSS [training: 0.13378240545529668 | validation: 0.14413722643266627]
	TIME [epoch: 8.81 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1171307479506328		[learning rate: 0.00076801]
		[batch 20/20] avg loss: 0.1438026142706421		[learning rate: 0.00076615]
	Learning Rate: 0.000766154
	LOSS [training: 0.13046668111063747 | validation: 0.1334326010463598]
	TIME [epoch: 8.84 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17519816601541027		[learning rate: 0.0007643]
		[batch 20/20] avg loss: 0.15831544164764025		[learning rate: 0.00076245]
	Learning Rate: 0.000762448
	LOSS [training: 0.16675680383152525 | validation: 0.05552616741246686]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_581.pth
	Model improved!!!
EPOCH 582/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12650090858697202		[learning rate: 0.0007606]
		[batch 20/20] avg loss: 0.11672111060220344		[learning rate: 0.00075876]
	Learning Rate: 0.000758761
	LOSS [training: 0.12161100959458777 | validation: 0.26667526676256825]
	TIME [epoch: 8.81 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1931753526173785		[learning rate: 0.00075692]
		[batch 20/20] avg loss: 0.11386408407377199		[learning rate: 0.00075509]
	Learning Rate: 0.000755092
	LOSS [training: 0.1535197183455752 | validation: 0.11314789175961866]
	TIME [epoch: 8.81 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12373101749646846		[learning rate: 0.00075326]
		[batch 20/20] avg loss: 0.14343022631257976		[learning rate: 0.00075144]
	Learning Rate: 0.000751441
	LOSS [training: 0.1335806219045241 | validation: 0.1214433552849363]
	TIME [epoch: 8.8 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16951120451873108		[learning rate: 0.00074962]
		[batch 20/20] avg loss: 0.13829866995183995		[learning rate: 0.00074781]
	Learning Rate: 0.000747807
	LOSS [training: 0.15390493723528553 | validation: 0.09362112028701144]
	TIME [epoch: 8.83 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12496677081927018		[learning rate: 0.000746]
		[batch 20/20] avg loss: 0.18123610228388398		[learning rate: 0.00074419]
	Learning Rate: 0.000744191
	LOSS [training: 0.1531014365515771 | validation: 0.15832939385959782]
	TIME [epoch: 8.81 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1265170971916975		[learning rate: 0.00074239]
		[batch 20/20] avg loss: 0.14047378345164954		[learning rate: 0.00074059]
	Learning Rate: 0.000740592
	LOSS [training: 0.13349544032167351 | validation: 0.1793658437640151]
	TIME [epoch: 8.8 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1096830011941646		[learning rate: 0.0007388]
		[batch 20/20] avg loss: 0.16060128442800725		[learning rate: 0.00073701]
	Learning Rate: 0.000737011
	LOSS [training: 0.13514214281108594 | validation: 0.3531128123328515]
	TIME [epoch: 8.81 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1508820765780003		[learning rate: 0.00073523]
		[batch 20/20] avg loss: 0.1568967065489972		[learning rate: 0.00073345]
	Learning Rate: 0.000733446
	LOSS [training: 0.15388939156349873 | validation: 0.239297493051488]
	TIME [epoch: 8.8 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1419511270188147		[learning rate: 0.00073167]
		[batch 20/20] avg loss: 0.11294901948402755		[learning rate: 0.0007299]
	Learning Rate: 0.0007299
	LOSS [training: 0.12745007325142113 | validation: 0.20054694579146717]
	TIME [epoch: 8.83 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14449597186646818		[learning rate: 0.00072813]
		[batch 20/20] avg loss: 0.12188881554687173		[learning rate: 0.00072637]
	Learning Rate: 0.00072637
	LOSS [training: 0.13319239370666996 | validation: 0.12207051774119394]
	TIME [epoch: 8.81 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1569456702181655		[learning rate: 0.00072461]
		[batch 20/20] avg loss: 0.19110526952060833		[learning rate: 0.00072286]
	Learning Rate: 0.000722857
	LOSS [training: 0.1740254698693869 | validation: 0.189158347345607]
	TIME [epoch: 8.81 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10628928833150927		[learning rate: 0.00072111]
		[batch 20/20] avg loss: 0.12740334469986742		[learning rate: 0.00071936]
	Learning Rate: 0.000719362
	LOSS [training: 0.11684631651568836 | validation: 0.07248600044187456]
	TIME [epoch: 8.82 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1291064643892776		[learning rate: 0.00071762]
		[batch 20/20] avg loss: 0.1359749249445095		[learning rate: 0.00071588]
	Learning Rate: 0.000715883
	LOSS [training: 0.13254069466689355 | validation: 0.09708309619507255]
	TIME [epoch: 8.81 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12908468455815475		[learning rate: 0.00071415]
		[batch 20/20] avg loss: 0.14808574148221829		[learning rate: 0.00071242]
	Learning Rate: 0.000712421
	LOSS [training: 0.1385852130201865 | validation: 0.08763131480697009]
	TIME [epoch: 8.85 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15842077737303528		[learning rate: 0.0007107]
		[batch 20/20] avg loss: 0.15842398796086207		[learning rate: 0.00070898]
	Learning Rate: 0.000708976
	LOSS [training: 0.1584223826669487 | validation: 0.14772137175210878]
	TIME [epoch: 8.81 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30700770166816665		[learning rate: 0.00070726]
		[batch 20/20] avg loss: 0.14933091513281155		[learning rate: 0.00070555]
	Learning Rate: 0.000705548
	LOSS [training: 0.22816930840048907 | validation: 0.18266959294885096]
	TIME [epoch: 8.81 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11488590413885694		[learning rate: 0.00070384]
		[batch 20/20] avg loss: 0.17285420245642472		[learning rate: 0.00070214]
	Learning Rate: 0.000702136
	LOSS [training: 0.1438700532976408 | validation: 0.1224415833081794]
	TIME [epoch: 8.8 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13436968254621354		[learning rate: 0.00070044]
		[batch 20/20] avg loss: 0.13742399258461893		[learning rate: 0.00069874]
	Learning Rate: 0.00069874
	LOSS [training: 0.1358968375654162 | validation: 0.213664263846599]
	TIME [epoch: 8.8 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1551473877393169		[learning rate: 0.00069705]
		[batch 20/20] avg loss: 0.13600424086596824		[learning rate: 0.00069536]
	Learning Rate: 0.000695361
	LOSS [training: 0.1455758143026426 | validation: 0.24944657474834256]
	TIME [epoch: 8.83 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18803035952554578		[learning rate: 0.00069368]
		[batch 20/20] avg loss: 0.19230368301859518		[learning rate: 0.000692]
	Learning Rate: 0.000691999
	LOSS [training: 0.19016702127207047 | validation: 0.07742578447198087]
	TIME [epoch: 8.82 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14640432116490634		[learning rate: 0.00069032]
		[batch 20/20] avg loss: 0.17036289674730537		[learning rate: 0.00068865]
	Learning Rate: 0.000688652
	LOSS [training: 0.1583836089561059 | validation: 0.3858186013364985]
	TIME [epoch: 8.82 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1503802024190879		[learning rate: 0.00068699]
		[batch 20/20] avg loss: 0.09460363318060806		[learning rate: 0.00068532]
	Learning Rate: 0.000685322
	LOSS [training: 0.12249191779984796 | validation: 0.25315575623715764]
	TIME [epoch: 8.82 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11007102085179646		[learning rate: 0.00068366]
		[batch 20/20] avg loss: 0.15098235937544244		[learning rate: 0.00068201]
	Learning Rate: 0.000682008
	LOSS [training: 0.13052669011361942 | validation: 0.1011446502742708]
	TIME [epoch: 8.82 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11646585969355483		[learning rate: 0.00068036]
		[batch 20/20] avg loss: 0.12511263775207682		[learning rate: 0.00067871]
	Learning Rate: 0.00067871
	LOSS [training: 0.12078924872281585 | validation: 0.06876202138755678]
	TIME [epoch: 8.83 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1643992199343154		[learning rate: 0.00067707]
		[batch 20/20] avg loss: 0.17809519868121632		[learning rate: 0.00067543]
	Learning Rate: 0.000675428
	LOSS [training: 0.17124720930776588 | validation: 0.1163775372493597]
	TIME [epoch: 8.83 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10792198144773617		[learning rate: 0.00067379]
		[batch 20/20] avg loss: 0.14278426662424362		[learning rate: 0.00067216]
	Learning Rate: 0.000672162
	LOSS [training: 0.12535312403598992 | validation: 0.12177457565276524]
	TIME [epoch: 8.82 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13587345212052018		[learning rate: 0.00067053]
		[batch 20/20] avg loss: 0.20154063074947662		[learning rate: 0.00066891]
	Learning Rate: 0.000668911
	LOSS [training: 0.1687070414349984 | validation: 0.30556233837627217]
	TIME [epoch: 8.82 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18303836975387108		[learning rate: 0.00066729]
		[batch 20/20] avg loss: 0.1667960137322757		[learning rate: 0.00066568]
	Learning Rate: 0.000665676
	LOSS [training: 0.1749171917430734 | validation: 0.3874453361692808]
	TIME [epoch: 8.81 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1838398204091078		[learning rate: 0.00066406]
		[batch 20/20] avg loss: 0.11957837447021472		[learning rate: 0.00066246]
	Learning Rate: 0.000662457
	LOSS [training: 0.15170909743966127 | validation: 0.07389489479856873]
	TIME [epoch: 8.82 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11189949720331552		[learning rate: 0.00066085]
		[batch 20/20] avg loss: 0.14404580013704057		[learning rate: 0.00065925]
	Learning Rate: 0.000659254
	LOSS [training: 0.12797264867017805 | validation: 0.3658105514223889]
	TIME [epoch: 8.82 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15594889800071715		[learning rate: 0.00065766]
		[batch 20/20] avg loss: 0.10868426024428948		[learning rate: 0.00065607]
	Learning Rate: 0.000656066
	LOSS [training: 0.13231657912250333 | validation: 0.16798269815243186]
	TIME [epoch: 8.81 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1149347981567159		[learning rate: 0.00065448]
		[batch 20/20] avg loss: 0.12479273954755894		[learning rate: 0.00065289]
	Learning Rate: 0.000652893
	LOSS [training: 0.11986376885213743 | validation: 0.2048254228974793]
	TIME [epoch: 8.81 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18368265047132792		[learning rate: 0.00065131]
		[batch 20/20] avg loss: 0.14738971904455783		[learning rate: 0.00064974]
	Learning Rate: 0.000649736
	LOSS [training: 0.1655361847579429 | validation: 0.322743111918775]
	TIME [epoch: 8.81 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14278248252608056		[learning rate: 0.00064816]
		[batch 20/20] avg loss: 0.1452652022180332		[learning rate: 0.00064659]
	Learning Rate: 0.000646594
	LOSS [training: 0.14402384237205687 | validation: 0.06030343878994582]
	TIME [epoch: 8.81 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13743579905004596		[learning rate: 0.00064503]
		[batch 20/20] avg loss: 0.1194088385873973		[learning rate: 0.00064347]
	Learning Rate: 0.000643467
	LOSS [training: 0.12842231881872163 | validation: 0.06696826372169375]
	TIME [epoch: 8.83 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13787702969929289		[learning rate: 0.00064191]
		[batch 20/20] avg loss: 0.14446233646569456		[learning rate: 0.00064036]
	Learning Rate: 0.000640355
	LOSS [training: 0.14116968308249372 | validation: 0.09413696399853674]
	TIME [epoch: 8.81 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09860857409698619		[learning rate: 0.00063881]
		[batch 20/20] avg loss: 0.10980292007532277		[learning rate: 0.00063726]
	Learning Rate: 0.000637259
	LOSS [training: 0.10420574708615449 | validation: 0.07030435965107439]
	TIME [epoch: 8.81 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13910539354960677		[learning rate: 0.00063572]
		[batch 20/20] avg loss: 0.15647228404459806		[learning rate: 0.00063418]
	Learning Rate: 0.000634177
	LOSS [training: 0.1477888387971024 | validation: 0.06872767255564996]
	TIME [epoch: 8.81 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12795764839753995		[learning rate: 0.00063264]
		[batch 20/20] avg loss: 0.11950693532201107		[learning rate: 0.00063111]
	Learning Rate: 0.00063111
	LOSS [training: 0.1237322918597755 | validation: 0.16200332124812789]
	TIME [epoch: 8.81 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17223792538735014		[learning rate: 0.00062958]
		[batch 20/20] avg loss: 0.147983067582031		[learning rate: 0.00062806]
	Learning Rate: 0.000628058
	LOSS [training: 0.16011049648469056 | validation: 0.10664576744676625]
	TIME [epoch: 8.84 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11863990571212779		[learning rate: 0.00062654]
		[batch 20/20] avg loss: 0.10555778061919541		[learning rate: 0.00062502]
	Learning Rate: 0.000625021
	LOSS [training: 0.11209884316566161 | validation: 0.20125724471649958]
	TIME [epoch: 8.81 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11239645503366562		[learning rate: 0.00062351]
		[batch 20/20] avg loss: 0.10624695987375739		[learning rate: 0.000622]
	Learning Rate: 0.000621999
	LOSS [training: 0.10932170745371153 | validation: 0.26723908841348404]
	TIME [epoch: 8.8 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12358561341650283		[learning rate: 0.00062049]
		[batch 20/20] avg loss: 0.21342440687863684		[learning rate: 0.00061899]
	Learning Rate: 0.000618991
	LOSS [training: 0.1685050101475698 | validation: 0.2145154735980196]
	TIME [epoch: 8.81 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12522959145759838		[learning rate: 0.00061749]
		[batch 20/20] avg loss: 0.13953756022883884		[learning rate: 0.000616]
	Learning Rate: 0.000615997
	LOSS [training: 0.13238357584321864 | validation: 0.07793078501669372]
	TIME [epoch: 8.81 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10877310087414142		[learning rate: 0.00061451]
		[batch 20/20] avg loss: 0.16698063572972827		[learning rate: 0.00061302]
	Learning Rate: 0.000613019
	LOSS [training: 0.13787686830193488 | validation: 0.11785310310220135]
	TIME [epoch: 8.83 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12487949035667845		[learning rate: 0.00061153]
		[batch 20/20] avg loss: 0.11797713523046068		[learning rate: 0.00061005]
	Learning Rate: 0.000610054
	LOSS [training: 0.12142831279356958 | validation: 0.0766208715054753]
	TIME [epoch: 8.82 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15351165076915357		[learning rate: 0.00060858]
		[batch 20/20] avg loss: 0.12048407385836454		[learning rate: 0.0006071]
	Learning Rate: 0.000607104
	LOSS [training: 0.13699786231375904 | validation: 0.07895205107897221]
	TIME [epoch: 8.81 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12318932724785044		[learning rate: 0.00060563]
		[batch 20/20] avg loss: 0.15183124262726203		[learning rate: 0.00060417]
	Learning Rate: 0.000604168
	LOSS [training: 0.13751028493755627 | validation: 0.10946834116362264]
	TIME [epoch: 8.81 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1545285160300075		[learning rate: 0.00060271]
		[batch 20/20] avg loss: 0.13265401894514764		[learning rate: 0.00060125]
	Learning Rate: 0.000601247
	LOSS [training: 0.1435912674875776 | validation: 0.1520834722439587]
	TIME [epoch: 8.81 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11481152178086693		[learning rate: 0.00059979]
		[batch 20/20] avg loss: 0.11170225615216056		[learning rate: 0.00059834]
	Learning Rate: 0.000598339
	LOSS [training: 0.11325688896651376 | validation: 0.06741421930211475]
	TIME [epoch: 8.84 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1077762533967351		[learning rate: 0.00059689]
		[batch 20/20] avg loss: 0.12615212679665383		[learning rate: 0.00059545]
	Learning Rate: 0.000595446
	LOSS [training: 0.11696419009669448 | validation: 0.06833100182736164]
	TIME [epoch: 8.81 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1124167366238954		[learning rate: 0.000594]
		[batch 20/20] avg loss: 0.1331283629039151		[learning rate: 0.00059257]
	Learning Rate: 0.000592566
	LOSS [training: 0.12277254976390524 | validation: 0.06009057822325091]
	TIME [epoch: 8.81 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0950538780302295		[learning rate: 0.00059113]
		[batch 20/20] avg loss: 0.152224638734129		[learning rate: 0.0005897]
	Learning Rate: 0.000589701
	LOSS [training: 0.12363925838217923 | validation: 0.23137285979467584]
	TIME [epoch: 8.82 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1113748348236037		[learning rate: 0.00058827]
		[batch 20/20] avg loss: 0.10876031865193758		[learning rate: 0.00058685]
	Learning Rate: 0.000586849
	LOSS [training: 0.11006757673777064 | validation: 0.13190072918151022]
	TIME [epoch: 8.82 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10968580022545585		[learning rate: 0.00058543]
		[batch 20/20] avg loss: 0.11825119843986684		[learning rate: 0.00058401]
	Learning Rate: 0.000584011
	LOSS [training: 0.11396849933266136 | validation: 0.0883757355331179]
	TIME [epoch: 8.84 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09425153243505278		[learning rate: 0.0005826]
		[batch 20/20] avg loss: 0.10631398363017394		[learning rate: 0.00058119]
	Learning Rate: 0.000581187
	LOSS [training: 0.1002827580326134 | validation: 0.15452313666338208]
	TIME [epoch: 8.82 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14847493165072262		[learning rate: 0.00057978]
		[batch 20/20] avg loss: 0.10840912387416621		[learning rate: 0.00057838]
	Learning Rate: 0.000578376
	LOSS [training: 0.12844202776244443 | validation: 0.10134251696897904]
	TIME [epoch: 8.81 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1348913975075366		[learning rate: 0.00057698]
		[batch 20/20] avg loss: 0.0908242620427692		[learning rate: 0.00057558]
	Learning Rate: 0.000575579
	LOSS [training: 0.1128578297751529 | validation: 0.22165586230777232]
	TIME [epoch: 8.81 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12421117525582101		[learning rate: 0.00057419]
		[batch 20/20] avg loss: 0.1266960356118236		[learning rate: 0.0005728]
	Learning Rate: 0.000572796
	LOSS [training: 0.1254536054338223 | validation: 0.07348829143762199]
	TIME [epoch: 8.81 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1103053485126075		[learning rate: 0.00057141]
		[batch 20/20] avg loss: 0.18871796534526383		[learning rate: 0.00057003]
	Learning Rate: 0.000570026
	LOSS [training: 0.14951165692893567 | validation: 0.11182308682803213]
	TIME [epoch: 8.83 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13883794503993496		[learning rate: 0.00056865]
		[batch 20/20] avg loss: 0.1152172156568737		[learning rate: 0.00056727]
	Learning Rate: 0.00056727
	LOSS [training: 0.12702758034840436 | validation: 0.3768810169302075]
	TIME [epoch: 8.83 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12481151609799113		[learning rate: 0.0005659]
		[batch 20/20] avg loss: 0.13763362712077135		[learning rate: 0.00056453]
	Learning Rate: 0.000564526
	LOSS [training: 0.13122257160938125 | validation: 0.06312976882113025]
	TIME [epoch: 8.8 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11944246613802253		[learning rate: 0.00056316]
		[batch 20/20] avg loss: 0.09705221373577		[learning rate: 0.0005618]
	Learning Rate: 0.000561796
	LOSS [training: 0.10824733993689625 | validation: 0.1938014143376157]
	TIME [epoch: 8.8 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14010562343899974		[learning rate: 0.00056044]
		[batch 20/20] avg loss: 0.14755306116465838		[learning rate: 0.00055908]
	Learning Rate: 0.00055908
	LOSS [training: 0.14382934230182906 | validation: 0.07639796980379786]
	TIME [epoch: 8.8 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09529102712566358		[learning rate: 0.00055773]
		[batch 20/20] avg loss: 0.1132245904745102		[learning rate: 0.00055638]
	Learning Rate: 0.000556376
	LOSS [training: 0.10425780880008688 | validation: 0.05568861095341636]
	TIME [epoch: 8.82 sec]
EPOCH 647/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11674800113619573		[learning rate: 0.00055503]
		[batch 20/20] avg loss: 0.12943466685510963		[learning rate: 0.00055369]
	Learning Rate: 0.000553685
	LOSS [training: 0.12309133399565271 | validation: 0.08115146199469842]
	TIME [epoch: 8.83 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09104697617686006		[learning rate: 0.00055235]
		[batch 20/20] avg loss: 0.1664849416257258		[learning rate: 0.00055101]
	Learning Rate: 0.000551008
	LOSS [training: 0.12876595890129292 | validation: 0.27741673289236646]
	TIME [epoch: 8.81 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15998939616322388		[learning rate: 0.00054967]
		[batch 20/20] avg loss: 0.10123741893116031		[learning rate: 0.00054834]
	Learning Rate: 0.000548343
	LOSS [training: 0.13061340754719214 | validation: 0.0583089433112447]
	TIME [epoch: 8.81 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10994676619315921		[learning rate: 0.00054702]
		[batch 20/20] avg loss: 0.11799260058314727		[learning rate: 0.00054569]
	Learning Rate: 0.000545692
	LOSS [training: 0.11396968338815325 | validation: 0.2561085633665976]
	TIME [epoch: 8.81 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13852922028567902		[learning rate: 0.00054437]
		[batch 20/20] avg loss: 0.10599113959888089		[learning rate: 0.00054305]
	Learning Rate: 0.000543053
	LOSS [training: 0.12226017994227996 | validation: 0.06299913193542511]
	TIME [epoch: 8.82 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18594908522798467		[learning rate: 0.00054174]
		[batch 20/20] avg loss: 0.10208312749422858		[learning rate: 0.00054043]
	Learning Rate: 0.000540427
	LOSS [training: 0.14401610636110662 | validation: 0.15219953428950106]
	TIME [epoch: 8.83 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11384351754679108		[learning rate: 0.00053912]
		[batch 20/20] avg loss: 0.13639813833610004		[learning rate: 0.00053781]
	Learning Rate: 0.000537813
	LOSS [training: 0.12512082794144558 | validation: 0.2578732534412822]
	TIME [epoch: 8.81 sec]
EPOCH 654/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17555737583647169		[learning rate: 0.00053651]
		[batch 20/20] avg loss: 0.13141404106473642		[learning rate: 0.00053521]
	Learning Rate: 0.000535213
	LOSS [training: 0.15348570845060408 | validation: 0.1720365047034687]
	TIME [epoch: 8.81 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10759481089083556		[learning rate: 0.00053392]
		[batch 20/20] avg loss: 0.11658249476162463		[learning rate: 0.00053262]
	Learning Rate: 0.000532624
	LOSS [training: 0.11208865282623008 | validation: 0.28442274545634455]
	TIME [epoch: 8.8 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14480152837963098		[learning rate: 0.00053134]
		[batch 20/20] avg loss: 0.08516225242822645		[learning rate: 0.00053005]
	Learning Rate: 0.000530049
	LOSS [training: 0.1149818904039287 | validation: 0.1059944104853167]
	TIME [epoch: 8.81 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10354717358107787		[learning rate: 0.00052877]
		[batch 20/20] avg loss: 0.17281684946102438		[learning rate: 0.00052749]
	Learning Rate: 0.000527485
	LOSS [training: 0.13818201152105108 | validation: 0.09415273465333886]
	TIME [epoch: 8.83 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10213902592517235		[learning rate: 0.00052621]
		[batch 20/20] avg loss: 0.1103859818372295		[learning rate: 0.00052493]
	Learning Rate: 0.000524935
	LOSS [training: 0.10626250388120088 | validation: 0.07482783307596919]
	TIME [epoch: 8.81 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09772848985158553		[learning rate: 0.00052366]
		[batch 20/20] avg loss: 0.11756669629921448		[learning rate: 0.0005224]
	Learning Rate: 0.000522396
	LOSS [training: 0.10764759307539999 | validation: 0.15437331611647762]
	TIME [epoch: 8.81 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1126934956021739		[learning rate: 0.00052113]
		[batch 20/20] avg loss: 0.09206484911998587		[learning rate: 0.00051987]
	Learning Rate: 0.00051987
	LOSS [training: 0.10237917236107989 | validation: 0.11082807581220797]
	TIME [epoch: 8.8 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08958117566597561		[learning rate: 0.00051861]
		[batch 20/20] avg loss: 0.10512521988722452		[learning rate: 0.00051736]
	Learning Rate: 0.000517356
	LOSS [training: 0.09735319777660006 | validation: 0.07751554901825462]
	TIME [epoch: 8.81 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14477264712257348		[learning rate: 0.0005161]
		[batch 20/20] avg loss: 0.09945275273613421		[learning rate: 0.00051485]
	Learning Rate: 0.000514854
	LOSS [training: 0.12211269992935389 | validation: 0.09726130715160085]
	TIME [epoch: 8.83 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1330884303715133		[learning rate: 0.00051361]
		[batch 20/20] avg loss: 0.1076210333348859		[learning rate: 0.00051236]
	Learning Rate: 0.000512364
	LOSS [training: 0.12035473185319959 | validation: 0.06037183066410912]
	TIME [epoch: 8.82 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16486159837548386		[learning rate: 0.00051112]
		[batch 20/20] avg loss: 0.09744676847145498		[learning rate: 0.00050989]
	Learning Rate: 0.000509887
	LOSS [training: 0.1311541834234694 | validation: 0.07170279249351849]
	TIME [epoch: 8.8 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1269956223610578		[learning rate: 0.00050865]
		[batch 20/20] avg loss: 0.10918234863781721		[learning rate: 0.00050742]
	Learning Rate: 0.000507421
	LOSS [training: 0.11808898549943754 | validation: 0.0981366318695813]
	TIME [epoch: 8.8 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13624476560725468		[learning rate: 0.00050619]
		[batch 20/20] avg loss: 0.09534558575436637		[learning rate: 0.00050497]
	Learning Rate: 0.000504967
	LOSS [training: 0.11579517568081055 | validation: 0.059942821194923175]
	TIME [epoch: 8.81 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11284716476939496		[learning rate: 0.00050374]
		[batch 20/20] avg loss: 0.12183957405271202		[learning rate: 0.00050253]
	Learning Rate: 0.000502525
	LOSS [training: 0.1173433694110535 | validation: 0.11949996353260381]
	TIME [epoch: 8.84 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14254526483832505		[learning rate: 0.00050131]
		[batch 20/20] avg loss: 0.11202372115226326		[learning rate: 0.0005001]
	Learning Rate: 0.000500095
	LOSS [training: 0.12728449299529418 | validation: 0.2435519875082453]
	TIME [epoch: 8.82 sec]
EPOCH 669/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14604098204336444		[learning rate: 0.00049888]
		[batch 20/20] avg loss: 0.11500228710354929		[learning rate: 0.00049768]
	Learning Rate: 0.000497677
	LOSS [training: 0.13052163457345686 | validation: 0.08650649007845829]
	TIME [epoch: 8.81 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1347962835578068		[learning rate: 0.00049647]
		[batch 20/20] avg loss: 0.09470454722947329		[learning rate: 0.00049527]
	Learning Rate: 0.00049527
	LOSS [training: 0.11475041539364006 | validation: 0.10518389714253361]
	TIME [epoch: 8.81 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09589816274200566		[learning rate: 0.00049407]
		[batch 20/20] avg loss: 0.10309701397198774		[learning rate: 0.00049288]
	Learning Rate: 0.000492875
	LOSS [training: 0.09949758835699671 | validation: 0.05733665495221234]
	TIME [epoch: 8.81 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1290748739754233		[learning rate: 0.00049168]
		[batch 20/20] avg loss: 0.10906995116195714		[learning rate: 0.00049049]
	Learning Rate: 0.000490492
	LOSS [training: 0.11907241256869024 | validation: 0.07279592680798613]
	TIME [epoch: 8.82 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09831446055977683		[learning rate: 0.0004893]
		[batch 20/20] avg loss: 0.1124796543466361		[learning rate: 0.00048812]
	Learning Rate: 0.00048812
	LOSS [training: 0.10539705745320645 | validation: 0.10880803891279534]
	TIME [epoch: 8.83 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1275815501732184		[learning rate: 0.00048694]
		[batch 20/20] avg loss: 0.11323041290112912		[learning rate: 0.00048576]
	Learning Rate: 0.000485759
	LOSS [training: 0.12040598153717377 | validation: 0.08692894291234574]
	TIME [epoch: 8.82 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1257580837170036		[learning rate: 0.00048458]
		[batch 20/20] avg loss: 0.12340372927459094		[learning rate: 0.00048341]
	Learning Rate: 0.00048341
	LOSS [training: 0.12458090649579727 | validation: 0.17903532569422043]
	TIME [epoch: 8.82 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10340918817371433		[learning rate: 0.00048224]
		[batch 20/20] avg loss: 0.09314839043572035		[learning rate: 0.00048107]
	Learning Rate: 0.000481072
	LOSS [training: 0.09827878930471734 | validation: 0.32607855918708245]
	TIME [epoch: 8.82 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1150815769469646		[learning rate: 0.00047991]
		[batch 20/20] avg loss: 0.16836958281493947		[learning rate: 0.00047875]
	Learning Rate: 0.000478746
	LOSS [training: 0.14172557988095205 | validation: 0.07780700409328233]
	TIME [epoch: 8.82 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0952125573256862		[learning rate: 0.00047759]
		[batch 20/20] avg loss: 0.11787267807882132		[learning rate: 0.00047643]
	Learning Rate: 0.000476431
	LOSS [training: 0.10654261770225378 | validation: 0.17265723854230366]
	TIME [epoch: 8.82 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1444566199180386		[learning rate: 0.00047528]
		[batch 20/20] avg loss: 0.10251782444640008		[learning rate: 0.00047413]
	Learning Rate: 0.000474127
	LOSS [training: 0.12348722218221934 | validation: 0.09567672134085956]
	TIME [epoch: 8.81 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10238877813292094		[learning rate: 0.00047298]
		[batch 20/20] avg loss: 0.11246006426597435		[learning rate: 0.00047183]
	Learning Rate: 0.000471834
	LOSS [training: 0.10742442119944766 | validation: 0.16614061738099928]
	TIME [epoch: 8.81 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10811347604803115		[learning rate: 0.00047069]
		[batch 20/20] avg loss: 0.11076347038882912		[learning rate: 0.00046955]
	Learning Rate: 0.000469553
	LOSS [training: 0.10943847321843014 | validation: 0.3338838512376873]
	TIME [epoch: 8.82 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11162746026554823		[learning rate: 0.00046842]
		[batch 20/20] avg loss: 0.194433015637548		[learning rate: 0.00046728]
	Learning Rate: 0.000467282
	LOSS [training: 0.15303023795154813 | validation: 0.22972547000740454]
	TIME [epoch: 8.82 sec]
EPOCH 683/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1256966695784707		[learning rate: 0.00046615]
		[batch 20/20] avg loss: 0.11749806051278391		[learning rate: 0.00046502]
	Learning Rate: 0.000465022
	LOSS [training: 0.1215973650456273 | validation: 0.056366246550105414]
	TIME [epoch: 8.82 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08624768691112278		[learning rate: 0.0004639]
		[batch 20/20] avg loss: 0.12206006307463628		[learning rate: 0.00046277]
	Learning Rate: 0.000462773
	LOSS [training: 0.10415387499287954 | validation: 0.06879437360099944]
	TIME [epoch: 8.8 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09259568290316511		[learning rate: 0.00046165]
		[batch 20/20] avg loss: 0.0921542461148245		[learning rate: 0.00046054]
	Learning Rate: 0.000460536
	LOSS [training: 0.09237496450899481 | validation: 0.08529283824733257]
	TIME [epoch: 8.81 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10518684273204175		[learning rate: 0.00045942]
		[batch 20/20] avg loss: 0.08905803382307494		[learning rate: 0.00045831]
	Learning Rate: 0.000458309
	LOSS [training: 0.09712243827755834 | validation: 0.052710147268823976]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_686.pth
	Model improved!!!
EPOCH 687/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08464454612065044		[learning rate: 0.0004572]
		[batch 20/20] avg loss: 0.07909837917891452		[learning rate: 0.00045609]
	Learning Rate: 0.000456092
	LOSS [training: 0.08187146264978247 | validation: 0.1187901339836506]
	TIME [epoch: 8.81 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12980382083002295		[learning rate: 0.00045499]
		[batch 20/20] avg loss: 0.08026571851066262		[learning rate: 0.00045389]
	Learning Rate: 0.000453887
	LOSS [training: 0.10503476967034278 | validation: 0.11181695133603874]
	TIME [epoch: 8.84 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0874255996765584		[learning rate: 0.00045279]
		[batch 20/20] avg loss: 0.14068864189353336		[learning rate: 0.00045169]
	Learning Rate: 0.000451692
	LOSS [training: 0.1140571207850459 | validation: 0.13863227763579736]
	TIME [epoch: 8.8 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11531967153939474		[learning rate: 0.0004506]
		[batch 20/20] avg loss: 0.11037227433710262		[learning rate: 0.00044951]
	Learning Rate: 0.000449507
	LOSS [training: 0.11284597293824869 | validation: 0.1190589228723901]
	TIME [epoch: 8.81 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12730680958755006		[learning rate: 0.00044842]
		[batch 20/20] avg loss: 0.14010363173711923		[learning rate: 0.00044733]
	Learning Rate: 0.000447334
	LOSS [training: 0.13370522066233465 | validation: 0.11174018851982744]
	TIME [epoch: 8.81 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08906889227704856		[learning rate: 0.00044625]
		[batch 20/20] avg loss: 0.060363190849463885		[learning rate: 0.00044517]
	Learning Rate: 0.00044517
	LOSS [training: 0.07471604156325623 | validation: 0.08629415135812166]
	TIME [epoch: 8.89 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09032607448464525		[learning rate: 0.00044409]
		[batch 20/20] avg loss: 0.0839612529880569		[learning rate: 0.00044302]
	Learning Rate: 0.000443018
	LOSS [training: 0.08714366373635106 | validation: 0.27540519019805443]
	TIME [epoch: 8.83 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09596543465924212		[learning rate: 0.00044195]
		[batch 20/20] avg loss: 0.08774850151756201		[learning rate: 0.00044088]
	Learning Rate: 0.000440875
	LOSS [training: 0.09185696808840207 | validation: 0.06042978754348369]
	TIME [epoch: 8.8 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07665297036481153		[learning rate: 0.00043981]
		[batch 20/20] avg loss: 0.10533860225230987		[learning rate: 0.00043874]
	Learning Rate: 0.000438743
	LOSS [training: 0.09099578630856071 | validation: 0.0914850409784733]
	TIME [epoch: 8.8 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08216389718683895		[learning rate: 0.00043768]
		[batch 20/20] avg loss: 0.11301541159983693		[learning rate: 0.00043662]
	Learning Rate: 0.000436622
	LOSS [training: 0.09758965439333793 | validation: 0.2237766991451629]
	TIME [epoch: 8.79 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1233297512441038		[learning rate: 0.00043556]
		[batch 20/20] avg loss: 0.09674685593966192		[learning rate: 0.00043451]
	Learning Rate: 0.00043451
	LOSS [training: 0.11003830359188285 | validation: 0.12053283732188919]
	TIME [epoch: 8.81 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11202660036865804		[learning rate: 0.00043346]
		[batch 20/20] avg loss: 0.0771378171467794		[learning rate: 0.00043241]
	Learning Rate: 0.000432409
	LOSS [training: 0.09458220875771874 | validation: 0.10312308067387448]
	TIME [epoch: 8.83 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13447607196636904		[learning rate: 0.00043136]
		[batch 20/20] avg loss: 0.08814272664644789		[learning rate: 0.00043032]
	Learning Rate: 0.000430318
	LOSS [training: 0.11130939930640844 | validation: 0.06235343114245108]
	TIME [epoch: 8.8 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1606452728406039		[learning rate: 0.00042928]
		[batch 20/20] avg loss: 0.1001098770333763		[learning rate: 0.00042824]
	Learning Rate: 0.000428237
	LOSS [training: 0.13037757493699012 | validation: 0.3169034517759354]
	TIME [epoch: 8.81 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13250111051715635		[learning rate: 0.0004272]
		[batch 20/20] avg loss: 0.09014760194529615		[learning rate: 0.00042617]
	Learning Rate: 0.000426166
	LOSS [training: 0.11132435623122623 | validation: 0.05446327016320048]
	TIME [epoch: 8.81 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09758381505364075		[learning rate: 0.00042513]
		[batch 20/20] avg loss: 0.07279589681105642		[learning rate: 0.00042411]
	Learning Rate: 0.000424105
	LOSS [training: 0.08518985593234858 | validation: 0.07489111455174048]
	TIME [epoch: 8.81 sec]
EPOCH 703/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0984121962891764		[learning rate: 0.00042308]
		[batch 20/20] avg loss: 0.12259210472994794		[learning rate: 0.00042205]
	Learning Rate: 0.000422054
	LOSS [training: 0.11050215050956216 | validation: 0.06449573085033981]
	TIME [epoch: 8.84 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09736940449998009		[learning rate: 0.00042103]
		[batch 20/20] avg loss: 0.09352105618250502		[learning rate: 0.00042001]
	Learning Rate: 0.000420013
	LOSS [training: 0.09544523034124255 | validation: 0.05332272805083445]
	TIME [epoch: 8.81 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08962082227986243		[learning rate: 0.000419]
		[batch 20/20] avg loss: 0.09946702449045772		[learning rate: 0.00041798]
	Learning Rate: 0.000417982
	LOSS [training: 0.09454392338516007 | validation: 0.06114812104064149]
	TIME [epoch: 8.81 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09441415087427597		[learning rate: 0.00041697]
		[batch 20/20] avg loss: 0.08461054792605442		[learning rate: 0.00041596]
	Learning Rate: 0.000415961
	LOSS [training: 0.0895123494001652 | validation: 0.13646927436822054]
	TIME [epoch: 8.8 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14190918006676242		[learning rate: 0.00041495]
		[batch 20/20] avg loss: 0.06480374564536413		[learning rate: 0.00041395]
	Learning Rate: 0.00041395
	LOSS [training: 0.10335646285606329 | validation: 0.05931534386985047]
	TIME [epoch: 8.81 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09075538608415229		[learning rate: 0.00041295]
		[batch 20/20] avg loss: 0.1029868821184801		[learning rate: 0.00041195]
	Learning Rate: 0.000411948
	LOSS [training: 0.09687113410131623 | validation: 0.05670624807320422]
	TIME [epoch: 8.83 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07295519889234156		[learning rate: 0.00041095]
		[batch 20/20] avg loss: 0.10577241873663226		[learning rate: 0.00040996]
	Learning Rate: 0.000409956
	LOSS [training: 0.0893638088144869 | validation: 0.10471504136947286]
	TIME [epoch: 8.81 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09108304547961374		[learning rate: 0.00040896]
		[batch 20/20] avg loss: 0.14521537189096548		[learning rate: 0.00040797]
	Learning Rate: 0.000407973
	LOSS [training: 0.1181492086852896 | validation: 0.4308791199654024]
	TIME [epoch: 8.81 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12139208568525146		[learning rate: 0.00040699]
		[batch 20/20] avg loss: 0.08984897400497106		[learning rate: 0.000406]
	Learning Rate: 0.000406
	LOSS [training: 0.10562052984511125 | validation: 0.1615401208371324]
	TIME [epoch: 8.8 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07685902877876592		[learning rate: 0.00040502]
		[batch 20/20] avg loss: 0.09134949025198956		[learning rate: 0.00040404]
	Learning Rate: 0.000404037
	LOSS [training: 0.08410425951537776 | validation: 0.16550347766326878]
	TIME [epoch: 8.81 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10760229458363693		[learning rate: 0.00040306]
		[batch 20/20] avg loss: 0.09053729141615899		[learning rate: 0.00040208]
	Learning Rate: 0.000402083
	LOSS [training: 0.09906979299989796 | validation: 0.05014229992643615]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_713.pth
	Model improved!!!
EPOCH 714/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0792223317804744		[learning rate: 0.00040111]
		[batch 20/20] avg loss: 0.09372283858046869		[learning rate: 0.00040014]
	Learning Rate: 0.000400139
	LOSS [training: 0.08647258518047155 | validation: 0.06388661098120349]
	TIME [epoch: 8.81 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08013519978421549		[learning rate: 0.00039917]
		[batch 20/20] avg loss: 0.0929920471368638		[learning rate: 0.0003982]
	Learning Rate: 0.000398204
	LOSS [training: 0.08656362346053964 | validation: 0.12014669788913335]
	TIME [epoch: 8.81 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0723414764821529		[learning rate: 0.00039724]
		[batch 20/20] avg loss: 0.1453516759681557		[learning rate: 0.00039628]
	Learning Rate: 0.000396278
	LOSS [training: 0.10884657622515428 | validation: 0.06962950407582122]
	TIME [epoch: 8.8 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0931158151577722		[learning rate: 0.00039532]
		[batch 20/20] avg loss: 0.0958973352276072		[learning rate: 0.00039436]
	Learning Rate: 0.000394362
	LOSS [training: 0.09450657519268969 | validation: 0.06529333326494231]
	TIME [epoch: 8.8 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14317780704516675		[learning rate: 0.00039341]
		[batch 20/20] avg loss: 0.1122913908167866		[learning rate: 0.00039245]
	Learning Rate: 0.000392455
	LOSS [training: 0.12773459893097666 | validation: 0.1474682656570323]
	TIME [epoch: 8.83 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07933950321172599		[learning rate: 0.0003915]
		[batch 20/20] avg loss: 0.12232344120583476		[learning rate: 0.00039056]
	Learning Rate: 0.000390557
	LOSS [training: 0.10083147220878037 | validation: 0.12599829042617852]
	TIME [epoch: 8.81 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08409544876175767		[learning rate: 0.00038961]
		[batch 20/20] avg loss: 0.09908018913916393		[learning rate: 0.00038867]
	Learning Rate: 0.000388668
	LOSS [training: 0.09158781895046081 | validation: 0.04463004695924369]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_720.pth
	Model improved!!!
EPOCH 721/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08270477957961835		[learning rate: 0.00038773]
		[batch 20/20] avg loss: 0.10277818971949548		[learning rate: 0.00038679]
	Learning Rate: 0.000386789
	LOSS [training: 0.09274148464955694 | validation: 0.1831478890654368]
	TIME [epoch: 8.79 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11981163182517854		[learning rate: 0.00038585]
		[batch 20/20] avg loss: 0.09435749893743867		[learning rate: 0.00038492]
	Learning Rate: 0.000384918
	LOSS [training: 0.10708456538130859 | validation: 0.18033187310638077]
	TIME [epoch: 8.8 sec]
EPOCH 723/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10041288796197448		[learning rate: 0.00038399]
		[batch 20/20] avg loss: 0.07603998257465429		[learning rate: 0.00038306]
	Learning Rate: 0.000383057
	LOSS [training: 0.08822643526831438 | validation: 0.0651235545513216]
	TIME [epoch: 8.82 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07835217092218463		[learning rate: 0.00038213]
		[batch 20/20] avg loss: 0.09911063465332351		[learning rate: 0.0003812]
	Learning Rate: 0.000381204
	LOSS [training: 0.08873140278775409 | validation: 0.7697973252811756]
	TIME [epoch: 8.8 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24271685942308457		[learning rate: 0.00038028]
		[batch 20/20] avg loss: 0.11037574196361315		[learning rate: 0.00037936]
	Learning Rate: 0.000379361
	LOSS [training: 0.1765463006933489 | validation: 0.07315405102512192]
	TIME [epoch: 8.8 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09854970091720384		[learning rate: 0.00037844]
		[batch 20/20] avg loss: 0.09015097118355586		[learning rate: 0.00037753]
	Learning Rate: 0.000377526
	LOSS [training: 0.09435033605037983 | validation: 0.05031923315574256]
	TIME [epoch: 8.8 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06898753980032749		[learning rate: 0.00037661]
		[batch 20/20] avg loss: 0.16163140463498415		[learning rate: 0.0003757]
	Learning Rate: 0.000375701
	LOSS [training: 0.11530947221765583 | validation: 0.11059262860675256]
	TIME [epoch: 8.81 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09588552418811633		[learning rate: 0.00037479]
		[batch 20/20] avg loss: 0.07688543857648016		[learning rate: 0.00037388]
	Learning Rate: 0.000373884
	LOSS [training: 0.08638548138229822 | validation: 0.08228634634269336]
	TIME [epoch: 8.83 sec]
EPOCH 729/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09331174733655048		[learning rate: 0.00037298]
		[batch 20/20] avg loss: 0.0625536607220412		[learning rate: 0.00037208]
	Learning Rate: 0.000372076
	LOSS [training: 0.07793270402929584 | validation: 0.0854401563775398]
	TIME [epoch: 8.81 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08896332109385457		[learning rate: 0.00037118]
		[batch 20/20] avg loss: 0.08750177058518502		[learning rate: 0.00037028]
	Learning Rate: 0.000370277
	LOSS [training: 0.08823254583951978 | validation: 0.09338441517562417]
	TIME [epoch: 8.81 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06401939013941296		[learning rate: 0.00036938]
		[batch 20/20] avg loss: 0.10860796029708153		[learning rate: 0.00036849]
	Learning Rate: 0.000368486
	LOSS [training: 0.08631367521824725 | validation: 0.05602976739655047]
	TIME [epoch: 8.81 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09542229156036802		[learning rate: 0.00036759]
		[batch 20/20] avg loss: 0.10249629537119924		[learning rate: 0.0003667]
	Learning Rate: 0.000366704
	LOSS [training: 0.09895929346578361 | validation: 0.22960133334311653]
	TIME [epoch: 8.82 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10440152895673091		[learning rate: 0.00036582]
		[batch 20/20] avg loss: 0.10115682191810267		[learning rate: 0.00036493]
	Learning Rate: 0.000364931
	LOSS [training: 0.1027791754374168 | validation: 0.10590722048734155]
	TIME [epoch: 8.84 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08593061539166355		[learning rate: 0.00036405]
		[batch 20/20] avg loss: 0.07438354818332024		[learning rate: 0.00036317]
	Learning Rate: 0.000363166
	LOSS [training: 0.0801570817874919 | validation: 0.15615966617404253]
	TIME [epoch: 8.81 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0991935224450789		[learning rate: 0.00036229]
		[batch 20/20] avg loss: 0.0851616779526171		[learning rate: 0.00036141]
	Learning Rate: 0.00036141
	LOSS [training: 0.09217760019884799 | validation: 0.06276716406980379]
	TIME [epoch: 8.8 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06320620329978899		[learning rate: 0.00036053]
		[batch 20/20] avg loss: 0.09044372462690707		[learning rate: 0.00035966]
	Learning Rate: 0.000359662
	LOSS [training: 0.07682496396334801 | validation: 0.06684049275371921]
	TIME [epoch: 8.81 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07952377670850017		[learning rate: 0.00035879]
		[batch 20/20] avg loss: 0.07313297598761734		[learning rate: 0.00035792]
	Learning Rate: 0.000357923
	LOSS [training: 0.07632837634805877 | validation: 0.05503693205337494]
	TIME [epoch: 8.8 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09503271725546161		[learning rate: 0.00035706]
		[batch 20/20] avg loss: 0.0654834741573199		[learning rate: 0.00035619]
	Learning Rate: 0.000356192
	LOSS [training: 0.08025809570639074 | validation: 0.09163212523260206]
	TIME [epoch: 8.83 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1094848689970587		[learning rate: 0.00035533]
		[batch 20/20] avg loss: 0.07852349779432541		[learning rate: 0.00035447]
	Learning Rate: 0.00035447
	LOSS [training: 0.09400418339569205 | validation: 0.12063415706704511]
	TIME [epoch: 8.81 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10437973671105502		[learning rate: 0.00035361]
		[batch 20/20] avg loss: 0.08424334349486735		[learning rate: 0.00035276]
	Learning Rate: 0.000352755
	LOSS [training: 0.09431154010296118 | validation: 0.06975693612892322]
	TIME [epoch: 8.8 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09010932166253618		[learning rate: 0.0003519]
		[batch 20/20] avg loss: 0.07231676929950323		[learning rate: 0.00035105]
	Learning Rate: 0.00035105
	LOSS [training: 0.08121304548101968 | validation: 0.12313141585281431]
	TIME [epoch: 8.81 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0956321237923617		[learning rate: 0.0003502]
		[batch 20/20] avg loss: 0.10304521366828423		[learning rate: 0.00034935]
	Learning Rate: 0.000349352
	LOSS [training: 0.09933866873032296 | validation: 0.05292607854574252]
	TIME [epoch: 8.81 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08235301869441039		[learning rate: 0.00034851]
		[batch 20/20] avg loss: 0.07963397588254599		[learning rate: 0.00034766]
	Learning Rate: 0.000347663
	LOSS [training: 0.0809934972884782 | validation: 0.10749036085772959]
	TIME [epoch: 8.96 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08944034780733477		[learning rate: 0.00034682]
		[batch 20/20] avg loss: 0.06477972029136475		[learning rate: 0.00034598]
	Learning Rate: 0.000345981
	LOSS [training: 0.07711003404934974 | validation: 0.07121364976626526]
	TIME [epoch: 8.81 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07899394310636239		[learning rate: 0.00034514]
		[batch 20/20] avg loss: 0.09264220667695452		[learning rate: 0.00034431]
	Learning Rate: 0.000344308
	LOSS [training: 0.08581807489165846 | validation: 0.08667778648474912]
	TIME [epoch: 8.8 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08832465203529657		[learning rate: 0.00034347]
		[batch 20/20] avg loss: 0.083009170702212		[learning rate: 0.00034264]
	Learning Rate: 0.000342643
	LOSS [training: 0.08566691136875429 | validation: 0.06359218878478654]
	TIME [epoch: 8.81 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09688399161780373		[learning rate: 0.00034181]
		[batch 20/20] avg loss: 0.10114196792641132		[learning rate: 0.00034099]
	Learning Rate: 0.000340986
	LOSS [training: 0.09901297977210752 | validation: 0.09859330371406129]
	TIME [epoch: 8.8 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10598831093181207		[learning rate: 0.00034016]
		[batch 20/20] avg loss: 0.11251504344945709		[learning rate: 0.00033934]
	Learning Rate: 0.000339337
	LOSS [training: 0.10925167719063456 | validation: 0.08392417841875668]
	TIME [epoch: 8.83 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0929418927627641		[learning rate: 0.00033852]
		[batch 20/20] avg loss: 0.08632892898183164		[learning rate: 0.0003377]
	Learning Rate: 0.000337696
	LOSS [training: 0.08963541087229789 | validation: 0.05256985758371893]
	TIME [epoch: 8.82 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09335606712057737		[learning rate: 0.00033688]
		[batch 20/20] avg loss: 0.11070895101326232		[learning rate: 0.00033606]
	Learning Rate: 0.000336063
	LOSS [training: 0.10203250906691985 | validation: 0.06841967607906004]
	TIME [epoch: 8.81 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07148282443550216		[learning rate: 0.00033525]
		[batch 20/20] avg loss: 0.06750400394031017		[learning rate: 0.00033444]
	Learning Rate: 0.000334438
	LOSS [training: 0.06949341418790618 | validation: 0.06730690445607547]
	TIME [epoch: 8.81 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10937254930306708		[learning rate: 0.00033363]
		[batch 20/20] avg loss: 0.11586595605083025		[learning rate: 0.00033282]
	Learning Rate: 0.000332821
	LOSS [training: 0.11261925267694868 | validation: 0.0578315052139381]
	TIME [epoch: 8.81 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09804571665611975		[learning rate: 0.00033202]
		[batch 20/20] avg loss: 0.07522547561694862		[learning rate: 0.00033121]
	Learning Rate: 0.000331211
	LOSS [training: 0.0866355961365342 | validation: 0.16050269286991964]
	TIME [epoch: 8.82 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11517704053444948		[learning rate: 0.00033041]
		[batch 20/20] avg loss: 0.10507589893801943		[learning rate: 0.00032961]
	Learning Rate: 0.00032961
	LOSS [training: 0.11012646973623444 | validation: 0.06050600448650281]
	TIME [epoch: 8.83 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10701908070922928		[learning rate: 0.00032881]
		[batch 20/20] avg loss: 0.09309822924571856		[learning rate: 0.00032802]
	Learning Rate: 0.000328016
	LOSS [training: 0.10005865497747393 | validation: 0.06774998471741182]
	TIME [epoch: 8.8 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08162634580746071		[learning rate: 0.00032722]
		[batch 20/20] avg loss: 0.09460312764166973		[learning rate: 0.00032643]
	Learning Rate: 0.00032643
	LOSS [training: 0.08811473672456523 | validation: 0.08765470289511783]
	TIME [epoch: 8.82 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14139834622646702		[learning rate: 0.00032564]
		[batch 20/20] avg loss: 0.09315475492370241		[learning rate: 0.00032485]
	Learning Rate: 0.000324851
	LOSS [training: 0.11727655057508472 | validation: 0.1678057569524629]
	TIME [epoch: 8.8 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10231586075817663		[learning rate: 0.00032406]
		[batch 20/20] avg loss: 0.08624140033657476		[learning rate: 0.00032328]
	Learning Rate: 0.00032328
	LOSS [training: 0.0942786305473757 | validation: 0.05586628173304908]
	TIME [epoch: 8.83 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07324837618329319		[learning rate: 0.0003225]
		[batch 20/20] avg loss: 0.08494605072984193		[learning rate: 0.00032172]
	Learning Rate: 0.000321717
	LOSS [training: 0.07909721345656756 | validation: 0.1599525803287312]
	TIME [epoch: 8.82 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08209466895538706		[learning rate: 0.00032094]
		[batch 20/20] avg loss: 0.09232059604689223		[learning rate: 0.00032016]
	Learning Rate: 0.000320161
	LOSS [training: 0.08720763250113965 | validation: 0.05897563744208269]
	TIME [epoch: 8.84 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10348582996927594		[learning rate: 0.00031939]
		[batch 20/20] avg loss: 0.08620475110676282		[learning rate: 0.00031861]
	Learning Rate: 0.000318613
	LOSS [training: 0.09484529053801939 | validation: 0.05452885741852381]
	TIME [epoch: 8.81 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09346851263462486		[learning rate: 0.00031784]
		[batch 20/20] avg loss: 0.10765360508737035		[learning rate: 0.00031707]
	Learning Rate: 0.000317072
	LOSS [training: 0.10056105886099762 | validation: 0.13940789863166522]
	TIME [epoch: 8.8 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08058458149103435		[learning rate: 0.0003163]
		[batch 20/20] avg loss: 0.08823824978597264		[learning rate: 0.00031554]
	Learning Rate: 0.000315539
	LOSS [training: 0.08441141563850349 | validation: 0.0723312174878597]
	TIME [epoch: 8.81 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07969249437414797		[learning rate: 0.00031477]
		[batch 20/20] avg loss: 0.08410266182228864		[learning rate: 0.00031401]
	Learning Rate: 0.000314013
	LOSS [training: 0.08189757809821831 | validation: 0.049409365684205925]
	TIME [epoch: 8.83 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0814925348538739		[learning rate: 0.00031325]
		[batch 20/20] avg loss: 0.09215273818569827		[learning rate: 0.00031249]
	Learning Rate: 0.000312494
	LOSS [training: 0.08682263651978608 | validation: 0.05866281500397795]
	TIME [epoch: 8.8 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0715192424663911		[learning rate: 0.00031174]
		[batch 20/20] avg loss: 0.0634440007024209		[learning rate: 0.00031098]
	Learning Rate: 0.000310983
	LOSS [training: 0.067481621584406 | validation: 0.0516102602073332]
	TIME [epoch: 8.81 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08225362156240089		[learning rate: 0.00031023]
		[batch 20/20] avg loss: 0.07844437860416605		[learning rate: 0.00030948]
	Learning Rate: 0.000309479
	LOSS [training: 0.08034900008328347 | validation: 0.054682221551523205]
	TIME [epoch: 8.81 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10787910153253714		[learning rate: 0.00030873]
		[batch 20/20] avg loss: 0.11087365671616131		[learning rate: 0.00030798]
	Learning Rate: 0.000307983
	LOSS [training: 0.10937637912434924 | validation: 0.08091992594990619]
	TIME [epoch: 8.82 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11163891558154426		[learning rate: 0.00030724]
		[batch 20/20] avg loss: 0.09504591406657477		[learning rate: 0.00030649]
	Learning Rate: 0.000306493
	LOSS [training: 0.10334241482405951 | validation: 0.12828792172826817]
	TIME [epoch: 8.83 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07671603452325153		[learning rate: 0.00030575]
		[batch 20/20] avg loss: 0.08415789825067305		[learning rate: 0.00030501]
	Learning Rate: 0.000305011
	LOSS [training: 0.08043696638696228 | validation: 0.05505591899507801]
	TIME [epoch: 8.83 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08873175551714667		[learning rate: 0.00030427]
		[batch 20/20] avg loss: 0.07834960939910092		[learning rate: 0.00030354]
	Learning Rate: 0.000303536
	LOSS [training: 0.08354068245812377 | validation: 0.12091918328259058]
	TIME [epoch: 8.82 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06892000842307368		[learning rate: 0.0003028]
		[batch 20/20] avg loss: 0.08137004287750112		[learning rate: 0.00030207]
	Learning Rate: 0.000302068
	LOSS [training: 0.07514502565028738 | validation: 0.0938554918251849]
	TIME [epoch: 8.81 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06564147947867782		[learning rate: 0.00030134]
		[batch 20/20] avg loss: 0.08252382780390413		[learning rate: 0.00030061]
	Learning Rate: 0.000300608
	LOSS [training: 0.07408265364129098 | validation: 0.15639341167070284]
	TIME [epoch: 8.81 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08897722398601804		[learning rate: 0.00029988]
		[batch 20/20] avg loss: 0.09623455119311611		[learning rate: 0.00029915]
	Learning Rate: 0.000299154
	LOSS [training: 0.09260588758956709 | validation: 0.05013668452322327]
	TIME [epoch: 8.83 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08201431511422305		[learning rate: 0.00029843]
		[batch 20/20] avg loss: 0.08486645520640808		[learning rate: 0.00029771]
	Learning Rate: 0.000297707
	LOSS [training: 0.08344038516031559 | validation: 0.058819747449486384]
	TIME [epoch: 8.81 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09673105168119708		[learning rate: 0.00029699]
		[batch 20/20] avg loss: 0.0701790872026073		[learning rate: 0.00029627]
	Learning Rate: 0.000296268
	LOSS [training: 0.08345506944190219 | validation: 0.03987398884824516]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_776.pth
	Model improved!!!
EPOCH 777/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07239589089815382		[learning rate: 0.00029555]
		[batch 20/20] avg loss: 0.05621068071431408		[learning rate: 0.00029483]
	Learning Rate: 0.000294835
	LOSS [training: 0.06430328580623396 | validation: 0.05615997273376228]
	TIME [epoch: 8.81 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07924997727943266		[learning rate: 0.00029412]
		[batch 20/20] avg loss: 0.07566804586092044		[learning rate: 0.00029341]
	Learning Rate: 0.000293409
	LOSS [training: 0.07745901157017654 | validation: 0.17798668766057088]
	TIME [epoch: 8.81 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09744389267622186		[learning rate: 0.0002927]
		[batch 20/20] avg loss: 0.07428260841915388		[learning rate: 0.00029199]
	Learning Rate: 0.00029199
	LOSS [training: 0.08586325054768787 | validation: 0.052819466652877]
	TIME [epoch: 8.83 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08463583653137961		[learning rate: 0.00029128]
		[batch 20/20] avg loss: 0.08444180613551622		[learning rate: 0.00029058]
	Learning Rate: 0.000290578
	LOSS [training: 0.0845388213334479 | validation: 0.2064811510047511]
	TIME [epoch: 8.81 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1446622736593069		[learning rate: 0.00028987]
		[batch 20/20] avg loss: 0.0870261707604121		[learning rate: 0.00028917]
	Learning Rate: 0.000289173
	LOSS [training: 0.1158442222098595 | validation: 0.12114495357864381]
	TIME [epoch: 8.81 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10184347991439709		[learning rate: 0.00028847]
		[batch 20/20] avg loss: 0.09019783991106511		[learning rate: 0.00028777]
	Learning Rate: 0.000287775
	LOSS [training: 0.09602065991273111 | validation: 0.06465407627279883]
	TIME [epoch: 8.81 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09664192763520543		[learning rate: 0.00028708]
		[batch 20/20] avg loss: 0.0931617148817662		[learning rate: 0.00028638]
	Learning Rate: 0.000286383
	LOSS [training: 0.09490182125848581 | validation: 0.05551324495287042]
	TIME [epoch: 8.82 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08544200743331701		[learning rate: 0.00028569]
		[batch 20/20] avg loss: 0.07650572265928321		[learning rate: 0.000285]
	Learning Rate: 0.000284998
	LOSS [training: 0.08097386504630008 | validation: 0.1321879511772749]
	TIME [epoch: 8.84 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10271616759781574		[learning rate: 0.00028431]
		[batch 20/20] avg loss: 0.08246916023054515		[learning rate: 0.00028362]
	Learning Rate: 0.00028362
	LOSS [training: 0.09259266391418046 | validation: 0.08868320580619228]
	TIME [epoch: 8.82 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11464393492856459		[learning rate: 0.00028293]
		[batch 20/20] avg loss: 0.09136617271433604		[learning rate: 0.00028225]
	Learning Rate: 0.000282248
	LOSS [training: 0.10300505382145034 | validation: 0.13356409264443456]
	TIME [epoch: 8.81 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0926792243020492		[learning rate: 0.00028157]
		[batch 20/20] avg loss: 0.057197636414656514		[learning rate: 0.00028088]
	Learning Rate: 0.000280884
	LOSS [training: 0.07493843035835289 | validation: 0.05964432820867199]
	TIME [epoch: 8.81 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07709760666536951		[learning rate: 0.0002802]
		[batch 20/20] avg loss: 0.076483163896633		[learning rate: 0.00027953]
	Learning Rate: 0.000279525
	LOSS [training: 0.07679038528100125 | validation: 0.058146421399245016]
	TIME [epoch: 8.81 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06778292932703056		[learning rate: 0.00027885]
		[batch 20/20] avg loss: 0.057616173634826164		[learning rate: 0.00027817]
	Learning Rate: 0.000278173
	LOSS [training: 0.06269955148092837 | validation: 0.05361949794602527]
	TIME [epoch: 8.83 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09072001245286856		[learning rate: 0.0002775]
		[batch 20/20] avg loss: 0.08504865769563202		[learning rate: 0.00027683]
	Learning Rate: 0.000276828
	LOSS [training: 0.0878843350742503 | validation: 0.06725988949828478]
	TIME [epoch: 8.82 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08708690327360145		[learning rate: 0.00027616]
		[batch 20/20] avg loss: 0.08615839643566531		[learning rate: 0.00027549]
	Learning Rate: 0.00027549
	LOSS [training: 0.08662264985463339 | validation: 0.07300010213632235]
	TIME [epoch: 8.81 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06505219224638029		[learning rate: 0.00027482]
		[batch 20/20] avg loss: 0.07440745355445096		[learning rate: 0.00027416]
	Learning Rate: 0.000274157
	LOSS [training: 0.06972982290041563 | validation: 0.18797001210210057]
	TIME [epoch: 8.81 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09764393157049094		[learning rate: 0.00027349]
		[batch 20/20] avg loss: 0.08423649590359308		[learning rate: 0.00027283]
	Learning Rate: 0.000272832
	LOSS [training: 0.09094021373704204 | validation: 0.09092689287531223]
	TIME [epoch: 8.81 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08448736294702015		[learning rate: 0.00027217]
		[batch 20/20] avg loss: 0.06392403949688856		[learning rate: 0.00027151]
	Learning Rate: 0.000271512
	LOSS [training: 0.07420570122195436 | validation: 0.07325974782457342]
	TIME [epoch: 8.87 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07191652161570268		[learning rate: 0.00027086]
		[batch 20/20] avg loss: 0.08130101145040838		[learning rate: 0.0002702]
	Learning Rate: 0.000270199
	LOSS [training: 0.07660876653305551 | validation: 0.09777010739304642]
	TIME [epoch: 8.81 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15148148990875315		[learning rate: 0.00026955]
		[batch 20/20] avg loss: 0.0770683041538442		[learning rate: 0.00026889]
	Learning Rate: 0.000268893
	LOSS [training: 0.1142748970312987 | validation: 0.07993710590299553]
	TIME [epoch: 8.81 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07965379772033607		[learning rate: 0.00026824]
		[batch 20/20] avg loss: 0.09193425217473376		[learning rate: 0.00026759]
	Learning Rate: 0.000267592
	LOSS [training: 0.08579402494753494 | validation: 0.05654690927858406]
	TIME [epoch: 8.82 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.058019520039578984		[learning rate: 0.00026694]
		[batch 20/20] avg loss: 0.0576266538939202		[learning rate: 0.0002663]
	Learning Rate: 0.000266298
	LOSS [training: 0.05782308696674959 | validation: 0.09317317791691815]
	TIME [epoch: 8.82 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08054525973525732		[learning rate: 0.00026565]
		[batch 20/20] avg loss: 0.0761016587882192		[learning rate: 0.00026501]
	Learning Rate: 0.000265011
	LOSS [training: 0.07832345926173827 | validation: 0.060461416059685656]
	TIME [epoch: 8.83 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05963716166764371		[learning rate: 0.00026437]
		[batch 20/20] avg loss: 0.0820897464870596		[learning rate: 0.00026373]
	Learning Rate: 0.000263729
	LOSS [training: 0.07086345407735166 | validation: 0.0842840816869316]
	TIME [epoch: 8.84 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09266804289978077		[learning rate: 0.00026309]
		[batch 20/20] avg loss: 0.062074154814967995		[learning rate: 0.00026245]
	Learning Rate: 0.000262454
	LOSS [training: 0.07737109885737438 | validation: 0.07062626438866035]
	TIME [epoch: 8.82 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0833361528760516		[learning rate: 0.00026182]
		[batch 20/20] avg loss: 0.09773790336665614		[learning rate: 0.00026118]
	Learning Rate: 0.000261184
	LOSS [training: 0.09053702812135386 | validation: 0.06248230428121901]
	TIME [epoch: 8.81 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06816494119809627		[learning rate: 0.00026055]
		[batch 20/20] avg loss: 0.08228507141001747		[learning rate: 0.00025992]
	Learning Rate: 0.000259921
	LOSS [training: 0.07522500630405686 | validation: 0.09633415357307093]
	TIME [epoch: 8.81 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08700315751151899		[learning rate: 0.00025929]
		[batch 20/20] avg loss: 0.07725367874987671		[learning rate: 0.00025866]
	Learning Rate: 0.000258665
	LOSS [training: 0.08212841813069785 | validation: 0.0694284123851448]
	TIME [epoch: 8.82 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15381715390557166		[learning rate: 0.00025804]
		[batch 20/20] avg loss: 0.07992347833320378		[learning rate: 0.00025741]
	Learning Rate: 0.000257414
	LOSS [training: 0.11687031611938772 | validation: 0.14480880248184896]
	TIME [epoch: 8.83 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09126394526541307		[learning rate: 0.00025679]
		[batch 20/20] avg loss: 0.08505620876541316		[learning rate: 0.00025617]
	Learning Rate: 0.000256169
	LOSS [training: 0.08816007701541313 | validation: 0.09746930282906299]
	TIME [epoch: 8.8 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06916158938620959		[learning rate: 0.00025555]
		[batch 20/20] avg loss: 0.06566266099457385		[learning rate: 0.00025493]
	Learning Rate: 0.00025493
	LOSS [training: 0.06741212519039172 | validation: 0.05725694187274234]
	TIME [epoch: 8.81 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06391004041140727		[learning rate: 0.00025431]
		[batch 20/20] avg loss: 0.06734707406808352		[learning rate: 0.0002537]
	Learning Rate: 0.000253697
	LOSS [training: 0.0656285572397454 | validation: 0.05560624053364778]
	TIME [epoch: 8.81 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06872322335354411		[learning rate: 0.00025308]
		[batch 20/20] avg loss: 0.09642075845743822		[learning rate: 0.00025247]
	Learning Rate: 0.00025247
	LOSS [training: 0.08257199090549118 | validation: 0.05615413089718752]
	TIME [epoch: 8.83 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07111649447418471		[learning rate: 0.00025186]
		[batch 20/20] avg loss: 0.046998945853971054		[learning rate: 0.00025125]
	Learning Rate: 0.00025125
	LOSS [training: 0.05905772016407786 | validation: 0.05981031879982632]
	TIME [epoch: 8.83 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06814580887756112		[learning rate: 0.00025064]
		[batch 20/20] avg loss: 0.0915519285907431		[learning rate: 0.00025003]
	Learning Rate: 0.000250035
	LOSS [training: 0.07984886873415212 | validation: 0.13577059287264448]
	TIME [epoch: 8.81 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07718179125937176		[learning rate: 0.00024943]
		[batch 20/20] avg loss: 0.06623258439870704		[learning rate: 0.00024883]
	Learning Rate: 0.000248825
	LOSS [training: 0.07170718782903941 | validation: 0.05608107221253472]
	TIME [epoch: 8.82 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10417768745529947		[learning rate: 0.00024822]
		[batch 20/20] avg loss: 0.05982953008646499		[learning rate: 0.00024762]
	Learning Rate: 0.000247622
	LOSS [training: 0.08200360877088224 | validation: 0.10339436628021281]
	TIME [epoch: 8.8 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06393064041131882		[learning rate: 0.00024702]
		[batch 20/20] avg loss: 0.06947881197133514		[learning rate: 0.00024642]
	Learning Rate: 0.000246425
	LOSS [training: 0.06670472619132697 | validation: 0.07756097859997524]
	TIME [epoch: 8.81 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07401884062434645		[learning rate: 0.00024583]
		[batch 20/20] avg loss: 0.08047296633203095		[learning rate: 0.00024523]
	Learning Rate: 0.000245233
	LOSS [training: 0.07724590347818872 | validation: 0.06861660058790954]
	TIME [epoch: 8.84 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05948745593216972		[learning rate: 0.00024464]
		[batch 20/20] avg loss: 0.0885879490994888		[learning rate: 0.00024405]
	Learning Rate: 0.000244047
	LOSS [training: 0.07403770251582927 | validation: 0.08610561380053039]
	TIME [epoch: 8.8 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0603197097022428		[learning rate: 0.00024346]
		[batch 20/20] avg loss: 0.07507802379833103		[learning rate: 0.00024287]
	Learning Rate: 0.000242867
	LOSS [training: 0.06769886675028691 | validation: 0.05792026826975544]
	TIME [epoch: 8.82 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0920635159354497		[learning rate: 0.00024228]
		[batch 20/20] avg loss: 0.08023274015040316		[learning rate: 0.00024169]
	Learning Rate: 0.000241693
	LOSS [training: 0.08614812804292643 | validation: 0.07260267352730557]
	TIME [epoch: 8.81 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07300159406129746		[learning rate: 0.00024111]
		[batch 20/20] avg loss: 0.052221976950889606		[learning rate: 0.00024052]
	Learning Rate: 0.000240524
	LOSS [training: 0.06261178550609352 | validation: 0.05404375922122571]
	TIME [epoch: 8.81 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0626478911372264		[learning rate: 0.00023994]
		[batch 20/20] avg loss: 0.06519803293843036		[learning rate: 0.00023936]
	Learning Rate: 0.000239361
	LOSS [training: 0.06392296203782838 | validation: 0.05318732324236276]
	TIME [epoch: 8.82 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.059970724966259395		[learning rate: 0.00023878]
		[batch 20/20] avg loss: 0.07253463524105344		[learning rate: 0.0002382]
	Learning Rate: 0.000238203
	LOSS [training: 0.06625268010365641 | validation: 0.07706572852922904]
	TIME [epoch: 8.81 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05828619853292881		[learning rate: 0.00023763]
		[batch 20/20] avg loss: 0.07302544361944605		[learning rate: 0.00023705]
	Learning Rate: 0.000237051
	LOSS [training: 0.06565582107618742 | validation: 0.07708672660717535]
	TIME [epoch: 8.8 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10026267259960706		[learning rate: 0.00023648]
		[batch 20/20] avg loss: 0.06248050935827175		[learning rate: 0.0002359]
	Learning Rate: 0.000235905
	LOSS [training: 0.0813715909789394 | validation: 0.06735695181320439]
	TIME [epoch: 8.81 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0920319288568627		[learning rate: 0.00023533]
		[batch 20/20] avg loss: 0.06296242604858629		[learning rate: 0.00023476]
	Learning Rate: 0.000234764
	LOSS [training: 0.0774971774527245 | validation: 0.07284341884189371]
	TIME [epoch: 8.81 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09212884243862283		[learning rate: 0.0002342]
		[batch 20/20] avg loss: 0.08456670566620385		[learning rate: 0.00023363]
	Learning Rate: 0.000233629
	LOSS [training: 0.08834777405241334 | validation: 0.061701399093149215]
	TIME [epoch: 8.83 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0628034401362201		[learning rate: 0.00023306]
		[batch 20/20] avg loss: 0.06031979678159648		[learning rate: 0.0002325]
	Learning Rate: 0.000232499
	LOSS [training: 0.061561618458908285 | validation: 0.07944501149455184]
	TIME [epoch: 8.81 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06463490441213479		[learning rate: 0.00023194]
		[batch 20/20] avg loss: 0.0611664662332478		[learning rate: 0.00023137]
	Learning Rate: 0.000231375
	LOSS [training: 0.06290068532269129 | validation: 0.06959136396023766]
	TIME [epoch: 8.81 sec]
EPOCH 828/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06300740738591182		[learning rate: 0.00023081]
		[batch 20/20] avg loss: 0.07166405364429583		[learning rate: 0.00023026]
	Learning Rate: 0.000230256
	LOSS [training: 0.06733573051510382 | validation: 0.05911914432507346]
	TIME [epoch: 8.81 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06670783679274751		[learning rate: 0.0002297]
		[batch 20/20] avg loss: 0.06375741929619923		[learning rate: 0.00022914]
	Learning Rate: 0.000229142
	LOSS [training: 0.06523262804447336 | validation: 0.04669479576415324]
	TIME [epoch: 8.81 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05987524793098918		[learning rate: 0.00022859]
		[batch 20/20] avg loss: 0.07321369372123986		[learning rate: 0.00022803]
	Learning Rate: 0.000228034
	LOSS [training: 0.06654447082611452 | validation: 0.0930334460141988]
	TIME [epoch: 8.82 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08940394989782541		[learning rate: 0.00022748]
		[batch 20/20] avg loss: 0.07756824741910259		[learning rate: 0.00022693]
	Learning Rate: 0.000226931
	LOSS [training: 0.08348609865846399 | validation: 0.10345934148274888]
	TIME [epoch: 8.82 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06518393584810617		[learning rate: 0.00022638]
		[batch 20/20] avg loss: 0.07651280511167677		[learning rate: 0.00022583]
	Learning Rate: 0.000225834
	LOSS [training: 0.07084837047989147 | validation: 0.04818745264418196]
	TIME [epoch: 8.81 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06579051755201705		[learning rate: 0.00022529]
		[batch 20/20] avg loss: 0.07709094412960196		[learning rate: 0.00022474]
	Learning Rate: 0.000224742
	LOSS [training: 0.07144073084080951 | validation: 0.06952094854923409]
	TIME [epoch: 8.81 sec]
EPOCH 834/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.060017915031895366		[learning rate: 0.0002242]
		[batch 20/20] avg loss: 0.09371884568608671		[learning rate: 0.00022366]
	Learning Rate: 0.000223655
	LOSS [training: 0.07686838035899105 | validation: 0.27139569280067083]
	TIME [epoch: 8.8 sec]
EPOCH 835/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1071097375769324		[learning rate: 0.00022311]
		[batch 20/20] avg loss: 0.06098304882174306		[learning rate: 0.00022257]
	Learning Rate: 0.000222574
	LOSS [training: 0.08404639319933771 | validation: 0.06305965779840067]
	TIME [epoch: 8.82 sec]
EPOCH 836/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08495245590460672		[learning rate: 0.00022203]
		[batch 20/20] avg loss: 0.06481310561025873		[learning rate: 0.0002215]
	Learning Rate: 0.000221497
	LOSS [training: 0.07488278075743274 | validation: 0.06805154619747482]
	TIME [epoch: 8.82 sec]
EPOCH 837/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07914685405300008		[learning rate: 0.00022096]
		[batch 20/20] avg loss: 0.08475767518632145		[learning rate: 0.00022043]
	Learning Rate: 0.000220426
	LOSS [training: 0.08195226461966076 | validation: 0.053734936174670775]
	TIME [epoch: 8.82 sec]
EPOCH 838/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06947641204793317		[learning rate: 0.00021989]
		[batch 20/20] avg loss: 0.08678728811602826		[learning rate: 0.00021936]
	Learning Rate: 0.00021936
	LOSS [training: 0.0781318500819807 | validation: 0.13364537886263345]
	TIME [epoch: 8.81 sec]
EPOCH 839/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06086676769043679		[learning rate: 0.00021883]
		[batch 20/20] avg loss: 0.08064911098045635		[learning rate: 0.0002183]
	Learning Rate: 0.000218299
	LOSS [training: 0.07075793933544658 | validation: 0.05128585999423285]
	TIME [epoch: 8.82 sec]
EPOCH 840/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07769270871131007		[learning rate: 0.00021777]
		[batch 20/20] avg loss: 0.08069602268879587		[learning rate: 0.00021724]
	Learning Rate: 0.000217244
	LOSS [training: 0.07919436570005298 | validation: 0.07581047975640845]
	TIME [epoch: 8.82 sec]
EPOCH 841/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06291842611583441		[learning rate: 0.00021672]
		[batch 20/20] avg loss: 0.07733984779714995		[learning rate: 0.00021619]
	Learning Rate: 0.000216193
	LOSS [training: 0.07012913695649217 | validation: 0.07889223459245134]
	TIME [epoch: 8.83 sec]
EPOCH 842/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06814621412200414		[learning rate: 0.00021567]
		[batch 20/20] avg loss: 0.05572404023045394		[learning rate: 0.00021515]
	Learning Rate: 0.000215148
	LOSS [training: 0.06193512717622905 | validation: 0.06016954126516343]
	TIME [epoch: 8.81 sec]
EPOCH 843/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05900706254100975		[learning rate: 0.00021463]
		[batch 20/20] avg loss: 0.08555173426047877		[learning rate: 0.00021411]
	Learning Rate: 0.000214107
	LOSS [training: 0.07227939840074425 | validation: 0.07092864590372873]
	TIME [epoch: 8.81 sec]
EPOCH 844/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06053995047191526		[learning rate: 0.00021359]
		[batch 20/20] avg loss: 0.07901537288847207		[learning rate: 0.00021307]
	Learning Rate: 0.000213072
	LOSS [training: 0.06977766168019367 | validation: 0.13142848058035506]
	TIME [epoch: 8.81 sec]
EPOCH 845/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.061689423658209674		[learning rate: 0.00021256]
		[batch 20/20] avg loss: 0.08474134465497393		[learning rate: 0.00021204]
	Learning Rate: 0.000212042
	LOSS [training: 0.0732153841565918 | validation: 0.09176656898321313]
	TIME [epoch: 8.8 sec]
EPOCH 846/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07536557531104036		[learning rate: 0.00021153]
		[batch 20/20] avg loss: 0.07906396922092475		[learning rate: 0.00021102]
	Learning Rate: 0.000211016
	LOSS [training: 0.07721477226598253 | validation: 0.055769296613176034]
	TIME [epoch: 8.83 sec]
EPOCH 847/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08391758520867525		[learning rate: 0.00021051]
		[batch 20/20] avg loss: 0.07625363042836313		[learning rate: 0.00021]
	Learning Rate: 0.000209996
	LOSS [training: 0.08008560781851917 | validation: 0.055747250107727554]
	TIME [epoch: 8.81 sec]
EPOCH 848/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.060507094562259486		[learning rate: 0.00020949]
		[batch 20/20] avg loss: 0.057024937468821466		[learning rate: 0.00020898]
	Learning Rate: 0.00020898
	LOSS [training: 0.058766016015540476 | validation: 0.049240115481489585]
	TIME [epoch: 8.81 sec]
EPOCH 849/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06485748790124128		[learning rate: 0.00020847]
		[batch 20/20] avg loss: 0.06369375998709338		[learning rate: 0.00020797]
	Learning Rate: 0.00020797
	LOSS [training: 0.06427562394416733 | validation: 0.06833298187089963]
	TIME [epoch: 8.81 sec]
EPOCH 850/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06507938361854301		[learning rate: 0.00020747]
		[batch 20/20] avg loss: 0.0854955489353525		[learning rate: 0.00020696]
	Learning Rate: 0.000206964
	LOSS [training: 0.07528746627694775 | validation: 0.11965379158522539]
	TIME [epoch: 8.81 sec]
EPOCH 851/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06696954916944399		[learning rate: 0.00020646]
		[batch 20/20] avg loss: 0.07838237380974325		[learning rate: 0.00020596]
	Learning Rate: 0.000205963
	LOSS [training: 0.07267596148959363 | validation: 0.06623599764301218]
	TIME [epoch: 8.84 sec]
EPOCH 852/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08175448856168958		[learning rate: 0.00020546]
		[batch 20/20] avg loss: 0.06455823533334992		[learning rate: 0.00020497]
	Learning Rate: 0.000204967
	LOSS [training: 0.07315636194751975 | validation: 0.15253222953801426]
	TIME [epoch: 8.81 sec]
EPOCH 853/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08469259073142384		[learning rate: 0.00020447]
		[batch 20/20] avg loss: 0.05627856620215964		[learning rate: 0.00020398]
	Learning Rate: 0.000203976
	LOSS [training: 0.07048557846679174 | validation: 0.08000803794502609]
	TIME [epoch: 8.81 sec]
EPOCH 854/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0741614368983338		[learning rate: 0.00020348]
		[batch 20/20] avg loss: 0.06794594856150955		[learning rate: 0.00020299]
	Learning Rate: 0.00020299
	LOSS [training: 0.07105369272992168 | validation: 0.05368993829921477]
	TIME [epoch: 8.81 sec]
EPOCH 855/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.066246260726645		[learning rate: 0.0002025]
		[batch 20/20] avg loss: 0.06779906976792935		[learning rate: 0.00020201]
	Learning Rate: 0.000202008
	LOSS [training: 0.06702266524728717 | validation: 0.10358173571482257]
	TIME [epoch: 8.8 sec]
EPOCH 856/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10226643678707632		[learning rate: 0.00020152]
		[batch 20/20] avg loss: 0.07117586597739016		[learning rate: 0.00020103]
	Learning Rate: 0.000201031
	LOSS [training: 0.08672115138223324 | validation: 0.0622920155533191]
	TIME [epoch: 8.83 sec]
EPOCH 857/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06783252619084594		[learning rate: 0.00020054]
		[batch 20/20] avg loss: 0.08598608497688716		[learning rate: 0.00020006]
	Learning Rate: 0.000200059
	LOSS [training: 0.07690930558386655 | validation: 0.0837175164835938]
	TIME [epoch: 8.8 sec]
EPOCH 858/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1052624240474354		[learning rate: 0.00019957]
		[batch 20/20] avg loss: 0.07658235561880739		[learning rate: 0.00019909]
	Learning Rate: 0.000199091
	LOSS [training: 0.0909223898331214 | validation: 0.052239172069246304]
	TIME [epoch: 8.8 sec]
EPOCH 859/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05476185708582324		[learning rate: 0.00019861]
		[batch 20/20] avg loss: 0.09756607481364625		[learning rate: 0.00019813]
	Learning Rate: 0.000198129
	LOSS [training: 0.07616396594973476 | validation: 0.11554425612599895]
	TIME [epoch: 8.8 sec]
EPOCH 860/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0818965424882661		[learning rate: 0.00019765]
		[batch 20/20] avg loss: 0.07345370316742814		[learning rate: 0.00019717]
	Learning Rate: 0.000197171
	LOSS [training: 0.07767512282784712 | validation: 0.07354787676521131]
	TIME [epoch: 8.8 sec]
EPOCH 861/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05754033814535583		[learning rate: 0.00019669]
		[batch 20/20] avg loss: 0.06759771460243		[learning rate: 0.00019622]
	Learning Rate: 0.000196217
	LOSS [training: 0.06256902637389292 | validation: 0.06970208562602506]
	TIME [epoch: 8.83 sec]
EPOCH 862/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08511256432770548		[learning rate: 0.00019574]
		[batch 20/20] avg loss: 0.062219309368250195		[learning rate: 0.00019527]
	Learning Rate: 0.000195268
	LOSS [training: 0.07366593684797783 | validation: 0.05736106428438536]
	TIME [epoch: 8.81 sec]
EPOCH 863/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06047390940024909		[learning rate: 0.0001948]
		[batch 20/20] avg loss: 0.06682462333580039		[learning rate: 0.00019432]
	Learning Rate: 0.000194324
	LOSS [training: 0.06364926636802475 | validation: 0.06607421970862173]
	TIME [epoch: 8.8 sec]
EPOCH 864/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05522498012837342		[learning rate: 0.00019385]
		[batch 20/20] avg loss: 0.06374921142632184		[learning rate: 0.00019338]
	Learning Rate: 0.000193384
	LOSS [training: 0.059487095777347634 | validation: 0.06491994625581679]
	TIME [epoch: 8.8 sec]
EPOCH 865/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06499216837731499		[learning rate: 0.00019292]
		[batch 20/20] avg loss: 0.05690700992919793		[learning rate: 0.00019245]
	Learning Rate: 0.000192449
	LOSS [training: 0.060949589153256455 | validation: 0.05535619453523455]
	TIME [epoch: 8.81 sec]
EPOCH 866/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05338475551357137		[learning rate: 0.00019198]
		[batch 20/20] avg loss: 0.048547822836120615		[learning rate: 0.00019152]
	Learning Rate: 0.000191518
	LOSS [training: 0.05096628917484599 | validation: 0.060240409868279374]
	TIME [epoch: 8.83 sec]
EPOCH 867/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06505055711327905		[learning rate: 0.00019105]
		[batch 20/20] avg loss: 0.06253316023846797		[learning rate: 0.00019059]
	Learning Rate: 0.000190592
	LOSS [training: 0.06379185867587352 | validation: 0.05948891967225095]
	TIME [epoch: 8.8 sec]
EPOCH 868/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07238580090129586		[learning rate: 0.00019013]
		[batch 20/20] avg loss: 0.07629256591754788		[learning rate: 0.00018967]
	Learning Rate: 0.000189671
	LOSS [training: 0.07433918340942187 | validation: 0.07517482370912018]
	TIME [epoch: 8.8 sec]
EPOCH 869/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05562483056032256		[learning rate: 0.00018921]
		[batch 20/20] avg loss: 0.06377103427087957		[learning rate: 0.00018875]
	Learning Rate: 0.000188753
	LOSS [training: 0.05969793241560106 | validation: 0.07242385639865136]
	TIME [epoch: 8.8 sec]
EPOCH 870/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06662252858632553		[learning rate: 0.0001883]
		[batch 20/20] avg loss: 0.06861909231678445		[learning rate: 0.00018784]
	Learning Rate: 0.000187841
	LOSS [training: 0.067620810451555 | validation: 0.0546244597553629]
	TIME [epoch: 8.79 sec]
EPOCH 871/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.053601397062666464		[learning rate: 0.00018739]
		[batch 20/20] avg loss: 0.06570849356116035		[learning rate: 0.00018693]
	Learning Rate: 0.000186932
	LOSS [training: 0.0596549453119134 | validation: 0.04980754694952021]
	TIME [epoch: 8.81 sec]
EPOCH 872/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0673011609138687		[learning rate: 0.00018648]
		[batch 20/20] avg loss: 0.0653574314681873		[learning rate: 0.00018603]
	Learning Rate: 0.000186028
	LOSS [training: 0.06632929619102801 | validation: 0.08705997644521224]
	TIME [epoch: 8.82 sec]
EPOCH 873/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06317685059658452		[learning rate: 0.00018558]
		[batch 20/20] avg loss: 0.05563804224359035		[learning rate: 0.00018513]
	Learning Rate: 0.000185129
	LOSS [training: 0.05940744642008744 | validation: 0.046525868087081895]
	TIME [epoch: 8.8 sec]
EPOCH 874/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05918992475134048		[learning rate: 0.00018468]
		[batch 20/20] avg loss: 0.06251408422413006		[learning rate: 0.00018423]
	Learning Rate: 0.000184233
	LOSS [training: 0.06085200448773528 | validation: 0.061008947109484915]
	TIME [epoch: 8.8 sec]
EPOCH 875/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.052665945690902535		[learning rate: 0.00018379]
		[batch 20/20] avg loss: 0.061878218481694294		[learning rate: 0.00018334]
	Learning Rate: 0.000183343
	LOSS [training: 0.05727208208629843 | validation: 0.03499088761732995]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240217_140926/states/model_tr_study2_875.pth
	Model improved!!!
EPOCH 876/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06620753673595951		[learning rate: 0.0001829]
		[batch 20/20] avg loss: 0.05544100008849796		[learning rate: 0.00018246]
	Learning Rate: 0.000182456
	LOSS [training: 0.06082426841222873 | validation: 0.06239247630984245]
	TIME [epoch: 8.81 sec]
EPOCH 877/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06983201605983856		[learning rate: 0.00018201]
		[batch 20/20] avg loss: 0.07707968661224243		[learning rate: 0.00018157]
	Learning Rate: 0.000181574
	LOSS [training: 0.07345585133604049 | validation: 0.050421765577998476]
	TIME [epoch: 8.82 sec]
EPOCH 878/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07085585793862721		[learning rate: 0.00018113]
		[batch 20/20] avg loss: 0.05786934544737905		[learning rate: 0.0001807]
	Learning Rate: 0.000180696
	LOSS [training: 0.06436260169300312 | validation: 0.04990548940944266]
	TIME [epoch: 8.81 sec]
EPOCH 879/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0657934583234999		[learning rate: 0.00018026]
		[batch 20/20] avg loss: 0.06630280464610819		[learning rate: 0.00017982]
	Learning Rate: 0.000179822
	LOSS [training: 0.06604813148480405 | validation: 0.09775612912112333]
	TIME [epoch: 8.81 sec]
EPOCH 880/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07357087060201849		[learning rate: 0.00017939]
		[batch 20/20] avg loss: 0.049803655504911046		[learning rate: 0.00017895]
	Learning Rate: 0.000178952
	LOSS [training: 0.061687263053464766 | validation: 0.10638037475687204]
	TIME [epoch: 8.81 sec]
EPOCH 881/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06955365426566142		[learning rate: 0.00017852]
		[batch 20/20] avg loss: 0.05549028163744203		[learning rate: 0.00017809]
	Learning Rate: 0.000178087
	LOSS [training: 0.06252196795155171 | validation: 0.048884931921735736]
	TIME [epoch: 8.8 sec]
EPOCH 882/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06420868666976084		[learning rate: 0.00017766]
		[batch 20/20] avg loss: 0.054903380547465776		[learning rate: 0.00017723]
	Learning Rate: 0.000177226
	LOSS [training: 0.059556033608613314 | validation: 0.06363378230948243]
	TIME [epoch: 8.81 sec]
EPOCH 883/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.062264289510371516		[learning rate: 0.0001768]
		[batch 20/20] avg loss: 0.0840675171594216		[learning rate: 0.00017637]
	Learning Rate: 0.000176369
	LOSS [training: 0.07316590333489656 | validation: 0.05022900776326642]
	TIME [epoch: 8.8 sec]
EPOCH 884/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08598964015946323		[learning rate: 0.00017594]
		[batch 20/20] avg loss: 0.05688034079890708		[learning rate: 0.00017552]
	Learning Rate: 0.000175516
	LOSS [training: 0.07143499047918514 | validation: 0.0721143440472315]
	TIME [epoch: 8.8 sec]
EPOCH 885/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07231189336786371		[learning rate: 0.00017509]
		[batch 20/20] avg loss: 0.0774927827959998		[learning rate: 0.00017467]
	Learning Rate: 0.000174667
	LOSS [training: 0.07490233808193178 | validation: 0.039507779140402044]
	TIME [epoch: 8.8 sec]
EPOCH 886/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.086413633605358		[learning rate: 0.00017424]
		[batch 20/20] avg loss: 0.04854487646208297		[learning rate: 0.00017382]
	Learning Rate: 0.000173822
	LOSS [training: 0.0674792550337205 | validation: 0.07418584600904818]
	TIME [epoch: 8.81 sec]
EPOCH 887/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0621928817044316		[learning rate: 0.0001734]
		[batch 20/20] avg loss: 0.061985756017078654		[learning rate: 0.00017298]
	Learning Rate: 0.000172982
	LOSS [training: 0.06208931886075514 | validation: 0.06961822296454659]
	TIME [epoch: 8.81 sec]
EPOCH 888/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.062260292658697466		[learning rate: 0.00017256]
		[batch 20/20] avg loss: 0.06032233433648583		[learning rate: 0.00017215]
	Learning Rate: 0.000172145
	LOSS [training: 0.06129131349759165 | validation: 0.0501660106794821]
	TIME [epoch: 8.8 sec]
EPOCH 889/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05996952129404498		[learning rate: 0.00017173]
		[batch 20/20] avg loss: 0.08858685265924207		[learning rate: 0.00017131]
	Learning Rate: 0.000171313
	LOSS [training: 0.07427818697664353 | validation: 0.0732549861983018]
	TIME [epoch: 8.79 sec]
EPOCH 890/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05836419158601641		[learning rate: 0.0001709]
		[batch 20/20] avg loss: 0.08004731031377504		[learning rate: 0.00017048]
	Learning Rate: 0.000170484
	LOSS [training: 0.06920575094989573 | validation: 0.0622301593379843]
	TIME [epoch: 8.8 sec]
EPOCH 891/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06448579028664857		[learning rate: 0.00017007]
		[batch 20/20] avg loss: 0.06093450207437088		[learning rate: 0.00016966]
	Learning Rate: 0.00016966
	LOSS [training: 0.06271014618050971 | validation: 0.04348394787403097]
	TIME [epoch: 8.81 sec]
EPOCH 892/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.061119836443561046		[learning rate: 0.00016925]
		[batch 20/20] avg loss: 0.05406427647792754		[learning rate: 0.00016884]
	Learning Rate: 0.000168839
	LOSS [training: 0.0575920564607443 | validation: 0.04529293416565557]
	TIME [epoch: 8.83 sec]
EPOCH 893/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08015153509674887		[learning rate: 0.00016843]
		[batch 20/20] avg loss: 0.07816195167075353		[learning rate: 0.00016802]
	Learning Rate: 0.000168023
	LOSS [training: 0.0791567433837512 | validation: 0.06020328719036084]
	TIME [epoch: 8.81 sec]
EPOCH 894/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.060015122871009574		[learning rate: 0.00016762]
		[batch 20/20] avg loss: 0.04941360286593906		[learning rate: 0.00016721]
	Learning Rate: 0.00016721
	LOSS [training: 0.05471436286847431 | validation: 0.08011621580012104]
	TIME [epoch: 8.8 sec]
EPOCH 895/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06640664711423461		[learning rate: 0.00016681]
		[batch 20/20] avg loss: 0.06989005384604276		[learning rate: 0.0001664]
	Learning Rate: 0.000166402
	LOSS [training: 0.0681483504801387 | validation: 0.07713622860384466]
	TIME [epoch: 8.8 sec]
EPOCH 896/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06046331900239045		[learning rate: 0.000166]
		[batch 20/20] avg loss: 0.08301304507987915		[learning rate: 0.0001656]
	Learning Rate: 0.000165597
	LOSS [training: 0.0717381820411348 | validation: 0.055416515729259895]
	TIME [epoch: 8.79 sec]
EPOCH 897/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0593508566008078		[learning rate: 0.0001652]
		[batch 20/20] avg loss: 0.05590083496616245		[learning rate: 0.0001648]
	Learning Rate: 0.000164796
	LOSS [training: 0.05762584578348512 | validation: 0.05590468423400384]
	TIME [epoch: 8.83 sec]
EPOCH 898/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05601012396038724		[learning rate: 0.0001644]
		[batch 20/20] avg loss: 0.08005331949173174		[learning rate: 0.000164]
	Learning Rate: 0.000163999
	LOSS [training: 0.0680317217260595 | validation: 0.09501395812765696]
	TIME [epoch: 8.79 sec]
EPOCH 899/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06427238684541867		[learning rate: 0.0001636]
		[batch 20/20] avg loss: 0.051199505871863074		[learning rate: 0.00016321]
	Learning Rate: 0.000163206
	LOSS [training: 0.05773594635864088 | validation: 0.05504629855302076]
	TIME [epoch: 8.8 sec]
EPOCH 900/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09270963477881837		[learning rate: 0.00016281]
		[batch 20/20] avg loss: 0.060750483844213055		[learning rate: 0.00016242]
	Learning Rate: 0.000162417
	LOSS [training: 0.07673005931151569 | validation: 0.05031494473701642]
	TIME [epoch: 8.8 sec]
EPOCH 901/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0676546416990705		[learning rate: 0.00016202]
		[batch 20/20] avg loss: 0.06221289736280222		[learning rate: 0.00016163]
	Learning Rate: 0.000161632
	LOSS [training: 0.06493376953093635 | validation: 0.055721383673446256]
	TIME [epoch: 8.8 sec]
EPOCH 902/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0575895488091525		[learning rate: 0.00016124]
		[batch 20/20] avg loss: 0.06993424731273502		[learning rate: 0.00016085]
	Learning Rate: 0.00016085
	LOSS [training: 0.06376189806094376 | validation: 0.0408794582714914]
	TIME [epoch: 8.82 sec]
EPOCH 903/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06107759832822409		[learning rate: 0.00016046]
		[batch 20/20] avg loss: 0.0678442951361732		[learning rate: 0.00016007]
	Learning Rate: 0.000160072
	LOSS [training: 0.06446094673219863 | validation: 0.05549932969099591]
	TIME [epoch: 8.8 sec]
EPOCH 904/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06926080173844115		[learning rate: 0.00015968]
		[batch 20/20] avg loss: 0.07550102945977438		[learning rate: 0.0001593]
	Learning Rate: 0.000159298
	LOSS [training: 0.07238091559910777 | validation: 0.06231862915344618]
	TIME [epoch: 8.8 sec]
EPOCH 905/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05425263484121786		[learning rate: 0.00015891]
		[batch 20/20] avg loss: 0.05276309158579658		[learning rate: 0.00015853]
	Learning Rate: 0.000158528
	LOSS [training: 0.05350786321350722 | validation: 0.049736164755852]
	TIME [epoch: 8.81 sec]
EPOCH 906/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05631721237251251		[learning rate: 0.00015814]
		[batch 20/20] avg loss: 0.08014377563315402		[learning rate: 0.00015776]
	Learning Rate: 0.000157761
	LOSS [training: 0.06823049400283325 | validation: 0.055738935842238005]
	TIME [epoch: 8.8 sec]
EPOCH 907/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06774025972633281		[learning rate: 0.00015738]
		[batch 20/20] avg loss: 0.07056448723666034		[learning rate: 0.000157]
	Learning Rate: 0.000156998
	LOSS [training: 0.06915237348149655 | validation: 0.05430393415666243]
	TIME [epoch: 8.83 sec]
EPOCH 908/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05482222042276112		[learning rate: 0.00015662]
		[batch 20/20] avg loss: 0.06808901944190907		[learning rate: 0.00015624]
	Learning Rate: 0.000156239
	LOSS [training: 0.0614556199323351 | validation: 0.05598678898444013]
	TIME [epoch: 8.8 sec]
EPOCH 909/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05678270771586817		[learning rate: 0.00015586]
		[batch 20/20] avg loss: 0.07048566751491706		[learning rate: 0.00015548]
	Learning Rate: 0.000155483
	LOSS [training: 0.0636341876153926 | validation: 0.05129259799142311]
	TIME [epoch: 8.8 sec]
EPOCH 910/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.048494338657409465		[learning rate: 0.00015511]
		[batch 20/20] avg loss: 0.053454452422038015		[learning rate: 0.00015473]
	Learning Rate: 0.000154732
	LOSS [training: 0.05097439553972374 | validation: 0.051657993924115235]
	TIME [epoch: 8.8 sec]
EPOCH 911/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05776182925065011		[learning rate: 0.00015436]
		[batch 20/20] avg loss: 0.06630112743580754		[learning rate: 0.00015398]
	Learning Rate: 0.000153983
	LOSS [training: 0.06203147834322884 | validation: 0.06154702446291154]
	TIME [epoch: 8.79 sec]
EPOCH 912/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05068325624842609		[learning rate: 0.00015361]
		[batch 20/20] avg loss: 0.06142716628345464		[learning rate: 0.00015324]
	Learning Rate: 0.000153239
	LOSS [training: 0.056055211265940375 | validation: 0.06912380493225675]
	TIME [epoch: 8.82 sec]
EPOCH 913/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.059661255236552614		[learning rate: 0.00015287]
		[batch 20/20] avg loss: 0.08537008057802328		[learning rate: 0.0001525]
	Learning Rate: 0.000152498
	LOSS [training: 0.07251566790728795 | validation: 0.053482862789135846]
	TIME [epoch: 8.8 sec]
EPOCH 914/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05806606720043735		[learning rate: 0.00015213]
		[batch 20/20] avg loss: 0.06286143554370471		[learning rate: 0.00015176]
	Learning Rate: 0.00015176
	LOSS [training: 0.06046375137207103 | validation: 0.04463408506906724]
	TIME [epoch: 8.83 sec]
EPOCH 915/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05230971209599746		[learning rate: 0.00015139]
		[batch 20/20] avg loss: 0.06109139239925454		[learning rate: 0.00015103]
	Learning Rate: 0.000151026
	LOSS [training: 0.05670055224762599 | validation: 0.04690894857493632]
	TIME [epoch: 8.79 sec]
EPOCH 916/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06563504645866212		[learning rate: 0.00015066]
		[batch 20/20] avg loss: 0.05981443552814173		[learning rate: 0.0001503]
	Learning Rate: 0.000150296
	LOSS [training: 0.06272474099340195 | validation: 0.04348870274522691]
	TIME [epoch: 8.8 sec]
EPOCH 917/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04387149176285607		[learning rate: 0.00014993]
		[batch 20/20] avg loss: 0.06135841979234139		[learning rate: 0.00014957]
	Learning Rate: 0.000149569
	LOSS [training: 0.052614955777598726 | validation: 0.08050341128024414]
	TIME [epoch: 8.81 sec]
EPOCH 918/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05824066746629273		[learning rate: 0.00014921]
		[batch 20/20] avg loss: 0.08040037898790389		[learning rate: 0.00014885]
	Learning Rate: 0.000148846
	LOSS [training: 0.06932052322709832 | validation: 0.09922054196354894]
	TIME [epoch: 8.81 sec]
EPOCH 919/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08001029230216795		[learning rate: 0.00014849]
		[batch 20/20] avg loss: 0.0704170801089803		[learning rate: 0.00014813]
	Learning Rate: 0.000148126
	LOSS [training: 0.07521368620557413 | validation: 0.11057524080716741]
	TIME [epoch: 8.8 sec]
EPOCH 920/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05632665933713199		[learning rate: 0.00014777]
		[batch 20/20] avg loss: 0.0570698042245895		[learning rate: 0.00014741]
	Learning Rate: 0.00014741
	LOSS [training: 0.05669823178086075 | validation: 0.04821722588452804]
	TIME [epoch: 8.8 sec]
EPOCH 921/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.062008601665959354		[learning rate: 0.00014705]
		[batch 20/20] avg loss: 0.051982430531770464		[learning rate: 0.0001467]
	Learning Rate: 0.000146697
	LOSS [training: 0.0569955160988649 | validation: 0.05379972296809505]
	TIME [epoch: 8.8 sec]
EPOCH 922/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.048684739254025396		[learning rate: 0.00014634]
		[batch 20/20] avg loss: 0.054264915741200424		[learning rate: 0.00014599]
	Learning Rate: 0.000145988
	LOSS [training: 0.05147482749761291 | validation: 0.05726001675731897]
	TIME [epoch: 8.81 sec]
EPOCH 923/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05831529451162458		[learning rate: 0.00014563]
		[batch 20/20] avg loss: 0.07794974252946169		[learning rate: 0.00014528]
	Learning Rate: 0.000145282
	LOSS [training: 0.06813251852054313 | validation: 0.046415022121065526]
	TIME [epoch: 8.81 sec]
EPOCH 924/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04957825866488027		[learning rate: 0.00014493]
		[batch 20/20] avg loss: 0.0640496746225083		[learning rate: 0.00014458]
	Learning Rate: 0.000144579
	LOSS [training: 0.05681396664369427 | validation: 0.059556683534497154]
	TIME [epoch: 8.8 sec]
EPOCH 925/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04771381072439739		[learning rate: 0.00014423]
		[batch 20/20] avg loss: 0.05456340200012734		[learning rate: 0.00014388]
	Learning Rate: 0.00014388
	LOSS [training: 0.05113860636226236 | validation: 0.0553206045139725]
	TIME [epoch: 8.79 sec]
EPOCH 926/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05430760719977353		[learning rate: 0.00014353]
		[batch 20/20] avg loss: 0.06588417547172404		[learning rate: 0.00014318]
	Learning Rate: 0.000143184
	LOSS [training: 0.06009589133574879 | validation: 0.055612077735362364]
	TIME [epoch: 8.8 sec]
EPOCH 927/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.058893599493187475		[learning rate: 0.00014284]
		[batch 20/20] avg loss: 0.059264753710637996		[learning rate: 0.00014249]
	Learning Rate: 0.000142492
	LOSS [training: 0.05907917660191273 | validation: 0.09329897965541575]
	TIME [epoch: 8.8 sec]
EPOCH 928/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06278847057312832		[learning rate: 0.00014215]
		[batch 20/20] avg loss: 0.06407015150621646		[learning rate: 0.0001418]
	Learning Rate: 0.000141803
	LOSS [training: 0.06342931103967239 | validation: 0.06750983564778762]
	TIME [epoch: 8.81 sec]
EPOCH 929/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.057606765478786945		[learning rate: 0.00014146]
		[batch 20/20] avg loss: 0.07576775832725234		[learning rate: 0.00014112]
	Learning Rate: 0.000141117
	LOSS [training: 0.06668726190301963 | validation: 0.06543615170416656]
	TIME [epoch: 8.79 sec]
EPOCH 930/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0789965485901139		[learning rate: 0.00014078]
		[batch 20/20] avg loss: 0.057053055929869735		[learning rate: 0.00014043]
	Learning Rate: 0.000140434
	LOSS [training: 0.06802480225999183 | validation: 0.05536591169328975]
	TIME [epoch: 8.79 sec]
EPOCH 931/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06540073678715409		[learning rate: 0.00014009]
		[batch 20/20] avg loss: 0.05657041149844283		[learning rate: 0.00013976]
	Learning Rate: 0.000139755
	LOSS [training: 0.06098557414279847 | validation: 0.04447691794058187]
	TIME [epoch: 8.79 sec]
EPOCH 932/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04824219379647955		[learning rate: 0.00013942]
		[batch 20/20] avg loss: 0.05843131682842147		[learning rate: 0.00013908]
	Learning Rate: 0.00013908
	LOSS [training: 0.05333675531245049 | validation: 0.0735812371075862]
	TIME [epoch: 8.79 sec]
EPOCH 933/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05968522280603922		[learning rate: 0.00013874]
		[batch 20/20] avg loss: 0.06071207965400569		[learning rate: 0.00013841]
	Learning Rate: 0.000138407
	LOSS [training: 0.06019865123002246 | validation: 0.0902069515626125]
	TIME [epoch: 8.83 sec]
EPOCH 934/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07398364723918355		[learning rate: 0.00013807]
		[batch 20/20] avg loss: 0.060972565783630195		[learning rate: 0.00013774]
	Learning Rate: 0.000137738
	LOSS [training: 0.06747810651140687 | validation: 0.05061758540805192]
	TIME [epoch: 8.8 sec]
EPOCH 935/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05300855939165193		[learning rate: 0.0001374]
		[batch 20/20] avg loss: 0.05271783084942677		[learning rate: 0.00013707]
	Learning Rate: 0.000137072
	LOSS [training: 0.052863195120539365 | validation: 0.06547445816451071]
	TIME [epoch: 8.79 sec]
EPOCH 936/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06256033958638237		[learning rate: 0.00013674]
		[batch 20/20] avg loss: 0.06135284881248086		[learning rate: 0.00013641]
	Learning Rate: 0.000136409
	LOSS [training: 0.061956594199431614 | validation: 0.06938250928512371]
	TIME [epoch: 8.8 sec]
EPOCH 937/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.061042413833474383		[learning rate: 0.00013608]
		[batch 20/20] avg loss: 0.06010350389086627		[learning rate: 0.00013575]
	Learning Rate: 0.000135749
	LOSS [training: 0.060572958862170324 | validation: 0.10373911119075521]
	TIME [epoch: 8.8 sec]
EPOCH 938/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06209035128370247		[learning rate: 0.00013542]
		[batch 20/20] avg loss: 0.060392376986197926		[learning rate: 0.00013509]
	Learning Rate: 0.000135093
	LOSS [training: 0.061241364134950196 | validation: 0.03805936817993372]
	TIME [epoch: 8.82 sec]
EPOCH 939/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05830765559155654		[learning rate: 0.00013477]
		[batch 20/20] avg loss: 0.05513375934151845		[learning rate: 0.00013444]
	Learning Rate: 0.000134439
	LOSS [training: 0.05672070746653751 | validation: 0.048293965147462294]
	TIME [epoch: 8.8 sec]
EPOCH 940/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07178113607000654		[learning rate: 0.00013411]
		[batch 20/20] avg loss: 0.05423624899471634		[learning rate: 0.00013379]
	Learning Rate: 0.000133789
	LOSS [training: 0.06300869253236144 | validation: 0.06881969516151887]
	TIME [epoch: 8.79 sec]
EPOCH 941/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06894188127325834		[learning rate: 0.00013347]
		[batch 20/20] avg loss: 0.05915568685381021		[learning rate: 0.00013314]
	Learning Rate: 0.000133142
	LOSS [training: 0.06404878406353429 | validation: 0.04526814575297711]
	TIME [epoch: 8.8 sec]
EPOCH 942/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05792562550540942		[learning rate: 0.00013282]
		[batch 20/20] avg loss: 0.04975470204487514		[learning rate: 0.0001325]
	Learning Rate: 0.000132498
	LOSS [training: 0.05384016377514228 | validation: 0.04920980490507783]
	TIME [epoch: 8.79 sec]
EPOCH 943/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.050620719814835166		[learning rate: 0.00013218]
		[batch 20/20] avg loss: 0.04834661478178026		[learning rate: 0.00013186]
	Learning Rate: 0.000131858
	LOSS [training: 0.049483667298307714 | validation: 0.08650560291613689]
	TIME [epoch: 8.82 sec]
EPOCH 944/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05906030580938994		[learning rate: 0.00013154]
		[batch 20/20] avg loss: 0.056887928925997776		[learning rate: 0.00013122]
	Learning Rate: 0.00013122
	LOSS [training: 0.05797411736769385 | validation: 0.05340497550274785]
	TIME [epoch: 8.8 sec]
EPOCH 945/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06059763013910517		[learning rate: 0.0001309]
		[batch 20/20] avg loss: 0.05609846865308164		[learning rate: 0.00013059]
	Learning Rate: 0.000130585
	LOSS [training: 0.05834804939609341 | validation: 0.04298294432607354]
	TIME [epoch: 8.8 sec]
EPOCH 946/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06402756645631355		[learning rate: 0.00013027]
		[batch 20/20] avg loss: 0.04748891719004347		[learning rate: 0.00012995]
	Learning Rate: 0.000129954
	LOSS [training: 0.05575824182317851 | validation: 0.04072592013050833]
	TIME [epoch: 8.81 sec]
EPOCH 947/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05399772216965278		[learning rate: 0.00012964]
		[batch 20/20] avg loss: 0.044415589309344396		[learning rate: 0.00012933]
	Learning Rate: 0.000129326
	LOSS [training: 0.04920665573949859 | validation: 0.047023738314556224]
	TIME [epoch: 8.8 sec]
EPOCH 948/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05107830657010335		[learning rate: 0.00012901]
		[batch 20/20] avg loss: 0.06018643396542632		[learning rate: 0.0001287]
	Learning Rate: 0.0001287
	LOSS [training: 0.05563237026776483 | validation: 0.04244965124652599]
	TIME [epoch: 8.82 sec]
EPOCH 949/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.060492685638494206		[learning rate: 0.00012839]
		[batch 20/20] avg loss: 0.06315429039581673		[learning rate: 0.00012808]
	Learning Rate: 0.000128078
	LOSS [training: 0.06182348801715545 | validation: 0.059329878924142924]
	TIME [epoch: 8.81 sec]
EPOCH 950/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04383038636591173		[learning rate: 0.00012777]
		[batch 20/20] avg loss: 0.06782275704834649		[learning rate: 0.00012746]
	Learning Rate: 0.000127458
	LOSS [training: 0.05582657170712911 | validation: 0.06784500754627511]
	TIME [epoch: 8.8 sec]
EPOCH 951/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05640235585795493		[learning rate: 0.00012715]
		[batch 20/20] avg loss: 0.061946934785598276		[learning rate: 0.00012684]
	Learning Rate: 0.000126842
	LOSS [training: 0.059174645321776596 | validation: 0.05305245704920423]
	TIME [epoch: 8.8 sec]
EPOCH 952/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06977211973898437		[learning rate: 0.00012653]
		[batch 20/20] avg loss: 0.06364135286930313		[learning rate: 0.00012623]
	Learning Rate: 0.000126229
	LOSS [training: 0.06670673630414373 | validation: 0.04541595368109049]
	TIME [epoch: 8.8 sec]
EPOCH 953/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05173441493446294		[learning rate: 0.00012592]
		[batch 20/20] avg loss: 0.07358314706312892		[learning rate: 0.00012562]
	Learning Rate: 0.000125618
	LOSS [training: 0.06265878099879593 | validation: 0.05771038485156821]
	TIME [epoch: 8.81 sec]
EPOCH 954/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0407478467668584		[learning rate: 0.00012531]
		[batch 20/20] avg loss: 0.06401669073733336		[learning rate: 0.00012501]
	Learning Rate: 0.000125011
	LOSS [training: 0.05238226875209588 | validation: 0.04804545126382041]
	TIME [epoch: 8.82 sec]
EPOCH 955/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05112613807272841		[learning rate: 0.00012471]
		[batch 20/20] avg loss: 0.06269142139091378		[learning rate: 0.00012441]
	Learning Rate: 0.000124406
	LOSS [training: 0.0569087797318211 | validation: 0.06352754571428922]
	TIME [epoch: 8.81 sec]
EPOCH 956/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05848018834193536		[learning rate: 0.00012411]
		[batch 20/20] avg loss: 0.06429851686709319		[learning rate: 0.0001238]
	Learning Rate: 0.000123805
	LOSS [training: 0.06138935260451428 | validation: 0.05039988433995966]
	TIME [epoch: 8.8 sec]
EPOCH 957/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06844646763312515		[learning rate: 0.0001235]
		[batch 20/20] avg loss: 0.07708092984510986		[learning rate: 0.00012321]
	Learning Rate: 0.000123206
	LOSS [training: 0.07276369873911749 | validation: 0.044391476017695386]
	TIME [epoch: 8.8 sec]
EPOCH 958/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05921773235604612		[learning rate: 0.00012291]
		[batch 20/20] avg loss: 0.05398346013808334		[learning rate: 0.00012261]
	Learning Rate: 0.00012261
	LOSS [training: 0.056600596247064726 | validation: 0.04345545722434334]
	TIME [epoch: 8.8 sec]
EPOCH 959/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.058000366557113235		[learning rate: 0.00012231]
		[batch 20/20] avg loss: 0.040990106688409876		[learning rate: 0.00012202]
	Learning Rate: 0.000122017
	LOSS [training: 0.04949523662276156 | validation: 0.046262715185366454]
	TIME [epoch: 8.82 sec]
EPOCH 960/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05407738729762538		[learning rate: 0.00012172]
		[batch 20/20] avg loss: 0.060173173370710833		[learning rate: 0.00012143]
	Learning Rate: 0.000121427
	LOSS [training: 0.05712528033416812 | validation: 0.06696498198714484]
	TIME [epoch: 8.81 sec]
EPOCH 961/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.058660441343512934		[learning rate: 0.00012113]
		[batch 20/20] avg loss: 0.055834143170666165		[learning rate: 0.00012084]
	Learning Rate: 0.00012084
	LOSS [training: 0.057247292257089556 | validation: 0.04344812408191404]
	TIME [epoch: 8.81 sec]
EPOCH 962/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.051629212256845115		[learning rate: 0.00012055]
		[batch 20/20] avg loss: 0.05007218122254738		[learning rate: 0.00012026]
	Learning Rate: 0.000120256
	LOSS [training: 0.05085069673969624 | validation: 0.05563495105831436]
	TIME [epoch: 8.8 sec]
EPOCH 963/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.047821210343587155		[learning rate: 0.00011996]
		[batch 20/20] avg loss: 0.061623384738992225		[learning rate: 0.00011967]
	Learning Rate: 0.000119674
	LOSS [training: 0.0547222975412897 | validation: 0.05622116388527779]
	TIME [epoch: 8.8 sec]
EPOCH 964/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06363144065100469		[learning rate: 0.00011938]
		[batch 20/20] avg loss: 0.05351597813390115		[learning rate: 0.0001191]
	Learning Rate: 0.000119095
	LOSS [training: 0.0585737093924529 | validation: 0.07130750955209414]
	TIME [epoch: 8.82 sec]
EPOCH 965/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06424615544557452		[learning rate: 0.00011881]
		[batch 20/20] avg loss: 0.04683374620243276		[learning rate: 0.00011852]
	Learning Rate: 0.000118519
	LOSS [training: 0.055539950824003634 | validation: 0.04626452925415516]
	TIME [epoch: 8.8 sec]
EPOCH 966/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0514893777229703		[learning rate: 0.00011823]
		[batch 20/20] avg loss: 0.054682386846046795		[learning rate: 0.00011795]
	Learning Rate: 0.000117946
	LOSS [training: 0.053085882284508536 | validation: 0.060834083681424456]
	TIME [epoch: 8.79 sec]
EPOCH 967/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04977173767508484		[learning rate: 0.00011766]
		[batch 20/20] avg loss: 0.06193828562703364		[learning rate: 0.00011738]
	Learning Rate: 0.000117376
	LOSS [training: 0.05585501165105923 | validation: 0.06681325247060078]
	TIME [epoch: 8.79 sec]
EPOCH 968/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.055805540450866685		[learning rate: 0.00011709]
		[batch 20/20] avg loss: 0.06226018109192175		[learning rate: 0.00011681]
	Learning Rate: 0.000116808
	LOSS [training: 0.059032860771394226 | validation: 0.05150812855571165]
	TIME [epoch: 8.8 sec]
EPOCH 969/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05128557223471283		[learning rate: 0.00011653]
		[batch 20/20] avg loss: 0.058793080647182604		[learning rate: 0.00011624]
	Learning Rate: 0.000116243
	LOSS [training: 0.05503932644094771 | validation: 0.04640366538004044]
	TIME [epoch: 8.81 sec]
EPOCH 970/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05826892147502903		[learning rate: 0.00011596]
		[batch 20/20] avg loss: 0.050063151407773995		[learning rate: 0.00011568]
	Learning Rate: 0.000115681
	LOSS [training: 0.05416603644140152 | validation: 0.06565251895737526]
	TIME [epoch: 8.8 sec]
EPOCH 971/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04877664518705675		[learning rate: 0.0001154]
		[batch 20/20] avg loss: 0.05825335427982899		[learning rate: 0.00011512]
	Learning Rate: 0.000115122
	LOSS [training: 0.05351499973344288 | validation: 0.05191141277579242]
	TIME [epoch: 8.79 sec]
EPOCH 972/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05975122229925406		[learning rate: 0.00011484]
		[batch 20/20] avg loss: 0.05607568838513114		[learning rate: 0.00011457]
	Learning Rate: 0.000114565
	LOSS [training: 0.057913455342192596 | validation: 0.05613712326301039]
	TIME [epoch: 8.8 sec]
EPOCH 973/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0472919806352798		[learning rate: 0.00011429]
		[batch 20/20] avg loss: 0.06759303267953716		[learning rate: 0.00011401]
	Learning Rate: 0.000114011
	LOSS [training: 0.057442506657408486 | validation: 0.04753804866082932]
	TIME [epoch: 8.8 sec]
EPOCH 974/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.054646637907358665		[learning rate: 0.00011374]
		[batch 20/20] avg loss: 0.05238633602423398		[learning rate: 0.00011346]
	Learning Rate: 0.00011346
	LOSS [training: 0.05351648696579633 | validation: 0.047129677291357745]
	TIME [epoch: 8.83 sec]
EPOCH 975/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04727496104283621		[learning rate: 0.00011319]
		[batch 20/20] avg loss: 0.07053088745819341		[learning rate: 0.00011291]
	Learning Rate: 0.000112911
	LOSS [training: 0.05890292425051481 | validation: 0.060193602561583806]
	TIME [epoch: 8.81 sec]
EPOCH 976/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0509836181122895		[learning rate: 0.00011264]
		[batch 20/20] avg loss: 0.04989021870150399		[learning rate: 0.00011237]
	Learning Rate: 0.000112365
	LOSS [training: 0.05043691840689675 | validation: 0.07142695588599826]
	TIME [epoch: 8.79 sec]
EPOCH 977/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05513857386047201		[learning rate: 0.00011209]
		[batch 20/20] avg loss: 0.05320255070558928		[learning rate: 0.00011182]
	Learning Rate: 0.000111822
	LOSS [training: 0.054170562283030646 | validation: 0.05007686232501205]
	TIME [epoch: 8.8 sec]
EPOCH 978/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05511074680020489		[learning rate: 0.00011155]
		[batch 20/20] avg loss: 0.0622249913004546		[learning rate: 0.00011128]
	Learning Rate: 0.000111281
	LOSS [training: 0.05866786905032974 | validation: 0.043476585892553966]
	TIME [epoch: 8.8 sec]
EPOCH 979/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.073693680279406		[learning rate: 0.00011101]
		[batch 20/20] avg loss: 0.05049975984394163		[learning rate: 0.00011074]
	Learning Rate: 0.000110743
	LOSS [training: 0.062096720061673806 | validation: 0.051700300671025066]
	TIME [epoch: 8.9 sec]
EPOCH 980/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05297297763123547		[learning rate: 0.00011047]
		[batch 20/20] avg loss: 0.050620088364638126		[learning rate: 0.00011021]
	Learning Rate: 0.000110207
	LOSS [training: 0.051796532997936795 | validation: 0.04906680567127421]
	TIME [epoch: 8.8 sec]
EPOCH 981/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.048631741964506266		[learning rate: 0.00010994]
		[batch 20/20] avg loss: 0.051157649105848024		[learning rate: 0.00010967]
	Learning Rate: 0.000109674
	LOSS [training: 0.04989469553517715 | validation: 0.042552083786281926]
	TIME [epoch: 8.8 sec]
EPOCH 982/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05858817950630389		[learning rate: 0.00010941]
		[batch 20/20] avg loss: 0.05855787515628376		[learning rate: 0.00010914]
	Learning Rate: 0.000109144
	LOSS [training: 0.05857302733129384 | validation: 0.04739075706944146]
	TIME [epoch: 8.8 sec]
EPOCH 983/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05118338560817994		[learning rate: 0.00010888]
		[batch 20/20] avg loss: 0.048810545337444125		[learning rate: 0.00010862]
	Learning Rate: 0.000108616
	LOSS [training: 0.04999696547281203 | validation: 0.05100830438757681]
	TIME [epoch: 8.79 sec]
EPOCH 984/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.051523678322689004		[learning rate: 0.00010835]
		[batch 20/20] avg loss: 0.05166281604717632		[learning rate: 0.00010809]
	Learning Rate: 0.000108091
	LOSS [training: 0.051593247184932654 | validation: 0.08162928411084484]
	TIME [epoch: 8.81 sec]
EPOCH 985/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05071166198060577		[learning rate: 0.00010783]
		[batch 20/20] avg loss: 0.04808925880565039		[learning rate: 0.00010757]
	Learning Rate: 0.000107568
	LOSS [training: 0.049400460393128086 | validation: 0.045782282342507995]
	TIME [epoch: 8.81 sec]
EPOCH 986/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.047495887681022794		[learning rate: 0.00010731]
		[batch 20/20] avg loss: 0.053806602731335716		[learning rate: 0.00010705]
	Learning Rate: 0.000107048
	LOSS [training: 0.05065124520617925 | validation: 0.053219247846510276]
	TIME [epoch: 8.8 sec]
EPOCH 987/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05326005878488249		[learning rate: 0.00010679]
		[batch 20/20] avg loss: 0.07518298341553457		[learning rate: 0.00010653]
	Learning Rate: 0.00010653
	LOSS [training: 0.06422152110020854 | validation: 0.14213086407544298]
	TIME [epoch: 8.8 sec]
EPOCH 988/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06241950845145727		[learning rate: 0.00010627]
		[batch 20/20] avg loss: 0.061706575562891774		[learning rate: 0.00010602]
	Learning Rate: 0.000106015
	LOSS [training: 0.06206304200717454 | validation: 0.07350239330123118]
	TIME [epoch: 8.8 sec]
EPOCH 989/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06618555870541551		[learning rate: 0.00010576]
		[batch 20/20] avg loss: 0.05691353164770081		[learning rate: 0.0001055]
	Learning Rate: 0.000105503
	LOSS [training: 0.061549545176558154 | validation: 0.07357114524365754]
	TIME [epoch: 8.8 sec]
EPOCH 990/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05139301520143209		[learning rate: 0.00010525]
		[batch 20/20] avg loss: 0.05770929014335548		[learning rate: 0.00010499]
	Learning Rate: 0.000104992
	LOSS [training: 0.054551152672393785 | validation: 0.04339554457646946]
	TIME [epoch: 8.81 sec]
EPOCH 991/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04287677406066663		[learning rate: 0.00010474]
		[batch 20/20] avg loss: 0.06447077251700392		[learning rate: 0.00010448]
	Learning Rate: 0.000104485
	LOSS [training: 0.05367377328883528 | validation: 0.043709606270910265]
	TIME [epoch: 8.79 sec]
EPOCH 992/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04926060060700828		[learning rate: 0.00010423]
		[batch 20/20] avg loss: 0.05411509501679844		[learning rate: 0.00010398]
	Learning Rate: 0.000103979
	LOSS [training: 0.051687847811903366 | validation: 0.06430950129976598]
	TIME [epoch: 8.8 sec]
EPOCH 993/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08320341301213421		[learning rate: 0.00010373]
		[batch 20/20] avg loss: 0.06283519704494518		[learning rate: 0.00010348]
	Learning Rate: 0.000103477
	LOSS [training: 0.0730193050285397 | validation: 0.06277249969062709]
	TIME [epoch: 8.8 sec]
EPOCH 994/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06285312213869773		[learning rate: 0.00010323]
		[batch 20/20] avg loss: 0.06299234499721038		[learning rate: 0.00010298]
	Learning Rate: 0.000102976
	LOSS [training: 0.06292273356795405 | validation: 0.054068682946342816]
	TIME [epoch: 8.79 sec]
EPOCH 995/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.055770159179361754		[learning rate: 0.00010273]
		[batch 20/20] avg loss: 0.0445410174489077		[learning rate: 0.00010248]
	Learning Rate: 0.000102478
	LOSS [training: 0.050155588314134726 | validation: 0.06778479696022266]
	TIME [epoch: 8.83 sec]
EPOCH 996/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06768429202908197		[learning rate: 0.00010223]
		[batch 20/20] avg loss: 0.05598544024819731		[learning rate: 0.00010198]
	Learning Rate: 0.000101983
	LOSS [training: 0.061834866138639644 | validation: 0.06048280409606949]
	TIME [epoch: 8.8 sec]
EPOCH 997/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04893251493296151		[learning rate: 0.00010174]
		[batch 20/20] avg loss: 0.051600877007211374		[learning rate: 0.00010149]
	Learning Rate: 0.000101489
	LOSS [training: 0.05026669597008644 | validation: 0.04177100858077279]
	TIME [epoch: 8.77 sec]
EPOCH 998/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0599234772550417		[learning rate: 0.00010124]
		[batch 20/20] avg loss: 0.05250532798169636		[learning rate: 0.000101]
	Learning Rate: 0.000100999
	LOSS [training: 0.05621440261836902 | validation: 0.050272214172135024]
	TIME [epoch: 8.74 sec]
EPOCH 999/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05512144217457575		[learning rate: 0.00010075]
		[batch 20/20] avg loss: 0.04435731285099766		[learning rate: 0.00010051]
	Learning Rate: 0.00010051
	LOSS [training: 0.04973937751278671 | validation: 0.04415092075397341]
	TIME [epoch: 8.74 sec]
EPOCH 1000/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04058628833045592		[learning rate: 0.00010027]
		[batch 20/20] avg loss: 0.0577120945419146		[learning rate: 0.00010002]
	Learning Rate: 0.000100024
	LOSS [training: 0.04914919143618525 | validation: 0.05015258685176078]
	TIME [epoch: 8.78 sec]
Finished training in 8915.460 seconds.
