Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r0', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2917908235

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.83620225620291		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.803590489154434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.819896372678672 | validation: 4.9278373477615025]
	TIME [epoch: 47.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.442113746508425		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.720966602221614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.08154017436502 | validation: 3.103363334896495]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.793155619925616		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6864991464567014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.739827383191158 | validation: 2.238593592265879]
	TIME [epoch: 8.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0797683369837703		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.079116098306705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.079442217645238 | validation: 1.7400928535980211]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7723856061976762		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7231152678261705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7477504370119235 | validation: 1.4685095598499767]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4661411580127879		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2747426582007066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3704419081067474 | validation: 1.1361334756393662]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3984351095768361		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2487850920724515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.323610100824644 | validation: 1.2570155823758729]
	TIME [epoch: 8.89 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.419658802563878		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0679757482636636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2438172754137709 | validation: 0.6835440199707625]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9920991416077316		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3033533881658805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.147726264886806 | validation: 0.8189731451333069]
	TIME [epoch: 8.85 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0652705864309544		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1086173575503526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.086943971990654 | validation: 0.818872836273761]
	TIME [epoch: 8.86 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8632299326299187		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9118514169279466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.887540674778933 | validation: 0.6703291082363672]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6818299240648809		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5685688181805219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6251993711227015 | validation: 0.6100892337100241]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8803607396344656		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6770061573786541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7786834485065597 | validation: 0.5521220665473885]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7489843128745559		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6265348687175871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6877595907960714 | validation: 0.6852898218113179]
	TIME [epoch: 8.87 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5443675594293003		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6490265642087472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5966970618190237 | validation: 0.4247987256284424]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6142808250063677		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6851367363475125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6497087806769402 | validation: 0.5532885361317565]
	TIME [epoch: 8.89 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4814194323142229		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4328219256178995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4571206789660612 | validation: 0.6159414087183993]
	TIME [epoch: 8.87 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5333198108128345		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5182663639173807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5257930873651075 | validation: 0.3491787936186768]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.547862916985156		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46300134713049934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5054321320578276 | validation: 0.5748470706166149]
	TIME [epoch: 8.88 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5385610030263434		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5139807551827218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5262708791045326 | validation: 0.48669336423679965]
	TIME [epoch: 8.9 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5118424779882046		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47440052253194304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49312150026007384 | validation: 0.3024517204632236]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4259543417567967		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6900724659872646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5580134038720306 | validation: 0.34119089696479343]
	TIME [epoch: 8.88 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49123341550259986		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5087288612596133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49998113838110647 | validation: 0.5712050242385935]
	TIME [epoch: 8.87 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.459581041165109		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4838878148807976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4717344280229533 | validation: 1.0663319272776217]
	TIME [epoch: 8.86 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5509273022245881		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.41374169001736405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4823344961209761 | validation: 0.4561739321781799]
	TIME [epoch: 8.9 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6639819862241735		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45645390645762063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5602179463408972 | validation: 0.39126179391137306]
	TIME [epoch: 8.87 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41856438054696704		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49767974098728507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.458122060767126 | validation: 0.3843545133332556]
	TIME [epoch: 8.86 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4196846160630255		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5492716126807309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4844781143718782 | validation: 0.5935929712325283]
	TIME [epoch: 8.86 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6016047796368797		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5321901050426201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5668974423397498 | validation: 0.3789401838822722]
	TIME [epoch: 8.88 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5650206158738158		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47648639288173483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5207535043777751 | validation: 0.3158180887657499]
	TIME [epoch: 8.87 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49164755205982613		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6158165487553161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5537320504075711 | validation: 0.7009947467396579]
	TIME [epoch: 8.87 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.467629348640682		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.445543345276885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4565863469587835 | validation: 0.5087128251420464]
	TIME [epoch: 8.86 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44307677840373627		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5166139315440141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47984535497387526 | validation: 0.449385087225708]
	TIME [epoch: 8.84 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43363626758764473		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43910058545611574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43636842652188035 | validation: 0.5448969103012004]
	TIME [epoch: 8.88 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5016923362637168		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4045765502751508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4531344432694338 | validation: 0.37987638163884907]
	TIME [epoch: 8.87 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44264760954126725		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.35529733499391647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3989724722675919 | validation: 0.355891722630742]
	TIME [epoch: 8.87 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39823249864664556		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48152488504017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4398786918434078 | validation: 0.5211580696673421]
	TIME [epoch: 8.87 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39897471103578375		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4058690989240154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40242190497989955 | validation: 0.45533822098181215]
	TIME [epoch: 8.87 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3801305411143728		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.489143091017345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43463681606585886 | validation: 0.29355996165702497]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37633498808654386		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.34837629544010806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36235564176332596 | validation: 0.354370885289199]
	TIME [epoch: 8.88 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46099166449911183		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45724077871963165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4591162216093717 | validation: 0.3426685885756321]
	TIME [epoch: 8.86 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4077768777436576		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43981595170955046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4237964147266041 | validation: 0.7875075638516952]
	TIME [epoch: 8.86 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4694921335913877		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38485831126466186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4271752224280248 | validation: 0.31700163100390466]
	TIME [epoch: 8.87 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39581196127094687		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4454790495137426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4206455053923448 | validation: 0.5106354281549716]
	TIME [epoch: 8.88 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42788944035596066		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6584675449879469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5431784926719537 | validation: 0.4213280305217817]
	TIME [epoch: 8.86 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37464150576168065		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40256041587397506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3886009608178278 | validation: 0.26853929171099783]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4432652696650964		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5052125684880535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4742389190765749 | validation: 0.2614587348663554]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32786306633503737		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29888141250317635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31337223941910686 | validation: 0.43718041277186914]
	TIME [epoch: 8.89 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4157791669120566		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32323584483049594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3695075058712763 | validation: 0.32274852901020057]
	TIME [epoch: 8.88 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3648980764599036		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.34480388340491086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3548509799324072 | validation: 0.31608575488220725]
	TIME [epoch: 8.88 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4058686416322999		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32523746926599156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3655530554491457 | validation: 0.31929478920049215]
	TIME [epoch: 8.87 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33460332436930695		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3716046811789485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3531040027741277 | validation: 0.384042785300898]
	TIME [epoch: 8.86 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39954766304969425		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4038410467225277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4016943548861109 | validation: 0.3666083362197544]
	TIME [epoch: 8.89 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35855424317786977		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4063154789929008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38243486108538527 | validation: 0.6923659224009687]
	TIME [epoch: 8.88 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4531364280859488		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36019399305810207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4066652105720254 | validation: 0.3042779798052535]
	TIME [epoch: 8.87 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26688508927795784		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.34348269184697916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30518389056246853 | validation: 0.2912054195115279]
	TIME [epoch: 8.87 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43941392313467176		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3283140089663671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3838639660505195 | validation: 0.2239449161267152]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2800678337335659		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26520963196177033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27263873284766815 | validation: 0.27161019363427913]
	TIME [epoch: 8.89 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3004771064670634		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2665342092616111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2835056578643373 | validation: 0.2263620644524792]
	TIME [epoch: 8.86 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2814871981775529		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25777642564385167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2696318119107023 | validation: 0.30422200763650853]
	TIME [epoch: 8.86 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2802068737261268		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3743724784729414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3272896760995341 | validation: 0.19429462220980737]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23216635900761315		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26472577559476546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24844606730118932 | validation: 0.2308405830865106]
	TIME [epoch: 8.89 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21612214430467053		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.34735044362995265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2817362939673116 | validation: 0.2251976527058175]
	TIME [epoch: 8.87 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21181383110881966		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21183721639784775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2118255237533337 | validation: 0.10756796080795859]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19856830747830384		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3255781154529537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26207321146562873 | validation: 0.2262950500246656]
	TIME [epoch: 8.87 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42276718482859393		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20349237954195826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3131297821852761 | validation: 0.14981306253119628]
	TIME [epoch: 8.88 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20737188207594587		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30758284460775015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.257477363341848 | validation: 0.16882751730419882]
	TIME [epoch: 8.89 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18447514378555432		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2071801842774213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1958276640314878 | validation: 0.2122320939064062]
	TIME [epoch: 8.88 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25032095393589127		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2252818258664307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.237801389901161 | validation: 0.14508835450843993]
	TIME [epoch: 8.87 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24629916553431128		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25946149144685304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2528803284905822 | validation: 0.13843820523237346]
	TIME [epoch: 8.87 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22734577700428643		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24392173075446041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23563375387937344 | validation: 0.11804963544827184]
	TIME [epoch: 8.88 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19172021511372703		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19995757669179953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19583889590276332 | validation: 0.109128035728378]
	TIME [epoch: 8.88 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19211530134317173		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18037850248092763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18624690191204968 | validation: 0.24653851832743684]
	TIME [epoch: 8.86 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2640784543331559		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2217551521333277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24291680323324183 | validation: 0.0789727246671228]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21961289737361872		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.31135200615550296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26548245176456087 | validation: 0.29228533749930624]
	TIME [epoch: 8.86 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2062937162110078		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.31744646954314587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26187009287707685 | validation: 0.18029468110976432]
	TIME [epoch: 8.89 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17952013779422005		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32305797677625847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25128905728523926 | validation: 0.1597764947822993]
	TIME [epoch: 8.87 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21036854583318684		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24422793234271295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22729823908794988 | validation: 0.29346671943873937]
	TIME [epoch: 8.88 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2390958837172017		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29939166914285364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2692437764300276 | validation: 0.1701791474179337]
	TIME [epoch: 8.87 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19321732790065355		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18330620813658957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18826176801862154 | validation: 0.12397217648248612]
	TIME [epoch: 8.87 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18436768210288146		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18187886382256255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18312327296272204 | validation: 0.18883559312254414]
	TIME [epoch: 8.89 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21342477405811805		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1839862077131953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1987054908856567 | validation: 0.10831755516347577]
	TIME [epoch: 8.87 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1542419684068806		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22501885378832504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18963041109760284 | validation: 0.1391750353504599]
	TIME [epoch: 8.87 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2554880798741436		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2634613447716505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25947471232289704 | validation: 0.1500645297699677]
	TIME [epoch: 8.86 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2977127495541668		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1555442994436599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2266285244989134 | validation: 0.08985941148051954]
	TIME [epoch: 8.86 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1514422032403462		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21714487106277577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.184293537151561 | validation: 0.11099108439494519]
	TIME [epoch: 8.88 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20047439676541012		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.263493360050352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23198387840788107 | validation: 0.22399970832572486]
	TIME [epoch: 8.86 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18806424343665598		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13554986112038622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16180705227852113 | validation: 0.07607040951421976]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1813136622026395		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14575400127245636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16353383173754796 | validation: 0.035719644361480056]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11799723938316653		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14270891956388093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13035307947352376 | validation: 0.14883950919793887]
	TIME [epoch: 8.89 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18869176013883082		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15294359301374313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17081767657628696 | validation: 0.18474362183895188]
	TIME [epoch: 8.87 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14030080833433378		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3074901919524243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22389550014337906 | validation: 0.16042346485739023]
	TIME [epoch: 8.87 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2594748664808729		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09983533851809764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17965510249948533 | validation: 0.24329020037456817]
	TIME [epoch: 8.87 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2541300425526217		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.166057513351857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21009377795223932 | validation: 0.11811034860790642]
	TIME [epoch: 8.87 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15921135324846541		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1595925050837746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15940192916612 | validation: 0.15398881568826983]
	TIME [epoch: 8.89 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13268051842937734		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20008851902862293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16638451872900012 | validation: 0.15897680858782615]
	TIME [epoch: 8.87 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19621706125149435		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1270648708264183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16164096603895628 | validation: 0.15310663150452775]
	TIME [epoch: 8.87 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14490934117905202		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12492800667381365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13491867392643286 | validation: 0.10886300857013041]
	TIME [epoch: 8.87 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12169578280229572		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11733738511263472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1195165839574652 | validation: 0.14210789768274384]
	TIME [epoch: 8.87 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14068174577651751		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14843778017934334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14455976297793044 | validation: 0.08869538684196725]
	TIME [epoch: 8.89 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12885153324472673		[learning rate: 0.0099891]
		[batch 20/20] avg loss: 0.14589623436410887		[learning rate: 0.009977]
	Learning Rate: 0.009977
	LOSS [training: 0.13737388380441778 | validation: 0.07705821374817241]
	TIME [epoch: 8.87 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16001451598705202		[learning rate: 0.0099649]
		[batch 20/20] avg loss: 0.14121428470242284		[learning rate: 0.0099528]
	Learning Rate: 0.00995285
	LOSS [training: 0.15061440034473744 | validation: 0.23875287151163185]
	TIME [epoch: 8.86 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13768234360487758		[learning rate: 0.0099408]
		[batch 20/20] avg loss: 0.15992498921167878		[learning rate: 0.0099288]
	Learning Rate: 0.00992875
	LOSS [training: 0.1488036664082782 | validation: 0.11727968492963689]
	TIME [epoch: 8.87 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10804154509682293		[learning rate: 0.0099167]
		[batch 20/20] avg loss: 0.10259651596654255		[learning rate: 0.0099047]
	Learning Rate: 0.00990472
	LOSS [training: 0.10531903053168275 | validation: 0.1197617411318877]
	TIME [epoch: 8.88 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19394691485959714		[learning rate: 0.0098927]
		[batch 20/20] avg loss: 0.12359669082981813		[learning rate: 0.0098807]
	Learning Rate: 0.00988074
	LOSS [training: 0.15877180284470763 | validation: 0.1295285190918697]
	TIME [epoch: 8.88 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2504094249187522		[learning rate: 0.0098688]
		[batch 20/20] avg loss: 0.2497261968952828		[learning rate: 0.0098568]
	Learning Rate: 0.00985682
	LOSS [training: 0.25006781090701746 | validation: 0.17729712994789898]
	TIME [epoch: 8.87 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29961136050166615		[learning rate: 0.0098449]
		[batch 20/20] avg loss: 0.12678884079787558		[learning rate: 0.009833]
	Learning Rate: 0.00983296
	LOSS [training: 0.21320010064977088 | validation: 0.09849471736459722]
	TIME [epoch: 8.87 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1436559744671912		[learning rate: 0.009821]
		[batch 20/20] avg loss: 0.12078764265898881		[learning rate: 0.0098092]
	Learning Rate: 0.00980915
	LOSS [training: 0.13222180856309002 | validation: 0.19107443708096222]
	TIME [epoch: 8.84 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16753757829495625		[learning rate: 0.0097973]
		[batch 20/20] avg loss: 0.1549745069740998		[learning rate: 0.0097854]
	Learning Rate: 0.00978541
	LOSS [training: 0.16125604263452806 | validation: 0.5567069561499374]
	TIME [epoch: 8.89 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32840607335187494		[learning rate: 0.0097736]
		[batch 20/20] avg loss: 0.10982898572930422		[learning rate: 0.0097617]
	Learning Rate: 0.00976172
	LOSS [training: 0.21911752954058955 | validation: 0.15360379646728534]
	TIME [epoch: 8.87 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20458700952465655		[learning rate: 0.0097499]
		[batch 20/20] avg loss: 0.14192683524505995		[learning rate: 0.0097381]
	Learning Rate: 0.00973809
	LOSS [training: 0.17325692238485824 | validation: 0.3593709642806582]
	TIME [epoch: 8.86 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19116991660985952		[learning rate: 0.0097263]
		[batch 20/20] avg loss: 0.2490674475284514		[learning rate: 0.0097145]
	Learning Rate: 0.00971451
	LOSS [training: 0.22011868206915547 | validation: 0.1493563012209414]
	TIME [epoch: 8.87 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10811371297644382		[learning rate: 0.0097027]
		[batch 20/20] avg loss: 0.2413075265722855		[learning rate: 0.009691]
	Learning Rate: 0.009691
	LOSS [training: 0.17471061977436464 | validation: 0.2426650769475951]
	TIME [epoch: 8.87 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3828301930268029		[learning rate: 0.0096793]
		[batch 20/20] avg loss: 0.4417154237850441		[learning rate: 0.0096675]
	Learning Rate: 0.00966754
	LOSS [training: 0.41227280840592345 | validation: 0.09687902675756878]
	TIME [epoch: 8.91 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3345332003815701		[learning rate: 0.0096558]
		[batch 20/20] avg loss: 0.11184104909173291		[learning rate: 0.0096441]
	Learning Rate: 0.00964413
	LOSS [training: 0.22318712473665148 | validation: 0.3101432403739224]
	TIME [epoch: 8.87 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2117029736380082		[learning rate: 0.0096325]
		[batch 20/20] avg loss: 0.11054221298691635		[learning rate: 0.0096208]
	Learning Rate: 0.00962078
	LOSS [training: 0.1611225933124623 | validation: 0.0544076592581017]
	TIME [epoch: 8.86 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09279283152006881		[learning rate: 0.0096091]
		[batch 20/20] avg loss: 0.1531545082585412		[learning rate: 0.0095975]
	Learning Rate: 0.00959749
	LOSS [training: 0.122973669889305 | validation: 0.18837853261921028]
	TIME [epoch: 8.87 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28808991847783316		[learning rate: 0.0095859]
		[batch 20/20] avg loss: 0.41504522758283163		[learning rate: 0.0095743]
	Learning Rate: 0.00957426
	LOSS [training: 0.3515675730303324 | validation: 0.11843845406757059]
	TIME [epoch: 8.89 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3079711046327217		[learning rate: 0.0095627]
		[batch 20/20] avg loss: 0.5045983983035216		[learning rate: 0.0095511]
	Learning Rate: 0.00955108
	LOSS [training: 0.40628475146812165 | validation: 0.2182594009147879]
	TIME [epoch: 8.87 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14287811769196154		[learning rate: 0.0095395]
		[batch 20/20] avg loss: 0.12230199463909533		[learning rate: 0.009528]
	Learning Rate: 0.00952796
	LOSS [training: 0.1325900561655284 | validation: 0.027525153290697574]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08749042133447757		[learning rate: 0.0095164]
		[batch 20/20] avg loss: 0.2278088300159673		[learning rate: 0.0095049]
	Learning Rate: 0.0095049
	LOSS [training: 0.15764962567522248 | validation: 0.13887735838918133]
	TIME [epoch: 8.86 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09108564015298674		[learning rate: 0.0094934]
		[batch 20/20] avg loss: 0.09301706168420068		[learning rate: 0.0094819]
	Learning Rate: 0.00948189
	LOSS [training: 0.09205135091859372 | validation: 0.1676126478529524]
	TIME [epoch: 8.87 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1085437362797674		[learning rate: 0.0094704]
		[batch 20/20] avg loss: 0.19483670907582468		[learning rate: 0.0094589]
	Learning Rate: 0.00945893
	LOSS [training: 0.15169022267779608 | validation: 0.1003924384026461]
	TIME [epoch: 8.88 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09666662304222629		[learning rate: 0.0094475]
		[batch 20/20] avg loss: 0.1160186415979706		[learning rate: 0.009436]
	Learning Rate: 0.00943603
	LOSS [training: 0.10634263232009847 | validation: 0.1129075371020776]
	TIME [epoch: 8.87 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11146869365853415		[learning rate: 0.0094246]
		[batch 20/20] avg loss: 0.19245796202698484		[learning rate: 0.0094132]
	Learning Rate: 0.00941319
	LOSS [training: 0.1519633278427595 | validation: 0.08308983925953922]
	TIME [epoch: 8.87 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10836923178367186		[learning rate: 0.0094018]
		[batch 20/20] avg loss: 0.1386342042667464		[learning rate: 0.0093904]
	Learning Rate: 0.0093904
	LOSS [training: 0.12350171802520915 | validation: 0.05133208848895296]
	TIME [epoch: 8.87 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13058209808831445		[learning rate: 0.009379]
		[batch 20/20] avg loss: 0.13029790015722095		[learning rate: 0.0093677]
	Learning Rate: 0.00936767
	LOSS [training: 0.1304399991227677 | validation: 0.08427847484503409]
	TIME [epoch: 8.86 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10499898134505412		[learning rate: 0.0093563]
		[batch 20/20] avg loss: 0.13378561809639883		[learning rate: 0.009345]
	Learning Rate: 0.00934499
	LOSS [training: 0.11939229972072647 | validation: 0.13616185930295788]
	TIME [epoch: 8.89 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1168359220340364		[learning rate: 0.0093337]
		[batch 20/20] avg loss: 0.1388273893465897		[learning rate: 0.0093224]
	Learning Rate: 0.00932237
	LOSS [training: 0.12783165569031302 | validation: 0.15623358862543468]
	TIME [epoch: 8.87 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1510985437955726		[learning rate: 0.0093111]
		[batch 20/20] avg loss: 0.14557270683287374		[learning rate: 0.0092998]
	Learning Rate: 0.0092998
	LOSS [training: 0.14833562531422315 | validation: 0.059326512067847184]
	TIME [epoch: 8.88 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12711139518184938		[learning rate: 0.0092885]
		[batch 20/20] avg loss: 0.17891219057794		[learning rate: 0.0092773]
	Learning Rate: 0.00927729
	LOSS [training: 0.1530117928798947 | validation: 0.2708306468870976]
	TIME [epoch: 8.87 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12985128558091025		[learning rate: 0.0092661]
		[batch 20/20] avg loss: 0.1218546690283103		[learning rate: 0.0092548]
	Learning Rate: 0.00925483
	LOSS [training: 0.1258529773046103 | validation: 0.07184859837426243]
	TIME [epoch: 8.87 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.131626514926249		[learning rate: 0.0092436]
		[batch 20/20] avg loss: 0.12530469849891446		[learning rate: 0.0092324]
	Learning Rate: 0.00923242
	LOSS [training: 0.1284656067125817 | validation: 0.13432015996935387]
	TIME [epoch: 8.88 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16448018810445747		[learning rate: 0.0092212]
		[batch 20/20] avg loss: 0.08977571072474758		[learning rate: 0.0092101]
	Learning Rate: 0.00921007
	LOSS [training: 0.12712794941460254 | validation: 0.06367603847606208]
	TIME [epoch: 8.87 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.137406136743912		[learning rate: 0.0091989]
		[batch 20/20] avg loss: 0.07209867258322017		[learning rate: 0.0091878]
	Learning Rate: 0.00918778
	LOSS [training: 0.10475240466356608 | validation: 0.04584797174159877]
	TIME [epoch: 8.87 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08679318598923048		[learning rate: 0.0091767]
		[batch 20/20] avg loss: 0.10636395320592815		[learning rate: 0.0091655]
	Learning Rate: 0.00916554
	LOSS [training: 0.09657856959757934 | validation: 0.2149075134142536]
	TIME [epoch: 8.86 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2717857573261873		[learning rate: 0.0091544]
		[batch 20/20] avg loss: 0.06832447066955874		[learning rate: 0.0091433]
	Learning Rate: 0.00914335
	LOSS [training: 0.17005511399787304 | validation: 0.06721973638591638]
	TIME [epoch: 8.88 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0961771161234523		[learning rate: 0.0091323]
		[batch 20/20] avg loss: 0.15368379369535295		[learning rate: 0.0091212]
	Learning Rate: 0.00912121
	LOSS [training: 0.12493045490940262 | validation: 0.05567815722002145]
	TIME [epoch: 8.88 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14299162913126698		[learning rate: 0.0091102]
		[batch 20/20] avg loss: 0.11907553071061505		[learning rate: 0.0090991]
	Learning Rate: 0.00909913
	LOSS [training: 0.131033579920941 | validation: 0.13213474501823433]
	TIME [epoch: 8.86 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12405325287624418		[learning rate: 0.0090881]
		[batch 20/20] avg loss: 0.2562463164423433		[learning rate: 0.0090771]
	Learning Rate: 0.0090771
	LOSS [training: 0.19014978465929375 | validation: 0.15481305743645835]
	TIME [epoch: 8.86 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12594849879626602		[learning rate: 0.0090661]
		[batch 20/20] avg loss: 0.12070115586366918		[learning rate: 0.0090551]
	Learning Rate: 0.00905513
	LOSS [training: 0.12332482732996763 | validation: 0.09742884779288466]
	TIME [epoch: 8.86 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07799074009723023		[learning rate: 0.0090442]
		[batch 20/20] avg loss: 0.15125868481121046		[learning rate: 0.0090332]
	Learning Rate: 0.00903321
	LOSS [training: 0.11462471245422032 | validation: 0.1304453612609991]
	TIME [epoch: 8.9 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1127529769981507		[learning rate: 0.0090223]
		[batch 20/20] avg loss: 0.10863950495328775		[learning rate: 0.0090113]
	Learning Rate: 0.00901134
	LOSS [training: 0.11069624097571922 | validation: 0.08765048353690197]
	TIME [epoch: 8.87 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1119202968588903		[learning rate: 0.0090004]
		[batch 20/20] avg loss: 0.09174506003172289		[learning rate: 0.0089895]
	Learning Rate: 0.00898953
	LOSS [training: 0.1018326784453066 | validation: 0.051553295872347686]
	TIME [epoch: 8.86 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0966671749276011		[learning rate: 0.0089786]
		[batch 20/20] avg loss: 0.1607314597338056		[learning rate: 0.0089678]
	Learning Rate: 0.00896776
	LOSS [training: 0.12869931733070333 | validation: 0.2857317974982108]
	TIME [epoch: 8.86 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1590479967099646		[learning rate: 0.0089569]
		[batch 20/20] avg loss: 0.2948184351000315		[learning rate: 0.0089461]
	Learning Rate: 0.00894605
	LOSS [training: 0.2269332159049981 | validation: 0.18073343756039278]
	TIME [epoch: 8.91 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11655296520988234		[learning rate: 0.0089352]
		[batch 20/20] avg loss: 0.10608431966669751		[learning rate: 0.0089244]
	Learning Rate: 0.0089244
	LOSS [training: 0.11131864243828993 | validation: 0.11544870527166304]
	TIME [epoch: 8.9 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0832447061333639		[learning rate: 0.0089136]
		[batch 20/20] avg loss: 0.11928171170593202		[learning rate: 0.0089028]
	Learning Rate: 0.00890279
	LOSS [training: 0.10126320891964798 | validation: 0.07597678454169811]
	TIME [epoch: 8.87 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10661209121858403		[learning rate: 0.008892]
		[batch 20/20] avg loss: 0.15459591349905616		[learning rate: 0.0088812]
	Learning Rate: 0.00888124
	LOSS [training: 0.1306040023588201 | validation: 0.12243954711264574]
	TIME [epoch: 8.86 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07942217336350021		[learning rate: 0.0088705]
		[batch 20/20] avg loss: 0.08130597250401506		[learning rate: 0.0088597]
	Learning Rate: 0.00885974
	LOSS [training: 0.08036407293375765 | validation: 0.17820346346592605]
	TIME [epoch: 8.86 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13200265147278162		[learning rate: 0.008849]
		[batch 20/20] avg loss: 0.10309115625780736		[learning rate: 0.0088383]
	Learning Rate: 0.00883829
	LOSS [training: 0.1175469038652945 | validation: 0.07168773809430631]
	TIME [epoch: 8.88 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0906712738512362		[learning rate: 0.0088276]
		[batch 20/20] avg loss: 0.09706632080964268		[learning rate: 0.0088169]
	Learning Rate: 0.0088169
	LOSS [training: 0.09386879733043943 | validation: 0.06282522074534384]
	TIME [epoch: 8.88 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15722099799851466		[learning rate: 0.0088062]
		[batch 20/20] avg loss: 0.07165249581156997		[learning rate: 0.0087956]
	Learning Rate: 0.00879555
	LOSS [training: 0.11443674690504232 | validation: 0.05476965458284798]
	TIME [epoch: 8.86 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13872182138267092		[learning rate: 0.0087849]
		[batch 20/20] avg loss: 0.11277890575728325		[learning rate: 0.0087743]
	Learning Rate: 0.00877426
	LOSS [training: 0.1257503635699771 | validation: 0.18409476319717116]
	TIME [epoch: 8.86 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1094400291204763		[learning rate: 0.0087636]
		[batch 20/20] avg loss: 0.12998707630779968		[learning rate: 0.008753]
	Learning Rate: 0.00875302
	LOSS [training: 0.11971355271413801 | validation: 0.11046763550113409]
	TIME [epoch: 8.86 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08425499770940628		[learning rate: 0.0087424]
		[batch 20/20] avg loss: 0.0957879031015249		[learning rate: 0.0087318]
	Learning Rate: 0.00873183
	LOSS [training: 0.0900214504054656 | validation: 0.11888581203796697]
	TIME [epoch: 8.89 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10513248344568178		[learning rate: 0.0087213]
		[batch 20/20] avg loss: 0.10697222139962718		[learning rate: 0.0087107]
	Learning Rate: 0.00871069
	LOSS [training: 0.1060523524226545 | validation: 0.05557143237979464]
	TIME [epoch: 8.88 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10533336730666967		[learning rate: 0.0087001]
		[batch 20/20] avg loss: 0.08964767119727791		[learning rate: 0.0086896]
	Learning Rate: 0.0086896
	LOSS [training: 0.09749051925197376 | validation: 0.028294149145624347]
	TIME [epoch: 8.86 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08792669957325712		[learning rate: 0.0086791]
		[batch 20/20] avg loss: 0.0837452989256956		[learning rate: 0.0086686]
	Learning Rate: 0.00866857
	LOSS [training: 0.08583599924947635 | validation: 0.08447655157590102]
	TIME [epoch: 8.87 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14578541339454523		[learning rate: 0.0086581]
		[batch 20/20] avg loss: 0.13340359715618258		[learning rate: 0.0086476]
	Learning Rate: 0.00864758
	LOSS [training: 0.13959450527536393 | validation: 0.17737512556813534]
	TIME [epoch: 8.87 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16748857611385043		[learning rate: 0.0086371]
		[batch 20/20] avg loss: 0.14350646930891992		[learning rate: 0.0086266]
	Learning Rate: 0.00862665
	LOSS [training: 0.1554975227113852 | validation: 0.17894006875741603]
	TIME [epoch: 8.89 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12526737343376		[learning rate: 0.0086162]
		[batch 20/20] avg loss: 0.12711643594431318		[learning rate: 0.0086058]
	Learning Rate: 0.00860576
	LOSS [training: 0.12619190468903657 | validation: 0.05529742664145231]
	TIME [epoch: 8.87 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07895861245139339		[learning rate: 0.0085953]
		[batch 20/20] avg loss: 0.09049518076980202		[learning rate: 0.0085849]
	Learning Rate: 0.00858493
	LOSS [training: 0.08472689661059771 | validation: 0.05585226069364216]
	TIME [epoch: 8.86 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11649633426598487		[learning rate: 0.0085745]
		[batch 20/20] avg loss: 0.11665756111775301		[learning rate: 0.0085641]
	Learning Rate: 0.00856415
	LOSS [training: 0.11657694769186892 | validation: 0.09733108120103784]
	TIME [epoch: 8.86 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1633954469412904		[learning rate: 0.0085538]
		[batch 20/20] avg loss: 0.20111982253907934		[learning rate: 0.0085434]
	Learning Rate: 0.00854342
	LOSS [training: 0.1822576347401849 | validation: 0.060018712362997564]
	TIME [epoch: 8.87 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08952143838723627		[learning rate: 0.0085331]
		[batch 20/20] avg loss: 0.09045896115975662		[learning rate: 0.0085227]
	Learning Rate: 0.00852273
	LOSS [training: 0.08999019977349645 | validation: 0.04348610305084627]
	TIME [epoch: 8.89 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09983485891137926		[learning rate: 0.0085124]
		[batch 20/20] avg loss: 0.14028011804719015		[learning rate: 0.0085021]
	Learning Rate: 0.0085021
	LOSS [training: 0.12005748847928469 | validation: 0.036275263752124705]
	TIME [epoch: 8.87 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0812714444579181		[learning rate: 0.0084918]
		[batch 20/20] avg loss: 0.11161806671318286		[learning rate: 0.0084815]
	Learning Rate: 0.00848152
	LOSS [training: 0.09644475558555048 | validation: 0.048200540763882145]
	TIME [epoch: 8.87 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08496721311422317		[learning rate: 0.0084712]
		[batch 20/20] avg loss: 0.125059473534151		[learning rate: 0.008461]
	Learning Rate: 0.00846099
	LOSS [training: 0.1050133433241871 | validation: 0.23180718091393424]
	TIME [epoch: 8.86 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10939943928683298		[learning rate: 0.0084507]
		[batch 20/20] avg loss: 0.10658467262411271		[learning rate: 0.0084405]
	Learning Rate: 0.0084405
	LOSS [training: 0.10799205595547286 | validation: 0.06624978355532603]
	TIME [epoch: 8.89 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08678486697436055		[learning rate: 0.0084303]
		[batch 20/20] avg loss: 0.10600015104300264		[learning rate: 0.0084201]
	Learning Rate: 0.00842007
	LOSS [training: 0.0963925090086816 | validation: 0.08287044547987973]
	TIME [epoch: 8.88 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08331106457784232		[learning rate: 0.0084099]
		[batch 20/20] avg loss: 0.0947771000695792		[learning rate: 0.0083997]
	Learning Rate: 0.00839969
	LOSS [training: 0.08904408232371078 | validation: 0.047138469812834885]
	TIME [epoch: 8.86 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08026091807323367		[learning rate: 0.0083895]
		[batch 20/20] avg loss: 0.12652518210529182		[learning rate: 0.0083794]
	Learning Rate: 0.00837935
	LOSS [training: 0.10339305008926274 | validation: 0.08476234104313621]
	TIME [epoch: 8.87 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1410473958026046		[learning rate: 0.0083692]
		[batch 20/20] avg loss: 0.22029768557313173		[learning rate: 0.0083591]
	Learning Rate: 0.00835907
	LOSS [training: 0.18067254068786814 | validation: 0.20541498177398193]
	TIME [epoch: 8.86 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22270630756894222		[learning rate: 0.0083489]
		[batch 20/20] avg loss: 0.22678943201738067		[learning rate: 0.0083388]
	Learning Rate: 0.00833883
	LOSS [training: 0.22474786979316144 | validation: 0.08651948724437009]
	TIME [epoch: 8.89 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09974325875236571		[learning rate: 0.0083287]
		[batch 20/20] avg loss: 0.11421127520684908		[learning rate: 0.0083186]
	Learning Rate: 0.00831864
	LOSS [training: 0.10697726697960738 | validation: 0.07418774532042478]
	TIME [epoch: 8.87 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09058523772913189		[learning rate: 0.0083086]
		[batch 20/20] avg loss: 0.12945085555440367		[learning rate: 0.0082985]
	Learning Rate: 0.00829851
	LOSS [training: 0.1100180466417678 | validation: 0.0713445237638468]
	TIME [epoch: 8.87 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1213329034971129		[learning rate: 0.0082885]
		[batch 20/20] avg loss: 0.09180595608125208		[learning rate: 0.0082784]
	Learning Rate: 0.00827842
	LOSS [training: 0.10656942978918246 | validation: 0.04823563331183611]
	TIME [epoch: 8.86 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07007982585249978		[learning rate: 0.0082684]
		[batch 20/20] avg loss: 0.07962759604127292		[learning rate: 0.0082584]
	Learning Rate: 0.00825838
	LOSS [training: 0.07485371094688634 | validation: 0.14393609125921789]
	TIME [epoch: 8.87 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11444523355128233		[learning rate: 0.0082484]
		[batch 20/20] avg loss: 0.20592288422228638		[learning rate: 0.0082384]
	Learning Rate: 0.00823839
	LOSS [training: 0.1601840588867844 | validation: 0.21136537502609146]
	TIME [epoch: 8.89 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1624443513792244		[learning rate: 0.0082284]
		[batch 20/20] avg loss: 0.3233883047906259		[learning rate: 0.0082184]
	Learning Rate: 0.00821844
	LOSS [training: 0.24291632808492514 | validation: 0.23833275627262288]
	TIME [epoch: 8.86 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12951861642393028		[learning rate: 0.0082085]
		[batch 20/20] avg loss: 0.15955544121010043		[learning rate: 0.0081985]
	Learning Rate: 0.00819855
	LOSS [training: 0.14453702881701533 | validation: 0.1811608194678056]
	TIME [epoch: 8.87 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2196435527900821		[learning rate: 0.0081886]
		[batch 20/20] avg loss: 0.16865672823691574		[learning rate: 0.0081787]
	Learning Rate: 0.0081787
	LOSS [training: 0.1941501405134989 | validation: 0.11775065539986872]
	TIME [epoch: 8.87 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12590483620902762		[learning rate: 0.0081688]
		[batch 20/20] avg loss: 0.08503527829002297		[learning rate: 0.0081589]
	Learning Rate: 0.0081589
	LOSS [training: 0.1054700572495253 | validation: 0.06538670438890642]
	TIME [epoch: 8.89 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11023901313628874		[learning rate: 0.008149]
		[batch 20/20] avg loss: 0.0881232336358403		[learning rate: 0.0081391]
	Learning Rate: 0.00813915
	LOSS [training: 0.09918112338606452 | validation: 0.1678857937150813]
	TIME [epoch: 8.88 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10669175929600168		[learning rate: 0.0081293]
		[batch 20/20] avg loss: 0.13262902213137467		[learning rate: 0.0081194]
	Learning Rate: 0.00811944
	LOSS [training: 0.11966039071368817 | validation: 0.07913805149923755]
	TIME [epoch: 8.87 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13585105260115457		[learning rate: 0.0081096]
		[batch 20/20] avg loss: 0.09349785923860107		[learning rate: 0.0080998]
	Learning Rate: 0.00809979
	LOSS [training: 0.11467445591987786 | validation: 0.06458739391624203]
	TIME [epoch: 8.86 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07718184673490533		[learning rate: 0.00809]
		[batch 20/20] avg loss: 0.11088730489024463		[learning rate: 0.0080802]
	Learning Rate: 0.00808018
	LOSS [training: 0.09403457581257496 | validation: 0.24166362244731304]
	TIME [epoch: 8.87 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1290028550032411		[learning rate: 0.0080704]
		[batch 20/20] avg loss: 0.09613711230013626		[learning rate: 0.0080606]
	Learning Rate: 0.00806062
	LOSS [training: 0.11256998365168869 | validation: 0.10942430515183742]
	TIME [epoch: 8.9 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11945794736649007		[learning rate: 0.0080509]
		[batch 20/20] avg loss: 0.09761836086373035		[learning rate: 0.0080411]
	Learning Rate: 0.00804111
	LOSS [training: 0.10853815411511022 | validation: 0.12010849638078978]
	TIME [epoch: 8.87 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07988566665374804		[learning rate: 0.0080314]
		[batch 20/20] avg loss: 0.13935268087057975		[learning rate: 0.0080216]
	Learning Rate: 0.00802164
	LOSS [training: 0.10961917376216387 | validation: 0.06433127725020613]
	TIME [epoch: 8.87 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07570131308795389		[learning rate: 0.0080119]
		[batch 20/20] avg loss: 0.10689839822539264		[learning rate: 0.0080022]
	Learning Rate: 0.00800222
	LOSS [training: 0.09129985565667328 | validation: 0.12082989754072618]
	TIME [epoch: 8.87 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08150048000171861		[learning rate: 0.0079925]
		[batch 20/20] avg loss: 0.10604426488090013		[learning rate: 0.0079828]
	Learning Rate: 0.00798285
	LOSS [training: 0.09377237244130937 | validation: 0.237033563138285]
	TIME [epoch: 8.87 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15298747289161382		[learning rate: 0.0079732]
		[batch 20/20] avg loss: 0.09527220262552896		[learning rate: 0.0079635]
	Learning Rate: 0.00796352
	LOSS [training: 0.1241298377585714 | validation: 0.16126238201057091]
	TIME [epoch: 8.89 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1286389491521812		[learning rate: 0.0079539]
		[batch 20/20] avg loss: 0.1725818883130042		[learning rate: 0.0079442]
	Learning Rate: 0.00794424
	LOSS [training: 0.15061041873259268 | validation: 0.22608034691154227]
	TIME [epoch: 8.87 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08269303442028447		[learning rate: 0.0079346]
		[batch 20/20] avg loss: 0.33329846334568203		[learning rate: 0.007925]
	Learning Rate: 0.00792501
	LOSS [training: 0.20799574888298325 | validation: 0.053816524018288535]
	TIME [epoch: 8.86 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12087741458208272		[learning rate: 0.0079154]
		[batch 20/20] avg loss: 0.10429972700827746		[learning rate: 0.0079058]
	Learning Rate: 0.00790583
	LOSS [training: 0.1125885707951801 | validation: 0.331736716916488]
	TIME [epoch: 8.87 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11551612848266749		[learning rate: 0.0078963]
		[batch 20/20] avg loss: 0.23505693760211047		[learning rate: 0.0078867]
	Learning Rate: 0.00788669
	LOSS [training: 0.17528653304238898 | validation: 0.24199480737218432]
	TIME [epoch: 8.88 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1746219997311147		[learning rate: 0.0078771]
		[batch 20/20] avg loss: 0.11462314891730149		[learning rate: 0.0078676]
	Learning Rate: 0.0078676
	LOSS [training: 0.14462257432420814 | validation: 0.05281257003365666]
	TIME [epoch: 8.91 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08019025907708707		[learning rate: 0.0078581]
		[batch 20/20] avg loss: 0.07944999624947766		[learning rate: 0.0078486]
	Learning Rate: 0.00784855
	LOSS [training: 0.07982012766328236 | validation: 0.07928131857747023]
	TIME [epoch: 8.87 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14695291090943516		[learning rate: 0.007839]
		[batch 20/20] avg loss: 0.08843925851122625		[learning rate: 0.0078296]
	Learning Rate: 0.00782955
	LOSS [training: 0.1176960847103307 | validation: 0.056220394241825865]
	TIME [epoch: 8.87 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09043541528619035		[learning rate: 0.0078201]
		[batch 20/20] avg loss: 0.06904661268015497		[learning rate: 0.0078106]
	Learning Rate: 0.0078106
	LOSS [training: 0.07974101398317265 | validation: 0.05894479974802939]
	TIME [epoch: 8.87 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3874741811585177		[learning rate: 0.0078011]
		[batch 20/20] avg loss: 0.11660244058665326		[learning rate: 0.0077917]
	Learning Rate: 0.00779169
	LOSS [training: 0.2520383108725855 | validation: 0.10794734551969773]
	TIME [epoch: 8.88 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06829262157882865		[learning rate: 0.0077823]
		[batch 20/20] avg loss: 0.16836719621750248		[learning rate: 0.0077728]
	Learning Rate: 0.00777283
	LOSS [training: 0.1183299088981656 | validation: 0.09602440233610847]
	TIME [epoch: 8.87 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13650143496113556		[learning rate: 0.0077634]
		[batch 20/20] avg loss: 0.061240991615943166		[learning rate: 0.007754]
	Learning Rate: 0.00775401
	LOSS [training: 0.09887121328853936 | validation: 0.08767343088600932]
	TIME [epoch: 8.86 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08592811022819125		[learning rate: 0.0077446]
		[batch 20/20] avg loss: 0.07795091536054011		[learning rate: 0.0077352]
	Learning Rate: 0.00773524
	LOSS [training: 0.08193951279436568 | validation: 0.1325847771340125]
	TIME [epoch: 8.87 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0760715647202194		[learning rate: 0.0077259]
		[batch 20/20] avg loss: 0.09087340642652361		[learning rate: 0.0077165]
	Learning Rate: 0.00771651
	LOSS [training: 0.08347248557337152 | validation: 0.13159953327338755]
	TIME [epoch: 8.88 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09722527612306231		[learning rate: 0.0077072]
		[batch 20/20] avg loss: 0.0895179981217894		[learning rate: 0.0076978]
	Learning Rate: 0.00769783
	LOSS [training: 0.09337163712242583 | validation: 0.1036341979187221]
	TIME [epoch: 8.89 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07741643736988195		[learning rate: 0.0076885]
		[batch 20/20] avg loss: 0.11449689362034868		[learning rate: 0.0076792]
	Learning Rate: 0.0076792
	LOSS [training: 0.0959566654951153 | validation: 0.08735418123474012]
	TIME [epoch: 8.87 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09884820145494234		[learning rate: 0.0076699]
		[batch 20/20] avg loss: 0.07293358292490267		[learning rate: 0.0076606]
	Learning Rate: 0.00766061
	LOSS [training: 0.0858908921899225 | validation: 0.05774534114856335]
	TIME [epoch: 8.87 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20271301092283517		[learning rate: 0.0076513]
		[batch 20/20] avg loss: 0.11227674318819396		[learning rate: 0.0076421]
	Learning Rate: 0.00764206
	LOSS [training: 0.15749487705551457 | validation: 0.02709929488530704]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06764084022683163		[learning rate: 0.0076328]
		[batch 20/20] avg loss: 0.061920934695011		[learning rate: 0.0076236]
	Learning Rate: 0.00762356
	LOSS [training: 0.06478088746092132 | validation: 0.07905627079680284]
	TIME [epoch: 8.87 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08609965918222584		[learning rate: 0.0076143]
		[batch 20/20] avg loss: 0.08433970695863616		[learning rate: 0.0076051]
	Learning Rate: 0.00760511
	LOSS [training: 0.085219683070431 | validation: 0.050386075421478715]
	TIME [epoch: 8.89 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07496681739102276		[learning rate: 0.0075959]
		[batch 20/20] avg loss: 0.07927839600130537		[learning rate: 0.0075867]
	Learning Rate: 0.00758669
	LOSS [training: 0.07712260669616407 | validation: 0.04374148715959557]
	TIME [epoch: 8.86 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08457417248600267		[learning rate: 0.0075775]
		[batch 20/20] avg loss: 0.13722938043257651		[learning rate: 0.0075683]
	Learning Rate: 0.00756833
	LOSS [training: 0.1109017764592896 | validation: 0.12845209999020274]
	TIME [epoch: 8.86 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15532700182493814		[learning rate: 0.0075592]
		[batch 20/20] avg loss: 0.14849402329974995		[learning rate: 0.00755]
	Learning Rate: 0.00755001
	LOSS [training: 0.15191051256234406 | validation: 0.07160470301900419]
	TIME [epoch: 8.87 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06681136841594489		[learning rate: 0.0075409]
		[batch 20/20] avg loss: 0.09405903221467851		[learning rate: 0.0075317]
	Learning Rate: 0.00753173
	LOSS [training: 0.0804352003153117 | validation: 0.19452299819567584]
	TIME [epoch: 8.89 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07826687036254751		[learning rate: 0.0075226]
		[batch 20/20] avg loss: 0.09477288520722996		[learning rate: 0.0075135]
	Learning Rate: 0.0075135
	LOSS [training: 0.08651987778488873 | validation: 0.105172198307376]
	TIME [epoch: 8.88 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07876614602514795		[learning rate: 0.0075044]
		[batch 20/20] avg loss: 0.08274531941302671		[learning rate: 0.0074953]
	Learning Rate: 0.00749531
	LOSS [training: 0.08075573271908734 | validation: 0.03926781212183556]
	TIME [epoch: 8.87 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060778739107148726		[learning rate: 0.0074862]
		[batch 20/20] avg loss: 0.1201561187969529		[learning rate: 0.0074772]
	Learning Rate: 0.00747716
	LOSS [training: 0.09046742895205082 | validation: 0.03253710274167769]
	TIME [epoch: 8.87 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1358204031805887		[learning rate: 0.0074681]
		[batch 20/20] avg loss: 0.07254489247436839		[learning rate: 0.0074591]
	Learning Rate: 0.00745906
	LOSS [training: 0.10418264782747857 | validation: 0.07381220269815336]
	TIME [epoch: 8.87 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12046672257822974		[learning rate: 0.00745]
		[batch 20/20] avg loss: 0.10972840296873816		[learning rate: 0.007441]
	Learning Rate: 0.007441
	LOSS [training: 0.11509756277348393 | validation: 0.0824269450733329]
	TIME [epoch: 8.9 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.090036545238592		[learning rate: 0.007432]
		[batch 20/20] avg loss: 0.06634397359558233		[learning rate: 0.007423]
	Learning Rate: 0.00742299
	LOSS [training: 0.07819025941708715 | validation: 0.10633387115279602]
	TIME [epoch: 8.87 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11505762059880613		[learning rate: 0.007414]
		[batch 20/20] avg loss: 0.11869815743993342		[learning rate: 0.007405]
	Learning Rate: 0.00740502
	LOSS [training: 0.11687788901936975 | validation: 0.06424514964551437]
	TIME [epoch: 8.86 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07948616895755603		[learning rate: 0.0073961]
		[batch 20/20] avg loss: 0.0788880711442745		[learning rate: 0.0073871]
	Learning Rate: 0.0073871
	LOSS [training: 0.07918712005091526 | validation: 0.01978951882303424]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07073632495924144		[learning rate: 0.0073781]
		[batch 20/20] avg loss: 0.10216048996785843		[learning rate: 0.0073692]
	Learning Rate: 0.00736921
	LOSS [training: 0.08644840746354993 | validation: 0.03427428407617421]
	TIME [epoch: 8.87 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08577621305192998		[learning rate: 0.0073603]
		[batch 20/20] avg loss: 0.08539412901699982		[learning rate: 0.0073514]
	Learning Rate: 0.00735137
	LOSS [training: 0.08558517103446492 | validation: 0.07370906138307112]
	TIME [epoch: 8.9 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11468938350456773		[learning rate: 0.0073425]
		[batch 20/20] avg loss: 0.1753144143993273		[learning rate: 0.0073336]
	Learning Rate: 0.00733358
	LOSS [training: 0.14500189895194754 | validation: 0.12029913357088629]
	TIME [epoch: 8.88 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13004211051397263		[learning rate: 0.0073247]
		[batch 20/20] avg loss: 0.1533837996939682		[learning rate: 0.0073158]
	Learning Rate: 0.00731582
	LOSS [training: 0.14171295510397044 | validation: 0.17667628660034032]
	TIME [epoch: 8.87 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09171402593649139		[learning rate: 0.007307]
		[batch 20/20] avg loss: 0.07421332486984546		[learning rate: 0.0072981]
	Learning Rate: 0.00729811
	LOSS [training: 0.08296367540316843 | validation: 0.10049207694339742]
	TIME [epoch: 8.87 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10826462401549522		[learning rate: 0.0072893]
		[batch 20/20] avg loss: 0.07950635921155795		[learning rate: 0.0072804]
	Learning Rate: 0.00728044
	LOSS [training: 0.0938854916135266 | validation: 0.04111961621568966]
	TIME [epoch: 8.88 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05469686687744689		[learning rate: 0.0072716]
		[batch 20/20] avg loss: 0.08306954501472699		[learning rate: 0.0072628]
	Learning Rate: 0.00726282
	LOSS [training: 0.06888320594608693 | validation: 0.07096557638275457]
	TIME [epoch: 8.88 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07598177973072777		[learning rate: 0.007254]
		[batch 20/20] avg loss: 0.10044254183114343		[learning rate: 0.0072452]
	Learning Rate: 0.00724524
	LOSS [training: 0.0882121607809356 | validation: 0.07861040522882314]
	TIME [epoch: 8.87 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12074047290516148		[learning rate: 0.0072365]
		[batch 20/20] avg loss: 0.08990058185912538		[learning rate: 0.0072277]
	Learning Rate: 0.0072277
	LOSS [training: 0.10532052738214341 | validation: 0.03934175813754034]
	TIME [epoch: 8.87 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08611867984068505		[learning rate: 0.0072189]
		[batch 20/20] avg loss: 0.11177422338438772		[learning rate: 0.0072102]
	Learning Rate: 0.0072102
	LOSS [training: 0.09894645161253639 | validation: 0.07430810617448252]
	TIME [epoch: 8.86 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08660889929238424		[learning rate: 0.0072015]
		[batch 20/20] avg loss: 0.09634673287820533		[learning rate: 0.0071927]
	Learning Rate: 0.00719275
	LOSS [training: 0.09147781608529476 | validation: 0.08910134579933253]
	TIME [epoch: 8.89 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15188962576325624		[learning rate: 0.007184]
		[batch 20/20] avg loss: 0.1498537539733455		[learning rate: 0.0071753]
	Learning Rate: 0.00717533
	LOSS [training: 0.1508716898683009 | validation: 0.1132274148852564]
	TIME [epoch: 8.88 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10242894155659443		[learning rate: 0.0071666]
		[batch 20/20] avg loss: 0.10523263109600996		[learning rate: 0.007158]
	Learning Rate: 0.00715796
	LOSS [training: 0.1038307863263022 | validation: 0.08331228208877976]
	TIME [epoch: 8.87 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07223014315073101		[learning rate: 0.0071493]
		[batch 20/20] avg loss: 0.10958028212785102		[learning rate: 0.0071406]
	Learning Rate: 0.00714064
	LOSS [training: 0.09090521263929101 | validation: 0.02717753568371975]
	TIME [epoch: 8.87 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08487041116413592		[learning rate: 0.007132]
		[batch 20/20] avg loss: 0.06445446109212538		[learning rate: 0.0071233]
	Learning Rate: 0.00712335
	LOSS [training: 0.07466243612813066 | validation: 0.1828204302295377]
	TIME [epoch: 8.83 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07702569714926623		[learning rate: 0.0071147]
		[batch 20/20] avg loss: 0.08632689580927531		[learning rate: 0.0071061]
	Learning Rate: 0.0071061
	LOSS [training: 0.08167629647927077 | validation: 0.05085438069948406]
	TIME [epoch: 8.9 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049860946909790774		[learning rate: 0.0070975]
		[batch 20/20] avg loss: 0.06656925440280996		[learning rate: 0.0070889]
	Learning Rate: 0.0070889
	LOSS [training: 0.05821510065630039 | validation: 0.06176867209004748]
	TIME [epoch: 8.86 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09394224673478492		[learning rate: 0.0070803]
		[batch 20/20] avg loss: 0.18092110960973973		[learning rate: 0.0070717]
	Learning Rate: 0.00707174
	LOSS [training: 0.13743167817226234 | validation: 0.07384843108507827]
	TIME [epoch: 8.87 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08394791588903489		[learning rate: 0.0070632]
		[batch 20/20] avg loss: 0.056000762828781324		[learning rate: 0.0070546]
	Learning Rate: 0.00705462
	LOSS [training: 0.0699743393589081 | validation: 0.039926742804407986]
	TIME [epoch: 8.87 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05606217276479841		[learning rate: 0.0070461]
		[batch 20/20] avg loss: 0.09052040205188255		[learning rate: 0.0070375]
	Learning Rate: 0.00703754
	LOSS [training: 0.07329128740834048 | validation: 0.12799314673472356]
	TIME [epoch: 8.89 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09544130765230698		[learning rate: 0.007029]
		[batch 20/20] avg loss: 0.07312714066483021		[learning rate: 0.0070205]
	Learning Rate: 0.00702051
	LOSS [training: 0.0842842241585686 | validation: 0.030575643789510193]
	TIME [epoch: 8.88 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06223884272714699		[learning rate: 0.007012]
		[batch 20/20] avg loss: 0.08758671491395406		[learning rate: 0.0070035]
	Learning Rate: 0.00700351
	LOSS [training: 0.07491277882055052 | validation: 0.06883144697569585]
	TIME [epoch: 8.87 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07666945195783471		[learning rate: 0.006995]
		[batch 20/20] avg loss: 0.09626543557552851		[learning rate: 0.0069866]
	Learning Rate: 0.00698656
	LOSS [training: 0.08646744376668161 | validation: 0.05095848788910257]
	TIME [epoch: 8.87 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08228644181568825		[learning rate: 0.0069781]
		[batch 20/20] avg loss: 0.09343879060693767		[learning rate: 0.0069696]
	Learning Rate: 0.00696964
	LOSS [training: 0.08786261621131297 | validation: 0.04641906340371231]
	TIME [epoch: 8.87 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1085796214153953		[learning rate: 0.0069612]
		[batch 20/20] avg loss: 0.11136873892908558		[learning rate: 0.0069528]
	Learning Rate: 0.00695277
	LOSS [training: 0.10997418017224045 | validation: 0.03229505370195728]
	TIME [epoch: 8.9 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041122515143942405		[learning rate: 0.0069443]
		[batch 20/20] avg loss: 0.06944164012387188		[learning rate: 0.0069359]
	Learning Rate: 0.00693594
	LOSS [training: 0.05528207763390715 | validation: 0.08518404349777134]
	TIME [epoch: 8.88 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06417098752001851		[learning rate: 0.0069275]
		[batch 20/20] avg loss: 0.06071167601006515		[learning rate: 0.0069191]
	Learning Rate: 0.00691915
	LOSS [training: 0.06244133176504184 | validation: 0.052285567712681745]
	TIME [epoch: 8.87 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08366560743125483		[learning rate: 0.0069108]
		[batch 20/20] avg loss: 0.059039676500575775		[learning rate: 0.0069024]
	Learning Rate: 0.0069024
	LOSS [training: 0.07135264196591529 | validation: 0.12548926501374924]
	TIME [epoch: 8.87 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0825373846091957		[learning rate: 0.006894]
		[batch 20/20] avg loss: 0.06937841905957173		[learning rate: 0.0068857]
	Learning Rate: 0.00688569
	LOSS [training: 0.07595790183438372 | validation: 0.06497459998042784]
	TIME [epoch: 8.88 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09099277336951732		[learning rate: 0.0068773]
		[batch 20/20] avg loss: 0.09681741165253327		[learning rate: 0.006869]
	Learning Rate: 0.00686902
	LOSS [training: 0.09390509251102531 | validation: 0.07859207132473187]
	TIME [epoch: 8.9 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06689577691411172		[learning rate: 0.0068607]
		[batch 20/20] avg loss: 0.07683001409035056		[learning rate: 0.0068524]
	Learning Rate: 0.00685239
	LOSS [training: 0.07186289550223114 | validation: 0.04922748938873161]
	TIME [epoch: 8.88 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05237954027619962		[learning rate: 0.0068441]
		[batch 20/20] avg loss: 0.10292188746785069		[learning rate: 0.0068358]
	Learning Rate: 0.0068358
	LOSS [training: 0.07765071387202516 | validation: 0.30057306428826697]
	TIME [epoch: 8.87 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1309130857375858		[learning rate: 0.0068275]
		[batch 20/20] avg loss: 0.05304105733509288		[learning rate: 0.0068193]
	Learning Rate: 0.00681925
	LOSS [training: 0.09197707153633936 | validation: 0.1037266060448568]
	TIME [epoch: 8.86 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.078625515944341		[learning rate: 0.006811]
		[batch 20/20] avg loss: 0.08352004090746701		[learning rate: 0.0068027]
	Learning Rate: 0.00680275
	LOSS [training: 0.081072778425904 | validation: 0.09014992206940171]
	TIME [epoch: 8.87 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06818758451036541		[learning rate: 0.0067945]
		[batch 20/20] avg loss: 0.04952106549902559		[learning rate: 0.0067863]
	Learning Rate: 0.00678628
	LOSS [training: 0.05885432500469551 | validation: 0.1097573923696164]
	TIME [epoch: 8.91 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12499825649677856		[learning rate: 0.0067781]
		[batch 20/20] avg loss: 0.049306825013931535		[learning rate: 0.0067698]
	Learning Rate: 0.00676985
	LOSS [training: 0.08715254075535506 | validation: 0.03480578549044779]
	TIME [epoch: 8.87 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07060149527353422		[learning rate: 0.0067616]
		[batch 20/20] avg loss: 0.05679956360344364		[learning rate: 0.0067535]
	Learning Rate: 0.00675346
	LOSS [training: 0.06370052943848892 | validation: 0.038139264827963315]
	TIME [epoch: 8.87 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06714524655515126		[learning rate: 0.0067453]
		[batch 20/20] avg loss: 0.07471079706946458		[learning rate: 0.0067371]
	Learning Rate: 0.00673711
	LOSS [training: 0.07092802181230792 | validation: 0.07102415909926167]
	TIME [epoch: 8.87 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09131488216654988		[learning rate: 0.006729]
		[batch 20/20] avg loss: 0.09412345777878704		[learning rate: 0.0067208]
	Learning Rate: 0.0067208
	LOSS [training: 0.09271916997266846 | validation: 0.05474855848867863]
	TIME [epoch: 8.88 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1108172552569721		[learning rate: 0.0067127]
		[batch 20/20] avg loss: 0.06553392595073322		[learning rate: 0.0067045]
	Learning Rate: 0.00670453
	LOSS [training: 0.08817559060385266 | validation: 0.018066725898774456]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06620507802174212		[learning rate: 0.0066964]
		[batch 20/20] avg loss: 0.07561582930521396		[learning rate: 0.0066883]
	Learning Rate: 0.0066883
	LOSS [training: 0.07091045366347805 | validation: 0.06638813847337549]
	TIME [epoch: 8.86 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06286585654370214		[learning rate: 0.0066802]
		[batch 20/20] avg loss: 0.11357635046522314		[learning rate: 0.0066721]
	Learning Rate: 0.00667211
	LOSS [training: 0.08822110350446263 | validation: 0.0734505484422643]
	TIME [epoch: 8.84 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0702049149535862		[learning rate: 0.006664]
		[batch 20/20] avg loss: 0.11147438899558534		[learning rate: 0.006656]
	Learning Rate: 0.00665596
	LOSS [training: 0.09083965197458577 | validation: 0.07661794879997792]
	TIME [epoch: 8.85 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059893946846168236		[learning rate: 0.0066479]
		[batch 20/20] avg loss: 0.12517458510648374		[learning rate: 0.0066398]
	Learning Rate: 0.00663984
	LOSS [training: 0.09253426597632598 | validation: 0.11168394582475853]
	TIME [epoch: 8.85 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16082214164880074		[learning rate: 0.0066318]
		[batch 20/20] avg loss: 0.09866485630137015		[learning rate: 0.0066238]
	Learning Rate: 0.00662377
	LOSS [training: 0.12974349897508544 | validation: 0.022978621418575394]
	TIME [epoch: 8.87 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07286966798767457		[learning rate: 0.0066157]
		[batch 20/20] avg loss: 0.05454753086627345		[learning rate: 0.0066077]
	Learning Rate: 0.00660774
	LOSS [training: 0.063708599426974 | validation: 0.039718660476100016]
	TIME [epoch: 8.85 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04793151375321189		[learning rate: 0.0065997]
		[batch 20/20] avg loss: 0.08057120068120024		[learning rate: 0.0065917]
	Learning Rate: 0.00659174
	LOSS [training: 0.06425135721720605 | validation: 0.007651922267308193]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07839646172391591		[learning rate: 0.0065838]
		[batch 20/20] avg loss: 0.06489716604740361		[learning rate: 0.0065758]
	Learning Rate: 0.00657578
	LOSS [training: 0.07164681388565977 | validation: 0.06418828681901544]
	TIME [epoch: 8.86 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06965521319435453		[learning rate: 0.0065678]
		[batch 20/20] avg loss: 0.0758072682413209		[learning rate: 0.0065599]
	Learning Rate: 0.00655986
	LOSS [training: 0.07273124071783771 | validation: 0.04794413038260504]
	TIME [epoch: 8.89 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06341332592432586		[learning rate: 0.0065519]
		[batch 20/20] avg loss: 0.06327096871038382		[learning rate: 0.006544]
	Learning Rate: 0.00654398
	LOSS [training: 0.06334214731735484 | validation: 0.0778821451518469]
	TIME [epoch: 8.86 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061361672081806616		[learning rate: 0.0065361]
		[batch 20/20] avg loss: 0.04666239770717321		[learning rate: 0.0065281]
	Learning Rate: 0.00652814
	LOSS [training: 0.05401203489448991 | validation: 0.05705049272890598]
	TIME [epoch: 8.84 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06494880539886967		[learning rate: 0.0065202]
		[batch 20/20] avg loss: 0.08003732475414087		[learning rate: 0.0065123]
	Learning Rate: 0.00651234
	LOSS [training: 0.07249306507650526 | validation: 0.028983548921239822]
	TIME [epoch: 8.87 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04574926303586639		[learning rate: 0.0065044]
		[batch 20/20] avg loss: 0.09868468196708745		[learning rate: 0.0064966]
	Learning Rate: 0.00649657
	LOSS [training: 0.07221697250147692 | validation: 0.10127758194594652]
	TIME [epoch: 8.89 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09413273067927554		[learning rate: 0.0064887]
		[batch 20/20] avg loss: 0.06911284053027802		[learning rate: 0.0064808]
	Learning Rate: 0.00648084
	LOSS [training: 0.08162278560477679 | validation: 0.022421968618861678]
	TIME [epoch: 8.86 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060683388890605816		[learning rate: 0.006473]
		[batch 20/20] avg loss: 0.07608097441784273		[learning rate: 0.0064652]
	Learning Rate: 0.00646516
	LOSS [training: 0.06838218165422427 | validation: 0.07496866890827573]
	TIME [epoch: 8.83 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05170717714688241		[learning rate: 0.0064573]
		[batch 20/20] avg loss: 0.07255301083591555		[learning rate: 0.0064495]
	Learning Rate: 0.0064495
	LOSS [training: 0.06213009399139898 | validation: 0.08173037627253482]
	TIME [epoch: 8.85 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05629841997107771		[learning rate: 0.0064417]
		[batch 20/20] avg loss: 0.07507751089931272		[learning rate: 0.0064339]
	Learning Rate: 0.00643389
	LOSS [training: 0.0656879654351952 | validation: 0.04937301073501911]
	TIME [epoch: 8.86 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062157883180977105		[learning rate: 0.0064261]
		[batch 20/20] avg loss: 0.05072228143348574		[learning rate: 0.0064183]
	Learning Rate: 0.00641832
	LOSS [training: 0.05644008230723141 | validation: 0.0566104086322787]
	TIME [epoch: 8.89 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06602659450331985		[learning rate: 0.0064105]
		[batch 20/20] avg loss: 0.08530317076435129		[learning rate: 0.0064028]
	Learning Rate: 0.00640278
	LOSS [training: 0.07566488263383556 | validation: 0.04675586056694932]
	TIME [epoch: 8.87 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0726343655091316		[learning rate: 0.006395]
		[batch 20/20] avg loss: 0.08893812111139264		[learning rate: 0.0063873]
	Learning Rate: 0.00638728
	LOSS [training: 0.08078624331026213 | validation: 0.02701547849215758]
	TIME [epoch: 8.86 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042206045155032686		[learning rate: 0.0063795]
		[batch 20/20] avg loss: 0.06015992252429346		[learning rate: 0.0063718]
	Learning Rate: 0.00637182
	LOSS [training: 0.05118298383966309 | validation: 0.03243587550100472]
	TIME [epoch: 8.86 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1415440214410212		[learning rate: 0.0063641]
		[batch 20/20] avg loss: 0.06760644361638853		[learning rate: 0.0063564]
	Learning Rate: 0.00635639
	LOSS [training: 0.10457523252870486 | validation: 0.09795639262052373]
	TIME [epoch: 8.85 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06604193455651862		[learning rate: 0.0063487]
		[batch 20/20] avg loss: 0.07910290955938144		[learning rate: 0.006341]
	Learning Rate: 0.006341
	LOSS [training: 0.07257242205795003 | validation: 0.04464260851276785]
	TIME [epoch: 8.88 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08746537285127819		[learning rate: 0.0063333]
		[batch 20/20] avg loss: 0.07161302293346501		[learning rate: 0.0063257]
	Learning Rate: 0.00632565
	LOSS [training: 0.07953919789237157 | validation: 0.04336823836350803]
	TIME [epoch: 8.85 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07165491436128088		[learning rate: 0.006318]
		[batch 20/20] avg loss: 0.07013404336486978		[learning rate: 0.0063103]
	Learning Rate: 0.00631034
	LOSS [training: 0.07089447886307533 | validation: 0.07032941782169581]
	TIME [epoch: 8.85 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07110185655562833		[learning rate: 0.0063027]
		[batch 20/20] avg loss: 0.0919601044625754		[learning rate: 0.0062951]
	Learning Rate: 0.00629506
	LOSS [training: 0.08153098050910187 | validation: 0.1411825569765093]
	TIME [epoch: 8.86 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07830564194378804		[learning rate: 0.0062874]
		[batch 20/20] avg loss: 0.06322131413584467		[learning rate: 0.0062798]
	Learning Rate: 0.00627982
	LOSS [training: 0.07076347803981636 | validation: 0.06622647309745482]
	TIME [epoch: 8.87 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1115297518065745		[learning rate: 0.0062722]
		[batch 20/20] avg loss: 0.04020271637904252		[learning rate: 0.0062646]
	Learning Rate: 0.00626462
	LOSS [training: 0.0758662340928085 | validation: 0.05743727261226082]
	TIME [epoch: 8.88 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.065475785352083		[learning rate: 0.006257]
		[batch 20/20] avg loss: 0.14264668317804147		[learning rate: 0.0062495]
	Learning Rate: 0.00624945
	LOSS [training: 0.10406123426506224 | validation: 0.05199404636242769]
	TIME [epoch: 8.87 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052259257586405684		[learning rate: 0.0062419]
		[batch 20/20] avg loss: 0.0398621093595305		[learning rate: 0.0062343]
	Learning Rate: 0.00623433
	LOSS [training: 0.04606068347296809 | validation: 0.035945841405176183]
	TIME [epoch: 8.87 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05747345311347709		[learning rate: 0.0062268]
		[batch 20/20] avg loss: 0.07776888224864026		[learning rate: 0.0062192]
	Learning Rate: 0.00621923
	LOSS [training: 0.06762116768105866 | validation: 0.04199032820638026]
	TIME [epoch: 8.86 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039181288541219834		[learning rate: 0.0062117]
		[batch 20/20] avg loss: 0.052425656056908396		[learning rate: 0.0062042]
	Learning Rate: 0.00620418
	LOSS [training: 0.04580347229906411 | validation: 0.04194113435903265]
	TIME [epoch: 8.89 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05303315796270332		[learning rate: 0.0061967]
		[batch 20/20] avg loss: 0.05361551368373687		[learning rate: 0.0061892]
	Learning Rate: 0.00618916
	LOSS [training: 0.053324335823220104 | validation: 0.024734564842340122]
	TIME [epoch: 8.88 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0469395193844839		[learning rate: 0.0061817]
		[batch 20/20] avg loss: 0.0525363568932975		[learning rate: 0.0061742]
	Learning Rate: 0.00617417
	LOSS [training: 0.04973793813889069 | validation: 0.13948530140177998]
	TIME [epoch: 8.87 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0862928273459688		[learning rate: 0.0061667]
		[batch 20/20] avg loss: 0.05278036809840516		[learning rate: 0.0061592]
	Learning Rate: 0.00615923
	LOSS [training: 0.06953659772218698 | validation: 0.03107203626809631]
	TIME [epoch: 8.87 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06130211817876243		[learning rate: 0.0061518]
		[batch 20/20] avg loss: 0.07049199396929619		[learning rate: 0.0061443]
	Learning Rate: 0.00614432
	LOSS [training: 0.06589705607402929 | validation: 0.031238353815614385]
	TIME [epoch: 8.87 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07499968987212771		[learning rate: 0.0061369]
		[batch 20/20] avg loss: 0.05609640957700333		[learning rate: 0.0061294]
	Learning Rate: 0.00612944
	LOSS [training: 0.06554804972456552 | validation: 0.04340802057128631]
	TIME [epoch: 8.9 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058552379509732314		[learning rate: 0.006122]
		[batch 20/20] avg loss: 0.06894495348136354		[learning rate: 0.0061146]
	Learning Rate: 0.0061146
	LOSS [training: 0.06374866649554792 | validation: 0.03673360149464978]
	TIME [epoch: 8.88 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041530140699288856		[learning rate: 0.0061072]
		[batch 20/20] avg loss: 0.0768355362310306		[learning rate: 0.0060998]
	Learning Rate: 0.0060998
	LOSS [training: 0.05918283846515972 | validation: 0.056522892146655956]
	TIME [epoch: 8.88 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0725728072530107		[learning rate: 0.0060924]
		[batch 20/20] avg loss: 0.0755282999514871		[learning rate: 0.006085]
	Learning Rate: 0.00608504
	LOSS [training: 0.07405055360224888 | validation: 0.06516892803295346]
	TIME [epoch: 8.88 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0848098631389758		[learning rate: 0.0060777]
		[batch 20/20] avg loss: 0.0684695417440698		[learning rate: 0.0060703]
	Learning Rate: 0.00607031
	LOSS [training: 0.0766397024415228 | validation: 0.03144561446984548]
	TIME [epoch: 8.89 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03953463048248345		[learning rate: 0.006063]
		[batch 20/20] avg loss: 0.07205975704864259		[learning rate: 0.0060556]
	Learning Rate: 0.00605561
	LOSS [training: 0.055797193765563025 | validation: 0.03672831607916311]
	TIME [epoch: 8.89 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058323711523665486		[learning rate: 0.0060483]
		[batch 20/20] avg loss: 0.07891957783180154		[learning rate: 0.006041]
	Learning Rate: 0.00604095
	LOSS [training: 0.06862164467773352 | validation: 0.10144389189959749]
	TIME [epoch: 8.88 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12273672392439836		[learning rate: 0.0060336]
		[batch 20/20] avg loss: 0.08168238449096679		[learning rate: 0.0060263]
	Learning Rate: 0.00602633
	LOSS [training: 0.10220955420768256 | validation: 0.12248604763417326]
	TIME [epoch: 8.87 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16033767579689745		[learning rate: 0.006019]
		[batch 20/20] avg loss: 0.20249014269143384		[learning rate: 0.0060117]
	Learning Rate: 0.00601174
	LOSS [training: 0.18141390924416562 | validation: 0.10113678390466836]
	TIME [epoch: 8.87 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06535563790513554		[learning rate: 0.0060045]
		[batch 20/20] avg loss: 0.0861896167882779		[learning rate: 0.0059972]
	Learning Rate: 0.00599718
	LOSS [training: 0.07577262734670673 | validation: 0.13471146144761365]
	TIME [epoch: 8.89 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10023943753885393		[learning rate: 0.0059899]
		[batch 20/20] avg loss: 0.07482150736159317		[learning rate: 0.0059827]
	Learning Rate: 0.00598267
	LOSS [training: 0.08753047245022356 | validation: 0.016916388427572933]
	TIME [epoch: 8.89 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05595023870310774		[learning rate: 0.0059754]
		[batch 20/20] avg loss: 0.05985403924806777		[learning rate: 0.0059682]
	Learning Rate: 0.00596818
	LOSS [training: 0.057902138975587755 | validation: 0.06872195388747188]
	TIME [epoch: 8.87 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04880618638937365		[learning rate: 0.005961]
		[batch 20/20] avg loss: 0.04203726661023961		[learning rate: 0.0059537]
	Learning Rate: 0.00595374
	LOSS [training: 0.04542172649980663 | validation: 0.03179279785443031]
	TIME [epoch: 8.87 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04947040576856839		[learning rate: 0.0059465]
		[batch 20/20] avg loss: 0.04473384635471919		[learning rate: 0.0059393]
	Learning Rate: 0.00593932
	LOSS [training: 0.047102126061643795 | validation: 0.04239565073379711]
	TIME [epoch: 8.87 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09473380101520056		[learning rate: 0.0059321]
		[batch 20/20] avg loss: 0.1055989669838959		[learning rate: 0.0059249]
	Learning Rate: 0.00592494
	LOSS [training: 0.10016638399954822 | validation: 0.06130628669401918]
	TIME [epoch: 8.9 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12912256251023244		[learning rate: 0.0059178]
		[batch 20/20] avg loss: 0.1359431487865375		[learning rate: 0.0059106]
	Learning Rate: 0.0059106
	LOSS [training: 0.13253285564838496 | validation: 0.09661630866442797]
	TIME [epoch: 8.88 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06391209084776021		[learning rate: 0.0059034]
		[batch 20/20] avg loss: 0.05465752513933182		[learning rate: 0.0058963]
	Learning Rate: 0.00589629
	LOSS [training: 0.05928480799354602 | validation: 0.0404089245108138]
	TIME [epoch: 8.87 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0443191536097137		[learning rate: 0.0058892]
		[batch 20/20] avg loss: 0.08033185525585661		[learning rate: 0.005882]
	Learning Rate: 0.00588202
	LOSS [training: 0.06232550443278516 | validation: 0.04787796999207862]
	TIME [epoch: 8.87 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09991944914897016		[learning rate: 0.0058749]
		[batch 20/20] avg loss: 0.0698431823243885		[learning rate: 0.0058678]
	Learning Rate: 0.00586778
	LOSS [training: 0.08488131573667933 | validation: 0.04987113704961034]
	TIME [epoch: 8.88 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062262833228663786		[learning rate: 0.0058607]
		[batch 20/20] avg loss: 0.06258820350052469		[learning rate: 0.0058536]
	Learning Rate: 0.00585357
	LOSS [training: 0.06242551836459424 | validation: 0.062405017400073336]
	TIME [epoch: 8.9 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0422694363150813		[learning rate: 0.0058465]
		[batch 20/20] avg loss: 0.07669611753887963		[learning rate: 0.0058394]
	Learning Rate: 0.0058394
	LOSS [training: 0.059482776926980466 | validation: 0.04205157953370905]
	TIME [epoch: 8.86 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05060954563968698		[learning rate: 0.0058323]
		[batch 20/20] avg loss: 0.044010640233285575		[learning rate: 0.0058253]
	Learning Rate: 0.00582527
	LOSS [training: 0.047310092936486275 | validation: 0.018747752045682336]
	TIME [epoch: 8.87 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05573432899078469		[learning rate: 0.0058182]
		[batch 20/20] avg loss: 0.07622529699135391		[learning rate: 0.0058112]
	Learning Rate: 0.00581116
	LOSS [training: 0.0659798129910693 | validation: 0.07597298423916529]
	TIME [epoch: 8.88 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10292729258222921		[learning rate: 0.0058041]
		[batch 20/20] avg loss: 0.056204829018880555		[learning rate: 0.0057971]
	Learning Rate: 0.0057971
	LOSS [training: 0.0795660608005549 | validation: 0.031788848591081026]
	TIME [epoch: 8.89 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04215512787418907		[learning rate: 0.0057901]
		[batch 20/20] avg loss: 0.03157438107538403		[learning rate: 0.0057831]
	Learning Rate: 0.00578306
	LOSS [training: 0.03686475447478655 | validation: 0.07393110846586391]
	TIME [epoch: 8.88 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06377866582831285		[learning rate: 0.0057761]
		[batch 20/20] avg loss: 0.066238416201258		[learning rate: 0.0057691]
	Learning Rate: 0.00576906
	LOSS [training: 0.06500854101478541 | validation: 0.03458732656670282]
	TIME [epoch: 8.87 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04774509956746991		[learning rate: 0.0057621]
		[batch 20/20] avg loss: 0.032381770472802655		[learning rate: 0.0057551]
	Learning Rate: 0.0057551
	LOSS [training: 0.04006343502013628 | validation: 0.03841365971328171]
	TIME [epoch: 8.87 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048854585518512884		[learning rate: 0.0057481]
		[batch 20/20] avg loss: 0.06173204412800828		[learning rate: 0.0057412]
	Learning Rate: 0.00574116
	LOSS [training: 0.05529331482326059 | validation: 0.012180947182712499]
	TIME [epoch: 8.87 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03362859378644523		[learning rate: 0.0057342]
		[batch 20/20] avg loss: 0.04253467401524626		[learning rate: 0.0057273]
	Learning Rate: 0.00572727
	LOSS [training: 0.03808163390084573 | validation: 0.06757331922292632]
	TIME [epoch: 8.9 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038153613695615825		[learning rate: 0.0057203]
		[batch 20/20] avg loss: 0.05721317633209001		[learning rate: 0.0057134]
	Learning Rate: 0.0057134
	LOSS [training: 0.04768339501385292 | validation: 0.05240983573030276]
	TIME [epoch: 8.87 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03524001561319676		[learning rate: 0.0057065]
		[batch 20/20] avg loss: 0.07851998257514171		[learning rate: 0.0056996]
	Learning Rate: 0.00569957
	LOSS [training: 0.05687999909416924 | validation: 0.04652242347242871]
	TIME [epoch: 8.86 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05934692994250407		[learning rate: 0.0056927]
		[batch 20/20] avg loss: 0.07377515690617402		[learning rate: 0.0056858]
	Learning Rate: 0.00568577
	LOSS [training: 0.06656104342433904 | validation: 0.12270809011278774]
	TIME [epoch: 8.87 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0955354363965394		[learning rate: 0.0056789]
		[batch 20/20] avg loss: 0.1390402082917515		[learning rate: 0.005672]
	Learning Rate: 0.00567201
	LOSS [training: 0.11728782234414545 | validation: 0.06630949766315242]
	TIME [epoch: 8.87 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05545075801437992		[learning rate: 0.0056651]
		[batch 20/20] avg loss: 0.08822078817063092		[learning rate: 0.0056583]
	Learning Rate: 0.00565828
	LOSS [training: 0.07183577309250541 | validation: 0.042453524897217695]
	TIME [epoch: 8.9 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07327543163324442		[learning rate: 0.0056514]
		[batch 20/20] avg loss: 0.060637246015756194		[learning rate: 0.0056446]
	Learning Rate: 0.00564458
	LOSS [training: 0.0669563388245003 | validation: 0.06341470901507759]
	TIME [epoch: 8.88 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057964938683019405		[learning rate: 0.0056377]
		[batch 20/20] avg loss: 0.06809447112787517		[learning rate: 0.0056309]
	Learning Rate: 0.00563092
	LOSS [training: 0.06302970490544729 | validation: 0.09585853812757146]
	TIME [epoch: 8.86 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05711019705006738		[learning rate: 0.0056241]
		[batch 20/20] avg loss: 0.04066700415160522		[learning rate: 0.0056173]
	Learning Rate: 0.00561728
	LOSS [training: 0.04888860060083631 | validation: 0.019634230874063453]
	TIME [epoch: 8.87 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03445558229344957		[learning rate: 0.0056105]
		[batch 20/20] avg loss: 0.03672593913894938		[learning rate: 0.0056037]
	Learning Rate: 0.00560368
	LOSS [training: 0.035590760716199474 | validation: 0.0352335922572253]
	TIME [epoch: 8.87 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05545162562468066		[learning rate: 0.0055969]
		[batch 20/20] avg loss: 0.0667507207393657		[learning rate: 0.0055901]
	Learning Rate: 0.00559012
	LOSS [training: 0.06110117318202317 | validation: 0.19102767082404334]
	TIME [epoch: 8.89 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08010797546656352		[learning rate: 0.0055833]
		[batch 20/20] avg loss: 0.05678687366303057		[learning rate: 0.0055766]
	Learning Rate: 0.00557659
	LOSS [training: 0.06844742456479704 | validation: 0.08670533424754175]
	TIME [epoch: 8.87 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050424192762261136		[learning rate: 0.0055698]
		[batch 20/20] avg loss: 0.08707405753962245		[learning rate: 0.0055631]
	Learning Rate: 0.00556309
	LOSS [training: 0.06874912515094181 | validation: 0.06065785732084632]
	TIME [epoch: 8.88 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06488713323507145		[learning rate: 0.0055563]
		[batch 20/20] avg loss: 0.05242562452343684		[learning rate: 0.0055496]
	Learning Rate: 0.00554962
	LOSS [training: 0.05865637887925414 | validation: 0.07244633424196027]
	TIME [epoch: 8.86 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05734076644239507		[learning rate: 0.0055429]
		[batch 20/20] avg loss: 0.04799972648744626		[learning rate: 0.0055362]
	Learning Rate: 0.00553618
	LOSS [training: 0.05267024646492067 | validation: 0.08281603093318053]
	TIME [epoch: 8.89 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09671701331528033		[learning rate: 0.0055295]
		[batch 20/20] avg loss: 0.11106568501913464		[learning rate: 0.0055228]
	Learning Rate: 0.00552278
	LOSS [training: 0.1038913491672075 | validation: 0.07599494717967031]
	TIME [epoch: 8.87 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06383142241936052		[learning rate: 0.0055161]
		[batch 20/20] avg loss: 0.05723303433893815		[learning rate: 0.0055094]
	Learning Rate: 0.00550941
	LOSS [training: 0.060532228379149344 | validation: 0.03225695290218148]
	TIME [epoch: 8.87 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06579925957346397		[learning rate: 0.0055027]
		[batch 20/20] avg loss: 0.15590381032998055		[learning rate: 0.0054961]
	Learning Rate: 0.00549607
	LOSS [training: 0.11085153495172224 | validation: 0.0992750944219085]
	TIME [epoch: 8.87 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10318784268236811		[learning rate: 0.0054894]
		[batch 20/20] avg loss: 0.06647881119999217		[learning rate: 0.0054828]
	Learning Rate: 0.00548277
	LOSS [training: 0.08483332694118015 | validation: 0.056634801502083054]
	TIME [epoch: 8.87 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06124523446307841		[learning rate: 0.0054761]
		[batch 20/20] avg loss: 0.07136769107729918		[learning rate: 0.0054695]
	Learning Rate: 0.0054695
	LOSS [training: 0.06630646277018878 | validation: 0.07000174566336934]
	TIME [epoch: 8.9 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05737397706106175		[learning rate: 0.0054629]
		[batch 20/20] avg loss: 0.046164867372138804		[learning rate: 0.0054563]
	Learning Rate: 0.00545626
	LOSS [training: 0.05176942221660028 | validation: 0.0545242483329853]
	TIME [epoch: 8.88 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07381423504741236		[learning rate: 0.0054496]
		[batch 20/20] avg loss: 0.06946644162250018		[learning rate: 0.005443]
	Learning Rate: 0.00544305
	LOSS [training: 0.07164033833495627 | validation: 0.03691086741097335]
	TIME [epoch: 8.87 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0604567992635995		[learning rate: 0.0054365]
		[batch 20/20] avg loss: 0.06217992769603611		[learning rate: 0.0054299]
	Learning Rate: 0.00542987
	LOSS [training: 0.061318363479817804 | validation: 0.04478910601165905]
	TIME [epoch: 8.86 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049636458990444846		[learning rate: 0.0054233]
		[batch 20/20] avg loss: 0.05231618917521343		[learning rate: 0.0054167]
	Learning Rate: 0.00541673
	LOSS [training: 0.050976324082829136 | validation: 0.07238723704540402]
	TIME [epoch: 8.87 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1726636704528307		[learning rate: 0.0054102]
		[batch 20/20] avg loss: 0.10580090213247173		[learning rate: 0.0054036]
	Learning Rate: 0.00540361
	LOSS [training: 0.13923228629265122 | validation: 0.10626021350218812]
	TIME [epoch: 8.98 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10928324268633731		[learning rate: 0.0053971]
		[batch 20/20] avg loss: 0.15003034837307622		[learning rate: 0.0053905]
	Learning Rate: 0.00539053
	LOSS [training: 0.12965679552970677 | validation: 0.1402090955761233]
	TIME [epoch: 8.87 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14334839053711404		[learning rate: 0.005384]
		[batch 20/20] avg loss: 0.14018771994736004		[learning rate: 0.0053775]
	Learning Rate: 0.00537748
	LOSS [training: 0.14176805524223704 | validation: 0.24595963126780246]
	TIME [epoch: 8.87 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10781482547203484		[learning rate: 0.005371]
		[batch 20/20] avg loss: 0.05858603788492199		[learning rate: 0.0053645]
	Learning Rate: 0.00536446
	LOSS [training: 0.08320043167847843 | validation: 0.06394573274575983]
	TIME [epoch: 8.87 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0504226719636194		[learning rate: 0.005358]
		[batch 20/20] avg loss: 0.10788057386296117		[learning rate: 0.0053515]
	Learning Rate: 0.00535148
	LOSS [training: 0.07915162291329028 | validation: 0.3368110862968263]
	TIME [epoch: 8.88 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20319445023917554		[learning rate: 0.005345]
		[batch 20/20] avg loss: 0.07306521023397076		[learning rate: 0.0053385]
	Learning Rate: 0.00533852
	LOSS [training: 0.13812983023657316 | validation: 0.15248841935960658]
	TIME [epoch: 8.88 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10205714733178654		[learning rate: 0.0053321]
		[batch 20/20] avg loss: 0.0792240309632993		[learning rate: 0.0053256]
	Learning Rate: 0.0053256
	LOSS [training: 0.09064058914754292 | validation: 0.050641950596104116]
	TIME [epoch: 8.87 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05189764130484102		[learning rate: 0.0053191]
		[batch 20/20] avg loss: 0.11373187404915046		[learning rate: 0.0053127]
	Learning Rate: 0.00531271
	LOSS [training: 0.08281475767699574 | validation: 0.16650447791278178]
	TIME [epoch: 8.86 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10914743858445712		[learning rate: 0.0053063]
		[batch 20/20] avg loss: 0.16580130453187436		[learning rate: 0.0052998]
	Learning Rate: 0.00529984
	LOSS [training: 0.13747437155816572 | validation: 0.0997169472189457]
	TIME [epoch: 8.87 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0892523862973947		[learning rate: 0.0052934]
		[batch 20/20] avg loss: 0.16611771876816003		[learning rate: 0.005287]
	Learning Rate: 0.00528701
	LOSS [training: 0.12768505253277737 | validation: 0.11319323316494898]
	TIME [epoch: 8.89 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14796584732647794		[learning rate: 0.0052806]
		[batch 20/20] avg loss: 0.2966692172850786		[learning rate: 0.0052742]
	Learning Rate: 0.00527422
	LOSS [training: 0.2223175323057783 | validation: 0.43504205906660964]
	TIME [epoch: 8.88 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25277436418097365		[learning rate: 0.0052678]
		[batch 20/20] avg loss: 0.11491503560753902		[learning rate: 0.0052614]
	Learning Rate: 0.00526145
	LOSS [training: 0.18384469989425636 | validation: 0.056682965715076444]
	TIME [epoch: 8.86 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06647557351621912		[learning rate: 0.0052551]
		[batch 20/20] avg loss: 0.04965198900253939		[learning rate: 0.0052487]
	Learning Rate: 0.00524871
	LOSS [training: 0.058063781259379256 | validation: 0.07023389354051275]
	TIME [epoch: 8.87 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04999956317083185		[learning rate: 0.0052424]
		[batch 20/20] avg loss: 0.05472953634153819		[learning rate: 0.005236]
	Learning Rate: 0.005236
	LOSS [training: 0.05236454975618502 | validation: 0.06449349119802883]
	TIME [epoch: 8.88 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08386270893910334		[learning rate: 0.0052297]
		[batch 20/20] avg loss: 0.0902196566912008		[learning rate: 0.0052233]
	Learning Rate: 0.00522333
	LOSS [training: 0.08704118281515208 | validation: 0.13084627732187631]
	TIME [epoch: 8.89 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13076532886319675		[learning rate: 0.005217]
		[batch 20/20] avg loss: 0.09951892658029857		[learning rate: 0.0052107]
	Learning Rate: 0.00521068
	LOSS [training: 0.11514212772174766 | validation: 0.07009387852387279]
	TIME [epoch: 8.87 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07216505637768555		[learning rate: 0.0052044]
		[batch 20/20] avg loss: 0.06586459288577651		[learning rate: 0.0051981]
	Learning Rate: 0.00519807
	LOSS [training: 0.06901482463173103 | validation: 0.06250629366537933]
	TIME [epoch: 8.87 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06083221011513118		[learning rate: 0.0051918]
		[batch 20/20] avg loss: 0.11507275000356518		[learning rate: 0.0051855]
	Learning Rate: 0.00518549
	LOSS [training: 0.08795248005934819 | validation: 0.19084337941555085]
	TIME [epoch: 8.87 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09021070251517455		[learning rate: 0.0051792]
		[batch 20/20] avg loss: 0.07732202561437453		[learning rate: 0.0051729]
	Learning Rate: 0.00517293
	LOSS [training: 0.08376636406477453 | validation: 0.1419493494961075]
	TIME [epoch: 8.88 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07572430588063916		[learning rate: 0.0051667]
		[batch 20/20] avg loss: 0.045072601046166914		[learning rate: 0.0051604]
	Learning Rate: 0.00516041
	LOSS [training: 0.06039845346340302 | validation: 0.038000513333986824]
	TIME [epoch: 8.88 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04841166707237872		[learning rate: 0.0051542]
		[batch 20/20] avg loss: 0.05998668450156577		[learning rate: 0.0051479]
	Learning Rate: 0.00514792
	LOSS [training: 0.05419917578697223 | validation: 0.05899174802561663]
	TIME [epoch: 8.88 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05195215330542776		[learning rate: 0.0051417]
		[batch 20/20] avg loss: 0.055558744970378285		[learning rate: 0.0051355]
	Learning Rate: 0.00513546
	LOSS [training: 0.05375544913790301 | validation: 0.024226687251152568]
	TIME [epoch: 8.87 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05548688385369025		[learning rate: 0.0051292]
		[batch 20/20] avg loss: 0.06495415549753111		[learning rate: 0.005123]
	Learning Rate: 0.00512302
	LOSS [training: 0.060220519675610686 | validation: 0.03193528727079576]
	TIME [epoch: 8.87 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04731793797164006		[learning rate: 0.0051168]
		[batch 20/20] avg loss: 0.0501940407993251		[learning rate: 0.0051106]
	Learning Rate: 0.00511062
	LOSS [training: 0.04875598938548257 | validation: 0.019649343954058165]
	TIME [epoch: 8.89 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048382943590004425		[learning rate: 0.0051044]
		[batch 20/20] avg loss: 0.048854069855945814		[learning rate: 0.0050982]
	Learning Rate: 0.00509825
	LOSS [training: 0.04861850672297512 | validation: 0.06152562298800326]
	TIME [epoch: 8.89 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04279059688591497		[learning rate: 0.0050921]
		[batch 20/20] avg loss: 0.04314139753647435		[learning rate: 0.0050859]
	Learning Rate: 0.00508591
	LOSS [training: 0.04296599721119466 | validation: 0.03609657402184182]
	TIME [epoch: 8.87 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04810190357365315		[learning rate: 0.0050797]
		[batch 20/20] avg loss: 0.031697955720870405		[learning rate: 0.0050736]
	Learning Rate: 0.00507359
	LOSS [training: 0.03989992964726177 | validation: 0.042091725604850645]
	TIME [epoch: 8.87 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03418111514243396		[learning rate: 0.0050674]
		[batch 20/20] avg loss: 0.04313320995546961		[learning rate: 0.0050613]
	Learning Rate: 0.00506131
	LOSS [training: 0.03865716254895179 | validation: 0.06013573098711429]
	TIME [epoch: 8.87 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054122662596229346		[learning rate: 0.0050552]
		[batch 20/20] avg loss: 0.12523525752175652		[learning rate: 0.0050491]
	Learning Rate: 0.00504906
	LOSS [training: 0.08967896005899292 | validation: 0.08929369783005142]
	TIME [epoch: 8.89 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04472947730311741		[learning rate: 0.0050429]
		[batch 20/20] avg loss: 0.0401106340220405		[learning rate: 0.0050368]
	Learning Rate: 0.00503684
	LOSS [training: 0.04242005566257896 | validation: 0.059726197730245185]
	TIME [epoch: 8.87 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034902110124479996		[learning rate: 0.0050307]
		[batch 20/20] avg loss: 0.05660957687665382		[learning rate: 0.0050246]
	Learning Rate: 0.00502464
	LOSS [training: 0.045755843500566905 | validation: 0.0278314174537712]
	TIME [epoch: 8.88 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039881075513337735		[learning rate: 0.0050186]
		[batch 20/20] avg loss: 0.04227920881642318		[learning rate: 0.0050125]
	Learning Rate: 0.00501248
	LOSS [training: 0.041080142164880463 | validation: 0.035742243242178964]
	TIME [epoch: 8.86 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05218721173302201		[learning rate: 0.0050064]
		[batch 20/20] avg loss: 0.045439898399989215		[learning rate: 0.0050003]
	Learning Rate: 0.00500034
	LOSS [training: 0.048813555066505615 | validation: 0.045467453640243606]
	TIME [epoch: 8.87 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04959218895499171		[learning rate: 0.0049943]
		[batch 20/20] avg loss: 0.04916172694281136		[learning rate: 0.0049882]
	Learning Rate: 0.00498824
	LOSS [training: 0.04937695794890153 | validation: 0.016399868085859533]
	TIME [epoch: 8.9 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034049290422013545		[learning rate: 0.0049822]
		[batch 20/20] avg loss: 0.05786206648124499		[learning rate: 0.0049762]
	Learning Rate: 0.00497616
	LOSS [training: 0.04595567845162927 | validation: 0.03996324694055654]
	TIME [epoch: 8.87 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045156260647616034		[learning rate: 0.0049701]
		[batch 20/20] avg loss: 0.07657668482341864		[learning rate: 0.0049641]
	Learning Rate: 0.00496412
	LOSS [training: 0.06086647273551732 | validation: 0.04189263460722484]
	TIME [epoch: 8.88 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033306865779086314		[learning rate: 0.0049581]
		[batch 20/20] avg loss: 0.04016644634481932		[learning rate: 0.0049521]
	Learning Rate: 0.0049521
	LOSS [training: 0.036736656061952824 | validation: 0.04221864698828143]
	TIME [epoch: 8.87 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03169873816968386		[learning rate: 0.0049461]
		[batch 20/20] avg loss: 0.037928787127924704		[learning rate: 0.0049401]
	Learning Rate: 0.00494011
	LOSS [training: 0.03481376264880428 | validation: 0.04284017353822254]
	TIME [epoch: 8.88 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033836830969567584		[learning rate: 0.0049341]
		[batch 20/20] avg loss: 0.04680597229182959		[learning rate: 0.0049282]
	Learning Rate: 0.00492815
	LOSS [training: 0.04032140163069858 | validation: 0.029664310185057455]
	TIME [epoch: 8.88 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04542413876075876		[learning rate: 0.0049222]
		[batch 20/20] avg loss: 0.04719535321451361		[learning rate: 0.0049162]
	Learning Rate: 0.00491622
	LOSS [training: 0.04630974598763619 | validation: 0.04955859619088071]
	TIME [epoch: 8.87 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1135897172246378		[learning rate: 0.0049103]
		[batch 20/20] avg loss: 0.04518136393544629		[learning rate: 0.0049043]
	Learning Rate: 0.00490432
	LOSS [training: 0.07938554058004203 | validation: 0.06563912367260809]
	TIME [epoch: 8.87 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04643431547845863		[learning rate: 0.0048984]
		[batch 20/20] avg loss: 0.04254760014462715		[learning rate: 0.0048924]
	Learning Rate: 0.00489245
	LOSS [training: 0.044490957811542886 | validation: 0.09549403400919393]
	TIME [epoch: 8.87 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07163284657542404		[learning rate: 0.0048865]
		[batch 20/20] avg loss: 0.043084003086171954		[learning rate: 0.0048806]
	Learning Rate: 0.00488061
	LOSS [training: 0.057358424830798005 | validation: 0.04787258249636401]
	TIME [epoch: 8.89 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03277908303585431		[learning rate: 0.0048747]
		[batch 20/20] avg loss: 0.04494654226669208		[learning rate: 0.0048688]
	Learning Rate: 0.00486879
	LOSS [training: 0.038862812651273185 | validation: 0.042391502308464585]
	TIME [epoch: 8.86 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04736016342549243		[learning rate: 0.0048629]
		[batch 20/20] avg loss: 0.04015576062677677		[learning rate: 0.004857]
	Learning Rate: 0.004857
	LOSS [training: 0.0437579620261346 | validation: 0.011294389475487164]
	TIME [epoch: 8.86 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027296148449107178		[learning rate: 0.0048511]
		[batch 20/20] avg loss: 0.019752354246055266		[learning rate: 0.0048452]
	Learning Rate: 0.00484525
	LOSS [training: 0.023524251347581217 | validation: 0.05046199759576964]
	TIME [epoch: 8.87 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045636233151044245		[learning rate: 0.0048394]
		[batch 20/20] avg loss: 0.03341199800908408		[learning rate: 0.0048335]
	Learning Rate: 0.00483352
	LOSS [training: 0.039524115580064176 | validation: 0.06580703430905223]
	TIME [epoch: 8.86 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04544480840226553		[learning rate: 0.0048277]
		[batch 20/20] avg loss: 0.049094753275968926		[learning rate: 0.0048218]
	Learning Rate: 0.00482181
	LOSS [training: 0.04726978083911722 | validation: 0.013972346972362354]
	TIME [epoch: 8.89 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03926724407981173		[learning rate: 0.004816]
		[batch 20/20] avg loss: 0.05621229038035689		[learning rate: 0.0048101]
	Learning Rate: 0.00481014
	LOSS [training: 0.047739767230084315 | validation: 0.004191915443829264]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023196081556196638		[learning rate: 0.0048043]
		[batch 20/20] avg loss: 0.03276872315251098		[learning rate: 0.0047985]
	Learning Rate: 0.0047985
	LOSS [training: 0.027982402354353803 | validation: 0.11140544042100743]
	TIME [epoch: 8.87 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08675512351963124		[learning rate: 0.0047927]
		[batch 20/20] avg loss: 0.03446813820346731		[learning rate: 0.0047869]
	Learning Rate: 0.00478688
	LOSS [training: 0.06061163086154927 | validation: 0.011549601892852854]
	TIME [epoch: 8.86 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060251591922925986		[learning rate: 0.0047811]
		[batch 20/20] avg loss: 0.06342108912518522		[learning rate: 0.0047753]
	Learning Rate: 0.00477529
	LOSS [training: 0.0618363405240556 | validation: 0.09064260429415594]
	TIME [epoch: 8.88 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06990397659116812		[learning rate: 0.0047695]
		[batch 20/20] avg loss: 0.05540798407469495		[learning rate: 0.0047637]
	Learning Rate: 0.00476373
	LOSS [training: 0.06265598033293153 | validation: 0.023272843659133017]
	TIME [epoch: 8.91 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04034408320629436		[learning rate: 0.004758]
		[batch 20/20] avg loss: 0.03919416532623453		[learning rate: 0.0047522]
	Learning Rate: 0.0047522
	LOSS [training: 0.03976912426626445 | validation: 0.021531844693464648]
	TIME [epoch: 8.86 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04396384609395606		[learning rate: 0.0047464]
		[batch 20/20] avg loss: 0.04871887708347057		[learning rate: 0.0047407]
	Learning Rate: 0.0047407
	LOSS [training: 0.04634136158871331 | validation: 0.026842876250737707]
	TIME [epoch: 8.86 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0506170041964227		[learning rate: 0.004735]
		[batch 20/20] avg loss: 0.045876682450626385		[learning rate: 0.0047292]
	Learning Rate: 0.00472922
	LOSS [training: 0.04824684332352454 | validation: 0.0393687737713606]
	TIME [epoch: 8.86 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04945222058391853		[learning rate: 0.0047235]
		[batch 20/20] avg loss: 0.04580700004195476		[learning rate: 0.0047178]
	Learning Rate: 0.00471777
	LOSS [training: 0.04762961031293664 | validation: 0.0852917328416909]
	TIME [epoch: 8.89 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20307455518117457		[learning rate: 0.0047121]
		[batch 20/20] avg loss: 0.19614275724967545		[learning rate: 0.0047064]
	Learning Rate: 0.00470635
	LOSS [training: 0.19960865621542498 | validation: 0.17339539356553008]
	TIME [epoch: 8.87 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13433462411013364		[learning rate: 0.0047006]
		[batch 20/20] avg loss: 0.07640934089127778		[learning rate: 0.004695]
	Learning Rate: 0.00469496
	LOSS [training: 0.10537198250070572 | validation: 0.04688499781251863]
	TIME [epoch: 8.87 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06645469685454278		[learning rate: 0.0046893]
		[batch 20/20] avg loss: 0.056509978514272106		[learning rate: 0.0046836]
	Learning Rate: 0.00468359
	LOSS [training: 0.06148233768440745 | validation: 0.06485279524190041]
	TIME [epoch: 8.86 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08937934152086226		[learning rate: 0.0046779]
		[batch 20/20] avg loss: 0.05919182645624231		[learning rate: 0.0046723]
	Learning Rate: 0.00467225
	LOSS [training: 0.07428558398855227 | validation: 0.034744449762114736]
	TIME [epoch: 8.87 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11110708454797733		[learning rate: 0.0046666]
		[batch 20/20] avg loss: 0.12196176239932208		[learning rate: 0.0046609]
	Learning Rate: 0.00466094
	LOSS [training: 0.11653442347364969 | validation: 0.07272402094910072]
	TIME [epoch: 8.89 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12951636323489418		[learning rate: 0.0046553]
		[batch 20/20] avg loss: 0.0760642434519477		[learning rate: 0.0046497]
	Learning Rate: 0.00464966
	LOSS [training: 0.10279030334342096 | validation: 0.09166553773869277]
	TIME [epoch: 8.88 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06715630640107298		[learning rate: 0.004644]
		[batch 20/20] avg loss: 0.059584094082705964		[learning rate: 0.0046384]
	Learning Rate: 0.0046384
	LOSS [training: 0.06337020024188947 | validation: 0.040166939217268126]
	TIME [epoch: 8.86 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04562115475463979		[learning rate: 0.0046328]
		[batch 20/20] avg loss: 0.0656707476503477		[learning rate: 0.0046272]
	Learning Rate: 0.00462717
	LOSS [training: 0.055645951202493736 | validation: 0.014032888749506204]
	TIME [epoch: 8.86 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02764081404504489		[learning rate: 0.0046216]
		[batch 20/20] avg loss: 0.025118074955466703		[learning rate: 0.004616]
	Learning Rate: 0.00461597
	LOSS [training: 0.026379444500255804 | validation: 0.02417903191479066]
	TIME [epoch: 8.88 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024046911259760332		[learning rate: 0.0046104]
		[batch 20/20] avg loss: 0.03156587823115513		[learning rate: 0.0046048]
	Learning Rate: 0.0046048
	LOSS [training: 0.02780639474545773 | validation: 0.044666160524317954]
	TIME [epoch: 8.88 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04819131328031695		[learning rate: 0.0045992]
		[batch 20/20] avg loss: 0.08396277227072839		[learning rate: 0.0045936]
	Learning Rate: 0.00459365
	LOSS [training: 0.06607704277552265 | validation: 0.06735904632536294]
	TIME [epoch: 8.87 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03521056370926545		[learning rate: 0.0045881]
		[batch 20/20] avg loss: 0.05283638699696156		[learning rate: 0.0045825]
	Learning Rate: 0.00458253
	LOSS [training: 0.0440234753531135 | validation: 0.039514872627661124]
	TIME [epoch: 8.88 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060277615211535296		[learning rate: 0.004577]
		[batch 20/20] avg loss: 0.07750188521061546		[learning rate: 0.0045714]
	Learning Rate: 0.00457144
	LOSS [training: 0.06888975021107538 | validation: 0.045009765476644314]
	TIME [epoch: 8.86 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07440180977530698		[learning rate: 0.0045659]
		[batch 20/20] avg loss: 0.06664816672497215		[learning rate: 0.0045604]
	Learning Rate: 0.00456037
	LOSS [training: 0.07052498825013954 | validation: 0.0389994636758578]
	TIME [epoch: 8.89 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03621676858842899		[learning rate: 0.0045548]
		[batch 20/20] avg loss: 0.04325849492596308		[learning rate: 0.0045493]
	Learning Rate: 0.00454933
	LOSS [training: 0.03973763175719604 | validation: 0.030322422780454238]
	TIME [epoch: 8.88 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049742772992159935		[learning rate: 0.0045438]
		[batch 20/20] avg loss: 0.03658235344772758		[learning rate: 0.0045383]
	Learning Rate: 0.00453832
	LOSS [training: 0.04316256321994376 | validation: 0.1006819236732011]
	TIME [epoch: 8.86 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13121820258959474		[learning rate: 0.0045328]
		[batch 20/20] avg loss: 0.14816788984058582		[learning rate: 0.0045273]
	Learning Rate: 0.00452733
	LOSS [training: 0.1396930462150903 | validation: 0.06426373218087209]
	TIME [epoch: 8.86 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10487940906805238		[learning rate: 0.0045218]
		[batch 20/20] avg loss: 0.15280198088058145		[learning rate: 0.0045164]
	Learning Rate: 0.00451637
	LOSS [training: 0.12884069497431688 | validation: 0.09603488890348888]
	TIME [epoch: 8.86 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11916512178494117		[learning rate: 0.0045109]
		[batch 20/20] avg loss: 0.16048985159808432		[learning rate: 0.0045054]
	Learning Rate: 0.00450544
	LOSS [training: 0.13982748669151274 | validation: 0.1466765835938869]
	TIME [epoch: 8.89 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09237249163284696		[learning rate: 0.0045]
		[batch 20/20] avg loss: 0.0540091740423463		[learning rate: 0.0044945]
	Learning Rate: 0.00449453
	LOSS [training: 0.07319083283759661 | validation: 0.10365068590994349]
	TIME [epoch: 8.87 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0420835090363741		[learning rate: 0.0044891]
		[batch 20/20] avg loss: 0.044913358030054355		[learning rate: 0.0044836]
	Learning Rate: 0.00448365
	LOSS [training: 0.043498433533214226 | validation: 0.037495862212381596]
	TIME [epoch: 8.87 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07811113246134598		[learning rate: 0.0044782]
		[batch 20/20] avg loss: 0.04074629007482017		[learning rate: 0.0044728]
	Learning Rate: 0.00447279
	LOSS [training: 0.05942871126808309 | validation: 0.017486494672648345]
	TIME [epoch: 8.87 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052116426170270846		[learning rate: 0.0044674]
		[batch 20/20] avg loss: 0.044468978734018214		[learning rate: 0.004462]
	Learning Rate: 0.00446197
	LOSS [training: 0.04829270245214452 | validation: 0.03923711459234726]
	TIME [epoch: 8.87 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039446512855930746		[learning rate: 0.0044566]
		[batch 20/20] avg loss: 0.0285065543313505		[learning rate: 0.0044512]
	Learning Rate: 0.00445116
	LOSS [training: 0.033976533593640626 | validation: 0.018381702452761668]
	TIME [epoch: 8.89 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03816712818359923		[learning rate: 0.0044458]
		[batch 20/20] avg loss: 0.024805707291132344		[learning rate: 0.0044404]
	Learning Rate: 0.00444039
	LOSS [training: 0.03148641773736578 | validation: 0.043244000986848594]
	TIME [epoch: 8.86 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04576579148517952		[learning rate: 0.004435]
		[batch 20/20] avg loss: 0.034007683351252364		[learning rate: 0.0044296]
	Learning Rate: 0.00442964
	LOSS [training: 0.039886737418215946 | validation: 0.019243209265382557]
	TIME [epoch: 8.87 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06983310816235341		[learning rate: 0.0044243]
		[batch 20/20] avg loss: 0.042183002893243796		[learning rate: 0.0044189]
	Learning Rate: 0.00441892
	LOSS [training: 0.056008055527798604 | validation: 0.020082060143714584]
	TIME [epoch: 8.86 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04164756793702587		[learning rate: 0.0044136]
		[batch 20/20] avg loss: 0.04994191531854922		[learning rate: 0.0044082]
	Learning Rate: 0.00440822
	LOSS [training: 0.04579474162778755 | validation: 0.06015289416181809]
	TIME [epoch: 8.88 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06701725807668181		[learning rate: 0.0044029]
		[batch 20/20] avg loss: 0.04886891796234794		[learning rate: 0.0043975]
	Learning Rate: 0.00439755
	LOSS [training: 0.057943088019514866 | validation: 0.03812442027057322]
	TIME [epoch: 8.84 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06359153277641409		[learning rate: 0.0043922]
		[batch 20/20] avg loss: 0.04061773171903648		[learning rate: 0.0043869]
	Learning Rate: 0.0043869
	LOSS [training: 0.052104632247725303 | validation: 0.010070605738322207]
	TIME [epoch: 8.86 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02741937898284716		[learning rate: 0.0043816]
		[batch 20/20] avg loss: 0.05658355857601601		[learning rate: 0.0043763]
	Learning Rate: 0.00437628
	LOSS [training: 0.04200146877943159 | validation: 0.061483890414948145]
	TIME [epoch: 8.87 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05414604652239674		[learning rate: 0.004371]
		[batch 20/20] avg loss: 0.1067954635095871		[learning rate: 0.0043657]
	Learning Rate: 0.00436569
	LOSS [training: 0.08047075501599192 | validation: 0.08030876776177136]
	TIME [epoch: 8.87 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07328632869908744		[learning rate: 0.0043604]
		[batch 20/20] avg loss: 0.04834828051780256		[learning rate: 0.0043551]
	Learning Rate: 0.00435512
	LOSS [training: 0.060817304608445 | validation: 0.01400193434254212]
	TIME [epoch: 8.89 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032547159027996306		[learning rate: 0.0043498]
		[batch 20/20] avg loss: 0.04798546477735581		[learning rate: 0.0043446]
	Learning Rate: 0.00434458
	LOSS [training: 0.04026631190267606 | validation: 0.026681742609637352]
	TIME [epoch: 8.86 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03297068148128466		[learning rate: 0.0043393]
		[batch 20/20] avg loss: 0.01589580774877381		[learning rate: 0.0043341]
	Learning Rate: 0.00433406
	LOSS [training: 0.024433244615029236 | validation: 0.07828032843129959]
	TIME [epoch: 8.87 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040688357243062336		[learning rate: 0.0043288]
		[batch 20/20] avg loss: 0.033846388596171645		[learning rate: 0.0043236]
	Learning Rate: 0.00432357
	LOSS [training: 0.037267372919616994 | validation: 0.018480809565667652]
	TIME [epoch: 8.86 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044677132358837225		[learning rate: 0.0043183]
		[batch 20/20] avg loss: 0.04068865281047368		[learning rate: 0.0043131]
	Learning Rate: 0.0043131
	LOSS [training: 0.04268289258465545 | validation: 0.030619428502331417]
	TIME [epoch: 8.86 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03674414202596708		[learning rate: 0.0043079]
		[batch 20/20] avg loss: 0.026227764311103435		[learning rate: 0.0043027]
	Learning Rate: 0.00430266
	LOSS [training: 0.03148595316853526 | validation: 0.03410497445294522]
	TIME [epoch: 8.89 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025958884373647562		[learning rate: 0.0042974]
		[batch 20/20] avg loss: 0.028486305989309534		[learning rate: 0.0042922]
	Learning Rate: 0.00429224
	LOSS [training: 0.027222595181478553 | validation: 0.023464856828740936]
	TIME [epoch: 8.87 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04562943722777707		[learning rate: 0.004287]
		[batch 20/20] avg loss: 0.032095335971750544		[learning rate: 0.0042819]
	Learning Rate: 0.00428185
	LOSS [training: 0.03886238659976381 | validation: 0.01884521350841957]
	TIME [epoch: 8.87 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0166327423412507		[learning rate: 0.0042767]
		[batch 20/20] avg loss: 0.025624728171651474		[learning rate: 0.0042715]
	Learning Rate: 0.00427149
	LOSS [training: 0.02112873525645109 | validation: 0.005829240551630194]
	TIME [epoch: 8.87 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02567734382123376		[learning rate: 0.0042663]
		[batch 20/20] avg loss: 0.04220800555011782		[learning rate: 0.0042611]
	Learning Rate: 0.00426114
	LOSS [training: 0.033942674685675786 | validation: 0.04193748427497032]
	TIME [epoch: 8.88 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02494741045926017		[learning rate: 0.004256]
		[batch 20/20] avg loss: 0.05156417849660051		[learning rate: 0.0042508]
	Learning Rate: 0.00425083
	LOSS [training: 0.03825579447793034 | validation: 0.03957400910314193]
	TIME [epoch: 8.87 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029104266991805017		[learning rate: 0.0042457]
		[batch 20/20] avg loss: 0.032061919563397004		[learning rate: 0.0042405]
	Learning Rate: 0.00424054
	LOSS [training: 0.030583093277601014 | validation: 0.027932067998410172]
	TIME [epoch: 8.87 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04801396281693351		[learning rate: 0.0042354]
		[batch 20/20] avg loss: 0.030382679838979226		[learning rate: 0.0042303]
	Learning Rate: 0.00423027
	LOSS [training: 0.039198321327956365 | validation: 0.026625534325242183]
	TIME [epoch: 8.87 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046936063537149866		[learning rate: 0.0042251]
		[batch 20/20] avg loss: 0.037527104996523036		[learning rate: 0.00422]
	Learning Rate: 0.00422003
	LOSS [training: 0.04223158426683645 | validation: 0.018570190549290114]
	TIME [epoch: 8.86 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03961038566846872		[learning rate: 0.0042149]
		[batch 20/20] avg loss: 0.04445885361098985		[learning rate: 0.0042098]
	Learning Rate: 0.00420982
	LOSS [training: 0.04203461963972928 | validation: 0.07629147488692162]
	TIME [epoch: 8.88 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05342609108063352		[learning rate: 0.0042047]
		[batch 20/20] avg loss: 0.03170197776379375		[learning rate: 0.0041996]
	Learning Rate: 0.00419962
	LOSS [training: 0.04256403442221363 | validation: 0.02975357759780584]
	TIME [epoch: 8.88 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03547078871782479		[learning rate: 0.0041945]
		[batch 20/20] avg loss: 0.034237645041893486		[learning rate: 0.0041895]
	Learning Rate: 0.00418946
	LOSS [training: 0.03485421687985914 | validation: 0.05038581854785547]
	TIME [epoch: 8.84 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04155630396897965		[learning rate: 0.0041844]
		[batch 20/20] avg loss: 0.04535075946168113		[learning rate: 0.0041793]
	Learning Rate: 0.00417932
	LOSS [training: 0.04345353171533038 | validation: 0.028432921696256063]
	TIME [epoch: 8.86 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028245364575619568		[learning rate: 0.0041743]
		[batch 20/20] avg loss: 0.03400342128164792		[learning rate: 0.0041692]
	Learning Rate: 0.0041692
	LOSS [training: 0.031124392928633754 | validation: 0.05867408276769297]
	TIME [epoch: 8.86 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040688299109937306		[learning rate: 0.0041641]
		[batch 20/20] avg loss: 0.056186783500096815		[learning rate: 0.0041591]
	Learning Rate: 0.00415911
	LOSS [training: 0.048437541305017054 | validation: 0.054728791086148934]
	TIME [epoch: 8.89 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039687347936602405		[learning rate: 0.0041541]
		[batch 20/20] avg loss: 0.06304118973437674		[learning rate: 0.004149]
	Learning Rate: 0.00414904
	LOSS [training: 0.05136426883548957 | validation: 0.03178411829655582]
	TIME [epoch: 8.87 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06638678322066463		[learning rate: 0.004144]
		[batch 20/20] avg loss: 0.06073369586124807		[learning rate: 0.004139]
	Learning Rate: 0.00413899
	LOSS [training: 0.06356023954095635 | validation: 0.011390584383500423]
	TIME [epoch: 8.87 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0667109774367933		[learning rate: 0.004134]
		[batch 20/20] avg loss: 0.039039522824750816		[learning rate: 0.004129]
	Learning Rate: 0.00412897
	LOSS [training: 0.05287525013077206 | validation: 0.01542620350707147]
	TIME [epoch: 8.87 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04709898452569382		[learning rate: 0.004124]
		[batch 20/20] avg loss: 0.028406379789984216		[learning rate: 0.004119]
	Learning Rate: 0.00411898
	LOSS [training: 0.03775268215783902 | validation: 0.018973378276851315]
	TIME [epoch: 8.86 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02799411759313572		[learning rate: 0.004114]
		[batch 20/20] avg loss: 0.04501108423654829		[learning rate: 0.004109]
	Learning Rate: 0.00410901
	LOSS [training: 0.036502600914842004 | validation: 0.04799963864355791]
	TIME [epoch: 8.89 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02963478782561501		[learning rate: 0.004104]
		[batch 20/20] avg loss: 0.05587864832378442		[learning rate: 0.0040991]
	Learning Rate: 0.00409906
	LOSS [training: 0.04275671807469972 | validation: 0.055451273207982564]
	TIME [epoch: 8.87 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07121404147901739		[learning rate: 0.0040941]
		[batch 20/20] avg loss: 0.04755968341140401		[learning rate: 0.0040891]
	Learning Rate: 0.00408914
	LOSS [training: 0.0593868624452107 | validation: 0.05365234180137448]
	TIME [epoch: 8.87 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06368438400290524		[learning rate: 0.0040842]
		[batch 20/20] avg loss: 0.13069670714035656		[learning rate: 0.0040792]
	Learning Rate: 0.00407924
	LOSS [training: 0.09719054557163091 | validation: 0.063292350972688]
	TIME [epoch: 8.87 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15070195507626769		[learning rate: 0.0040743]
		[batch 20/20] avg loss: 0.09098851082268458		[learning rate: 0.0040694]
	Learning Rate: 0.00406936
	LOSS [training: 0.12084523294947613 | validation: 0.07125930147837198]
	TIME [epoch: 8.88 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048162948576265074		[learning rate: 0.0040644]
		[batch 20/20] avg loss: 0.09547146350920568		[learning rate: 0.0040595]
	Learning Rate: 0.00405951
	LOSS [training: 0.07181720604273538 | validation: 0.041337722832933715]
	TIME [epoch: 8.88 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.072087300381046		[learning rate: 0.0040546]
		[batch 20/20] avg loss: 0.03477227246444613		[learning rate: 0.0040497]
	Learning Rate: 0.00404968
	LOSS [training: 0.05342978642274606 | validation: 0.017116316602546786]
	TIME [epoch: 8.87 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030415288416709325		[learning rate: 0.0040448]
		[batch 20/20] avg loss: 0.06334553004463206		[learning rate: 0.0040399]
	Learning Rate: 0.00403988
	LOSS [training: 0.04688040923067069 | validation: 0.031476719426276126]
	TIME [epoch: 8.87 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07659418388966632		[learning rate: 0.004035]
		[batch 20/20] avg loss: 0.046035649697340864		[learning rate: 0.0040301]
	Learning Rate: 0.0040301
	LOSS [training: 0.06131491679350359 | validation: 0.03281546250406502]
	TIME [epoch: 8.87 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05886988055488379		[learning rate: 0.0040252]
		[batch 20/20] avg loss: 0.04748999182786936		[learning rate: 0.0040203]
	Learning Rate: 0.00402034
	LOSS [training: 0.05317993619137658 | validation: 0.02937178945506569]
	TIME [epoch: 8.89 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0327374476974775		[learning rate: 0.0040155]
		[batch 20/20] avg loss: 0.040075571387767836		[learning rate: 0.0040106]
	Learning Rate: 0.00401061
	LOSS [training: 0.036406509542622666 | validation: 0.025421830199574995]
	TIME [epoch: 8.88 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026026692322731283		[learning rate: 0.0040058]
		[batch 20/20] avg loss: 0.02789374581358379		[learning rate: 0.0040009]
	Learning Rate: 0.0040009
	LOSS [training: 0.026960219068157536 | validation: 0.012063013924817768]
	TIME [epoch: 8.87 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06296645311274966		[learning rate: 0.0039961]
		[batch 20/20] avg loss: 0.16496205772441513		[learning rate: 0.0039912]
	Learning Rate: 0.00399122
	LOSS [training: 0.11396425541858238 | validation: 0.11226208624894285]
	TIME [epoch: 8.87 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12348624907591092		[learning rate: 0.0039864]
		[batch 20/20] avg loss: 0.04309265613080035		[learning rate: 0.0039816]
	Learning Rate: 0.00398155
	LOSS [training: 0.08328945260335564 | validation: 0.017508647478919795]
	TIME [epoch: 8.86 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04498638263344322		[learning rate: 0.0039767]
		[batch 20/20] avg loss: 0.03791141918679218		[learning rate: 0.0039719]
	Learning Rate: 0.00397192
	LOSS [training: 0.041448900910117714 | validation: 0.0158426296258122]
	TIME [epoch: 8.89 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014706994050355766		[learning rate: 0.0039671]
		[batch 20/20] avg loss: 0.0356869535187316		[learning rate: 0.0039623]
	Learning Rate: 0.0039623
	LOSS [training: 0.02519697378454368 | validation: 0.06508778602671532]
	TIME [epoch: 8.87 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04252337811370319		[learning rate: 0.0039575]
		[batch 20/20] avg loss: 0.04471270219221728		[learning rate: 0.0039527]
	Learning Rate: 0.00395271
	LOSS [training: 0.04361804015296023 | validation: 0.012387770937898411]
	TIME [epoch: 8.88 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03526723763558573		[learning rate: 0.0039479]
		[batch 20/20] avg loss: 0.03573825368683152		[learning rate: 0.0039431]
	Learning Rate: 0.00394314
	LOSS [training: 0.03550274566120863 | validation: 0.03866692176360342]
	TIME [epoch: 8.87 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056641571274213945		[learning rate: 0.0039384]
		[batch 20/20] avg loss: 0.06947184530364307		[learning rate: 0.0039336]
	Learning Rate: 0.00393359
	LOSS [training: 0.06305670828892851 | validation: 0.027438488058468952]
	TIME [epoch: 8.88 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03563025537367845		[learning rate: 0.0039288]
		[batch 20/20] avg loss: 0.048455644332435564		[learning rate: 0.0039241]
	Learning Rate: 0.00392407
	LOSS [training: 0.04204294985305701 | validation: 0.10973966403133509]
	TIME [epoch: 8.88 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11256348470581387		[learning rate: 0.0039193]
		[batch 20/20] avg loss: 0.046048153960872786		[learning rate: 0.0039146]
	Learning Rate: 0.00391457
	LOSS [training: 0.07930581933334332 | validation: 0.04752680679753831]
	TIME [epoch: 8.87 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033511411198344984		[learning rate: 0.0039098]
		[batch 20/20] avg loss: 0.02998831675365869		[learning rate: 0.0039051]
	Learning Rate: 0.00390509
	LOSS [training: 0.03174986397600183 | validation: 0.03290385680576923]
	TIME [epoch: 8.87 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03783770264380307		[learning rate: 0.0039004]
		[batch 20/20] avg loss: 0.025069909538173313		[learning rate: 0.0038956]
	Learning Rate: 0.00389564
	LOSS [training: 0.0314538060909882 | validation: 0.009496429505252398]
	TIME [epoch: 8.87 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019903362467124805		[learning rate: 0.0038909]
		[batch 20/20] avg loss: 0.049399214209080375		[learning rate: 0.0038862]
	Learning Rate: 0.00388621
	LOSS [training: 0.03465128833810259 | validation: 0.08804184688526602]
	TIME [epoch: 8.9 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05166740146663691		[learning rate: 0.0038815]
		[batch 20/20] avg loss: 0.09411158184655014		[learning rate: 0.0038768]
	Learning Rate: 0.0038768
	LOSS [training: 0.07288949165659353 | validation: 0.06450815578239524]
	TIME [epoch: 8.87 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044618218518483556		[learning rate: 0.0038721]
		[batch 20/20] avg loss: 0.0475524504498382		[learning rate: 0.0038674]
	Learning Rate: 0.00386742
	LOSS [training: 0.04608533448416088 | validation: 0.09802899018510638]
	TIME [epoch: 8.87 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059502647393228525		[learning rate: 0.0038627]
		[batch 20/20] avg loss: 0.053105545007458455		[learning rate: 0.0038581]
	Learning Rate: 0.00385805
	LOSS [training: 0.056304096200343504 | validation: 0.016708871758048662]
	TIME [epoch: 8.86 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02536076932604623		[learning rate: 0.0038534]
		[batch 20/20] avg loss: 0.06216715168571794		[learning rate: 0.0038487]
	Learning Rate: 0.00384871
	LOSS [training: 0.04376396050588209 | validation: 0.03937467136583158]
	TIME [epoch: 8.86 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05504257453944365		[learning rate: 0.0038441]
		[batch 20/20] avg loss: 0.03644803663326966		[learning rate: 0.0038394]
	Learning Rate: 0.0038394
	LOSS [training: 0.045745305586356665 | validation: 0.07348678328213115]
	TIME [epoch: 8.89 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13738666454666357		[learning rate: 0.0038347]
		[batch 20/20] avg loss: 0.03307697766302799		[learning rate: 0.0038301]
	Learning Rate: 0.0038301
	LOSS [training: 0.0852318211048458 | validation: 0.07898267054888852]
	TIME [epoch: 8.86 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10645029100409382		[learning rate: 0.0038255]
		[batch 20/20] avg loss: 0.07283408578097603		[learning rate: 0.0038208]
	Learning Rate: 0.00382083
	LOSS [training: 0.08964218839253493 | validation: 0.045115202407927796]
	TIME [epoch: 8.88 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0643767089479215		[learning rate: 0.0038162]
		[batch 20/20] avg loss: 0.061249067566116186		[learning rate: 0.0038116]
	Learning Rate: 0.00381158
	LOSS [training: 0.06281288825701883 | validation: 0.1287280812412473]
	TIME [epoch: 8.87 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08058002643482598		[learning rate: 0.003807]
		[batch 20/20] avg loss: 0.026767098861186788		[learning rate: 0.0038024]
	Learning Rate: 0.00380235
	LOSS [training: 0.05367356264800638 | validation: 0.0261040664392045]
	TIME [epoch: 8.87 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03313035502090418		[learning rate: 0.0037977]
		[batch 20/20] avg loss: 0.0444471520843686		[learning rate: 0.0037931]
	Learning Rate: 0.00379315
	LOSS [training: 0.03878875355263639 | validation: 0.039283264266691174]
	TIME [epoch: 8.89 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04707815084818236		[learning rate: 0.0037886]
		[batch 20/20] avg loss: 0.04430617392500815		[learning rate: 0.003784]
	Learning Rate: 0.00378397
	LOSS [training: 0.04569216238659525 | validation: 0.011101448861813133]
	TIME [epoch: 8.86 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019991057972698744		[learning rate: 0.0037794]
		[batch 20/20] avg loss: 0.02910220779499325		[learning rate: 0.0037748]
	Learning Rate: 0.00377481
	LOSS [training: 0.024546632883846 | validation: 0.032613265376005014]
	TIME [epoch: 8.87 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07919726395212635		[learning rate: 0.0037702]
		[batch 20/20] avg loss: 0.08357347846137855		[learning rate: 0.0037657]
	Learning Rate: 0.00376567
	LOSS [training: 0.08138537120675242 | validation: 0.1995686327571012]
	TIME [epoch: 8.88 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1808776236020529		[learning rate: 0.0037611]
		[batch 20/20] avg loss: 0.15174917912333444		[learning rate: 0.0037566]
	Learning Rate: 0.00375655
	LOSS [training: 0.16631340136269362 | validation: 0.07341543521156875]
	TIME [epoch: 8.89 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05258420279006415		[learning rate: 0.003752]
		[batch 20/20] avg loss: 0.04552286415594962		[learning rate: 0.0037475]
	Learning Rate: 0.00374746
	LOSS [training: 0.04905353347300688 | validation: 0.03140051061204659]
	TIME [epoch: 8.87 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07480694731555107		[learning rate: 0.0037429]
		[batch 20/20] avg loss: 0.03858801665544115		[learning rate: 0.0037384]
	Learning Rate: 0.00373839
	LOSS [training: 0.0566974819854961 | validation: 0.03371862754087024]
	TIME [epoch: 8.87 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07305429407041959		[learning rate: 0.0037339]
		[batch 20/20] avg loss: 0.06233249857200165		[learning rate: 0.0037293]
	Learning Rate: 0.00372934
	LOSS [training: 0.0676933963212106 | validation: 0.07278321187422775]
	TIME [epoch: 8.89 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052895778486525304		[learning rate: 0.0037248]
		[batch 20/20] avg loss: 0.05510879659100678		[learning rate: 0.0037203]
	Learning Rate: 0.00372031
	LOSS [training: 0.054002287538766035 | validation: 0.031578020580670774]
	TIME [epoch: 8.85 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06063192115530038		[learning rate: 0.0037158]
		[batch 20/20] avg loss: 0.037567269278414764		[learning rate: 0.0037113]
	Learning Rate: 0.0037113
	LOSS [training: 0.04909959521685757 | validation: 0.019364207840398454]
	TIME [epoch: 8.87 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027434858903526836		[learning rate: 0.0037068]
		[batch 20/20] avg loss: 0.04716336104878713		[learning rate: 0.0037023]
	Learning Rate: 0.00370232
	LOSS [training: 0.037299109976156986 | validation: 0.019245409156478814]
	TIME [epoch: 8.84 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046934215249920815		[learning rate: 0.0036978]
		[batch 20/20] avg loss: 0.029952691931266224		[learning rate: 0.0036934]
	Learning Rate: 0.00369336
	LOSS [training: 0.03844345359059352 | validation: 0.05578645336051702]
	TIME [epoch: 8.87 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03162526799110574		[learning rate: 0.0036889]
		[batch 20/20] avg loss: 0.042286133466797715		[learning rate: 0.0036844]
	Learning Rate: 0.00368441
	LOSS [training: 0.036955700728951726 | validation: 0.03982858106670996]
	TIME [epoch: 8.87 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02722874393698928		[learning rate: 0.00368]
		[batch 20/20] avg loss: 0.030152239314828577		[learning rate: 0.0036755]
	Learning Rate: 0.00367549
	LOSS [training: 0.028690491625908927 | validation: 0.02461405935413715]
	TIME [epoch: 8.87 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03546075738448604		[learning rate: 0.003671]
		[batch 20/20] avg loss: 0.026215877636344875		[learning rate: 0.0036666]
	Learning Rate: 0.0036666
	LOSS [training: 0.030838317510415457 | validation: 0.03339385576887884]
	TIME [epoch: 8.89 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034302894082704184		[learning rate: 0.0036622]
		[batch 20/20] avg loss: 0.04026431298207471		[learning rate: 0.0036577]
	Learning Rate: 0.00365772
	LOSS [training: 0.03728360353238945 | validation: 0.09018230233395233]
	TIME [epoch: 8.87 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029565687076583864		[learning rate: 0.0036533]
		[batch 20/20] avg loss: 0.03669141923386764		[learning rate: 0.0036489]
	Learning Rate: 0.00364887
	LOSS [training: 0.03312855315522576 | validation: 0.0445258918648422]
	TIME [epoch: 8.85 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028405773974660004		[learning rate: 0.0036444]
		[batch 20/20] avg loss: 0.04174071959028157		[learning rate: 0.00364]
	Learning Rate: 0.00364003
	LOSS [training: 0.03507324678247079 | validation: 0.029278771133165463]
	TIME [epoch: 8.86 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025966168088170344		[learning rate: 0.0036356]
		[batch 20/20] avg loss: 0.037674976170698475		[learning rate: 0.0036312]
	Learning Rate: 0.00363122
	LOSS [training: 0.03182057212943441 | validation: 0.03242015573449033]
	TIME [epoch: 8.88 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033939930350427736		[learning rate: 0.0036268]
		[batch 20/20] avg loss: 0.022368337898753594		[learning rate: 0.0036224]
	Learning Rate: 0.00362243
	LOSS [training: 0.028154134124590658 | validation: 0.010077955806698218]
	TIME [epoch: 8.89 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05038803447623412		[learning rate: 0.003618]
		[batch 20/20] avg loss: 0.05053774475889281		[learning rate: 0.0036137]
	Learning Rate: 0.00361366
	LOSS [training: 0.050462889617563456 | validation: 0.034456533180761445]
	TIME [epoch: 8.87 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02848499960287091		[learning rate: 0.0036093]
		[batch 20/20] avg loss: 0.040914483547785284		[learning rate: 0.0036049]
	Learning Rate: 0.00360491
	LOSS [training: 0.03469974157532809 | validation: 0.018694106264056126]
	TIME [epoch: 8.86 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03423266449700428		[learning rate: 0.0036005]
		[batch 20/20] avg loss: 0.042525193339642324		[learning rate: 0.0035962]
	Learning Rate: 0.00359619
	LOSS [training: 0.0383789289183233 | validation: 0.02315398828555158]
	TIME [epoch: 8.87 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03590582336883731		[learning rate: 0.0035918]
		[batch 20/20] avg loss: 0.03577200513106503		[learning rate: 0.0035875]
	Learning Rate: 0.00358748
	LOSS [training: 0.03583891424995118 | validation: 0.001443321282685962]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015416311336530403		[learning rate: 0.0035831]
		[batch 20/20] avg loss: 0.026023594811011935		[learning rate: 0.0035788]
	Learning Rate: 0.0035788
	LOSS [training: 0.02071995307377117 | validation: 0.025404313622428162]
	TIME [epoch: 8.87 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050243790837659355		[learning rate: 0.0035745]
		[batch 20/20] avg loss: 0.024769513001411208		[learning rate: 0.0035701]
	Learning Rate: 0.00357013
	LOSS [training: 0.037506651919535276 | validation: 0.02804741464421169]
	TIME [epoch: 8.86 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04761871321117532		[learning rate: 0.0035658]
		[batch 20/20] avg loss: 0.022498403563064293		[learning rate: 0.0035615]
	Learning Rate: 0.00356149
	LOSS [training: 0.035058558387119805 | validation: 0.003313583127384093]
	TIME [epoch: 8.87 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04474250437654487		[learning rate: 0.0035572]
		[batch 20/20] avg loss: 0.015226535267816945		[learning rate: 0.0035529]
	Learning Rate: 0.00355287
	LOSS [training: 0.0299845198221809 | validation: 0.029337831018654094]
	TIME [epoch: 8.86 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022011538261610484		[learning rate: 0.0035486]
		[batch 20/20] avg loss: 0.02030252673474396		[learning rate: 0.0035443]
	Learning Rate: 0.00354427
	LOSS [training: 0.021157032498177224 | validation: 0.011360698879550041]
	TIME [epoch: 8.89 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021610597519458586		[learning rate: 0.00354]
		[batch 20/20] avg loss: 0.025098209332437164		[learning rate: 0.0035357]
	Learning Rate: 0.00353569
	LOSS [training: 0.023354403425947877 | validation: 0.03406628605479933]
	TIME [epoch: 8.87 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049920373719679195		[learning rate: 0.0035314]
		[batch 20/20] avg loss: 0.02589425032268266		[learning rate: 0.0035271]
	Learning Rate: 0.00352713
	LOSS [training: 0.037907312021180924 | validation: 0.015514383653699849]
	TIME [epoch: 8.87 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04706075624410992		[learning rate: 0.0035229]
		[batch 20/20] avg loss: 0.06519932730733134		[learning rate: 0.0035186]
	Learning Rate: 0.00351859
	LOSS [training: 0.05613004177572063 | validation: 0.04644786053611266]
	TIME [epoch: 8.86 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031667387979751334		[learning rate: 0.0035143]
		[batch 20/20] avg loss: 0.02032968481641144		[learning rate: 0.0035101]
	Learning Rate: 0.00351007
	LOSS [training: 0.02599853639808139 | validation: 0.0709440474806485]
	TIME [epoch: 8.88 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035849660777791045		[learning rate: 0.0035058]
		[batch 20/20] avg loss: 0.02042001437826014		[learning rate: 0.0035016]
	Learning Rate: 0.00350157
	LOSS [training: 0.028134837578025595 | validation: 0.015860089105556308]
	TIME [epoch: 8.88 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01677675609752963		[learning rate: 0.0034973]
		[batch 20/20] avg loss: 0.019106429618329483		[learning rate: 0.0034931]
	Learning Rate: 0.0034931
	LOSS [training: 0.017941592857929557 | validation: 0.0028363922549250664]
	TIME [epoch: 8.87 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032136523550244586		[learning rate: 0.0034889]
		[batch 20/20] avg loss: 0.02255314412986968		[learning rate: 0.0034846]
	Learning Rate: 0.00348464
	LOSS [training: 0.027344833840057137 | validation: 0.01418826963685117]
	TIME [epoch: 8.86 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02818254079091756		[learning rate: 0.0034804]
		[batch 20/20] avg loss: 0.03263628743346442		[learning rate: 0.0034762]
	Learning Rate: 0.0034762
	LOSS [training: 0.030409414112190995 | validation: 0.020859460553690902]
	TIME [epoch: 8.87 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0412434935461564		[learning rate: 0.003472]
		[batch 20/20] avg loss: 0.03251595686674684		[learning rate: 0.0034678]
	Learning Rate: 0.00346779
	LOSS [training: 0.03687972520645162 | validation: 0.03600172184540646]
	TIME [epoch: 8.89 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03862795118873044		[learning rate: 0.0034636]
		[batch 20/20] avg loss: 0.039544760438312135		[learning rate: 0.0034594]
	Learning Rate: 0.00345939
	LOSS [training: 0.03908635581352129 | validation: 0.044597821726551995]
	TIME [epoch: 8.89 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03844233145543239		[learning rate: 0.0034552]
		[batch 20/20] avg loss: 0.019118917558355035		[learning rate: 0.003451]
	Learning Rate: 0.00345102
	LOSS [training: 0.02878062450689371 | validation: -0.0002173106687485037]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05456107255615207		[learning rate: 0.0034468]
		[batch 20/20] avg loss: 0.036174948463599685		[learning rate: 0.0034427]
	Learning Rate: 0.00344266
	LOSS [training: 0.04536801050987588 | validation: 0.02383719404643759]
	TIME [epoch: 8.85 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033028887407822595		[learning rate: 0.0034385]
		[batch 20/20] avg loss: 0.029457249941302405		[learning rate: 0.0034343]
	Learning Rate: 0.00343433
	LOSS [training: 0.031243068674562502 | validation: 0.016615082599871402]
	TIME [epoch: 8.86 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029094792620226944		[learning rate: 0.0034302]
		[batch 20/20] avg loss: 0.02472590319907749		[learning rate: 0.003426]
	Learning Rate: 0.00342602
	LOSS [training: 0.026910347909652216 | validation: 0.008768395447810474]
	TIME [epoch: 8.88 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020710589881158915		[learning rate: 0.0034219]
		[batch 20/20] avg loss: 0.035703268875394115		[learning rate: 0.0034177]
	Learning Rate: 0.00341772
	LOSS [training: 0.028206929378276513 | validation: 0.013489714337891694]
	TIME [epoch: 8.86 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018406396534026065		[learning rate: 0.0034136]
		[batch 20/20] avg loss: 0.04790606267283774		[learning rate: 0.0034094]
	Learning Rate: 0.00340945
	LOSS [training: 0.03315622960343191 | validation: 0.04510948646763403]
	TIME [epoch: 8.86 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03268098164225937		[learning rate: 0.0034053]
		[batch 20/20] avg loss: 0.03323676710445124		[learning rate: 0.0034012]
	Learning Rate: 0.0034012
	LOSS [training: 0.0329588743733553 | validation: 0.02013842479505091]
	TIME [epoch: 8.8 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03603475045670387		[learning rate: 0.0033971]
		[batch 20/20] avg loss: 0.028847386839648975		[learning rate: 0.003393]
	Learning Rate: 0.00339296
	LOSS [training: 0.032441068648176426 | validation: 0.015957493132410423]
	TIME [epoch: 8.84 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020585621184305496		[learning rate: 0.0033889]
		[batch 20/20] avg loss: 0.02672356571416762		[learning rate: 0.0033847]
	Learning Rate: 0.00338475
	LOSS [training: 0.02365459344923656 | validation: 0.015430814865976092]
	TIME [epoch: 8.88 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018599554688952667		[learning rate: 0.0033806]
		[batch 20/20] avg loss: 0.029346270986711703		[learning rate: 0.0033766]
	Learning Rate: 0.00337655
	LOSS [training: 0.023972912837832185 | validation: 0.02379964157610929]
	TIME [epoch: 8.86 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025790500590193978		[learning rate: 0.0033725]
		[batch 20/20] avg loss: 0.03923306689057986		[learning rate: 0.0033684]
	Learning Rate: 0.00336838
	LOSS [training: 0.032511783740386914 | validation: 0.014450758751211955]
	TIME [epoch: 8.87 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021602093653057493		[learning rate: 0.0033643]
		[batch 20/20] avg loss: 0.022187135081110494		[learning rate: 0.0033602]
	Learning Rate: 0.00336023
	LOSS [training: 0.02189461436708399 | validation: 0.015349160148376671]
	TIME [epoch: 8.82 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024807382088999724		[learning rate: 0.0033562]
		[batch 20/20] avg loss: 0.01556536170612625		[learning rate: 0.0033521]
	Learning Rate: 0.00335209
	LOSS [training: 0.020186371897562984 | validation: 0.06007435110581141]
	TIME [epoch: 8.89 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05088237982426145		[learning rate: 0.003348]
		[batch 20/20] avg loss: 0.02260793304601818		[learning rate: 0.003344]
	Learning Rate: 0.00334398
	LOSS [training: 0.036745156435139814 | validation: 0.02232230607205256]
	TIME [epoch: 8.88 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023558752307997803		[learning rate: 0.0033399]
		[batch 20/20] avg loss: 0.021484410105684013		[learning rate: 0.0033359]
	Learning Rate: 0.00333588
	LOSS [training: 0.02252158120684091 | validation: 0.011526920591421309]
	TIME [epoch: 8.86 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027515088803016324		[learning rate: 0.0033318]
		[batch 20/20] avg loss: 0.04161663755660627		[learning rate: 0.0033278]
	Learning Rate: 0.00332781
	LOSS [training: 0.03456586317981129 | validation: 0.0255164869146906]
	TIME [epoch: 8.86 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03156515559516819		[learning rate: 0.0033238]
		[batch 20/20] avg loss: 0.03411099847714472		[learning rate: 0.0033197]
	Learning Rate: 0.00331975
	LOSS [training: 0.032838077036156446 | validation: 0.02720269716355265]
	TIME [epoch: 8.86 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020890830909919098		[learning rate: 0.0033157]
		[batch 20/20] avg loss: 0.02009255031029451		[learning rate: 0.0033117]
	Learning Rate: 0.00331171
	LOSS [training: 0.020491690610106805 | validation: 0.01819417057602495]
	TIME [epoch: 8.89 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026451280508699732		[learning rate: 0.0033077]
		[batch 20/20] avg loss: 0.027612830364687157		[learning rate: 0.0033037]
	Learning Rate: 0.0033037
	LOSS [training: 0.02703205543669344 | validation: 0.04704758459984347]
	TIME [epoch: 8.84 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027546818746854645		[learning rate: 0.0032997]
		[batch 20/20] avg loss: 0.03826558330053169		[learning rate: 0.0032957]
	Learning Rate: 0.0032957
	LOSS [training: 0.03290620102369317 | validation: 0.05805416040329553]
	TIME [epoch: 8.82 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02964233945116555		[learning rate: 0.0032917]
		[batch 20/20] avg loss: 0.014889067686276108		[learning rate: 0.0032877]
	Learning Rate: 0.00328772
	LOSS [training: 0.022265703568720832 | validation: 0.007982663563478553]
	TIME [epoch: 8.86 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02245510297325992		[learning rate: 0.0032837]
		[batch 20/20] avg loss: 0.03616551575660861		[learning rate: 0.0032798]
	Learning Rate: 0.00327976
	LOSS [training: 0.029310309364934267 | validation: 0.01775329309711471]
	TIME [epoch: 8.87 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01721509058108075		[learning rate: 0.0032758]
		[batch 20/20] avg loss: 0.026502651625755547		[learning rate: 0.0032718]
	Learning Rate: 0.00327182
	LOSS [training: 0.02185887110341815 | validation: 0.010856909380622382]
	TIME [epoch: 8.88 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02034910938233537		[learning rate: 0.0032679]
		[batch 20/20] avg loss: 0.0319516814537351		[learning rate: 0.0032639]
	Learning Rate: 0.0032639
	LOSS [training: 0.02615039541803523 | validation: 0.031137340898529216]
	TIME [epoch: 8.87 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024747414513615285		[learning rate: 0.0032599]
		[batch 20/20] avg loss: 0.022117043204942028		[learning rate: 0.003256]
	Learning Rate: 0.003256
	LOSS [training: 0.023432228859278655 | validation: 0.013225565044602365]
	TIME [epoch: 8.86 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027478355613363183		[learning rate: 0.0032521]
		[batch 20/20] avg loss: 0.022127075013088443		[learning rate: 0.0032481]
	Learning Rate: 0.00324812
	LOSS [training: 0.024802715313225806 | validation: 0.04202610547301123]
	TIME [epoch: 8.87 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04381679395706671		[learning rate: 0.0032442]
		[batch 20/20] avg loss: 0.02992683569695337		[learning rate: 0.0032403]
	Learning Rate: 0.00324025
	LOSS [training: 0.03687181482701004 | validation: 0.045947351546656204]
	TIME [epoch: 8.89 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04788216015828855		[learning rate: 0.0032363]
		[batch 20/20] avg loss: 0.042978305228040775		[learning rate: 0.0032324]
	Learning Rate: 0.00323241
	LOSS [training: 0.045430232693164666 | validation: 0.02428162853514365]
	TIME [epoch: 8.87 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03413444163638253		[learning rate: 0.0032285]
		[batch 20/20] avg loss: 0.030073976620890515		[learning rate: 0.0032246]
	Learning Rate: 0.00322458
	LOSS [training: 0.03210420912863651 | validation: 0.00844758459945013]
	TIME [epoch: 8.86 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02197717646566168		[learning rate: 0.0032207]
		[batch 20/20] avg loss: 0.031836557696461584		[learning rate: 0.0032168]
	Learning Rate: 0.00321678
	LOSS [training: 0.026906867081061624 | validation: 0.048833232063042825]
	TIME [epoch: 8.84 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044196373675630306		[learning rate: 0.0032129]
		[batch 20/20] avg loss: 0.05054788319862248		[learning rate: 0.003209]
	Learning Rate: 0.00320899
	LOSS [training: 0.0473721284371264 | validation: 0.024621561920808076]
	TIME [epoch: 8.84 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02715187769486845		[learning rate: 0.0032051]
		[batch 20/20] avg loss: 0.023185245063185818		[learning rate: 0.0032012]
	Learning Rate: 0.00320122
	LOSS [training: 0.02516856137902713 | validation: 0.006303204102981631]
	TIME [epoch: 8.87 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024952134206576014		[learning rate: 0.0031973]
		[batch 20/20] avg loss: 0.014488888262636895		[learning rate: 0.0031935]
	Learning Rate: 0.00319347
	LOSS [training: 0.019720511234606452 | validation: 0.028023557963823498]
	TIME [epoch: 8.85 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03697662444809889		[learning rate: 0.0031896]
		[batch 20/20] avg loss: 0.02792628095489407		[learning rate: 0.0031857]
	Learning Rate: 0.00318574
	LOSS [training: 0.03245145270149648 | validation: 0.040581836605560584]
	TIME [epoch: 8.85 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044488953283365065		[learning rate: 0.0031819]
		[batch 20/20] avg loss: 0.05032297545902929		[learning rate: 0.003178]
	Learning Rate: 0.00317803
	LOSS [training: 0.047405964371197186 | validation: 0.06083895420917204]
	TIME [epoch: 8.85 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030695959216600545		[learning rate: 0.0031742]
		[batch 20/20] avg loss: 0.029083411126804848		[learning rate: 0.0031703]
	Learning Rate: 0.00317034
	LOSS [training: 0.029889685171702696 | validation: 0.023901472612784223]
	TIME [epoch: 8.86 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023562334392353296		[learning rate: 0.0031665]
		[batch 20/20] avg loss: 0.051788450132459564		[learning rate: 0.0031627]
	Learning Rate: 0.00316266
	LOSS [training: 0.037675392262406425 | validation: 0.025728417088804273]
	TIME [epoch: 8.87 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059426630238807364		[learning rate: 0.0031588]
		[batch 20/20] avg loss: 0.03448842733272554		[learning rate: 0.003155]
	Learning Rate: 0.003155
	LOSS [training: 0.04695752878576644 | validation: 0.054965260147215075]
	TIME [epoch: 8.87 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01743558166482757		[learning rate: 0.0031512]
		[batch 20/20] avg loss: 0.02129120279557633		[learning rate: 0.0031474]
	Learning Rate: 0.00314737
	LOSS [training: 0.01936339223020195 | validation: 0.02077499946556791]
	TIME [epoch: 8.87 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030948143627770625		[learning rate: 0.0031436]
		[batch 20/20] avg loss: 0.029851432175600955		[learning rate: 0.0031397]
	Learning Rate: 0.00313975
	LOSS [training: 0.030399787901685793 | validation: 0.02382868561227156]
	TIME [epoch: 8.86 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02775612958576784		[learning rate: 0.0031359]
		[batch 20/20] avg loss: 0.04609257743457808		[learning rate: 0.0031321]
	Learning Rate: 0.00313215
	LOSS [training: 0.03692435351017296 | validation: 0.03990185344048356]
	TIME [epoch: 8.87 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02073098876680101		[learning rate: 0.0031284]
		[batch 20/20] avg loss: 0.02143740188476521		[learning rate: 0.0031246]
	Learning Rate: 0.00312456
	LOSS [training: 0.02108419532578311 | validation: 0.032161860511240994]
	TIME [epoch: 8.87 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037866686862760776		[learning rate: 0.0031208]
		[batch 20/20] avg loss: 0.01610236852442327		[learning rate: 0.003117]
	Learning Rate: 0.003117
	LOSS [training: 0.026984527693592025 | validation: 0.03503810018304461]
	TIME [epoch: 8.85 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02183626084362352		[learning rate: 0.0031132]
		[batch 20/20] avg loss: 0.015886363570635373		[learning rate: 0.0031095]
	Learning Rate: 0.00310945
	LOSS [training: 0.018861312207129447 | validation: 0.016745039544773323]
	TIME [epoch: 8.87 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01006823772359674		[learning rate: 0.0031057]
		[batch 20/20] avg loss: 0.01897248882746597		[learning rate: 0.0031019]
	Learning Rate: 0.00310193
	LOSS [training: 0.014520363275531353 | validation: -0.0009376863374907434]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_583.pth
	Model improved!!!
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018570683860822827		[learning rate: 0.0030982]
		[batch 20/20] avg loss: 0.019298427281382494		[learning rate: 0.0030944]
	Learning Rate: 0.00309442
	LOSS [training: 0.018934555571102664 | validation: 0.022583226071732947]
	TIME [epoch: 8.88 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16951895157008162		[learning rate: 0.0030907]
		[batch 20/20] avg loss: 0.09123190242112808		[learning rate: 0.0030869]
	Learning Rate: 0.00308693
	LOSS [training: 0.13037542699560484 | validation: 0.026682776021084115]
	TIME [epoch: 8.88 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038414272885666303		[learning rate: 0.0030832]
		[batch 20/20] avg loss: 0.025363096801541345		[learning rate: 0.0030795]
	Learning Rate: 0.00307945
	LOSS [training: 0.031888684843603826 | validation: 0.022054273215277156]
	TIME [epoch: 8.86 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021007688274739887		[learning rate: 0.0030757]
		[batch 20/20] avg loss: 0.025990834097876205		[learning rate: 0.003072]
	Learning Rate: 0.003072
	LOSS [training: 0.023499261186308046 | validation: 0.0815542251018132]
	TIME [epoch: 8.87 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04679179607603902		[learning rate: 0.0030683]
		[batch 20/20] avg loss: 0.021107577741095555		[learning rate: 0.0030646]
	Learning Rate: 0.00306456
	LOSS [training: 0.03394968690856729 | validation: 0.03798326137904987]
	TIME [epoch: 8.86 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023438196021186616		[learning rate: 0.0030609]
		[batch 20/20] avg loss: 0.0581349517797393		[learning rate: 0.0030571]
	Learning Rate: 0.00305714
	LOSS [training: 0.04078657390046296 | validation: 0.0209325775765367]
	TIME [epoch: 8.88 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017167764459531963		[learning rate: 0.0030534]
		[batch 20/20] avg loss: 0.036589258764629796		[learning rate: 0.0030497]
	Learning Rate: 0.00304974
	LOSS [training: 0.026878511612080887 | validation: 0.028025926268994337]
	TIME [epoch: 8.86 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022869808868513853		[learning rate: 0.003046]
		[batch 20/20] avg loss: 0.02215632630973973		[learning rate: 0.0030424]
	Learning Rate: 0.00304236
	LOSS [training: 0.022513067589126783 | validation: 0.026285252484156054]
	TIME [epoch: 8.86 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03378112303159841		[learning rate: 0.0030387]
		[batch 20/20] avg loss: 0.026546480211890995		[learning rate: 0.003035]
	Learning Rate: 0.00303499
	LOSS [training: 0.03016380162174471 | validation: 0.04565550121543779]
	TIME [epoch: 8.86 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052288620861985415		[learning rate: 0.0030313]
		[batch 20/20] avg loss: 0.0250982705342613		[learning rate: 0.0030276]
	Learning Rate: 0.00302765
	LOSS [training: 0.038693445698123365 | validation: 0.014387708584882308]
	TIME [epoch: 8.86 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03369894381954214		[learning rate: 0.003024]
		[batch 20/20] avg loss: 0.03127665937266914		[learning rate: 0.0030203]
	Learning Rate: 0.00302032
	LOSS [training: 0.032487801596105634 | validation: 0.02872666406337255]
	TIME [epoch: 8.89 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02453980445258388		[learning rate: 0.0030167]
		[batch 20/20] avg loss: 0.07308574621951841		[learning rate: 0.003013]
	Learning Rate: 0.00301301
	LOSS [training: 0.04881277533605115 | validation: 0.03319748300411449]
	TIME [epoch: 8.86 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04374129424029409		[learning rate: 0.0030094]
		[batch 20/20] avg loss: 0.07497683718338163		[learning rate: 0.0030057]
	Learning Rate: 0.00300571
	LOSS [training: 0.05935906571183787 | validation: 0.04159462941200767]
	TIME [epoch: 8.86 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07098295436247778		[learning rate: 0.0030021]
		[batch 20/20] avg loss: 0.14243861643685557		[learning rate: 0.0029984]
	Learning Rate: 0.00299844
	LOSS [training: 0.10671078539966668 | validation: 0.08253578618245255]
	TIME [epoch: 8.86 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08671120736165264		[learning rate: 0.0029948]
		[batch 20/20] avg loss: 0.07204872014556077		[learning rate: 0.0029912]
	Learning Rate: 0.00299118
	LOSS [training: 0.0793799637536067 | validation: 0.07858803784812038]
	TIME [epoch: 8.88 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058601681358344884		[learning rate: 0.0029876]
		[batch 20/20] avg loss: 0.021180766067411642		[learning rate: 0.0029839]
	Learning Rate: 0.00298394
	LOSS [training: 0.039891223712878274 | validation: 0.04678757366464749]
	TIME [epoch: 8.88 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03848797718512826		[learning rate: 0.0029803]
		[batch 20/20] avg loss: 0.02827812501994949		[learning rate: 0.0029767]
	Learning Rate: 0.00297671
	LOSS [training: 0.03338305110253887 | validation: 0.01922769118044745]
	TIME [epoch: 8.87 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039501387634040985		[learning rate: 0.0029731]
		[batch 20/20] avg loss: 0.028028522980588323		[learning rate: 0.0029695]
	Learning Rate: 0.00296951
	LOSS [training: 0.03376495530731465 | validation: 0.013697804839247334]
	TIME [epoch: 8.86 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029314278658961062		[learning rate: 0.0029659]
		[batch 20/20] avg loss: 0.019854233395727963		[learning rate: 0.0029623]
	Learning Rate: 0.00296232
	LOSS [training: 0.02458425602734451 | validation: 0.014965526427850224]
	TIME [epoch: 8.87 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023233886400699344		[learning rate: 0.0029587]
		[batch 20/20] avg loss: 0.024788470388915376		[learning rate: 0.0029551]
	Learning Rate: 0.00295515
	LOSS [training: 0.02401117839480736 | validation: 0.015885419463249326]
	TIME [epoch: 8.9 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017150938158585993		[learning rate: 0.0029516]
		[batch 20/20] avg loss: 0.0363819882536117		[learning rate: 0.002948]
	Learning Rate: 0.00294799
	LOSS [training: 0.026766463206098844 | validation: 0.014369663558499486]
	TIME [epoch: 8.87 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019458691015033062		[learning rate: 0.0029444]
		[batch 20/20] avg loss: 0.035907674566475214		[learning rate: 0.0029409]
	Learning Rate: 0.00294086
	LOSS [training: 0.027683182790754136 | validation: 0.041064796519443475]
	TIME [epoch: 8.87 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027169937209722477		[learning rate: 0.0029373]
		[batch 20/20] avg loss: 0.03625709509836646		[learning rate: 0.0029337]
	Learning Rate: 0.00293374
	LOSS [training: 0.03171351615404446 | validation: 0.026218256146390593]
	TIME [epoch: 8.86 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03486042945060393		[learning rate: 0.0029302]
		[batch 20/20] avg loss: 0.03239925969436562		[learning rate: 0.0029266]
	Learning Rate: 0.00292663
	LOSS [training: 0.03362984457248478 | validation: 0.08308493666708905]
	TIME [epoch: 8.86 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0678147426957453		[learning rate: 0.0029231]
		[batch 20/20] avg loss: 0.05237301888321485		[learning rate: 0.0029195]
	Learning Rate: 0.00291955
	LOSS [training: 0.06009388078948007 | validation: 0.024509023446051548]
	TIME [epoch: 8.9 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03435349546485894		[learning rate: 0.002916]
		[batch 20/20] avg loss: 0.049379413130727204		[learning rate: 0.0029125]
	Learning Rate: 0.00291248
	LOSS [training: 0.04186645429779307 | validation: 0.006259408433638785]
	TIME [epoch: 8.87 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038583325904491844		[learning rate: 0.002909]
		[batch 20/20] avg loss: 0.028126031317040102		[learning rate: 0.0029054]
	Learning Rate: 0.00290543
	LOSS [training: 0.03335467861076598 | validation: 0.02821891934409331]
	TIME [epoch: 8.86 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03532220989433158		[learning rate: 0.0029019]
		[batch 20/20] avg loss: 0.023849296495275382		[learning rate: 0.0028984]
	Learning Rate: 0.0028984
	LOSS [training: 0.02958575319480349 | validation: 0.011413219592104602]
	TIME [epoch: 8.86 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029000892938650834		[learning rate: 0.0028949]
		[batch 20/20] avg loss: 0.03161899932966705		[learning rate: 0.0028914]
	Learning Rate: 0.00289138
	LOSS [training: 0.030309946134158938 | validation: 0.050548378817536155]
	TIME [epoch: 8.89 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02856961707973597		[learning rate: 0.0028879]
		[batch 20/20] avg loss: 0.024966927810178657		[learning rate: 0.0028844]
	Learning Rate: 0.00288438
	LOSS [training: 0.02676827244495732 | validation: 0.012133025964795447]
	TIME [epoch: 8.87 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0280423984690207		[learning rate: 0.0028809]
		[batch 20/20] avg loss: 0.03194394520461892		[learning rate: 0.0028774]
	Learning Rate: 0.0028774
	LOSS [training: 0.02999317183681981 | validation: 0.025990900033173798]
	TIME [epoch: 8.86 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031365814825736714		[learning rate: 0.0028739]
		[batch 20/20] avg loss: 0.03834262718146556		[learning rate: 0.0028704]
	Learning Rate: 0.00287043
	LOSS [training: 0.034854221003601145 | validation: 0.006425452683474628]
	TIME [epoch: 8.87 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0235470870359981		[learning rate: 0.002867]
		[batch 20/20] avg loss: 0.019159764843564242		[learning rate: 0.0028635]
	Learning Rate: 0.00286348
	LOSS [training: 0.021353425939781175 | validation: 0.029342649032997825]
	TIME [epoch: 8.87 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025664903897492646		[learning rate: 0.00286]
		[batch 20/20] avg loss: 0.020397808092606533		[learning rate: 0.0028566]
	Learning Rate: 0.00285655
	LOSS [training: 0.023031355995049595 | validation: 0.046185193544294394]
	TIME [epoch: 8.9 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03937356141768076		[learning rate: 0.0028531]
		[batch 20/20] avg loss: 0.026275841658207233		[learning rate: 0.0028496]
	Learning Rate: 0.00284964
	LOSS [training: 0.032824701537944 | validation: 0.0231175918894983]
	TIME [epoch: 8.88 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021312665561064627		[learning rate: 0.0028462]
		[batch 20/20] avg loss: 0.020230106751922328		[learning rate: 0.0028427]
	Learning Rate: 0.00284274
	LOSS [training: 0.020771386156493477 | validation: 0.03131108973794387]
	TIME [epoch: 8.87 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02484700530832646		[learning rate: 0.0028393]
		[batch 20/20] avg loss: 0.016921690175460368		[learning rate: 0.0028359]
	Learning Rate: 0.00283586
	LOSS [training: 0.020884347741893412 | validation: 0.003685420798380852]
	TIME [epoch: 8.86 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009965892186726355		[learning rate: 0.0028324]
		[batch 20/20] avg loss: 0.024546411168570006		[learning rate: 0.002829]
	Learning Rate: 0.00282899
	LOSS [training: 0.017256151677648185 | validation: 0.01273794835494053]
	TIME [epoch: 8.87 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007583899757316473		[learning rate: 0.0028256]
		[batch 20/20] avg loss: 0.025071690899623817		[learning rate: 0.0028221]
	Learning Rate: 0.00282214
	LOSS [training: 0.016327795328470143 | validation: 0.019177839799567206]
	TIME [epoch: 8.89 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02028036860949552		[learning rate: 0.0028187]
		[batch 20/20] avg loss: 0.02593457621088674		[learning rate: 0.0028153]
	Learning Rate: 0.00281531
	LOSS [training: 0.02310747241019113 | validation: 0.03795279685936788]
	TIME [epoch: 8.87 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022367287691226322		[learning rate: 0.0028119]
		[batch 20/20] avg loss: 0.02671415946572251		[learning rate: 0.0028085]
	Learning Rate: 0.00280849
	LOSS [training: 0.024540723578474415 | validation: 0.024492124205873458]
	TIME [epoch: 8.88 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03176443911796225		[learning rate: 0.0028051]
		[batch 20/20] avg loss: 0.03690154424841467		[learning rate: 0.0028017]
	Learning Rate: 0.0028017
	LOSS [training: 0.034332991683188464 | validation: 0.01940399489154357]
	TIME [epoch: 8.87 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01770029458422012		[learning rate: 0.0027983]
		[batch 20/20] avg loss: 0.0228011856058695		[learning rate: 0.0027949]
	Learning Rate: 0.00279491
	LOSS [training: 0.02025074009504481 | validation: 0.003434752177815526]
	TIME [epoch: 8.87 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021437076123052134		[learning rate: 0.0027915]
		[batch 20/20] avg loss: 0.012353408848386933		[learning rate: 0.0027881]
	Learning Rate: 0.00278815
	LOSS [training: 0.016895242485719532 | validation: 0.007934422840876991]
	TIME [epoch: 8.88 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022269175517755313		[learning rate: 0.0027848]
		[batch 20/20] avg loss: 0.03130579358096539		[learning rate: 0.0027814]
	Learning Rate: 0.0027814
	LOSS [training: 0.026787484549360353 | validation: 0.022478233303440304]
	TIME [epoch: 8.86 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028442633429802865		[learning rate: 0.002778]
		[batch 20/20] avg loss: 0.018111552493099616		[learning rate: 0.0027747]
	Learning Rate: 0.00277466
	LOSS [training: 0.023277092961451244 | validation: 0.03605394432329164]
	TIME [epoch: 8.87 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07695250145087346		[learning rate: 0.0027713]
		[batch 20/20] avg loss: 0.013759473234092882		[learning rate: 0.0027679]
	Learning Rate: 0.00276795
	LOSS [training: 0.04535598734248317 | validation: 0.025330882971319135]
	TIME [epoch: 8.86 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027643091542126026		[learning rate: 0.0027646]
		[batch 20/20] avg loss: 0.029614172426229178		[learning rate: 0.0027612]
	Learning Rate: 0.00276125
	LOSS [training: 0.028628631984177606 | validation: 0.009818355250573218]
	TIME [epoch: 8.89 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024872116055239053		[learning rate: 0.0027579]
		[batch 20/20] avg loss: 0.021719133545904764		[learning rate: 0.0027546]
	Learning Rate: 0.00275456
	LOSS [training: 0.023295624800571908 | validation: -0.001055463986255455]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_632.pth
	Model improved!!!
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02812033025634263		[learning rate: 0.0027512]
		[batch 20/20] avg loss: 0.027875177135785984		[learning rate: 0.0027479]
	Learning Rate: 0.00274789
	LOSS [training: 0.02799775369606431 | validation: 0.01647693824458414]
	TIME [epoch: 8.86 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019740427330327513		[learning rate: 0.0027446]
		[batch 20/20] avg loss: 0.02311365785809837		[learning rate: 0.0027412]
	Learning Rate: 0.00274124
	LOSS [training: 0.021427042594212943 | validation: 0.008239464796031354]
	TIME [epoch: 8.86 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021597148271725666		[learning rate: 0.0027379]
		[batch 20/20] avg loss: 0.016481592415857987		[learning rate: 0.0027346]
	Learning Rate: 0.00273461
	LOSS [training: 0.019039370343791828 | validation: 0.015452033391388084]
	TIME [epoch: 8.86 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014550178853468727		[learning rate: 0.0027313]
		[batch 20/20] avg loss: 0.011951025061476975		[learning rate: 0.002728]
	Learning Rate: 0.00272799
	LOSS [training: 0.013250601957472848 | validation: -0.0006114598572915838]
	TIME [epoch: 8.88 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012244365991558499		[learning rate: 0.0027247]
		[batch 20/20] avg loss: 0.02189213987509532		[learning rate: 0.0027214]
	Learning Rate: 0.00272138
	LOSS [training: 0.01706825293332691 | validation: 0.0035059692199716566]
	TIME [epoch: 8.86 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029043563138704365		[learning rate: 0.0027181]
		[batch 20/20] avg loss: 0.012965572654981994		[learning rate: 0.0027148]
	Learning Rate: 0.00271479
	LOSS [training: 0.021004567896843178 | validation: 0.044231173475953164]
	TIME [epoch: 8.86 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023394668005765628		[learning rate: 0.0027115]
		[batch 20/20] avg loss: 0.04384109025140009		[learning rate: 0.0027082]
	Learning Rate: 0.00270822
	LOSS [training: 0.03361787912858285 | validation: 0.011826284453315326]
	TIME [epoch: 8.87 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016251882855493206		[learning rate: 0.0027049]
		[batch 20/20] avg loss: 0.022369977789495756		[learning rate: 0.0027017]
	Learning Rate: 0.00270167
	LOSS [training: 0.019310930322494483 | validation: 0.029279069333412126]
	TIME [epoch: 8.86 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027210230037899203		[learning rate: 0.0026984]
		[batch 20/20] avg loss: 0.0290475390379968		[learning rate: 0.0026951]
	Learning Rate: 0.00269513
	LOSS [training: 0.028128884537948005 | validation: 0.03178962632494804]
	TIME [epoch: 8.89 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027529004003964446		[learning rate: 0.0026919]
		[batch 20/20] avg loss: 0.029363580793477007		[learning rate: 0.0026886]
	Learning Rate: 0.0026886
	LOSS [training: 0.028446292398720735 | validation: 0.029235009055407428]
	TIME [epoch: 8.86 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019273702421269853		[learning rate: 0.0026853]
		[batch 20/20] avg loss: 0.012226824017688025		[learning rate: 0.0026821]
	Learning Rate: 0.00268209
	LOSS [training: 0.01575026321947894 | validation: 0.01632232388721684]
	TIME [epoch: 8.86 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02039288352489393		[learning rate: 0.0026788]
		[batch 20/20] avg loss: 0.018727471591023927		[learning rate: 0.0026756]
	Learning Rate: 0.0026756
	LOSS [training: 0.01956017755795893 | validation: 0.0037412198151459556]
	TIME [epoch: 8.86 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013924201024034888		[learning rate: 0.0026724]
		[batch 20/20] avg loss: 0.01622466040740991		[learning rate: 0.0026691]
	Learning Rate: 0.00266912
	LOSS [training: 0.015074430715722401 | validation: 0.04072064605183629]
	TIME [epoch: 8.87 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02274472760646538		[learning rate: 0.0026659]
		[batch 20/20] avg loss: 0.011525460798144264		[learning rate: 0.0026627]
	Learning Rate: 0.00266266
	LOSS [training: 0.017135094202304822 | validation: 0.0005938022934959945]
	TIME [epoch: 8.87 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022742642594671104		[learning rate: 0.0026594]
		[batch 20/20] avg loss: 0.024602762310631788		[learning rate: 0.0026562]
	Learning Rate: 0.00265621
	LOSS [training: 0.02367270245265145 | validation: 0.005176722132205051]
	TIME [epoch: 8.85 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019695600932338967		[learning rate: 0.002653]
		[batch 20/20] avg loss: 0.015578718563705971		[learning rate: 0.0026498]
	Learning Rate: 0.00264978
	LOSS [training: 0.017637159748022466 | validation: -0.0023907291744672526]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020516344926470573		[learning rate: 0.0026466]
		[batch 20/20] avg loss: 0.03388779632110915		[learning rate: 0.0026434]
	Learning Rate: 0.00264337
	LOSS [training: 0.02720207062378986 | validation: 0.07614698736593793]
	TIME [epoch: 8.87 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03370542423362028		[learning rate: 0.0026402]
		[batch 20/20] avg loss: 0.01736973224249482		[learning rate: 0.002637]
	Learning Rate: 0.00263697
	LOSS [training: 0.02553757823805755 | validation: 0.027008350566358114]
	TIME [epoch: 8.88 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02278696769684486		[learning rate: 0.0026338]
		[batch 20/20] avg loss: 0.026198305487999868		[learning rate: 0.0026306]
	Learning Rate: 0.00263059
	LOSS [training: 0.024492636592422366 | validation: 0.02181664479357656]
	TIME [epoch: 8.85 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07237871987218808		[learning rate: 0.0026274]
		[batch 20/20] avg loss: 0.04387383763012772		[learning rate: 0.0026242]
	Learning Rate: 0.00262422
	LOSS [training: 0.05812627875115789 | validation: 0.014551755870057683]
	TIME [epoch: 8.86 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03766829444161452		[learning rate: 0.002621]
		[batch 20/20] avg loss: 0.01714546272203869		[learning rate: 0.0026179]
	Learning Rate: 0.00261787
	LOSS [training: 0.02740687858182661 | validation: -0.0015109439261075416]
	TIME [epoch: 8.86 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01192663151511975		[learning rate: 0.0026147]
		[batch 20/20] avg loss: 0.022684224947062688		[learning rate: 0.0026115]
	Learning Rate: 0.00261153
	LOSS [training: 0.01730542823109122 | validation: 0.0005132200888560493]
	TIME [epoch: 8.85 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016014077335789297		[learning rate: 0.0026084]
		[batch 20/20] avg loss: 0.008205618736708012		[learning rate: 0.0026052]
	Learning Rate: 0.00260521
	LOSS [training: 0.012109848036248655 | validation: 0.012893448119782568]
	TIME [epoch: 8.88 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030135545041640587		[learning rate: 0.0026021]
		[batch 20/20] avg loss: 0.013422893084084453		[learning rate: 0.0025989]
	Learning Rate: 0.0025989
	LOSS [training: 0.02177921906286252 | validation: 0.013070810182195889]
	TIME [epoch: 8.87 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01297809163860222		[learning rate: 0.0025958]
		[batch 20/20] avg loss: 0.014921796891977557		[learning rate: 0.0025926]
	Learning Rate: 0.00259261
	LOSS [training: 0.013949944265289888 | validation: 0.010426219689014558]
	TIME [epoch: 8.87 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02568185136144876		[learning rate: 0.0025895]
		[batch 20/20] avg loss: 0.0299168422043437		[learning rate: 0.0025863]
	Learning Rate: 0.00258633
	LOSS [training: 0.027799346782896233 | validation: 0.04567272331384779]
	TIME [epoch: 8.86 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013691759262095596		[learning rate: 0.0025832]
		[batch 20/20] avg loss: 0.013997962308159004		[learning rate: 0.0025801]
	Learning Rate: 0.00258007
	LOSS [training: 0.0138448607851273 | validation: 0.03713357450520853]
	TIME [epoch: 8.9 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01567623071100561		[learning rate: 0.0025769]
		[batch 20/20] avg loss: 0.02376457062418456		[learning rate: 0.0025738]
	Learning Rate: 0.00257382
	LOSS [training: 0.019720400667595082 | validation: 0.014988745957879914]
	TIME [epoch: 8.88 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0229959303228043		[learning rate: 0.0025707]
		[batch 20/20] avg loss: 0.013464902402331039		[learning rate: 0.0025676]
	Learning Rate: 0.00256759
	LOSS [training: 0.018230416362567666 | validation: 0.025825962959573193]
	TIME [epoch: 8.88 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017209953421293785		[learning rate: 0.0025645]
		[batch 20/20] avg loss: 0.014185369613506976		[learning rate: 0.0025614]
	Learning Rate: 0.00256138
	LOSS [training: 0.01569766151740038 | validation: 0.024398409217794088]
	TIME [epoch: 8.86 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010281035744115758		[learning rate: 0.0025583]
		[batch 20/20] avg loss: 0.0193993379744169		[learning rate: 0.0025552]
	Learning Rate: 0.00255518
	LOSS [training: 0.01484018685926633 | validation: 0.005559297149799908]
	TIME [epoch: 8.86 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01560377890037517		[learning rate: 0.0025521]
		[batch 20/20] avg loss: 0.0082586682348875		[learning rate: 0.002549]
	Learning Rate: 0.00254899
	LOSS [training: 0.011931223567631337 | validation: -0.003755776697885414]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_664.pth
	Model improved!!!
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022330722798591728		[learning rate: 0.0025459]
		[batch 20/20] avg loss: 0.01978177346144876		[learning rate: 0.0025428]
	Learning Rate: 0.00254282
	LOSS [training: 0.021056248130020244 | validation: 0.02080917165789555]
	TIME [epoch: 8.86 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016669772543889107		[learning rate: 0.0025397]
		[batch 20/20] avg loss: 0.011893749968664522		[learning rate: 0.0025367]
	Learning Rate: 0.00253667
	LOSS [training: 0.014281761256276814 | validation: 0.0021217992745114767]
	TIME [epoch: 8.87 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013847482881937217		[learning rate: 0.0025336]
		[batch 20/20] avg loss: 0.0313234867142442		[learning rate: 0.0025305]
	Learning Rate: 0.00253052
	LOSS [training: 0.02258548479809071 | validation: 0.03906560433048635]
	TIME [epoch: 8.86 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04768197639558085		[learning rate: 0.0025275]
		[batch 20/20] avg loss: 0.010866279791757279		[learning rate: 0.0025244]
	Learning Rate: 0.0025244
	LOSS [training: 0.029274128093669066 | validation: 0.023084355631995342]
	TIME [epoch: 8.86 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019007809975147755		[learning rate: 0.0025213]
		[batch 20/20] avg loss: 0.020914373217184395		[learning rate: 0.0025183]
	Learning Rate: 0.00251829
	LOSS [training: 0.01996109159616607 | validation: 0.02776938128565872]
	TIME [epoch: 8.88 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029137604469612464		[learning rate: 0.0025152]
		[batch 20/20] avg loss: 0.0262106868802394		[learning rate: 0.0025122]
	Learning Rate: 0.00251219
	LOSS [training: 0.027674145674925932 | validation: 0.015952064061784316]
	TIME [epoch: 8.88 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04132368220374276		[learning rate: 0.0025091]
		[batch 20/20] avg loss: 0.06324936832036154		[learning rate: 0.0025061]
	Learning Rate: 0.00250611
	LOSS [training: 0.052286525262052144 | validation: 0.02079273127541286]
	TIME [epoch: 8.86 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028404805877920918		[learning rate: 0.0025031]
		[batch 20/20] avg loss: 0.04663041468774198		[learning rate: 0.0025]
	Learning Rate: 0.00250004
	LOSS [training: 0.037517610282831446 | validation: 0.02825745502779707]
	TIME [epoch: 8.86 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039783926610020955		[learning rate: 0.002497]
		[batch 20/20] avg loss: 0.02878695969026575		[learning rate: 0.002494]
	Learning Rate: 0.00249399
	LOSS [training: 0.034285443150143355 | validation: 0.015052897751477945]
	TIME [epoch: 8.87 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019580989123190364		[learning rate: 0.002491]
		[batch 20/20] avg loss: 0.024380804178498985		[learning rate: 0.002488]
	Learning Rate: 0.00248795
	LOSS [training: 0.021980896650844677 | validation: 0.014078069263797332]
	TIME [epoch: 8.87 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020414963831184053		[learning rate: 0.0024849]
		[batch 20/20] avg loss: 0.019670081860370798		[learning rate: 0.0024819]
	Learning Rate: 0.00248193
	LOSS [training: 0.02004252284577743 | validation: 0.01621933974866432]
	TIME [epoch: 8.86 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022066146473961514		[learning rate: 0.0024789]
		[batch 20/20] avg loss: 0.012047767680367458		[learning rate: 0.0024759]
	Learning Rate: 0.00247592
	LOSS [training: 0.017056957077164488 | validation: 0.03156126144004368]
	TIME [epoch: 8.85 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023675611624466013		[learning rate: 0.0024729]
		[batch 20/20] avg loss: 0.026676767657718446		[learning rate: 0.0024699]
	Learning Rate: 0.00246993
	LOSS [training: 0.02517618964109223 | validation: 0.03387195430520094]
	TIME [epoch: 8.85 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020051538238264187		[learning rate: 0.0024669]
		[batch 20/20] avg loss: 0.02270508352019125		[learning rate: 0.0024639]
	Learning Rate: 0.00246395
	LOSS [training: 0.021378310879227723 | validation: 0.03545820774575778]
	TIME [epoch: 8.88 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02638401268258141		[learning rate: 0.002461]
		[batch 20/20] avg loss: 0.02069467523393561		[learning rate: 0.002458]
	Learning Rate: 0.00245798
	LOSS [training: 0.023539343958258508 | validation: 0.006524682792036159]
	TIME [epoch: 8.85 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02693973759346798		[learning rate: 0.002455]
		[batch 20/20] avg loss: 0.02242463855531986		[learning rate: 0.002452]
	Learning Rate: 0.00245203
	LOSS [training: 0.02468218807439392 | validation: 0.00739887540286548]
	TIME [epoch: 8.86 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0205411724589773		[learning rate: 0.0024491]
		[batch 20/20] avg loss: 0.0033858279952416865		[learning rate: 0.0024461]
	Learning Rate: 0.0024461
	LOSS [training: 0.011963500227109495 | validation: 0.007199847757581369]
	TIME [epoch: 8.85 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022290254013559004		[learning rate: 0.0024431]
		[batch 20/20] avg loss: 0.03172139990125937		[learning rate: 0.0024402]
	Learning Rate: 0.00244018
	LOSS [training: 0.02700582695740919 | validation: 0.014158527815172784]
	TIME [epoch: 8.86 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03117261035894773		[learning rate: 0.0024372]
		[batch 20/20] avg loss: 0.041552029111782865		[learning rate: 0.0024343]
	Learning Rate: 0.00243427
	LOSS [training: 0.036362319735365294 | validation: 0.03225279340683963]
	TIME [epoch: 8.88 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031672155480204446		[learning rate: 0.0024313]
		[batch 20/20] avg loss: 0.011119383690680669		[learning rate: 0.0024284]
	Learning Rate: 0.00242837
	LOSS [training: 0.021395769585442558 | validation: 0.01292301002403325]
	TIME [epoch: 8.86 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007807620505835974		[learning rate: 0.0024254]
		[batch 20/20] avg loss: 0.008916496915931247		[learning rate: 0.0024225]
	Learning Rate: 0.0024225
	LOSS [training: 0.008362058710883611 | validation: 0.006248129517318315]
	TIME [epoch: 8.86 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01573870032035799		[learning rate: 0.0024196]
		[batch 20/20] avg loss: 0.012206197102336499		[learning rate: 0.0024166]
	Learning Rate: 0.00241663
	LOSS [training: 0.01397244871134724 | validation: 0.0012348155757811233]
	TIME [epoch: 8.86 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02700336394892742		[learning rate: 0.0024137]
		[batch 20/20] avg loss: 0.014987942549891813		[learning rate: 0.0024108]
	Learning Rate: 0.00241078
	LOSS [training: 0.02099565324940962 | validation: 0.005603016756658624]
	TIME [epoch: 8.86 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020486206841976193		[learning rate: 0.0024079]
		[batch 20/20] avg loss: 0.034749746354998355		[learning rate: 0.0024049]
	Learning Rate: 0.00240495
	LOSS [training: 0.027617976598487276 | validation: 0.03530673678706296]
	TIME [epoch: 8.88 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03330778178794454		[learning rate: 0.002402]
		[batch 20/20] avg loss: 0.04329755711032254		[learning rate: 0.0023991]
	Learning Rate: 0.00239912
	LOSS [training: 0.03830266944913354 | validation: 0.04008226721634186]
	TIME [epoch: 8.86 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06230260685739203		[learning rate: 0.0023962]
		[batch 20/20] avg loss: 0.12151072047669802		[learning rate: 0.0023933]
	Learning Rate: 0.00239332
	LOSS [training: 0.09190666366704502 | validation: 0.10799392340606115]
	TIME [epoch: 8.86 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06908667982851088		[learning rate: 0.0023904]
		[batch 20/20] avg loss: 0.03473166764022319		[learning rate: 0.0023875]
	Learning Rate: 0.00238752
	LOSS [training: 0.05190917373436703 | validation: 0.03196397798278178]
	TIME [epoch: 8.85 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04019124496427113		[learning rate: 0.0023846]
		[batch 20/20] avg loss: 0.032828173778082045		[learning rate: 0.0023817]
	Learning Rate: 0.00238174
	LOSS [training: 0.03650970937117659 | validation: 0.03200372185061298]
	TIME [epoch: 8.88 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02841048223395794		[learning rate: 0.0023789]
		[batch 20/20] avg loss: 0.01766205194232049		[learning rate: 0.002376]
	Learning Rate: 0.00237598
	LOSS [training: 0.02303626708813922 | validation: 0.008548534326042207]
	TIME [epoch: 8.86 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027794533065091		[learning rate: 0.0023731]
		[batch 20/20] avg loss: 0.04085138918920898		[learning rate: 0.0023702]
	Learning Rate: 0.00237022
	LOSS [training: 0.034322961127149984 | validation: 0.022281428575810065]
	TIME [epoch: 8.86 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03085735712860624		[learning rate: 0.0023674]
		[batch 20/20] avg loss: 0.021042420084277307		[learning rate: 0.0023645]
	Learning Rate: 0.00236449
	LOSS [training: 0.025949888606441773 | validation: 0.008124568439357077]
	TIME [epoch: 8.85 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023378639948647466		[learning rate: 0.0023616]
		[batch 20/20] avg loss: 0.02050005608639294		[learning rate: 0.0023588]
	Learning Rate: 0.00235876
	LOSS [training: 0.021939348017520205 | validation: 0.006249054782889245]
	TIME [epoch: 8.86 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021457875977895065		[learning rate: 0.0023559]
		[batch 20/20] avg loss: 0.049488671116294		[learning rate: 0.0023531]
	Learning Rate: 0.00235305
	LOSS [training: 0.035473273547094536 | validation: 0.02942830724687302]
	TIME [epoch: 8.88 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01271488984118764		[learning rate: 0.0023502]
		[batch 20/20] avg loss: 0.015103222055684661		[learning rate: 0.0023474]
	Learning Rate: 0.00234736
	LOSS [training: 0.013909055948436153 | validation: 0.009584998454939942]
	TIME [epoch: 8.86 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012066146933638312		[learning rate: 0.0023445]
		[batch 20/20] avg loss: 0.01593808038450239		[learning rate: 0.0023417]
	Learning Rate: 0.00234167
	LOSS [training: 0.014002113659070347 | validation: 0.011039619501216652]
	TIME [epoch: 8.85 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01317442104186232		[learning rate: 0.0023388]
		[batch 20/20] avg loss: 0.020022664091825883		[learning rate: 0.002336]
	Learning Rate: 0.002336
	LOSS [training: 0.0165985425668441 | validation: 0.015597896948716813]
	TIME [epoch: 8.86 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02570772598976818		[learning rate: 0.0023332]
		[batch 20/20] avg loss: 0.01711745972655594		[learning rate: 0.0023303]
	Learning Rate: 0.00233035
	LOSS [training: 0.02141259285816206 | validation: 0.0030805526141305445]
	TIME [epoch: 8.86 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005812399329332228		[learning rate: 0.0023275]
		[batch 20/20] avg loss: 0.024627378258093035		[learning rate: 0.0023247]
	Learning Rate: 0.00232471
	LOSS [training: 0.01521988879371263 | validation: -0.0010087609287501506]
	TIME [epoch: 8.88 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017806746370006985		[learning rate: 0.0023219]
		[batch 20/20] avg loss: 0.023199444712393016		[learning rate: 0.0023191]
	Learning Rate: 0.00231908
	LOSS [training: 0.020503095541199997 | validation: 0.04678195383465107]
	TIME [epoch: 8.86 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0188812516137959		[learning rate: 0.0023163]
		[batch 20/20] avg loss: 0.012978280011746769		[learning rate: 0.0023135]
	Learning Rate: 0.00231347
	LOSS [training: 0.01592976581277133 | validation: 0.000848363544539792]
	TIME [epoch: 8.86 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012711699324853892		[learning rate: 0.0023107]
		[batch 20/20] avg loss: 0.04368808244795788		[learning rate: 0.0023079]
	Learning Rate: 0.00230787
	LOSS [training: 0.028199890886405888 | validation: 0.023555771023812566]
	TIME [epoch: 8.86 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03499533910808344		[learning rate: 0.0023051]
		[batch 20/20] avg loss: 0.06214526093586813		[learning rate: 0.0023023]
	Learning Rate: 0.00230228
	LOSS [training: 0.048570300021975786 | validation: 0.035722286919200465]
	TIME [epoch: 8.88 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029792796646626596		[learning rate: 0.0022995]
		[batch 20/20] avg loss: 0.01086507614964238		[learning rate: 0.0022967]
	Learning Rate: 0.00229671
	LOSS [training: 0.020328936398134483 | validation: 0.012733716345490368]
	TIME [epoch: 8.86 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026035425426780147		[learning rate: 0.0022939]
		[batch 20/20] avg loss: 0.02022604964318091		[learning rate: 0.0022911]
	Learning Rate: 0.00229115
	LOSS [training: 0.023130737534980524 | validation: 0.02294364872895513]
	TIME [epoch: 8.86 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02201990573260504		[learning rate: 0.0022884]
		[batch 20/20] avg loss: 0.017754225320720237		[learning rate: 0.0022856]
	Learning Rate: 0.0022856
	LOSS [training: 0.019887065526662636 | validation: 0.01940601731091591]
	TIME [epoch: 8.86 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021112960326496767		[learning rate: 0.0022828]
		[batch 20/20] avg loss: 0.022456455138376706		[learning rate: 0.0022801]
	Learning Rate: 0.00228007
	LOSS [training: 0.021784707732436744 | validation: 0.008134825664957327]
	TIME [epoch: 8.85 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012385874087661937		[learning rate: 0.0022773]
		[batch 20/20] avg loss: 0.016119943707434824		[learning rate: 0.0022745]
	Learning Rate: 0.00227455
	LOSS [training: 0.014252908897548378 | validation: 0.01009897903436606]
	TIME [epoch: 8.88 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01703009073175844		[learning rate: 0.0022718]
		[batch 20/20] avg loss: 0.0105680153519351		[learning rate: 0.002269]
	Learning Rate: 0.00226904
	LOSS [training: 0.013799053041846771 | validation: 0.0012617428304311279]
	TIME [epoch: 8.86 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017681441622669408		[learning rate: 0.0022663]
		[batch 20/20] avg loss: 0.05558198696097481		[learning rate: 0.0022635]
	Learning Rate: 0.00226355
	LOSS [training: 0.036631714291822104 | validation: 0.0752480842011122]
	TIME [epoch: 8.86 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03197395224826817		[learning rate: 0.0022608]
		[batch 20/20] avg loss: 0.016226859649228417		[learning rate: 0.0022581]
	Learning Rate: 0.00225807
	LOSS [training: 0.02410040594874829 | validation: 0.00673549562702962]
	TIME [epoch: 8.86 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00973362848292381		[learning rate: 0.0022553]
		[batch 20/20] avg loss: 0.0055252862788888355		[learning rate: 0.0022526]
	Learning Rate: 0.0022526
	LOSS [training: 0.007629457380906322 | validation: -0.005561861966790202]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_715.pth
	Model improved!!!
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008801021119661695		[learning rate: 0.0022499]
		[batch 20/20] avg loss: 0.012855293432627321		[learning rate: 0.0022471]
	Learning Rate: 0.00224715
	LOSS [training: 0.010828157276144508 | validation: 0.0045212432775876025]
	TIME [epoch: 8.89 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019151674230773574		[learning rate: 0.0022444]
		[batch 20/20] avg loss: 0.014335249677779466		[learning rate: 0.0022417]
	Learning Rate: 0.00224171
	LOSS [training: 0.01674346195427652 | validation: 0.027507330126222817]
	TIME [epoch: 8.87 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00666288508084618		[learning rate: 0.002239]
		[batch 20/20] avg loss: 0.010840259866243167		[learning rate: 0.0022363]
	Learning Rate: 0.00223628
	LOSS [training: 0.008751572473544673 | validation: 0.009540980594389275]
	TIME [epoch: 8.87 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02741543644103936		[learning rate: 0.0022336]
		[batch 20/20] avg loss: 0.016231207741230873		[learning rate: 0.0022309]
	Learning Rate: 0.00223087
	LOSS [training: 0.021823322091135113 | validation: 0.023374859051733803]
	TIME [epoch: 8.87 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012475141657944671		[learning rate: 0.0022282]
		[batch 20/20] avg loss: 0.01662106618778104		[learning rate: 0.0022255]
	Learning Rate: 0.00222547
	LOSS [training: 0.014548103922862857 | validation: 0.018979802017897993]
	TIME [epoch: 8.89 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01630917770138567		[learning rate: 0.0022228]
		[batch 20/20] avg loss: 0.026857303887741203		[learning rate: 0.0022201]
	Learning Rate: 0.00222008
	LOSS [training: 0.021583240794563436 | validation: 0.007897278934853988]
	TIME [epoch: 8.88 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01987387109481431		[learning rate: 0.0022174]
		[batch 20/20] avg loss: 0.014577254797077032		[learning rate: 0.0022147]
	Learning Rate: 0.0022147
	LOSS [training: 0.017225562945945672 | validation: 0.038550714984008055]
	TIME [epoch: 8.87 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018810492946812356		[learning rate: 0.002212]
		[batch 20/20] avg loss: 0.008008839790581595		[learning rate: 0.0022093]
	Learning Rate: 0.00220934
	LOSS [training: 0.01340966636869698 | validation: 0.01660284195894133]
	TIME [epoch: 8.86 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009854450579732617		[learning rate: 0.0022067]
		[batch 20/20] avg loss: 0.006272291130640347		[learning rate: 0.002204]
	Learning Rate: 0.00220399
	LOSS [training: 0.008063370855186481 | validation: -0.0034184116156186083]
	TIME [epoch: 8.87 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005170288118466771		[learning rate: 0.0022013]
		[batch 20/20] avg loss: 0.019111216804671823		[learning rate: 0.0021987]
	Learning Rate: 0.00219866
	LOSS [training: 0.012140752461569298 | validation: 0.0017033032803295835]
	TIME [epoch: 8.9 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007049222079499666		[learning rate: 0.002196]
		[batch 20/20] avg loss: 0.006375632520274714		[learning rate: 0.0021933]
	Learning Rate: 0.00219334
	LOSS [training: 0.00671242729988719 | validation: 0.012861953892234178]
	TIME [epoch: 8.87 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012771273670187568		[learning rate: 0.0021907]
		[batch 20/20] avg loss: 0.01110135557792789		[learning rate: 0.002188]
	Learning Rate: 0.00218803
	LOSS [training: 0.011936314624057734 | validation: 0.007899344786379561]
	TIME [epoch: 8.86 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013601735526852943		[learning rate: 0.0021854]
		[batch 20/20] avg loss: 0.02551758796384364		[learning rate: 0.0021827]
	Learning Rate: 0.00218273
	LOSS [training: 0.019559661745348296 | validation: 0.021270553841096666]
	TIME [epoch: 8.87 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010281024667623321		[learning rate: 0.0021801]
		[batch 20/20] avg loss: 0.012951564120844642		[learning rate: 0.0021774]
	Learning Rate: 0.00217745
	LOSS [training: 0.011616294394233981 | validation: 0.012094486829582118]
	TIME [epoch: 8.86 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01803564664140924		[learning rate: 0.0021748]
		[batch 20/20] avg loss: 0.012974234331929457		[learning rate: 0.0021722]
	Learning Rate: 0.00217217
	LOSS [training: 0.015504940486669349 | validation: 0.02075977673110433]
	TIME [epoch: 8.9 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008170960364588472		[learning rate: 0.0021695]
		[batch 20/20] avg loss: 0.010476546587623491		[learning rate: 0.0021669]
	Learning Rate: 0.00216692
	LOSS [training: 0.009323753476105982 | validation: 0.0030940739167041893]
	TIME [epoch: 8.86 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011110916698175287		[learning rate: 0.0021643]
		[batch 20/20] avg loss: 0.02086487318599166		[learning rate: 0.0021617]
	Learning Rate: 0.00216167
	LOSS [training: 0.015987894942083473 | validation: 0.005268237718869882]
	TIME [epoch: 8.87 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008209747653444576		[learning rate: 0.0021591]
		[batch 20/20] avg loss: 0.012236724127617993		[learning rate: 0.0021564]
	Learning Rate: 0.00215644
	LOSS [training: 0.010223235890531287 | validation: 0.011101711971507583]
	TIME [epoch: 8.87 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018330943097012695		[learning rate: 0.0021538]
		[batch 20/20] avg loss: 0.010345767056166286		[learning rate: 0.0021512]
	Learning Rate: 0.00215122
	LOSS [training: 0.014338355076589487 | validation: 0.010905254902975753]
	TIME [epoch: 8.87 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024983706180469793		[learning rate: 0.0021486]
		[batch 20/20] avg loss: 0.018787285520164086		[learning rate: 0.002146]
	Learning Rate: 0.00214601
	LOSS [training: 0.02188549585031694 | validation: -1.1145123475399775e-05]
	TIME [epoch: 8.88 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01499943697301205		[learning rate: 0.0021434]
		[batch 20/20] avg loss: 0.03243897383068265		[learning rate: 0.0021408]
	Learning Rate: 0.00214081
	LOSS [training: 0.02371920540184735 | validation: 0.05349203089840796]
	TIME [epoch: 8.86 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023840829760490276		[learning rate: 0.0021382]
		[batch 20/20] avg loss: 0.016310270889251383		[learning rate: 0.0021356]
	Learning Rate: 0.00213563
	LOSS [training: 0.02007555032487083 | validation: -0.003014103842418127]
	TIME [epoch: 8.88 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019749963545319722		[learning rate: 0.002133]
		[batch 20/20] avg loss: 0.016396030859300494		[learning rate: 0.0021305]
	Learning Rate: 0.00213046
	LOSS [training: 0.01807299720231011 | validation: 0.006513981344900787]
	TIME [epoch: 8.87 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0230847098161678		[learning rate: 0.0021279]
		[batch 20/20] avg loss: 0.011587981315508103		[learning rate: 0.0021253]
	Learning Rate: 0.0021253
	LOSS [training: 0.01733634556583795 | validation: -0.006235621537935599]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_739.pth
	Model improved!!!
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019623362584232362		[learning rate: 0.0021227]
		[batch 20/20] avg loss: 0.019576252763969203		[learning rate: 0.0021202]
	Learning Rate: 0.00212016
	LOSS [training: 0.01959980767410078 | validation: 0.016034767530510186]
	TIME [epoch: 8.88 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01595667325676342		[learning rate: 0.0021176]
		[batch 20/20] avg loss: 0.006192675497432182		[learning rate: 0.002115]
	Learning Rate: 0.00211503
	LOSS [training: 0.011074674377097802 | validation: 0.006840183313729065]
	TIME [epoch: 8.85 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013399641698301993		[learning rate: 0.0021125]
		[batch 20/20] avg loss: 0.02174786829082519		[learning rate: 0.0021099]
	Learning Rate: 0.00210991
	LOSS [training: 0.01757375499456359 | validation: 0.02454198991105874]
	TIME [epoch: 8.86 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020972303881548422		[learning rate: 0.0021074]
		[batch 20/20] avg loss: 0.022195889521111685		[learning rate: 0.0021048]
	Learning Rate: 0.0021048
	LOSS [training: 0.021584096701330054 | validation: 0.004991240254078709]
	TIME [epoch: 8.85 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014289185426225332		[learning rate: 0.0021022]
		[batch 20/20] avg loss: 0.009384949775630338		[learning rate: 0.0020997]
	Learning Rate: 0.0020997
	LOSS [training: 0.011837067600927833 | validation: 0.023018842522471335]
	TIME [epoch: 8.88 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028950381039878747		[learning rate: 0.0020972]
		[batch 20/20] avg loss: 0.029163472437875863		[learning rate: 0.0020946]
	Learning Rate: 0.00209462
	LOSS [training: 0.02905692673887731 | validation: 0.02007359969664385]
	TIME [epoch: 8.86 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011660483073230381		[learning rate: 0.0020921]
		[batch 20/20] avg loss: 0.02229488453807115		[learning rate: 0.0020895]
	Learning Rate: 0.00208955
	LOSS [training: 0.01697768380565077 | validation: 0.01582144637880744]
	TIME [epoch: 8.86 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014527062277508223		[learning rate: 0.002087]
		[batch 20/20] avg loss: 0.013885835309158916		[learning rate: 0.0020845]
	Learning Rate: 0.00208449
	LOSS [training: 0.01420644879333357 | validation: -0.0004980280282333026]
	TIME [epoch: 8.86 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01622580593597621		[learning rate: 0.002082]
		[batch 20/20] avg loss: 0.01624557615404328		[learning rate: 0.0020794]
	Learning Rate: 0.00207944
	LOSS [training: 0.016235691045009742 | validation: 0.05612841139004939]
	TIME [epoch: 8.85 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02207306492168616		[learning rate: 0.0020769]
		[batch 20/20] avg loss: 0.01731338179925395		[learning rate: 0.0020744]
	Learning Rate: 0.00207441
	LOSS [training: 0.019693223360470056 | validation: 0.017058728873187833]
	TIME [epoch: 8.88 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024829395735621214		[learning rate: 0.0020719]
		[batch 20/20] avg loss: 0.027206826464243876		[learning rate: 0.0020694]
	Learning Rate: 0.00206939
	LOSS [training: 0.026018111099932552 | validation: 0.01536232602867309]
	TIME [epoch: 8.86 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020639216807048562		[learning rate: 0.0020669]
		[batch 20/20] avg loss: 0.01677968774648511		[learning rate: 0.0020644]
	Learning Rate: 0.00206438
	LOSS [training: 0.01870945227676684 | validation: 0.04069922464059838]
	TIME [epoch: 8.87 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01587599855235359		[learning rate: 0.0020619]
		[batch 20/20] avg loss: 0.015562170723217434		[learning rate: 0.0020594]
	Learning Rate: 0.00205938
	LOSS [training: 0.01571908463778551 | validation: 0.008532079079762188]
	TIME [epoch: 8.87 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01673413940680255		[learning rate: 0.0020569]
		[batch 20/20] avg loss: 0.013969709050388922		[learning rate: 0.0020544]
	Learning Rate: 0.0020544
	LOSS [training: 0.015351924228595734 | validation: 0.0008168574582993258]
	TIME [epoch: 8.9 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028011860229103902		[learning rate: 0.0020519]
		[batch 20/20] avg loss: 0.018924212821544287		[learning rate: 0.0020494]
	Learning Rate: 0.00204942
	LOSS [training: 0.023468036525324094 | validation: 0.03637425230056142]
	TIME [epoch: 8.87 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0139699400126236		[learning rate: 0.0020469]
		[batch 20/20] avg loss: 0.012295246582152537		[learning rate: 0.0020445]
	Learning Rate: 0.00204446
	LOSS [training: 0.013132593297388067 | validation: 0.028479388641791763]
	TIME [epoch: 8.86 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029236575590367814		[learning rate: 0.002042]
		[batch 20/20] avg loss: 0.014702561535382921		[learning rate: 0.0020395]
	Learning Rate: 0.00203951
	LOSS [training: 0.021969568562875373 | validation: 0.01873024978792018]
	TIME [epoch: 8.86 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02173945647741916		[learning rate: 0.002037]
		[batch 20/20] avg loss: 0.01024010942741459		[learning rate: 0.0020346]
	Learning Rate: 0.00203457
	LOSS [training: 0.01598978295241688 | validation: -0.004827206931034305]
	TIME [epoch: 8.86 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009517184679349344		[learning rate: 0.0020321]
		[batch 20/20] avg loss: 0.012892430221032942		[learning rate: 0.0020296]
	Learning Rate: 0.00202965
	LOSS [training: 0.011204807450191142 | validation: 0.018194534210882865]
	TIME [epoch: 8.88 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00785602056368857		[learning rate: 0.0020272]
		[batch 20/20] avg loss: 0.01657755822423628		[learning rate: 0.0020247]
	Learning Rate: 0.00202474
	LOSS [training: 0.012216789393962426 | validation: 0.0030347807934860615]
	TIME [epoch: 8.86 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01681798862767478		[learning rate: 0.0020223]
		[batch 20/20] avg loss: 0.01648409577792164		[learning rate: 0.0020198]
	Learning Rate: 0.00201983
	LOSS [training: 0.016651042202798207 | validation: 0.0025156591643977575]
	TIME [epoch: 8.87 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008122994128425536		[learning rate: 0.0020174]
		[batch 20/20] avg loss: 0.028393844558942637		[learning rate: 0.0020149]
	Learning Rate: 0.00201494
	LOSS [training: 0.018258419343684086 | validation: 0.01633163306603079]
	TIME [epoch: 8.86 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016686953156766743		[learning rate: 0.0020125]
		[batch 20/20] avg loss: 0.009909633752226196		[learning rate: 0.0020101]
	Learning Rate: 0.00201007
	LOSS [training: 0.013298293454496471 | validation: 0.008952264811791635]
	TIME [epoch: 8.86 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010669798254817653		[learning rate: 0.0020076]
		[batch 20/20] avg loss: 0.009757988143435922		[learning rate: 0.0020052]
	Learning Rate: 0.0020052
	LOSS [training: 0.010213893199126786 | validation: 0.008840359091199014]
	TIME [epoch: 8.88 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01750129626860758		[learning rate: 0.0020028]
		[batch 20/20] avg loss: 0.016598005147686626		[learning rate: 0.0020003]
	Learning Rate: 0.00200035
	LOSS [training: 0.017049650708147103 | validation: 0.008797571696453621]
	TIME [epoch: 8.86 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011986244559438489		[learning rate: 0.0019979]
		[batch 20/20] avg loss: 0.02089267656344927		[learning rate: 0.0019955]
	Learning Rate: 0.0019955
	LOSS [training: 0.016439460561443877 | validation: 0.03455933131835062]
	TIME [epoch: 8.87 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030512731615327925		[learning rate: 0.0019931]
		[batch 20/20] avg loss: 0.024629171195444055		[learning rate: 0.0019907]
	Learning Rate: 0.00199067
	LOSS [training: 0.027570951405385992 | validation: 0.012204024518024256]
	TIME [epoch: 8.87 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009926641865972613		[learning rate: 0.0019883]
		[batch 20/20] avg loss: 0.0069644693928809205		[learning rate: 0.0019859]
	Learning Rate: 0.00198585
	LOSS [training: 0.008445555629426766 | validation: 0.029459231852913992]
	TIME [epoch: 8.88 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016202607970005743		[learning rate: 0.0019834]
		[batch 20/20] avg loss: 0.030239498660590374		[learning rate: 0.001981]
	Learning Rate: 0.00198105
	LOSS [training: 0.023221053315298057 | validation: 0.007541066417118863]
	TIME [epoch: 8.86 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009018410003944743		[learning rate: 0.0019786]
		[batch 20/20] avg loss: 0.024522562023998235		[learning rate: 0.0019763]
	Learning Rate: 0.00197625
	LOSS [training: 0.01677048601397149 | validation: 0.007531354585377639]
	TIME [epoch: 8.86 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022713648583996028		[learning rate: 0.0019739]
		[batch 20/20] avg loss: 0.017461377363364387		[learning rate: 0.0019715]
	Learning Rate: 0.00197147
	LOSS [training: 0.020087512973680208 | validation: 0.017325495750613505]
	TIME [epoch: 8.86 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027116696761859786		[learning rate: 0.0019691]
		[batch 20/20] avg loss: 0.020687265343814664		[learning rate: 0.0019667]
	Learning Rate: 0.00196669
	LOSS [training: 0.023901981052837227 | validation: 0.01940921801802711]
	TIME [epoch: 8.86 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01696310540183154		[learning rate: 0.0019643]
		[batch 20/20] avg loss: 0.015232484259732374		[learning rate: 0.0019619]
	Learning Rate: 0.00196193
	LOSS [training: 0.01609779483078196 | validation: 0.011697807088749002]
	TIME [epoch: 8.88 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01100977553472387		[learning rate: 0.0019596]
		[batch 20/20] avg loss: 0.023943643435747376		[learning rate: 0.0019572]
	Learning Rate: 0.00195718
	LOSS [training: 0.017476709485235626 | validation: 0.008367361487518297]
	TIME [epoch: 8.86 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017697914603391228		[learning rate: 0.0019548]
		[batch 20/20] avg loss: 0.027579092899448886		[learning rate: 0.0019524]
	Learning Rate: 0.00195245
	LOSS [training: 0.02263850375142006 | validation: 0.024597036802043098]
	TIME [epoch: 8.86 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01565876728924381		[learning rate: 0.0019501]
		[batch 20/20] avg loss: 0.018118491766532144		[learning rate: 0.0019477]
	Learning Rate: 0.00194772
	LOSS [training: 0.016888629527887975 | validation: 0.0035069170754523458]
	TIME [epoch: 8.86 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025343407028959895		[learning rate: 0.0019454]
		[batch 20/20] avg loss: 0.011694902763737669		[learning rate: 0.001943]
	Learning Rate: 0.001943
	LOSS [training: 0.018519154896348776 | validation: 0.025918354755978368]
	TIME [epoch: 8.86 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017279571784941557		[learning rate: 0.0019407]
		[batch 20/20] avg loss: 0.028708973302273545		[learning rate: 0.0019383]
	Learning Rate: 0.0019383
	LOSS [training: 0.022994272543607554 | validation: 0.013268618103727862]
	TIME [epoch: 8.88 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014706647243960624		[learning rate: 0.001936]
		[batch 20/20] avg loss: 0.01261934402585839		[learning rate: 0.0019336]
	Learning Rate: 0.00193361
	LOSS [training: 0.013662995634909506 | validation: 0.01307654266962806]
	TIME [epoch: 8.87 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029333815348509113		[learning rate: 0.0019313]
		[batch 20/20] avg loss: 0.020491298550288434		[learning rate: 0.0019289]
	Learning Rate: 0.00192893
	LOSS [training: 0.024912556949398777 | validation: 0.012615245435423635]
	TIME [epoch: 8.87 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022983183381878918		[learning rate: 0.0019266]
		[batch 20/20] avg loss: 0.020447716847491314		[learning rate: 0.0019243]
	Learning Rate: 0.00192426
	LOSS [training: 0.021715450114685116 | validation: 0.017106737844646478]
	TIME [epoch: 8.86 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01431350816104457		[learning rate: 0.0019219]
		[batch 20/20] avg loss: 0.018714635542752715		[learning rate: 0.0019196]
	Learning Rate: 0.0019196
	LOSS [training: 0.016514071851898647 | validation: 0.013316948746002678]
	TIME [epoch: 8.87 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014311326513811794		[learning rate: 0.0019173]
		[batch 20/20] avg loss: 0.012628127090703375		[learning rate: 0.001915]
	Learning Rate: 0.00191495
	LOSS [training: 0.013469726802257586 | validation: 0.023704268703938276]
	TIME [epoch: 8.88 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022471938389667732		[learning rate: 0.0019126]
		[batch 20/20] avg loss: 0.022057409338108524		[learning rate: 0.0019103]
	Learning Rate: 0.00191032
	LOSS [training: 0.02226467386388813 | validation: 0.03903056449693042]
	TIME [epoch: 8.85 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04006826325047814		[learning rate: 0.001908]
		[batch 20/20] avg loss: 0.030536299388723286		[learning rate: 0.0019057]
	Learning Rate: 0.00190569
	LOSS [training: 0.03530228131960071 | validation: 0.009589293179762596]
	TIME [epoch: 8.86 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015492723351652804		[learning rate: 0.0019034]
		[batch 20/20] avg loss: 0.013140086965764944		[learning rate: 0.0019011]
	Learning Rate: 0.00190108
	LOSS [training: 0.014316405158708875 | validation: 0.016806332488205313]
	TIME [epoch: 8.86 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014667546196175713		[learning rate: 0.0018988]
		[batch 20/20] avg loss: 0.009899959728124002		[learning rate: 0.0018965]
	Learning Rate: 0.00189648
	LOSS [training: 0.012283752962149857 | validation: 0.01880504085331855]
	TIME [epoch: 8.88 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008883153823313695		[learning rate: 0.0018942]
		[batch 20/20] avg loss: 0.009774023667569857		[learning rate: 0.0018919]
	Learning Rate: 0.00189188
	LOSS [training: 0.009328588745441775 | validation: 0.0002952680938325342]
	TIME [epoch: 8.87 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002625252013276021		[learning rate: 0.0018896]
		[batch 20/20] avg loss: 0.0070475166759578906		[learning rate: 0.0018873]
	Learning Rate: 0.00188731
	LOSS [training: 0.004836384344616955 | validation: 0.01466647444543162]
	TIME [epoch: 8.86 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00688445683004033		[learning rate: 0.001885]
		[batch 20/20] avg loss: 0.01129648438150705		[learning rate: 0.0018827]
	Learning Rate: 0.00188274
	LOSS [training: 0.00909047060577369 | validation: 0.008117264564428869]
	TIME [epoch: 8.86 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011583425776515652		[learning rate: 0.0018805]
		[batch 20/20] avg loss: 0.03227075591065488		[learning rate: 0.0018782]
	Learning Rate: 0.00187818
	LOSS [training: 0.021927090843585264 | validation: 0.08173132707900475]
	TIME [epoch: 8.85 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02225722616116329		[learning rate: 0.0018759]
		[batch 20/20] avg loss: 0.014382594501057167		[learning rate: 0.0018736]
	Learning Rate: 0.00187363
	LOSS [training: 0.01831991033111023 | validation: 0.004562309103482576]
	TIME [epoch: 8.88 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031735013939346276		[learning rate: 0.0018714]
		[batch 20/20] avg loss: 0.011510635613144854		[learning rate: 0.0018691]
	Learning Rate: 0.0018691
	LOSS [training: 0.0073420685035397405 | validation: 0.00683806460163679]
	TIME [epoch: 8.88 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004808312161904663		[learning rate: 0.0018668]
		[batch 20/20] avg loss: 0.013671495154443228		[learning rate: 0.0018646]
	Learning Rate: 0.00186457
	LOSS [training: 0.009239903658173947 | validation: 0.003168069446010153]
	TIME [epoch: 8.86 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011315894947644106		[learning rate: 0.0018623]
		[batch 20/20] avg loss: 0.01746130993560621		[learning rate: 0.0018601]
	Learning Rate: 0.00186006
	LOSS [training: 0.014388602441625153 | validation: 0.016281711345741776]
	TIME [epoch: 8.86 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02933948215559769		[learning rate: 0.0018578]
		[batch 20/20] avg loss: 0.048993646730774124		[learning rate: 0.0018556]
	Learning Rate: 0.00185555
	LOSS [training: 0.039166564443185906 | validation: 0.0054481593477340206]
	TIME [epoch: 8.85 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014636591637227128		[learning rate: 0.0018533]
		[batch 20/20] avg loss: 0.004948800966429442		[learning rate: 0.0018511]
	Learning Rate: 0.00185106
	LOSS [training: 0.009792696301828285 | validation: 0.0048653783275221536]
	TIME [epoch: 8.89 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023570810381732175		[learning rate: 0.0018488]
		[batch 20/20] avg loss: 0.03230645877791229		[learning rate: 0.0018466]
	Learning Rate: 0.00184658
	LOSS [training: 0.02793863457982223 | validation: 0.01586745252712364]
	TIME [epoch: 8.86 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024862769223826416		[learning rate: 0.0018443]
		[batch 20/20] avg loss: 0.01351120685638848		[learning rate: 0.0018421]
	Learning Rate: 0.00184211
	LOSS [training: 0.01918698804010745 | validation: 0.015178129804817088]
	TIME [epoch: 8.85 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017232190553553548		[learning rate: 0.0018399]
		[batch 20/20] avg loss: 0.015568748463118395		[learning rate: 0.0018377]
	Learning Rate: 0.00183765
	LOSS [training: 0.016400469508335975 | validation: -0.0017417176649649727]
	TIME [epoch: 8.86 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020371470639149195		[learning rate: 0.0018354]
		[batch 20/20] avg loss: 0.014404071404268423		[learning rate: 0.0018332]
	Learning Rate: 0.0018332
	LOSS [training: 0.017387771021708815 | validation: 0.0017336421918945828]
	TIME [epoch: 8.87 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010013368852144763		[learning rate: 0.001831]
		[batch 20/20] avg loss: 0.018912666404522386		[learning rate: 0.0018288]
	Learning Rate: 0.00182876
	LOSS [training: 0.014463017628333578 | validation: 0.027313030832384823]
	TIME [epoch: 8.87 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029462835916425234		[learning rate: 0.0018265]
		[batch 20/20] avg loss: 0.01460441529246358		[learning rate: 0.0018243]
	Learning Rate: 0.00182434
	LOSS [training: 0.022033625604444402 | validation: 0.007484544147223306]
	TIME [epoch: 8.83 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014975746996839224		[learning rate: 0.0018221]
		[batch 20/20] avg loss: 0.035952988096994085		[learning rate: 0.0018199]
	Learning Rate: 0.00181992
	LOSS [training: 0.025464367546916656 | validation: 0.008621155018139617]
	TIME [epoch: 8.86 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01624123890674506		[learning rate: 0.0018177]
		[batch 20/20] avg loss: 0.010307155768306582		[learning rate: 0.0018155]
	Learning Rate: 0.00181552
	LOSS [training: 0.01327419733752582 | validation: 0.0036063489410414233]
	TIME [epoch: 8.86 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013621920047582555		[learning rate: 0.0018133]
		[batch 20/20] avg loss: 0.01742250757347235		[learning rate: 0.0018111]
	Learning Rate: 0.00181112
	LOSS [training: 0.015522213810527454 | validation: 0.011561464934328362]
	TIME [epoch: 8.88 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012785020831852475		[learning rate: 0.0018089]
		[batch 20/20] avg loss: 0.010692342231980944		[learning rate: 0.0018067]
	Learning Rate: 0.00180674
	LOSS [training: 0.011738681531916711 | validation: 0.004225174112226871]
	TIME [epoch: 8.87 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03847936854880892		[learning rate: 0.0018045]
		[batch 20/20] avg loss: 0.028971572068806767		[learning rate: 0.0018024]
	Learning Rate: 0.00180236
	LOSS [training: 0.03372547030880784 | validation: 0.020411505966759683]
	TIME [epoch: 8.86 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012181225368809404		[learning rate: 0.0018002]
		[batch 20/20] avg loss: 0.019624208498134378		[learning rate: 0.001798]
	Learning Rate: 0.001798
	LOSS [training: 0.015902716933471887 | validation: 0.003989160626530497]
	TIME [epoch: 8.87 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01245114860830898		[learning rate: 0.0017958]
		[batch 20/20] avg loss: 0.02050300117416858		[learning rate: 0.0017936]
	Learning Rate: 0.00179365
	LOSS [training: 0.01647707489123878 | validation: 0.04984422198485275]
	TIME [epoch: 8.86 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01662703872777098		[learning rate: 0.0017915]
		[batch 20/20] avg loss: 0.012231078273803517		[learning rate: 0.0017893]
	Learning Rate: 0.0017893
	LOSS [training: 0.014429058500787248 | validation: 0.004733578432097323]
	TIME [epoch: 8.88 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015212734712068226		[learning rate: 0.0017871]
		[batch 20/20] avg loss: 0.017694741141731266		[learning rate: 0.001785]
	Learning Rate: 0.00178497
	LOSS [training: 0.016453737926899747 | validation: 0.002069184406617775]
	TIME [epoch: 8.87 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0042802297448553475		[learning rate: 0.0017828]
		[batch 20/20] avg loss: 0.014748083257685212		[learning rate: 0.0017807]
	Learning Rate: 0.00178065
	LOSS [training: 0.009514156501270282 | validation: 0.00545550972191588]
	TIME [epoch: 8.87 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007700057822785085		[learning rate: 0.0017785]
		[batch 20/20] avg loss: 0.024879718245416207		[learning rate: 0.0017763]
	Learning Rate: 0.00177634
	LOSS [training: 0.016289888034100645 | validation: 0.03615257156553487]
	TIME [epoch: 8.86 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010792488564456368		[learning rate: 0.0017742]
		[batch 20/20] avg loss: 0.010066150893091534		[learning rate: 0.001772]
	Learning Rate: 0.00177204
	LOSS [training: 0.010429319728773949 | validation: -0.0018557484793261425]
	TIME [epoch: 8.87 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010706974125927035		[learning rate: 0.0017699]
		[batch 20/20] avg loss: 0.013958389826852737		[learning rate: 0.0017678]
	Learning Rate: 0.00176775
	LOSS [training: 0.012332681976389885 | validation: 0.00048074552961066075]
	TIME [epoch: 8.88 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011147425495205409		[learning rate: 0.0017656]
		[batch 20/20] avg loss: 0.0172987310767426		[learning rate: 0.0017635]
	Learning Rate: 0.00176347
	LOSS [training: 0.014223078285973998 | validation: -0.0026870255809254167]
	TIME [epoch: 8.85 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01054988534417743		[learning rate: 0.0017613]
		[batch 20/20] avg loss: 0.016847636345314138		[learning rate: 0.0017592]
	Learning Rate: 0.0017592
	LOSS [training: 0.013698760844745783 | validation: 0.006170079860839792]
	TIME [epoch: 8.85 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01571132592088397		[learning rate: 0.0017571]
		[batch 20/20] avg loss: 0.021288127611318375		[learning rate: 0.0017549]
	Learning Rate: 0.00175494
	LOSS [training: 0.018499726766101177 | validation: 0.009243038144267607]
	TIME [epoch: 8.87 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008776623265296095		[learning rate: 0.0017528]
		[batch 20/20] avg loss: 0.014472927972971224		[learning rate: 0.0017507]
	Learning Rate: 0.0017507
	LOSS [training: 0.01162477561913366 | validation: 0.008010516276124296]
	TIME [epoch: 8.87 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034854037402716906		[learning rate: 0.0017486]
		[batch 20/20] avg loss: 0.01442882926914584		[learning rate: 0.0017465]
	Learning Rate: 0.00174646
	LOSS [training: 0.02464143333593137 | validation: -0.0033493761002819705]
	TIME [epoch: 8.88 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011427593062351058		[learning rate: 0.0017443]
		[batch 20/20] avg loss: 0.01942964486851769		[learning rate: 0.0017422]
	Learning Rate: 0.00174223
	LOSS [training: 0.015428618965434376 | validation: 0.0794948924323579]
	TIME [epoch: 8.88 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01863760697722297		[learning rate: 0.0017401]
		[batch 20/20] avg loss: 0.0074611775194977055		[learning rate: 0.001738]
	Learning Rate: 0.00173801
	LOSS [training: 0.01304939224836034 | validation: -0.007353385417305484]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_822.pth
	Model improved!!!
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00832695504761624		[learning rate: 0.0017359]
		[batch 20/20] avg loss: 0.012380203081737293		[learning rate: 0.0017338]
	Learning Rate: 0.0017338
	LOSS [training: 0.010353579064676767 | validation: 0.015254053022887975]
	TIME [epoch: 8.85 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009106951909194508		[learning rate: 0.0017317]
		[batch 20/20] avg loss: 0.0047341215138797885		[learning rate: 0.0017296]
	Learning Rate: 0.00172961
	LOSS [training: 0.006920536711537149 | validation: 0.005195345827821924]
	TIME [epoch: 8.88 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0057389526609352825		[learning rate: 0.0017275]
		[batch 20/20] avg loss: 0.008997481103986512		[learning rate: 0.0017254]
	Learning Rate: 0.00172542
	LOSS [training: 0.007368216882460898 | validation: 0.007771681472084659]
	TIME [epoch: 8.87 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014991204649422354		[learning rate: 0.0017233]
		[batch 20/20] avg loss: 0.018170234067813533		[learning rate: 0.0017212]
	Learning Rate: 0.00172124
	LOSS [training: 0.016580719358617945 | validation: 0.019720899896449123]
	TIME [epoch: 8.87 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016117348163864193		[learning rate: 0.0017192]
		[batch 20/20] avg loss: 0.011829230641241922		[learning rate: 0.0017171]
	Learning Rate: 0.00171708
	LOSS [training: 0.013973289402553057 | validation: 0.0037093751107242063]
	TIME [epoch: 8.85 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008544766799867796		[learning rate: 0.001715]
		[batch 20/20] avg loss: 0.0065942338799060835		[learning rate: 0.0017129]
	Learning Rate: 0.00171292
	LOSS [training: 0.007569500339886942 | validation: 0.02554597141424823]
	TIME [epoch: 8.86 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011651142932671301		[learning rate: 0.0017108]
		[batch 20/20] avg loss: 0.010964343657411744		[learning rate: 0.0017088]
	Learning Rate: 0.00170877
	LOSS [training: 0.011307743295041525 | validation: 0.012588506013112318]
	TIME [epoch: 8.87 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017499318133345692		[learning rate: 0.0017067]
		[batch 20/20] avg loss: 0.01030977771883922		[learning rate: 0.0017046]
	Learning Rate: 0.00170464
	LOSS [training: 0.01390454792609246 | validation: 0.011416577438595962]
	TIME [epoch: 8.86 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01136594909800012		[learning rate: 0.0017026]
		[batch 20/20] avg loss: 0.006946117453178605		[learning rate: 0.0017005]
	Learning Rate: 0.00170051
	LOSS [training: 0.009156033275589365 | validation: 0.0009571157945782769]
	TIME [epoch: 8.86 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01218547894467801		[learning rate: 0.0016984]
		[batch 20/20] avg loss: 0.013247743273040555		[learning rate: 0.0016964]
	Learning Rate: 0.00169639
	LOSS [training: 0.012716611108859282 | validation: 0.023321077576916052]
	TIME [epoch: 8.87 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012811433124736132		[learning rate: 0.0016943]
		[batch 20/20] avg loss: 0.007148467362599192		[learning rate: 0.0016923]
	Learning Rate: 0.00169229
	LOSS [training: 0.009979950243667662 | validation: 0.012341824980391684]
	TIME [epoch: 8.88 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001910907211381961		[learning rate: 0.0016902]
		[batch 20/20] avg loss: 0.017234687039854608		[learning rate: 0.0016882]
	Learning Rate: 0.00168819
	LOSS [training: 0.009572797125618282 | validation: 0.0052616107003028555]
	TIME [epoch: 8.88 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008698828249856908		[learning rate: 0.0016861]
		[batch 20/20] avg loss: 0.016575819800571514		[learning rate: 0.0016841]
	Learning Rate: 0.0016841
	LOSS [training: 0.012637324025214208 | validation: 0.03555379590558791]
	TIME [epoch: 8.86 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007865274301632717		[learning rate: 0.0016821]
		[batch 20/20] avg loss: 0.020307779460391813		[learning rate: 0.00168]
	Learning Rate: 0.00168003
	LOSS [training: 0.014086526881012265 | validation: -0.00540845341137966]
	TIME [epoch: 8.86 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008622108173986535		[learning rate: 0.001678]
		[batch 20/20] avg loss: 0.01931908874891091		[learning rate: 0.001676]
	Learning Rate: 0.00167596
	LOSS [training: 0.013970598461448724 | validation: 0.011961569718752427]
	TIME [epoch: 8.87 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018862804910558067		[learning rate: 0.0016739]
		[batch 20/20] avg loss: 0.0063928916863142254		[learning rate: 0.0016719]
	Learning Rate: 0.0016719
	LOSS [training: 0.012627848298436144 | validation: 0.0014624740315077286]
	TIME [epoch: 8.89 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011673106797996297		[learning rate: 0.0016699]
		[batch 20/20] avg loss: 0.013513657910716192		[learning rate: 0.0016679]
	Learning Rate: 0.00166785
	LOSS [training: 0.012593382354356244 | validation: 0.03738082920549672]
	TIME [epoch: 8.86 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04033256269504296		[learning rate: 0.0016658]
		[batch 20/20] avg loss: 0.007763419846441545		[learning rate: 0.0016638]
	Learning Rate: 0.00166382
	LOSS [training: 0.024047991270742256 | validation: 0.0054441196727740265]
	TIME [epoch: 8.86 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015418101981623217		[learning rate: 0.0016618]
		[batch 20/20] avg loss: 0.024049172949603986		[learning rate: 0.0016598]
	Learning Rate: 0.00165979
	LOSS [training: 0.019733637465613598 | validation: 0.013401981582208261]
	TIME [epoch: 8.87 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02265280491224885		[learning rate: 0.0016578]
		[batch 20/20] avg loss: 0.026844047509483375		[learning rate: 0.0016558]
	Learning Rate: 0.00165577
	LOSS [training: 0.024748426210866113 | validation: 0.0073545060897196495]
	TIME [epoch: 8.85 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013403834710396958		[learning rate: 0.0016538]
		[batch 20/20] avg loss: 0.01728568874458682		[learning rate: 0.0016518]
	Learning Rate: 0.00165176
	LOSS [training: 0.01534476172749189 | validation: 0.0066318557773440414]
	TIME [epoch: 8.89 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012645916293430138		[learning rate: 0.0016498]
		[batch 20/20] avg loss: 0.01710674745130448		[learning rate: 0.0016478]
	Learning Rate: 0.00164776
	LOSS [training: 0.01487633187236731 | validation: 0.01837567664820548]
	TIME [epoch: 8.86 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018583563491504952		[learning rate: 0.0016458]
		[batch 20/20] avg loss: 0.011917090678286661		[learning rate: 0.0016438]
	Learning Rate: 0.00164377
	LOSS [training: 0.015250327084895803 | validation: 0.014334732672069358]
	TIME [epoch: 8.86 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010402686971783131		[learning rate: 0.0016418]
		[batch 20/20] avg loss: 0.013021664770544691		[learning rate: 0.0016398]
	Learning Rate: 0.00163979
	LOSS [training: 0.01171217587116391 | validation: 0.008973214063544243]
	TIME [epoch: 8.87 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015423233712898905		[learning rate: 0.0016378]
		[batch 20/20] avg loss: 0.015617645824019893		[learning rate: 0.0016358]
	Learning Rate: 0.00163583
	LOSS [training: 0.0155204397684594 | validation: 0.027221309638472715]
	TIME [epoch: 8.88 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015689971846394715		[learning rate: 0.0016338]
		[batch 20/20] avg loss: 0.010534103183676818		[learning rate: 0.0016319]
	Learning Rate: 0.00163186
	LOSS [training: 0.013112037515035768 | validation: 0.00935437486662851]
	TIME [epoch: 8.87 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017109236900972526		[learning rate: 0.0016299]
		[batch 20/20] avg loss: 0.01569207832853705		[learning rate: 0.0016279]
	Learning Rate: 0.00162791
	LOSS [training: 0.016400657614754788 | validation: 0.01803252658288758]
	TIME [epoch: 8.86 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016780479463345925		[learning rate: 0.0016259]
		[batch 20/20] avg loss: 0.007911496670419089		[learning rate: 0.001624]
	Learning Rate: 0.00162397
	LOSS [training: 0.012345988066882505 | validation: 0.015414301294485594]
	TIME [epoch: 8.86 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02372240648289263		[learning rate: 0.001622]
		[batch 20/20] avg loss: 0.015750716668122947		[learning rate: 0.00162]
	Learning Rate: 0.00162004
	LOSS [training: 0.01973656157550779 | validation: 0.01644506093696913]
	TIME [epoch: 8.85 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031320822110400146		[learning rate: 0.0016181]
		[batch 20/20] avg loss: 0.02628857033461033		[learning rate: 0.0016161]
	Learning Rate: 0.00161612
	LOSS [training: 0.028804696222505245 | validation: 0.0011793703847988468]
	TIME [epoch: 8.88 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014104120701703828		[learning rate: 0.0016142]
		[batch 20/20] avg loss: 0.01825241276173873		[learning rate: 0.0016122]
	Learning Rate: 0.00161221
	LOSS [training: 0.01617826673172128 | validation: 0.012864598331389407]
	TIME [epoch: 8.86 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020493704312830756		[learning rate: 0.0016103]
		[batch 20/20] avg loss: 0.009240087517031606		[learning rate: 0.0016083]
	Learning Rate: 0.0016083
	LOSS [training: 0.014866895914931186 | validation: 0.01676008418003079]
	TIME [epoch: 8.86 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014534021206869242		[learning rate: 0.0016064]
		[batch 20/20] avg loss: 0.011646867736393837		[learning rate: 0.0016044]
	Learning Rate: 0.00160441
	LOSS [training: 0.01309044447163154 | validation: 0.01350265159053862]
	TIME [epoch: 8.85 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016379513864296556		[learning rate: 0.0016025]
		[batch 20/20] avg loss: 0.012301663638879749		[learning rate: 0.0016005]
	Learning Rate: 0.00160053
	LOSS [training: 0.014340588751588154 | validation: 0.018214677467557137]
	TIME [epoch: 8.85 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017239772331194343		[learning rate: 0.0015986]
		[batch 20/20] avg loss: 0.019565766035027084		[learning rate: 0.0015967]
	Learning Rate: 0.00159665
	LOSS [training: 0.018402769183110713 | validation: 0.018074263490906947]
	TIME [epoch: 8.87 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03156760319652867		[learning rate: 0.0015947]
		[batch 20/20] avg loss: 0.022127441173741935		[learning rate: 0.0015928]
	Learning Rate: 0.00159279
	LOSS [training: 0.026847522185135298 | validation: 0.0004487587712910068]
	TIME [epoch: 8.86 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012053090448265006		[learning rate: 0.0015909]
		[batch 20/20] avg loss: 0.018404211746584433		[learning rate: 0.0015889]
	Learning Rate: 0.00158893
	LOSS [training: 0.015228651097424722 | validation: 0.027870100038758082]
	TIME [epoch: 8.85 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011980556784669147		[learning rate: 0.001587]
		[batch 20/20] avg loss: 0.003744185315539856		[learning rate: 0.0015851]
	Learning Rate: 0.00158509
	LOSS [training: 0.007862371050104502 | validation: -0.00651518291056509]
	TIME [epoch: 8.84 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030154280299480086		[learning rate: 0.0015832]
		[batch 20/20] avg loss: 0.021641308802248727		[learning rate: 0.0015812]
	Learning Rate: 0.00158125
	LOSS [training: 0.012328368416098367 | validation: 0.01314571464257552]
	TIME [epoch: 8.86 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009627248856484649		[learning rate: 0.0015793]
		[batch 20/20] avg loss: 0.0068455941763536626		[learning rate: 0.0015774]
	Learning Rate: 0.00157742
	LOSS [training: 0.008236421516419155 | validation: 0.006994601925021684]
	TIME [epoch: 8.85 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007134513490211165		[learning rate: 0.0015755]
		[batch 20/20] avg loss: 0.007066322185435012		[learning rate: 0.0015736]
	Learning Rate: 0.0015736
	LOSS [training: 0.007100417837823088 | validation: 0.008051974559498612]
	TIME [epoch: 8.85 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005892957389796981		[learning rate: 0.0015717]
		[batch 20/20] avg loss: 0.007931678150352338		[learning rate: 0.0015698]
	Learning Rate: 0.00156979
	LOSS [training: 0.006912317770074661 | validation: 0.0033414911260817425]
	TIME [epoch: 8.85 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005810170659670598		[learning rate: 0.0015679]
		[batch 20/20] avg loss: 0.012711801305625544		[learning rate: 0.001566]
	Learning Rate: 0.00156599
	LOSS [training: 0.00926098598264807 | validation: 0.009492998290544893]
	TIME [epoch: 8.85 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0076805009792976124		[learning rate: 0.0015641]
		[batch 20/20] avg loss: 0.011574030197993804		[learning rate: 0.0015622]
	Learning Rate: 0.0015622
	LOSS [training: 0.009627265588645708 | validation: -0.0023725588847852367]
	TIME [epoch: 8.87 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006167868852685472		[learning rate: 0.0015603]
		[batch 20/20] avg loss: 0.016808569431076996		[learning rate: 0.0015584]
	Learning Rate: 0.00155842
	LOSS [training: 0.011488219141881235 | validation: 0.00232035060440091]
	TIME [epoch: 8.87 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007243344715720685		[learning rate: 0.0015565]
		[batch 20/20] avg loss: 0.011051940315329644		[learning rate: 0.0015546]
	Learning Rate: 0.00155465
	LOSS [training: 0.009147642515525162 | validation: -0.003321085112984283]
	TIME [epoch: 8.85 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000861175894352258		[learning rate: 0.0015528]
		[batch 20/20] avg loss: 0.021901462196887173		[learning rate: 0.0015509]
	Learning Rate: 0.00155088
	LOSS [training: 0.011381319045619717 | validation: 0.01672269707642162]
	TIME [epoch: 8.85 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009752236686014812		[learning rate: 0.001549]
		[batch 20/20] avg loss: 0.006168877276697396		[learning rate: 0.0015471]
	Learning Rate: 0.00154713
	LOSS [training: 0.007960556981356105 | validation: -0.0007776364019966668]
	TIME [epoch: 8.86 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019468583571775278		[learning rate: 0.0015453]
		[batch 20/20] avg loss: 0.009762392245054305		[learning rate: 0.0015434]
	Learning Rate: 0.00154338
	LOSS [training: 0.014615487908414793 | validation: 0.004435405216506637]
	TIME [epoch: 8.87 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01092755354609545		[learning rate: 0.0015415]
		[batch 20/20] avg loss: 0.008814103207750005		[learning rate: 0.0015396]
	Learning Rate: 0.00153965
	LOSS [training: 0.009870828376922729 | validation: 0.004065326300324057]
	TIME [epoch: 8.86 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004808824782172119		[learning rate: 0.0015378]
		[batch 20/20] avg loss: 0.004458511693853763		[learning rate: 0.0015359]
	Learning Rate: 0.00153592
	LOSS [training: 0.004633668238012941 | validation: -0.007801762015440269]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_873.pth
	Model improved!!!
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034525990997556024		[learning rate: 0.0015341]
		[batch 20/20] avg loss: 0.008591236849749221		[learning rate: 0.0015322]
	Learning Rate: 0.0015322
	LOSS [training: 0.006021917974752411 | validation: -0.0033801957232497386]
	TIME [epoch: 8.86 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010674821985681206		[learning rate: 0.0015303]
		[batch 20/20] avg loss: 0.004300755516681196		[learning rate: 0.0015285]
	Learning Rate: 0.00152849
	LOSS [training: 0.007487788751181201 | validation: 0.0028974943052914297]
	TIME [epoch: 8.84 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00871044188896033		[learning rate: 0.0015266]
		[batch 20/20] avg loss: 0.008574496252574119		[learning rate: 0.0015248]
	Learning Rate: 0.00152479
	LOSS [training: 0.008642469070767224 | validation: -0.0005437925757127962]
	TIME [epoch: 8.87 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009602710463352277		[learning rate: 0.0015229]
		[batch 20/20] avg loss: 0.005538036308584731		[learning rate: 0.0015211]
	Learning Rate: 0.0015211
	LOSS [training: 0.0075703733859685055 | validation: -0.006028228978377095]
	TIME [epoch: 8.85 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03337785375335323		[learning rate: 0.0015193]
		[batch 20/20] avg loss: 0.03339127148452874		[learning rate: 0.0015174]
	Learning Rate: 0.00151742
	LOSS [training: 0.033384562618940995 | validation: 0.01228493076134728]
	TIME [epoch: 8.85 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02249092931956634		[learning rate: 0.0015156]
		[batch 20/20] avg loss: 0.010285806986498012		[learning rate: 0.0015137]
	Learning Rate: 0.00151374
	LOSS [training: 0.016388368153032174 | validation: 0.0075860989340532405]
	TIME [epoch: 8.85 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00863697324500715		[learning rate: 0.0015119]
		[batch 20/20] avg loss: 0.005506358553980068		[learning rate: 0.0015101]
	Learning Rate: 0.00151008
	LOSS [training: 0.007071665899493608 | validation: 0.002118750695959579]
	TIME [epoch: 8.86 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015498315312042325		[learning rate: 0.0015083]
		[batch 20/20] avg loss: 0.022604448777008075		[learning rate: 0.0015064]
	Learning Rate: 0.00150642
	LOSS [training: 0.0190513820445252 | validation: 0.02897783714944096]
	TIME [epoch: 8.87 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05645110925335687		[learning rate: 0.0015046]
		[batch 20/20] avg loss: 0.07437405872490228		[learning rate: 0.0015028]
	Learning Rate: 0.00150278
	LOSS [training: 0.06541258398912958 | validation: 0.06735650723521834]
	TIME [epoch: 8.86 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0423615081218746		[learning rate: 0.001501]
		[batch 20/20] avg loss: 0.0350021663501797		[learning rate: 0.0014991]
	Learning Rate: 0.00149914
	LOSS [training: 0.038681837236027146 | validation: 0.03243129848970912]
	TIME [epoch: 8.86 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02405230885545359		[learning rate: 0.0014973]
		[batch 20/20] avg loss: 0.015422333999476744		[learning rate: 0.0014955]
	Learning Rate: 0.00149551
	LOSS [training: 0.01973732142746517 | validation: 2.4796143966850104e-05]
	TIME [epoch: 8.85 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008695509423917808		[learning rate: 0.0014937]
		[batch 20/20] avg loss: 0.02067433851985919		[learning rate: 0.0014919]
	Learning Rate: 0.00149189
	LOSS [training: 0.014684923971888498 | validation: 0.0039872735499260105]
	TIME [epoch: 8.88 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012644932010657143		[learning rate: 0.0014901]
		[batch 20/20] avg loss: 0.013121309143175736		[learning rate: 0.0014883]
	Learning Rate: 0.00148828
	LOSS [training: 0.01288312057691644 | validation: 0.0067864237995768194]
	TIME [epoch: 8.87 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013437401298785035		[learning rate: 0.0014865]
		[batch 20/20] avg loss: 0.004062466725134136		[learning rate: 0.0014847]
	Learning Rate: 0.00148468
	LOSS [training: 0.008749934011959584 | validation: 0.004848469631979856]
	TIME [epoch: 8.86 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007618138143396072		[learning rate: 0.0014829]
		[batch 20/20] avg loss: 0.015629143544269794		[learning rate: 0.0014811]
	Learning Rate: 0.00148108
	LOSS [training: 0.01162364084383293 | validation: 0.01135236760443468]
	TIME [epoch: 8.86 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004175884735077491		[learning rate: 0.0014793]
		[batch 20/20] avg loss: 0.008029115163850114		[learning rate: 0.0014775]
	Learning Rate: 0.0014775
	LOSS [training: 0.0061024999494638035 | validation: 0.005577547269252354]
	TIME [epoch: 8.86 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011560536599275917		[learning rate: 0.0014757]
		[batch 20/20] avg loss: 0.01092032130623014		[learning rate: 0.0014739]
	Learning Rate: 0.00147392
	LOSS [training: 0.01124042895275303 | validation: -0.0005061671495388848]
	TIME [epoch: 8.87 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012368782215630817		[learning rate: 0.0014721]
		[batch 20/20] avg loss: 0.008731839073383502		[learning rate: 0.0014704]
	Learning Rate: 0.00147035
	LOSS [training: 0.01055031064450716 | validation: -0.004220651497755175]
	TIME [epoch: 8.86 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012982270476824451		[learning rate: 0.0014686]
		[batch 20/20] avg loss: 0.010021558419563786		[learning rate: 0.0014668]
	Learning Rate: 0.00146679
	LOSS [training: 0.005659892733623116 | validation: -0.0006050530332278219]
	TIME [epoch: 8.86 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011588455172133796		[learning rate: 0.001465]
		[batch 20/20] avg loss: 0.016415535421958654		[learning rate: 0.0014632]
	Learning Rate: 0.00146324
	LOSS [training: 0.014001995297046228 | validation: 0.0102061443561197]
	TIME [epoch: 8.87 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010133068498190258		[learning rate: 0.0014615]
		[batch 20/20] avg loss: 0.017951572213970667		[learning rate: 0.0014597]
	Learning Rate: 0.0014597
	LOSS [training: 0.014042320356080459 | validation: 0.0259560180569876]
	TIME [epoch: 8.88 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030293897906128464		[learning rate: 0.0014579]
		[batch 20/20] avg loss: 0.007019431642932522		[learning rate: 0.0014562]
	Learning Rate: 0.00145616
	LOSS [training: 0.01865666477453049 | validation: 0.005052316029610833]
	TIME [epoch: 8.86 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019615030777858484		[learning rate: 0.0014544]
		[batch 20/20] avg loss: 0.02539425855441398		[learning rate: 0.0014526]
	Learning Rate: 0.00145264
	LOSS [training: 0.013677880816099913 | validation: 0.024358365471012354]
	TIME [epoch: 8.85 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0124567213113834		[learning rate: 0.0014509]
		[batch 20/20] avg loss: 0.014607374052201628		[learning rate: 0.0014491]
	Learning Rate: 0.00144912
	LOSS [training: 0.013532047681792512 | validation: 0.0193025028247449]
	TIME [epoch: 8.85 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008711138607410839		[learning rate: 0.0014474]
		[batch 20/20] avg loss: 0.007280121934683541		[learning rate: 0.0014456]
	Learning Rate: 0.00144562
	LOSS [training: 0.00799563027104719 | validation: -0.001182166946311011]
	TIME [epoch: 8.85 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004319708771180556		[learning rate: 0.0014439]
		[batch 20/20] avg loss: 0.004557340238132128		[learning rate: 0.0014421]
	Learning Rate: 0.00144212
	LOSS [training: 0.004438524504656342 | validation: 0.006489859911370553]
	TIME [epoch: 8.87 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003718164165016374		[learning rate: 0.0014404]
		[batch 20/20] avg loss: 0.007584560677294479		[learning rate: 0.0014386]
	Learning Rate: 0.00143862
	LOSS [training: 0.005651362421155427 | validation: -0.004075713029986365]
	TIME [epoch: 8.86 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003732130300921241		[learning rate: 0.0014369]
		[batch 20/20] avg loss: 0.011430002766325813		[learning rate: 0.0014351]
	Learning Rate: 0.00143514
	LOSS [training: 0.0075810665336235265 | validation: 0.025067055155142782]
	TIME [epoch: 8.84 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013440423600454407		[learning rate: 0.0014334]
		[batch 20/20] avg loss: 0.008397713023423787		[learning rate: 0.0014317]
	Learning Rate: 0.00143167
	LOSS [training: 0.010919068311939097 | validation: -0.0018309453033049099]
	TIME [epoch: 8.86 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01176137817159227		[learning rate: 0.0014299]
		[batch 20/20] avg loss: 0.008912135736767492		[learning rate: 0.0014282]
	Learning Rate: 0.0014282
	LOSS [training: 0.010336756954179882 | validation: 0.00593280196857295]
	TIME [epoch: 8.86 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010228041188011728		[learning rate: 0.0014265]
		[batch 20/20] avg loss: 0.005458418589388334		[learning rate: 0.0014247]
	Learning Rate: 0.00142474
	LOSS [training: 0.00784322988870003 | validation: 0.01442037166699584]
	TIME [epoch: 8.88 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017011917880096768		[learning rate: 0.001423]
		[batch 20/20] avg loss: 0.00760654001401692		[learning rate: 0.0014213]
	Learning Rate: 0.00142129
	LOSS [training: 0.012309228947056845 | validation: -0.0018061020088717874]
	TIME [epoch: 8.85 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006842648744657115		[learning rate: 0.0014196]
		[batch 20/20] avg loss: 0.013175093873226787		[learning rate: 0.0014179]
	Learning Rate: 0.00141785
	LOSS [training: 0.01000887130894195 | validation: 0.019883288494897296]
	TIME [epoch: 8.84 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011886035954491492		[learning rate: 0.0014161]
		[batch 20/20] avg loss: 0.014797083550309394		[learning rate: 0.0014144]
	Learning Rate: 0.00141442
	LOSS [training: 0.013341559752400445 | validation: 0.011082978057156065]
	TIME [epoch: 8.84 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013952731369931628		[learning rate: 0.0014127]
		[batch 20/20] avg loss: 0.01022064843124101		[learning rate: 0.001411]
	Learning Rate: 0.001411
	LOSS [training: 0.012086689900586318 | validation: -0.003815277089403723]
	TIME [epoch: 8.85 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005862176550259951		[learning rate: 0.0014093]
		[batch 20/20] avg loss: 0.011694818832172039		[learning rate: 0.0014076]
	Learning Rate: 0.00140758
	LOSS [training: 0.008778497691215994 | validation: 0.01778647650214061]
	TIME [epoch: 8.87 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014904480168099301		[learning rate: 0.0014059]
		[batch 20/20] avg loss: 0.006851316643908728		[learning rate: 0.0014042]
	Learning Rate: 0.00140417
	LOSS [training: 0.010877898406004016 | validation: 0.006950537149717411]
	TIME [epoch: 8.85 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014450309758675669		[learning rate: 0.0014025]
		[batch 20/20] avg loss: 0.02422186743979525		[learning rate: 0.0014008]
	Learning Rate: 0.00140078
	LOSS [training: 0.019336088599235465 | validation: 0.004569374070518211]
	TIME [epoch: 8.85 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021257631820053675		[learning rate: 0.0013991]
		[batch 20/20] avg loss: 0.011690041223907535		[learning rate: 0.0013974]
	Learning Rate: 0.00139738
	LOSS [training: 0.004782139020951084 | validation: -0.0008422999009800075]
	TIME [epoch: 8.85 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004497010368376094		[learning rate: 0.0013957]
		[batch 20/20] avg loss: 0.005387405400406808		[learning rate: 0.001394]
	Learning Rate: 0.001394
	LOSS [training: 0.004942207884391451 | validation: 0.00586116853941082]
	TIME [epoch: 8.87 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008003800341084175		[learning rate: 0.0013923]
		[batch 20/20] avg loss: 0.008288268286765304		[learning rate: 0.0013906]
	Learning Rate: 0.00139063
	LOSS [training: 0.00814603431392474 | validation: 0.0081984002222448]
	TIME [epoch: 8.86 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007511670843270815		[learning rate: 0.0013889]
		[batch 20/20] avg loss: 0.008066500290404982		[learning rate: 0.0013873]
	Learning Rate: 0.00138726
	LOSS [training: 0.007789085566837899 | validation: 0.0012237258106285826]
	TIME [epoch: 8.85 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006614204555133912		[learning rate: 0.0013856]
		[batch 20/20] avg loss: 0.003509798386853854		[learning rate: 0.0013839]
	Learning Rate: 0.0013839
	LOSS [training: 0.005062001470993884 | validation: 0.005338634342983035]
	TIME [epoch: 8.84 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0044331698824026785		[learning rate: 0.0013822]
		[batch 20/20] avg loss: 0.0050886574192475515		[learning rate: 0.0013806]
	Learning Rate: 0.00138055
	LOSS [training: 0.004760913650825115 | validation: -0.0017773603823984202]
	TIME [epoch: 8.85 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004274985908564538		[learning rate: 0.0013789]
		[batch 20/20] avg loss: 0.008821175574357697		[learning rate: 0.0013772]
	Learning Rate: 0.00137721
	LOSS [training: 0.002273094832896579 | validation: 0.0026390463471643614]
	TIME [epoch: 8.87 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00030252163829657075		[learning rate: 0.0013755]
		[batch 20/20] avg loss: 0.010546425963535187		[learning rate: 0.0013739]
	Learning Rate: 0.00137388
	LOSS [training: 0.005121952162619308 | validation: 0.0048939669567575365]
	TIME [epoch: 8.85 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007556573126326358		[learning rate: 0.0013722]
		[batch 20/20] avg loss: 0.00032863556009588636		[learning rate: 0.0013705]
	Learning Rate: 0.00137055
	LOSS [training: 0.0039426043432111224 | validation: 0.007000991258221997]
	TIME [epoch: 8.86 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009427313671383959		[learning rate: 0.0013689]
		[batch 20/20] avg loss: 0.008002502546098932		[learning rate: 0.0013672]
	Learning Rate: 0.00136723
	LOSS [training: 0.008714908108741442 | validation: -0.005751201648140623]
	TIME [epoch: 8.85 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025510586722506737		[learning rate: 0.0013656]
		[batch 20/20] avg loss: 0.014333737787032546		[learning rate: 0.0013639]
	Learning Rate: 0.00136392
	LOSS [training: 0.008442398229641612 | validation: 0.014236062543238039]
	TIME [epoch: 8.85 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007262728201963421		[learning rate: 0.0013623]
		[batch 20/20] avg loss: 0.010572483559203438		[learning rate: 0.0013606]
	Learning Rate: 0.00136062
	LOSS [training: 0.008917605880583431 | validation: 0.0011467930551602148]
	TIME [epoch: 8.86 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031496623254102267		[learning rate: 0.001359]
		[batch 20/20] avg loss: -0.0009259121284580726		[learning rate: 0.0013573]
	Learning Rate: 0.00135733
	LOSS [training: 0.0011118750984760765 | validation: -0.004654690196393394]
	TIME [epoch: 8.85 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031922742861933565		[learning rate: 0.0013557]
		[batch 20/20] avg loss: 0.001564209414899144		[learning rate: 0.001354]
	Learning Rate: 0.00135404
	LOSS [training: 0.00237824185054625 | validation: -0.006638711716956042]
	TIME [epoch: 8.84 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0051287663709056516		[learning rate: 0.0013524]
		[batch 20/20] avg loss: 0.002045766658528056		[learning rate: 0.0013508]
	Learning Rate: 0.00135076
	LOSS [training: 0.0035872665147168538 | validation: 0.004133843302230312]
	TIME [epoch: 8.85 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002981735879825823		[learning rate: 0.0013491]
		[batch 20/20] avg loss: -0.001617953836404987		[learning rate: 0.0013475]
	Learning Rate: 0.00134749
	LOSS [training: 0.0006818910217104178 | validation: -0.0013677532315565307]
	TIME [epoch: 8.87 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003695139786938903		[learning rate: 0.0013459]
		[batch 20/20] avg loss: -0.0046381396502535215		[learning rate: 0.0013442]
	Learning Rate: 0.00134423
	LOSS [training: -0.0021343128357798153 | validation: 0.002750487983848596]
	TIME [epoch: 8.85 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028356648720336457		[learning rate: 0.0013426]
		[batch 20/20] avg loss: 0.0061186694343356215		[learning rate: 0.001341]
	Learning Rate: 0.00134098
	LOSS [training: 0.004477167153184633 | validation: 0.017113783523212772]
	TIME [epoch: 8.79 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005066218744559895		[learning rate: 0.0013394]
		[batch 20/20] avg loss: 0.009840346028609667		[learning rate: 0.0013377]
	Learning Rate: 0.00133773
	LOSS [training: 0.007453282386584779 | validation: -0.0019829264866347805]
	TIME [epoch: 8.85 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021233615934786953		[learning rate: 0.0013361]
		[batch 20/20] avg loss: 0.010768112170217694		[learning rate: 0.0013345]
	Learning Rate: 0.00133449
	LOSS [training: 0.006445736881848195 | validation: 0.005260020452567144]
	TIME [epoch: 8.85 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0076518365353961905		[learning rate: 0.0013329]
		[batch 20/20] avg loss: 0.00871681045258244		[learning rate: 0.0013313]
	Learning Rate: 0.00133126
	LOSS [training: 0.008184323493989317 | validation: 0.00851701220542598]
	TIME [epoch: 8.86 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006495503604233947		[learning rate: 0.0013296]
		[batch 20/20] avg loss: 0.009063807616777315		[learning rate: 0.001328]
	Learning Rate: 0.00132804
	LOSS [training: 0.007779655610505629 | validation: 0.005626188635182694]
	TIME [epoch: 8.87 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011679417989605392		[learning rate: 0.0013264]
		[batch 20/20] avg loss: 0.01408641531433046		[learning rate: 0.0013248]
	Learning Rate: 0.00132482
	LOSS [training: 0.012882916651967924 | validation: 0.0025179928061829573]
	TIME [epoch: 8.85 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0050002140968108955		[learning rate: 0.0013232]
		[batch 20/20] avg loss: 0.005387498306184761		[learning rate: 0.0013216]
	Learning Rate: 0.00132162
	LOSS [training: 0.005193856201497827 | validation: 0.006165875775268861]
	TIME [epoch: 8.86 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014308432594768634		[learning rate: 0.00132]
		[batch 20/20] avg loss: 0.004232957634335979		[learning rate: 0.0013184]
	Learning Rate: 0.00131842
	LOSS [training: 0.0014010571874295576 | validation: 0.0025711000847939607]
	TIME [epoch: 8.85 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021276689154734803		[learning rate: 0.0013168]
		[batch 20/20] avg loss: 0.024201101792776594		[learning rate: 0.0013152]
	Learning Rate: 0.00131522
	LOSS [training: 0.022738895473755693 | validation: 0.023535570827308932]
	TIME [epoch: 8.87 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012958757431012297		[learning rate: 0.0013136]
		[batch 20/20] avg loss: 0.005884619968651826		[learning rate: 0.001312]
	Learning Rate: 0.00131204
	LOSS [training: 0.009421688699832061 | validation: 0.022893351713918278]
	TIME [epoch: 8.85 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01771445605007596		[learning rate: 0.0013105]
		[batch 20/20] avg loss: 0.01477449115211234		[learning rate: 0.0013089]
	Learning Rate: 0.00130886
	LOSS [training: 0.01624447360109415 | validation: 0.0017061637021303388]
	TIME [epoch: 8.86 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00024301454684977694		[learning rate: 0.0013073]
		[batch 20/20] avg loss: 0.003016132794843116		[learning rate: 0.0013057]
	Learning Rate: 0.0013057
	LOSS [training: 0.0016295736708464466 | validation: 8.312443049236762e-05]
	TIME [epoch: 8.83 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005530986003207922		[learning rate: 0.0013041]
		[batch 20/20] avg loss: 0.012676243631325307		[learning rate: 0.0013025]
	Learning Rate: 0.00130254
	LOSS [training: 0.006614671115823051 | validation: 0.009931837430563155]
	TIME [epoch: 8.86 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014434581108189704		[learning rate: 0.001301]
		[batch 20/20] avg loss: 0.001138143743675418		[learning rate: 0.0012994]
	Learning Rate: 0.00129938
	LOSS [training: 0.0012908009272471943 | validation: -0.005860943300704162]
	TIME [epoch: 8.88 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0039075917785810444		[learning rate: 0.0012978]
		[batch 20/20] avg loss: 0.0038780254661567822		[learning rate: 0.0012962]
	Learning Rate: 0.00129624
	LOSS [training: 0.0038928086223689123 | validation: 0.005442444886879117]
	TIME [epoch: 8.86 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015637612856411697		[learning rate: 0.0012947]
		[batch 20/20] avg loss: -0.0006903519495634542		[learning rate: 0.0012931]
	Learning Rate: 0.0012931
	LOSS [training: 0.007473630453424122 | validation: 0.003330869653184076]
	TIME [epoch: 8.79 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010957794395196792		[learning rate: 0.0012915]
		[batch 20/20] avg loss: 0.009181523334200383		[learning rate: 0.00129]
	Learning Rate: 0.00128997
	LOSS [training: 0.010069658864698587 | validation: 0.002999394186782729]
	TIME [epoch: 8.84 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011594611168051152		[learning rate: 0.0012884]
		[batch 20/20] avg loss: 0.002529700983428679		[learning rate: 0.0012868]
	Learning Rate: 0.00128685
	LOSS [training: 0.007062156075739916 | validation: 0.0016106298475811198]
	TIME [epoch: 8.87 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00026710319780454464		[learning rate: 0.0012853]
		[batch 20/20] avg loss: 0.010213314125556587		[learning rate: 0.0012837]
	Learning Rate: 0.00128373
	LOSS [training: 0.004973105463876021 | validation: 0.008632859896043914]
	TIME [epoch: 8.85 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004197211437947582		[learning rate: 0.0012822]
		[batch 20/20] avg loss: 0.0028024681345767957		[learning rate: 0.0012806]
	Learning Rate: 0.00128062
	LOSS [training: 0.0034998397862621893 | validation: 0.0026912100346507227]
	TIME [epoch: 8.84 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013003601159046311		[learning rate: 0.0012791]
		[batch 20/20] avg loss: 0.007667891050489718		[learning rate: 0.0012775]
	Learning Rate: 0.00127752
	LOSS [training: 0.004484125583197175 | validation: 0.007228781094288359]
	TIME [epoch: 8.85 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006230863437311375		[learning rate: 0.001276]
		[batch 20/20] avg loss: 0.012622078300713426		[learning rate: 0.0012744]
	Learning Rate: 0.00127443
	LOSS [training: 0.0094264708690124 | validation: -0.003082883974939067]
	TIME [epoch: 8.86 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00296276158442916		[learning rate: 0.0012729]
		[batch 20/20] avg loss: 0.0020435210575816126		[learning rate: 0.0012713]
	Learning Rate: 0.00127134
	LOSS [training: 0.0025031413210053867 | validation: 9.900575573767458e-05]
	TIME [epoch: 8.89 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032555536513160046		[learning rate: 0.0012698]
		[batch 20/20] avg loss: 0.0038932912066601635		[learning rate: 0.0012683]
	Learning Rate: 0.00126827
	LOSS [training: 0.003574422428988083 | validation: -0.0012218192165737427]
	TIME [epoch: 8.87 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004719041344238707		[learning rate: 0.0012667]
		[batch 20/20] avg loss: 0.004667605156107255		[learning rate: 0.0012652]
	Learning Rate: 0.0012652
	LOSS [training: 0.004693323250172981 | validation: -7.747355069331321e-05]
	TIME [epoch: 8.86 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00016052743820511674		[learning rate: 0.0012637]
		[batch 20/20] avg loss: 0.005195810944161094		[learning rate: 0.0012621]
	Learning Rate: 0.00126213
	LOSS [training: 0.0025176417529779887 | validation: 0.0034675723378692815]
	TIME [epoch: 8.86 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025369552552874353		[learning rate: 0.0012606]
		[batch 20/20] avg loss: 0.007062684296499996		[learning rate: 0.0012591]
	Learning Rate: 0.00125908
	LOSS [training: 0.004799819775893716 | validation: 0.002092548970509062]
	TIME [epoch: 8.86 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003262898351813537		[learning rate: 0.0012576]
		[batch 20/20] avg loss: 0.005784477381508409		[learning rate: 0.001256]
	Learning Rate: 0.00125603
	LOSS [training: 0.004523687866660972 | validation: 0.0059659369675704515]
	TIME [epoch: 8.88 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008698823519188723		[learning rate: 0.0012545]
		[batch 20/20] avg loss: 0.009212823582250056		[learning rate: 0.001253]
	Learning Rate: 0.00125299
	LOSS [training: 0.008955823550719388 | validation: 0.012146426240911522]
	TIME [epoch: 8.85 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0045754285606128525		[learning rate: 0.0012515]
		[batch 20/20] avg loss: 0.0034671081236339572		[learning rate: 0.00125]
	Learning Rate: 0.00124996
	LOSS [training: 0.004021268342123406 | validation: 0.012534831599552796]
	TIME [epoch: 8.86 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01011176643135505		[learning rate: 0.0012484]
		[batch 20/20] avg loss: 0.008058357784185552		[learning rate: 0.0012469]
	Learning Rate: 0.00124693
	LOSS [training: 0.009085062107770301 | validation: 0.006320822914758662]
	TIME [epoch: 8.86 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008198260594556335		[learning rate: 0.0012454]
		[batch 20/20] avg loss: 0.0106416946275062		[learning rate: 0.0012439]
	Learning Rate: 0.00124391
	LOSS [training: 0.009419977611031269 | validation: 0.020834224269071545]
	TIME [epoch: 8.87 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015346655327570166		[learning rate: 0.0012424]
		[batch 20/20] avg loss: 0.0002509134353590196		[learning rate: 0.0012409]
	Learning Rate: 0.0012409
	LOSS [training: 0.007798784381464592 | validation: 0.0007349971833726407]
	TIME [epoch: 8.87 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012195179188335145		[learning rate: 0.0012394]
		[batch 20/20] avg loss: 0.01062096678095348		[learning rate: 0.0012379]
	Learning Rate: 0.0012379
	LOSS [training: 0.011408072984644312 | validation: 0.012476647666993482]
	TIME [epoch: 8.85 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004884872856965495		[learning rate: 0.0012364]
		[batch 20/20] avg loss: 0.0029999576178740333		[learning rate: 0.0012349]
	Learning Rate: 0.0012349
	LOSS [training: 0.003942415237419766 | validation: -0.0014055603199031896]
	TIME [epoch: 8.85 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005595927632635918		[learning rate: 0.0012334]
		[batch 20/20] avg loss: 0.0032381440410310172		[learning rate: 0.0012319]
	Learning Rate: 0.00123191
	LOSS [training: 0.0018988684021473043 | validation: -0.0038269845657233955]
	TIME [epoch: 8.86 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003870253348701698		[learning rate: 0.0012304]
		[batch 20/20] avg loss: 0.006323738939482062		[learning rate: 0.0012289]
	Learning Rate: 0.00122893
	LOSS [training: 0.005096996144091879 | validation: 0.0032649135019549615]
	TIME [epoch: 8.87 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008614416503693548		[learning rate: 0.0012274]
		[batch 20/20] avg loss: 0.011970928990996364		[learning rate: 0.001226]
	Learning Rate: 0.00122595
	LOSS [training: 0.010292672747344958 | validation: -0.0040573939661831]
	TIME [epoch: 8.86 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009462456407873777		[learning rate: 0.0012245]
		[batch 20/20] avg loss: 0.0021175675766412333		[learning rate: 0.001223]
	Learning Rate: 0.00122298
	LOSS [training: 0.005790011992257505 | validation: 0.01421682308409164]
	TIME [epoch: 8.85 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032448550682410023		[learning rate: 0.0012215]
		[batch 20/20] avg loss: -0.00014916427450331978		[learning rate: 0.00122]
	Learning Rate: 0.00122002
	LOSS [training: 0.0015478453968688416 | validation: -0.005231875797233798]
	TIME [epoch: 8.86 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019163644836949017		[learning rate: 0.0012185]
		[batch 20/20] avg loss: -0.0050809726865055594		[learning rate: 0.0012171]
	Learning Rate: 0.00121707
	LOSS [training: -0.0034986685851002308 | validation: -0.0037737863422088835]
	TIME [epoch: 8.84 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007796103882675337		[learning rate: 0.0012156]
		[batch 20/20] avg loss: 0.006365668764940992		[learning rate: 0.0012141]
	Learning Rate: 0.00121412
	LOSS [training: 0.007080886323808164 | validation: -0.00493763734490586]
	TIME [epoch: 8.88 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036210077193662795		[learning rate: 0.0012127]
		[batch 20/20] avg loss: 0.003730965619401793		[learning rate: 0.0012112]
	Learning Rate: 0.00121119
	LOSS [training: 0.003675986669384036 | validation: 0.003243848140429252]
	TIME [epoch: 8.86 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034471573508366915		[learning rate: 0.0012097]
		[batch 20/20] avg loss: 0.01191050923082114		[learning rate: 0.0012083]
	Learning Rate: 0.00120825
	LOSS [training: 0.007678833290828915 | validation: 0.029198541504864103]
	TIME [epoch: 8.85 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025139532693546834		[learning rate: 0.0012068]
		[batch 20/20] avg loss: 0.012033476787925067		[learning rate: 0.0012053]
	Learning Rate: 0.00120533
	LOSS [training: 0.01858650474073595 | validation: 0.019544486410716464]
	TIME [epoch: 8.85 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0107152688807074		[learning rate: 0.0012039]
		[batch 20/20] avg loss: 0.00378733337659541		[learning rate: 0.0012024]
	Learning Rate: 0.00120241
	LOSS [training: 0.007251301128651405 | validation: 0.01665038075318742]
	TIME [epoch: 8.85 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036800242124168893		[learning rate: 0.001201]
		[batch 20/20] avg loss: 0.010783889240594414		[learning rate: 0.0011995]
	Learning Rate: 0.0011995
	LOSS [training: 0.00723195672650565 | validation: -0.008610905283417073]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_975.pth
	Model improved!!!
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002470267916615887		[learning rate: 0.001198]
		[batch 20/20] avg loss: 0.005666066816561987		[learning rate: 0.0011966]
	Learning Rate: 0.0011966
	LOSS [training: 0.004068167366588937 | validation: -0.005210771114827787]
	TIME [epoch: 8.85 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031698317058059073		[learning rate: 0.0011951]
		[batch 20/20] avg loss: 0.006085445323418655		[learning rate: 0.0011937]
	Learning Rate: 0.0011937
	LOSS [training: 0.0046276385146122815 | validation: 0.0032218091818426928]
	TIME [epoch: 8.85 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014555950819151306		[learning rate: 0.0011923]
		[batch 20/20] avg loss: 0.0015767760278681167		[learning rate: 0.0011908]
	Learning Rate: 0.00119081
	LOSS [training: 0.008066363423509712 | validation: 0.012418854232880302]
	TIME [epoch: 8.85 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003793489929317191		[learning rate: 0.0011894]
		[batch 20/20] avg loss: 0.0029707571595089955		[learning rate: 0.0011879]
	Learning Rate: 0.00118793
	LOSS [training: 0.003382123544413093 | validation: -0.0044548291636339705]
	TIME [epoch: 8.87 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002447860744316465		[learning rate: 0.0011865]
		[batch 20/20] avg loss: -0.0017698175519544538		[learning rate: 0.0011851]
	Learning Rate: 0.00118505
	LOSS [training: 0.00033902159618100556 | validation: -0.002272509821078624]
	TIME [epoch: 8.85 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01055073356810829		[learning rate: 0.0011836]
		[batch 20/20] avg loss: 0.011637485897285723		[learning rate: 0.0011822]
	Learning Rate: 0.00118218
	LOSS [training: 0.011094109732697005 | validation: 0.0060243353019289415]
	TIME [epoch: 8.86 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010515011267936188		[learning rate: 0.0011807]
		[batch 20/20] avg loss: 0.0148381754411691		[learning rate: 0.0011793]
	Learning Rate: 0.00117932
	LOSS [training: 0.012676593354552642 | validation: 0.001010646171841228]
	TIME [epoch: 8.85 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004253098689149527		[learning rate: 0.0011779]
		[batch 20/20] avg loss: 0.025350253869317513		[learning rate: 0.0011765]
	Learning Rate: 0.00117646
	LOSS [training: 0.01480167627923352 | validation: 0.051849926404529664]
	TIME [epoch: 8.85 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032406258089845444		[learning rate: 0.001175]
		[batch 20/20] avg loss: 0.016107189986914173		[learning rate: 0.0011736]
	Learning Rate: 0.00117362
	LOSS [training: 0.02425672403837981 | validation: -0.005708095940414609]
	TIME [epoch: 8.87 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000258529783710083		[learning rate: 0.0011722]
		[batch 20/20] avg loss: 0.013182330479480869		[learning rate: 0.0011708]
	Learning Rate: 0.00117078
	LOSS [training: 0.006720430131595475 | validation: 0.011517017343055661]
	TIME [epoch: 8.85 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009209830718872495		[learning rate: 0.0011694]
		[batch 20/20] avg loss: 0.005440670979506597		[learning rate: 0.0011679]
	Learning Rate: 0.00116794
	LOSS [training: 0.007325250849189548 | validation: -0.005535315213436401]
	TIME [epoch: 8.85 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015656911689816097		[learning rate: 0.0011665]
		[batch 20/20] avg loss: 0.0009175305981701179		[learning rate: 0.0011651]
	Learning Rate: 0.00116511
	LOSS [training: -0.00032408028540574594 | validation: -0.003508988850274392]
	TIME [epoch: 8.84 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011543449824777006		[learning rate: 0.0011637]
		[batch 20/20] avg loss: 0.007349579931360123		[learning rate: 0.0011623]
	Learning Rate: 0.00116229
	LOSS [training: 0.004251962456918912 | validation: 0.010487696318869004]
	TIME [epoch: 8.85 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007216240749027036		[learning rate: 0.0011609]
		[batch 20/20] avg loss: 0.007137376502736587		[learning rate: 0.0011595]
	Learning Rate: 0.00115948
	LOSS [training: 0.00717680862588181 | validation: 0.005346861786354021]
	TIME [epoch: 8.87 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016086582775485584		[learning rate: 0.0011581]
		[batch 20/20] avg loss: 0.011772552858352337		[learning rate: 0.0011567]
	Learning Rate: 0.00115667
	LOSS [training: 0.013929567816918962 | validation: 0.012807500366474696]
	TIME [epoch: 8.85 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007051054452394794		[learning rate: 0.0011553]
		[batch 20/20] avg loss: 0.013649326513879323		[learning rate: 0.0011539]
	Learning Rate: 0.00115387
	LOSS [training: 0.010350190483137057 | validation: 0.014566849078430064]
	TIME [epoch: 8.85 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018687129127221654		[learning rate: 0.0011525]
		[batch 20/20] avg loss: 0.013095496514980029		[learning rate: 0.0011511]
	Learning Rate: 0.00115108
	LOSS [training: 0.01589131282110084 | validation: 0.012947552053575895]
	TIME [epoch: 8.84 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010797265122549216		[learning rate: 0.0011497]
		[batch 20/20] avg loss: 0.01273452074355294		[learning rate: 0.0011483]
	Learning Rate: 0.00114829
	LOSS [training: 0.011765892933051079 | validation: -0.0005152715770303669]
	TIME [epoch: 8.88 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0043730705287951584		[learning rate: 0.0011469]
		[batch 20/20] avg loss: 0.012329791465444287		[learning rate: 0.0011455]
	Learning Rate: 0.00114551
	LOSS [training: 0.008351430997119722 | validation: -0.0038294723115404944]
	TIME [epoch: 8.87 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026255605802430475		[learning rate: 0.0011441]
		[batch 20/20] avg loss: 0.012621810926240536		[learning rate: 0.0011427]
	Learning Rate: 0.00114274
	LOSS [training: 0.007623685753241792 | validation: 0.008789464119420776]
	TIME [epoch: 8.85 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005437367558092284		[learning rate: 0.0011414]
		[batch 20/20] avg loss: 0.005502219377023376		[learning rate: 0.00114]
	Learning Rate: 0.00113997
	LOSS [training: 0.005469793467557831 | validation: 0.006883469475732142]
	TIME [epoch: 8.85 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005790554578018128		[learning rate: 0.0011386]
		[batch 20/20] avg loss: 0.01076474245002908		[learning rate: 0.0011372]
	Learning Rate: 0.00113721
	LOSS [training: 0.008277648514023605 | validation: -0.006246577616069025]
	TIME [epoch: 8.84 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034201217881999453		[learning rate: 0.0011358]
		[batch 20/20] avg loss: 0.0011575461626972675		[learning rate: 0.0011345]
	Learning Rate: 0.00113446
	LOSS [training: 0.0022888339754486065 | validation: 0.000264354541239061]
	TIME [epoch: 8.88 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00483253461609911		[learning rate: 0.0011331]
		[batch 20/20] avg loss: 0.0041786559016114885		[learning rate: 0.0011317]
	Learning Rate: 0.00113171
	LOSS [training: 0.0045055952588552994 | validation: -0.00043000150484409226]
	TIME [epoch: 8.85 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036951009897668906		[learning rate: 0.0011303]
		[batch 20/20] avg loss: -0.0028018468371176714		[learning rate: 0.001129]
	Learning Rate: 0.00112897
	LOSS [training: 0.0004466270763246103 | validation: 0.0011597252064729211]
	TIME [epoch: 8.85 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008251189849401857		[learning rate: 0.0011276]
		[batch 20/20] avg loss: 0.009592430012321549		[learning rate: 0.0011262]
	Learning Rate: 0.00112624
	LOSS [training: 0.008921809930861705 | validation: 0.02162766321907799]
	TIME [epoch: 8.85 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008032289310597148		[learning rate: 0.0011249]
		[batch 20/20] avg loss: 0.011195290503791058		[learning rate: 0.0011235]
	Learning Rate: 0.00112352
	LOSS [training: 0.0096137899071941 | validation: 0.00905391244207867]
	TIME [epoch: 8.85 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008244520780503998		[learning rate: 0.0011222]
		[batch 20/20] avg loss: 0.0035982824417402277		[learning rate: 0.0011208]
	Learning Rate: 0.0011208
	LOSS [training: 0.005921401611122113 | validation: 0.004821327688616936]
	TIME [epoch: 8.87 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0058773589944646955		[learning rate: 0.0011194]
		[batch 20/20] avg loss: 0.002045955362398515		[learning rate: 0.0011181]
	Learning Rate: 0.00111808
	LOSS [training: 0.003961657178431607 | validation: -0.004704175639692686]
	TIME [epoch: 8.86 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010975644211048827		[learning rate: 0.0011167]
		[batch 20/20] avg loss: 0.011371504774067825		[learning rate: 0.0011154]
	Learning Rate: 0.00111538
	LOSS [training: 0.011173574492558326 | validation: 0.0015033148295578872]
	TIME [epoch: 8.86 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008704848916195405		[learning rate: 0.001114]
		[batch 20/20] avg loss: 0.00857765587904207		[learning rate: 0.0011127]
	Learning Rate: 0.00111268
	LOSS [training: 0.008641252397618737 | validation: 0.00511565972590153]
	TIME [epoch: 8.85 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016249770253941677		[learning rate: 0.0011113]
		[batch 20/20] avg loss: 0.005471787427589737		[learning rate: 0.00111]
	Learning Rate: 0.00110998
	LOSS [training: 0.010860778840765707 | validation: -0.0013807936845752489]
	TIME [epoch: 8.88 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024039834929854833		[learning rate: 0.0011086]
		[batch 20/20] avg loss: 0.005724999442989071		[learning rate: 0.0011073]
	Learning Rate: 0.00110729
	LOSS [training: 0.0040644914679872765 | validation: 0.0015600535998225057]
	TIME [epoch: 8.87 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006125560743703619		[learning rate: 0.001106]
		[batch 20/20] avg loss: 0.011850639200422132		[learning rate: 0.0011046]
	Learning Rate: 0.00110461
	LOSS [training: 0.008988099972062876 | validation: -0.003395549470164699]
	TIME [epoch: 8.85 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005197343795391128		[learning rate: 0.0011033]
		[batch 20/20] avg loss: 0.0009918548504456797		[learning rate: 0.0011019]
	Learning Rate: 0.00110194
	LOSS [training: 0.0030945993229184047 | validation: -0.000433552584552683]
	TIME [epoch: 8.86 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023480299417062154		[learning rate: 0.0011006]
		[batch 20/20] avg loss: -0.002460995544940045		[learning rate: 0.0010993]
	Learning Rate: 0.00109927
	LOSS [training: -5.648280161691492e-05 | validation: -0.00836479078078844]
	TIME [epoch: 8.86 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037813267667293924		[learning rate: 0.0010979]
		[batch 20/20] avg loss: 0.008315351284739026		[learning rate: 0.0010966]
	Learning Rate: 0.00109661
	LOSS [training: 0.00604833902573421 | validation: 0.009151267298061795]
	TIME [epoch: 8.87 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006194286894705273		[learning rate: 0.0010953]
		[batch 20/20] avg loss: 0.0012026991398313698		[learning rate: 0.001094]
	Learning Rate: 0.00109396
	LOSS [training: 0.0002916352251804212 | validation: -0.0004246371799710195]
	TIME [epoch: 8.84 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004423045405499373		[learning rate: 0.0010926]
		[batch 20/20] avg loss: 0.00309537007577603		[learning rate: 0.0010913]
	Learning Rate: 0.00109131
	LOSS [training: 0.003759207740637702 | validation: 0.006827793677551671]
	TIME [epoch: 8.85 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006030918629410116		[learning rate: 0.00109]
		[batch 20/20] avg loss: 0.007338469097227534		[learning rate: 0.0010887]
	Learning Rate: 0.00108867
	LOSS [training: 0.006684693863318826 | validation: 0.0006094406347513135]
	TIME [epoch: 8.85 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009211723228754796		[learning rate: 0.0010873]
		[batch 20/20] avg loss: 0.0028689909630320006		[learning rate: 0.001086]
	Learning Rate: 0.00108603
	LOSS [training: 0.006040357095893399 | validation: 0.002371153090460813]
	TIME [epoch: 8.84 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003935000735178482		[learning rate: 0.0010847]
		[batch 20/20] avg loss: 0.011836488517320847		[learning rate: 0.0010834]
	Learning Rate: 0.0010834
	LOSS [training: 0.007885744626249665 | validation: 0.004430118644030094]
	TIME [epoch: 8.87 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005870222902325099		[learning rate: 0.0010821]
		[batch 20/20] avg loss: 0.008423176659983805		[learning rate: 0.0010808]
	Learning Rate: 0.00108078
	LOSS [training: 0.007146699781154453 | validation: 0.0075293804274859395]
	TIME [epoch: 8.86 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012738177816181981		[learning rate: 0.0010795]
		[batch 20/20] avg loss: 0.007296792736148602		[learning rate: 0.0010782]
	Learning Rate: 0.00107816
	LOSS [training: 0.010017485276165288 | validation: -0.006279648670199443]
	TIME [epoch: 8.85 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005652709213562533		[learning rate: 0.0010769]
		[batch 20/20] avg loss: 0.002932111870505192		[learning rate: 0.0010756]
	Learning Rate: 0.00107555
	LOSS [training: 0.004292410542033863 | validation: -0.002799438419398677]
	TIME [epoch: 8.85 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00404574218169037		[learning rate: 0.0010742]
		[batch 20/20] avg loss: 0.004396163467282582		[learning rate: 0.0010729]
	Learning Rate: 0.00107295
	LOSS [training: 0.004220952824486475 | validation: -0.014568084188732044]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_1021.pth
	Model improved!!!
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031139088257187445		[learning rate: 0.0010716]
		[batch 20/20] avg loss: 0.003322288233275037		[learning rate: 0.0010704]
	Learning Rate: 0.00107035
	LOSS [training: 0.00010418970377814671 | validation: -0.009719306245270636]
	TIME [epoch: 8.89 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009247677133878172		[learning rate: 0.0010691]
		[batch 20/20] avg loss: 0.006646090641213782		[learning rate: 0.0010678]
	Learning Rate: 0.00106776
	LOSS [training: 0.00794688388754598 | validation: -0.0003743044555963274]
	TIME [epoch: 8.87 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006708381644883638		[learning rate: 0.0010665]
		[batch 20/20] avg loss: 0.015383191696417595		[learning rate: 0.0010652]
	Learning Rate: 0.00106518
	LOSS [training: 0.00802701493045298 | validation: 0.009517953419982376]
	TIME [epoch: 8.87 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013095075951782909		[learning rate: 0.0010639]
		[batch 20/20] avg loss: 0.015812612110923118		[learning rate: 0.0010626]
	Learning Rate: 0.0010626
	LOSS [training: 0.014453844031353014 | validation: 0.0202389288300443]
	TIME [epoch: 8.86 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02717904485767253		[learning rate: 0.0010613]
		[batch 20/20] avg loss: 0.007291401170309432		[learning rate: 0.00106]
	Learning Rate: 0.00106002
	LOSS [training: 0.017235223013990983 | validation: 0.00014766392054072776]
	TIME [epoch: 8.88 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016681445529623934		[learning rate: 0.0010587]
		[batch 20/20] avg loss: 0.001155590600696548		[learning rate: 0.0010575]
	Learning Rate: 0.00105746
	LOSS [training: 0.001411867576829471 | validation: 0.007752945004429279]
	TIME [epoch: 8.87 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010925113337373707		[learning rate: 0.0010562]
		[batch 20/20] avg loss: 0.00010218219556078056		[learning rate: 0.0010549]
	Learning Rate: 0.0010549
	LOSS [training: 0.005513647766467244 | validation: -0.006022335442468691]
	TIME [epoch: 8.87 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007560404340024153		[learning rate: 0.0010536]
		[batch 20/20] avg loss: -0.0018412751117399128		[learning rate: 0.0010523]
	Learning Rate: 0.00105234
	LOSS [training: 0.00285956461414212 | validation: -0.014717151627010283]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_1029.pth
	Model improved!!!
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027248944174930985		[learning rate: 0.0010511]
		[batch 20/20] avg loss: 0.0016919467549821958		[learning rate: 0.0010498]
	Learning Rate: 0.0010498
	LOSS [training: 0.002208420586237647 | validation: 0.00636533622268806]
	TIME [epoch: 8.86 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00474113471200877		[learning rate: 0.0010485]
		[batch 20/20] avg loss: -0.0007635644643537349		[learning rate: 0.0010473]
	Learning Rate: 0.00104726
	LOSS [training: -0.002752349588181253 | validation: -0.012845462820723486]
	TIME [epoch: 8.88 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013499878633265751		[learning rate: 0.001046]
		[batch 20/20] avg loss: 0.008635828650275838		[learning rate: 0.0010447]
	Learning Rate: 0.00104472
	LOSS [training: 0.004992908256801206 | validation: -0.00048142573580263145]
	TIME [epoch: 8.86 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017848960401841058		[learning rate: 0.0010435]
		[batch 20/20] avg loss: -0.0023446822599623913		[learning rate: 0.0010422]
	Learning Rate: 0.00104219
	LOSS [training: -0.0020647891500732477 | validation: -2.770001464316523e-05]
	TIME [epoch: 8.86 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026295511293529888		[learning rate: 0.0010409]
		[batch 20/20] avg loss: 0.0014055839478209324		[learning rate: 0.0010397]
	Learning Rate: 0.00103967
	LOSS [training: -0.0006119835907660293 | validation: -0.003552164007412495]
	TIME [epoch: 8.85 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004745860284776112		[learning rate: 0.0010384]
		[batch 20/20] avg loss: -0.0043431313040304936		[learning rate: 0.0010372]
	Learning Rate: 0.00103715
	LOSS [training: 0.00020136449037280903 | validation: 0.005839002445964338]
	TIME [epoch: 8.86 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0049103328910114746		[learning rate: 0.0010359]
		[batch 20/20] avg loss: 0.005635745346210636		[learning rate: 0.0010346]
	Learning Rate: 0.00103464
	LOSS [training: 0.0052730391186110535 | validation: 0.0009598900661836664]
	TIME [epoch: 8.88 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023495016240221507		[learning rate: 0.0010334]
		[batch 20/20] avg loss: 0.0034787836829715964		[learning rate: 0.0010321]
	Learning Rate: 0.00103214
	LOSS [training: 0.0029141426534968736 | validation: 0.009786320432835605]
	TIME [epoch: 8.86 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012358904357969876		[learning rate: 0.0010309]
		[batch 20/20] avg loss: 0.005210104775021633		[learning rate: 0.0010296]
	Learning Rate: 0.00102964
	LOSS [training: 0.008784504566495753 | validation: 0.009229371564832294]
	TIME [epoch: 8.85 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012536437372154064		[learning rate: 0.0010284]
		[batch 20/20] avg loss: 0.007855604926168925		[learning rate: 0.0010271]
	Learning Rate: 0.00102714
	LOSS [training: 0.010196021149161495 | validation: -0.00036200589590479066]
	TIME [epoch: 8.86 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004130832588411876		[learning rate: 0.0010259]
		[batch 20/20] avg loss: 0.004741176559529898		[learning rate: 0.0010247]
	Learning Rate: 0.00102466
	LOSS [training: 0.004436004573970888 | validation: -0.0055633470677058814]
	TIME [epoch: 8.87 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011541336281729967		[learning rate: 0.0010234]
		[batch 20/20] avg loss: 0.007107038381748506		[learning rate: 0.0010222]
	Learning Rate: 0.00102218
	LOSS [training: 0.009324187331739236 | validation: -0.0020074480976743705]
	TIME [epoch: 8.87 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005914152451619192		[learning rate: 0.0010209]
		[batch 20/20] avg loss: 0.018347092780280273		[learning rate: 0.0010197]
	Learning Rate: 0.0010197
	LOSS [training: 0.012130622615949733 | validation: 0.0028689961127561763]
	TIME [epoch: 8.85 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00136312797258066		[learning rate: 0.0010185]
		[batch 20/20] avg loss: 0.0005614466446613297		[learning rate: 0.0010172]
	Learning Rate: 0.00101723
	LOSS [training: 0.0009622873086209949 | validation: -0.0002192850013315738]
	TIME [epoch: 8.85 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007011550782711297		[learning rate: 0.001016]
		[batch 20/20] avg loss: 0.007404692903467425		[learning rate: 0.0010148]
	Learning Rate: 0.00101477
	LOSS [training: 0.007208121843089362 | validation: -0.006161407743861456]
	TIME [epoch: 8.87 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003367485964645235		[learning rate: 0.0010135]
		[batch 20/20] avg loss: 0.009113759876628092		[learning rate: 0.0010123]
	Learning Rate: 0.00101232
	LOSS [training: 0.006240622920636663 | validation: 0.005594107576985837]
	TIME [epoch: 8.88 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010524010977240992		[learning rate: 0.0010111]
		[batch 20/20] avg loss: 0.016083706934377932		[learning rate: 0.0010099]
	Learning Rate: 0.00100986
	LOSS [training: 0.013303858955809465 | validation: 0.0003904734018182423]
	TIME [epoch: 8.87 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003606493130518344		[learning rate: 0.0010086]
		[batch 20/20] avg loss: 0.003167218586222932		[learning rate: 0.0010074]
	Learning Rate: 0.00100742
	LOSS [training: -0.00021963727214770564 | validation: -0.004824851318982835]
	TIME [epoch: 8.86 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028023877736898786		[learning rate: 0.0010062]
		[batch 20/20] avg loss: 0.0030701095286119243		[learning rate: 0.001005]
	Learning Rate: 0.00100498
	LOSS [training: 0.002936248651150902 | validation: -0.00048037615762483205]
	TIME [epoch: 8.87 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008887847463121244		[learning rate: 0.0010038]
		[batch 20/20] avg loss: 0.0036701753202711407		[learning rate: 0.0010025]
	Learning Rate: 0.00100255
	LOSS [training: 0.00627901139169619 | validation: -0.00011303410121539178]
	TIME [epoch: 8.86 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00017590069449042404		[learning rate: 0.0010013]
		[batch 20/20] avg loss: 0.0028548970449791284		[learning rate: 0.0010001]
	Learning Rate: 0.00100012
	LOSS [training: 0.001339498175244352 | validation: -0.003969654812709007]
	TIME [epoch: 8.89 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032151620130835635		[learning rate: 0.00099891]
		[batch 20/20] avg loss: 0.0028110707521925293		[learning rate: 0.0009977]
	Learning Rate: 0.0009977
	LOSS [training: -0.0002020456304455169 | validation: -0.0009324148689473251]
	TIME [epoch: 8.86 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00861754126358537		[learning rate: 0.00099649]
		[batch 20/20] avg loss: 0.006921662865067062		[learning rate: 0.00099528]
	Learning Rate: 0.000995285
	LOSS [training: 0.007769602064326216 | validation: -0.003023686204428547]
	TIME [epoch: 8.86 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00998496826059244		[learning rate: 0.00099408]
		[batch 20/20] avg loss: 0.003116153222153668		[learning rate: 0.00099288]
	Learning Rate: 0.000992875
	LOSS [training: 0.006550560741373053 | validation: -0.0031081490459436982]
	TIME [epoch: 8.86 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008767586607121392		[learning rate: 0.00099167]
		[batch 20/20] avg loss: -0.00338795358298722		[learning rate: 0.00099047]
	Learning Rate: 0.000990472
	LOSS [training: -0.0012555974611375398 | validation: -0.010893488176703003]
	TIME [epoch: 8.87 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004068058801742536		[learning rate: 0.00098927]
		[batch 20/20] avg loss: 0.007141967253246206		[learning rate: 0.00098807]
	Learning Rate: 0.000988074
	LOSS [training: 0.0037743865667102298 | validation: -0.004695753699421825]
	TIME [epoch: 8.87 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010272511235954792		[learning rate: 0.00098688]
		[batch 20/20] avg loss: 0.002046841322381093		[learning rate: 0.00098568]
	Learning Rate: 0.000985682
	LOSS [training: 0.006159676279167943 | validation: -0.009054458188623226]
	TIME [epoch: 8.87 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00514718729006732		[learning rate: 0.00098449]
		[batch 20/20] avg loss: 0.01105721244946759		[learning rate: 0.0009833]
	Learning Rate: 0.000983296
	LOSS [training: 0.002955012579700136 | validation: -0.003204270363321865]
	TIME [epoch: 8.86 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004387787874649522		[learning rate: 0.0009821]
		[batch 20/20] avg loss: 0.00446559512986878		[learning rate: 0.00098092]
	Learning Rate: 0.000980916
	LOSS [training: 0.0024521869586668654 | validation: -0.003919690773956156]
	TIME [epoch: 8.87 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010786834153495648		[learning rate: 0.00097973]
		[batch 20/20] avg loss: 0.004611032131463963		[learning rate: 0.00097854]
	Learning Rate: 0.000978541
	LOSS [training: 0.0028448577734067637 | validation: -0.0028569060783811516]
	TIME [epoch: 8.87 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00346390844733573		[learning rate: 0.00097736]
		[batch 20/20] avg loss: -0.0013292925914874846		[learning rate: 0.00097617]
	Learning Rate: 0.000976172
	LOSS [training: 0.0010673079279241225 | validation: 0.0045209711541924925]
	TIME [epoch: 8.87 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005468103539257161		[learning rate: 0.00097499]
		[batch 20/20] avg loss: -0.0043428991426002405		[learning rate: 0.00097381]
	Learning Rate: 0.000973809
	LOSS [training: 0.0005626021983284604 | validation: -0.008315735992366291]
	TIME [epoch: 8.85 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002159415939072834		[learning rate: 0.00097263]
		[batch 20/20] avg loss: 0.007434638987066526		[learning rate: 0.00097145]
	Learning Rate: 0.000971451
	LOSS [training: 0.0047970274630696795 | validation: 0.0064307670401729]
	TIME [epoch: 8.86 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000803948180396808		[learning rate: 0.00097027]
		[batch 20/20] avg loss: -0.003991259575410032		[learning rate: 0.0009691]
	Learning Rate: 0.0009691
	LOSS [training: -0.0015936556975066115 | validation: 0.00021571664726219948]
	TIME [epoch: 8.86 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00867138633995377		[learning rate: 0.00096793]
		[batch 20/20] avg loss: 0.004426668257269952		[learning rate: 0.00096675]
	Learning Rate: 0.000966754
	LOSS [training: 0.006549027298611861 | validation: 0.0020477851599616037]
	TIME [epoch: 8.87 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008429528534417474		[learning rate: 0.00096558]
		[batch 20/20] avg loss: 0.020935103274218117		[learning rate: 0.00096441]
	Learning Rate: 0.000964413
	LOSS [training: 0.014682315904317799 | validation: 0.0022841213501238787]
	TIME [epoch: 8.86 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016058043121447954		[learning rate: 0.00096325]
		[batch 20/20] avg loss: 0.01070548579177825		[learning rate: 0.00096208]
	Learning Rate: 0.000962079
	LOSS [training: 0.006155645051961521 | validation: 0.01734545463926976]
	TIME [epoch: 8.85 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014104338932381314		[learning rate: 0.00096091]
		[batch 20/20] avg loss: 0.005849005655199626		[learning rate: 0.00095975]
	Learning Rate: 0.00095975
	LOSS [training: 0.00997667229379047 | validation: 0.01069517824433115]
	TIME [epoch: 8.85 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004021398717669709		[learning rate: 0.00095859]
		[batch 20/20] avg loss: 0.003868793539429092		[learning rate: 0.00095743]
	Learning Rate: 0.000957426
	LOSS [training: 0.003945096128549401 | validation: -0.0036209050962111656]
	TIME [epoch: 8.86 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002803507543527009		[learning rate: 0.00095627]
		[batch 20/20] avg loss: -0.00035581761341666113		[learning rate: 0.00095511]
	Learning Rate: 0.000955108
	LOSS [training: 0.0012238449650551742 | validation: -0.005503965492074828]
	TIME [epoch: 8.87 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0001077362448423597		[learning rate: 0.00095395]
		[batch 20/20] avg loss: -0.003755445170980706		[learning rate: 0.0009528]
	Learning Rate: 0.000952796
	LOSS [training: -0.0019315907079115325 | validation: -0.0016621434760999954]
	TIME [epoch: 8.86 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019172691880860592		[learning rate: 0.00095164]
		[batch 20/20] avg loss: -0.0037403502843659777		[learning rate: 0.00095049]
	Learning Rate: 0.00095049
	LOSS [training: -0.0009115405481399592 | validation: -0.002110954331345019]
	TIME [epoch: 8.85 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00138684110167769		[learning rate: 0.00094934]
		[batch 20/20] avg loss: 0.0030319898099417366		[learning rate: 0.00094819]
	Learning Rate: 0.000948189
	LOSS [training: 0.0022094154558097135 | validation: -0.004626319650969369]
	TIME [epoch: 8.85 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005509679551095244		[learning rate: 0.00094704]
		[batch 20/20] avg loss: 0.00023250752871920933		[learning rate: 0.00094589]
	Learning Rate: 0.000945893
	LOSS [training: 0.0028710935399072265 | validation: 0.0010550180811917007]
	TIME [epoch: 8.87 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003688800756130041		[learning rate: 0.00094475]
		[batch 20/20] avg loss: 0.006283533767028728		[learning rate: 0.0009436]
	Learning Rate: 0.000943603
	LOSS [training: 0.004986167261579384 | validation: 0.004178553701190183]
	TIME [epoch: 8.85 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035329261908860626		[learning rate: 0.00094246]
		[batch 20/20] avg loss: 0.000911183362194878		[learning rate: 0.00094132]
	Learning Rate: 0.000941319
	LOSS [training: -0.001310871414345593 | validation: 0.006426768508138988]
	TIME [epoch: 8.86 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025367110072001845		[learning rate: 0.00094018]
		[batch 20/20] avg loss: 0.0006683090585333385		[learning rate: 0.00093904]
	Learning Rate: 0.00093904
	LOSS [training: -0.0009342009743334238 | validation: -0.007577078397348353]
	TIME [epoch: 8.85 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0046187959600208495		[learning rate: 0.0009379]
		[batch 20/20] avg loss: -0.0002695064605044206		[learning rate: 0.00093677]
	Learning Rate: 0.000936767
	LOSS [training: 0.002174644749758215 | validation: -0.0058610981313528855]
	TIME [epoch: 8.86 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018723293527927697		[learning rate: 0.00093563]
		[batch 20/20] avg loss: -0.0028730692476237095		[learning rate: 0.0009345]
	Learning Rate: 0.000934499
	LOSS [training: -0.0005003699474154704 | validation: 0.00048057324801401534]
	TIME [epoch: 8.88 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00038718912072727136		[learning rate: 0.00093337]
		[batch 20/20] avg loss: 0.00823296125315743		[learning rate: 0.00093224]
	Learning Rate: 0.000932237
	LOSS [training: 0.003922886066215079 | validation: 0.014749641798926304]
	TIME [epoch: 8.85 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006029225721515528		[learning rate: 0.00093111]
		[batch 20/20] avg loss: 0.010163470059096447		[learning rate: 0.00092998]
	Learning Rate: 0.00092998
	LOSS [training: 0.008096347890305989 | validation: 0.007000337047463663]
	TIME [epoch: 8.86 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0046852957722017105		[learning rate: 0.00092885]
		[batch 20/20] avg loss: 0.005560440699444299		[learning rate: 0.00092773]
	Learning Rate: 0.000927729
	LOSS [training: 0.005122868235823005 | validation: 0.0033519511392941987]
	TIME [epoch: 8.85 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020355080800057144		[learning rate: 0.00092661]
		[batch 20/20] avg loss: 0.012178878810581722		[learning rate: 0.00092548]
	Learning Rate: 0.000925483
	LOSS [training: 0.007107193445293719 | validation: -0.002713910838607968]
	TIME [epoch: 8.85 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002798683172952017		[learning rate: 0.00092436]
		[batch 20/20] avg loss: 0.007380149508537631		[learning rate: 0.00092324]
	Learning Rate: 0.000923243
	LOSS [training: 0.002290733167792807 | validation: -0.004894356355621363]
	TIME [epoch: 8.88 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00470367825668547		[learning rate: 0.00092212]
		[batch 20/20] avg loss: -0.003243648242273106		[learning rate: 0.00092101]
	Learning Rate: 0.000921008
	LOSS [training: -0.003973663249479288 | validation: -0.002185105575188884]
	TIME [epoch: 8.85 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00386307356879002		[learning rate: 0.00091989]
		[batch 20/20] avg loss: 0.0059060128436191645		[learning rate: 0.00091878]
	Learning Rate: 0.000918778
	LOSS [training: 0.001021469637414572 | validation: 0.009406463898024964]
	TIME [epoch: 8.86 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008446329048502307		[learning rate: 0.00091767]
		[batch 20/20] avg loss: -0.0056235197496980124		[learning rate: 0.00091655]
	Learning Rate: 0.000916554
	LOSS [training: 0.0014114046494021466 | validation: -0.010039649216928105]
	TIME [epoch: 8.86 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023749049797615043		[learning rate: 0.00091544]
		[batch 20/20] avg loss: 0.004286881679869468		[learning rate: 0.00091433]
	Learning Rate: 0.000914335
	LOSS [training: 0.0033308933298154863 | validation: 0.00031477937967220367]
	TIME [epoch: 8.86 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006444665196333275		[learning rate: 0.00091323]
		[batch 20/20] avg loss: 0.0026815313999700506		[learning rate: 0.00091212]
	Learning Rate: 0.000912121
	LOSS [training: 0.0010185324401683613 | validation: 0.003999278301568586]
	TIME [epoch: 8.86 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000262902630296551		[learning rate: 0.00091102]
		[batch 20/20] avg loss: -0.0015314366300049158		[learning rate: 0.00090991]
	Learning Rate: 0.000909913
	LOSS [training: -0.0006342669998541824 | validation: -0.004404226912220752]
	TIME [epoch: 8.86 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005020030082955472		[learning rate: 0.00090881]
		[batch 20/20] avg loss: -0.0004542821908149693		[learning rate: 0.00090771]
	Learning Rate: 0.00090771
	LOSS [training: 0.002282873946070251 | validation: -0.004866041074752499]
	TIME [epoch: 8.85 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002521887592178906		[learning rate: 0.00090661]
		[batch 20/20] avg loss: 4.218877348121817e-05		[learning rate: 0.00090551]
	Learning Rate: 0.000905513
	LOSS [training: -0.0012398494093488438 | validation: 0.0036244762340650995]
	TIME [epoch: 8.85 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024901609195183104		[learning rate: 0.00090442]
		[batch 20/20] avg loss: 0.002372095269234703		[learning rate: 0.00090332]
	Learning Rate: 0.000903321
	LOSS [training: -5.903282514180372e-05 | validation: -0.005640020123040893]
	TIME [epoch: 8.88 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0042150491821329045		[learning rate: 0.00090223]
		[batch 20/20] avg loss: 0.002082121111326193		[learning rate: 0.00090113]
	Learning Rate: 0.000901134
	LOSS [training: 0.003148585146729549 | validation: 0.0032706613248998013]
	TIME [epoch: 8.86 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004076281371335299		[learning rate: 0.00090004]
		[batch 20/20] avg loss: 0.01149820449363462		[learning rate: 0.00089895]
	Learning Rate: 0.000898953
	LOSS [training: 0.00778724293248496 | validation: 0.002079278233779228]
	TIME [epoch: 8.85 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006680440538816492		[learning rate: 0.00089786]
		[batch 20/20] avg loss: 0.00627670113076558		[learning rate: 0.00089678]
	Learning Rate: 0.000896777
	LOSS [training: 0.006478570834791038 | validation: -0.0055115063942165875]
	TIME [epoch: 8.85 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00579469937315938		[learning rate: 0.00089569]
		[batch 20/20] avg loss: 0.02253182201494006		[learning rate: 0.00089461]
	Learning Rate: 0.000894605
	LOSS [training: 0.014163260694049721 | validation: 0.010644382779180714]
	TIME [epoch: 8.85 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00772663506397967		[learning rate: 0.00089352]
		[batch 20/20] avg loss: 0.002518303636632303		[learning rate: 0.00089244]
	Learning Rate: 0.00089244
	LOSS [training: 0.005122469350305986 | validation: -0.011709002119210438]
	TIME [epoch: 8.88 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00018934829721289984		[learning rate: 0.00089136]
		[batch 20/20] avg loss: 0.012029792312813158		[learning rate: 0.00089028]
	Learning Rate: 0.000890279
	LOSS [training: 0.00592022200780013 | validation: 0.0021709747382594605]
	TIME [epoch: 8.86 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010969333821285131		[learning rate: 0.0008892]
		[batch 20/20] avg loss: 0.009875117827175487		[learning rate: 0.00088812]
	Learning Rate: 0.000888124
	LOSS [training: 0.01042222582423031 | validation: -0.00023690517511617052]
	TIME [epoch: 8.86 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006419732623695008		[learning rate: 0.00088705]
		[batch 20/20] avg loss: 0.001940783834524068		[learning rate: 0.00088597]
	Learning Rate: 0.000885974
	LOSS [training: 0.004180258229109538 | validation: -0.001601878118219689]
	TIME [epoch: 8.85 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005539307460015292		[learning rate: 0.0008849]
		[batch 20/20] avg loss: 0.004091781210508646		[learning rate: 0.00088383]
	Learning Rate: 0.000883829
	LOSS [training: 0.004815544335261969 | validation: 0.011746268980598107]
	TIME [epoch: 8.86 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0047604441024450255		[learning rate: 0.00088276]
		[batch 20/20] avg loss: -0.0014662007921847837		[learning rate: 0.00088169]
	Learning Rate: 0.00088169
	LOSS [training: 0.001647121655130121 | validation: -0.004578472777336724]
	TIME [epoch: 8.88 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003189806408964056		[learning rate: 0.00088062]
		[batch 20/20] avg loss: -0.0008367660510747831		[learning rate: 0.00087956]
	Learning Rate: 0.000879555
	LOSS [training: -0.0020132862300194197 | validation: 0.0005520868017761605]
	TIME [epoch: 8.85 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003183777252928341		[learning rate: 0.00087849]
		[batch 20/20] avg loss: 0.002293108107619694		[learning rate: 0.00087743]
	Learning Rate: 0.000877426
	LOSS [training: -0.0004453345726543233 | validation: -0.003803086076736146]
	TIME [epoch: 8.86 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012513044735031297		[learning rate: 0.00087636]
		[batch 20/20] avg loss: 0.003494940680428276		[learning rate: 0.0008753]
	Learning Rate: 0.000875302
	LOSS [training: 0.0011218181034625733 | validation: 0.0031162272363506587]
	TIME [epoch: 8.86 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00036237065647006916		[learning rate: 0.00087424]
		[batch 20/20] avg loss: -0.001204257036877078		[learning rate: 0.00087318]
	Learning Rate: 0.000873183
	LOSS [training: -0.000783313846673574 | validation: -0.0036534536634838287]
	TIME [epoch: 8.87 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004558563424057709		[learning rate: 0.00087213]
		[batch 20/20] avg loss: -0.002739454499502149		[learning rate: 0.00087107]
	Learning Rate: 0.000871069
	LOSS [training: -0.0036490089617799296 | validation: 0.01882829642995993]
	TIME [epoch: 8.87 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006903885248870635		[learning rate: 0.00087001]
		[batch 20/20] avg loss: 0.002560129814487464		[learning rate: 0.00086896]
	Learning Rate: 0.00086896
	LOSS [training: 0.00473200753167905 | validation: -0.002135557628924315]
	TIME [epoch: 8.86 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001091480563117989		[learning rate: 0.00086791]
		[batch 20/20] avg loss: -0.00032538552822186477		[learning rate: 0.00086686]
	Learning Rate: 0.000866857
	LOSS [training: -0.0007084330456699271 | validation: 0.004867785282219616]
	TIME [epoch: 8.86 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.01440365683867e-05		[learning rate: 0.00086581]
		[batch 20/20] avg loss: 0.0009895206463349944		[learning rate: 0.00086476]
	Learning Rate: 0.000864758
	LOSS [training: 0.0005298323414516904 | validation: 0.0018982912007780729]
	TIME [epoch: 8.86 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009926298708770546		[learning rate: 0.00086371]
		[batch 20/20] avg loss: 0.0003411178598238276		[learning rate: 0.00086266]
	Learning Rate: 0.000862665
	LOSS [training: 0.005133708284297187 | validation: -0.009535792956498082]
	TIME [epoch: 8.88 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007402931798720566		[learning rate: 0.00086162]
		[batch 20/20] avg loss: 0.0013355501215294924		[learning rate: 0.00086058]
	Learning Rate: 0.000860577
	LOSS [training: 0.004369240960125029 | validation: -0.00129048110562256]
	TIME [epoch: 8.86 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020745290828898992		[learning rate: 0.00085953]
		[batch 20/20] avg loss: -0.005589438094603235		[learning rate: 0.00085849]
	Learning Rate: 0.000858493
	LOSS [training: -0.003831983588746567 | validation: -0.0058248446163957175]
	TIME [epoch: 8.86 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00018270224218785262		[learning rate: 0.00085745]
		[batch 20/20] avg loss: 0.00214685176804107		[learning rate: 0.00085641]
	Learning Rate: 0.000856415
	LOSS [training: 0.0009820747629266087 | validation: -0.0021875469809377003]
	TIME [epoch: 8.86 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028539890693578688		[learning rate: 0.00085538]
		[batch 20/20] avg loss: 0.012744816090355762		[learning rate: 0.00085434]
	Learning Rate: 0.000854342
	LOSS [training: 0.007799402579856815 | validation: 0.013189940227686131]
	TIME [epoch: 8.86 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010862061595018246		[learning rate: 0.00085331]
		[batch 20/20] avg loss: 0.0006240496253556558		[learning rate: 0.00085227]
	Learning Rate: 0.000852273
	LOSS [training: 0.00574305561018695 | validation: -0.0014893010309194941]
	TIME [epoch: 8.88 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007179446411568834		[learning rate: 0.00085124]
		[batch 20/20] avg loss: 0.008848389667248532		[learning rate: 0.00085021]
	Learning Rate: 0.00085021
	LOSS [training: 0.008013918039408685 | validation: -0.0013098289651314612]
	TIME [epoch: 8.86 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014279144743823157		[learning rate: 0.00084918]
		[batch 20/20] avg loss: -0.001093475291693589		[learning rate: 0.00084815]
	Learning Rate: 0.000848152
	LOSS [training: -0.0012606948830379521 | validation: -0.0016305652597864851]
	TIME [epoch: 8.87 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002961471359444742		[learning rate: 0.00084712]
		[batch 20/20] avg loss: 0.00019788499944558245		[learning rate: 0.0008461]
	Learning Rate: 0.000846099
	LOSS [training: -0.0013817931799995798 | validation: -0.005122279594349357]
	TIME [epoch: 8.87 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008489491495735323		[learning rate: 0.00084507]
		[batch 20/20] avg loss: 0.003970245917656712		[learning rate: 0.00084405]
	Learning Rate: 0.000844051
	LOSS [training: 0.0015606483840415895 | validation: -0.005953175053459366]
	TIME [epoch: 8.87 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005109727370582667		[learning rate: 0.00084303]
		[batch 20/20] avg loss: 0.006124360135784444		[learning rate: 0.00084201]
	Learning Rate: 0.000842007
	LOSS [training: 0.003317666436421355 | validation: -0.008933338916476012]
	TIME [epoch: 8.88 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023114678182451906		[learning rate: 0.00084099]
		[batch 20/20] avg loss: 0.010797877953640152		[learning rate: 0.00083997]
	Learning Rate: 0.000839969
	LOSS [training: 0.004243205067697481 | validation: -0.008620554711647327]
	TIME [epoch: 8.86 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001869559004818891		[learning rate: 0.00083895]
		[batch 20/20] avg loss: 0.004036209312415971		[learning rate: 0.00083794]
	Learning Rate: 0.000837935
	LOSS [training: 0.002952884158617431 | validation: -0.011385974098837726]
	TIME [epoch: 8.87 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003319282572224985		[learning rate: 0.00083692]
		[batch 20/20] avg loss: 0.005328626638882377		[learning rate: 0.00083591]
	Learning Rate: 0.000835907
	LOSS [training: 0.004323954605553681 | validation: -0.003438073142565828]
	TIME [epoch: 8.86 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031084768200016918		[learning rate: 0.00083489]
		[batch 20/20] avg loss: -0.003659716995020734		[learning rate: 0.00083388]
	Learning Rate: 0.000833883
	LOSS [training: -0.003384096907511214 | validation: -0.007978161424042025]
	TIME [epoch: 8.89 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005129418020731129		[learning rate: 0.00083287]
		[batch 20/20] avg loss: -0.0009876201352663583		[learning rate: 0.00083186]
	Learning Rate: 0.000831865
	LOSS [training: -0.0007502809686697357 | validation: -0.008165029075128408]
	TIME [epoch: 8.87 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005228342030740986		[learning rate: 0.00083086]
		[batch 20/20] avg loss: -0.00624987028755368		[learning rate: 0.00082985]
	Learning Rate: 0.000829851
	LOSS [training: -0.005739106159147332 | validation: -0.008666454096276664]
	TIME [epoch: 8.85 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027811067252214226		[learning rate: 0.00082885]
		[batch 20/20] avg loss: 0.003797761479024249		[learning rate: 0.00082784]
	Learning Rate: 0.000827842
	LOSS [training: 0.0005083273769014126 | validation: 0.007970964497350179]
	TIME [epoch: 8.85 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004211385028437041		[learning rate: 0.00082684]
		[batch 20/20] avg loss: -0.0005046547064355607		[learning rate: 0.00082584]
	Learning Rate: 0.000825838
	LOSS [training: 0.0018533651610007403 | validation: 0.00029463822574214786]
	TIME [epoch: 8.85 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003991645311899519		[learning rate: 0.00082484]
		[batch 20/20] avg loss: 0.003546029697743124		[learning rate: 0.00082384]
	Learning Rate: 0.000823839
	LOSS [training: 0.003768837504821322 | validation: -0.008052181577773019]
	TIME [epoch: 8.87 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008328576532709919		[learning rate: 0.00082284]
		[batch 20/20] avg loss: -0.0015765412050907698		[learning rate: 0.00082184]
	Learning Rate: 0.000821844
	LOSS [training: 0.003376017663809574 | validation: -0.004177557949528595]
	TIME [epoch: 8.85 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.1807691569183587e-05		[learning rate: 0.00082085]
		[batch 20/20] avg loss: 0.013743112954675073		[learning rate: 0.00081985]
	Learning Rate: 0.000819855
	LOSS [training: 0.006897460323122129 | validation: 0.0030186863513176863]
	TIME [epoch: 8.86 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004363636008794567		[learning rate: 0.00081886]
		[batch 20/20] avg loss: -0.0024427962332661327		[learning rate: 0.00081787]
	Learning Rate: 0.00081787
	LOSS [training: 0.0009604198877642172 | validation: -0.0021137992522546624]
	TIME [epoch: 8.85 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004211213083273943		[learning rate: 0.00081688]
		[batch 20/20] avg loss: 0.003015492325378231		[learning rate: 0.00081589]
	Learning Rate: 0.00081589
	LOSS [training: 0.0036133527043260873 | validation: -0.0023559349541301044]
	TIME [epoch: 8.86 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00678664831265841		[learning rate: 0.0008149]
		[batch 20/20] avg loss: 0.00388564041793397		[learning rate: 0.00081391]
	Learning Rate: 0.000813915
	LOSS [training: 0.005336144365296189 | validation: 0.006002305593969524]
	TIME [epoch: 8.86 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003303746246948966		[learning rate: 0.00081293]
		[batch 20/20] avg loss: -0.004478420100393861		[learning rate: 0.00081194]
	Learning Rate: 0.000811944
	LOSS [training: -0.0005873369267224471 | validation: -0.007548657547521579]
	TIME [epoch: 8.86 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004088768610335355		[learning rate: 0.00081096]
		[batch 20/20] avg loss: 0.005081293012413855		[learning rate: 0.00080998]
	Learning Rate: 0.000809979
	LOSS [training: 0.004585030811374605 | validation: -0.0072049909092588025]
	TIME [epoch: 8.85 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012215527082988351		[learning rate: 0.000809]
		[batch 20/20] avg loss: 0.003336356279741328		[learning rate: 0.00080802]
	Learning Rate: 0.000808018
	LOSS [training: 0.007775941681364841 | validation: -0.004498584479266012]
	TIME [epoch: 8.85 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006660392437196833		[learning rate: 0.00080704]
		[batch 20/20] avg loss: 0.0024521403475662026		[learning rate: 0.00080606]
	Learning Rate: 0.000806062
	LOSS [training: -0.0021041260448153147 | validation: -0.006409597056129701]
	TIME [epoch: 8.87 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00114771051881637		[learning rate: 0.00080509]
		[batch 20/20] avg loss: -0.0018515915032866737		[learning rate: 0.00080411]
	Learning Rate: 0.000804111
	LOSS [training: -0.001499651011051522 | validation: -0.008035576493262768]
	TIME [epoch: 8.86 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003128166533071719		[learning rate: 0.00080314]
		[batch 20/20] avg loss: 0.005299637158412766		[learning rate: 0.00080216]
	Learning Rate: 0.000802164
	LOSS [training: 0.001085735312670524 | validation: -0.007135891737580547]
	TIME [epoch: 8.86 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003033058119338057		[learning rate: 0.00080119]
		[batch 20/20] avg loss: 0.007514552761864181		[learning rate: 0.00080022]
	Learning Rate: 0.000800222
	LOSS [training: 0.0036056234749651883 | validation: -0.005462635176796673]
	TIME [epoch: 8.86 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006072647140557793		[learning rate: 0.00079925]
		[batch 20/20] avg loss: 0.0066092572924761565		[learning rate: 0.00079828]
	Learning Rate: 0.000798285
	LOSS [training: 0.006340952216516973 | validation: -0.00013438820639364614]
	TIME [epoch: 8.85 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025756040860938313		[learning rate: 0.00079732]
		[batch 20/20] avg loss: -0.0007362396053137812		[learning rate: 0.00079635]
	Learning Rate: 0.000796352
	LOSS [training: -0.0016559218457038061 | validation: -0.0006783556449587385]
	TIME [epoch: 8.88 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009859844410351328		[learning rate: 0.00079539]
		[batch 20/20] avg loss: 0.0004499576313243646		[learning rate: 0.00079442]
	Learning Rate: 0.000794424
	LOSS [training: 0.0007179710361797487 | validation: 0.005699010829853755]
	TIME [epoch: 8.86 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006712407250035077		[learning rate: 0.00079346]
		[batch 20/20] avg loss: 0.0038617314647844173		[learning rate: 0.0007925]
	Learning Rate: 0.000792501
	LOSS [training: 0.005287069357409748 | validation: -0.00452893570211294]
	TIME [epoch: 8.85 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004498656852729795		[learning rate: 0.00079154]
		[batch 20/20] avg loss: -0.005876384167770227		[learning rate: 0.00079058]
	Learning Rate: 0.000790583
	LOSS [training: -0.005187520510250012 | validation: -0.009349660977062673]
	TIME [epoch: 8.86 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027430936370621824		[learning rate: 0.00078963]
		[batch 20/20] avg loss: -0.0010185677260882593		[learning rate: 0.00078867]
	Learning Rate: 0.000788669
	LOSS [training: -0.0018808306815752212 | validation: -0.002023778160482306]
	TIME [epoch: 8.86 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005103415618022531		[learning rate: 0.00078771]
		[batch 20/20] avg loss: -0.0021959143032848675		[learning rate: 0.00078676]
	Learning Rate: 0.00078676
	LOSS [training: -0.0036496649606536993 | validation: -0.008664491644386351]
	TIME [epoch: 8.87 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009944002886914838		[learning rate: 0.00078581]
		[batch 20/20] avg loss: 0.006404635328767161		[learning rate: 0.00078486]
	Learning Rate: 0.000784855
	LOSS [training: 0.0027051175200378383 | validation: 0.0032765562875257275]
	TIME [epoch: 8.85 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00032852616385479734		[learning rate: 0.0007839]
		[batch 20/20] avg loss: 0.007400563372084725		[learning rate: 0.00078296]
	Learning Rate: 0.000782955
	LOSS [training: 0.0035360186041149635 | validation: 0.013763061986556975]
	TIME [epoch: 8.85 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012292439957168572		[learning rate: 0.00078201]
		[batch 20/20] avg loss: -0.0001291616982045502		[learning rate: 0.00078106]
	Learning Rate: 0.00078106
	LOSS [training: 0.006081639129482011 | validation: 0.0007780757219493945]
	TIME [epoch: 8.85 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006214802361012444		[learning rate: 0.00078011]
		[batch 20/20] avg loss: -0.0003676278347385724		[learning rate: 0.00077917]
	Learning Rate: 0.000779169
	LOSS [training: 0.002923587263136935 | validation: 0.004404866006257115]
	TIME [epoch: 8.87 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010118141664141596		[learning rate: 0.00077823]
		[batch 20/20] avg loss: -0.002388165528385764		[learning rate: 0.00077728]
	Learning Rate: 0.000777283
	LOSS [training: -0.001699989847399962 | validation: -0.0017367882189105897]
	TIME [epoch: 8.86 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032541540048477424		[learning rate: 0.00077634]
		[batch 20/20] avg loss: -0.0033581733218454673		[learning rate: 0.0007754]
	Learning Rate: 0.000775401
	LOSS [training: -0.003306163663346605 | validation: 0.0014029004821205621]
	TIME [epoch: 8.86 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001306507596690476		[learning rate: 0.00077446]
		[batch 20/20] avg loss: -0.0024414765198514923		[learning rate: 0.00077352]
	Learning Rate: 0.000773524
	LOSS [training: -0.0018739920582709838 | validation: -0.004180510933954609]
	TIME [epoch: 8.85 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001344128032373351		[learning rate: 0.00077259]
		[batch 20/20] avg loss: -0.003470779550279253		[learning rate: 0.00077165]
	Learning Rate: 0.000771651
	LOSS [training: -0.0010633257589529508 | validation: -0.004393159897791267]
	TIME [epoch: 8.85 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015952827168484872		[learning rate: 0.00077072]
		[batch 20/20] avg loss: 0.00042449906452300364		[learning rate: 0.00076978]
	Learning Rate: 0.000769783
	LOSS [training: 0.0010098908906857455 | validation: 0.0047400840042946734]
	TIME [epoch: 8.88 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010959601227696302		[learning rate: 0.00076885]
		[batch 20/20] avg loss: 0.0019329168743516898		[learning rate: 0.00076792]
	Learning Rate: 0.00076792
	LOSS [training: 0.0015144384985606599 | validation: 0.003346844379240917]
	TIME [epoch: 8.86 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00037310038569825433		[learning rate: 0.00076699]
		[batch 20/20] avg loss: -0.006003913241940713		[learning rate: 0.00076606]
	Learning Rate: 0.000766061
	LOSS [training: -0.0028154064281212298 | validation: -0.008730720855433433]
	TIME [epoch: 8.85 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009443595808542472		[learning rate: 0.00076513]
		[batch 20/20] avg loss: 0.003844925840633015		[learning rate: 0.00076421]
	Learning Rate: 0.000764206
	LOSS [training: 0.0014502831298893837 | validation: -0.007759964000916621]
	TIME [epoch: 8.85 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00040349025037011494		[learning rate: 0.00076328]
		[batch 20/20] avg loss: 0.00010280151630450256		[learning rate: 0.00076236]
	Learning Rate: 0.000762356
	LOSS [training: 0.00025314588333730877 | validation: 0.007913523465274387]
	TIME [epoch: 8.85 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00462726765490174		[learning rate: 0.00076143]
		[batch 20/20] avg loss: 0.004313462111914584		[learning rate: 0.00076051]
	Learning Rate: 0.000760511
	LOSS [training: 0.004470364883408162 | validation: -0.008970042251762844]
	TIME [epoch: 8.88 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002535763529575117		[learning rate: 0.00075959]
		[batch 20/20] avg loss: 0.005896335791672482		[learning rate: 0.00075867]
	Learning Rate: 0.00075867
	LOSS [training: 0.0042160496606238 | validation: -0.00020532548193549424]
	TIME [epoch: 8.86 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018078251120986245		[learning rate: 0.00075775]
		[batch 20/20] avg loss: 0.002182449805525154		[learning rate: 0.00075683]
	Learning Rate: 0.000756833
	LOSS [training: 0.0019951374588118888 | validation: -0.009666696829756911]
	TIME [epoch: 8.86 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007571030439918923		[learning rate: 0.00075592]
		[batch 20/20] avg loss: 0.01050629339457729		[learning rate: 0.000755]
	Learning Rate: 0.000755001
	LOSS [training: 0.009038661917248109 | validation: 0.0092012582529069]
	TIME [epoch: 8.85 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022372220494607012		[learning rate: 0.00075409]
		[batch 20/20] avg loss: 0.004129510724978637		[learning rate: 0.00075317]
	Learning Rate: 0.000753173
	LOSS [training: 0.0031833663872196686 | validation: 0.0007193306106028269]
	TIME [epoch: 8.88 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006367141920312599		[learning rate: 0.00075226]
		[batch 20/20] avg loss: -0.0013450857962331668		[learning rate: 0.00075135]
	Learning Rate: 0.00075135
	LOSS [training: 0.002511028062039717 | validation: -0.011474548607874141]
	TIME [epoch: 8.86 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006039126569527887		[learning rate: 0.00075044]
		[batch 20/20] avg loss: -0.0027705571937682713		[learning rate: 0.00074953]
	Learning Rate: 0.000749531
	LOSS [training: -0.004404841881648079 | validation: -0.008496390824544614]
	TIME [epoch: 8.87 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008935746621984606		[learning rate: 0.00074862]
		[batch 20/20] avg loss: 0.0035573641340502825		[learning rate: 0.00074772]
	Learning Rate: 0.000747716
	LOSS [training: 0.0013318947359259104 | validation: -0.006232358965438048]
	TIME [epoch: 8.85 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.402228484170362e-05		[learning rate: 0.00074681]
		[batch 20/20] avg loss: -0.0003264941140508861		[learning rate: 0.00074591]
	Learning Rate: 0.000745906
	LOSS [training: -0.00013623591460459116 | validation: 0.00012593152369433738]
	TIME [epoch: 8.86 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006004386439705125		[learning rate: 0.000745]
		[batch 20/20] avg loss: 0.013053626565838116		[learning rate: 0.0007441]
	Learning Rate: 0.0007441
	LOSS [training: 0.00952900650277162 | validation: -0.005239303241289501]
	TIME [epoch: 8.88 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003150835532125004		[learning rate: 0.0007432]
		[batch 20/20] avg loss: 0.011718041925542402		[learning rate: 0.0007423]
	Learning Rate: 0.000742299
	LOSS [training: 0.004283603196708699 | validation: -0.002748889803663522]
	TIME [epoch: 8.86 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008859505273697358		[learning rate: 0.0007414]
		[batch 20/20] avg loss: -0.005439137791418953		[learning rate: 0.0007405]
	Learning Rate: 0.000740502
	LOSS [training: 0.001710183741139203 | validation: -0.0039094481910702875]
	TIME [epoch: 8.86 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036513046589430214		[learning rate: 0.00073961]
		[batch 20/20] avg loss: -4.4664536324404495e-05		[learning rate: 0.00073871]
	Learning Rate: 0.000738709
	LOSS [training: 0.001803320061309309 | validation: -0.00039788216376947434]
	TIME [epoch: 8.86 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013898775926310483		[learning rate: 0.00073781]
		[batch 20/20] avg loss: -0.0057237912931192856		[learning rate: 0.00073692]
	Learning Rate: 0.000736921
	LOSS [training: -0.002166956850244119 | validation: 0.0035143822896643584]
	TIME [epoch: 8.85 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003885670120343294		[learning rate: 0.00073603]
		[batch 20/20] avg loss: -0.005040802002015576		[learning rate: 0.00073514]
	Learning Rate: 0.000735137
	LOSS [training: -0.000577565940836141 | validation: -0.0011868000726934337]
	TIME [epoch: 8.88 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007259799867145158		[learning rate: 0.00073425]
		[batch 20/20] avg loss: -0.004392138717216026		[learning rate: 0.00073336]
	Learning Rate: 0.000733358
	LOSS [training: -0.002559059351965271 | validation: -0.006902887065063893]
	TIME [epoch: 8.86 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002630259931224511		[learning rate: 0.00073247]
		[batch 20/20] avg loss: 0.008372980238981857		[learning rate: 0.00073158]
	Learning Rate: 0.000731582
	LOSS [training: 0.004318003116052154 | validation: -0.007797160452095793]
	TIME [epoch: 8.86 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003344953703863479		[learning rate: 0.0007307]
		[batch 20/20] avg loss: -0.0009339959818516688		[learning rate: 0.00072981]
	Learning Rate: 0.000729811
	LOSS [training: -0.002139474842857574 | validation: 0.010810992402026815]
	TIME [epoch: 8.87 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006947086146180369		[learning rate: 0.00072893]
		[batch 20/20] avg loss: 0.0017381583465952157		[learning rate: 0.00072804]
	Learning Rate: 0.000728044
	LOSS [training: 0.004342622246387792 | validation: -0.0010070158165311534]
	TIME [epoch: 8.86 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004055474833420348		[learning rate: 0.00072716]
		[batch 20/20] avg loss: -0.004636203735104259		[learning rate: 0.00072628]
	Learning Rate: 0.000726282
	LOSS [training: -0.0002903644508419555 | validation: -0.006928487625973074]
	TIME [epoch: 8.88 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005577381252092358		[learning rate: 0.0007254]
		[batch 20/20] avg loss: -0.0010588933276018306		[learning rate: 0.00072452]
	Learning Rate: 0.000724524
	LOSS [training: -0.003318137289847093 | validation: -0.00010870585996263096]
	TIME [epoch: 8.86 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010977408009766349		[learning rate: 0.00072365]
		[batch 20/20] avg loss: -0.001379818753273131		[learning rate: 0.00072277]
	Learning Rate: 0.00072277
	LOSS [training: -0.0012387797771248829 | validation: 8.075887361073323e-05]
	TIME [epoch: 8.86 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005357354449347101		[learning rate: 0.00072189]
		[batch 20/20] avg loss: -0.0060890011026751446		[learning rate: 0.00072102]
	Learning Rate: 0.00072102
	LOSS [training: -0.005723177776011123 | validation: -0.006963932705540236]
	TIME [epoch: 8.86 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008767289938038209		[learning rate: 0.00072015]
		[batch 20/20] avg loss: 0.005650957267633737		[learning rate: 0.00071927]
	Learning Rate: 0.000719275
	LOSS [training: -0.0015581663352022374 | validation: 0.012967375649825408]
	TIME [epoch: 8.86 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015974089813720962		[learning rate: 0.0007184]
		[batch 20/20] avg loss: 0.005740368692018042		[learning rate: 0.00071753]
	Learning Rate: 0.000717533
	LOSS [training: 0.010857229252869503 | validation: -0.00413333370036071]
	TIME [epoch: 8.87 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016179156167763042		[learning rate: 0.00071666]
		[batch 20/20] avg loss: -0.0005390627234151336		[learning rate: 0.0007158]
	Learning Rate: 0.000715796
	LOSS [training: 0.0005394264466805855 | validation: -0.004317654385039414]
	TIME [epoch: 8.86 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010352815064658813		[learning rate: 0.00071493]
		[batch 20/20] avg loss: 0.005509148692429612		[learning rate: 0.00071406]
	Learning Rate: 0.000714064
	LOSS [training: 0.003272215099447747 | validation: -0.0017384726821674055]
	TIME [epoch: 8.85 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00877213445619383		[learning rate: 0.0007132]
		[batch 20/20] avg loss: -0.003695254574569277		[learning rate: 0.00071233]
	Learning Rate: 0.000712335
	LOSS [training: -0.006233694515381553 | validation: -0.0073826513662780605]
	TIME [epoch: 8.86 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016379533967859968		[learning rate: 0.00071147]
		[batch 20/20] avg loss: -0.0061570493720937034		[learning rate: 0.00071061]
	Learning Rate: 0.00071061
	LOSS [training: -0.002259547987653854 | validation: -0.007477247218952031]
	TIME [epoch: 8.89 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008468383284557224		[learning rate: 0.00070975]
		[batch 20/20] avg loss: -0.004562762772808193		[learning rate: 0.00070889]
	Learning Rate: 0.00070889
	LOSS [training: -0.0065155730286827074 | validation: 0.006090667595107431]
	TIME [epoch: 8.86 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002882168388451883		[learning rate: 0.00070803]
		[batch 20/20] avg loss: 0.011870651737724966		[learning rate: 0.00070717]
	Learning Rate: 0.000707174
	LOSS [training: 0.007376410063088425 | validation: 0.01410846117365971]
	TIME [epoch: 8.87 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004727961618466373		[learning rate: 0.00070632]
		[batch 20/20] avg loss: 0.0018954434223439746		[learning rate: 0.00070546]
	Learning Rate: 0.000705462
	LOSS [training: 0.0033117025204051744 | validation: 0.00015197936862067533]
	TIME [epoch: 8.86 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037160529821364243		[learning rate: 0.00070461]
		[batch 20/20] avg loss: -0.00023846113540085262		[learning rate: 0.00070375]
	Learning Rate: 0.000703754
	LOSS [training: -0.001977257058768638 | validation: -0.0031748887089133614]
	TIME [epoch: 8.86 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005330563149603064		[learning rate: 0.0007029]
		[batch 20/20] avg loss: 0.0012780964610442464		[learning rate: 0.00070205]
	Learning Rate: 0.000702051
	LOSS [training: -0.0020262333442794085 | validation: -0.006048872389627657]
	TIME [epoch: 8.89 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007889366371679427		[learning rate: 0.0007012]
		[batch 20/20] avg loss: 0.008146203598974132		[learning rate: 0.00070035]
	Learning Rate: 0.000700351
	LOSS [training: 0.00012841861364735146 | validation: 0.0030324662075183646]
	TIME [epoch: 8.86 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009400619471990359		[learning rate: 0.0006995]
		[batch 20/20] avg loss: 0.002602644944406428		[learning rate: 0.00069866]
	Learning Rate: 0.000698656
	LOSS [training: 0.006001632208198394 | validation: -0.002229000721022962]
	TIME [epoch: 8.86 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002188763832816274		[learning rate: 0.00069781]
		[batch 20/20] avg loss: -0.002716950957587997		[learning rate: 0.00069696]
	Learning Rate: 0.000696964
	LOSS [training: -0.0024528573952021346 | validation: -0.004192240014486259]
	TIME [epoch: 8.87 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00628057293078432		[learning rate: 0.00069612]
		[batch 20/20] avg loss: -0.005787091716321604		[learning rate: 0.00069528]
	Learning Rate: 0.000695277
	LOSS [training: -0.0060338323235529625 | validation: -0.0019388781336738673]
	TIME [epoch: 8.87 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006747992062980431		[learning rate: 0.00069444]
		[batch 20/20] avg loss: -0.005106943344419161		[learning rate: 0.00069359]
	Learning Rate: 0.000693594
	LOSS [training: -0.0022160720690605593 | validation: -0.0072227954626294605]
	TIME [epoch: 8.87 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037272392494821205		[learning rate: 0.00069275]
		[batch 20/20] avg loss: 0.0019482321313392006		[learning rate: 0.00069191]
	Learning Rate: 0.000691915
	LOSS [training: -0.0008895035590714596 | validation: -0.012758603566341776]
	TIME [epoch: 8.86 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031833474779073877		[learning rate: 0.00069108]
		[batch 20/20] avg loss: -0.00457017844718545		[learning rate: 0.00069024]
	Learning Rate: 0.00069024
	LOSS [training: -0.003876762962546419 | validation: 0.004478753589252345]
	TIME [epoch: 8.86 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: -4.740505302317371e-05		[learning rate: 0.0006894]
		[batch 20/20] avg loss: -0.004206965721491375		[learning rate: 0.00068857]
	Learning Rate: 0.000688569
	LOSS [training: -0.002127185387257274 | validation: -0.007644765092342182]
	TIME [epoch: 8.86 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002258885212994075		[learning rate: 0.00068773]
		[batch 20/20] avg loss: -0.006306591687950091		[learning rate: 0.0006869]
	Learning Rate: 0.000686902
	LOSS [training: -0.004282738450472083 | validation: -0.008312280820644639]
	TIME [epoch: 8.88 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007036041621954023		[learning rate: 0.00068607]
		[batch 20/20] avg loss: -0.0029900513410818447		[learning rate: 0.00068524]
	Learning Rate: 0.000685239
	LOSS [training: -0.005013046481517935 | validation: -0.007760817027459831]
	TIME [epoch: 8.87 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014765571381099491		[learning rate: 0.00068441]
		[batch 20/20] avg loss: -0.005774818655632603		[learning rate: 0.00068358]
	Learning Rate: 0.00068358
	LOSS [training: -0.0021491307587613276 | validation: -0.001263337687598641]
	TIME [epoch: 8.86 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018327035231907097		[learning rate: 0.00068275]
		[batch 20/20] avg loss: -0.0019279041946051079		[learning rate: 0.00068193]
	Learning Rate: 0.000681925
	LOSS [training: -4.760033570719909e-05 | validation: -0.00394381219875603]
	TIME [epoch: 8.86 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004501667809314588		[learning rate: 0.0006811]
		[batch 20/20] avg loss: -0.007141568274532574		[learning rate: 0.00068027]
	Learning Rate: 0.000680275
	LOSS [training: -0.0058216180419235816 | validation: -0.006758069654239937]
	TIME [epoch: 8.87 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004008450953614251		[learning rate: 0.00067945]
		[batch 20/20] avg loss: 0.00392540529798903		[learning rate: 0.00067863]
	Learning Rate: 0.000678628
	LOSS [training: -4.1522827812610953e-05 | validation: -0.0046650980829814484]
	TIME [epoch: 8.88 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008183182756613849		[learning rate: 0.00067781]
		[batch 20/20] avg loss: -0.0016348397519770977		[learning rate: 0.00067698]
	Learning Rate: 0.000676985
	LOSS [training: -0.004909011254295472 | validation: -0.0009352572523956295]
	TIME [epoch: 8.87 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007762199847843529		[learning rate: 0.00067616]
		[batch 20/20] avg loss: -0.0002015174024292736		[learning rate: 0.00067535]
	Learning Rate: 0.000675346
	LOSS [training: 0.0037803412227071288 | validation: 0.0013033722987005822]
	TIME [epoch: 8.85 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010557548416760348		[learning rate: 0.00067453]
		[batch 20/20] avg loss: -0.0005989588125597358		[learning rate: 0.00067371]
	Learning Rate: 0.000673711
	LOSS [training: -0.0008273568271178853 | validation: -0.003804015444386083]
	TIME [epoch: 8.87 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005013368496869796		[learning rate: 0.0006729]
		[batch 20/20] avg loss: 0.0014320101960006138		[learning rate: 0.00067208]
	Learning Rate: 0.00067208
	LOSS [training: -0.0017906791504345913 | validation: -0.001707020927812503]
	TIME [epoch: 8.87 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029466424038047936		[learning rate: 0.00067127]
		[batch 20/20] avg loss: -0.006305452255034793		[learning rate: 0.00067045]
	Learning Rate: 0.000670453
	LOSS [training: -0.004626047329419794 | validation: -0.010388197034904035]
	TIME [epoch: 8.87 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032610576743427404		[learning rate: 0.00066964]
		[batch 20/20] avg loss: 0.003931042034157417		[learning rate: 0.00066883]
	Learning Rate: 0.00066883
	LOSS [training: 0.0003349921799073387 | validation: 8.457222437321694e-05]
	TIME [epoch: 8.86 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003438679121279744		[learning rate: 0.00066802]
		[batch 20/20] avg loss: 0.0018642354483142047		[learning rate: 0.00066721]
	Learning Rate: 0.000667211
	LOSS [training: 0.0026514572847969745 | validation: -0.0082902447429594]
	TIME [epoch: 8.86 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009281130266218882		[learning rate: 0.0006664]
		[batch 20/20] avg loss: -0.0045686773293899		[learning rate: 0.0006656]
	Learning Rate: 0.000665596
	LOSS [training: -0.006924903797804393 | validation: -0.006871667877115437]
	TIME [epoch: 8.86 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004839622449234128		[learning rate: 0.00066479]
		[batch 20/20] avg loss: -0.005084307900171934		[learning rate: 0.00066398]
	Learning Rate: 0.000663984
	LOSS [training: -0.00496196517470303 | validation: -0.007339719654033464]
	TIME [epoch: 8.88 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00473946825254293		[learning rate: 0.00066318]
		[batch 20/20] avg loss: 0.002957255543947456		[learning rate: 0.00066238]
	Learning Rate: 0.000662377
	LOSS [training: 0.0038483618982451937 | validation: -0.007738268770867884]
	TIME [epoch: 8.86 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004463062275661035		[learning rate: 0.00066157]
		[batch 20/20] avg loss: -0.005737226304616236		[learning rate: 0.00066077]
	Learning Rate: 0.000660773
	LOSS [training: -0.005100144290138635 | validation: -0.007804888560262109]
	TIME [epoch: 8.86 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009227511618979455		[learning rate: 0.00065997]
		[batch 20/20] avg loss: -0.005568062315924303		[learning rate: 0.00065917]
	Learning Rate: 0.000659174
	LOSS [training: -0.007397786967451879 | validation: -0.0035050658961535133]
	TIME [epoch: 8.86 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004401287273221698		[learning rate: 0.00065838]
		[batch 20/20] avg loss: -0.0033326106359163428		[learning rate: 0.00065758]
	Learning Rate: 0.000657578
	LOSS [training: -0.003866948954569021 | validation: -0.008151398648129833]
	TIME [epoch: 8.87 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003311231250742318		[learning rate: 0.00065678]
		[batch 20/20] avg loss: -0.009041710280166686		[learning rate: 0.00065599]
	Learning Rate: 0.000655986
	LOSS [training: -0.002865239514712184 | validation: -0.003871128137883945]
	TIME [epoch: 8.89 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016831225507338622		[learning rate: 0.00065519]
		[batch 20/20] avg loss: -0.01030827474028324		[learning rate: 0.0006544]
	Learning Rate: 0.000654398
	LOSS [training: -0.00599569864550855 | validation: -0.006997140200338644]
	TIME [epoch: 8.87 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027656711158429333		[learning rate: 0.00065361]
		[batch 20/20] avg loss: -0.0006270293282843681		[learning rate: 0.00065281]
	Learning Rate: 0.000652814
	LOSS [training: -0.0016963502220636508 | validation: -0.0034927095906391943]
	TIME [epoch: 8.87 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005073918365620813		[learning rate: 0.00065202]
		[batch 20/20] avg loss: -0.005711523279481395		[learning rate: 0.00065123]
	Learning Rate: 0.000651234
	LOSS [training: -0.0053927208225511055 | validation: 0.0022462713805063254]
	TIME [epoch: 8.87 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035997012020214194		[learning rate: 0.00065045]
		[batch 20/20] avg loss: -0.0049086412218543865		[learning rate: 0.00064966]
	Learning Rate: 0.000649657
	LOSS [training: -0.004254171211937904 | validation: -0.01052227284043611]
	TIME [epoch: 8.86 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0067136071324660745		[learning rate: 0.00064887]
		[batch 20/20] avg loss: -0.0048440329485420425		[learning rate: 0.00064808]
	Learning Rate: 0.000648084
	LOSS [training: -0.005778820040504057 | validation: -0.0035469589457972255]
	TIME [epoch: 8.88 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004730500323730098		[learning rate: 0.0006473]
		[batch 20/20] avg loss: -0.009732735494335707		[learning rate: 0.00064652]
	Learning Rate: 0.000646515
	LOSS [training: -0.007231617909032902 | validation: -0.0067508668695823795]
	TIME [epoch: 8.86 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005736708901703682		[learning rate: 0.00064573]
		[batch 20/20] avg loss: 0.002306372925175439		[learning rate: 0.00064495]
	Learning Rate: 0.00064495
	LOSS [training: -0.0017151679882641217 | validation: -0.00013148140511806488]
	TIME [epoch: 8.87 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003501717023473307		[learning rate: 0.00064417]
		[batch 20/20] avg loss: -0.002861334591918272		[learning rate: 0.00064339]
	Learning Rate: 0.000643389
	LOSS [training: -0.0031815258076957897 | validation: -0.01063452890342259]
	TIME [epoch: 8.86 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002913587343966813		[learning rate: 0.00064261]
		[batch 20/20] avg loss: -0.000685426186913096		[learning rate: 0.00064183]
	Learning Rate: 0.000641832
	LOSS [training: -0.0017995067654399549 | validation: -0.0007032770221388944]
	TIME [epoch: 8.88 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0068585889587871995		[learning rate: 0.00064105]
		[batch 20/20] avg loss: -0.0027163063303599		[learning rate: 0.00064028]
	Learning Rate: 0.000640278
	LOSS [training: -0.004787447644573549 | validation: -0.0035578641775869313]
	TIME [epoch: 8.87 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010388591638040259		[learning rate: 0.0006395]
		[batch 20/20] avg loss: -0.002827805375110191		[learning rate: 0.00063873]
	Learning Rate: 0.000638728
	LOSS [training: -0.0066081985065752245 | validation: 0.004362808184154817]
	TIME [epoch: 8.86 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003262939966006485		[learning rate: 0.00063795]
		[batch 20/20] avg loss: -0.005651178505274459		[learning rate: 0.00063718]
	Learning Rate: 0.000637182
	LOSS [training: -0.001194119269633987 | validation: -0.009423230770197329]
	TIME [epoch: 8.86 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006034575675488887		[learning rate: 0.00063641]
		[batch 20/20] avg loss: -0.007851688833141529		[learning rate: 0.00063564]
	Learning Rate: 0.000635639
	LOSS [training: -0.006943132254315208 | validation: -0.0074859093397999735]
	TIME [epoch: 8.87 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006543134869916862		[learning rate: 0.00063487]
		[batch 20/20] avg loss: -0.00756419057664751		[learning rate: 0.0006341]
	Learning Rate: 0.0006341
	LOSS [training: -0.007053662723282188 | validation: -0.00961257809319865]
	TIME [epoch: 8.89 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003487987832286761		[learning rate: 0.00063333]
		[batch 20/20] avg loss: -0.008597092954082893		[learning rate: 0.00063257]
	Learning Rate: 0.000632565
	LOSS [training: -0.0025545525608980666 | validation: 0.008362082698664803]
	TIME [epoch: 8.87 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013263610086537924		[learning rate: 0.0006318]
		[batch 20/20] avg loss: -0.005344431781744187		[learning rate: 0.00063103]
	Learning Rate: 0.000631034
	LOSS [training: -0.002009035386545197 | validation: 0.001968468958024095]
	TIME [epoch: 8.86 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020346103922226143		[learning rate: 0.00063027]
		[batch 20/20] avg loss: 0.0031331259159754154		[learning rate: 0.00062951]
	Learning Rate: 0.000629506
	LOSS [training: 0.0005492577618764003 | validation: -0.0038341545750730337]
	TIME [epoch: 8.86 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023779788162447643		[learning rate: 0.00062874]
		[batch 20/20] avg loss: -0.006032531054283657		[learning rate: 0.00062798]
	Learning Rate: 0.000627982
	LOSS [training: -0.00420525493526421 | validation: -0.008250577883462652]
	TIME [epoch: 8.86 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005524661182073842		[learning rate: 0.00062722]
		[batch 20/20] avg loss: -0.0041660199503116115		[learning rate: 0.00062646]
	Learning Rate: 0.000626462
	LOSS [training: -0.0048453405661927264 | validation: -0.0028442214710903716]
	TIME [epoch: 8.88 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008175644425885678		[learning rate: 0.0006257]
		[batch 20/20] avg loss: -0.0008880663214127987		[learning rate: 0.00062495]
	Learning Rate: 0.000624946
	LOSS [training: -0.00453185537364924 | validation: -0.0062115087084727485]
	TIME [epoch: 8.86 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005179848185511085		[learning rate: 0.00062419]
		[batch 20/20] avg loss: -0.00625822988693302		[learning rate: 0.00062343]
	Learning Rate: 0.000623433
	LOSS [training: -0.0057190390362220535 | validation: -0.004595618709031886]
	TIME [epoch: 8.86 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004938270573645012		[learning rate: 0.00062268]
		[batch 20/20] avg loss: -0.00829481131438066		[learning rate: 0.00062192]
	Learning Rate: 0.000621923
	LOSS [training: -0.0066165409440128355 | validation: -0.00898893092730143]
	TIME [epoch: 8.87 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008415883227566636		[learning rate: 0.00062117]
		[batch 20/20] avg loss: -0.009299089946755584		[learning rate: 0.00062042]
	Learning Rate: 0.000620418
	LOSS [training: -0.00885748658716111 | validation: -0.011578583647121315]
	TIME [epoch: 8.88 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003135096462473378		[learning rate: 0.00061967]
		[batch 20/20] avg loss: -0.007008397415848422		[learning rate: 0.00061892]
	Learning Rate: 0.000618916
	LOSS [training: -0.0050717469391608995 | validation: -0.006033377731470738]
	TIME [epoch: 8.87 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0074936574544000765		[learning rate: 0.00061817]
		[batch 20/20] avg loss: 0.0006818075971538756		[learning rate: 0.00061742]
	Learning Rate: 0.000617418
	LOSS [training: -0.0034059249286231006 | validation: 0.00047221908464039177]
	TIME [epoch: 8.87 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00048234591584897686		[learning rate: 0.00061667]
		[batch 20/20] avg loss: -0.006394836522602647		[learning rate: 0.00061592]
	Learning Rate: 0.000615923
	LOSS [training: -0.0034385912192258115 | validation: -0.003843138087121401]
	TIME [epoch: 8.88 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004499922290998612		[learning rate: 0.00061518]
		[batch 20/20] avg loss: -3.207093453614848e-05		[learning rate: 0.00061443]
	Learning Rate: 0.000614432
	LOSS [training: -0.0022659966127673804 | validation: -0.005357057976678921]
	TIME [epoch: 8.88 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0040370015374736265		[learning rate: 0.00061369]
		[batch 20/20] avg loss: 0.00011528428099600403		[learning rate: 0.00061294]
	Learning Rate: 0.000612944
	LOSS [training: 0.002076142909234815 | validation: -0.016070416712615873]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_1252.pth
	Model improved!!!
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004041725107548457		[learning rate: 0.0006122]
		[batch 20/20] avg loss: -0.00020741302965849602		[learning rate: 0.00061146]
	Learning Rate: 0.000611461
	LOSS [training: -0.002124569068603476 | validation: -0.0030857454303898817]
	TIME [epoch: 8.86 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00827088966904733		[learning rate: 0.00061072]
		[batch 20/20] avg loss: -0.003000979223663284		[learning rate: 0.00060998]
	Learning Rate: 0.00060998
	LOSS [training: -0.005635934446355307 | validation: -0.006292285483786965]
	TIME [epoch: 8.86 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007868303096956044		[learning rate: 0.00060924]
		[batch 20/20] avg loss: -0.009566177076777132		[learning rate: 0.0006085]
	Learning Rate: 0.000608504
	LOSS [training: -0.008717240086866591 | validation: -0.009062913236221094]
	TIME [epoch: 8.85 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005262821270787676		[learning rate: 0.00060777]
		[batch 20/20] avg loss: -0.0010171678829853762		[learning rate: 0.00060703]
	Learning Rate: 0.00060703
	LOSS [training: -0.0031399945768865256 | validation: -0.003656127788900354]
	TIME [epoch: 8.85 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0040982237829182405		[learning rate: 0.0006063]
		[batch 20/20] avg loss: 0.003813004039390894		[learning rate: 0.00060556]
	Learning Rate: 0.000605561
	LOSS [training: 0.003955613911154567 | validation: -0.007513739616851449]
	TIME [epoch: 8.89 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012950371543972398		[learning rate: 0.00060483]
		[batch 20/20] avg loss: 0.0047902517165945915		[learning rate: 0.00060409]
	Learning Rate: 0.000604095
	LOSS [training: 0.001747607281098676 | validation: 0.005175029508260107]
	TIME [epoch: 8.86 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002611820509923114		[learning rate: 0.00060336]
		[batch 20/20] avg loss: 0.0003406267319007145		[learning rate: 0.00060263]
	Learning Rate: 0.000602633
	LOSS [training: -0.0011355968890111997 | validation: -0.009277363212872482]
	TIME [epoch: 8.86 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00834913260811275		[learning rate: 0.0006019]
		[batch 20/20] avg loss: -0.007828398077710116		[learning rate: 0.00060117]
	Learning Rate: 0.000601174
	LOSS [training: -0.008088765342911431 | validation: -0.0058957158114525685]
	TIME [epoch: 8.86 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010306097889458394		[learning rate: 0.00060045]
		[batch 20/20] avg loss: -0.0030720936304458163		[learning rate: 0.00059972]
	Learning Rate: 0.000599718
	LOSS [training: -0.0066890957599521055 | validation: -0.0053935934654960405]
	TIME [epoch: 8.86 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00019917547087599927		[learning rate: 0.00059899]
		[batch 20/20] avg loss: -0.004251748672802966		[learning rate: 0.00059827]
	Learning Rate: 0.000598267
	LOSS [training: -0.0020262866009634834 | validation: 0.0004138216469268907]
	TIME [epoch: 8.82 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027242504698744632		[learning rate: 0.00059754]
		[batch 20/20] avg loss: -0.009171987197743307		[learning rate: 0.00059682]
	Learning Rate: 0.000596818
	LOSS [training: -0.005948118833808885 | validation: -0.012814964887427775]
	TIME [epoch: 8.86 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0075752529036548725		[learning rate: 0.0005961]
		[batch 20/20] avg loss: 0.008186775330307663		[learning rate: 0.00059537]
	Learning Rate: 0.000595373
	LOSS [training: 0.0003057612133263956 | validation: 0.012214240148720253]
	TIME [epoch: 8.87 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005584144880984123		[learning rate: 0.00059465]
		[batch 20/20] avg loss: -0.006594057846646084		[learning rate: 0.00059393]
	Learning Rate: 0.000593932
	LOSS [training: -0.0005049564828309811 | validation: -0.01131327248679704]
	TIME [epoch: 8.87 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006607417657778859		[learning rate: 0.00059321]
		[batch 20/20] avg loss: -0.004125528239560355		[learning rate: 0.00059249]
	Learning Rate: 0.000592494
	LOSS [training: -0.005366472948669608 | validation: -0.008552622336754214]
	TIME [epoch: 8.89 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010206556677693143		[learning rate: 0.00059178]
		[batch 20/20] avg loss: -0.0018842910353925734		[learning rate: 0.00059106]
	Learning Rate: 0.00059106
	LOSS [training: -0.006045423856542857 | validation: -0.010253731429094876]
	TIME [epoch: 8.87 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00715365713855427		[learning rate: 0.00059034]
		[batch 20/20] avg loss: -0.0015563328573547919		[learning rate: 0.00058963]
	Learning Rate: 0.000589629
	LOSS [training: -0.0043549949979545316 | validation: -0.007474906795425963]
	TIME [epoch: 8.86 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0043985175942546665		[learning rate: 0.00058892]
		[batch 20/20] avg loss: -0.008100624911679611		[learning rate: 0.0005882]
	Learning Rate: 0.000588202
	LOSS [training: -0.006249571252967139 | validation: -0.004091579822198062]
	TIME [epoch: 8.85 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005574836508860846		[learning rate: 0.00058749]
		[batch 20/20] avg loss: -0.0005545266983660304		[learning rate: 0.00058678]
	Learning Rate: 0.000586778
	LOSS [training: -0.0030646816036134393 | validation: -0.00077712998509845]
	TIME [epoch: 8.86 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015138550335873993		[learning rate: 0.00058607]
		[batch 20/20] avg loss: -0.002085145568053508		[learning rate: 0.00058536]
	Learning Rate: 0.000585357
	LOSS [training: -0.0017995003008204537 | validation: -0.009910256326315162]
	TIME [epoch: 8.89 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008580252216283492		[learning rate: 0.00058465]
		[batch 20/20] avg loss: -0.007111566224829004		[learning rate: 0.00058394]
	Learning Rate: 0.00058394
	LOSS [training: -0.003984795723228676 | validation: -0.0072330660913695445]
	TIME [epoch: 8.87 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005719620159411049		[learning rate: 0.00058323]
		[batch 20/20] avg loss: -0.006500188951964493		[learning rate: 0.00058253]
	Learning Rate: 0.000582527
	LOSS [training: -0.00610990455568777 | validation: 0.003448609485310874]
	TIME [epoch: 8.85 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011236504670487078		[learning rate: 0.00058182]
		[batch 20/20] avg loss: -9.164292191152823e-05		[learning rate: 0.00058112]
	Learning Rate: 0.000581116
	LOSS [training: -0.000607646694480118 | validation: -0.01185239346230358]
	TIME [epoch: 8.87 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037494486056909282		[learning rate: 0.00058041]
		[batch 20/20] avg loss: -0.0019702713002087537		[learning rate: 0.00057971]
	Learning Rate: 0.00057971
	LOSS [training: -0.0028598599529498408 | validation: -0.00868619136758858]
	TIME [epoch: 8.88 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009919604736318467		[learning rate: 0.00057901]
		[batch 20/20] avg loss: -0.010784785000864116		[learning rate: 0.00057831]
	Learning Rate: 0.000578306
	LOSS [training: -0.005888372737247982 | validation: -0.01079719815565624]
	TIME [epoch: 8.87 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024571188751252742		[learning rate: 0.00057761]
		[batch 20/20] avg loss: -0.00551986161744602		[learning rate: 0.00057691]
	Learning Rate: 0.000576906
	LOSS [training: -0.003988490246285647 | validation: -0.009658626225807494]
	TIME [epoch: 8.86 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007188386569681457		[learning rate: 0.00057621]
		[batch 20/20] avg loss: -0.0022353567772371435		[learning rate: 0.00057551]
	Learning Rate: 0.00057551
	LOSS [training: -0.0047118716734593 | validation: 0.0035545576175825795]
	TIME [epoch: 8.87 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006055980294277083		[learning rate: 0.00057481]
		[batch 20/20] avg loss: -0.0042083872256213845		[learning rate: 0.00057412]
	Learning Rate: 0.000574117
	LOSS [training: -0.005132183759949234 | validation: -0.009979191021788807]
	TIME [epoch: 8.86 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002123847688337562		[learning rate: 0.00057342]
		[batch 20/20] avg loss: -0.00513204436451208		[learning rate: 0.00057273]
	Learning Rate: 0.000572727
	LOSS [training: -0.0036279460264248215 | validation: -0.007640265201263256]
	TIME [epoch: 8.88 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004767651307805541		[learning rate: 0.00057203]
		[batch 20/20] avg loss: -0.007495603533684817		[learning rate: 0.00057134]
	Learning Rate: 0.00057134
	LOSS [training: -0.003986184332232685 | validation: 0.007273612320804505]
	TIME [epoch: 8.87 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028926423809819937		[learning rate: 0.00057065]
		[batch 20/20] avg loss: 0.003828704938361506		[learning rate: 0.00056996]
	Learning Rate: 0.000569957
	LOSS [training: 0.0033606736596717493 | validation: 0.0002629728204986971]
	TIME [epoch: 8.87 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003664213306362571		[learning rate: 0.00056927]
		[batch 20/20] avg loss: -0.0028601888636102957		[learning rate: 0.00056858]
	Learning Rate: 0.000568577
	LOSS [training: -0.003262201084986433 | validation: -0.010165608248585587]
	TIME [epoch: 8.86 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002615646087109789		[learning rate: 0.00056789]
		[batch 20/20] avg loss: -0.006377796228501964		[learning rate: 0.0005672]
	Learning Rate: 0.000567201
	LOSS [training: -0.004496721157805876 | validation: -0.010626699732204656]
	TIME [epoch: 8.86 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004292148188335021		[learning rate: 0.00056651]
		[batch 20/20] avg loss: -0.008320272014088402		[learning rate: 0.00056583]
	Learning Rate: 0.000565828
	LOSS [training: -0.006306210101211712 | validation: -0.008844365005077135]
	TIME [epoch: 8.88 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007863119240347795		[learning rate: 0.00056514]
		[batch 20/20] avg loss: -0.003775387613726435		[learning rate: 0.00056446]
	Learning Rate: 0.000564458
	LOSS [training: -0.005819253427037115 | validation: -0.00577469938468567]
	TIME [epoch: 8.87 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007589941341849535		[learning rate: 0.00056377]
		[batch 20/20] avg loss: -0.010172749698227103		[learning rate: 0.00056309]
	Learning Rate: 0.000563092
	LOSS [training: -0.0054658719162060275 | validation: -0.011950738811341182]
	TIME [epoch: 8.86 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034718785890544028		[learning rate: 0.00056241]
		[batch 20/20] avg loss: -0.008752313075092742		[learning rate: 0.00056173]
	Learning Rate: 0.000561728
	LOSS [training: -0.006112095832073573 | validation: -0.010475520471555165]
	TIME [epoch: 8.85 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005758148285818648		[learning rate: 0.00056105]
		[batch 20/20] avg loss: -0.006239247303545934		[learning rate: 0.00056037]
	Learning Rate: 0.000560369
	LOSS [training: -0.00599869779468229 | validation: -0.007084017334140276]
	TIME [epoch: 8.86 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004029015421923505		[learning rate: 0.00055969]
		[batch 20/20] avg loss: -0.0024950806816613977		[learning rate: 0.00055901]
	Learning Rate: 0.000559012
	LOSS [training: -0.0032620480517924514 | validation: -0.0070255144562129885]
	TIME [epoch: 8.89 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007214551874838519		[learning rate: 0.00055833]
		[batch 20/20] avg loss: -0.005482087481478463		[learning rate: 0.00055766]
	Learning Rate: 0.000557659
	LOSS [training: -0.006348319678158491 | validation: -0.011047689945239424]
	TIME [epoch: 8.87 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005970968073037137		[learning rate: 0.00055698]
		[batch 20/20] avg loss: -0.0003065212608583925		[learning rate: 0.00055631]
	Learning Rate: 0.000556309
	LOSS [training: -0.0031387446669477645 | validation: -0.006023740422236309]
	TIME [epoch: 8.88 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026266955308766196		[learning rate: 0.00055563]
		[batch 20/20] avg loss: -0.002478029821436987		[learning rate: 0.00055496]
	Learning Rate: 0.000554962
	LOSS [training: -0.0025523626761568036 | validation: -0.008525788363595913]
	TIME [epoch: 8.87 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0058742627376340536		[learning rate: 0.00055429]
		[batch 20/20] avg loss: -0.005429855655425546		[learning rate: 0.00055362]
	Learning Rate: 0.000553618
	LOSS [training: -0.0056520591965298 | validation: -0.01233506778950777]
	TIME [epoch: 8.87 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006943098850242338		[learning rate: 0.00055295]
		[batch 20/20] avg loss: -0.00986027305421863		[learning rate: 0.00055228]
	Learning Rate: 0.000552278
	LOSS [training: -0.008401685952230483 | validation: -0.0067697406327415525]
	TIME [epoch: 8.87 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006652691195078134		[learning rate: 0.00055161]
		[batch 20/20] avg loss: -0.002580346334532689		[learning rate: 0.00055094]
	Learning Rate: 0.000550941
	LOSS [training: -0.004616518764805411 | validation: -0.014660013482763884]
	TIME [epoch: 8.85 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030480893201567825		[learning rate: 0.00055027]
		[batch 20/20] avg loss: -0.006917010696429311		[learning rate: 0.00054961]
	Learning Rate: 0.000549608
	LOSS [training: -0.004982550008293047 | validation: -0.00809441688405843]
	TIME [epoch: 8.85 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004455196265445853		[learning rate: 0.00054894]
		[batch 20/20] avg loss: -0.007410644537751289		[learning rate: 0.00054828]
	Learning Rate: 0.000548277
	LOSS [training: -0.00593292040159857 | validation: -0.0042042254738691495]
	TIME [epoch: 8.88 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008863126501320569		[learning rate: 0.00054761]
		[batch 20/20] avg loss: -0.0015156475999409133		[learning rate: 0.00054695]
	Learning Rate: 0.00054695
	LOSS [training: -0.0012009801250364848 | validation: 0.006635972727555251]
	TIME [epoch: 8.87 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001715640374146989		[learning rate: 0.00054629]
		[batch 20/20] avg loss: -0.0017234325977663928		[learning rate: 0.00054563]
	Learning Rate: 0.000545626
	LOSS [training: -3.896111809702056e-06 | validation: -0.006535063399269506]
	TIME [epoch: 8.86 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005973686024092289		[learning rate: 0.00054496]
		[batch 20/20] avg loss: 3.7448359224103756e-05		[learning rate: 0.0005443]
	Learning Rate: 0.000544305
	LOSS [training: 0.00031740848081666614 | validation: 0.001776679855169713]
	TIME [epoch: 8.86 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0049921672719592525		[learning rate: 0.00054365]
		[batch 20/20] avg loss: -0.005274596515147844		[learning rate: 0.00054299]
	Learning Rate: 0.000542987
	LOSS [training: -0.005133381893553548 | validation: -0.011771293526489757]
	TIME [epoch: 8.86 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008684310892086006		[learning rate: 0.00054233]
		[batch 20/20] avg loss: -0.003607306748015065		[learning rate: 0.00054167]
	Learning Rate: 0.000541673
	LOSS [training: -0.006145808820050535 | validation: -0.009277019673489456]
	TIME [epoch: 8.86 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030046803987454815		[learning rate: 0.00054102]
		[batch 20/20] avg loss: -0.0035657752576153437		[learning rate: 0.00054036]
	Learning Rate: 0.000540361
	LOSS [training: -0.003285227828180412 | validation: -0.00020408752117249788]
	TIME [epoch: 8.88 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005438427570238116		[learning rate: 0.00053971]
		[batch 20/20] avg loss: -0.00504460313643834		[learning rate: 0.00053905]
	Learning Rate: 0.000539053
	LOSS [training: -0.0027942229467310758 | validation: -0.013533070124637966]
	TIME [epoch: 8.86 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030695178100351287		[learning rate: 0.0005384]
		[batch 20/20] avg loss: -0.0027946721842077187		[learning rate: 0.00053775]
	Learning Rate: 0.000537748
	LOSS [training: -0.002932094997121424 | validation: -0.005192453159660156]
	TIME [epoch: 8.87 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0077082777143554744		[learning rate: 0.0005371]
		[batch 20/20] avg loss: 0.003386126166596071		[learning rate: 0.00053645]
	Learning Rate: 0.000536446
	LOSS [training: -0.002161075773879702 | validation: 0.003970951927175781]
	TIME [epoch: 8.86 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003998967946970762		[learning rate: 0.0005358]
		[batch 20/20] avg loss: -0.00486219122083301		[learning rate: 0.00053515]
	Learning Rate: 0.000535148
	LOSS [training: -0.004430579583901886 | validation: -0.005798762965619271]
	TIME [epoch: 8.87 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003794695474535021		[learning rate: 0.0005345]
		[batch 20/20] avg loss: -0.00545380205677864		[learning rate: 0.00053385]
	Learning Rate: 0.000533852
	LOSS [training: -0.00462424876565683 | validation: -0.008402512334976686]
	TIME [epoch: 8.87 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014822009221157418		[learning rate: 0.00053321]
		[batch 20/20] avg loss: -0.007882969286674275		[learning rate: 0.00053256]
	Learning Rate: 0.00053256
	LOSS [training: -0.004682585104395008 | validation: -0.002425009918651261]
	TIME [epoch: 8.87 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00207461836469685		[learning rate: 0.00053191]
		[batch 20/20] avg loss: 0.003873651372270905		[learning rate: 0.00053127]
	Learning Rate: 0.000531271
	LOSS [training: 0.0029741348684838775 | validation: -0.004769412078429618]
	TIME [epoch: 8.86 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00477953916887339		[learning rate: 0.00053063]
		[batch 20/20] avg loss: -0.0014103915650995847		[learning rate: 0.00052998]
	Learning Rate: 0.000529985
	LOSS [training: -0.003094965366986487 | validation: -0.0053905215059930406]
	TIME [epoch: 8.86 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0044385784542632116		[learning rate: 0.00052934]
		[batch 20/20] avg loss: -0.0021053607916198323		[learning rate: 0.0005287]
	Learning Rate: 0.000528702
	LOSS [training: -0.003271969622941522 | validation: -0.013908706066779894]
	TIME [epoch: 8.87 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002993468406114994		[learning rate: 0.00052806]
		[batch 20/20] avg loss: -0.004656706552259019		[learning rate: 0.00052742]
	Learning Rate: 0.000527422
	LOSS [training: -0.0038250874791870074 | validation: -0.0046170250155487666]
	TIME [epoch: 8.86 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005148810010604425		[learning rate: 0.00052678]
		[batch 20/20] avg loss: -0.007091297769019252		[learning rate: 0.00052614]
	Learning Rate: 0.000526145
	LOSS [training: -0.00612005388981184 | validation: -0.009280645837259667]
	TIME [epoch: 8.86 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009692965437138068		[learning rate: 0.00052551]
		[batch 20/20] avg loss: -0.009973082649659938		[learning rate: 0.00052487]
	Learning Rate: 0.000524871
	LOSS [training: -0.009833024043399 | validation: -0.002696276456801209]
	TIME [epoch: 8.86 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023395109105927656		[learning rate: 0.00052424]
		[batch 20/20] avg loss: -0.010579689580999826		[learning rate: 0.0005236]
	Learning Rate: 0.0005236
	LOSS [training: -0.006459600245796296 | validation: -0.005712643879965291]
	TIME [epoch: 8.86 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023145154492096687		[learning rate: 0.00052297]
		[batch 20/20] avg loss: -0.007671235818991711		[learning rate: 0.00052233]
	Learning Rate: 0.000522333
	LOSS [training: -0.0049928756341006885 | validation: -0.013144489079796394]
	TIME [epoch: 8.89 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012783203696562526		[learning rate: 0.0005217]
		[batch 20/20] avg loss: -0.00564123526344675		[learning rate: 0.00052107]
	Learning Rate: 0.000521068
	LOSS [training: -0.009212219480004637 | validation: -0.0017252725319954472]
	TIME [epoch: 8.87 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009885231547560122		[learning rate: 0.00052044]
		[batch 20/20] avg loss: -0.00768063195167675		[learning rate: 0.00051981]
	Learning Rate: 0.000519807
	LOSS [training: -0.008782931749618436 | validation: -0.00962184381702649]
	TIME [epoch: 8.87 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007406291513078024		[learning rate: 0.00051918]
		[batch 20/20] avg loss: -0.010398234412791632		[learning rate: 0.00051855]
	Learning Rate: 0.000518549
	LOSS [training: -0.008902262962934828 | validation: -0.008213135145336113]
	TIME [epoch: 8.87 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016618063584042861		[learning rate: 0.00051792]
		[batch 20/20] avg loss: -0.007822466125083924		[learning rate: 0.00051729]
	Learning Rate: 0.000517293
	LOSS [training: -0.004742136241744107 | validation: -0.0044176231440680655]
	TIME [epoch: 8.86 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0088492920097868		[learning rate: 0.00051667]
		[batch 20/20] avg loss: -0.00034956369600518155		[learning rate: 0.00051604]
	Learning Rate: 0.000516041
	LOSS [training: -0.0045994278528959905 | validation: -0.004639179127727126]
	TIME [epoch: 8.88 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0057990506635483266		[learning rate: 0.00051542]
		[batch 20/20] avg loss: -0.0022813826439755614		[learning rate: 0.00051479]
	Learning Rate: 0.000514792
	LOSS [training: -0.004040216653761944 | validation: -0.01449899975055828]
	TIME [epoch: 8.86 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007196948032015584		[learning rate: 0.00051417]
		[batch 20/20] avg loss: -0.005701188376314305		[learning rate: 0.00051355]
	Learning Rate: 0.000513545
	LOSS [training: -0.006449068204164944 | validation: 0.004483082598239055]
	TIME [epoch: 8.87 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028694492457351124		[learning rate: 0.00051292]
		[batch 20/20] avg loss: 0.0031910414848006262		[learning rate: 0.0005123]
	Learning Rate: 0.000512302
	LOSS [training: 0.00016079611953275695 | validation: -0.008531394432737888]
	TIME [epoch: 8.86 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008213556422446323		[learning rate: 0.00051168]
		[batch 20/20] avg loss: -0.007872085537748106		[learning rate: 0.00051106]
	Learning Rate: 0.000511062
	LOSS [training: -0.008042820980097214 | validation: -0.0011878931873160559]
	TIME [epoch: 8.88 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032009178324288083		[learning rate: 0.00051044]
		[batch 20/20] avg loss: -0.0052592753776011195		[learning rate: 0.00050982]
	Learning Rate: 0.000509825
	LOSS [training: -0.0042300966050149635 | validation: -0.011358157981891426]
	TIME [epoch: 8.87 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005540955118599251		[learning rate: 0.00050921]
		[batch 20/20] avg loss: -0.005017450664541241		[learning rate: 0.00050859]
	Learning Rate: 0.000508591
	LOSS [training: -0.005279202891570247 | validation: -0.003113379929546563]
	TIME [epoch: 8.86 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004951051203277194		[learning rate: 0.00050797]
		[batch 20/20] avg loss: -0.006955009237598611		[learning rate: 0.00050736]
	Learning Rate: 0.00050736
	LOSS [training: -0.005953030220437903 | validation: -0.0020306830510446574]
	TIME [epoch: 8.86 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009205020422092607		[learning rate: 0.00050675]
		[batch 20/20] avg loss: -0.0045858565994880765		[learning rate: 0.00050613]
	Learning Rate: 0.000506131
	LOSS [training: -0.006895438510790342 | validation: -0.011880222933924392]
	TIME [epoch: 8.87 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008767119998188053		[learning rate: 0.00050552]
		[batch 20/20] avg loss: -0.00870981427384952		[learning rate: 0.00050491]
	Learning Rate: 0.000504906
	LOSS [training: -0.008738467136018788 | validation: -0.009779737329702425]
	TIME [epoch: 8.89 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008795144995650952		[learning rate: 0.00050429]
		[batch 20/20] avg loss: -0.006236541381704019		[learning rate: 0.00050368]
	Learning Rate: 0.000503684
	LOSS [training: -0.007515843188677485 | validation: -0.015313337085524747]
	TIME [epoch: 8.88 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004365010540666929		[learning rate: 0.00050307]
		[batch 20/20] avg loss: -0.006149069439056387		[learning rate: 0.00050246]
	Learning Rate: 0.000502464
	LOSS [training: -0.005257039989861658 | validation: -0.0055148838263398975]
	TIME [epoch: 8.85 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004831952902924054		[learning rate: 0.00050186]
		[batch 20/20] avg loss: -0.002465690082164783		[learning rate: 0.00050125]
	Learning Rate: 0.000501248
	LOSS [training: -0.0036488214925444185 | validation: -0.011427100323172462]
	TIME [epoch: 8.86 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00026876165104144505		[learning rate: 0.00050064]
		[batch 20/20] avg loss: -0.008665393810441069		[learning rate: 0.00050003]
	Learning Rate: 0.000500034
	LOSS [training: -0.0041983160796998115 | validation: -0.011246976457586828]
	TIME [epoch: 8.85 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038222185568735855		[learning rate: 0.00049943]
		[batch 20/20] avg loss: -0.007248719316379655		[learning rate: 0.00049882]
	Learning Rate: 0.000498824
	LOSS [training: -0.00553546893662662 | validation: -0.008596399567635988]
	TIME [epoch: 8.88 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004792810353928076		[learning rate: 0.00049822]
		[batch 20/20] avg loss: 0.0005033425008140289		[learning rate: 0.00049762]
	Learning Rate: 0.000497616
	LOSS [training: -0.0021447339265570245 | validation: 0.0016334200299824315]
	TIME [epoch: 8.86 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006924940493298065		[learning rate: 0.00049701]
		[batch 20/20] avg loss: 0.002310848833740951		[learning rate: 0.00049641]
	Learning Rate: 0.000496412
	LOSS [training: 0.0046178946635195076 | validation: -0.015015787980141252]
	TIME [epoch: 8.85 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007529421891123591		[learning rate: 0.00049581]
		[batch 20/20] avg loss: -0.008014925929801617		[learning rate: 0.00049521]
	Learning Rate: 0.00049521
	LOSS [training: -0.007772173910462601 | validation: -0.01911321784956664]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_1340.pth
	Model improved!!!
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008328232758600479		[learning rate: 0.00049461]
		[batch 20/20] avg loss: -0.003181980865489882		[learning rate: 0.00049401]
	Learning Rate: 0.000494011
	LOSS [training: -0.00575510681204518 | validation: -0.014326032612209706]
	TIME [epoch: 8.86 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0044161906071493095		[learning rate: 0.00049341]
		[batch 20/20] avg loss: -0.007213932570541827		[learning rate: 0.00049282]
	Learning Rate: 0.000492815
	LOSS [training: -0.0058150615888455685 | validation: -0.015170314074952219]
	TIME [epoch: 8.86 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007001752565745657		[learning rate: 0.00049222]
		[batch 20/20] avg loss: -0.00350541962073097		[learning rate: 0.00049162]
	Learning Rate: 0.000491622
	LOSS [training: -0.005253586093238315 | validation: -0.0017948490231093116]
	TIME [epoch: 8.86 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006196840174320453		[learning rate: 0.00049103]
		[batch 20/20] avg loss: -0.006419884209708936		[learning rate: 0.00049043]
	Learning Rate: 0.000490432
	LOSS [training: -0.0063083621920146955 | validation: -0.0051414417875598325]
	TIME [epoch: 8.85 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006399035330409972		[learning rate: 0.00048984]
		[batch 20/20] avg loss: -0.007128551961099558		[learning rate: 0.00048924]
	Learning Rate: 0.000489245
	LOSS [training: -0.0067637936457547645 | validation: -0.009900691627676088]
	TIME [epoch: 8.86 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00837491661579537		[learning rate: 0.00048865]
		[batch 20/20] avg loss: -0.0004202135002610184		[learning rate: 0.00048806]
	Learning Rate: 0.000488061
	LOSS [training: -0.004397565058028194 | validation: -0.014179726016562567]
	TIME [epoch: 8.87 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018875740335420512		[learning rate: 0.00048747]
		[batch 20/20] avg loss: -0.002956626603022071		[learning rate: 0.00048688]
	Learning Rate: 0.000486879
	LOSS [training: -0.0024221003182820612 | validation: -0.004158903044316964]
	TIME [epoch: 8.86 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006986773586728813		[learning rate: 0.00048629]
		[batch 20/20] avg loss: -0.0007071134807987066		[learning rate: 0.0004857]
	Learning Rate: 0.0004857
	LOSS [training: -0.0038469435337637596 | validation: -0.014458167877005917]
	TIME [epoch: 8.85 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008832576969745331		[learning rate: 0.00048511]
		[batch 20/20] avg loss: 0.002151667299816847		[learning rate: 0.00048452]
	Learning Rate: 0.000484525
	LOSS [training: -0.003340454834964241 | validation: -0.008518492702781526]
	TIME [epoch: 8.85 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003987599090756808		[learning rate: 0.00048394]
		[batch 20/20] avg loss: -0.0046340446514835714		[learning rate: 0.00048335]
	Learning Rate: 0.000483352
	LOSS [training: -0.004310821871120189 | validation: -0.0032619842576394878]
	TIME [epoch: 8.86 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018259347953469354		[learning rate: 0.00048277]
		[batch 20/20] avg loss: -0.005896118517863818		[learning rate: 0.00048218]
	Learning Rate: 0.000482181
	LOSS [training: -0.0038610266566053756 | validation: -0.008446279552617583]
	TIME [epoch: 8.87 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007409341623252954		[learning rate: 0.0004816]
		[batch 20/20] avg loss: -0.004275574848034473		[learning rate: 0.00048101]
	Learning Rate: 0.000481014
	LOSS [training: -0.005842458235643713 | validation: -0.012344023501034525]
	TIME [epoch: 8.85 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006882559524255519		[learning rate: 0.00048043]
		[batch 20/20] avg loss: -0.011354543042399981		[learning rate: 0.00047985]
	Learning Rate: 0.00047985
	LOSS [training: -0.00911855128332775 | validation: -0.013219095974584958]
	TIME [epoch: 8.86 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006011357544481131		[learning rate: 0.00047927]
		[batch 20/20] avg loss: -0.0054861159465551266		[learning rate: 0.00047869]
	Learning Rate: 0.000478688
	LOSS [training: -0.005748736745518129 | validation: -0.011058818584184357]
	TIME [epoch: 8.84 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004638366717921784		[learning rate: 0.00047811]
		[batch 20/20] avg loss: -0.006286590469221051		[learning rate: 0.00047753]
	Learning Rate: 0.000477529
	LOSS [training: -0.005462478593571417 | validation: -0.0019175320620297628]
	TIME [epoch: 8.86 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01236269609720323		[learning rate: 0.00047695]
		[batch 20/20] avg loss: -0.0005090752289700397		[learning rate: 0.00047637]
	Learning Rate: 0.000476373
	LOSS [training: -0.006435885663086635 | validation: 0.004642308676723884]
	TIME [epoch: 8.86 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017155517364388472		[learning rate: 0.0004758]
		[batch 20/20] avg loss: -0.0066957079294705695		[learning rate: 0.00047522]
	Learning Rate: 0.00047522
	LOSS [training: -0.004205629832954708 | validation: -0.013766030767801411]
	TIME [epoch: 8.85 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005550942289354901		[learning rate: 0.00047464]
		[batch 20/20] avg loss: -0.008192223197721664		[learning rate: 0.00047407]
	Learning Rate: 0.00047407
	LOSS [training: -0.006871582743538282 | validation: -0.014000179371786759]
	TIME [epoch: 8.86 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031127903713194112		[learning rate: 0.0004735]
		[batch 20/20] avg loss: -0.002886786875422231		[learning rate: 0.00047292]
	Learning Rate: 0.000472922
	LOSS [training: -0.0029997886233708213 | validation: -0.016211650432311445]
	TIME [epoch: 8.86 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00730407349565899		[learning rate: 0.00047235]
		[batch 20/20] avg loss: -0.00634756839056458		[learning rate: 0.00047178]
	Learning Rate: 0.000471777
	LOSS [training: -0.006825820943111786 | validation: -0.008999152684545203]
	TIME [epoch: 8.89 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004401438820189018		[learning rate: 0.00047121]
		[batch 20/20] avg loss: -0.0006693242399223468		[learning rate: 0.00047063]
	Learning Rate: 0.000470635
	LOSS [training: -0.002535381530055683 | validation: -0.00690972201835354]
	TIME [epoch: 8.87 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006824889656028892		[learning rate: 0.00047006]
		[batch 20/20] avg loss: -0.0034021341240398585		[learning rate: 0.0004695]
	Learning Rate: 0.000469496
	LOSS [training: -0.0051135118900343755 | validation: -0.007095208661918933]
	TIME [epoch: 8.85 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006877632496321359		[learning rate: 0.00046893]
		[batch 20/20] avg loss: -0.0020846633734957682		[learning rate: 0.00046836]
	Learning Rate: 0.000468359
	LOSS [training: -0.004481147934908563 | validation: -0.0044173032323967425]
	TIME [epoch: 8.85 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007374563445440002		[learning rate: 0.00046779]
		[batch 20/20] avg loss: -0.006879616790046526		[learning rate: 0.00046723]
	Learning Rate: 0.000467225
	LOSS [training: -0.007127090117743264 | validation: -0.005500876907754075]
	TIME [epoch: 8.86 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007167493381126979		[learning rate: 0.00046666]
		[batch 20/20] avg loss: -0.005505967300638936		[learning rate: 0.00046609]
	Learning Rate: 0.000466094
	LOSS [training: -0.006336730340882958 | validation: -0.009635896999672895]
	TIME [epoch: 8.88 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01106755949268256		[learning rate: 0.00046553]
		[batch 20/20] avg loss: -0.0010936670487094228		[learning rate: 0.00046497]
	Learning Rate: 0.000464966
	LOSS [training: -0.006080613270695991 | validation: -0.00018017024495008355]
	TIME [epoch: 8.85 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035579567374607137		[learning rate: 0.0004644]
		[batch 20/20] avg loss: -0.00138441190880232		[learning rate: 0.00046384]
	Learning Rate: 0.00046384
	LOSS [training: -0.002471184323131517 | validation: -0.011350509232166002]
	TIME [epoch: 8.85 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024679332005491583		[learning rate: 0.00046328]
		[batch 20/20] avg loss: 0.000801239104113299		[learning rate: 0.00046272]
	Learning Rate: 0.000462717
	LOSS [training: 0.0016345861523312285 | validation: -0.006503384504368578]
	TIME [epoch: 8.86 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003933338228328821		[learning rate: 0.00046216]
		[batch 20/20] avg loss: 0.0021532112571983936		[learning rate: 0.0004616]
	Learning Rate: 0.000461597
	LOSS [training: -0.0008900634855652142 | validation: -0.008856033659905518]
	TIME [epoch: 8.87 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004632504083248402		[learning rate: 0.00046104]
		[batch 20/20] avg loss: -0.004612909257261497		[learning rate: 0.00046048]
	Learning Rate: 0.00046048
	LOSS [training: -0.004622706670254949 | validation: -0.0128326905110802]
	TIME [epoch: 8.87 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024458381720407047		[learning rate: 0.00045992]
		[batch 20/20] avg loss: -0.003253127526055837		[learning rate: 0.00045937]
	Learning Rate: 0.000459365
	LOSS [training: -0.0028494828490482713 | validation: -0.015283398500457386]
	TIME [epoch: 8.86 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021046849765371705		[learning rate: 0.00045881]
		[batch 20/20] avg loss: -0.0003784189601919367		[learning rate: 0.00045825]
	Learning Rate: 0.000458253
	LOSS [training: -0.0012415519683645535 | validation: -0.009540019014015699]
	TIME [epoch: 8.86 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003915516412408699		[learning rate: 0.0004577]
		[batch 20/20] avg loss: 0.0034461292107706627		[learning rate: 0.00045714]
	Learning Rate: 0.000457144
	LOSS [training: -0.0002346936008190181 | validation: 0.002935845676430187]
	TIME [epoch: 8.86 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002674702280584288		[learning rate: 0.00045659]
		[batch 20/20] avg loss: -0.004122063894431684		[learning rate: 0.00045604]
	Learning Rate: 0.000456037
	LOSS [training: -0.0033983830875079857 | validation: -0.016010826339667276]
	TIME [epoch: 8.87 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002513931794251916		[learning rate: 0.00045548]
		[batch 20/20] avg loss: -0.0006891857603113386		[learning rate: 0.00045493]
	Learning Rate: 0.000454933
	LOSS [training: -0.0016015587772816274 | validation: -0.0022064151347978927]
	TIME [epoch: 8.86 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005543551390004448		[learning rate: 0.00045438]
		[batch 20/20] avg loss: -0.004592407723848035		[learning rate: 0.00045383]
	Learning Rate: 0.000453832
	LOSS [training: -0.005067979556926241 | validation: -0.004549228998973086]
	TIME [epoch: 8.86 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003116471582304515		[learning rate: 0.00045328]
		[batch 20/20] avg loss: -0.004434417956440451		[learning rate: 0.00045273]
	Learning Rate: 0.000452733
	LOSS [training: -0.0037754447693724826 | validation: -0.008961514467696783]
	TIME [epoch: 8.86 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011972459629858892		[learning rate: 0.00045218]
		[batch 20/20] avg loss: -0.0028739125635977017		[learning rate: 0.00045164]
	Learning Rate: 0.000451637
	LOSS [training: -0.0074231860967282955 | validation: -0.005035616057112769]
	TIME [epoch: 8.85 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00964756882123444		[learning rate: 0.00045109]
		[batch 20/20] avg loss: -0.006666298474637199		[learning rate: 0.00045054]
	Learning Rate: 0.000450544
	LOSS [training: -0.00815693364793582 | validation: -0.013469519363335753]
	TIME [epoch: 8.89 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007523876946387992		[learning rate: 0.00045]
		[batch 20/20] avg loss: -0.0034524718410281034		[learning rate: 0.00044945]
	Learning Rate: 0.000449453
	LOSS [training: -0.005488174393708048 | validation: -0.002649253369679867]
	TIME [epoch: 8.87 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004814668312733068		[learning rate: 0.00044891]
		[batch 20/20] avg loss: 0.0012257684881796565		[learning rate: 0.00044836]
	Learning Rate: 0.000448365
	LOSS [training: 0.0008536176597264817 | validation: -0.007556023419953749]
	TIME [epoch: 8.86 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005613182284205595		[learning rate: 0.00044782]
		[batch 20/20] avg loss: -0.0046299892817400976		[learning rate: 0.00044728]
	Learning Rate: 0.000447279
	LOSS [training: -0.005121585782972845 | validation: -0.014583943732232959]
	TIME [epoch: 8.86 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002242226475926974		[learning rate: 0.00044674]
		[batch 20/20] avg loss: -0.0008812239702726895		[learning rate: 0.0004462]
	Learning Rate: 0.000446197
	LOSS [training: -0.0015617252230998315 | validation: -0.007783997675394273]
	TIME [epoch: 8.85 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0072734522902111074		[learning rate: 0.00044566]
		[batch 20/20] avg loss: -0.006453897248241326		[learning rate: 0.00044512]
	Learning Rate: 0.000445117
	LOSS [training: -0.006863674769226216 | validation: -0.010841048816094146]
	TIME [epoch: 8.88 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036082590036833158		[learning rate: 0.00044458]
		[batch 20/20] avg loss: -0.005339419065339914		[learning rate: 0.00044404]
	Learning Rate: 0.000444039
	LOSS [training: -0.004473839034511616 | validation: -0.008901611552712215]
	TIME [epoch: 8.86 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014478298391295383		[learning rate: 0.0004435]
		[batch 20/20] avg loss: -0.009927049924999353		[learning rate: 0.00044296]
	Learning Rate: 0.000442964
	LOSS [training: -0.0056874398820644465 | validation: -0.009509852807015738]
	TIME [epoch: 8.86 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005202780755229277		[learning rate: 0.00044243]
		[batch 20/20] avg loss: -0.004241362220020533		[learning rate: 0.00044189]
	Learning Rate: 0.000441892
	LOSS [training: -0.004722071487624905 | validation: -0.009452765077385341]
	TIME [epoch: 8.86 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004967308436238038		[learning rate: 0.00044136]
		[batch 20/20] avg loss: -0.008423597946005052		[learning rate: 0.00044082]
	Learning Rate: 0.000440822
	LOSS [training: -0.0066954531911215445 | validation: -0.0056939650589874135]
	TIME [epoch: 8.88 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034813936895056233		[learning rate: 0.00044029]
		[batch 20/20] avg loss: -0.007521724508261629		[learning rate: 0.00043975]
	Learning Rate: 0.000439755
	LOSS [training: -0.005501559098883625 | validation: -0.009739067460420237]
	TIME [epoch: 8.85 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0062403051148692415		[learning rate: 0.00043922]
		[batch 20/20] avg loss: -0.0019701393929534407		[learning rate: 0.00043869]
	Learning Rate: 0.00043869
	LOSS [training: -0.00410522225391134 | validation: -0.005419055937111692]
	TIME [epoch: 8.84 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037251913670803155		[learning rate: 0.00043816]
		[batch 20/20] avg loss: -0.00887717731456329		[learning rate: 0.00043763]
	Learning Rate: 0.000437628
	LOSS [training: -0.006301184340821804 | validation: -0.010454177028965232]
	TIME [epoch: 8.85 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006768347331793495		[learning rate: 0.0004371]
		[batch 20/20] avg loss: -0.007832134387508544		[learning rate: 0.00043657]
	Learning Rate: 0.000436569
	LOSS [training: -0.007300240859651019 | validation: -0.00788279537276366]
	TIME [epoch: 8.85 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00445644559326711		[learning rate: 0.00043604]
		[batch 20/20] avg loss: -0.003812405631009281		[learning rate: 0.00043551]
	Learning Rate: 0.000435512
	LOSS [training: -0.004134425612138195 | validation: -0.005605260374047917]
	TIME [epoch: 8.86 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006865812195206887		[learning rate: 0.00043498]
		[batch 20/20] avg loss: -0.010284086211146655		[learning rate: 0.00043446]
	Learning Rate: 0.000434458
	LOSS [training: -0.008574949203176774 | validation: -0.006564714382485653]
	TIME [epoch: 8.86 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00838299380077412		[learning rate: 0.00043393]
		[batch 20/20] avg loss: -0.003435993072303496		[learning rate: 0.00043341]
	Learning Rate: 0.000433406
	LOSS [training: -0.005909493436538809 | validation: -0.007532255617719684]
	TIME [epoch: 8.84 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009370613799002033		[learning rate: 0.00043288]
		[batch 20/20] avg loss: -0.010442892617550914		[learning rate: 0.00043236]
	Learning Rate: 0.000432357
	LOSS [training: -0.0047529156188253555 | validation: -0.016889104711299428]
	TIME [epoch: 8.85 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007746952732780088		[learning rate: 0.00043183]
		[batch 20/20] avg loss: -0.0022946395400547663		[learning rate: 0.00043131]
	Learning Rate: 0.00043131
	LOSS [training: -0.005020796136417429 | validation: -0.013188867909825392]
	TIME [epoch: 8.85 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007438725674258205		[learning rate: 0.00043079]
		[batch 20/20] avg loss: -0.0071498361025124375		[learning rate: 0.00043027]
	Learning Rate: 0.000430266
	LOSS [training: -0.007294280888385321 | validation: -0.012594543083044016]
	TIME [epoch: 8.88 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006182290780826218		[learning rate: 0.00042974]
		[batch 20/20] avg loss: -0.004910000456929323		[learning rate: 0.00042922]
	Learning Rate: 0.000429224
	LOSS [training: -0.00554614561887777 | validation: -0.005916795982019878]
	TIME [epoch: 8.87 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00267428929053638		[learning rate: 0.0004287]
		[batch 20/20] avg loss: -0.004876599370342542		[learning rate: 0.00042819]
	Learning Rate: 0.000428185
	LOSS [training: -0.003775444330439461 | validation: -0.01015250168447186]
	TIME [epoch: 8.85 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00878544050350376		[learning rate: 0.00042767]
		[batch 20/20] avg loss: -0.0038142039167401234		[learning rate: 0.00042715]
	Learning Rate: 0.000427149
	LOSS [training: -0.006299822210121942 | validation: -0.007843066979845867]
	TIME [epoch: 8.85 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008073974008507099		[learning rate: 0.00042663]
		[batch 20/20] avg loss: -0.005750223048700556		[learning rate: 0.00042611]
	Learning Rate: 0.000426115
	LOSS [training: -0.003278810224775632 | validation: -0.003253920106725993]
	TIME [epoch: 8.86 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037226121428334835		[learning rate: 0.0004256]
		[batch 20/20] avg loss: 0.0021294361108238407		[learning rate: 0.00042508]
	Learning Rate: 0.000425083
	LOSS [training: -0.0007965880160048213 | validation: -0.010330056524987866]
	TIME [epoch: 8.87 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0045268846488866876		[learning rate: 0.00042457]
		[batch 20/20] avg loss: -0.0016348070078891358		[learning rate: 0.00042405]
	Learning Rate: 0.000424054
	LOSS [training: -0.003080845828387911 | validation: -0.00040640418966483545]
	TIME [epoch: 8.86 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00114781265963432		[learning rate: 0.00042354]
		[batch 20/20] avg loss: -0.00842797230881719		[learning rate: 0.00042303]
	Learning Rate: 0.000423027
	LOSS [training: -0.0036400798245914334 | validation: -0.0094447622296156]
	TIME [epoch: 8.83 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006475557255803802		[learning rate: 0.00042251]
		[batch 20/20] avg loss: -0.005948614416495893		[learning rate: 0.000422]
	Learning Rate: 0.000422003
	LOSS [training: -0.006212085836149848 | validation: -0.0026614485020984725]
	TIME [epoch: 8.85 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003928775709893239		[learning rate: 0.00042149]
		[batch 20/20] avg loss: -0.007656622154487531		[learning rate: 0.00042098]
	Learning Rate: 0.000420982
	LOSS [training: -0.0057926989321903846 | validation: -0.01087040707636529]
	TIME [epoch: 8.87 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004116644335892527		[learning rate: 0.00042047]
		[batch 20/20] avg loss: -0.006797414410917553		[learning rate: 0.00041996]
	Learning Rate: 0.000419963
	LOSS [training: -0.00545702937340504 | validation: -0.0020813094628936452]
	TIME [epoch: 8.85 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0056925721785235734		[learning rate: 0.00041945]
		[batch 20/20] avg loss: -0.002009203016639332		[learning rate: 0.00041895]
	Learning Rate: 0.000418946
	LOSS [training: -0.0038508875975814525 | validation: -0.011323346580374089]
	TIME [epoch: 8.85 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009761082531534881		[learning rate: 0.00041844]
		[batch 20/20] avg loss: -0.005151237559583935		[learning rate: 0.00041793]
	Learning Rate: 0.000417932
	LOSS [training: -0.002087564653215223 | validation: -0.009326198004864198]
	TIME [epoch: 8.84 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004835588278156522		[learning rate: 0.00041743]
		[batch 20/20] avg loss: -0.0037174861395969103		[learning rate: 0.00041692]
	Learning Rate: 0.00041692
	LOSS [training: -0.004276537208876717 | validation: -0.009670754094761277]
	TIME [epoch: 8.84 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005016395130030741		[learning rate: 0.00041641]
		[batch 20/20] avg loss: -0.004007843797176164		[learning rate: 0.00041591]
	Learning Rate: 0.000415911
	LOSS [training: -0.004512119463603454 | validation: -0.007306783804834833]
	TIME [epoch: 8.88 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00881408802421146		[learning rate: 0.00041541]
		[batch 20/20] avg loss: -0.004743508107536005		[learning rate: 0.0004149]
	Learning Rate: 0.000414904
	LOSS [training: -0.006778798065873733 | validation: -0.008349833659269428]
	TIME [epoch: 8.84 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008352834626838784		[learning rate: 0.0004144]
		[batch 20/20] avg loss: -0.004186035933551857		[learning rate: 0.0004139]
	Learning Rate: 0.000413899
	LOSS [training: -0.00626943528019532 | validation: -0.01422716982908731]
	TIME [epoch: 8.85 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01089031601469852		[learning rate: 0.0004134]
		[batch 20/20] avg loss: -0.005382520938817625		[learning rate: 0.0004129]
	Learning Rate: 0.000412897
	LOSS [training: -0.008136418476758073 | validation: -0.011296964892931511]
	TIME [epoch: 8.85 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0110716205674413		[learning rate: 0.0004124]
		[batch 20/20] avg loss: -0.003766659069178404		[learning rate: 0.0004119]
	Learning Rate: 0.000411898
	LOSS [training: -0.007419139818309849 | validation: -0.008689566207256059]
	TIME [epoch: 8.85 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00362068822481004		[learning rate: 0.0004114]
		[batch 20/20] avg loss: -0.005356683802631415		[learning rate: 0.0004109]
	Learning Rate: 0.000410901
	LOSS [training: -0.004488686013720726 | validation: -0.005134160458577378]
	TIME [epoch: 8.87 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007194358838684205		[learning rate: 0.0004104]
		[batch 20/20] avg loss: -0.00771744855719632		[learning rate: 0.00040991]
	Learning Rate: 0.000409906
	LOSS [training: -0.007455903697940261 | validation: -0.007387156804698787]
	TIME [epoch: 8.85 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004050276310930486		[learning rate: 0.00040941]
		[batch 20/20] avg loss: 0.00047926020134393416		[learning rate: 0.00040891]
	Learning Rate: 0.000408914
	LOSS [training: -0.001785508054793275 | validation: -0.010124600258771696]
	TIME [epoch: 8.85 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004567567300483378		[learning rate: 0.00040842]
		[batch 20/20] avg loss: -0.0021929638095366044		[learning rate: 0.00040792]
	Learning Rate: 0.000407924
	LOSS [training: -0.0033802655550099897 | validation: -0.008290838825163009]
	TIME [epoch: 8.85 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007768074916189988		[learning rate: 0.00040743]
		[batch 20/20] avg loss: -0.007757486560360127		[learning rate: 0.00040694]
	Learning Rate: 0.000406936
	LOSS [training: -0.0034903395343705637 | validation: -0.010332789728987681]
	TIME [epoch: 8.88 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006597191943286601		[learning rate: 0.00040644]
		[batch 20/20] avg loss: -0.004294353778303927		[learning rate: 0.00040595]
	Learning Rate: 0.000405951
	LOSS [training: -0.005445772860795265 | validation: -0.010462283737014067]
	TIME [epoch: 8.85 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009108122541679625		[learning rate: 0.00040546]
		[batch 20/20] avg loss: -0.0031741264047442075		[learning rate: 0.00040497]
	Learning Rate: 0.000404968
	LOSS [training: -0.006141124473211916 | validation: -0.015370450606575424]
	TIME [epoch: 8.85 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004544207546874913		[learning rate: 0.00040448]
		[batch 20/20] avg loss: -0.009740403657943196		[learning rate: 0.00040399]
	Learning Rate: 0.000403988
	LOSS [training: -0.007142305602409055 | validation: -0.008399608392521235]
	TIME [epoch: 8.85 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006295925462841155		[learning rate: 0.0004035]
		[batch 20/20] avg loss: -0.006395884058323313		[learning rate: 0.00040301]
	Learning Rate: 0.00040301
	LOSS [training: -0.006345904760582234 | validation: -0.008873391946992616]
	TIME [epoch: 8.84 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00745108422017564		[learning rate: 0.00040252]
		[batch 20/20] avg loss: -0.009177501637857145		[learning rate: 0.00040203]
	Learning Rate: 0.000402034
	LOSS [training: -0.008314292929016394 | validation: -0.015284472312208251]
	TIME [epoch: 8.88 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004873225971257713		[learning rate: 0.00040155]
		[batch 20/20] avg loss: -0.0025158305931985763		[learning rate: 0.00040106]
	Learning Rate: 0.000401061
	LOSS [training: -0.0036945282822281452 | validation: -0.007832112009039816]
	TIME [epoch: 8.86 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022159223211684507		[learning rate: 0.00040058]
		[batch 20/20] avg loss: -0.009394022779192745		[learning rate: 0.00040009]
	Learning Rate: 0.00040009
	LOSS [training: -0.0058049725501805975 | validation: -0.014522753058650898]
	TIME [epoch: 8.85 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009908961714428273		[learning rate: 0.00039961]
		[batch 20/20] avg loss: -0.0075741992994219675		[learning rate: 0.00039912]
	Learning Rate: 0.000399122
	LOSS [training: -0.00874158050692512 | validation: -0.009010322433420872]
	TIME [epoch: 8.84 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023720247840603994		[learning rate: 0.00039864]
		[batch 20/20] avg loss: -0.009256477622159687		[learning rate: 0.00039816]
	Learning Rate: 0.000398155
	LOSS [training: -0.005814251203110044 | validation: -0.007070054035973028]
	TIME [epoch: 8.85 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009481784887006944		[learning rate: 0.00039767]
		[batch 20/20] avg loss: -0.003383494928098409		[learning rate: 0.00039719]
	Learning Rate: 0.000397192
	LOSS [training: -0.0064326399075526775 | validation: -0.007711572114004147]
	TIME [epoch: 8.87 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008188582127115521		[learning rate: 0.00039671]
		[batch 20/20] avg loss: -0.006010334296503833		[learning rate: 0.00039623]
	Learning Rate: 0.00039623
	LOSS [training: -0.007099458211809677 | validation: -0.00048238737909029076]
	TIME [epoch: 8.86 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036904695072427114		[learning rate: 0.00039575]
		[batch 20/20] avg loss: -0.006466187675127316		[learning rate: 0.00039527]
	Learning Rate: 0.000395271
	LOSS [training: -0.005078328591185015 | validation: -0.012573145598850012]
	TIME [epoch: 8.85 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004619512196797638		[learning rate: 0.00039479]
		[batch 20/20] avg loss: -0.0061196393423343515		[learning rate: 0.00039431]
	Learning Rate: 0.000394314
	LOSS [training: -0.005369575769565994 | validation: -0.01168051301256156]
	TIME [epoch: 8.85 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009068174810212224		[learning rate: 0.00039384]
		[batch 20/20] avg loss: -0.008767840829073444		[learning rate: 0.00039336]
	Learning Rate: 0.000393359
	LOSS [training: -0.004837329155047334 | validation: -0.011867886968693595]
	TIME [epoch: 8.85 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029415607756957604		[learning rate: 0.00039288]
		[batch 20/20] avg loss: -0.008981498726040931		[learning rate: 0.00039241]
	Learning Rate: 0.000392407
	LOSS [training: -0.005961529750868345 | validation: -0.008467888319094553]
	TIME [epoch: 8.86 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028230695922352883		[learning rate: 0.00039193]
		[batch 20/20] avg loss: -0.004131870601527079		[learning rate: 0.00039146]
	Learning Rate: 0.000391457
	LOSS [training: -0.0034774700968811835 | validation: -0.013208309383180558]
	TIME [epoch: 8.84 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019016366455626057		[learning rate: 0.00039098]
		[batch 20/20] avg loss: -0.003672906845427479		[learning rate: 0.00039051]
	Learning Rate: 0.00039051
	LOSS [training: -0.0027872717454950416 | validation: -0.009009700478399996]
	TIME [epoch: 8.84 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002339471140051585		[learning rate: 0.00039004]
		[batch 20/20] avg loss: -0.008717321863695907		[learning rate: 0.00038956]
	Learning Rate: 0.000389564
	LOSS [training: -0.005528396501873746 | validation: -0.002384683893100295]
	TIME [epoch: 8.85 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007446353008597563		[learning rate: 0.00038909]
		[batch 20/20] avg loss: -0.0050033283165902415		[learning rate: 0.00038862]
	Learning Rate: 0.000388621
	LOSS [training: -0.006224840662593902 | validation: -0.008738869385556809]
	TIME [epoch: 8.88 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009322981492240524		[learning rate: 0.00038815]
		[batch 20/20] avg loss: -0.006967043285017506		[learning rate: 0.00038768]
	Learning Rate: 0.00038768
	LOSS [training: -0.008145012388629016 | validation: -0.005723722270125028]
	TIME [epoch: 8.86 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010672115558472337		[learning rate: 0.00038721]
		[batch 20/20] avg loss: -0.00617671377065763		[learning rate: 0.00038674]
	Learning Rate: 0.000386742
	LOSS [training: -0.008424414664564981 | validation: -0.011913968857041206]
	TIME [epoch: 8.85 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004084740875683051		[learning rate: 0.00038627]
		[batch 20/20] avg loss: -0.010031025173868157		[learning rate: 0.00038581]
	Learning Rate: 0.000385805
	LOSS [training: -0.002973142149092552 | validation: -0.013849406610808322]
	TIME [epoch: 8.86 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007706241075126926		[learning rate: 0.00038534]
		[batch 20/20] avg loss: -0.010520718241403448		[learning rate: 0.00038487]
	Learning Rate: 0.000384872
	LOSS [training: -0.009113479658265188 | validation: -0.008185503958120044]
	TIME [epoch: 8.85 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031931049708558332		[learning rate: 0.00038441]
		[batch 20/20] avg loss: 0.001183103101020706		[learning rate: 0.00038394]
	Learning Rate: 0.00038394
	LOSS [training: -0.0010050009349175635 | validation: -0.005528172365212187]
	TIME [epoch: 8.88 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005761901794001116		[learning rate: 0.00038347]
		[batch 20/20] avg loss: -0.005910720808475152		[learning rate: 0.00038301]
	Learning Rate: 0.00038301
	LOSS [training: -0.005836311301238134 | validation: -0.015406416538371704]
	TIME [epoch: 8.85 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00516603736375188		[learning rate: 0.00038255]
		[batch 20/20] avg loss: -0.01366745915609736		[learning rate: 0.00038208]
	Learning Rate: 0.000382083
	LOSS [training: -0.009416748259924619 | validation: -0.009919798932404827]
	TIME [epoch: 8.86 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00682035971069609		[learning rate: 0.00038162]
		[batch 20/20] avg loss: -0.003913987714268731		[learning rate: 0.00038116]
	Learning Rate: 0.000381158
	LOSS [training: -0.005367173712482411 | validation: -0.005937087011531016]
	TIME [epoch: 8.84 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010528571044331496		[learning rate: 0.0003807]
		[batch 20/20] avg loss: -0.0026002921778439316		[learning rate: 0.00038024]
	Learning Rate: 0.000380235
	LOSS [training: -0.0018265746411385403 | validation: -0.002598742524567285]
	TIME [epoch: 8.85 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008253639071195444		[learning rate: 0.00037977]
		[batch 20/20] avg loss: -0.009476843750844274		[learning rate: 0.00037931]
	Learning Rate: 0.000379315
	LOSS [training: -0.008865241411019858 | validation: -0.013224677561700917]
	TIME [epoch: 8.87 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0048685186133625834		[learning rate: 0.00037886]
		[batch 20/20] avg loss: -0.0048286513645897835		[learning rate: 0.0003784]
	Learning Rate: 0.000378397
	LOSS [training: -0.0048485849889761835 | validation: -0.007081630270138289]
	TIME [epoch: 8.86 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015559104224804326		[learning rate: 0.00037794]
		[batch 20/20] avg loss: -0.0016379573152349417		[learning rate: 0.00037748]
	Learning Rate: 0.000377481
	LOSS [training: -0.0015969338688576872 | validation: -0.0055642989303744264]
	TIME [epoch: 8.86 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026779592993767293		[learning rate: 0.00037702]
		[batch 20/20] avg loss: -0.009433091137923972		[learning rate: 0.00037657]
	Learning Rate: 0.000376567
	LOSS [training: -0.006055525218650351 | validation: -0.006381700677657753]
	TIME [epoch: 8.85 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005866271857689861		[learning rate: 0.00037611]
		[batch 20/20] avg loss: -0.009991644528242706		[learning rate: 0.00037566]
	Learning Rate: 0.000375655
	LOSS [training: -0.007928958192966283 | validation: -0.007268129158316443]
	TIME [epoch: 8.86 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011041279706565318		[learning rate: 0.0003752]
		[batch 20/20] avg loss: -0.006123519959756652		[learning rate: 0.00037475]
	Learning Rate: 0.000374746
	LOSS [training: -0.008582399833160984 | validation: -0.00840891990539739]
	TIME [epoch: 8.86 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006944409221840806		[learning rate: 0.00037429]
		[batch 20/20] avg loss: -0.0019969916226517414		[learning rate: 0.00037384]
	Learning Rate: 0.000373839
	LOSS [training: -0.004470700422246274 | validation: -0.010058461523479115]
	TIME [epoch: 8.84 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00582058387929929		[learning rate: 0.00037339]
		[batch 20/20] avg loss: -0.009328444900200262		[learning rate: 0.00037293]
	Learning Rate: 0.000372934
	LOSS [training: -0.007574514389749776 | validation: -0.002235274531236069]
	TIME [epoch: 8.84 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010274733044300636		[learning rate: 0.00037248]
		[batch 20/20] avg loss: -0.004669500331444395		[learning rate: 0.00037203]
	Learning Rate: 0.000372031
	LOSS [training: -0.007472116687872515 | validation: -0.011466114353917731]
	TIME [epoch: 8.86 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008787623866942224		[learning rate: 0.00037158]
		[batch 20/20] avg loss: -0.005783084668755819		[learning rate: 0.00037113]
	Learning Rate: 0.00037113
	LOSS [training: -0.007285354267849022 | validation: -0.005532212245178086]
	TIME [epoch: 8.88 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004679877421129476		[learning rate: 0.00037068]
		[batch 20/20] avg loss: -0.004395638451858369		[learning rate: 0.00037023]
	Learning Rate: 0.000370232
	LOSS [training: -0.004537757936493922 | validation: -0.014768710604273537]
	TIME [epoch: 8.85 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008662813981614245		[learning rate: 0.00036978]
		[batch 20/20] avg loss: -0.008088109155268197		[learning rate: 0.00036934]
	Learning Rate: 0.000369336
	LOSS [training: -0.008375461568441222 | validation: -0.010332067389574023]
	TIME [epoch: 8.86 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009637355549421236		[learning rate: 0.00036889]
		[batch 20/20] avg loss: -0.009928008299590878		[learning rate: 0.00036844]
	Learning Rate: 0.000368441
	LOSS [training: -0.00978268192450606 | validation: -0.007874199254438542]
	TIME [epoch: 8.84 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005446726447280753		[learning rate: 0.000368]
		[batch 20/20] avg loss: -0.01313716410789017		[learning rate: 0.00036755]
	Learning Rate: 0.00036755
	LOSS [training: -0.009291945277585461 | validation: -0.0056366227223557915]
	TIME [epoch: 8.85 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006549682022802856		[learning rate: 0.0003671]
		[batch 20/20] avg loss: 0.0016949095606814842		[learning rate: 0.00036666]
	Learning Rate: 0.00036666
	LOSS [training: -0.002427386231060686 | validation: -0.0012009101481830994]
	TIME [epoch: 8.89 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004922533790701114		[learning rate: 0.00036622]
		[batch 20/20] avg loss: 0.004108641505359076		[learning rate: 0.00036577]
	Learning Rate: 0.000365772
	LOSS [training: -0.00040694614267101887 | validation: -0.011107656596846135]
	TIME [epoch: 8.86 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005197499765983815		[learning rate: 0.00036533]
		[batch 20/20] avg loss: -0.006543844633828633		[learning rate: 0.00036489]
	Learning Rate: 0.000364887
	LOSS [training: -0.005870672199906225 | validation: -0.009570936205849489]
	TIME [epoch: 8.86 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006453867511210297		[learning rate: 0.00036444]
		[batch 20/20] avg loss: -0.005030044136837373		[learning rate: 0.000364]
	Learning Rate: 0.000364003
	LOSS [training: -0.005741955824023835 | validation: -0.00497705772976694]
	TIME [epoch: 8.86 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007380537651518078		[learning rate: 0.00036356]
		[batch 20/20] avg loss: -0.007681531545621488		[learning rate: 0.00036312]
	Learning Rate: 0.000363122
	LOSS [training: -0.007531034598569783 | validation: -0.008987505390195565]
	TIME [epoch: 8.88 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005123784412159181		[learning rate: 0.00036268]
		[batch 20/20] avg loss: -0.003812655349553501		[learning rate: 0.00036224]
	Learning Rate: 0.000362243
	LOSS [training: -0.004468219880856342 | validation: -0.0043373272626390106]
	TIME [epoch: 8.86 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005086806073660438		[learning rate: 0.0003618]
		[batch 20/20] avg loss: -0.006311834917157752		[learning rate: 0.00036137]
	Learning Rate: 0.000361366
	LOSS [training: -0.005699320495409095 | validation: -0.01196619132031107]
	TIME [epoch: 8.86 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009092645659548926		[learning rate: 0.00036093]
		[batch 20/20] avg loss: -0.0052862907420551265		[learning rate: 0.00036049]
	Learning Rate: 0.000360491
	LOSS [training: -0.007189468200802025 | validation: -0.0036831862291572823]
	TIME [epoch: 8.86 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003890345095863728		[learning rate: 0.00036005]
		[batch 20/20] avg loss: 0.0028670513303950115		[learning rate: 0.00035962]
	Learning Rate: 0.000359619
	LOSS [training: -0.0005116468827343578 | validation: -0.0075429295818626495]
	TIME [epoch: 8.85 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005269739421324976		[learning rate: 0.00035918]
		[batch 20/20] avg loss: -0.004455297389104751		[learning rate: 0.00035875]
	Learning Rate: 0.000358748
	LOSS [training: -0.004862518405214864 | validation: -0.006623166314558315]
	TIME [epoch: 8.87 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004640096623754487		[learning rate: 0.00035831]
		[batch 20/20] avg loss: -0.007504465273024875		[learning rate: 0.00035788]
	Learning Rate: 0.00035788
	LOSS [training: -0.00607228094838968 | validation: -0.010629412044134173]
	TIME [epoch: 8.86 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005306631990084404		[learning rate: 0.00035745]
		[batch 20/20] avg loss: -0.009915931290607412		[learning rate: 0.00035701]
	Learning Rate: 0.000357013
	LOSS [training: -0.005223297244807927 | validation: -0.010188221692317886]
	TIME [epoch: 8.86 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006107865250712361		[learning rate: 0.00035658]
		[batch 20/20] avg loss: -0.009721514204340653		[learning rate: 0.00035615]
	Learning Rate: 0.000356149
	LOSS [training: -0.007914689727526507 | validation: -0.012371879941390725]
	TIME [epoch: 8.86 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010679652109243288		[learning rate: 0.00035572]
		[batch 20/20] avg loss: -0.00677106706507917		[learning rate: 0.00035529]
	Learning Rate: 0.000355287
	LOSS [training: -0.008725359587161228 | validation: -0.009796419782336885]
	TIME [epoch: 8.85 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008401675865040332		[learning rate: 0.00035486]
		[batch 20/20] avg loss: -0.010812224204981452		[learning rate: 0.00035443]
	Learning Rate: 0.000354427
	LOSS [training: -0.009606950035010893 | validation: -0.004463673782282326]
	TIME [epoch: 8.88 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010302389802628814		[learning rate: 0.000354]
		[batch 20/20] avg loss: -0.011338661604971417		[learning rate: 0.00035357]
	Learning Rate: 0.000353569
	LOSS [training: -0.010820525703800116 | validation: -0.008552343251859894]
	TIME [epoch: 8.86 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007607973660482244		[learning rate: 0.00035314]
		[batch 20/20] avg loss: -0.008677818988563388		[learning rate: 0.00035271]
	Learning Rate: 0.000352713
	LOSS [training: -0.008142896324522817 | validation: -0.00582573152099565]
	TIME [epoch: 8.86 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00813568600850043		[learning rate: 0.00035229]
		[batch 20/20] avg loss: -0.006491696287073007		[learning rate: 0.00035186]
	Learning Rate: 0.000351859
	LOSS [training: -0.0073136911477867175 | validation: -0.00859888883577964]
	TIME [epoch: 8.86 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007986102437434337		[learning rate: 0.00035143]
		[batch 20/20] avg loss: -0.002399691004760723		[learning rate: 0.00035101]
	Learning Rate: 0.000351007
	LOSS [training: -0.00519289672109753 | validation: -0.0021274432447027565]
	TIME [epoch: 8.88 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007230174687717623		[learning rate: 0.00035058]
		[batch 20/20] avg loss: -0.005435738069106009		[learning rate: 0.00035016]
	Learning Rate: 0.000350157
	LOSS [training: -0.006332956378411815 | validation: -0.012061437750692683]
	TIME [epoch: 8.87 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008429450330233604		[learning rate: 0.00034973]
		[batch 20/20] avg loss: -0.009094079017945848		[learning rate: 0.00034931]
	Learning Rate: 0.00034931
	LOSS [training: -0.008761764674089725 | validation: -0.006997101783796728]
	TIME [epoch: 8.86 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003285087728905125		[learning rate: 0.00034889]
		[batch 20/20] avg loss: -0.0068726476647273495		[learning rate: 0.00034846]
	Learning Rate: 0.000348464
	LOSS [training: -0.003272069445918418 | validation: -0.004261625588635945]
	TIME [epoch: 8.86 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009615751068319568		[learning rate: 0.00034804]
		[batch 20/20] avg loss: -0.009911565912154464		[learning rate: 0.00034762]
	Learning Rate: 0.00034762
	LOSS [training: -0.009763658490237016 | validation: -0.007065654505662332]
	TIME [epoch: 8.85 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00908479380121452		[learning rate: 0.0003472]
		[batch 20/20] avg loss: -0.007764861991446513		[learning rate: 0.00034678]
	Learning Rate: 0.000346779
	LOSS [training: -0.008424827896330516 | validation: -0.01374335025551749]
	TIME [epoch: 8.87 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003552651309390797		[learning rate: 0.00034636]
		[batch 20/20] avg loss: -0.008548142277459087		[learning rate: 0.00034594]
	Learning Rate: 0.000345939
	LOSS [training: -0.0060503967934249425 | validation: -0.014564454659853315]
	TIME [epoch: 8.87 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01520929677969412		[learning rate: 0.00034552]
		[batch 20/20] avg loss: -0.0022220753766446087		[learning rate: 0.0003451]
	Learning Rate: 0.000345102
	LOSS [training: -0.008715686078169364 | validation: -0.009296020175351118]
	TIME [epoch: 8.85 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009775976705427233		[learning rate: 0.00034468]
		[batch 20/20] avg loss: -0.005814644395763168		[learning rate: 0.00034427]
	Learning Rate: 0.000344267
	LOSS [training: -0.0077953105505951985 | validation: -0.007087638727283933]
	TIME [epoch: 8.85 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002421286507416898		[learning rate: 0.00034385]
		[batch 20/20] avg loss: -0.004642163521222258		[learning rate: 0.00034343]
	Learning Rate: 0.000343433
	LOSS [training: -0.003531725014319579 | validation: -0.003675528542990207]
	TIME [epoch: 8.86 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009008709158819293		[learning rate: 0.00034302]
		[batch 20/20] avg loss: -0.004379213883965387		[learning rate: 0.0003426]
	Learning Rate: 0.000342602
	LOSS [training: -0.00669396152139234 | validation: -0.010521343884668723]
	TIME [epoch: 8.87 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007307585032773445		[learning rate: 0.00034219]
		[batch 20/20] avg loss: -0.007203804726719552		[learning rate: 0.00034177]
	Learning Rate: 0.000341772
	LOSS [training: -0.007255694879746498 | validation: -0.013810127117341846]
	TIME [epoch: 8.85 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0062813340637700886		[learning rate: 0.00034136]
		[batch 20/20] avg loss: -0.004706754244821108		[learning rate: 0.00034094]
	Learning Rate: 0.000340945
	LOSS [training: -0.005494044154295599 | validation: -0.010927245190565947]
	TIME [epoch: 8.86 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006646560312609512		[learning rate: 0.00034053]
		[batch 20/20] avg loss: -0.00585298226396782		[learning rate: 0.00034012]
	Learning Rate: 0.00034012
	LOSS [training: -0.006249771288288666 | validation: -0.009841813836009447]
	TIME [epoch: 8.86 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002970728980796274		[learning rate: 0.00033971]
		[batch 20/20] avg loss: -0.007258410475395783		[learning rate: 0.0003393]
	Learning Rate: 0.000339296
	LOSS [training: -0.005114569728096029 | validation: -0.004728464203279387]
	TIME [epoch: 8.85 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006919651940230193		[learning rate: 0.00033889]
		[batch 20/20] avg loss: -0.006050916781800685		[learning rate: 0.00033847]
	Learning Rate: 0.000338475
	LOSS [training: -0.006485284361015441 | validation: -0.0119310461359333]
	TIME [epoch: 8.87 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008501393360969698		[learning rate: 0.00033806]
		[batch 20/20] avg loss: -0.00010279259423197412		[learning rate: 0.00033766]
	Learning Rate: 0.000337655
	LOSS [training: -0.004302092977600836 | validation: -0.006898222832767325]
	TIME [epoch: 8.85 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007601203679266544		[learning rate: 0.00033725]
		[batch 20/20] avg loss: -0.007198335799480577		[learning rate: 0.00033684]
	Learning Rate: 0.000336838
	LOSS [training: -0.007399769739373561 | validation: -0.011601443051930456]
	TIME [epoch: 8.84 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012646098565846106		[learning rate: 0.00033643]
		[batch 20/20] avg loss: -0.006860961207723441		[learning rate: 0.00033602]
	Learning Rate: 0.000336023
	LOSS [training: -0.009753529886784774 | validation: -0.0114604970501536]
	TIME [epoch: 8.85 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008963079054134349		[learning rate: 0.00033562]
		[batch 20/20] avg loss: -0.006393308205812263		[learning rate: 0.00033521]
	Learning Rate: 0.000335209
	LOSS [training: -0.007678193629973305 | validation: -0.014179794109940365]
	TIME [epoch: 8.86 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004904657159059183		[learning rate: 0.0003348]
		[batch 20/20] avg loss: -0.008098170598491158		[learning rate: 0.0003344]
	Learning Rate: 0.000334398
	LOSS [training: -0.00650141387877517 | validation: -0.016010570417542867]
	TIME [epoch: 8.85 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00642347650552086		[learning rate: 0.00033399]
		[batch 20/20] avg loss: -0.005585709956738438		[learning rate: 0.00033359]
	Learning Rate: 0.000333588
	LOSS [training: -0.00600459323112965 | validation: -0.00766528250263579]
	TIME [epoch: 8.85 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005639959535586006		[learning rate: 0.00033318]
		[batch 20/20] avg loss: -0.005930398270139705		[learning rate: 0.00033278]
	Learning Rate: 0.000332781
	LOSS [training: -0.005785178902862857 | validation: -0.002420510534547341]
	TIME [epoch: 8.85 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003218444856256251		[learning rate: 0.00033238]
		[batch 20/20] avg loss: -0.008389500563882279		[learning rate: 0.00033197]
	Learning Rate: 0.000331975
	LOSS [training: -0.005803972710069264 | validation: -0.005551562913481272]
	TIME [epoch: 8.85 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009306347800406364		[learning rate: 0.00033157]
		[batch 20/20] avg loss: -0.00038015932883404374		[learning rate: 0.00033117]
	Learning Rate: 0.000331171
	LOSS [training: -0.0048432535646202035 | validation: -0.0002540276019253867]
	TIME [epoch: 8.88 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027786085989361426		[learning rate: 0.00033077]
		[batch 20/20] avg loss: -0.0039058908333545984		[learning rate: 0.00033037]
	Learning Rate: 0.00033037
	LOSS [training: -0.00334224971614537 | validation: -0.004229456553368205]
	TIME [epoch: 8.87 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006839196281644856		[learning rate: 0.00032997]
		[batch 20/20] avg loss: -0.001836821309788723		[learning rate: 0.00032957]
	Learning Rate: 0.00032957
	LOSS [training: -0.0005764508408121186 | validation: -0.005546551697705595]
	TIME [epoch: 8.86 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010061458359235423		[learning rate: 0.00032917]
		[batch 20/20] avg loss: -0.005704020473555395		[learning rate: 0.00032877]
	Learning Rate: 0.000328772
	LOSS [training: -0.007882739416395407 | validation: -0.01184724025297217]
	TIME [epoch: 8.86 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005039858727038669		[learning rate: 0.00032837]
		[batch 20/20] avg loss: -0.009766004275887693		[learning rate: 0.00032798]
	Learning Rate: 0.000327976
	LOSS [training: -0.007402931501463182 | validation: -0.00495302660976658]
	TIME [epoch: 8.85 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005704561506520564		[learning rate: 0.00032758]
		[batch 20/20] avg loss: -0.009371531169709344		[learning rate: 0.00032718]
	Learning Rate: 0.000327182
	LOSS [training: -0.007538046338114954 | validation: -0.01510356859687176]
	TIME [epoch: 8.88 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010225181299718694		[learning rate: 0.00032679]
		[batch 20/20] avg loss: -0.004413323027286467		[learning rate: 0.00032639]
	Learning Rate: 0.00032639
	LOSS [training: -0.00731925216350258 | validation: -0.007150206115796132]
	TIME [epoch: 8.86 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036421828756528635		[learning rate: 0.00032599]
		[batch 20/20] avg loss: -0.004152398255419858		[learning rate: 0.0003256]
	Learning Rate: 0.0003256
	LOSS [training: -0.0038972905655363607 | validation: -0.004326368620296402]
	TIME [epoch: 8.85 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008969886853948852		[learning rate: 0.00032521]
		[batch 20/20] avg loss: -0.005191394473909048		[learning rate: 0.00032481]
	Learning Rate: 0.000324812
	LOSS [training: -0.00708064066392895 | validation: -0.009708096073624571]
	TIME [epoch: 8.86 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004014177695700029		[learning rate: 0.00032442]
		[batch 20/20] avg loss: -0.00938487393483104		[learning rate: 0.00032403]
	Learning Rate: 0.000324025
	LOSS [training: -0.006699525815265532 | validation: -0.015593501877643485]
	TIME [epoch: 8.87 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007242950687574519		[learning rate: 0.00032363]
		[batch 20/20] avg loss: -0.009924866450112054		[learning rate: 0.00032324]
	Learning Rate: 0.000323241
	LOSS [training: -0.008583908568843288 | validation: -0.014091516430737945]
	TIME [epoch: 8.85 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006787106078690227		[learning rate: 0.00032285]
		[batch 20/20] avg loss: -0.005340141781843058		[learning rate: 0.00032246]
	Learning Rate: 0.000322458
	LOSS [training: -0.006063623930266642 | validation: -0.008417289332562725]
	TIME [epoch: 8.85 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007361345403691766		[learning rate: 0.00032207]
		[batch 20/20] avg loss: -0.004564955610289454		[learning rate: 0.00032168]
	Learning Rate: 0.000321678
	LOSS [training: -0.005963150506990609 | validation: -0.011570762165812515]
	TIME [epoch: 8.86 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036544560814486746		[learning rate: 0.00032129]
		[batch 20/20] avg loss: -0.008008349095650583		[learning rate: 0.0003209]
	Learning Rate: 0.000320899
	LOSS [training: -0.005831402588549628 | validation: -0.00011982243374626862]
	TIME [epoch: 8.85 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009519060723349643		[learning rate: 0.00032051]
		[batch 20/20] avg loss: -0.0017009383658541765		[learning rate: 0.00032012]
	Learning Rate: 0.000320122
	LOSS [training: -0.0056099995446019105 | validation: -0.01047918518253818]
	TIME [epoch: 8.88 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007141269612352692		[learning rate: 0.00031973]
		[batch 20/20] avg loss: -0.012477991083432265		[learning rate: 0.00031935]
	Learning Rate: 0.000319347
	LOSS [training: -0.009809630347892475 | validation: -0.016974632501380205]
	TIME [epoch: 8.87 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012672413898054694		[learning rate: 0.00031896]
		[batch 20/20] avg loss: -0.004705983692067772		[learning rate: 0.00031857]
	Learning Rate: 0.000318574
	LOSS [training: -0.008689198795061233 | validation: -0.00885391510668539]
	TIME [epoch: 8.86 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013708752261075211		[learning rate: 0.00031819]
		[batch 20/20] avg loss: -0.007385753879976599		[learning rate: 0.0003178]
	Learning Rate: 0.000317803
	LOSS [training: -0.010547253070525906 | validation: -0.013837143167869256]
	TIME [epoch: 8.85 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005436114651750637		[learning rate: 0.00031742]
		[batch 20/20] avg loss: -0.00875004523601339		[learning rate: 0.00031703]
	Learning Rate: 0.000317034
	LOSS [training: -0.007093079943882014 | validation: -0.015015139859552468]
	TIME [epoch: 8.85 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009462130414118284		[learning rate: 0.00031665]
		[batch 20/20] avg loss: -0.012458510363688344		[learning rate: 0.00031627]
	Learning Rate: 0.000316266
	LOSS [training: -0.010960320388903313 | validation: -0.00627901903427684]
	TIME [epoch: 8.87 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00844422636858806		[learning rate: 0.00031588]
		[batch 20/20] avg loss: -0.012578915444896838		[learning rate: 0.0003155]
	Learning Rate: 0.0003155
	LOSS [training: -0.010511570906742448 | validation: -0.015306182180584257]
	TIME [epoch: 8.85 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008164611476999987		[learning rate: 0.00031512]
		[batch 20/20] avg loss: -0.00628606137517371		[learning rate: 0.00031474]
	Learning Rate: 0.000314737
	LOSS [training: -0.007225336426086848 | validation: -0.009561107740563214]
	TIME [epoch: 8.86 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009426346545068262		[learning rate: 0.00031436]
		[batch 20/20] avg loss: -0.00810972134193439		[learning rate: 0.00031397]
	Learning Rate: 0.000313975
	LOSS [training: -0.008768033943501327 | validation: -0.006032931644956795]
	TIME [epoch: 8.85 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011932192893764614		[learning rate: 0.00031359]
		[batch 20/20] avg loss: -0.011207048838206451		[learning rate: 0.00031321]
	Learning Rate: 0.000313215
	LOSS [training: -0.011569620865985532 | validation: -0.011743483124238466]
	TIME [epoch: 8.86 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006465175082766573		[learning rate: 0.00031284]
		[batch 20/20] avg loss: -0.012524116510146538		[learning rate: 0.00031246]
	Learning Rate: 0.000312456
	LOSS [training: -0.009494645796456555 | validation: -0.007754197836980697]
	TIME [epoch: 8.87 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008645459206444814		[learning rate: 0.00031208]
		[batch 20/20] avg loss: -0.004574978489822194		[learning rate: 0.0003117]
	Learning Rate: 0.0003117
	LOSS [training: -0.006610218848133505 | validation: -0.014548553589257543]
	TIME [epoch: 8.85 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011569006442930956		[learning rate: 0.00031132]
		[batch 20/20] avg loss: -0.004849208962214047		[learning rate: 0.00031095]
	Learning Rate: 0.000310945
	LOSS [training: -0.0082091077025725 | validation: -0.01079520055763836]
	TIME [epoch: 8.86 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0040753108694356124		[learning rate: 0.00031057]
		[batch 20/20] avg loss: -0.0057328120889423954		[learning rate: 0.00031019]
	Learning Rate: 0.000310193
	LOSS [training: -0.004904061479189004 | validation: -0.013783584159226044]
	TIME [epoch: 8.85 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006509480954395956		[learning rate: 0.00030982]
		[batch 20/20] avg loss: -0.008658731790834803		[learning rate: 0.00030944]
	Learning Rate: 0.000309442
	LOSS [training: -0.007584106372615379 | validation: -0.004330002252604342]
	TIME [epoch: 8.88 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009794233713638953		[learning rate: 0.00030907]
		[batch 20/20] avg loss: -0.009352324121330663		[learning rate: 0.00030869]
	Learning Rate: 0.000308693
	LOSS [training: -0.009573278917484808 | validation: -0.011236938930321036]
	TIME [epoch: 8.87 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011856814513482803		[learning rate: 0.00030832]
		[batch 20/20] avg loss: -0.011793456027175634		[learning rate: 0.00030795]
	Learning Rate: 0.000307945
	LOSS [training: -0.011825135270329221 | validation: -0.013986397108004287]
	TIME [epoch: 8.86 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00531353023300362		[learning rate: 0.00030757]
		[batch 20/20] avg loss: -0.012133270593114006		[learning rate: 0.0003072]
	Learning Rate: 0.0003072
	LOSS [training: -0.008723400413058814 | validation: -0.011921396730787785]
	TIME [epoch: 8.86 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001661680084163743		[learning rate: 0.00030683]
		[batch 20/20] avg loss: -0.010102870138729974		[learning rate: 0.00030646]
	Learning Rate: 0.000306456
	LOSS [training: -0.004220595027283115 | validation: -0.004196838043433955]
	TIME [epoch: 8.85 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005041032570580844		[learning rate: 0.00030609]
		[batch 20/20] avg loss: -0.00626578988661739		[learning rate: 0.00030571]
	Learning Rate: 0.000305714
	LOSS [training: -0.005653411228599116 | validation: -0.015141418471936313]
	TIME [epoch: 8.88 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007339081149888578		[learning rate: 0.00030534]
		[batch 20/20] avg loss: -0.011340644593342383		[learning rate: 0.00030497]
	Learning Rate: 0.000304974
	LOSS [training: -0.00933986287161548 | validation: -0.019992146091089437]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_1540.pth
	Model improved!!!
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007161327524779224		[learning rate: 0.0003046]
		[batch 20/20] avg loss: -0.011133444188451183		[learning rate: 0.00030424]
	Learning Rate: 0.000304236
	LOSS [training: -0.009147385856615203 | validation: -0.0164812876236351]
	TIME [epoch: 8.85 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011441047023000351		[learning rate: 0.00030387]
		[batch 20/20] avg loss: -0.0062507546015743935		[learning rate: 0.0003035]
	Learning Rate: 0.000303499
	LOSS [training: -0.008845900812287373 | validation: -0.01286260518437813]
	TIME [epoch: 8.85 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006164925297150073		[learning rate: 0.00030313]
		[batch 20/20] avg loss: -0.003924479298958611		[learning rate: 0.00030276]
	Learning Rate: 0.000302765
	LOSS [training: -0.0050447022980543415 | validation: -0.009158669495091948]
	TIME [epoch: 8.85 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005020010152614513		[learning rate: 0.0003024]
		[batch 20/20] avg loss: -0.0046283623704894096		[learning rate: 0.00030203]
	Learning Rate: 0.000302032
	LOSS [training: -0.00482418626155196 | validation: -0.018848349078517276]
	TIME [epoch: 8.88 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008693775320446634		[learning rate: 0.00030167]
		[batch 20/20] avg loss: -0.008495059367499716		[learning rate: 0.0003013]
	Learning Rate: 0.000301301
	LOSS [training: -0.008594417343973173 | validation: -0.008644097229122337]
	TIME [epoch: 8.84 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005128208145392137		[learning rate: 0.00030094]
		[batch 20/20] avg loss: -0.0060348377002634465		[learning rate: 0.00030057]
	Learning Rate: 0.000300571
	LOSS [training: -0.005581522922827791 | validation: -0.014933051326374265]
	TIME [epoch: 8.85 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004695539667379588		[learning rate: 0.00030021]
		[batch 20/20] avg loss: -0.010060635270411435		[learning rate: 0.00029984]
	Learning Rate: 0.000299844
	LOSS [training: -0.007378087468895509 | validation: -0.008089162566512974]
	TIME [epoch: 8.85 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007132640562526262		[learning rate: 0.00029948]
		[batch 20/20] avg loss: -0.006096529565775487		[learning rate: 0.00029912]
	Learning Rate: 0.000299118
	LOSS [training: -0.006614585064150874 | validation: -0.010721227420791643]
	TIME [epoch: 8.87 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006298436123073221		[learning rate: 0.00029876]
		[batch 20/20] avg loss: -0.01185873684930845		[learning rate: 0.00029839]
	Learning Rate: 0.000298394
	LOSS [training: -0.009078586486190835 | validation: -0.014498373124020459]
	TIME [epoch: 8.86 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008854245807162655		[learning rate: 0.00029803]
		[batch 20/20] avg loss: -0.00981262358446559		[learning rate: 0.00029767]
	Learning Rate: 0.000297671
	LOSS [training: -0.00933343469581412 | validation: -0.007464099014225107]
	TIME [epoch: 8.86 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006032148644602914		[learning rate: 0.00029731]
		[batch 20/20] avg loss: -0.010955858802226989		[learning rate: 0.00029695]
	Learning Rate: 0.000296951
	LOSS [training: -0.008494003723414951 | validation: -0.015961497619925186]
	TIME [epoch: 8.84 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005816320891546652		[learning rate: 0.00029659]
		[batch 20/20] avg loss: -0.008990386635484935		[learning rate: 0.00029623]
	Learning Rate: 0.000296232
	LOSS [training: -0.007403353763515791 | validation: -0.010563481005306187]
	TIME [epoch: 8.83 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006346272881987722		[learning rate: 0.00029587]
		[batch 20/20] avg loss: -0.010384973397276176		[learning rate: 0.00029551]
	Learning Rate: 0.000295515
	LOSS [training: -0.00836562313963195 | validation: -0.005714861066339895]
	TIME [epoch: 8.88 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005885871574558709		[learning rate: 0.00029516]
		[batch 20/20] avg loss: -0.002065606684011184		[learning rate: 0.0002948]
	Learning Rate: 0.000294799
	LOSS [training: -0.003975739129284947 | validation: -0.00732680476061476]
	TIME [epoch: 8.85 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017949844872676118		[learning rate: 0.00029444]
		[batch 20/20] avg loss: -0.011459958451610085		[learning rate: 0.00029409]
	Learning Rate: 0.000294086
	LOSS [training: -0.006627471469438847 | validation: -0.010617036609823358]
	TIME [epoch: 8.85 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028081209353006025		[learning rate: 0.00029373]
		[batch 20/20] avg loss: -0.010175937527239153		[learning rate: 0.00029337]
	Learning Rate: 0.000293374
	LOSS [training: -0.006492029231269876 | validation: -0.009340690970558143]
	TIME [epoch: 8.84 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00665927688057528		[learning rate: 0.00029302]
		[batch 20/20] avg loss: -0.007500640288352095		[learning rate: 0.00029266]
	Learning Rate: 0.000292663
	LOSS [training: -0.007079958584463686 | validation: -0.013295584813344818]
	TIME [epoch: 8.84 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009570929202766247		[learning rate: 0.00029231]
		[batch 20/20] avg loss: -0.004354445830100489		[learning rate: 0.00029195]
	Learning Rate: 0.000291955
	LOSS [training: -0.006962687516433369 | validation: -0.00878033443653759]
	TIME [epoch: 8.86 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004499780411114669		[learning rate: 0.0002916]
		[batch 20/20] avg loss: 0.0033237459356491596		[learning rate: 0.00029125]
	Learning Rate: 0.000291248
	LOSS [training: -0.0005880172377327547 | validation: -0.007204248267764904]
	TIME [epoch: 8.84 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003854847621162608		[learning rate: 0.0002909]
		[batch 20/20] avg loss: -0.014971918688766975		[learning rate: 0.00029054]
	Learning Rate: 0.000290543
	LOSS [training: -0.009413383154964792 | validation: -0.01163918343935455]
	TIME [epoch: 8.86 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008996442780291606		[learning rate: 0.00029019]
		[batch 20/20] avg loss: -0.011475671781889279		[learning rate: 0.00028984]
	Learning Rate: 0.00028984
	LOSS [training: -0.010236057281090446 | validation: -0.012990883542736604]
	TIME [epoch: 8.86 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009654946337561952		[learning rate: 0.00028949]
		[batch 20/20] avg loss: -0.0074775448541800375		[learning rate: 0.00028914]
	Learning Rate: 0.000289138
	LOSS [training: -0.008566245595870995 | validation: -0.01474652363497932]
	TIME [epoch: 8.87 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034037001563517813		[learning rate: 0.00028879]
		[batch 20/20] avg loss: 0.0002125740102897667		[learning rate: 0.00028844]
	Learning Rate: 0.000288438
	LOSS [training: -0.0015955630730310074 | validation: -0.008699600763500225]
	TIME [epoch: 8.86 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002962543233950763		[learning rate: 0.00028809]
		[batch 20/20] avg loss: -0.008286379329058523		[learning rate: 0.00028774]
	Learning Rate: 0.00028774
	LOSS [training: -0.005624461281504642 | validation: -0.016518078534871118]
	TIME [epoch: 8.84 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001367939465748227		[learning rate: 0.00028739]
		[batch 20/20] avg loss: -0.002982969446265025		[learning rate: 0.00028704]
	Learning Rate: 0.000287043
	LOSS [training: -0.002175454456006626 | validation: -0.018977439756229078]
	TIME [epoch: 8.84 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0076385675639646326		[learning rate: 0.0002867]
		[batch 20/20] avg loss: -0.008015345007359304		[learning rate: 0.00028635]
	Learning Rate: 0.000286348
	LOSS [training: -0.007826956285661968 | validation: -0.012821174029089423]
	TIME [epoch: 8.84 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007688846802364461		[learning rate: 0.000286]
		[batch 20/20] avg loss: -0.01020284502417423		[learning rate: 0.00028566]
	Learning Rate: 0.000285655
	LOSS [training: -0.008945845913269346 | validation: -0.014942516398138497]
	TIME [epoch: 8.86 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01027389375186725		[learning rate: 0.00028531]
		[batch 20/20] avg loss: -0.002424759975533148		[learning rate: 0.00028496]
	Learning Rate: 0.000284964
	LOSS [training: -0.006349326863700199 | validation: -0.011421165791509223]
	TIME [epoch: 8.85 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009877835467275003		[learning rate: 0.00028462]
		[batch 20/20] avg loss: -0.0075263095544937705		[learning rate: 0.00028427]
	Learning Rate: 0.000284274
	LOSS [training: -0.008702072510884385 | validation: -0.008850162850724796]
	TIME [epoch: 8.85 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021961523492146826		[learning rate: 0.00028393]
		[batch 20/20] avg loss: -0.004112112703192758		[learning rate: 0.00028359]
	Learning Rate: 0.000283586
	LOSS [training: -0.003154132526203721 | validation: -0.006854780042150267]
	TIME [epoch: 8.85 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023267600929522355		[learning rate: 0.00028324]
		[batch 20/20] avg loss: -0.011514711515267595		[learning rate: 0.0002829]
	Learning Rate: 0.000282899
	LOSS [training: -0.006920735804109917 | validation: -0.014951648165097024]
	TIME [epoch: 8.86 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008325442736898054		[learning rate: 0.00028256]
		[batch 20/20] avg loss: -0.004359425166502532		[learning rate: 0.00028221]
	Learning Rate: 0.000282214
	LOSS [training: -0.006342433951700292 | validation: -0.0067830537470141075]
	TIME [epoch: 8.87 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007410866794588489		[learning rate: 0.00028187]
		[batch 20/20] avg loss: -0.011978204790406195		[learning rate: 0.00028153]
	Learning Rate: 0.000281531
	LOSS [training: -0.009694535792497341 | validation: -0.013614102022132129]
	TIME [epoch: 8.86 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011371364914024705		[learning rate: 0.00028119]
		[batch 20/20] avg loss: -0.004884028960825409		[learning rate: 0.00028085]
	Learning Rate: 0.000280849
	LOSS [training: -0.008127696937425058 | validation: -0.018204235534441643]
	TIME [epoch: 8.86 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010625436242238284		[learning rate: 0.00028051]
		[batch 20/20] avg loss: -0.005230326875448589		[learning rate: 0.00028017]
	Learning Rate: 0.00028017
	LOSS [training: -0.003146435249836209 | validation: -0.008622897766860305]
	TIME [epoch: 8.86 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007144004147202188		[learning rate: 0.00027983]
		[batch 20/20] avg loss: -0.008002352483236315		[learning rate: 0.00027949]
	Learning Rate: 0.000279491
	LOSS [training: -0.007573178315219252 | validation: -0.013755698720503423]
	TIME [epoch: 8.86 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007061358639041787		[learning rate: 0.00027915]
		[batch 20/20] avg loss: -0.006071756799163386		[learning rate: 0.00027881]
	Learning Rate: 0.000278815
	LOSS [training: -0.006566557719102586 | validation: -0.014507578944759139]
	TIME [epoch: 8.87 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008239474067611264		[learning rate: 0.00027848]
		[batch 20/20] avg loss: -0.005981083633159231		[learning rate: 0.00027814]
	Learning Rate: 0.00027814
	LOSS [training: -0.007110278850385249 | validation: -0.002901555646004919]
	TIME [epoch: 8.85 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00879746903015872		[learning rate: 0.0002778]
		[batch 20/20] avg loss: 0.000727367536297326		[learning rate: 0.00027747]
	Learning Rate: 0.000277467
	LOSS [training: -0.004035050746930696 | validation: -0.007908499057225203]
	TIME [epoch: 8.85 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010018522915446675		[learning rate: 0.00027713]
		[batch 20/20] avg loss: -0.0032079803782204203		[learning rate: 0.00027679]
	Learning Rate: 0.000276795
	LOSS [training: -0.006613251646833548 | validation: -0.007642451085008141]
	TIME [epoch: 8.84 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00952635437363403		[learning rate: 0.00027646]
		[batch 20/20] avg loss: -0.003535694031330887		[learning rate: 0.00027612]
	Learning Rate: 0.000276125
	LOSS [training: -0.00653102420248246 | validation: -0.012566042045952205]
	TIME [epoch: 8.87 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007791564149464569		[learning rate: 0.00027579]
		[batch 20/20] avg loss: -0.012082403900014697		[learning rate: 0.00027546]
	Learning Rate: 0.000275456
	LOSS [training: -0.009936984024739633 | validation: -0.012716195834187927]
	TIME [epoch: 8.86 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011180233594870581		[learning rate: 0.00027512]
		[batch 20/20] avg loss: -0.008856996584507299		[learning rate: 0.00027479]
	Learning Rate: 0.000274789
	LOSS [training: -0.010018615089688938 | validation: -0.01188266714846643]
	TIME [epoch: 8.86 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007652712056734605		[learning rate: 0.00027446]
		[batch 20/20] avg loss: -0.0003886695330715871		[learning rate: 0.00027412]
	Learning Rate: 0.000274124
	LOSS [training: -0.004020690794903097 | validation: -0.013585366862999925]
	TIME [epoch: 8.85 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004294797591372105		[learning rate: 0.00027379]
		[batch 20/20] avg loss: -0.006105122894917959		[learning rate: 0.00027346]
	Learning Rate: 0.000273461
	LOSS [training: -0.005199960243145033 | validation: -0.007062804274319634]
	TIME [epoch: 8.85 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011815870571260812		[learning rate: 0.00027313]
		[batch 20/20] avg loss: -0.007962195961279322		[learning rate: 0.0002728]
	Learning Rate: 0.000272799
	LOSS [training: -0.009889033266270069 | validation: -0.013793694885204023]
	TIME [epoch: 8.88 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007370798973022742		[learning rate: 0.00027247]
		[batch 20/20] avg loss: -0.006933816351012688		[learning rate: 0.00027214]
	Learning Rate: 0.000272138
	LOSS [training: -0.007152307662017715 | validation: -0.007874087939914568]
	TIME [epoch: 8.86 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0051026531030666416		[learning rate: 0.00027181]
		[batch 20/20] avg loss: -0.014606378461216277		[learning rate: 0.00027148]
	Learning Rate: 0.000271479
	LOSS [training: -0.00985451578214146 | validation: -0.011946312101067315]
	TIME [epoch: 8.86 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012713269354678484		[learning rate: 0.00027115]
		[batch 20/20] avg loss: -0.0069536513054464805		[learning rate: 0.00027082]
	Learning Rate: 0.000270822
	LOSS [training: -0.009833460330062483 | validation: -0.015754560404921108]
	TIME [epoch: 8.86 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006223429099236633		[learning rate: 0.00027049]
		[batch 20/20] avg loss: -0.008549193118845801		[learning rate: 0.00027017]
	Learning Rate: 0.000270167
	LOSS [training: -0.007386311109041216 | validation: -0.012872669798681284]
	TIME [epoch: 8.85 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0047194112637066795		[learning rate: 0.00026984]
		[batch 20/20] avg loss: -0.004771795169328999		[learning rate: 0.00026951]
	Learning Rate: 0.000269513
	LOSS [training: -0.00474560321651784 | validation: -0.017087480013052013]
	TIME [epoch: 8.88 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002508341214794755		[learning rate: 0.00026919]
		[batch 20/20] avg loss: -0.0055568223232321376		[learning rate: 0.00026886]
	Learning Rate: 0.00026886
	LOSS [training: -0.004032581769013448 | validation: -0.008728102114111541]
	TIME [epoch: 8.85 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007088209014350197		[learning rate: 0.00026853]
		[batch 20/20] avg loss: -0.003352146772554022		[learning rate: 0.00026821]
	Learning Rate: 0.000268209
	LOSS [training: -0.005220177893452109 | validation: -0.007407505115972805]
	TIME [epoch: 8.86 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005303950065621234		[learning rate: 0.00026788]
		[batch 20/20] avg loss: -0.01062600033578199		[learning rate: 0.00026756]
	Learning Rate: 0.00026756
	LOSS [training: -0.00796497520070161 | validation: -0.01160379535935218]
	TIME [epoch: 8.85 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037901736993873696		[learning rate: 0.00026724]
		[batch 20/20] avg loss: -0.00663598866966181		[learning rate: 0.00026691]
	Learning Rate: 0.000266912
	LOSS [training: -0.00521308118452459 | validation: -0.012591221241970138]
	TIME [epoch: 8.86 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029210429469852714		[learning rate: 0.00026659]
		[batch 20/20] avg loss: -0.004352477712601312		[learning rate: 0.00026627]
	Learning Rate: 0.000266266
	LOSS [training: -0.003636760329793292 | validation: -0.01068058438805838]
	TIME [epoch: 8.86 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008936836222138317		[learning rate: 0.00026594]
		[batch 20/20] avg loss: -0.001694386745010617		[learning rate: 0.00026562]
	Learning Rate: 0.000265621
	LOSS [training: -0.0053156114835744665 | validation: -0.010405829128080194]
	TIME [epoch: 8.85 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0042566548584283045		[learning rate: 0.0002653]
		[batch 20/20] avg loss: -0.010521847283391655		[learning rate: 0.00026498]
	Learning Rate: 0.000264978
	LOSS [training: -0.007389251070909979 | validation: -0.013709506364864583]
	TIME [epoch: 8.85 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008004921297812002		[learning rate: 0.00026466]
		[batch 20/20] avg loss: -0.009704035747961711		[learning rate: 0.00026434]
	Learning Rate: 0.000264337
	LOSS [training: -0.00885447852288686 | validation: -0.0146191520880036]
	TIME [epoch: 8.84 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0056835096261723785		[learning rate: 0.00026402]
		[batch 20/20] avg loss: -0.0033874509491672034		[learning rate: 0.0002637]
	Learning Rate: 0.000263697
	LOSS [training: -0.004535480287669791 | validation: -0.011575121210078124]
	TIME [epoch: 8.88 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005725968243813774		[learning rate: 0.00026338]
		[batch 20/20] avg loss: -0.007031921421148486		[learning rate: 0.00026306]
	Learning Rate: 0.000263059
	LOSS [training: -0.00637894483248113 | validation: -0.010237256660506299]
	TIME [epoch: 8.86 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005562341514923172		[learning rate: 0.00026274]
		[batch 20/20] avg loss: -0.011778060863325618		[learning rate: 0.00026242]
	Learning Rate: 0.000262422
	LOSS [training: -0.008670201189124396 | validation: -0.007358903633360503]
	TIME [epoch: 8.86 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005830591147917275		[learning rate: 0.0002621]
		[batch 20/20] avg loss: -0.004003056230010524		[learning rate: 0.00026179]
	Learning Rate: 0.000261787
	LOSS [training: -0.004916823688963899 | validation: -0.010480961394965604]
	TIME [epoch: 8.86 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011925588471405615		[learning rate: 0.00026147]
		[batch 20/20] avg loss: -0.0033384675335360317		[learning rate: 0.00026115]
	Learning Rate: 0.000261153
	LOSS [training: -0.007632028002470823 | validation: -0.008103760022774582]
	TIME [epoch: 8.86 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006267364753344789		[learning rate: 0.00026084]
		[batch 20/20] avg loss: -0.0051863869253633175		[learning rate: 0.00026052]
	Learning Rate: 0.000260521
	LOSS [training: -0.0057268758393540535 | validation: -0.009886897394897903]
	TIME [epoch: 8.88 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009429150847792103		[learning rate: 0.00026021]
		[batch 20/20] avg loss: -0.004392043837599592		[learning rate: 0.00025989]
	Learning Rate: 0.00025989
	LOSS [training: -0.006910597342695847 | validation: -0.013369078602545884]
	TIME [epoch: 8.85 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010457763846500797		[learning rate: 0.00025958]
		[batch 20/20] avg loss: -0.004344157377682981		[learning rate: 0.00025926]
	Learning Rate: 0.000259261
	LOSS [training: -0.0074009606120918866 | validation: -0.004513771050462458]
	TIME [epoch: 8.86 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005595619896331017		[learning rate: 0.00025895]
		[batch 20/20] avg loss: -0.007125807595004528		[learning rate: 0.00025863]
	Learning Rate: 0.000258633
	LOSS [training: -0.006360713745667772 | validation: -0.01168016926654323]
	TIME [epoch: 8.85 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007371355943746204		[learning rate: 0.00025832]
		[batch 20/20] avg loss: -0.009520154748300548		[learning rate: 0.00025801]
	Learning Rate: 0.000258007
	LOSS [training: -0.008445755346023376 | validation: -0.0028046759521883615]
	TIME [epoch: 8.86 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004438027015745458		[learning rate: 0.00025769]
		[batch 20/20] avg loss: -0.009327425504654136		[learning rate: 0.00025738]
	Learning Rate: 0.000257382
	LOSS [training: -0.006882726260199795 | validation: -0.014639792781500674]
	TIME [epoch: 8.88 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008772256656530992		[learning rate: 0.00025707]
		[batch 20/20] avg loss: -0.00873859952047146		[learning rate: 0.00025676]
	Learning Rate: 0.000256759
	LOSS [training: -0.008755428088501226 | validation: -0.009252001669927975]
	TIME [epoch: 8.85 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007964375092217328		[learning rate: 0.00025645]
		[batch 20/20] avg loss: -0.011604974292000908		[learning rate: 0.00025614]
	Learning Rate: 0.000256138
	LOSS [training: -0.009784674692109117 | validation: -0.012619263639451112]
	TIME [epoch: 8.85 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005428531702215301		[learning rate: 0.00025583]
		[batch 20/20] avg loss: -0.010021715795771585		[learning rate: 0.00025552]
	Learning Rate: 0.000255518
	LOSS [training: -0.0077251237489934415 | validation: -0.010286265499296229]
	TIME [epoch: 8.85 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005619927284637451		[learning rate: 0.00025521]
		[batch 20/20] avg loss: -0.008374710995411008		[learning rate: 0.0002549]
	Learning Rate: 0.000254899
	LOSS [training: -0.006997319140024228 | validation: -0.009518095167552702]
	TIME [epoch: 8.87 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00849521877865204		[learning rate: 0.00025459]
		[batch 20/20] avg loss: -0.0065002563435564474		[learning rate: 0.00025428]
	Learning Rate: 0.000254282
	LOSS [training: -0.007497737561104242 | validation: -0.008101833100727969]
	TIME [epoch: 8.87 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00946878495617507		[learning rate: 0.00025397]
		[batch 20/20] avg loss: -0.004559569338877446		[learning rate: 0.00025367]
	Learning Rate: 0.000253667
	LOSS [training: -0.0070141771475262585 | validation: -0.00981274331523855]
	TIME [epoch: 8.86 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010735094152505786		[learning rate: 0.00025336]
		[batch 20/20] avg loss: -0.0074025637265708625		[learning rate: 0.00025305]
	Learning Rate: 0.000253052
	LOSS [training: -0.009068828939538328 | validation: -0.01254485395769343]
	TIME [epoch: 8.86 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008298804192211925		[learning rate: 0.00025275]
		[batch 20/20] avg loss: -0.007896745859843027		[learning rate: 0.00025244]
	Learning Rate: 0.00025244
	LOSS [training: -0.008097775026027477 | validation: -0.007549784837373018]
	TIME [epoch: 8.85 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009609024414690822		[learning rate: 0.00025213]
		[batch 20/20] avg loss: -0.0070938071165321		[learning rate: 0.00025183]
	Learning Rate: 0.000251829
	LOSS [training: -0.00835141576561146 | validation: -0.012137848577030179]
	TIME [epoch: 8.87 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00813922617608996		[learning rate: 0.00025152]
		[batch 20/20] avg loss: -0.006753475664064647		[learning rate: 0.00025122]
	Learning Rate: 0.000251219
	LOSS [training: -0.007446350920077303 | validation: -0.014483785370131076]
	TIME [epoch: 8.86 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008294296691401862		[learning rate: 0.00025091]
		[batch 20/20] avg loss: -0.005751434968074164		[learning rate: 0.00025061]
	Learning Rate: 0.000250611
	LOSS [training: -0.007022865829738013 | validation: -0.010077061260271572]
	TIME [epoch: 8.85 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009747959987866085		[learning rate: 0.00025031]
		[batch 20/20] avg loss: -0.01181159151810773		[learning rate: 0.00025]
	Learning Rate: 0.000250004
	LOSS [training: -0.010779775752986906 | validation: -0.010431579293213485]
	TIME [epoch: 8.86 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0072057197536977355		[learning rate: 0.0002497]
		[batch 20/20] avg loss: -0.009831071710999895		[learning rate: 0.0002494]
	Learning Rate: 0.000249399
	LOSS [training: -0.008518395732348812 | validation: -0.014613989982989403]
	TIME [epoch: 8.86 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011341379931458524		[learning rate: 0.0002491]
		[batch 20/20] avg loss: -0.007346440139213881		[learning rate: 0.0002488]
	Learning Rate: 0.000248795
	LOSS [training: -0.009343910035336201 | validation: -0.012213405460137414]
	TIME [epoch: 8.88 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038506321974200015		[learning rate: 0.00024849]
		[batch 20/20] avg loss: -0.010991080466305099		[learning rate: 0.00024819]
	Learning Rate: 0.000248193
	LOSS [training: -0.00742085633186255 | validation: -0.009626336016774695]
	TIME [epoch: 8.85 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008042159066390678		[learning rate: 0.00024789]
		[batch 20/20] avg loss: -0.009956460561217847		[learning rate: 0.00024759]
	Learning Rate: 0.000247592
	LOSS [training: -0.008999309813804266 | validation: -0.01259937248918401]
	TIME [epoch: 8.84 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011915316763701975		[learning rate: 0.00024729]
		[batch 20/20] avg loss: -0.006334615938798954		[learning rate: 0.00024699]
	Learning Rate: 0.000246993
	LOSS [training: -0.009124966351250466 | validation: -0.011249642558395409]
	TIME [epoch: 8.86 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032680550758216213		[learning rate: 0.00024669]
		[batch 20/20] avg loss: -0.008131839962130369		[learning rate: 0.00024639]
	Learning Rate: 0.000246395
	LOSS [training: -0.0056999475189759945 | validation: -0.010235330370563497]
	TIME [epoch: 8.88 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006064886234581425		[learning rate: 0.0002461]
		[batch 20/20] avg loss: -0.010254016535591803		[learning rate: 0.0002458]
	Learning Rate: 0.000245798
	LOSS [training: -0.008159451385086615 | validation: -0.01552108484189947]
	TIME [epoch: 8.86 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006188954964759052		[learning rate: 0.0002455]
		[batch 20/20] avg loss: -0.00730591802160523		[learning rate: 0.0002452]
	Learning Rate: 0.000245203
	LOSS [training: -0.006747436493182141 | validation: -0.008404035553549074]
	TIME [epoch: 8.86 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009930250000353209		[learning rate: 0.00024491]
		[batch 20/20] avg loss: -0.009228010605073852		[learning rate: 0.00024461]
	Learning Rate: 0.00024461
	LOSS [training: -0.009579130302713531 | validation: -0.006564444399231404]
	TIME [epoch: 8.86 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0053264807298632946		[learning rate: 0.00024431]
		[batch 20/20] avg loss: -0.004716376544048809		[learning rate: 0.00024402]
	Learning Rate: 0.000244018
	LOSS [training: -0.005021428636956052 | validation: -0.013664184146019581]
	TIME [epoch: 8.93 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006539894069483611		[learning rate: 0.00024372]
		[batch 20/20] avg loss: -0.006834960387527978		[learning rate: 0.00024343]
	Learning Rate: 0.000243427
	LOSS [training: -0.006687427228505793 | validation: -0.0074069894966590916]
	TIME [epoch: 8.87 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010203651315329913		[learning rate: 0.00024313]
		[batch 20/20] avg loss: -0.007523172393786873		[learning rate: 0.00024284]
	Learning Rate: 0.000242838
	LOSS [training: -0.008863411854558391 | validation: -0.012021205412836187]
	TIME [epoch: 8.86 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006154578677192225		[learning rate: 0.00024254]
		[batch 20/20] avg loss: -0.011268485134716321		[learning rate: 0.00024225]
	Learning Rate: 0.00024225
	LOSS [training: -0.008711531905954273 | validation: -0.01418285202850195]
	TIME [epoch: 8.85 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005765677442329845		[learning rate: 0.00024196]
		[batch 20/20] avg loss: -0.011519624381467053		[learning rate: 0.00024166]
	Learning Rate: 0.000241663
	LOSS [training: -0.008642650911898449 | validation: -0.013093402269628629]
	TIME [epoch: 8.85 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00856971696143465		[learning rate: 0.00024137]
		[batch 20/20] avg loss: -0.00676925607349711		[learning rate: 0.00024108]
	Learning Rate: 0.000241078
	LOSS [training: -0.007669486517465879 | validation: -0.011629379632756547]
	TIME [epoch: 8.85 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009310030164934504		[learning rate: 0.00024079]
		[batch 20/20] avg loss: -0.014088564663993436		[learning rate: 0.00024049]
	Learning Rate: 0.000240495
	LOSS [training: -0.011699297414463971 | validation: -0.013469710573298127]
	TIME [epoch: 8.87 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007457437088122851		[learning rate: 0.0002402]
		[batch 20/20] avg loss: -0.008854348012263268		[learning rate: 0.00023991]
	Learning Rate: 0.000239912
	LOSS [training: -0.00815589255019306 | validation: -0.0043749244983936324]
	TIME [epoch: 8.85 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007223126921753488		[learning rate: 0.00023962]
		[batch 20/20] avg loss: -0.004687012619692952		[learning rate: 0.00023933]
	Learning Rate: 0.000239332
	LOSS [training: -0.00595506977072322 | validation: -0.014329675627944108]
	TIME [epoch: 8.85 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01029610011209988		[learning rate: 0.00023904]
		[batch 20/20] avg loss: -0.011857744995837909		[learning rate: 0.00023875]
	Learning Rate: 0.000238752
	LOSS [training: -0.011076922553968895 | validation: -0.016095794321197544]
	TIME [epoch: 8.85 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007120822651875027		[learning rate: 0.00023846]
		[batch 20/20] avg loss: -0.007912799638239761		[learning rate: 0.00023817]
	Learning Rate: 0.000238174
	LOSS [training: -0.007516811145057395 | validation: -0.013595541748641641]
	TIME [epoch: 8.87 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007884134547875754		[learning rate: 0.00023789]
		[batch 20/20] avg loss: -0.012639343450856069		[learning rate: 0.0002376]
	Learning Rate: 0.000237598
	LOSS [training: -0.01026173899936591 | validation: -0.010468083819349248]
	TIME [epoch: 8.86 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01210322655969864		[learning rate: 0.00023731]
		[batch 20/20] avg loss: -0.012227544341292359		[learning rate: 0.00023702]
	Learning Rate: 0.000237022
	LOSS [training: -0.012165385450495503 | validation: -0.012919670559989905]
	TIME [epoch: 8.85 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009742983971834443		[learning rate: 0.00023674]
		[batch 20/20] avg loss: -0.011586946156717745		[learning rate: 0.00023645]
	Learning Rate: 0.000236449
	LOSS [training: -0.010664965064276093 | validation: -0.01260827313174375]
	TIME [epoch: 8.85 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008089234943787951		[learning rate: 0.00023616]
		[batch 20/20] avg loss: -0.007967528685644775		[learning rate: 0.00023588]
	Learning Rate: 0.000235876
	LOSS [training: -0.008028381814716365 | validation: -0.013621973568226805]
	TIME [epoch: 8.84 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007345078820707114		[learning rate: 0.00023559]
		[batch 20/20] avg loss: -0.01147343303446222		[learning rate: 0.00023531]
	Learning Rate: 0.000235305
	LOSS [training: -0.009409255927584667 | validation: -0.0009557122293992859]
	TIME [epoch: 8.86 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011336279175487117		[learning rate: 0.00023502]
		[batch 20/20] avg loss: -0.00653856902317902		[learning rate: 0.00023474]
	Learning Rate: 0.000234736
	LOSS [training: -0.008937424099333067 | validation: -0.011382445578464385]
	TIME [epoch: 8.85 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011472688902757652		[learning rate: 0.00023445]
		[batch 20/20] avg loss: -0.00884890091815438		[learning rate: 0.00023417]
	Learning Rate: 0.000234167
	LOSS [training: -0.010160794910456015 | validation: -0.008494098878115797]
	TIME [epoch: 8.84 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009489477882735483		[learning rate: 0.00023388]
		[batch 20/20] avg loss: -0.005193647603182211		[learning rate: 0.0002336]
	Learning Rate: 0.0002336
	LOSS [training: -0.0073415627429588475 | validation: -0.015468979350751226]
	TIME [epoch: 8.85 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012568245431860404		[learning rate: 0.00023332]
		[batch 20/20] avg loss: -0.013380293510822708		[learning rate: 0.00023303]
	Learning Rate: 0.000233035
	LOSS [training: -0.012974269471341556 | validation: -0.010757580324982623]
	TIME [epoch: 8.84 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0075247968832239335		[learning rate: 0.00023275]
		[batch 20/20] avg loss: -0.009084706605727591		[learning rate: 0.00023247]
	Learning Rate: 0.000232471
	LOSS [training: -0.008304751744475761 | validation: -0.01873640518407348]
	TIME [epoch: 8.86 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0066691097846427756		[learning rate: 0.00023219]
		[batch 20/20] avg loss: -0.010581453141877018		[learning rate: 0.00023191]
	Learning Rate: 0.000231908
	LOSS [training: -0.008625281463259896 | validation: -0.009437021602968276]
	TIME [epoch: 8.86 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01345063025003857		[learning rate: 0.00023163]
		[batch 20/20] avg loss: -0.009180137285003461		[learning rate: 0.00023135]
	Learning Rate: 0.000231347
	LOSS [training: -0.011315383767521016 | validation: -0.014232525535624832]
	TIME [epoch: 8.85 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008685380851170219		[learning rate: 0.00023107]
		[batch 20/20] avg loss: -0.005023566981103247		[learning rate: 0.00023079]
	Learning Rate: 0.000230787
	LOSS [training: -0.006854473916136733 | validation: -0.013160917355260484]
	TIME [epoch: 8.85 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005821686985902255		[learning rate: 0.00023051]
		[batch 20/20] avg loss: -0.005518968525023972		[learning rate: 0.00023023]
	Learning Rate: 0.000230228
	LOSS [training: -0.005670327755463113 | validation: -0.00319123650791999]
	TIME [epoch: 8.86 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: -8.092729645976602e-05		[learning rate: 0.00022995]
		[batch 20/20] avg loss: 0.0036390001680401893		[learning rate: 0.00022967]
	Learning Rate: 0.000229671
	LOSS [training: 0.001779036435790212 | validation: -0.004346701777364365]
	TIME [epoch: 8.87 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005815724864090169		[learning rate: 0.00022939]
		[batch 20/20] avg loss: -0.008495482404384618		[learning rate: 0.00022911]
	Learning Rate: 0.000229115
	LOSS [training: -0.0071556036342373935 | validation: -0.015373048747997196]
	TIME [epoch: 8.86 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012072307332459719		[learning rate: 0.00022884]
		[batch 20/20] avg loss: -0.0074078142813486		[learning rate: 0.00022856]
	Learning Rate: 0.00022856
	LOSS [training: -0.00974006080690416 | validation: -0.01136438683531345]
	TIME [epoch: 8.85 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007861299731255055		[learning rate: 0.00022828]
		[batch 20/20] avg loss: -0.016810137460901483		[learning rate: 0.00022801]
	Learning Rate: 0.000228007
	LOSS [training: -0.008798133717013493 | validation: -0.01381088240957704]
	TIME [epoch: 8.85 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008046137450707643		[learning rate: 0.00022773]
		[batch 20/20] avg loss: -0.008038934512901247		[learning rate: 0.00022745]
	Learning Rate: 0.000227455
	LOSS [training: -0.008042535981804443 | validation: -0.012479122738040081]
	TIME [epoch: 8.86 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011484349052285755		[learning rate: 0.00022718]
		[batch 20/20] avg loss: -0.005513343669302137		[learning rate: 0.0002269]
	Learning Rate: 0.000226904
	LOSS [training: -0.008498846360793944 | validation: -0.009043627510413977]
	TIME [epoch: 8.86 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01150360679267666		[learning rate: 0.00022663]
		[batch 20/20] avg loss: -0.00797935225562373		[learning rate: 0.00022635]
	Learning Rate: 0.000226355
	LOSS [training: -0.009741479524150196 | validation: -0.010484541289020348]
	TIME [epoch: 8.84 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006877044327530377		[learning rate: 0.00022608]
		[batch 20/20] avg loss: -0.00863074473168593		[learning rate: 0.00022581]
	Learning Rate: 0.000225807
	LOSS [training: -0.007753894529608153 | validation: -0.009183437764784181]
	TIME [epoch: 8.86 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01240547739223961		[learning rate: 0.00022553]
		[batch 20/20] avg loss: -0.008938482604314953		[learning rate: 0.00022526]
	Learning Rate: 0.00022526
	LOSS [training: -0.010671979998277282 | validation: -0.005734936548064338]
	TIME [epoch: 8.86 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00872094666313453		[learning rate: 0.00022499]
		[batch 20/20] avg loss: -0.007315155036900651		[learning rate: 0.00022471]
	Learning Rate: 0.000224715
	LOSS [training: -0.008018050850017592 | validation: -0.012719593016291345]
	TIME [epoch: 8.87 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008622343184887625		[learning rate: 0.00022444]
		[batch 20/20] avg loss: -0.010894043024583579		[learning rate: 0.00022417]
	Learning Rate: 0.000224171
	LOSS [training: -0.009758193104735604 | validation: -0.012131413150808368]
	TIME [epoch: 8.85 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009695988772757416		[learning rate: 0.0002239]
		[batch 20/20] avg loss: -0.005924371671782608		[learning rate: 0.00022363]
	Learning Rate: 0.000223628
	LOSS [training: -0.0078101802222700105 | validation: -0.016017576126111207]
	TIME [epoch: 8.85 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005649853106654006		[learning rate: 0.00022336]
		[batch 20/20] avg loss: -0.008117674210381133		[learning rate: 0.00022309]
	Learning Rate: 0.000223087
	LOSS [training: -0.00688376365851757 | validation: -0.006210376862176075]
	TIME [epoch: 8.85 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006800466886112079		[learning rate: 0.00022282]
		[batch 20/20] avg loss: -0.0063076542393000114		[learning rate: 0.00022255]
	Learning Rate: 0.000222547
	LOSS [training: -0.0065540605627060445 | validation: -0.009907562903544909]
	TIME [epoch: 8.85 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010831228831992373		[learning rate: 0.00022228]
		[batch 20/20] avg loss: -0.008052844286491109		[learning rate: 0.00022201]
	Learning Rate: 0.000222008
	LOSS [training: -0.009442036559241739 | validation: -0.013091157751075146]
	TIME [epoch: 8.87 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014274776407455895		[learning rate: 0.00022174]
		[batch 20/20] avg loss: -0.006958008919522219		[learning rate: 0.00022147]
	Learning Rate: 0.00022147
	LOSS [training: -0.010616392663489058 | validation: -0.010414935112088946]
	TIME [epoch: 8.85 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007469360588856752		[learning rate: 0.0002212]
		[batch 20/20] avg loss: -0.007675866497571811		[learning rate: 0.00022093]
	Learning Rate: 0.000220934
	LOSS [training: -0.007572613543214282 | validation: -0.01699825237476251]
	TIME [epoch: 8.85 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0061550843948692075		[learning rate: 0.00022067]
		[batch 20/20] avg loss: -0.008995604040912642		[learning rate: 0.0002204]
	Learning Rate: 0.000220399
	LOSS [training: -0.0075753442178909235 | validation: -0.012532395038609763]
	TIME [epoch: 8.85 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009705443928722959		[learning rate: 0.00022013]
		[batch 20/20] avg loss: -0.010778818918738383		[learning rate: 0.00021987]
	Learning Rate: 0.000219866
	LOSS [training: -0.01024213142373067 | validation: -0.012624269596647227]
	TIME [epoch: 8.87 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012691501634695651		[learning rate: 0.0002196]
		[batch 20/20] avg loss: -0.007147785689366035		[learning rate: 0.00021933]
	Learning Rate: 0.000219334
	LOSS [training: -0.009919643662030844 | validation: -0.018878537979515303]
	TIME [epoch: 8.86 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011318294343136194		[learning rate: 0.00021907]
		[batch 20/20] avg loss: -0.01008301077104665		[learning rate: 0.0002188]
	Learning Rate: 0.000218803
	LOSS [training: -0.010700652557091421 | validation: -0.014064563968354449]
	TIME [epoch: 8.85 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010862696538285924		[learning rate: 0.00021854]
		[batch 20/20] avg loss: -0.006981132529582074		[learning rate: 0.00021827]
	Learning Rate: 0.000218273
	LOSS [training: -0.008921914533934 | validation: -0.011901470433309563]
	TIME [epoch: 8.84 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037631468027174804		[learning rate: 0.00021801]
		[batch 20/20] avg loss: -0.013454765807186356		[learning rate: 0.00021774]
	Learning Rate: 0.000217745
	LOSS [training: -0.008608956304951917 | validation: -0.012907120097177472]
	TIME [epoch: 8.84 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009354293687772516		[learning rate: 0.00021748]
		[batch 20/20] avg loss: -0.013053706911037119		[learning rate: 0.00021722]
	Learning Rate: 0.000217217
	LOSS [training: -0.011204000299404819 | validation: -0.018744706605519912]
	TIME [epoch: 8.86 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008134098302518747		[learning rate: 0.00021695]
		[batch 20/20] avg loss: -0.009337063466469325		[learning rate: 0.00021669]
	Learning Rate: 0.000216692
	LOSS [training: -0.008735580884494033 | validation: -0.007078507535866623]
	TIME [epoch: 8.86 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005372616858845304		[learning rate: 0.00021643]
		[batch 20/20] avg loss: -0.007168914776893344		[learning rate: 0.00021617]
	Learning Rate: 0.000216167
	LOSS [training: -0.006270765817869323 | validation: -0.013068016929473266]
	TIME [epoch: 8.86 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011173848318489644		[learning rate: 0.00021591]
		[batch 20/20] avg loss: -0.0037766064359016165		[learning rate: 0.00021564]
	Learning Rate: 0.000215644
	LOSS [training: -0.00747522737719563 | validation: -0.011097186635754578]
	TIME [epoch: 8.86 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007966785946540425		[learning rate: 0.00021538]
		[batch 20/20] avg loss: -0.012211827180137882		[learning rate: 0.00021512]
	Learning Rate: 0.000215122
	LOSS [training: -0.010089306563339156 | validation: -0.019032196709972835]
	TIME [epoch: 8.85 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00812330687968221		[learning rate: 0.00021486]
		[batch 20/20] avg loss: -0.011249160679273187		[learning rate: 0.0002146]
	Learning Rate: 0.000214601
	LOSS [training: -0.009686233779477697 | validation: -0.01576592465825433]
	TIME [epoch: 8.87 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.015621252724088158		[learning rate: 0.00021434]
		[batch 20/20] avg loss: -0.009788033857330439		[learning rate: 0.00021408]
	Learning Rate: 0.000214081
	LOSS [training: -0.0127046432907093 | validation: -0.01189339374376955]
	TIME [epoch: 8.84 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00645438222039776		[learning rate: 0.00021382]
		[batch 20/20] avg loss: -0.012608962343509434		[learning rate: 0.00021356]
	Learning Rate: 0.000213563
	LOSS [training: -0.009531672281953596 | validation: -0.011254412841804787]
	TIME [epoch: 8.86 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009189294528896965		[learning rate: 0.0002133]
		[batch 20/20] avg loss: -0.012789291103306344		[learning rate: 0.00021305]
	Learning Rate: 0.000213046
	LOSS [training: -0.010989292816101653 | validation: -0.006732982592163983]
	TIME [epoch: 8.86 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007165587512729936		[learning rate: 0.00021279]
		[batch 20/20] avg loss: -0.010172231713795648		[learning rate: 0.00021253]
	Learning Rate: 0.00021253
	LOSS [training: -0.00866890961326279 | validation: -0.009968020613867225]
	TIME [epoch: 8.85 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010724343026133575		[learning rate: 0.00021227]
		[batch 20/20] avg loss: -0.012382366506965553		[learning rate: 0.00021202]
	Learning Rate: 0.000212016
	LOSS [training: -0.011553354766549565 | validation: -0.008563806156600715]
	TIME [epoch: 8.86 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010434533102146513		[learning rate: 0.00021176]
		[batch 20/20] avg loss: -0.005390458667348226		[learning rate: 0.0002115]
	Learning Rate: 0.000211503
	LOSS [training: -0.00791249588474737 | validation: -0.01374376092876839]
	TIME [epoch: 8.85 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010266860194269327		[learning rate: 0.00021125]
		[batch 20/20] avg loss: -0.009029843355886701		[learning rate: 0.00021099]
	Learning Rate: 0.000210991
	LOSS [training: -0.009648351775078015 | validation: -0.013078096326530415]
	TIME [epoch: 8.86 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010203162782505073		[learning rate: 0.00021074]
		[batch 20/20] avg loss: -0.009475257852800692		[learning rate: 0.00021048]
	Learning Rate: 0.00021048
	LOSS [training: -0.009839210317652882 | validation: -0.010431728283299156]
	TIME [epoch: 8.85 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007417324337394779		[learning rate: 0.00021022]
		[batch 20/20] avg loss: -0.005452978315685624		[learning rate: 0.00020997]
	Learning Rate: 0.00020997
	LOSS [training: -0.006435151326540202 | validation: -0.018135580908389896]
	TIME [epoch: 8.87 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004808032636571483		[learning rate: 0.00020972]
		[batch 20/20] avg loss: -0.00915874303641247		[learning rate: 0.00020946]
	Learning Rate: 0.000209462
	LOSS [training: -0.006983387836491976 | validation: -0.007691302964843715]
	TIME [epoch: 8.87 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0091207898519023		[learning rate: 0.00020921]
		[batch 20/20] avg loss: -0.006050612554838736		[learning rate: 0.00020895]
	Learning Rate: 0.000208955
	LOSS [training: -0.007585701203370518 | validation: -0.01572424511471395]
	TIME [epoch: 8.86 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008933591934611324		[learning rate: 0.0002087]
		[batch 20/20] avg loss: -0.0027979717051677443		[learning rate: 0.00020845]
	Learning Rate: 0.000208449
	LOSS [training: -0.005865781819889534 | validation: -0.005845165098550185]
	TIME [epoch: 8.86 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029915449384523424		[learning rate: 0.0002082]
		[batch 20/20] avg loss: -0.0030482279326984993		[learning rate: 0.00020794]
	Learning Rate: 0.000207944
	LOSS [training: -0.0030198864355754206 | validation: -0.006491147093194838]
	TIME [epoch: 8.85 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010341723463594623		[learning rate: 0.00020769]
		[batch 20/20] avg loss: -0.009869946514156241		[learning rate: 0.00020744]
	Learning Rate: 0.000207441
	LOSS [training: -0.010105834988875432 | validation: -0.008855835632511933]
	TIME [epoch: 8.86 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010346773666697983		[learning rate: 0.00020719]
		[batch 20/20] avg loss: -0.009471956120114752		[learning rate: 0.00020694]
	Learning Rate: 0.000206939
	LOSS [training: -0.00990936489340637 | validation: -0.012581599463747754]
	TIME [epoch: 8.85 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014069595322187083		[learning rate: 0.00020669]
		[batch 20/20] avg loss: -0.007172702850737872		[learning rate: 0.00020644]
	Learning Rate: 0.000206438
	LOSS [training: -0.010621149086462478 | validation: -0.013208327898746416]
	TIME [epoch: 8.85 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010894122886652629		[learning rate: 0.00020619]
		[batch 20/20] avg loss: -0.0068650569036821955		[learning rate: 0.00020594]
	Learning Rate: 0.000205938
	LOSS [training: -0.008879589895167411 | validation: -0.013757880612180291]
	TIME [epoch: 8.85 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012166692400809545		[learning rate: 0.00020569]
		[batch 20/20] avg loss: -0.01080522823210045		[learning rate: 0.00020544]
	Learning Rate: 0.00020544
	LOSS [training: -0.011485960316455 | validation: -0.015152916008210813]
	TIME [epoch: 8.85 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009420193572931279		[learning rate: 0.00020519]
		[batch 20/20] avg loss: -0.006765933747767719		[learning rate: 0.00020494]
	Learning Rate: 0.000204942
	LOSS [training: -0.008093063660349498 | validation: -0.013210952613080982]
	TIME [epoch: 8.87 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01116144252906016		[learning rate: 0.00020469]
		[batch 20/20] avg loss: -0.012188897865864563		[learning rate: 0.00020445]
	Learning Rate: 0.000204446
	LOSS [training: -0.011675170197462366 | validation: -0.008237235781637764]
	TIME [epoch: 8.85 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007703403649393714		[learning rate: 0.0002042]
		[batch 20/20] avg loss: -0.01339361781327297		[learning rate: 0.00020395]
	Learning Rate: 0.000203951
	LOSS [training: -0.010548510731333343 | validation: -0.01567060508510241]
	TIME [epoch: 8.85 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013463916384197663		[learning rate: 0.0002037]
		[batch 20/20] avg loss: -0.015150786913378875		[learning rate: 0.00020346]
	Learning Rate: 0.000203457
	LOSS [training: -0.014307351648788269 | validation: -0.01714174232789684]
	TIME [epoch: 8.86 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010300542058408897		[learning rate: 0.00020321]
		[batch 20/20] avg loss: -0.010076631502762225		[learning rate: 0.00020296]
	Learning Rate: 0.000202965
	LOSS [training: -0.010188586780585561 | validation: -0.014231337847992903]
	TIME [epoch: 8.88 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010104275672564497		[learning rate: 0.00020272]
		[batch 20/20] avg loss: -0.00805468299408529		[learning rate: 0.00020247]
	Learning Rate: 0.000202474
	LOSS [training: -0.009079479333324893 | validation: -0.018944264593055607]
	TIME [epoch: 8.86 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014847132387186848		[learning rate: 0.00020223]
		[batch 20/20] avg loss: -0.00948647090573788		[learning rate: 0.00020198]
	Learning Rate: 0.000201983
	LOSS [training: -0.012166801646462364 | validation: -0.009836749184425038]
	TIME [epoch: 8.85 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005470322441335237		[learning rate: 0.00020174]
		[batch 20/20] avg loss: -0.006332478628781525		[learning rate: 0.00020149]
	Learning Rate: 0.000201495
	LOSS [training: -0.00590140053505838 | validation: -0.007297381529396059]
	TIME [epoch: 8.84 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012644534204483613		[learning rate: 0.00020125]
		[batch 20/20] avg loss: -0.012013631019895474		[learning rate: 0.00020101]
	Learning Rate: 0.000201007
	LOSS [training: -0.012329082612189544 | validation: -0.01654365438212562]
	TIME [epoch: 8.85 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006498710920833461		[learning rate: 0.00020076]
		[batch 20/20] avg loss: -0.009540013339044731		[learning rate: 0.00020052]
	Learning Rate: 0.00020052
	LOSS [training: -0.008019362129939095 | validation: -0.018818244989230633]
	TIME [epoch: 8.86 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006796011149494148		[learning rate: 0.00020028]
		[batch 20/20] avg loss: -0.013241108884491721		[learning rate: 0.00020003]
	Learning Rate: 0.000200035
	LOSS [training: -0.010018560016992934 | validation: -0.014024557700651457]
	TIME [epoch: 8.84 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01115910246461446		[learning rate: 0.00019979]
		[batch 20/20] avg loss: -0.004704288595660898		[learning rate: 0.00019955]
	Learning Rate: 0.00019955
	LOSS [training: -0.007931695530137678 | validation: -0.01149632137102518]
	TIME [epoch: 8.85 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007179843932172777		[learning rate: 0.00019931]
		[batch 20/20] avg loss: -0.013612836090731087		[learning rate: 0.00019907]
	Learning Rate: 0.000199067
	LOSS [training: -0.01039634001145193 | validation: -0.011685646753858131]
	TIME [epoch: 8.84 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009418234468330929		[learning rate: 0.00019883]
		[batch 20/20] avg loss: -0.010234648291978196		[learning rate: 0.00019859]
	Learning Rate: 0.000198585
	LOSS [training: -0.009826441380154562 | validation: -0.015568045409240363]
	TIME [epoch: 8.85 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009893108081716843		[learning rate: 0.00019834]
		[batch 20/20] avg loss: -0.007396301709097811		[learning rate: 0.0001981]
	Learning Rate: 0.000198105
	LOSS [training: -0.008644704895407324 | validation: -0.009805727919011322]
	TIME [epoch: 8.87 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010099490599899872		[learning rate: 0.00019786]
		[batch 20/20] avg loss: -0.007643961769802957		[learning rate: 0.00019763]
	Learning Rate: 0.000197625
	LOSS [training: -0.008871726184851414 | validation: -0.006552257718891157]
	TIME [epoch: 8.85 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010673380570235532		[learning rate: 0.00019739]
		[batch 20/20] avg loss: -0.0076005915527335485		[learning rate: 0.00019715]
	Learning Rate: 0.000197147
	LOSS [training: -0.009136986061484538 | validation: -0.006269156825074715]
	TIME [epoch: 8.85 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008122185188194755		[learning rate: 0.00019691]
		[batch 20/20] avg loss: -0.003811285648923498		[learning rate: 0.00019667]
	Learning Rate: 0.000196669
	LOSS [training: -0.0059667354185591265 | validation: -0.008616659620717964]
	TIME [epoch: 8.85 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010786562530937672		[learning rate: 0.00019643]
		[batch 20/20] avg loss: -0.009752540591218955		[learning rate: 0.00019619]
	Learning Rate: 0.000196193
	LOSS [training: -0.010269551561078314 | validation: -0.011818350973655578]
	TIME [epoch: 8.87 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0072307056884102616		[learning rate: 0.00019596]
		[batch 20/20] avg loss: -0.01070257381870377		[learning rate: 0.00019572]
	Learning Rate: 0.000195718
	LOSS [training: -0.008966639753557016 | validation: -0.012148171651376976]
	TIME [epoch: 8.86 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009451322606781134		[learning rate: 0.00019548]
		[batch 20/20] avg loss: -0.011339069305071827		[learning rate: 0.00019524]
	Learning Rate: 0.000195245
	LOSS [training: -0.010395195955926478 | validation: -0.010397952364161769]
	TIME [epoch: 8.85 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008950345507877001		[learning rate: 0.00019501]
		[batch 20/20] avg loss: -0.005667234605126383		[learning rate: 0.00019477]
	Learning Rate: 0.000194772
	LOSS [training: -0.007308790056501692 | validation: -0.012717481009002427]
	TIME [epoch: 8.84 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008793775556279563		[learning rate: 0.00019454]
		[batch 20/20] avg loss: -0.010493934770015088		[learning rate: 0.0001943]
	Learning Rate: 0.0001943
	LOSS [training: -0.009643855163147325 | validation: -0.012807032095973959]
	TIME [epoch: 8.85 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006918055059563348		[learning rate: 0.00019407]
		[batch 20/20] avg loss: -0.011014349300134814		[learning rate: 0.00019383]
	Learning Rate: 0.00019383
	LOSS [training: -0.008966202179849081 | validation: -0.012788885401320199]
	TIME [epoch: 8.87 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00859321578290911		[learning rate: 0.0001936]
		[batch 20/20] avg loss: -0.00922787329536317		[learning rate: 0.00019336]
	Learning Rate: 0.000193361
	LOSS [training: -0.008910544539136141 | validation: -0.01564172748574459]
	TIME [epoch: 8.85 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013079069510481798		[learning rate: 0.00019313]
		[batch 20/20] avg loss: -0.010806005985683926		[learning rate: 0.00019289]
	Learning Rate: 0.000192893
	LOSS [training: -0.011942537748082863 | validation: -0.01758440305394713]
	TIME [epoch: 8.85 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014553520042631		[learning rate: 0.00019266]
		[batch 20/20] avg loss: -0.003601619606893204		[learning rate: 0.00019243]
	Learning Rate: 0.000192426
	LOSS [training: -0.009077569824762101 | validation: -0.01442094751961797]
	TIME [epoch: 8.85 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007992121235225896		[learning rate: 0.00019219]
		[batch 20/20] avg loss: -0.009228300565920961		[learning rate: 0.00019196]
	Learning Rate: 0.00019196
	LOSS [training: -0.008610210900573428 | validation: -0.012675312614657881]
	TIME [epoch: 8.85 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008986532247834782		[learning rate: 0.00019173]
		[batch 20/20] avg loss: -0.00862192805091504		[learning rate: 0.0001915]
	Learning Rate: 0.000191495
	LOSS [training: -0.008804230149374913 | validation: -0.0131636861493463]
	TIME [epoch: 8.88 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007705556675458455		[learning rate: 0.00019126]
		[batch 20/20] avg loss: -0.006315869300149164		[learning rate: 0.00019103]
	Learning Rate: 0.000191032
	LOSS [training: -0.007010712987803811 | validation: -0.005335336294876657]
	TIME [epoch: 8.86 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006572674245138484		[learning rate: 0.0001908]
		[batch 20/20] avg loss: -0.011441926682570755		[learning rate: 0.00019057]
	Learning Rate: 0.000190569
	LOSS [training: -0.00900730046385462 | validation: -0.009553468173333643]
	TIME [epoch: 8.86 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011324962636308448		[learning rate: 0.00019034]
		[batch 20/20] avg loss: -0.0050322937321039055		[learning rate: 0.00019011]
	Learning Rate: 0.000190108
	LOSS [training: -0.008178628184206177 | validation: -0.013167786070481743]
	TIME [epoch: 8.86 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008492921250048023		[learning rate: 0.00018988]
		[batch 20/20] avg loss: -0.008269923352741074		[learning rate: 0.00018965]
	Learning Rate: 0.000189648
	LOSS [training: -0.008381422301394548 | validation: -0.01111906198538982]
	TIME [epoch: 8.88 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01058869285767229		[learning rate: 0.00018942]
		[batch 20/20] avg loss: -0.006805631314568986		[learning rate: 0.00018919]
	Learning Rate: 0.000189189
	LOSS [training: -0.00869716208612064 | validation: -0.010429681985735854]
	TIME [epoch: 8.88 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011529806734283181		[learning rate: 0.00018896]
		[batch 20/20] avg loss: -0.00851193256224789		[learning rate: 0.00018873]
	Learning Rate: 0.000188731
	LOSS [training: -0.010020869648265535 | validation: -0.010053989416153779]
	TIME [epoch: 8.86 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010129740114072892		[learning rate: 0.0001885]
		[batch 20/20] avg loss: -0.004261849435789852		[learning rate: 0.00018827]
	Learning Rate: 0.000188274
	LOSS [training: -0.007195794774931372 | validation: -0.019453209353431943]
	TIME [epoch: 8.86 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005972999930166296		[learning rate: 0.00018805]
		[batch 20/20] avg loss: -0.0067657987589253244		[learning rate: 0.00018782]
	Learning Rate: 0.000187818
	LOSS [training: -0.006369399344545809 | validation: -0.02008411920331578]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_1740.pth
	Model improved!!!
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010363840028367901		[learning rate: 0.00018759]
		[batch 20/20] avg loss: -0.007792349314961816		[learning rate: 0.00018736]
	Learning Rate: 0.000187363
	LOSS [training: -0.009078094671664858 | validation: -0.00591895685149048]
	TIME [epoch: 8.86 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008454621569660209		[learning rate: 0.00018714]
		[batch 20/20] avg loss: -0.009156187115197738		[learning rate: 0.00018691]
	Learning Rate: 0.00018691
	LOSS [training: -0.008805404342428973 | validation: -0.018077742992196708]
	TIME [epoch: 8.85 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010890075716566187		[learning rate: 0.00018668]
		[batch 20/20] avg loss: -0.002584377372215382		[learning rate: 0.00018646]
	Learning Rate: 0.000186457
	LOSS [training: -0.006737226544390784 | validation: -0.015435943932823567]
	TIME [epoch: 8.84 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008103192396949673		[learning rate: 0.00018623]
		[batch 20/20] avg loss: -0.010587964611630316		[learning rate: 0.00018601]
	Learning Rate: 0.000186006
	LOSS [training: -0.009345578504289995 | validation: -0.008567645410932672]
	TIME [epoch: 8.85 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008949154077800007		[learning rate: 0.00018578]
		[batch 20/20] avg loss: -0.012785486384213651		[learning rate: 0.00018556]
	Learning Rate: 0.000185555
	LOSS [training: -0.010867320231006826 | validation: -0.02344964638576101]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r0_20240219_183145/states/model_tr_study2_1745.pth
	Model improved!!!
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008805257687513179		[learning rate: 0.00018533]
		[batch 20/20] avg loss: -0.013852311733086792		[learning rate: 0.00018511]
	Learning Rate: 0.000185106
	LOSS [training: -0.011328784710299983 | validation: -0.014049544150225713]
	TIME [epoch: 8.86 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012948794929442056		[learning rate: 0.00018488]
		[batch 20/20] avg loss: -0.010427723545087295		[learning rate: 0.00018466]
	Learning Rate: 0.000184658
	LOSS [training: -0.011688259237264676 | validation: -0.007385932745739291]
	TIME [epoch: 8.85 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008351046291554882		[learning rate: 0.00018443]
		[batch 20/20] avg loss: -0.014349391964941732		[learning rate: 0.00018421]
	Learning Rate: 0.000184211
	LOSS [training: -0.011350219128248306 | validation: -0.009018816095771313]
	TIME [epoch: 8.85 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006348465087291857		[learning rate: 0.00018399]
		[batch 20/20] avg loss: -0.007757231948600276		[learning rate: 0.00018377]
	Learning Rate: 0.000183765
	LOSS [training: -0.007052848517946066 | validation: -0.006417584497425691]
	TIME [epoch: 8.85 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009211854414698666		[learning rate: 0.00018354]
		[batch 20/20] avg loss: -0.009775303048086739		[learning rate: 0.00018332]
	Learning Rate: 0.00018332
	LOSS [training: -0.009493578731392702 | validation: -0.017879396046082956]
	TIME [epoch: 8.85 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004808037540823798		[learning rate: 0.0001831]
		[batch 20/20] avg loss: -0.01111522092692953		[learning rate: 0.00018288]
	Learning Rate: 0.000182876
	LOSS [training: -0.007961629233876662 | validation: -0.0072091036099240025]
	TIME [epoch: 8.85 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006872874633060376		[learning rate: 0.00018266]
		[batch 20/20] avg loss: -0.011489131591867294		[learning rate: 0.00018243]
	Learning Rate: 0.000182434
	LOSS [training: -0.009181003112463835 | validation: -0.008172445062782174]
	TIME [epoch: 8.84 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007536353739255033		[learning rate: 0.00018221]
		[batch 20/20] avg loss: -0.012094198745367008		[learning rate: 0.00018199]
	Learning Rate: 0.000181992
	LOSS [training: -0.00981527624231102 | validation: -0.012395555249377233]
	TIME [epoch: 8.84 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00824379786094094		[learning rate: 0.00018177]
		[batch 20/20] avg loss: -0.008422345561302653		[learning rate: 0.00018155]
	Learning Rate: 0.000181552
	LOSS [training: -0.0083330717111218 | validation: -0.010193278493367439]
	TIME [epoch: 8.84 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010941115320647777		[learning rate: 0.00018133]
		[batch 20/20] avg loss: -0.009173171639315787		[learning rate: 0.00018111]
	Learning Rate: 0.000181112
	LOSS [training: -0.01005714347998178 | validation: -0.012568727593109747]
	TIME [epoch: 8.85 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007754789641260423		[learning rate: 0.00018089]
		[batch 20/20] avg loss: -0.010029629742902241		[learning rate: 0.00018067]
	Learning Rate: 0.000180674
	LOSS [training: -0.008892209692081331 | validation: -0.012523232425280447]
	TIME [epoch: 8.85 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006987817884744609		[learning rate: 0.00018045]
		[batch 20/20] avg loss: -0.011017503003070587		[learning rate: 0.00018024]
	Learning Rate: 0.000180236
	LOSS [training: -0.009002660443907599 | validation: -0.012177756761888934]
	TIME [epoch: 8.84 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0056059973684811385		[learning rate: 0.00018002]
		[batch 20/20] avg loss: -0.013326227165636122		[learning rate: 0.0001798]
	Learning Rate: 0.0001798
	LOSS [training: -0.009466112267058631 | validation: -0.00797778739219973]
	TIME [epoch: 8.84 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009823356992128121		[learning rate: 0.00017958]
		[batch 20/20] avg loss: -0.00652311763845922		[learning rate: 0.00017936]
	Learning Rate: 0.000179365
	LOSS [training: -0.00817323731529367 | validation: -0.012517533580006162]
	TIME [epoch: 8.84 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006178506579166659		[learning rate: 0.00017915]
		[batch 20/20] avg loss: -0.01075809730311576		[learning rate: 0.00017893]
	Learning Rate: 0.00017893
	LOSS [training: -0.008468301941141208 | validation: -0.010768814563657698]
	TIME [epoch: 8.87 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008005390415346971		[learning rate: 0.00017871]
		[batch 20/20] avg loss: -0.01340076070040463		[learning rate: 0.0001785]
	Learning Rate: 0.000178497
	LOSS [training: -0.010703075557875799 | validation: -0.014091710416723451]
	TIME [epoch: 8.85 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003348643136667053		[learning rate: 0.00017828]
		[batch 20/20] avg loss: -0.004792364906433781		[learning rate: 0.00017807]
	Learning Rate: 0.000178065
	LOSS [training: -0.004070504021550417 | validation: -0.014015966096026022]
	TIME [epoch: 8.84 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009141753116281471		[learning rate: 0.00017785]
		[batch 20/20] avg loss: -0.005233421838072495		[learning rate: 0.00017763]
	Learning Rate: 0.000177634
	LOSS [training: -0.007187587477176983 | validation: -0.013841315086905503]
	TIME [epoch: 8.85 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010840960113141304		[learning rate: 0.00017742]
		[batch 20/20] avg loss: -0.008246999049926273		[learning rate: 0.0001772]
	Learning Rate: 0.000177204
	LOSS [training: -0.00954397958153379 | validation: -0.015656708628174185]
	TIME [epoch: 8.82 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011639914035203936		[learning rate: 0.00017699]
		[batch 20/20] avg loss: -0.004828466586495843		[learning rate: 0.00017678]
	Learning Rate: 0.000176775
	LOSS [training: -0.00823419031084989 | validation: -0.009905222646404283]
	TIME [epoch: 8.85 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010895330374861028		[learning rate: 0.00017656]
		[batch 20/20] avg loss: -0.00805210052481584		[learning rate: 0.00017635]
	Learning Rate: 0.000176347
	LOSS [training: -0.009473715449838436 | validation: -0.010837137516450605]
	TIME [epoch: 8.84 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009148468893871204		[learning rate: 0.00017613]
		[batch 20/20] avg loss: -0.010253517504400723		[learning rate: 0.00017592]
	Learning Rate: 0.00017592
	LOSS [training: -0.009700993199135964 | validation: -0.02133787450102775]
	TIME [epoch: 8.84 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011017451936990599		[learning rate: 0.00017571]
		[batch 20/20] avg loss: -0.0069304160810651425		[learning rate: 0.00017549]
	Learning Rate: 0.000175494
	LOSS [training: -0.008973934009027872 | validation: -0.010947307923909294]
	TIME [epoch: 8.83 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0124665278189825		[learning rate: 0.00017528]
		[batch 20/20] avg loss: -0.008155611653869814		[learning rate: 0.00017507]
	Learning Rate: 0.00017507
	LOSS [training: -0.010311069736426155 | validation: -0.01846028612840367]
	TIME [epoch: 8.87 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010557597607165254		[learning rate: 0.00017486]
		[batch 20/20] avg loss: -0.010671781427275569		[learning rate: 0.00017465]
	Learning Rate: 0.000174646
	LOSS [training: -0.01061468951722041 | validation: -0.012784882419378122]
	TIME [epoch: 8.85 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009240871746249199		[learning rate: 0.00017443]
		[batch 20/20] avg loss: -0.007778989062880844		[learning rate: 0.00017422]
	Learning Rate: 0.000174223
	LOSS [training: -0.00850993040456502 | validation: -0.013093206996109967]
	TIME [epoch: 8.84 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008227724719155255		[learning rate: 0.00017401]
		[batch 20/20] avg loss: -0.008069825547723626		[learning rate: 0.0001738]
	Learning Rate: 0.000173801
	LOSS [training: -0.00814877513343944 | validation: -0.012580711607711568]
	TIME [epoch: 8.83 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01132125888793328		[learning rate: 0.00017359]
		[batch 20/20] avg loss: -0.010145469645647267		[learning rate: 0.00017338]
	Learning Rate: 0.00017338
	LOSS [training: -0.010733364266790276 | validation: -0.008411288908465998]
	TIME [epoch: 8.84 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010502127162250635		[learning rate: 0.00017317]
		[batch 20/20] avg loss: -0.008495026211523326		[learning rate: 0.00017296]
	Learning Rate: 0.000172961
	LOSS [training: -0.009498576686886983 | validation: -0.014089850774318876]
	TIME [epoch: 8.86 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006132928306462811		[learning rate: 0.00017275]
		[batch 20/20] avg loss: -0.015461576858044463		[learning rate: 0.00017254]
	Learning Rate: 0.000172542
	LOSS [training: -0.010797252582253636 | validation: -0.007118026237029487]
	TIME [epoch: 8.84 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00808801859343058		[learning rate: 0.00017233]
		[batch 20/20] avg loss: -0.011750708874469802		[learning rate: 0.00017212]
	Learning Rate: 0.000172124
	LOSS [training: -0.009919363733950191 | validation: -0.017554526694395743]
	TIME [epoch: 8.85 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0074644720763003035		[learning rate: 0.00017192]
		[batch 20/20] avg loss: -0.007605263479281381		[learning rate: 0.00017171]
	Learning Rate: 0.000171708
	LOSS [training: -0.007534867777790842 | validation: -0.015178930984699922]
	TIME [epoch: 8.85 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005824423139936264		[learning rate: 0.0001715]
		[batch 20/20] avg loss: -0.010515304614691554		[learning rate: 0.00017129]
	Learning Rate: 0.000171292
	LOSS [training: -0.00816986387731391 | validation: -0.01585466359412901]
	TIME [epoch: 8.85 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009129844706504731		[learning rate: 0.00017108]
		[batch 20/20] avg loss: -0.008698326408827445		[learning rate: 0.00017088]
	Learning Rate: 0.000170877
	LOSS [training: -0.008914085557666087 | validation: -0.005122337933039304]
	TIME [epoch: 8.87 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01040631233474965		[learning rate: 0.00017067]
		[batch 20/20] avg loss: -0.009513164286003857		[learning rate: 0.00017046]
	Learning Rate: 0.000170464
	LOSS [training: -0.009959738310376755 | validation: -0.006998819029109211]
	TIME [epoch: 8.86 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01430669715204864		[learning rate: 0.00017026]
		[batch 20/20] avg loss: -0.008944467310054684		[learning rate: 0.00017005]
	Learning Rate: 0.000170051
	LOSS [training: -0.011625582231051663 | validation: -0.013745723190672252]
	TIME [epoch: 8.85 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0105599715914973		[learning rate: 0.00016984]
		[batch 20/20] avg loss: -0.008561442207190031		[learning rate: 0.00016964]
	Learning Rate: 0.000169639
	LOSS [training: -0.009560706899343662 | validation: -0.015544799372647744]
	TIME [epoch: 8.83 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012793931874113675		[learning rate: 0.00016943]
		[batch 20/20] avg loss: -0.006589095411057744		[learning rate: 0.00016923]
	Learning Rate: 0.000169229
	LOSS [training: -0.00969151364258571 | validation: -0.010635065820652706]
	TIME [epoch: 8.86 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010734858205705303		[learning rate: 0.00016902]
		[batch 20/20] avg loss: -0.009511634702442387		[learning rate: 0.00016882]
	Learning Rate: 0.000168819
	LOSS [training: -0.010123246454073847 | validation: -0.015035456872389025]
	TIME [epoch: 8.86 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010611681018924915		[learning rate: 0.00016861]
		[batch 20/20] avg loss: -0.007005259178191625		[learning rate: 0.00016841]
	Learning Rate: 0.00016841
	LOSS [training: -0.00880847009855827 | validation: -0.012416741740807813]
	TIME [epoch: 8.84 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00857948361016261		[learning rate: 0.00016821]
		[batch 20/20] avg loss: -0.013199849122742146		[learning rate: 0.000168]
	Learning Rate: 0.000168003
	LOSS [training: -0.010889666366452378 | validation: -0.01766616629759767]
	TIME [epoch: 8.85 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011384085603973668		[learning rate: 0.0001678]
		[batch 20/20] avg loss: -0.010703453597808693		[learning rate: 0.0001676]
	Learning Rate: 0.000167596
	LOSS [training: -0.01104376960089118 | validation: -0.011968724143088732]
	TIME [epoch: 8.85 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012063181568461935		[learning rate: 0.00016739]
		[batch 20/20] avg loss: -0.007573762023354094		[learning rate: 0.00016719]
	Learning Rate: 0.00016719
	LOSS [training: -0.009818471795908015 | validation: -0.008871727435273894]
	TIME [epoch: 8.87 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008210318561167365		[learning rate: 0.00016699]
		[batch 20/20] avg loss: -0.009633436560441622		[learning rate: 0.00016679]
	Learning Rate: 0.000166785
	LOSS [training: -0.008921877560804495 | validation: -0.014256493457430257]
	TIME [epoch: 8.86 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009743764499474847		[learning rate: 0.00016658]
		[batch 20/20] avg loss: -0.011055437217135387		[learning rate: 0.00016638]
	Learning Rate: 0.000166382
	LOSS [training: -0.010399600858305118 | validation: -0.015044374962550507]
	TIME [epoch: 8.86 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01393404649972329		[learning rate: 0.00016618]
		[batch 20/20] avg loss: -0.009985525903189721		[learning rate: 0.00016598]
	Learning Rate: 0.000165979
	LOSS [training: -0.011959786201456506 | validation: -0.014229012971940524]
	TIME [epoch: 8.85 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010280792800876643		[learning rate: 0.00016578]
		[batch 20/20] avg loss: -0.015476171327633723		[learning rate: 0.00016558]
	Learning Rate: 0.000165577
	LOSS [training: -0.012878482064255185 | validation: -0.016704513505815706]
	TIME [epoch: 8.86 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00813658272200218		[learning rate: 0.00016538]
		[batch 20/20] avg loss: -0.005518502486140482		[learning rate: 0.00016518]
	Learning Rate: 0.000165176
	LOSS [training: -0.006827542604071329 | validation: -0.014401582181487318]
	TIME [epoch: 8.86 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0069661186674751545		[learning rate: 0.00016498]
		[batch 20/20] avg loss: -0.007351092763594008		[learning rate: 0.00016478]
	Learning Rate: 0.000164776
	LOSS [training: -0.007158605715534581 | validation: -0.011087010959528645]
	TIME [epoch: 8.85 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010575018476634222		[learning rate: 0.00016458]
		[batch 20/20] avg loss: -0.0070465194805319434		[learning rate: 0.00016438]
	Learning Rate: 0.000164377
	LOSS [training: -0.008810768978583083 | validation: -0.010210285598020548]
	TIME [epoch: 8.85 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005740730550739969		[learning rate: 0.00016418]
		[batch 20/20] avg loss: -0.01180679873239995		[learning rate: 0.00016398]
	Learning Rate: 0.000163979
	LOSS [training: -0.00877376464156996 | validation: -0.010249376383324797]
	TIME [epoch: 8.85 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006513243080991843		[learning rate: 0.00016378]
		[batch 20/20] avg loss: -0.013742183455524753		[learning rate: 0.00016358]
	Learning Rate: 0.000163583
	LOSS [training: -0.010127713268258298 | validation: -0.00957542376795075]
	TIME [epoch: 8.84 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011130480559277213		[learning rate: 0.00016338]
		[batch 20/20] avg loss: -0.014247990712646944		[learning rate: 0.00016319]
	Learning Rate: 0.000163187
	LOSS [training: -0.012689235635962078 | validation: -0.013970695805219481]
	TIME [epoch: 8.87 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.015391239572846504		[learning rate: 0.00016299]
		[batch 20/20] avg loss: -0.008509071414689345		[learning rate: 0.00016279]
	Learning Rate: 0.000162791
	LOSS [training: -0.011950155493767926 | validation: -0.01001591706249855]
	TIME [epoch: 8.85 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0064252882647601334		[learning rate: 0.00016259]
		[batch 20/20] avg loss: -0.009634595690713826		[learning rate: 0.0001624]
	Learning Rate: 0.000162397
	LOSS [training: -0.008029941977736979 | validation: -0.009566551923408544]
	TIME [epoch: 8.84 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009703001933036158		[learning rate: 0.0001622]
		[batch 20/20] avg loss: -0.009588886662890348		[learning rate: 0.000162]
	Learning Rate: 0.000162004
	LOSS [training: -0.009645944297963251 | validation: -0.014421393226968254]
	TIME [epoch: 8.85 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007493350835440235		[learning rate: 0.00016181]
		[batch 20/20] avg loss: -0.012542761449868494		[learning rate: 0.00016161]
	Learning Rate: 0.000161612
	LOSS [training: -0.010018056142654363 | validation: -0.01546588554742655]
	TIME [epoch: 8.87 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008167999157828513		[learning rate: 0.00016142]
		[batch 20/20] avg loss: -0.008624992043458828		[learning rate: 0.00016122]
	Learning Rate: 0.000161221
	LOSS [training: -0.00839649560064367 | validation: -0.007703167664231734]
	TIME [epoch: 8.86 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010447255332663467		[learning rate: 0.00016103]
		[batch 20/20] avg loss: -0.008586819750673044		[learning rate: 0.00016083]
	Learning Rate: 0.000160831
	LOSS [training: -0.009517037541668254 | validation: -0.013095144441204902]
	TIME [epoch: 8.86 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011699354265056996		[learning rate: 0.00016064]
		[batch 20/20] avg loss: -0.004891550527044189		[learning rate: 0.00016044]
	Learning Rate: 0.000160441
	LOSS [training: -0.008295452396050593 | validation: -0.016636498179219642]
	TIME [epoch: 8.85 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009640069561752706		[learning rate: 0.00016025]
		[batch 20/20] avg loss: -0.011905498290324829		[learning rate: 0.00016005]
	Learning Rate: 0.000160053
	LOSS [training: -0.01077278392603877 | validation: -0.010403677648912164]
	TIME [epoch: 8.85 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010416026982303315		[learning rate: 0.00015986]
		[batch 20/20] avg loss: -0.008531571773399164		[learning rate: 0.00015967]
	Learning Rate: 0.000159665
	LOSS [training: -0.009473799377851238 | validation: -0.0119749671194955]
	TIME [epoch: 8.89 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008752945135095282		[learning rate: 0.00015947]
		[batch 20/20] avg loss: -0.012117380555514137		[learning rate: 0.00015928]
	Learning Rate: 0.000159279
	LOSS [training: -0.01043516284530471 | validation: -0.007485689173924726]
	TIME [epoch: 8.87 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012799323892923976		[learning rate: 0.00015909]
		[batch 20/20] avg loss: -0.013004564968585033		[learning rate: 0.00015889]
	Learning Rate: 0.000158893
	LOSS [training: -0.012901944430754504 | validation: -0.006853484075223522]
	TIME [epoch: 8.86 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009568505430701531		[learning rate: 0.0001587]
		[batch 20/20] avg loss: -0.01018182040499093		[learning rate: 0.00015851]
	Learning Rate: 0.000158509
	LOSS [training: -0.009875162917846231 | validation: -0.016098534705896794]
	TIME [epoch: 8.87 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038587886252845344		[learning rate: 0.00015832]
		[batch 20/20] avg loss: -0.014232996548931698		[learning rate: 0.00015812]
	Learning Rate: 0.000158125
	LOSS [training: -0.009045892587108115 | validation: -0.010996591191528692]
	TIME [epoch: 8.85 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0074801165323630184		[learning rate: 0.00015793]
		[batch 20/20] avg loss: -0.009030826419224718		[learning rate: 0.00015774]
	Learning Rate: 0.000157742
	LOSS [training: -0.008255471475793868 | validation: -0.011826242733602428]
	TIME [epoch: 8.88 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010239764421800883		[learning rate: 0.00015755]
		[batch 20/20] avg loss: -0.006511066849449196		[learning rate: 0.00015736]
	Learning Rate: 0.00015736
	LOSS [training: -0.00837541563562504 | validation: -0.01671840805913967]
	TIME [epoch: 8.86 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008670128046124725		[learning rate: 0.00015717]
		[batch 20/20] avg loss: -0.01274842106864752		[learning rate: 0.00015698]
	Learning Rate: 0.000156979
	LOSS [training: -0.010709274557386125 | validation: -0.011226850733025607]
	TIME [epoch: 8.86 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014825327757606597		[learning rate: 0.00015679]
		[batch 20/20] avg loss: -0.00257194033097912		[learning rate: 0.0001566]
	Learning Rate: 0.000156599
	LOSS [training: -0.008698634044292856 | validation: -0.015828585124143375]
	TIME [epoch: 8.87 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010544298977512977		[learning rate: 0.00015641]
		[batch 20/20] avg loss: -0.004874427574302539		[learning rate: 0.00015622]
	Learning Rate: 0.00015622
	LOSS [training: -0.007709363275907759 | validation: -0.01426264381092281]
	TIME [epoch: 8.87 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009626797649904501		[learning rate: 0.00015603]
		[batch 20/20] avg loss: -0.007264587878949952		[learning rate: 0.00015584]
	Learning Rate: 0.000155842
	LOSS [training: -0.008445692764427226 | validation: -0.014811453759958858]
	TIME [epoch: 8.87 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004270452998112356		[learning rate: 0.00015565]
		[batch 20/20] avg loss: -0.004714696741039266		[learning rate: 0.00015546]
	Learning Rate: 0.000155465
	LOSS [training: -0.004492574869575812 | validation: -0.006843297782912991]
	TIME [epoch: 8.87 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007680459570496986		[learning rate: 0.00015528]
		[batch 20/20] avg loss: -0.006322924356283325		[learning rate: 0.00015509]
	Learning Rate: 0.000155088
	LOSS [training: -0.007001691963390157 | validation: -0.010457347644231315]
	TIME [epoch: 8.86 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009593498121792093		[learning rate: 0.0001549]
		[batch 20/20] avg loss: -0.006678575452372779		[learning rate: 0.00015471]
	Learning Rate: 0.000154713
	LOSS [training: -0.008136036787082437 | validation: -0.012082313356685635]
	TIME [epoch: 8.85 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008307303529512911		[learning rate: 0.00015453]
		[batch 20/20] avg loss: 0.0034817254682116354		[learning rate: 0.00015434]
	Learning Rate: 0.000154338
	LOSS [training: -0.002412789030650637 | validation: -0.010897378350382137]
	TIME [epoch: 8.88 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009085043027131739		[learning rate: 0.00015415]
		[batch 20/20] avg loss: -0.006759495267792147		[learning rate: 0.00015396]
	Learning Rate: 0.000153965
	LOSS [training: -0.007922269147461943 | validation: -0.008049884313801579]
	TIME [epoch: 8.85 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011694749445458385		[learning rate: 0.00015378]
		[batch 20/20] avg loss: -0.004489881443379191		[learning rate: 0.00015359]
	Learning Rate: 0.000153592
	LOSS [training: -0.00809231544441879 | validation: -0.007043079695130221]
	TIME [epoch: 8.85 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033709748916827195		[learning rate: 0.00015341]
		[batch 20/20] avg loss: -0.008884406725906988		[learning rate: 0.00015322]
	Learning Rate: 0.00015322
	LOSS [training: -0.006127690808794853 | validation: -0.012037607234818816]
	TIME [epoch: 8.86 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023158054175050654		[learning rate: 0.00015303]
		[batch 20/20] avg loss: -0.010522735428132255		[learning rate: 0.00015285]
	Learning Rate: 0.000152849
	LOSS [training: -0.00641927042281866 | validation: -0.008231413318354542]
	TIME [epoch: 8.85 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011336706128487023		[learning rate: 0.00015266]
		[batch 20/20] avg loss: -0.008426501316681342		[learning rate: 0.00015248]
	Learning Rate: 0.000152479
	LOSS [training: -0.009881603722584184 | validation: -0.011843399346224668]
	TIME [epoch: 8.88 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008749759862247792		[learning rate: 0.00015229]
		[batch 20/20] avg loss: -0.0067573691812265165		[learning rate: 0.00015211]
	Learning Rate: 0.00015211
	LOSS [training: -0.007753564521737154 | validation: -0.012488646817531363]
	TIME [epoch: 8.85 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008278039806398824		[learning rate: 0.00015193]
		[batch 20/20] avg loss: -0.006356693703148925		[learning rate: 0.00015174]
	Learning Rate: 0.000151742
	LOSS [training: -0.007317366754773874 | validation: -0.005745792846704087]
	TIME [epoch: 8.84 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0098083254283811		[learning rate: 0.00015156]
		[batch 20/20] avg loss: -0.008764004137575141		[learning rate: 0.00015137]
	Learning Rate: 0.000151374
	LOSS [training: -0.00928616478297812 | validation: -0.008359079082065674]
	TIME [epoch: 8.85 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009932766140094803		[learning rate: 0.00015119]
		[batch 20/20] avg loss: -0.006158756891814637		[learning rate: 0.00015101]
	Learning Rate: 0.000151008
	LOSS [training: -0.008045761515954722 | validation: -0.014431267006496492]
	TIME [epoch: 8.86 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010531906537435288		[learning rate: 0.00015083]
		[batch 20/20] avg loss: -0.005672113894700905		[learning rate: 0.00015064]
	Learning Rate: 0.000150642
	LOSS [training: -0.008102010216068097 | validation: -0.01124808057593497]
	TIME [epoch: 8.87 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010476882551482703		[learning rate: 0.00015046]
		[batch 20/20] avg loss: -0.007331764362526102		[learning rate: 0.00015028]
	Learning Rate: 0.000150278
	LOSS [training: -0.008904323457004402 | validation: -0.01662661347520509]
	TIME [epoch: 8.86 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008772314660395228		[learning rate: 0.0001501]
		[batch 20/20] avg loss: -0.002672900017524451		[learning rate: 0.00014991]
	Learning Rate: 0.000149914
	LOSS [training: -0.005722607338959839 | validation: -0.0050315914492582835]
	TIME [epoch: 8.85 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0056742774790497314		[learning rate: 0.00014973]
		[batch 20/20] avg loss: -0.0025141104984630503		[learning rate: 0.00014955]
	Learning Rate: 0.000149551
	LOSS [training: -0.004094193988756391 | validation: -0.00979399889202571]
	TIME [epoch: 8.85 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00947334720694529		[learning rate: 0.00014937]
		[batch 20/20] avg loss: -0.007525260352125115		[learning rate: 0.00014919]
	Learning Rate: 0.000149189
	LOSS [training: -0.008499303779535203 | validation: -0.009527503831758238]
	TIME [epoch: 8.86 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0110765792326436		[learning rate: 0.00014901]
		[batch 20/20] avg loss: -0.007275395042972683		[learning rate: 0.00014883]
	Learning Rate: 0.000148828
	LOSS [training: -0.009175987137808145 | validation: -0.013013189215820696]
	TIME [epoch: 8.86 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013362758236216752		[learning rate: 0.00014865]
		[batch 20/20] avg loss: -0.006270989113008424		[learning rate: 0.00014847]
	Learning Rate: 0.000148468
	LOSS [training: -0.009816873674612588 | validation: -0.012667011097863546]
	TIME [epoch: 8.86 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010507639020517904		[learning rate: 0.00014829]
		[batch 20/20] avg loss: -0.015366667810927898		[learning rate: 0.00014811]
	Learning Rate: 0.000148108
	LOSS [training: -0.012937153415722899 | validation: -0.01485324311686204]
	TIME [epoch: 8.85 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00991797896920695		[learning rate: 0.00014793]
		[batch 20/20] avg loss: -0.012331569204998599		[learning rate: 0.00014775]
	Learning Rate: 0.00014775
	LOSS [training: -0.011124774087102777 | validation: -0.01132751579476886]
	TIME [epoch: 8.86 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008786679503934013		[learning rate: 0.00014757]
		[batch 20/20] avg loss: -0.010167604325690235		[learning rate: 0.00014739]
	Learning Rate: 0.000147392
	LOSS [training: -0.009477141914812127 | validation: -0.009796835626076595]
	TIME [epoch: 8.87 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009447031447031243		[learning rate: 0.00014721]
		[batch 20/20] avg loss: -0.011492828449639302		[learning rate: 0.00014704]
	Learning Rate: 0.000147035
	LOSS [training: -0.01046992994833527 | validation: -0.009452769241762042]
	TIME [epoch: 8.85 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01128656530280642		[learning rate: 0.00014686]
		[batch 20/20] avg loss: -0.01205369387649633		[learning rate: 0.00014668]
	Learning Rate: 0.000146679
	LOSS [training: -0.01167012958965138 | validation: -0.0029630736783831527]
	TIME [epoch: 8.85 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013627551337550859		[learning rate: 0.0001465]
		[batch 20/20] avg loss: -0.011277820310993695		[learning rate: 0.00014632]
	Learning Rate: 0.000146324
	LOSS [training: -0.012452685824272277 | validation: -0.009842177408127338]
	TIME [epoch: 8.85 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007247410378106637		[learning rate: 0.00014615]
		[batch 20/20] avg loss: -0.010171555944760848		[learning rate: 0.00014597]
	Learning Rate: 0.00014597
	LOSS [training: -0.008709483161433742 | validation: -0.01715823890249689]
	TIME [epoch: 8.86 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010207762526391464		[learning rate: 0.00014579]
		[batch 20/20] avg loss: -0.012492327908484717		[learning rate: 0.00014562]
	Learning Rate: 0.000145616
	LOSS [training: -0.011350045217438091 | validation: -0.01570122196458233]
	TIME [epoch: 8.89 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017817423536682156		[learning rate: 0.00014544]
		[batch 20/20] avg loss: -0.008446281933173054		[learning rate: 0.00014526]
	Learning Rate: 0.000145264
	LOSS [training: -0.005114012143420635 | validation: -0.009941410767794867]
	TIME [epoch: 8.85 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009517580508580176		[learning rate: 0.00014509]
		[batch 20/20] avg loss: -0.00332349872740412		[learning rate: 0.00014491]
	Learning Rate: 0.000144912
	LOSS [training: -0.006420539617992148 | validation: -0.014976371499999582]
	TIME [epoch: 8.86 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011454320794224794		[learning rate: 0.00014474]
		[batch 20/20] avg loss: -0.004771457827127995		[learning rate: 0.00014456]
	Learning Rate: 0.000144561
	LOSS [training: -0.008112889310676396 | validation: -0.016547068772875047]
	TIME [epoch: 8.85 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006572283463763646		[learning rate: 0.00014439]
		[batch 20/20] avg loss: -0.0076562087197151		[learning rate: 0.00014421]
	Learning Rate: 0.000144212
	LOSS [training: -0.007114246091739373 | validation: -0.011701531697290168]
	TIME [epoch: 8.87 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008216144796069634		[learning rate: 0.00014404]
		[batch 20/20] avg loss: -0.011405669620716017		[learning rate: 0.00014386]
	Learning Rate: 0.000143862
	LOSS [training: -0.009810907208392828 | validation: -0.019128644095504264]
	TIME [epoch: 8.86 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009211593086963687		[learning rate: 0.00014369]
		[batch 20/20] avg loss: -0.005973981323247729		[learning rate: 0.00014351]
	Learning Rate: 0.000143514
	LOSS [training: -0.007592787205105707 | validation: -0.010699393240597]
	TIME [epoch: 8.84 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00941031736052313		[learning rate: 0.00014334]
		[batch 20/20] avg loss: -0.00590935693888636		[learning rate: 0.00014317]
	Learning Rate: 0.000143167
	LOSS [training: -0.007659837149704746 | validation: -0.016704638193966947]
	TIME [epoch: 8.85 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006892041915290255		[learning rate: 0.00014299]
		[batch 20/20] avg loss: -0.004069117108311022		[learning rate: 0.00014282]
	Learning Rate: 0.00014282
	LOSS [training: -0.0054805795118006385 | validation: -0.0150433546353821]
	TIME [epoch: 8.85 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006944652370838892		[learning rate: 0.00014265]
		[batch 20/20] avg loss: -0.010437909157604728		[learning rate: 0.00014247]
	Learning Rate: 0.000142474
	LOSS [training: -0.00869128076422181 | validation: -0.015459592454723559]
	TIME [epoch: 8.87 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007541165332777674		[learning rate: 0.0001423]
		[batch 20/20] avg loss: -0.012386808331685671		[learning rate: 0.00014213]
	Learning Rate: 0.000142129
	LOSS [training: -0.009963986832231672 | validation: -0.012491545174241305]
	TIME [epoch: 8.85 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011553738731275456		[learning rate: 0.00014196]
		[batch 20/20] avg loss: -0.009461063103317437		[learning rate: 0.00014179]
	Learning Rate: 0.000141785
	LOSS [training: -0.010507400917296447 | validation: -0.014088041146586131]
	TIME [epoch: 8.86 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007224134263661926		[learning rate: 0.00014161]
		[batch 20/20] avg loss: -0.011778758845962972		[learning rate: 0.00014144]
	Learning Rate: 0.000141442
	LOSS [training: -0.00950144655481245 | validation: -0.013418667024791218]
	TIME [epoch: 8.85 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006654700773124894		[learning rate: 0.00014127]
		[batch 20/20] avg loss: -0.012654669166384242		[learning rate: 0.0001411]
	Learning Rate: 0.0001411
	LOSS [training: -0.00965468496975457 | validation: -0.013720328153003437]
	TIME [epoch: 8.86 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009049165250865864		[learning rate: 0.00014093]
		[batch 20/20] avg loss: -0.00932899073386553		[learning rate: 0.00014076]
	Learning Rate: 0.000140758
	LOSS [training: -0.009189077992365697 | validation: -0.012399142596937347]
	TIME [epoch: 8.87 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00821478777139221		[learning rate: 0.00014059]
		[batch 20/20] avg loss: -0.010402814203826806		[learning rate: 0.00014042]
	Learning Rate: 0.000140417
	LOSS [training: -0.009308800987609509 | validation: -0.01876319058213433]
	TIME [epoch: 8.85 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009767856517877595		[learning rate: 0.00014025]
		[batch 20/20] avg loss: -0.011577549611652872		[learning rate: 0.00014008]
	Learning Rate: 0.000140078
	LOSS [training: -0.010672703064765232 | validation: -0.011779178074152019]
	TIME [epoch: 8.86 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002749539240005289		[learning rate: 0.00013991]
		[batch 20/20] avg loss: -0.011042837257338917		[learning rate: 0.00013974]
	Learning Rate: 0.000139738
	LOSS [training: -0.006896188248672104 | validation: -0.006364807255624255]
	TIME [epoch: 8.84 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011067041081282354		[learning rate: 0.00013957]
		[batch 20/20] avg loss: -0.0077770966949302816		[learning rate: 0.0001394]
	Learning Rate: 0.0001394
	LOSS [training: -0.009422068888106318 | validation: -0.011216063612887642]
	TIME [epoch: 8.88 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011040322687403185		[learning rate: 0.00013923]
		[batch 20/20] avg loss: -0.01187004641051445		[learning rate: 0.00013906]
	Learning Rate: 0.000139063
	LOSS [training: -0.01145518454895882 | validation: -0.014439350590428533]
	TIME [epoch: 8.87 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011714142983378008		[learning rate: 0.00013889]
		[batch 20/20] avg loss: -0.006288383037811982		[learning rate: 0.00013873]
	Learning Rate: 0.000138726
	LOSS [training: -0.009001263010594996 | validation: -0.010308997203822544]
	TIME [epoch: 8.86 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.018402266891475526		[learning rate: 0.00013856]
		[batch 20/20] avg loss: -0.006790276402603809		[learning rate: 0.00013839]
	Learning Rate: 0.00013839
	LOSS [training: -0.012596271647039664 | validation: -0.01734878453660443]
	TIME [epoch: 8.86 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011036739357344252		[learning rate: 0.00013822]
		[batch 20/20] avg loss: -0.006614907200823033		[learning rate: 0.00013806]
	Learning Rate: 0.000138055
	LOSS [training: -0.008825823279083645 | validation: -0.013415814541634197]
	TIME [epoch: 8.85 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006439935958342348		[learning rate: 0.00013789]
		[batch 20/20] avg loss: -0.012787435168028874		[learning rate: 0.00013772]
	Learning Rate: 0.000137721
	LOSS [training: -0.009613685563185611 | validation: -0.01747243527795944]
	TIME [epoch: 8.88 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01122568586300998		[learning rate: 0.00013755]
		[batch 20/20] avg loss: -0.009472821289189621		[learning rate: 0.00013739]
	Learning Rate: 0.000137388
	LOSS [training: -0.010349253576099804 | validation: -0.01320170607619708]
	TIME [epoch: 8.85 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011188872937625818		[learning rate: 0.00013722]
		[batch 20/20] avg loss: -0.0132466401662023		[learning rate: 0.00013705]
	Learning Rate: 0.000137055
	LOSS [training: -0.01221775655191406 | validation: -0.018992535245420127]
	TIME [epoch: 8.86 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012393836454630325		[learning rate: 0.00013689]
		[batch 20/20] avg loss: -0.008968991265224808		[learning rate: 0.00013672]
	Learning Rate: 0.000136723
	LOSS [training: -0.010681413859927567 | validation: -0.01679680928354873]
	TIME [epoch: 8.86 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011896298268804908		[learning rate: 0.00013656]
		[batch 20/20] avg loss: -0.011798813427090698		[learning rate: 0.00013639]
	Learning Rate: 0.000136392
	LOSS [training: -0.011847555847947803 | validation: -0.017231859877167195]
	TIME [epoch: 8.87 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010836833432905466		[learning rate: 0.00013623]
		[batch 20/20] avg loss: -0.007268257213061657		[learning rate: 0.00013606]
	Learning Rate: 0.000136062
	LOSS [training: -0.00905254532298356 | validation: -0.010615586182247179]
	TIME [epoch: 8.87 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010608526868868538		[learning rate: 0.0001359]
		[batch 20/20] avg loss: -0.007864497009271628		[learning rate: 0.00013573]
	Learning Rate: 0.000135733
	LOSS [training: -0.009236511939070085 | validation: -0.016120192684317705]
	TIME [epoch: 8.86 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008251007836523359		[learning rate: 0.00013557]
		[batch 20/20] avg loss: -0.0076049150701773885		[learning rate: 0.0001354]
	Learning Rate: 0.000135404
	LOSS [training: -0.007927961453350375 | validation: -0.008643086538157709]
	TIME [epoch: 8.85 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007642650470024759		[learning rate: 0.00013524]
		[batch 20/20] avg loss: -0.011252199183891958		[learning rate: 0.00013508]
	Learning Rate: 0.000135076
	LOSS [training: -0.00944742482695836 | validation: -0.01220075477493238]
	TIME [epoch: 8.86 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0126642933577454		[learning rate: 0.00013491]
		[batch 20/20] avg loss: -0.00790223914189079		[learning rate: 0.00013475]
	Learning Rate: 0.000134749
	LOSS [training: -0.010283266249818093 | validation: -0.009087549428881701]
	TIME [epoch: 8.87 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009049041511227493		[learning rate: 0.00013459]
		[batch 20/20] avg loss: -0.010618010784729017		[learning rate: 0.00013442]
	Learning Rate: 0.000134423
	LOSS [training: -0.009833526147978253 | validation: -0.014854765473317108]
	TIME [epoch: 8.88 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009141778730579999		[learning rate: 0.00013426]
		[batch 20/20] avg loss: -0.012858200105896259		[learning rate: 0.0001341]
	Learning Rate: 0.000134098
	LOSS [training: -0.010999989418238132 | validation: -0.007860160426299044]
	TIME [epoch: 8.85 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010247472553089528		[learning rate: 0.00013394]
		[batch 20/20] avg loss: -0.008362479673544422		[learning rate: 0.00013377]
	Learning Rate: 0.000133773
	LOSS [training: -0.009304976113316975 | validation: -0.009932464372549937]
	TIME [epoch: 8.86 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014114611693241996		[learning rate: 0.00013361]
		[batch 20/20] avg loss: -0.004573208576783266		[learning rate: 0.00013345]
	Learning Rate: 0.000133449
	LOSS [training: -0.009343910135012632 | validation: -0.015524861856078861]
	TIME [epoch: 8.85 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00619917075874827		[learning rate: 0.00013329]
		[batch 20/20] avg loss: -0.010996922898077733		[learning rate: 0.00013313]
	Learning Rate: 0.000133126
	LOSS [training: -0.008598046828413 | validation: -0.013668835146799983]
	TIME [epoch: 8.87 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007998691591438964		[learning rate: 0.00013296]
		[batch 20/20] avg loss: -0.016740193151315367		[learning rate: 0.0001328]
	Learning Rate: 0.000132804
	LOSS [training: -0.012369442371377166 | validation: -0.014569768673613507]
	TIME [epoch: 8.86 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009966284292770376		[learning rate: 0.00013264]
		[batch 20/20] avg loss: -0.010436336484072686		[learning rate: 0.00013248]
	Learning Rate: 0.000132482
	LOSS [training: -0.010201310388421531 | validation: -0.00959843034245629]
	TIME [epoch: 8.86 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00801505454121131		[learning rate: 0.00013232]
		[batch 20/20] avg loss: -0.011002627691901521		[learning rate: 0.00013216]
	Learning Rate: 0.000132162
	LOSS [training: -0.009508841116556416 | validation: -0.012846775687198162]
	TIME [epoch: 8.87 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011148071539237688		[learning rate: 0.000132]
		[batch 20/20] avg loss: -0.008435130719517719		[learning rate: 0.00013184]
	Learning Rate: 0.000131842
	LOSS [training: -0.0097916011293777 | validation: -0.014762232829718178]
	TIME [epoch: 8.87 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.016370965308128118		[learning rate: 0.00013168]
		[batch 20/20] avg loss: -0.008580903971223994		[learning rate: 0.00013152]
	Learning Rate: 0.000131522
	LOSS [training: -0.012475934639676053 | validation: -0.012150879975813525]
	TIME [epoch: 8.89 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01468394649051337		[learning rate: 0.00013136]
		[batch 20/20] avg loss: -0.008954627614170175		[learning rate: 0.0001312]
	Learning Rate: 0.000131204
	LOSS [training: -0.011819287052341773 | validation: -0.01349945877167345]
	TIME [epoch: 8.85 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010047168458797714		[learning rate: 0.00013105]
		[batch 20/20] avg loss: -0.0076458850053128715		[learning rate: 0.00013089]
	Learning Rate: 0.000130886
	LOSS [training: -0.008846526732055294 | validation: -0.015100668377042943]
	TIME [epoch: 8.85 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010253228060929293		[learning rate: 0.00013073]
		[batch 20/20] avg loss: -0.015179483555591677		[learning rate: 0.00013057]
	Learning Rate: 0.00013057
	LOSS [training: -0.012716355808260481 | validation: -0.014138075086880903]
	TIME [epoch: 8.85 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012351360872834482		[learning rate: 0.00013041]
		[batch 20/20] avg loss: -0.013347938826551864		[learning rate: 0.00013025]
	Learning Rate: 0.000130254
	LOSS [training: -0.012849649849693173 | validation: -0.011725944671902067]
	TIME [epoch: 8.85 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011170056155563468		[learning rate: 0.0001301]
		[batch 20/20] avg loss: -0.005547702050820372		[learning rate: 0.00012994]
	Learning Rate: 0.000129938
	LOSS [training: -0.008358879103191918 | validation: -0.015432101908264086]
	TIME [epoch: 8.88 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00777595498799437		[learning rate: 0.00012978]
		[batch 20/20] avg loss: -0.01065295867086959		[learning rate: 0.00012962]
	Learning Rate: 0.000129624
	LOSS [training: -0.009214456829431979 | validation: -0.008848209465251024]
	TIME [epoch: 8.85 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00967336207391634		[learning rate: 0.00012947]
		[batch 20/20] avg loss: -0.010741940450357614		[learning rate: 0.00012931]
	Learning Rate: 0.00012931
	LOSS [training: -0.010207651262136973 | validation: -0.01510446316494338]
	TIME [epoch: 8.86 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009244343490177356		[learning rate: 0.00012915]
		[batch 20/20] avg loss: -0.009348560054926739		[learning rate: 0.000129]
	Learning Rate: 0.000128997
	LOSS [training: -0.009296451772552047 | validation: -0.004914143340638725]
	TIME [epoch: 8.85 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00448490215105975		[learning rate: 0.00012884]
		[batch 20/20] avg loss: -0.008865747691622166		[learning rate: 0.00012868]
	Learning Rate: 0.000128685
	LOSS [training: -0.006675324921340958 | validation: -0.015551473801467215]
	TIME [epoch: 8.86 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011894853410298504		[learning rate: 0.00012853]
		[batch 20/20] avg loss: -0.008248637163849337		[learning rate: 0.00012837]
	Learning Rate: 0.000128373
	LOSS [training: -0.01007174528707392 | validation: -0.016238622499505535]
	TIME [epoch: 8.86 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007097343015377941		[learning rate: 0.00012822]
		[batch 20/20] avg loss: -0.012977136242046916		[learning rate: 0.00012806]
	Learning Rate: 0.000128062
	LOSS [training: -0.010037239628712427 | validation: -0.005331991457038656]
	TIME [epoch: 8.85 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0075528027230053005		[learning rate: 0.00012791]
		[batch 20/20] avg loss: -0.012958074255481464		[learning rate: 0.00012775]
	Learning Rate: 0.000127752
	LOSS [training: -0.01025543848924338 | validation: -0.009678844111283596]
	TIME [epoch: 8.87 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012522899702431308		[learning rate: 0.0001276]
		[batch 20/20] avg loss: -0.008744922271234638		[learning rate: 0.00012744]
	Learning Rate: 0.000127443
	LOSS [training: -0.010633910986832973 | validation: -0.017494448422172487]
	TIME [epoch: 8.86 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011633330133837709		[learning rate: 0.00012729]
		[batch 20/20] avg loss: -0.013167162613563049		[learning rate: 0.00012713]
	Learning Rate: 0.000127134
	LOSS [training: -0.012400246373700378 | validation: -0.019504707186674238]
	TIME [epoch: 8.88 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010751534535629933		[learning rate: 0.00012698]
		[batch 20/20] avg loss: -0.012713490871471153		[learning rate: 0.00012683]
	Learning Rate: 0.000126827
	LOSS [training: -0.011732512703550543 | validation: -0.018223514338640157]
	TIME [epoch: 8.86 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010707944561807797		[learning rate: 0.00012667]
		[batch 20/20] avg loss: -0.008986195438014018		[learning rate: 0.00012652]
	Learning Rate: 0.00012652
	LOSS [training: -0.009847069999910908 | validation: -0.01277005325102287]
	TIME [epoch: 8.85 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00864463588375949		[learning rate: 0.00012637]
		[batch 20/20] avg loss: -0.005640572041744828		[learning rate: 0.00012621]
	Learning Rate: 0.000126213
	LOSS [training: -0.0071426039627521575 | validation: -0.008986619812091]
	TIME [epoch: 8.86 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00984440659873547		[learning rate: 0.00012606]
		[batch 20/20] avg loss: -0.009623061795136454		[learning rate: 0.00012591]
	Learning Rate: 0.000125908
	LOSS [training: -0.00973373419693596 | validation: -0.01050059427260531]
	TIME [epoch: 8.85 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00953294519669083		[learning rate: 0.00012576]
		[batch 20/20] avg loss: -0.01333811215080108		[learning rate: 0.0001256]
	Learning Rate: 0.000125603
	LOSS [training: -0.011435528673745955 | validation: -0.016194579451972356]
	TIME [epoch: 8.88 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010737117405335075		[learning rate: 0.00012545]
		[batch 20/20] avg loss: -0.007390443684923925		[learning rate: 0.0001253]
	Learning Rate: 0.000125299
	LOSS [training: -0.009063780545129501 | validation: -0.01461775474552167]
	TIME [epoch: 8.85 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007270447895092752		[learning rate: 0.00012515]
		[batch 20/20] avg loss: -0.013637810397072288		[learning rate: 0.000125]
	Learning Rate: 0.000124996
	LOSS [training: -0.01045412914608252 | validation: -0.01174105822046032]
	TIME [epoch: 8.86 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011667105911695043		[learning rate: 0.00012484]
		[batch 20/20] avg loss: -0.011041765762781475		[learning rate: 0.00012469]
	Learning Rate: 0.000124693
	LOSS [training: -0.011354435837238257 | validation: -0.013315563345931095]
	TIME [epoch: 8.84 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009574210693070618		[learning rate: 0.00012454]
		[batch 20/20] avg loss: -0.01444203216262788		[learning rate: 0.00012439]
	Learning Rate: 0.000124391
	LOSS [training: -0.01200812142784925 | validation: -0.011837806659585452]
	TIME [epoch: 8.86 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01100985545862548		[learning rate: 0.00012424]
		[batch 20/20] avg loss: -0.007982424036315413		[learning rate: 0.00012409]
	Learning Rate: 0.00012409
	LOSS [training: -0.009496139747470447 | validation: -0.012563720920185201]
	TIME [epoch: 8.86 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014415150656630488		[learning rate: 0.00012394]
		[batch 20/20] avg loss: -0.010570809446188921		[learning rate: 0.00012379]
	Learning Rate: 0.00012379
	LOSS [training: -0.012492980051409702 | validation: -0.013334017091242583]
	TIME [epoch: 8.87 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008367078367912366		[learning rate: 0.00012364]
		[batch 20/20] avg loss: -0.01100482245777905		[learning rate: 0.00012349]
	Learning Rate: 0.00012349
	LOSS [training: -0.009685950412845708 | validation: -0.015279762741924087]
	TIME [epoch: 8.86 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009914394270977185		[learning rate: 0.00012334]
		[batch 20/20] avg loss: -0.012941069367304608		[learning rate: 0.00012319]
	Learning Rate: 0.000123191
	LOSS [training: -0.011427731819140894 | validation: -0.009063893909541935]
	TIME [epoch: 8.86 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011761844089405612		[learning rate: 0.00012304]
		[batch 20/20] avg loss: -0.007387167409439496		[learning rate: 0.00012289]
	Learning Rate: 0.000122893
	LOSS [training: -0.009574505749422554 | validation: -0.012108499655000783]
	TIME [epoch: 8.87 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00961267620443608		[learning rate: 0.00012274]
		[batch 20/20] avg loss: -0.014519442806310156		[learning rate: 0.0001226]
	Learning Rate: 0.000122595
	LOSS [training: -0.012066059505373119 | validation: -0.013787072204042048]
	TIME [epoch: 8.86 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01339960427990577		[learning rate: 0.00012245]
		[batch 20/20] avg loss: -0.008398518710369346		[learning rate: 0.0001223]
	Learning Rate: 0.000122298
	LOSS [training: -0.01089906149513756 | validation: -0.012833749099215454]
	TIME [epoch: 8.81 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009642282467129275		[learning rate: 0.00012215]
		[batch 20/20] avg loss: -0.012075203641570931		[learning rate: 0.000122]
	Learning Rate: 0.000122002
	LOSS [training: -0.010858743054350101 | validation: -0.014097701500728858]
	TIME [epoch: 8.86 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013095293592636847		[learning rate: 0.00012185]
		[batch 20/20] avg loss: -0.00833300644598794		[learning rate: 0.00012171]
	Learning Rate: 0.000121707
	LOSS [training: -0.010714150019312394 | validation: -0.016932393111642786]
	TIME [epoch: 8.85 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010623207040707745		[learning rate: 0.00012156]
		[batch 20/20] avg loss: -0.010473795838246161		[learning rate: 0.00012141]
	Learning Rate: 0.000121412
	LOSS [training: -0.010548501439476953 | validation: -0.010522705402697539]
	TIME [epoch: 8.87 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006599351476279135		[learning rate: 0.00012127]
		[batch 20/20] avg loss: -0.011256459962491173		[learning rate: 0.00012112]
	Learning Rate: 0.000121119
	LOSS [training: -0.008927905719385155 | validation: -0.012365672826474963]
	TIME [epoch: 8.87 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01458826839970167		[learning rate: 0.00012097]
		[batch 20/20] avg loss: -0.007918991378644978		[learning rate: 0.00012083]
	Learning Rate: 0.000120825
	LOSS [training: -0.011253629889173323 | validation: -0.013698569907653549]
	TIME [epoch: 8.86 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012657396292887726		[learning rate: 0.00012068]
		[batch 20/20] avg loss: -0.007014472878596788		[learning rate: 0.00012053]
	Learning Rate: 0.000120533
	LOSS [training: -0.009835934585742256 | validation: -0.00878697745666065]
	TIME [epoch: 8.86 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008097710175942591		[learning rate: 0.00012039]
		[batch 20/20] avg loss: -0.0077916346907121155		[learning rate: 0.00012024]
	Learning Rate: 0.000120241
	LOSS [training: -0.007944672433327355 | validation: -0.012099253860305338]
	TIME [epoch: 8.87 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009174965045371313		[learning rate: 0.0001201]
		[batch 20/20] avg loss: -0.009200290865682189		[learning rate: 0.00011995]
	Learning Rate: 0.00011995
	LOSS [training: -0.00918762795552675 | validation: -0.01719456769489564]
	TIME [epoch: 8.9 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008320169577857583		[learning rate: 0.0001198]
		[batch 20/20] avg loss: -0.0076948624825291315		[learning rate: 0.00011966]
	Learning Rate: 0.00011966
	LOSS [training: -0.00800751603019336 | validation: -0.01616567055183927]
	TIME [epoch: 8.86 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00842568187170314		[learning rate: 0.00011951]
		[batch 20/20] avg loss: -0.010659411117638193		[learning rate: 0.00011937]
	Learning Rate: 0.00011937
	LOSS [training: -0.009542546494670666 | validation: -0.01389241006683467]
	TIME [epoch: 8.85 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013932689107128207		[learning rate: 0.00011923]
		[batch 20/20] avg loss: -0.01183764461209213		[learning rate: 0.00011908]
	Learning Rate: 0.000119081
	LOSS [training: -0.012885166859610168 | validation: -0.015045632576557772]
	TIME [epoch: 8.86 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005253668302747929		[learning rate: 0.00011894]
		[batch 20/20] avg loss: -0.014514573306317613		[learning rate: 0.00011879]
	Learning Rate: 0.000118793
	LOSS [training: -0.00988412080453277 | validation: -0.01269304179262836]
	TIME [epoch: 8.88 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01274949965101681		[learning rate: 0.00011865]
		[batch 20/20] avg loss: -0.007978254012902154		[learning rate: 0.00011851]
	Learning Rate: 0.000118505
	LOSS [training: -0.010363876831959485 | validation: -0.01366727028691501]
	TIME [epoch: 8.85 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008889041232549518		[learning rate: 0.00011836]
		[batch 20/20] avg loss: -0.013341600285457517		[learning rate: 0.00011822]
	Learning Rate: 0.000118218
	LOSS [training: -0.011115320759003518 | validation: -0.014985006763103121]
	TIME [epoch: 8.85 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007787199542410228		[learning rate: 0.00011808]
		[batch 20/20] avg loss: -0.009827784614851107		[learning rate: 0.00011793]
	Learning Rate: 0.000117932
	LOSS [training: -0.008807492078630668 | validation: -0.010131971713234093]
	TIME [epoch: 8.86 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011559511428217784		[learning rate: 0.00011779]
		[batch 20/20] avg loss: -0.008362845670636701		[learning rate: 0.00011765]
	Learning Rate: 0.000117646
	LOSS [training: -0.00996117854942724 | validation: -0.01715919723372753]
	TIME [epoch: 8.85 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010695312662468526		[learning rate: 0.0001175]
		[batch 20/20] avg loss: -0.009127519500478524		[learning rate: 0.00011736]
	Learning Rate: 0.000117362
	LOSS [training: -0.009911416081473525 | validation: -0.01260931815129997]
	TIME [epoch: 8.87 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01148335786288207		[learning rate: 0.00011722]
		[batch 20/20] avg loss: -0.014499495372090661		[learning rate: 0.00011708]
	Learning Rate: 0.000117078
	LOSS [training: -0.012991426617486364 | validation: -0.012076380191646814]
	TIME [epoch: 8.86 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014828294289080012		[learning rate: 0.00011694]
		[batch 20/20] avg loss: -0.01147997751559866		[learning rate: 0.00011679]
	Learning Rate: 0.000116794
	LOSS [training: -0.013154135902339336 | validation: -0.0160467035442425]
	TIME [epoch: 8.86 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011043800958126456		[learning rate: 0.00011665]
		[batch 20/20] avg loss: -0.008383156036541443		[learning rate: 0.00011651]
	Learning Rate: 0.000116511
	LOSS [training: -0.00971347849733395 | validation: -0.0076508354999781855]
	TIME [epoch: 8.86 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013564830481322166		[learning rate: 0.00011637]
		[batch 20/20] avg loss: -0.009235988081745135		[learning rate: 0.00011623]
	Learning Rate: 0.000116229
	LOSS [training: -0.011400409281533649 | validation: -0.01054482486162584]
	TIME [epoch: 8.86 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012155661382690672		[learning rate: 0.00011609]
		[batch 20/20] avg loss: -0.015208777861292566		[learning rate: 0.00011595]
	Learning Rate: 0.000115948
	LOSS [training: -0.013682219621991615 | validation: -0.011471784938993073]
	TIME [epoch: 8.86 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011172724001281097		[learning rate: 0.00011581]
		[batch 20/20] avg loss: -0.00873876582958265		[learning rate: 0.00011567]
	Learning Rate: 0.000115667
	LOSS [training: -0.009955744915431873 | validation: -0.01409807694581181]
	TIME [epoch: 8.86 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010786133671663117		[learning rate: 0.00011553]
		[batch 20/20] avg loss: -0.009616918824176661		[learning rate: 0.00011539]
	Learning Rate: 0.000115387
	LOSS [training: -0.010201526247919888 | validation: -0.012625725890329065]
	TIME [epoch: 8.85 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009336127990197314		[learning rate: 0.00011525]
		[batch 20/20] avg loss: -0.009776394418856986		[learning rate: 0.00011511]
	Learning Rate: 0.000115108
	LOSS [training: -0.00955626120452715 | validation: -0.01594100515784439]
	TIME [epoch: 8.86 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009374402403889493		[learning rate: 0.00011497]
		[batch 20/20] avg loss: -0.010094149484333143		[learning rate: 0.00011483]
	Learning Rate: 0.000114829
	LOSS [training: -0.009734275944111317 | validation: -0.01410109694081068]
	TIME [epoch: 8.85 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010528539318597346		[learning rate: 0.00011469]
		[batch 20/20] avg loss: -0.007176679221218662		[learning rate: 0.00011455]
	Learning Rate: 0.000114551
	LOSS [training: -0.008852609269908007 | validation: -0.008481746124198379]
	TIME [epoch: 8.87 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008376357005373342		[learning rate: 0.00011441]
		[batch 20/20] avg loss: -0.008320800713339103		[learning rate: 0.00011427]
	Learning Rate: 0.000114274
	LOSS [training: -0.008348578859356224 | validation: -0.005685112405199303]
	TIME [epoch: 8.86 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012998305697519776		[learning rate: 0.00011414]
		[batch 20/20] avg loss: -0.0127098587606068		[learning rate: 0.000114]
	Learning Rate: 0.000113997
	LOSS [training: -0.012854082229063288 | validation: -0.015999279071882963]
	TIME [epoch: 8.86 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010657803284546326		[learning rate: 0.00011386]
		[batch 20/20] avg loss: -0.011511846885417959		[learning rate: 0.00011372]
	Learning Rate: 0.000113721
	LOSS [training: -0.011084825084982144 | validation: -0.008094970838655275]
	TIME [epoch: 8.86 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010608358005062943		[learning rate: 0.00011358]
		[batch 20/20] avg loss: -0.010643309808574771		[learning rate: 0.00011345]
	Learning Rate: 0.000113446
	LOSS [training: -0.010625833906818859 | validation: -0.014590825094837678]
	TIME [epoch: 8.89 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012824070464606738		[learning rate: 0.00011331]
		[batch 20/20] avg loss: -0.008494782842423592		[learning rate: 0.00011317]
	Learning Rate: 0.000113171
	LOSS [training: -0.010659426653515165 | validation: -0.013281059176654095]
	TIME [epoch: 8.86 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007285353692108578		[learning rate: 0.00011303]
		[batch 20/20] avg loss: -0.015189038921564424		[learning rate: 0.0001129]
	Learning Rate: 0.000112897
	LOSS [training: -0.0112371963068365 | validation: -0.011967390766310104]
	TIME [epoch: 8.87 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007693504811897499		[learning rate: 0.00011276]
		[batch 20/20] avg loss: -0.010402062031796212		[learning rate: 0.00011262]
	Learning Rate: 0.000112624
	LOSS [training: -0.009047783421846855 | validation: -0.012321315507599292]
	TIME [epoch: 8.85 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009453656473561162		[learning rate: 0.00011249]
		[batch 20/20] avg loss: -0.009119053794625253		[learning rate: 0.00011235]
	Learning Rate: 0.000112352
	LOSS [training: -0.009286355134093207 | validation: -0.011082248503092028]
	TIME [epoch: 8.84 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013018119551819834		[learning rate: 0.00011222]
		[batch 20/20] avg loss: -0.007364572929687614		[learning rate: 0.00011208]
	Learning Rate: 0.00011208
	LOSS [training: -0.010191346240753721 | validation: -0.017520184124950676]
	TIME [epoch: 8.86 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009146007601571946		[learning rate: 0.00011194]
		[batch 20/20] avg loss: -0.00906377426826111		[learning rate: 0.00011181]
	Learning Rate: 0.000111808
	LOSS [training: -0.009104890934916527 | validation: -0.01369667540093355]
	TIME [epoch: 8.87 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009283426721431218		[learning rate: 0.00011167]
		[batch 20/20] avg loss: -0.011486585950383132		[learning rate: 0.00011154]
	Learning Rate: 0.000111538
	LOSS [training: -0.010385006335907176 | validation: -0.01530569716857984]
	TIME [epoch: 8.85 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013901784446839862		[learning rate: 0.0001114]
		[batch 20/20] avg loss: -0.008211459871692115		[learning rate: 0.00011127]
	Learning Rate: 0.000111268
	LOSS [training: -0.011056622159265988 | validation: -0.010523640245618391]
	TIME [epoch: 8.85 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008945340055752468		[learning rate: 0.00011113]
		[batch 20/20] avg loss: -0.013917551685864707		[learning rate: 0.000111]
	Learning Rate: 0.000110998
	LOSS [training: -0.011431445870808587 | validation: -0.01634849257877668]
	TIME [epoch: 8.87 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010971791826740544		[learning rate: 0.00011086]
		[batch 20/20] avg loss: -0.004889157573826038		[learning rate: 0.00011073]
	Learning Rate: 0.000110729
	LOSS [training: -0.007930474700283293 | validation: -0.015588681650903023]
	TIME [epoch: 8.87 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010901820642924206		[learning rate: 0.0001106]
		[batch 20/20] avg loss: -0.010071670436260659		[learning rate: 0.00011046]
	Learning Rate: 0.000110461
	LOSS [training: -0.010486745539592434 | validation: -0.012577386335649568]
	TIME [epoch: 8.85 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0069246275599756085		[learning rate: 0.00011033]
		[batch 20/20] avg loss: -0.007467195987916291		[learning rate: 0.00011019]
	Learning Rate: 0.000110194
	LOSS [training: -0.00719591177394595 | validation: -0.011406955023323298]
	TIME [epoch: 8.85 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007339433782383402		[learning rate: 0.00011006]
		[batch 20/20] avg loss: -0.012299883721768165		[learning rate: 0.00010993]
	Learning Rate: 0.000109927
	LOSS [training: -0.009819658752075782 | validation: -0.009983299051215512]
	TIME [epoch: 8.86 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010720313222438957		[learning rate: 0.00010979]
		[batch 20/20] avg loss: -0.011146045783888935		[learning rate: 0.00010966]
	Learning Rate: 0.000109661
	LOSS [training: -0.010933179503163946 | validation: -0.015700999904451292]
	TIME [epoch: 8.87 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006492599108306761		[learning rate: 0.00010953]
		[batch 20/20] avg loss: -0.01194193613333355		[learning rate: 0.0001094]
	Learning Rate: 0.000109396
	LOSS [training: -0.009217267620820157 | validation: -0.01571281853294756]
	TIME [epoch: 8.85 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014096645544543948		[learning rate: 0.00010926]
		[batch 20/20] avg loss: -0.009597142591307593		[learning rate: 0.00010913]
	Learning Rate: 0.000109131
	LOSS [training: -0.01184689406792577 | validation: -0.015987825991219454]
	TIME [epoch: 8.86 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011016710711479836		[learning rate: 0.000109]
		[batch 20/20] avg loss: -0.006345110067387633		[learning rate: 0.00010887]
	Learning Rate: 0.000108867
	LOSS [training: -0.008680910389433734 | validation: -0.008955067216982884]
	TIME [epoch: 8.86 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012888210318670084		[learning rate: 0.00010873]
		[batch 20/20] avg loss: -0.00749421587749006		[learning rate: 0.0001086]
	Learning Rate: 0.000108603
	LOSS [training: -0.010191213098080073 | validation: -0.014406823819780562]
	TIME [epoch: 8.86 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011622065628838387		[learning rate: 0.00010847]
		[batch 20/20] avg loss: -0.015382976245964288		[learning rate: 0.00010834]
	Learning Rate: 0.00010834
	LOSS [training: -0.013502520937401338 | validation: -0.01464191155595988]
	TIME [epoch: 8.88 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011094044876676718		[learning rate: 0.00010821]
		[batch 20/20] avg loss: -0.01084650204259793		[learning rate: 0.00010808]
	Learning Rate: 0.000108078
	LOSS [training: -0.010970273459637324 | validation: -0.015207248675831387]
	TIME [epoch: 8.87 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013376556240948792		[learning rate: 0.00010795]
		[batch 20/20] avg loss: -0.005835323496580637		[learning rate: 0.00010782]
	Learning Rate: 0.000107816
	LOSS [training: -0.00960593986876471 | validation: -0.010937064510428529]
	TIME [epoch: 8.85 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009406069390320782		[learning rate: 0.00010769]
		[batch 20/20] avg loss: -0.012125909950348287		[learning rate: 0.00010756]
	Learning Rate: 0.000107555
	LOSS [training: -0.010765989670334535 | validation: -0.01916530042754855]
	TIME [epoch: 8.86 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010016685634787494		[learning rate: 0.00010742]
		[batch 20/20] avg loss: -0.012013584828290225		[learning rate: 0.00010729]
	Learning Rate: 0.000107295
	LOSS [training: -0.011015135231538862 | validation: -0.007375020315537867]
	TIME [epoch: 8.87 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013191506831434585		[learning rate: 0.00010716]
		[batch 20/20] avg loss: -0.012272767841591845		[learning rate: 0.00010704]
	Learning Rate: 0.000107035
	LOSS [training: -0.01273213733651322 | validation: -0.016578166248742563]
	TIME [epoch: 8.86 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004946181765465825		[learning rate: 0.00010691]
		[batch 20/20] avg loss: -0.01251933680474768		[learning rate: 0.00010678]
	Learning Rate: 0.000106776
	LOSS [training: -0.008732759285106754 | validation: -0.01600525666020317]
	TIME [epoch: 8.85 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0058204976717329705		[learning rate: 0.00010665]
		[batch 20/20] avg loss: -0.008160953574264738		[learning rate: 0.00010652]
	Learning Rate: 0.000106518
	LOSS [training: -0.0069907256229988545 | validation: -0.015687867348587174]
	TIME [epoch: 8.86 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010137870005260891		[learning rate: 0.00010639]
		[batch 20/20] avg loss: -0.011105419032803574		[learning rate: 0.00010626]
	Learning Rate: 0.00010626
	LOSS [training: -0.010621644519032232 | validation: -0.013012990827126873]
	TIME [epoch: 8.86 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007700618998209123		[learning rate: 0.00010613]
		[batch 20/20] avg loss: -0.010329992336865473		[learning rate: 0.000106]
	Learning Rate: 0.000106002
	LOSS [training: -0.0090153056675373 | validation: -0.010142088555414958]
	TIME [epoch: 8.86 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008736505555965599		[learning rate: 0.00010587]
		[batch 20/20] avg loss: -0.008204653568677693		[learning rate: 0.00010575]
	Learning Rate: 0.000105746
	LOSS [training: -0.008470579562321648 | validation: -0.011283195348440064]
	TIME [epoch: 8.86 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010157557940759674		[learning rate: 0.00010562]
		[batch 20/20] avg loss: -0.008718489232500395		[learning rate: 0.00010549]
	Learning Rate: 0.00010549
	LOSS [training: -0.009438023586630035 | validation: -0.011256586374962387]
	TIME [epoch: 8.86 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008045164218235173		[learning rate: 0.00010536]
		[batch 20/20] avg loss: -0.009231273904810898		[learning rate: 0.00010523]
	Learning Rate: 0.000105234
	LOSS [training: -0.008638219061523036 | validation: -0.013972841423651728]
	TIME [epoch: 8.86 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010478362374153215		[learning rate: 0.00010511]
		[batch 20/20] avg loss: -0.007071250635543093		[learning rate: 0.00010498]
	Learning Rate: 0.00010498
	LOSS [training: -0.008774806504848154 | validation: -0.01256850279058357]
	TIME [epoch: 8.86 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010988873262778172		[learning rate: 0.00010485]
		[batch 20/20] avg loss: -0.011385500687358902		[learning rate: 0.00010473]
	Learning Rate: 0.000104726
	LOSS [training: -0.01118718697506854 | validation: -0.011201785714077315]
	TIME [epoch: 8.88 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012921296449985572		[learning rate: 0.0001046]
		[batch 20/20] avg loss: -0.008210273611684863		[learning rate: 0.00010447]
	Learning Rate: 0.000104472
	LOSS [training: -0.010565785030835217 | validation: -0.011055209895343136]
	TIME [epoch: 8.87 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00994974452240166		[learning rate: 0.00010435]
		[batch 20/20] avg loss: -0.013043032516150455		[learning rate: 0.00010422]
	Learning Rate: 0.000104219
	LOSS [training: -0.011496388519276058 | validation: -0.008296151043194781]
	TIME [epoch: 8.85 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00902941303440396		[learning rate: 0.00010409]
		[batch 20/20] avg loss: -0.012130279377630705		[learning rate: 0.00010397]
	Learning Rate: 0.000103967
	LOSS [training: -0.010579846206017336 | validation: -0.01670309626393674]
	TIME [epoch: 8.84 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008519353831060016		[learning rate: 0.00010384]
		[batch 20/20] avg loss: -0.01087126270205801		[learning rate: 0.00010372]
	Learning Rate: 0.000103715
	LOSS [training: -0.009695308266559014 | validation: -0.012124901521822991]
	TIME [epoch: 8.85 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013390540966558565		[learning rate: 0.00010359]
		[batch 20/20] avg loss: -0.015398513095935793		[learning rate: 0.00010346]
	Learning Rate: 0.000103464
	LOSS [training: -0.01439452703124718 | validation: -0.01259955486675172]
	TIME [epoch: 8.89 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012120640149907276		[learning rate: 0.00010334]
		[batch 20/20] avg loss: -0.013763794708848356		[learning rate: 0.00010321]
	Learning Rate: 0.000103214
	LOSS [training: -0.012942217429377817 | validation: -0.012801365631252181]
	TIME [epoch: 8.85 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009351906387409313		[learning rate: 0.00010309]
		[batch 20/20] avg loss: -0.01044384460007224		[learning rate: 0.00010296]
	Learning Rate: 0.000102964
	LOSS [training: -0.009897875493740776 | validation: -0.006541931041026117]
	TIME [epoch: 8.85 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0099137267629682		[learning rate: 0.00010284]
		[batch 20/20] avg loss: -0.01595644976597549		[learning rate: 0.00010271]
	Learning Rate: 0.000102714
	LOSS [training: -0.012935088264471841 | validation: -0.009777636541095855]
	TIME [epoch: 8.85 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009052956104312346		[learning rate: 0.00010259]
		[batch 20/20] avg loss: -0.013887327990156467		[learning rate: 0.00010247]
	Learning Rate: 0.000102466
	LOSS [training: -0.011470142047234408 | validation: -0.011500740435590127]
	TIME [epoch: 8.86 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011011729908397412		[learning rate: 0.00010234]
		[batch 20/20] avg loss: -0.01415641388389816		[learning rate: 0.00010222]
	Learning Rate: 0.000102218
	LOSS [training: -0.012584071896147787 | validation: -0.017045395738553282]
	TIME [epoch: 8.85 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012729679618636747		[learning rate: 0.00010209]
		[batch 20/20] avg loss: -0.00952143708830855		[learning rate: 0.00010197]
	Learning Rate: 0.00010197
	LOSS [training: -0.011125558353472646 | validation: -0.009294765916964204]
	TIME [epoch: 8.85 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01383017171451705		[learning rate: 0.00010185]
		[batch 20/20] avg loss: -0.01188034559390993		[learning rate: 0.00010172]
	Learning Rate: 0.000101723
	LOSS [training: -0.012855258654213489 | validation: -0.014455347965235793]
	TIME [epoch: 8.86 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009515891423039846		[learning rate: 0.0001016]
		[batch 20/20] avg loss: -0.014614311369567073		[learning rate: 0.00010148]
	Learning Rate: 0.000101477
	LOSS [training: -0.01206510139630346 | validation: -0.014795512815297944]
	TIME [epoch: 8.85 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009870826751794229		[learning rate: 0.00010135]
		[batch 20/20] avg loss: -0.012228604682812835		[learning rate: 0.00010123]
	Learning Rate: 0.000101232
	LOSS [training: -0.011049715717303534 | validation: -0.015819182786430266]
	TIME [epoch: 8.87 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011209598337495082		[learning rate: 0.00010111]
		[batch 20/20] avg loss: -0.010812559806573929		[learning rate: 0.00010099]
	Learning Rate: 0.000100986
	LOSS [training: -0.011011079072034503 | validation: -0.022442905284815823]
	TIME [epoch: 8.85 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012734774451007114		[learning rate: 0.00010086]
		[batch 20/20] avg loss: -0.01311298275277623		[learning rate: 0.00010074]
	Learning Rate: 0.000100742
	LOSS [training: -0.012923878601891672 | validation: -0.010724509576471612]
	TIME [epoch: 8.84 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005504350182492733		[learning rate: 0.00010062]
		[batch 20/20] avg loss: -0.006552842147384311		[learning rate: 0.0001005]
	Learning Rate: 0.000100498
	LOSS [training: -0.006028596164938522 | validation: -0.016337808275444607]
	TIME [epoch: 8.85 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00944149193521807		[learning rate: 0.00010038]
		[batch 20/20] avg loss: -0.008422191207566945		[learning rate: 0.00010025]
	Learning Rate: 0.000100255
	LOSS [training: -0.008931841571392509 | validation: -0.014051113032560153]
	TIME [epoch: 8.84 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0050036991995537645		[learning rate: 0.00010013]
		[batch 20/20] avg loss: -0.012196638392002263		[learning rate: 0.00010001]
	Learning Rate: 0.000100012
	LOSS [training: -0.008600168795778015 | validation: -0.011467143182975923]
	TIME [epoch: 8.86 sec]
Finished training in 17849.750 seconds.
