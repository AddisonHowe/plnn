Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r3', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2695205401

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.703385280012562		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.413527685647509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.558456482830037 | validation: 5.297684243580903]
	TIME [epoch: 48.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.992686230646543		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8407027346594105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.416694482652977 | validation: 3.4764701977224646]
	TIME [epoch: 8.91 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.195063764223142		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.610230942506786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9026473533649635 | validation: 1.9498225421449633]
	TIME [epoch: 8.91 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.313635320432185		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.173906712791587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2437710166118863 | validation: 1.9664382696559026]
	TIME [epoch: 8.89 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0919754801438195		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2179120183060332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.154943749224926 | validation: 2.7307402255141096]
	TIME [epoch: 8.89 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9443000490917406		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.02889763251795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9865988408048456 | validation: 1.57699109587455]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.013211604106474		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8417830559589015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9274973300326876 | validation: 3.223716828813047]
	TIME [epoch: 8.91 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8762055208113069		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6381388215573107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7571721711843087 | validation: 1.8502326805059925]
	TIME [epoch: 8.89 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7760509509632343		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9292556959148552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.852653323439045 | validation: 1.3448959547811588]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6307765615710543		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6261340911943787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6284553263827164 | validation: 1.5346932978906607]
	TIME [epoch: 8.89 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7088302863181766		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6408075910498847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6748189386840306 | validation: 1.1393032321958902]
	TIME [epoch: 8.92 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6138604026042223		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7860113157630324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6999358591836278 | validation: 1.887517068500177]
	TIME [epoch: 8.88 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5679198865876323		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.542921896743475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5554208916655534 | validation: 0.9569404267709072]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4130945093611902		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4353813953873829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4242379523742863 | validation: 1.4766445301636526]
	TIME [epoch: 8.88 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4250516483118374		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5665107199745876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4957811841432125 | validation: 1.1462923136348673]
	TIME [epoch: 8.89 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4353547804447893		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5289234109400922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4821390956924412 | validation: 0.9706072511672426]
	TIME [epoch: 8.9 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.263827799530217		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1792945982484937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2215611988893555 | validation: 1.3028793318063985]
	TIME [epoch: 8.88 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.093686292038742		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1837679506421455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1387271213404437 | validation: 0.7736673443546422]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.114349846780907		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9550766686572437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0347132577190754 | validation: 0.7889245574758801]
	TIME [epoch: 8.89 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1838480935251854		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9951975240951091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0895228088101474 | validation: 0.8310419963207004]
	TIME [epoch: 8.89 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8455061631260085		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8677521809581762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8566291720420922 | validation: 0.6849945304128009]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7527909343941924		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9062957311639112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8295433327790518 | validation: 0.5279265534819715]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8086028965676697		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8724481893005678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8405255429341185 | validation: 0.8369578328389581]
	TIME [epoch: 8.89 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0898707014861342		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7512221651396607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9205464333128974 | validation: 0.667353307048024]
	TIME [epoch: 8.91 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5944516953602361		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7520092804155663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6732304878879012 | validation: 0.6240184272015168]
	TIME [epoch: 8.89 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7236437574672854		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7566246430844329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7401342002758591 | validation: 0.4604703876857616]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.709069648042205		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.691261759949358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001657039957815 | validation: 0.3560272847924929]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5812274192492033		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7171718096426505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.649199614445927 | validation: 0.4123017748892782]
	TIME [epoch: 8.91 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5827869666540509		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6592145489262505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6210007577901508 | validation: 0.5262149642971803]
	TIME [epoch: 8.88 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2245135127775248		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5728095746152976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8986615436964109 | validation: 0.4512330301845551]
	TIME [epoch: 8.88 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5771573440203772		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5044328436810799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5407950938507285 | validation: 0.6153862443735617]
	TIME [epoch: 8.87 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48036268773259677		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4896453252811773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48500400650688713 | validation: 0.34712918730071773]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.547688409375212		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5492006222284355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5484445158018236 | validation: 0.6862773431839744]
	TIME [epoch: 9.15 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5074715272646816		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47923390366205965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4933527154633707 | validation: 0.3687157687064607]
	TIME [epoch: 8.9 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5709436979561455		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5404090208340875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5556763593951166 | validation: 0.4340913418138385]
	TIME [epoch: 8.96 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5072378519549524		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4035374449392329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45538764844709256 | validation: 1.2564755669151584]
	TIME [epoch: 8.9 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5288682528445575		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8951712885576925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7120197707011251 | validation: 0.44553789605373395]
	TIME [epoch: 8.92 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38046622414670506		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47682622023451227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42864622219060877 | validation: 0.6175892561370733]
	TIME [epoch: 8.9 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3253870501600934		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6920761411663374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5087315956632154 | validation: 0.7622246599974817]
	TIME [epoch: 8.89 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49591785516642933		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3770081940483091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4364630246073693 | validation: 0.1844792830494471]
	TIME [epoch: 8.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5280826769331733		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43796768775034745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4830251823417605 | validation: 0.40739430337524934]
	TIME [epoch: 8.92 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4057333666021014		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7892021899240632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5974677782630823 | validation: 0.301700075388064]
	TIME [epoch: 8.89 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3703459614652972		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40106453601529324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3857052487402953 | validation: 0.7084176087141155]
	TIME [epoch: 8.89 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5757907272969328		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4261345181104349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5009626227036839 | validation: 0.4478235724636003]
	TIME [epoch: 8.89 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6419088486717555		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6717971727688786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6568530107203171 | validation: 0.2751426755751867]
	TIME [epoch: 8.9 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31638930815062627		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5096577640792597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.413023536114943 | validation: 0.25082505332991356]
	TIME [epoch: 8.91 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3542925135496781		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4718430414720756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41306777751087687 | validation: 0.9200482793880618]
	TIME [epoch: 8.89 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4537196091090472		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5307438886889058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4922317488989765 | validation: 0.2393969328553563]
	TIME [epoch: 8.89 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29603551191123295		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36272168761157986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3293785997614064 | validation: 0.16552030068912787]
	TIME [epoch: 8.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3442000375074454		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49520363373478543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41970183562111546 | validation: 0.31824693791816794]
	TIME [epoch: 8.92 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34011777451761255		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3835593988899943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3618385867038034 | validation: 0.28257422504218555]
	TIME [epoch: 8.89 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42235134740658237		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4494070954039577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4358792214052699 | validation: 0.26754306891049967]
	TIME [epoch: 8.89 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2594719421221875		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32787276995102677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2936723560366071 | validation: 0.2868306975553898]
	TIME [epoch: 8.89 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2725488543930359		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32098552487182735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29676718963243165 | validation: 0.32874145308374714]
	TIME [epoch: 8.92 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46043563908267693		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2921817810621142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3763087100723955 | validation: 0.22265442711393768]
	TIME [epoch: 8.89 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3233670001659142		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3521792562634447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3377731282146794 | validation: 0.41432467427324254]
	TIME [epoch: 8.89 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2969386103876041		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4206867930246646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3588127017061344 | validation: 1.6390361255835826]
	TIME [epoch: 8.89 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5424744813891744		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5670377885578066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5547561349734905 | validation: 0.2994752113964405]
	TIME [epoch: 8.91 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31224355875655097		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3262329467081702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31923825273236056 | validation: 0.21479847712227335]
	TIME [epoch: 8.89 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35643797762043067		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30226452648544166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32935125205293614 | validation: 0.19259901348549713]
	TIME [epoch: 8.88 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2521870750444918		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2452126611851356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24869986811481368 | validation: 0.21714878229655143]
	TIME [epoch: 8.88 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20547419698411634		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2901400491926444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2478071230883804 | validation: 0.4668419356544411]
	TIME [epoch: 8.89 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3681671583756004		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.213909148990014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2910381536828072 | validation: 0.16636004036533425]
	TIME [epoch: 8.9 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21360363168186383		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2386017719553592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22610270181861156 | validation: 0.16356877278871998]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6747545654515351		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4838748099225089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.579314687687022 | validation: 0.17340332715199316]
	TIME [epoch: 8.89 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20531926501883596		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25700896838528453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23116411670206025 | validation: 0.2706636964017652]
	TIME [epoch: 8.88 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3205398921098439		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4113679967326117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3659539444212279 | validation: 0.4238280486738843]
	TIME [epoch: 8.91 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24399818082133481		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20457453662137448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22428635872135466 | validation: 0.30812353486145116]
	TIME [epoch: 8.88 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19787048533097867		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.35712922029998995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2774998528154843 | validation: 0.4105503407638006]
	TIME [epoch: 8.88 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24396501178908087		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2591143215888535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25153966668896716 | validation: 0.21459392442407532]
	TIME [epoch: 8.88 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2818036518382286		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2017294418534014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24176654684581494 | validation: 0.22284914086433785]
	TIME [epoch: 8.91 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21289991114003493		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2834421386359779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2481710248880064 | validation: 0.323858663043204]
	TIME [epoch: 8.88 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24950404210720953		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16759062537668895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20854733374194928 | validation: 0.11093244023465566]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3347482510616772		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3354890303997709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33511864073072406 | validation: 0.14761810542081485]
	TIME [epoch: 8.89 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36482804526819185		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17283948250317993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26883376388568586 | validation: 0.2648756800741261]
	TIME [epoch: 8.9 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21179679495763337		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23405292616529155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22292486056146243 | validation: 0.1679144799882336]
	TIME [epoch: 8.9 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18458511567638064		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2442141538365735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21439963475647708 | validation: 0.17142217449796066]
	TIME [epoch: 8.89 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17362043705597266		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2120862373573928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1928533372066827 | validation: 0.19089480568401318]
	TIME [epoch: 8.88 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30677594211902887		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24279788038649777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27478691125276333 | validation: 0.15747506721047383]
	TIME [epoch: 8.9 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20112102689101413		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.268440859440523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23478094316576853 | validation: 0.3840839047264895]
	TIME [epoch: 8.91 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1842506881919776		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12539695180602634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15482381999900197 | validation: 0.13742170288054806]
	TIME [epoch: 8.89 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14542209887177832		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23485770808766787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1901399034797231 | validation: 0.15236086884476702]
	TIME [epoch: 8.89 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1614178131976049		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23696127478253218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1991895439900685 | validation: 0.14204684426249511]
	TIME [epoch: 8.89 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22474878037496993		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19974932070642515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21224905054069754 | validation: 0.36732206240126686]
	TIME [epoch: 8.92 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28432409201632103		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14656839646312145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2154462442397212 | validation: 0.12904630598003694]
	TIME [epoch: 8.89 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15118254207322118		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26163604566916915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20640929387119517 | validation: 0.3914952968461871]
	TIME [epoch: 8.89 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2784121014106989		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2672480780329665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2728300897218327 | validation: 0.4670798484644563]
	TIME [epoch: 8.89 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28032243400103385		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18153500446629478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23092871923366437 | validation: 0.15177272806954784]
	TIME [epoch: 8.91 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15055902871709353		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1376393256267306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14409917717191206 | validation: 0.12934960358609932]
	TIME [epoch: 8.89 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22550259087498664		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11332043035066919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16941151061282794 | validation: 0.1305769249621153]
	TIME [epoch: 8.89 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14544417459544295		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18606874952521527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16575646206032907 | validation: 0.15657921937801528]
	TIME [epoch: 8.97 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13315705228419333		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24758675725290247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1903719047685479 | validation: 0.1502991947758328]
	TIME [epoch: 8.89 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2916950946837319		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3497354187891329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3207152567364324 | validation: 0.4593866581617507]
	TIME [epoch: 8.9 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6160742039548184		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5021008176205178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5590875107876683 | validation: 0.2067030862922768]
	TIME [epoch: 8.88 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2244235403528818		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2761042826017747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2502639114773283 | validation: 0.15711899958994024]
	TIME [epoch: 8.88 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2243894034431914		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2588938796138449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24164164152851816 | validation: 0.1514894197212893]
	TIME [epoch: 8.89 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2046934163735398		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13652093043864422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.170607173406092 | validation: 0.1572144901944051]
	TIME [epoch: 8.91 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16364335232958496		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1165508902297249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1400971212796549 | validation: 0.205470434170476]
	TIME [epoch: 8.88 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3190511306240027		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30697209321854213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3130116119212725 | validation: 0.1904120404235076]
	TIME [epoch: 8.89 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18894749019351526		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26466977693078986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22680863356215256 | validation: 0.3501188109329866]
	TIME [epoch: 8.92 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.305618827426694		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3429818376199153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3243003325233047 | validation: 0.19506800354997958]
	TIME [epoch: 8.95 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15492229687507067		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22031167321677061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18761698504592064 | validation: 0.17653555594079207]
	TIME [epoch: 8.91 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23053397493327094		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18556143538482306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20804770515904697 | validation: 0.18237806443230867]
	TIME [epoch: 8.91 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2603243357647359		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17210867048364392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21621650312418988 | validation: 0.057030691832050825]
	TIME [epoch: 8.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09421233115558068		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15057589553061906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12239411334309987 | validation: 0.1365685179839101]
	TIME [epoch: 8.94 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15953349710334297		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10034409024210514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12993879367272404 | validation: 0.389632945840555]
	TIME [epoch: 8.91 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21409007757390638		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15886885550423208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18647946653906922 | validation: 0.4605079471198887]
	TIME [epoch: 8.89 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13840691427834725		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13313653666545552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13577172547190142 | validation: 0.08867659409963746]
	TIME [epoch: 8.88 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1401742127804197		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16116951136052543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15067186207047253 | validation: 0.19455568114389338]
	TIME [epoch: 8.9 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12498982575065443		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23154128109923772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17826555342494604 | validation: 0.32895062479600806]
	TIME [epoch: 8.9 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4030184253530213		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15094976983685454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2769840975949379 | validation: 0.06753893628848767]
	TIME [epoch: 8.89 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09490894536191205		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14215443472479872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11853169004335543 | validation: 0.06380222086407407]
	TIME [epoch: 8.89 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08407155338479817		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15317474281568266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11862314810024044 | validation: 0.22550016482751334]
	TIME [epoch: 8.89 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14757459299841888		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0966061473704843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1220903701844516 | validation: 0.06766634791522319]
	TIME [epoch: 8.91 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11576823753011796		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10336246295022296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10956535024017047 | validation: 0.23546201111880705]
	TIME [epoch: 8.88 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15313468879794828		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.39844335968100797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2757890242394781 | validation: 0.2369150545835328]
	TIME [epoch: 8.89 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31374002913991667		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1587163715851126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23622820036251463 | validation: 0.19782657003746945]
	TIME [epoch: 8.89 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23689059858082903		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16082963671642095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.198860117648625 | validation: 0.1546431040879185]
	TIME [epoch: 8.91 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12071466007678917		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10577962820254135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11324714413966526 | validation: 0.08487671599402125]
	TIME [epoch: 8.89 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2414372546085856		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1093778066344943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17540753062153994 | validation: 0.14221415616142463]
	TIME [epoch: 8.89 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13013980355802096		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12521118863622466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12767549609712286 | validation: 0.07793227315052048]
	TIME [epoch: 8.89 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13938544859964627		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1868582219668198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16312183528323304 | validation: 0.07691799879060657]
	TIME [epoch: 8.91 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1404101493337247		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09329745351813391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11685380142592931 | validation: 0.08097922697568626]
	TIME [epoch: 8.9 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0812877625610009		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1588422685397522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12006501555037652 | validation: 0.08154500882680474]
	TIME [epoch: 8.89 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09606576684918888		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13676524608047017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11641550646482952 | validation: 0.09368674401758648]
	TIME [epoch: 8.89 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18584293291347548		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2258358370623769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20583938498792623 | validation: 0.09687689053189669]
	TIME [epoch: 8.9 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16786709371448777		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26357773577311994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21572241474380388 | validation: 0.17687696542553183]
	TIME [epoch: 8.9 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12028645290029913		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.113031286081574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11665886949093658 | validation: 0.11504905453302013]
	TIME [epoch: 8.88 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10115186264208031		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09182097626580528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09648641945394279 | validation: 0.09499096929333296]
	TIME [epoch: 8.89 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10204893205651752		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08127884728704483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09166388967178118 | validation: 0.09865334543858173]
	TIME [epoch: 8.89 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12403093257835404		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15253654518652113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1382837388824376 | validation: 0.1079686971009487]
	TIME [epoch: 8.91 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16463353190891822		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19844502938249636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1815392806457073 | validation: 0.2133364977285401]
	TIME [epoch: 8.89 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1441890439974137		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2622756479334494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2032323459654315 | validation: 0.26198353512791295]
	TIME [epoch: 8.89 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.158478537237757		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13498191457415085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14673022590595394 | validation: 0.22489889367239824]
	TIME [epoch: 8.89 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19676042675411237		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09415100446085947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1454557156074859 | validation: 0.18388632746038244]
	TIME [epoch: 8.91 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19608984603331464		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13396278878399132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16502631740865298 | validation: 0.2289590313404552]
	TIME [epoch: 8.89 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13872350472343634		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1098515098304598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12428750727694807 | validation: 0.11079442014357577]
	TIME [epoch: 8.89 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12435076664890876		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12561345061563928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12498210863227402 | validation: 0.25053965836789865]
	TIME [epoch: 8.88 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15376250726027635		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14079641202625148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14727945964326392 | validation: 0.054027379859666394]
	TIME [epoch: 8.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10611135867799852		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13228742902723267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1191993938526156 | validation: 0.24186450122764314]
	TIME [epoch: 8.91 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1449843469089187		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1012409681697678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12311265753934326 | validation: 0.1626493562140072]
	TIME [epoch: 8.88 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15040324722092327		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09740543148560879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12390433935326602 | validation: 0.12157750704353268]
	TIME [epoch: 8.88 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18625641829319237		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30605414314353374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24615528071836307 | validation: 0.23316934570263687]
	TIME [epoch: 8.89 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24371654529605088		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3062203681013508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27496845669870085 | validation: 0.5271021612880217]
	TIME [epoch: 8.9 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21931560487724533		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1549673202772922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18714146257726877 | validation: 0.19411627258269112]
	TIME [epoch: 8.89 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09857928393355989		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12036591159461052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1094725977640852 | validation: 0.0748937743457614]
	TIME [epoch: 8.88 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10240306438739984		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10095825181273063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10168065810006521 | validation: 0.06671740154564065]
	TIME [epoch: 8.89 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13980924532916594		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10813092373943686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12397008453430139 | validation: 0.11405063582304148]
	TIME [epoch: 8.91 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12766862079298347		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11220023685963779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11993442882631063 | validation: 0.10707645494309995]
	TIME [epoch: 8.89 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11805372537894025		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1690098767976444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14353180108829236 | validation: 0.3024671958655225]
	TIME [epoch: 8.89 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12608005067820652		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07529312008338336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10068658538079496 | validation: 0.04405958981830287]
	TIME [epoch: 8.89 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09410707103352871		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1124111038184838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10325908742600627 | validation: 0.12245158159640042]
	TIME [epoch: 8.91 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17861596105076036		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0904022696002163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13450911532548834 | validation: 0.03224925451268901]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1535205057745254		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09519544610427258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12435797593939894 | validation: 0.041954646985509006]
	TIME [epoch: 8.88 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10816139926193628		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17005061156764154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1391060054147889 | validation: 0.13729442786241058]
	TIME [epoch: 8.88 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1277465059380871		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10048875892242796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11411763243025752 | validation: 0.09740117972314549]
	TIME [epoch: 8.89 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1621949560836037		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1053102301938921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1337525931387479 | validation: 0.08945513017548053]
	TIME [epoch: 8.9 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14023512980799774		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10642841011816744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12333176996308262 | validation: 0.06484275483936218]
	TIME [epoch: 8.88 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09148385655354593		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13297752714067754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11223069184711176 | validation: 0.07047366161922916]
	TIME [epoch: 8.88 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10872286486878775		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15476821266778978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1317455387682888 | validation: 0.06313115216577046]
	TIME [epoch: 8.88 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11591313365036204		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14328733298970114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1296002333200316 | validation: 0.1296863313934039]
	TIME [epoch: 8.91 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1368053118718388		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1372444853838213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13702489862783004 | validation: 0.11472055501160372]
	TIME [epoch: 8.88 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15646522113408798		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10420208511175441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13033365312292117 | validation: 0.12708023209782354]
	TIME [epoch: 8.88 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12688404587692498		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10640075015647459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11664239801669976 | validation: 0.11802582522792934]
	TIME [epoch: 8.88 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13263156983139407		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12242638434345292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12752897708742345 | validation: 0.14585544023506763]
	TIME [epoch: 8.91 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10508219937724214		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1412646630541914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1231734312157168 | validation: 0.24506894032710372]
	TIME [epoch: 8.9 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10240789451751464		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11370068134414857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1080542879308316 | validation: 0.12644242183401344]
	TIME [epoch: 8.89 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11149143146820409		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09612673401318772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10380908274069592 | validation: 0.05383279089453978]
	TIME [epoch: 8.89 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.111279656350702		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11552538663298266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11340252149184235 | validation: 0.04139460941064728]
	TIME [epoch: 8.91 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09528774443877952		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14763763459379578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12146268951628765 | validation: 0.13056198415681608]
	TIME [epoch: 8.9 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11449488675929125		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12338677075677129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11894082875803128 | validation: 0.06641010327040178]
	TIME [epoch: 8.88 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1544221616767935		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08334993833989493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11888605000834421 | validation: 0.14887702038268258]
	TIME [epoch: 8.89 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15124062093598442		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12694440314572025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13909251204085232 | validation: 0.0678280097730571]
	TIME [epoch: 8.9 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11265823758551718		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08925603074674596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10095713416613157 | validation: 0.09409511652333502]
	TIME [epoch: 8.9 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1051475998678059		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13546031486196383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12030395736488489 | validation: 0.2828472586309636]
	TIME [epoch: 8.9 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12575027747929265		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1243923935914549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12507133553537378 | validation: 0.06765094598939915]
	TIME [epoch: 8.89 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1393079582081786		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08883524211497315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11407160016157589 | validation: 0.04501498541086378]
	TIME [epoch: 8.89 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.105894245417922		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12642028652351137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11615726597071667 | validation: 0.15164557701903142]
	TIME [epoch: 8.92 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1571282953224091		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11054649250653777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1338373939144734 | validation: 0.11680589429341098]
	TIME [epoch: 8.89 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16939410507836802		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13784482379391994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.153619464436144 | validation: 0.1127296348128905]
	TIME [epoch: 8.88 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15022461154710517		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11185450774656858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1310395596468369 | validation: 0.060539112075187994]
	TIME [epoch: 8.89 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09207434852960746		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0917518734592733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0919131109944404 | validation: 0.12000289123729228]
	TIME [epoch: 8.92 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12168569876896976		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24595017603797026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18381793740347002 | validation: 0.13807898479840242]
	TIME [epoch: 8.9 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2647740505070292		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20194191066569017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23335798058635962 | validation: 0.08479453955119232]
	TIME [epoch: 8.89 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09784697295748832		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11216884500934277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10500790898341557 | validation: 0.08208634864481193]
	TIME [epoch: 8.89 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10476067526790005		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07292131117607001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08884099322198503 | validation: 0.08381868995766303]
	TIME [epoch: 8.9 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10017241168621951		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16080468281277505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13048854724949727 | validation: 0.09856694645882254]
	TIME [epoch: 8.91 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08165683500330567		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10828115937508778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09496899718919671 | validation: 0.11867883035091292]
	TIME [epoch: 8.89 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11333456030019115		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07263108408420328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09298282219219722 | validation: 0.04306960434827366]
	TIME [epoch: 8.89 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16534171124090352		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14478887525573414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15506529324831883 | validation: 0.045945841109702734]
	TIME [epoch: 8.9 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10231361874590246		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18772762399483114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14502062137036678 | validation: 0.16011099821202524]
	TIME [epoch: 8.9 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.089008099502809		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09440193085935793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09170501518108347 | validation: 0.0694545558044791]
	TIME [epoch: 8.89 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10559493198668987		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09098406800965732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0982894999981736 | validation: 0.050540334938828736]
	TIME [epoch: 8.89 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13867686769510806		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10233524966175975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12050605867843386 | validation: 0.05827656878259238]
	TIME [epoch: 8.89 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1285924222359946		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28855639808569633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2085744101608454 | validation: 0.10348023653531459]
	TIME [epoch: 8.91 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15352526368193914		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09510278808189765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12431402588191835 | validation: 0.12361178924403407]
	TIME [epoch: 8.89 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10492499261688355		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10914034440748457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10703266851218407 | validation: 0.06612847395753224]
	TIME [epoch: 8.88 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08851436866947437		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16673174918168537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1276230589255799 | validation: 0.05729063307819668]
	TIME [epoch: 8.9 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09648096593532779		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15126581778144638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12387339185838711 | validation: 0.06716067459043185]
	TIME [epoch: 8.92 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1182867673122022		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14566073316624786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13197375023922503 | validation: 0.42191764950997485]
	TIME [epoch: 8.9 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24242526199490128		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11711001532600918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17976763866045523 | validation: 0.21162344253744214]
	TIME [epoch: 8.91 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1349830537448884		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10540088532434785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12019196953461812 | validation: 0.21504087084446605]
	TIME [epoch: 8.9 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14233128291937971		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1713496532426191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15684046808099944 | validation: 0.1312445259825495]
	TIME [epoch: 8.92 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19250981156067082		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1831077303542015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18780877095743617 | validation: 0.1061924046473305]
	TIME [epoch: 8.91 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1576691227124423		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07233424572087589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11500168421665909 | validation: 0.03537397217466894]
	TIME [epoch: 8.9 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08741642399618503		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08435061944251718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08588352171935112 | validation: 0.1608248137067661]
	TIME [epoch: 8.9 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09746434020036834		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17614881036250912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13680657528143872 | validation: 0.06349836468805371]
	TIME [epoch: 8.9 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19228774738106255		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09363249282214305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14296012010160275 | validation: 0.061847954890687507]
	TIME [epoch: 8.93 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09316776150095384		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1369947734955595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11508126749825667 | validation: 0.043913115796814846]
	TIME [epoch: 8.9 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10573746018163868		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1124979064955098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10911768333857426 | validation: 0.07497916319532547]
	TIME [epoch: 8.91 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19106698567905095		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18367547959855512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18737123263880304 | validation: 0.4734665376277058]
	TIME [epoch: 8.9 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3673043928856362		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20116664228353578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28423551758458593 | validation: 0.06884232882419064]
	TIME [epoch: 8.92 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10637892478315555		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16729373452952342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13683632965633946 | validation: 0.22314244124564522]
	TIME [epoch: 8.89 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15130445387760347		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2860709095975021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21868768173755276 | validation: 0.39863696305065205]
	TIME [epoch: 8.89 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19815831201761736		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09525622983732199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14670727092746966 | validation: 0.07674285451673654]
	TIME [epoch: 8.89 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24186171714289006		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20774730031102545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22480450872695776 | validation: 0.07812680746717025]
	TIME [epoch: 8.92 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11561324590027577		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0754044832731181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09550886458669695 | validation: 0.1189414557900253]
	TIME [epoch: 8.9 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19180940634416976		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08975287997971403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14078114316194187 | validation: 0.07440065271880297]
	TIME [epoch: 8.9 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08102270780173353		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10239990702893612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09171130741533481 | validation: 0.04156733761412384]
	TIME [epoch: 8.89 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08719400533469279		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.140192070501114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11369303791790344 | validation: 0.09639956273943219]
	TIME [epoch: 8.91 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09291076627703004		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15207659237531762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12249367932617379 | validation: 0.08743108105887751]
	TIME [epoch: 8.91 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16849367628207584		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10479461245542679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13664414436875133 | validation: 0.05559175078017134]
	TIME [epoch: 8.89 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22804603673284993		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14647636809857784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1872612024157139 | validation: 0.1358530351709035]
	TIME [epoch: 8.88 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12271901700803042		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1401908484901447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13145493274908754 | validation: 0.15146906560071383]
	TIME [epoch: 8.89 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15159176586875742		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16836852359897966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15998014473386854 | validation: 0.2712194114038856]
	TIME [epoch: 8.91 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1725698851971969		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09951939179935185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1360446384982744 | validation: 0.2125314524910108]
	TIME [epoch: 8.88 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1052709095573611		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12616441438960044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11571766197348077 | validation: 0.18918076735297204]
	TIME [epoch: 8.93 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2742675188918796		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2684431117241623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27135531530802093 | validation: 0.24911792541508224]
	TIME [epoch: 8.89 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18512565366571226		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11910513872542261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15211539619556744 | validation: 0.14141009585706968]
	TIME [epoch: 8.91 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14198554089853183		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16744293492014842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1547142379093401 | validation: 0.24922297215382194]
	TIME [epoch: 8.89 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14456133753230022		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16248021101681037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1535207742745553 | validation: 0.08982572620854837]
	TIME [epoch: 8.89 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1712770324329384		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1493977396301523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16033738603154538 | validation: 0.1255262601076763]
	TIME [epoch: 8.88 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09028737574176844		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20524968509987573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1477685304208221 | validation: 0.1981292480598934]
	TIME [epoch: 8.9 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22993689146316293		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1538065429551834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19187171720917315 | validation: 0.11581174318306613]
	TIME [epoch: 8.9 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13444009928769768		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10477460038690653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11960734983730212 | validation: 0.2550244392568138]
	TIME [epoch: 8.89 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19008612647955164		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12041748118003374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15525180382979267 | validation: 0.03175560082991701]
	TIME [epoch: 8.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10171839075843128		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1118512761110348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10678483343473306 | validation: 0.27674241459425664]
	TIME [epoch: 8.89 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16974963927339154		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11795236569401121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14385100248370136 | validation: 0.06277164998629911]
	TIME [epoch: 8.89 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09035861002923709		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09830786123158872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0943332356304129 | validation: 0.030203016579852644]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15585088291746393		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11173486209392351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13379287250569374 | validation: 0.24336990522022617]
	TIME [epoch: 8.88 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1670908526931873		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10103168328803376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13406126799061052 | validation: 0.10292139270723077]
	TIME [epoch: 8.87 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08863292467138743		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10030362866679757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09446827666909251 | validation: 0.15221636753754525]
	TIME [epoch: 8.89 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12654924876784152		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1222459752930384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12439761203043995 | validation: 0.09791815182245667]
	TIME [epoch: 8.87 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12694833919281961		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1704668712732474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14870760523303353 | validation: 0.19446687601066956]
	TIME [epoch: 8.87 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16121697493092269		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06823152983536533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11472425238314403 | validation: 0.055799911956153664]
	TIME [epoch: 8.88 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08777496048864589		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06874840831249193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07826168440056892 | validation: 0.06227056583462325]
	TIME [epoch: 8.89 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0904019016237197		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10396119343172772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0971815475277237 | validation: 0.08327447571598191]
	TIME [epoch: 8.88 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10447726208589687		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1468912920347797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1256842770603383 | validation: 0.028989683659607243]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r3_20240216_212949/states/model_tr_study2_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09580163552821122		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1461352168255116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12096842617686139 | validation: 0.18033442809210998]
	TIME [epoch: 8.88 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14457390508731902		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12918376437569287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13687883473150594 | validation: 0.23916514643555153]
	TIME [epoch: 8.88 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11798217208326807		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06855087432570732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09326652320448772 | validation: 0.0577140719567331]
	TIME [epoch: 8.88 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12172545581007155		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11612322332721421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11892433956864287 | validation: 0.09317690740728156]
	TIME [epoch: 8.87 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12129990781588178		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15230664367452576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13680327574520373 | validation: 0.07907808374099408]
	TIME [epoch: 8.87 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11747936043507894		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10739518027886419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11243727035697156 | validation: 0.11340997461975179]
	TIME [epoch: 8.88 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1493982760904413		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09306762081946893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1212329484549551 | validation: 0.056930487239968394]
	TIME [epoch: 8.88 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19460109278501075		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07095336125912317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13277722702206698 | validation: 0.04832495346803779]
	TIME [epoch: 8.88 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08006634194419797		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1250310933151453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10254871762967162 | validation: 0.06726206715845591]
	TIME [epoch: 8.87 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09647591149154429		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19253249752089222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14450420450621826 | validation: 0.19936456507370293]
	TIME [epoch: 8.88 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1818115494965131		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19153787774914952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18667471362283128 | validation: 0.22360476504098714]
	TIME [epoch: 8.9 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22643494429228556		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13419875753483856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18031685091356203 | validation: 0.35815986757163765]
	TIME [epoch: 8.87 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1784113018495955		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1176325750058886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14802193842774203 | validation: 0.1051792809660223]
	TIME [epoch: 8.87 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11002013252841888		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22479190531275056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1674060189205847 | validation: 0.2858335951712307]
	TIME [epoch: 8.87 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1310726888426204		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10620545623181588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11863907253721813 | validation: 0.11701645649382073]
	TIME [epoch: 8.89 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08721057890457147		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1264809091049178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10684574400474464 | validation: 0.19905137940535633]
	TIME [epoch: 8.87 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12315641087379736		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1421311027355765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13264375680468693 | validation: 0.2004229052257436]
	TIME [epoch: 8.87 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09084335700597349		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1296636672520765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11025351212902498 | validation: 0.07464912762007193]
	TIME [epoch: 8.87 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1220799824673829		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1416986866443332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13188933455585802 | validation: 0.047460575180506615]
	TIME [epoch: 8.88 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1043972135137965		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10752187131548126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10595954241463887 | validation: 0.06685969460155303]
	TIME [epoch: 8.88 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12143716060680809		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10389990693339887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11266853377010348 | validation: 0.13675899599033536]
	TIME [epoch: 8.87 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11230493287819263		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10385549775996841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10808021531908052 | validation: 0.11808799493691406]
	TIME [epoch: 8.87 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09302370766894316		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07843597909023434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08572984337958872 | validation: 0.05272618594478454]
	TIME [epoch: 8.86 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10699469383902889		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1214673069292194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11423100038412412 | validation: 0.09136550676263905]
	TIME [epoch: 8.9 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14463859511197827		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15405856384437866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14934857947817853 | validation: 0.09106667240432056]
	TIME [epoch: 8.87 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10034837648322323		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17067472960925653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13551155304623988 | validation: 0.24880703926384543]
	TIME [epoch: 8.87 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17632157847631366		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16510518314798275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17071338081214818 | validation: 0.04620672416556727]
	TIME [epoch: 8.87 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06379062724815271		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1524845100708086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10813756865948065 | validation: 0.22896791924221288]
	TIME [epoch: 8.9 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16655814022689477		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10302509348854158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13479161685771815 | validation: 0.09150407737965621]
	TIME [epoch: 8.87 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1081351583964335		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10346299142087703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1057990749086553 | validation: 0.174171998961751]
	TIME [epoch: 8.87 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22681722049706649		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2119881879932671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21940270424516678 | validation: 0.0860327396032391]
	TIME [epoch: 8.87 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12453410130062174		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1908811873443532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1577076443224875 | validation: 0.3551482929874953]
	TIME [epoch: 8.89 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14696921058820303		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1485161921657975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1477427013770003 | validation: 0.04823581677946005]
	TIME [epoch: 8.89 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12500396085186796		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11615934300431237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12058165192809019 | validation: 0.1472013850914304]
	TIME [epoch: 8.86 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18817092656196582		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20782844558140368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19799968607168472 | validation: 0.04596095622323618]
	TIME [epoch: 8.87 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16606834816644567		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13037506749022296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1482217078283343 | validation: 0.064242652985329]
	TIME [epoch: 8.88 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09932086714394105		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11723321345236262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10827704029815184 | validation: 0.13508458698897632]
	TIME [epoch: 8.88 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10241965118926524		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0997532867611794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1010864689752223 | validation: 0.0851508602712978]
	TIME [epoch: 8.87 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11208890790116736		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27479908270208203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1934439953016247 | validation: 0.34800488680184355]
	TIME [epoch: 8.87 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1959117276707219		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14586161595145186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17088667181108694 | validation: 0.2198796578329975]
	TIME [epoch: 8.86 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2915622775216267		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3937869572465177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3426746173840722 | validation: 0.1694364983257113]
	TIME [epoch: 8.89 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.182385031639306		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2897503302577588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23606768094853242 | validation: 0.2832143646765665]
	TIME [epoch: 8.87 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16469037553161084		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14457877273333944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1546345741324751 | validation: 0.18330571715511706]
	TIME [epoch: 8.87 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13269632841186935		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08248925183113846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10759279012150387 | validation: 0.1046377987742281]
	TIME [epoch: 8.87 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10526692555477264		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17332737533064274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1392971504427077 | validation: 0.09745580320377563]
	TIME [epoch: 8.88 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21425705107445894		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.279484000650489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24687052586247402 | validation: 0.08662927194210655]
	TIME [epoch: 8.87 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17247167534412616		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2263979506011531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19943481297263962 | validation: 0.2549381300637129]
	TIME [epoch: 8.87 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2182269005770765		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20496140714604563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21159415386156102 | validation: 0.12111491660344939]
	TIME [epoch: 8.87 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27276348076212914		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13691832178267394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2048409012724015 | validation: 0.2285891709314521]
	TIME [epoch: 8.88 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14648494902276102		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1277324499533055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13710869948803325 | validation: 0.08902096037493393]
	TIME [epoch: 8.88 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08399575449752501		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11086562914481961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09743069182117231 | validation: 0.11962519894244926]
	TIME [epoch: 8.87 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1845928750066656		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11873767051713671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15166527276190114 | validation: 0.2324752265131441]
	TIME [epoch: 8.87 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18419102033366733		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11143306607241461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14781204320304095 | validation: 0.061371791737962096]
	TIME [epoch: 8.86 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1359546758042937		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30171533403064543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2188350049174696 | validation: 0.08953944323967336]
	TIME [epoch: 8.9 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12835719863799905		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09964237288071634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11399978575935772 | validation: 0.05148874221676847]
	TIME [epoch: 8.87 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1691953220899016		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13429754269662375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15174643239326266 | validation: 0.11164044405992658]
	TIME [epoch: 8.87 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22958706358708197		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12222611150232306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1759065875447025 | validation: 0.05169286136878455]
	TIME [epoch: 8.87 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11806737921861066		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11465990529273558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11636364225567311 | validation: 0.12725572879086663]
	TIME [epoch: 8.89 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1122481253721084		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11815367239362998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11520089888286918 | validation: 0.36543181144428766]
	TIME [epoch: 8.87 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18840814659540406		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15768558991305331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1730468682542287 | validation: 0.23518234595390847]
	TIME [epoch: 8.87 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12764495051647956		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12932615051465446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.128485550515567 | validation: 0.20283147420847714]
	TIME [epoch: 8.87 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1331358612868659		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10569352861195658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11941469494941122 | validation: 0.046251336802494006]
	TIME [epoch: 8.89 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1346798590272747		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11589362301523731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.125286741021256 | validation: 0.0920439168282026]
	TIME [epoch: 8.89 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24529165878154352		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1824834838266812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21388757130411232 | validation: 0.1455997877277887]
	TIME [epoch: 8.87 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12770184601175869		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09953055946687205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11361620273931536 | validation: 0.18614837385698912]
	TIME [epoch: 8.87 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12786438524046592		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08332186664490968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10559312594268781 | validation: 0.30493796336146073]
	TIME [epoch: 8.89 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17207802314425893		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09791914793863718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13499858554144806 | validation: 0.13870471267040033]
	TIME [epoch: 8.89 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13469148008206963		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0947595157476426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11472549791485613 | validation: 0.19068876313271077]
	TIME [epoch: 8.88 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11386763607868529		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11712310937230755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11549537272549641 | validation: 0.1261217314049724]
	TIME [epoch: 8.87 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12768751630579273		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11055886013519149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1191231882204921 | validation: 0.07224141460525058]
	TIME [epoch: 8.88 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10630565281696062		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1392915757105795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12279861426377006 | validation: 0.09750473318010588]
	TIME [epoch: 8.9 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1339044869445357		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11200678739778698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12295563717116131 | validation: 0.2692839036597377]
	TIME [epoch: 8.88 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13144022626082175		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10123227840201154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11633625233141667 | validation: 0.06862596232296111]
	TIME [epoch: 8.88 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13541256080055997		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1606417523615367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1480271565810483 | validation: 0.05630150721273624]
	TIME [epoch: 8.88 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10834320360351324		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13881628348865319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12357974354608321 | validation: 0.146825750310307]
	TIME [epoch: 8.9 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1289349759346936		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12020953347712562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1245722547059096 | validation: 0.14937602701775476]
	TIME [epoch: 8.88 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11510541828536876		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14106644993462214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12808593410999547 | validation: 0.13540045728735803]
	TIME [epoch: 8.87 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.169466929452619		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14178092385166224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1556239266521406 | validation: 0.08357271483444453]
	TIME [epoch: 8.87 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18535960613306443		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10279209983759947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14407585298533193 | validation: 0.09620175429021675]
	TIME [epoch: 8.88 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14048265923519554		[learning rate: 0.01]
		[batch 20/20] avg loss: nan		[learning rate: 0.01]
ERROR:
nan encountered in epoch 327 (training loss).
