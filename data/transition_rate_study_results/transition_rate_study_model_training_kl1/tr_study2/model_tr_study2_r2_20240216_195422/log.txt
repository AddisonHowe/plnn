Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r2', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3026114092

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.031493363290895		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.752384946752595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.391939155021744 | validation: 6.4113662183412625]
	TIME [epoch: 47.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.857231316193603		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.799274442233169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3282528792133865 | validation: 4.356247642154058]
	TIME [epoch: 8.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8602741002174943		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.153703687319625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.50698889376856 | validation: 2.5120535046460017]
	TIME [epoch: 8.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3893002382048882		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3417457148463883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3655229765256385 | validation: 2.179514680356658]
	TIME [epoch: 8.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9954469695683699		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.814665035963327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9050560027658485 | validation: 1.6281947327854096]
	TIME [epoch: 8.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6261244841979317		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5163442567526177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5712343704752745 | validation: 1.6829121109845304]
	TIME [epoch: 8.69 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5298358991935497		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3964335241045442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4631347116490467 | validation: 1.6772592902997376]
	TIME [epoch: 8.71 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3753579407462675		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.375042449970978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3752001953586226 | validation: 1.031674676738295]
	TIME [epoch: 8.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1556396652429808		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.084207520862614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1199235930527975 | validation: 0.8066533505569087]
	TIME [epoch: 8.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.159678506072885		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1633342894009062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1615063977368956 | validation: 1.2375163099793578]
	TIME [epoch: 8.68 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9420489190750599		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3692770931241776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.155663006099619 | validation: 1.1499602709258763]
	TIME [epoch: 8.69 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1056582383608995		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.874869272046117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9902637552035083 | validation: 0.898346797247274]
	TIME [epoch: 8.7 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0368520178275311		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7902648917560333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9135584547917823 | validation: 0.9735038162859732]
	TIME [epoch: 8.69 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.808554055370516		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8052336578870698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8068938566287928 | validation: 1.441208658890706]
	TIME [epoch: 8.68 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8776538285553148		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6449351837878589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7612945061715869 | validation: 0.6806226516174992]
	TIME [epoch: 8.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5892254890397048		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5525063683680849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5708659287038949 | validation: 1.4756511691055092]
	TIME [epoch: 8.73 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6942672343381671		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6762887219444322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6852779781412996 | validation: 0.48286920178330334]
	TIME [epoch: 8.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5524998019643533		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7916522903783535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6720760461713536 | validation: 0.4040916922604407]
	TIME [epoch: 8.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.459536370228175		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7239966248298474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5917664975290111 | validation: 0.48282909207964386]
	TIME [epoch: 8.68 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5958542855212393		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5553601995206585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5756072425209491 | validation: 0.5160356237891384]
	TIME [epoch: 8.7 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48581726313910767		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5260321591356336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5059247111373706 | validation: 0.3695813138569803]
	TIME [epoch: 8.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4879239954201068		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45971779746006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47382089644008324 | validation: 0.3875681009417098]
	TIME [epoch: 8.71 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5326517585425347		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5546047223256756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.543628240434105 | validation: 0.9787423482770932]
	TIME [epoch: 8.68 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46150456882863233		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3954924138520968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42849849134036455 | validation: 0.7730797455127824]
	TIME [epoch: 8.69 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4508128833006107		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7787981743479524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6148055288242817 | validation: 0.3845244940127663]
	TIME [epoch: 8.7 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4308659389577968		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.42470565203110366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4277857954944502 | validation: 0.5401302134403567]
	TIME [epoch: 8.69 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6469062301938737		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49650993192444093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5717080810591574 | validation: 0.445398451026226]
	TIME [epoch: 8.69 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38537114912697124		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.39964520995873304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39250817954285216 | validation: 0.35737159155666337]
	TIME [epoch: 8.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42163689067091736		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.39817021535656594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40990355301374154 | validation: 0.32971053629532454]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4878799113284539		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4623231456963386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47510152851239623 | validation: 0.5595712418419883]
	TIME [epoch: 8.66 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42310159553724774		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4089189959964675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41601029576685766 | validation: 0.41513264975242226]
	TIME [epoch: 8.66 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4217611872274455		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38017902436291706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4009701057951813 | validation: 0.47962791770985364]
	TIME [epoch: 8.66 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4274888090779423		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3151809279877632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3713348685328527 | validation: 0.4280239713791907]
	TIME [epoch: 8.68 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3601887303734729		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5313666560800121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4457776932267426 | validation: 0.3976144301096727]
	TIME [epoch: 8.66 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44675617593905004		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3785513058096255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41265374087433776 | validation: 0.3433677374491703]
	TIME [epoch: 8.65 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4370485514046091		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3205545072401853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37880152932239713 | validation: 0.23378645086123068]
	TIME [epoch: 8.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3123292855904101		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3118681761920061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3120987308912081 | validation: 0.4282540785718788]
	TIME [epoch: 8.68 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35108711209533067		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32713276376982536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33910993793257804 | validation: 0.5498349588273032]
	TIME [epoch: 8.7 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3272112345187258		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4645702504249224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3958907424718241 | validation: 0.24712825595992105]
	TIME [epoch: 8.66 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3463921474753876		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38411884111211125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3652554942937495 | validation: 0.3683000906567351]
	TIME [epoch: 8.66 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31447764785194143		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27870363744536497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2965906426486532 | validation: 0.28396717768985963]
	TIME [epoch: 8.67 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3249396392441981		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.376603656403106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3507716478236521 | validation: 0.24618422949933233]
	TIME [epoch: 8.68 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30028549444020347		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32186859022666475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31107704233343403 | validation: 0.23377290116781663]
	TIME [epoch: 8.65 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33591074574806573		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29394005535298634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.314925400550526 | validation: 0.3716433634174044]
	TIME [epoch: 8.66 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3273747621552645		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2694260917485314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29840042695189795 | validation: 0.3083725937583434]
	TIME [epoch: 8.66 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3100472850453847		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2954272465010057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30273726577319515 | validation: 0.2599224194948289]
	TIME [epoch: 8.69 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2586736174177947		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25028269473679926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25447815607729696 | validation: 0.5411852918512623]
	TIME [epoch: 8.66 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3616769325239682		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2821276983123318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32190231541815006 | validation: 0.31424362269204287]
	TIME [epoch: 8.66 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2630387012899586		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3165872635837982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28981298243687836 | validation: 0.24794100504992836]
	TIME [epoch: 8.65 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3099044192428386		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3283695261268461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3191369726848424 | validation: 0.19993765760631504]
	TIME [epoch: 8.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28928322403993284		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3975523821505137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3434178030952233 | validation: 0.20363010887396513]
	TIME [epoch: 8.69 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27172247727650334		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2599112910699748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26581688417323907 | validation: 0.2636030044709504]
	TIME [epoch: 8.67 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4122798066015275		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22658395590539157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31943188125345956 | validation: 0.3910787811030813]
	TIME [epoch: 8.68 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38851841789460007		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24518873421755522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31685357605607767 | validation: 0.17915172960013973]
	TIME [epoch: 8.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20092636043192108		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24549138076130764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22320887059661437 | validation: 0.4315290021436788]
	TIME [epoch: 8.71 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38841566547013534		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2351444671252388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3117800662976871 | validation: 0.18703439971659336]
	TIME [epoch: 8.68 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19726837627930724		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29206978596844035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2446690811238738 | validation: 0.3300438488048135]
	TIME [epoch: 8.7 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3523579121322683		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2788645983629828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31561125524762557 | validation: 0.4744469103673942]
	TIME [epoch: 8.7 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3197348755604029		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24891585556146953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2843253655609362 | validation: 0.19728297124932556]
	TIME [epoch: 8.72 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2600226011695752		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2515854238698294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2558040125197023 | validation: 0.36272361543758946]
	TIME [epoch: 8.7 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2821396546415259		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3018988600505023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.292019257346014 | validation: 0.32468113722382635]
	TIME [epoch: 8.68 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6513911064966037		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2786819019161519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4650365042063778 | validation: 0.31104356858075444]
	TIME [epoch: 8.7 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2993536998654406		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22342087031906738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.261387285092254 | validation: 0.18392290167307673]
	TIME [epoch: 8.69 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2612697256930065		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30878789154156755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28502880861728697 | validation: 0.37180356338241793]
	TIME [epoch: 8.7 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2701386406911822		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22308249796094617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24661056932606415 | validation: 0.27077173923246833]
	TIME [epoch: 8.69 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2140358984151053		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3475381756826636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2807870370488844 | validation: 1.3361669739043833]
	TIME [epoch: 8.7 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5288895415967103		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3400572266360618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43447338411638603 | validation: 0.3598091008557749]
	TIME [epoch: 8.71 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5160000038657978		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7146573467781205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6153286753219591 | validation: 1.5639932788246056]
	TIME [epoch: 8.72 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8588194356299151		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26807608448720666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5634477600585609 | validation: 0.21926223830374011]
	TIME [epoch: 8.69 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3171364079724942		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2601111782292639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28862379310087904 | validation: 0.335796613313169]
	TIME [epoch: 8.69 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23905735586516172		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22712812063445198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23309273824980684 | validation: 0.3797012154899417]
	TIME [epoch: 8.68 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3285537697779321		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5828877894710892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45572077962451074 | validation: 0.508996022678067]
	TIME [epoch: 8.71 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37417862177963096		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38929278175273957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38173570176618526 | validation: 0.2479072057615351]
	TIME [epoch: 8.7 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3308143508689799		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2937969658419787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3123056583554794 | validation: 0.5461199917670478]
	TIME [epoch: 8.68 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26609936631455067		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27408017634863857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2700897713315947 | validation: 0.23311333991265853]
	TIME [epoch: 8.68 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23981756276357605		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22028271838273517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2300501405731556 | validation: 0.14431090759955173]
	TIME [epoch: 8.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26005366966387955		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2029816148987095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2315176422812945 | validation: 0.314450658194831]
	TIME [epoch: 8.69 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23371112793573962		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.34024108860143143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28697610826858555 | validation: 0.42010822812500764]
	TIME [epoch: 8.68 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25429718575694066		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26452164291863556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25940941433778814 | validation: 0.15517418812022296]
	TIME [epoch: 8.69 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4188352475904689		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5388224967181976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47882887215433334 | validation: 0.36960054543076326]
	TIME [epoch: 8.67 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3147354561027834		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21825269576945852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26649407593612096 | validation: 0.21974151178008694]
	TIME [epoch: 8.7 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21010034091146376		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19903609920234494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2045682200569044 | validation: 0.15784590333917442]
	TIME [epoch: 8.68 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37732898249451313		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4207107049010771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39901984369779514 | validation: 0.3039038829069048]
	TIME [epoch: 8.69 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.500896959529649		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2314008372962241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36614889841293663 | validation: 0.1776892335551194]
	TIME [epoch: 8.68 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21250728823208248		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23163235005513277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2220698191436076 | validation: 0.2355953891661432]
	TIME [epoch: 8.69 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3203844352654515		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2309460606449075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27566524795517944 | validation: 0.26540300151003043]
	TIME [epoch: 8.7 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2431390116870118		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.35033157536046056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2967352935237362 | validation: 0.17847882369361118]
	TIME [epoch: 8.69 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22664170371385142		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30481005957146384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26572588164265765 | validation: 0.16657960624364465]
	TIME [epoch: 8.68 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2044531201255492		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2471723283171896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22581272422136944 | validation: 0.1379250576830925]
	TIME [epoch: 8.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2271997403401617		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28294297440110394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2550713573706328 | validation: 0.1775112897742482]
	TIME [epoch: 8.7 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45811138583035993		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3428908330214765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4005011094259182 | validation: 0.251484254678798]
	TIME [epoch: 8.68 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2590998728702244		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4109883412456713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33504410705794785 | validation: 0.27016304466359364]
	TIME [epoch: 8.69 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37689519658730186		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2778246170091186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32735990679821014 | validation: 0.20591137544620602]
	TIME [epoch: 8.69 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1715826408915776		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17411526651813697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17284895370485728 | validation: 0.21151107544023362]
	TIME [epoch: 8.73 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6611453638633762		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36230365271344933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5117245082884128 | validation: 0.30850620685667657]
	TIME [epoch: 8.7 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24848036381904054		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2727986969525023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26063953038577137 | validation: 0.2033388396275155]
	TIME [epoch: 8.69 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22085666832863443		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1935150773488974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20718587283876594 | validation: 0.20847761340067789]
	TIME [epoch: 8.7 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.250810020965871		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19253621271187055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2216731168388708 | validation: 0.18188142113297]
	TIME [epoch: 8.7 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26486250356194113		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3374984247490005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3011804641554708 | validation: 0.2831728808663307]
	TIME [epoch: 8.72 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3644754028460976		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30139227024733617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3329338365467169 | validation: 0.2634763196212101]
	TIME [epoch: 8.71 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3403704303665467		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23814082338346373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2892556268750052 | validation: 0.22874456584770714]
	TIME [epoch: 8.69 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19384384543607358		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2222379285173667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20804088697672013 | validation: 0.22696116673247288]
	TIME [epoch: 8.68 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17907817374144558		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1829904679128407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18103432082714316 | validation: 0.2553679904346332]
	TIME [epoch: 8.7 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2337915522779249		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2552275077922369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24450953003508086 | validation: 0.39470892824038095]
	TIME [epoch: 8.7 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43204031554497363		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20394737458941034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.317993845067192 | validation: 0.17465152901726844]
	TIME [epoch: 8.71 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.192390544756961		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19197474349361407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1921826441252875 | validation: 0.17124211471834633]
	TIME [epoch: 8.69 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39980854442273384		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23116612703211065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31548733572742227 | validation: 0.19249683288294064]
	TIME [epoch: 8.72 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2142728520481331		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2644836061255609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23937822908684697 | validation: 0.1441832850412775]
	TIME [epoch: 8.7 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17166140383558642		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36086677287390667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2662640883547465 | validation: 0.5283653338591642]
	TIME [epoch: 8.69 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2886150944753288		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25186839908345504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27024174677939183 | validation: 0.21888186006445612]
	TIME [epoch: 8.68 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.240172877364803		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1991291662394646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21965102180213378 | validation: 0.25339635009178757]
	TIME [epoch: 8.69 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17573074743963632		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24801205758950315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21187140251456973 | validation: 0.17540743479563067]
	TIME [epoch: 8.71 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1854556836677455		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19483068625840044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19014318496307298 | validation: 0.178018234198572]
	TIME [epoch: 8.7 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1768875006031755		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20184093191950536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18936421626134045 | validation: 0.1846735274158069]
	TIME [epoch: 8.7 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18635302237371326		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2774097302275007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23188137630060696 | validation: 0.33269517553170047]
	TIME [epoch: 8.7 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2740129300064036		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17146907782168247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22274100391404303 | validation: 0.32444833041687826]
	TIME [epoch: 8.7 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26651087339900503		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1942889883235356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23039993086127025 | validation: 0.20666950520636299]
	TIME [epoch: 8.69 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1759457978432012		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3242007043805337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2500732511118675 | validation: 0.15172314168461826]
	TIME [epoch: 8.68 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22905777766000684		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2596303813997708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24434407952988885 | validation: 0.1716713310365526]
	TIME [epoch: 8.68 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23531222829864276		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3040529295069544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2696825789027985 | validation: 0.17730668277146813]
	TIME [epoch: 8.7 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16485925810477395		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21052619346603038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18769272578540222 | validation: 0.2075013018230978]
	TIME [epoch: 8.7 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17942032489164322		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20966014947073588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19454023718118957 | validation: 0.15856653236024743]
	TIME [epoch: 8.68 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2254294737578956		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16857419662877876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19700183519333717 | validation: 0.1509668395828757]
	TIME [epoch: 8.69 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2858540132595149		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18460077389322843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2352273935763717 | validation: 0.20179474198729863]
	TIME [epoch: 8.68 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23921900499939572		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16756342138590283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20339121319264927 | validation: 0.11907232307745103]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20727280706036186		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36101396331670077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2841433851885314 | validation: 0.25155986492149207]
	TIME [epoch: 8.68 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39026859715285483		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24072780341669692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3154982002847759 | validation: 0.20261910763134156]
	TIME [epoch: 8.68 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23630223378976703		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29945763770426725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2678799357470171 | validation: 0.17786374829112733]
	TIME [epoch: 8.66 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29051001267199356		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27324160354325977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2818758081076267 | validation: 0.43804884732030225]
	TIME [epoch: 8.69 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21653502607660752		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17563700240000943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19608601423830851 | validation: 0.12256031660779178]
	TIME [epoch: 8.68 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14896342582344982		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2906337592866829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21979859255506637 | validation: 0.18335256223631696]
	TIME [epoch: 8.66 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18739574945853427		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2886056538096028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23800070163406847 | validation: 0.1919749711430126]
	TIME [epoch: 8.66 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.215417396960482		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1778613633228226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1966393801416523 | validation: 0.14088913748828719]
	TIME [epoch: 8.67 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16449468609993317		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1966275036709298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18056109488543148 | validation: 0.287614107108435]
	TIME [epoch: 8.68 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2373439661162567		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24958522122543733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24346459367084697 | validation: 0.27427577321590874]
	TIME [epoch: 8.66 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1967589169500407		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15518504912917927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17597198303960998 | validation: 0.1735552741834782]
	TIME [epoch: 8.67 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1600690603622914		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17187603504271204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1659725477025017 | validation: 0.13752913518092505]
	TIME [epoch: 8.68 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17202061768802657		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1840377292708915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17802917347945904 | validation: 0.3174441498972561]
	TIME [epoch: 8.68 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21529898029585906		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16391697894710738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18960797962148326 | validation: 0.16488467451586666]
	TIME [epoch: 8.66 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1596578745284027		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19634237735043666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17800012593941966 | validation: 0.22035778539761686]
	TIME [epoch: 8.66 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16110786304612534		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17964772640653004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1703777947263277 | validation: 0.2207560467677475]
	TIME [epoch: 8.67 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18287893159280283		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27180038335460277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22733965747370277 | validation: 0.1692769702483363]
	TIME [epoch: 8.68 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16673010375640504		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.232909749031564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19981992639398452 | validation: 0.15871180608651772]
	TIME [epoch: 8.69 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1754685201451943		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19071728012548717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18309290013534069 | validation: 0.22987000745299976]
	TIME [epoch: 8.66 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16817083626105286		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.242751091190665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20546096372585895 | validation: 0.30098731749349583]
	TIME [epoch: 8.68 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19926292604854792		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16176419302874745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1805135595386477 | validation: 0.11732138219023674]
	TIME [epoch: 8.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15839496663278374		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19462210326292897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17650853494785634 | validation: 0.1400444468908045]
	TIME [epoch: 8.71 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13518132511313913		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19200332356643077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1635923243397849 | validation: 0.14729363436799414]
	TIME [epoch: 8.69 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18508472096022993		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14481169447817996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16494820771920493 | validation: 0.13169290864979263]
	TIME [epoch: 8.67 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14725407491619172		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40948475564842746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2783694152823096 | validation: 0.19112698986714463]
	TIME [epoch: 8.67 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1749840673624608		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16287536144685677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16892971440465876 | validation: 0.15344010674860298]
	TIME [epoch: 8.7 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14686936180807106		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1665188059839355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15669408389600328 | validation: 0.22610326349283277]
	TIME [epoch: 8.68 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13543174138720335		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19689528186302205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16616351162511273 | validation: 0.1367905518613625]
	TIME [epoch: 8.66 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2266956325997478		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1797950655364991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20324534906812342 | validation: 0.2142552369193965]
	TIME [epoch: 8.68 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28969100279582277		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1561829825369126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22293699266636766 | validation: 0.14619137546289812]
	TIME [epoch: 8.69 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45214506115414493		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1788800333970805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3155125472756127 | validation: 0.1166548139464581]
	TIME [epoch: 8.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15128175695071577		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1756659349055733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1634738459281445 | validation: 0.1533437330355344]
	TIME [epoch: 8.69 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14483800822957787		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17271462556530542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15877631689744165 | validation: 0.19292564618695013]
	TIME [epoch: 8.69 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21008954212230774		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20179194350540217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20594074281385494 | validation: 0.12980416234762734]
	TIME [epoch: 8.69 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46220676329518245		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12628190561103034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29424433445310644 | validation: 0.16748530923044938]
	TIME [epoch: 8.72 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17910618656930324		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16390312794580902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17150465725755612 | validation: 0.14833736166562686]
	TIME [epoch: 8.67 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14507832465012166		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1356733301049363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.140375827377529 | validation: 0.0969000482438039]
	TIME [epoch: 8.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15719796000935737		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47269285848857984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31494540924896863 | validation: 0.23234099481057213]
	TIME [epoch: 8.71 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13381832239012198		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14514343563785298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13948087901398748 | validation: 0.14668241253530526]
	TIME [epoch: 8.72 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16350730868051816		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16155318428863458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16253024648457642 | validation: 0.13310587706754048]
	TIME [epoch: 8.71 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14865430201154034		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13917839229150303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14391634715152168 | validation: 0.09094542130824067]
	TIME [epoch: 8.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11076427444745571		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12156913767701887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1161667060622373 | validation: 0.17808563002696362]
	TIME [epoch: 8.69 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5283070676616817		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13790844325265694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33310775545716925 | validation: 0.16815184908545164]
	TIME [epoch: 8.69 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17710518513377		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12684454499397047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15197486506387026 | validation: 0.09253160081302252]
	TIME [epoch: 8.71 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17525205763936205		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10045413019993366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13785309391964784 | validation: 0.11009994734456366]
	TIME [epoch: 8.68 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1667857286621282		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15966709229574483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16322641047893652 | validation: 0.25989971376617166]
	TIME [epoch: 8.69 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19089745432867639		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23612031072076456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21350888252472044 | validation: 0.12793804509886184]
	TIME [epoch: 8.68 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14626375571071779		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1547344575402373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15049910662547755 | validation: 0.08334902900488736]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2817019819973978		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.166070032240886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22388600711914192 | validation: 0.12107654965150405]
	TIME [epoch: 8.7 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1736388845851446		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11116914766840813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14240401612677633 | validation: 0.1592606809862925]
	TIME [epoch: 8.7 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17464024968511688		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15858307408352373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1666116618843203 | validation: 0.14983582855953076]
	TIME [epoch: 8.71 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1499890458094993		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14108443191126102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14553673886038015 | validation: 0.09017656817346166]
	TIME [epoch: 8.73 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14437102447802405		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1921646263539739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.168267825415999 | validation: 0.11399107540364346]
	TIME [epoch: 8.7 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09968383860366667		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11002895742757945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10485639801562305 | validation: 0.15671683309402012]
	TIME [epoch: 8.68 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17665221738518072		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1190456586443444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14784893801476257 | validation: 0.10356870518663822]
	TIME [epoch: 8.69 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18108189644593525		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1743551401539063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17771851829992077 | validation: 0.09427854254265]
	TIME [epoch: 8.69 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13837290682786593		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18019020219068776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15928155450927686 | validation: 0.12221749098666414]
	TIME [epoch: 8.7 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12440861344319798		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12271273383972785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1235606736414629 | validation: 0.1481919502306144]
	TIME [epoch: 8.68 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14400787026168604		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12460870159794162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1343082859298138 | validation: 0.17772104287126236]
	TIME [epoch: 8.69 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17946891954574948		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2153607150632766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19741481730451302 | validation: 0.14951210787027364]
	TIME [epoch: 8.67 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15483632447897278		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1398433090617684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14733981677037056 | validation: 0.20782525054890302]
	TIME [epoch: 8.71 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17226870046092413		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27595942451094835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22411406248593627 | validation: 0.2723454081651874]
	TIME [epoch: 8.68 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6729770631474865		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1366610932208379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40481907818416224 | validation: 0.27535066061904795]
	TIME [epoch: 8.69 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17400562394950753		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13971195073886997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15685878734418873 | validation: 0.19768859529855673]
	TIME [epoch: 8.68 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16770237948416725		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14771533116020774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1577088553221875 | validation: 0.13670259890944397]
	TIME [epoch: 8.71 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15829579310882036		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1755091968979372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16690249500337875 | validation: 0.26397276905766726]
	TIME [epoch: 8.69 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15336016088110696		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1336323435232642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1434962522021856 | validation: 0.21905924252603545]
	TIME [epoch: 8.68 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.217101678078966		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15616739164900645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18663453486398623 | validation: 0.2834532972018564]
	TIME [epoch: 8.69 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29079073826228224		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16816983760695722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22948028793461975 | validation: 0.1516969198153381]
	TIME [epoch: 8.71 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14961395049050225		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1357055100434189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14265973026696058 | validation: 0.11396911994500222]
	TIME [epoch: 8.7 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16128875582257646		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18833326112705245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17481100847481446 | validation: 0.2164052032458737]
	TIME [epoch: 8.7 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16571800484135094		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17064748041514294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1681827426282469 | validation: 0.14511881407038585]
	TIME [epoch: 8.69 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22321290135040045		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12989753757571665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17655521946305858 | validation: 0.1553121219513361]
	TIME [epoch: 8.68 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17063082866708765		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1087008413810671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13966583502407742 | validation: 0.28340643581867053]
	TIME [epoch: 8.7 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16797119036502217		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4485394526026399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30825532148383095 | validation: 0.14730810733100258]
	TIME [epoch: 8.68 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19191930063780843		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27593282456086937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2339260625993389 | validation: 0.3547116514633618]
	TIME [epoch: 8.67 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17769839233683896		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20906382322761177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1933811077822254 | validation: 0.1158028807134904]
	TIME [epoch: 8.68 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13718546194244247		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15069019393610092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1439378279392717 | validation: 0.17949254938274956]
	TIME [epoch: 8.71 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25129694065662106		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15682110105508684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20405902085585392 | validation: 0.11011647969319292]
	TIME [epoch: 8.68 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16164864584625038		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17303035561232602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16733950072928822 | validation: 0.15893999965035077]
	TIME [epoch: 8.69 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11882362897599812		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1426447040069613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1307341664914797 | validation: 0.11672138569237515]
	TIME [epoch: 8.68 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2513163600483827		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09765086780748902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17448361392793582 | validation: 0.1556106838374629]
	TIME [epoch: 8.69 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15408397990718858		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15011483556365599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15209940773542227 | validation: 0.10034595886190381]
	TIME [epoch: 8.7 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12814184865022563		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14185565184499116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13499875024760838 | validation: 0.34248063203849965]
	TIME [epoch: 8.69 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17553739021185039		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13806050031011563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.156798945260983 | validation: 0.1022604880389156]
	TIME [epoch: 8.69 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12781746599857297		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15285877817523227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1403381220869026 | validation: 0.14198992515286818]
	TIME [epoch: 8.68 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1326928358006063		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13566989529269197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13418136554664914 | validation: 0.1152473931550525]
	TIME [epoch: 8.7 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20259342905042116		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11637180492563251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15948261698802685 | validation: 0.11653780640017476]
	TIME [epoch: 8.68 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1323232747612631		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17032296691059595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15132312083592953 | validation: 0.1648557211896024]
	TIME [epoch: 8.68 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20056558339127095		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1465117002765256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1735386418338983 | validation: 0.20209364775447974]
	TIME [epoch: 8.69 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15181063288368893		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15245819282541195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15213441285455045 | validation: 0.14999318712784185]
	TIME [epoch: 8.71 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19414923298889003		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20165018118561945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19789970708725474 | validation: 0.5763826522295141]
	TIME [epoch: 8.72 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39263225672338214		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27336254496991996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3329974008466511 | validation: 0.2131990314189613]
	TIME [epoch: 8.71 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18305687389461386		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11844710693119687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15075199041290538 | validation: 0.2311557139056495]
	TIME [epoch: 8.71 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13317579409154348		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12385422560343848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12851500984749098 | validation: 0.18355590543473463]
	TIME [epoch: 8.69 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17266834459968766		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1419960905188271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1573322175592574 | validation: 0.12327368013856838]
	TIME [epoch: 8.74 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17251347552362567		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1781986983799673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17535608695179647 | validation: 0.1766426063207414]
	TIME [epoch: 8.7 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13460491173850953		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14901855395339728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14181173284595341 | validation: 0.11950300934632738]
	TIME [epoch: 8.7 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3207779666431999		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6148900321871642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46783399941518206 | validation: 0.14448430258892203]
	TIME [epoch: 8.69 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1400923367031216		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19067747740743551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16538490705527856 | validation: 0.09317556342424513]
	TIME [epoch: 8.71 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24395933394734107		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3136652649839876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2788122994656644 | validation: 0.28938928658015506]
	TIME [epoch: 8.69 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1429975856687355		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16393891598737434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15346825082805488 | validation: 0.2327982494798142]
	TIME [epoch: 8.68 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15870972890380247		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14122884814366848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1499692885237355 | validation: 0.08522210116429114]
	TIME [epoch: 8.69 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10646714501556087		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20690901534480965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15668808018018524 | validation: 0.2067880821887512]
	TIME [epoch: 8.7 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18172356882106128		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19809564064657192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18990960473381654 | validation: 0.20909203778277036]
	TIME [epoch: 8.7 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24265196044745047		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14245770877028557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19255483460886802 | validation: 0.18732788602972958]
	TIME [epoch: 8.68 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11858766491238029		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1750355027699588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14681158384116957 | validation: 0.116203994347352]
	TIME [epoch: 8.68 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19005156910819312		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1354200794975533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1627358243028732 | validation: 0.1384164476662897]
	TIME [epoch: 8.68 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2206483764328575		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1677011828939443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1941747796634009 | validation: 0.1580427784848014]
	TIME [epoch: 8.72 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17426531582496924		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1972914092105808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.185778362517775 | validation: 0.36582963569535715]
	TIME [epoch: 8.67 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17932754902376086		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10714228612570187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14323491757473134 | validation: 0.10856509056206796]
	TIME [epoch: 8.69 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12959287657192717		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1525699390071364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14108140778953177 | validation: 0.09687176245980479]
	TIME [epoch: 8.67 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15954955566238702		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4103907767223795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28497016619238325 | validation: 0.21641450744847396]
	TIME [epoch: 8.71 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17654628195833005		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1202292236427839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14838775280055697 | validation: 0.12938854327134952]
	TIME [epoch: 8.71 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19498542121118737		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32541939609118875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2602024086511881 | validation: 0.12404144612651598]
	TIME [epoch: 8.69 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14562251700578166		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12300414989809608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13431333345193888 | validation: 0.10054443022429414]
	TIME [epoch: 8.68 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12133151561030933		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1364652078755641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12889836174293673 | validation: 0.1036127071195953]
	TIME [epoch: 8.69 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14376929538851727		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3019309875008339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22285014144467558 | validation: 0.1385389497999843]
	TIME [epoch: 8.7 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15084961536033262		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1406755911220559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14576260324119422 | validation: 0.0943285710195696]
	TIME [epoch: 8.69 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11979067809899732		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14663645380990956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13321356595445344 | validation: 0.15942515244668873]
	TIME [epoch: 8.68 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1340754242471409		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1327272226393896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13340132344326525 | validation: 0.13058111461363806]
	TIME [epoch: 8.67 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11048278806234924		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.31609671583380716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21328975194807823 | validation: 0.261317294540874]
	TIME [epoch: 8.7 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43680431363647154		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2287258449801278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3327650793082996 | validation: 0.15155379577687672]
	TIME [epoch: 8.67 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1293769843502443		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11146081760717193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12041890097870814 | validation: 0.11247277179050943]
	TIME [epoch: 8.66 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1444950056222448		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13790212856162892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14119856709193684 | validation: 0.11152901706657613]
	TIME [epoch: 8.66 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14181888056655695		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1978088611707312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16981387086864408 | validation: 0.10166292700544483]
	TIME [epoch: 8.68 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08070083658821961		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10928440030003177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0949926184441257 | validation: 0.08720083521651623]
	TIME [epoch: 8.68 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13621646867867462		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1581096298846787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14716304928167667 | validation: 0.21781039745196484]
	TIME [epoch: 8.67 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15854135952685408		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16367324133848127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16110730043266766 | validation: 0.11168464522650028]
	TIME [epoch: 8.67 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1293840190390808		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1470108282444053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1381974236417431 | validation: 0.202903571385794]
	TIME [epoch: 8.68 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20838385744942464		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17557781918878307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19198083831910387 | validation: 0.08908917011851539]
	TIME [epoch: 8.72 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1342959775632359		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18476641919473907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1595311983789875 | validation: 0.1918004284045652]
	TIME [epoch: 8.7 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13664605350818587		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12770792475409404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13217698913114 | validation: 0.13052993872275692]
	TIME [epoch: 8.69 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14096745758419238		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14379108780126643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1423792726927294 | validation: 0.20375867269238307]
	TIME [epoch: 8.71 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1787857539911824		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2252531538703026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2020194539307425 | validation: 0.10671868924312003]
	TIME [epoch: 8.72 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1031118354277284		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16656831290817986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13484007416795413 | validation: 0.10941099658258839]
	TIME [epoch: 8.7 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14247846741344988		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1568706882971729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14967457785531135 | validation: 0.12494013465825587]
	TIME [epoch: 8.7 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1240489690589545		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17513757886901762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14959327396398608 | validation: 0.30842082187650616]
	TIME [epoch: 8.7 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3789424158678165		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3221801524193492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3505612841435829 | validation: 0.2734418355201983]
	TIME [epoch: 8.71 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12879354647224978		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11047448113501965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11963401380363473 | validation: 0.16708209513644867]
	TIME [epoch: 8.72 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14705172051914492		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14136935246697782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14421053649306134 | validation: 0.12443724413519956]
	TIME [epoch: 8.7 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10467232880023476		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16923831745622203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1369553231282284 | validation: 0.25280750076872327]
	TIME [epoch: 8.69 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18445643828290065		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11533017816471398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14989330822380728 | validation: 0.11746696596496131]
	TIME [epoch: 8.69 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11851079628043368		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11488409599348895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11669744613696134 | validation: 0.09643034110482288]
	TIME [epoch: 8.73 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12621924401894968		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12427104044162152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12524514223028554 | validation: 0.1413938725554273]
	TIME [epoch: 8.69 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17426867946414035		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13253703510557793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1534028572848591 | validation: 0.09571074628317944]
	TIME [epoch: 8.71 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12019892753731962		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13441611314400895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12730752034066425 | validation: 0.09968199935866642]
	TIME [epoch: 8.7 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.148043975157312		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16753522987478428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15778960251604815 | validation: 0.16382712291301113]
	TIME [epoch: 8.73 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3098119012014814		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2917449462601117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30077842373079655 | validation: 0.15244391745022395]
	TIME [epoch: 8.72 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16454310879673767		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2359635040198389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2002533064082883 | validation: 0.21256854762460214]
	TIME [epoch: 8.69 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29533678155861254		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4406716370380065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3680042092983095 | validation: 0.7658645605237526]
	TIME [epoch: 8.71 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3990362497734873		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1636382868025795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2813372682880334 | validation: 0.09689135698979495]
	TIME [epoch: 8.71 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1732603438625035		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15228961121609313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16277497753929832 | validation: 0.38308158049061114]
	TIME [epoch: 8.73 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20435195639305737		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45712416969388875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33073806304347303 | validation: 1.0264498870801217]
	TIME [epoch: 8.71 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29409242112352035		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12475514456325924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2094237828433898 | validation: 0.10253816908122805]
	TIME [epoch: 8.7 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1188473800622886		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10947367156005054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11416052581116957 | validation: 0.19314140768662824]
	TIME [epoch: 8.71 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12555084564963276		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11094012693358472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11824548629160876 | validation: 0.10859588308228754]
	TIME [epoch: 8.75 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10665536807936771		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12953363108620145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11809449958278459 | validation: 0.18480079755716586]
	TIME [epoch: 8.72 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2073891368238534		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15616970224867444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18177941953626392 | validation: 0.09005877769383497]
	TIME [epoch: 8.72 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11120274931652283		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14351689443119997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12735982187386138 | validation: 0.2007285043266948]
	TIME [epoch: 8.72 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19855010358282735		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17327319664999472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18591165011641106 | validation: 0.1535679441608136]
	TIME [epoch: 8.76 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17307645340134015		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09304779201506833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1330621227082042 | validation: 0.08304782433662573]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10956286926559702		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1003212756276399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10494207244661846 | validation: 0.15368079369436244]
	TIME [epoch: 8.73 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1837382831068996		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12880894263373788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15627361287031874 | validation: 0.08143324668182014]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09917416902567254		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14072797367323936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11995107134945596 | validation: 0.16331598002104056]
	TIME [epoch: 8.77 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14174689484422123		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0974735505185554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11961022268138832 | validation: 0.07767243491340427]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14661722848843714		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26883804714170545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20772763781507134 | validation: 0.1528931779754195]
	TIME [epoch: 8.74 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1023616937270719		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.152573209667111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12746745169709145 | validation: 0.16293124382567237]
	TIME [epoch: 8.74 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11064359146406921		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12563596991318304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11813978068862614 | validation: 0.22155806048740323]
	TIME [epoch: 8.74 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15810296311246297		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16374281589896367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1609228895057133 | validation: 0.08126076305483121]
	TIME [epoch: 8.76 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.166608062806599		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13803402778162072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1523210452941099 | validation: 0.096955149300316]
	TIME [epoch: 8.73 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13628140940683958		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1960793856753668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16618039754110317 | validation: 0.2998939021498703]
	TIME [epoch: 8.74 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30371327666141984		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17427433872156523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2389938076914925 | validation: 0.0897614247840782]
	TIME [epoch: 8.74 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08146326492030988		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08967822534588246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08557074513309619 | validation: 0.08463423726655842]
	TIME [epoch: 8.78 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.157023885964842		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10859012299311617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13280700447897906 | validation: 0.0873712546705975]
	TIME [epoch: 8.75 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2253186878346035		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19215690825611365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20873779804535855 | validation: 0.117020815188272]
	TIME [epoch: 8.74 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10007067183608605		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1407946849292659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12043267838267598 | validation: 0.13141135292324693]
	TIME [epoch: 8.75 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22562975912203553		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12700420977903615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17631698445053587 | validation: 0.13089827001410603]
	TIME [epoch: 8.74 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16332196848790895		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11203122427362736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13767659638076818 | validation: 0.09543026338322715]
	TIME [epoch: 8.76 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13797697876193377		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18472004507891393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16134851192042382 | validation: 0.15711700216866736]
	TIME [epoch: 8.74 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11514859935415171		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07780397181433907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0964762855842454 | validation: 0.076396955199675]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11738634971669828		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07839045913596504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09788840442633165 | validation: 0.05808727710424769]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13880688631775617		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1670042061441072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15290554623093167 | validation: 0.23145856735421919]
	TIME [epoch: 8.73 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12133008186817577		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11708785656517451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11920896921667512 | validation: 0.12781289214393368]
	TIME [epoch: 8.74 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10077881272615893		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1535038395322912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12714132612922507 | validation: 0.28983808078379636]
	TIME [epoch: 8.73 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24103946935803977		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08800749036620106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1645234798621204 | validation: 0.09412487841470193]
	TIME [epoch: 8.71 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09990171501390883		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2135844482440062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15674308162895748 | validation: 0.14090501840381084]
	TIME [epoch: 8.76 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1446229726663058		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22695338071425616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18578817669028097 | validation: 0.5836515611845317]
	TIME [epoch: 8.73 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.611319143810633		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38287596868167334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49709755624615315 | validation: 0.19038634336463256]
	TIME [epoch: 8.72 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17718895052593323		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15032815093649804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16375855073121565 | validation: 0.11343614725767734]
	TIME [epoch: 8.72 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15535326392354915		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21280869934798127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1840809816357652 | validation: 0.0951297787198758]
	TIME [epoch: 8.73 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0947677550774777		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12919810054527128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1119829278113745 | validation: 0.09744626789674259]
	TIME [epoch: 8.74 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08202447792442766		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0854478153357814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08373614663010452 | validation: 0.11708257825845717]
	TIME [epoch: 8.74 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11642982334697297		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12863125658643335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12253053996670316 | validation: 0.42035617972070216]
	TIME [epoch: 8.7 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27415914707857786		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1646935006977839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21942632388818092 | validation: 0.08106911372891268]
	TIME [epoch: 8.7 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11057867469571439		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10138411001128309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10598139235349872 | validation: 0.10171575365886412]
	TIME [epoch: 8.73 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1305095829663314		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.330659611656893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2305845973116122 | validation: 0.1313130449641588]
	TIME [epoch: 8.71 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1184273153104346		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10497026240711498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11169878885877482 | validation: 0.11415812103864237]
	TIME [epoch: 8.7 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10569401462469044		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08926322174599115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09747861818534079 | validation: 0.1710780925079155]
	TIME [epoch: 8.72 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13877315065111184		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10058519346165551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11967917205638368 | validation: 0.10287737181550322]
	TIME [epoch: 8.72 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08890256912859455		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1208814053868128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1048919872577037 | validation: 0.08614908495773031]
	TIME [epoch: 8.72 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12281982517369375		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09818628269556992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11050305393463185 | validation: 0.11512313457348691]
	TIME [epoch: 8.71 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10847188046813247		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10152454696017048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10499821371415148 | validation: 0.0865445934880954]
	TIME [epoch: 8.7 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08218903099093398		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12174860635433034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10196881867263216 | validation: 0.12304166309737977]
	TIME [epoch: 8.72 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12288668423505714		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10187575657315162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11238122040410436 | validation: 0.40961744953002865]
	TIME [epoch: 8.75 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19368507097897855		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14324301660798933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16846404379348395 | validation: 0.1489725573321647]
	TIME [epoch: 8.71 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15332166981350098		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20607593706207913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17969880343779004 | validation: 0.11772577625451075]
	TIME [epoch: 8.74 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22506187534918348		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09454532774150895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15980360154534626 | validation: 0.13784018901140188]
	TIME [epoch: 8.73 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15189910285879493		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16937445719307986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16063678002593737 | validation: 0.1304894116228556]
	TIME [epoch: 8.74 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14356772688571445		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1778004163791566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16068407163243553 | validation: 0.12841979247231178]
	TIME [epoch: 8.72 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10382659718819805		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09186769262024128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09784714490421967 | validation: 0.09335618854939629]
	TIME [epoch: 8.71 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10031342184028555		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09102452439957909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09566897311993233 | validation: 0.12725469160365369]
	TIME [epoch: 8.72 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10642974267713501		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1780785966675885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14225416967236176 | validation: 0.49916089519988865]
	TIME [epoch: 8.72 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23599331632669243		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09658994122713058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1662916287769115 | validation: 0.08190281315962004]
	TIME [epoch: 8.76 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10018073822771767		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09349593072612626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09683833447692196 | validation: 0.09506791123758863]
	TIME [epoch: 8.74 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14809637216726598		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0824038592612614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11525011571426372 | validation: 0.14276816447173835]
	TIME [epoch: 8.72 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16311595944862073		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09101434304761752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12706515124811912 | validation: 0.15746635955543395]
	TIME [epoch: 8.74 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09790042725223992		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09253644628722868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09521843676973432 | validation: 0.05346668391732873]
	TIME [epoch: 8.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07868581489739096		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.44326214258779706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.260973978742594 | validation: 0.373290918951253]
	TIME [epoch: 8.74 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22120914602457636		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10457366174228593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1628914038834311 | validation: 0.09587360745613793]
	TIME [epoch: 8.73 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12403130742209398		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1247497081287233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12439050777540864 | validation: 0.20394387791689902]
	TIME [epoch: 8.73 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1341988937109635		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1108168460252712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12250786986811738 | validation: 0.10735250570620264]
	TIME [epoch: 8.76 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11289444472371768		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09960286319262422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10624865395817096 | validation: 0.07753218543268964]
	TIME [epoch: 8.75 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09992000042393301		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.31544729933519716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20768364987956506 | validation: 0.27866645839396026]
	TIME [epoch: 8.75 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20012455120963707		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09696417614305067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1485443636763439 | validation: 0.09084818457649349]
	TIME [epoch: 8.76 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3440211253379806		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15125743214666668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2476392787423237 | validation: 0.16276537985211492]
	TIME [epoch: 8.75 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11966264795847334		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09894827775497152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10930546285672242 | validation: 0.12465525729185982]
	TIME [epoch: 8.75 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10011465738858441		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.130270632562495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11519264497553969 | validation: 0.23315892513117695]
	TIME [epoch: 8.72 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1783614870667845		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11334490233447683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14585319470063068 | validation: 0.10984147020480686]
	TIME [epoch: 8.76 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2222225447658289		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1100525680109429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1661375563883859 | validation: 0.08376617711931385]
	TIME [epoch: 8.73 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1007591352381307		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10801445312573524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10438679418193295 | validation: 0.10787968849787373]
	TIME [epoch: 8.74 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09494250021131498		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10013442604087094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09753846312609295 | validation: 0.3042770727330908]
	TIME [epoch: 8.72 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0995081348255735		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10967214448601377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10459013965579363 | validation: 0.27090985292156533]
	TIME [epoch: 8.73 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11425482031734033		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.180011079236661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14713294977700067 | validation: 0.20328552532484803]
	TIME [epoch: 8.72 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.184288174168245		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08805472468811429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13617144942817966 | validation: 0.07987581513800382]
	TIME [epoch: 8.74 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09397838416387712		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15101995800995197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12249917108691452 | validation: 0.07684298127286426]
	TIME [epoch: 8.71 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08885039331742187		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15047083152661223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11966061242201702 | validation: 0.17506364425023835]
	TIME [epoch: 8.73 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13903196275421065		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09209397051456687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11556296663438874 | validation: 0.09917603778923646]
	TIME [epoch: 8.86 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15444548965086777		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08644846409288004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12044697687187392 | validation: 0.07769567726380253]
	TIME [epoch: 8.74 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07904526523239988		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16685815691594924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12295171107417455 | validation: 0.10411772210261061]
	TIME [epoch: 8.74 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08459787506048663		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10948148757745109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09703968131896887 | validation: 0.05790882100968299]
	TIME [epoch: 8.74 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10883610117550768		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.083842696639789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09633939890764834 | validation: 0.07437458157491586]
	TIME [epoch: 8.74 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09254960557077871		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1732728153577145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13291121046424662 | validation: 0.18943761763168002]
	TIME [epoch: 8.74 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12935069317005254		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20050139867291056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16492604592148155 | validation: 0.3664805480842193]
	TIME [epoch: 8.78 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26112029214840826		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11515853583870166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.188139413993555 | validation: 0.09238108664645706]
	TIME [epoch: 8.75 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07774193090300865		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13613543181102386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10693868135701626 | validation: 0.15460071827989416]
	TIME [epoch: 8.74 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1280362073821309		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10183445319553686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11493533028883388 | validation: 0.0846781327194055]
	TIME [epoch: 8.74 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10935137966735546		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10463148653981318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10699143310358432 | validation: 0.06605041552126245]
	TIME [epoch: 8.79 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25626351937005853		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15377106864462478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20501729400734164 | validation: 0.16386655225493138]
	TIME [epoch: 8.75 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10713423078116605		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09237220484478544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09975321781297572 | validation: 0.1164898044405333]
	TIME [epoch: 8.75 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.118253244171585		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11766657508071396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11795990962614948 | validation: 0.1822241247286961]
	TIME [epoch: 8.74 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15748462990779258		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09949858085919999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1284916053834963 | validation: 0.284014038665]
	TIME [epoch: 8.76 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14948513728851384		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1439930063324956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14673907181050472 | validation: 0.1242338145268243]
	TIME [epoch: 8.77 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18188502011676963		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13842095234911572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1601529862329427 | validation: 0.08456299378896805]
	TIME [epoch: 8.75 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08850467097886361		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15869447666470649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12359957382178503 | validation: 0.3277717067756994]
	TIME [epoch: 8.74 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29865560417427445		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12469074550524209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21167317483975828 | validation: 0.09747868337318866]
	TIME [epoch: 8.74 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1391325289879537		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09941092033808645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11927172466302008 | validation: 0.08270942424037889]
	TIME [epoch: 8.78 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10785163204707915		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10529898818686463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1065753101169719 | validation: 0.032544185211368634]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11962672050875242		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06622122379048967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09292397214962103 | validation: 0.08614388639076409]
	TIME [epoch: 8.68 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14067689609298784		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10127995078131895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12097842343715337 | validation: 0.11399922286870466]
	TIME [epoch: 8.68 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.126398777670983		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40904241938364283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26772059852731295 | validation: 0.2299149975006745]
	TIME [epoch: 8.71 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15140673179943404		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1194059458627302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13540633883108213 | validation: 0.1264331727160452]
	TIME [epoch: 8.69 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26441810691745776		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13225755729607735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1983378321067676 | validation: 0.10625652226163702]
	TIME [epoch: 8.69 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1031135896984251		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.34261025465231526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22286192217537018 | validation: 0.5847948784287109]
	TIME [epoch: 8.69 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22631020998060153		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10011003935578255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16321012466819207 | validation: 0.09423360745027215]
	TIME [epoch: 8.67 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11392641971279946		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1348811964918633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12440380810233136 | validation: 0.13890720554975383]
	TIME [epoch: 8.71 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18539058052772295		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13880935565407573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16209996809089938 | validation: 0.09206868689176684]
	TIME [epoch: 8.68 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12586789049349303		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14903532659532986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13745160854441144 | validation: 0.1640145423316395]
	TIME [epoch: 8.68 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13967171402194928		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23961490230845475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18964330816520197 | validation: 0.10068620659798518]
	TIME [epoch: 8.68 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09322702561961842		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1076919836385769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1004595046290977 | validation: 0.10880261413769965]
	TIME [epoch: 8.72 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09519988385121692		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1576671365813333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12643351021627508 | validation: 0.06654433486766732]
	TIME [epoch: 8.68 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09640397266625608		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14133033182081553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11886715224353583 | validation: 0.11458517995866374]
	TIME [epoch: 8.68 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.184153740972065		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1230402692863907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15359700512922783 | validation: 0.10512819589388149]
	TIME [epoch: 8.67 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14068894355530356		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09878487653665262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11973691004597811 | validation: 0.0782604067016809]
	TIME [epoch: 8.69 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08484314408331027		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1142169047127896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09953002439804993 | validation: 0.11588818391828512]
	TIME [epoch: 8.7 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09271887272718657		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09574482361341578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09423184817030117 | validation: 0.0713531274992569]
	TIME [epoch: 8.68 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08484930964553918		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08006495865186587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08245713414870251 | validation: 0.06365386111411767]
	TIME [epoch: 8.68 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07259347776507695		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1465639655886306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10957872167685376 | validation: 0.3031915685710418]
	TIME [epoch: 8.67 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2877942504064663		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4930363846002222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3904153175033442 | validation: 0.24572709307336416]
	TIME [epoch: 8.7 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17965611235759385		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1867552535275845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18320568294258913 | validation: 0.13761975185457934]
	TIME [epoch: 8.68 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07740020565878089		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09427236673661768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08583628619769927 | validation: 0.24127452318123493]
	TIME [epoch: 8.67 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1332275629839738		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10947849397244631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12135302847821008 | validation: 0.12953753670654578]
	TIME [epoch: 8.68 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1318289546956654		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09662037583833025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11422466526699782 | validation: 0.05665885336574478]
	TIME [epoch: 8.69 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09807060718049836		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09514449761823299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09660755239936572 | validation: 0.11319708296574532]
	TIME [epoch: 8.7 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09775093265808169		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11278093446945414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10526593356376793 | validation: 0.09151891263262883]
	TIME [epoch: 8.68 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08277396861886767		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10969711691647906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09623554276767336 | validation: 0.08437699271070179]
	TIME [epoch: 8.76 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11424310791824495		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13554161876829213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12489236334326852 | validation: 0.08512917759007879]
	TIME [epoch: 8.67 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09335466587512636		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19975117598542314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14655292093027475 | validation: 0.14133350933880084]
	TIME [epoch: 8.72 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11205307222314695		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3513948514363634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23172396182975522 | validation: 0.2163152033794258]
	TIME [epoch: 8.68 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1296226798299976		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09272505416177701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11117386699588727 | validation: 0.0993978130845001]
	TIME [epoch: 8.67 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10303363572176716		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10527077135326228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10415220353751471 | validation: 0.08618785138202469]
	TIME [epoch: 8.68 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09122002393933878		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11411223861087098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10266613127510489 | validation: 0.04671382050577912]
	TIME [epoch: 8.7 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08402720322849919		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06965049313141275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07683884817995597 | validation: 0.11804517460910571]
	TIME [epoch: 8.68 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0960519571530857		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2640811568830386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18006655701806212 | validation: 0.17091010763313133]
	TIME [epoch: 8.66 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13446188698286027		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09100685965005062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11273437331645544 | validation: 0.10830273147205836]
	TIME [epoch: 8.67 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10751182795737894		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11083241312941823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10917212054339857 | validation: 0.08499597979757552]
	TIME [epoch: 8.69 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11187178334804519		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18191122985499156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14689150660151837 | validation: 0.17127573468635407]
	TIME [epoch: 8.69 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10363390557999556		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07352793767545232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08858092162772394 | validation: 0.07783822790283353]
	TIME [epoch: 8.67 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10068273410038753		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2926997573353476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19669124571786756 | validation: 0.47002091733087287]
	TIME [epoch: 8.69 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23374610602111973		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09792563309869132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16583586955990554 | validation: 0.09215671184758617]
	TIME [epoch: 8.68 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12135522707530558		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10373742364283664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11254632535907114 | validation: 0.08648389491542699]
	TIME [epoch: 8.71 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07372947743314337		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09299023825045402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08335985784179868 | validation: 0.07464940200693038]
	TIME [epoch: 8.69 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08942378880596391		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11437339106572104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10189858993584247 | validation: 0.1369983355442832]
	TIME [epoch: 8.68 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07202845797212173		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08368823211096263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07785834504154218 | validation: 0.07351015882173005]
	TIME [epoch: 8.68 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12343278469518178		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14284783952014188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1331403121076618 | validation: 0.08067216380486053]
	TIME [epoch: 8.7 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09940532390771531		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07954503076292835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08947517733532183 | validation: 0.07978832156298248]
	TIME [epoch: 8.69 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0893056837588829		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09259636361087514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09095102368487903 | validation: 0.06453422932697572]
	TIME [epoch: 8.68 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09279634013497047		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08089560358885955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.086845971861915 | validation: 0.06467665127904237]
	TIME [epoch: 8.67 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11500634238984427		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18398965546615514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1494979989279997 | validation: 0.30606854660272853]
	TIME [epoch: 8.69 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11646975725869302		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08501193900768701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10074084813319002 | validation: 0.07794287899381004]
	TIME [epoch: 8.7 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08220268870536207		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13002149068448793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10611208969492499 | validation: 0.3188944952707329]
	TIME [epoch: 8.68 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21076866794847104		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1489725038301111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17987058588929106 | validation: 0.1321824347792065]
	TIME [epoch: 8.69 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09837761423590391		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08607955529465668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09222858476528029 | validation: 0.07967607691357731]
	TIME [epoch: 8.68 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09160307751897899		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16746806483478888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1295355711768839 | validation: 0.13487608936082415]
	TIME [epoch: 8.71 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10723456410701382		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2668177141379519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18702613912248287 | validation: 0.483157682071911]
	TIME [epoch: 8.68 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19934993184255514		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10653635855262036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1529431451975877 | validation: 0.14045501952786957]
	TIME [epoch: 8.69 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10334711105894881		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3468638873232296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2251054991910892 | validation: 0.648891476952485]
	TIME [epoch: 8.69 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2772691047186925		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1611667409362803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21921792282748637 | validation: 0.08193184946981319]
	TIME [epoch: 8.71 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14495980268713998		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08971042203243576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11733511235978789 | validation: 0.3005971682375619]
	TIME [epoch: 8.73 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18086658115252022		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11434432626291668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14760545370771844 | validation: 0.12401383052830467]
	TIME [epoch: 8.69 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08782989031216495		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11832637996095467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10307813513655983 | validation: 0.15638956288384018]
	TIME [epoch: 8.67 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14911531158730185		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09633129547859143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12272330353294658 | validation: 0.07915118569330869]
	TIME [epoch: 8.69 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07298292587395414		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10370531616701142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08834412102048277 | validation: 0.4451523392355812]
	TIME [epoch: 8.71 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18519353743018646		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09366843158937063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1394309845097786 | validation: 0.09293113957863028]
	TIME [epoch: 8.7 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07234751000202033		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07890341111943175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07562546056072604 | validation: 0.08506632444541175]
	TIME [epoch: 8.69 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08516817931926328		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08173635164804183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08345226548365255 | validation: 0.14765449700994077]
	TIME [epoch: 8.7 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14016855688670055		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12142996601479109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1307992614507458 | validation: 0.09830681493176086]
	TIME [epoch: 8.72 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1375635912817435		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11581172424956668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1266876577656551 | validation: 0.08937467022780367]
	TIME [epoch: 8.69 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07378156609028311		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09794103719088634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08586130164058473 | validation: 0.0762170738787424]
	TIME [epoch: 8.7 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07841952308049668		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17835478254431636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12838715281240654 | validation: 0.09648782009750728]
	TIME [epoch: 8.7 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14100984462782723		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12659452387670284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13380218425226503 | validation: 0.10547237510832119]
	TIME [epoch: 8.71 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10253029566698657		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15638264126771748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.129456468467352 | validation: 0.055850137183967245]
	TIME [epoch: 8.71 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07659854024830516		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1120535159513661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09432602809983562 | validation: 0.08722103951408655]
	TIME [epoch: 8.68 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12611932979538595		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0987772151231597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11244827245927284 | validation: 0.06539624314592968]
	TIME [epoch: 8.69 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09416621037775111		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08649906743797117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09033263890786111 | validation: 0.10904672023031725]
	TIME [epoch: 8.68 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10149113204598689		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10847618676125981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10498365940362335 | validation: 0.09243969496418425]
	TIME [epoch: 8.73 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10613404682391403		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10388741318247363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10501073000319386 | validation: 0.05799981146780846]
	TIME [epoch: 8.69 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10145665439188581		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07576666229570836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08861165834379708 | validation: 0.0908174939513963]
	TIME [epoch: 8.69 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1032462686409074		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17482724306268996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13903675585179864 | validation: 0.1457204981582388]
	TIME [epoch: 8.69 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10804895215063089		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09448296184443439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10126595699753263 | validation: 0.10402738528929864]
	TIME [epoch: 8.71 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1276054200674384		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08996548844068755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10878545425406298 | validation: 0.22820183034211544]
	TIME [epoch: 8.69 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24206729484896092		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08488117605307743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1634742354510192 | validation: 0.09436859517128096]
	TIME [epoch: 8.68 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1011322842936733		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11154513711656507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10633871070511917 | validation: 0.07926649172923507]
	TIME [epoch: 8.68 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11224251229317299		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09114342531138583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10169296880227943 | validation: 0.09060527165407703]
	TIME [epoch: 8.69 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10263229254387066		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22029044295395428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16146136774891245 | validation: 0.08948758324200184]
	TIME [epoch: 8.69 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1202277096680108		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11902510068554677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11962640517677876 | validation: 0.11829925074702616]
	TIME [epoch: 8.68 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08770682891459998		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10421052428094682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09595867659777338 | validation: 0.09255494988704811]
	TIME [epoch: 8.66 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10578158768968919		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09900343650346004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1023925120965746 | validation: 0.16004812827973308]
	TIME [epoch: 8.66 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2150270193703557		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0805379207893672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14778247007986142 | validation: 0.08380050384948924]
	TIME [epoch: 8.68 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1207149407940475		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07085997662353877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09578745870879314 | validation: 0.04399499082031686]
	TIME [epoch: 8.65 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10791990931367926		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17800676922918063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14296333927142996 | validation: 0.0950310225686044]
	TIME [epoch: 8.65 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12481605595303231		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08423420000915668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1045251279810945 | validation: 0.15557156347337545]
	TIME [epoch: 8.66 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10743795281398401		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12309310421605602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11526552851502 | validation: 0.07622053237710363]
	TIME [epoch: 8.67 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09284815311188824		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16512099436985192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12898457374087008 | validation: 0.07762487820738]
	TIME [epoch: 8.69 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12707546076043535		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14850262451390633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13778904263717084 | validation: 0.13341304565246798]
	TIME [epoch: 8.65 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2129130764280292		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18094084470286712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19692696056544814 | validation: 0.11029297517248543]
	TIME [epoch: 8.66 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13273580590963469		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14778823604019195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1402620209749133 | validation: 0.1021349473517919]
	TIME [epoch: 8.65 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13503596245963564		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09344221331543283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11423908788753423 | validation: 0.14223902629561108]
	TIME [epoch: 8.69 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13011495714183113		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13404395880381875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13207945797282492 | validation: 0.23168396354990414]
	TIME [epoch: 8.66 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1268562684935925		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10517333374711191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1160148011203522 | validation: 0.11348071104700995]
	TIME [epoch: 8.66 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16433195114674265		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12409466319868137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.144213307172712 | validation: 0.1361497699400115]
	TIME [epoch: 8.66 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12076529680210761		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09623855747864148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10850192714037457 | validation: 0.0703054723130528]
	TIME [epoch: 8.68 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13095508630055797		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18795755546545945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15945632088300865 | validation: 0.2073219355188584]
	TIME [epoch: 8.66 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1741307525562017		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08709799715816173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13061437485718175 | validation: 0.07607828470610088]
	TIME [epoch: 8.65 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08394767281275492		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11242327191544403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0981854723640995 | validation: 0.15160044572460366]
	TIME [epoch: 8.66 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1393602528482348		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2239367619959626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1816485074220987 | validation: 0.11887672689347095]
	TIME [epoch: 8.67 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12924305487555313		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07255726885913219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10090016186734266 | validation: 0.06414204908384057]
	TIME [epoch: 8.68 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09308842364917949		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13654990358726724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11481916361822338 | validation: 0.09449263542001085]
	TIME [epoch: 8.67 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10044394158239238		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17697372611175727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13870883384707483 | validation: 0.15619401836180913]
	TIME [epoch: 8.66 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2055803639143432		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12234104517888993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16396070454661657 | validation: 0.11150798362252354]
	TIME [epoch: 8.66 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20390194583585552		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0990892415967716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15149559371631355 | validation: 0.11002184473119384]
	TIME [epoch: 8.69 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10124945774435135		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09043773124051531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09584359449243332 | validation: 0.0928409691567485]
	TIME [epoch: 8.66 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09864717138165416		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08934910995597767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09399814066881591 | validation: 0.07351098313654131]
	TIME [epoch: 8.67 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1514816231056996		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11117447014431488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13132804662500724 | validation: 0.13202890197443679]
	TIME [epoch: 8.66 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1809005117175332		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17478878592819103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17784464882286216 | validation: 0.16081193859952814]
	TIME [epoch: 8.68 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3192231181197457		[learning rate: 0.0099862]
		[batch 20/20] avg loss: 0.13596800393489947		[learning rate: 0.0099709]
	Learning Rate: 0.00997088
	LOSS [training: 0.2275955610273225 | validation: 0.10475312228038242]
	TIME [epoch: 8.66 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08803141784544674		[learning rate: 0.0099556]
		[batch 20/20] avg loss: 0.18405295877590597		[learning rate: 0.0099403]
	Learning Rate: 0.00994031
	LOSS [training: 0.13604218831067635 | validation: 0.14353859942360195]
	TIME [epoch: 8.65 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1041467195305038		[learning rate: 0.0099251]
		[batch 20/20] avg loss: 0.10718739130519879		[learning rate: 0.0099098]
	Learning Rate: 0.00990984
	LOSS [training: 0.10566705541785129 | validation: 0.17968585767398443]
	TIME [epoch: 8.65 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13700257211169437		[learning rate: 0.0098946]
		[batch 20/20] avg loss: 0.12079636121506152		[learning rate: 0.0098795]
	Learning Rate: 0.00987946
	LOSS [training: 0.12889946666337795 | validation: 0.23009816989252757]
	TIME [epoch: 8.67 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18680175799284643		[learning rate: 0.0098643]
		[batch 20/20] avg loss: 0.16275615300435636		[learning rate: 0.0098492]
	Learning Rate: 0.00984918
	LOSS [training: 0.17477895549860142 | validation: 0.08511121604115315]
	TIME [epoch: 8.69 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12536035291298317		[learning rate: 0.0098341]
		[batch 20/20] avg loss: 0.15395110735054762		[learning rate: 0.009819]
	Learning Rate: 0.00981899
	LOSS [training: 0.13965573013176538 | validation: 0.2611331257900886]
	TIME [epoch: 8.65 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.164395701371117		[learning rate: 0.0098039]
		[batch 20/20] avg loss: 0.07075677104831284		[learning rate: 0.0097889]
	Learning Rate: 0.00978889
	LOSS [training: 0.11757623620971491 | validation: 0.07995290032486776]
	TIME [epoch: 8.67 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10990117980324413		[learning rate: 0.0097739]
		[batch 20/20] avg loss: 0.09257182510026511		[learning rate: 0.0097589]
	Learning Rate: 0.00975888
	LOSS [training: 0.10123650245175461 | validation: 0.0808621458738816]
	TIME [epoch: 8.65 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0820972333605627		[learning rate: 0.0097439]
		[batch 20/20] avg loss: 0.11501661766663454		[learning rate: 0.009729]
	Learning Rate: 0.00972897
	LOSS [training: 0.09855692551359861 | validation: 0.09958778354477629]
	TIME [epoch: 8.69 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1072086181334129		[learning rate: 0.009714]
		[batch 20/20] avg loss: 0.06954768325995414		[learning rate: 0.0096991]
	Learning Rate: 0.00969914
	LOSS [training: 0.0883781506966835 | validation: 0.07408170588652888]
	TIME [epoch: 8.66 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06931471200150108		[learning rate: 0.0096843]
		[batch 20/20] avg loss: 0.22538030183772584		[learning rate: 0.0096694]
	Learning Rate: 0.00966941
	LOSS [training: 0.14734750691961346 | validation: 0.056981374361311024]
	TIME [epoch: 8.66 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1070652170455029		[learning rate: 0.0096546]
		[batch 20/20] avg loss: 0.09093800419181823		[learning rate: 0.0096398]
	Learning Rate: 0.00963977
	LOSS [training: 0.09900161061866056 | validation: 0.2385019065472371]
	TIME [epoch: 8.67 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13107465908813537		[learning rate: 0.009625]
		[batch 20/20] avg loss: 0.13518797213575257		[learning rate: 0.0096102]
	Learning Rate: 0.00961022
	LOSS [training: 0.13313131561194397 | validation: 0.16768715522263095]
	TIME [epoch: 8.67 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12260527413969365		[learning rate: 0.0095955]
		[batch 20/20] avg loss: 0.1047706964318367		[learning rate: 0.0095808]
	Learning Rate: 0.00958076
	LOSS [training: 0.11368798528576518 | validation: 0.10427257478916857]
	TIME [epoch: 8.69 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08739141631432235		[learning rate: 0.0095661]
		[batch 20/20] avg loss: 0.1216541752406913		[learning rate: 0.0095514]
	Learning Rate: 0.00955139
	LOSS [training: 0.10452279577750682 | validation: 0.10821483136481773]
	TIME [epoch: 8.66 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08760706373663588		[learning rate: 0.0095367]
		[batch 20/20] avg loss: 0.08074109631482371		[learning rate: 0.0095221]
	Learning Rate: 0.00952211
	LOSS [training: 0.08417408002572978 | validation: 0.19951273400243558]
	TIME [epoch: 8.66 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10653383009413572		[learning rate: 0.0095075]
		[batch 20/20] avg loss: 0.08387317493866313		[learning rate: 0.0094929]
	Learning Rate: 0.00949292
	LOSS [training: 0.09520350251639943 | validation: 0.0923248686313437]
	TIME [epoch: 8.66 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0933445969293308		[learning rate: 0.0094784]
		[batch 20/20] avg loss: 0.08307058417597268		[learning rate: 0.0094638]
	Learning Rate: 0.00946382
	LOSS [training: 0.08820759055265173 | validation: 0.11138397797816685]
	TIME [epoch: 8.68 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10695952316547708		[learning rate: 0.0094493]
		[batch 20/20] avg loss: 0.1010150196823193		[learning rate: 0.0094348]
	Learning Rate: 0.00943481
	LOSS [training: 0.10398727142389816 | validation: 0.10852269372232612]
	TIME [epoch: 8.66 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10216813586103286		[learning rate: 0.0094203]
		[batch 20/20] avg loss: 0.3574352879438473		[learning rate: 0.0094059]
	Learning Rate: 0.00940589
	LOSS [training: 0.22980171190244011 | validation: 0.11469815021128343]
	TIME [epoch: 8.65 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10400734052402583		[learning rate: 0.0093915]
		[batch 20/20] avg loss: 0.1351018160862321		[learning rate: 0.0093771]
	Learning Rate: 0.00937706
	LOSS [training: 0.11955457830512897 | validation: 0.09583335824773975]
	TIME [epoch: 8.66 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13092928440334556		[learning rate: 0.0093627]
		[batch 20/20] avg loss: 0.09534580796651648		[learning rate: 0.0093483]
	Learning Rate: 0.00934831
	LOSS [training: 0.11313754618493099 | validation: 0.052443652485936805]
	TIME [epoch: 8.69 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06711705649746139		[learning rate: 0.009334]
		[batch 20/20] avg loss: 0.09731615561017885		[learning rate: 0.0093197]
	Learning Rate: 0.00931966
	LOSS [training: 0.08221660605382013 | validation: 0.08609256084856166]
	TIME [epoch: 8.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08809968827145741		[learning rate: 0.0093054]
		[batch 20/20] avg loss: 0.165011425318137		[learning rate: 0.0092911]
	Learning Rate: 0.00929109
	LOSS [training: 0.1265555567947972 | validation: 0.16098984673217476]
	TIME [epoch: 8.67 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21544173466972408		[learning rate: 0.0092768]
		[batch 20/20] avg loss: 0.10285707325310658		[learning rate: 0.0092626]
	Learning Rate: 0.00926261
	LOSS [training: 0.1591494039614153 | validation: 0.07248175920543086]
	TIME [epoch: 8.66 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07026230684813588		[learning rate: 0.0092484]
		[batch 20/20] avg loss: 0.09122208531257282		[learning rate: 0.0092342]
	Learning Rate: 0.00923422
	LOSS [training: 0.08074219608035435 | validation: 0.08122586355507727]
	TIME [epoch: 8.69 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06106336617271082		[learning rate: 0.0092201]
		[batch 20/20] avg loss: 0.06815721716347475		[learning rate: 0.0092059]
	Learning Rate: 0.00920591
	LOSS [training: 0.0646102916680928 | validation: 0.10158533889997853]
	TIME [epoch: 8.69 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14846852266391078		[learning rate: 0.0091918]
		[batch 20/20] avg loss: 0.07984796520065865		[learning rate: 0.0091777]
	Learning Rate: 0.00917769
	LOSS [training: 0.11415824393228471 | validation: 0.11887260137148759]
	TIME [epoch: 8.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11021156165182573		[learning rate: 0.0091636]
		[batch 20/20] avg loss: 0.06115820768746265		[learning rate: 0.0091496]
	Learning Rate: 0.00914956
	LOSS [training: 0.0856848846696442 | validation: 0.1088500297344551]
	TIME [epoch: 8.66 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10651742454689003		[learning rate: 0.0091355]
		[batch 20/20] avg loss: 0.10085231134531299		[learning rate: 0.0091215]
	Learning Rate: 0.00912151
	LOSS [training: 0.1036848679461015 | validation: 0.0706854765086079]
	TIME [epoch: 8.66 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09092845506061491		[learning rate: 0.0091075]
		[batch 20/20] avg loss: 0.056506749449076424		[learning rate: 0.0090935]
	Learning Rate: 0.00909355
	LOSS [training: 0.07371760225484565 | validation: 0.07918209314657962]
	TIME [epoch: 8.68 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07391401629175849		[learning rate: 0.0090796]
		[batch 20/20] avg loss: 0.12661809949470643		[learning rate: 0.0090657]
	Learning Rate: 0.00906567
	LOSS [training: 0.10026605789323247 | validation: 0.07364505692828599]
	TIME [epoch: 8.66 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08979478772618671		[learning rate: 0.0090518]
		[batch 20/20] avg loss: 0.07939036725904854		[learning rate: 0.0090379]
	Learning Rate: 0.00903788
	LOSS [training: 0.08459257749261763 | validation: 0.039859797372111073]
	TIME [epoch: 8.65 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12251850400634749		[learning rate: 0.009024]
		[batch 20/20] avg loss: 0.1018653417574233		[learning rate: 0.0090102]
	Learning Rate: 0.00901018
	LOSS [training: 0.11219192288188537 | validation: 0.07856574785865625]
	TIME [epoch: 8.66 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07991578848949726		[learning rate: 0.0089964]
		[batch 20/20] avg loss: 0.061686180523993814		[learning rate: 0.0089826]
	Learning Rate: 0.00898256
	LOSS [training: 0.07080098450674553 | validation: 0.0741135788581582]
	TIME [epoch: 8.68 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056616802999695445		[learning rate: 0.0089688]
		[batch 20/20] avg loss: 0.0949876444167491		[learning rate: 0.008955]
	Learning Rate: 0.00895502
	LOSS [training: 0.07580222370822227 | validation: 0.04660089389560053]
	TIME [epoch: 8.68 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11782015622777645		[learning rate: 0.0089413]
		[batch 20/20] avg loss: 0.07569510153969727		[learning rate: 0.0089276]
	Learning Rate: 0.00892757
	LOSS [training: 0.09675762888373685 | validation: 0.1290698344180074]
	TIME [epoch: 8.66 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11239560098414829		[learning rate: 0.0089139]
		[batch 20/20] avg loss: 0.20274002198579644		[learning rate: 0.0089002]
	Learning Rate: 0.0089002
	LOSS [training: 0.1575678114849724 | validation: 0.15683593919668107]
	TIME [epoch: 8.66 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11041925182104236		[learning rate: 0.0088866]
		[batch 20/20] avg loss: 0.11101903383943204		[learning rate: 0.0088729]
	Learning Rate: 0.00887292
	LOSS [training: 0.1107191428302372 | validation: 0.08561643049839202]
	TIME [epoch: 8.66 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10943315614720688		[learning rate: 0.0088593]
		[batch 20/20] avg loss: 0.09127980895603499		[learning rate: 0.0088457]
	Learning Rate: 0.00884572
	LOSS [training: 0.10035648255162095 | validation: 0.060792041074447925]
	TIME [epoch: 8.69 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08980221698674161		[learning rate: 0.0088322]
		[batch 20/20] avg loss: 0.10857296112376105		[learning rate: 0.0088186]
	Learning Rate: 0.00881861
	LOSS [training: 0.09918758905525134 | validation: 0.25606228001532705]
	TIME [epoch: 8.66 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12028170077131232		[learning rate: 0.0088051]
		[batch 20/20] avg loss: 0.10166867947502123		[learning rate: 0.0087916]
	Learning Rate: 0.00879157
	LOSS [training: 0.11097519012316678 | validation: 0.18886473877211993]
	TIME [epoch: 8.66 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12950199784768363		[learning rate: 0.0087781]
		[batch 20/20] avg loss: 0.10492849509409086		[learning rate: 0.0087646]
	Learning Rate: 0.00876462
	LOSS [training: 0.11721524647088724 | validation: 0.11472097355160428]
	TIME [epoch: 8.67 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10809631420983738		[learning rate: 0.0087512]
		[batch 20/20] avg loss: 0.09597521763667857		[learning rate: 0.0087378]
	Learning Rate: 0.00873776
	LOSS [training: 0.10203576592325798 | validation: 0.10792203937047312]
	TIME [epoch: 8.68 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11227075068051855		[learning rate: 0.0087244]
		[batch 20/20] avg loss: 0.1739078956097379		[learning rate: 0.008711]
	Learning Rate: 0.00871097
	LOSS [training: 0.14308932314512823 | validation: 0.23160222750580298]
	TIME [epoch: 8.65 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1545244266788976		[learning rate: 0.0086976]
		[batch 20/20] avg loss: 0.090008894670465		[learning rate: 0.0086843]
	Learning Rate: 0.00868427
	LOSS [training: 0.12226666067468132 | validation: 0.07930747161937653]
	TIME [epoch: 8.66 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11165929843407019		[learning rate: 0.0086709]
		[batch 20/20] avg loss: 0.08782283407330584		[learning rate: 0.0086576]
	Learning Rate: 0.00865765
	LOSS [training: 0.09974106625368802 | validation: 0.04398619060154905]
	TIME [epoch: 8.66 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07909731858609448		[learning rate: 0.0086444]
		[batch 20/20] avg loss: 0.11738327441020709		[learning rate: 0.0086311]
	Learning Rate: 0.00863111
	LOSS [training: 0.09824029649815078 | validation: 0.07609729660389214]
	TIME [epoch: 8.67 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0699992186278614		[learning rate: 0.0086179]
		[batch 20/20] avg loss: 0.09664388176515537		[learning rate: 0.0086047]
	Learning Rate: 0.00860465
	LOSS [training: 0.08332155019650839 | validation: 0.08991111483249609]
	TIME [epoch: 8.67 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1392509857772624		[learning rate: 0.0085915]
		[batch 20/20] avg loss: 0.06793452700553068		[learning rate: 0.0085783]
	Learning Rate: 0.00857828
	LOSS [training: 0.10359275639139653 | validation: 0.024319427668271505]
	TIME [epoch: 8.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0683654790898656		[learning rate: 0.0085651]
		[batch 20/20] avg loss: 0.06861355435034203		[learning rate: 0.008552]
	Learning Rate: 0.00855198
	LOSS [training: 0.06848951672010381 | validation: 0.04479848410857349]
	TIME [epoch: 8.67 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10756598017028318		[learning rate: 0.0085389]
		[batch 20/20] avg loss: 0.11480637232295487		[learning rate: 0.0085258]
	Learning Rate: 0.00852576
	LOSS [training: 0.11118617624661901 | validation: 0.08256470679162553]
	TIME [epoch: 8.68 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12334796355232105		[learning rate: 0.0085127]
		[batch 20/20] avg loss: 0.06870874635214361		[learning rate: 0.0084996]
	Learning Rate: 0.00849963
	LOSS [training: 0.09602835495223233 | validation: 0.0679449699498634]
	TIME [epoch: 8.72 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04831929326262492		[learning rate: 0.0084866]
		[batch 20/20] avg loss: 0.07822975111906917		[learning rate: 0.0084736]
	Learning Rate: 0.00847358
	LOSS [training: 0.06327452219084705 | validation: 0.11830194453982562]
	TIME [epoch: 8.7 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07829821933751686		[learning rate: 0.0084606]
		[batch 20/20] avg loss: 0.07043451082379441		[learning rate: 0.0084476]
	Learning Rate: 0.0084476
	LOSS [training: 0.07436636508065564 | validation: 0.08072666301686471]
	TIME [epoch: 8.69 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.206144278255907		[learning rate: 0.0084346]
		[batch 20/20] avg loss: 0.2731960461030481		[learning rate: 0.0084217]
	Learning Rate: 0.0084217
	LOSS [training: 0.2396701621794775 | validation: 0.2499606873684742]
	TIME [epoch: 8.68 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16207485246916034		[learning rate: 0.0084088]
		[batch 20/20] avg loss: 0.17338523387902907		[learning rate: 0.0083959]
	Learning Rate: 0.00839589
	LOSS [training: 0.16773004317409473 | validation: 0.24878109766461837]
	TIME [epoch: 8.73 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12573761813456136		[learning rate: 0.008383]
		[batch 20/20] avg loss: 0.0824036089432328		[learning rate: 0.0083702]
	Learning Rate: 0.00837015
	LOSS [training: 0.10407061353889709 | validation: 0.05544012423512973]
	TIME [epoch: 8.7 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0722801841283564		[learning rate: 0.0083573]
		[batch 20/20] avg loss: 0.13111041381096405		[learning rate: 0.0083445]
	Learning Rate: 0.00834449
	LOSS [training: 0.1016952989696602 | validation: 0.05850887935398309]
	TIME [epoch: 8.69 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061006645762805956		[learning rate: 0.0083317]
		[batch 20/20] avg loss: 0.07793390324653243		[learning rate: 0.0083189]
	Learning Rate: 0.00831891
	LOSS [training: 0.0694702745046692 | validation: 0.09787790880536462]
	TIME [epoch: 8.69 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07712165934526684		[learning rate: 0.0083062]
		[batch 20/20] avg loss: 0.11227177276599368		[learning rate: 0.0082934]
	Learning Rate: 0.00829341
	LOSS [training: 0.09469671605563025 | validation: 0.07563903818634261]
	TIME [epoch: 8.71 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07483345540855091		[learning rate: 0.0082807]
		[batch 20/20] avg loss: 0.07942055964480156		[learning rate: 0.008268]
	Learning Rate: 0.00826799
	LOSS [training: 0.07712700752667624 | validation: 0.06690213038975854]
	TIME [epoch: 8.71 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07021721106292765		[learning rate: 0.0082553]
		[batch 20/20] avg loss: 0.08809575110522401		[learning rate: 0.0082426]
	Learning Rate: 0.00824265
	LOSS [training: 0.07915648108407583 | validation: 0.07608094586538394]
	TIME [epoch: 8.7 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09919200010361187		[learning rate: 0.00823]
		[batch 20/20] avg loss: 0.08355913503739273		[learning rate: 0.0082174]
	Learning Rate: 0.00821738
	LOSS [training: 0.0913755675705023 | validation: 0.08689629158738718]
	TIME [epoch: 8.7 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08141746993967672		[learning rate: 0.0082048]
		[batch 20/20] avg loss: 0.10474672455897868		[learning rate: 0.0081922]
	Learning Rate: 0.00819219
	LOSS [training: 0.09308209724932769 | validation: 0.18873818021014102]
	TIME [epoch: 8.69 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14964991213089568		[learning rate: 0.0081796]
		[batch 20/20] avg loss: 0.08610630159693557		[learning rate: 0.0081671]
	Learning Rate: 0.00816708
	LOSS [training: 0.11787810686391562 | validation: 0.08444611817268283]
	TIME [epoch: 8.73 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07196440320545733		[learning rate: 0.0081545]
		[batch 20/20] avg loss: 0.07139785105505013		[learning rate: 0.008142]
	Learning Rate: 0.00814204
	LOSS [training: 0.07168112713025374 | validation: 0.0692399736178946]
	TIME [epoch: 8.7 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0742972594230044		[learning rate: 0.0081296]
		[batch 20/20] avg loss: 0.11289379550344514		[learning rate: 0.0081171]
	Learning Rate: 0.00811708
	LOSS [training: 0.09359552746322478 | validation: 0.09274663737252581]
	TIME [epoch: 8.69 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1062871958290718		[learning rate: 0.0081046]
		[batch 20/20] avg loss: 0.06511982895225467		[learning rate: 0.0080922]
	Learning Rate: 0.0080922
	LOSS [training: 0.08570351239066323 | validation: 0.061335327270467914]
	TIME [epoch: 8.7 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05627678234716248		[learning rate: 0.0080798]
		[batch 20/20] avg loss: 0.09633238554216375		[learning rate: 0.0080674]
	Learning Rate: 0.00806739
	LOSS [training: 0.07630458394466309 | validation: 0.18313110940692867]
	TIME [epoch: 8.71 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1026977599364276		[learning rate: 0.008055]
		[batch 20/20] avg loss: 0.09364777083152934		[learning rate: 0.0080427]
	Learning Rate: 0.00804267
	LOSS [training: 0.09817276538397848 | validation: 0.08556909322393946]
	TIME [epoch: 8.7 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09656451040976602		[learning rate: 0.0080303]
		[batch 20/20] avg loss: 0.06534127224248831		[learning rate: 0.008018]
	Learning Rate: 0.00801801
	LOSS [training: 0.08095289132612715 | validation: 0.10455976887233862]
	TIME [epoch: 8.69 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09490340276221478		[learning rate: 0.0080057]
		[batch 20/20] avg loss: 0.13016955492073762		[learning rate: 0.0079934]
	Learning Rate: 0.00799343
	LOSS [training: 0.11253647884147619 | validation: 0.05156693349512298]
	TIME [epoch: 8.69 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06572298458259011		[learning rate: 0.0079812]
		[batch 20/20] avg loss: 0.0784358969269251		[learning rate: 0.0079689]
	Learning Rate: 0.00796893
	LOSS [training: 0.07207944075475761 | validation: 0.07006034829643282]
	TIME [epoch: 8.71 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16146925880959748		[learning rate: 0.0079567]
		[batch 20/20] avg loss: 0.0809796788648111		[learning rate: 0.0079445]
	Learning Rate: 0.0079445
	LOSS [training: 0.12122446883720431 | validation: 0.0544738579584533]
	TIME [epoch: 8.71 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06975428741375532		[learning rate: 0.0079323]
		[batch 20/20] avg loss: 0.06415082315025572		[learning rate: 0.0079201]
	Learning Rate: 0.00792015
	LOSS [training: 0.06695255528200551 | validation: 0.02876996391637747]
	TIME [epoch: 8.69 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10391678562262678		[learning rate: 0.007908]
		[batch 20/20] avg loss: 0.0747943692090669		[learning rate: 0.0078959]
	Learning Rate: 0.00789587
	LOSS [training: 0.08935557741584685 | validation: 0.08183811100841656]
	TIME [epoch: 8.69 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0731235163331988		[learning rate: 0.0078838]
		[batch 20/20] avg loss: 0.08703790723691254		[learning rate: 0.0078717]
	Learning Rate: 0.00787167
	LOSS [training: 0.08008071178505566 | validation: 0.08297058721497527]
	TIME [epoch: 8.7 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07304258168516839		[learning rate: 0.0078596]
		[batch 20/20] avg loss: 0.05601284924331296		[learning rate: 0.0078475]
	Learning Rate: 0.00784754
	LOSS [training: 0.06452771546424066 | validation: 0.046018157061788495]
	TIME [epoch: 8.72 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06541682885285498		[learning rate: 0.0078355]
		[batch 20/20] avg loss: 0.08426518263802799		[learning rate: 0.0078235]
	Learning Rate: 0.00782348
	LOSS [training: 0.0748410057454415 | validation: 0.05762041054485865]
	TIME [epoch: 8.7 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05475100568223357		[learning rate: 0.0078115]
		[batch 20/20] avg loss: 0.05460460876516256		[learning rate: 0.0077995]
	Learning Rate: 0.0077995
	LOSS [training: 0.05467780722369805 | validation: 0.12442659322169192]
	TIME [epoch: 8.69 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07488692239153807		[learning rate: 0.0077875]
		[batch 20/20] avg loss: 0.1212178888739188		[learning rate: 0.0077756]
	Learning Rate: 0.00777559
	LOSS [training: 0.09805240563272846 | validation: 0.09486742011906772]
	TIME [epoch: 8.7 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08482244297374278		[learning rate: 0.0077637]
		[batch 20/20] avg loss: 0.05963628500550196		[learning rate: 0.0077518]
	Learning Rate: 0.00775175
	LOSS [training: 0.07222936398962235 | validation: 0.06188087824923928]
	TIME [epoch: 8.7 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06732006485177268		[learning rate: 0.0077399]
		[batch 20/20] avg loss: 0.08243218278098444		[learning rate: 0.007728]
	Learning Rate: 0.00772799
	LOSS [training: 0.07487612381637855 | validation: 0.05588708629735228]
	TIME [epoch: 8.72 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12486485515027021		[learning rate: 0.0077161]
		[batch 20/20] avg loss: 0.08703438094249169		[learning rate: 0.0077043]
	Learning Rate: 0.0077043
	LOSS [training: 0.10594961804638095 | validation: 0.07418431129250146]
	TIME [epoch: 8.69 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0846441170114373		[learning rate: 0.0076925]
		[batch 20/20] avg loss: 0.0904780031525508		[learning rate: 0.0076807]
	Learning Rate: 0.00768069
	LOSS [training: 0.08756106008199405 | validation: 0.08037332830833935]
	TIME [epoch: 8.69 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13114281222972274		[learning rate: 0.0076689]
		[batch 20/20] avg loss: 0.12809978210917475		[learning rate: 0.0076571]
	Learning Rate: 0.00765714
	LOSS [training: 0.12962129716944876 | validation: 0.11251646430051328]
	TIME [epoch: 8.7 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26708721449942796		[learning rate: 0.0076454]
		[batch 20/20] avg loss: 0.11766362862833613		[learning rate: 0.0076337]
	Learning Rate: 0.00763367
	LOSS [training: 0.19237542156388204 | validation: 0.14379217907223893]
	TIME [epoch: 8.73 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16173658623450285		[learning rate: 0.007622]
		[batch 20/20] avg loss: 0.18561856011309058		[learning rate: 0.0076103]
	Learning Rate: 0.00761027
	LOSS [training: 0.1736775731737967 | validation: 0.08330925995880742]
	TIME [epoch: 8.69 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07434941557748402		[learning rate: 0.0075986]
		[batch 20/20] avg loss: 0.07767112637226278		[learning rate: 0.0075869]
	Learning Rate: 0.00758694
	LOSS [training: 0.07601027097487341 | validation: 0.0691678235993755]
	TIME [epoch: 8.7 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08345716311559674		[learning rate: 0.0075753]
		[batch 20/20] avg loss: 0.10795601564812426		[learning rate: 0.0075637]
	Learning Rate: 0.00756368
	LOSS [training: 0.09570658938186051 | validation: 0.07301864514783417]
	TIME [epoch: 8.69 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08664289203470697		[learning rate: 0.0075521]
		[batch 20/20] avg loss: 0.08168115139911335		[learning rate: 0.0075405]
	Learning Rate: 0.0075405
	LOSS [training: 0.08416202171691016 | validation: 0.05743987799512522]
	TIME [epoch: 8.72 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060328045034810554		[learning rate: 0.0075289]
		[batch 20/20] avg loss: 0.0726931262853482		[learning rate: 0.0075174]
	Learning Rate: 0.00751738
	LOSS [training: 0.06651058566007936 | validation: 0.05580923202902918]
	TIME [epoch: 8.71 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07457364714886552		[learning rate: 0.0075059]
		[batch 20/20] avg loss: 0.051891428513606966		[learning rate: 0.0074943]
	Learning Rate: 0.00749434
	LOSS [training: 0.06323253783123625 | validation: 0.07774740441570664]
	TIME [epoch: 8.7 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04086238706242025		[learning rate: 0.0074828]
		[batch 20/20] avg loss: 0.0985848309480119		[learning rate: 0.0074714]
	Learning Rate: 0.00747137
	LOSS [training: 0.06972360900521608 | validation: 0.10873272800746779]
	TIME [epoch: 8.7 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12886823468423417		[learning rate: 0.0074599]
		[batch 20/20] avg loss: 0.06219245856735669		[learning rate: 0.0074485]
	Learning Rate: 0.00744846
	LOSS [training: 0.09553034662579543 | validation: 0.08862034269277985]
	TIME [epoch: 8.72 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08718659321924976		[learning rate: 0.007437]
		[batch 20/20] avg loss: 0.07628343296107609		[learning rate: 0.0074256]
	Learning Rate: 0.00742563
	LOSS [training: 0.08173501309016293 | validation: 0.05519408912113608]
	TIME [epoch: 8.72 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07330310402813808		[learning rate: 0.0074142]
		[batch 20/20] avg loss: 0.11133635857425535		[learning rate: 0.0074029]
	Learning Rate: 0.00740287
	LOSS [training: 0.09231973130119672 | validation: 0.09255887562163324]
	TIME [epoch: 8.7 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07062583245482684		[learning rate: 0.0073915]
		[batch 20/20] avg loss: 0.08799317943952532		[learning rate: 0.0073802]
	Learning Rate: 0.00738017
	LOSS [training: 0.07930950594717608 | validation: 0.08300813452702181]
	TIME [epoch: 8.69 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14978076842099525		[learning rate: 0.0073689]
		[batch 20/20] avg loss: 0.0900883356517643		[learning rate: 0.0073576]
	Learning Rate: 0.00735755
	LOSS [training: 0.11993455203637979 | validation: 0.06545567810447389]
	TIME [epoch: 8.7 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.100228438509222		[learning rate: 0.0073463]
		[batch 20/20] avg loss: 0.09431280350394045		[learning rate: 0.007335]
	Learning Rate: 0.007335
	LOSS [training: 0.09727062100658124 | validation: 0.09449833859546192]
	TIME [epoch: 8.73 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0676708813008437		[learning rate: 0.0073237]
		[batch 20/20] avg loss: 0.08034213008444706		[learning rate: 0.0073125]
	Learning Rate: 0.00731251
	LOSS [training: 0.07400650569264539 | validation: 0.041699773053240956]
	TIME [epoch: 8.7 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06637848787722805		[learning rate: 0.0073013]
		[batch 20/20] avg loss: 0.11729599562932866		[learning rate: 0.0072901]
	Learning Rate: 0.0072901
	LOSS [training: 0.09183724175327834 | validation: 0.09360088578071207]
	TIME [epoch: 8.7 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07742617159420326		[learning rate: 0.0072789]
		[batch 20/20] avg loss: 0.08658828381791986		[learning rate: 0.0072678]
	Learning Rate: 0.00726775
	LOSS [training: 0.08200722770606156 | validation: 0.07839447078965224]
	TIME [epoch: 8.7 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06797934763613978		[learning rate: 0.0072566]
		[batch 20/20] avg loss: 0.062127728893883405		[learning rate: 0.0072455]
	Learning Rate: 0.00724547
	LOSS [training: 0.06505353826501158 | validation: 0.06227516846852154]
	TIME [epoch: 8.73 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06406861245715376		[learning rate: 0.0072344]
		[batch 20/20] avg loss: 0.10097429389261439		[learning rate: 0.0072233]
	Learning Rate: 0.00722326
	LOSS [training: 0.08252145317488406 | validation: 0.10041311917289958]
	TIME [epoch: 8.71 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09434376350348747		[learning rate: 0.0072122]
		[batch 20/20] avg loss: 0.1269277902515188		[learning rate: 0.0072011]
	Learning Rate: 0.00720112
	LOSS [training: 0.11063577687750312 | validation: 0.033678440963255604]
	TIME [epoch: 8.7 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06447401174217567		[learning rate: 0.0071901]
		[batch 20/20] avg loss: 0.0797086431199925		[learning rate: 0.007179]
	Learning Rate: 0.00717904
	LOSS [training: 0.07209132743108407 | validation: 0.2469899034701204]
	TIME [epoch: 8.69 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12258529850556518		[learning rate: 0.007168]
		[batch 20/20] avg loss: 0.11026565381875746		[learning rate: 0.007157]
	Learning Rate: 0.00715704
	LOSS [training: 0.11642547616216134 | validation: 0.04143341278919127]
	TIME [epoch: 8.72 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06654680279417205		[learning rate: 0.0071461]
		[batch 20/20] avg loss: 0.04574603699317148		[learning rate: 0.0071351]
	Learning Rate: 0.0071351
	LOSS [training: 0.05614641989367177 | validation: 0.07109226040337813]
	TIME [epoch: 8.81 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06805833813351361		[learning rate: 0.0071242]
		[batch 20/20] avg loss: 0.05060143793364282		[learning rate: 0.0071132]
	Learning Rate: 0.00711323
	LOSS [training: 0.059329888033578214 | validation: 0.024599287564630673]
	TIME [epoch: 8.7 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06470263608081175		[learning rate: 0.0071023]
		[batch 20/20] avg loss: 0.13218261386440197		[learning rate: 0.0070914]
	Learning Rate: 0.00709142
	LOSS [training: 0.09844262497260688 | validation: 0.03472040886458007]
	TIME [epoch: 8.7 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06223175853077286		[learning rate: 0.0070805]
		[batch 20/20] avg loss: 0.06165586887304619		[learning rate: 0.0070697]
	Learning Rate: 0.00706968
	LOSS [training: 0.061943813701909524 | validation: 0.028932711111880477]
	TIME [epoch: 8.71 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12407402958611229		[learning rate: 0.0070588]
		[batch 20/20] avg loss: 0.13611611889227476		[learning rate: 0.007048]
	Learning Rate: 0.00704801
	LOSS [training: 0.13009507423919353 | validation: 0.1383162917087209]
	TIME [epoch: 8.74 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14441442631751522		[learning rate: 0.0070372]
		[batch 20/20] avg loss: 0.1669184792543072		[learning rate: 0.0070264]
	Learning Rate: 0.00702641
	LOSS [training: 0.15566645278591124 | validation: 0.1522570491974664]
	TIME [epoch: 8.72 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11293127006693142		[learning rate: 0.0070156]
		[batch 20/20] avg loss: 0.12036410689193447		[learning rate: 0.0070049]
	Learning Rate: 0.00700487
	LOSS [training: 0.11664768847943296 | validation: 0.0601168369545503]
	TIME [epoch: 8.71 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06524352921301692		[learning rate: 0.0069941]
		[batch 20/20] avg loss: 0.09635727126359761		[learning rate: 0.0069834]
	Learning Rate: 0.0069834
	LOSS [training: 0.08080040023830729 | validation: 0.04857530644146526]
	TIME [epoch: 8.7 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0890006982172987		[learning rate: 0.0069727]
		[batch 20/20] avg loss: 0.07175948466390432		[learning rate: 0.006962]
	Learning Rate: 0.00696199
	LOSS [training: 0.0803800914406015 | validation: 0.023250571880788724]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_618.pth
	Model improved!!!
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04735550634515785		[learning rate: 0.0069513]
		[batch 20/20] avg loss: 0.09981395575648552		[learning rate: 0.0069406]
	Learning Rate: 0.00694065
	LOSS [training: 0.07358473105082167 | validation: 0.13613000382337914]
	TIME [epoch: 8.73 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1033365310576698		[learning rate: 0.00693]
		[batch 20/20] avg loss: 0.09798811394982072		[learning rate: 0.0069194]
	Learning Rate: 0.00691937
	LOSS [training: 0.1006623225037453 | validation: 0.10997626901551438]
	TIME [epoch: 8.71 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08393198571464153		[learning rate: 0.0069088]
		[batch 20/20] avg loss: 0.0579462617975515		[learning rate: 0.0068982]
	Learning Rate: 0.00689816
	LOSS [training: 0.07093912375609654 | validation: 0.08767821233164924]
	TIME [epoch: 8.71 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05704738127193776		[learning rate: 0.0068876]
		[batch 20/20] avg loss: 0.06122478033401697		[learning rate: 0.006877]
	Learning Rate: 0.00687702
	LOSS [training: 0.059136080802977366 | validation: 0.041082843996842436]
	TIME [epoch: 8.71 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1194904245251778		[learning rate: 0.0068665]
		[batch 20/20] avg loss: 0.037751914880971425		[learning rate: 0.0068559]
	Learning Rate: 0.00685593
	LOSS [training: 0.07862116970307462 | validation: 0.06504218676021878]
	TIME [epoch: 8.73 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08132261400951404		[learning rate: 0.0068454]
		[batch 20/20] avg loss: 0.10064461767657704		[learning rate: 0.0068349]
	Learning Rate: 0.00683492
	LOSS [training: 0.09098361584304555 | validation: 0.13480918515123172]
	TIME [epoch: 8.7 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08814385583181998		[learning rate: 0.0068244]
		[batch 20/20] avg loss: 0.07934294444070158		[learning rate: 0.006814]
	Learning Rate: 0.00681397
	LOSS [training: 0.08374340013626078 | validation: 0.08877139317885187]
	TIME [epoch: 8.7 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08882557325403612		[learning rate: 0.0068035]
		[batch 20/20] avg loss: 0.09695743030976667		[learning rate: 0.0067931]
	Learning Rate: 0.00679308
	LOSS [training: 0.09289150178190139 | validation: 0.15720078932559903]
	TIME [epoch: 8.71 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09993330945374815		[learning rate: 0.0067827]
		[batch 20/20] avg loss: 0.0841984599355946		[learning rate: 0.0067723]
	Learning Rate: 0.00677225
	LOSS [training: 0.09206588469467136 | validation: 0.05705951518275888]
	TIME [epoch: 8.71 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07697426031615114		[learning rate: 0.0067619]
		[batch 20/20] avg loss: 0.09088950231309134		[learning rate: 0.0067515]
	Learning Rate: 0.0067515
	LOSS [training: 0.08393188131462123 | validation: 0.22081982159838787]
	TIME [epoch: 8.72 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10349216043680928		[learning rate: 0.0067411]
		[batch 20/20] avg loss: 0.09690954485274887		[learning rate: 0.0067308]
	Learning Rate: 0.0067308
	LOSS [training: 0.10020085264477907 | validation: 0.22983091457987326]
	TIME [epoch: 8.71 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13006149117522123		[learning rate: 0.0067205]
		[batch 20/20] avg loss: 0.09974985448122586		[learning rate: 0.0067102]
	Learning Rate: 0.00671017
	LOSS [training: 0.11490567282822355 | validation: 0.06648712700078899]
	TIME [epoch: 8.7 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04776305907957122		[learning rate: 0.0066999]
		[batch 20/20] avg loss: 0.073087732025285		[learning rate: 0.0066896]
	Learning Rate: 0.0066896
	LOSS [training: 0.06042539555242811 | validation: 0.08219682385752566]
	TIME [epoch: 8.72 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07437389937824086		[learning rate: 0.0066793]
		[batch 20/20] avg loss: 0.08007894096068321		[learning rate: 0.0066691]
	Learning Rate: 0.00666909
	LOSS [training: 0.07722642016946205 | validation: 0.06379089538680265]
	TIME [epoch: 8.72 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07974206630730242		[learning rate: 0.0066589]
		[batch 20/20] avg loss: 0.06721904242279172		[learning rate: 0.0066486]
	Learning Rate: 0.00664865
	LOSS [training: 0.07348055436504707 | validation: 0.06291832702114285]
	TIME [epoch: 8.71 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04910009911504812		[learning rate: 0.0066384]
		[batch 20/20] avg loss: 0.1724599951705616		[learning rate: 0.0066283]
	Learning Rate: 0.00662827
	LOSS [training: 0.11078004714280487 | validation: 0.06942060012296634]
	TIME [epoch: 8.72 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06489662639515067		[learning rate: 0.0066181]
		[batch 20/20] avg loss: 0.058661370315470786		[learning rate: 0.0066079]
	Learning Rate: 0.00660795
	LOSS [training: 0.06177899835531074 | validation: 0.01892298527754776]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05691222658183881		[learning rate: 0.0065978]
		[batch 20/20] avg loss: 0.06768895031936858		[learning rate: 0.0065877]
	Learning Rate: 0.00658769
	LOSS [training: 0.0623005884506037 | validation: 0.05761507437499272]
	TIME [epoch: 8.74 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23580446191231993		[learning rate: 0.0065776]
		[batch 20/20] avg loss: 0.41955288675900215		[learning rate: 0.0065675]
	Learning Rate: 0.0065675
	LOSS [training: 0.32767867433566106 | validation: 0.21666128217794178]
	TIME [epoch: 8.71 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.270262574035029		[learning rate: 0.0065574]
		[batch 20/20] avg loss: 0.28076758644603805		[learning rate: 0.0065474]
	Learning Rate: 0.00654737
	LOSS [training: 0.2755150802405335 | validation: 0.25193801814163463]
	TIME [epoch: 8.73 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17252786718948937		[learning rate: 0.0065373]
		[batch 20/20] avg loss: 0.13497202157183072		[learning rate: 0.0065273]
	Learning Rate: 0.0065273
	LOSS [training: 0.15374994438066003 | validation: 0.08417722275753747]
	TIME [epoch: 8.71 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22604742183408616		[learning rate: 0.0065173]
		[batch 20/20] avg loss: 0.26128996357769846		[learning rate: 0.0065073]
	Learning Rate: 0.00650729
	LOSS [training: 0.24366869270589228 | validation: 0.25098238245344456]
	TIME [epoch: 8.75 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20666202515901594		[learning rate: 0.0064973]
		[batch 20/20] avg loss: 0.20421545856987122		[learning rate: 0.0064873]
	Learning Rate: 0.00648734
	LOSS [training: 0.20543874186444358 | validation: 0.1277967069836462]
	TIME [epoch: 8.73 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1759582942009626		[learning rate: 0.0064774]
		[batch 20/20] avg loss: 0.17691958479547473		[learning rate: 0.0064675]
	Learning Rate: 0.00646745
	LOSS [training: 0.17643893949821865 | validation: 0.06173681129723943]
	TIME [epoch: 8.73 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1588450186329507		[learning rate: 0.0064575]
		[batch 20/20] avg loss: 0.09290651805441863		[learning rate: 0.0064476]
	Learning Rate: 0.00644763
	LOSS [training: 0.12587576834368466 | validation: 0.14328358897570748]
	TIME [epoch: 8.71 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06079681528370591		[learning rate: 0.0064377]
		[batch 20/20] avg loss: 0.06694402032494176		[learning rate: 0.0064279]
	Learning Rate: 0.00642786
	LOSS [training: 0.06387041780432386 | validation: 0.03208572624460864]
	TIME [epoch: 8.74 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029124795241110522		[learning rate: 0.006418]
		[batch 20/20] avg loss: 0.06347363943451285		[learning rate: 0.0064082]
	Learning Rate: 0.00640816
	LOSS [training: 0.046299217337811685 | validation: 0.17607069281582327]
	TIME [epoch: 8.75 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07929526131292267		[learning rate: 0.0063983]
		[batch 20/20] avg loss: 0.053183067441185575		[learning rate: 0.0063885]
	Learning Rate: 0.00638852
	LOSS [training: 0.06623916437705413 | validation: 0.08144998858170693]
	TIME [epoch: 8.72 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05795504620532173		[learning rate: 0.0063787]
		[batch 20/20] avg loss: 0.0678468963458694		[learning rate: 0.0063689]
	Learning Rate: 0.00636893
	LOSS [training: 0.06290097127559555 | validation: 0.0812221665908551]
	TIME [epoch: 8.73 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059287074320883225		[learning rate: 0.0063592]
		[batch 20/20] avg loss: 0.05793460297374785		[learning rate: 0.0063494]
	Learning Rate: 0.00634941
	LOSS [training: 0.05861083864731553 | validation: 0.04392258814126749]
	TIME [epoch: 8.74 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04708233193067551		[learning rate: 0.0063397]
		[batch 20/20] avg loss: 0.06453359223638772		[learning rate: 0.0063299]
	Learning Rate: 0.00632995
	LOSS [training: 0.055807962083531615 | validation: 0.08320901312881387]
	TIME [epoch: 8.75 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05564944005064587		[learning rate: 0.0063202]
		[batch 20/20] avg loss: 0.07185351693516492		[learning rate: 0.0063105]
	Learning Rate: 0.00631054
	LOSS [training: 0.0637514784929054 | validation: 0.028255648285810953]
	TIME [epoch: 8.74 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07136713017216319		[learning rate: 0.0063009]
		[batch 20/20] avg loss: 0.06731958034117891		[learning rate: 0.0062912]
	Learning Rate: 0.0062912
	LOSS [training: 0.06934335525667104 | validation: 0.03357118731505086]
	TIME [epoch: 8.74 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0595082276727968		[learning rate: 0.0062815]
		[batch 20/20] avg loss: 0.11824103647227054		[learning rate: 0.0062719]
	Learning Rate: 0.00627191
	LOSS [training: 0.08887463207253365 | validation: 0.14478340628369682]
	TIME [epoch: 8.73 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10475841203472376		[learning rate: 0.0062623]
		[batch 20/20] avg loss: 0.07058951737280698		[learning rate: 0.0062527]
	Learning Rate: 0.00625269
	LOSS [training: 0.08767396470376537 | validation: 0.04917967566668214]
	TIME [epoch: 8.78 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06760907539507292		[learning rate: 0.0062431]
		[batch 20/20] avg loss: 0.05265596324147921		[learning rate: 0.0062335]
	Learning Rate: 0.00623352
	LOSS [training: 0.06013251931827607 | validation: 0.05669471260767837]
	TIME [epoch: 8.73 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06335507069719518		[learning rate: 0.006224]
		[batch 20/20] avg loss: 0.09004632046399864		[learning rate: 0.0062144]
	Learning Rate: 0.00621441
	LOSS [training: 0.07670069558059692 | validation: 0.08040597034823724]
	TIME [epoch: 8.73 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054759015641200726		[learning rate: 0.0062049]
		[batch 20/20] avg loss: 0.0582849931803173		[learning rate: 0.0061954]
	Learning Rate: 0.00619536
	LOSS [training: 0.05652200441075903 | validation: 0.050630586115035396]
	TIME [epoch: 8.73 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05366656776134886		[learning rate: 0.0061859]
		[batch 20/20] avg loss: 0.04815482555428038		[learning rate: 0.0061764]
	Learning Rate: 0.00617637
	LOSS [training: 0.050910696657814614 | validation: 0.048976001323911944]
	TIME [epoch: 8.74 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05574488741102277		[learning rate: 0.0061669]
		[batch 20/20] avg loss: 0.06028678152508391		[learning rate: 0.0061574]
	Learning Rate: 0.00615744
	LOSS [training: 0.05801583446805334 | validation: 0.04472431779077404]
	TIME [epoch: 8.75 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11965355042829198		[learning rate: 0.006148]
		[batch 20/20] avg loss: 0.07617710260011706		[learning rate: 0.0061386]
	Learning Rate: 0.00613856
	LOSS [training: 0.09791532651420452 | validation: 0.07679031777806132]
	TIME [epoch: 8.72 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11112908493413742		[learning rate: 0.0061291]
		[batch 20/20] avg loss: 0.07591126583017724		[learning rate: 0.0061197]
	Learning Rate: 0.00611974
	LOSS [training: 0.09352017538215732 | validation: 0.0584759556898065]
	TIME [epoch: 8.72 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03992226280488438		[learning rate: 0.0061104]
		[batch 20/20] avg loss: 0.05301781758147023		[learning rate: 0.006101]
	Learning Rate: 0.00610099
	LOSS [training: 0.04647004019317731 | validation: 0.03996892424170391]
	TIME [epoch: 8.74 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056462462978811634		[learning rate: 0.0060916]
		[batch 20/20] avg loss: 0.0756438103390957		[learning rate: 0.0060823]
	Learning Rate: 0.00608228
	LOSS [training: 0.06605313665895367 | validation: 0.041915643549057365]
	TIME [epoch: 8.74 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046808961440994674		[learning rate: 0.006073]
		[batch 20/20] avg loss: 0.057987299371943316		[learning rate: 0.0060636]
	Learning Rate: 0.00606364
	LOSS [training: 0.05239813040646898 | validation: 0.0729577845541223]
	TIME [epoch: 8.72 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05060634198231186		[learning rate: 0.0060543]
		[batch 20/20] avg loss: 0.06501391346819782		[learning rate: 0.0060451]
	Learning Rate: 0.00604505
	LOSS [training: 0.057810127725254845 | validation: 0.06538747774607537]
	TIME [epoch: 8.72 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05946822278921952		[learning rate: 0.0060358]
		[batch 20/20] avg loss: 0.04771233957250209		[learning rate: 0.0060265]
	Learning Rate: 0.00602652
	LOSS [training: 0.0535902811808608 | validation: 0.05151939170781226]
	TIME [epoch: 8.73 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048136240266241644		[learning rate: 0.0060173]
		[batch 20/20] avg loss: 0.07690443002513742		[learning rate: 0.006008]
	Learning Rate: 0.00600805
	LOSS [training: 0.06252033514568953 | validation: 0.10559508115931902]
	TIME [epoch: 8.75 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11090210922787554		[learning rate: 0.0059988]
		[batch 20/20] avg loss: 0.07891660644650585		[learning rate: 0.0059896]
	Learning Rate: 0.00598963
	LOSS [training: 0.09490935783719066 | validation: 0.0323462596117991]
	TIME [epoch: 8.73 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049201140444966616		[learning rate: 0.0059804]
		[batch 20/20] avg loss: 0.05712691749935271		[learning rate: 0.0059713]
	Learning Rate: 0.00597127
	LOSS [training: 0.05316402897215966 | validation: 0.06136102595626232]
	TIME [epoch: 8.71 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0709236259745702		[learning rate: 0.0059621]
		[batch 20/20] avg loss: 0.05494425686976652		[learning rate: 0.005953]
	Learning Rate: 0.00595297
	LOSS [training: 0.06293394142216836 | validation: 0.05926433502786831]
	TIME [epoch: 8.72 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04253835566965482		[learning rate: 0.0059438]
		[batch 20/20] avg loss: 0.04800496577708671		[learning rate: 0.0059347]
	Learning Rate: 0.00593472
	LOSS [training: 0.04527166072337076 | validation: 0.0802951870642083]
	TIME [epoch: 8.74 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07877966104512599		[learning rate: 0.0059256]
		[batch 20/20] avg loss: 0.05182717189820082		[learning rate: 0.0059165]
	Learning Rate: 0.00591652
	LOSS [training: 0.06530341647166339 | validation: 0.05862556064865475]
	TIME [epoch: 8.75 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06266849545298994		[learning rate: 0.0059074]
		[batch 20/20] avg loss: 0.032448415265660405		[learning rate: 0.0058984]
	Learning Rate: 0.00589839
	LOSS [training: 0.04755845535932517 | validation: 0.025826826868522775]
	TIME [epoch: 8.72 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07002922393337192		[learning rate: 0.0058893]
		[batch 20/20] avg loss: 0.04743112745640274		[learning rate: 0.0058803]
	Learning Rate: 0.00588031
	LOSS [training: 0.058730175694887324 | validation: 0.039859324869313165]
	TIME [epoch: 8.73 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040928871493512986		[learning rate: 0.0058713]
		[batch 20/20] avg loss: 0.06324365143324209		[learning rate: 0.0058623]
	Learning Rate: 0.00586228
	LOSS [training: 0.05208626146337754 | validation: 0.05598640872894775]
	TIME [epoch: 8.72 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039251689366091955		[learning rate: 0.0058533]
		[batch 20/20] avg loss: 0.056103122730177035		[learning rate: 0.0058443]
	Learning Rate: 0.00584431
	LOSS [training: 0.047677406048134495 | validation: 0.0796391517369738]
	TIME [epoch: 8.76 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05528627589468763		[learning rate: 0.0058353]
		[batch 20/20] avg loss: 0.061734604440639926		[learning rate: 0.0058264]
	Learning Rate: 0.0058264
	LOSS [training: 0.05851044016766378 | validation: 0.04180878896508161]
	TIME [epoch: 8.73 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05342575112192962		[learning rate: 0.0058175]
		[batch 20/20] avg loss: 0.035231583817893		[learning rate: 0.0058085]
	Learning Rate: 0.00580854
	LOSS [training: 0.04432866746991132 | validation: 0.11711243022614543]
	TIME [epoch: 8.74 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0734311675218378		[learning rate: 0.0057996]
		[batch 20/20] avg loss: 0.05024060922087066		[learning rate: 0.0057907]
	Learning Rate: 0.00579073
	LOSS [training: 0.06183588837135422 | validation: 0.09187545218640522]
	TIME [epoch: 8.74 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06844427191556159		[learning rate: 0.0057818]
		[batch 20/20] avg loss: 0.05886274449419843		[learning rate: 0.005773]
	Learning Rate: 0.00577298
	LOSS [training: 0.06365350820488001 | validation: 0.04549844201493896]
	TIME [epoch: 8.76 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06419984223101718		[learning rate: 0.0057641]
		[batch 20/20] avg loss: 0.06107718229877819		[learning rate: 0.0057553]
	Learning Rate: 0.00575528
	LOSS [training: 0.06263851226489767 | validation: 0.09348815314370328]
	TIME [epoch: 8.73 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058971725475121695		[learning rate: 0.0057465]
		[batch 20/20] avg loss: 0.05512416837125682		[learning rate: 0.0057376]
	Learning Rate: 0.00573764
	LOSS [training: 0.05704794692318925 | validation: 0.05594574672653544]
	TIME [epoch: 8.73 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08607478914327191		[learning rate: 0.0057288]
		[batch 20/20] avg loss: 0.07590646046808265		[learning rate: 0.0057201]
	Learning Rate: 0.00572005
	LOSS [training: 0.08099062480567729 | validation: 0.10297458115663741]
	TIME [epoch: 8.73 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06458708466184529		[learning rate: 0.0057113]
		[batch 20/20] avg loss: 0.07547780911265754		[learning rate: 0.0057025]
	Learning Rate: 0.00570252
	LOSS [training: 0.07003244688725141 | validation: 0.17079570413210626]
	TIME [epoch: 8.73 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07378470637307817		[learning rate: 0.0056938]
		[batch 20/20] avg loss: 0.05972045064759498		[learning rate: 0.005685]
	Learning Rate: 0.00568504
	LOSS [training: 0.06675257851033659 | validation: 0.03311276376528837]
	TIME [epoch: 8.75 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047758539832217485		[learning rate: 0.0056763]
		[batch 20/20] avg loss: 0.05374083577517973		[learning rate: 0.0056676]
	Learning Rate: 0.00566761
	LOSS [training: 0.05074968780369861 | validation: 0.056745841497455055]
	TIME [epoch: 8.75 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050284799606603446		[learning rate: 0.0056589]
		[batch 20/20] avg loss: 0.03664231163596762		[learning rate: 0.0056502]
	Learning Rate: 0.00565024
	LOSS [training: 0.043463555621285524 | validation: 0.024644970168177825]
	TIME [epoch: 8.73 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033849762611117315		[learning rate: 0.0056416]
		[batch 20/20] avg loss: 0.03770106009916015		[learning rate: 0.0056329]
	Learning Rate: 0.00563292
	LOSS [training: 0.03577541135513874 | validation: 0.04791264434361603]
	TIME [epoch: 8.74 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046494072215481075		[learning rate: 0.0056243]
		[batch 20/20] avg loss: 0.03261918504036673		[learning rate: 0.0056156]
	Learning Rate: 0.00561565
	LOSS [training: 0.03955662862792391 | validation: 0.02119094939541036]
	TIME [epoch: 8.77 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06949353067076314		[learning rate: 0.005607]
		[batch 20/20] avg loss: 0.07899766703370718		[learning rate: 0.0055984]
	Learning Rate: 0.00559843
	LOSS [training: 0.07424559885223517 | validation: 0.045983099289987804]
	TIME [epoch: 8.73 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041052462660671425		[learning rate: 0.0055898]
		[batch 20/20] avg loss: 0.03250857192995878		[learning rate: 0.0055813]
	Learning Rate: 0.00558127
	LOSS [training: 0.036780517295315104 | validation: 0.04783765222452868]
	TIME [epoch: 8.72 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04030094852613793		[learning rate: 0.0055727]
		[batch 20/20] avg loss: 0.04333582134863861		[learning rate: 0.0055642]
	Learning Rate: 0.00556416
	LOSS [training: 0.04181838493738826 | validation: 0.025520782489553393]
	TIME [epoch: 8.72 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036540562869505855		[learning rate: 0.0055556]
		[batch 20/20] avg loss: 0.047671477184151365		[learning rate: 0.0055471]
	Learning Rate: 0.00554711
	LOSS [training: 0.04210602002682862 | validation: 0.03356087497956751]
	TIME [epoch: 8.75 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04813694026688733		[learning rate: 0.0055386]
		[batch 20/20] avg loss: 0.03676128849879791		[learning rate: 0.0055301]
	Learning Rate: 0.0055301
	LOSS [training: 0.04244911438284262 | validation: 0.018684130743396452]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_693.pth
	Model improved!!!
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0535943159805473		[learning rate: 0.0055216]
		[batch 20/20] avg loss: 0.04214002329735314		[learning rate: 0.0055132]
	Learning Rate: 0.00551315
	LOSS [training: 0.04786716963895022 | validation: 0.06296163783332964]
	TIME [epoch: 8.7 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06804759730467377		[learning rate: 0.0055047]
		[batch 20/20] avg loss: 0.06623084768384122		[learning rate: 0.0054963]
	Learning Rate: 0.00549625
	LOSS [training: 0.06713922249425748 | validation: 0.08213960511801881]
	TIME [epoch: 8.73 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08365265800905429		[learning rate: 0.0054878]
		[batch 20/20] avg loss: 0.07486620947023047		[learning rate: 0.0054794]
	Learning Rate: 0.0054794
	LOSS [training: 0.07925943373964238 | validation: 0.05897981621743583]
	TIME [epoch: 8.72 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0649464704358152		[learning rate: 0.005471]
		[batch 20/20] avg loss: 0.08901900434225987		[learning rate: 0.0054626]
	Learning Rate: 0.00546261
	LOSS [training: 0.07698273738903753 | validation: 0.08733400946826306]
	TIME [epoch: 8.76 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05016813785441201		[learning rate: 0.0054542]
		[batch 20/20] avg loss: 0.04333381348223107		[learning rate: 0.0054459]
	Learning Rate: 0.00544586
	LOSS [training: 0.04675097566832155 | validation: 0.017769064809802736]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r2_20240216_195422/states/model_tr_study2_698.pth
	Model improved!!!
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043971022109210216		[learning rate: 0.0054375]
		[batch 20/20] avg loss: 0.045315273063182816		[learning rate: 0.0054292]
	Learning Rate: 0.00542917
	LOSS [training: 0.044643147586196516 | validation: 0.04601580834110854]
	TIME [epoch: 8.7 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03956070961568163		[learning rate: 0.0054208]
		[batch 20/20] avg loss: 0.06109167087267888		[learning rate: 0.0054125]
	Learning Rate: 0.00541253
	LOSS [training: 0.05032619024418026 | validation: 0.09208785589497179]
	TIME [epoch: 8.69 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056708554902718665		[learning rate: 0.0054042]
		[batch 20/20] avg loss: 0.06952349641062744		[learning rate: 0.0053959]
	Learning Rate: 0.00539593
	LOSS [training: 0.06311602565667306 | validation: 0.08326063965368678]
	TIME [epoch: 8.71 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06272297987208718		[learning rate: 0.0053877]
		[batch 20/20] avg loss: 0.08071342546346545		[learning rate: 0.0053794]
	Learning Rate: 0.00537939
	LOSS [training: 0.07171820266777632 | validation: 0.06764282312855041]
	TIME [epoch: 8.68 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06539987592632138		[learning rate: 0.0053711]
		[batch 20/20] avg loss: 0.061185184022071806		[learning rate: 0.0053629]
	Learning Rate: 0.0053629
	LOSS [training: 0.06329252997419658 | validation: 0.03778682246985383]
	TIME [epoch: 8.68 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03924154028545267		[learning rate: 0.0053547]
		[batch 20/20] avg loss: 0.04916508023605366		[learning rate: 0.0053465]
	Learning Rate: 0.00534646
	LOSS [training: 0.044203310260753165 | validation: 0.059640551391611125]
	TIME [epoch: 8.68 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06604907665051146		[learning rate: 0.0053383]
		[batch 20/20] avg loss: 0.06857426314310373		[learning rate: 0.0053301]
	Learning Rate: 0.00533008
	LOSS [training: 0.06731166989680759 | validation: 0.05553410591027471]
	TIME [epoch: 8.71 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054723681363616994		[learning rate: 0.0053219]
		[batch 20/20] avg loss: 0.0556397856249088		[learning rate: 0.0053137]
	Learning Rate: 0.00531374
	LOSS [training: 0.0551817334942629 | validation: 0.085740758140861]
	TIME [epoch: 8.69 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12182033399457948		[learning rate: 0.0053056]
		[batch 20/20] avg loss: 0.0529574633970076		[learning rate: 0.0052974]
	Learning Rate: 0.00529745
	LOSS [training: 0.08738889869579355 | validation: 0.024331379278803522]
	TIME [epoch: 8.68 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044848561526371136		[learning rate: 0.0052893]
		[batch 20/20] avg loss: 0.051353088770673974		[learning rate: 0.0052812]
	Learning Rate: 0.00528121
	LOSS [training: 0.048100825148522555 | validation: 0.027411185310303073]
	TIME [epoch: 8.7 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03539918943506995		[learning rate: 0.0052731]
		[batch 20/20] avg loss: 0.05587905944157169		[learning rate: 0.005265]
	Learning Rate: 0.00526502
	LOSS [training: 0.04563912443832083 | validation: 0.03911356298923738]
	TIME [epoch: 8.7 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042456117698590916		[learning rate: 0.0052569]
		[batch 20/20] avg loss: 0.04657660559902617		[learning rate: 0.0052489]
	Learning Rate: 0.00524888
	LOSS [training: 0.04451636164880854 | validation: 0.048334299034690824]
	TIME [epoch: 8.69 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041284890861306975		[learning rate: 0.0052408]
		[batch 20/20] avg loss: 0.07019508518101762		[learning rate: 0.0052328]
	Learning Rate: 0.00523279
	LOSS [training: 0.05573998802116231 | validation: 0.04115556466116557]
	TIME [epoch: 8.69 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051988513529122295		[learning rate: 0.0052248]
		[batch 20/20] avg loss: 0.05203026582324284		[learning rate: 0.0052167]
	Learning Rate: 0.00521675
	LOSS [training: 0.05200938967618255 | validation: 0.02574601149894305]
	TIME [epoch: 8.68 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05035604111674676		[learning rate: 0.0052087]
		[batch 20/20] avg loss: 0.0609016153455586		[learning rate: 0.0052008]
	Learning Rate: 0.00520076
	LOSS [training: 0.055628828231152685 | validation: 0.1405170018750856]
	TIME [epoch: 8.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09495599600080233		[learning rate: 0.0051928]
		[batch 20/20] avg loss: 0.07888249981259104		[learning rate: 0.0051848]
	Learning Rate: 0.00518482
	LOSS [training: 0.08691924790669668 | validation: 0.0600017894549712]
	TIME [epoch: 8.71 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05272331485554649		[learning rate: 0.0051769]
		[batch 20/20] avg loss: 0.0888127780323954		[learning rate: 0.0051689]
	Learning Rate: 0.00516892
	LOSS [training: 0.07076804644397094 | validation: 0.10941809448066009]
	TIME [epoch: 8.7 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10437774770270943		[learning rate: 0.005161]
		[batch 20/20] avg loss: 0.05662084654197014		[learning rate: 0.0051531]
	Learning Rate: 0.00515308
	LOSS [training: 0.08049929712233979 | validation: 0.025102234950700922]
	TIME [epoch: 8.69 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026477303345470393		[learning rate: 0.0051452]
		[batch 20/20] avg loss: 0.05055903768816825		[learning rate: 0.0051373]
	Learning Rate: 0.00513728
	LOSS [training: 0.03851817051681933 | validation: 0.056403239462730216]
	TIME [epoch: 8.68 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03471957702117741		[learning rate: 0.0051294]
		[batch 20/20] avg loss: 0.041666532429408716		[learning rate: 0.0051215]
	Learning Rate: 0.00512153
	LOSS [training: 0.038193054725293066 | validation: 0.04915083767372814]
	TIME [epoch: 8.69 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05637142845727268		[learning rate: 0.0051137]
		[batch 20/20] avg loss: 0.038840653001486584		[learning rate: 0.0051058]
	Learning Rate: 0.00510583
	LOSS [training: 0.04760604072937964 | validation: 0.06724418261229051]
	TIME [epoch: 8.68 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0715378877526634		[learning rate: 0.005098]
		[batch 20/20] avg loss: 0.04075448917921526		[learning rate: 0.0050902]
	Learning Rate: 0.00509018
	LOSS [training: 0.05614618846593933 | validation: 0.025315893210520184]
	TIME [epoch: 8.7 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028978427103142346		[learning rate: 0.0050824]
		[batch 20/20] avg loss: 0.03405627014388495		[learning rate: 0.0050746]
	Learning Rate: 0.00507458
	LOSS [training: 0.031517348623513644 | validation: 0.09045437566920946]
	TIME [epoch: 8.7 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05417309229097946		[learning rate: 0.0050668]
		[batch 20/20] avg loss: 0.02976288497070876		[learning rate: 0.005059]
	Learning Rate: 0.00505902
	LOSS [training: 0.04196798863084411 | validation: 0.03353421461182346]
	TIME [epoch: 8.71 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05706022352840916		[learning rate: 0.0050513]
		[batch 20/20] avg loss: 0.10138108213495567		[learning rate: 0.0050435]
	Learning Rate: 0.00504352
	LOSS [training: 0.07922065283168242 | validation: 0.27377699217079554]
	TIME [epoch: 8.74 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11286147345290957		[learning rate: 0.0050358]
		[batch 20/20] avg loss: 0.06182779929938449		[learning rate: 0.0050281]
	Learning Rate: 0.00502805
	LOSS [training: 0.08734463637614703 | validation: 0.06945327058105472]
	TIME [epoch: 8.69 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049679833093977346		[learning rate: 0.0050203]
		[batch 20/20] avg loss: 0.035128184569841354		[learning rate: 0.0050126]
	Learning Rate: 0.00501264
	LOSS [training: 0.04240400883190935 | validation: 0.023190364438297424]
	TIME [epoch: 8.72 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04033225969093658		[learning rate: 0.005005]
		[batch 20/20] avg loss: 0.06733667481247632		[learning rate: 0.0049973]
	Learning Rate: 0.00499728
	LOSS [training: 0.05383446725170645 | validation: 0.0358025517689199]
	TIME [epoch: 8.71 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02923844187224985		[learning rate: 0.0049896]
		[batch 20/20] avg loss: 0.06792517474039564		[learning rate: 0.004982]
	Learning Rate: 0.00498196
	LOSS [training: 0.04858180830632275 | validation: 0.05285823840643135]
	TIME [epoch: 8.75 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08943576658926367		[learning rate: 0.0049743]
		[batch 20/20] avg loss: 0.06829760437811419		[learning rate: 0.0049667]
	Learning Rate: 0.00496669
	LOSS [training: 0.07886668548368894 | validation: 0.038687994922109135]
	TIME [epoch: 8.72 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04277654827044568		[learning rate: 0.0049591]
		[batch 20/20] avg loss: 0.05478964784016046		[learning rate: 0.0049515]
	Learning Rate: 0.00495146
	LOSS [training: 0.048783098055303065 | validation: 0.04931257532767]
	TIME [epoch: 8.71 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05218441325204519		[learning rate: 0.0049439]
		[batch 20/20] avg loss: 0.05304718463874958		[learning rate: 0.0049363]
	Learning Rate: 0.00493628
	LOSS [training: 0.052615798945397384 | validation: 0.03498738089879684]
	TIME [epoch: 8.7 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037499824908304336		[learning rate: 0.0049287]
		[batch 20/20] avg loss: 0.041838634231573774		[learning rate: 0.0049211]
	Learning Rate: 0.00492115
	LOSS [training: 0.03966922956993906 | validation: 0.07243504386789072]
	TIME [epoch: 8.73 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04113679378052364		[learning rate: 0.0049136]
		[batch 20/20] avg loss: 0.03709968376256569		[learning rate: 0.0049061]
	Learning Rate: 0.00490607
	LOSS [training: 0.03911823877154466 | validation: 0.04308435920011349]
	TIME [epoch: 8.72 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044182172203665646		[learning rate: 0.0048985]
		[batch 20/20] avg loss: 0.03707278027137721		[learning rate: 0.004891]
	Learning Rate: 0.00489103
	LOSS [training: 0.04062747623752142 | validation: 0.019242367080689052]
	TIME [epoch: 8.71 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05826935896642351		[learning rate: 0.0048835]
		[batch 20/20] avg loss: 0.02616072556557193		[learning rate: 0.004876]
	Learning Rate: 0.00487603
	LOSS [training: 0.04221504226599772 | validation: 0.03067056660617569]
	TIME [epoch: 8.72 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043991114195318134		[learning rate: 0.0048686]
		[batch 20/20] avg loss: 0.09268440978605219		[learning rate: 0.0048611]
	Learning Rate: 0.00486109
	LOSS [training: 0.06833776199068516 | validation: 0.11020258829709238]
	TIME [epoch: 8.71 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09280298008693552		[learning rate: 0.0048536]
		[batch 20/20] avg loss: 0.07654312121982002		[learning rate: 0.0048462]
	Learning Rate: 0.00484618
	LOSS [training: 0.08467305065337775 | validation: 0.1599276932833668]
	TIME [epoch: 8.75 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0912492418369968		[learning rate: 0.0048388]
		[batch 20/20] avg loss: 0.06604865595033925		[learning rate: 0.0048313]
	Learning Rate: 0.00483133
	LOSS [training: 0.07864894889366801 | validation: 0.05748946710031595]
	TIME [epoch: 8.71 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06493645826056167		[learning rate: 0.0048239]
		[batch 20/20] avg loss: 0.0607781161062166		[learning rate: 0.0048165]
	Learning Rate: 0.00481652
	LOSS [training: 0.06285728718338915 | validation: 0.06933864697381151]
	TIME [epoch: 8.71 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15563238666714513		[learning rate: 0.0048091]
		[batch 20/20] avg loss: 0.06396207740455633		[learning rate: 0.0048018]
	Learning Rate: 0.00480176
	LOSS [training: 0.10979723203585075 | validation: 0.050351729842600286]
	TIME [epoch: 8.71 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03753692300437028		[learning rate: 0.0047944]
		[batch 20/20] avg loss: 0.03951840427065388		[learning rate: 0.004787]
	Learning Rate: 0.00478704
	LOSS [training: 0.03852766363751209 | validation: 0.050515764772458165]
	TIME [epoch: 8.74 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06596422487232159		[learning rate: 0.0047797]
		[batch 20/20] avg loss: 0.03009844542594799		[learning rate: 0.0047724]
	Learning Rate: 0.00477236
	LOSS [training: 0.048031335149134796 | validation: 0.04992092866379322]
	TIME [epoch: 8.71 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050502318518078716		[learning rate: 0.004765]
		[batch 20/20] avg loss: 0.05864851082604897		[learning rate: 0.0047577]
	Learning Rate: 0.00475773
	LOSS [training: 0.05457541467206384 | validation: 0.038266678045383584]
	TIME [epoch: 8.72 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07084356969687806		[learning rate: 0.0047504]
		[batch 20/20] avg loss: 0.059018603623009404		[learning rate: 0.0047431]
	Learning Rate: 0.00474315
	LOSS [training: 0.06493108665994372 | validation: 0.0649655872743505]
	TIME [epoch: 8.71 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045490684314825114		[learning rate: 0.0047359]
		[batch 20/20] avg loss: 0.0725053402381946		[learning rate: 0.0047286]
	Learning Rate: 0.00472861
	LOSS [training: 0.05899801227650986 | validation: 0.04439357769683735]
	TIME [epoch: 8.71 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043283656789640176		[learning rate: 0.0047214]
		[batch 20/20] avg loss: 0.05759068804918131		[learning rate: 0.0047141]
	Learning Rate: 0.00471411
	LOSS [training: 0.050437172419410745 | validation: 0.05193794303726874]
	TIME [epoch: 8.72 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032719386826513		[learning rate: 0.0047069]
		[batch 20/20] avg loss: 0.08298752256008689		[learning rate: 0.0046997]
	Learning Rate: 0.00469966
	LOSS [training: 0.057853454693299944 | validation: 0.07105844789610577]
	TIME [epoch: 8.72 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04043820505086006		[learning rate: 0.0046925]
		[batch 20/20] avg loss: 0.03319942570697317		[learning rate: 0.0046853]
	Learning Rate: 0.00468526
	LOSS [training: 0.03681881537891661 | validation: 0.021208783860004167]
	TIME [epoch: 8.7 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04235340588341014		[learning rate: 0.0046781]
		[batch 20/20] avg loss: 0.03415530186676849		[learning rate: 0.0046709]
	Learning Rate: 0.00467089
	LOSS [training: 0.03825435387508931 | validation: 0.022820157947368414]
	TIME [epoch: 8.68 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034588347985875165		[learning rate: 0.0046637]
		[batch 20/20] avg loss: 0.10175213148180813		[learning rate: 0.0046566]
	Learning Rate: 0.00465658
	LOSS [training: 0.06817023973384165 | validation: 0.04408283346590607]
	TIME [epoch: 8.73 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05024971695734243		[learning rate: 0.0046494]
		[batch 20/20] avg loss: 0.06449653317071867		[learning rate: 0.0046423]
	Learning Rate: 0.0046423
	LOSS [training: 0.05737312506403055 | validation: 0.025831546212647934]
	TIME [epoch: 8.69 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035020895222027434		[learning rate: 0.0046352]
		[batch 20/20] avg loss: 0.0374242432503367		[learning rate: 0.0046281]
	Learning Rate: 0.00462807
	LOSS [training: 0.03622256923618207 | validation: 0.039881256794532596]
	TIME [epoch: 8.7 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03288438787554814		[learning rate: 0.004621]
		[batch 20/20] avg loss: 0.04362287861757993		[learning rate: 0.0046139]
	Learning Rate: 0.00461388
	LOSS [training: 0.03825363324656404 | validation: 0.025431387915705324]
	TIME [epoch: 8.69 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06345689187950532		[learning rate: 0.0046068]
		[batch 20/20] avg loss: 0.04484759337114598		[learning rate: 0.0045997]
	Learning Rate: 0.00459974
	LOSS [training: 0.05415224262532563 | validation: 0.03056803157672242]
	TIME [epoch: 8.74 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046836884049381945		[learning rate: 0.0045927]
		[batch 20/20] avg loss: 0.03400758992832414		[learning rate: 0.0045856]
	Learning Rate: 0.00458564
	LOSS [training: 0.040422236988853055 | validation: 0.055857641913052716]
	TIME [epoch: 8.71 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035546967492902024		[learning rate: 0.0045786]
		[batch 20/20] avg loss: 0.05544013275594539		[learning rate: 0.0045716]
	Learning Rate: 0.00457158
	LOSS [training: 0.04549355012442371 | validation: 0.12529116616800642]
	TIME [epoch: 8.69 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08262562229311435		[learning rate: 0.0045646]
		[batch 20/20] avg loss: nan		[learning rate: 0.0045576]
ERROR:
nan encountered in epoch 755 (training loss).
