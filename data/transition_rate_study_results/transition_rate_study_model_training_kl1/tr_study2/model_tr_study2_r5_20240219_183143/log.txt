Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r5', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3909036665

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.97749456286359		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.217720936345957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.597607749604773 | validation: 9.916848640797056]
	TIME [epoch: 53 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.675513851884931		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.971561774108894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.323537812996913 | validation: 9.36920567521424]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.796759845476254		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.098165607713119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.447462726594685 | validation: 9.041005984484219]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.763742238280516		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.113981743991411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.938861991135962 | validation: 4.791271187605386]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.175346391658899		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6715142815956674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.923430336627283 | validation: 2.9047285838399226]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.547980657044616		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.991688720785639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2698346889151275 | validation: 1.6866952289982127]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0011492970409197		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.67688299946727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.839016148254095 | validation: 1.2160238902217162]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7248257999199814		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4473616418873274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5860937209036545 | validation: 2.171381628194145]
	TIME [epoch: 8.34 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4844212228654847		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2571353116807973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3707782672731408 | validation: 1.0405150297950028]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2455634681249896		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8659545000086121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0557589840668007 | validation: 0.5718440730282175]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9577300513393228		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8807204041414088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9192252277403659 | validation: 0.761310421459521]
	TIME [epoch: 8.33 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7916269321450132		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9253042550514875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8584655935982501 | validation: 0.5412380688561083]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7493337297943979		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5695571783686779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6594454540815378 | validation: 0.45876064068134853]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6978316314303871		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6937171009815455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6957743662059661 | validation: 0.5458310255886141]
	TIME [epoch: 8.35 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6352265963151268		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5998601510849357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6175433737000312 | validation: 0.5332333869410046]
	TIME [epoch: 8.32 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7960573539253107		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8343615255983814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8152094397618461 | validation: 0.5097607487531256]
	TIME [epoch: 8.33 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7200700684537529		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7095211707656731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.714795619609713 | validation: 0.5477224735685415]
	TIME [epoch: 8.35 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5623615681459115		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5932352899742492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5777984290600803 | validation: 0.5226111860119423]
	TIME [epoch: 8.36 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6707031699379635		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.669611362262983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6701572661004733 | validation: 0.6333829694125946]
	TIME [epoch: 8.33 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.662455440366644		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5535686420247747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6080120411957093 | validation: 0.4885657150527349]
	TIME [epoch: 8.35 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.619089191124217		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6408458069796925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6299674990519548 | validation: 0.868144977600062]
	TIME [epoch: 8.34 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.554649248089123		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6532359507367063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6039425994129146 | validation: 0.4882752256629757]
	TIME [epoch: 8.35 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6354255835780643		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7471566365841559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6912911100811102 | validation: 0.5649558001786756]
	TIME [epoch: 8.34 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5536011976331527		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5573107236431505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5554559606381516 | validation: 0.5082976865113841]
	TIME [epoch: 8.33 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49149147378074687		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6756844262899142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5835879500353304 | validation: 0.5298730435851842]
	TIME [epoch: 8.35 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.619639080659449		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5250174675925702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5723282741260096 | validation: 0.4086817224924779]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5523760421753963		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8035618018783331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6779689220268648 | validation: 0.5508448856730164]
	TIME [epoch: 8.34 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5803714236996068		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6304355976653259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6054035106824662 | validation: 0.835670186877741]
	TIME [epoch: 8.36 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6376605032934768		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4968789861295598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5672697447115185 | validation: 0.4112038160492593]
	TIME [epoch: 8.33 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48858331513566844		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4712748357040642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4799290754198663 | validation: 0.45255193275605493]
	TIME [epoch: 8.35 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5554042155900198		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5544765059333614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5549403607616906 | validation: 0.3629640963305844]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5601273463244573		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5463574149233231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5532423806238902 | validation: 0.4534352353600555]
	TIME [epoch: 8.33 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7038620271446278		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49041178601697544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5971369065808017 | validation: 0.5717548087783259]
	TIME [epoch: 8.34 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5036460475835902		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4780333639914133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49083970578750175 | validation: 0.5474868169107686]
	TIME [epoch: 8.35 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4872765741340836		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5129252171090168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5001008956215502 | validation: 0.5786198952072586]
	TIME [epoch: 8.32 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4237814442485225		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5119926274772287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4678870358628756 | validation: 0.581257044818632]
	TIME [epoch: 8.36 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4332729718403952		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4714037782779556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4523383750591754 | validation: 0.40248751978550346]
	TIME [epoch: 8.31 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3930209674875406		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4016543179519442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3973376427197423 | validation: 0.36314155688056293]
	TIME [epoch: 8.33 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3883095250375824		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.34505191979963623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36668072241860933 | validation: 0.24742280223612473]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3973386732450262		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.42657403856441184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.411956355904719 | validation: 0.5094833736139666]
	TIME [epoch: 8.32 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5168656970371949		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49157750229976643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5042215996684807 | validation: 0.38460577689695213]
	TIME [epoch: 8.34 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4051987658703139		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5831335606760931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49416616327320345 | validation: 0.2362706330278733]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3680729274425702		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3687310958534847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36840201164802744 | validation: 0.34389929254810775]
	TIME [epoch: 8.35 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39317302306260743		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.41932317086010473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.406248096961356 | validation: 0.5002528768380863]
	TIME [epoch: 8.36 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3049128206849815		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28243026504143176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2936715428632066 | validation: 0.3467174930458601]
	TIME [epoch: 8.34 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2837364094141907		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30804255268258346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29588948104838714 | validation: 0.6513108248480635]
	TIME [epoch: 8.33 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31806030813127456		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3446966036582576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33137845589476606 | validation: 0.21852700284307952]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.297973976289276		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36880639081317074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33339018355122335 | validation: 0.378002849479783]
	TIME [epoch: 8.37 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33369510218164783		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2598104215812973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2967527618814726 | validation: 0.17943961977877776]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44052159422713444		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21694575670857685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3287336754678556 | validation: 1.1427733722989744]
	TIME [epoch: 8.36 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43503424799787505		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20884773168402146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3219409898409483 | validation: 0.1972581525311875]
	TIME [epoch: 8.37 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21397068017472556		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.43375143050653087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32386105534062826 | validation: 0.20996469799058837]
	TIME [epoch: 8.36 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22501761491739924		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32698561303573315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2760016139765662 | validation: 0.1950508119350074]
	TIME [epoch: 8.38 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24607074011489302		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2035577624206562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22481425126777452 | validation: 0.18536617114707596]
	TIME [epoch: 8.35 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29525531226352253		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22135528240779592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2583052973356592 | validation: 0.24158837869170946]
	TIME [epoch: 8.37 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21720079371887016		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.224511140353128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22085596703599902 | validation: 0.19461845292805655]
	TIME [epoch: 8.35 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15969156157459968		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21264955397933946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1861705577769696 | validation: 0.22421118643854698]
	TIME [epoch: 8.33 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3205859926037724		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16727754185129845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24393176722753535 | validation: 0.13388398971793417]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21324163754881784		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22756740014857368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22040451884869575 | validation: 0.29554094267926884]
	TIME [epoch: 8.37 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36688133702729		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1870135035969438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2769474203121169 | validation: 0.29327013586680994]
	TIME [epoch: 8.34 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1532214387468325		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16893371837335197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16107757856009225 | validation: 0.17876344239428346]
	TIME [epoch: 8.36 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2573623714562866		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28089660443915154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26912948794771907 | validation: 0.3830102940143061]
	TIME [epoch: 8.36 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3724495446802513		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15259440631466598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26252197549745865 | validation: 0.08246613750213698]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1816997816493903		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23029709770760382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20599843967849701 | validation: 0.08597313785952393]
	TIME [epoch: 8.33 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5184501206766006		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25066174613709147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3845559334068461 | validation: 0.2859772990570563]
	TIME [epoch: 8.32 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3110229458216594		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2010718335430126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25604738968233604 | validation: 0.407063176057944]
	TIME [epoch: 8.34 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3430623515659514		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18928569136873744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2661740214673444 | validation: 0.176808336560974]
	TIME [epoch: 8.36 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13515212963384793		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.202696040842049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1689240852379485 | validation: 0.23831301801547408]
	TIME [epoch: 8.34 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1912966116404344		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.141452789874923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16637470075767874 | validation: 0.11244021965393519]
	TIME [epoch: 8.37 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22225633814526374		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18987841748878745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2060673778170256 | validation: 0.2866521050345162]
	TIME [epoch: 8.33 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24049212527700772		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29952095762463826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.270006541450823 | validation: 0.16032253388170867]
	TIME [epoch: 8.35 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20416224431110774		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14470398096638742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1744331126387476 | validation: 0.14574965550723373]
	TIME [epoch: 8.33 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2564642805316449		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22895587393893707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2427100772352909 | validation: 0.1592301296663254]
	TIME [epoch: 8.34 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22769822198609177		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3594888312450339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2935935266155628 | validation: 0.18651848437592566]
	TIME [epoch: 8.34 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1707600450353775		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1517867619601393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1612734034977584 | validation: 0.08968712249519045]
	TIME [epoch: 8.36 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12996529666178175		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28765210718306944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20880870192242557 | validation: 0.09568122926489217]
	TIME [epoch: 8.33 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16317250603946643		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26005149967041513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21161200285494072 | validation: 0.1352089128002533]
	TIME [epoch: 8.36 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2219461522442978		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23036360315394688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22615487769912238 | validation: 0.2914493336011424]
	TIME [epoch: 8.33 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17844243901644227		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1656536140524897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17204802653446596 | validation: 0.13108860356617114]
	TIME [epoch: 8.35 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2176132501548272		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21528802216102388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21645063615792556 | validation: 0.149184232629858]
	TIME [epoch: 8.33 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17239429613085708		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1451795077893017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15878690196007939 | validation: 0.13673326522420975]
	TIME [epoch: 8.33 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1509574791216663		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24014862487073918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19555305199620276 | validation: 0.4476473907094186]
	TIME [epoch: 8.33 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35433238007062484		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18841749876738795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27137493941900637 | validation: 0.20261333307216228]
	TIME [epoch: 8.38 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2705582463390683		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3131110419109449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2918346441250066 | validation: 0.16782401464415753]
	TIME [epoch: 8.34 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24866839677877958		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5352582805436459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39196333866121275 | validation: 0.3014600320540789]
	TIME [epoch: 8.34 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21170365095966934		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.827515742592864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5196096967762667 | validation: 0.5061709017445986]
	TIME [epoch: 8.37 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49494301767342835		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15088783603471495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32291542685407176 | validation: 0.437903638363106]
	TIME [epoch: 8.35 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30612249391695423		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23268973739511772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26940611565603595 | validation: 0.3795548756782251]
	TIME [epoch: 8.33 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2500832169973203		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23818030170545143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24413175935138587 | validation: 0.2107349054460132]
	TIME [epoch: 8.33 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14005576348100882		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28169204434478856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2108739039128987 | validation: 0.24745148205691153]
	TIME [epoch: 8.32 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17958642386899518		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3317168660469866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25565164495799086 | validation: 0.21735594410013834]
	TIME [epoch: 8.37 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17409152299146205		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16387438049809344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16898295174477773 | validation: 0.20792017077442093]
	TIME [epoch: 8.34 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21351805187936557		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23783632279697958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22567718733817252 | validation: 0.08986016026784752]
	TIME [epoch: 8.34 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2586954301814261		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2077082327125146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23320183144697032 | validation: 0.19516492586040188]
	TIME [epoch: 8.34 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16428364717269694		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5777680537676879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37102585047019243 | validation: 0.14484081150621878]
	TIME [epoch: 8.37 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22068147974399932		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18287660667337566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2017790432086875 | validation: 0.07722295960563505]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14320520519764918		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1687275793239167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15596639226078296 | validation: 0.17059903478922844]
	TIME [epoch: 8.33 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15856891059839762		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17809168554618499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16833029807229133 | validation: 0.1774342292482006]
	TIME [epoch: 8.33 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28563853179830045		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19229063021818021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2389645810082403 | validation: 0.20026846801520076]
	TIME [epoch: 8.35 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23727307425331684		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17486183118911144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20606745272121416 | validation: 0.1466360247290737]
	TIME [epoch: 8.36 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20040049159755235		[learning rate: 0.0099891]
		[batch 20/20] avg loss: 0.18518517016137392		[learning rate: 0.009977]
	Learning Rate: 0.009977
	LOSS [training: 0.19279283087946317 | validation: 0.20169896697707979]
	TIME [epoch: 8.34 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1478562675439512		[learning rate: 0.0099649]
		[batch 20/20] avg loss: 1.1577188846329203		[learning rate: 0.0099528]
	Learning Rate: 0.00995285
	LOSS [training: 0.6527875760884357 | validation: 0.2191919497724664]
	TIME [epoch: 8.34 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22312483012173553		[learning rate: 0.0099408]
		[batch 20/20] avg loss: 0.12422728516132313		[learning rate: 0.0099288]
	Learning Rate: 0.00992875
	LOSS [training: 0.17367605764152932 | validation: 0.07218195821484832]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13588807996535573		[learning rate: 0.0099167]
		[batch 20/20] avg loss: 0.1382138146306045		[learning rate: 0.0099047]
	Learning Rate: 0.00990472
	LOSS [training: 0.13705094729798012 | validation: 0.11745056651007799]
	TIME [epoch: 8.34 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2035786654417958		[learning rate: 0.0098927]
		[batch 20/20] avg loss: 0.1897652713792929		[learning rate: 0.0098807]
	Learning Rate: 0.00988074
	LOSS [training: 0.19667196841054435 | validation: 0.1730733193221839]
	TIME [epoch: 8.33 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13677368730086015		[learning rate: 0.0098688]
		[batch 20/20] avg loss: 0.11871137054065492		[learning rate: 0.0098568]
	Learning Rate: 0.00985682
	LOSS [training: 0.12774252892075752 | validation: 0.27318613360428806]
	TIME [epoch: 8.33 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24547929007348035		[learning rate: 0.0098449]
		[batch 20/20] avg loss: 0.17867930054046627		[learning rate: 0.009833]
	Learning Rate: 0.00983296
	LOSS [training: 0.21207929530697336 | validation: 0.10308671955238781]
	TIME [epoch: 8.36 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1274291860267256		[learning rate: 0.009821]
		[batch 20/20] avg loss: 0.17844544092369516		[learning rate: 0.0098092]
	Learning Rate: 0.00980915
	LOSS [training: 0.1529373134752104 | validation: 0.12825685225718805]
	TIME [epoch: 8.34 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13998381232208257		[learning rate: 0.0097973]
		[batch 20/20] avg loss: 0.2716117569368012		[learning rate: 0.0097854]
	Learning Rate: 0.00978541
	LOSS [training: 0.20579778462944187 | validation: 0.19302757360041706]
	TIME [epoch: 8.34 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16710974714268995		[learning rate: 0.0097736]
		[batch 20/20] avg loss: 0.24133489910775366		[learning rate: 0.0097617]
	Learning Rate: 0.00976172
	LOSS [training: 0.20422232312522176 | validation: 0.10571441751372633]
	TIME [epoch: 8.34 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14265069146631876		[learning rate: 0.0097499]
		[batch 20/20] avg loss: 0.16852179685355992		[learning rate: 0.0097381]
	Learning Rate: 0.00973809
	LOSS [training: 0.15558624415993932 | validation: 0.08771733337685621]
	TIME [epoch: 8.36 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16115860359429374		[learning rate: 0.0097263]
		[batch 20/20] avg loss: 0.32304837448545304		[learning rate: 0.0097145]
	Learning Rate: 0.00971451
	LOSS [training: 0.2421034890398734 | validation: 0.2626203488659057]
	TIME [epoch: 8.37 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28152337528603255		[learning rate: 0.0097027]
		[batch 20/20] avg loss: 0.277426879197213		[learning rate: 0.009691]
	Learning Rate: 0.009691
	LOSS [training: 0.27947512724162277 | validation: 0.10576464414008015]
	TIME [epoch: 8.32 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16062684262811697		[learning rate: 0.0096793]
		[batch 20/20] avg loss: 0.12462780077498314		[learning rate: 0.0096675]
	Learning Rate: 0.00966754
	LOSS [training: 0.14262732170155007 | validation: 0.09868298071469517]
	TIME [epoch: 8.33 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15594066587453018		[learning rate: 0.0096558]
		[batch 20/20] avg loss: 0.20294915433720964		[learning rate: 0.0096441]
	Learning Rate: 0.00964413
	LOSS [training: 0.17944491010586988 | validation: 0.23047677488990648]
	TIME [epoch: 8.35 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21259436769227347		[learning rate: 0.0096325]
		[batch 20/20] avg loss: 0.17359318196021775		[learning rate: 0.0096208]
	Learning Rate: 0.00962078
	LOSS [training: 0.19309377482624562 | validation: 0.17994587223421163]
	TIME [epoch: 8.33 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15820450520466317		[learning rate: 0.0096091]
		[batch 20/20] avg loss: 0.2419703468377436		[learning rate: 0.0095975]
	Learning Rate: 0.00959749
	LOSS [training: 0.2000874260212034 | validation: 0.10841154649131196]
	TIME [epoch: 8.34 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22061378505682608		[learning rate: 0.0095859]
		[batch 20/20] avg loss: 0.1936951677798477		[learning rate: 0.0095743]
	Learning Rate: 0.00957426
	LOSS [training: 0.2071544764183369 | validation: 0.2096769219522986]
	TIME [epoch: 8.34 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14308002528789449		[learning rate: 0.0095627]
		[batch 20/20] avg loss: 0.29601935205827135		[learning rate: 0.0095511]
	Learning Rate: 0.00955108
	LOSS [training: 0.21954968867308286 | validation: 0.1268023527199891]
	TIME [epoch: 8.36 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13424840504100424		[learning rate: 0.0095395]
		[batch 20/20] avg loss: 0.16470811925048018		[learning rate: 0.009528]
	Learning Rate: 0.00952796
	LOSS [training: 0.14947826214574225 | validation: 0.13128855859993355]
	TIME [epoch: 8.36 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16064687551450674		[learning rate: 0.0095164]
		[batch 20/20] avg loss: 0.23575358797402002		[learning rate: 0.0095049]
	Learning Rate: 0.0095049
	LOSS [training: 0.1982002317442634 | validation: 0.11951091813087379]
	TIME [epoch: 8.35 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1450289134326456		[learning rate: 0.0094934]
		[batch 20/20] avg loss: 0.8100811862063251		[learning rate: 0.0094819]
	Learning Rate: 0.00948189
	LOSS [training: 0.47755504981948543 | validation: 1.049996348415365]
	TIME [epoch: 8.34 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0137241293316843		[learning rate: 0.0094704]
		[batch 20/20] avg loss: 0.13342652276186157		[learning rate: 0.0094589]
	Learning Rate: 0.00945893
	LOSS [training: 0.573575326046773 | validation: 0.06265951425100358]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13024538888149526		[learning rate: 0.0094475]
		[batch 20/20] avg loss: 0.1358255540381182		[learning rate: 0.009436]
	Learning Rate: 0.00943603
	LOSS [training: 0.13303547145980674 | validation: 0.11703071119835864]
	TIME [epoch: 8.33 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.127601992265803		[learning rate: 0.0094246]
		[batch 20/20] avg loss: 0.1516189208539139		[learning rate: 0.0094132]
	Learning Rate: 0.00941319
	LOSS [training: 0.13961045655985843 | validation: 0.14431387545433078]
	TIME [epoch: 8.35 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1428134095389566		[learning rate: 0.0094018]
		[batch 20/20] avg loss: 0.19649560617955644		[learning rate: 0.0093904]
	Learning Rate: 0.0093904
	LOSS [training: 0.16965450785925654 | validation: 0.1399206688530794]
	TIME [epoch: 8.34 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10997108792207799		[learning rate: 0.009379]
		[batch 20/20] avg loss: 0.10251940797787014		[learning rate: 0.0093677]
	Learning Rate: 0.00936767
	LOSS [training: 0.10624524794997406 | validation: 0.07225712916243707]
	TIME [epoch: 8.36 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37286576796862025		[learning rate: 0.0093563]
		[batch 20/20] avg loss: 1.2903881875920127		[learning rate: 0.009345]
	Learning Rate: 0.00934499
	LOSS [training: 0.8316269777803166 | validation: 0.5347359756114269]
	TIME [epoch: 8.36 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40119419463656475		[learning rate: 0.0093337]
		[batch 20/20] avg loss: 0.1942743769141772		[learning rate: 0.0093224]
	Learning Rate: 0.00932237
	LOSS [training: 0.297734285775371 | validation: 0.09842266932273508]
	TIME [epoch: 8.33 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13643331736101405		[learning rate: 0.0093111]
		[batch 20/20] avg loss: 0.1558144073525673		[learning rate: 0.0092998]
	Learning Rate: 0.0092998
	LOSS [training: 0.14612386235679065 | validation: 0.18058229057569972]
	TIME [epoch: 8.33 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3006129189927407		[learning rate: 0.0092885]
		[batch 20/20] avg loss: 0.9314390755732737		[learning rate: 0.0092773]
	Learning Rate: 0.00927729
	LOSS [training: 0.6160259972830071 | validation: 0.262717025324082]
	TIME [epoch: 8.35 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18887603367387504		[learning rate: 0.0092661]
		[batch 20/20] avg loss: 0.12455404066920699		[learning rate: 0.0092548]
	Learning Rate: 0.00925483
	LOSS [training: 0.156715037171541 | validation: 0.08195417777855643]
	TIME [epoch: 8.34 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2292211338203526		[learning rate: 0.0092436]
		[batch 20/20] avg loss: 0.5209446264884416		[learning rate: 0.0092324]
	Learning Rate: 0.00923242
	LOSS [training: 0.3750828801543971 | validation: 0.1633147004682132]
	TIME [epoch: 8.33 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1701174982968635		[learning rate: 0.0092212]
		[batch 20/20] avg loss: 0.16335171773747187		[learning rate: 0.0092101]
	Learning Rate: 0.00921007
	LOSS [training: 0.16673460801716772 | validation: 0.16258793656136547]
	TIME [epoch: 8.33 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16137561792574476		[learning rate: 0.0091989]
		[batch 20/20] avg loss: 0.18337266228927732		[learning rate: 0.0091878]
	Learning Rate: 0.00918778
	LOSS [training: 0.1723741401075111 | validation: 0.07725216113403698]
	TIME [epoch: 8.38 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13717650453877775		[learning rate: 0.0091767]
		[batch 20/20] avg loss: 0.17004376092687426		[learning rate: 0.0091655]
	Learning Rate: 0.00916554
	LOSS [training: 0.153610132732826 | validation: 0.11376482182552602]
	TIME [epoch: 8.34 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3364653475903892		[learning rate: 0.0091544]
		[batch 20/20] avg loss: 0.24719482549291247		[learning rate: 0.0091433]
	Learning Rate: 0.00914335
	LOSS [training: 0.29183008654165077 | validation: 0.11339097021968807]
	TIME [epoch: 8.33 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16401070423138003		[learning rate: 0.0091323]
		[batch 20/20] avg loss: 0.1495614788018041		[learning rate: 0.0091212]
	Learning Rate: 0.00912121
	LOSS [training: 0.15678609151659204 | validation: 0.081254858399778]
	TIME [epoch: 8.34 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17370368120214608		[learning rate: 0.0091102]
		[batch 20/20] avg loss: 0.19156683196074076		[learning rate: 0.0090991]
	Learning Rate: 0.00909913
	LOSS [training: 0.18263525658144342 | validation: 0.10153813194018366]
	TIME [epoch: 8.34 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13870416367235308		[learning rate: 0.0090881]
		[batch 20/20] avg loss: 0.12811363823168403		[learning rate: 0.0090771]
	Learning Rate: 0.0090771
	LOSS [training: 0.13340890095201857 | validation: 0.19286583813622063]
	TIME [epoch: 8.33 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1584063959806596		[learning rate: 0.0090661]
		[batch 20/20] avg loss: 0.37368548345321506		[learning rate: 0.0090551]
	Learning Rate: 0.00905513
	LOSS [training: 0.26604593971693735 | validation: 0.07416459031965417]
	TIME [epoch: 8.35 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27147697443324803		[learning rate: 0.0090442]
		[batch 20/20] avg loss: 0.15975431076808533		[learning rate: 0.0090332]
	Learning Rate: 0.00903321
	LOSS [training: 0.21561564260066665 | validation: 0.24456987146981024]
	TIME [epoch: 8.35 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1779000409282727		[learning rate: 0.0090223]
		[batch 20/20] avg loss: 0.15555439215382572		[learning rate: 0.0090113]
	Learning Rate: 0.00901134
	LOSS [training: 0.16672721654104922 | validation: 0.09861954420215768]
	TIME [epoch: 8.35 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1343121962923658		[learning rate: 0.0090004]
		[batch 20/20] avg loss: 0.1169761150870214		[learning rate: 0.0089895]
	Learning Rate: 0.00898953
	LOSS [training: 0.12564415568969362 | validation: 0.25919870355971425]
	TIME [epoch: 8.36 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1640784160409851		[learning rate: 0.0089786]
		[batch 20/20] avg loss: 0.19786797345724916		[learning rate: 0.0089678]
	Learning Rate: 0.00896776
	LOSS [training: 0.18097319474911713 | validation: 0.2033436795138998]
	TIME [epoch: 8.33 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1760377419328712		[learning rate: 0.0089569]
		[batch 20/20] avg loss: 0.13335451811181034		[learning rate: 0.0089461]
	Learning Rate: 0.00894605
	LOSS [training: 0.15469613002234076 | validation: 0.17707538604796724]
	TIME [epoch: 8.34 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1346358341510842		[learning rate: 0.0089352]
		[batch 20/20] avg loss: 0.14478935559260522		[learning rate: 0.0089244]
	Learning Rate: 0.0089244
	LOSS [training: 0.1397125948718447 | validation: 0.20334694761471284]
	TIME [epoch: 8.34 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25581983074890396		[learning rate: 0.0089136]
		[batch 20/20] avg loss: 0.1438607634788158		[learning rate: 0.0089028]
	Learning Rate: 0.00890279
	LOSS [training: 0.1998402971138599 | validation: 0.199668853070355]
	TIME [epoch: 8.33 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20131989267478367		[learning rate: 0.008892]
		[batch 20/20] avg loss: 0.20329966327639637		[learning rate: 0.0088812]
	Learning Rate: 0.00888124
	LOSS [training: 0.20230977797559002 | validation: 0.1702038625222522]
	TIME [epoch: 8.35 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15989079112293608		[learning rate: 0.0088705]
		[batch 20/20] avg loss: 0.1628992316107163		[learning rate: 0.0088597]
	Learning Rate: 0.00885974
	LOSS [training: 0.16139501136682616 | validation: 0.13798726782038997]
	TIME [epoch: 8.35 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14693645024348337		[learning rate: 0.008849]
		[batch 20/20] avg loss: 0.1467658441496022		[learning rate: 0.0088383]
	Learning Rate: 0.00883829
	LOSS [training: 0.14685114719654274 | validation: 0.10302851723152154]
	TIME [epoch: 8.34 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17089090404809493		[learning rate: 0.0088276]
		[batch 20/20] avg loss: 0.11050015289059807		[learning rate: 0.0088169]
	Learning Rate: 0.0088169
	LOSS [training: 0.1406955284693465 | validation: 0.09113427283881467]
	TIME [epoch: 8.37 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1362618032717215		[learning rate: 0.0088062]
		[batch 20/20] avg loss: 0.1547283028895861		[learning rate: 0.0087956]
	Learning Rate: 0.00879555
	LOSS [training: 0.1454950530806538 | validation: 0.1748337409499005]
	TIME [epoch: 8.35 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1364901202820195		[learning rate: 0.0087849]
		[batch 20/20] avg loss: 0.20111630936536623		[learning rate: 0.0087743]
	Learning Rate: 0.00877426
	LOSS [training: 0.16880321482369282 | validation: 0.10822420628741068]
	TIME [epoch: 8.34 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11215200554194205		[learning rate: 0.0087636]
		[batch 20/20] avg loss: 0.12109172750740047		[learning rate: 0.008753]
	Learning Rate: 0.00875302
	LOSS [training: 0.11662186652467126 | validation: 0.11841741382864615]
	TIME [epoch: 8.35 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09054557485915439		[learning rate: 0.0087424]
		[batch 20/20] avg loss: 0.15110472053074323		[learning rate: 0.0087318]
	Learning Rate: 0.00873183
	LOSS [training: 0.12082514769494881 | validation: 0.08906856539528685]
	TIME [epoch: 8.36 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1389143421776738		[learning rate: 0.0087213]
		[batch 20/20] avg loss: 0.1123265294554943		[learning rate: 0.0087107]
	Learning Rate: 0.00871069
	LOSS [training: 0.12562043581658405 | validation: 0.22227990485154692]
	TIME [epoch: 8.34 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1288210156463892		[learning rate: 0.0087001]
		[batch 20/20] avg loss: 0.17307073664985045		[learning rate: 0.0086896]
	Learning Rate: 0.0086896
	LOSS [training: 0.1509458761481198 | validation: 0.08044392177769331]
	TIME [epoch: 8.36 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1305360130795229		[learning rate: 0.0086791]
		[batch 20/20] avg loss: 0.1036460422345555		[learning rate: 0.0086686]
	Learning Rate: 0.00866857
	LOSS [training: 0.1170910276570392 | validation: 0.1356088737680063]
	TIME [epoch: 8.36 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15859225864691642		[learning rate: 0.0086581]
		[batch 20/20] avg loss: 0.15241602183204858		[learning rate: 0.0086476]
	Learning Rate: 0.00864758
	LOSS [training: 0.15550414023948247 | validation: 0.16155037725871202]
	TIME [epoch: 8.33 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14289427310537847		[learning rate: 0.0086371]
		[batch 20/20] avg loss: 0.1263223392204313		[learning rate: 0.0086266]
	Learning Rate: 0.00862665
	LOSS [training: 0.1346083061629049 | validation: 0.16881374199721944]
	TIME [epoch: 8.33 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12830624393471685		[learning rate: 0.0086162]
		[batch 20/20] avg loss: 0.1352196292365802		[learning rate: 0.0086058]
	Learning Rate: 0.00860576
	LOSS [training: 0.13176293658564855 | validation: 0.15842573528387216]
	TIME [epoch: 8.35 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15333008001533632		[learning rate: 0.0085953]
		[batch 20/20] avg loss: 0.23601578853479316		[learning rate: 0.0085849]
	Learning Rate: 0.00858493
	LOSS [training: 0.19467293427506477 | validation: 0.092487160864199]
	TIME [epoch: 8.34 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12622638620958274		[learning rate: 0.0085745]
		[batch 20/20] avg loss: 0.12224926605647676		[learning rate: 0.0085641]
	Learning Rate: 0.00856415
	LOSS [training: 0.12423782613302974 | validation: 0.21012429031080337]
	TIME [epoch: 8.35 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12142975315258069		[learning rate: 0.0085538]
		[batch 20/20] avg loss: 0.21242941182645483		[learning rate: 0.0085434]
	Learning Rate: 0.00854342
	LOSS [training: 0.16692958248951772 | validation: 0.10619569494381485]
	TIME [epoch: 8.34 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18950814001449162		[learning rate: 0.0085331]
		[batch 20/20] avg loss: 0.09884064013609883		[learning rate: 0.0085227]
	Learning Rate: 0.00852273
	LOSS [training: 0.14417439007529523 | validation: 0.17583910349482945]
	TIME [epoch: 8.36 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15441185410919886		[learning rate: 0.0085124]
		[batch 20/20] avg loss: 0.1172130223545023		[learning rate: 0.0085021]
	Learning Rate: 0.0085021
	LOSS [training: 0.13581243823185057 | validation: 0.16192436980494235]
	TIME [epoch: 8.37 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1845295367919912		[learning rate: 0.0084918]
		[batch 20/20] avg loss: 0.17919313227728167		[learning rate: 0.0084815]
	Learning Rate: 0.00848152
	LOSS [training: 0.18186133453463643 | validation: 0.44738570504184744]
	TIME [epoch: 8.34 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1596974870948782		[learning rate: 0.0084712]
		[batch 20/20] avg loss: 0.133920745219036		[learning rate: 0.008461]
	Learning Rate: 0.00846099
	LOSS [training: 0.14680911615695708 | validation: 0.27366225047421705]
	TIME [epoch: 8.34 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1909939844691484		[learning rate: 0.0084507]
		[batch 20/20] avg loss: 0.25645704664233343		[learning rate: 0.0084405]
	Learning Rate: 0.0084405
	LOSS [training: 0.223725515555741 | validation: 0.09617694102907154]
	TIME [epoch: 8.35 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13286675987626814		[learning rate: 0.0084303]
		[batch 20/20] avg loss: 0.17761011875637026		[learning rate: 0.0084201]
	Learning Rate: 0.00842007
	LOSS [training: 0.1552384393163192 | validation: 0.15337740371966127]
	TIME [epoch: 8.34 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1311330962681268		[learning rate: 0.0084099]
		[batch 20/20] avg loss: 0.1528980531396182		[learning rate: 0.0083997]
	Learning Rate: 0.00839969
	LOSS [training: 0.14201557470387247 | validation: 0.08592935491841999]
	TIME [epoch: 8.35 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11338876269569123		[learning rate: 0.0083895]
		[batch 20/20] avg loss: 0.13845620078768014		[learning rate: 0.0083794]
	Learning Rate: 0.00837935
	LOSS [training: 0.1259224817416857 | validation: 0.20979853520724986]
	TIME [epoch: 8.34 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16007981255305836		[learning rate: 0.0083692]
		[batch 20/20] avg loss: 0.12964449637117925		[learning rate: 0.0083591]
	Learning Rate: 0.00835907
	LOSS [training: 0.1448621544621188 | validation: 0.0877929584901839]
	TIME [epoch: 8.36 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1264232889981413		[learning rate: 0.0083489]
		[batch 20/20] avg loss: 0.12519339992765405		[learning rate: 0.0083388]
	Learning Rate: 0.00833883
	LOSS [training: 0.12580834446289768 | validation: 0.10168944457956629]
	TIME [epoch: 8.38 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23470288001569456		[learning rate: 0.0083287]
		[batch 20/20] avg loss: 0.2035864943048625		[learning rate: 0.0083186]
	Learning Rate: 0.00831864
	LOSS [training: 0.21914468716027855 | validation: 0.08371653559421051]
	TIME [epoch: 8.33 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15132823975065118		[learning rate: 0.0083086]
		[batch 20/20] avg loss: 0.11244233077861407		[learning rate: 0.0082985]
	Learning Rate: 0.00829851
	LOSS [training: 0.13188528526463267 | validation: 0.10463123240401051]
	TIME [epoch: 8.32 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09716484391006983		[learning rate: 0.0082885]
		[batch 20/20] avg loss: 0.09184317645533265		[learning rate: 0.0082784]
	Learning Rate: 0.00827842
	LOSS [training: 0.09450401018270124 | validation: 0.09501479811016912]
	TIME [epoch: 8.35 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18063156426753765		[learning rate: 0.0082684]
		[batch 20/20] avg loss: 0.1374285229548016		[learning rate: 0.0082584]
	Learning Rate: 0.00825838
	LOSS [training: 0.15903004361116962 | validation: 0.06978939310277192]
	TIME [epoch: 8.36 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1579816628107411		[learning rate: 0.0082484]
		[batch 20/20] avg loss: 0.1204420894257181		[learning rate: 0.0082384]
	Learning Rate: 0.00823839
	LOSS [training: 0.1392118761182296 | validation: 0.1114487676035472]
	TIME [epoch: 8.34 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12345699675127794		[learning rate: 0.0082284]
		[batch 20/20] avg loss: 0.18760314486519805		[learning rate: 0.0082184]
	Learning Rate: 0.00821844
	LOSS [training: 0.155530070808238 | validation: 0.16142922124394793]
	TIME [epoch: 8.34 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1358535416292992		[learning rate: 0.0082085]
		[batch 20/20] avg loss: 0.15587836615614684		[learning rate: 0.0081985]
	Learning Rate: 0.00819855
	LOSS [training: 0.145865953892723 | validation: 0.19830463955495017]
	TIME [epoch: 8.39 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15072231899278823		[learning rate: 0.0081886]
		[batch 20/20] avg loss: 0.1763658310690933		[learning rate: 0.0081787]
	Learning Rate: 0.0081787
	LOSS [training: 0.16354407503094073 | validation: 0.07292494639294127]
	TIME [epoch: 8.35 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17239920897842934		[learning rate: 0.0081688]
		[batch 20/20] avg loss: 0.0961972844057873		[learning rate: 0.0081589]
	Learning Rate: 0.0081589
	LOSS [training: 0.1342982466921083 | validation: 0.07842224647710708]
	TIME [epoch: 8.33 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12651659270521784		[learning rate: 0.008149]
		[batch 20/20] avg loss: 0.21037275289698526		[learning rate: 0.0081391]
	Learning Rate: 0.00813915
	LOSS [training: 0.16844467280110154 | validation: 0.17896273381597239]
	TIME [epoch: 8.33 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21035828568906104		[learning rate: 0.0081293]
		[batch 20/20] avg loss: 0.1306575250205139		[learning rate: 0.0081194]
	Learning Rate: 0.00811944
	LOSS [training: 0.17050790535478746 | validation: 0.14129449396189114]
	TIME [epoch: 8.38 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11698367933430395		[learning rate: 0.0081096]
		[batch 20/20] avg loss: 0.18314802909867128		[learning rate: 0.0080998]
	Learning Rate: 0.00809979
	LOSS [training: 0.1500658542164876 | validation: 0.12858671972803132]
	TIME [epoch: 8.35 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09013402672507888		[learning rate: 0.00809]
		[batch 20/20] avg loss: 0.10134007890822982		[learning rate: 0.0080802]
	Learning Rate: 0.00808018
	LOSS [training: 0.09573705281665434 | validation: 0.11219863735514066]
	TIME [epoch: 8.35 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12182058215638103		[learning rate: 0.0080704]
		[batch 20/20] avg loss: 0.12198709583865108		[learning rate: 0.0080606]
	Learning Rate: 0.00806062
	LOSS [training: 0.12190383899751604 | validation: 0.06192905745091976]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12821255038404283		[learning rate: 0.0080509]
		[batch 20/20] avg loss: 0.14869635515315535		[learning rate: 0.0080411]
	Learning Rate: 0.00804111
	LOSS [training: 0.13845445276859908 | validation: 0.25812619194252473]
	TIME [epoch: 8.36 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11268625782342918		[learning rate: 0.0080314]
		[batch 20/20] avg loss: 0.13271124633096726		[learning rate: 0.0080216]
	Learning Rate: 0.00802164
	LOSS [training: 0.1226987520771982 | validation: 0.06675373215484451]
	TIME [epoch: 8.32 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12421597915502429		[learning rate: 0.0080119]
		[batch 20/20] avg loss: 0.11132864287609437		[learning rate: 0.0080022]
	Learning Rate: 0.00800222
	LOSS [training: 0.11777231101555934 | validation: 0.15181201710778364]
	TIME [epoch: 8.32 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14762777112826228		[learning rate: 0.0079925]
		[batch 20/20] avg loss: 0.22516096935046045		[learning rate: 0.0079828]
	Learning Rate: 0.00798285
	LOSS [training: 0.1863943702393614 | validation: 0.2993112967438178]
	TIME [epoch: 8.31 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13335302728184903		[learning rate: 0.0079732]
		[batch 20/20] avg loss: 0.2216774342867461		[learning rate: 0.0079635]
	Learning Rate: 0.00796352
	LOSS [training: 0.17751523078429754 | validation: 0.17125576885450167]
	TIME [epoch: 8.36 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12495403615812392		[learning rate: 0.0079539]
		[batch 20/20] avg loss: 0.2216405480381715		[learning rate: 0.0079442]
	Learning Rate: 0.00794424
	LOSS [training: 0.17329729209814773 | validation: 0.17861999354844937]
	TIME [epoch: 8.33 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13355395976989942		[learning rate: 0.0079346]
		[batch 20/20] avg loss: 0.0980372119529057		[learning rate: 0.007925]
	Learning Rate: 0.00792501
	LOSS [training: 0.11579558586140257 | validation: 0.0957016775903305]
	TIME [epoch: 8.33 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07900152219893744		[learning rate: 0.0079154]
		[batch 20/20] avg loss: 0.13964102669980533		[learning rate: 0.0079058]
	Learning Rate: 0.00790583
	LOSS [training: 0.10932127444937138 | validation: 0.266309981928918]
	TIME [epoch: 8.35 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14931671375660932		[learning rate: 0.0078963]
		[batch 20/20] avg loss: 0.13538111750674164		[learning rate: 0.0078867]
	Learning Rate: 0.00788669
	LOSS [training: 0.14234891563167548 | validation: 0.05898739939588356]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14280217847887716		[learning rate: 0.0078771]
		[batch 20/20] avg loss: 0.11583135028409644		[learning rate: 0.0078676]
	Learning Rate: 0.0078676
	LOSS [training: 0.12931676438148682 | validation: 0.11130639787183438]
	TIME [epoch: 8.33 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10543564111792572		[learning rate: 0.0078581]
		[batch 20/20] avg loss: 0.12963243856760773		[learning rate: 0.0078486]
	Learning Rate: 0.00784855
	LOSS [training: 0.11753403984276671 | validation: 0.1294154599021449]
	TIME [epoch: 8.33 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12541999690993144		[learning rate: 0.007839]
		[batch 20/20] avg loss: 0.15030792044515348		[learning rate: 0.0078296]
	Learning Rate: 0.00782955
	LOSS [training: 0.13786395867754245 | validation: 0.12320185977139626]
	TIME [epoch: 8.32 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09468708746673091		[learning rate: 0.0078201]
		[batch 20/20] avg loss: 0.13207428545100103		[learning rate: 0.0078106]
	Learning Rate: 0.0078106
	LOSS [training: 0.11338068645886594 | validation: 0.09787584363928516]
	TIME [epoch: 8.36 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15345706004616122		[learning rate: 0.0078011]
		[batch 20/20] avg loss: 0.16022446062973722		[learning rate: 0.0077917]
	Learning Rate: 0.00779169
	LOSS [training: 0.15684076033794922 | validation: 0.0772754416445472]
	TIME [epoch: 8.34 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08244806794369883		[learning rate: 0.0077823]
		[batch 20/20] avg loss: 0.12479854707470524		[learning rate: 0.0077728]
	Learning Rate: 0.00777283
	LOSS [training: 0.10362330750920201 | validation: 0.08178859053506327]
	TIME [epoch: 8.33 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12635532792709464		[learning rate: 0.0077634]
		[batch 20/20] avg loss: 0.1367646333060934		[learning rate: 0.007754]
	Learning Rate: 0.00775401
	LOSS [training: 0.13155998061659402 | validation: 0.21612255201699349]
	TIME [epoch: 8.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12960321549417125		[learning rate: 0.0077446]
		[batch 20/20] avg loss: 0.141223210999816		[learning rate: 0.0077352]
	Learning Rate: 0.00773524
	LOSS [training: 0.1354132132469936 | validation: 0.19321002863203032]
	TIME [epoch: 8.37 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14587426705548584		[learning rate: 0.0077259]
		[batch 20/20] avg loss: 0.1187026543508157		[learning rate: 0.0077165]
	Learning Rate: 0.00771651
	LOSS [training: 0.13228846070315076 | validation: 0.1495075987690901]
	TIME [epoch: 8.33 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1467037778586561		[learning rate: 0.0077072]
		[batch 20/20] avg loss: 0.18502781096835658		[learning rate: 0.0076978]
	Learning Rate: 0.00769783
	LOSS [training: 0.1658657944135063 | validation: 0.14709221658825672]
	TIME [epoch: 8.33 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1624746027560316		[learning rate: 0.0076885]
		[batch 20/20] avg loss: 0.16202087980651383		[learning rate: 0.0076792]
	Learning Rate: 0.0076792
	LOSS [training: 0.16224774128127267 | validation: 0.33623604615315306]
	TIME [epoch: 8.32 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17386584442412106		[learning rate: 0.0076699]
		[batch 20/20] avg loss: 0.08473662678249475		[learning rate: 0.0076606]
	Learning Rate: 0.00766061
	LOSS [training: 0.12930123560330792 | validation: 0.23854366836728066]
	TIME [epoch: 8.34 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0939358439726748		[learning rate: 0.0076513]
		[batch 20/20] avg loss: 0.11916633021152985		[learning rate: 0.0076421]
	Learning Rate: 0.00764206
	LOSS [training: 0.10655108709210234 | validation: 0.17076934565750618]
	TIME [epoch: 8.33 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17600589892987523		[learning rate: 0.0076328]
		[batch 20/20] avg loss: 0.15673628646777232		[learning rate: 0.0076236]
	Learning Rate: 0.00762356
	LOSS [training: 0.16637109269882372 | validation: 0.10813302559802732]
	TIME [epoch: 8.34 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11773138817905279		[learning rate: 0.0076143]
		[batch 20/20] avg loss: 0.09284484433094112		[learning rate: 0.0076051]
	Learning Rate: 0.00760511
	LOSS [training: 0.10528811625499697 | validation: 0.04555725278914358]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08073943772210954		[learning rate: 0.0075959]
		[batch 20/20] avg loss: 0.3439905787628065		[learning rate: 0.0075867]
	Learning Rate: 0.00758669
	LOSS [training: 0.21236500824245802 | validation: 0.2050531183411089]
	TIME [epoch: 8.38 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09055524160761705		[learning rate: 0.0075775]
		[batch 20/20] avg loss: 0.1451040026486037		[learning rate: 0.0075683]
	Learning Rate: 0.00756833
	LOSS [training: 0.11782962212811035 | validation: 0.09171359859404003]
	TIME [epoch: 8.39 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11404209295869758		[learning rate: 0.0075592]
		[batch 20/20] avg loss: 0.12743799561663005		[learning rate: 0.00755]
	Learning Rate: 0.00755001
	LOSS [training: 0.12074004428766379 | validation: 0.09332418045082577]
	TIME [epoch: 8.37 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18920153617705288		[learning rate: 0.0075409]
		[batch 20/20] avg loss: 0.15725751538303928		[learning rate: 0.0075317]
	Learning Rate: 0.00753173
	LOSS [training: 0.17322952578004608 | validation: 0.42722117546767213]
	TIME [epoch: 8.34 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1860054294957819		[learning rate: 0.0075226]
		[batch 20/20] avg loss: 0.10993044417713942		[learning rate: 0.0075135]
	Learning Rate: 0.0075135
	LOSS [training: 0.14796793683646065 | validation: 0.06640429347644933]
	TIME [epoch: 8.37 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13232270980627855		[learning rate: 0.0075044]
		[batch 20/20] avg loss: 0.16087949021383957		[learning rate: 0.0074953]
	Learning Rate: 0.00749531
	LOSS [training: 0.14660110001005905 | validation: 0.08930142395398812]
	TIME [epoch: 8.34 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11764060068074741		[learning rate: 0.0074862]
		[batch 20/20] avg loss: 0.10998810515849085		[learning rate: 0.0074772]
	Learning Rate: 0.00747716
	LOSS [training: 0.11381435291961912 | validation: 0.15895424691524176]
	TIME [epoch: 8.34 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12909202405409037		[learning rate: 0.0074681]
		[batch 20/20] avg loss: 0.11726179736940875		[learning rate: 0.0074591]
	Learning Rate: 0.00745906
	LOSS [training: 0.12317691071174955 | validation: 0.08263132024723882]
	TIME [epoch: 8.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11965689734402143		[learning rate: 0.00745]
		[batch 20/20] avg loss: 0.22554039022133213		[learning rate: 0.007441]
	Learning Rate: 0.007441
	LOSS [training: 0.17259864378267675 | validation: 0.08670067553779971]
	TIME [epoch: 8.39 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10330643313421077		[learning rate: 0.007432]
		[batch 20/20] avg loss: 0.16871075342726935		[learning rate: 0.007423]
	Learning Rate: 0.00742299
	LOSS [training: 0.13600859328074005 | validation: 0.0924377613260054]
	TIME [epoch: 8.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12886138509015718		[learning rate: 0.007414]
		[batch 20/20] avg loss: 0.09984946085682159		[learning rate: 0.007405]
	Learning Rate: 0.00740502
	LOSS [training: 0.11435542297348937 | validation: 0.11582770343524619]
	TIME [epoch: 8.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10096899013162375		[learning rate: 0.0073961]
		[batch 20/20] avg loss: 0.08659173595130173		[learning rate: 0.0073871]
	Learning Rate: 0.0073871
	LOSS [training: 0.09378036304146274 | validation: 0.18168238252502042]
	TIME [epoch: 8.38 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1483416299464439		[learning rate: 0.0073781]
		[batch 20/20] avg loss: 0.11694471401469379		[learning rate: 0.0073692]
	Learning Rate: 0.00736921
	LOSS [training: 0.13264317198056885 | validation: 0.11202169860719494]
	TIME [epoch: 8.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10775634393886555		[learning rate: 0.0073603]
		[batch 20/20] avg loss: 0.13647345984881237		[learning rate: 0.0073514]
	Learning Rate: 0.00735137
	LOSS [training: 0.12211490189383893 | validation: 0.04979945204555937]
	TIME [epoch: 8.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13413178176614954		[learning rate: 0.0073425]
		[batch 20/20] avg loss: 0.0837779796943131		[learning rate: 0.0073336]
	Learning Rate: 0.00733358
	LOSS [training: 0.10895488073023132 | validation: 0.10231454727202773]
	TIME [epoch: 8.37 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10537336450947932		[learning rate: 0.0073247]
		[batch 20/20] avg loss: 0.1177468336814969		[learning rate: 0.0073158]
	Learning Rate: 0.00731582
	LOSS [training: 0.11156009909548811 | validation: 0.15535531007155304]
	TIME [epoch: 8.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1019955677678234		[learning rate: 0.007307]
		[batch 20/20] avg loss: 0.10040670448386116		[learning rate: 0.0072981]
	Learning Rate: 0.00729811
	LOSS [training: 0.10120113612584228 | validation: 0.1511128649619359]
	TIME [epoch: 8.37 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11059447574773167		[learning rate: 0.0072893]
		[batch 20/20] avg loss: 0.15025322014456927		[learning rate: 0.0072804]
	Learning Rate: 0.00728044
	LOSS [training: 0.13042384794615045 | validation: 0.20620239618843836]
	TIME [epoch: 8.37 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24246781667958048		[learning rate: 0.0072716]
		[batch 20/20] avg loss: 0.5365172441092737		[learning rate: 0.0072628]
	Learning Rate: 0.00726282
	LOSS [training: 0.3894925303944271 | validation: 0.1618290259610282]
	TIME [epoch: 8.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14773446517797578		[learning rate: 0.007254]
		[batch 20/20] avg loss: 0.3139930665836486		[learning rate: 0.0072452]
	Learning Rate: 0.00724524
	LOSS [training: 0.2308637658808122 | validation: 0.19031106120375157]
	TIME [epoch: 8.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.328106651657563		[learning rate: 0.0072365]
		[batch 20/20] avg loss: 0.8151775115087876		[learning rate: 0.0072277]
	Learning Rate: 0.0072277
	LOSS [training: 0.5716420815831753 | validation: 0.8187849471240136]
	TIME [epoch: 8.37 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5137621390089492		[learning rate: 0.0072189]
		[batch 20/20] avg loss: 0.29496371000925264		[learning rate: 0.0072102]
	Learning Rate: 0.0072102
	LOSS [training: 0.40436292450910083 | validation: 0.3920414757225521]
	TIME [epoch: 8.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1620414958752884		[learning rate: 0.0072015]
		[batch 20/20] avg loss: 0.18274261913274398		[learning rate: 0.0071927]
	Learning Rate: 0.00719275
	LOSS [training: 0.17239205750401618 | validation: 0.1704809305523508]
	TIME [epoch: 8.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27295211364656397		[learning rate: 0.007184]
		[batch 20/20] avg loss: 0.169045751353912		[learning rate: 0.0071753]
	Learning Rate: 0.00717533
	LOSS [training: 0.220998932500238 | validation: 0.13823602790284545]
	TIME [epoch: 8.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14841805069021868		[learning rate: 0.0071666]
		[batch 20/20] avg loss: 0.2183457092687779		[learning rate: 0.007158]
	Learning Rate: 0.00715796
	LOSS [training: 0.1833818799794983 | validation: 0.15517166967452697]
	TIME [epoch: 8.38 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28851943851271156		[learning rate: 0.0071493]
		[batch 20/20] avg loss: 0.17723923399598301		[learning rate: 0.0071406]
	Learning Rate: 0.00714064
	LOSS [training: 0.2328793362543473 | validation: 0.19728629901164443]
	TIME [epoch: 8.38 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15711676739118074		[learning rate: 0.007132]
		[batch 20/20] avg loss: 0.12980023908793817		[learning rate: 0.0071233]
	Learning Rate: 0.00712335
	LOSS [training: 0.14345850323955944 | validation: 0.1846554766994361]
	TIME [epoch: 8.34 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21803017176132675		[learning rate: 0.0071147]
		[batch 20/20] avg loss: 0.16506534291949698		[learning rate: 0.0071061]
	Learning Rate: 0.0071061
	LOSS [training: 0.19154775734041188 | validation: 0.10856023060677236]
	TIME [epoch: 8.34 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1078405129060738		[learning rate: 0.0070975]
		[batch 20/20] avg loss: 0.1956285103167203		[learning rate: 0.0070889]
	Learning Rate: 0.0070889
	LOSS [training: 0.15173451161139706 | validation: 0.06310881418952911]
	TIME [epoch: 8.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11204674564385472		[learning rate: 0.0070803]
		[batch 20/20] avg loss: 0.12611199687372154		[learning rate: 0.0070717]
	Learning Rate: 0.00707174
	LOSS [training: 0.11907937125878812 | validation: 0.08137394610885845]
	TIME [epoch: 8.37 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2224219751873482		[learning rate: 0.0070632]
		[batch 20/20] avg loss: 0.17830028994762787		[learning rate: 0.0070546]
	Learning Rate: 0.00705462
	LOSS [training: 0.20036113256748803 | validation: 0.11676655738407271]
	TIME [epoch: 8.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14301436093579603		[learning rate: 0.0070461]
		[batch 20/20] avg loss: 0.09962340634625845		[learning rate: 0.0070375]
	Learning Rate: 0.00703754
	LOSS [training: 0.12131888364102725 | validation: 0.09627659522558912]
	TIME [epoch: 8.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11496011409499696		[learning rate: 0.007029]
		[batch 20/20] avg loss: 0.13233783525994688		[learning rate: 0.0070205]
	Learning Rate: 0.00702051
	LOSS [training: 0.1236489746774719 | validation: 0.07032258772418112]
	TIME [epoch: 8.39 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11321369724917336		[learning rate: 0.007012]
		[batch 20/20] avg loss: 0.11349600955689743		[learning rate: 0.0070035]
	Learning Rate: 0.00700351
	LOSS [training: 0.11335485340303542 | validation: 0.051600039329634424]
	TIME [epoch: 8.37 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09042960384765689		[learning rate: 0.006995]
		[batch 20/20] avg loss: 0.14976761951314216		[learning rate: 0.0069866]
	Learning Rate: 0.00698656
	LOSS [training: 0.12009861168039955 | validation: 0.12724974893668028]
	TIME [epoch: 8.34 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1525889321125689		[learning rate: 0.0069781]
		[batch 20/20] avg loss: 0.11179350177072414		[learning rate: 0.0069696]
	Learning Rate: 0.00696964
	LOSS [training: 0.13219121694164654 | validation: 0.17778049802528664]
	TIME [epoch: 8.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10346310126826201		[learning rate: 0.0069612]
		[batch 20/20] avg loss: 0.07014643741336472		[learning rate: 0.0069528]
	Learning Rate: 0.00695277
	LOSS [training: 0.08680476934081335 | validation: 0.14371486590288163]
	TIME [epoch: 8.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10805621192086787		[learning rate: 0.0069443]
		[batch 20/20] avg loss: 0.10801583177679248		[learning rate: 0.0069359]
	Learning Rate: 0.00693594
	LOSS [training: 0.10803602184883018 | validation: 0.07361302546428171]
	TIME [epoch: 8.34 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12763804990104527		[learning rate: 0.0069275]
		[batch 20/20] avg loss: 0.12471946397525938		[learning rate: 0.0069191]
	Learning Rate: 0.00691915
	LOSS [training: 0.12617875693815234 | validation: 0.11172115429987907]
	TIME [epoch: 8.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13313787670988436		[learning rate: 0.0069108]
		[batch 20/20] avg loss: 0.1078021642810455		[learning rate: 0.0069024]
	Learning Rate: 0.0069024
	LOSS [training: 0.12047002049546492 | validation: 0.0517852523773013]
	TIME [epoch: 8.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08159220613652138		[learning rate: 0.006894]
		[batch 20/20] avg loss: 0.09372375042854525		[learning rate: 0.0068857]
	Learning Rate: 0.00688569
	LOSS [training: 0.0876579782825333 | validation: 0.05885324196115645]
	TIME [epoch: 8.37 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07530741962489793		[learning rate: 0.0068773]
		[batch 20/20] avg loss: 0.12643819653685134		[learning rate: 0.006869]
	Learning Rate: 0.00686902
	LOSS [training: 0.10087280808087465 | validation: 0.07529362428529983]
	TIME [epoch: 8.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07340230924408694		[learning rate: 0.0068607]
		[batch 20/20] avg loss: 0.1548051863913334		[learning rate: 0.0068524]
	Learning Rate: 0.00685239
	LOSS [training: 0.11410374781771015 | validation: 0.059947189391962086]
	TIME [epoch: 8.38 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10316181282180426		[learning rate: 0.0068441]
		[batch 20/20] avg loss: 0.14663878853049706		[learning rate: 0.0068358]
	Learning Rate: 0.0068358
	LOSS [training: 0.12490030067615066 | validation: 0.100554372127952]
	TIME [epoch: 8.34 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11128822856748584		[learning rate: 0.0068275]
		[batch 20/20] avg loss: 0.08978926116094887		[learning rate: 0.0068193]
	Learning Rate: 0.00681925
	LOSS [training: 0.10053874486421736 | validation: 0.04927609456190025]
	TIME [epoch: 8.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10154319705296799		[learning rate: 0.006811]
		[batch 20/20] avg loss: 0.12499688041115202		[learning rate: 0.0068027]
	Learning Rate: 0.00680275
	LOSS [training: 0.11327003873206001 | validation: 0.2759689361565962]
	TIME [epoch: 8.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17643230979073363		[learning rate: 0.0067945]
		[batch 20/20] avg loss: 0.10123487785362877		[learning rate: 0.0067863]
	Learning Rate: 0.00678628
	LOSS [training: 0.1388335938221812 | validation: 0.11426053700780314]
	TIME [epoch: 8.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10604435596296469		[learning rate: 0.0067781]
		[batch 20/20] avg loss: 0.0789580588762814		[learning rate: 0.0067698]
	Learning Rate: 0.00676985
	LOSS [training: 0.09250120741962306 | validation: 0.04530581475319948]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12392010769980417		[learning rate: 0.0067616]
		[batch 20/20] avg loss: 0.08462387530718979		[learning rate: 0.0067535]
	Learning Rate: 0.00675346
	LOSS [training: 0.10427199150349697 | validation: 0.11742946059643666]
	TIME [epoch: 8.39 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09642388389138344		[learning rate: 0.0067453]
		[batch 20/20] avg loss: 0.13660871778389644		[learning rate: 0.0067371]
	Learning Rate: 0.00673711
	LOSS [training: 0.11651630083763993 | validation: 0.1270288993335167]
	TIME [epoch: 8.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09577073846380332		[learning rate: 0.006729]
		[batch 20/20] avg loss: 0.0687397308465908		[learning rate: 0.0067208]
	Learning Rate: 0.0067208
	LOSS [training: 0.08225523465519703 | validation: 0.07310263987413078]
	TIME [epoch: 8.33 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0648613243664595		[learning rate: 0.0067127]
		[batch 20/20] avg loss: 0.11628051360109451		[learning rate: 0.0067045]
	Learning Rate: 0.00670453
	LOSS [training: 0.090570918983777 | validation: 0.10438211980454842]
	TIME [epoch: 8.34 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08642444152775473		[learning rate: 0.0066964]
		[batch 20/20] avg loss: 0.19704969006082967		[learning rate: 0.0066883]
	Learning Rate: 0.0066883
	LOSS [training: 0.14173706579429218 | validation: 0.07478106492942363]
	TIME [epoch: 8.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1673321951889383		[learning rate: 0.0066802]
		[batch 20/20] avg loss: 0.09041161881135926		[learning rate: 0.0066721]
	Learning Rate: 0.00667211
	LOSS [training: 0.12887190700014878 | validation: 0.15386707769149457]
	TIME [epoch: 8.34 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10124496735780572		[learning rate: 0.006664]
		[batch 20/20] avg loss: 0.12998484081042455		[learning rate: 0.006656]
	Learning Rate: 0.00665596
	LOSS [training: 0.11561490408411515 | validation: 0.17351162255145608]
	TIME [epoch: 8.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10298893693643618		[learning rate: 0.0066479]
		[batch 20/20] avg loss: 0.09994279247991812		[learning rate: 0.0066398]
	Learning Rate: 0.00663984
	LOSS [training: 0.10146586470817716 | validation: 0.14201177345394395]
	TIME [epoch: 8.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10551493858940572		[learning rate: 0.0066318]
		[batch 20/20] avg loss: 0.48359740734177264		[learning rate: 0.0066238]
	Learning Rate: 0.00662377
	LOSS [training: 0.29455617296558917 | validation: 0.2848047321844577]
	TIME [epoch: 8.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1945391097957869		[learning rate: 0.0066157]
		[batch 20/20] avg loss: 0.09297022695011208		[learning rate: 0.0066077]
	Learning Rate: 0.00660774
	LOSS [training: 0.14375466837294948 | validation: 0.3348458281555079]
	TIME [epoch: 8.37 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16946927179661375		[learning rate: 0.0065997]
		[batch 20/20] avg loss: 0.11660440432387106		[learning rate: 0.0065917]
	Learning Rate: 0.00659174
	LOSS [training: 0.14303683806024237 | validation: 0.09265609984099563]
	TIME [epoch: 8.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10944070845482234		[learning rate: 0.0065838]
		[batch 20/20] avg loss: 0.10399022060303724		[learning rate: 0.0065758]
	Learning Rate: 0.00657578
	LOSS [training: 0.1067154645289298 | validation: 0.2321828678971725]
	TIME [epoch: 8.35 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11155023166090154		[learning rate: 0.0065678]
		[batch 20/20] avg loss: 0.07930120278930675		[learning rate: 0.0065599]
	Learning Rate: 0.00655986
	LOSS [training: 0.09542571722510415 | validation: 0.10379798796302551]
	TIME [epoch: 8.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08972259574919314		[learning rate: 0.0065519]
		[batch 20/20] avg loss: 0.090821432322114		[learning rate: 0.006544]
	Learning Rate: 0.00654398
	LOSS [training: 0.09027201403565357 | validation: 0.10645425751090716]
	TIME [epoch: 8.35 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.072962965490177		[learning rate: 0.0065361]
		[batch 20/20] avg loss: 0.10835776634354695		[learning rate: 0.0065281]
	Learning Rate: 0.00652814
	LOSS [training: 0.09066036591686197 | validation: 0.028763799628390405]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0908597419272686		[learning rate: 0.0065202]
		[batch 20/20] avg loss: 0.13794246181498096		[learning rate: 0.0065123]
	Learning Rate: 0.00651234
	LOSS [training: 0.11440110187112477 | validation: 0.1155405989733948]
	TIME [epoch: 8.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1264386630133825		[learning rate: 0.0065044]
		[batch 20/20] avg loss: 0.10067950921427338		[learning rate: 0.0064966]
	Learning Rate: 0.00649657
	LOSS [training: 0.11355908611382795 | validation: 0.051511563631159205]
	TIME [epoch: 8.33 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11386145133372842		[learning rate: 0.0064887]
		[batch 20/20] avg loss: 0.10574393507068916		[learning rate: 0.0064808]
	Learning Rate: 0.00648084
	LOSS [training: 0.10980269320220881 | validation: 0.15887454504627155]
	TIME [epoch: 8.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09271355066544837		[learning rate: 0.006473]
		[batch 20/20] avg loss: 0.08680532916106579		[learning rate: 0.0064652]
	Learning Rate: 0.00646516
	LOSS [training: 0.08975943991325709 | validation: 0.07656881204358454]
	TIME [epoch: 8.32 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10150071488803906		[learning rate: 0.0064573]
		[batch 20/20] avg loss: 0.08749563958802152		[learning rate: 0.0064495]
	Learning Rate: 0.0064495
	LOSS [training: 0.09449817723803029 | validation: 0.14187403607085564]
	TIME [epoch: 8.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.158347138339639		[learning rate: 0.0064417]
		[batch 20/20] avg loss: 0.10292948435565419		[learning rate: 0.0064339]
	Learning Rate: 0.00643389
	LOSS [training: 0.13063831134764659 | validation: 0.10304105406673933]
	TIME [epoch: 8.32 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10171934508713656		[learning rate: 0.0064261]
		[batch 20/20] avg loss: 0.12231800633766121		[learning rate: 0.0064183]
	Learning Rate: 0.00641832
	LOSS [training: 0.1120186757123989 | validation: 0.09718692720074795]
	TIME [epoch: 8.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06848120123223454		[learning rate: 0.0064105]
		[batch 20/20] avg loss: 0.09663798032961202		[learning rate: 0.0064028]
	Learning Rate: 0.00640278
	LOSS [training: 0.0825595907809233 | validation: 0.28062973589965157]
	TIME [epoch: 8.34 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1449364526486389		[learning rate: 0.006395]
		[batch 20/20] avg loss: 0.07535596971560538		[learning rate: 0.0063873]
	Learning Rate: 0.00638728
	LOSS [training: 0.11014621118212213 | validation: 0.028646677723825682]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06714300389480407		[learning rate: 0.0063795]
		[batch 20/20] avg loss: 0.07180032925060509		[learning rate: 0.0063718]
	Learning Rate: 0.00637182
	LOSS [training: 0.06947166657270457 | validation: 0.06047114745978584]
	TIME [epoch: 8.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10946521878817117		[learning rate: 0.0063641]
		[batch 20/20] avg loss: 0.23730878904446762		[learning rate: 0.0063564]
	Learning Rate: 0.00635639
	LOSS [training: 0.1733870039163194 | validation: 0.29222144506949554]
	TIME [epoch: 8.33 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20775749601846014		[learning rate: 0.0063487]
		[batch 20/20] avg loss: 0.13631363457742865		[learning rate: 0.006341]
	Learning Rate: 0.006341
	LOSS [training: 0.17203556529794434 | validation: 0.11882077571022279]
	TIME [epoch: 8.32 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09984565740501435		[learning rate: 0.0063333]
		[batch 20/20] avg loss: 0.09164048650602827		[learning rate: 0.0063257]
	Learning Rate: 0.00632565
	LOSS [training: 0.09574307195552131 | validation: 0.08241006684593172]
	TIME [epoch: 8.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07270369233835636		[learning rate: 0.006318]
		[batch 20/20] avg loss: 0.05527221684053577		[learning rate: 0.0063103]
	Learning Rate: 0.00631034
	LOSS [training: 0.06398795458944607 | validation: 0.10723998923706214]
	TIME [epoch: 8.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08290552650850143		[learning rate: 0.0063027]
		[batch 20/20] avg loss: 0.08900871223883929		[learning rate: 0.0062951]
	Learning Rate: 0.00629506
	LOSS [training: 0.08595711937367034 | validation: 0.06299366940215718]
	TIME [epoch: 8.33 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06859026390253073		[learning rate: 0.0062874]
		[batch 20/20] avg loss: 0.100763724243664		[learning rate: 0.0062798]
	Learning Rate: 0.00627982
	LOSS [training: 0.0846769940730974 | validation: 0.0524805952485133]
	TIME [epoch: 8.33 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07066131100346802		[learning rate: 0.0062722]
		[batch 20/20] avg loss: 0.10764146389853368		[learning rate: 0.0062646]
	Learning Rate: 0.00626462
	LOSS [training: 0.08915138745100086 | validation: 0.09013654743468458]
	TIME [epoch: 8.37 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09336323306788281		[learning rate: 0.006257]
		[batch 20/20] avg loss: 0.1656956528663819		[learning rate: 0.0062495]
	Learning Rate: 0.00624945
	LOSS [training: 0.12952944296713237 | validation: 0.1361037068683885]
	TIME [epoch: 8.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11116140896117592		[learning rate: 0.0062419]
		[batch 20/20] avg loss: 0.07831592080319978		[learning rate: 0.0062343]
	Learning Rate: 0.00623433
	LOSS [training: 0.09473866488218785 | validation: 0.11138325478908266]
	TIME [epoch: 8.33 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09070756696472096		[learning rate: 0.0062268]
		[batch 20/20] avg loss: 0.06128984220391477		[learning rate: 0.0062192]
	Learning Rate: 0.00621923
	LOSS [training: 0.07599870458431787 | validation: 0.16061574844374213]
	TIME [epoch: 8.33 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08643655258843677		[learning rate: 0.0062117]
		[batch 20/20] avg loss: 0.10153169443313795		[learning rate: 0.0062042]
	Learning Rate: 0.00620418
	LOSS [training: 0.09398412351078736 | validation: 0.16044196176103534]
	TIME [epoch: 8.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11181999897475162		[learning rate: 0.0061967]
		[batch 20/20] avg loss: 0.1210712706063988		[learning rate: 0.0061892]
	Learning Rate: 0.00618916
	LOSS [training: 0.1164456347905752 | validation: 0.17943693413706674]
	TIME [epoch: 8.33 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11063720044543461		[learning rate: 0.0061817]
		[batch 20/20] avg loss: 0.2005723863407059		[learning rate: 0.0061742]
	Learning Rate: 0.00617417
	LOSS [training: 0.15560479339307026 | validation: 0.11060058668146512]
	TIME [epoch: 8.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15883534392310616		[learning rate: 0.0061667]
		[batch 20/20] avg loss: 0.11237301738314882		[learning rate: 0.0061592]
	Learning Rate: 0.00615923
	LOSS [training: 0.13560418065312754 | validation: 0.1014426415001431]
	TIME [epoch: 8.34 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12910953416702434		[learning rate: 0.0061518]
		[batch 20/20] avg loss: 0.1505554442687412		[learning rate: 0.0061443]
	Learning Rate: 0.00614432
	LOSS [training: 0.13983248921788277 | validation: 0.09884675245091717]
	TIME [epoch: 8.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12797308186419548		[learning rate: 0.0061369]
		[batch 20/20] avg loss: 0.2649122966016501		[learning rate: 0.0061294]
	Learning Rate: 0.00612944
	LOSS [training: 0.19644268923292282 | validation: 0.0835493063266512]
	TIME [epoch: 8.38 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08898089625528494		[learning rate: 0.006122]
		[batch 20/20] avg loss: 0.09465683452868066		[learning rate: 0.0061146]
	Learning Rate: 0.0061146
	LOSS [training: 0.0918188653919828 | validation: 0.07512955011895234]
	TIME [epoch: 8.34 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08557107349664592		[learning rate: 0.0061072]
		[batch 20/20] avg loss: 0.08268766726797677		[learning rate: 0.0060998]
	Learning Rate: 0.0060998
	LOSS [training: 0.08412937038231134 | validation: 0.05759994140230923]
	TIME [epoch: 8.33 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09772286418860743		[learning rate: 0.0060924]
		[batch 20/20] avg loss: 0.08604449262588328		[learning rate: 0.006085]
	Learning Rate: 0.00608504
	LOSS [training: 0.09188367840724537 | validation: 0.609180210280855]
	TIME [epoch: 8.35 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13163480848587944		[learning rate: 0.0060777]
		[batch 20/20] avg loss: 0.09555853371240706		[learning rate: 0.0060703]
	Learning Rate: 0.00607031
	LOSS [training: 0.11359667109914326 | validation: 0.07282902071021927]
	TIME [epoch: 8.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08116242267118948		[learning rate: 0.006063]
		[batch 20/20] avg loss: 0.08092140206957851		[learning rate: 0.0060556]
	Learning Rate: 0.00605561
	LOSS [training: 0.08104191237038402 | validation: 0.049780422131598584]
	TIME [epoch: 8.34 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07023023379235294		[learning rate: 0.0060483]
		[batch 20/20] avg loss: 0.10521706950367146		[learning rate: 0.006041]
	Learning Rate: 0.00604095
	LOSS [training: 0.08772365164801223 | validation: 0.07555901098067566]
	TIME [epoch: 8.34 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10051153438839586		[learning rate: 0.0060336]
		[batch 20/20] avg loss: 0.0891722541870682		[learning rate: 0.0060263]
	Learning Rate: 0.00602633
	LOSS [training: 0.09484189428773204 | validation: 0.043662307426311976]
	TIME [epoch: 8.39 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08110171402823892		[learning rate: 0.006019]
		[batch 20/20] avg loss: 0.10416930243622824		[learning rate: 0.0060117]
	Learning Rate: 0.00601174
	LOSS [training: 0.09263550823223357 | validation: 0.13883173354267275]
	TIME [epoch: 8.34 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09421267612943204		[learning rate: 0.0060045]
		[batch 20/20] avg loss: 0.06961960161084586		[learning rate: 0.0059972]
	Learning Rate: 0.00599718
	LOSS [training: 0.08191613887013895 | validation: 0.03851819516830833]
	TIME [epoch: 8.33 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08149701782173266		[learning rate: 0.0059899]
		[batch 20/20] avg loss: 0.21613057865088967		[learning rate: 0.0059827]
	Learning Rate: 0.00598267
	LOSS [training: 0.1488137982363112 | validation: 0.09763749930467273]
	TIME [epoch: 8.33 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1424105179650905		[learning rate: 0.0059754]
		[batch 20/20] avg loss: 0.08301312772470153		[learning rate: 0.0059682]
	Learning Rate: 0.00596818
	LOSS [training: 0.11271182284489603 | validation: 0.09144035416964606]
	TIME [epoch: 8.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0854644651552011		[learning rate: 0.005961]
		[batch 20/20] avg loss: 0.09411281336231282		[learning rate: 0.0059537]
	Learning Rate: 0.00595374
	LOSS [training: 0.08978863925875695 | validation: 0.20375959305465466]
	TIME [epoch: 8.33 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15633632569446507		[learning rate: 0.0059465]
		[batch 20/20] avg loss: 0.13485805121742722		[learning rate: 0.0059393]
	Learning Rate: 0.00593932
	LOSS [training: 0.14559718845594619 | validation: 0.11819551279850143]
	TIME [epoch: 8.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09164358366339709		[learning rate: 0.0059321]
		[batch 20/20] avg loss: 0.057150909478548506		[learning rate: 0.0059249]
	Learning Rate: 0.00592494
	LOSS [training: 0.07439724657097278 | validation: 0.0954981773032205]
	TIME [epoch: 8.34 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08298483699826806		[learning rate: 0.0059178]
		[batch 20/20] avg loss: 0.10098210083764028		[learning rate: 0.0059106]
	Learning Rate: 0.0059106
	LOSS [training: 0.09198346891795417 | validation: 0.1399808047135437]
	TIME [epoch: 8.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11737611623467921		[learning rate: 0.0059034]
		[batch 20/20] avg loss: 0.05709102761873408		[learning rate: 0.0058963]
	Learning Rate: 0.00589629
	LOSS [training: 0.08723357192670665 | validation: 0.04680858743276819]
	TIME [epoch: 8.38 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06908930689640706		[learning rate: 0.0058892]
		[batch 20/20] avg loss: 0.10628641198927347		[learning rate: 0.005882]
	Learning Rate: 0.00588202
	LOSS [training: 0.08768785944284027 | validation: 0.15990110442643585]
	TIME [epoch: 8.34 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09938712788159344		[learning rate: 0.0058749]
		[batch 20/20] avg loss: 0.059638730079112655		[learning rate: 0.0058678]
	Learning Rate: 0.00586778
	LOSS [training: 0.07951292898035303 | validation: 0.0723922086208119]
	TIME [epoch: 8.33 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06668512982060286		[learning rate: 0.0058607]
		[batch 20/20] avg loss: 0.13529266308255328		[learning rate: 0.0058536]
	Learning Rate: 0.00585357
	LOSS [training: 0.10098889645157808 | validation: 0.15014813973282307]
	TIME [epoch: 8.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07753372796774896		[learning rate: 0.0058465]
		[batch 20/20] avg loss: 0.15905702685891038		[learning rate: 0.0058394]
	Learning Rate: 0.0058394
	LOSS [training: 0.1182953774133297 | validation: 0.11827570692748145]
	TIME [epoch: 8.34 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1125294245522602		[learning rate: 0.0058323]
		[batch 20/20] avg loss: 0.07987919437093224		[learning rate: 0.0058253]
	Learning Rate: 0.00582527
	LOSS [training: 0.09620430946159624 | validation: 0.07756985939269219]
	TIME [epoch: 8.34 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07343235549023773		[learning rate: 0.0058182]
		[batch 20/20] avg loss: 0.06600579664140117		[learning rate: 0.0058112]
	Learning Rate: 0.00581116
	LOSS [training: 0.06971907606581945 | validation: 0.11534353318364661]
	TIME [epoch: 8.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09263989079085237		[learning rate: 0.0058041]
		[batch 20/20] avg loss: 0.14285858423571024		[learning rate: 0.0057971]
	Learning Rate: 0.0057971
	LOSS [training: 0.11774923751328133 | validation: 0.14442070942025914]
	TIME [epoch: 8.37 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34743798735285136		[learning rate: 0.0057901]
		[batch 20/20] avg loss: 0.4931584907979746		[learning rate: 0.0057831]
	Learning Rate: 0.00578306
	LOSS [training: 0.4202982390754131 | validation: 0.15845523008255213]
	TIME [epoch: 8.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2836554708005349		[learning rate: 0.0057761]
		[batch 20/20] avg loss: 0.1835244056000278		[learning rate: 0.0057691]
	Learning Rate: 0.00576906
	LOSS [training: 0.23358993820028134 | validation: 0.15781852686162806]
	TIME [epoch: 8.38 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08967666948995415		[learning rate: 0.0057621]
		[batch 20/20] avg loss: 0.0892315250424561		[learning rate: 0.0057551]
	Learning Rate: 0.0057551
	LOSS [training: 0.08945409726620514 | validation: 0.062110585085004916]
	TIME [epoch: 8.34 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07882591531072533		[learning rate: 0.0057481]
		[batch 20/20] avg loss: 0.05876026929950856		[learning rate: 0.0057412]
	Learning Rate: 0.00574116
	LOSS [training: 0.06879309230511695 | validation: 0.07984994469998521]
	TIME [epoch: 8.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11118550677028627		[learning rate: 0.0057342]
		[batch 20/20] avg loss: 0.08245172200746478		[learning rate: 0.0057273]
	Learning Rate: 0.00572727
	LOSS [training: 0.09681861438887551 | validation: 0.05884812477467953]
	TIME [epoch: 8.34 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0741171115575323		[learning rate: 0.0057203]
		[batch 20/20] avg loss: 0.06310219911152637		[learning rate: 0.0057134]
	Learning Rate: 0.0057134
	LOSS [training: 0.06860965533452934 | validation: 0.042468960912089966]
	TIME [epoch: 8.34 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09357010994373192		[learning rate: 0.0057065]
		[batch 20/20] avg loss: 0.0750857746655385		[learning rate: 0.0056996]
	Learning Rate: 0.00569957
	LOSS [training: 0.08432794230463522 | validation: 0.10644069851302912]
	TIME [epoch: 8.37 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.222940926335755		[learning rate: 0.0056927]
		[batch 20/20] avg loss: 0.09578975515361898		[learning rate: 0.0056858]
	Learning Rate: 0.00568577
	LOSS [training: 0.15936534074468697 | validation: 0.10229146534259959]
	TIME [epoch: 8.38 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1034993364108141		[learning rate: 0.0056789]
		[batch 20/20] avg loss: 0.13164734114367732		[learning rate: 0.005672]
	Learning Rate: 0.00567201
	LOSS [training: 0.11757333877724571 | validation: 0.04613490318905134]
	TIME [epoch: 8.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05412172775993318		[learning rate: 0.0056651]
		[batch 20/20] avg loss: 0.054623676891372305		[learning rate: 0.0056583]
	Learning Rate: 0.00565828
	LOSS [training: 0.05437270232565276 | validation: 0.05713579847048633]
	TIME [epoch: 8.37 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06103086856930397		[learning rate: 0.0056514]
		[batch 20/20] avg loss: 0.06033390777917523		[learning rate: 0.0056446]
	Learning Rate: 0.00564458
	LOSS [training: 0.0606823881742396 | validation: 0.12269743364976299]
	TIME [epoch: 8.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11753926712562704		[learning rate: 0.0056377]
		[batch 20/20] avg loss: 0.09614390184784005		[learning rate: 0.0056309]
	Learning Rate: 0.00563092
	LOSS [training: 0.10684158448673356 | validation: 0.10635325699792178]
	TIME [epoch: 8.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07801116993568875		[learning rate: 0.0056241]
		[batch 20/20] avg loss: 0.06292109508458502		[learning rate: 0.0056173]
	Learning Rate: 0.00561728
	LOSS [training: 0.07046613251013688 | validation: 0.06872395862009803]
	TIME [epoch: 8.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09759284165555918		[learning rate: 0.0056105]
		[batch 20/20] avg loss: 0.06507021828033614		[learning rate: 0.0056037]
	Learning Rate: 0.00560368
	LOSS [training: 0.08133152996794765 | validation: 0.03996589040002158]
	TIME [epoch: 8.37 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08012232717806562		[learning rate: 0.0055969]
		[batch 20/20] avg loss: 0.059594978972480415		[learning rate: 0.0055901]
	Learning Rate: 0.00559012
	LOSS [training: 0.069858653075273 | validation: 0.08447559764604647]
	TIME [epoch: 8.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08484191657926785		[learning rate: 0.0055833]
		[batch 20/20] avg loss: 0.09004337462906412		[learning rate: 0.0055766]
	Learning Rate: 0.00557659
	LOSS [training: 0.08744264560416597 | validation: 0.053510952381645244]
	TIME [epoch: 8.37 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19522672513788764		[learning rate: 0.0055698]
		[batch 20/20] avg loss: 0.12155673841407258		[learning rate: 0.0055631]
	Learning Rate: 0.00556309
	LOSS [training: 0.15839173177598012 | validation: 0.09768312999042653]
	TIME [epoch: 8.38 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11006805577110315		[learning rate: 0.0055563]
		[batch 20/20] avg loss: 0.1762474904020427		[learning rate: 0.0055496]
	Learning Rate: 0.00554962
	LOSS [training: 0.1431577730865729 | validation: 0.13377108510938587]
	TIME [epoch: 8.34 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10357527115291258		[learning rate: 0.0055429]
		[batch 20/20] avg loss: 0.06289968907662982		[learning rate: 0.0055362]
	Learning Rate: 0.00553618
	LOSS [training: 0.0832374801147712 | validation: 0.06460122534794345]
	TIME [epoch: 8.33 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06660684172163031		[learning rate: 0.0055295]
		[batch 20/20] avg loss: 0.06976349897571707		[learning rate: 0.0055228]
	Learning Rate: 0.00552278
	LOSS [training: 0.0681851703486737 | validation: 0.0920631567740596]
	TIME [epoch: 8.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2618988351412527		[learning rate: 0.0055161]
		[batch 20/20] avg loss: 0.1128830197705768		[learning rate: 0.0055094]
	Learning Rate: 0.00550941
	LOSS [training: 0.18739092745591474 | validation: 0.1046295213946409]
	TIME [epoch: 8.34 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06520552117360086		[learning rate: 0.0055027]
		[batch 20/20] avg loss: 0.08772400460737359		[learning rate: 0.0054961]
	Learning Rate: 0.00549607
	LOSS [training: 0.0764647628904872 | validation: 0.05874233100373302]
	TIME [epoch: 8.37 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11663251973645368		[learning rate: 0.0054894]
		[batch 20/20] avg loss: 0.06433554445604418		[learning rate: 0.0054828]
	Learning Rate: 0.00548277
	LOSS [training: 0.09048403209624892 | validation: 0.15195931888698586]
	TIME [epoch: 8.34 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1235355997717178		[learning rate: 0.0054761]
		[batch 20/20] avg loss: 0.08830953791167914		[learning rate: 0.0054695]
	Learning Rate: 0.0054695
	LOSS [training: 0.10592256884169848 | validation: 0.03073005279582611]
	TIME [epoch: 8.37 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05623271864206421		[learning rate: 0.0054629]
		[batch 20/20] avg loss: 0.051277706300821145		[learning rate: 0.0054563]
	Learning Rate: 0.00545626
	LOSS [training: 0.05375521247144268 | validation: 0.06954922628055835]
	TIME [epoch: 8.37 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10745725228856173		[learning rate: 0.0054496]
		[batch 20/20] avg loss: 0.04529883397237163		[learning rate: 0.005443]
	Learning Rate: 0.00544305
	LOSS [training: 0.07637804313046667 | validation: 0.05935921647640528]
	TIME [epoch: 8.34 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10489852211008391		[learning rate: 0.0054365]
		[batch 20/20] avg loss: 0.09041992655702827		[learning rate: 0.0054299]
	Learning Rate: 0.00542987
	LOSS [training: 0.0976592243335561 | validation: 0.12445534490389251]
	TIME [epoch: 8.34 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08981541023959742		[learning rate: 0.0054233]
		[batch 20/20] avg loss: 0.05825851251785654		[learning rate: 0.0054167]
	Learning Rate: 0.00541673
	LOSS [training: 0.07403696137872696 | validation: 0.04780762949987441]
	TIME [epoch: 8.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07018391204076799		[learning rate: 0.0054102]
		[batch 20/20] avg loss: 0.08632827792429246		[learning rate: 0.0054036]
	Learning Rate: 0.00540361
	LOSS [training: 0.07825609498253024 | validation: 0.08376872080805428]
	TIME [epoch: 8.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07250235423454506		[learning rate: 0.0053971]
		[batch 20/20] avg loss: 0.06611479257187348		[learning rate: 0.0053905]
	Learning Rate: 0.00539053
	LOSS [training: 0.06930857340320926 | validation: 0.055867962708246025]
	TIME [epoch: 8.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11357189884821092		[learning rate: 0.005384]
		[batch 20/20] avg loss: 0.08287591186728979		[learning rate: 0.0053775]
	Learning Rate: 0.00537748
	LOSS [training: 0.09822390535775037 | validation: 0.05175372284242137]
	TIME [epoch: 8.34 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09517778422601451		[learning rate: 0.005371]
		[batch 20/20] avg loss: 0.08859047371593191		[learning rate: 0.0053645]
	Learning Rate: 0.00536446
	LOSS [training: 0.09188412897097323 | validation: 0.056979154225211805]
	TIME [epoch: 8.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07136904736264466		[learning rate: 0.005358]
		[batch 20/20] avg loss: 0.09961231373189214		[learning rate: 0.0053515]
	Learning Rate: 0.00535148
	LOSS [training: 0.08549068054726841 | validation: 0.07837895642990042]
	TIME [epoch: 8.37 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07615937406093651		[learning rate: 0.005345]
		[batch 20/20] avg loss: 0.0679591184688388		[learning rate: 0.0053385]
	Learning Rate: 0.00533852
	LOSS [training: 0.07205924626488765 | validation: 0.07012643392124374]
	TIME [epoch: 8.34 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07225438845347264		[learning rate: 0.0053321]
		[batch 20/20] avg loss: 0.08444870723674598		[learning rate: 0.0053256]
	Learning Rate: 0.0053256
	LOSS [training: 0.07835154784510931 | validation: 0.16082542872065514]
	TIME [epoch: 8.34 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10808127410861215		[learning rate: 0.0053191]
		[batch 20/20] avg loss: 0.11304830882691161		[learning rate: 0.0053127]
	Learning Rate: 0.00531271
	LOSS [training: 0.11056479146776188 | validation: 0.13320309494846255]
	TIME [epoch: 8.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33018416179311383		[learning rate: 0.0053063]
		[batch 20/20] avg loss: 0.1739256553661221		[learning rate: 0.0052998]
	Learning Rate: 0.00529984
	LOSS [training: 0.252054908579618 | validation: 0.07026803308560951]
	TIME [epoch: 8.34 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06171371437506399		[learning rate: 0.0052934]
		[batch 20/20] avg loss: 0.08077610303098223		[learning rate: 0.005287]
	Learning Rate: 0.00528701
	LOSS [training: 0.07124490870302312 | validation: 0.08152173693906718]
	TIME [epoch: 8.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14498319447373587		[learning rate: 0.0052806]
		[batch 20/20] avg loss: 0.06168701272607343		[learning rate: 0.0052742]
	Learning Rate: 0.00527422
	LOSS [training: 0.10333510359990465 | validation: 0.05455635690385968]
	TIME [epoch: 8.34 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08132050485518541		[learning rate: 0.0052678]
		[batch 20/20] avg loss: 0.06314195643540829		[learning rate: 0.0052614]
	Learning Rate: 0.00526145
	LOSS [training: 0.07223123064529686 | validation: 0.12307059372263268]
	TIME [epoch: 8.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15071622504140442		[learning rate: 0.0052551]
		[batch 20/20] avg loss: 0.11302199093956852		[learning rate: 0.0052487]
	Learning Rate: 0.00524871
	LOSS [training: 0.13186910799048643 | validation: 0.12502744143663003]
	TIME [epoch: 8.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09438883837024278		[learning rate: 0.0052424]
		[batch 20/20] avg loss: 0.13473578416026102		[learning rate: 0.005236]
	Learning Rate: 0.005236
	LOSS [training: 0.11456231126525192 | validation: 0.10725639287190339]
	TIME [epoch: 8.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12828519976344538		[learning rate: 0.0052297]
		[batch 20/20] avg loss: 0.13541287910741656		[learning rate: 0.0052233]
	Learning Rate: 0.00522333
	LOSS [training: 0.13184903943543097 | validation: 0.09015166994982081]
	TIME [epoch: 8.33 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07365790263127697		[learning rate: 0.005217]
		[batch 20/20] avg loss: 0.0912688024098969		[learning rate: 0.0052107]
	Learning Rate: 0.00521068
	LOSS [training: 0.08246335252058694 | validation: 0.0774217728325956]
	TIME [epoch: 8.35 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07519819783447743		[learning rate: 0.0052044]
		[batch 20/20] avg loss: 0.08013212178311847		[learning rate: 0.0051981]
	Learning Rate: 0.00519807
	LOSS [training: 0.07766515980879796 | validation: 0.14588581902546105]
	TIME [epoch: 8.33 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15550287329837487		[learning rate: 0.0051918]
		[batch 20/20] avg loss: 0.08215500894022908		[learning rate: 0.0051855]
	Learning Rate: 0.00518549
	LOSS [training: 0.11882894111930198 | validation: 0.11246221357731148]
	TIME [epoch: 8.35 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0856283412099871		[learning rate: 0.0051792]
		[batch 20/20] avg loss: 0.11294254973926635		[learning rate: 0.0051729]
	Learning Rate: 0.00517293
	LOSS [training: 0.09928544547462671 | validation: 0.04755642931102188]
	TIME [epoch: 8.34 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09343661389454158		[learning rate: 0.0051667]
		[batch 20/20] avg loss: 0.06652977863066886		[learning rate: 0.0051604]
	Learning Rate: 0.00516041
	LOSS [training: 0.07998319626260522 | validation: 0.096502706928457]
	TIME [epoch: 8.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07678411116252551		[learning rate: 0.0051542]
		[batch 20/20] avg loss: 0.08059843835807592		[learning rate: 0.0051479]
	Learning Rate: 0.00514792
	LOSS [training: 0.07869127476030072 | validation: 0.04480327364713611]
	TIME [epoch: 8.37 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05901558100868681		[learning rate: 0.0051417]
		[batch 20/20] avg loss: 0.055260834388189004		[learning rate: 0.0051355]
	Learning Rate: 0.00513546
	LOSS [training: 0.0571382076984379 | validation: 0.049346736467532806]
	TIME [epoch: 8.33 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054774917160300765		[learning rate: 0.0051292]
		[batch 20/20] avg loss: 0.066584011889922		[learning rate: 0.005123]
	Learning Rate: 0.00512302
	LOSS [training: 0.060679464525111394 | validation: 0.08370791466350384]
	TIME [epoch: 8.33 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09761160619204975		[learning rate: 0.0051168]
		[batch 20/20] avg loss: 0.06510886460572497		[learning rate: 0.0051106]
	Learning Rate: 0.00511062
	LOSS [training: 0.08136023539888734 | validation: 0.03763101425399773]
	TIME [epoch: 8.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06114402836017933		[learning rate: 0.0051044]
		[batch 20/20] avg loss: 0.064885202242344		[learning rate: 0.0050982]
	Learning Rate: 0.00509825
	LOSS [training: 0.06301461530126168 | validation: 0.06774355534783831]
	TIME [epoch: 8.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06449003880802012		[learning rate: 0.0050921]
		[batch 20/20] avg loss: 0.06735684926066461		[learning rate: 0.0050859]
	Learning Rate: 0.00508591
	LOSS [training: 0.06592344403434237 | validation: 0.03253545968429487]
	TIME [epoch: 8.34 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05505412579219047		[learning rate: 0.0050797]
		[batch 20/20] avg loss: 0.08897758368533025		[learning rate: 0.0050736]
	Learning Rate: 0.00507359
	LOSS [training: 0.07201585473876036 | validation: 0.0338383600387987]
	TIME [epoch: 8.33 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052997052575820235		[learning rate: 0.0050674]
		[batch 20/20] avg loss: 0.05769127253989803		[learning rate: 0.0050613]
	Learning Rate: 0.00506131
	LOSS [training: 0.05534416255785912 | validation: 0.12673170653115592]
	TIME [epoch: 8.38 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16927408940911415		[learning rate: 0.0050552]
		[batch 20/20] avg loss: 0.1474528654184563		[learning rate: 0.0050491]
	Learning Rate: 0.00504906
	LOSS [training: 0.1583634774137852 | validation: 0.06538315793546365]
	TIME [epoch: 8.34 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18862721498573057		[learning rate: 0.0050429]
		[batch 20/20] avg loss: 0.1251307209809198		[learning rate: 0.0050368]
	Learning Rate: 0.00503684
	LOSS [training: 0.1568789679833252 | validation: 0.1688021619893974]
	TIME [epoch: 8.33 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10102121104675421		[learning rate: 0.0050307]
		[batch 20/20] avg loss: 0.07232867172590009		[learning rate: 0.0050246]
	Learning Rate: 0.00502464
	LOSS [training: 0.08667494138632717 | validation: 0.04780711099954414]
	TIME [epoch: 8.33 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1072940218077008		[learning rate: 0.0050186]
		[batch 20/20] avg loss: 0.06318091668176247		[learning rate: 0.0050125]
	Learning Rate: 0.00501248
	LOSS [training: 0.0852374692447316 | validation: 0.12798983889421672]
	TIME [epoch: 8.37 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06267156819266578		[learning rate: 0.0050064]
		[batch 20/20] avg loss: 0.07062095483476687		[learning rate: 0.0050003]
	Learning Rate: 0.00500034
	LOSS [training: 0.06664626151371632 | validation: 0.29072810184467374]
	TIME [epoch: 8.33 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060534359518526294		[learning rate: 0.0049943]
		[batch 20/20] avg loss: 0.04556756064932126		[learning rate: 0.0049882]
	Learning Rate: 0.00498824
	LOSS [training: 0.05305096008392378 | validation: 0.036163893384631024]
	TIME [epoch: 8.33 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07022970696762262		[learning rate: 0.0049822]
		[batch 20/20] avg loss: 0.09910309382014774		[learning rate: 0.0049762]
	Learning Rate: 0.00497616
	LOSS [training: 0.0846664003938852 | validation: 0.1475866692335717]
	TIME [epoch: 8.34 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07041416824917365		[learning rate: 0.0049701]
		[batch 20/20] avg loss: 0.0497241811585736		[learning rate: 0.0049641]
	Learning Rate: 0.00496412
	LOSS [training: 0.06006917470387363 | validation: 0.11604897128697962]
	TIME [epoch: 8.38 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08003790954490461		[learning rate: 0.0049581]
		[batch 20/20] avg loss: 0.06366212939122043		[learning rate: 0.0049521]
	Learning Rate: 0.0049521
	LOSS [training: 0.07185001946806252 | validation: 0.06130337738525138]
	TIME [epoch: 8.33 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10728917186329201		[learning rate: 0.0049461]
		[batch 20/20] avg loss: 0.07082410311864555		[learning rate: 0.0049401]
	Learning Rate: 0.00494011
	LOSS [training: 0.08905663749096876 | validation: 0.08147390298303037]
	TIME [epoch: 8.32 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09785419055484132		[learning rate: 0.0049341]
		[batch 20/20] avg loss: 0.08862949517011591		[learning rate: 0.0049282]
	Learning Rate: 0.00492815
	LOSS [training: 0.09324184286247864 | validation: 0.07974540930247508]
	TIME [epoch: 8.33 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06164928625195981		[learning rate: 0.0049222]
		[batch 20/20] avg loss: 0.0653470904649722		[learning rate: 0.0049162]
	Learning Rate: 0.00491622
	LOSS [training: 0.063498188358466 | validation: 0.06654307927787753]
	TIME [epoch: 8.37 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07757578523228806		[learning rate: 0.0049103]
		[batch 20/20] avg loss: 0.1559053695500296		[learning rate: 0.0049043]
	Learning Rate: 0.00490432
	LOSS [training: 0.11674057739115884 | validation: 0.18219743345043163]
	TIME [epoch: 8.33 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17289011967731763		[learning rate: 0.0048984]
		[batch 20/20] avg loss: 0.11992668241706905		[learning rate: 0.0048924]
	Learning Rate: 0.00489245
	LOSS [training: 0.14640840104719335 | validation: 0.08236615774757482]
	TIME [epoch: 8.33 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06533802653485125		[learning rate: 0.0048865]
		[batch 20/20] avg loss: 0.06477921722579133		[learning rate: 0.0048806]
	Learning Rate: 0.00488061
	LOSS [training: 0.0650586218803213 | validation: 0.05157839641810311]
	TIME [epoch: 8.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0680189450590671		[learning rate: 0.0048747]
		[batch 20/20] avg loss: 0.06447200647812047		[learning rate: 0.0048688]
	Learning Rate: 0.00486879
	LOSS [training: 0.06624547576859377 | validation: 0.10404884988390707]
	TIME [epoch: 8.34 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07795026630702065		[learning rate: 0.0048629]
		[batch 20/20] avg loss: 0.1470615936548853		[learning rate: 0.004857]
	Learning Rate: 0.004857
	LOSS [training: 0.11250592998095295 | validation: 0.2070346915752017]
	TIME [epoch: 8.32 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1351902136843393		[learning rate: 0.0048511]
		[batch 20/20] avg loss: 0.08613784587157988		[learning rate: 0.0048452]
	Learning Rate: 0.00484525
	LOSS [training: 0.1106640297779596 | validation: 0.09269397342237877]
	TIME [epoch: 8.32 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10091294186972813		[learning rate: 0.0048394]
		[batch 20/20] avg loss: 0.056254317218947816		[learning rate: 0.0048335]
	Learning Rate: 0.00483352
	LOSS [training: 0.07858362954433798 | validation: 0.08456902588868567]
	TIME [epoch: 8.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05910251556645966		[learning rate: 0.0048277]
		[batch 20/20] avg loss: 0.08162810627983938		[learning rate: 0.0048218]
	Learning Rate: 0.00482181
	LOSS [training: 0.07036531092314953 | validation: 0.15962593761754904]
	TIME [epoch: 8.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09362741573268782		[learning rate: 0.004816]
		[batch 20/20] avg loss: 0.05342876658202504		[learning rate: 0.0048101]
	Learning Rate: 0.00481014
	LOSS [training: 0.07352809115735645 | validation: 0.03876672124274583]
	TIME [epoch: 8.33 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0520095479953413		[learning rate: 0.0048043]
		[batch 20/20] avg loss: 0.09484318665392512		[learning rate: 0.0047985]
	Learning Rate: 0.0047985
	LOSS [training: 0.07342636732463322 | validation: 0.09693948379163561]
	TIME [epoch: 8.35 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08258420526238452		[learning rate: 0.0047927]
		[batch 20/20] avg loss: 0.08622224383320631		[learning rate: 0.0047869]
	Learning Rate: 0.00478688
	LOSS [training: 0.08440322454779543 | validation: 0.09081644609132107]
	TIME [epoch: 8.32 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1074613299724276		[learning rate: 0.0047811]
		[batch 20/20] avg loss: 0.03646744003844975		[learning rate: 0.0047753]
	Learning Rate: 0.00477529
	LOSS [training: 0.07196438500543868 | validation: 0.03139037098216637]
	TIME [epoch: 8.35 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04048599584331362		[learning rate: 0.0047695]
		[batch 20/20] avg loss: 0.19489626724703799		[learning rate: 0.0047637]
	Learning Rate: 0.00476373
	LOSS [training: 0.11769113154517581 | validation: 0.15745540052141255]
	TIME [epoch: 8.32 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24547349039911848		[learning rate: 0.004758]
		[batch 20/20] avg loss: 0.2592520068292609		[learning rate: 0.0047522]
	Learning Rate: 0.0047522
	LOSS [training: 0.2523627486141897 | validation: 0.2501148604250857]
	TIME [epoch: 8.34 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16872046632921278		[learning rate: 0.0047464]
		[batch 20/20] avg loss: 0.0735153146990387		[learning rate: 0.0047407]
	Learning Rate: 0.0047407
	LOSS [training: 0.12111789051412576 | validation: 0.04677893098656392]
	TIME [epoch: 8.33 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06459047936877862		[learning rate: 0.004735]
		[batch 20/20] avg loss: 0.09660405690953497		[learning rate: 0.0047292]
	Learning Rate: 0.00472922
	LOSS [training: 0.08059726813915678 | validation: 0.1036206723558198]
	TIME [epoch: 8.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06737494222274434		[learning rate: 0.0047235]
		[batch 20/20] avg loss: 0.1112840877331576		[learning rate: 0.0047178]
	Learning Rate: 0.00471777
	LOSS [training: 0.08932951497795097 | validation: 0.14008256104186345]
	TIME [epoch: 8.33 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08794843397040548		[learning rate: 0.0047121]
		[batch 20/20] avg loss: 0.08777475806258601		[learning rate: 0.0047064]
	Learning Rate: 0.00470635
	LOSS [training: 0.08786159601649574 | validation: 0.19266848248288657]
	TIME [epoch: 8.35 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0675536074598047		[learning rate: 0.0047006]
		[batch 20/20] avg loss: 0.06021093376244472		[learning rate: 0.004695]
	Learning Rate: 0.00469496
	LOSS [training: 0.0638822706111247 | validation: 0.03490498067887154]
	TIME [epoch: 8.32 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037142105934490716		[learning rate: 0.0046893]
		[batch 20/20] avg loss: 0.05232280631852339		[learning rate: 0.0046836]
	Learning Rate: 0.00468359
	LOSS [training: 0.044732456126507045 | validation: 0.05601410241130416]
	TIME [epoch: 8.34 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12877977532013418		[learning rate: 0.0046779]
		[batch 20/20] avg loss: 0.2496934075757601		[learning rate: 0.0046723]
	Learning Rate: 0.00467225
	LOSS [training: 0.18923659144794716 | validation: 0.23091973465399246]
	TIME [epoch: 8.32 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13504766794504058		[learning rate: 0.0046666]
		[batch 20/20] avg loss: 0.14228295841574592		[learning rate: 0.0046609]
	Learning Rate: 0.00466094
	LOSS [training: 0.13866531318039327 | validation: 0.1976875317874081]
	TIME [epoch: 8.34 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22275545876683972		[learning rate: 0.0046553]
		[batch 20/20] avg loss: 0.12169895634563974		[learning rate: 0.0046497]
	Learning Rate: 0.00464966
	LOSS [training: 0.17222720755623971 | validation: 0.0911489664096082]
	TIME [epoch: 8.32 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08630935486187861		[learning rate: 0.004644]
		[batch 20/20] avg loss: 0.06448080663016022		[learning rate: 0.0046384]
	Learning Rate: 0.0046384
	LOSS [training: 0.07539508074601942 | validation: 0.02665157765914263]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_417.pth
	Model improved!!!
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1294213655960069		[learning rate: 0.0046328]
		[batch 20/20] avg loss: 0.1228958260947178		[learning rate: 0.0046272]
	Learning Rate: 0.00462717
	LOSS [training: 0.12615859584536238 | validation: 0.10862344242426626]
	TIME [epoch: 8.35 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0630598641787139		[learning rate: 0.0046216]
		[batch 20/20] avg loss: 0.0935473395762862		[learning rate: 0.004616]
	Learning Rate: 0.00461597
	LOSS [training: 0.07830360187750005 | validation: 0.045806746950317424]
	TIME [epoch: 8.32 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05595547319386431		[learning rate: 0.0046104]
		[batch 20/20] avg loss: 0.05807653758011937		[learning rate: 0.0046048]
	Learning Rate: 0.0046048
	LOSS [training: 0.05701600538699183 | validation: 0.05645243669778875]
	TIME [epoch: 8.32 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05742917250999126		[learning rate: 0.0045992]
		[batch 20/20] avg loss: 0.07016868238647522		[learning rate: 0.0045936]
	Learning Rate: 0.00459365
	LOSS [training: 0.06379892744823325 | validation: 0.04462057275163068]
	TIME [epoch: 8.33 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1099001465451466		[learning rate: 0.0045881]
		[batch 20/20] avg loss: 0.05443149213044246		[learning rate: 0.0045825]
	Learning Rate: 0.00458253
	LOSS [training: 0.08216581933779452 | validation: 0.08203629442963078]
	TIME [epoch: 8.31 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06742517303520815		[learning rate: 0.004577]
		[batch 20/20] avg loss: 0.05929144147465319		[learning rate: 0.0045714]
	Learning Rate: 0.00457144
	LOSS [training: 0.06335830725493066 | validation: 0.08152661341470792]
	TIME [epoch: 8.33 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05737959150908691		[learning rate: 0.0045659]
		[batch 20/20] avg loss: 0.0525072028623072		[learning rate: 0.0045604]
	Learning Rate: 0.00456037
	LOSS [training: 0.054943397185697074 | validation: 0.11762074535260725]
	TIME [epoch: 8.33 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05845655695967607		[learning rate: 0.0045548]
		[batch 20/20] avg loss: 0.06341487958741726		[learning rate: 0.0045493]
	Learning Rate: 0.00454933
	LOSS [training: 0.06093571827354667 | validation: 0.03655551694759995]
	TIME [epoch: 8.32 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07305175093376487		[learning rate: 0.0045438]
		[batch 20/20] avg loss: 0.09749533642068378		[learning rate: 0.0045383]
	Learning Rate: 0.00453832
	LOSS [training: 0.08527354367722434 | validation: 0.030360917287642675]
	TIME [epoch: 8.34 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05430045675804599		[learning rate: 0.0045328]
		[batch 20/20] avg loss: 0.10735534058085486		[learning rate: 0.0045273]
	Learning Rate: 0.00452733
	LOSS [training: 0.08082789866945042 | validation: 0.08185609793865832]
	TIME [epoch: 8.32 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07632296975992058		[learning rate: 0.0045218]
		[batch 20/20] avg loss: 0.04910930670616904		[learning rate: 0.0045164]
	Learning Rate: 0.00451637
	LOSS [training: 0.06271613823304481 | validation: 0.06296744273917534]
	TIME [epoch: 8.32 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049002512243876244		[learning rate: 0.0045109]
		[batch 20/20] avg loss: 0.05665036272314955		[learning rate: 0.0045054]
	Learning Rate: 0.00450544
	LOSS [training: 0.0528264374835129 | validation: 0.05447665910166587]
	TIME [epoch: 8.32 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04930292768156246		[learning rate: 0.0045]
		[batch 20/20] avg loss: 0.039664664535850214		[learning rate: 0.0044945]
	Learning Rate: 0.00449453
	LOSS [training: 0.04448379610870634 | validation: 0.03848171680386567]
	TIME [epoch: 8.31 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04678343231278258		[learning rate: 0.0044891]
		[batch 20/20] avg loss: 0.09006633537314135		[learning rate: 0.0044836]
	Learning Rate: 0.00448365
	LOSS [training: 0.068424883842962 | validation: 0.06489869427190137]
	TIME [epoch: 8.34 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06239841327494494		[learning rate: 0.0044782]
		[batch 20/20] avg loss: 0.04329169435057345		[learning rate: 0.0044728]
	Learning Rate: 0.00447279
	LOSS [training: 0.05284505381275919 | validation: 0.12137286686961755]
	TIME [epoch: 8.34 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06693287364995414		[learning rate: 0.0044674]
		[batch 20/20] avg loss: 0.08075628792005342		[learning rate: 0.004462]
	Learning Rate: 0.00446197
	LOSS [training: 0.07384458078500376 | validation: 0.08008241737489841]
	TIME [epoch: 8.33 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06264507922802194		[learning rate: 0.0044566]
		[batch 20/20] avg loss: 0.10836722141866251		[learning rate: 0.0044512]
	Learning Rate: 0.00445116
	LOSS [training: 0.08550615032334222 | validation: 0.07355408007225943]
	TIME [epoch: 8.35 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1358565472710372		[learning rate: 0.0044458]
		[batch 20/20] avg loss: 0.15220755292743687		[learning rate: 0.0044404]
	Learning Rate: 0.00444039
	LOSS [training: 0.14403205009923706 | validation: 0.15090446187197043]
	TIME [epoch: 8.31 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1332125587822968		[learning rate: 0.004435]
		[batch 20/20] avg loss: 0.14063216355731797		[learning rate: 0.0044296]
	Learning Rate: 0.00442964
	LOSS [training: 0.1369223611698074 | validation: 0.0792913423954768]
	TIME [epoch: 8.33 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15537813236896417		[learning rate: 0.0044243]
		[batch 20/20] avg loss: 0.09547486476752719		[learning rate: 0.0044189]
	Learning Rate: 0.00441892
	LOSS [training: 0.12542649856824567 | validation: 0.03654444947698626]
	TIME [epoch: 8.33 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04158565272227611		[learning rate: 0.0044136]
		[batch 20/20] avg loss: 0.04187270034693273		[learning rate: 0.0044082]
	Learning Rate: 0.00440822
	LOSS [training: 0.04172917653460442 | validation: 0.08666320362654495]
	TIME [epoch: 8.32 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0795067816865677		[learning rate: 0.0044029]
		[batch 20/20] avg loss: 0.0754544830070559		[learning rate: 0.0043975]
	Learning Rate: 0.00439755
	LOSS [training: 0.07748063234681178 | validation: 0.07204100752782315]
	TIME [epoch: 8.34 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06403312774764017		[learning rate: 0.0043922]
		[batch 20/20] avg loss: 0.0603381873093802		[learning rate: 0.0043869]
	Learning Rate: 0.0043869
	LOSS [training: 0.06218565752851017 | validation: 0.049384829971928904]
	TIME [epoch: 8.34 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13000920559953874		[learning rate: 0.0043816]
		[batch 20/20] avg loss: 0.08992886082654149		[learning rate: 0.0043763]
	Learning Rate: 0.00437628
	LOSS [training: 0.10996903321304012 | validation: 0.08245415039779999]
	TIME [epoch: 8.33 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11602954289520113		[learning rate: 0.004371]
		[batch 20/20] avg loss: 0.0636594693446864		[learning rate: 0.0043657]
	Learning Rate: 0.00436569
	LOSS [training: 0.08984450611994375 | validation: 0.03939202523032939]
	TIME [epoch: 8.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046283841172656236		[learning rate: 0.0043604]
		[batch 20/20] avg loss: 0.06039387585619629		[learning rate: 0.0043551]
	Learning Rate: 0.00435512
	LOSS [training: 0.05333885851442627 | validation: 0.03964382698830597]
	TIME [epoch: 8.32 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05990974586419021		[learning rate: 0.0043498]
		[batch 20/20] avg loss: 0.0733824561014682		[learning rate: 0.0043446]
	Learning Rate: 0.00434458
	LOSS [training: 0.0666461009828292 | validation: 0.14564258710994635]
	TIME [epoch: 8.34 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05897571243982933		[learning rate: 0.0043393]
		[batch 20/20] avg loss: 0.05782906905430518		[learning rate: 0.0043341]
	Learning Rate: 0.00433406
	LOSS [training: 0.058402390747067254 | validation: 0.05415733749178467]
	TIME [epoch: 8.32 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05840581508113367		[learning rate: 0.0043288]
		[batch 20/20] avg loss: 0.08259099996207792		[learning rate: 0.0043236]
	Learning Rate: 0.00432357
	LOSS [training: 0.0704984075216058 | validation: 0.11160663458027777]
	TIME [epoch: 8.33 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06465582230486544		[learning rate: 0.0043183]
		[batch 20/20] avg loss: 0.1682261377576431		[learning rate: 0.0043131]
	Learning Rate: 0.0043131
	LOSS [training: 0.11644098003125429 | validation: 0.08666042197803628]
	TIME [epoch: 8.33 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11281115046391185		[learning rate: 0.0043079]
		[batch 20/20] avg loss: 0.04801171585879469		[learning rate: 0.0043027]
	Learning Rate: 0.00430266
	LOSS [training: 0.08041143316135327 | validation: 0.019634974048101377]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03918726025585956		[learning rate: 0.0042974]
		[batch 20/20] avg loss: 0.08603753280082607		[learning rate: 0.0042922]
	Learning Rate: 0.00429224
	LOSS [training: 0.0626123965283428 | validation: 0.01892705373626572]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03315704754073172		[learning rate: 0.004287]
		[batch 20/20] avg loss: 0.1320724174488259		[learning rate: 0.0042819]
	Learning Rate: 0.00428185
	LOSS [training: 0.08261473249477881 | validation: 0.08684352212465248]
	TIME [epoch: 8.32 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09198140686377373		[learning rate: 0.0042767]
		[batch 20/20] avg loss: 0.15431484036595636		[learning rate: 0.0042715]
	Learning Rate: 0.00427149
	LOSS [training: 0.12314812361486502 | validation: 0.33166493145629844]
	TIME [epoch: 8.32 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4521743484949046		[learning rate: 0.0042663]
		[batch 20/20] avg loss: 0.15948216321793168		[learning rate: 0.0042611]
	Learning Rate: 0.00426114
	LOSS [training: 0.30582825585641815 | validation: 0.15490362032297106]
	TIME [epoch: 8.34 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13401875635504246		[learning rate: 0.004256]
		[batch 20/20] avg loss: 0.13046114852217355		[learning rate: 0.0042508]
	Learning Rate: 0.00425083
	LOSS [training: 0.132239952438608 | validation: 0.056405881225638096]
	TIME [epoch: 8.32 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06746648695550189		[learning rate: 0.0042457]
		[batch 20/20] avg loss: 0.05036990584894067		[learning rate: 0.0042405]
	Learning Rate: 0.00424054
	LOSS [training: 0.05891819640222129 | validation: 0.03895862668926971]
	TIME [epoch: 8.33 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08179153804140135		[learning rate: 0.0042354]
		[batch 20/20] avg loss: 0.1013345338841722		[learning rate: 0.0042303]
	Learning Rate: 0.00423027
	LOSS [training: 0.0915630359627868 | validation: 0.08515117401347065]
	TIME [epoch: 8.32 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1735326346050349		[learning rate: 0.0042251]
		[batch 20/20] avg loss: 0.14289311879584526		[learning rate: 0.00422]
	Learning Rate: 0.00422003
	LOSS [training: 0.1582128767004401 | validation: 0.1523889838607253]
	TIME [epoch: 8.34 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14108874736803972		[learning rate: 0.0042149]
		[batch 20/20] avg loss: 0.10630931883073884		[learning rate: 0.0042098]
	Learning Rate: 0.00420982
	LOSS [training: 0.12369903309938927 | validation: 0.11943668754276723]
	TIME [epoch: 8.34 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3379192891541263		[learning rate: 0.0042047]
		[batch 20/20] avg loss: 0.2838460982051309		[learning rate: 0.0041996]
	Learning Rate: 0.00419962
	LOSS [training: 0.3108826936796286 | validation: 0.2513178595721096]
	TIME [epoch: 8.32 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24093794881172767		[learning rate: 0.0041945]
		[batch 20/20] avg loss: 0.33164630688105257		[learning rate: 0.0041895]
	Learning Rate: 0.00418946
	LOSS [training: 0.2862921278463902 | validation: 0.3663347949464212]
	TIME [epoch: 8.31 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3217747976904949		[learning rate: 0.0041844]
		[batch 20/20] avg loss: 0.2393480750589921		[learning rate: 0.0041793]
	Learning Rate: 0.00417932
	LOSS [training: 0.2805614363747435 | validation: 0.20761248613412664]
	TIME [epoch: 8.33 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2380677029744951		[learning rate: 0.0041743]
		[batch 20/20] avg loss: 0.23827966030971778		[learning rate: 0.0041692]
	Learning Rate: 0.0041692
	LOSS [training: 0.23817368164210642 | validation: 0.8573184552693172]
	TIME [epoch: 8.31 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2598741914041903		[learning rate: 0.0041641]
		[batch 20/20] avg loss: 0.10580101389702153		[learning rate: 0.0041591]
	Learning Rate: 0.00415911
	LOSS [training: 0.18283760265060592 | validation: 0.2662486357334217]
	TIME [epoch: 8.34 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12277242859337074		[learning rate: 0.0041541]
		[batch 20/20] avg loss: 0.06966549533847627		[learning rate: 0.004149]
	Learning Rate: 0.00414904
	LOSS [training: 0.09621896196592351 | validation: 0.07098476734202122]
	TIME [epoch: 8.32 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04134603263253262		[learning rate: 0.004144]
		[batch 20/20] avg loss: 0.03976046710575208		[learning rate: 0.004139]
	Learning Rate: 0.00413899
	LOSS [training: 0.04055324986914235 | validation: 0.04495522527982774]
	TIME [epoch: 8.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13254246426285643		[learning rate: 0.004134]
		[batch 20/20] avg loss: 0.1323028751043303		[learning rate: 0.004129]
	Learning Rate: 0.00412897
	LOSS [training: 0.1324226696835934 | validation: 0.2203433402331177]
	TIME [epoch: 8.33 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08583927728069615		[learning rate: 0.004124]
		[batch 20/20] avg loss: 0.04952435765786865		[learning rate: 0.004119]
	Learning Rate: 0.00411898
	LOSS [training: 0.0676818174692824 | validation: 0.026607461760513897]
	TIME [epoch: 8.34 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09447615296737626		[learning rate: 0.004114]
		[batch 20/20] avg loss: 0.04414098234818405		[learning rate: 0.004109]
	Learning Rate: 0.00410901
	LOSS [training: 0.06930856765778015 | validation: 0.05583400866084727]
	TIME [epoch: 8.31 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04936038795201843		[learning rate: 0.004104]
		[batch 20/20] avg loss: 0.03700907852426445		[learning rate: 0.0040991]
	Learning Rate: 0.00409906
	LOSS [training: 0.04318473323814143 | validation: 0.033977696828730815]
	TIME [epoch: 8.35 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04600310097549936		[learning rate: 0.0040941]
		[batch 20/20] avg loss: 0.08039122356332165		[learning rate: 0.0040891]
	Learning Rate: 0.00408914
	LOSS [training: 0.0631971622694105 | validation: 0.10678195082395957]
	TIME [epoch: 8.32 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09980984386679288		[learning rate: 0.0040842]
		[batch 20/20] avg loss: 0.13906267898570385		[learning rate: 0.0040792]
	Learning Rate: 0.00407924
	LOSS [training: 0.11943626142624834 | validation: 0.09280507728932941]
	TIME [epoch: 8.33 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1500173529898523		[learning rate: 0.0040743]
		[batch 20/20] avg loss: 0.08063338297989925		[learning rate: 0.0040694]
	Learning Rate: 0.00406936
	LOSS [training: 0.1153253679848758 | validation: 0.08869930288752455]
	TIME [epoch: 8.31 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055891763601088565		[learning rate: 0.0040644]
		[batch 20/20] avg loss: 0.09337962319027782		[learning rate: 0.0040595]
	Learning Rate: 0.00405951
	LOSS [training: 0.0746356933956832 | validation: 0.1646688559086652]
	TIME [epoch: 8.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07957308166447641		[learning rate: 0.0040546]
		[batch 20/20] avg loss: 0.06065555129638782		[learning rate: 0.0040497]
	Learning Rate: 0.00404968
	LOSS [training: 0.07011431648043212 | validation: 0.04341857082378202]
	TIME [epoch: 8.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07340958681191118		[learning rate: 0.0040448]
		[batch 20/20] avg loss: 0.05742276217872907		[learning rate: 0.0040399]
	Learning Rate: 0.00403988
	LOSS [training: 0.06541617449532011 | validation: 0.05266922082116161]
	TIME [epoch: 8.3 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036512484983807894		[learning rate: 0.004035]
		[batch 20/20] avg loss: 0.0724414637196685		[learning rate: 0.0040301]
	Learning Rate: 0.0040301
	LOSS [training: 0.054476974351738205 | validation: 0.15084027509270412]
	TIME [epoch: 8.3 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10519404029630582		[learning rate: 0.0040252]
		[batch 20/20] avg loss: 0.1445639617171787		[learning rate: 0.0040203]
	Learning Rate: 0.00402034
	LOSS [training: 0.12487900100674225 | validation: 0.0691391305754498]
	TIME [epoch: 8.33 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0782409335919007		[learning rate: 0.0040155]
		[batch 20/20] avg loss: 0.08332766730995653		[learning rate: 0.0040106]
	Learning Rate: 0.00401061
	LOSS [training: 0.08078430045092862 | validation: 0.08317048980327063]
	TIME [epoch: 8.33 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08982678482479613		[learning rate: 0.0040058]
		[batch 20/20] avg loss: 0.09843780698282623		[learning rate: 0.0040009]
	Learning Rate: 0.0040009
	LOSS [training: 0.09413229590381117 | validation: 0.05591598512137847]
	TIME [epoch: 8.32 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09490451295530335		[learning rate: 0.0039961]
		[batch 20/20] avg loss: 0.09088815265291954		[learning rate: 0.0039912]
	Learning Rate: 0.00399122
	LOSS [training: 0.09289633280411144 | validation: 0.08201384640046544]
	TIME [epoch: 8.32 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07361064529362868		[learning rate: 0.0039864]
		[batch 20/20] avg loss: 0.06528610133498461		[learning rate: 0.0039816]
	Learning Rate: 0.00398155
	LOSS [training: 0.06944837331430663 | validation: 0.0548426417710413]
	TIME [epoch: 8.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05634214968670657		[learning rate: 0.0039767]
		[batch 20/20] avg loss: 0.06647013659200861		[learning rate: 0.0039719]
	Learning Rate: 0.00397192
	LOSS [training: 0.061406143139357605 | validation: 0.04680621188381166]
	TIME [epoch: 8.34 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07112240089228322		[learning rate: 0.0039671]
		[batch 20/20] avg loss: 0.077898981402252		[learning rate: 0.0039623]
	Learning Rate: 0.0039623
	LOSS [training: 0.07451069114726763 | validation: 0.06019719293018884]
	TIME [epoch: 8.31 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10621805876581754		[learning rate: 0.0039575]
		[batch 20/20] avg loss: 0.06409795295033582		[learning rate: 0.0039527]
	Learning Rate: 0.00395271
	LOSS [training: 0.0851580058580767 | validation: 0.06753534961938955]
	TIME [epoch: 8.31 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051910787189094434		[learning rate: 0.0039479]
		[batch 20/20] avg loss: 0.05067840426705873		[learning rate: 0.0039431]
	Learning Rate: 0.00394314
	LOSS [training: 0.05129459572807658 | validation: 0.0996176010558213]
	TIME [epoch: 8.33 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09235443419504823		[learning rate: 0.0039384]
		[batch 20/20] avg loss: 0.06562188445794413		[learning rate: 0.0039336]
	Learning Rate: 0.00393359
	LOSS [training: 0.0789881593264962 | validation: 0.06077728010791407]
	TIME [epoch: 8.32 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05257145375674562		[learning rate: 0.0039288]
		[batch 20/20] avg loss: 0.03510270865557875		[learning rate: 0.0039241]
	Learning Rate: 0.00392407
	LOSS [training: 0.043837081206162185 | validation: 0.035217056526213854]
	TIME [epoch: 8.33 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05109986745108178		[learning rate: 0.0039193]
		[batch 20/20] avg loss: 0.18770512653237495		[learning rate: 0.0039146]
	Learning Rate: 0.00391457
	LOSS [training: 0.11940249699172838 | validation: 0.12662028777962542]
	TIME [epoch: 8.31 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10245933213617224		[learning rate: 0.0039098]
		[batch 20/20] avg loss: 0.13791303781519507		[learning rate: 0.0039051]
	Learning Rate: 0.00390509
	LOSS [training: 0.12018618497568363 | validation: 0.0868071113589982]
	TIME [epoch: 8.34 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10279174992651366		[learning rate: 0.0039004]
		[batch 20/20] avg loss: 0.10182005415520137		[learning rate: 0.0038956]
	Learning Rate: 0.00389564
	LOSS [training: 0.10230590204085752 | validation: 0.07806475472633817]
	TIME [epoch: 8.34 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07612799166897437		[learning rate: 0.0038909]
		[batch 20/20] avg loss: 0.10748308239430274		[learning rate: 0.0038862]
	Learning Rate: 0.00388621
	LOSS [training: 0.09180553703163856 | validation: 0.026665350076142118]
	TIME [epoch: 8.33 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0688137530748941		[learning rate: 0.0038815]
		[batch 20/20] avg loss: 0.05427603343290004		[learning rate: 0.0038768]
	Learning Rate: 0.0038768
	LOSS [training: 0.06154489325389707 | validation: 0.02504145515006519]
	TIME [epoch: 8.3 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03422580208264714		[learning rate: 0.0038721]
		[batch 20/20] avg loss: 0.049323809629224225		[learning rate: 0.0038674]
	Learning Rate: 0.00386742
	LOSS [training: 0.04177480585593569 | validation: 0.024932752590517433]
	TIME [epoch: 8.33 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050496841440476704		[learning rate: 0.0038627]
		[batch 20/20] avg loss: 0.04070675374104307		[learning rate: 0.0038581]
	Learning Rate: 0.00385805
	LOSS [training: 0.04560179759075989 | validation: 0.05068135282582278]
	TIME [epoch: 8.32 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08322057282407432		[learning rate: 0.0038534]
		[batch 20/20] avg loss: 0.07131438568022908		[learning rate: 0.0038487]
	Learning Rate: 0.00384871
	LOSS [training: 0.07726747925215169 | validation: 0.051877546968834426]
	TIME [epoch: 8.33 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05106710169044752		[learning rate: 0.0038441]
		[batch 20/20] avg loss: 0.07418233306296954		[learning rate: 0.0038394]
	Learning Rate: 0.0038394
	LOSS [training: 0.06262471737670855 | validation: 0.039099671082482086]
	TIME [epoch: 8.32 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03533834963028762		[learning rate: 0.0038347]
		[batch 20/20] avg loss: 0.052857869767776434		[learning rate: 0.0038301]
	Learning Rate: 0.0038301
	LOSS [training: 0.04409810969903203 | validation: 0.0665686619224871]
	TIME [epoch: 8.34 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07204112787394336		[learning rate: 0.0038255]
		[batch 20/20] avg loss: 0.044641530049174885		[learning rate: 0.0038208]
	Learning Rate: 0.00382083
	LOSS [training: 0.05834132896155912 | validation: 0.08076273836581616]
	TIME [epoch: 8.34 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06007565121353203		[learning rate: 0.0038162]
		[batch 20/20] avg loss: 0.07797090538178449		[learning rate: 0.0038116]
	Learning Rate: 0.00381158
	LOSS [training: 0.06902327829765825 | validation: 0.08610911633410964]
	TIME [epoch: 8.34 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06613620601938149		[learning rate: 0.003807]
		[batch 20/20] avg loss: 0.14399192674045827		[learning rate: 0.0038024]
	Learning Rate: 0.00380235
	LOSS [training: 0.10506406637991987 | validation: 0.10966149595657351]
	TIME [epoch: 8.31 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08022866257868319		[learning rate: 0.0037977]
		[batch 20/20] avg loss: 0.12441400461827992		[learning rate: 0.0037931]
	Learning Rate: 0.00379315
	LOSS [training: 0.10232133359848154 | validation: 0.1976883651034629]
	TIME [epoch: 8.33 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057707268393552925		[learning rate: 0.0037886]
		[batch 20/20] avg loss: 0.060473137454039594		[learning rate: 0.003784]
	Learning Rate: 0.00378397
	LOSS [training: 0.05909020292379626 | validation: 0.019114613255713855]
	TIME [epoch: 8.31 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05540923541137995		[learning rate: 0.0037794]
		[batch 20/20] avg loss: 0.09239702819600618		[learning rate: 0.0037748]
	Learning Rate: 0.00377481
	LOSS [training: 0.07390313180369308 | validation: 0.07376032879779675]
	TIME [epoch: 8.32 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07517194319422377		[learning rate: 0.0037702]
		[batch 20/20] avg loss: 0.06664413389531473		[learning rate: 0.0037657]
	Learning Rate: 0.00376567
	LOSS [training: 0.07090803854476925 | validation: 0.05198752147313526]
	TIME [epoch: 8.35 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06866838908473971		[learning rate: 0.0037611]
		[batch 20/20] avg loss: 0.09826257400029072		[learning rate: 0.0037566]
	Learning Rate: 0.00375655
	LOSS [training: 0.08346548154251522 | validation: 0.04315399515494535]
	TIME [epoch: 8.34 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05244431155124794		[learning rate: 0.003752]
		[batch 20/20] avg loss: 0.053572132564791916		[learning rate: 0.0037475]
	Learning Rate: 0.00374746
	LOSS [training: 0.05300822205801993 | validation: 0.06264352513479043]
	TIME [epoch: 8.32 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054570752333444106		[learning rate: 0.0037429]
		[batch 20/20] avg loss: 0.03780862400308207		[learning rate: 0.0037384]
	Learning Rate: 0.00373839
	LOSS [training: 0.04618968816826309 | validation: 0.04674594605581662]
	TIME [epoch: 8.34 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031073121361332928		[learning rate: 0.0037339]
		[batch 20/20] avg loss: 0.03636182549760878		[learning rate: 0.0037293]
	Learning Rate: 0.00372934
	LOSS [training: 0.033717473429470846 | validation: 0.04975472249234078]
	TIME [epoch: 8.3 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047484308830444875		[learning rate: 0.0037248]
		[batch 20/20] avg loss: 0.07363237622921802		[learning rate: 0.0037203]
	Learning Rate: 0.00372031
	LOSS [training: 0.060558342529831444 | validation: 0.12773591688696276]
	TIME [epoch: 8.34 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16345683563446972		[learning rate: 0.0037158]
		[batch 20/20] avg loss: 0.4808805429958774		[learning rate: 0.0037113]
	Learning Rate: 0.0037113
	LOSS [training: 0.3221686893151735 | validation: 0.5013064425171173]
	TIME [epoch: 8.31 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23065533401428615		[learning rate: 0.0037068]
		[batch 20/20] avg loss: 0.07167427364561693		[learning rate: 0.0037023]
	Learning Rate: 0.00370232
	LOSS [training: 0.15116480382995157 | validation: 0.07524588828558823]
	TIME [epoch: 8.31 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06386644159493537		[learning rate: 0.0036978]
		[batch 20/20] avg loss: 0.08317293984922382		[learning rate: 0.0036934]
	Learning Rate: 0.00369336
	LOSS [training: 0.07351969072207959 | validation: 0.027924743072413702]
	TIME [epoch: 8.33 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03327179470338322		[learning rate: 0.0036889]
		[batch 20/20] avg loss: 0.08084053952416574		[learning rate: 0.0036844]
	Learning Rate: 0.00368441
	LOSS [training: 0.05705616711377448 | validation: 0.12137118988489828]
	TIME [epoch: 8.34 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04239384533995787		[learning rate: 0.00368]
		[batch 20/20] avg loss: 0.06490486491905456		[learning rate: 0.0036755]
	Learning Rate: 0.00367549
	LOSS [training: 0.053649355129506214 | validation: 0.06095930049871688]
	TIME [epoch: 8.31 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03998953961188807		[learning rate: 0.003671]
		[batch 20/20] avg loss: 0.05298284004666729		[learning rate: 0.0036666]
	Learning Rate: 0.0036666
	LOSS [training: 0.04648618982927768 | validation: 0.147629171680936]
	TIME [epoch: 8.32 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05846050840523821		[learning rate: 0.0036622]
		[batch 20/20] avg loss: 0.052483192942554976		[learning rate: 0.0036577]
	Learning Rate: 0.00365772
	LOSS [training: 0.055471850673896594 | validation: 0.05945228639201867]
	TIME [epoch: 8.33 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043911837505387304		[learning rate: 0.0036533]
		[batch 20/20] avg loss: 0.04497796532223216		[learning rate: 0.0036489]
	Learning Rate: 0.00364887
	LOSS [training: 0.04444490141380973 | validation: 0.0460558004580861]
	TIME [epoch: 8.33 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03327172529792618		[learning rate: 0.0036444]
		[batch 20/20] avg loss: 0.06766928110274316		[learning rate: 0.00364]
	Learning Rate: 0.00364003
	LOSS [training: 0.05047050320033466 | validation: 0.07541796927881277]
	TIME [epoch: 8.31 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10235625891946065		[learning rate: 0.0036356]
		[batch 20/20] avg loss: 0.05300951864797826		[learning rate: 0.0036312]
	Learning Rate: 0.00363122
	LOSS [training: 0.07768288878371946 | validation: 0.07801643620574655]
	TIME [epoch: 8.31 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07040584421709857		[learning rate: 0.0036268]
		[batch 20/20] avg loss: 0.04677185750619214		[learning rate: 0.0036224]
	Learning Rate: 0.00362243
	LOSS [training: 0.05858885086164536 | validation: 0.027635613690449687]
	TIME [epoch: 8.3 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04077065846242726		[learning rate: 0.003618]
		[batch 20/20] avg loss: 0.058081737878360795		[learning rate: 0.0036137]
	Learning Rate: 0.00361366
	LOSS [training: 0.04942619817039404 | validation: 0.07773531601423378]
	TIME [epoch: 8.33 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04944885830213716		[learning rate: 0.0036093]
		[batch 20/20] avg loss: 0.057756339882313434		[learning rate: 0.0036049]
	Learning Rate: 0.00360491
	LOSS [training: 0.0536025990922253 | validation: 0.028778359107396782]
	TIME [epoch: 8.32 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04017952253888516		[learning rate: 0.0036005]
		[batch 20/20] avg loss: 0.03872327894959832		[learning rate: 0.0035962]
	Learning Rate: 0.00359619
	LOSS [training: 0.03945140074424174 | validation: 0.06704114954582543]
	TIME [epoch: 8.32 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04947312623134772		[learning rate: 0.0035918]
		[batch 20/20] avg loss: 0.061921924475664115		[learning rate: 0.0035875]
	Learning Rate: 0.00358748
	LOSS [training: 0.05569752535350592 | validation: 0.09486611867370767]
	TIME [epoch: 8.31 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13059069338091475		[learning rate: 0.0035831]
		[batch 20/20] avg loss: 0.11691528002096188		[learning rate: 0.0035788]
	Learning Rate: 0.0035788
	LOSS [training: 0.1237529867009383 | validation: 0.06789725008576994]
	TIME [epoch: 8.33 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054486079122822426		[learning rate: 0.0035745]
		[batch 20/20] avg loss: 0.06528769291064024		[learning rate: 0.0035701]
	Learning Rate: 0.00357013
	LOSS [training: 0.05988688601673133 | validation: 0.05194796290784265]
	TIME [epoch: 8.34 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08064100050970521		[learning rate: 0.0035658]
		[batch 20/20] avg loss: 0.06674612471994171		[learning rate: 0.0035615]
	Learning Rate: 0.00356149
	LOSS [training: 0.07369356261482347 | validation: 0.10406520397442877]
	TIME [epoch: 8.32 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09153051356524053		[learning rate: 0.0035572]
		[batch 20/20] avg loss: 0.053615031337127285		[learning rate: 0.0035529]
	Learning Rate: 0.00355287
	LOSS [training: 0.0725727724511839 | validation: 0.025386844864274158]
	TIME [epoch: 8.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05288789942788401		[learning rate: 0.0035486]
		[batch 20/20] avg loss: 0.0392202607655885		[learning rate: 0.0035443]
	Learning Rate: 0.00354427
	LOSS [training: 0.04605408009673625 | validation: 0.03486390937208605]
	TIME [epoch: 8.34 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04361530158124338		[learning rate: 0.00354]
		[batch 20/20] avg loss: 0.05791055057478377		[learning rate: 0.0035357]
	Learning Rate: 0.00353569
	LOSS [training: 0.05076292607801357 | validation: 0.042043614072927754]
	TIME [epoch: 8.33 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039058265805718514		[learning rate: 0.0035314]
		[batch 20/20] avg loss: 0.0643819377764722		[learning rate: 0.0035271]
	Learning Rate: 0.00352713
	LOSS [training: 0.05172010179109536 | validation: 0.06036555694067723]
	TIME [epoch: 8.32 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03905453685875961		[learning rate: 0.0035229]
		[batch 20/20] avg loss: 0.04262765035454921		[learning rate: 0.0035186]
	Learning Rate: 0.00351859
	LOSS [training: 0.04084109360665441 | validation: 0.03554322463993373]
	TIME [epoch: 8.34 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03381576555611937		[learning rate: 0.0035143]
		[batch 20/20] avg loss: 0.0912860629479394		[learning rate: 0.0035101]
	Learning Rate: 0.00351007
	LOSS [training: 0.06255091425202938 | validation: 0.11336832544960651]
	TIME [epoch: 8.35 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05291905406289671		[learning rate: 0.0035058]
		[batch 20/20] avg loss: 0.03852667307839377		[learning rate: 0.0035016]
	Learning Rate: 0.00350157
	LOSS [training: 0.04572286357064524 | validation: 0.03135493557455963]
	TIME [epoch: 8.33 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04115521664422502		[learning rate: 0.0034973]
		[batch 20/20] avg loss: 0.04936861759909227		[learning rate: 0.0034931]
	Learning Rate: 0.0034931
	LOSS [training: 0.04526191712165865 | validation: 0.07009368679643654]
	TIME [epoch: 8.35 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03393224781820391		[learning rate: 0.0034889]
		[batch 20/20] avg loss: 0.033729196778633404		[learning rate: 0.0034846]
	Learning Rate: 0.00348464
	LOSS [training: 0.03383072229841866 | validation: 0.0860706273263902]
	TIME [epoch: 8.31 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08640512487177568		[learning rate: 0.0034804]
		[batch 20/20] avg loss: 0.0629173529920831		[learning rate: 0.0034762]
	Learning Rate: 0.0034762
	LOSS [training: 0.07466123893192941 | validation: 0.12347063789972454]
	TIME [epoch: 8.34 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06985965510516992		[learning rate: 0.003472]
		[batch 20/20] avg loss: 0.07170899475754759		[learning rate: 0.0034678]
	Learning Rate: 0.00346779
	LOSS [training: 0.07078432493135875 | validation: 0.10709517968435377]
	TIME [epoch: 8.31 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05021195598137781		[learning rate: 0.0034636]
		[batch 20/20] avg loss: 0.04044852882089964		[learning rate: 0.0034594]
	Learning Rate: 0.00345939
	LOSS [training: 0.04533024240113873 | validation: 0.0752822183047508]
	TIME [epoch: 8.32 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06442945230063475		[learning rate: 0.0034552]
		[batch 20/20] avg loss: 0.035723898564594916		[learning rate: 0.003451]
	Learning Rate: 0.00345102
	LOSS [training: 0.05007667543261483 | validation: 0.04980200051885261]
	TIME [epoch: 8.34 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029314079275367666		[learning rate: 0.0034468]
		[batch 20/20] avg loss: 0.069403992945781		[learning rate: 0.0034427]
	Learning Rate: 0.00344266
	LOSS [training: 0.04935903611057434 | validation: 0.11520644413698208]
	TIME [epoch: 8.34 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06811369908609434		[learning rate: 0.0034385]
		[batch 20/20] avg loss: 0.08637749531984573		[learning rate: 0.0034343]
	Learning Rate: 0.00343433
	LOSS [training: 0.07724559720297004 | validation: 0.07711844735893135]
	TIME [epoch: 8.33 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09069654158914889		[learning rate: 0.0034302]
		[batch 20/20] avg loss: 0.03283858731530492		[learning rate: 0.003426]
	Learning Rate: 0.00342602
	LOSS [training: 0.061767564452226906 | validation: 0.04027314089439829]
	TIME [epoch: 8.35 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04401693933486964		[learning rate: 0.0034219]
		[batch 20/20] avg loss: 0.05210285581129674		[learning rate: 0.0034177]
	Learning Rate: 0.00341772
	LOSS [training: 0.04805989757308319 | validation: 0.02726600675248976]
	TIME [epoch: 8.32 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052581700504120935		[learning rate: 0.0034136]
		[batch 20/20] avg loss: 0.03403746686414161		[learning rate: 0.0034094]
	Learning Rate: 0.00340945
	LOSS [training: 0.04330958368413127 | validation: 0.02598008772508591]
	TIME [epoch: 8.34 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05097989753809325		[learning rate: 0.0034053]
		[batch 20/20] avg loss: 0.07974588686205579		[learning rate: 0.0034012]
	Learning Rate: 0.0034012
	LOSS [training: 0.06536289220007455 | validation: 0.11836529943266591]
	TIME [epoch: 8.32 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09564375293997782		[learning rate: 0.0033971]
		[batch 20/20] avg loss: 0.04424224764992354		[learning rate: 0.003393]
	Learning Rate: 0.00339296
	LOSS [training: 0.06994300029495067 | validation: 0.05850537077432695]
	TIME [epoch: 8.32 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04538368289887241		[learning rate: 0.0033889]
		[batch 20/20] avg loss: 0.046830437075295224		[learning rate: 0.0033847]
	Learning Rate: 0.00338475
	LOSS [training: 0.04610705998708382 | validation: 0.07047877913447885]
	TIME [epoch: 8.32 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03772436018040791		[learning rate: 0.0033806]
		[batch 20/20] avg loss: 0.03833566603873983		[learning rate: 0.0033766]
	Learning Rate: 0.00337655
	LOSS [training: 0.038030013109573865 | validation: 0.07860168208143184]
	TIME [epoch: 8.36 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08243109513836692		[learning rate: 0.0033725]
		[batch 20/20] avg loss: 0.052008741499769875		[learning rate: 0.0033684]
	Learning Rate: 0.00336838
	LOSS [training: 0.06721991831906839 | validation: 0.0438289770229827]
	TIME [epoch: 8.32 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06146421766406586		[learning rate: 0.0033643]
		[batch 20/20] avg loss: 0.03585721326879975		[learning rate: 0.0033602]
	Learning Rate: 0.00336023
	LOSS [training: 0.048660715466432816 | validation: 0.04436421634865524]
	TIME [epoch: 8.31 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05232622725165641		[learning rate: 0.0033562]
		[batch 20/20] avg loss: 0.06654855545740326		[learning rate: 0.0033521]
	Learning Rate: 0.00335209
	LOSS [training: 0.059437391354529835 | validation: 0.029592496612438363]
	TIME [epoch: 8.34 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04533780558891625		[learning rate: 0.003348]
		[batch 20/20] avg loss: 0.053771785194423896		[learning rate: 0.003344]
	Learning Rate: 0.00334398
	LOSS [training: 0.04955479539167008 | validation: 0.07517431641288959]
	TIME [epoch: 8.34 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05556613290436292		[learning rate: 0.0033399]
		[batch 20/20] avg loss: 0.05379197076572358		[learning rate: 0.0033359]
	Learning Rate: 0.00333588
	LOSS [training: 0.05467905183504325 | validation: 0.03964416665091461]
	TIME [epoch: 8.3 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05542973919937079		[learning rate: 0.0033318]
		[batch 20/20] avg loss: 0.045420904131394116		[learning rate: 0.0033278]
	Learning Rate: 0.00332781
	LOSS [training: 0.05042532166538245 | validation: 0.09850600219363077]
	TIME [epoch: 8.31 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05349214774277482		[learning rate: 0.0033238]
		[batch 20/20] avg loss: 0.04697433581247575		[learning rate: 0.0033197]
	Learning Rate: 0.00331975
	LOSS [training: 0.050233241777625295 | validation: 0.05760303921045205]
	TIME [epoch: 8.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04760448975449051		[learning rate: 0.0033157]
		[batch 20/20] avg loss: 0.05363030388792763		[learning rate: 0.0033117]
	Learning Rate: 0.00331171
	LOSS [training: 0.05061739682120907 | validation: 0.04639953517980068]
	TIME [epoch: 8.34 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03867689691467417		[learning rate: 0.0033077]
		[batch 20/20] avg loss: 0.045656673744238624		[learning rate: 0.0033037]
	Learning Rate: 0.0033037
	LOSS [training: 0.04216678532945639 | validation: 0.021155454908585954]
	TIME [epoch: 8.31 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03306390353171292		[learning rate: 0.0032997]
		[batch 20/20] avg loss: 0.039561428186572364		[learning rate: 0.0032957]
	Learning Rate: 0.0032957
	LOSS [training: 0.03631266585914265 | validation: 0.14230465428591568]
	TIME [epoch: 8.31 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13767923818786948		[learning rate: 0.0032917]
		[batch 20/20] avg loss: 0.05680554982939915		[learning rate: 0.0032877]
	Learning Rate: 0.00328772
	LOSS [training: 0.0972423940086343 | validation: 0.045331922889915104]
	TIME [epoch: 8.31 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04503390736296235		[learning rate: 0.0032837]
		[batch 20/20] avg loss: 0.04232017747344039		[learning rate: 0.0032798]
	Learning Rate: 0.00327976
	LOSS [training: 0.043677042418201376 | validation: 0.016341516646694518]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_560.pth
	Model improved!!!
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028840665191298197		[learning rate: 0.0032758]
		[batch 20/20] avg loss: 0.04879884686339714		[learning rate: 0.0032718]
	Learning Rate: 0.00327182
	LOSS [training: 0.03881975602734766 | validation: 0.10469010217129293]
	TIME [epoch: 8.32 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05465331326466545		[learning rate: 0.0032679]
		[batch 20/20] avg loss: 0.02717435252910531		[learning rate: 0.0032639]
	Learning Rate: 0.0032639
	LOSS [training: 0.04091383289688539 | validation: 0.06086215638561406]
	TIME [epoch: 8.31 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038603965199176894		[learning rate: 0.0032599]
		[batch 20/20] avg loss: 0.06317087645756794		[learning rate: 0.003256]
	Learning Rate: 0.003256
	LOSS [training: 0.05088742082837242 | validation: 0.07924726519101766]
	TIME [epoch: 8.31 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05649041708352216		[learning rate: 0.0032521]
		[batch 20/20] avg loss: 0.027020557688445766		[learning rate: 0.0032481]
	Learning Rate: 0.00324812
	LOSS [training: 0.04175548738598396 | validation: 0.03278970552433411]
	TIME [epoch: 8.35 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03842702419001682		[learning rate: 0.0032442]
		[batch 20/20] avg loss: 0.040620908898983274		[learning rate: 0.0032403]
	Learning Rate: 0.00324025
	LOSS [training: 0.03952396654450004 | validation: 0.023587820768499773]
	TIME [epoch: 8.32 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025655872315678126		[learning rate: 0.0032363]
		[batch 20/20] avg loss: 0.024610981517933374		[learning rate: 0.0032324]
	Learning Rate: 0.00323241
	LOSS [training: 0.025133426916805746 | validation: 0.02076269529136178]
	TIME [epoch: 8.31 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0424276852584211		[learning rate: 0.0032285]
		[batch 20/20] avg loss: 0.056011988269204414		[learning rate: 0.0032246]
	Learning Rate: 0.00322458
	LOSS [training: 0.04921983676381275 | validation: 0.1071279136734189]
	TIME [epoch: 8.33 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08773134044699425		[learning rate: 0.0032207]
		[batch 20/20] avg loss: 0.032789472663851876		[learning rate: 0.0032168]
	Learning Rate: 0.00321678
	LOSS [training: 0.06026040655542307 | validation: 0.03559200819821008]
	TIME [epoch: 8.36 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034795999490487534		[learning rate: 0.0032129]
		[batch 20/20] avg loss: 0.04961535855114463		[learning rate: 0.003209]
	Learning Rate: 0.00320899
	LOSS [training: 0.042205679020816074 | validation: 0.05595157719804469]
	TIME [epoch: 8.31 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06162541251451187		[learning rate: 0.0032051]
		[batch 20/20] avg loss: 0.060638597266158556		[learning rate: 0.0032012]
	Learning Rate: 0.00320122
	LOSS [training: 0.061132004890335226 | validation: 0.05684975163525291]
	TIME [epoch: 8.31 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049991411163577956		[learning rate: 0.0031973]
		[batch 20/20] avg loss: 0.05733850268630518		[learning rate: 0.0031935]
	Learning Rate: 0.00319347
	LOSS [training: 0.053664956924941574 | validation: 0.04560264350697142]
	TIME [epoch: 8.3 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04923608383896554		[learning rate: 0.0031896]
		[batch 20/20] avg loss: 0.037144838088427994		[learning rate: 0.0031857]
	Learning Rate: 0.00318574
	LOSS [training: 0.04319046096369676 | validation: 0.07908094917409848]
	TIME [epoch: 8.33 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040406336094705196		[learning rate: 0.0031819]
		[batch 20/20] avg loss: 0.03906526177505516		[learning rate: 0.003178]
	Learning Rate: 0.00317803
	LOSS [training: 0.03973579893488018 | validation: 0.0393553997703995]
	TIME [epoch: 8.3 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036642525909010536		[learning rate: 0.0031742]
		[batch 20/20] avg loss: 0.039517156957255724		[learning rate: 0.0031703]
	Learning Rate: 0.00317034
	LOSS [training: 0.03807984143313313 | validation: 0.04619406420517528]
	TIME [epoch: 8.3 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04425084738172252		[learning rate: 0.0031665]
		[batch 20/20] avg loss: 0.06984575534567199		[learning rate: 0.0031627]
	Learning Rate: 0.00316266
	LOSS [training: 0.05704830136369725 | validation: 0.02522888147429113]
	TIME [epoch: 8.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03370614715230747		[learning rate: 0.0031588]
		[batch 20/20] avg loss: 0.05774088902499204		[learning rate: 0.003155]
	Learning Rate: 0.003155
	LOSS [training: 0.04572351808864976 | validation: 0.05801574241749421]
	TIME [epoch: 8.34 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024413080355958974		[learning rate: 0.0031512]
		[batch 20/20] avg loss: 0.04573400748012249		[learning rate: 0.0031474]
	Learning Rate: 0.00314737
	LOSS [training: 0.035073543918040725 | validation: 0.0168101239968128]
	TIME [epoch: 8.33 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018133488805738878		[learning rate: 0.0031436]
		[batch 20/20] avg loss: 0.03547654320549702		[learning rate: 0.0031397]
	Learning Rate: 0.00313975
	LOSS [training: 0.026805016005617948 | validation: 0.023156291787258455]
	TIME [epoch: 8.3 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04809163090204417		[learning rate: 0.0031359]
		[batch 20/20] avg loss: 0.047171857153127064		[learning rate: 0.0031321]
	Learning Rate: 0.00313215
	LOSS [training: 0.047631744027585625 | validation: 0.05251393121052606]
	TIME [epoch: 8.34 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03307011504061547		[learning rate: 0.0031284]
		[batch 20/20] avg loss: 0.05367488973261373		[learning rate: 0.0031246]
	Learning Rate: 0.00312456
	LOSS [training: 0.043372502386614595 | validation: 0.02654733743659119]
	TIME [epoch: 8.35 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04165768373974531		[learning rate: 0.0031208]
		[batch 20/20] avg loss: 0.04748495208383298		[learning rate: 0.003117]
	Learning Rate: 0.003117
	LOSS [training: 0.04457131791178914 | validation: 0.042380112400707125]
	TIME [epoch: 8.31 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04351766277828957		[learning rate: 0.0031132]
		[batch 20/20] avg loss: 0.050533316145810525		[learning rate: 0.0031095]
	Learning Rate: 0.00310945
	LOSS [training: 0.04702548946205004 | validation: 0.1188739993568154]
	TIME [epoch: 8.31 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06126710992828218		[learning rate: 0.0031057]
		[batch 20/20] avg loss: 0.05128228249179604		[learning rate: 0.0031019]
	Learning Rate: 0.00310193
	LOSS [training: 0.05627469621003911 | validation: 0.05248866753946656]
	TIME [epoch: 8.31 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041723800790845186		[learning rate: 0.0030982]
		[batch 20/20] avg loss: 0.04758746992286787		[learning rate: 0.0030944]
	Learning Rate: 0.00309442
	LOSS [training: 0.04465563535685653 | validation: 0.035028990635642486]
	TIME [epoch: 8.33 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046976991899921695		[learning rate: 0.0030907]
		[batch 20/20] avg loss: 0.043298934307422925		[learning rate: 0.0030869]
	Learning Rate: 0.00308693
	LOSS [training: 0.04513796310367231 | validation: 0.07026434802119262]
	TIME [epoch: 8.33 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04834294174380229		[learning rate: 0.0030832]
		[batch 20/20] avg loss: 0.037340880565108044		[learning rate: 0.0030795]
	Learning Rate: 0.00307945
	LOSS [training: 0.04284191115445517 | validation: 0.09145434582281634]
	TIME [epoch: 8.32 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03305318966739426		[learning rate: 0.0030757]
		[batch 20/20] avg loss: 0.02767789796256757		[learning rate: 0.003072]
	Learning Rate: 0.003072
	LOSS [training: 0.030365543814980918 | validation: 0.020904165168378196]
	TIME [epoch: 8.31 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036025047025795794		[learning rate: 0.0030683]
		[batch 20/20] avg loss: 0.049306998162179205		[learning rate: 0.0030646]
	Learning Rate: 0.00306456
	LOSS [training: 0.042666022593987496 | validation: 0.057756020855479956]
	TIME [epoch: 8.38 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07810219194925375		[learning rate: 0.0030609]
		[batch 20/20] avg loss: 0.06553736268542806		[learning rate: 0.0030571]
	Learning Rate: 0.00305714
	LOSS [training: 0.07181977731734092 | validation: 0.07509544706626994]
	TIME [epoch: 8.31 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023626280807465806		[learning rate: 0.0030534]
		[batch 20/20] avg loss: 0.030336185867054877		[learning rate: 0.0030497]
	Learning Rate: 0.00304974
	LOSS [training: 0.02698123333726034 | validation: 0.03231736457439699]
	TIME [epoch: 8.31 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034680008615574257		[learning rate: 0.003046]
		[batch 20/20] avg loss: 0.04686275302100543		[learning rate: 0.0030424]
	Learning Rate: 0.00304236
	LOSS [training: 0.04077138081828984 | validation: 0.05146509593260905]
	TIME [epoch: 8.31 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04258585396797091		[learning rate: 0.0030387]
		[batch 20/20] avg loss: 0.048104902108731265		[learning rate: 0.003035]
	Learning Rate: 0.00303499
	LOSS [training: 0.0453453780383511 | validation: 0.0418957455613219]
	TIME [epoch: 8.35 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05428073142443435		[learning rate: 0.0030313]
		[batch 20/20] avg loss: 0.041115397027611925		[learning rate: 0.0030276]
	Learning Rate: 0.00302765
	LOSS [training: 0.04769806422602313 | validation: 0.07964600512368822]
	TIME [epoch: 8.33 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03426024842158567		[learning rate: 0.003024]
		[batch 20/20] avg loss: 0.0501532234312678		[learning rate: 0.0030203]
	Learning Rate: 0.00302032
	LOSS [training: 0.042206735926426736 | validation: 0.024029585916770235]
	TIME [epoch: 8.32 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028926179534117875		[learning rate: 0.0030167]
		[batch 20/20] avg loss: 0.02487330454484177		[learning rate: 0.003013]
	Learning Rate: 0.00301301
	LOSS [training: 0.026899742039479823 | validation: 0.024656976166194167]
	TIME [epoch: 8.32 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04004783710590469		[learning rate: 0.0030094]
		[batch 20/20] avg loss: 0.06268548964870858		[learning rate: 0.0030057]
	Learning Rate: 0.00300571
	LOSS [training: 0.051366663377306644 | validation: 0.02864823799080867]
	TIME [epoch: 8.38 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028669909337477935		[learning rate: 0.0030021]
		[batch 20/20] avg loss: 0.02356168280722645		[learning rate: 0.0029984]
	Learning Rate: 0.00299844
	LOSS [training: 0.026115796072352187 | validation: 0.017873670622237162]
	TIME [epoch: 8.31 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025425691282429323		[learning rate: 0.0029948]
		[batch 20/20] avg loss: 0.07134691203559815		[learning rate: 0.0029912]
	Learning Rate: 0.00299118
	LOSS [training: 0.04838630165901374 | validation: 0.03482234131600698]
	TIME [epoch: 8.31 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06094113569232962		[learning rate: 0.0029876]
		[batch 20/20] avg loss: 0.04622823856821422		[learning rate: 0.0029839]
	Learning Rate: 0.00298394
	LOSS [training: 0.05358468713027191 | validation: 0.08944903451154491]
	TIME [epoch: 8.31 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02978566583226857		[learning rate: 0.0029803]
		[batch 20/20] avg loss: 0.029911349194439895		[learning rate: 0.0029767]
	Learning Rate: 0.00297671
	LOSS [training: 0.029848507513354237 | validation: 0.019552912854878896]
	TIME [epoch: 8.33 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04277736493156083		[learning rate: 0.0029731]
		[batch 20/20] avg loss: 0.08222004097358063		[learning rate: 0.0029695]
	Learning Rate: 0.00296951
	LOSS [training: 0.06249870295257074 | validation: 0.06280046264827384]
	TIME [epoch: 8.31 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04910009784704395		[learning rate: 0.0029659]
		[batch 20/20] avg loss: 0.0201214247119321		[learning rate: 0.0029623]
	Learning Rate: 0.00296232
	LOSS [training: 0.03461076127948803 | validation: 0.030049155503253253]
	TIME [epoch: 8.32 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0584401692584795		[learning rate: 0.0029587]
		[batch 20/20] avg loss: 0.027743732188092236		[learning rate: 0.0029551]
	Learning Rate: 0.00295515
	LOSS [training: 0.04309195072328588 | validation: 0.07796459104655754]
	TIME [epoch: 8.32 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04046919964498298		[learning rate: 0.0029516]
		[batch 20/20] avg loss: 0.019338966612246815		[learning rate: 0.002948]
	Learning Rate: 0.00294799
	LOSS [training: 0.029904083128614894 | validation: 0.03143143571000102]
	TIME [epoch: 8.34 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03772121980550115		[learning rate: 0.0029444]
		[batch 20/20] avg loss: 0.03066059591486155		[learning rate: 0.0029409]
	Learning Rate: 0.00294086
	LOSS [training: 0.03419090786018134 | validation: 0.015559161006291901]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_605.pth
	Model improved!!!
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030208117831966464		[learning rate: 0.0029373]
		[batch 20/20] avg loss: 0.03121617171782213		[learning rate: 0.0029337]
	Learning Rate: 0.00293374
	LOSS [training: 0.03071214477489429 | validation: 0.06912576092672244]
	TIME [epoch: 8.36 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03703266126400338		[learning rate: 0.0029302]
		[batch 20/20] avg loss: 0.024757228083777956		[learning rate: 0.0029266]
	Learning Rate: 0.00292663
	LOSS [training: 0.03089494467389068 | validation: 0.022922881495523444]
	TIME [epoch: 8.33 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03737557549704735		[learning rate: 0.0029231]
		[batch 20/20] avg loss: 0.05856032032901147		[learning rate: 0.0029195]
	Learning Rate: 0.00291955
	LOSS [training: 0.047967947913029406 | validation: 0.021599860895429528]
	TIME [epoch: 8.35 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029084676180508428		[learning rate: 0.002916]
		[batch 20/20] avg loss: 0.04753816526801237		[learning rate: 0.0029125]
	Learning Rate: 0.00291248
	LOSS [training: 0.0383114207242604 | validation: 0.0415657708140814]
	TIME [epoch: 8.33 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03960132620386125		[learning rate: 0.002909]
		[batch 20/20] avg loss: 0.05303877886803774		[learning rate: 0.0029054]
	Learning Rate: 0.00290543
	LOSS [training: 0.04632005253594951 | validation: 0.051824613040786616]
	TIME [epoch: 8.33 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040709466746049285		[learning rate: 0.0029019]
		[batch 20/20] avg loss: 0.03147705768266078		[learning rate: 0.0028984]
	Learning Rate: 0.0028984
	LOSS [training: 0.03609326221435502 | validation: 0.044680345804363995]
	TIME [epoch: 8.35 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04200022432057709		[learning rate: 0.0028949]
		[batch 20/20] avg loss: 0.04306196006839989		[learning rate: 0.0028914]
	Learning Rate: 0.00289138
	LOSS [training: 0.04253109219448849 | validation: 0.04697308639527428]
	TIME [epoch: 8.37 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04563157181349334		[learning rate: 0.0028879]
		[batch 20/20] avg loss: 0.03346426613241908		[learning rate: 0.0028844]
	Learning Rate: 0.00288438
	LOSS [training: 0.0395479189729562 | validation: 0.06246785895454933]
	TIME [epoch: 8.33 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045730062949953715		[learning rate: 0.0028809]
		[batch 20/20] avg loss: 0.02062767509609215		[learning rate: 0.0028774]
	Learning Rate: 0.0028774
	LOSS [training: 0.03317886902302293 | validation: 0.02349978831201471]
	TIME [epoch: 8.35 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03224583634747332		[learning rate: 0.0028739]
		[batch 20/20] avg loss: 0.03089449856510444		[learning rate: 0.0028704]
	Learning Rate: 0.00287043
	LOSS [training: 0.03157016745628888 | validation: 0.022128415289417867]
	TIME [epoch: 8.35 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04885316381422389		[learning rate: 0.002867]
		[batch 20/20] avg loss: 0.027834404900941433		[learning rate: 0.0028635]
	Learning Rate: 0.00286348
	LOSS [training: 0.03834378435758266 | validation: 0.0777539194020192]
	TIME [epoch: 8.36 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04128698059130702		[learning rate: 0.00286]
		[batch 20/20] avg loss: 0.02925342628699714		[learning rate: 0.0028566]
	Learning Rate: 0.00285655
	LOSS [training: 0.03527020343915208 | validation: 0.022446971816240138]
	TIME [epoch: 8.32 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035740278627016946		[learning rate: 0.0028531]
		[batch 20/20] avg loss: 0.04765964310338296		[learning rate: 0.0028496]
	Learning Rate: 0.00284964
	LOSS [training: 0.04169996086519997 | validation: 0.022841041503323296]
	TIME [epoch: 8.32 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03224948834949635		[learning rate: 0.0028462]
		[batch 20/20] avg loss: 0.04719487006614981		[learning rate: 0.0028427]
	Learning Rate: 0.00284274
	LOSS [training: 0.039722179207823075 | validation: 0.03727286652440555]
	TIME [epoch: 8.33 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07010192841294681		[learning rate: 0.0028393]
		[batch 20/20] avg loss: 0.04718453051889321		[learning rate: 0.0028359]
	Learning Rate: 0.00283586
	LOSS [training: 0.05864322946592 | validation: 0.03962133876753901]
	TIME [epoch: 8.37 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04521588944948267		[learning rate: 0.0028324]
		[batch 20/20] avg loss: 0.03451230561180024		[learning rate: 0.002829]
	Learning Rate: 0.00282899
	LOSS [training: 0.039864097530641456 | validation: 0.03045553677104895]
	TIME [epoch: 8.33 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02879015266232619		[learning rate: 0.0028256]
		[batch 20/20] avg loss: 0.06373644364187182		[learning rate: 0.0028221]
	Learning Rate: 0.00282214
	LOSS [training: 0.046263298152098994 | validation: 0.03807826483937282]
	TIME [epoch: 8.33 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026029290093795045		[learning rate: 0.0028187]
		[batch 20/20] avg loss: 0.030949571539122693		[learning rate: 0.0028153]
	Learning Rate: 0.00281531
	LOSS [training: 0.028489430816458872 | validation: 0.043904403496631716]
	TIME [epoch: 8.37 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043014563864621255		[learning rate: 0.0028119]
		[batch 20/20] avg loss: 0.048365727119838144		[learning rate: 0.0028085]
	Learning Rate: 0.00280849
	LOSS [training: 0.0456901454922297 | validation: 0.04256280592941424]
	TIME [epoch: 8.35 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04225028707222221		[learning rate: 0.0028051]
		[batch 20/20] avg loss: 0.08516463430289584		[learning rate: 0.0028017]
	Learning Rate: 0.0028017
	LOSS [training: 0.06370746068755902 | validation: 0.3500750492337259]
	TIME [epoch: 8.32 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13534770332379734		[learning rate: 0.0027983]
		[batch 20/20] avg loss: 0.05230138866217053		[learning rate: 0.0027949]
	Learning Rate: 0.00279491
	LOSS [training: 0.09382454599298393 | validation: 0.030521598560617508]
	TIME [epoch: 8.33 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04548796007142362		[learning rate: 0.0027915]
		[batch 20/20] avg loss: 0.05301050825070478		[learning rate: 0.0027881]
	Learning Rate: 0.00278815
	LOSS [training: 0.049249234161064206 | validation: 0.02533676149191552]
	TIME [epoch: 8.33 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054453854231919854		[learning rate: 0.0027848]
		[batch 20/20] avg loss: 0.029197516370710906		[learning rate: 0.0027814]
	Learning Rate: 0.0027814
	LOSS [training: 0.04182568530131538 | validation: 0.11555106955092297]
	TIME [epoch: 8.33 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07302662481691596		[learning rate: 0.002778]
		[batch 20/20] avg loss: 0.03075903907735742		[learning rate: 0.0027747]
	Learning Rate: 0.00277466
	LOSS [training: 0.05189283194713669 | validation: 0.08275586064000844]
	TIME [epoch: 8.32 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05248090048861213		[learning rate: 0.0027713]
		[batch 20/20] avg loss: 0.06644075885984706		[learning rate: 0.0027679]
	Learning Rate: 0.00276795
	LOSS [training: 0.059460829674229596 | validation: 0.07328336159866512]
	TIME [epoch: 8.32 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04797095097851274		[learning rate: 0.0027646]
		[batch 20/20] avg loss: 0.018475263518881832		[learning rate: 0.0027612]
	Learning Rate: 0.00276125
	LOSS [training: 0.03322310724869729 | validation: 0.02240721851671528]
	TIME [epoch: 8.35 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04084161042276085		[learning rate: 0.0027579]
		[batch 20/20] avg loss: 0.03979704260414126		[learning rate: 0.0027546]
	Learning Rate: 0.00275456
	LOSS [training: 0.04031932651345106 | validation: 0.04064233173372249]
	TIME [epoch: 8.35 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06598361692611668		[learning rate: 0.0027512]
		[batch 20/20] avg loss: 0.11656433590371593		[learning rate: 0.0027479]
	Learning Rate: 0.00274789
	LOSS [training: 0.0912739764149163 | validation: 0.03163212447081951]
	TIME [epoch: 8.33 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03748317650818707		[learning rate: 0.0027446]
		[batch 20/20] avg loss: 0.03575961528422214		[learning rate: 0.0027412]
	Learning Rate: 0.00274124
	LOSS [training: 0.03662139589620461 | validation: 0.019107267163685996]
	TIME [epoch: 8.33 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04972206377314411		[learning rate: 0.0027379]
		[batch 20/20] avg loss: 0.037174816939204214		[learning rate: 0.0027346]
	Learning Rate: 0.00273461
	LOSS [training: 0.043448440356174164 | validation: 0.029201500314712487]
	TIME [epoch: 8.37 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03433150251951604		[learning rate: 0.0027313]
		[batch 20/20] avg loss: 0.04838019718829599		[learning rate: 0.002728]
	Learning Rate: 0.00272799
	LOSS [training: 0.041355849853906015 | validation: 0.04465891567261277]
	TIME [epoch: 8.34 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04107912783725273		[learning rate: 0.0027247]
		[batch 20/20] avg loss: 0.04753322163156536		[learning rate: 0.0027214]
	Learning Rate: 0.00272138
	LOSS [training: 0.04430617473440905 | validation: 0.0737227448333773]
	TIME [epoch: 8.32 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06508849968395022		[learning rate: 0.0027181]
		[batch 20/20] avg loss: 0.06246565951972214		[learning rate: 0.0027148]
	Learning Rate: 0.00271479
	LOSS [training: 0.06377707960183618 | validation: 0.06245757205896461]
	TIME [epoch: 8.33 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030838682777687604		[learning rate: 0.0027115]
		[batch 20/20] avg loss: 0.05349426363674753		[learning rate: 0.0027082]
	Learning Rate: 0.00270822
	LOSS [training: 0.04216647320721757 | validation: 0.0320222607262955]
	TIME [epoch: 8.35 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03247134313041821		[learning rate: 0.0027049]
		[batch 20/20] avg loss: 0.040464351077806805		[learning rate: 0.0027017]
	Learning Rate: 0.00270167
	LOSS [training: 0.03646784710411251 | validation: 0.04906272171171863]
	TIME [epoch: 8.35 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04130766231242827		[learning rate: 0.0026984]
		[batch 20/20] avg loss: 0.04956865996963799		[learning rate: 0.0026951]
	Learning Rate: 0.00269513
	LOSS [training: 0.045438161141033134 | validation: 0.042924353875315485]
	TIME [epoch: 8.33 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023994690766885228		[learning rate: 0.0026919]
		[batch 20/20] avg loss: 0.042869357408889544		[learning rate: 0.0026886]
	Learning Rate: 0.0026886
	LOSS [training: 0.03343202408788738 | validation: 0.03336820671977243]
	TIME [epoch: 8.33 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02792789010646062		[learning rate: 0.0026853]
		[batch 20/20] avg loss: 0.031997190887798566		[learning rate: 0.0026821]
	Learning Rate: 0.00268209
	LOSS [training: 0.02996254049712959 | validation: 0.017789098059560964]
	TIME [epoch: 8.36 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047850352103638215		[learning rate: 0.0026788]
		[batch 20/20] avg loss: 0.08014566815818816		[learning rate: 0.0026756]
	Learning Rate: 0.0026756
	LOSS [training: 0.06399801013091318 | validation: 0.04830369405240973]
	TIME [epoch: 8.36 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0582656704705246		[learning rate: 0.0026724]
		[batch 20/20] avg loss: 0.0642997976149779		[learning rate: 0.0026691]
	Learning Rate: 0.00266912
	LOSS [training: 0.06128273404275123 | validation: 0.02356116359545525]
	TIME [epoch: 8.33 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018192594023602092		[learning rate: 0.0026659]
		[batch 20/20] avg loss: 0.028370941096842482		[learning rate: 0.0026627]
	Learning Rate: 0.00266266
	LOSS [training: 0.02328176756022229 | validation: 0.018341582374868683]
	TIME [epoch: 8.33 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0269596748961394		[learning rate: 0.0026594]
		[batch 20/20] avg loss: 0.040461290456706295		[learning rate: 0.0026562]
	Learning Rate: 0.00265621
	LOSS [training: 0.033710482676422855 | validation: 0.026567382538431578]
	TIME [epoch: 8.35 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03544331194403688		[learning rate: 0.002653]
		[batch 20/20] avg loss: 0.020991387084328357		[learning rate: 0.0026498]
	Learning Rate: 0.00264978
	LOSS [training: 0.028217349514182623 | validation: 0.039729385893606195]
	TIME [epoch: 8.33 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037186018516138036		[learning rate: 0.0026466]
		[batch 20/20] avg loss: 0.042616627143889786		[learning rate: 0.0026434]
	Learning Rate: 0.00264337
	LOSS [training: 0.039901322830013915 | validation: 0.10446992457596507]
	TIME [epoch: 8.32 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050504681894828464		[learning rate: 0.0026402]
		[batch 20/20] avg loss: 0.08893062909122504		[learning rate: 0.002637]
	Learning Rate: 0.00263697
	LOSS [training: 0.06971765549302675 | validation: 0.05942288011207918]
	TIME [epoch: 8.32 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04851079171987743		[learning rate: 0.0026338]
		[batch 20/20] avg loss: 0.04959306254898403		[learning rate: 0.0026306]
	Learning Rate: 0.00263059
	LOSS [training: 0.049051927134430726 | validation: 0.07594641422761908]
	TIME [epoch: 8.35 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037852570760698756		[learning rate: 0.0026274]
		[batch 20/20] avg loss: 0.028540959919948778		[learning rate: 0.0026242]
	Learning Rate: 0.00262422
	LOSS [training: 0.03319676534032376 | validation: 0.01688980600831958]
	TIME [epoch: 8.36 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05987311250229741		[learning rate: 0.002621]
		[batch 20/20] avg loss: 0.029203730380379		[learning rate: 0.0026179]
	Learning Rate: 0.00261787
	LOSS [training: 0.044538421441338204 | validation: 0.011456798854863707]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_653.pth
	Model improved!!!
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03824446557237111		[learning rate: 0.0026147]
		[batch 20/20] avg loss: 0.02526622292831296		[learning rate: 0.0026115]
	Learning Rate: 0.00261153
	LOSS [training: 0.03175534425034204 | validation: 0.06974266877154359]
	TIME [epoch: 8.33 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05926151438839774		[learning rate: 0.0026084]
		[batch 20/20] avg loss: 0.03412742696808751		[learning rate: 0.0026052]
	Learning Rate: 0.00260521
	LOSS [training: 0.046694470678242624 | validation: 0.025572299301851106]
	TIME [epoch: 8.38 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03077324519932942		[learning rate: 0.0026021]
		[batch 20/20] avg loss: 0.03779817488061271		[learning rate: 0.0025989]
	Learning Rate: 0.0025989
	LOSS [training: 0.034285710039971065 | validation: 0.03189821476521631]
	TIME [epoch: 8.33 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02524415519863073		[learning rate: 0.0025958]
		[batch 20/20] avg loss: 0.04385321767980878		[learning rate: 0.0025926]
	Learning Rate: 0.00259261
	LOSS [training: 0.03454868643921975 | validation: 0.043963705750482265]
	TIME [epoch: 8.32 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041459057029805534		[learning rate: 0.0025895]
		[batch 20/20] avg loss: 0.03390938292488459		[learning rate: 0.0025863]
	Learning Rate: 0.00258633
	LOSS [training: 0.03768421997734506 | validation: 0.057824701081456736]
	TIME [epoch: 8.32 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030304931427042835		[learning rate: 0.0025832]
		[batch 20/20] avg loss: 0.03548460061879109		[learning rate: 0.0025801]
	Learning Rate: 0.00258007
	LOSS [training: 0.03289476602291696 | validation: 0.050948836420649304]
	TIME [epoch: 8.35 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02906512672164443		[learning rate: 0.0025769]
		[batch 20/20] avg loss: 0.035917061293841444		[learning rate: 0.0025738]
	Learning Rate: 0.00257382
	LOSS [training: 0.03249109400774294 | validation: 0.016711997894428957]
	TIME [epoch: 8.35 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02439607418683653		[learning rate: 0.0025707]
		[batch 20/20] avg loss: 0.04575483779413944		[learning rate: 0.0025676]
	Learning Rate: 0.00256759
	LOSS [training: 0.03507545599048799 | validation: 0.02127345531877382]
	TIME [epoch: 8.33 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032202102253034215		[learning rate: 0.0025645]
		[batch 20/20] avg loss: 0.027233636107487996		[learning rate: 0.0025614]
	Learning Rate: 0.00256138
	LOSS [training: 0.02971786918026111 | validation: 0.029575372830327094]
	TIME [epoch: 8.32 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019242812282890535		[learning rate: 0.0025583]
		[batch 20/20] avg loss: 0.0415778932076768		[learning rate: 0.0025552]
	Learning Rate: 0.00255518
	LOSS [training: 0.030410352745283663 | validation: 0.027938831039411378]
	TIME [epoch: 8.37 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044811951176326095		[learning rate: 0.0025521]
		[batch 20/20] avg loss: 0.049095621151689986		[learning rate: 0.002549]
	Learning Rate: 0.00254899
	LOSS [training: 0.04695378616400803 | validation: 0.06083237565692309]
	TIME [epoch: 8.35 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03860011395082212		[learning rate: 0.0025459]
		[batch 20/20] avg loss: 0.021361716619280338		[learning rate: 0.0025428]
	Learning Rate: 0.00254282
	LOSS [training: 0.029980915285051236 | validation: 0.02819449346030388]
	TIME [epoch: 8.32 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03336460121834882		[learning rate: 0.0025397]
		[batch 20/20] avg loss: 0.045352063928811644		[learning rate: 0.0025367]
	Learning Rate: 0.00253667
	LOSS [training: 0.03935833257358022 | validation: 0.0671907142735834]
	TIME [epoch: 8.31 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03409107019653682		[learning rate: 0.0025336]
		[batch 20/20] avg loss: 0.015256286038153086		[learning rate: 0.0025305]
	Learning Rate: 0.00253052
	LOSS [training: 0.02467367811734495 | validation: 0.02179419251473122]
	TIME [epoch: 8.34 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03629807847102758		[learning rate: 0.0025275]
		[batch 20/20] avg loss: 0.05286370394579783		[learning rate: 0.0025244]
	Learning Rate: 0.0025244
	LOSS [training: 0.0445808912084127 | validation: 0.02978643161157067]
	TIME [epoch: 8.35 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04054471276319211		[learning rate: 0.0025213]
		[batch 20/20] avg loss: 0.03885225848986955		[learning rate: 0.0025183]
	Learning Rate: 0.00251829
	LOSS [training: 0.039698485626530826 | validation: 0.02382048398286564]
	TIME [epoch: 8.33 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028142025964134516		[learning rate: 0.0025152]
		[batch 20/20] avg loss: 0.036704806848249893		[learning rate: 0.0025122]
	Learning Rate: 0.00251219
	LOSS [training: 0.03242341640619221 | validation: 0.0286397451184063]
	TIME [epoch: 8.33 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026060509805622505		[learning rate: 0.0025091]
		[batch 20/20] avg loss: 0.0503654937567808		[learning rate: 0.0025061]
	Learning Rate: 0.00250611
	LOSS [training: 0.03821300178120164 | validation: 0.02434940684844486]
	TIME [epoch: 8.36 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05775384492481426		[learning rate: 0.0025031]
		[batch 20/20] avg loss: 0.04082514855944317		[learning rate: 0.0025]
	Learning Rate: 0.00250004
	LOSS [training: 0.04928949674212872 | validation: 0.028458688136706007]
	TIME [epoch: 8.35 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02519575792048649		[learning rate: 0.002497]
		[batch 20/20] avg loss: 0.049682742568679085		[learning rate: 0.002494]
	Learning Rate: 0.00249399
	LOSS [training: 0.037439250244582785 | validation: 0.08651699273912497]
	TIME [epoch: 8.33 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0433480612651876		[learning rate: 0.002491]
		[batch 20/20] avg loss: 0.030540568917175527		[learning rate: 0.002488]
	Learning Rate: 0.00248795
	LOSS [training: 0.03694431509118156 | validation: 0.03344585729574829]
	TIME [epoch: 8.32 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03333205998722893		[learning rate: 0.0024849]
		[batch 20/20] avg loss: 0.03305898796741589		[learning rate: 0.0024819]
	Learning Rate: 0.00248193
	LOSS [training: 0.03319552397732241 | validation: 0.010204319725171485]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04250963295551531		[learning rate: 0.0024789]
		[batch 20/20] avg loss: 0.032165175874132625		[learning rate: 0.0024759]
	Learning Rate: 0.00247592
	LOSS [training: 0.037337404414823964 | validation: 0.04401540437884505]
	TIME [epoch: 8.32 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036304710276162264		[learning rate: 0.0024729]
		[batch 20/20] avg loss: 0.04831600390523463		[learning rate: 0.0024699]
	Learning Rate: 0.00246993
	LOSS [training: 0.04231035709069845 | validation: 0.04895544179374453]
	TIME [epoch: 8.32 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03599148708254504		[learning rate: 0.0024669]
		[batch 20/20] avg loss: 0.03162856039011473		[learning rate: 0.0024639]
	Learning Rate: 0.00246395
	LOSS [training: 0.03381002373632989 | validation: 0.04878093388721339]
	TIME [epoch: 8.33 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03546740897559146		[learning rate: 0.002461]
		[batch 20/20] avg loss: 0.07801174284221402		[learning rate: 0.002458]
	Learning Rate: 0.00245798
	LOSS [training: 0.05673957590890274 | validation: 0.08482215645676058]
	TIME [epoch: 8.34 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07322748346859091		[learning rate: 0.002455]
		[batch 20/20] avg loss: 0.06397521983306939		[learning rate: 0.002452]
	Learning Rate: 0.00245203
	LOSS [training: 0.06860135165083014 | validation: 0.037725212332239445]
	TIME [epoch: 8.32 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054917545960981695		[learning rate: 0.0024491]
		[batch 20/20] avg loss: 0.04080111172244575		[learning rate: 0.0024461]
	Learning Rate: 0.0024461
	LOSS [training: 0.047859328841713726 | validation: 0.020355126145639717]
	TIME [epoch: 8.34 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03351608434624696		[learning rate: 0.0024431]
		[batch 20/20] avg loss: 0.03438211620196242		[learning rate: 0.0024402]
	Learning Rate: 0.00244018
	LOSS [training: 0.03394910027410469 | validation: 0.03703439733354008]
	TIME [epoch: 8.33 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04490389296078977		[learning rate: 0.0024372]
		[batch 20/20] avg loss: 0.07532859927260915		[learning rate: 0.0024343]
	Learning Rate: 0.00243427
	LOSS [training: 0.060116246116699454 | validation: 0.08147528646252658]
	TIME [epoch: 8.33 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056120467035677		[learning rate: 0.0024313]
		[batch 20/20] avg loss: 0.02033613350338381		[learning rate: 0.0024284]
	Learning Rate: 0.00242837
	LOSS [training: 0.03822830026953041 | validation: 0.032123318491156225]
	TIME [epoch: 8.31 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03661458742566314		[learning rate: 0.0024254]
		[batch 20/20] avg loss: 0.052263316207986396		[learning rate: 0.0024225]
	Learning Rate: 0.0024225
	LOSS [training: 0.044438951816824765 | validation: 0.11060247856145461]
	TIME [epoch: 8.31 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07802383374074133		[learning rate: 0.0024196]
		[batch 20/20] avg loss: 0.05456547430927744		[learning rate: 0.0024166]
	Learning Rate: 0.00241663
	LOSS [training: 0.06629465402500936 | validation: 0.042155066142317635]
	TIME [epoch: 8.34 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11179331027937642		[learning rate: 0.0024137]
		[batch 20/20] avg loss: 0.07835697224150576		[learning rate: 0.0024108]
	Learning Rate: 0.00241078
	LOSS [training: 0.0950751412604411 | validation: 0.03098654796564232]
	TIME [epoch: 8.34 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0744019593390058		[learning rate: 0.0024079]
		[batch 20/20] avg loss: 0.06466823102149727		[learning rate: 0.0024049]
	Learning Rate: 0.00240495
	LOSS [training: 0.06953509518025155 | validation: 0.043709710458864265]
	TIME [epoch: 8.33 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08628277411281562		[learning rate: 0.002402]
		[batch 20/20] avg loss: 0.044332000180079545		[learning rate: 0.0023991]
	Learning Rate: 0.00239912
	LOSS [training: 0.06530738714644758 | validation: 0.022683681421003268]
	TIME [epoch: 8.34 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031973995990602286		[learning rate: 0.0023962]
		[batch 20/20] avg loss: 0.023080503605842594		[learning rate: 0.0023933]
	Learning Rate: 0.00239332
	LOSS [training: 0.027527249798222443 | validation: 0.010707230441486762]
	TIME [epoch: 8.32 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04555734291120741		[learning rate: 0.0023904]
		[batch 20/20] avg loss: 0.03751894181991349		[learning rate: 0.0023875]
	Learning Rate: 0.00238752
	LOSS [training: 0.04153814236556046 | validation: 0.03584662293211698]
	TIME [epoch: 8.33 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01884987252591074		[learning rate: 0.0023846]
		[batch 20/20] avg loss: 0.03569697452253476		[learning rate: 0.0023817]
	Learning Rate: 0.00238174
	LOSS [training: 0.027273423524222747 | validation: 0.01768289604281061]
	TIME [epoch: 8.32 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026483788750908066		[learning rate: 0.0023789]
		[batch 20/20] avg loss: 0.02485411562824667		[learning rate: 0.002376]
	Learning Rate: 0.00237598
	LOSS [training: 0.02566895218957737 | validation: 0.08110672113100759]
	TIME [epoch: 8.31 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023912271453537283		[learning rate: 0.0023731]
		[batch 20/20] avg loss: 0.044251558704503764		[learning rate: 0.0023702]
	Learning Rate: 0.00237022
	LOSS [training: 0.03408191507902052 | validation: 0.021820189100577946]
	TIME [epoch: 8.33 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04678640277305578		[learning rate: 0.0023674]
		[batch 20/20] avg loss: 0.03864877121481008		[learning rate: 0.0023645]
	Learning Rate: 0.00236449
	LOSS [training: 0.04271758699393293 | validation: 0.006036014047597276]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_695.pth
	Model improved!!!
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027410551758492434		[learning rate: 0.0023616]
		[batch 20/20] avg loss: 0.04141012075523474		[learning rate: 0.0023588]
	Learning Rate: 0.00235876
	LOSS [training: 0.034410336256863586 | validation: 0.04719026537271316]
	TIME [epoch: 8.31 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028349214935422123		[learning rate: 0.0023559]
		[batch 20/20] avg loss: 0.025907276429781013		[learning rate: 0.0023531]
	Learning Rate: 0.00235305
	LOSS [training: 0.02712824568260157 | validation: 0.009464767242006192]
	TIME [epoch: 8.35 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03446720893607237		[learning rate: 0.0023502]
		[batch 20/20] avg loss: 0.03512687165012947		[learning rate: 0.0023474]
	Learning Rate: 0.00234736
	LOSS [training: 0.03479704029310092 | validation: 0.03203961344372896]
	TIME [epoch: 8.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022653821097491998		[learning rate: 0.0023445]
		[batch 20/20] avg loss: 0.053811461863905154		[learning rate: 0.0023417]
	Learning Rate: 0.00234167
	LOSS [training: 0.03823264148069858 | validation: 0.09579205524373514]
	TIME [epoch: 8.33 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04030295660708423		[learning rate: 0.0023388]
		[batch 20/20] avg loss: 0.05731374394866956		[learning rate: 0.002336]
	Learning Rate: 0.002336
	LOSS [training: 0.0488083502778769 | validation: 0.02671211608425548]
	TIME [epoch: 8.31 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036584694925036514		[learning rate: 0.0023332]
		[batch 20/20] avg loss: 0.036898465769413025		[learning rate: 0.0023303]
	Learning Rate: 0.00233035
	LOSS [training: 0.03674158034722477 | validation: 0.06946288442842666]
	TIME [epoch: 8.31 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044406842333458776		[learning rate: 0.0023275]
		[batch 20/20] avg loss: 0.03139331641833003		[learning rate: 0.0023247]
	Learning Rate: 0.00232471
	LOSS [training: 0.0379000793758944 | validation: 0.06366490661888215]
	TIME [epoch: 8.31 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038197147662485935		[learning rate: 0.0023219]
		[batch 20/20] avg loss: 0.02668851044264512		[learning rate: 0.0023191]
	Learning Rate: 0.00231908
	LOSS [training: 0.03244282905256553 | validation: 0.009802281394122238]
	TIME [epoch: 8.35 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03361359010233367		[learning rate: 0.0023163]
		[batch 20/20] avg loss: 0.04417342704573321		[learning rate: 0.0023135]
	Learning Rate: 0.00231347
	LOSS [training: 0.03889350857403344 | validation: 0.02894357521707383]
	TIME [epoch: 8.31 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025450456028141437		[learning rate: 0.0023107]
		[batch 20/20] avg loss: 0.01843150898255259		[learning rate: 0.0023079]
	Learning Rate: 0.00230787
	LOSS [training: 0.021940982505347015 | validation: 0.020657185699719777]
	TIME [epoch: 8.32 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027280484286055978		[learning rate: 0.0023051]
		[batch 20/20] avg loss: 0.04128304908706337		[learning rate: 0.0023023]
	Learning Rate: 0.00230228
	LOSS [training: 0.03428176668655967 | validation: 0.03474247602827464]
	TIME [epoch: 8.34 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02041533640971821		[learning rate: 0.0022995]
		[batch 20/20] avg loss: 0.03405342112458998		[learning rate: 0.0022967]
	Learning Rate: 0.00229671
	LOSS [training: 0.027234378767154094 | validation: 0.0352947639255166]
	TIME [epoch: 8.33 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02017471339646771		[learning rate: 0.0022939]
		[batch 20/20] avg loss: 0.03567807154363912		[learning rate: 0.0022911]
	Learning Rate: 0.00229115
	LOSS [training: 0.027926392470053418 | validation: 0.02988460094744464]
	TIME [epoch: 8.31 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02950736249039057		[learning rate: 0.0022884]
		[batch 20/20] avg loss: 0.025765440091740692		[learning rate: 0.0022856]
	Learning Rate: 0.0022856
	LOSS [training: 0.02763640129106563 | validation: 0.037690499727144666]
	TIME [epoch: 8.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02013318852796303		[learning rate: 0.0022828]
		[batch 20/20] avg loss: 0.03473549573587177		[learning rate: 0.0022801]
	Learning Rate: 0.00228007
	LOSS [training: 0.0274343421319174 | validation: 0.02572220616413439]
	TIME [epoch: 8.31 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021888717408444155		[learning rate: 0.0022773]
		[batch 20/20] avg loss: 0.03488566024397122		[learning rate: 0.0022745]
	Learning Rate: 0.00227455
	LOSS [training: 0.028387188826207686 | validation: 0.03247041286587841]
	TIME [epoch: 8.35 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03312397661642526		[learning rate: 0.0022718]
		[batch 20/20] avg loss: 0.033177795913245274		[learning rate: 0.002269]
	Learning Rate: 0.00226904
	LOSS [training: 0.03315088626483527 | validation: 0.05667511475867449]
	TIME [epoch: 8.31 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015395717627708843		[learning rate: 0.0022663]
		[batch 20/20] avg loss: 0.02850351328148342		[learning rate: 0.0022635]
	Learning Rate: 0.00226355
	LOSS [training: 0.021949615454596127 | validation: 0.014974421609529154]
	TIME [epoch: 8.31 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021930789321786576		[learning rate: 0.0022608]
		[batch 20/20] avg loss: 0.033040942160002876		[learning rate: 0.0022581]
	Learning Rate: 0.00225807
	LOSS [training: 0.02748586574089474 | validation: 0.01970498933617213]
	TIME [epoch: 8.35 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026159239672048622		[learning rate: 0.0022553]
		[batch 20/20] avg loss: 0.040209607800698044		[learning rate: 0.0022526]
	Learning Rate: 0.0022526
	LOSS [training: 0.033184423736373335 | validation: 0.007809574011707598]
	TIME [epoch: 8.34 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012379645134549101		[learning rate: 0.0022499]
		[batch 20/20] avg loss: 0.025074747374865695		[learning rate: 0.0022471]
	Learning Rate: 0.00224715
	LOSS [training: 0.018727196254707403 | validation: 0.006355053379903974]
	TIME [epoch: 8.31 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02629016104635181		[learning rate: 0.0022444]
		[batch 20/20] avg loss: 0.023440850688967414		[learning rate: 0.0022417]
	Learning Rate: 0.00224171
	LOSS [training: 0.02486550586765961 | validation: 0.016961992566277024]
	TIME [epoch: 8.3 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02837428300624169		[learning rate: 0.002239]
		[batch 20/20] avg loss: 0.026713414371723497		[learning rate: 0.0022363]
	Learning Rate: 0.00223628
	LOSS [training: 0.027543848688982592 | validation: 0.01145890363488774]
	TIME [epoch: 8.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01912125581558579		[learning rate: 0.0022336]
		[batch 20/20] avg loss: 0.02228631249968841		[learning rate: 0.0022309]
	Learning Rate: 0.00223087
	LOSS [training: 0.020703784157637097 | validation: 0.04279777113694904]
	TIME [epoch: 8.35 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021854155232384438		[learning rate: 0.0022282]
		[batch 20/20] avg loss: 0.020607185183276828		[learning rate: 0.0022255]
	Learning Rate: 0.00222547
	LOSS [training: 0.021230670207830628 | validation: 0.013257364152532845]
	TIME [epoch: 8.31 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024146254761249725		[learning rate: 0.0022228]
		[batch 20/20] avg loss: 0.031155124765997706		[learning rate: 0.0022201]
	Learning Rate: 0.00222008
	LOSS [training: 0.02765068976362372 | validation: 0.00982957029525671]
	TIME [epoch: 8.31 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022997801446272937		[learning rate: 0.0022174]
		[batch 20/20] avg loss: 0.021287536174334872		[learning rate: 0.0022147]
	Learning Rate: 0.0022147
	LOSS [training: 0.022142668810303908 | validation: 0.025650410253596913]
	TIME [epoch: 8.34 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018908514489452927		[learning rate: 0.002212]
		[batch 20/20] avg loss: 0.028416979729064235		[learning rate: 0.0022093]
	Learning Rate: 0.00220934
	LOSS [training: 0.023662747109258583 | validation: 0.0376519369264325]
	TIME [epoch: 8.32 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019271786412541472		[learning rate: 0.0022067]
		[batch 20/20] avg loss: 0.022874685030774156		[learning rate: 0.002204]
	Learning Rate: 0.00220399
	LOSS [training: 0.02107323572165782 | validation: 0.05008365508636955]
	TIME [epoch: 8.31 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04313403535861601		[learning rate: 0.0022013]
		[batch 20/20] avg loss: 0.023737253928782103		[learning rate: 0.0021987]
	Learning Rate: 0.00219866
	LOSS [training: 0.03343564464369906 | validation: 0.02303439864776361]
	TIME [epoch: 8.3 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028246216835700454		[learning rate: 0.002196]
		[batch 20/20] avg loss: 0.020917240190339418		[learning rate: 0.0021933]
	Learning Rate: 0.00219334
	LOSS [training: 0.024581728513019933 | validation: 0.04005143080252482]
	TIME [epoch: 8.31 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03676151862247245		[learning rate: 0.0021907]
		[batch 20/20] avg loss: 0.04119931258273709		[learning rate: 0.002188]
	Learning Rate: 0.00218803
	LOSS [training: 0.03898041560260477 | validation: 0.03106583683953691]
	TIME [epoch: 8.34 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03881939382006682		[learning rate: 0.0021854]
		[batch 20/20] avg loss: 0.0249734783053061		[learning rate: 0.0021827]
	Learning Rate: 0.00218273
	LOSS [training: 0.031896436062686453 | validation: 0.03135352344922655]
	TIME [epoch: 8.32 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035417607963491665		[learning rate: 0.0021801]
		[batch 20/20] avg loss: 0.03870629725042206		[learning rate: 0.0021774]
	Learning Rate: 0.00217745
	LOSS [training: 0.037061952606956855 | validation: 0.0571516202439117]
	TIME [epoch: 8.31 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03814031802002181		[learning rate: 0.0021748]
		[batch 20/20] avg loss: 0.030046675496066343		[learning rate: 0.0021722]
	Learning Rate: 0.00217217
	LOSS [training: 0.03409349675804409 | validation: 0.04967843119447818]
	TIME [epoch: 8.33 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028819007951309854		[learning rate: 0.0021695]
		[batch 20/20] avg loss: 0.02825060722518204		[learning rate: 0.0021669]
	Learning Rate: 0.00216692
	LOSS [training: 0.028534807588245954 | validation: 0.02245783744475928]
	TIME [epoch: 8.35 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032356099205865844		[learning rate: 0.0021643]
		[batch 20/20] avg loss: 0.08090647231519503		[learning rate: 0.0021617]
	Learning Rate: 0.00216167
	LOSS [training: 0.056631285760530434 | validation: 0.07084452625826669]
	TIME [epoch: 8.31 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02873573247406524		[learning rate: 0.0021591]
		[batch 20/20] avg loss: 0.058263510213950785		[learning rate: 0.0021564]
	Learning Rate: 0.00215644
	LOSS [training: 0.04349962134400802 | validation: 0.03628611552091804]
	TIME [epoch: 8.3 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056850188882606734		[learning rate: 0.0021538]
		[batch 20/20] avg loss: 0.028680333261247914		[learning rate: 0.0021512]
	Learning Rate: 0.00215122
	LOSS [training: 0.04276526107192733 | validation: 0.036474012145376426]
	TIME [epoch: 8.31 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05907314543830935		[learning rate: 0.0021486]
		[batch 20/20] avg loss: 0.03204217814797888		[learning rate: 0.002146]
	Learning Rate: 0.00214601
	LOSS [training: 0.04555766179314412 | validation: 0.04669000785314895]
	TIME [epoch: 8.35 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05627103121189786		[learning rate: 0.0021434]
		[batch 20/20] avg loss: 0.13133446144452418		[learning rate: 0.0021408]
	Learning Rate: 0.00214081
	LOSS [training: 0.09380274632821102 | validation: 0.08357323459299819]
	TIME [epoch: 8.31 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06100589729133056		[learning rate: 0.0021382]
		[batch 20/20] avg loss: 0.042825596900719666		[learning rate: 0.0021356]
	Learning Rate: 0.00213563
	LOSS [training: 0.05191574709602511 | validation: 0.035649783145973224]
	TIME [epoch: 8.31 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03621906576484544		[learning rate: 0.002133]
		[batch 20/20] avg loss: 0.02481342719073944		[learning rate: 0.0021305]
	Learning Rate: 0.00213046
	LOSS [training: 0.03051624647779244 | validation: 0.025168829013194057]
	TIME [epoch: 8.34 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029160234677960566		[learning rate: 0.0021279]
		[batch 20/20] avg loss: 0.029838553167741484		[learning rate: 0.0021253]
	Learning Rate: 0.0021253
	LOSS [training: 0.02949939392285103 | validation: 0.028059719204937585]
	TIME [epoch: 8.32 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028976697124035018		[learning rate: 0.0021227]
		[batch 20/20] avg loss: 0.04735070796443118		[learning rate: 0.0021202]
	Learning Rate: 0.00212016
	LOSS [training: 0.0381637025442331 | validation: 0.042005772076873116]
	TIME [epoch: 8.3 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035822816509455034		[learning rate: 0.0021176]
		[batch 20/20] avg loss: 0.035915021499742864		[learning rate: 0.002115]
	Learning Rate: 0.00211503
	LOSS [training: 0.03586891900459895 | validation: 0.02341624438971155]
	TIME [epoch: 8.31 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023302411348691827		[learning rate: 0.0021125]
		[batch 20/20] avg loss: 0.028799233745230503		[learning rate: 0.0021099]
	Learning Rate: 0.00210991
	LOSS [training: 0.026050822546961165 | validation: 0.07255209760673353]
	TIME [epoch: 8.3 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03935406269706264		[learning rate: 0.0021074]
		[batch 20/20] avg loss: 0.026374172346195816		[learning rate: 0.0021048]
	Learning Rate: 0.0021048
	LOSS [training: 0.032864117521629224 | validation: 0.032562007733279906]
	TIME [epoch: 8.35 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014727336951261893		[learning rate: 0.0021022]
		[batch 20/20] avg loss: 0.022338049835721144		[learning rate: 0.0020997]
	Learning Rate: 0.0020997
	LOSS [training: 0.01853269339349152 | validation: 0.019822689816263696]
	TIME [epoch: 8.31 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02338272460419361		[learning rate: 0.0020972]
		[batch 20/20] avg loss: 0.039768361204274676		[learning rate: 0.0020946]
	Learning Rate: 0.00209462
	LOSS [training: 0.031575542904234144 | validation: 0.022175265484442732]
	TIME [epoch: 8.31 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023346003362657082		[learning rate: 0.0020921]
		[batch 20/20] avg loss: 0.01747092996327846		[learning rate: 0.0020895]
	Learning Rate: 0.00208955
	LOSS [training: 0.02040846666296777 | validation: 0.017225556315651695]
	TIME [epoch: 8.33 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02032649839630471		[learning rate: 0.002087]
		[batch 20/20] avg loss: 0.024475262440278246		[learning rate: 0.0020845]
	Learning Rate: 0.00208449
	LOSS [training: 0.022400880418291476 | validation: 0.04320321811326029]
	TIME [epoch: 8.35 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026324286244589885		[learning rate: 0.002082]
		[batch 20/20] avg loss: 0.01606531872171294		[learning rate: 0.0020794]
	Learning Rate: 0.00207944
	LOSS [training: 0.021194802483151413 | validation: 0.0022148738543289526]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030527971571965073		[learning rate: 0.0020769]
		[batch 20/20] avg loss: 0.031606859419792174		[learning rate: 0.0020744]
	Learning Rate: 0.00207441
	LOSS [training: 0.03106741549587862 | validation: 0.02975359385779483]
	TIME [epoch: 8.31 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06565222279041191		[learning rate: 0.0020719]
		[batch 20/20] avg loss: 0.016698754852240892		[learning rate: 0.0020694]
	Learning Rate: 0.00206939
	LOSS [training: 0.0411754888213264 | validation: 0.04141902855041403]
	TIME [epoch: 8.31 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027374275089364526		[learning rate: 0.0020669]
		[batch 20/20] avg loss: 0.04453701956812933		[learning rate: 0.0020644]
	Learning Rate: 0.00206438
	LOSS [training: 0.03595564732874693 | validation: 0.022622470597299126]
	TIME [epoch: 8.36 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04176950401873618		[learning rate: 0.0020619]
		[batch 20/20] avg loss: 0.02352436073833936		[learning rate: 0.0020594]
	Learning Rate: 0.00205938
	LOSS [training: 0.03264693237853776 | validation: 0.016863620659602816]
	TIME [epoch: 8.31 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021195136252116182		[learning rate: 0.0020569]
		[batch 20/20] avg loss: 0.021735945060909194		[learning rate: 0.0020544]
	Learning Rate: 0.0020544
	LOSS [training: 0.02146554065651269 | validation: 0.010290956749425812]
	TIME [epoch: 8.31 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017450610349051463		[learning rate: 0.0020519]
		[batch 20/20] avg loss: 0.05241726044918019		[learning rate: 0.0020494]
	Learning Rate: 0.00204942
	LOSS [training: 0.03493393539911582 | validation: 0.0654540559023131]
	TIME [epoch: 8.34 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028043143442165862		[learning rate: 0.0020469]
		[batch 20/20] avg loss: 0.03658999034497706		[learning rate: 0.0020445]
	Learning Rate: 0.00204446
	LOSS [training: 0.032316566893571465 | validation: 0.04023403416663998]
	TIME [epoch: 8.33 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04665278645678165		[learning rate: 0.002042]
		[batch 20/20] avg loss: 0.03192325402154052		[learning rate: 0.0020395]
	Learning Rate: 0.00203951
	LOSS [training: 0.03928802023916109 | validation: 0.026597904998343684]
	TIME [epoch: 8.31 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025850198963185307		[learning rate: 0.002037]
		[batch 20/20] avg loss: 0.04388752622439934		[learning rate: 0.0020346]
	Learning Rate: 0.00203457
	LOSS [training: 0.03486886259379232 | validation: 0.019820913487596462]
	TIME [epoch: 8.31 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02812165174335641		[learning rate: 0.0020321]
		[batch 20/20] avg loss: 0.025003814393309575		[learning rate: 0.0020296]
	Learning Rate: 0.00202965
	LOSS [training: 0.02656273306833299 | validation: 0.020527827141864102]
	TIME [epoch: 8.31 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036408475687456254		[learning rate: 0.0020272]
		[batch 20/20] avg loss: 0.06823242167249949		[learning rate: 0.0020247]
	Learning Rate: 0.00202474
	LOSS [training: 0.05232044867997787 | validation: 0.017345697406488225]
	TIME [epoch: 8.33 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014233202514812035		[learning rate: 0.0020223]
		[batch 20/20] avg loss: 0.017600023942526098		[learning rate: 0.0020198]
	Learning Rate: 0.00201983
	LOSS [training: 0.015916613228669066 | validation: 0.03433815078668444]
	TIME [epoch: 8.31 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032335559246444996		[learning rate: 0.0020174]
		[batch 20/20] avg loss: 0.02395509755790285		[learning rate: 0.0020149]
	Learning Rate: 0.00201494
	LOSS [training: 0.028145328402173923 | validation: 0.04264619267516034]
	TIME [epoch: 8.32 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021392769492836944		[learning rate: 0.0020125]
		[batch 20/20] avg loss: 0.02459863891979553		[learning rate: 0.0020101]
	Learning Rate: 0.00201007
	LOSS [training: 0.022995704206316237 | validation: 0.040144701292334245]
	TIME [epoch: 8.32 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03534484865134972		[learning rate: 0.0020076]
		[batch 20/20] avg loss: 0.022873095944721138		[learning rate: 0.0020052]
	Learning Rate: 0.0020052
	LOSS [training: 0.02910897229803543 | validation: 0.03447704506187199]
	TIME [epoch: 8.34 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03543002621402992		[learning rate: 0.0020028]
		[batch 20/20] avg loss: 0.021675767065899527		[learning rate: 0.0020003]
	Learning Rate: 0.00200035
	LOSS [training: 0.02855289663996473 | validation: 0.02283334906482465]
	TIME [epoch: 8.31 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020412953791909685		[learning rate: 0.0019979]
		[batch 20/20] avg loss: 0.033055402815769075		[learning rate: 0.0019955]
	Learning Rate: 0.0019955
	LOSS [training: 0.026734178303839378 | validation: 0.018988031172809137]
	TIME [epoch: 8.34 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016739545225291644		[learning rate: 0.0019931]
		[batch 20/20] avg loss: 0.0261498299941866		[learning rate: 0.0019907]
	Learning Rate: 0.00199067
	LOSS [training: 0.021444687609739117 | validation: 0.021560481187496018]
	TIME [epoch: 8.31 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01895330404692671		[learning rate: 0.0019883]
		[batch 20/20] avg loss: 0.03211533891837985		[learning rate: 0.0019859]
	Learning Rate: 0.00198585
	LOSS [training: 0.025534321482653284 | validation: 0.022764808880058093]
	TIME [epoch: 8.33 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03389817738396546		[learning rate: 0.0019834]
		[batch 20/20] avg loss: 0.029864909042923448		[learning rate: 0.001981]
	Learning Rate: 0.00198105
	LOSS [training: 0.031881543213444455 | validation: 0.023275571218137683]
	TIME [epoch: 8.31 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025640562614071827		[learning rate: 0.0019786]
		[batch 20/20] avg loss: 0.03493490210261502		[learning rate: 0.0019763]
	Learning Rate: 0.00197625
	LOSS [training: 0.030287732358343415 | validation: 0.029050724548983477]
	TIME [epoch: 8.31 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023240667058131624		[learning rate: 0.0019739]
		[batch 20/20] avg loss: 0.012709352940395158		[learning rate: 0.0019715]
	Learning Rate: 0.00197147
	LOSS [training: 0.017975009999263387 | validation: 0.013014511111036728]
	TIME [epoch: 8.31 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020762296905820106		[learning rate: 0.0019691]
		[batch 20/20] avg loss: 0.012352372940625942		[learning rate: 0.0019667]
	Learning Rate: 0.00196669
	LOSS [training: 0.016557334923223026 | validation: 0.011431358140002429]
	TIME [epoch: 8.36 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02336521584757476		[learning rate: 0.0019643]
		[batch 20/20] avg loss: 0.029489167898006374		[learning rate: 0.0019619]
	Learning Rate: 0.00196193
	LOSS [training: 0.026427191872790568 | validation: 0.019904450236588822]
	TIME [epoch: 8.32 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010919431087362536		[learning rate: 0.0019596]
		[batch 20/20] avg loss: 0.012014371182330544		[learning rate: 0.0019572]
	Learning Rate: 0.00195718
	LOSS [training: 0.01146690113484654 | validation: 0.02125126091334047]
	TIME [epoch: 8.32 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025532767723201975		[learning rate: 0.0019548]
		[batch 20/20] avg loss: 0.023738090396641744		[learning rate: 0.0019524]
	Learning Rate: 0.00195245
	LOSS [training: 0.024635429059921856 | validation: 0.02404386393852601]
	TIME [epoch: 8.34 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029885276173181596		[learning rate: 0.0019501]
		[batch 20/20] avg loss: 0.03808261487016819		[learning rate: 0.0019477]
	Learning Rate: 0.00194772
	LOSS [training: 0.03398394552167489 | validation: 0.039794426061391865]
	TIME [epoch: 8.33 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03654837901582843		[learning rate: 0.0019454]
		[batch 20/20] avg loss: 0.03657484210886527		[learning rate: 0.001943]
	Learning Rate: 0.001943
	LOSS [training: 0.036561610562346844 | validation: 0.07771545469827598]
	TIME [epoch: 8.3 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058574815602042085		[learning rate: 0.0019407]
		[batch 20/20] avg loss: 0.052110277457004785		[learning rate: 0.0019383]
	Learning Rate: 0.0019383
	LOSS [training: 0.05534254652952344 | validation: 0.025486763590954503]
	TIME [epoch: 8.31 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028366509335585856		[learning rate: 0.001936]
		[batch 20/20] avg loss: 0.0375404490593914		[learning rate: 0.0019336]
	Learning Rate: 0.00193361
	LOSS [training: 0.03295347919748863 | validation: 0.04445266494161603]
	TIME [epoch: 8.31 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03631197205563083		[learning rate: 0.0019313]
		[batch 20/20] avg loss: 0.03154855647652242		[learning rate: 0.0019289]
	Learning Rate: 0.00192893
	LOSS [training: 0.03393026426607663 | validation: 0.06800743060018756]
	TIME [epoch: 8.32 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057755323895571434		[learning rate: 0.0019266]
		[batch 20/20] avg loss: 0.06027574093326169		[learning rate: 0.0019243]
	Learning Rate: 0.00192426
	LOSS [training: 0.05901553241441656 | validation: 0.07579367466523496]
	TIME [epoch: 8.33 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05016977414796823		[learning rate: 0.0019219]
		[batch 20/20] avg loss: 0.035261929199961815		[learning rate: 0.0019196]
	Learning Rate: 0.0019196
	LOSS [training: 0.04271585167396502 | validation: 0.010401246215201712]
	TIME [epoch: 8.31 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024344977807432527		[learning rate: 0.0019173]
		[batch 20/20] avg loss: 0.015709748159035086		[learning rate: 0.001915]
	Learning Rate: 0.00191495
	LOSS [training: 0.020027362983233807 | validation: 0.01884852103714486]
	TIME [epoch: 8.31 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0363198939916023		[learning rate: 0.0019126]
		[batch 20/20] avg loss: 0.025220496614526113		[learning rate: 0.0019103]
	Learning Rate: 0.00191032
	LOSS [training: 0.030770195303064206 | validation: 0.02671165165129594]
	TIME [epoch: 8.34 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02370098777399848		[learning rate: 0.001908]
		[batch 20/20] avg loss: 0.02548102503031131		[learning rate: 0.0019057]
	Learning Rate: 0.00190569
	LOSS [training: 0.024591006402154898 | validation: 0.03425051347997161]
	TIME [epoch: 8.33 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03477235388896942		[learning rate: 0.0019034]
		[batch 20/20] avg loss: 0.02420939414828336		[learning rate: 0.0019011]
	Learning Rate: 0.00190108
	LOSS [training: 0.02949087401862639 | validation: 0.026170380408669668]
	TIME [epoch: 8.3 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021851326203475373		[learning rate: 0.0018988]
		[batch 20/20] avg loss: 0.02023606398169864		[learning rate: 0.0018965]
	Learning Rate: 0.00189648
	LOSS [training: 0.021043695092587012 | validation: 0.021666649162557557]
	TIME [epoch: 8.3 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028046463665666887		[learning rate: 0.0018942]
		[batch 20/20] avg loss: 0.019286607403592594		[learning rate: 0.0018919]
	Learning Rate: 0.00189188
	LOSS [training: 0.02366653553462974 | validation: 0.00918227865612059]
	TIME [epoch: 8.32 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023216919607958846		[learning rate: 0.0018896]
		[batch 20/20] avg loss: 0.020191491466126085		[learning rate: 0.0018873]
	Learning Rate: 0.00188731
	LOSS [training: 0.021704205537042463 | validation: 0.02887235705450103]
	TIME [epoch: 8.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017217093247795225		[learning rate: 0.001885]
		[batch 20/20] avg loss: 0.03155480089319815		[learning rate: 0.0018827]
	Learning Rate: 0.00188274
	LOSS [training: 0.024385947070496686 | validation: 0.0373306586963946]
	TIME [epoch: 8.33 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023145849405766417		[learning rate: 0.0018805]
		[batch 20/20] avg loss: 0.025545102990072392		[learning rate: 0.0018782]
	Learning Rate: 0.00187818
	LOSS [training: 0.024345476197919404 | validation: 0.05297168038774494]
	TIME [epoch: 8.31 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01914292154300683		[learning rate: 0.0018759]
		[batch 20/20] avg loss: 0.01907174709272523		[learning rate: 0.0018736]
	Learning Rate: 0.00187363
	LOSS [training: 0.019107334317866032 | validation: 0.027637393899356252]
	TIME [epoch: 8.33 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03314359334194891		[learning rate: 0.0018714]
		[batch 20/20] avg loss: 0.02691511163453384		[learning rate: 0.0018691]
	Learning Rate: 0.0018691
	LOSS [training: 0.03002935248824138 | validation: 0.03453203346902544]
	TIME [epoch: 8.33 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05317143910007534		[learning rate: 0.0018668]
		[batch 20/20] avg loss: 0.028180169202584072		[learning rate: 0.0018646]
	Learning Rate: 0.00186457
	LOSS [training: 0.04067580415132971 | validation: 0.03775385421432108]
	TIME [epoch: 8.31 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033460488449446484		[learning rate: 0.0018623]
		[batch 20/20] avg loss: 0.03973100170004256		[learning rate: 0.0018601]
	Learning Rate: 0.00186006
	LOSS [training: 0.036595745074744536 | validation: 0.02042071693830684]
	TIME [epoch: 8.31 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02239277134606138		[learning rate: 0.0018578]
		[batch 20/20] avg loss: 0.04322669485722139		[learning rate: 0.0018556]
	Learning Rate: 0.00185555
	LOSS [training: 0.03280973310164138 | validation: 0.020856894610317]
	TIME [epoch: 8.33 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030728972552639105		[learning rate: 0.0018533]
		[batch 20/20] avg loss: 0.015758225947651187		[learning rate: 0.0018511]
	Learning Rate: 0.00185106
	LOSS [training: 0.023243599250145148 | validation: 0.020107616306552573]
	TIME [epoch: 8.31 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023952493707737516		[learning rate: 0.0018488]
		[batch 20/20] avg loss: 0.029853175794994108		[learning rate: 0.0018466]
	Learning Rate: 0.00184658
	LOSS [training: 0.02690283475136581 | validation: 0.015930175665778937]
	TIME [epoch: 8.31 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02304196939991724		[learning rate: 0.0018443]
		[batch 20/20] avg loss: 0.017108931943041396		[learning rate: 0.0018421]
	Learning Rate: 0.00184211
	LOSS [training: 0.020075450671479318 | validation: 0.01843945957846839]
	TIME [epoch: 8.32 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019130550760500416		[learning rate: 0.0018399]
		[batch 20/20] avg loss: 0.024314369742543623		[learning rate: 0.0018377]
	Learning Rate: 0.00183765
	LOSS [training: 0.021722460251522017 | validation: 0.02836341010910371]
	TIME [epoch: 8.33 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026496760114389856		[learning rate: 0.0018354]
		[batch 20/20] avg loss: 0.02160277696231037		[learning rate: 0.0018332]
	Learning Rate: 0.0018332
	LOSS [training: 0.024049768538350107 | validation: 0.0699575285369276]
	TIME [epoch: 8.31 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024485011146513355		[learning rate: 0.001831]
		[batch 20/20] avg loss: 0.02868965707268272		[learning rate: 0.0018288]
	Learning Rate: 0.00182876
	LOSS [training: 0.026587334109598043 | validation: 0.02915320815457072]
	TIME [epoch: 8.34 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029183462677410333		[learning rate: 0.0018265]
		[batch 20/20] avg loss: 0.06119874404283508		[learning rate: 0.0018243]
	Learning Rate: 0.00182434
	LOSS [training: 0.045191103360122706 | validation: 0.12251642944891575]
	TIME [epoch: 8.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054479596103150486		[learning rate: 0.0018221]
		[batch 20/20] avg loss: 0.05359619641744652		[learning rate: 0.0018199]
	Learning Rate: 0.00181992
	LOSS [training: 0.0540378962602985 | validation: 0.056148608796564206]
	TIME [epoch: 8.33 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04245387381820614		[learning rate: 0.0018177]
		[batch 20/20] avg loss: 0.0531134882144783		[learning rate: 0.0018155]
	Learning Rate: 0.00181552
	LOSS [training: 0.047783681016342214 | validation: 0.020255414692089822]
	TIME [epoch: 8.31 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030036965739737263		[learning rate: 0.0018133]
		[batch 20/20] avg loss: 0.03340062823773762		[learning rate: 0.0018111]
	Learning Rate: 0.00181112
	LOSS [training: 0.03171879698873745 | validation: 0.0417110574937754]
	TIME [epoch: 8.31 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046690831677125996		[learning rate: 0.0018089]
		[batch 20/20] avg loss: 0.05868352224409634		[learning rate: 0.0018067]
	Learning Rate: 0.00180674
	LOSS [training: 0.05268717696061117 | validation: 0.05684233842372004]
	TIME [epoch: 8.32 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06542292029039155		[learning rate: 0.0018045]
		[batch 20/20] avg loss: 0.09287888969928211		[learning rate: 0.0018024]
	Learning Rate: 0.00180236
	LOSS [training: 0.07915090499483685 | validation: 0.07671378113991066]
	TIME [epoch: 8.33 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08020281504184759		[learning rate: 0.0018002]
		[batch 20/20] avg loss: 0.06144560984281544		[learning rate: 0.001798]
	Learning Rate: 0.001798
	LOSS [training: 0.07082421244233152 | validation: 0.05582555761913417]
	TIME [epoch: 8.31 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05485774391398832		[learning rate: 0.0017958]
		[batch 20/20] avg loss: 0.02848045402877474		[learning rate: 0.0017936]
	Learning Rate: 0.00179365
	LOSS [training: 0.04166909897138153 | validation: 0.026484845289564976]
	TIME [epoch: 8.34 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03509024878309595		[learning rate: 0.0017915]
		[batch 20/20] avg loss: 0.016591431180007835		[learning rate: 0.0017893]
	Learning Rate: 0.0017893
	LOSS [training: 0.02584083998155189 | validation: 0.024891402121902974]
	TIME [epoch: 8.31 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018300371834093544		[learning rate: 0.0017871]
		[batch 20/20] avg loss: 0.020277275731587142		[learning rate: 0.001785]
	Learning Rate: 0.00178497
	LOSS [training: 0.019288823782840345 | validation: 0.030234808021384535]
	TIME [epoch: 8.32 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025605286352074895		[learning rate: 0.0017828]
		[batch 20/20] avg loss: 0.018557016274247856		[learning rate: 0.0017807]
	Learning Rate: 0.00178065
	LOSS [training: 0.022081151313161376 | validation: 0.024721922392506684]
	TIME [epoch: 8.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03506803920309619		[learning rate: 0.0017785]
		[batch 20/20] avg loss: 0.014522940681775398		[learning rate: 0.0017763]
	Learning Rate: 0.00177634
	LOSS [training: 0.024795489942435796 | validation: 0.01940047938453377]
	TIME [epoch: 8.31 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01668386915040219		[learning rate: 0.0017742]
		[batch 20/20] avg loss: 0.00928409939813071		[learning rate: 0.001772]
	Learning Rate: 0.00177204
	LOSS [training: 0.012983984274266452 | validation: 0.024044545063581065]
	TIME [epoch: 8.34 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03699434910552044		[learning rate: 0.0017699]
		[batch 20/20] avg loss: 0.04058353963502684		[learning rate: 0.0017678]
	Learning Rate: 0.00176775
	LOSS [training: 0.038788944370273644 | validation: 0.04093055920055194]
	TIME [epoch: 8.33 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03554931638670136		[learning rate: 0.0017656]
		[batch 20/20] avg loss: 0.026917167766198197		[learning rate: 0.0017635]
	Learning Rate: 0.00176347
	LOSS [training: 0.031233242076449768 | validation: 0.052939237980310166]
	TIME [epoch: 8.32 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032590115404474596		[learning rate: 0.0017613]
		[batch 20/20] avg loss: 0.02622928739725921		[learning rate: 0.0017592]
	Learning Rate: 0.0017592
	LOSS [training: 0.029409701400866912 | validation: 0.020877139623570375]
	TIME [epoch: 8.31 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026148918499080038		[learning rate: 0.0017571]
		[batch 20/20] avg loss: 0.03766563284932737		[learning rate: 0.0017549]
	Learning Rate: 0.00175494
	LOSS [training: 0.0319072756742037 | validation: 0.02392537588474588]
	TIME [epoch: 8.36 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029570733755680773		[learning rate: 0.0017528]
		[batch 20/20] avg loss: 0.03627555479654416		[learning rate: 0.0017507]
	Learning Rate: 0.0017507
	LOSS [training: 0.03292314427611247 | validation: 0.040205282490519]
	TIME [epoch: 8.32 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03757678013836663		[learning rate: 0.0017486]
		[batch 20/20] avg loss: 0.034358965559829854		[learning rate: 0.0017465]
	Learning Rate: 0.00174646
	LOSS [training: 0.03596787284909824 | validation: 0.03034529840570555]
	TIME [epoch: 8.31 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016738231840693597		[learning rate: 0.0017443]
		[batch 20/20] avg loss: 0.034650655566266786		[learning rate: 0.0017422]
	Learning Rate: 0.00174223
	LOSS [training: 0.02569444370348019 | validation: 0.033561330545727]
	TIME [epoch: 8.31 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01903015688286449		[learning rate: 0.0017401]
		[batch 20/20] avg loss: 0.014501662190700069		[learning rate: 0.001738]
	Learning Rate: 0.00173801
	LOSS [training: 0.016765909536782277 | validation: 0.016815584001769218]
	TIME [epoch: 8.34 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021415005915834093		[learning rate: 0.0017359]
		[batch 20/20] avg loss: 0.019858331210515417		[learning rate: 0.0017338]
	Learning Rate: 0.0017338
	LOSS [training: 0.02063666856317476 | validation: 0.018095256393834216]
	TIME [epoch: 8.32 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025540660532053238		[learning rate: 0.0017317]
		[batch 20/20] avg loss: 0.044922819261007045		[learning rate: 0.0017296]
	Learning Rate: 0.00172961
	LOSS [training: 0.03523173989653015 | validation: 0.023714250676954346]
	TIME [epoch: 8.31 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008451941980909951		[learning rate: 0.0017275]
		[batch 20/20] avg loss: 0.031978949441310994		[learning rate: 0.0017254]
	Learning Rate: 0.00172542
	LOSS [training: 0.020215445711110468 | validation: 0.06866698289946303]
	TIME [epoch: 8.32 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041112000909876975		[learning rate: 0.0017233]
		[batch 20/20] avg loss: 0.031333963416105955		[learning rate: 0.0017212]
	Learning Rate: 0.00172124
	LOSS [training: 0.03622298216299146 | validation: 0.026270447280276124]
	TIME [epoch: 8.35 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020660226111782254		[learning rate: 0.0017192]
		[batch 20/20] avg loss: 0.019029458264028444		[learning rate: 0.0017171]
	Learning Rate: 0.00171708
	LOSS [training: 0.019844842187905352 | validation: 0.017714823466870602]
	TIME [epoch: 8.32 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02728304902215318		[learning rate: 0.001715]
		[batch 20/20] avg loss: 0.05216758777323684		[learning rate: 0.0017129]
	Learning Rate: 0.00171292
	LOSS [training: 0.03972531839769501 | validation: 0.016007702339478697]
	TIME [epoch: 8.31 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01855757482193394		[learning rate: 0.0017108]
		[batch 20/20] avg loss: 0.046142445955363834		[learning rate: 0.0017088]
	Learning Rate: 0.00170877
	LOSS [training: 0.03235001038864889 | validation: 0.0359858075617964]
	TIME [epoch: 8.31 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022584113978986204		[learning rate: 0.0017067]
		[batch 20/20] avg loss: 0.03397208344551465		[learning rate: 0.0017046]
	Learning Rate: 0.00170464
	LOSS [training: 0.028278098712250427 | validation: 0.021997794685435536]
	TIME [epoch: 8.33 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04176604404458904		[learning rate: 0.0017026]
		[batch 20/20] avg loss: 0.025290225195797756		[learning rate: 0.0017005]
	Learning Rate: 0.00170051
	LOSS [training: 0.0335281346201934 | validation: 0.0244004154350371]
	TIME [epoch: 8.34 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03216896865947787		[learning rate: 0.0016984]
		[batch 20/20] avg loss: 0.02924288332221354		[learning rate: 0.0016964]
	Learning Rate: 0.00169639
	LOSS [training: 0.0307059259908457 | validation: 0.1140890398263445]
	TIME [epoch: 8.32 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03596055017736003		[learning rate: 0.0016943]
		[batch 20/20] avg loss: 0.049499771962491224		[learning rate: 0.0016923]
	Learning Rate: 0.00169229
	LOSS [training: 0.04273016106992562 | validation: 0.1266171471905818]
	TIME [epoch: 8.31 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0757532918091289		[learning rate: 0.0016902]
		[batch 20/20] avg loss: 0.055310758204032084		[learning rate: 0.0016882]
	Learning Rate: 0.00168819
	LOSS [training: 0.06553202500658051 | validation: 0.10836794459142383]
	TIME [epoch: 8.36 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08216402319615905		[learning rate: 0.0016861]
		[batch 20/20] avg loss: 0.05643886596867407		[learning rate: 0.0016841]
	Learning Rate: 0.0016841
	LOSS [training: 0.06930144458241655 | validation: 0.05013479938284611]
	TIME [epoch: 8.31 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036339282465318444		[learning rate: 0.0016821]
		[batch 20/20] avg loss: 0.024883273786364263		[learning rate: 0.00168]
	Learning Rate: 0.00168003
	LOSS [training: 0.03061127812584135 | validation: 0.03141049397336172]
	TIME [epoch: 8.31 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03590255551902358		[learning rate: 0.001678]
		[batch 20/20] avg loss: 0.051203706702056095		[learning rate: 0.001676]
	Learning Rate: 0.00167596
	LOSS [training: 0.043553131110539836 | validation: 0.024706288505267283]
	TIME [epoch: 8.31 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03133609826443684		[learning rate: 0.0016739]
		[batch 20/20] avg loss: 0.026292479079653326		[learning rate: 0.0016719]
	Learning Rate: 0.0016719
	LOSS [training: 0.028814288672045087 | validation: 0.02523320843571676]
	TIME [epoch: 8.33 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024890534423067684		[learning rate: 0.0016699]
		[batch 20/20] avg loss: 0.015380845009642818		[learning rate: 0.0016679]
	Learning Rate: 0.00166785
	LOSS [training: 0.020135689716355255 | validation: 0.019936200855077274]
	TIME [epoch: 8.31 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017024328985342056		[learning rate: 0.0016658]
		[batch 20/20] avg loss: 0.00728933757263629		[learning rate: 0.0016638]
	Learning Rate: 0.00166382
	LOSS [training: 0.012156833278989173 | validation: 0.011501397985107851]
	TIME [epoch: 8.33 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00870794377549641		[learning rate: 0.0016618]
		[batch 20/20] avg loss: 0.022539216260250326		[learning rate: 0.0016598]
	Learning Rate: 0.00165979
	LOSS [training: 0.015623580017873367 | validation: 0.038416312416571355]
	TIME [epoch: 8.32 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022826779769003297		[learning rate: 0.0016578]
		[batch 20/20] avg loss: 0.008637950253994645		[learning rate: 0.0016558]
	Learning Rate: 0.00165577
	LOSS [training: 0.01573236501149897 | validation: 0.010182297619376213]
	TIME [epoch: 8.34 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01684664033228305		[learning rate: 0.0016538]
		[batch 20/20] avg loss: 0.02401387887917309		[learning rate: 0.0016518]
	Learning Rate: 0.00165176
	LOSS [training: 0.02043025960572807 | validation: 0.0035434454912701987]
	TIME [epoch: 8.32 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018959769573060472		[learning rate: 0.0016498]
		[batch 20/20] avg loss: 0.02479464458760537		[learning rate: 0.0016478]
	Learning Rate: 0.00164776
	LOSS [training: 0.02187720708033292 | validation: 0.013852328725333055]
	TIME [epoch: 8.36 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020489676739446027		[learning rate: 0.0016458]
		[batch 20/20] avg loss: 0.026700337708047715		[learning rate: 0.0016438]
	Learning Rate: 0.00164377
	LOSS [training: 0.023595007223746873 | validation: 0.02393067475546875]
	TIME [epoch: 8.31 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01931855321184971		[learning rate: 0.0016418]
		[batch 20/20] avg loss: 0.022500635449908473		[learning rate: 0.0016398]
	Learning Rate: 0.00163979
	LOSS [training: 0.0209095943308791 | validation: 0.022036805957833284]
	TIME [epoch: 8.33 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02774816371100931		[learning rate: 0.0016378]
		[batch 20/20] avg loss: 0.06580093296347649		[learning rate: 0.0016358]
	Learning Rate: 0.00163583
	LOSS [training: 0.04677454833724289 | validation: 0.031052241949308362]
	TIME [epoch: 8.31 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03855420981570275		[learning rate: 0.0016338]
		[batch 20/20] avg loss: 0.03686055694446638		[learning rate: 0.0016319]
	Learning Rate: 0.00163186
	LOSS [training: 0.03770738338008457 | validation: 0.11096201568260111]
	TIME [epoch: 8.32 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058615599007623066		[learning rate: 0.0016299]
		[batch 20/20] avg loss: 0.05199391669545058		[learning rate: 0.0016279]
	Learning Rate: 0.00162791
	LOSS [training: 0.05530475785153684 | validation: 0.03882975436576018]
	TIME [epoch: 8.33 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026152487229533367		[learning rate: 0.0016259]
		[batch 20/20] avg loss: 0.02210676559953705		[learning rate: 0.001624]
	Learning Rate: 0.00162397
	LOSS [training: 0.024129626414535206 | validation: 0.04190026798837452]
	TIME [epoch: 8.33 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027482128423746366		[learning rate: 0.001622]
		[batch 20/20] avg loss: 0.031375535646863885		[learning rate: 0.00162]
	Learning Rate: 0.00162004
	LOSS [training: 0.029428832035305125 | validation: 0.019546260581723214]
	TIME [epoch: 8.31 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04039975922584639		[learning rate: 0.0016181]
		[batch 20/20] avg loss: 0.027730415081682013		[learning rate: 0.0016161]
	Learning Rate: 0.00161612
	LOSS [training: 0.034065087153764204 | validation: 0.026151510013161954]
	TIME [epoch: 8.34 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023457782406964618		[learning rate: 0.0016142]
		[batch 20/20] avg loss: 0.012422421425080992		[learning rate: 0.0016122]
	Learning Rate: 0.00161221
	LOSS [training: 0.017940101916022806 | validation: 0.013797961844471072]
	TIME [epoch: 8.31 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01970314013069146		[learning rate: 0.0016103]
		[batch 20/20] avg loss: 0.028391065941700926		[learning rate: 0.0016083]
	Learning Rate: 0.0016083
	LOSS [training: 0.024047103036196192 | validation: 0.027525497535605806]
	TIME [epoch: 8.32 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03024306991142677		[learning rate: 0.0016064]
		[batch 20/20] avg loss: 0.026439925291032913		[learning rate: 0.0016044]
	Learning Rate: 0.00160441
	LOSS [training: 0.028341497601229844 | validation: 0.01774279425964355]
	TIME [epoch: 8.32 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011745739392509152		[learning rate: 0.0016025]
		[batch 20/20] avg loss: 0.02101024923582439		[learning rate: 0.0016005]
	Learning Rate: 0.00160053
	LOSS [training: 0.016377994314166772 | validation: 0.025601079085991598]
	TIME [epoch: 8.34 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011948811882987454		[learning rate: 0.0015986]
		[batch 20/20] avg loss: 0.020279588847012725		[learning rate: 0.0015967]
	Learning Rate: 0.00159665
	LOSS [training: 0.01611420036500009 | validation: 0.017362671018294508]
	TIME [epoch: 8.32 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014038100955090688		[learning rate: 0.0015947]
		[batch 20/20] avg loss: 0.021156293299700996		[learning rate: 0.0015928]
	Learning Rate: 0.00159279
	LOSS [training: 0.017597197127395846 | validation: 0.004188770225981521]
	TIME [epoch: 8.34 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014678981436714086		[learning rate: 0.0015909]
		[batch 20/20] avg loss: 0.03896107570920661		[learning rate: 0.0015889]
	Learning Rate: 0.00158893
	LOSS [training: 0.02021448692643901 | validation: 0.04232651794118514]
	TIME [epoch: 8.33 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02160935532871059		[learning rate: 0.001587]
		[batch 20/20] avg loss: 0.01225234897204032		[learning rate: 0.0015851]
	Learning Rate: 0.00158509
	LOSS [training: 0.01693085215037545 | validation: 0.033670047192753544]
	TIME [epoch: 8.33 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02948578883725344		[learning rate: 0.0015832]
		[batch 20/20] avg loss: 0.01231284083138378		[learning rate: 0.0015812]
	Learning Rate: 0.00158125
	LOSS [training: 0.02089931483431861 | validation: 0.05150504271962262]
	TIME [epoch: 8.31 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023144387615520852		[learning rate: 0.0015793]
		[batch 20/20] avg loss: 0.022784973522269072		[learning rate: 0.0015774]
	Learning Rate: 0.00157742
	LOSS [training: 0.022964680568894967 | validation: 0.016816545671116057]
	TIME [epoch: 8.33 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01566146016611034		[learning rate: 0.0015755]
		[batch 20/20] avg loss: 0.005136705318263161		[learning rate: 0.0015736]
	Learning Rate: 0.0015736
	LOSS [training: 0.010399082742186748 | validation: 0.016499210263759344]
	TIME [epoch: 8.31 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020180938997287047		[learning rate: 0.0015717]
		[batch 20/20] avg loss: 0.018228153274082035		[learning rate: 0.0015698]
	Learning Rate: 0.00156979
	LOSS [training: 0.01920454613568454 | validation: 0.02871710767949104]
	TIME [epoch: 8.32 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023608287373292507		[learning rate: 0.0015679]
		[batch 20/20] avg loss: 0.011977035890572649		[learning rate: 0.001566]
	Learning Rate: 0.00156599
	LOSS [training: 0.01779266163193258 | validation: 0.027585103322035252]
	TIME [epoch: 8.32 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02274145243740482		[learning rate: 0.0015641]
		[batch 20/20] avg loss: 0.01988606777057693		[learning rate: 0.0015622]
	Learning Rate: 0.0015622
	LOSS [training: 0.021313760103990873 | validation: 0.003966717281870646]
	TIME [epoch: 8.35 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005843255809492979		[learning rate: 0.0015603]
		[batch 20/20] avg loss: 0.020350864817013696		[learning rate: 0.0015584]
	Learning Rate: 0.00155842
	LOSS [training: 0.013097060313253337 | validation: 0.02662161471133232]
	TIME [epoch: 8.33 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019216218583604262		[learning rate: 0.0015565]
		[batch 20/20] avg loss: 0.037340726707477526		[learning rate: 0.0015546]
	Learning Rate: 0.00155465
	LOSS [training: 0.028278472645540904 | validation: 0.041262415118549424]
	TIME [epoch: 8.35 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019589244275128096		[learning rate: 0.0015528]
		[batch 20/20] avg loss: 0.027405357729988495		[learning rate: 0.0015509]
	Learning Rate: 0.00155088
	LOSS [training: 0.023497301002558292 | validation: 0.022969307243237775]
	TIME [epoch: 8.32 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027181835180033188		[learning rate: 0.001549]
		[batch 20/20] avg loss: 0.015264328859734388		[learning rate: 0.0015471]
	Learning Rate: 0.00154713
	LOSS [training: 0.021223082019883786 | validation: 0.013083176433199289]
	TIME [epoch: 8.34 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0074867013481334805		[learning rate: 0.0015453]
		[batch 20/20] avg loss: 0.023502929740520085		[learning rate: 0.0015434]
	Learning Rate: 0.00154338
	LOSS [training: 0.015494815544326778 | validation: 0.028624577329085785]
	TIME [epoch: 8.34 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01529280415512591		[learning rate: 0.0015415]
		[batch 20/20] avg loss: 0.01720803953663557		[learning rate: 0.0015396]
	Learning Rate: 0.00153965
	LOSS [training: 0.016250421845880738 | validation: 0.021997257085849126]
	TIME [epoch: 8.33 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013546243121295626		[learning rate: 0.0015378]
		[batch 20/20] avg loss: 0.020930528611192063		[learning rate: 0.0015359]
	Learning Rate: 0.00153592
	LOSS [training: 0.017238385866243842 | validation: 0.025433634698170546]
	TIME [epoch: 8.33 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014268147361637349		[learning rate: 0.0015341]
		[batch 20/20] avg loss: 0.01462526575470908		[learning rate: 0.0015322]
	Learning Rate: 0.0015322
	LOSS [training: 0.014446706558173214 | validation: 0.022932924917931086]
	TIME [epoch: 8.36 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014528507439343325		[learning rate: 0.0015303]
		[batch 20/20] avg loss: 0.01618385816904558		[learning rate: 0.0015285]
	Learning Rate: 0.00152849
	LOSS [training: 0.015356182804194454 | validation: 0.015319327293796208]
	TIME [epoch: 8.35 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024604528424536427		[learning rate: 0.0015266]
		[batch 20/20] avg loss: 0.023135407514340702		[learning rate: 0.0015248]
	Learning Rate: 0.00152479
	LOSS [training: 0.023869967969438566 | validation: 0.0195611470932217]
	TIME [epoch: 8.32 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013281428973779088		[learning rate: 0.0015229]
		[batch 20/20] avg loss: 0.01864726217675492		[learning rate: 0.0015211]
	Learning Rate: 0.0015211
	LOSS [training: 0.015964345575267005 | validation: 0.025348898958671953]
	TIME [epoch: 8.32 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023597019249441804		[learning rate: 0.0015193]
		[batch 20/20] avg loss: 0.012410319422463362		[learning rate: 0.0015174]
	Learning Rate: 0.00151742
	LOSS [training: 0.018003669335952578 | validation: 0.03271910541722854]
	TIME [epoch: 8.34 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040295039779660806		[learning rate: 0.0015156]
		[batch 20/20] avg loss: 0.02679782605857029		[learning rate: 0.0015137]
	Learning Rate: 0.00151374
	LOSS [training: 0.033546432919115556 | validation: 0.0136527759295814]
	TIME [epoch: 8.35 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01419101824425536		[learning rate: 0.0015119]
		[batch 20/20] avg loss: 0.011705049061192603		[learning rate: 0.0015101]
	Learning Rate: 0.00151008
	LOSS [training: 0.012948033652723978 | validation: 0.022720675571177157]
	TIME [epoch: 8.32 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005100687801603147		[learning rate: 0.0015083]
		[batch 20/20] avg loss: 0.026914023646214195		[learning rate: 0.0015064]
	Learning Rate: 0.00150642
	LOSS [training: 0.01600735572390867 | validation: 0.020913596542821933]
	TIME [epoch: 8.32 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018790064210596015		[learning rate: 0.0015046]
		[batch 20/20] avg loss: 0.0144382990673603		[learning rate: 0.0015028]
	Learning Rate: 0.00150278
	LOSS [training: 0.016614181638978157 | validation: 0.028880321812734807]
	TIME [epoch: 8.37 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01979309712307122		[learning rate: 0.001501]
		[batch 20/20] avg loss: 0.01808000805883621		[learning rate: 0.0014991]
	Learning Rate: 0.00149914
	LOSS [training: 0.01893655259095371 | validation: 0.017936585413689567]
	TIME [epoch: 8.34 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010786425439417987		[learning rate: 0.0014973]
		[batch 20/20] avg loss: 0.026503296611992612		[learning rate: 0.0014955]
	Learning Rate: 0.00149551
	LOSS [training: 0.0186448610257053 | validation: 0.015568088104103213]
	TIME [epoch: 8.32 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010687056359913311		[learning rate: 0.0014937]
		[batch 20/20] avg loss: 0.011607307289865323		[learning rate: 0.0014919]
	Learning Rate: 0.00149189
	LOSS [training: 0.011147181824889317 | validation: 0.03464301756506177]
	TIME [epoch: 8.32 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02572984645502822		[learning rate: 0.0014901]
		[batch 20/20] avg loss: 0.018418239019457817		[learning rate: 0.0014883]
	Learning Rate: 0.00148828
	LOSS [training: 0.022074042737243012 | validation: 0.03523188394457834]
	TIME [epoch: 8.35 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02802143999278895		[learning rate: 0.0014865]
		[batch 20/20] avg loss: 0.02696041819322618		[learning rate: 0.0014847]
	Learning Rate: 0.00148468
	LOSS [training: 0.027490929093007554 | validation: 0.02682467702901592]
	TIME [epoch: 8.33 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014266977314997792		[learning rate: 0.0014829]
		[batch 20/20] avg loss: 0.014119188982756593		[learning rate: 0.0014811]
	Learning Rate: 0.00148108
	LOSS [training: 0.014193083148877194 | validation: 0.013162219687875485]
	TIME [epoch: 8.34 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01264303762036614		[learning rate: 0.0014793]
		[batch 20/20] avg loss: 0.022076996672876062		[learning rate: 0.0014775]
	Learning Rate: 0.0014775
	LOSS [training: 0.017360017146621096 | validation: 0.009726682904714599]
	TIME [epoch: 8.33 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012782610375150726		[learning rate: 0.0014757]
		[batch 20/20] avg loss: 0.015082395833402162		[learning rate: 0.0014739]
	Learning Rate: 0.00147392
	LOSS [training: 0.013932503104276444 | validation: 0.022610397117014448]
	TIME [epoch: 8.35 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023433880184978977		[learning rate: 0.0014721]
		[batch 20/20] avg loss: 0.017509910280776808		[learning rate: 0.0014704]
	Learning Rate: 0.00147035
	LOSS [training: 0.020471895232877894 | validation: 0.01726929373333308]
	TIME [epoch: 8.36 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02546033169761816		[learning rate: 0.0014686]
		[batch 20/20] avg loss: 0.022282971373288992		[learning rate: 0.0014668]
	Learning Rate: 0.00146679
	LOSS [training: 0.023871651535453575 | validation: 0.021699815265525564]
	TIME [epoch: 8.33 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01874551165675107		[learning rate: 0.001465]
		[batch 20/20] avg loss: 0.031143680563923925		[learning rate: 0.0014632]
	Learning Rate: 0.00146324
	LOSS [training: 0.024944596110337496 | validation: 0.02916113979270265]
	TIME [epoch: 8.32 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01784023735644927		[learning rate: 0.0014615]
		[batch 20/20] avg loss: 0.0216171059295742		[learning rate: 0.0014597]
	Learning Rate: 0.0014597
	LOSS [training: 0.019728671643011734 | validation: 0.03615373781904799]
	TIME [epoch: 8.35 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028447043993572306		[learning rate: 0.0014579]
		[batch 20/20] avg loss: 0.013920919303151877		[learning rate: 0.0014562]
	Learning Rate: 0.00145616
	LOSS [training: 0.021183981648362094 | validation: 0.017007312578456318]
	TIME [epoch: 8.33 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01231376537569813		[learning rate: 0.0014544]
		[batch 20/20] avg loss: 0.026729790749380006		[learning rate: 0.0014526]
	Learning Rate: 0.00145264
	LOSS [training: 0.019521778062539068 | validation: 0.03736990799662782]
	TIME [epoch: 8.33 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021032209084791104		[learning rate: 0.0014509]
		[batch 20/20] avg loss: 0.017637162726200563		[learning rate: 0.0014491]
	Learning Rate: 0.00144912
	LOSS [training: 0.01933468590549583 | validation: 0.02040526340327365]
	TIME [epoch: 8.33 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013841381701706392		[learning rate: 0.0014474]
		[batch 20/20] avg loss: 0.01729439462145679		[learning rate: 0.0014456]
	Learning Rate: 0.00144562
	LOSS [training: 0.015567888161581589 | validation: 0.021521905899692828]
	TIME [epoch: 8.35 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01870961492074383		[learning rate: 0.0014439]
		[batch 20/20] avg loss: 0.025817172660113286		[learning rate: 0.0014421]
	Learning Rate: 0.00144212
	LOSS [training: 0.022263393790428555 | validation: 0.029069797770623435]
	TIME [epoch: 8.36 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029318957657213603		[learning rate: 0.0014404]
		[batch 20/20] avg loss: 0.020922426669537454		[learning rate: 0.0014386]
	Learning Rate: 0.00143862
	LOSS [training: 0.02512069216337553 | validation: 0.00952571211972537]
	TIME [epoch: 8.32 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015805698040911233		[learning rate: 0.0014369]
		[batch 20/20] avg loss: 0.019036490253676694		[learning rate: 0.0014351]
	Learning Rate: 0.00143514
	LOSS [training: 0.017421094147293963 | validation: 0.013836700295195563]
	TIME [epoch: 8.32 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024009813463012813		[learning rate: 0.0014334]
		[batch 20/20] avg loss: 0.02131827051907079		[learning rate: 0.0014317]
	Learning Rate: 0.00143167
	LOSS [training: 0.022664041991041793 | validation: 0.02881010211108615]
	TIME [epoch: 8.35 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029276981498400655		[learning rate: 0.0014299]
		[batch 20/20] avg loss: 0.02597896054968941		[learning rate: 0.0014282]
	Learning Rate: 0.0014282
	LOSS [training: 0.027627971024045038 | validation: 0.011921971873986748]
	TIME [epoch: 8.34 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02142944829482704		[learning rate: 0.0014265]
		[batch 20/20] avg loss: 0.01847479395096265		[learning rate: 0.0014247]
	Learning Rate: 0.00142474
	LOSS [training: 0.019952121122894843 | validation: 0.008625672786701555]
	TIME [epoch: 8.33 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015415883493001967		[learning rate: 0.001423]
		[batch 20/20] avg loss: 0.017636703379527153		[learning rate: 0.0014213]
	Learning Rate: 0.00142129
	LOSS [training: 0.01652629343626456 | validation: 0.010954140589298837]
	TIME [epoch: 8.32 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004352238442888306		[learning rate: 0.0014196]
		[batch 20/20] avg loss: 0.030189730052665155		[learning rate: 0.0014179]
	Learning Rate: 0.00141785
	LOSS [training: 0.017270984247776733 | validation: 0.02272066542431494]
	TIME [epoch: 8.35 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015886318024484584		[learning rate: 0.0014161]
		[batch 20/20] avg loss: 0.021846027087050465		[learning rate: 0.0014144]
	Learning Rate: 0.00141442
	LOSS [training: 0.018866172555767526 | validation: 0.01350489933564173]
	TIME [epoch: 8.36 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013701678987024702		[learning rate: 0.0014127]
		[batch 20/20] avg loss: 0.017768822210626624		[learning rate: 0.001411]
	Learning Rate: 0.001411
	LOSS [training: 0.015735250598825665 | validation: 0.02063983850558914]
	TIME [epoch: 8.32 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011120153986129349		[learning rate: 0.0014093]
		[batch 20/20] avg loss: 0.01515614338356491		[learning rate: 0.0014076]
	Learning Rate: 0.00140758
	LOSS [training: 0.01313814868484713 | validation: 0.03382673024660801]
	TIME [epoch: 8.32 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009703153673159843		[learning rate: 0.0014059]
		[batch 20/20] avg loss: 0.015745250780346064		[learning rate: 0.0014042]
	Learning Rate: 0.00140417
	LOSS [training: 0.012724202226752954 | validation: 0.013070105773343596]
	TIME [epoch: 8.34 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021080052237738407		[learning rate: 0.0014025]
		[batch 20/20] avg loss: 0.011866978416598575		[learning rate: 0.0014008]
	Learning Rate: 0.00140078
	LOSS [training: 0.016473515327168488 | validation: 0.018624263199931976]
	TIME [epoch: 8.32 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01829556416216091		[learning rate: 0.0013991]
		[batch 20/20] avg loss: 0.01614229867579924		[learning rate: 0.0013974]
	Learning Rate: 0.00139738
	LOSS [training: 0.017218931418980073 | validation: 0.030206678910172598]
	TIME [epoch: 8.34 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021318330886441063		[learning rate: 0.0013957]
		[batch 20/20] avg loss: 0.011382211613976201		[learning rate: 0.001394]
	Learning Rate: 0.001394
	LOSS [training: 0.01635027125020863 | validation: 0.037803559425425515]
	TIME [epoch: 8.33 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022066447414019096		[learning rate: 0.0013923]
		[batch 20/20] avg loss: 0.02808805049860989		[learning rate: 0.0013906]
	Learning Rate: 0.00139063
	LOSS [training: 0.025077248956314496 | validation: 0.055188531258272625]
	TIME [epoch: 8.34 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01739742713713264		[learning rate: 0.0013889]
		[batch 20/20] avg loss: 0.030477632197934007		[learning rate: 0.0013873]
	Learning Rate: 0.00138726
	LOSS [training: 0.023937529667533316 | validation: 0.026497360652011154]
	TIME [epoch: 8.35 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01828456062928837		[learning rate: 0.0013856]
		[batch 20/20] avg loss: 0.02127489268114941		[learning rate: 0.0013839]
	Learning Rate: 0.0013839
	LOSS [training: 0.019779726655218884 | validation: 0.04348077417436297]
	TIME [epoch: 8.33 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02015957746908526		[learning rate: 0.0013822]
		[batch 20/20] avg loss: 0.02588624152798205		[learning rate: 0.0013806]
	Learning Rate: 0.00138055
	LOSS [training: 0.023022909498533656 | validation: 0.026799395215925]
	TIME [epoch: 8.31 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03158777168726345		[learning rate: 0.0013789]
		[batch 20/20] avg loss: 0.014300951730529612		[learning rate: 0.0013772]
	Learning Rate: 0.00137721
	LOSS [training: 0.02294436170889653 | validation: 0.02986965955563495]
	TIME [epoch: 8.33 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024922667257844745		[learning rate: 0.0013755]
		[batch 20/20] avg loss: 0.0235654428851516		[learning rate: 0.0013739]
	Learning Rate: 0.00137388
	LOSS [training: 0.024244055071498172 | validation: 0.023283768404649033]
	TIME [epoch: 8.31 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02058194160070171		[learning rate: 0.0013722]
		[batch 20/20] avg loss: 0.010685326822826491		[learning rate: 0.0013705]
	Learning Rate: 0.00137055
	LOSS [training: 0.015633634211764103 | validation: 0.0237197878835251]
	TIME [epoch: 8.32 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019819108423293368		[learning rate: 0.0013689]
		[batch 20/20] avg loss: 0.026138414130111393		[learning rate: 0.0013672]
	Learning Rate: 0.00136723
	LOSS [training: 0.02297876127670238 | validation: 0.02215377072444525]
	TIME [epoch: 8.33 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015500251932455241		[learning rate: 0.0013656]
		[batch 20/20] avg loss: 0.014812399038281052		[learning rate: 0.0013639]
	Learning Rate: 0.00136392
	LOSS [training: 0.015156325485368143 | validation: 0.019388529881111817]
	TIME [epoch: 8.34 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01926829236772769		[learning rate: 0.0013623]
		[batch 20/20] avg loss: 0.029510671625636763		[learning rate: 0.0013606]
	Learning Rate: 0.00136062
	LOSS [training: 0.02438948199668223 | validation: 0.02282276314686594]
	TIME [epoch: 8.32 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02998952871541761		[learning rate: 0.001359]
		[batch 20/20] avg loss: 0.016058445989711605		[learning rate: 0.0013573]
	Learning Rate: 0.00135733
	LOSS [training: 0.023023987352564605 | validation: 0.01882454505552342]
	TIME [epoch: 8.35 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0143479188858436		[learning rate: 0.0013557]
		[batch 20/20] avg loss: 0.03753044353921108		[learning rate: 0.001354]
	Learning Rate: 0.00135404
	LOSS [training: 0.025939181212527345 | validation: 0.037661753173558694]
	TIME [epoch: 8.33 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015959500311857402		[learning rate: 0.0013524]
		[batch 20/20] avg loss: 0.017504625766622907		[learning rate: 0.0013508]
	Learning Rate: 0.00135076
	LOSS [training: 0.016732063039240155 | validation: 0.02239797786677565]
	TIME [epoch: 8.33 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009794869168828718		[learning rate: 0.0013491]
		[batch 20/20] avg loss: 0.007857614238605305		[learning rate: 0.0013475]
	Learning Rate: 0.00134749
	LOSS [training: 0.008826241703717012 | validation: 0.03619575355182622]
	TIME [epoch: 8.32 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018799385825669115		[learning rate: 0.0013459]
		[batch 20/20] avg loss: 0.021784349583400347		[learning rate: 0.0013442]
	Learning Rate: 0.00134423
	LOSS [training: 0.020291867704534734 | validation: 0.008627370244479234]
	TIME [epoch: 8.33 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018127133094786703		[learning rate: 0.0013426]
		[batch 20/20] avg loss: 0.013627122053485671		[learning rate: 0.001341]
	Learning Rate: 0.00134098
	LOSS [training: 0.015877127574136184 | validation: 0.027526635379278044]
	TIME [epoch: 8.32 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019129988712517063		[learning rate: 0.0013394]
		[batch 20/20] avg loss: 0.015785391236629552		[learning rate: 0.0013377]
	Learning Rate: 0.00133773
	LOSS [training: 0.017457689974573306 | validation: 0.007289968157393101]
	TIME [epoch: 8.34 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01416929188406298		[learning rate: 0.0013361]
		[batch 20/20] avg loss: 0.010053120283825078		[learning rate: 0.0013345]
	Learning Rate: 0.00133449
	LOSS [training: 0.01211120608394403 | validation: 0.016862733431158843]
	TIME [epoch: 8.35 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015870126363311986		[learning rate: 0.0013329]
		[batch 20/20] avg loss: 0.010156321946333358		[learning rate: 0.0013313]
	Learning Rate: 0.00133126
	LOSS [training: 0.013013224154822672 | validation: 0.00626929004301211]
	TIME [epoch: 8.32 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009551600972132607		[learning rate: 0.0013296]
		[batch 20/20] avg loss: 0.013901059646292282		[learning rate: 0.001328]
	Learning Rate: 0.00132804
	LOSS [training: 0.011726330309212442 | validation: 0.02371643867565816]
	TIME [epoch: 8.31 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020128755884165635		[learning rate: 0.0013264]
		[batch 20/20] avg loss: 0.03155400279317052		[learning rate: 0.0013248]
	Learning Rate: 0.00132482
	LOSS [training: 0.02584137933866808 | validation: 0.018053954726307175]
	TIME [epoch: 8.34 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027915242503268833		[learning rate: 0.0013232]
		[batch 20/20] avg loss: 0.03219406671940988		[learning rate: 0.0013216]
	Learning Rate: 0.00132162
	LOSS [training: 0.030054654611339347 | validation: 0.014018005698980169]
	TIME [epoch: 8.33 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01467415356979403		[learning rate: 0.00132]
		[batch 20/20] avg loss: 0.018996067068923518		[learning rate: 0.0013184]
	Learning Rate: 0.00131842
	LOSS [training: 0.016835110319358772 | validation: 0.011819436785547213]
	TIME [epoch: 8.33 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038189118730849896		[learning rate: 0.0013168]
		[batch 20/20] avg loss: 0.02532942600951076		[learning rate: 0.0013152]
	Learning Rate: 0.00131522
	LOSS [training: 0.014574168941297875 | validation: 0.027884107740344378]
	TIME [epoch: 8.32 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016301273137619948		[learning rate: 0.0013136]
		[batch 20/20] avg loss: 0.029322070639886615		[learning rate: 0.001312]
	Learning Rate: 0.00131204
	LOSS [training: 0.02281167188875328 | validation: 0.013777704012746661]
	TIME [epoch: 8.35 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01022618180175493		[learning rate: 0.0013105]
		[batch 20/20] avg loss: 0.012070119987455793		[learning rate: 0.0013089]
	Learning Rate: 0.00130886
	LOSS [training: 0.011148150894605361 | validation: 0.010362172544502685]
	TIME [epoch: 8.36 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010864887462169414		[learning rate: 0.0013073]
		[batch 20/20] avg loss: 0.017508378720802544		[learning rate: 0.0013057]
	Learning Rate: 0.0013057
	LOSS [training: 0.014186633091485976 | validation: 0.004197715910115233]
	TIME [epoch: 8.32 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015046490549474042		[learning rate: 0.0013041]
		[batch 20/20] avg loss: 0.036912270195133004		[learning rate: 0.0013025]
	Learning Rate: 0.00130254
	LOSS [training: 0.025979380372303525 | validation: 0.040824102967541936]
	TIME [epoch: 8.31 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03165938110540049		[learning rate: 0.001301]
		[batch 20/20] avg loss: 0.019322109883288634		[learning rate: 0.0012994]
	Learning Rate: 0.00129938
	LOSS [training: 0.025490745494344564 | validation: 0.05334755477065556]
	TIME [epoch: 8.34 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025171026912727622		[learning rate: 0.0012978]
		[batch 20/20] avg loss: 0.017388104377092935		[learning rate: 0.0012962]
	Learning Rate: 0.00129624
	LOSS [training: 0.021279565644910278 | validation: 0.02081552043765026]
	TIME [epoch: 8.34 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016008012557772195		[learning rate: 0.0012947]
		[batch 20/20] avg loss: 0.014573904485208164		[learning rate: 0.0012931]
	Learning Rate: 0.0012931
	LOSS [training: 0.015290958521490174 | validation: 0.004605644655283994]
	TIME [epoch: 8.32 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016450744822788303		[learning rate: 0.0012915]
		[batch 20/20] avg loss: 0.012327304967372646		[learning rate: 0.00129]
	Learning Rate: 0.00128997
	LOSS [training: 0.014389024895080477 | validation: 0.024249684177539175]
	TIME [epoch: 8.32 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029502154149319105		[learning rate: 0.0012884]
		[batch 20/20] avg loss: 0.01396774749584806		[learning rate: 0.0012868]
	Learning Rate: 0.00128685
	LOSS [training: 0.021734950822583577 | validation: 0.0003452488616541422]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_946.pth
	Model improved!!!
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013781967276258581		[learning rate: 0.0012853]
		[batch 20/20] avg loss: 0.0012460109176498247		[learning rate: 0.0012837]
	Learning Rate: 0.00128373
	LOSS [training: 0.007513989096954203 | validation: 0.020762565121989773]
	TIME [epoch: 8.34 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010765287742621452		[learning rate: 0.0012822]
		[batch 20/20] avg loss: 0.0069291548963762994		[learning rate: 0.0012806]
	Learning Rate: 0.00128062
	LOSS [training: 0.008847221319498876 | validation: 0.015308771171568678]
	TIME [epoch: 8.31 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03233871451398045		[learning rate: 0.0012791]
		[batch 20/20] avg loss: 0.01929309523321067		[learning rate: 0.0012775]
	Learning Rate: 0.00127752
	LOSS [training: 0.025815904873595558 | validation: 0.010080937115179671]
	TIME [epoch: 8.31 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010571493475776853		[learning rate: 0.001276]
		[batch 20/20] avg loss: 0.021791556327663296		[learning rate: 0.0012744]
	Learning Rate: 0.00127443
	LOSS [training: 0.016181524901720076 | validation: 0.015319760860813061]
	TIME [epoch: 8.34 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007679173660862621		[learning rate: 0.0012729]
		[batch 20/20] avg loss: 0.01616014191973349		[learning rate: 0.0012713]
	Learning Rate: 0.00127134
	LOSS [training: 0.011919657790298053 | validation: 0.02303673272649856]
	TIME [epoch: 8.33 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016655635588285207		[learning rate: 0.0012698]
		[batch 20/20] avg loss: 0.009969748121762292		[learning rate: 0.0012683]
	Learning Rate: 0.00126827
	LOSS [training: 0.013312691855023748 | validation: 0.01449498415540153]
	TIME [epoch: 8.32 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018482669080374273		[learning rate: 0.0012667]
		[batch 20/20] avg loss: 0.0057474332325310245		[learning rate: 0.0012652]
	Learning Rate: 0.0012652
	LOSS [training: 0.012115051156452648 | validation: 0.013228309650279085]
	TIME [epoch: 8.32 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01763582877574095		[learning rate: 0.0012637]
		[batch 20/20] avg loss: 0.015092849081845527		[learning rate: 0.0012621]
	Learning Rate: 0.00126213
	LOSS [training: 0.016364338928793236 | validation: 0.005367469236599616]
	TIME [epoch: 8.37 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0061805399810271906		[learning rate: 0.0012606]
		[batch 20/20] avg loss: 0.009127469178886688		[learning rate: 0.0012591]
	Learning Rate: 0.00125908
	LOSS [training: 0.007654004579956941 | validation: 0.007917051007355192]
	TIME [epoch: 8.33 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004132660514683496		[learning rate: 0.0012576]
		[batch 20/20] avg loss: 0.012531083521123507		[learning rate: 0.001256]
	Learning Rate: 0.00125603
	LOSS [training: 0.008331872017903502 | validation: 0.013967346055422363]
	TIME [epoch: 8.31 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031560849862412546		[learning rate: 0.0012545]
		[batch 20/20] avg loss: 0.018870417429064867		[learning rate: 0.001253]
	Learning Rate: 0.00125299
	LOSS [training: 0.02521563364573871 | validation: 0.017038889747165595]
	TIME [epoch: 8.32 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02754264479892889		[learning rate: 0.0012515]
		[batch 20/20] avg loss: 0.008985341735945734		[learning rate: 0.00125]
	Learning Rate: 0.00124996
	LOSS [training: 0.01826399326743731 | validation: 0.00948311097758792]
	TIME [epoch: 8.36 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016173875248507434		[learning rate: 0.0012484]
		[batch 20/20] avg loss: 0.014747785871479657		[learning rate: 0.0012469]
	Learning Rate: 0.00124693
	LOSS [training: 0.015460830559993541 | validation: 0.007772182743560206]
	TIME [epoch: 8.32 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018138585874176622		[learning rate: 0.0012454]
		[batch 20/20] avg loss: 0.019064801924259182		[learning rate: 0.0012439]
	Learning Rate: 0.00124391
	LOSS [training: 0.0186016938992179 | validation: 0.03462395934259646]
	TIME [epoch: 8.32 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015798773302168168		[learning rate: 0.0012424]
		[batch 20/20] avg loss: 0.022186222968976585		[learning rate: 0.0012409]
	Learning Rate: 0.0012409
	LOSS [training: 0.018992498135572373 | validation: 0.014157919770769967]
	TIME [epoch: 8.35 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012773476270131447		[learning rate: 0.0012394]
		[batch 20/20] avg loss: 0.01372883337696972		[learning rate: 0.0012379]
	Learning Rate: 0.0012379
	LOSS [training: 0.013251154823550581 | validation: 0.01889621506598837]
	TIME [epoch: 8.34 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02093682324941768		[learning rate: 0.0012364]
		[batch 20/20] avg loss: 0.010695451280635981		[learning rate: 0.0012349]
	Learning Rate: 0.0012349
	LOSS [training: 0.01581613726502683 | validation: 0.013882799697973517]
	TIME [epoch: 8.32 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008576318472415415		[learning rate: 0.0012334]
		[batch 20/20] avg loss: 0.004281201898461319		[learning rate: 0.0012319]
	Learning Rate: 0.00123191
	LOSS [training: 0.006428760185438367 | validation: 0.009444145577701362]
	TIME [epoch: 8.31 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003942219457642405		[learning rate: 0.0012304]
		[batch 20/20] avg loss: 0.018500741002518792		[learning rate: 0.0012289]
	Learning Rate: 0.00122893
	LOSS [training: 0.0112214802300806 | validation: 0.028503403010983926]
	TIME [epoch: 8.31 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020495306595895744		[learning rate: 0.0012274]
		[batch 20/20] avg loss: 0.016074914657330092		[learning rate: 0.001226]
	Learning Rate: 0.00122595
	LOSS [training: 0.01828511062661292 | validation: 0.016451221121001898]
	TIME [epoch: 8.38 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021202533848817353		[learning rate: 0.0012245]
		[batch 20/20] avg loss: 0.007996478066930083		[learning rate: 0.001223]
	Learning Rate: 0.00122298
	LOSS [training: 0.005058365725905908 | validation: 0.0020968522033059374]
	TIME [epoch: 8.34 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006782503875709736		[learning rate: 0.0012215]
		[batch 20/20] avg loss: 0.012441674098219608		[learning rate: 0.00122]
	Learning Rate: 0.00122002
	LOSS [training: 0.009612088986964674 | validation: 0.019227419933953594]
	TIME [epoch: 8.35 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018965246125953412		[learning rate: 0.0012185]
		[batch 20/20] avg loss: 0.01727671816370844		[learning rate: 0.0012171]
	Learning Rate: 0.00121707
	LOSS [training: 0.018120982144830926 | validation: 0.011049731892575401]
	TIME [epoch: 8.34 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014564500820850677		[learning rate: 0.0012156]
		[batch 20/20] avg loss: 0.012034643494942928		[learning rate: 0.0012141]
	Learning Rate: 0.00121412
	LOSS [training: 0.013299572157896799 | validation: 0.009992709344210194]
	TIME [epoch: 8.35 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020054576424417946		[learning rate: 0.0012127]
		[batch 20/20] avg loss: 0.010753142437189958		[learning rate: 0.0012112]
	Learning Rate: 0.00121119
	LOSS [training: 0.015403859430803954 | validation: 0.0007106123896611478]
	TIME [epoch: 8.33 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008641424654134287		[learning rate: 0.0012097]
		[batch 20/20] avg loss: 0.018030400497507502		[learning rate: 0.0012083]
	Learning Rate: 0.00120825
	LOSS [training: 0.013335912575820896 | validation: 0.007578458801337238]
	TIME [epoch: 8.34 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013452207488540128		[learning rate: 0.0012068]
		[batch 20/20] avg loss: 0.013906897696021903		[learning rate: 0.0012053]
	Learning Rate: 0.00120533
	LOSS [training: 0.013679552592281017 | validation: 0.013372888365419873]
	TIME [epoch: 8.33 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012481988456381406		[learning rate: 0.0012039]
		[batch 20/20] avg loss: 0.01136354951642916		[learning rate: 0.0012024]
	Learning Rate: 0.00120241
	LOSS [training: 0.011922768986405282 | validation: 0.012146119485645087]
	TIME [epoch: 8.35 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016074588958271137		[learning rate: 0.001201]
		[batch 20/20] avg loss: 0.02013073834174795		[learning rate: 0.0011995]
	Learning Rate: 0.0011995
	LOSS [training: 0.018102663650009544 | validation: 0.010322981709749556]
	TIME [epoch: 8.34 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017856375847937204		[learning rate: 0.001198]
		[batch 20/20] avg loss: 0.014690126006978327		[learning rate: 0.0011966]
	Learning Rate: 0.0011966
	LOSS [training: 0.016273250927457766 | validation: 0.01587250869959954]
	TIME [epoch: 8.33 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014387473021242584		[learning rate: 0.0011951]
		[batch 20/20] avg loss: 0.021558565656006015		[learning rate: 0.0011937]
	Learning Rate: 0.0011937
	LOSS [training: 0.017973019338624295 | validation: 0.01583366698630437]
	TIME [epoch: 8.31 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02496813567953174		[learning rate: 0.0011923]
		[batch 20/20] avg loss: 0.012974542627392694		[learning rate: 0.0011908]
	Learning Rate: 0.00119081
	LOSS [training: 0.01897133915346222 | validation: 0.017529081838550536]
	TIME [epoch: 8.33 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02695835483469087		[learning rate: 0.0011894]
		[batch 20/20] avg loss: 0.014791925782883861		[learning rate: 0.0011879]
	Learning Rate: 0.00118793
	LOSS [training: 0.020875140308787364 | validation: 0.013637559595523006]
	TIME [epoch: 8.31 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004144217629706283		[learning rate: 0.0011865]
		[batch 20/20] avg loss: 0.012328063195227213		[learning rate: 0.0011851]
	Learning Rate: 0.00118505
	LOSS [training: 0.008236140412466748 | validation: 0.014559267678430281]
	TIME [epoch: 8.31 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016802942754025555		[learning rate: 0.0011836]
		[batch 20/20] avg loss: 0.011751203355422762		[learning rate: 0.0011822]
	Learning Rate: 0.00118218
	LOSS [training: 0.014277073054724157 | validation: 0.00882077696026987]
	TIME [epoch: 8.31 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014091023495330058		[learning rate: 0.0011807]
		[batch 20/20] avg loss: 0.013050340122609088		[learning rate: 0.0011793]
	Learning Rate: 0.00117932
	LOSS [training: 0.013570681808969576 | validation: 0.01153788713041616]
	TIME [epoch: 8.33 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024797667098688244		[learning rate: 0.0011779]
		[batch 20/20] avg loss: 0.011189126395822691		[learning rate: 0.0011765]
	Learning Rate: 0.00117646
	LOSS [training: 0.017993396747255468 | validation: 0.015241552173883744]
	TIME [epoch: 8.35 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023067449151754443		[learning rate: 0.001175]
		[batch 20/20] avg loss: 0.02121361510529782		[learning rate: 0.0011736]
	Learning Rate: 0.00117362
	LOSS [training: 0.02214053212852613 | validation: 0.031953701492795555]
	TIME [epoch: 8.32 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01559107185380106		[learning rate: 0.0011722]
		[batch 20/20] avg loss: 0.00980477492886337		[learning rate: 0.0011708]
	Learning Rate: 0.00117078
	LOSS [training: 0.012697923391332213 | validation: 0.01112878630970248]
	TIME [epoch: 8.32 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013155226156733202		[learning rate: 0.0011694]
		[batch 20/20] avg loss: 0.006701541243980158		[learning rate: 0.0011679]
	Learning Rate: 0.00116794
	LOSS [training: 0.00992838370035668 | validation: 0.020889190533804516]
	TIME [epoch: 8.34 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008872481576209007		[learning rate: 0.0011665]
		[batch 20/20] avg loss: 0.013541619725442936		[learning rate: 0.0011651]
	Learning Rate: 0.00116511
	LOSS [training: 0.011207050650825971 | validation: 0.013436608930899948]
	TIME [epoch: 8.37 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007253488931217146		[learning rate: 0.0011637]
		[batch 20/20] avg loss: 0.014953719623014783		[learning rate: 0.0011623]
	Learning Rate: 0.00116229
	LOSS [training: 0.011103604277115963 | validation: 0.03400522570471248]
	TIME [epoch: 8.32 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01987935102000861		[learning rate: 0.0011609]
		[batch 20/20] avg loss: 0.008679225567771056		[learning rate: 0.0011595]
	Learning Rate: 0.00115948
	LOSS [training: 0.014279288293889835 | validation: 0.02705965118412946]
	TIME [epoch: 8.31 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023149608627666674		[learning rate: 0.0011581]
		[batch 20/20] avg loss: 0.019209853628674077		[learning rate: 0.0011567]
	Learning Rate: 0.00115667
	LOSS [training: 0.021179731128170374 | validation: 0.01807454135638493]
	TIME [epoch: 8.34 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02790387463544124		[learning rate: 0.0011553]
		[batch 20/20] avg loss: 0.01416256595922127		[learning rate: 0.0011539]
	Learning Rate: 0.00115387
	LOSS [training: 0.02103322029733125 | validation: 0.018399223004621358]
	TIME [epoch: 8.31 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007940183958825927		[learning rate: 0.0011525]
		[batch 20/20] avg loss: 0.014900941541948792		[learning rate: 0.0011511]
	Learning Rate: 0.00115108
	LOSS [training: 0.01142056275038736 | validation: 0.017005355136339093]
	TIME [epoch: 8.31 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029149486978621443		[learning rate: 0.0011497]
		[batch 20/20] avg loss: 0.010233569358939947		[learning rate: 0.0011483]
	Learning Rate: 0.00114829
	LOSS [training: 0.019691528168780694 | validation: 0.019200250191601168]
	TIME [epoch: 8.34 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007648284905458704		[learning rate: 0.0011469]
		[batch 20/20] avg loss: 0.04944945140298103		[learning rate: 0.0011455]
	Learning Rate: 0.00114551
	LOSS [training: 0.028548868154219864 | validation: 0.04056725225479275]
	TIME [epoch: 8.36 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028199477122641453		[learning rate: 0.0011441]
		[batch 20/20] avg loss: 0.01864840397893742		[learning rate: 0.0011427]
	Learning Rate: 0.00114274
	LOSS [training: 0.02342394055078944 | validation: 0.013122336067468902]
	TIME [epoch: 8.32 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02320737919322176		[learning rate: 0.0011414]
		[batch 20/20] avg loss: 0.02689842838761089		[learning rate: 0.00114]
	Learning Rate: 0.00113997
	LOSS [training: 0.02505290379041632 | validation: 0.028732336981297225]
	TIME [epoch: 8.32 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03541345759074911		[learning rate: 0.0011386]
		[batch 20/20] avg loss: 0.04170663696663089		[learning rate: 0.0011372]
	Learning Rate: 0.00113721
	LOSS [training: 0.03856004727868999 | validation: 0.03873315991395049]
	TIME [epoch: 8.34 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019383568239292128		[learning rate: 0.0011358]
		[batch 20/20] avg loss: 0.020834875639421404		[learning rate: 0.0011345]
	Learning Rate: 0.00113446
	LOSS [training: 0.020109221939356763 | validation: 0.02020843124380255]
	TIME [epoch: 8.35 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018599395733175938		[learning rate: 0.0011331]
		[batch 20/20] avg loss: 0.008246948426691231		[learning rate: 0.0011317]
	Learning Rate: 0.00113171
	LOSS [training: 0.013423172079933585 | validation: 0.007229597269111817]
	TIME [epoch: 8.31 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012245501299291136		[learning rate: 0.0011303]
		[batch 20/20] avg loss: 0.014526254685993811		[learning rate: 0.001129]
	Learning Rate: 0.00112897
	LOSS [training: 0.013385877992642473 | validation: 0.015290107502979853]
	TIME [epoch: 8.31 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004368394281131038		[learning rate: 0.0011276]
		[batch 20/20] avg loss: 0.010497928384879557		[learning rate: 0.0011262]
	Learning Rate: 0.00112624
	LOSS [training: 0.007433161333005298 | validation: 0.018830076774210933]
	TIME [epoch: 8.31 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009993907662336658		[learning rate: 0.0011249]
		[batch 20/20] avg loss: 0.023841375092850448		[learning rate: 0.0011235]
	Learning Rate: 0.00112352
	LOSS [training: 0.016917641377593554 | validation: 0.024130382410427097]
	TIME [epoch: 8.33 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008290785712880663		[learning rate: 0.0011222]
		[batch 20/20] avg loss: 0.013410312252630585		[learning rate: 0.0011208]
	Learning Rate: 0.0011208
	LOSS [training: 0.010850548982755627 | validation: 0.02586711346020382]
	TIME [epoch: 8.32 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029567705183346855		[learning rate: 0.0011194]
		[batch 20/20] avg loss: 0.014792440100016726		[learning rate: 0.0011181]
	Learning Rate: 0.00111808
	LOSS [training: 0.022180072641681793 | validation: 0.01127870894065076]
	TIME [epoch: 8.33 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027028726986700346		[learning rate: 0.0011167]
		[batch 20/20] avg loss: 0.03455537680055473		[learning rate: 0.0011154]
	Learning Rate: 0.00111538
	LOSS [training: 0.030792051893627537 | validation: 0.04039971517541885]
	TIME [epoch: 8.32 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05256459294621087		[learning rate: 0.001114]
		[batch 20/20] avg loss: 0.04892438424621985		[learning rate: 0.0011127]
	Learning Rate: 0.00111268
	LOSS [training: 0.05074448859621535 | validation: 0.058517410337405575]
	TIME [epoch: 8.34 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03606372426783247		[learning rate: 0.0011113]
		[batch 20/20] avg loss: 0.028611536835385965		[learning rate: 0.00111]
	Learning Rate: 0.00110998
	LOSS [training: 0.03233763055160921 | validation: 0.024913835858505026]
	TIME [epoch: 8.35 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017041481383171145		[learning rate: 0.0011086]
		[batch 20/20] avg loss: 0.022995890111172927		[learning rate: 0.0011073]
	Learning Rate: 0.00110729
	LOSS [training: 0.020018685747172034 | validation: 0.01924086329529417]
	TIME [epoch: 8.32 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011385282098819798		[learning rate: 0.001106]
		[batch 20/20] avg loss: 0.021416529718219352		[learning rate: 0.0011046]
	Learning Rate: 0.00110461
	LOSS [training: 0.016400905908519576 | validation: 0.03420803617943222]
	TIME [epoch: 8.31 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021974311692541267		[learning rate: 0.0011033]
		[batch 20/20] avg loss: 0.017809745694386255		[learning rate: 0.0011019]
	Learning Rate: 0.00110194
	LOSS [training: 0.019892028693463758 | validation: 0.01361534612419547]
	TIME [epoch: 8.34 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016287526733725632		[learning rate: 0.0011006]
		[batch 20/20] avg loss: 0.018380242391800385		[learning rate: 0.0010993]
	Learning Rate: 0.00109927
	LOSS [training: 0.017333884562763007 | validation: 0.017583629864242743]
	TIME [epoch: 8.31 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012451884929444202		[learning rate: 0.0010979]
		[batch 20/20] avg loss: 0.014428708091802259		[learning rate: 0.0010966]
	Learning Rate: 0.00109661
	LOSS [training: 0.013440296510623229 | validation: 0.01179319353281531]
	TIME [epoch: 8.31 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009156282813485465		[learning rate: 0.0010953]
		[batch 20/20] avg loss: 0.037110903352311195		[learning rate: 0.001094]
	Learning Rate: 0.00109396
	LOSS [training: 0.023133593082898328 | validation: 0.028832759297197426]
	TIME [epoch: 8.36 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03641311349757351		[learning rate: 0.0010926]
		[batch 20/20] avg loss: 0.02168707626573526		[learning rate: 0.0010913]
	Learning Rate: 0.00109131
	LOSS [training: 0.02905009488165438 | validation: 0.01476549049870975]
	TIME [epoch: 8.33 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011064104139321341		[learning rate: 0.00109]
		[batch 20/20] avg loss: 0.019365978663348722		[learning rate: 0.0010887]
	Learning Rate: 0.00108867
	LOSS [training: 0.015215041401335028 | validation: 0.01567337846396303]
	TIME [epoch: 8.32 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02306989834486893		[learning rate: 0.0010873]
		[batch 20/20] avg loss: 0.021505838787803204		[learning rate: 0.001086]
	Learning Rate: 0.00108603
	LOSS [training: 0.02228786856633607 | validation: 0.006402569575612859]
	TIME [epoch: 8.33 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009136165200093914		[learning rate: 0.0010847]
		[batch 20/20] avg loss: 0.01755725323434697		[learning rate: 0.0010834]
	Learning Rate: 0.0010834
	LOSS [training: 0.01334670921722044 | validation: 0.010461192461824169]
	TIME [epoch: 8.35 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01514003717215901		[learning rate: 0.0010821]
		[batch 20/20] avg loss: 0.019569723828828968		[learning rate: 0.0010808]
	Learning Rate: 0.00108078
	LOSS [training: 0.01735488050049399 | validation: 0.033508337447302486]
	TIME [epoch: 8.32 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02660800905235189		[learning rate: 0.0010795]
		[batch 20/20] avg loss: 0.012916238079807222		[learning rate: 0.0010782]
	Learning Rate: 0.00107816
	LOSS [training: 0.019762123566079558 | validation: 0.028682652492433024]
	TIME [epoch: 8.32 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017793139543981727		[learning rate: 0.0010769]
		[batch 20/20] avg loss: 0.02034720368524531		[learning rate: 0.0010756]
	Learning Rate: 0.00107555
	LOSS [training: 0.01907017161461352 | validation: 0.012760396981293196]
	TIME [epoch: 8.32 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020786759363562394		[learning rate: 0.0010742]
		[batch 20/20] avg loss: 0.01379829378109447		[learning rate: 0.0010729]
	Learning Rate: 0.00107295
	LOSS [training: 0.01729252657232843 | validation: 0.025198387305044794]
	TIME [epoch: 8.33 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029204749599544656		[learning rate: 0.0010716]
		[batch 20/20] avg loss: 0.014837926088820494		[learning rate: 0.0010704]
	Learning Rate: 0.00107035
	LOSS [training: 0.022021337844182576 | validation: 0.017598094744082138]
	TIME [epoch: 8.35 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016239519467597767		[learning rate: 0.0010691]
		[batch 20/20] avg loss: 0.018215879384126533		[learning rate: 0.0010678]
	Learning Rate: 0.00106776
	LOSS [training: 0.017227699425862154 | validation: 0.03104505529642735]
	TIME [epoch: 8.32 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011342997148820934		[learning rate: 0.0010665]
		[batch 20/20] avg loss: 0.011400247551422495		[learning rate: 0.0010652]
	Learning Rate: 0.00106518
	LOSS [training: 0.011371622350121714 | validation: 0.01395982781712858]
	TIME [epoch: 8.33 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02306334992426074		[learning rate: 0.0010639]
		[batch 20/20] avg loss: 0.012975623276915146		[learning rate: 0.0010626]
	Learning Rate: 0.0010626
	LOSS [training: 0.018019486600587945 | validation: 0.013381612546914917]
	TIME [epoch: 8.35 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015544092926721001		[learning rate: 0.0010613]
		[batch 20/20] avg loss: 0.006200335727465459		[learning rate: 0.00106]
	Learning Rate: 0.00106002
	LOSS [training: 0.010872214327093229 | validation: 0.018372656963505818]
	TIME [epoch: 8.35 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014963773060278875		[learning rate: 0.0010587]
		[batch 20/20] avg loss: 0.013476425951600698		[learning rate: 0.0010575]
	Learning Rate: 0.00105746
	LOSS [training: 0.014220099505939787 | validation: 0.006086801761817112]
	TIME [epoch: 8.31 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012196902232490472		[learning rate: 0.0010562]
		[batch 20/20] avg loss: 0.006256805683584402		[learning rate: 0.0010549]
	Learning Rate: 0.0010549
	LOSS [training: 0.009226853958037438 | validation: 0.02124588472107521]
	TIME [epoch: 8.31 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014381030421625524		[learning rate: 0.0010536]
		[batch 20/20] avg loss: 0.00451821933736211		[learning rate: 0.0010523]
	Learning Rate: 0.00105234
	LOSS [training: 0.009449624879493818 | validation: 0.008619005509449367]
	TIME [epoch: 8.32 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008978390107206951		[learning rate: 0.0010511]
		[batch 20/20] avg loss: 0.004846059614113597		[learning rate: 0.0010498]
	Learning Rate: 0.0010498
	LOSS [training: 0.0069122248606602755 | validation: 0.015748765002471254]
	TIME [epoch: 8.32 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00853407055384276		[learning rate: 0.0010485]
		[batch 20/20] avg loss: 0.00854363344154141		[learning rate: 0.0010473]
	Learning Rate: 0.00104726
	LOSS [training: 0.008538851997692085 | validation: 0.012126033165427985]
	TIME [epoch: 8.34 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008627874008666051		[learning rate: 0.001046]
		[batch 20/20] avg loss: 0.003220864332371691		[learning rate: 0.0010447]
	Learning Rate: 0.00104472
	LOSS [training: 0.005924369170518872 | validation: 0.004779827637530284]
	TIME [epoch: 8.35 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007735830700929228		[learning rate: 0.0010435]
		[batch 20/20] avg loss: 0.01861480519218594		[learning rate: 0.0010422]
	Learning Rate: 0.00104219
	LOSS [training: 0.013175317946557585 | validation: 0.01629627127675108]
	TIME [epoch: 8.34 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010407943876060147		[learning rate: 0.0010409]
		[batch 20/20] avg loss: 0.01672726996336063		[learning rate: 0.0010397]
	Learning Rate: 0.00103967
	LOSS [training: 0.013567606919710387 | validation: 0.0065446310925772674]
	TIME [epoch: 8.32 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010674691138728449		[learning rate: 0.0010384]
		[batch 20/20] avg loss: 0.01429114922567079		[learning rate: 0.0010372]
	Learning Rate: 0.00103715
	LOSS [training: 0.01248292018219962 | validation: 0.006472960091543476]
	TIME [epoch: 8.34 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011252029982838608		[learning rate: 0.0010359]
		[batch 20/20] avg loss: 0.008524905595022392		[learning rate: 0.0010346]
	Learning Rate: 0.00103464
	LOSS [training: 0.009888467788930502 | validation: 0.009844502430411786]
	TIME [epoch: 8.34 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008634718733381681		[learning rate: 0.0010334]
		[batch 20/20] avg loss: 0.005507980769574287		[learning rate: 0.0010321]
	Learning Rate: 0.00103214
	LOSS [training: 0.007071349751477986 | validation: 0.005059003665312286]
	TIME [epoch: 8.33 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010657448847735524		[learning rate: 0.0010309]
		[batch 20/20] avg loss: 0.022273371526390095		[learning rate: 0.0010296]
	Learning Rate: 0.00102964
	LOSS [training: 0.01646541018706281 | validation: 0.020708749139900465]
	TIME [epoch: 8.33 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013739515441231049		[learning rate: 0.0010284]
		[batch 20/20] avg loss: 0.01804545578469715		[learning rate: 0.0010271]
	Learning Rate: 0.00102714
	LOSS [training: 0.015892485612964094 | validation: 0.01667768838767853]
	TIME [epoch: 8.31 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019304642406425168		[learning rate: 0.0010259]
		[batch 20/20] avg loss: 0.010969304559376326		[learning rate: 0.0010247]
	Learning Rate: 0.00102466
	LOSS [training: 0.015136973482900745 | validation: 0.021776963514877352]
	TIME [epoch: 8.33 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010866912643069717		[learning rate: 0.0010234]
		[batch 20/20] avg loss: 0.022461360582412326		[learning rate: 0.0010222]
	Learning Rate: 0.00102218
	LOSS [training: 0.016664136612741017 | validation: 0.017499960563640247]
	TIME [epoch: 8.37 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010194072129006889		[learning rate: 0.0010209]
		[batch 20/20] avg loss: 0.020088902177000913		[learning rate: 0.0010197]
	Learning Rate: 0.0010197
	LOSS [training: 0.015141487153003902 | validation: 0.030481360119786784]
	TIME [epoch: 8.33 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020067064039170773		[learning rate: 0.0010185]
		[batch 20/20] avg loss: 0.019231700170626496		[learning rate: 0.0010172]
	Learning Rate: 0.00101723
	LOSS [training: 0.019649382104898642 | validation: 0.007116550233460105]
	TIME [epoch: 8.32 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010479004557432661		[learning rate: 0.001016]
		[batch 20/20] avg loss: 0.018035471750541106		[learning rate: 0.0010148]
	Learning Rate: 0.00101477
	LOSS [training: 0.014257238153986888 | validation: 0.028100315748081427]
	TIME [epoch: 8.33 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025857389427498056		[learning rate: 0.0010135]
		[batch 20/20] avg loss: 0.020450574251386716		[learning rate: 0.0010123]
	Learning Rate: 0.00101232
	LOSS [training: 0.02315398183944238 | validation: 0.021353933755326793]
	TIME [epoch: 8.39 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018738316390801955		[learning rate: 0.0010111]
		[batch 20/20] avg loss: 0.025772396527813386		[learning rate: 0.0010099]
	Learning Rate: 0.00100986
	LOSS [training: 0.02225535645930767 | validation: 0.04167396534763742]
	TIME [epoch: 8.32 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027227236740040316		[learning rate: 0.0010086]
		[batch 20/20] avg loss: 0.013546755419652534		[learning rate: 0.0010074]
	Learning Rate: 0.00100742
	LOSS [training: 0.020386996079846426 | validation: 0.013397241575847915]
	TIME [epoch: 8.32 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020480002286236077		[learning rate: 0.0010062]
		[batch 20/20] avg loss: 0.014354228502141455		[learning rate: 0.001005]
	Learning Rate: 0.00100498
	LOSS [training: 0.01741711539418877 | validation: 0.022367165550642682]
	TIME [epoch: 8.32 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016695977342235567		[learning rate: 0.0010038]
		[batch 20/20] avg loss: 0.021834179421342102		[learning rate: 0.0010025]
	Learning Rate: 0.00100255
	LOSS [training: 0.019265078381788835 | validation: 0.014132462339525287]
	TIME [epoch: 8.34 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025281066123025525		[learning rate: 0.0010013]
		[batch 20/20] avg loss: 0.009297220308930251		[learning rate: 0.0010001]
	Learning Rate: 0.00100012
	LOSS [training: 0.017289143215977888 | validation: 0.024681600743135516]
	TIME [epoch: 8.34 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01905987405012407		[learning rate: 0.00099891]
		[batch 20/20] avg loss: 0.01934839986348275		[learning rate: 0.0009977]
	Learning Rate: 0.0009977
	LOSS [training: 0.019204136956803407 | validation: 0.023648076447085276]
	TIME [epoch: 8.34 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01113999170381727		[learning rate: 0.00099649]
		[batch 20/20] avg loss: 0.016099665276850788		[learning rate: 0.00099528]
	Learning Rate: 0.000995285
	LOSS [training: 0.013619828490334029 | validation: 0.01533451899718105]
	TIME [epoch: 8.32 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009894160932576921		[learning rate: 0.00099408]
		[batch 20/20] avg loss: 0.005818782261887369		[learning rate: 0.00099288]
	Learning Rate: 0.000992875
	LOSS [training: 0.007856471597232146 | validation: 0.012231079021045598]
	TIME [epoch: 8.35 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010474034308372834		[learning rate: 0.00099167]
		[batch 20/20] avg loss: 0.0069811034686413365		[learning rate: 0.00099047]
	Learning Rate: 0.000990472
	LOSS [training: 0.008727568888507085 | validation: 0.018370913250718934]
	TIME [epoch: 8.35 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01996843920568582		[learning rate: 0.00098927]
		[batch 20/20] avg loss: 0.03785000518192945		[learning rate: 0.00098807]
	Learning Rate: 0.000988074
	LOSS [training: 0.02890922219380763 | validation: 0.019311417102941014]
	TIME [epoch: 8.32 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029085202082149507		[learning rate: 0.00098688]
		[batch 20/20] avg loss: 0.025518493625505262		[learning rate: 0.00098568]
	Learning Rate: 0.000985682
	LOSS [training: 0.02730184785382738 | validation: 0.03172667130265698]
	TIME [epoch: 8.32 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03856528976191502		[learning rate: 0.00098449]
		[batch 20/20] avg loss: 0.015680542757208597		[learning rate: 0.0009833]
	Learning Rate: 0.000983296
	LOSS [training: 0.027122916259561806 | validation: 0.00979854210709825]
	TIME [epoch: 8.37 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010125295257749977		[learning rate: 0.0009821]
		[batch 20/20] avg loss: 0.011448540581929375		[learning rate: 0.00098092]
	Learning Rate: 0.000980916
	LOSS [training: 0.01078691791983968 | validation: 0.011813704587643348]
	TIME [epoch: 8.36 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009085323900979157		[learning rate: 0.00097973]
		[batch 20/20] avg loss: 0.01718783727615903		[learning rate: 0.00097854]
	Learning Rate: 0.000978541
	LOSS [training: 0.013136580588569091 | validation: 0.026730070742461587]
	TIME [epoch: 8.33 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019600608207555083		[learning rate: 0.00097736]
		[batch 20/20] avg loss: 0.008874642735633808		[learning rate: 0.00097617]
	Learning Rate: 0.000976172
	LOSS [training: 0.014237625471594448 | validation: 0.02155570316112727]
	TIME [epoch: 8.33 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012849625020574231		[learning rate: 0.00097499]
		[batch 20/20] avg loss: 0.016795280650599386		[learning rate: 0.00097381]
	Learning Rate: 0.000973809
	LOSS [training: 0.014822452835586811 | validation: 0.013516532149340616]
	TIME [epoch: 8.35 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013384636835071346		[learning rate: 0.00097263]
		[batch 20/20] avg loss: 0.007288242014262319		[learning rate: 0.00097145]
	Learning Rate: 0.000971451
	LOSS [training: 0.010336439424666834 | validation: 0.012258117924519526]
	TIME [epoch: 8.37 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01714877755606422		[learning rate: 0.00097027]
		[batch 20/20] avg loss: 0.011540397648058144		[learning rate: 0.0009691]
	Learning Rate: 0.0009691
	LOSS [training: 0.014344587602061185 | validation: 0.019646794344315054]
	TIME [epoch: 8.31 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01325665909265406		[learning rate: 0.00096793]
		[batch 20/20] avg loss: 0.01627803746318665		[learning rate: 0.00096675]
	Learning Rate: 0.000966754
	LOSS [training: 0.014767348277920358 | validation: 0.009468727037892626]
	TIME [epoch: 8.32 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01496369304774298		[learning rate: 0.00096558]
		[batch 20/20] avg loss: 0.01050315561039174		[learning rate: 0.00096441]
	Learning Rate: 0.000964413
	LOSS [training: 0.01273342432906736 | validation: 0.00582297457106753]
	TIME [epoch: 8.34 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009556156687527408		[learning rate: 0.00096325]
		[batch 20/20] avg loss: 0.019775697535045527		[learning rate: 0.00096208]
	Learning Rate: 0.000962079
	LOSS [training: 0.014665927111286467 | validation: 0.023711286534223535]
	TIME [epoch: 8.32 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012220764087779733		[learning rate: 0.00096091]
		[batch 20/20] avg loss: 0.009299875467500244		[learning rate: 0.00095975]
	Learning Rate: 0.00095975
	LOSS [training: 0.01076031977763999 | validation: 0.01831360132039011]
	TIME [epoch: 8.34 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009596608875626467		[learning rate: 0.00095859]
		[batch 20/20] avg loss: 0.018532075305883418		[learning rate: 0.00095743]
	Learning Rate: 0.000957426
	LOSS [training: 0.014064342090754942 | validation: 0.021076538441769384]
	TIME [epoch: 8.35 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009907360375478015		[learning rate: 0.00095627]
		[batch 20/20] avg loss: 0.009128542725916572		[learning rate: 0.00095511]
	Learning Rate: 0.000955108
	LOSS [training: 0.009517951550697294 | validation: 0.02267600262693581]
	TIME [epoch: 8.36 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007400750701070905		[learning rate: 0.00095395]
		[batch 20/20] avg loss: 0.0173085144365749		[learning rate: 0.0009528]
	Learning Rate: 0.000952796
	LOSS [training: 0.012354632568822903 | validation: 0.006919740102340969]
	TIME [epoch: 8.33 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019832776799281774		[learning rate: 0.00095164]
		[batch 20/20] avg loss: 0.015530618708527838		[learning rate: 0.00095049]
	Learning Rate: 0.00095049
	LOSS [training: 0.008756948194228006 | validation: 0.06436314495843408]
	TIME [epoch: 8.32 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014178360622812382		[learning rate: 0.00094934]
		[batch 20/20] avg loss: 0.018161368492216147		[learning rate: 0.00094819]
	Learning Rate: 0.000948189
	LOSS [training: 0.01616986455751427 | validation: 0.0173129748647484]
	TIME [epoch: 8.35 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0104740554169948		[learning rate: 0.00094704]
		[batch 20/20] avg loss: 0.01068655706133824		[learning rate: 0.00094589]
	Learning Rate: 0.000945893
	LOSS [training: 0.010580306239166521 | validation: 0.02133290458167062]
	TIME [epoch: 8.36 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012555864221099017		[learning rate: 0.00094475]
		[batch 20/20] avg loss: 0.01986532631155492		[learning rate: 0.0009436]
	Learning Rate: 0.000943603
	LOSS [training: 0.016210595266326965 | validation: 0.005817343546529274]
	TIME [epoch: 8.32 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013652836821404854		[learning rate: 0.00094246]
		[batch 20/20] avg loss: 0.007233294969975583		[learning rate: 0.00094132]
	Learning Rate: 0.000941319
	LOSS [training: 0.010443065895690218 | validation: 0.014762332519584889]
	TIME [epoch: 8.32 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007453240987103006		[learning rate: 0.00094018]
		[batch 20/20] avg loss: 0.004993922127395046		[learning rate: 0.00093904]
	Learning Rate: 0.00093904
	LOSS [training: 0.006223581557249026 | validation: 0.0026234579890925596]
	TIME [epoch: 8.32 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011512264995625332		[learning rate: 0.0009379]
		[batch 20/20] avg loss: 0.014630100144823846		[learning rate: 0.00093677]
	Learning Rate: 0.000936767
	LOSS [training: 0.013071182570224587 | validation: 0.012446569664844087]
	TIME [epoch: 8.33 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007225773507143603		[learning rate: 0.00093563]
		[batch 20/20] avg loss: 0.008254732543002902		[learning rate: 0.0009345]
	Learning Rate: 0.000934499
	LOSS [training: 0.007740253025073253 | validation: 0.015569518581800572]
	TIME [epoch: 8.33 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014574767558894364		[learning rate: 0.00093337]
		[batch 20/20] avg loss: 0.014068610281045838		[learning rate: 0.00093224]
	Learning Rate: 0.000932237
	LOSS [training: 0.0143216889199701 | validation: 0.03138782945381812]
	TIME [epoch: 8.35 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008546572037341969		[learning rate: 0.00093111]
		[batch 20/20] avg loss: 0.009759756875466933		[learning rate: 0.00092998]
	Learning Rate: 0.00092998
	LOSS [training: 0.00915316445640445 | validation: 0.019337824038376677]
	TIME [epoch: 8.35 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009356975521601297		[learning rate: 0.00092885]
		[batch 20/20] avg loss: 0.002161596716019865		[learning rate: 0.00092773]
	Learning Rate: 0.000927729
	LOSS [training: 0.005759286118810582 | validation: 0.01061111088456609]
	TIME [epoch: 8.35 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009912193200612538		[learning rate: 0.00092661]
		[batch 20/20] avg loss: 0.009315979599347855		[learning rate: 0.00092548]
	Learning Rate: 0.000925483
	LOSS [training: 0.009614086399980199 | validation: 0.010801849897668435]
	TIME [epoch: 8.32 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01725509943885168		[learning rate: 0.00092436]
		[batch 20/20] avg loss: 0.02661658626503947		[learning rate: 0.00092324]
	Learning Rate: 0.000923243
	LOSS [training: 0.021935842851945577 | validation: 0.027440859026944184]
	TIME [epoch: 8.32 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016071381578890966		[learning rate: 0.00092212]
		[batch 20/20] avg loss: 0.01273327160725354		[learning rate: 0.00092101]
	Learning Rate: 0.000921008
	LOSS [training: 0.014402326593072255 | validation: 0.012006061675426793]
	TIME [epoch: 8.35 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014737194387566549		[learning rate: 0.00091989]
		[batch 20/20] avg loss: 0.01739122736252308		[learning rate: 0.00091878]
	Learning Rate: 0.000918778
	LOSS [training: 0.016064210875044817 | validation: 0.017190211495476036]
	TIME [epoch: 8.35 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007223941466560043		[learning rate: 0.00091767]
		[batch 20/20] avg loss: 0.0177198613687381		[learning rate: 0.00091655]
	Learning Rate: 0.000916554
	LOSS [training: 0.01247190141764907 | validation: 0.03375510310367941]
	TIME [epoch: 8.32 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023524610861421054		[learning rate: 0.00091544]
		[batch 20/20] avg loss: 0.004559445380823625		[learning rate: 0.00091433]
	Learning Rate: 0.000914335
	LOSS [training: 0.014042028121122336 | validation: 0.006454025324489007]
	TIME [epoch: 8.31 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01133304563946071		[learning rate: 0.00091323]
		[batch 20/20] avg loss: 0.004948381730007369		[learning rate: 0.00091212]
	Learning Rate: 0.000912121
	LOSS [training: 0.008140713684734038 | validation: 0.007665690778782163]
	TIME [epoch: 8.36 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013351592195315004		[learning rate: 0.00091102]
		[batch 20/20] avg loss: 0.013193857684654609		[learning rate: 0.00090991]
	Learning Rate: 0.000909913
	LOSS [training: 0.013272724939984807 | validation: 0.01824480549059757]
	TIME [epoch: 8.34 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01074421764279907		[learning rate: 0.00090881]
		[batch 20/20] avg loss: 0.005486552762654604		[learning rate: 0.00090771]
	Learning Rate: 0.00090771
	LOSS [training: 0.008115385202726836 | validation: 0.01979612360965778]
	TIME [epoch: 8.33 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008548341818394697		[learning rate: 0.00090661]
		[batch 20/20] avg loss: 0.00676352178410769		[learning rate: 0.00090551]
	Learning Rate: 0.000905513
	LOSS [training: 0.007655931801251193 | validation: 0.004423127129663558]
	TIME [epoch: 8.32 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003763739548245898		[learning rate: 0.00090442]
		[batch 20/20] avg loss: 0.01602466085721176		[learning rate: 0.00090332]
	Learning Rate: 0.000903321
	LOSS [training: 0.009894200202728829 | validation: 0.020749044677903505]
	TIME [epoch: 8.36 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011979615763932226		[learning rate: 0.00090223]
		[batch 20/20] avg loss: 0.009412713545527546		[learning rate: 0.00090113]
	Learning Rate: 0.000901134
	LOSS [training: 0.010696164654729884 | validation: 0.012077240382117632]
	TIME [epoch: 8.34 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010479029295449693		[learning rate: 0.00090004]
		[batch 20/20] avg loss: 0.007928971069002673		[learning rate: 0.00089895]
	Learning Rate: 0.000898953
	LOSS [training: 0.009204000182226184 | validation: 0.007828801991505705]
	TIME [epoch: 8.32 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005529722197457153		[learning rate: 0.00089786]
		[batch 20/20] avg loss: 0.005580169420394491		[learning rate: 0.00089678]
	Learning Rate: 0.000896777
	LOSS [training: 0.0055549458089258225 | validation: 0.02302395522151928]
	TIME [epoch: 8.31 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021073971921458535		[learning rate: 0.00089569]
		[batch 20/20] avg loss: 0.001936399659213664		[learning rate: 0.00089461]
	Learning Rate: 0.000894605
	LOSS [training: 0.0115051857903361 | validation: 0.0076125245513331]
	TIME [epoch: 8.31 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00884007166066257		[learning rate: 0.00089352]
		[batch 20/20] avg loss: 0.0026807702004631576		[learning rate: 0.00089244]
	Learning Rate: 0.00089244
	LOSS [training: 0.005760420930562864 | validation: 0.006281640111796223]
	TIME [epoch: 8.34 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005730047747373783		[learning rate: 0.00089136]
		[batch 20/20] avg loss: 0.014336301903839027		[learning rate: 0.00089028]
	Learning Rate: 0.000890279
	LOSS [training: 0.010033174825606405 | validation: 0.017690555961027123]
	TIME [epoch: 8.31 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01558860634666113		[learning rate: 0.0008892]
		[batch 20/20] avg loss: 0.00789871521637937		[learning rate: 0.00088812]
	Learning Rate: 0.000888124
	LOSS [training: 0.011743660781520249 | validation: 0.01025073901426415]
	TIME [epoch: 8.34 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0061720604559455826		[learning rate: 0.00088705]
		[batch 20/20] avg loss: 0.008077364525243652		[learning rate: 0.00088597]
	Learning Rate: 0.000885974
	LOSS [training: 0.007124712490594616 | validation: 0.011203838265611374]
	TIME [epoch: 8.35 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005506592229564396		[learning rate: 0.0008849]
		[batch 20/20] avg loss: 0.011950629953015043		[learning rate: 0.00088383]
	Learning Rate: 0.000883829
	LOSS [training: 0.00872861109128972 | validation: 0.006095679040062109]
	TIME [epoch: 8.35 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006779836087726453		[learning rate: 0.00088276]
		[batch 20/20] avg loss: 0.004465033991837964		[learning rate: 0.00088169]
	Learning Rate: 0.00088169
	LOSS [training: 0.005622435039782209 | validation: 0.03543680325565473]
	TIME [epoch: 8.32 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019820739972993502		[learning rate: 0.00088062]
		[batch 20/20] avg loss: 0.010301832629503602		[learning rate: 0.00087956]
	Learning Rate: 0.000879555
	LOSS [training: 0.015061286301248552 | validation: 0.013579068988174646]
	TIME [epoch: 8.32 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02451507022749687		[learning rate: 0.00087849]
		[batch 20/20] avg loss: 0.010793371311037987		[learning rate: 0.00087743]
	Learning Rate: 0.000877426
	LOSS [training: 0.017654220769267433 | validation: 0.01929061743586953]
	TIME [epoch: 8.36 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01513175905234001		[learning rate: 0.00087636]
		[batch 20/20] avg loss: 0.01587090796875245		[learning rate: 0.0008753]
	Learning Rate: 0.000875302
	LOSS [training: 0.015501333510546231 | validation: 0.018772727311880046]
	TIME [epoch: 8.34 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0064617431723252225		[learning rate: 0.00087424]
		[batch 20/20] avg loss: 0.018399125358466058		[learning rate: 0.00087318]
	Learning Rate: 0.000873183
	LOSS [training: 0.01243043426539564 | validation: 0.03491141136970527]
	TIME [epoch: 8.32 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018387036383607995		[learning rate: 0.00087213]
		[batch 20/20] avg loss: 0.012377365535066423		[learning rate: 0.00087107]
	Learning Rate: 0.000871069
	LOSS [training: 0.015382200959337206 | validation: 0.007927842065095446]
	TIME [epoch: 8.31 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008807995070016826		[learning rate: 0.00087001]
		[batch 20/20] avg loss: 0.016600900151962812		[learning rate: 0.00086896]
	Learning Rate: 0.00086896
	LOSS [training: 0.012704447610989818 | validation: 0.0190909770437473]
	TIME [epoch: 8.31 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019519518594988954		[learning rate: 0.00086791]
		[batch 20/20] avg loss: 0.01820526934671402		[learning rate: 0.00086686]
	Learning Rate: 0.000866857
	LOSS [training: 0.018862393970851495 | validation: 0.014014588213974551]
	TIME [epoch: 8.34 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011467316147460127		[learning rate: 0.00086581]
		[batch 20/20] avg loss: 0.005545343376655977		[learning rate: 0.00086476]
	Learning Rate: 0.000864758
	LOSS [training: 0.008506329762058052 | validation: 0.023696091215106845]
	TIME [epoch: 8.33 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016449045836141363		[learning rate: 0.00086371]
		[batch 20/20] avg loss: 0.010706404810338745		[learning rate: 0.00086266]
	Learning Rate: 0.000862665
	LOSS [training: 0.013577725323240053 | validation: 0.014353967810347156]
	TIME [epoch: 8.34 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009560147487800708		[learning rate: 0.00086162]
		[batch 20/20] avg loss: 0.010312308496840127		[learning rate: 0.00086058]
	Learning Rate: 0.000860577
	LOSS [training: 0.009936227992320421 | validation: 0.016754052445470395]
	TIME [epoch: 8.32 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013721709585350886		[learning rate: 0.00085953]
		[batch 20/20] avg loss: 0.017921490825609756		[learning rate: 0.00085849]
	Learning Rate: 0.000858493
	LOSS [training: 0.015821600205480322 | validation: 0.004472189113683699]
	TIME [epoch: 8.34 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037172485842552234		[learning rate: 0.00085745]
		[batch 20/20] avg loss: 0.007209585474698796		[learning rate: 0.00085641]
	Learning Rate: 0.000856415
	LOSS [training: 0.005463417029477009 | validation: 0.017623346565657443]
	TIME [epoch: 8.34 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013119287073891145		[learning rate: 0.00085538]
		[batch 20/20] avg loss: 0.006703635530581338		[learning rate: 0.00085434]
	Learning Rate: 0.000854342
	LOSS [training: 0.009911461302236241 | validation: 0.00944576998019402]
	TIME [epoch: 8.34 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013547922013358197		[learning rate: 0.00085331]
		[batch 20/20] avg loss: 0.015846121399527104		[learning rate: 0.00085227]
	Learning Rate: 0.000852273
	LOSS [training: 0.014697021706442654 | validation: 0.028982293389290936]
	TIME [epoch: 8.31 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008263813572256749		[learning rate: 0.00085124]
		[batch 20/20] avg loss: 0.025091458055191425		[learning rate: 0.00085021]
	Learning Rate: 0.00085021
	LOSS [training: 0.016677635813724088 | validation: 0.012809717719821532]
	TIME [epoch: 8.34 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009120508347843585		[learning rate: 0.00084918]
		[batch 20/20] avg loss: 0.006056954450016231		[learning rate: 0.00084815]
	Learning Rate: 0.000848152
	LOSS [training: 0.007588731398929907 | validation: 0.004856726673315936]
	TIME [epoch: 8.32 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007883884659134185		[learning rate: 0.00084712]
		[batch 20/20] avg loss: 0.001196116970520791		[learning rate: 0.0008461]
	Learning Rate: 0.000846099
	LOSS [training: 0.004540000814827487 | validation: 0.011522735150015173]
	TIME [epoch: 8.31 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009732510144445984		[learning rate: 0.00084507]
		[batch 20/20] avg loss: 0.003003841528792077		[learning rate: 0.00084405]
	Learning Rate: 0.000844051
	LOSS [training: 0.00636817583661903 | validation: 0.021832999426176225]
	TIME [epoch: 8.32 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015940304863430125		[learning rate: 0.00084303]
		[batch 20/20] avg loss: 0.005790557868210306		[learning rate: 0.00084201]
	Learning Rate: 0.000842007
	LOSS [training: 0.010865431365820216 | validation: 0.013515503688568044]
	TIME [epoch: 8.37 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0043913966680821		[learning rate: 0.00084099]
		[batch 20/20] avg loss: 0.012729559502711615		[learning rate: 0.00083997]
	Learning Rate: 0.000839969
	LOSS [training: 0.008560478085396858 | validation: 0.005168765174766627]
	TIME [epoch: 8.32 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003349108322535717		[learning rate: 0.00083895]
		[batch 20/20] avg loss: 0.010736806405537784		[learning rate: 0.00083794]
	Learning Rate: 0.000837935
	LOSS [training: 0.00704295736403675 | validation: 0.0068825602436470255]
	TIME [epoch: 8.32 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00842085000391266		[learning rate: 0.00083692]
		[batch 20/20] avg loss: 0.01679072067289599		[learning rate: 0.00083591]
	Learning Rate: 0.000835907
	LOSS [training: 0.01260578533840433 | validation: 0.013165063260016523]
	TIME [epoch: 8.34 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010609737416493714		[learning rate: 0.00083489]
		[batch 20/20] avg loss: 0.009416727171820082		[learning rate: 0.00083388]
	Learning Rate: 0.000833883
	LOSS [training: 0.010013232294156897 | validation: 0.009561648782386297]
	TIME [epoch: 8.36 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018219017850217311		[learning rate: 0.00083287]
		[batch 20/20] avg loss: 0.007549526055575129		[learning rate: 0.00083186]
	Learning Rate: 0.000831865
	LOSS [training: 0.0046857139202984305 | validation: 0.024612805834663236]
	TIME [epoch: 8.32 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011538241009667598		[learning rate: 0.00083086]
		[batch 20/20] avg loss: 0.00918519428697601		[learning rate: 0.00082985]
	Learning Rate: 0.000829851
	LOSS [training: 0.010361717648321802 | validation: 0.016791583499950642]
	TIME [epoch: 8.31 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014761529392535546		[learning rate: 0.00082885]
		[batch 20/20] avg loss: 0.007410250005425738		[learning rate: 0.00082784]
	Learning Rate: 0.000827842
	LOSS [training: 0.01108588969898064 | validation: 0.012309971285972322]
	TIME [epoch: 8.31 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010303520693241986		[learning rate: 0.00082684]
		[batch 20/20] avg loss: 0.002630642462576329		[learning rate: 0.00082584]
	Learning Rate: 0.000825838
	LOSS [training: 0.006467081577909156 | validation: 0.007627511936591941]
	TIME [epoch: 8.34 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0044420863705037965		[learning rate: 0.00082484]
		[batch 20/20] avg loss: 0.0177769627803366		[learning rate: 0.00082384]
	Learning Rate: 0.000823839
	LOSS [training: 0.011109524575420197 | validation: 0.012496011494054396]
	TIME [epoch: 8.32 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009905884854942856		[learning rate: 0.00082284]
		[batch 20/20] avg loss: 0.005941161918759971		[learning rate: 0.00082184]
	Learning Rate: 0.000821844
	LOSS [training: 0.007923523386851414 | validation: 0.00658265228218482]
	TIME [epoch: 8.31 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013610384945423848		[learning rate: 0.00082085]
		[batch 20/20] avg loss: 0.01796414735412424		[learning rate: 0.00081985]
	Learning Rate: 0.000819855
	LOSS [training: 0.01578726614977404 | validation: 0.03152813741987482]
	TIME [epoch: 8.34 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00762675194039594		[learning rate: 0.00081886]
		[batch 20/20] avg loss: 0.00645081638147899		[learning rate: 0.00081787]
	Learning Rate: 0.00081787
	LOSS [training: 0.007038784160937466 | validation: 0.007310022253723985]
	TIME [epoch: 8.35 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028226910667101352		[learning rate: 0.00081688]
		[batch 20/20] avg loss: 0.01187940934215376		[learning rate: 0.00081589]
	Learning Rate: 0.00081589
	LOSS [training: 0.007351050204431947 | validation: 0.015476412989599819]
	TIME [epoch: 8.33 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006595478158068749		[learning rate: 0.0008149]
		[batch 20/20] avg loss: 0.0037954317717353893		[learning rate: 0.00081391]
	Learning Rate: 0.000813915
	LOSS [training: 0.005195454964902069 | validation: 0.013704904238604635]
	TIME [epoch: 8.33 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010966357500443101		[learning rate: 0.00081293]
		[batch 20/20] avg loss: 0.0069492105612314874		[learning rate: 0.00081194]
	Learning Rate: 0.000811944
	LOSS [training: 0.008957784030837293 | validation: 0.017520780505308114]
	TIME [epoch: 8.35 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005497905213717514		[learning rate: 0.00081096]
		[batch 20/20] avg loss: 0.0030393321932259934		[learning rate: 0.00080998]
	Learning Rate: 0.000809979
	LOSS [training: 0.004268618703471755 | validation: 0.0005508019691531089]
	TIME [epoch: 8.34 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032099425913344574		[learning rate: 0.000809]
		[batch 20/20] avg loss: 0.013508580207608863		[learning rate: 0.00080802]
	Learning Rate: 0.000808018
	LOSS [training: 0.005149318808137203 | validation: 0.0012565940018648657]
	TIME [epoch: 8.31 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01009140064499274		[learning rate: 0.00080704]
		[batch 20/20] avg loss: 0.00048479253665396227		[learning rate: 0.00080606]
	Learning Rate: 0.000806062
	LOSS [training: 0.00528809659082335 | validation: 0.014172131845466803]
	TIME [epoch: 8.31 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038647247211108043		[learning rate: 0.00080509]
		[batch 20/20] avg loss: 0.001582339036044524		[learning rate: 0.00080411]
	Learning Rate: 0.000804111
	LOSS [training: 0.002723531878577664 | validation: 0.02179349746776629]
	TIME [epoch: 8.32 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02079996017487629		[learning rate: 0.00080314]
		[batch 20/20] avg loss: 0.022160208161635386		[learning rate: 0.00080216]
	Learning Rate: 0.000802164
	LOSS [training: 0.02148008416825584 | validation: 0.0034185538436343347]
	TIME [epoch: 8.34 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021693597515836786		[learning rate: 0.00080119]
		[batch 20/20] avg loss: 0.007449803479196095		[learning rate: 0.00080022]
	Learning Rate: 0.000800222
	LOSS [training: 0.004809581615389887 | validation: 0.012387239560522834]
	TIME [epoch: 8.32 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011281967417799129		[learning rate: 0.00079925]
		[batch 20/20] avg loss: 0.00936189495026725		[learning rate: 0.00079828]
	Learning Rate: 0.000798285
	LOSS [training: 0.010321931184033187 | validation: 0.015495310537342544]
	TIME [epoch: 8.31 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005832083277597366		[learning rate: 0.00079732]
		[batch 20/20] avg loss: -0.003115476777809755		[learning rate: 0.00079635]
	Learning Rate: 0.000796352
	LOSS [training: 0.0013583032498938048 | validation: 0.0015694449032495126]
	TIME [epoch: 8.34 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007941128321318054		[learning rate: 0.00079539]
		[batch 20/20] avg loss: 0.005590933851280077		[learning rate: 0.00079442]
	Learning Rate: 0.000794424
	LOSS [training: 0.0031925233417059414 | validation: 0.014079468677489607]
	TIME [epoch: 8.36 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004601580717036518		[learning rate: 0.00079346]
		[batch 20/20] avg loss: 0.006174060986855294		[learning rate: 0.0007925]
	Learning Rate: 0.000792501
	LOSS [training: 0.005387820851945906 | validation: 0.013667226829309178]
	TIME [epoch: 8.33 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009202760809152156		[learning rate: 0.00079154]
		[batch 20/20] avg loss: 0.005774051178804602		[learning rate: 0.00079058]
	Learning Rate: 0.000790583
	LOSS [training: 0.007488405993978378 | validation: 0.011794678876730632]
	TIME [epoch: 8.33 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028360236912229734		[learning rate: 0.00078963]
		[batch 20/20] avg loss: 0.007024890553100278		[learning rate: 0.00078867]
	Learning Rate: 0.000788669
	LOSS [training: 0.004930457122161625 | validation: 0.007250239313546791]
	TIME [epoch: 8.35 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031265681044338593		[learning rate: 0.00078771]
		[batch 20/20] avg loss: 0.012729125802037883		[learning rate: 0.00078676]
	Learning Rate: 0.00078676
	LOSS [training: 0.007927846953235871 | validation: 0.001322610039768839]
	TIME [epoch: 8.36 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00666489541060441		[learning rate: 0.00078581]
		[batch 20/20] avg loss: 0.012862495952441256		[learning rate: 0.00078486]
	Learning Rate: 0.000784855
	LOSS [training: 0.009763695681522832 | validation: 0.016585812606531414]
	TIME [epoch: 8.32 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009949767975960137		[learning rate: 0.0007839]
		[batch 20/20] avg loss: 0.03434509328657974		[learning rate: 0.00078296]
	Learning Rate: 0.000782955
	LOSS [training: 0.022147430631269942 | validation: 0.038917114375479483]
	TIME [epoch: 8.32 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01455957402717951		[learning rate: 0.00078201]
		[batch 20/20] avg loss: 0.01927156561418338		[learning rate: 0.00078106]
	Learning Rate: 0.00078106
	LOSS [training: 0.016915569820681448 | validation: 0.022427899749080063]
	TIME [epoch: 8.31 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016650524094101348		[learning rate: 0.00078011]
		[batch 20/20] avg loss: 0.00981191174754506		[learning rate: 0.00077917]
	Learning Rate: 0.000779169
	LOSS [training: 0.013231217920823202 | validation: 0.00952117043609814]
	TIME [epoch: 8.34 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010184217710057456		[learning rate: 0.00077823]
		[batch 20/20] avg loss: 0.003183189981236393		[learning rate: 0.00077728]
	Learning Rate: 0.000777283
	LOSS [training: 0.006683703845646924 | validation: 0.01086418874544231]
	TIME [epoch: 8.32 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014131792579844274		[learning rate: 0.00077634]
		[batch 20/20] avg loss: 0.0037468187349726004		[learning rate: 0.0007754]
	Learning Rate: 0.000775401
	LOSS [training: 0.008939305657408438 | validation: 0.015174967909487441]
	TIME [epoch: 8.32 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009730135130741962		[learning rate: 0.00077446]
		[batch 20/20] avg loss: -0.0014186052731491065		[learning rate: 0.00077352]
	Learning Rate: 0.000773524
	LOSS [training: 0.004155764928796427 | validation: 0.015620934699899898]
	TIME [epoch: 8.32 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0064576714690504885		[learning rate: 0.00077259]
		[batch 20/20] avg loss: 0.0029966597004380015		[learning rate: 0.00077165]
	Learning Rate: 0.000771651
	LOSS [training: 0.004727165584744245 | validation: 0.004787843291862214]
	TIME [epoch: 8.34 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014899786339741003		[learning rate: 0.00077072]
		[batch 20/20] avg loss: 0.02351103477952778		[learning rate: 0.00076978]
	Learning Rate: 0.000769783
	LOSS [training: 0.019205410559634394 | validation: 0.022573510638477654]
	TIME [epoch: 8.32 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010423182377351025		[learning rate: 0.00076885]
		[batch 20/20] avg loss: 0.019274081888537097		[learning rate: 0.00076792]
	Learning Rate: 0.00076792
	LOSS [training: 0.014848632132944063 | validation: 0.021643277416501957]
	TIME [epoch: 8.32 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018959818131363482		[learning rate: 0.00076699]
		[batch 20/20] avg loss: 0.011447467046166912		[learning rate: 0.00076606]
	Learning Rate: 0.000766061
	LOSS [training: 0.015203642588765196 | validation: 0.010575307861708256]
	TIME [epoch: 8.32 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009003006016477768		[learning rate: 0.00076513]
		[batch 20/20] avg loss: 0.009375519963875345		[learning rate: 0.00076421]
	Learning Rate: 0.000764206
	LOSS [training: 0.009189262990176557 | validation: 0.007461020389530782]
	TIME [epoch: 8.34 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023603922703337747		[learning rate: 0.00076328]
		[batch 20/20] avg loss: 0.010719674937570013		[learning rate: 0.00076236]
	Learning Rate: 0.000762356
	LOSS [training: 0.006540033603951892 | validation: 0.012014120047379832]
	TIME [epoch: 8.32 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006464395454767878		[learning rate: 0.00076143]
		[batch 20/20] avg loss: 0.0002594411045623277		[learning rate: 0.00076051]
	Learning Rate: 0.000760511
	LOSS [training: 0.003361918279665103 | validation: 0.008000408153041568]
	TIME [epoch: 8.32 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007364783907300462		[learning rate: 0.00075959]
		[batch 20/20] avg loss: 0.022249128801893118		[learning rate: 0.00075867]
	Learning Rate: 0.00075867
	LOSS [training: 0.014806956354596789 | validation: 0.02031617757068359]
	TIME [epoch: 8.34 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014065545789817427		[learning rate: 0.00075775]
		[batch 20/20] avg loss: 0.0136414424280287		[learning rate: 0.00075683]
	Learning Rate: 0.000756833
	LOSS [training: 0.013853494108923062 | validation: 0.007345238482575921]
	TIME [epoch: 8.35 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012993637645262032		[learning rate: 0.00075592]
		[batch 20/20] avg loss: 0.004049243439530411		[learning rate: 0.000755]
	Learning Rate: 0.000755001
	LOSS [training: 0.008521440542396222 | validation: 0.00579686986333167]
	TIME [epoch: 8.33 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012034262478598422		[learning rate: 0.00075409]
		[batch 20/20] avg loss: 0.01674266968085289		[learning rate: 0.00075317]
	Learning Rate: 0.000753173
	LOSS [training: 0.014388466079725656 | validation: 0.025216711611554338]
	TIME [epoch: 8.34 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021857747769989257		[learning rate: 0.00075226]
		[batch 20/20] avg loss: 0.014578514838831621		[learning rate: 0.00075135]
	Learning Rate: 0.00075135
	LOSS [training: 0.01821813130441044 | validation: 0.021906655459762663]
	TIME [epoch: 8.34 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016927124658622582		[learning rate: 0.00075044]
		[batch 20/20] avg loss: 0.01605923373736467		[learning rate: 0.00074953]
	Learning Rate: 0.000749531
	LOSS [training: 0.01649317919799362 | validation: 0.011218317186791489]
	TIME [epoch: 8.34 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009947041103093818		[learning rate: 0.00074862]
		[batch 20/20] avg loss: 0.008123426719644446		[learning rate: 0.00074772]
	Learning Rate: 0.000747716
	LOSS [training: 0.009035233911369133 | validation: 0.009322981945768676]
	TIME [epoch: 8.32 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0066625337293311476		[learning rate: 0.00074681]
		[batch 20/20] avg loss: 0.005237563654220864		[learning rate: 0.00074591]
	Learning Rate: 0.000745906
	LOSS [training: 0.0059500486917760075 | validation: 0.004553649898948442]
	TIME [epoch: 8.31 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00964514185485046		[learning rate: 0.000745]
		[batch 20/20] avg loss: 0.005915535345115884		[learning rate: 0.0007441]
	Learning Rate: 0.0007441
	LOSS [training: 0.007780338599983172 | validation: 0.020546868229610396]
	TIME [epoch: 8.32 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009937932931900617		[learning rate: 0.0007432]
		[batch 20/20] avg loss: 0.007583942470980628		[learning rate: 0.0007423]
	Learning Rate: 0.000742299
	LOSS [training: 0.00876093770144062 | validation: 0.0131384882034685]
	TIME [epoch: 8.36 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0054016459827858024		[learning rate: 0.0007414]
		[batch 20/20] avg loss: 0.005616632161702411		[learning rate: 0.0007405]
	Learning Rate: 0.000740502
	LOSS [training: 0.005509139072244105 | validation: 0.0014899075454122032]
	TIME [epoch: 8.31 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003232118222699419		[learning rate: 0.00073961]
		[batch 20/20] avg loss: 0.0035852135378332886		[learning rate: 0.00073871]
	Learning Rate: 0.000738709
	LOSS [training: 0.0034086658802663537 | validation: 0.009598051237168205]
	TIME [epoch: 8.35 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007325979110326321		[learning rate: 0.00073781]
		[batch 20/20] avg loss: 0.013592433567854514		[learning rate: 0.00073692]
	Learning Rate: 0.000736921
	LOSS [training: 0.010459206339090418 | validation: 0.003827146802546733]
	TIME [epoch: 8.32 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010528392890544778		[learning rate: 0.00073603]
		[batch 20/20] avg loss: 0.009119196670061789		[learning rate: 0.00073514]
	Learning Rate: 0.000735137
	LOSS [training: 0.009823794780303284 | validation: -0.0020846948204057553]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_1177.pth
	Model improved!!!
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016608308250522694		[learning rate: 0.00073425]
		[batch 20/20] avg loss: 0.012002261563749134		[learning rate: 0.00073336]
	Learning Rate: 0.000733358
	LOSS [training: 0.0068315461944007 | validation: 0.010359841545568647]
	TIME [epoch: 8.32 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005189364655838561		[learning rate: 0.00073247]
		[batch 20/20] avg loss: 0.002675930158802785		[learning rate: 0.00073158]
	Learning Rate: 0.000731582
	LOSS [training: 0.003932647407320673 | validation: 0.004814087097141602]
	TIME [epoch: 8.36 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00969892085126076		[learning rate: 0.0007307]
		[batch 20/20] avg loss: 0.0031928442572138117		[learning rate: 0.00072981]
	Learning Rate: 0.000729811
	LOSS [training: 0.006445882554237285 | validation: 0.0006112563241809951]
	TIME [epoch: 8.31 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009915140658853053		[learning rate: 0.00072893]
		[batch 20/20] avg loss: 0.009750749515594593		[learning rate: 0.00072804]
	Learning Rate: 0.000728044
	LOSS [training: 0.009832945087223821 | validation: 0.007269374667132383]
	TIME [epoch: 8.33 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005657150985516615		[learning rate: 0.00072716]
		[batch 20/20] avg loss: 0.002040260776765322		[learning rate: 0.00072628]
	Learning Rate: 0.000726282
	LOSS [training: 0.003848705881140969 | validation: 0.006226963605813628]
	TIME [epoch: 8.31 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005702447236996345		[learning rate: 0.0007254]
		[batch 20/20] avg loss: 0.012336815896423239		[learning rate: 0.00072452]
	Learning Rate: 0.000724524
	LOSS [training: 0.009019631566709792 | validation: 0.011087041923723277]
	TIME [epoch: 8.32 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01469096800587063		[learning rate: 0.00072365]
		[batch 20/20] avg loss: 0.014544526490463516		[learning rate: 0.00072277]
	Learning Rate: 0.00072277
	LOSS [training: 0.014617747248167074 | validation: 0.031558546570191026]
	TIME [epoch: 8.33 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01593666668029173		[learning rate: 0.00072189]
		[batch 20/20] avg loss: 0.006670802814085143		[learning rate: 0.00072102]
	Learning Rate: 0.00072102
	LOSS [training: 0.011303734747188435 | validation: 0.012872752144955313]
	TIME [epoch: 8.33 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011696077221856047		[learning rate: 0.00072015]
		[batch 20/20] avg loss: 0.010443014555552698		[learning rate: 0.00071927]
	Learning Rate: 0.000719275
	LOSS [training: 0.011069545888704376 | validation: 0.007387831370951198]
	TIME [epoch: 8.34 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013296299687981492		[learning rate: 0.0007184]
		[batch 20/20] avg loss: 0.01174817890154255		[learning rate: 0.00071753]
	Learning Rate: 0.000717533
	LOSS [training: 0.012522239294762021 | validation: 0.011476119723365237]
	TIME [epoch: 8.32 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011254663879728665		[learning rate: 0.00071666]
		[batch 20/20] avg loss: 0.014229556773868304		[learning rate: 0.0007158]
	Learning Rate: 0.000715796
	LOSS [training: 0.012742110326798489 | validation: 0.014545794171260345]
	TIME [epoch: 8.33 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009279321151808233		[learning rate: 0.00071493]
		[batch 20/20] avg loss: 0.01298177292515782		[learning rate: 0.00071406]
	Learning Rate: 0.000714064
	LOSS [training: 0.011130547038483027 | validation: 0.019602215021135534]
	TIME [epoch: 8.35 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009225193946426437		[learning rate: 0.0007132]
		[batch 20/20] avg loss: 0.012724349702276077		[learning rate: 0.00071233]
	Learning Rate: 0.000712335
	LOSS [training: 0.010974771824351257 | validation: 0.004661046121482862]
	TIME [epoch: 8.34 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011617066248410036		[learning rate: 0.00071147]
		[batch 20/20] avg loss: 0.007213125864710006		[learning rate: 0.00071061]
	Learning Rate: 0.00071061
	LOSS [training: 0.009415096056560022 | validation: 0.025617588980922312]
	TIME [epoch: 8.31 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012655991636578054		[learning rate: 0.00070975]
		[batch 20/20] avg loss: 0.004951528111232695		[learning rate: 0.00070889]
	Learning Rate: 0.00070889
	LOSS [training: 0.008803759873905373 | validation: 0.013244895954512854]
	TIME [epoch: 8.33 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00814671262971017		[learning rate: 0.00070803]
		[batch 20/20] avg loss: 0.013349825161049243		[learning rate: 0.00070717]
	Learning Rate: 0.000707174
	LOSS [training: 0.010748268895379706 | validation: 0.01028318755677339]
	TIME [epoch: 8.32 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006387370165924657		[learning rate: 0.00070632]
		[batch 20/20] avg loss: 0.004431710019115075		[learning rate: 0.00070546]
	Learning Rate: 0.000705462
	LOSS [training: 0.005409540092519867 | validation: 0.0022964580119941283]
	TIME [epoch: 8.31 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011057816516411058		[learning rate: 0.00070461]
		[batch 20/20] avg loss: 0.014733865678003699		[learning rate: 0.00070375]
	Learning Rate: 0.000703754
	LOSS [training: 0.012895841097207381 | validation: 0.017177203083952007]
	TIME [epoch: 8.31 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007907945046834137		[learning rate: 0.0007029]
		[batch 20/20] avg loss: 0.002332361078284085		[learning rate: 0.00070205]
	Learning Rate: 0.000702051
	LOSS [training: 0.005120153062559111 | validation: -0.0005022134431279107]
	TIME [epoch: 8.35 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037816627977320704		[learning rate: 0.0007012]
		[batch 20/20] avg loss: 0.0026696293311877285		[learning rate: 0.00070035]
	Learning Rate: 0.000700351
	LOSS [training: 0.0032256460644598988 | validation: 0.011050418007081168]
	TIME [epoch: 8.34 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00015128538566569727		[learning rate: 0.0006995]
		[batch 20/20] avg loss: 0.01259806407852341		[learning rate: 0.00069866]
	Learning Rate: 0.000698656
	LOSS [training: 0.006223389346428855 | validation: 0.030341667630837104]
	TIME [epoch: 8.32 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003948986355541787		[learning rate: 0.00069781]
		[batch 20/20] avg loss: 0.003578058519739815		[learning rate: 0.00069696]
	Learning Rate: 0.000696964
	LOSS [training: 0.0037635224376408013 | validation: 0.008733383026710562]
	TIME [epoch: 8.32 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010072247129633036		[learning rate: 0.00069612]
		[batch 20/20] avg loss: 0.015527196430346932		[learning rate: 0.00069528]
	Learning Rate: 0.000695277
	LOSS [training: 0.012799721779989984 | validation: 0.04272739575016109]
	TIME [epoch: 8.37 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015266752335619003		[learning rate: 0.00069444]
		[batch 20/20] avg loss: 0.00822538791202538		[learning rate: 0.00069359]
	Learning Rate: 0.000693594
	LOSS [training: 0.011746070123822193 | validation: 0.009612265718267553]
	TIME [epoch: 8.34 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007394215070146847		[learning rate: 0.00069275]
		[batch 20/20] avg loss: 0.0005325443539417208		[learning rate: 0.00069191]
	Learning Rate: 0.000691915
	LOSS [training: 0.003963379712044284 | validation: 0.012011967840116614]
	TIME [epoch: 8.31 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011396234569626426		[learning rate: 0.00069108]
		[batch 20/20] avg loss: 0.0073867672846583575		[learning rate: 0.00069024]
	Learning Rate: 0.00069024
	LOSS [training: 0.009391500927142391 | validation: 0.011173605934937679]
	TIME [epoch: 8.31 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0078060208151371395		[learning rate: 0.0006894]
		[batch 20/20] avg loss: 0.004809069390868521		[learning rate: 0.00068857]
	Learning Rate: 0.000688569
	LOSS [training: 0.0063075451030028306 | validation: 0.024538073041125902]
	TIME [epoch: 8.33 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013612206080592933		[learning rate: 0.00068773]
		[batch 20/20] avg loss: 0.002614335325711656		[learning rate: 0.0006869]
	Learning Rate: 0.000686902
	LOSS [training: 0.008113270703152296 | validation: 0.006168670850531334]
	TIME [epoch: 8.32 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006935645776363644		[learning rate: 0.00068607]
		[batch 20/20] avg loss: 0.010943183907141992		[learning rate: 0.00068524]
	Learning Rate: 0.000685239
	LOSS [training: 0.008939414841752818 | validation: 0.007392469660174723]
	TIME [epoch: 8.34 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010096124865980516		[learning rate: 0.00068441]
		[batch 20/20] avg loss: 0.005959901674390496		[learning rate: 0.00068358]
	Learning Rate: 0.00068358
	LOSS [training: 0.008028013270185503 | validation: 0.0254519588320329]
	TIME [epoch: 8.32 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013056054500868528		[learning rate: 0.00068275]
		[batch 20/20] avg loss: 0.020953111534246983		[learning rate: 0.00068193]
	Learning Rate: 0.000681925
	LOSS [training: 0.017004583017557758 | validation: 0.013926386125672914]
	TIME [epoch: 8.34 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017556788971180258		[learning rate: 0.0006811]
		[batch 20/20] avg loss: 0.008325434181062043		[learning rate: 0.00068027]
	Learning Rate: 0.000680275
	LOSS [training: 0.01294111157612115 | validation: 0.012032817168586746]
	TIME [epoch: 8.34 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012498712956148862		[learning rate: 0.00067945]
		[batch 20/20] avg loss: 0.007122563061882725		[learning rate: 0.00067863]
	Learning Rate: 0.000678628
	LOSS [training: 0.009810638009015795 | validation: 0.019932261427648586]
	TIME [epoch: 8.33 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008301044521660014		[learning rate: 0.00067781]
		[batch 20/20] avg loss: 0.003328865664390096		[learning rate: 0.00067698]
	Learning Rate: 0.000676985
	LOSS [training: 0.005814955093025055 | validation: 0.009894328777706268]
	TIME [epoch: 8.31 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010672679417645228		[learning rate: 0.00067616]
		[batch 20/20] avg loss: 0.0014820864069772783		[learning rate: 0.00067535]
	Learning Rate: 0.000675346
	LOSS [training: 0.006077382912311253 | validation: 0.009301466594192953]
	TIME [epoch: 8.33 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00018184654304185047		[learning rate: 0.00067453]
		[batch 20/20] avg loss: 0.0012422325452831691		[learning rate: 0.00067371]
	Learning Rate: 0.000673711
	LOSS [training: 0.0005301930011206593 | validation: 0.011636495992590246]
	TIME [epoch: 8.31 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030927219516342233		[learning rate: 0.0006729]
		[batch 20/20] avg loss: 0.009083262410616692		[learning rate: 0.00067208]
	Learning Rate: 0.00067208
	LOSS [training: 0.006087992181125457 | validation: 0.004430669468042884]
	TIME [epoch: 8.31 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008196215604556223		[learning rate: 0.00067127]
		[batch 20/20] avg loss: 0.004926616289481489		[learning rate: 0.00067045]
	Learning Rate: 0.000670453
	LOSS [training: 0.006561415947018856 | validation: 0.010942715919381979]
	TIME [epoch: 8.31 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027023607139754643		[learning rate: 0.00066964]
		[batch 20/20] avg loss: 0.001006334471839098		[learning rate: 0.00066883]
	Learning Rate: 0.00066883
	LOSS [training: 0.001854347592907281 | validation: 0.007446885037777589]
	TIME [epoch: 8.33 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00583946558114847		[learning rate: 0.00066802]
		[batch 20/20] avg loss: -0.0037121735385547716		[learning rate: 0.00066721]
	Learning Rate: 0.000667211
	LOSS [training: 0.001063646021296849 | validation: 0.003062564957241784]
	TIME [epoch: 8.34 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006058076350963284		[learning rate: 0.0006664]
		[batch 20/20] avg loss: 0.003194761531145109		[learning rate: 0.0006656]
	Learning Rate: 0.000665596
	LOSS [training: 0.004626418941054198 | validation: 0.013649126164056431]
	TIME [epoch: 8.32 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0039023156517266817		[learning rate: 0.00066479]
		[batch 20/20] avg loss: 0.00627081465370444		[learning rate: 0.00066398]
	Learning Rate: 0.000663984
	LOSS [training: 0.005086565152715562 | validation: 0.011181927989948524]
	TIME [epoch: 8.32 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028138813146093913		[learning rate: 0.00066318]
		[batch 20/20] avg loss: 0.006806743396232083		[learning rate: 0.00066238]
	Learning Rate: 0.000662377
	LOSS [training: 0.004810312355420737 | validation: 0.008318860662933258]
	TIME [epoch: 8.34 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005777573959793289		[learning rate: 0.00066157]
		[batch 20/20] avg loss: 0.002084005168316993		[learning rate: 0.00066077]
	Learning Rate: 0.000660773
	LOSS [training: 0.003930789564055141 | validation: 0.00221652715152118]
	TIME [epoch: 8.35 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025898879708421856		[learning rate: 0.00065997]
		[batch 20/20] avg loss: 0.007332386697987954		[learning rate: 0.00065917]
	Learning Rate: 0.000659174
	LOSS [training: 0.004961137334415069 | validation: 0.007716537430758363]
	TIME [epoch: 8.31 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004515487005487874		[learning rate: 0.00065838]
		[batch 20/20] avg loss: 0.012841400153340104		[learning rate: 0.00065758]
	Learning Rate: 0.000657578
	LOSS [training: 0.00867844357941399 | validation: 0.021695825728650565]
	TIME [epoch: 8.31 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01001946022251383		[learning rate: 0.00065678]
		[batch 20/20] avg loss: 0.006027244799787209		[learning rate: 0.00065599]
	Learning Rate: 0.000655986
	LOSS [training: 0.008023352511150519 | validation: 0.011849488399279321]
	TIME [epoch: 8.33 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012779216552661782		[learning rate: 0.00065519]
		[batch 20/20] avg loss: -0.0012129699070081196		[learning rate: 0.0006544]
	Learning Rate: 0.000654398
	LOSS [training: 0.005783123322826831 | validation: 0.01404389343551625]
	TIME [epoch: 8.31 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01019236759670621		[learning rate: 0.00065361]
		[batch 20/20] avg loss: 0.008803310075557706		[learning rate: 0.00065281]
	Learning Rate: 0.000652814
	LOSS [training: 0.009497838836131959 | validation: 0.008549030772439986]
	TIME [epoch: 8.31 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016300860968126347		[learning rate: 0.00065202]
		[batch 20/20] avg loss: 0.01434038585279759		[learning rate: 0.00065123]
	Learning Rate: 0.000651234
	LOSS [training: 0.015320623410461967 | validation: 0.011151010418323679]
	TIME [epoch: 8.34 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003421236017823453		[learning rate: 0.00065045]
		[batch 20/20] avg loss: 0.006824264418210496		[learning rate: 0.00064966]
	Learning Rate: 0.000649657
	LOSS [training: 0.005122750218016975 | validation: -0.002418664312079525]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_1228.pth
	Model improved!!!
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007007427373438929		[learning rate: 0.00064887]
		[batch 20/20] avg loss: 0.007978451501244503		[learning rate: 0.00064808]
	Learning Rate: 0.000648084
	LOSS [training: 0.0074929394373417155 | validation: 0.021898704052117118]
	TIME [epoch: 8.33 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013307368717321966		[learning rate: 0.0006473]
		[batch 20/20] avg loss: 0.03204222442753359		[learning rate: 0.00064652]
	Learning Rate: 0.000646515
	LOSS [training: 0.02267479657242778 | validation: 0.011249592855887051]
	TIME [epoch: 8.33 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0141372826529066		[learning rate: 0.00064573]
		[batch 20/20] avg loss: 0.0071593619069704765		[learning rate: 0.00064495]
	Learning Rate: 0.00064495
	LOSS [training: 0.010648322279938538 | validation: 0.011814188263498943]
	TIME [epoch: 8.34 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008571598330347995		[learning rate: 0.00064417]
		[batch 20/20] avg loss: 0.005920749367763935		[learning rate: 0.00064339]
	Learning Rate: 0.000643389
	LOSS [training: 0.007246173849055964 | validation: 0.00690746990961801]
	TIME [epoch: 8.33 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007713311185323576		[learning rate: 0.00064261]
		[batch 20/20] avg loss: 0.009397132514891177		[learning rate: 0.00064183]
	Learning Rate: 0.000641832
	LOSS [training: 0.008555221850107373 | validation: 0.020452191762791408]
	TIME [epoch: 8.31 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016310894600723647		[learning rate: 0.00064105]
		[batch 20/20] avg loss: 0.005279343309304534		[learning rate: 0.00064028]
	Learning Rate: 0.000640278
	LOSS [training: 0.010795118955014088 | validation: 0.025891095429307016]
	TIME [epoch: 8.31 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008180610773739818		[learning rate: 0.0006395]
		[batch 20/20] avg loss: 0.00631303350647591		[learning rate: 0.00063873]
	Learning Rate: 0.000638728
	LOSS [training: 0.0072468221401078645 | validation: 0.012736485443554855]
	TIME [epoch: 8.31 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007771194092712513		[learning rate: 0.00063795]
		[batch 20/20] avg loss: 0.006114093178635973		[learning rate: 0.00063718]
	Learning Rate: 0.000637182
	LOSS [training: 0.006942643635674243 | validation: 0.00577739210996505]
	TIME [epoch: 8.33 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007049190906569714		[learning rate: 0.00063641]
		[batch 20/20] avg loss: 0.014387377004600469		[learning rate: 0.00063564]
	Learning Rate: 0.000635639
	LOSS [training: 0.00754614804762872 | validation: 0.018658404921380083]
	TIME [epoch: 8.31 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005063471356098455		[learning rate: 0.00063487]
		[batch 20/20] avg loss: 0.004364674392096637		[learning rate: 0.0006341]
	Learning Rate: 0.0006341
	LOSS [training: 0.004714072874097546 | validation: 0.001889359488953205]
	TIME [epoch: 8.3 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003964357005898608		[learning rate: 0.00063333]
		[batch 20/20] avg loss: 0.007564499662398479		[learning rate: 0.00063257]
	Learning Rate: 0.000632565
	LOSS [training: 0.005764428334148543 | validation: 0.004551856571112718]
	TIME [epoch: 8.32 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012305348181793686		[learning rate: 0.0006318]
		[batch 20/20] avg loss: 0.0015028567539938675		[learning rate: 0.00063103]
	Learning Rate: 0.000631034
	LOSS [training: 0.00013616096790724933 | validation: 0.004158778100081509]
	TIME [epoch: 8.35 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0093557893459471		[learning rate: 0.00063027]
		[batch 20/20] avg loss: 0.0035498145573497884		[learning rate: 0.00062951]
	Learning Rate: 0.000629506
	LOSS [training: 0.006452801951648442 | validation: 0.004861689780418408]
	TIME [epoch: 8.32 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008551324871092		[learning rate: 0.00062874]
		[batch 20/20] avg loss: 0.005253621307405675		[learning rate: 0.00062798]
	Learning Rate: 0.000627982
	LOSS [training: 0.006902473089248837 | validation: 0.007685547375764302]
	TIME [epoch: 8.32 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00329208932569255		[learning rate: 0.00062722]
		[batch 20/20] avg loss: 0.010359008106515183		[learning rate: 0.00062646]
	Learning Rate: 0.000626462
	LOSS [training: 0.006825548716103867 | validation: 0.007826089785377624]
	TIME [epoch: 8.34 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004275518424725619		[learning rate: 0.0006257]
		[batch 20/20] avg loss: 0.0002795406326882023		[learning rate: 0.00062495]
	Learning Rate: 0.000624946
	LOSS [training: 0.002277529528706911 | validation: 0.003337557765160545]
	TIME [epoch: 8.35 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017810237915114307		[learning rate: 0.00062419]
		[batch 20/20] avg loss: 0.0013636270786363078		[learning rate: 0.00062343]
	Learning Rate: 0.000623433
	LOSS [training: -0.00020869835643756154 | validation: 0.00642896159160126]
	TIME [epoch: 8.31 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009577193216190608		[learning rate: 0.00062268]
		[batch 20/20] avg loss: 0.002469469597875457		[learning rate: 0.00062192]
	Learning Rate: 0.000621923
	LOSS [training: 0.0017135944597472589 | validation: 0.006693991206682041]
	TIME [epoch: 8.31 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007084171942081513		[learning rate: 0.00062117]
		[batch 20/20] avg loss: 0.003845087688479228		[learning rate: 0.00062042]
	Learning Rate: 0.000620418
	LOSS [training: 0.00227675244134369 | validation: 0.0042692222577751645]
	TIME [epoch: 8.31 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004684585948241641		[learning rate: 0.00061967]
		[batch 20/20] avg loss: 0.0022736251219237437		[learning rate: 0.00061892]
	Learning Rate: 0.000618916
	LOSS [training: 0.003479105535082692 | validation: -0.0004205790973934734]
	TIME [epoch: 8.33 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0054832113767904365		[learning rate: 0.00061817]
		[batch 20/20] avg loss: 0.006229530841263734		[learning rate: 0.00061742]
	Learning Rate: 0.000617418
	LOSS [training: 0.00037315973223664883 | validation: 0.006748760927984734]
	TIME [epoch: 8.31 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004322886799483598		[learning rate: 0.00061667]
		[batch 20/20] avg loss: 0.008349162212543993		[learning rate: 0.00061592]
	Learning Rate: 0.000615923
	LOSS [training: 0.006336024506013796 | validation: 0.019504711778923545]
	TIME [epoch: 8.33 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004116542894487954		[learning rate: 0.00061518]
		[batch 20/20] avg loss: 0.009188305314427176		[learning rate: 0.00061443]
	Learning Rate: 0.000614432
	LOSS [training: 0.006652424104457566 | validation: 0.014858635988154838]
	TIME [epoch: 8.32 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004756416945959087		[learning rate: 0.00061369]
		[batch 20/20] avg loss: 0.002326324030394558		[learning rate: 0.00061294]
	Learning Rate: 0.000612944
	LOSS [training: 0.0035413704881768226 | validation: 0.006274620627829351]
	TIME [epoch: 8.34 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003658119488360964		[learning rate: 0.0006122]
		[batch 20/20] avg loss: 0.001127451938537569		[learning rate: 0.00061146]
	Learning Rate: 0.000611461
	LOSS [training: 0.002392785713449267 | validation: 0.017156194671789835]
	TIME [epoch: 8.34 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006151452126072603		[learning rate: 0.00061072]
		[batch 20/20] avg loss: 0.005398327958634775		[learning rate: 0.00060998]
	Learning Rate: 0.00060998
	LOSS [training: 0.0057748900423536895 | validation: 0.017363242072719495]
	TIME [epoch: 8.33 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007746168837035415		[learning rate: 0.00060924]
		[batch 20/20] avg loss: 0.0006412819013697812		[learning rate: 0.0006085]
	Learning Rate: 0.000608504
	LOSS [training: 0.004193725369202599 | validation: 0.015157944948888427]
	TIME [epoch: 8.31 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.953462701540046e-05		[learning rate: 0.00060777]
		[batch 20/20] avg loss: 0.001117910856734801		[learning rate: 0.00060703]
	Learning Rate: 0.00060703
	LOSS [training: 0.0005787227418751007 | validation: 0.00330189507260583]
	TIME [epoch: 8.33 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021816146196396635		[learning rate: 0.0006063]
		[batch 20/20] avg loss: 0.004426612285643922		[learning rate: 0.00060556]
	Learning Rate: 0.000605561
	LOSS [training: 0.003304113452641792 | validation: 0.009675444437315196]
	TIME [epoch: 8.31 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005349430202545991		[learning rate: 0.00060483]
		[batch 20/20] avg loss: 0.004198790259460262		[learning rate: 0.00060409]
	Learning Rate: 0.000604095
	LOSS [training: 0.004774110231003126 | validation: 0.008467329867203579]
	TIME [epoch: 8.3 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006458709226523812		[learning rate: 0.00060336]
		[batch 20/20] avg loss: 0.008036521444030807		[learning rate: 0.00060263]
	Learning Rate: 0.000602633
	LOSS [training: 0.007247615335277309 | validation: 0.0073260648203762905]
	TIME [epoch: 8.31 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007479510525090054		[learning rate: 0.0006019]
		[batch 20/20] avg loss: 0.004970735085355734		[learning rate: 0.00060117]
	Learning Rate: 0.000601174
	LOSS [training: 0.006225122805222895 | validation: -0.0008002108333867404]
	TIME [epoch: 8.35 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0046718408790337495		[learning rate: 0.00060045]
		[batch 20/20] avg loss: 0.00016708959283452397		[learning rate: 0.00059972]
	Learning Rate: 0.000599718
	LOSS [training: -0.0022523756430996122 | validation: 0.013553152104420027]
	TIME [epoch: 8.33 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0001494064802309545		[learning rate: 0.00059899]
		[batch 20/20] avg loss: 0.006325558572599997		[learning rate: 0.00059827]
	Learning Rate: 0.000598267
	LOSS [training: 0.003237482526415476 | validation: 0.02175800801659886]
	TIME [epoch: 8.32 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0065047143027610775		[learning rate: 0.00059754]
		[batch 20/20] avg loss: 0.007959198947071344		[learning rate: 0.00059682]
	Learning Rate: 0.000596818
	LOSS [training: 0.007231956624916211 | validation: 0.006072693010145472]
	TIME [epoch: 8.31 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011943756301466612		[learning rate: 0.0005961]
		[batch 20/20] avg loss: 0.007470160297675932		[learning rate: 0.00059537]
	Learning Rate: 0.000595373
	LOSS [training: 0.00970695829957127 | validation: 0.009267908423424655]
	TIME [epoch: 8.37 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025016607602735896		[learning rate: 0.00059465]
		[batch 20/20] avg loss: 0.009701362516554499		[learning rate: 0.00059393]
	Learning Rate: 0.000593932
	LOSS [training: 0.0173589850596452 | validation: 0.005924936111494927]
	TIME [epoch: 8.31 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015101819186158305		[learning rate: 0.00059321]
		[batch 20/20] avg loss: 0.0015410148090412965		[learning rate: 0.00059249]
	Learning Rate: 0.000592494
	LOSS [training: 0.008321416997599801 | validation: 0.007002090017869616]
	TIME [epoch: 8.31 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013599746862562171		[learning rate: 0.00059178]
		[batch 20/20] avg loss: 0.013376478489573518		[learning rate: 0.00059106]
	Learning Rate: 0.00059106
	LOSS [training: 0.0073682265879148655 | validation: 0.01299682204802439]
	TIME [epoch: 8.31 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0063961458688054805		[learning rate: 0.00059034]
		[batch 20/20] avg loss: 0.008859084686962022		[learning rate: 0.00058963]
	Learning Rate: 0.000589629
	LOSS [training: 0.007627615277883751 | validation: 0.0014346350854721715]
	TIME [epoch: 8.33 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021906827565574562		[learning rate: 0.00058892]
		[batch 20/20] avg loss: 0.003475383428596416		[learning rate: 0.0005882]
	Learning Rate: 0.000588202
	LOSS [training: 0.002833033092576937 | validation: 0.0063150880548122025]
	TIME [epoch: 8.31 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009616800063417927		[learning rate: 0.00058749]
		[batch 20/20] avg loss: 0.009910989642476305		[learning rate: 0.00058678]
	Learning Rate: 0.000586778
	LOSS [training: 0.009763894852947114 | validation: 0.0007214376421938555]
	TIME [epoch: 8.31 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00402676961455435		[learning rate: 0.00058607]
		[batch 20/20] avg loss: 0.021067628245832255		[learning rate: 0.00058536]
	Learning Rate: 0.000585357
	LOSS [training: 0.012547198930193302 | validation: 0.010582079342293577]
	TIME [epoch: 8.32 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004718185380039061		[learning rate: 0.00058465]
		[batch 20/20] avg loss: 0.006567026273999523		[learning rate: 0.00058394]
	Learning Rate: 0.00058394
	LOSS [training: 0.005642605827019291 | validation: 0.010356419265469706]
	TIME [epoch: 8.34 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004035420726687335		[learning rate: 0.00058323]
		[batch 20/20] avg loss: 0.01388108317733562		[learning rate: 0.00058253]
	Learning Rate: 0.000582527
	LOSS [training: 0.008958251952011479 | validation: 0.002766110798224368]
	TIME [epoch: 8.32 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005438709351791029		[learning rate: 0.00058182]
		[batch 20/20] avg loss: 0.0070081938221978335		[learning rate: 0.00058112]
	Learning Rate: 0.000581116
	LOSS [training: 0.006223451586994432 | validation: 0.00706212632080775]
	TIME [epoch: 8.32 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0056143733066849714		[learning rate: 0.00058041]
		[batch 20/20] avg loss: 0.0002522779143431993		[learning rate: 0.00057971]
	Learning Rate: 0.00057971
	LOSS [training: 0.0029333256105140847 | validation: 0.0017606851842008806]
	TIME [epoch: 8.34 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029942461397304		[learning rate: 0.00057901]
		[batch 20/20] avg loss: -0.0005237241872744514		[learning rate: 0.00057831]
	Learning Rate: 0.000578306
	LOSS [training: 0.001235260976227974 | validation: 0.00802603273425328]
	TIME [epoch: 8.34 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008017849332517483		[learning rate: 0.00057761]
		[batch 20/20] avg loss: 0.004622611013729565		[learning rate: 0.00057691]
	Learning Rate: 0.000576906
	LOSS [training: 0.006320230173123524 | validation: 0.008451634364344854]
	TIME [epoch: 8.31 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008230382236176873		[learning rate: 0.00057621]
		[batch 20/20] avg loss: 0.00788465008790597		[learning rate: 0.00057551]
	Learning Rate: 0.00057551
	LOSS [training: 0.008057516162041423 | validation: 0.01335448637942549]
	TIME [epoch: 8.31 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004724052095773227		[learning rate: 0.00057481]
		[batch 20/20] avg loss: 0.01287059092805878		[learning rate: 0.00057412]
	Learning Rate: 0.000574117
	LOSS [training: 0.008797321511916003 | validation: 0.0070367844204803414]
	TIME [epoch: 8.3 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007992513459065678		[learning rate: 0.00057342]
		[batch 20/20] avg loss: 0.010402630977021429		[learning rate: 0.00057273]
	Learning Rate: 0.000572727
	LOSS [training: 0.009197572218043553 | validation: 0.013149622611052519]
	TIME [epoch: 8.33 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012834289737769303		[learning rate: 0.00057203]
		[batch 20/20] avg loss: 0.012143317338234441		[learning rate: 0.00057134]
	Learning Rate: 0.00057134
	LOSS [training: 0.012488803538001872 | validation: 0.01853561169469504]
	TIME [epoch: 8.31 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010197343709366495		[learning rate: 0.00057065]
		[batch 20/20] avg loss: 0.0038968856218628022		[learning rate: 0.00056996]
	Learning Rate: 0.000569957
	LOSS [training: 0.0070471146656146495 | validation: 0.013928470837967522]
	TIME [epoch: 8.34 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012659092478812112		[learning rate: 0.00056927]
		[batch 20/20] avg loss: 0.008260249259915321		[learning rate: 0.00056858]
	Learning Rate: 0.000568577
	LOSS [training: 0.010459670869363715 | validation: 0.004706504750143466]
	TIME [epoch: 8.31 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011851087790617097		[learning rate: 0.00056789]
		[batch 20/20] avg loss: 0.004305583240384795		[learning rate: 0.0005672]
	Learning Rate: 0.000567201
	LOSS [training: 0.0027453460097232527 | validation: 0.010415609671226022]
	TIME [epoch: 8.34 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033629316698405264		[learning rate: 0.00056651]
		[batch 20/20] avg loss: 0.0048879256043994216		[learning rate: 0.00056583]
	Learning Rate: 0.000565828
	LOSS [training: 0.0041254286371199735 | validation: 0.009452393644732892]
	TIME [epoch: 8.33 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002419735405760202		[learning rate: 0.00056514]
		[batch 20/20] avg loss: 0.00018530572609071363		[learning rate: 0.00056446]
	Learning Rate: 0.000564458
	LOSS [training: 0.0013025205659254581 | validation: 0.01310940126521727]
	TIME [epoch: 8.33 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002118101223933236		[learning rate: 0.00056377]
		[batch 20/20] avg loss: 0.012373717374150484		[learning rate: 0.00056309]
	Learning Rate: 0.000563092
	LOSS [training: 0.00724590929904186 | validation: 0.007459173511213976]
	TIME [epoch: 8.31 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019458785539657596		[learning rate: 0.00056241]
		[batch 20/20] avg loss: 0.0010290552954597353		[learning rate: 0.00056173]
	Learning Rate: 0.000561728
	LOSS [training: 0.0014874669247127476 | validation: 0.0027962415878961255]
	TIME [epoch: 8.33 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007293444891148908		[learning rate: 0.00056105]
		[batch 20/20] avg loss: 0.0034402683225744544		[learning rate: 0.00056037]
	Learning Rate: 0.000560369
	LOSS [training: 0.00536685660686168 | validation: 0.004865209187475494]
	TIME [epoch: 8.31 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000996600086224558		[learning rate: 0.00055969]
		[batch 20/20] avg loss: 0.010041169575585183		[learning rate: 0.00055901]
	Learning Rate: 0.000559012
	LOSS [training: 0.00551888483090487 | validation: 0.006519816585491241]
	TIME [epoch: 8.31 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007424910561432091		[learning rate: 0.00055833]
		[batch 20/20] avg loss: 0.004811982582925514		[learning rate: 0.00055766]
	Learning Rate: 0.000557659
	LOSS [training: 0.006118446572178801 | validation: 0.007899308425107936]
	TIME [epoch: 8.3 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004372614797113611		[learning rate: 0.00055698]
		[batch 20/20] avg loss: 0.006114666153914355		[learning rate: 0.00055631]
	Learning Rate: 0.000556309
	LOSS [training: 0.005243640475513982 | validation: 0.004604120682360234]
	TIME [epoch: 8.33 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006155002009913317		[learning rate: 0.00055563]
		[batch 20/20] avg loss: 0.010226804596698846		[learning rate: 0.00055496]
	Learning Rate: 0.000554962
	LOSS [training: 0.00819090330330608 | validation: 0.03183843685562411]
	TIME [epoch: 8.34 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010335034001505698		[learning rate: 0.00055429]
		[batch 20/20] avg loss: -0.00223309976185577		[learning rate: 0.00055362]
	Learning Rate: 0.000553618
	LOSS [training: 0.004050967119824963 | validation: 0.005776201285282268]
	TIME [epoch: 8.32 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00023140704528290526		[learning rate: 0.00055295]
		[batch 20/20] avg loss: 0.00416613187770032		[learning rate: 0.00055228]
	Learning Rate: 0.000552278
	LOSS [training: 0.002198769461491613 | validation: 0.00642621347454141]
	TIME [epoch: 8.32 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021139923439901865		[learning rate: 0.00055161]
		[batch 20/20] avg loss: 0.004811367782898228		[learning rate: 0.00055094]
	Learning Rate: 0.000550941
	LOSS [training: 0.0034626800634442066 | validation: 0.002552007337492774]
	TIME [epoch: 8.34 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0069418830940929325		[learning rate: 0.00055027]
		[batch 20/20] avg loss: 0.02157261702433897		[learning rate: 0.00054961]
	Learning Rate: 0.000549608
	LOSS [training: 0.01425725005921595 | validation: 0.010759159070375111]
	TIME [epoch: 8.34 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008385221490564917		[learning rate: 0.00054894]
		[batch 20/20] avg loss: 0.007386596808212312		[learning rate: 0.00054828]
	Learning Rate: 0.000548277
	LOSS [training: 0.007885909149388614 | validation: 0.014019095601322893]
	TIME [epoch: 8.34 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009513163188377673		[learning rate: 0.00054761]
		[batch 20/20] avg loss: 0.0036344621169037477		[learning rate: 0.00054695]
	Learning Rate: 0.00054695
	LOSS [training: 0.006573812652640709 | validation: 0.009172437271082292]
	TIME [epoch: 8.31 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004689827308388725		[learning rate: 0.00054629]
		[batch 20/20] avg loss: 0.0019037879516942149		[learning rate: 0.00054563]
	Learning Rate: 0.000545626
	LOSS [training: 0.003296807630041469 | validation: 0.006347429548615318]
	TIME [epoch: 8.33 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0070152101785592585		[learning rate: 0.00054496]
		[batch 20/20] avg loss: 0.006709971104222219		[learning rate: 0.0005443]
	Learning Rate: 0.000544305
	LOSS [training: 0.006862590641390738 | validation: 0.01070825937900623]
	TIME [epoch: 8.31 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006262701253240026		[learning rate: 0.00054365]
		[batch 20/20] avg loss: 0.002065301063763572		[learning rate: 0.00054299]
	Learning Rate: 0.000542987
	LOSS [training: 0.0041640011585018 | validation: 0.00646008933574025]
	TIME [epoch: 8.31 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0066839482941772		[learning rate: 0.00054233]
		[batch 20/20] avg loss: 0.0030102202033509455		[learning rate: 0.00054167]
	Learning Rate: 0.000541673
	LOSS [training: 0.004847084248764072 | validation: 0.00033273027323391503]
	TIME [epoch: 8.31 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0001733512839862802		[learning rate: 0.00054102]
		[batch 20/20] avg loss: 0.0030072897252928477		[learning rate: 0.00054036]
	Learning Rate: 0.000540361
	LOSS [training: 0.0015903205046395642 | validation: 0.0038158444674404636]
	TIME [epoch: 8.33 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006067532031473784		[learning rate: 0.00053971]
		[batch 20/20] avg loss: 0.0006802069999775557		[learning rate: 0.00053905]
	Learning Rate: 0.000539053
	LOSS [training: 3.672689841508845e-05 | validation: 0.004269675144617716]
	TIME [epoch: 8.32 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032273371406887977		[learning rate: 0.0005384]
		[batch 20/20] avg loss: 0.005653702685945999		[learning rate: 0.00053775]
	Learning Rate: 0.000537748
	LOSS [training: 0.0012131827726286004 | validation: 0.0025770347604193453]
	TIME [epoch: 8.33 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021280921047213226		[learning rate: 0.0005371]
		[batch 20/20] avg loss: 0.007418382892055887		[learning rate: 0.00053645]
	Learning Rate: 0.000536446
	LOSS [training: 0.004773237498388606 | validation: 0.004522093057582858]
	TIME [epoch: 8.32 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002148281499210089		[learning rate: 0.0005358]
		[batch 20/20] avg loss: -0.0005739747910928076		[learning rate: 0.00053515]
	Learning Rate: 0.000535148
	LOSS [training: -0.001361128145151448 | validation: 0.014171716013749624]
	TIME [epoch: 8.33 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0048949432173547875		[learning rate: 0.0005345]
		[batch 20/20] avg loss: 0.005893027607470916		[learning rate: 0.00053385]
	Learning Rate: 0.000533852
	LOSS [training: 0.005393985412412853 | validation: 0.0026035855966833124]
	TIME [epoch: 8.34 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004732337255210606		[learning rate: 0.00053321]
		[batch 20/20] avg loss: 0.005075296684196916		[learning rate: 0.00053256]
	Learning Rate: 0.00053256
	LOSS [training: 0.004903816969703761 | validation: 0.019803888088543938]
	TIME [epoch: 8.33 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009237889551293286		[learning rate: 0.00053191]
		[batch 20/20] avg loss: 0.0036024994368751223		[learning rate: 0.00053127]
	Learning Rate: 0.000531271
	LOSS [training: 0.006420194494084204 | validation: 0.005843411137169959]
	TIME [epoch: 8.31 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012880945604598908		[learning rate: 0.00053063]
		[batch 20/20] avg loss: -0.0011541108808412665		[learning rate: 0.00052998]
	Learning Rate: 0.000529985
	LOSS [training: 6.699183980931185e-05 | validation: 0.01294123185419821]
	TIME [epoch: 8.33 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004538197408938171		[learning rate: 0.00052934]
		[batch 20/20] avg loss: 0.006148481671866963		[learning rate: 0.0005287]
	Learning Rate: 0.000528702
	LOSS [training: 0.005343339540402567 | validation: 0.006111952849322384]
	TIME [epoch: 8.31 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009032085761450395		[learning rate: 0.00052806]
		[batch 20/20] avg loss: 0.0001595698186621644		[learning rate: 0.00052742]
	Learning Rate: 0.000527422
	LOSS [training: 0.004595827790056279 | validation: 0.005070810681476804]
	TIME [epoch: 8.31 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008845157933861268		[learning rate: 0.00052678]
		[batch 20/20] avg loss: 0.002211836093817347		[learning rate: 0.00052614]
	Learning Rate: 0.000526145
	LOSS [training: 0.0006636601502156104 | validation: 0.005784872046770292]
	TIME [epoch: 8.31 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003229808160337397		[learning rate: 0.00052551]
		[batch 20/20] avg loss: 0.005892206138216567		[learning rate: 0.00052487]
	Learning Rate: 0.000524871
	LOSS [training: 0.004561007149276982 | validation: 0.004976279599271949]
	TIME [epoch: 8.33 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004657967788883706		[learning rate: 0.00052424]
		[batch 20/20] avg loss: 0.0011452809557189118		[learning rate: 0.0005236]
	Learning Rate: 0.0005236
	LOSS [training: 0.002901624372301309 | validation: 0.0069758582728747295]
	TIME [epoch: 8.31 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003900472332387872		[learning rate: 0.00052297]
		[batch 20/20] avg loss: 0.00015293015253758127		[learning rate: 0.00052233]
	Learning Rate: 0.000522333
	LOSS [training: 0.0020267012424627267 | validation: 0.006084515523862716]
	TIME [epoch: 8.31 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008546663499664032		[learning rate: 0.0005217]
		[batch 20/20] avg loss: 0.0018357869197972408		[learning rate: 0.00052107]
	Learning Rate: 0.000521068
	LOSS [training: 0.0013452266348818215 | validation: 0.012170297236680122]
	TIME [epoch: 8.33 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007671027494746398		[learning rate: 0.00052044]
		[batch 20/20] avg loss: 0.0012776547976252855		[learning rate: 0.00051981]
	Learning Rate: 0.000519807
	LOSS [training: 0.004474341146185842 | validation: 0.007685593706186208]
	TIME [epoch: 8.35 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: -8.841826631081203e-05		[learning rate: 0.00051918]
		[batch 20/20] avg loss: 0.001087911217664033		[learning rate: 0.00051855]
	Learning Rate: 0.000518549
	LOSS [training: 0.0004997464756766104 | validation: 0.0004244592115287722]
	TIME [epoch: 8.32 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021566176947075966		[learning rate: 0.00051792]
		[batch 20/20] avg loss: 0.0006859644487698252		[learning rate: 0.00051729]
	Learning Rate: 0.000517293
	LOSS [training: 0.0014212910717387112 | validation: 0.0016695989929932937]
	TIME [epoch: 8.32 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032336513671881325		[learning rate: 0.00051667]
		[batch 20/20] avg loss: 4.355490409740509e-05		[learning rate: 0.00051604]
	Learning Rate: 0.000516041
	LOSS [training: 0.001638603135642769 | validation: 0.012864286223996997]
	TIME [epoch: 8.34 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014286511464720272		[learning rate: 0.00051542]
		[batch 20/20] avg loss: 0.007128037476941841		[learning rate: 0.00051479]
	Learning Rate: 0.000514792
	LOSS [training: 0.0042783443117069345 | validation: -0.0008905870692305583]
	TIME [epoch: 8.35 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005486816939294933		[learning rate: 0.00051417]
		[batch 20/20] avg loss: 0.0039484455304261935		[learning rate: 0.00051355]
	Learning Rate: 0.000513545
	LOSS [training: 0.004717631234860562 | validation: -0.0025186035948381756]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_1325.pth
	Model improved!!!
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013594425576391401		[learning rate: 0.00051292]
		[batch 20/20] avg loss: -0.0005462181782190057		[learning rate: 0.0005123]
	Learning Rate: 0.000512302
	LOSS [training: 0.00040661218971006714 | validation: 0.0008445589664836364]
	TIME [epoch: 8.34 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034745941622959207		[learning rate: 0.00051168]
		[batch 20/20] avg loss: 0.0070944529983086885		[learning rate: 0.00051106]
	Learning Rate: 0.000511062
	LOSS [training: 0.0018099294180063835 | validation: -0.0009655744805965332]
	TIME [epoch: 8.34 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002800505944480343		[learning rate: 0.00051044]
		[batch 20/20] avg loss: -0.0023548256423600446		[learning rate: 0.00050982]
	Learning Rate: 0.000509825
	LOSS [training: 0.0002228401510601493 | validation: 0.004736431836974839]
	TIME [epoch: 8.37 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018722529455934907		[learning rate: 0.00050921]
		[batch 20/20] avg loss: 0.0008021686933407624		[learning rate: 0.00050859]
	Learning Rate: 0.000508591
	LOSS [training: -0.0005350421261263642 | validation: 0.004223121159921156]
	TIME [epoch: 8.36 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019931391307251634		[learning rate: 0.00050797]
		[batch 20/20] avg loss: 0.006367898263484051		[learning rate: 0.00050736]
	Learning Rate: 0.00050736
	LOSS [training: 0.002187379566379444 | validation: 0.007075929542149992]
	TIME [epoch: 8.37 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004841722523034396		[learning rate: 0.00050675]
		[batch 20/20] avg loss: 0.005983044289936935		[learning rate: 0.00050613]
	Learning Rate: 0.000506131
	LOSS [training: 0.0005706608834512696 | validation: 0.004570439834662033]
	TIME [epoch: 8.35 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00872556653684282		[learning rate: 0.00050552]
		[batch 20/20] avg loss: 0.0014889393483702034		[learning rate: 0.00050491]
	Learning Rate: 0.000504906
	LOSS [training: 0.005107252942606512 | validation: 0.0022000244125549057]
	TIME [epoch: 8.38 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00028573573976252675		[learning rate: 0.00050429]
		[batch 20/20] avg loss: 0.005171001565649801		[learning rate: 0.00050368]
	Learning Rate: 0.000503684
	LOSS [training: 0.002442632912943637 | validation: 0.002651142729743884]
	TIME [epoch: 8.38 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002267089702948446		[learning rate: 0.00050307]
		[batch 20/20] avg loss: 0.004008247309073338		[learning rate: 0.00050246]
	Learning Rate: 0.000502464
	LOSS [training: 0.0031376685060108926 | validation: 0.012792707327922242]
	TIME [epoch: 8.35 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032724770515871852		[learning rate: 0.00050186]
		[batch 20/20] avg loss: 0.004193613691417285		[learning rate: 0.00050125]
	Learning Rate: 0.000501248
	LOSS [training: 0.00046056831991504965 | validation: -0.0002499565792956596]
	TIME [epoch: 8.33 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026700639117152427		[learning rate: 0.00050064]
		[batch 20/20] avg loss: -8.092325389670893e-05		[learning rate: 0.00050003]
	Learning Rate: 0.000500034
	LOSS [training: -0.001375493582805976 | validation: 0.013638560634307146]
	TIME [epoch: 8.37 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006497512071853051		[learning rate: 0.00049943]
		[batch 20/20] avg loss: 0.005970819961078917		[learning rate: 0.00049882]
	Learning Rate: 0.000498824
	LOSS [training: 0.006234166016465984 | validation: 0.006908664654267293]
	TIME [epoch: 8.33 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010336280679846472		[learning rate: 0.00049822]
		[batch 20/20] avg loss: 0.0066482339289515874		[learning rate: 0.00049762]
	Learning Rate: 0.000497616
	LOSS [training: 0.008492257304399032 | validation: 0.0017565943227080252]
	TIME [epoch: 8.34 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006838736194007354		[learning rate: 0.00049701]
		[batch 20/20] avg loss: 0.0015432456908173744		[learning rate: 0.00049641]
	Learning Rate: 0.000496412
	LOSS [training: 0.004190990942412364 | validation: 0.0005342792875122134]
	TIME [epoch: 8.37 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003383198225120168		[learning rate: 0.00049581]
		[batch 20/20] avg loss: 0.002936011804201497		[learning rate: 0.00049521]
	Learning Rate: 0.00049521
	LOSS [training: 0.003159605014660833 | validation: 0.001582198738527656]
	TIME [epoch: 8.37 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012998428091132931		[learning rate: 0.00049461]
		[batch 20/20] avg loss: 0.009515325204542213		[learning rate: 0.00049401]
	Learning Rate: 0.000494011
	LOSS [training: 0.004107741197714459 | validation: 0.005453736091656494]
	TIME [epoch: 8.35 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024187574333256094		[learning rate: 0.00049341]
		[batch 20/20] avg loss: 0.003176764555104853		[learning rate: 0.00049282]
	Learning Rate: 0.000492815
	LOSS [training: 0.0003790035608896216 | validation: 0.006469806079723584]
	TIME [epoch: 8.35 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013108898012998869		[learning rate: 0.00049222]
		[batch 20/20] avg loss: 0.001691323083068312		[learning rate: 0.00049162]
	Learning Rate: 0.000491622
	LOSS [training: 0.0015011064421840991 | validation: 0.000975139733169583]
	TIME [epoch: 8.4 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019063723417791592		[learning rate: 0.00049103]
		[batch 20/20] avg loss: -0.0011697633151981975		[learning rate: 0.00049043]
	Learning Rate: 0.000490432
	LOSS [training: 0.0003683045132904807 | validation: 0.00291004344716731]
	TIME [epoch: 8.35 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014527838951086721		[learning rate: 0.00048984]
		[batch 20/20] avg loss: 0.0027453422370664433		[learning rate: 0.00048924]
	Learning Rate: 0.000489245
	LOSS [training: 0.008636590594076583 | validation: 0.005316421599712928]
	TIME [epoch: 8.34 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007946828764592529		[learning rate: 0.00048865]
		[batch 20/20] avg loss: 0.004477550961183469		[learning rate: 0.00048806]
	Learning Rate: 0.000488061
	LOSS [training: 0.0018414340423621076 | validation: 0.013039393130325927]
	TIME [epoch: 8.34 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000916749895635729		[learning rate: 0.00048747]
		[batch 20/20] avg loss: -0.0007398114213691658		[learning rate: 0.00048688]
	Learning Rate: 0.000486879
	LOSS [training: -0.0008282806585024474 | validation: 0.009926406685434123]
	TIME [epoch: 8.35 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0044779335052122626		[learning rate: 0.00048629]
		[batch 20/20] avg loss: 0.005128512360707498		[learning rate: 0.0004857]
	Learning Rate: 0.0004857
	LOSS [training: 0.004803222932959881 | validation: 0.009347731821295684]
	TIME [epoch: 8.35 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00536884283270216		[learning rate: 0.00048511]
		[batch 20/20] avg loss: 0.0033308199652423848		[learning rate: 0.00048452]
	Learning Rate: 0.000484525
	LOSS [training: 0.004349831398972272 | validation: 0.0026994135197328673]
	TIME [epoch: 8.33 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00478652995591277		[learning rate: 0.00048394]
		[batch 20/20] avg loss: -0.0022376040599909595		[learning rate: 0.00048335]
	Learning Rate: 0.000483352
	LOSS [training: 0.0012744629479609056 | validation: 0.013409171357892113]
	TIME [epoch: 8.37 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008276241975097198		[learning rate: 0.00048277]
		[batch 20/20] avg loss: 0.0035753718041763257		[learning rate: 0.00048218]
	Learning Rate: 0.000482181
	LOSS [training: 0.005925806889636761 | validation: -0.002927254670121095]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_1351.pth
	Model improved!!!
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001353090559148346		[learning rate: 0.0004816]
		[batch 20/20] avg loss: 0.00037501488420503044		[learning rate: 0.00048101]
	Learning Rate: 0.000481014
	LOSS [training: 0.0008640527216766883 | validation: 0.01720218912974794]
	TIME [epoch: 8.35 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022236958134215093		[learning rate: 0.00048043]
		[batch 20/20] avg loss: -0.0005210633854155978		[learning rate: 0.00047985]
	Learning Rate: 0.00047985
	LOSS [training: 0.0008513162140029557 | validation: 0.0015665962812172656]
	TIME [epoch: 8.34 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010737107702014103		[learning rate: 0.00047927]
		[batch 20/20] avg loss: 0.004912719714574401		[learning rate: 0.00047869]
	Learning Rate: 0.000478688
	LOSS [training: 0.0029932152423879053 | validation: 0.0033080155725192873]
	TIME [epoch: 8.37 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001335130003701658		[learning rate: 0.00047811]
		[batch 20/20] avg loss: -0.0012593127180351785		[learning rate: 0.00047753]
	Learning Rate: 0.000477529
	LOSS [training: 3.790864283323976e-05 | validation: 0.006113709911545443]
	TIME [epoch: 8.39 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00044814960709242387		[learning rate: 0.00047695]
		[batch 20/20] avg loss: 0.001829739702710282		[learning rate: 0.00047637]
	Learning Rate: 0.000476373
	LOSS [training: 0.001138944654901353 | validation: 0.0028714704765464543]
	TIME [epoch: 8.34 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006067757895565842		[learning rate: 0.0004758]
		[batch 20/20] avg loss: 0.0011373538208031269		[learning rate: 0.00047522]
	Learning Rate: 0.00047522
	LOSS [training: 0.0036025558581844843 | validation: 0.007237019245082383]
	TIME [epoch: 8.33 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017685478866908302		[learning rate: 0.00047464]
		[batch 20/20] avg loss: 0.004000839374324897		[learning rate: 0.00047407]
	Learning Rate: 0.00047407
	LOSS [training: 0.002884693630507864 | validation: 0.0005566650332403649]
	TIME [epoch: 8.33 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007931121007838902		[learning rate: 0.0004735]
		[batch 20/20] avg loss: -0.0004533243848570881		[learning rate: 0.00047292]
	Learning Rate: 0.000472922
	LOSS [training: 0.00016989385796340096 | validation: 0.00862854119443347]
	TIME [epoch: 8.36 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037399239064379573		[learning rate: 0.00047235]
		[batch 20/20] avg loss: -0.001986925960831837		[learning rate: 0.00047178]
	Learning Rate: 0.000471777
	LOSS [training: 0.0008764989728030602 | validation: 0.007791545263519292]
	TIME [epoch: 8.33 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021126158446199577		[learning rate: 0.00047121]
		[batch 20/20] avg loss: 0.00035229701705918545		[learning rate: 0.00047063]
	Learning Rate: 0.000470635
	LOSS [training: 0.0012324564308395717 | validation: -0.0045484059312138085]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_1361.pth
	Model improved!!!
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026786045769200107		[learning rate: 0.00047006]
		[batch 20/20] avg loss: 0.005805886718808319		[learning rate: 0.0004695]
	Learning Rate: 0.000469496
	LOSS [training: 0.0015636410709441537 | validation: 0.00998478070563014]
	TIME [epoch: 8.34 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0072195433960060675		[learning rate: 0.00046893]
		[batch 20/20] avg loss: 0.010954352976791207		[learning rate: 0.00046836]
	Learning Rate: 0.000468359
	LOSS [training: 0.009086948186398639 | validation: 0.006038821839563253]
	TIME [epoch: 8.36 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013280126331151984		[learning rate: 0.00046779]
		[batch 20/20] avg loss: -0.0011839242432594202		[learning rate: 0.00046723]
	Learning Rate: 0.000467225
	LOSS [training: 0.006048101043946282 | validation: -0.0031383521517025525]
	TIME [epoch: 8.33 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000261308136272163		[learning rate: 0.00046666]
		[batch 20/20] avg loss: 0.004167543046614882		[learning rate: 0.00046609]
	Learning Rate: 0.000466094
	LOSS [training: 0.0022144255914435225 | validation: 0.007077110686270788]
	TIME [epoch: 8.32 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004891756510174139		[learning rate: 0.00046553]
		[batch 20/20] avg loss: 0.004421849163030638		[learning rate: 0.00046497]
	Learning Rate: 0.000464966
	LOSS [training: 0.004656802836602388 | validation: 0.013673268948595274]
	TIME [epoch: 8.36 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004863523635813446		[learning rate: 0.0004644]
		[batch 20/20] avg loss: 0.005257229105019564		[learning rate: 0.00046384]
	Learning Rate: 0.00046384
	LOSS [training: 0.0023854383707191096 | validation: 0.016062615372489795]
	TIME [epoch: 8.34 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006312144098934286		[learning rate: 0.00046328]
		[batch 20/20] avg loss: 0.0019805333408360534		[learning rate: 0.00046272]
	Learning Rate: 0.000462717
	LOSS [training: 0.00414633871988517 | validation: 0.009108672977111935]
	TIME [epoch: 8.32 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005595680272160243		[learning rate: 0.00046216]
		[batch 20/20] avg loss: 0.00801822481783854		[learning rate: 0.0004616]
	Learning Rate: 0.000461597
	LOSS [training: 0.006806952544999391 | validation: 0.00018473769997477013]
	TIME [epoch: 8.31 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004733915619014281		[learning rate: 0.00046104]
		[batch 20/20] avg loss: 0.006934895829837013		[learning rate: 0.00046048]
	Learning Rate: 0.00046048
	LOSS [training: 0.005834405724425647 | validation: -0.0011157087707681992]
	TIME [epoch: 8.32 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00041349384818120183		[learning rate: 0.00045992]
		[batch 20/20] avg loss: 0.005210211977210911		[learning rate: 0.00045937]
	Learning Rate: 0.000459365
	LOSS [training: 0.0028118529126960564 | validation: 0.004919372689131793]
	TIME [epoch: 8.34 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009388520471427613		[learning rate: 0.00045881]
		[batch 20/20] avg loss: 0.0046755475965221645		[learning rate: 0.00045825]
	Learning Rate: 0.000458253
	LOSS [training: 0.007032034033974889 | validation: 0.0031977383015480395]
	TIME [epoch: 8.32 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00379201333516899		[learning rate: 0.0004577]
		[batch 20/20] avg loss: 0.004714105802718166		[learning rate: 0.00045714]
	Learning Rate: 0.000457144
	LOSS [training: 0.004253059568943577 | validation: 0.008303402730213117]
	TIME [epoch: 8.33 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007752300956983512		[learning rate: 0.00045659]
		[batch 20/20] avg loss: -0.0002225969163203341		[learning rate: 0.00045604]
	Learning Rate: 0.000456037
	LOSS [training: 0.0037648520203315875 | validation: 0.007898105181936952]
	TIME [epoch: 8.35 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002563186587075122		[learning rate: 0.00045548]
		[batch 20/20] avg loss: 0.007262453473915171		[learning rate: 0.00045493]
	Learning Rate: 0.000454933
	LOSS [training: 0.0023496334434200254 | validation: 0.005949202205325086]
	TIME [epoch: 8.35 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033973522754051607		[learning rate: 0.00045438]
		[batch 20/20] avg loss: 0.007315125931462857		[learning rate: 0.00045383]
	Learning Rate: 0.000453832
	LOSS [training: 0.005356239103434008 | validation: 0.0015569049972369545]
	TIME [epoch: 8.33 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006584532773166967		[learning rate: 0.00045328]
		[batch 20/20] avg loss: 0.0031320907845557495		[learning rate: 0.00045273]
	Learning Rate: 0.000452733
	LOSS [training: 0.004858311778861359 | validation: 0.004037820732304052]
	TIME [epoch: 8.34 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004595081639054336		[learning rate: 0.00045218]
		[batch 20/20] avg loss: 0.00038692595515208637		[learning rate: 0.00045164]
	Learning Rate: 0.000451637
	LOSS [training: 0.0024910037971032123 | validation: -0.00014997535675975466]
	TIME [epoch: 8.34 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003064531299089352		[learning rate: 0.00045109]
		[batch 20/20] avg loss: 0.0014088650603323403		[learning rate: 0.00045054]
	Learning Rate: 0.000450544
	LOSS [training: 0.0022366981797108457 | validation: 0.005995127692835695]
	TIME [epoch: 8.34 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007478569408128017		[learning rate: 0.00045]
		[batch 20/20] avg loss: 0.007186320215773419		[learning rate: 0.00044945]
	Learning Rate: 0.000449453
	LOSS [training: 0.007332444811950718 | validation: 0.005944279713184273]
	TIME [epoch: 8.32 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002521181322235044		[learning rate: 0.00044891]
		[batch 20/20] avg loss: 0.006632642670765065		[learning rate: 0.00044836]
	Learning Rate: 0.000448365
	LOSS [training: 0.004576911996500054 | validation: 0.005361442979289087]
	TIME [epoch: 8.32 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038893955410755294		[learning rate: 0.00044782]
		[batch 20/20] avg loss: 0.0027981127042557134		[learning rate: 0.00044728]
	Learning Rate: 0.000447279
	LOSS [training: 0.0033437541226656203 | validation: 0.0020465689646914716]
	TIME [epoch: 8.32 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001827898547523857		[learning rate: 0.00044674]
		[batch 20/20] avg loss: 0.0038249358406186214		[learning rate: 0.0004462]
	Learning Rate: 0.000446197
	LOSS [training: 0.0028264171940712386 | validation: 8.860653784407155e-05]
	TIME [epoch: 8.34 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003825621133032066		[learning rate: 0.00044566]
		[batch 20/20] avg loss: 0.004695724402374397		[learning rate: 0.00044512]
	Learning Rate: 0.000445117
	LOSS [training: 0.004260672767703231 | validation: 0.0007777254687204476]
	TIME [epoch: 8.33 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023987081768980408		[learning rate: 0.00044458]
		[batch 20/20] avg loss: 0.005989803982157009		[learning rate: 0.00044404]
	Learning Rate: 0.000444039
	LOSS [training: 0.0041942560795275256 | validation: 0.004039106848117819]
	TIME [epoch: 8.35 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022018547273302933		[learning rate: 0.0004435]
		[batch 20/20] avg loss: -0.001141733911122269		[learning rate: 0.00044296]
	Learning Rate: 0.000442964
	LOSS [training: 0.000530060408104012 | validation: -0.0012040779936982556]
	TIME [epoch: 8.32 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002002012338265158		[learning rate: 0.00044243]
		[batch 20/20] avg loss: 0.0075793709577659865		[learning rate: 0.00044189]
	Learning Rate: 0.000441892
	LOSS [training: 0.002788679309750414 | validation: -0.0004112189110722273]
	TIME [epoch: 8.35 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00630903528181036		[learning rate: 0.00044136]
		[batch 20/20] avg loss: 0.0016525738163342997		[learning rate: 0.00044082]
	Learning Rate: 0.000440822
	LOSS [training: 0.003980804549072329 | validation: 0.006761568936596989]
	TIME [epoch: 8.34 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0049498699451673905		[learning rate: 0.00044029]
		[batch 20/20] avg loss: 2.1353521416737563e-05		[learning rate: 0.00043975]
	Learning Rate: 0.000439755
	LOSS [training: 0.0024856117332920638 | validation: 0.0014014391045611067]
	TIME [epoch: 8.34 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035464423957290055		[learning rate: 0.00043922]
		[batch 20/20] avg loss: 0.004151246735181202		[learning rate: 0.00043869]
	Learning Rate: 0.00043869
	LOSS [training: 0.0038488445654551035 | validation: 0.01247230796884854]
	TIME [epoch: 8.32 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007509242669872599		[learning rate: 0.00043816]
		[batch 20/20] avg loss: 0.0008609478467984967		[learning rate: 0.00043763]
	Learning Rate: 0.000437628
	LOSS [training: 0.004185095258335549 | validation: 0.009923388345033844]
	TIME [epoch: 8.34 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034370772914162477		[learning rate: 0.0004371]
		[batch 20/20] avg loss: 0.002744976610064864		[learning rate: 0.00043657]
	Learning Rate: 0.000436569
	LOSS [training: 0.0030910269507405555 | validation: 0.0032385891935308316]
	TIME [epoch: 8.32 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015825014579425305		[learning rate: 0.00043604]
		[batch 20/20] avg loss: 0.00039267375779767223		[learning rate: 0.00043551]
	Learning Rate: 0.000435512
	LOSS [training: 0.0009875876078701015 | validation: 0.002838795869511317]
	TIME [epoch: 8.33 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018978728110407074		[learning rate: 0.00043498]
		[batch 20/20] avg loss: 0.0006890645591127904		[learning rate: 0.00043446]
	Learning Rate: 0.000434458
	LOSS [training: -0.0006044041259639585 | validation: 0.0005911687494211542]
	TIME [epoch: 8.32 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005668614161632853		[learning rate: 0.00043393]
		[batch 20/20] avg loss: 0.004180573228619497		[learning rate: 0.00043341]
	Learning Rate: 0.000433406
	LOSS [training: 0.004924593695126175 | validation: 0.0005993355919835844]
	TIME [epoch: 8.35 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018173200634240242		[learning rate: 0.00043288]
		[batch 20/20] avg loss: 0.009429893289157651		[learning rate: 0.00043236]
	Learning Rate: 0.000432357
	LOSS [training: 0.005623606676290837 | validation: 0.008497271514905185]
	TIME [epoch: 8.32 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017607525780349148		[learning rate: 0.00043183]
		[batch 20/20] avg loss: 0.0023304150299789045		[learning rate: 0.00043131]
	Learning Rate: 0.00043131
	LOSS [training: 0.0002848312259719949 | validation: 0.0009516371145888742]
	TIME [epoch: 8.34 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024215707126935874		[learning rate: 0.00043079]
		[batch 20/20] avg loss: 0.001060361163900397		[learning rate: 0.00043027]
	Learning Rate: 0.000430266
	LOSS [training: -0.0006806047743965954 | validation: 0.0015278110962024423]
	TIME [epoch: 8.34 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008369319293837559		[learning rate: 0.00042974]
		[batch 20/20] avg loss: 0.0046250068417339075		[learning rate: 0.00042922]
	Learning Rate: 0.000429224
	LOSS [training: 0.006497163067785733 | validation: 0.0018363047611894227]
	TIME [epoch: 8.35 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022123113110252652		[learning rate: 0.0004287]
		[batch 20/20] avg loss: 0.003208946044731898		[learning rate: 0.00042819]
	Learning Rate: 0.000428185
	LOSS [training: 0.002710628677878582 | validation: 0.002043526831572192]
	TIME [epoch: 8.34 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003223700797162044		[learning rate: 0.00042767]
		[batch 20/20] avg loss: 0.005550553650825614		[learning rate: 0.00042715]
	Learning Rate: 0.000427149
	LOSS [training: 0.004387127223993829 | validation: 0.00013422738149753777]
	TIME [epoch: 8.35 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004612381664304795		[learning rate: 0.00042663]
		[batch 20/20] avg loss: 0.004614600376684109		[learning rate: 0.00042611]
	Learning Rate: 0.000426115
	LOSS [training: 0.0046134910204944524 | validation: 0.004628872352436915]
	TIME [epoch: 8.34 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007651740664528594		[learning rate: 0.0004256]
		[batch 20/20] avg loss: 0.005152738729025313		[learning rate: 0.00042508]
	Learning Rate: 0.000425083
	LOSS [training: 0.002958956397739086 | validation: 0.0031192263582217046]
	TIME [epoch: 8.34 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006716873773529186		[learning rate: 0.00042457]
		[batch 20/20] avg loss: 0.0033163273002828828		[learning rate: 0.00042405]
	Learning Rate: 0.000424054
	LOSS [training: 0.005016600536906034 | validation: 0.01108368863728652]
	TIME [epoch: 8.33 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006318091779086599		[learning rate: 0.00042354]
		[batch 20/20] avg loss: 0.020229511739909518		[learning rate: 0.00042303]
	Learning Rate: 0.000423027
	LOSS [training: 0.013273801759498055 | validation: 0.02810672452412538]
	TIME [epoch: 8.32 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00851027290019396		[learning rate: 0.00042251]
		[batch 20/20] avg loss: 0.012568610157805984		[learning rate: 0.000422]
	Learning Rate: 0.000422003
	LOSS [training: 0.01053944152899997 | validation: 0.017316179502925164]
	TIME [epoch: 8.32 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0065758343701630955		[learning rate: 0.00042149]
		[batch 20/20] avg loss: 0.005021842663384147		[learning rate: 0.00042098]
	Learning Rate: 0.000420982
	LOSS [training: 0.005798838516773623 | validation: 0.008153682271821153]
	TIME [epoch: 8.34 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00043930505339800685		[learning rate: 0.00042047]
		[batch 20/20] avg loss: 0.008648634206084476		[learning rate: 0.00041996]
	Learning Rate: 0.000419963
	LOSS [training: 0.004543969629741242 | validation: 0.007104563101889524]
	TIME [epoch: 8.32 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005835240556763631		[learning rate: 0.00041945]
		[batch 20/20] avg loss: 0.007660397351642788		[learning rate: 0.00041895]
	Learning Rate: 0.000418946
	LOSS [training: 0.00674781895420321 | validation: 0.010171266922170668]
	TIME [epoch: 8.32 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003031894762946281		[learning rate: 0.00041844]
		[batch 20/20] avg loss: 0.00526245865026632		[learning rate: 0.00041793]
	Learning Rate: 0.000417932
	LOSS [training: 0.004147176706606299 | validation: 0.005882558324672594]
	TIME [epoch: 8.32 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010315998166972506		[learning rate: 0.00041743]
		[batch 20/20] avg loss: -0.0013757926150954648		[learning rate: 0.00041692]
	Learning Rate: 0.00041692
	LOSS [training: -0.00017209639919910712 | validation: 0.012027590889832067]
	TIME [epoch: 8.37 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038144310347801476		[learning rate: 0.00041641]
		[batch 20/20] avg loss: 0.0009915477810538836		[learning rate: 0.00041591]
	Learning Rate: 0.000415911
	LOSS [training: 0.002402989407917015 | validation: 0.00245929750601532]
	TIME [epoch: 8.33 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006476175431754353		[learning rate: 0.00041541]
		[batch 20/20] avg loss: 0.0007464592904802581		[learning rate: 0.0004149]
	Learning Rate: 0.000414904
	LOSS [training: -0.002864858070637047 | validation: 0.00361925738185476]
	TIME [epoch: 8.33 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017372325341309888		[learning rate: 0.0004144]
		[batch 20/20] avg loss: -0.0019044912637769251		[learning rate: 0.0004139]
	Learning Rate: 0.000413899
	LOSS [training: -8.362936482296823e-05 | validation: 0.0009843476010551802]
	TIME [epoch: 8.35 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001869953598919143		[learning rate: 0.0004134]
		[batch 20/20] avg loss: 0.0027333846365396692		[learning rate: 0.0004129]
	Learning Rate: 0.000412897
	LOSS [training: 0.00043171551881026346 | validation: 0.003375998282079587]
	TIME [epoch: 8.36 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015581306419492332		[learning rate: 0.0004124]
		[batch 20/20] avg loss: 0.003951928165681502		[learning rate: 0.0004119]
	Learning Rate: 0.000411898
	LOSS [training: 0.001196898761866135 | validation: 0.006349461435268506]
	TIME [epoch: 8.33 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020566822918091426		[learning rate: 0.0004114]
		[batch 20/20] avg loss: 0.006697948332309805		[learning rate: 0.0004109]
	Learning Rate: 0.000410901
	LOSS [training: 0.004377315312059475 | validation: 0.003298770577602685]
	TIME [epoch: 8.32 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003928944926944159		[learning rate: 0.0004104]
		[batch 20/20] avg loss: 0.0006897532243411563		[learning rate: 0.00040991]
	Learning Rate: 0.000409906
	LOSS [training: 0.002309349075642657 | validation: 0.0040972007171669825]
	TIME [epoch: 8.32 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006407893121802597		[learning rate: 0.00040941]
		[batch 20/20] avg loss: 0.003308656056157401		[learning rate: 0.00040891]
	Learning Rate: 0.000408914
	LOSS [training: 0.0013339333719885704 | validation: 0.0009054804971838019]
	TIME [epoch: 8.34 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037416792910401155		[learning rate: 0.00040842]
		[batch 20/20] avg loss: 0.0007872125212094721		[learning rate: 0.00040792]
	Learning Rate: 0.000407924
	LOSS [training: 0.002264445906124793 | validation: 0.00650919840179125]
	TIME [epoch: 8.32 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004722223461408918		[learning rate: 0.00040743]
		[batch 20/20] avg loss: 0.0007889021789408469		[learning rate: 0.00040694]
	Learning Rate: 0.000406936
	LOSS [training: 0.002755562820174883 | validation: 0.008178191997504719]
	TIME [epoch: 8.31 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0046298983721849134		[learning rate: 0.00040644]
		[batch 20/20] avg loss: 0.007335279509363353		[learning rate: 0.00040595]
	Learning Rate: 0.000405951
	LOSS [training: 0.0059825889407741336 | validation: -0.002828496508557224]
	TIME [epoch: 8.35 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00044375739357079697		[learning rate: 0.00040546]
		[batch 20/20] avg loss: 0.002072327239560056		[learning rate: 0.00040497]
	Learning Rate: 0.000404968
	LOSS [training: 0.0012580423165654264 | validation: 0.0049885141101017605]
	TIME [epoch: 8.35 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0045113828111347015		[learning rate: 0.00040448]
		[batch 20/20] avg loss: 0.0005723211292044891		[learning rate: 0.00040399]
	Learning Rate: 0.000403988
	LOSS [training: -0.0019695308409651063 | validation: -0.005670675777755209]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_1424.pth
	Model improved!!!
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025166937881917085		[learning rate: 0.0004035]
		[batch 20/20] avg loss: -0.0029145829615129485		[learning rate: 0.00040301]
	Learning Rate: 0.00040301
	LOSS [training: -0.0001989445866606196 | validation: 0.005429903983347242]
	TIME [epoch: 8.33 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005080265075721209		[learning rate: 0.00040252]
		[batch 20/20] avg loss: -0.003223575524565393		[learning rate: 0.00040203]
	Learning Rate: 0.000402034
	LOSS [training: 0.0009283447755779083 | validation: 0.005646260996356523]
	TIME [epoch: 8.35 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00507774522863564		[learning rate: 0.00040155]
		[batch 20/20] avg loss: -0.0008884664026340472		[learning rate: 0.00040106]
	Learning Rate: 0.000401061
	LOSS [training: 0.002094639413000795 | validation: 0.002375957437652319]
	TIME [epoch: 8.34 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006530805107768168		[learning rate: 0.00040058]
		[batch 20/20] avg loss: 0.00028548426665263445		[learning rate: 0.00040009]
	Learning Rate: 0.00040009
	LOSS [training: 0.0034081446872104002 | validation: 0.001486714253452556]
	TIME [epoch: 8.31 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00020241869122128363		[learning rate: 0.00039961]
		[batch 20/20] avg loss: -0.002290558701711947		[learning rate: 0.00039912]
	Learning Rate: 0.000399122
	LOSS [training: -0.0012464886964666153 | validation: 0.006638905540786238]
	TIME [epoch: 8.31 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020035146860085135		[learning rate: 0.00039864]
		[batch 20/20] avg loss: -0.001870408730017877		[learning rate: 0.00039816]
	Learning Rate: 0.000398155
	LOSS [training: 6.65529779953186e-05 | validation: 0.006635469218046604]
	TIME [epoch: 8.31 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002855469395158679		[learning rate: 0.00039767]
		[batch 20/20] avg loss: 0.003929791987323156		[learning rate: 0.00039719]
	Learning Rate: 0.000397192
	LOSS [training: 0.0005371612960822378 | validation: 0.004938205872284286]
	TIME [epoch: 8.34 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022098533384110166		[learning rate: 0.00039671]
		[batch 20/20] avg loss: 0.003934640057006428		[learning rate: 0.00039623]
	Learning Rate: 0.00039623
	LOSS [training: 0.0030722466977087222 | validation: -2.4469711838208657e-05]
	TIME [epoch: 8.32 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018079528118079432		[learning rate: 0.00039575]
		[batch 20/20] avg loss: 0.003695798336161147		[learning rate: 0.00039527]
	Learning Rate: 0.000395271
	LOSS [training: 0.0027518755739845443 | validation: 0.005067046977417845]
	TIME [epoch: 8.32 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004014183989977001		[learning rate: 0.00039479]
		[batch 20/20] avg loss: 0.0015457570301142616		[learning rate: 0.00039431]
	Learning Rate: 0.000394314
	LOSS [training: -0.0012342134799313697 | validation: 0.003098385696259181]
	TIME [epoch: 8.32 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008677397313321138		[learning rate: 0.00039384]
		[batch 20/20] avg loss: -0.007104372407478627		[learning rate: 0.00039336]
	Learning Rate: 0.000393359
	LOSS [training: -0.0031183163380732563 | validation: -0.00291816867387858]
	TIME [epoch: 8.34 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004145373959546986		[learning rate: 0.00039288]
		[batch 20/20] avg loss: -2.767445424239172e-05		[learning rate: 0.00039241]
	Learning Rate: 0.000392407
	LOSS [training: 0.002058849752652297 | validation: 0.009062903149757127]
	TIME [epoch: 8.33 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009937121985048609		[learning rate: 0.00039193]
		[batch 20/20] avg loss: 0.0029763283814916762		[learning rate: 0.00039146]
	Learning Rate: 0.000391457
	LOSS [training: 0.001985020289998268 | validation: -0.0025197513680927514]
	TIME [epoch: 8.33 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034716871773285566		[learning rate: 0.00039098]
		[batch 20/20] avg loss: 0.004593815725572789		[learning rate: 0.00039051]
	Learning Rate: 0.00039051
	LOSS [training: 0.004032751451450671 | validation: 0.0070020638845037005]
	TIME [epoch: 8.32 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000838067612427292		[learning rate: 0.00039004]
		[batch 20/20] avg loss: 0.0028622436793387454		[learning rate: 0.00038956]
	Learning Rate: 0.000389564
	LOSS [training: 0.0010120880334557267 | validation: 2.3800997306845434e-05]
	TIME [epoch: 8.34 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038986043487833875		[learning rate: 0.00038909]
		[batch 20/20] avg loss: -0.0013218303047959257		[learning rate: 0.00038862]
	Learning Rate: 0.000388621
	LOSS [training: -0.0026102173267896567 | validation: -0.0023897451834625208]
	TIME [epoch: 8.35 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00284374194831953		[learning rate: 0.00038815]
		[batch 20/20] avg loss: 0.005730227299190468		[learning rate: 0.00038768]
	Learning Rate: 0.00038768
	LOSS [training: 0.0014432426754354692 | validation: 0.004318669175980342]
	TIME [epoch: 8.33 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000441872604633756		[learning rate: 0.00038721]
		[batch 20/20] avg loss: -0.0021585005753896546		[learning rate: 0.00038674]
	Learning Rate: 0.000386742
	LOSS [training: -0.0013001865900117053 | validation: 0.004045415358553052]
	TIME [epoch: 8.32 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027604439454644463		[learning rate: 0.00038627]
		[batch 20/20] avg loss: 0.0017146384860457769		[learning rate: 0.00038581]
	Learning Rate: 0.000385805
	LOSS [training: 0.002237541215755112 | validation: 0.006673577135443499]
	TIME [epoch: 8.34 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01430758050337814		[learning rate: 0.00038534]
		[batch 20/20] avg loss: 0.012293110391811865		[learning rate: 0.00038487]
	Learning Rate: 0.000384872
	LOSS [training: 0.013300345447595003 | validation: 0.01099363268838332]
	TIME [epoch: 8.31 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006628430576470445		[learning rate: 0.00038441]
		[batch 20/20] avg loss: 0.006151572267667156		[learning rate: 0.00038394]
	Learning Rate: 0.00038394
	LOSS [training: 0.0063900014220688 | validation: 0.005474260483965342]
	TIME [epoch: 8.32 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019255887823208685		[learning rate: 0.00038347]
		[batch 20/20] avg loss: 0.004347290673034112		[learning rate: 0.00038301]
	Learning Rate: 0.00038301
	LOSS [training: 0.0031364397276774903 | validation: -0.00285562914364637]
	TIME [epoch: 8.32 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010569549610226375		[learning rate: 0.00038255]
		[batch 20/20] avg loss: 0.004576758422438246		[learning rate: 0.00038208]
	Learning Rate: 0.000382083
	LOSS [training: 0.002816856691730442 | validation: 0.009092564286530455]
	TIME [epoch: 8.36 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003055693707039027		[learning rate: 0.00038162]
		[batch 20/20] avg loss: -0.00024795797296729043		[learning rate: 0.00038116]
	Learning Rate: 0.000381158
	LOSS [training: 0.0014038678670358685 | validation: 0.006820521570742529]
	TIME [epoch: 8.32 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003970028468802539		[learning rate: 0.0003807]
		[batch 20/20] avg loss: 0.001825095396264628		[learning rate: 0.00038024]
	Learning Rate: 0.000380235
	LOSS [training: 0.0028975619325335837 | validation: 0.005033117233781229]
	TIME [epoch: 8.32 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032228016986865066		[learning rate: 0.00037977]
		[batch 20/20] avg loss: -0.001281356187923785		[learning rate: 0.00037931]
	Learning Rate: 0.000379315
	LOSS [training: 0.0009707227553813607 | validation: 0.010995857488054811]
	TIME [epoch: 8.34 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004874599705653519		[learning rate: 0.00037886]
		[batch 20/20] avg loss: 0.0004975771328786221		[learning rate: 0.0003784]
	Learning Rate: 0.000378397
	LOSS [training: 0.0026860884192660708 | validation: 0.005776393509485666]
	TIME [epoch: 8.36 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015507242951438983		[learning rate: 0.00037794]
		[batch 20/20] avg loss: 0.004528839947002715		[learning rate: 0.00037748]
	Learning Rate: 0.000377481
	LOSS [training: 0.0030397821210733067 | validation: 0.006844424819265006]
	TIME [epoch: 8.32 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005082536841069651		[learning rate: 0.00037702]
		[batch 20/20] avg loss: -0.004403504005470184		[learning rate: 0.00037657]
	Learning Rate: 0.000376567
	LOSS [training: 0.00033951641779973306 | validation: 0.0032677151351941784]
	TIME [epoch: 8.32 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006098978694463013		[learning rate: 0.00037611]
		[batch 20/20] avg loss: -0.003351344568337205		[learning rate: 0.00037566]
	Learning Rate: 0.000375655
	LOSS [training: 0.001373817063062904 | validation: 0.004689520611276544]
	TIME [epoch: 8.32 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007395835709169722		[learning rate: 0.0003752]
		[batch 20/20] avg loss: -0.0006841420995006307		[learning rate: 0.00037475]
	Learning Rate: 0.000374746
	LOSS [training: 0.0033558468048345454 | validation: 0.006749551039879984]
	TIME [epoch: 8.33 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0051613549341378035		[learning rate: 0.00037429]
		[batch 20/20] avg loss: -0.0006177088852837529		[learning rate: 0.00037384]
	Learning Rate: 0.000373839
	LOSS [training: 0.0022718230244270247 | validation: 0.00983317802738593]
	TIME [epoch: 8.32 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012153544246623251		[learning rate: 0.00037339]
		[batch 20/20] avg loss: 0.0008554068027692771		[learning rate: 0.00037293]
	Learning Rate: 0.000372934
	LOSS [training: 0.001035380613715801 | validation: -0.005364465474821249]
	TIME [epoch: 8.31 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017865112651055995		[learning rate: 0.00037248]
		[batch 20/20] avg loss: 0.005296567831032958		[learning rate: 0.00037203]
	Learning Rate: 0.000372031
	LOSS [training: 0.0017550282829636795 | validation: -0.002027538927828014]
	TIME [epoch: 8.32 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022440877876201554		[learning rate: 0.00037158]
		[batch 20/20] avg loss: -0.005959924418674599		[learning rate: 0.00037113]
	Learning Rate: 0.00037113
	LOSS [training: -0.0018579183155272214 | validation: 0.0022449760212784253]
	TIME [epoch: 8.34 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001990609185801078		[learning rate: 0.00037068]
		[batch 20/20] avg loss: -0.0024795173461290365		[learning rate: 0.00037023]
	Learning Rate: 0.000370232
	LOSS [training: -0.00024445408016397915 | validation: 0.002144953993219619]
	TIME [epoch: 8.34 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024009892442846585		[learning rate: 0.00036978]
		[batch 20/20] avg loss: -0.0003265601848872528		[learning rate: 0.00036934]
	Learning Rate: 0.000369336
	LOSS [training: 0.0010372145296987027 | validation: 0.0017385658010089804]
	TIME [epoch: 8.32 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005826712611064324		[learning rate: 0.00036889]
		[batch 20/20] avg loss: 0.0016104289244760928		[learning rate: 0.00036844]
	Learning Rate: 0.000368441
	LOSS [training: 0.003718570767770208 | validation: -0.0005926402624655]
	TIME [epoch: 8.33 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006462427067997626		[learning rate: 0.000368]
		[batch 20/20] avg loss: 0.00015059735913858186		[learning rate: 0.00036755]
	Learning Rate: 0.00036755
	LOSS [training: -0.0002478226738305903 | validation: 0.005342294526994826]
	TIME [epoch: 8.36 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004452950119213881		[learning rate: 0.0003671]
		[batch 20/20] avg loss: -0.0018461256224627212		[learning rate: 0.00036666]
	Learning Rate: 0.00036666
	LOSS [training: -0.0031495378708383003 | validation: 0.0047035061211532884]
	TIME [epoch: 8.33 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004283967886176147		[learning rate: 0.00036622]
		[batch 20/20] avg loss: 0.0008753091133960028		[learning rate: 0.00036577]
	Learning Rate: 0.000365772
	LOSS [training: 0.002579638499786075 | validation: 0.01337636811140038]
	TIME [epoch: 8.32 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002728595528316523		[learning rate: 0.00036533]
		[batch 20/20] avg loss: 0.0007615298696088652		[learning rate: 0.00036489]
	Learning Rate: 0.000364887
	LOSS [training: 0.001745062698962694 | validation: 0.0033577170745426065]
	TIME [epoch: 8.33 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030352730465553405		[learning rate: 0.00036444]
		[batch 20/20] avg loss: -0.002234399491714928		[learning rate: 0.000364]
	Learning Rate: 0.000364003
	LOSS [training: 0.00040043677742020655 | validation: 0.0058860149532166985]
	TIME [epoch: 8.32 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005713526098443864		[learning rate: 0.00036356]
		[batch 20/20] avg loss: -0.00134039901726745		[learning rate: 0.00036312]
	Learning Rate: 0.000363122
	LOSS [training: -0.0009558758135559183 | validation: 0.007352480113802349]
	TIME [epoch: 8.31 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001529537592376112		[learning rate: 0.00036268]
		[batch 20/20] avg loss: 0.0060637754075680925		[learning rate: 0.00036224]
	Learning Rate: 0.000362243
	LOSS [training: 0.002267118907595991 | validation: 0.011564346722206385]
	TIME [epoch: 8.31 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038331188439996314		[learning rate: 0.0003618]
		[batch 20/20] avg loss: -0.0019072160093000507		[learning rate: 0.00036137]
	Learning Rate: 0.000361366
	LOSS [training: 0.0009629514173497902 | validation: 0.00875862486719859]
	TIME [epoch: 8.34 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028091882359740383		[learning rate: 0.00036093]
		[batch 20/20] avg loss: -0.0013111897402340203		[learning rate: 0.00036049]
	Learning Rate: 0.000360491
	LOSS [training: 0.0007489992478700089 | validation: 0.006841745124722744]
	TIME [epoch: 8.34 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002684782629728803		[learning rate: 0.00036005]
		[batch 20/20] avg loss: -7.6903171924393e-05		[learning rate: 0.00035962]
	Learning Rate: 0.000359619
	LOSS [training: -0.0013808429008265982 | validation: -5.9710589465950805e-05]
	TIME [epoch: 8.32 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018906905021324216		[learning rate: 0.00035918]
		[batch 20/20] avg loss: 0.001557255705806304		[learning rate: 0.00035875]
	Learning Rate: 0.000358748
	LOSS [training: 0.0017239731039693635 | validation: 0.007713575464607902]
	TIME [epoch: 8.32 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006877319456084196		[learning rate: 0.00035831]
		[batch 20/20] avg loss: 0.0021622977167993266		[learning rate: 0.00035788]
	Learning Rate: 0.00035788
	LOSS [training: 0.004519808586441762 | validation: 0.0033739371598888205]
	TIME [epoch: 8.37 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021510406989808657		[learning rate: 0.00035745]
		[batch 20/20] avg loss: -0.0005008876054247629		[learning rate: 0.00035701]
	Learning Rate: 0.000357013
	LOSS [training: -0.0013259641522028137 | validation: 0.001516448927245219]
	TIME [epoch: 8.33 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018447950104689265		[learning rate: 0.00035658]
		[batch 20/20] avg loss: -0.0023475209183439214		[learning rate: 0.00035615]
	Learning Rate: 0.000356149
	LOSS [training: -0.002096157964406424 | validation: 0.007312729963079426]
	TIME [epoch: 8.31 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010568556699423635		[learning rate: 0.00035572]
		[batch 20/20] avg loss: 0.007350072808256819		[learning rate: 0.00035529]
	Learning Rate: 0.000355287
	LOSS [training: 0.0031466085691572282 | validation: 0.005340614016793673]
	TIME [epoch: 8.31 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006474055054289353		[learning rate: 0.00035486]
		[batch 20/20] avg loss: 0.0028440644472761236		[learning rate: 0.00035443]
	Learning Rate: 0.000354427
	LOSS [training: -0.0018149953035066148 | validation: 0.004247196202444017]
	TIME [epoch: 8.33 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014664974601189334		[learning rate: 0.000354]
		[batch 20/20] avg loss: -0.00013442402213211646		[learning rate: 0.00035357]
	Learning Rate: 0.000353569
	LOSS [training: -0.0008004607411255247 | validation: -0.00012244577843449736]
	TIME [epoch: 8.32 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005232646305093961		[learning rate: 0.00035314]
		[batch 20/20] avg loss: -0.004122003144162417		[learning rate: 0.00035271]
	Learning Rate: 0.000352713
	LOSS [training: -0.002322633887335907 | validation: 0.006868964665059088]
	TIME [epoch: 8.31 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003417229328529023		[learning rate: 0.00035229]
		[batch 20/20] avg loss: -0.0002684968654466199		[learning rate: 0.00035186]
	Learning Rate: 0.000351859
	LOSS [training: 0.0015743662315412014 | validation: 0.005656735125904665]
	TIME [epoch: 8.34 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005821862368417003		[learning rate: 0.00035143]
		[batch 20/20] avg loss: 0.0066002998468179756		[learning rate: 0.00035101]
	Learning Rate: 0.000351007
	LOSS [training: 0.003591243041829838 | validation: 0.005063199932675337]
	TIME [epoch: 8.34 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00250925207251281		[learning rate: 0.00035058]
		[batch 20/20] avg loss: 0.004697148800723371		[learning rate: 0.00035016]
	Learning Rate: 0.000350157
	LOSS [training: 0.003603200436618091 | validation: 0.004267003464963804]
	TIME [epoch: 8.33 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002123022760747651		[learning rate: 0.00034973]
		[batch 20/20] avg loss: 0.0005334062609682579		[learning rate: 0.00034931]
	Learning Rate: 0.00034931
	LOSS [training: 0.0013282145108579543 | validation: 0.005533634834295576]
	TIME [epoch: 8.33 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004823912327232142		[learning rate: 0.00034889]
		[batch 20/20] avg loss: 0.00426687645819261		[learning rate: 0.00034846]
	Learning Rate: 0.000348464
	LOSS [training: 0.004545394392712375 | validation: 0.004394462497275053]
	TIME [epoch: 8.32 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008515068087356442		[learning rate: 0.00034804]
		[batch 20/20] avg loss: 0.009585350230388008		[learning rate: 0.00034762]
	Learning Rate: 0.00034762
	LOSS [training: 0.009050209158872226 | validation: 0.014833016717823022]
	TIME [epoch: 8.34 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00456301828864574		[learning rate: 0.0003472]
		[batch 20/20] avg loss: 0.001245920144711063		[learning rate: 0.00034678]
	Learning Rate: 0.000346779
	LOSS [training: 0.0029044692166784016 | validation: 0.0112057197144193]
	TIME [epoch: 8.32 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002593370067611872		[learning rate: 0.00034636]
		[batch 20/20] avg loss: -0.00544641450046787		[learning rate: 0.00034594]
	Learning Rate: 0.000345939
	LOSS [training: -0.0014265222164279992 | validation: 0.00019739423633332925]
	TIME [epoch: 8.32 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025260757110922433		[learning rate: 0.00034552]
		[batch 20/20] avg loss: 0.0028476413943596744		[learning rate: 0.0003451]
	Learning Rate: 0.000345102
	LOSS [training: 0.002686858552725958 | validation: 0.0044196241436365215]
	TIME [epoch: 8.31 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007623380749837116		[learning rate: 0.00034468]
		[batch 20/20] avg loss: 0.0003662707987289105		[learning rate: 0.00034427]
	Learning Rate: 0.000344267
	LOSS [training: 0.0005643044368563111 | validation: 0.003488885551056699]
	TIME [epoch: 8.34 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034012551808266407		[learning rate: 0.00034385]
		[batch 20/20] avg loss: 0.0062709083464025355		[learning rate: 0.00034343]
	Learning Rate: 0.000343433
	LOSS [training: 0.004836081763614589 | validation: 0.003729274375567038]
	TIME [epoch: 8.32 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001127065901033373		[learning rate: 0.00034302]
		[batch 20/20] avg loss: 0.0038493877253556854		[learning rate: 0.0003426]
	Learning Rate: 0.000342602
	LOSS [training: 0.0024882268131945287 | validation: 0.010095900321260981]
	TIME [epoch: 8.34 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00042418067786734805		[learning rate: 0.00034219]
		[batch 20/20] avg loss: 0.005182976456801128		[learning rate: 0.00034177]
	Learning Rate: 0.000341772
	LOSS [training: 0.00237939788946689 | validation: 0.003645854143019613]
	TIME [epoch: 8.31 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004718381566512652		[learning rate: 0.00034136]
		[batch 20/20] avg loss: 0.0014791897289381528		[learning rate: 0.00034094]
	Learning Rate: 0.000340945
	LOSS [training: 0.0030987856477254017 | validation: 0.0022142770558434177]
	TIME [epoch: 8.34 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00215646348531177		[learning rate: 0.00034053]
		[batch 20/20] avg loss: 0.00642573075203607		[learning rate: 0.00034012]
	Learning Rate: 0.00034012
	LOSS [training: 0.00429109711867392 | validation: 0.012767300971001243]
	TIME [epoch: 8.32 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01447989677593208		[learning rate: 0.00033971]
		[batch 20/20] avg loss: 0.0114538023955368		[learning rate: 0.0003393]
	Learning Rate: 0.000339296
	LOSS [training: 0.012966849585734441 | validation: 0.02162304229100026]
	TIME [epoch: 8.35 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008321704200282234		[learning rate: 0.00033889]
		[batch 20/20] avg loss: 0.003618742084629389		[learning rate: 0.00033847]
	Learning Rate: 0.000338475
	LOSS [training: 0.005970223142455811 | validation: -0.002797826610361094]
	TIME [epoch: 8.31 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001288745584051662		[learning rate: 0.00033806]
		[batch 20/20] avg loss: 0.0010027568268301272		[learning rate: 0.00033766]
	Learning Rate: 0.000337655
	LOSS [training: 0.0011457512054408943 | validation: 0.000544796922057222]
	TIME [epoch: 8.33 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007692979798299762		[learning rate: 0.00033725]
		[batch 20/20] avg loss: 0.005622327182807539		[learning rate: 0.00033684]
	Learning Rate: 0.000336838
	LOSS [training: 0.0031958125813187576 | validation: 0.007723032094473679]
	TIME [epoch: 8.32 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015123397273697184		[learning rate: 0.00033643]
		[batch 20/20] avg loss: 0.00843692700617571		[learning rate: 0.00033602]
	Learning Rate: 0.000336023
	LOSS [training: 0.004974633366772715 | validation: 0.0048910319803334885]
	TIME [epoch: 8.31 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003110569607664941		[learning rate: 0.00033562]
		[batch 20/20] avg loss: 0.007067141844437581		[learning rate: 0.00033521]
	Learning Rate: 0.000335209
	LOSS [training: 0.005088855726051262 | validation: -0.0007335675888801105]
	TIME [epoch: 8.31 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005950661664920552		[learning rate: 0.0003348]
		[batch 20/20] avg loss: 0.007924849898203933		[learning rate: 0.0003344]
	Learning Rate: 0.000334398
	LOSS [training: 0.003664891865855938 | validation: 0.006024744061967771]
	TIME [epoch: 8.33 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004020437136714033		[learning rate: 0.00033399]
		[batch 20/20] avg loss: 0.005287596317673846		[learning rate: 0.00033359]
	Learning Rate: 0.000333588
	LOSS [training: 0.00465401672719394 | validation: 0.004341503004377104]
	TIME [epoch: 8.34 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00827320451068386		[learning rate: 0.00033318]
		[batch 20/20] avg loss: 0.007001994295246269		[learning rate: 0.00033278]
	Learning Rate: 0.000332781
	LOSS [training: 0.007637599402965066 | validation: 0.005103279646647839]
	TIME [epoch: 8.33 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006309558409752317		[learning rate: 0.00033238]
		[batch 20/20] avg loss: 0.010191853554463464		[learning rate: 0.00033197]
	Learning Rate: 0.000331975
	LOSS [training: 0.00825070598210789 | validation: 0.01340692678830187]
	TIME [epoch: 8.32 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008034650156810184		[learning rate: 0.00033157]
		[batch 20/20] avg loss: 0.00997614465707257		[learning rate: 0.00033117]
	Learning Rate: 0.000331171
	LOSS [training: 0.009005397406941376 | validation: 0.0076030402227073544]
	TIME [epoch: 8.34 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008455084895184657		[learning rate: 0.00033077]
		[batch 20/20] avg loss: 0.004863295673692926		[learning rate: 0.00033037]
	Learning Rate: 0.00033037
	LOSS [training: 0.00665919028443879 | validation: 0.004042411939537721]
	TIME [epoch: 8.36 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010101937202153365		[learning rate: 0.00032997]
		[batch 20/20] avg loss: 0.009485033413819361		[learning rate: 0.00032957]
	Learning Rate: 0.00032957
	LOSS [training: 0.009793485307986363 | validation: 0.009554227193690857]
	TIME [epoch: 8.31 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006532696171294738		[learning rate: 0.00032917]
		[batch 20/20] avg loss: 0.008179900982131233		[learning rate: 0.00032877]
	Learning Rate: 0.000328772
	LOSS [training: 0.007356298576712987 | validation: 0.014391387266449028]
	TIME [epoch: 8.31 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01168509504034142		[learning rate: 0.00032837]
		[batch 20/20] avg loss: 0.0062562298144690515		[learning rate: 0.00032798]
	Learning Rate: 0.000327976
	LOSS [training: 0.008970662427405235 | validation: 0.003809924436955979]
	TIME [epoch: 8.33 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008898096527555147		[learning rate: 0.00032758]
		[batch 20/20] avg loss: 0.004250680193290866		[learning rate: 0.00032718]
	Learning Rate: 0.000327182
	LOSS [training: 0.006574388360423006 | validation: -0.0006426344942549772]
	TIME [epoch: 8.32 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006775167578484826		[learning rate: 0.00032679]
		[batch 20/20] avg loss: 0.0066561139676092505		[learning rate: 0.00032639]
	Learning Rate: 0.00032639
	LOSS [training: 0.006715640773047038 | validation: 0.004965209779383906]
	TIME [epoch: 8.31 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008944102481743512		[learning rate: 0.00032599]
		[batch 20/20] avg loss: 0.0044011033078967775		[learning rate: 0.0003256]
	Learning Rate: 0.0003256
	LOSS [training: 0.006672602894820146 | validation: 0.006678864864784467]
	TIME [epoch: 8.31 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00574337534359556		[learning rate: 0.00032521]
		[batch 20/20] avg loss: -0.0017785646206763522		[learning rate: 0.00032481]
	Learning Rate: 0.000324812
	LOSS [training: 0.001982405361459604 | validation: 0.0024237807528962123]
	TIME [epoch: 8.35 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003336929013068375		[learning rate: 0.00032442]
		[batch 20/20] avg loss: 0.0017108924720937921		[learning rate: 0.00032403]
	Learning Rate: 0.000324025
	LOSS [training: 0.0025239107425810837 | validation: 0.008475537813623693]
	TIME [epoch: 8.33 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008328874221010078		[learning rate: 0.00032363]
		[batch 20/20] avg loss: -0.0007943245608502344		[learning rate: 0.00032324]
	Learning Rate: 0.000323241
	LOSS [training: 0.0037672748300799216 | validation: 0.01483796299587651]
	TIME [epoch: 8.32 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00360401171007312		[learning rate: 0.00032285]
		[batch 20/20] avg loss: 0.0033033437983658086		[learning rate: 0.00032246]
	Learning Rate: 0.000322458
	LOSS [training: 0.0034536777542194656 | validation: 0.008946750903594845]
	TIME [epoch: 8.32 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031114458270272093		[learning rate: 0.00032207]
		[batch 20/20] avg loss: 0.011574184873873359		[learning rate: 0.00032168]
	Learning Rate: 0.000321678
	LOSS [training: 0.0073428153504502824 | validation: 0.009713796142118319]
	TIME [epoch: 8.37 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010400659109205939		[learning rate: 0.00032129]
		[batch 20/20] avg loss: 0.012991193939259935		[learning rate: 0.0003209]
	Learning Rate: 0.000320899
	LOSS [training: 0.011695926524232937 | validation: 0.010178803623972358]
	TIME [epoch: 8.32 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014341598848889161		[learning rate: 0.00032051]
		[batch 20/20] avg loss: 0.001679234011598767		[learning rate: 0.00032012]
	Learning Rate: 0.000320122
	LOSS [training: 0.008010416430243963 | validation: 0.009220421432337264]
	TIME [epoch: 8.31 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005344743091447235		[learning rate: 0.00031973]
		[batch 20/20] avg loss: 0.010045987120973317		[learning rate: 0.00031935]
	Learning Rate: 0.000319347
	LOSS [training: 0.007695365106210275 | validation: 0.0024010999434005845]
	TIME [epoch: 8.31 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001641943089516782		[learning rate: 0.00031896]
		[batch 20/20] avg loss: 0.007375620716076561		[learning rate: 0.00031857]
	Learning Rate: 0.000318574
	LOSS [training: 0.0045087819027966715 | validation: 0.0030071370728029937]
	TIME [epoch: 8.33 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0047368514388505785		[learning rate: 0.00031819]
		[batch 20/20] avg loss: 0.0042618511973966985		[learning rate: 0.0003178]
	Learning Rate: 0.000317803
	LOSS [training: 0.004499351318123639 | validation: 0.007775390023006869]
	TIME [epoch: 8.32 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0046243217508661625		[learning rate: 0.00031742]
		[batch 20/20] avg loss: 0.004541861512894109		[learning rate: 0.00031703]
	Learning Rate: 0.000317034
	LOSS [training: 0.004583091631880135 | validation: 0.009169634010836347]
	TIME [epoch: 8.31 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015321300741545287		[learning rate: 0.00031665]
		[batch 20/20] avg loss: 0.006302603310958279		[learning rate: 0.00031627]
	Learning Rate: 0.000316266
	LOSS [training: 0.003917366692556404 | validation: 0.014094987065574292]
	TIME [epoch: 8.33 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010787960814863946		[learning rate: 0.00031588]
		[batch 20/20] avg loss: 0.0019732293116332443		[learning rate: 0.0003155]
	Learning Rate: 0.0003155
	LOSS [training: 0.006380595063248598 | validation: 0.00937282873399893]
	TIME [epoch: 8.35 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037342882821751926		[learning rate: 0.00031512]
		[batch 20/20] avg loss: -0.0006868735699777455		[learning rate: 0.00031474]
	Learning Rate: 0.000314737
	LOSS [training: 0.0015237073560987232 | validation: 0.008980576425891023]
	TIME [epoch: 8.32 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005586471437110895		[learning rate: 0.00031436]
		[batch 20/20] avg loss: -0.004643160945839766		[learning rate: 0.00031397]
	Learning Rate: 0.000313975
	LOSS [training: 0.0004716552456355645 | validation: 0.0069986365797963905]
	TIME [epoch: 8.32 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004979874104025348		[learning rate: 0.00031359]
		[batch 20/20] avg loss: 0.00893118208731331		[learning rate: 0.00031321]
	Learning Rate: 0.000313215
	LOSS [training: 0.001975653991643982 | validation: 0.0027522759331770888]
	TIME [epoch: 8.35 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005003506073930227		[learning rate: 0.00031284]
		[batch 20/20] avg loss: 0.0010945486862279658		[learning rate: 0.00031246]
	Learning Rate: 0.000312456
	LOSS [training: 0.003049027380079097 | validation: 0.003755690096299439]
	TIME [epoch: 8.34 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008012458622459414		[learning rate: 0.00031208]
		[batch 20/20] avg loss: 0.0014238738115147536		[learning rate: 0.0003117]
	Learning Rate: 0.0003117
	LOSS [training: 0.0011125598368803475 | validation: 0.004385044649686435]
	TIME [epoch: 8.32 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021187807172477146		[learning rate: 0.00031132]
		[batch 20/20] avg loss: 0.005490630364031278		[learning rate: 0.00031095]
	Learning Rate: 0.000310945
	LOSS [training: 0.003804705540639496 | validation: 0.0024577989511125505]
	TIME [epoch: 8.31 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006925519192040725		[learning rate: 0.00031057]
		[batch 20/20] avg loss: 0.0013017526370731297		[learning rate: 0.00031019]
	Learning Rate: 0.000310193
	LOSS [training: 0.0009971522781386008 | validation: -0.0019314911621275117]
	TIME [epoch: 8.31 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00481019955106172		[learning rate: 0.00030982]
		[batch 20/20] avg loss: 0.002551790072130605		[learning rate: 0.00030944]
	Learning Rate: 0.000309442
	LOSS [training: -0.0011292047394655571 | validation: 0.005733854009856677]
	TIME [epoch: 8.34 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001850356109183981		[learning rate: 0.00030907]
		[batch 20/20] avg loss: 0.0034962355248947864		[learning rate: 0.00030869]
	Learning Rate: 0.000308693
	LOSS [training: 0.0008229397078554027 | validation: 0.011592044719619454]
	TIME [epoch: 8.34 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006983574219596014		[learning rate: 0.00030832]
		[batch 20/20] avg loss: 0.00942986226162664		[learning rate: 0.00030795]
	Learning Rate: 0.000307945
	LOSS [training: 0.008206718240611326 | validation: 0.003234783760146535]
	TIME [epoch: 8.32 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002581058107720696		[learning rate: 0.00030757]
		[batch 20/20] avg loss: -0.003192505588311287		[learning rate: 0.0003072]
	Learning Rate: 0.0003072
	LOSS [training: -0.0003057237402952955 | validation: 0.006755379082806432]
	TIME [epoch: 8.32 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008386491258594861		[learning rate: 0.00030683]
		[batch 20/20] avg loss: 0.004044711492392479		[learning rate: 0.00030646]
	Learning Rate: 0.000306456
	LOSS [training: 0.0016030311832664964 | validation: 0.006584095693685188]
	TIME [epoch: 8.35 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022692808446546585		[learning rate: 0.00030609]
		[batch 20/20] avg loss: -0.0011627172374157287		[learning rate: 0.00030571]
	Learning Rate: 0.000305714
	LOSS [training: -0.0017159990410351936 | validation: 0.0012585361830300588]
	TIME [epoch: 8.36 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020812913844804885		[learning rate: 0.00030534]
		[batch 20/20] avg loss: -0.0009322752235983731		[learning rate: 0.00030497]
	Learning Rate: 0.000304974
	LOSS [training: -0.0015067833040394308 | validation: -0.006708374591477827]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_1540.pth
	Model improved!!!
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015478968727845042		[learning rate: 0.0003046]
		[batch 20/20] avg loss: 0.0014966817144610968		[learning rate: 0.00030424]
	Learning Rate: 0.000304236
	LOSS [training: -2.5607579161703802e-05 | validation: 0.0014584510561045363]
	TIME [epoch: 8.32 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001243705512831071		[learning rate: 0.00030387]
		[batch 20/20] avg loss: -0.0017497416650240193		[learning rate: 0.0003035]
	Learning Rate: 0.000303499
	LOSS [training: -0.0014967235889275453 | validation: 0.010195577474153407]
	TIME [epoch: 8.34 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: -6.701704392529442e-05		[learning rate: 0.00030313]
		[batch 20/20] avg loss: 0.00010018822494136676		[learning rate: 0.00030276]
	Learning Rate: 0.000302765
	LOSS [training: 1.658559050803617e-05 | validation: -0.0003420136828952208]
	TIME [epoch: 8.31 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004949907220073091		[learning rate: 0.0003024]
		[batch 20/20] avg loss: -0.00022487984004583328		[learning rate: 0.00030203]
	Learning Rate: 0.000302032
	LOSS [training: -0.002587393530059462 | validation: -0.0013859708266106131]
	TIME [epoch: 8.32 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018140182916768		[learning rate: 0.00030167]
		[batch 20/20] avg loss: -0.0013682523747609444		[learning rate: 0.0003013]
	Learning Rate: 0.000301301
	LOSS [training: -0.0015911353332188724 | validation: -4.3768952200638994e-05]
	TIME [epoch: 8.31 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0054526991423692805		[learning rate: 0.00030094]
		[batch 20/20] avg loss: 0.0011054631267901265		[learning rate: 0.00030057]
	Learning Rate: 0.000300571
	LOSS [training: -0.002173618007789577 | validation: 0.0006806453980191245]
	TIME [epoch: 8.33 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007548095888998322		[learning rate: 0.00030021]
		[batch 20/20] avg loss: -0.0008135181033515016		[learning rate: 0.00029984]
	Learning Rate: 0.000299844
	LOSS [training: -2.9354257225834915e-05 | validation: 0.005938570086664487]
	TIME [epoch: 8.31 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000553067979018951		[learning rate: 0.00029948]
		[batch 20/20] avg loss: -0.0023067854308781973		[learning rate: 0.00029912]
	Learning Rate: 0.000299118
	LOSS [training: -0.0008768587259296232 | validation: 0.004018128820199531]
	TIME [epoch: 8.32 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001076862963669279		[learning rate: 0.00029876]
		[batch 20/20] avg loss: -0.0006413386160852768		[learning rate: 0.00029839]
	Learning Rate: 0.000298394
	LOSS [training: 0.0002177621737920011 | validation: 0.0028430581977913396]
	TIME [epoch: 8.33 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025525301699556107		[learning rate: 0.00029803]
		[batch 20/20] avg loss: 0.0021852478674421576		[learning rate: 0.00029767]
	Learning Rate: 0.000297671
	LOSS [training: 0.0023688890186988843 | validation: -0.004737454453707734]
	TIME [epoch: 8.34 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004618832946624636		[learning rate: 0.00029731]
		[batch 20/20] avg loss: -0.0007169777313027593		[learning rate: 0.00029695]
	Learning Rate: 0.000296951
	LOSS [training: 0.0019509276076609381 | validation: -0.0009022867970823725]
	TIME [epoch: 8.32 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031366896623560834		[learning rate: 0.00029659]
		[batch 20/20] avg loss: -0.0015468953281969453		[learning rate: 0.00029623]
	Learning Rate: 0.000296232
	LOSS [training: 0.0007948971670795689 | validation: 0.002647863217600535]
	TIME [epoch: 8.33 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007616571762563339		[learning rate: 0.00029587]
		[batch 20/20] avg loss: 0.0008143780076708367		[learning rate: 0.00029551]
	Learning Rate: 0.000295515
	LOSS [training: 0.0007880175919635854 | validation: 0.005853910739040682]
	TIME [epoch: 8.33 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003216586150446873		[learning rate: 0.00029516]
		[batch 20/20] avg loss: 0.0020642853185785385		[learning rate: 0.0002948]
	Learning Rate: 0.000294799
	LOSS [training: 0.0026404357345127054 | validation: 0.01363023833253502]
	TIME [epoch: 8.33 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004739828239531681		[learning rate: 0.00029444]
		[batch 20/20] avg loss: -0.009003522410618973		[learning rate: 0.00029409]
	Learning Rate: 0.000294086
	LOSS [training: -0.004264769793332903 | validation: -0.0022188653829431845]
	TIME [epoch: 8.31 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025671856130085407		[learning rate: 0.00029373]
		[batch 20/20] avg loss: -0.0031070325322507903		[learning rate: 0.00029337]
	Learning Rate: 0.000293374
	LOSS [training: -0.00026992345962112505 | validation: -0.0008968512375691004]
	TIME [epoch: 8.31 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020791815461632516		[learning rate: 0.00029302]
		[batch 20/20] avg loss: 0.002737625555345894		[learning rate: 0.00029266]
	Learning Rate: 0.000292663
	LOSS [training: 0.00032922200459132103 | validation: 0.003133019779960663]
	TIME [epoch: 8.31 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003607191906748801		[learning rate: 0.00029231]
		[batch 20/20] avg loss: 0.0035638106468231638		[learning rate: 0.00029195]
	Learning Rate: 0.000291955
	LOSS [training: -2.1690629962819313e-05 | validation: 0.0052975560477841865]
	TIME [epoch: 8.33 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002060837394590307		[learning rate: 0.0002916]
		[batch 20/20] avg loss: 0.001999981229656206		[learning rate: 0.00029125]
	Learning Rate: 0.000291248
	LOSS [training: -3.042808246705079e-05 | validation: 0.007197348958996735]
	TIME [epoch: 8.32 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015457485725973465		[learning rate: 0.0002909]
		[batch 20/20] avg loss: 0.00037428371184004643		[learning rate: 0.00029054]
	Learning Rate: 0.000290543
	LOSS [training: -0.0005857324303786502 | validation: 0.008506592631389552]
	TIME [epoch: 8.35 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017143411220032641		[learning rate: 0.00029019]
		[batch 20/20] avg loss: -0.001985611150464836		[learning rate: 0.00028984]
	Learning Rate: 0.00028984
	LOSS [training: -0.000135635014230786 | validation: 0.007074646517047684]
	TIME [epoch: 8.32 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007130681958186714		[learning rate: 0.00028949]
		[batch 20/20] avg loss: 0.0030654182095121454		[learning rate: 0.00028914]
	Learning Rate: 0.000289138
	LOSS [training: 0.0018892432026654082 | validation: 0.0025663039304824656]
	TIME [epoch: 8.34 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017214017369750968		[learning rate: 0.00028879]
		[batch 20/20] avg loss: 0.0006118442227671837		[learning rate: 0.00028844]
	Learning Rate: 0.000288438
	LOSS [training: 0.0011666229798711404 | validation: 0.007638539901533449]
	TIME [epoch: 8.34 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00017947697082263872		[learning rate: 0.00028809]
		[batch 20/20] avg loss: -0.0013558904330745953		[learning rate: 0.00028774]
	Learning Rate: 0.00028774
	LOSS [training: -0.0007676837019486174 | validation: 0.0050562975297474885]
	TIME [epoch: 8.32 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004975324085856791		[learning rate: 0.00028739]
		[batch 20/20] avg loss: -0.0016205640343770165		[learning rate: 0.00028704]
	Learning Rate: 0.000287043
	LOSS [training: -0.0032979440601169038 | validation: 0.007679770789208749]
	TIME [epoch: 8.31 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003131444156133248		[learning rate: 0.0002867]
		[batch 20/20] avg loss: -0.00032534371112405875		[learning rate: 0.00028635]
	Learning Rate: 0.000286348
	LOSS [training: -0.001728393933628653 | validation: 0.007181645932766389]
	TIME [epoch: 8.33 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00010756207859230273		[learning rate: 0.000286]
		[batch 20/20] avg loss: 0.001762681667972367		[learning rate: 0.00028566]
	Learning Rate: 0.000285655
	LOSS [training: 0.0008275597946900322 | validation: 0.004963065073014104]
	TIME [epoch: 8.32 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004328192017623177		[learning rate: 0.00028531]
		[batch 20/20] avg loss: -0.004485723666736044		[learning rate: 0.00028496]
	Learning Rate: 0.000284964
	LOSS [training: -0.0024592714342491807 | validation: 0.00041697599400090967]
	TIME [epoch: 8.31 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002248784945338804		[learning rate: 0.00028462]
		[batch 20/20] avg loss: 0.00107012288337158		[learning rate: 0.00028427]
	Learning Rate: 0.000284274
	LOSS [training: -0.0005893310309836117 | validation: 0.0034216378483803477]
	TIME [epoch: 8.31 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019256695588478674		[learning rate: 0.00028393]
		[batch 20/20] avg loss: 0.0098238207493385		[learning rate: 0.00028359]
	Learning Rate: 0.000283586
	LOSS [training: 0.003949075595245316 | validation: 0.0026995880842595604]
	TIME [epoch: 8.37 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002982642907835064		[learning rate: 0.00028324]
		[batch 20/20] avg loss: -0.00463836233928047		[learning rate: 0.0002829]
	Learning Rate: 0.000282899
	LOSS [training: -0.003810502623557766 | validation: 0.0037266142879663513]
	TIME [epoch: 8.32 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008976394980049317		[learning rate: 0.00028256]
		[batch 20/20] avg loss: -0.0009977048689107932		[learning rate: 0.00028221]
	Learning Rate: 0.000282214
	LOSS [training: -0.0009476721834578625 | validation: -0.002567842131812809]
	TIME [epoch: 8.32 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003213933307519342		[learning rate: 0.00028187]
		[batch 20/20] avg loss: -0.00013780548778576096		[learning rate: 0.00028153]
	Learning Rate: 0.000281531
	LOSS [training: -0.0016758693976525521 | validation: 0.0013915533156803178]
	TIME [epoch: 8.32 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001120223674771324		[learning rate: 0.00028119]
		[batch 20/20] avg loss: 0.00031417239101043365		[learning rate: 0.00028085]
	Learning Rate: 0.000280849
	LOSS [training: -0.00040302564188044517 | validation: 6.01618455578576e-05]
	TIME [epoch: 8.36 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00355568124897009		[learning rate: 0.00028051]
		[batch 20/20] avg loss: -0.004580292451933764		[learning rate: 0.00028017]
	Learning Rate: 0.00028017
	LOSS [training: -0.0005123056014818372 | validation: -0.0007848587967557864]
	TIME [epoch: 8.31 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006708576193249964		[learning rate: 0.00027983]
		[batch 20/20] avg loss: -0.00032830893188980846		[learning rate: 0.00027949]
	Learning Rate: 0.000279491
	LOSS [training: -0.0035184425625698875 | validation: 0.004966738646553489]
	TIME [epoch: 8.31 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00078655279113958		[learning rate: 0.00027915]
		[batch 20/20] avg loss: 0.0028468709662014834		[learning rate: 0.00027881]
	Learning Rate: 0.000278815
	LOSS [training: 0.0010301590875309518 | validation: 0.000582753117211624]
	TIME [epoch: 8.31 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003732745133232409		[learning rate: 0.00027848]
		[batch 20/20] avg loss: -0.001309007925134113		[learning rate: 0.00027814]
	Learning Rate: 0.00027814
	LOSS [training: 0.0012118686040491484 | validation: -0.003886018460340425]
	TIME [epoch: 8.33 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0045255034777736585		[learning rate: 0.0002778]
		[batch 20/20] avg loss: 0.0011351960250218338		[learning rate: 0.00027747]
	Learning Rate: 0.000277467
	LOSS [training: 0.0028303497513977463 | validation: 0.005308548374037617]
	TIME [epoch: 8.31 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.725670656684829e-05		[learning rate: 0.00027713]
		[batch 20/20] avg loss: 0.001174527593800481		[learning rate: 0.00027679]
	Learning Rate: 0.000276795
	LOSS [training: 0.0006108921501836648 | validation: 0.0006082801447879298]
	TIME [epoch: 8.31 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004176513573551253		[learning rate: 0.00027646]
		[batch 20/20] avg loss: -0.0017814202673480939		[learning rate: 0.00027612]
	Learning Rate: 0.000276125
	LOSS [training: -0.0029789669204496735 | validation: -0.007473872876109417]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240219_183143/states/model_tr_study2_1581.pth
	Model improved!!!
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003143182711671277		[learning rate: 0.00027579]
		[batch 20/20] avg loss: 0.0019473484640516378		[learning rate: 0.00027546]
	Learning Rate: 0.000275456
	LOSS [training: 0.001130833367609383 | validation: 0.009726163156393457]
	TIME [epoch: 8.36 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: -7.11592841841521e-05		[learning rate: 0.00027512]
		[batch 20/20] avg loss: 0.000586236586546891		[learning rate: 0.00027479]
	Learning Rate: 0.000274789
	LOSS [training: 0.00025753865118136943 | validation: 0.0012218982423588412]
	TIME [epoch: 8.32 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031539081400862126		[learning rate: 0.00027446]
		[batch 20/20] avg loss: -0.003140550239901789		[learning rate: 0.00027412]
	Learning Rate: 0.000274124
	LOSS [training: 6.678950092211655e-06 | validation: -0.0009504739306711485]
	TIME [epoch: 8.32 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012603297598151489		[learning rate: 0.00027379]
		[batch 20/20] avg loss: -0.002067597001133976		[learning rate: 0.00027346]
	Learning Rate: 0.000273461
	LOSS [training: -0.0004036336206594135 | validation: 0.006292772334499328]
	TIME [epoch: 8.34 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008485359290674772		[learning rate: 0.00027313]
		[batch 20/20] avg loss: 0.0015380516209686963		[learning rate: 0.0002728]
	Learning Rate: 0.000272799
	LOSS [training: 0.0050117054558217345 | validation: 0.0056621769798369526]
	TIME [epoch: 8.35 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036025818348170936		[learning rate: 0.00027247]
		[batch 20/20] avg loss: 0.0010468599736231127		[learning rate: 0.00027214]
	Learning Rate: 0.000272138
	LOSS [training: -0.001277860930596991 | validation: 0.010254707103817484]
	TIME [epoch: 8.32 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008124086879936004		[learning rate: 0.00027181]
		[batch 20/20] avg loss: 0.0001288989911354126		[learning rate: 0.00027148]
	Learning Rate: 0.000271479
	LOSS [training: -0.0003417548484290939 | validation: 0.004530614423872364]
	TIME [epoch: 8.31 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00018193480056183947		[learning rate: 0.00027115]
		[batch 20/20] avg loss: -0.00023750388702628716		[learning rate: 0.00027082]
	Learning Rate: 0.000270822
	LOSS [training: -0.00020971934379406332 | validation: 0.005876225378483988]
	TIME [epoch: 8.32 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015133606839127984		[learning rate: 0.00027049]
		[batch 20/20] avg loss: 0.00356819472588289		[learning rate: 0.00027017]
	Learning Rate: 0.000270167
	LOSS [training: 0.0010274170209850458 | validation: -0.0028655536208578754]
	TIME [epoch: 8.34 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.244463924771344e-05		[learning rate: 0.00026984]
		[batch 20/20] avg loss: -0.0010879503036732574		[learning rate: 0.00026951]
	Learning Rate: 0.000269513
	LOSS [training: -0.000537752832212772 | validation: 0.002360933964381688]
	TIME [epoch: 8.33 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00010854013368309301		[learning rate: 0.00026919]
		[batch 20/20] avg loss: 0.0006274891906173691		[learning rate: 0.00026886]
	Learning Rate: 0.00026886
	LOSS [training: 0.0002594745284671382 | validation: 0.005834452892094901]
	TIME [epoch: 8.33 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003564331894800863		[learning rate: 0.00026853]
		[batch 20/20] avg loss: -0.005443329834608402		[learning rate: 0.00026821]
	Learning Rate: 0.000268209
	LOSS [training: -0.0009394989699037694 | validation: 0.0021577543351115284]
	TIME [epoch: 8.32 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00016622547548860133		[learning rate: 0.00026788]
		[batch 20/20] avg loss: 0.006386610853980864		[learning rate: 0.00026756]
	Learning Rate: 0.00026756
	LOSS [training: 0.0031101926892461314 | validation: -0.0026942603104192307]
	TIME [epoch: 8.34 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00466988088673276		[learning rate: 0.00026724]
		[batch 20/20] avg loss: -0.0020080856168178916		[learning rate: 0.00026691]
	Learning Rate: 0.000266912
	LOSS [training: 0.0013308976349574338 | validation: 0.002283632603634035]
	TIME [epoch: 8.34 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00521988658666838		[learning rate: 0.00026659]
		[batch 20/20] avg loss: -0.004184258080523428		[learning rate: 0.00026627]
	Learning Rate: 0.000266266
	LOSS [training: -0.004702072333595904 | validation: 0.002555159848905908]
	TIME [epoch: 8.33 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006656427768731907		[learning rate: 0.00026594]
		[batch 20/20] avg loss: -0.00011551940114942197		[learning rate: 0.00026562]
	Learning Rate: 0.000265621
	LOSS [training: -0.0003905810890113063 | validation: -0.0032485629660251665]
	TIME [epoch: 8.31 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016388540606970883		[learning rate: 0.0002653]
		[batch 20/20] avg loss: 0.0042189990399248214		[learning rate: 0.00026498]
	Learning Rate: 0.000264978
	LOSS [training: 0.0012900724896138666 | validation: 0.004587768227007221]
	TIME [epoch: 8.33 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016603707346912826		[learning rate: 0.00026466]
		[batch 20/20] avg loss: 0.006964485666029724		[learning rate: 0.00026434]
	Learning Rate: 0.000264337
	LOSS [training: 0.0026520574656692204 | validation: 0.004640128693884525]
	TIME [epoch: 8.31 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005584402409013273		[learning rate: 0.00026402]
		[batch 20/20] avg loss: -0.0026403612556681563		[learning rate: 0.0002637]
	Learning Rate: 0.000263697
	LOSS [training: 0.0014720205766725585 | validation: 0.0007820133285056994]
	TIME [epoch: 8.31 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029840614854426814		[learning rate: 0.00026338]
		[batch 20/20] avg loss: -0.0005577623329319891		[learning rate: 0.00026306]
	Learning Rate: 0.000263059
	LOSS [training: 0.0012131495762553461 | validation: -0.0002112290926732049]
	TIME [epoch: 8.31 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.289109716623914e-05		[learning rate: 0.00026274]
		[batch 20/20] avg loss: 0.0005202336180864204		[learning rate: 0.00026242]
	Learning Rate: 0.000262422
	LOSS [training: 0.00029156235762632985 | validation: 0.00017561922002534637]
	TIME [epoch: 8.34 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005212891110319176		[learning rate: 0.0002621]
		[batch 20/20] avg loss: -0.005110458759444897		[learning rate: 0.00026179]
	Learning Rate: 0.000261787
	LOSS [training: 5.121617543713958e-05 | validation: -0.005884797710872404]
	TIME [epoch: 8.34 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004155907187465986		[learning rate: 0.00026147]
		[batch 20/20] avg loss: 0.0038138008165682505		[learning rate: 0.00026115]
	Learning Rate: 0.000261153
	LOSS [training: -0.00017105318544886729 | validation: 0.002423767605378115]
	TIME [epoch: 8.32 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003505616920188688		[learning rate: 0.00026084]
		[batch 20/20] avg loss: -0.0025474334440793935		[learning rate: 0.00026052]
	Learning Rate: 0.000260521
	LOSS [training: 0.0004790917380546473 | validation: -0.0017084572409976696]
	TIME [epoch: 8.32 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003706717201043074		[learning rate: 0.00026021]
		[batch 20/20] avg loss: -0.006446381705158965		[learning rate: 0.00025989]
	Learning Rate: 0.00025989
	LOSS [training: -0.00507654945310102 | validation: -0.004906701070595686]
	TIME [epoch: 8.35 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013194085368242645		[learning rate: 0.00025958]
		[batch 20/20] avg loss: 0.00039959960669575965		[learning rate: 0.00025926]
	Learning Rate: 0.000259261
	LOSS [training: -0.00045990446506425243 | validation: -0.005947965140568896]
	TIME [epoch: 8.34 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003232023623336914		[learning rate: 0.00025895]
		[batch 20/20] avg loss: -0.0016636301659536772		[learning rate: 0.00025863]
	Learning Rate: 0.000258633
	LOSS [training: -0.000670213901809993 | validation: 0.001940704901290681]
	TIME [epoch: 8.31 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024100072801602696		[learning rate: 0.00025832]
		[batch 20/20] avg loss: -0.0016107878308597018		[learning rate: 0.00025801]
	Learning Rate: 0.000258007
	LOSS [training: 0.0003996097246502838 | validation: 0.0025620970517158136]
	TIME [epoch: 8.32 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012432789537872072		[learning rate: 0.00025769]
		[batch 20/20] avg loss: -0.004347317651368297		[learning rate: 0.00025738]
	Learning Rate: 0.000257382
	LOSS [training: -0.001552019348790545 | validation: 0.007508571680101658]
	TIME [epoch: 8.32 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002487082867271251		[learning rate: 0.00025707]
		[batch 20/20] avg loss: 0.0005420942799218462		[learning rate: 0.00025676]
	Learning Rate: 0.000256759
	LOSS [training: 0.00014669299659736062 | validation: 0.008984858886182438]
	TIME [epoch: 8.32 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026123981029779475		[learning rate: 0.00025645]
		[batch 20/20] avg loss: 0.0003498750087395832		[learning rate: 0.00025614]
	Learning Rate: 0.000256138
	LOSS [training: -0.0011312615471191817 | validation: -0.00023783499479036415]
	TIME [epoch: 8.31 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001694787681794218		[learning rate: 0.00025583]
		[batch 20/20] avg loss: 0.001508004345051749		[learning rate: 0.00025552]
	Learning Rate: 0.000255518
	LOSS [training: -9.339166837123432e-05 | validation: 0.00922970665401251]
	TIME [epoch: 8.33 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034936770922928103		[learning rate: 0.00025521]
		[batch 20/20] avg loss: -0.0007576467084663333		[learning rate: 0.0002549]
	Learning Rate: 0.000254899
	LOSS [training: -0.002125661900379572 | validation: 0.0029222789737536883]
	TIME [epoch: 8.32 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011217386606616917		[learning rate: 0.00025459]
		[batch 20/20] avg loss: -0.0017861270132584375		[learning rate: 0.00025428]
	Learning Rate: 0.000254282
	LOSS [training: -0.000332194176298373 | validation: 0.006474571877052797]
	TIME [epoch: 8.32 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006058818021315095		[learning rate: 0.00025397]
		[batch 20/20] avg loss: -0.005401064780986649		[learning rate: 0.00025367]
	Learning Rate: 0.000253667
	LOSS [training: 0.0003288766201642231 | validation: 0.0047238620517403825]
	TIME [epoch: 8.34 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017403829694444604		[learning rate: 0.00025336]
		[batch 20/20] avg loss: -0.003615871786533014		[learning rate: 0.00025305]
	Learning Rate: 0.000253052
	LOSS [training: -0.0026781273779887373 | validation: -0.0036308114626907376]
	TIME [epoch: 8.33 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025132318482002		[learning rate: 0.00025275]
		[batch 20/20] avg loss: 0.00019484386880361753		[learning rate: 0.00025244]
	Learning Rate: 0.00025244
	LOSS [training: -0.0011591939896982912 | validation: 0.0067613532303403695]
	TIME [epoch: 8.33 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003815729725775587		[learning rate: 0.00025213]
		[batch 20/20] avg loss: -0.0037210164186456895		[learning rate: 0.00025183]
	Learning Rate: 0.000251829
	LOSS [training: -0.0016697217230340657 | validation: 0.0013329895300872425]
	TIME [epoch: 8.32 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005125558796367509		[learning rate: 0.00025152]
		[batch 20/20] avg loss: -0.005204458921227262		[learning rate: 0.00025122]
	Learning Rate: 0.000251219
	LOSS [training: -0.0023459515207952556 | validation: -0.0023349449174159933]
	TIME [epoch: 8.35 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014810745813026826		[learning rate: 0.00025091]
		[batch 20/20] avg loss: -0.0014109883200679636		[learning rate: 0.00025061]
	Learning Rate: 0.000250611
	LOSS [training: -0.0014460314506853232 | validation: -0.006267193456333883]
	TIME [epoch: 8.33 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016308295898640076		[learning rate: 0.00025031]
		[batch 20/20] avg loss: -0.003211654011630173		[learning rate: 0.00025]
	Learning Rate: 0.000250004
	LOSS [training: -0.0024212418007470904 | validation: -0.0019682054450720476]
	TIME [epoch: 8.32 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003887083152283367		[learning rate: 0.0002497]
		[batch 20/20] avg loss: 0.00092686908587269		[learning rate: 0.0002494]
	Learning Rate: 0.000249399
	LOSS [training: -0.0014801070332053383 | validation: 5.628266916217787e-05]
	TIME [epoch: 8.31 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021979954482942363		[learning rate: 0.0002491]
		[batch 20/20] avg loss: 0.0025096954604414187		[learning rate: 0.0002488]
	Learning Rate: 0.000248795
	LOSS [training: 0.0023538454543678275 | validation: 0.007554656395569946]
	TIME [epoch: 8.31 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006082364016055865		[learning rate: 0.00024849]
		[batch 20/20] avg loss: -0.00847809022197101		[learning rate: 0.00024819]
	Learning Rate: 0.000248193
	LOSS [training: -0.003934926910182712 | validation: -0.0019621249923282586]
	TIME [epoch: 8.34 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00106342177219353		[learning rate: 0.00024789]
		[batch 20/20] avg loss: 0.0024506144617671916		[learning rate: 0.00024759]
	Learning Rate: 0.000247592
	LOSS [training: 0.000693596344786831 | validation: 0.008131005280016524]
	TIME [epoch: 8.31 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035602985991141124		[learning rate: 0.00024729]
		[batch 20/20] avg loss: 7.212859748908125e-05		[learning rate: 0.00024699]
	Learning Rate: 0.000246993
	LOSS [training: -0.0017440850008125155 | validation: 0.001110530869491887]
	TIME [epoch: 8.31 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009016154437189968		[learning rate: 0.00024669]
		[batch 20/20] avg loss: -0.0014104249232203946		[learning rate: 0.00024639]
	Learning Rate: 0.000246395
	LOSS [training: -0.0011560201834696956 | validation: 0.00018414298801626558]
	TIME [epoch: 8.32 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000764567103466162		[learning rate: 0.0002461]
		[batch 20/20] avg loss: -0.005359918592880731		[learning rate: 0.0002458]
	Learning Rate: 0.000245798
	LOSS [training: -0.0030622428481734466 | validation: -0.0016431944678555453]
	TIME [epoch: 8.36 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026872569671852137		[learning rate: 0.0002455]
		[batch 20/20] avg loss: -0.0017490015664922052		[learning rate: 0.0002452]
	Learning Rate: 0.000245203
	LOSS [training: 0.00046912770034650394 | validation: -0.002984134974614488]
	TIME [epoch: 8.33 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020005794683502975		[learning rate: 0.00024491]
		[batch 20/20] avg loss: -0.003044104130253777		[learning rate: 0.00024461]
	Learning Rate: 0.00024461
	LOSS [training: -0.0025223417993020377 | validation: 0.0046939579229724245]
	TIME [epoch: 8.32 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013525204803284782		[learning rate: 0.00024431]
		[batch 20/20] avg loss: -0.0003766364231875707		[learning rate: 0.00024402]
	Learning Rate: 0.000244018
	LOSS [training: -0.0008645784517580243 | validation: -0.002816609607381905]
	TIME [epoch: 8.34 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002790367443735658		[learning rate: 0.00024372]
		[batch 20/20] avg loss: 0.00038769709164070435		[learning rate: 0.00024343]
	Learning Rate: 0.000243427
	LOSS [training: 0.0015890322676881813 | validation: 0.00870622894153547]
	TIME [epoch: 8.36 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011358512771384621		[learning rate: 0.00024313]
		[batch 20/20] avg loss: -0.00294234473584994		[learning rate: 0.00024284]
	Learning Rate: 0.000242838
	LOSS [training: -0.0009032467293557389 | validation: 0.0019320486799689837]
	TIME [epoch: 8.32 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001786010958439211		[learning rate: 0.00024254]
		[batch 20/20] avg loss: -0.0036159155720971786		[learning rate: 0.00024225]
	Learning Rate: 0.00024225
	LOSS [training: -0.002700963265268195 | validation: 0.002601236351897909]
	TIME [epoch: 8.33 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037291513256648146		[learning rate: 0.00024196]
		[batch 20/20] avg loss: 0.0010847353602070568		[learning rate: 0.00024166]
	Learning Rate: 0.000241663
	LOSS [training: -0.0013222079827288793 | validation: 0.006147933914061007]
	TIME [epoch: 8.32 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002936003127029137		[learning rate: 0.00024137]
		[batch 20/20] avg loss: 0.0016807303828100869		[learning rate: 0.00024108]
	Learning Rate: 0.000241078
	LOSS [training: 0.0006935650350535867 | validation: 0.005358024058001555]
	TIME [epoch: 8.34 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002668534990854547		[learning rate: 0.00024079]
		[batch 20/20] avg loss: -0.0006350777233983622		[learning rate: 0.00024049]
	Learning Rate: 0.000240495
	LOSS [training: -0.0016518063571264543 | validation: 0.0037185517191808052]
	TIME [epoch: 8.34 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016642465207071446		[learning rate: 0.0002402]
		[batch 20/20] avg loss: -0.0012733879881720819		[learning rate: 0.00023991]
	Learning Rate: 0.000239912
	LOSS [training: -0.0014688172544396138 | validation: -2.7522605844555403e-05]
	TIME [epoch: 8.34 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006505766539854778		[learning rate: 0.00023962]
		[batch 20/20] avg loss: -0.002061009144588132		[learning rate: 0.00023933]
	Learning Rate: 0.000239332
	LOSS [training: -0.004283387842221455 | validation: -0.0004987455205503245]
	TIME [epoch: 8.31 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003926915372805194		[learning rate: 0.00023904]
		[batch 20/20] avg loss: -0.0011596242879920128		[learning rate: 0.00023875]
	Learning Rate: 0.000238752
	LOSS [training: -0.0025432698303986035 | validation: -0.001372630993762957]
	TIME [epoch: 8.34 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018427042958618592		[learning rate: 0.00023846]
		[batch 20/20] avg loss: 0.00040327466414839705		[learning rate: 0.00023817]
	Learning Rate: 0.000238174
	LOSS [training: 0.0011229894800051281 | validation: -0.004736977712862958]
	TIME [epoch: 8.32 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003405120220548416		[learning rate: 0.00023789]
		[batch 20/20] avg loss: -0.0014552444218948121		[learning rate: 0.0002376]
	Learning Rate: 0.000237598
	LOSS [training: -0.002430182321221614 | validation: 0.006152347716538533]
	TIME [epoch: 8.32 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033160832916577843		[learning rate: 0.00023731]
		[batch 20/20] avg loss: 0.0010973880693535211		[learning rate: 0.00023702]
	Learning Rate: 0.000237022
	LOSS [training: -0.0011093476111521314 | validation: -0.0017746267907531512]
	TIME [epoch: 8.32 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002845218153502349		[learning rate: 0.00023674]
		[batch 20/20] avg loss: -0.003841309190124096		[learning rate: 0.00023645]
	Learning Rate: 0.000236449
	LOSS [training: -0.0004980455183108735 | validation: 0.0017370952598608437]
	TIME [epoch: 8.33 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027629648873013514		[learning rate: 0.00023616]
		[batch 20/20] avg loss: 0.0019360446296795984		[learning rate: 0.00023588]
	Learning Rate: 0.000235876
	LOSS [training: 0.002349504758490475 | validation: -0.00037984455327504346]
	TIME [epoch: 8.33 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010550966586514508		[learning rate: 0.00023559]
		[batch 20/20] avg loss: 0.0029740670504809974		[learning rate: 0.00023531]
	Learning Rate: 0.000235305
	LOSS [training: 0.0009594851959147732 | validation: -0.0015442815480576552]
	TIME [epoch: 8.34 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00020415295886441832		[learning rate: 0.00023502]
		[batch 20/20] avg loss: 0.001189028567435725		[learning rate: 0.00023474]
	Learning Rate: 0.000234736
	LOSS [training: 0.0004924378042856535 | validation: 0.007130921728739785]
	TIME [epoch: 8.32 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001820292718094143		[learning rate: 0.00023445]
		[batch 20/20] avg loss: -0.00029746696923152536		[learning rate: 0.00023417]
	Learning Rate: 0.000234167
	LOSS [training: 0.0007614128744313088 | validation: 0.006677089659844317]
	TIME [epoch: 8.34 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002937225745859873		[learning rate: 0.00023388]
		[batch 20/20] avg loss: -0.0032855909707789287		[learning rate: 0.0002336]
	Learning Rate: 0.0002336
	LOSS [training: -0.0001741826124595283 | validation: 0.0072888623173028785]
	TIME [epoch: 8.34 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004037380107733583		[learning rate: 0.00023332]
		[batch 20/20] avg loss: -0.001706931514593423		[learning rate: 0.00023303]
	Learning Rate: 0.000233035
	LOSS [training: -0.0028721558111635026 | validation: 0.001696766028909647]
	TIME [epoch: 8.33 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032703796506005425		[learning rate: 0.00023275]
		[batch 20/20] avg loss: -0.001923393038156881		[learning rate: 0.00023247]
	Learning Rate: 0.000232471
	LOSS [training: 0.0006734933062218307 | validation: -0.00046217306411410516]
	TIME [epoch: 8.32 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003937725756402504		[learning rate: 0.00023219]
		[batch 20/20] avg loss: 0.0037192476823647777		[learning rate: 0.00023191]
	Learning Rate: 0.000231908
	LOSS [training: -0.00010923903701886409 | validation: 0.005289497775625689]
	TIME [epoch: 8.33 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015785326457938228		[learning rate: 0.00023163]
		[batch 20/20] avg loss: 0.00698289799012473		[learning rate: 0.00023135]
	Learning Rate: 0.000231347
	LOSS [training: 0.0027021826721654535 | validation: 0.0007985747368904579]
	TIME [epoch: 8.32 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001094805392416259		[learning rate: 0.00023107]
		[batch 20/20] avg loss: 0.0006928656185426777		[learning rate: 0.00023079]
	Learning Rate: 0.000230787
	LOSS [training: -0.00020096988693679078 | validation: 0.002296089532239668]
	TIME [epoch: 8.31 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005802058899893384		[learning rate: 0.00023051]
		[batch 20/20] avg loss: -0.0036697356880683817		[learning rate: 0.00023023]
	Learning Rate: 0.000230228
	LOSS [training: 0.0010661616059125009 | validation: 0.005991250017006913]
	TIME [epoch: 8.32 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025488369847116		[learning rate: 0.00022995]
		[batch 20/20] avg loss: 0.0069429063690133646		[learning rate: 0.00022967]
	Learning Rate: 0.000229671
	LOSS [training: 0.0047458716768624815 | validation: 0.010795859840376126]
	TIME [epoch: 8.34 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014238746309270442		[learning rate: 0.00022939]
		[batch 20/20] avg loss: 0.00045741850150370566		[learning rate: 0.00022911]
	Learning Rate: 0.000229115
	LOSS [training: 0.0009406465662153748 | validation: 0.0058480962364665726]
	TIME [epoch: 8.35 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027792819696390815		[learning rate: 0.00022884]
		[batch 20/20] avg loss: 5.6026238558243115e-05		[learning rate: 0.00022856]
	Learning Rate: 0.00022856
	LOSS [training: 0.0014176541040986622 | validation: 0.0006054983786850299]
	TIME [epoch: 8.32 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.180388638090696e-05		[learning rate: 0.00022828]
		[batch 20/20] avg loss: 0.0007987912953267964		[learning rate: 0.00022801]
	Learning Rate: 0.000228007
	LOSS [training: 0.0004252975908538516 | validation: 0.002702638438431818]
	TIME [epoch: 8.32 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002267093889539884		[learning rate: 0.00022773]
		[batch 20/20] avg loss: -0.0006338701512696041		[learning rate: 0.00022745]
	Learning Rate: 0.000227455
	LOSS [training: -0.0004302897701117962 | validation: 0.005301788080011013]
	TIME [epoch: 8.37 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005208529924101156		[learning rate: 0.00022718]
		[batch 20/20] avg loss: -0.003898799892590754		[learning rate: 0.0002269]
	Learning Rate: 0.000226904
	LOSS [training: 0.0006548650157552006 | validation: -0.0016935490841737373]
	TIME [epoch: 8.34 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00011030133442412144		[learning rate: 0.00022663]
		[batch 20/20] avg loss: -0.003955288771337852		[learning rate: 0.00022635]
	Learning Rate: 0.000226355
	LOSS [training: -0.001922493718456865 | validation: -0.004780294857597977]
	TIME [epoch: 8.32 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006541382807083362		[learning rate: 0.00022608]
		[batch 20/20] avg loss: 0.0015943599043763475		[learning rate: 0.00022581]
	Learning Rate: 0.000225807
	LOSS [training: 0.001124249092542342 | validation: 0.009658942828525042]
	TIME [epoch: 8.32 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001427527010741027		[learning rate: 0.00022553]
		[batch 20/20] avg loss: -0.002122767967155801		[learning rate: 0.00022526]
	Learning Rate: 0.00022526
	LOSS [training: -0.0003476204782073869 | validation: -0.0047243142270795326]
	TIME [epoch: 8.34 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017772704920908221		[learning rate: 0.00022499]
		[batch 20/20] avg loss: -0.0031443209839436548		[learning rate: 0.00022471]
	Learning Rate: 0.000224715
	LOSS [training: -0.0006835252459264164 | validation: -0.0030611892445970125]
	TIME [epoch: 8.31 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019510018921706138		[learning rate: 0.00022444]
		[batch 20/20] avg loss: 0.0021429350986733875		[learning rate: 0.00022417]
	Learning Rate: 0.000224171
	LOSS [training: 9.596660325138698e-05 | validation: -0.0011237767064159827]
	TIME [epoch: 8.32 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00102018055003289		[learning rate: 0.0002239]
		[batch 20/20] avg loss: -0.0007080120347223636		[learning rate: 0.00022363]
	Learning Rate: 0.000223628
	LOSS [training: 0.00015608425765526306 | validation: 0.00029621064336065683]
	TIME [epoch: 8.34 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027048631945411528		[learning rate: 0.00022336]
		[batch 20/20] avg loss: 0.0019484202124138694		[learning rate: 0.00022309]
	Learning Rate: 0.000223087
	LOSS [training: -0.00037822149106364177 | validation: -0.0012172964467442688]
	TIME [epoch: 8.35 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013794621207217038		[learning rate: 0.00022282]
		[batch 20/20] avg loss: -0.0025272770228457983		[learning rate: 0.00022255]
	Learning Rate: 0.000222547
	LOSS [training: -0.0005739074510620473 | validation: -0.002064076115768385]
	TIME [epoch: 8.32 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003795999801556223		[learning rate: 0.00022228]
		[batch 20/20] avg loss: -0.00664205413161946		[learning rate: 0.00022201]
	Learning Rate: 0.000222008
	LOSS [training: -0.0014230271650316185 | validation: 0.0017977961841768838]
	TIME [epoch: 8.33 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020557793419620635		[learning rate: 0.00022174]
		[batch 20/20] avg loss: 0.0012030004087030405		[learning rate: 0.00022147]
	Learning Rate: 0.00022147
	LOSS [training: 0.0016293898753325519 | validation: 0.006173273420227036]
	TIME [epoch: 8.35 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003884824080004831		[learning rate: 0.0002212]
		[batch 20/20] avg loss: -0.0008829820419940855		[learning rate: 0.00022093]
	Learning Rate: 0.000220934
	LOSS [training: 0.0015009210190053727 | validation: 0.0030476517728051013]
	TIME [epoch: 8.34 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018273902513295374		[learning rate: 0.00022067]
		[batch 20/20] avg loss: -0.00043468399953921593		[learning rate: 0.0002204]
	Learning Rate: 0.000220399
	LOSS [training: 0.0006963531258951606 | validation: 0.008078826463223873]
	TIME [epoch: 8.32 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030294463908281826		[learning rate: 0.00022013]
		[batch 20/20] avg loss: 0.004948093351138679		[learning rate: 0.00021987]
	Learning Rate: 0.000219866
	LOSS [training: 0.00398876987098343 | validation: 0.009405738667517628]
	TIME [epoch: 8.31 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0050654044873270155		[learning rate: 0.0002196]
		[batch 20/20] avg loss: 0.0006489892018870613		[learning rate: 0.00021933]
	Learning Rate: 0.000219334
	LOSS [training: 0.002857196844607038 | validation: 0.005492583490558316]
	TIME [epoch: 8.32 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004984207488630067		[learning rate: 0.00021907]
		[batch 20/20] avg loss: -0.0028699682143715306		[learning rate: 0.0002188]
	Learning Rate: 0.000218803
	LOSS [training: 0.001057119637129268 | validation: -0.0013509763162865012]
	TIME [epoch: 8.34 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037865522975805083		[learning rate: 0.00021854]
		[batch 20/20] avg loss: 0.0012403035704743823		[learning rate: 0.00021827]
	Learning Rate: 0.000218273
	LOSS [training: 0.0025134279340274456 | validation: 0.0018990016281449406]
	TIME [epoch: 8.35 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007971062781923152		[learning rate: 0.00021801]
		[batch 20/20] avg loss: -0.0006844968792436388		[learning rate: 0.00021774]
	Learning Rate: 0.000217745
	LOSS [training: 0.003643282951339756 | validation: 0.00776168414291749]
	TIME [epoch: 8.32 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007091385383409541		[learning rate: 0.00021748]
		[batch 20/20] avg loss: -0.00037047945692766373		[learning rate: 0.00021722]
	Learning Rate: 0.000217217
	LOSS [training: 0.003360452963240939 | validation: 0.0038032351524792786]
	TIME [epoch: 8.33 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004198554463000012		[learning rate: 0.00021695]
		[batch 20/20] avg loss: 0.0015020748102959353		[learning rate: 0.00021669]
	Learning Rate: 0.000216692
	LOSS [training: -0.0013482398263520384 | validation: 0.0008548324047992783]
	TIME [epoch: 8.36 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016736473074889808		[learning rate: 0.00021643]
		[batch 20/20] avg loss: -0.002889379690352731		[learning rate: 0.00021617]
	Learning Rate: 0.000216167
	LOSS [training: -0.0022815134989208562 | validation: 0.006603289399347881]
	TIME [epoch: 8.35 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015238925507216855		[learning rate: 0.00021591]
		[batch 20/20] avg loss: 0.0013389137064866398		[learning rate: 0.00021564]
	Learning Rate: 0.000215644
	LOSS [training: 0.0014314031286041626 | validation: 0.0014789389912518668]
	TIME [epoch: 8.31 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005115056478049392		[learning rate: 0.00021538]
		[batch 20/20] avg loss: 0.0038208278456631104		[learning rate: 0.00021512]
	Learning Rate: 0.000215122
	LOSS [training: 0.004467942161856251 | validation: 0.00857125530694277]
	TIME [epoch: 8.32 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025524791231727193		[learning rate: 0.00021486]
		[batch 20/20] avg loss: -0.003127460966936978		[learning rate: 0.0002146]
	Learning Rate: 0.000214601
	LOSS [training: -0.0028399700450548486 | validation: 0.002045638971197527]
	TIME [epoch: 8.34 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01031317153012708		[learning rate: 0.00021434]
		[batch 20/20] avg loss: 0.0016307071504214946		[learning rate: 0.00021408]
	Learning Rate: 0.000214081
	LOSS [training: -0.004341232189852794 | validation: 0.0004174515789453202]
	TIME [epoch: 8.32 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004281304810776499		[learning rate: 0.00021382]
		[batch 20/20] avg loss: -0.0020508579715363255		[learning rate: 0.00021356]
	Learning Rate: 0.000213563
	LOSS [training: -0.003166081391156413 | validation: 0.0011774443987493098]
	TIME [epoch: 8.32 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0041501581518528455		[learning rate: 0.0002133]
		[batch 20/20] avg loss: 0.0004024614099658432		[learning rate: 0.00021305]
	Learning Rate: 0.000213046
	LOSS [training: -0.0018738483709435003 | validation: 0.0012240161992793505]
	TIME [epoch: 8.33 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027160279801278454		[learning rate: 0.00021279]
		[batch 20/20] avg loss: 0.002622597716944539		[learning rate: 0.00021253]
	Learning Rate: 0.00021253
	LOSS [training: 0.0026693128485361928 | validation: 0.012479704481177741]
	TIME [epoch: 8.36 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014375958506479825		[learning rate: 0.00021227]
		[batch 20/20] avg loss: -0.0010556374554107635		[learning rate: 0.00021202]
	Learning Rate: 0.000212016
	LOSS [training: 0.00019097919761860982 | validation: 0.0012137037218952584]
	TIME [epoch: 8.32 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025947843471802747		[learning rate: 0.00021176]
		[batch 20/20] avg loss: -0.0031327433961647914		[learning rate: 0.0002115]
	Learning Rate: 0.000211503
	LOSS [training: -0.00026897952449225825 | validation: 0.002944748353488173]
	TIME [epoch: 8.32 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00514196826995656		[learning rate: 0.00021125]
		[batch 20/20] avg loss: 0.003058470722356508		[learning rate: 0.00021099]
	Learning Rate: 0.000210991
	LOSS [training: -0.0010417487738000261 | validation: 0.007343159339478594]
	TIME [epoch: 8.35 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008912734638283563		[learning rate: 0.00021074]
		[batch 20/20] avg loss: -0.004164508162851024		[learning rate: 0.00021048]
	Learning Rate: 0.00021048
	LOSS [training: -0.0025278908133396904 | validation: 0.0056644046774675]
	TIME [epoch: 8.34 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004049534249065399		[learning rate: 0.00021022]
		[batch 20/20] avg loss: -0.0016181505612227764		[learning rate: 0.00020997]
	Learning Rate: 0.00020997
	LOSS [training: 0.0012156918439213114 | validation: 0.0036892023292743235]
	TIME [epoch: 8.32 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: -2.580537015789558e-05		[learning rate: 0.00020972]
		[batch 20/20] avg loss: -0.0008306081365990012		[learning rate: 0.00020946]
	Learning Rate: 0.000209462
	LOSS [training: -0.0004282067533784484 | validation: 0.00185051073400617]
	TIME [epoch: 8.31 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016587983869956794		[learning rate: 0.00020921]
		[batch 20/20] avg loss: 0.0019346782940677573		[learning rate: 0.00020895]
	Learning Rate: 0.000208955
	LOSS [training: 0.0017967383405317187 | validation: -0.0011614663162017936]
	TIME [epoch: 8.31 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030516249435827278		[learning rate: 0.0002087]
		[batch 20/20] avg loss: -0.0020223512988084107		[learning rate: 0.00020845]
	Learning Rate: 0.000208449
	LOSS [training: -0.002536988121195569 | validation: -0.0005996482117235011]
	TIME [epoch: 8.34 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011893892343039197		[learning rate: 0.0002082]
		[batch 20/20] avg loss: 0.0028514801150503677		[learning rate: 0.00020794]
	Learning Rate: 0.000207944
	LOSS [training: 0.0020204346746771434 | validation: -0.0010533893898058352]
	TIME [epoch: 8.32 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029460084029575917		[learning rate: 0.00020769]
		[batch 20/20] avg loss: 0.0006086047859685626		[learning rate: 0.00020744]
	Learning Rate: 0.000207441
	LOSS [training: 0.0017773065944630775 | validation: 0.004317268903732442]
	TIME [epoch: 8.34 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006027626900177911		[learning rate: 0.00020719]
		[batch 20/20] avg loss: -0.005047359698052208		[learning rate: 0.00020694]
	Learning Rate: 0.000206939
	LOSS [training: -0.0028250611940349992 | validation: 0.0014648647115579473]
	TIME [epoch: 8.33 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003921608221818449		[learning rate: 0.00020669]
		[batch 20/20] avg loss: -0.0017552336776056742		[learning rate: 0.00020644]
	Learning Rate: 0.000206438
	LOSS [training: -0.002838420949712061 | validation: -0.002561715835328405]
	TIME [epoch: 8.34 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00012802494481453167		[learning rate: 0.00020619]
		[batch 20/20] avg loss: -0.00225404278529205		[learning rate: 0.00020594]
	Learning Rate: 0.000205938
	LOSS [training: -0.001191033865053291 | validation: 0.002005912272926201]
	TIME [epoch: 8.32 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001978828856934256		[learning rate: 0.00020569]
		[batch 20/20] avg loss: -0.001999852019078447		[learning rate: 0.00020544]
	Learning Rate: 0.00020544
	LOSS [training: -1.0511581072095415e-05 | validation: 0.00282741056169346]
	TIME [epoch: 8.35 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001102108488677281		[learning rate: 0.00020519]
		[batch 20/20] avg loss: 0.0008733544396022574		[learning rate: 0.00020494]
	Learning Rate: 0.000204942
	LOSS [training: -0.00011437702453751184 | validation: 0.004956611104968794]
	TIME [epoch: 8.33 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002705506825820955		[learning rate: 0.00020469]
		[batch 20/20] avg loss: -0.0012233818745436874		[learning rate: 0.00020445]
	Learning Rate: 0.000204446
	LOSS [training: -0.0019644443501823207 | validation: 0.0018263085747193808]
	TIME [epoch: 8.34 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008228292525804714		[learning rate: 0.0002042]
		[batch 20/20] avg loss: 0.001310461066318722		[learning rate: 0.00020395]
	Learning Rate: 0.000203951
	LOSS [training: -0.003458915729742996 | validation: 0.005612337066269936]
	TIME [epoch: 8.32 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035268763010574563		[learning rate: 0.0002037]
		[batch 20/20] avg loss: -0.00384918961459903		[learning rate: 0.00020346]
	Learning Rate: 0.000203457
	LOSS [training: -0.00016115665677078682 | validation: 0.0046823937425318485]
	TIME [epoch: 8.32 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019181958015560326		[learning rate: 0.00020321]
		[batch 20/20] avg loss: -0.004993447285483085		[learning rate: 0.00020296]
	Learning Rate: 0.000202965
	LOSS [training: -0.003455821543519559 | validation: 0.0015962128928009005]
	TIME [epoch: 8.32 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002737989667979081		[learning rate: 0.00020272]
		[batch 20/20] avg loss: -0.0047611099468625965		[learning rate: 0.00020247]
	Learning Rate: 0.000202474
	LOSS [training: -0.002243655490032344 | validation: -0.0012525713136719862]
	TIME [epoch: 8.38 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026677132522472564		[learning rate: 0.00020223]
		[batch 20/20] avg loss: -0.00486726433059243		[learning rate: 0.00020198]
	Learning Rate: 0.000201983
	LOSS [training: -0.0037674887914198422 | validation: -0.000801431081697058]
	TIME [epoch: 8.35 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022517566970159824		[learning rate: 0.00020174]
		[batch 20/20] avg loss: 0.0014506647933296116		[learning rate: 0.00020149]
	Learning Rate: 0.000201495
	LOSS [training: -0.00040054595184318525 | validation: 0.005939619333940947]
	TIME [epoch: 8.32 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020081413818675885		[learning rate: 0.00020125]
		[batch 20/20] avg loss: -0.00418505932134518		[learning rate: 0.00020101]
	Learning Rate: 0.000201007
	LOSS [training: -0.003096600351606384 | validation: 0.005673179670811612]
	TIME [epoch: 8.32 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007446008493161869		[learning rate: 0.00020076]
		[batch 20/20] avg loss: -0.0018348059318132191		[learning rate: 0.00020052]
	Learning Rate: 0.00020052
	LOSS [training: -0.0005451025412485161 | validation: -0.001177036691752468]
	TIME [epoch: 8.36 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005807738375403914		[learning rate: 0.00020028]
		[batch 20/20] avg loss: -0.004745465083215812		[learning rate: 0.00020003]
	Learning Rate: 0.000200035
	LOSS [training: -0.005276601729309863 | validation: 0.00460616425429084]
	TIME [epoch: 8.34 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019802234829823548		[learning rate: 0.00019979]
		[batch 20/20] avg loss: -0.002251093995018294		[learning rate: 0.00019955]
	Learning Rate: 0.00019955
	LOSS [training: -0.0021156587390003243 | validation: -0.0004596105959940699]
	TIME [epoch: 8.31 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003463989985304273		[learning rate: 0.00019931]
		[batch 20/20] avg loss: 0.001513583975110535		[learning rate: 0.00019907]
	Learning Rate: 0.000199067
	LOSS [training: 0.0005835924882900538 | validation: 0.0014921471455287749]
	TIME [epoch: 8.32 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001027975582382152		[learning rate: 0.00019883]
		[batch 20/20] avg loss: 0.0015065551734099885		[learning rate: 0.00019859]
	Learning Rate: 0.000198585
	LOSS [training: 0.0002392897955139181 | validation: 0.004901391892327918]
	TIME [epoch: 8.34 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012290267794714347		[learning rate: 0.00019834]
		[batch 20/20] avg loss: -0.0022627271702280447		[learning rate: 0.0001981]
	Learning Rate: 0.000198105
	LOSS [training: -0.00174587697484974 | validation: 0.005926971706232053]
	TIME [epoch: 8.32 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00811812027951766		[learning rate: 0.00019786]
		[batch 20/20] avg loss: -0.0006769814184340549		[learning rate: 0.00019763]
	Learning Rate: 0.000197625
	LOSS [training: -0.004397550848975858 | validation: 0.001691425654305518]
	TIME [epoch: 8.32 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021202267594179897		[learning rate: 0.00019739]
		[batch 20/20] avg loss: -0.003953729469113218		[learning rate: 0.00019715]
	Learning Rate: 0.000197147
	LOSS [training: -0.003036978114265604 | validation: 0.0014878126455186783]
	TIME [epoch: 8.32 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004847483733153897		[learning rate: 0.00019691]
		[batch 20/20] avg loss: -0.0018526183281004417		[learning rate: 0.00019667]
	Learning Rate: 0.000196669
	LOSS [training: -0.00335005103062717 | validation: -0.001831645276837828]
	TIME [epoch: 8.36 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013029078887940512		[learning rate: 0.00019643]
		[batch 20/20] avg loss: -0.0023915773283419177		[learning rate: 0.00019619]
	Learning Rate: 0.000196193
	LOSS [training: -0.000544334719773933 | validation: 0.0007542369581665904]
	TIME [epoch: 8.35 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00028139255483170553		[learning rate: 0.00019596]
		[batch 20/20] avg loss: 0.0015499875429952133		[learning rate: 0.00019572]
	Learning Rate: 0.000195718
	LOSS [training: 0.0009156900489134595 | validation: -0.0005286283399664379]
	TIME [epoch: 8.33 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036101476611811343		[learning rate: 0.00019548]
		[batch 20/20] avg loss: -0.00014139084649978165		[learning rate: 0.00019524]
	Learning Rate: 0.000195245
	LOSS [training: -0.0018757692538404584 | validation: -0.006789126783373682]
	TIME [epoch: 8.32 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: -8.963845673127297e-05		[learning rate: 0.00019501]
		[batch 20/20] avg loss: -0.0007757474158847606		[learning rate: 0.00019477]
	Learning Rate: 0.000194772
	LOSS [training: -0.0004326929363080165 | validation: 0.0038023432752603966]
	TIME [epoch: 8.36 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033150618303692224		[learning rate: 0.00019454]
		[batch 20/20] avg loss: -0.0018795763566689446		[learning rate: 0.0001943]
	Learning Rate: 0.0001943
	LOSS [training: -0.002597319093519084 | validation: 0.001863927232157993]
	TIME [epoch: 8.35 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026172279658869145		[learning rate: 0.00019407]
		[batch 20/20] avg loss: 0.004099312348202505		[learning rate: 0.00019383]
	Learning Rate: 0.00019383
	LOSS [training: 0.0007410421911577953 | validation: 0.0019623991634917515]
	TIME [epoch: 8.31 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003833514060802873		[learning rate: 0.0001936]
		[batch 20/20] avg loss: -0.0047665317006356355		[learning rate: 0.00019336]
	Learning Rate: 0.000193361
	LOSS [training: -0.00046650881991638205 | validation: 0.005187343800095773]
	TIME [epoch: 8.32 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005244961594259602		[learning rate: 0.00019313]
		[batch 20/20] avg loss: -0.002026615533759186		[learning rate: 0.00019289]
	Learning Rate: 0.000192893
	LOSS [training: -0.0007510596871666128 | validation: -0.0018487257327762842]
	TIME [epoch: 8.34 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013243210631859758		[learning rate: 0.00019266]
		[batch 20/20] avg loss: -0.003982514732338151		[learning rate: 0.00019243]
	Learning Rate: 0.000192426
	LOSS [training: -0.002653417897762064 | validation: 0.007266658856077249]
	TIME [epoch: 8.31 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00020749668839076252		[learning rate: 0.00019219]
		[batch 20/20] avg loss: -0.005185878852537098		[learning rate: 0.00019196]
	Learning Rate: 0.00019196
	LOSS [training: -0.0024891910820731677 | validation: 0.0021946078730468235]
	TIME [epoch: 8.31 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005510014875934269		[learning rate: 0.00019173]
		[batch 20/20] avg loss: 0.0019991668567321754		[learning rate: 0.0001915]
	Learning Rate: 0.000191495
	LOSS [training: -0.001755424009601047 | validation: 0.0035858818459980667]
	TIME [epoch: 8.32 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012751345281107047		[learning rate: 0.00019126]
		[batch 20/20] avg loss: 0.003908701271177008		[learning rate: 0.00019103]
	Learning Rate: 0.000191032
	LOSS [training: 0.0013167833715331517 | validation: 0.004164182857132829]
	TIME [epoch: 8.37 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015470800055424525		[learning rate: 0.0001908]
		[batch 20/20] avg loss: -0.001380145078650619		[learning rate: 0.00019057]
	Learning Rate: 0.000190569
	LOSS [training: -0.0014636125420965358 | validation: -0.007423916407228115]
	TIME [epoch: 8.34 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005191548739775995		[learning rate: 0.00019034]
		[batch 20/20] avg loss: -0.0008704735493362403		[learning rate: 0.00019011]
	Learning Rate: 0.000190108
	LOSS [training: -0.003031011144556117 | validation: 0.0013111892046779642]
	TIME [epoch: 8.32 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007696647701001818		[learning rate: 0.00018988]
		[batch 20/20] avg loss: 0.0031829445856364053		[learning rate: 0.00018965]
	Learning Rate: 0.000189648
	LOSS [training: 0.0019763046778682935 | validation: -0.0040164982371301595]
	TIME [epoch: 8.32 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016581666310037956		[learning rate: 0.00018942]
		[batch 20/20] avg loss: -0.0008076172189121721		[learning rate: 0.00018919]
	Learning Rate: 0.000189189
	LOSS [training: -0.0012328919249579838 | validation: 0.0014857304699626604]
	TIME [epoch: 8.37 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004687112181049504		[learning rate: 0.00018896]
		[batch 20/20] avg loss: 0.0028235214915283587		[learning rate: 0.00018873]
	Learning Rate: 0.000188731
	LOSS [training: -0.0009317953447605725 | validation: 0.00178700760606911]
	TIME [epoch: 8.32 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015942731162951677		[learning rate: 0.0001885]
		[batch 20/20] avg loss: 0.0004530524776689639		[learning rate: 0.00018827]
	Learning Rate: 0.000188274
	LOSS [training: 0.001023662796982066 | validation: 0.0022256223736563485]
	TIME [epoch: 8.32 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006773940188741272		[learning rate: 0.00018805]
		[batch 20/20] avg loss: 0.004850303826764185		[learning rate: 0.00018782]
	Learning Rate: 0.000187818
	LOSS [training: 0.0020864549039450292 | validation: 0.007001193700347224]
	TIME [epoch: 8.32 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011320969181484012		[learning rate: 0.00018759]
		[batch 20/20] avg loss: 0.005264079873174943		[learning rate: 0.00018736]
	Learning Rate: 0.000187363
	LOSS [training: 0.003198088395661672 | validation: 0.0009725815465631249]
	TIME [epoch: 8.34 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005711625223052701		[learning rate: 0.00018714]
		[batch 20/20] avg loss: -0.0026141387459895134		[learning rate: 0.00018691]
	Learning Rate: 0.00018691
	LOSS [training: 0.0015487432385315936 | validation: -0.0011510563967812024]
	TIME [epoch: 8.32 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008190648318540143		[learning rate: 0.00018668]
		[batch 20/20] avg loss: -0.00242041508822518		[learning rate: 0.00018646]
	Learning Rate: 0.000186457
	LOSS [training: -0.0008006751281855832 | validation: -0.0029276397522041853]
	TIME [epoch: 8.31 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023419979995174386		[learning rate: 0.00018623]
		[batch 20/20] avg loss: 0.00047078053158951927		[learning rate: 0.00018601]
	Learning Rate: 0.000186006
	LOSS [training: -0.0009356087339639597 | validation: 0.004527758736484719]
	TIME [epoch: 8.32 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031866996550563285		[learning rate: 0.00018578]
		[batch 20/20] avg loss: -0.002484915182533596		[learning rate: 0.00018556]
	Learning Rate: 0.000185555
	LOSS [training: 0.0003508922362613659 | validation: -0.0013804408354629472]
	TIME [epoch: 8.37 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002623753454148557		[learning rate: 0.00018533]
		[batch 20/20] avg loss: -0.005437326862898034		[learning rate: 0.00018511]
	Learning Rate: 0.000185106
	LOSS [training: -0.0028498511041564452 | validation: -0.003933118729870073]
	TIME [epoch: 8.32 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015482874497535725		[learning rate: 0.00018488]
		[batch 20/20] avg loss: -0.0025198304926121393		[learning rate: 0.00018466]
	Learning Rate: 0.000184658
	LOSS [training: -0.0020340589711828555 | validation: 0.0034683879802088575]
	TIME [epoch: 8.32 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00036535508048585587		[learning rate: 0.00018443]
		[batch 20/20] avg loss: -0.0044576125527701965		[learning rate: 0.00018421]
	Learning Rate: 0.000184211
	LOSS [training: -0.0024114838166280263 | validation: -0.0026418653099416958]
	TIME [epoch: 8.32 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008354517528535138		[learning rate: 0.00018399]
		[batch 20/20] avg loss: -0.0028846471977662363		[learning rate: 0.00018377]
	Learning Rate: 0.000183765
	LOSS [training: -0.0018600494753098751 | validation: 0.001724434836245159]
	TIME [epoch: 8.37 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026641363077285147		[learning rate: 0.00018354]
		[batch 20/20] avg loss: 0.0021308258729321205		[learning rate: 0.00018332]
	Learning Rate: 0.00018332
	LOSS [training: 0.002397481090330318 | validation: 0.003625222547127334]
	TIME [epoch: 8.31 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005978380217009501		[learning rate: 0.0001831]
		[batch 20/20] avg loss: 0.0005630474440578031		[learning rate: 0.00018288]
	Learning Rate: 0.000182876
	LOSS [training: -0.0027076663864758492 | validation: 0.006484840907296159]
	TIME [epoch: 8.31 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026665182141266076		[learning rate: 0.00018266]
		[batch 20/20] avg loss: 0.00047766595265809266		[learning rate: 0.00018243]
	Learning Rate: 0.000182434
	LOSS [training: 0.0015720920833923503 | validation: 0.003791397806646825]
	TIME [epoch: 8.32 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004127680452944675		[learning rate: 0.00018221]
		[batch 20/20] avg loss: -0.0026830165747923533		[learning rate: 0.00018199]
	Learning Rate: 0.000181992
	LOSS [training: -0.0034053485138685136 | validation: 0.0006336903066627507]
	TIME [epoch: 8.34 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014985635946627098		[learning rate: 0.00018177]
		[batch 20/20] avg loss: 0.0031356223406625674		[learning rate: 0.00018155]
	Learning Rate: 0.000181552
	LOSS [training: 0.000818529372999929 | validation: 0.0024638294534223195]
	TIME [epoch: 8.32 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005383920695811867		[learning rate: 0.00018133]
		[batch 20/20] avg loss: -0.0029468726048093793		[learning rate: 0.00018111]
	Learning Rate: 0.000181112
	LOSS [training: -0.0012042402676140963 | validation: 0.0032494011787087226]
	TIME [epoch: 8.32 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001133102925192014		[learning rate: 0.00018089]
		[batch 20/20] avg loss: -0.004977128948697692		[learning rate: 0.00018067]
	Learning Rate: 0.000180674
	LOSS [training: -0.0019220130117528391 | validation: -0.0018646005872001619]
	TIME [epoch: 8.3 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021180261205558893		[learning rate: 0.00018045]
		[batch 20/20] avg loss: -0.001328307986331058		[learning rate: 0.00018024]
	Learning Rate: 0.000180236
	LOSS [training: -0.001723167053443474 | validation: 0.004004853597669238]
	TIME [epoch: 8.36 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001136653203043216		[learning rate: 0.00018002]
		[batch 20/20] avg loss: 2.4768656678556337e-05		[learning rate: 0.0001798]
	Learning Rate: 0.0001798
	LOSS [training: -0.0005559422731823296 | validation: -0.0033337832080516487]
	TIME [epoch: 8.32 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004462913549519341		[learning rate: 0.00017958]
		[batch 20/20] avg loss: -0.0019208446442422664		[learning rate: 0.00017936]
	Learning Rate: 0.000179365
	LOSS [training: -0.0031918790968808036 | validation: -0.0013244838688289034]
	TIME [epoch: 8.33 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002888477931964072		[learning rate: 0.00017915]
		[batch 20/20] avg loss: 0.0038695132029996915		[learning rate: 0.00017893]
	Learning Rate: 0.00017893
	LOSS [training: 0.0004905176355178105 | validation: 0.0035393648704759788]
	TIME [epoch: 8.34 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004707225185735542		[learning rate: 0.00017871]
		[batch 20/20] avg loss: -0.00034756339534225167		[learning rate: 0.0001785]
	Learning Rate: 0.000178497
	LOSS [training: -0.0025273942905388973 | validation: -0.0021085922246222555]
	TIME [epoch: 8.35 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003672599799697802		[learning rate: 0.00017828]
		[batch 20/20] avg loss: -0.004375847201634497		[learning rate: 0.00017807]
	Learning Rate: 0.000178065
	LOSS [training: -0.004024223500666148 | validation: 0.001679047473705324]
	TIME [epoch: 8.31 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008353039838577235		[learning rate: 0.00017785]
		[batch 20/20] avg loss: -0.003570467346996003		[learning rate: 0.00017763]
	Learning Rate: 0.000177634
	LOSS [training: -0.0013675816815691393 | validation: 0.0022503641342555103]
	TIME [epoch: 8.31 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009648006781655333		[learning rate: 0.00017742]
		[batch 20/20] avg loss: -0.00037549408164877263		[learning rate: 0.0001772]
	Learning Rate: 0.000177204
	LOSS [training: -0.0006701473799071532 | validation: 0.009870886737789146]
	TIME [epoch: 8.33 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038158509423684406		[learning rate: 0.00017699]
		[batch 20/20] avg loss: 7.526934962819986e-05		[learning rate: 0.00017678]
	Learning Rate: 0.000176775
	LOSS [training: 0.0019455601459983205 | validation: 0.002854154530285616]
	TIME [epoch: 8.32 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020457680650956206		[learning rate: 0.00017656]
		[batch 20/20] avg loss: 0.0016591126345542437		[learning rate: 0.00017635]
	Learning Rate: 0.000176347
	LOSS [training: -0.0001933277152706886 | validation: -0.00522258086132388]
	TIME [epoch: 8.31 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026311018262455805		[learning rate: 0.00017613]
		[batch 20/20] avg loss: -0.0037709233647428087		[learning rate: 0.00017592]
	Learning Rate: 0.00017592
	LOSS [training: -0.0032010125954941944 | validation: 0.003292237291129343]
	TIME [epoch: 8.32 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022434807157777855		[learning rate: 0.00017571]
		[batch 20/20] avg loss: -0.008148166694317966		[learning rate: 0.00017549]
	Learning Rate: 0.000175494
	LOSS [training: -0.005195823705047876 | validation: 0.006565511007976436]
	TIME [epoch: 8.36 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007072415430400628		[learning rate: 0.00017528]
		[batch 20/20] avg loss: 0.001493550835533846		[learning rate: 0.00017507]
	Learning Rate: 0.00017507
	LOSS [training: 0.0011003961892869544 | validation: 0.0029018957805561248]
	TIME [epoch: 8.34 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003092048668380603		[learning rate: 0.00017486]
		[batch 20/20] avg loss: -0.004137171260165067		[learning rate: 0.00017465]
	Learning Rate: 0.000174646
	LOSS [training: -0.0036146099642728356 | validation: 0.0068628128871855276]
	TIME [epoch: 8.32 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002991458333890619		[learning rate: 0.00017443]
		[batch 20/20] avg loss: -0.002610453479821023		[learning rate: 0.00017422]
	Learning Rate: 0.000174223
	LOSS [training: 0.00019050242703479782 | validation: 0.004190862181299913]
	TIME [epoch: 8.32 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022853377976763374		[learning rate: 0.00017401]
		[batch 20/20] avg loss: -5.6066596802035235e-05		[learning rate: 0.0001738]
	Learning Rate: 0.000173801
	LOSS [training: 0.0011146356004371513 | validation: 0.0029184878807881204]
	TIME [epoch: 8.37 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.813073887786495e-05		[learning rate: 0.00017359]
		[batch 20/20] avg loss: -0.000567719520856074		[learning rate: 0.00017338]
	Learning Rate: 0.00017338
	LOSS [training: -0.00023979439098910452 | validation: -7.005335672388528e-05]
	TIME [epoch: 8.32 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036396479008413154		[learning rate: 0.00017317]
		[batch 20/20] avg loss: 0.0009135253592771585		[learning rate: 0.00017296]
	Learning Rate: 0.000172961
	LOSS [training: -0.0013630612707820784 | validation: 0.0011339800565314038]
	TIME [epoch: 8.31 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004038221752810367		[learning rate: 0.00017275]
		[batch 20/20] avg loss: -0.0030475279956714524		[learning rate: 0.00017254]
	Learning Rate: 0.000172542
	LOSS [training: -0.0035428748742409094 | validation: -0.0008759490123034083]
	TIME [epoch: 8.32 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00019707537826610491		[learning rate: 0.00017233]
		[batch 20/20] avg loss: 0.001152526852704921		[learning rate: 0.00017212]
	Learning Rate: 0.000172124
	LOSS [training: 0.0006748011154855129 | validation: 0.005995420971242972]
	TIME [epoch: 8.33 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028035811184467263		[learning rate: 0.00017192]
		[batch 20/20] avg loss: -0.0005364349006590659		[learning rate: 0.00017171]
	Learning Rate: 0.000171708
	LOSS [training: -0.0016700080095528966 | validation: 0.0037286440954194477]
	TIME [epoch: 8.32 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002947921891958859		[learning rate: 0.0001715]
		[batch 20/20] avg loss: -0.004448076075560702		[learning rate: 0.00017129]
	Learning Rate: 0.000171292
	LOSS [training: -0.0036979989837597808 | validation: 0.0018607048142334688]
	TIME [epoch: 8.31 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016323505577273412		[learning rate: 0.00017108]
		[batch 20/20] avg loss: -9.277759582926101e-05		[learning rate: 0.00017088]
	Learning Rate: 0.000170877
	LOSS [training: 0.00076978648094904 | validation: 0.0021688879301060234]
	TIME [epoch: 8.31 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034576521089337148		[learning rate: 0.00017067]
		[batch 20/20] avg loss: 0.0008049148007216149		[learning rate: 0.00017046]
	Learning Rate: 0.000170464
	LOSS [training: -0.0013263686541060498 | validation: 0.005030563789153489]
	TIME [epoch: 8.34 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014961144033839747		[learning rate: 0.00017026]
		[batch 20/20] avg loss: -0.0005335890282921767		[learning rate: 0.00017005]
	Learning Rate: 0.000170051
	LOSS [training: -0.0010148517158380757 | validation: -0.004030254431850041]
	TIME [epoch: 8.34 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009838655151369505		[learning rate: 0.00016984]
		[batch 20/20] avg loss: -0.007913844606756652		[learning rate: 0.00016964]
	Learning Rate: 0.000169639
	LOSS [training: -0.0034649895458098512 | validation: 0.002954184004946078]
	TIME [epoch: 8.31 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018211209375053386		[learning rate: 0.00016943]
		[batch 20/20] avg loss: -0.00510122511617229		[learning rate: 0.00016923]
	Learning Rate: 0.000169229
	LOSS [training: -0.003461173026838814 | validation: -0.007437755566176553]
	TIME [epoch: 8.31 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003467020895612045		[learning rate: 0.00016902]
		[batch 20/20] avg loss: -0.0036475841280719885		[learning rate: 0.00016882]
	Learning Rate: 0.000168819
	LOSS [training: -0.003557302511842017 | validation: -0.0017385933728221495]
	TIME [epoch: 8.35 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014792990490681403		[learning rate: 0.00016861]
		[batch 20/20] avg loss: -0.003706187590826208		[learning rate: 0.00016841]
	Learning Rate: 0.00016841
	LOSS [training: -0.002592743319947174 | validation: -0.00015484916239547975]
	TIME [epoch: 8.33 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027588056144550945		[learning rate: 0.00016821]
		[batch 20/20] avg loss: 0.0021430201192571365		[learning rate: 0.000168]
	Learning Rate: 0.000168003
	LOSS [training: -0.00030789274759897864 | validation: 0.0006726758935996448]
	TIME [epoch: 8.3 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007431978381107201		[learning rate: 0.0001678]
		[batch 20/20] avg loss: -0.0020437146324159415		[learning rate: 0.0001676]
	Learning Rate: 0.000167596
	LOSS [training: -0.0013934562352633307 | validation: -0.0007615592866978211]
	TIME [epoch: 8.31 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029154880558322875		[learning rate: 0.00016739]
		[batch 20/20] avg loss: 0.004870786648213521		[learning rate: 0.00016719]
	Learning Rate: 0.00016719
	LOSS [training: 0.0009776492961906164 | validation: 0.0028753957148949733]
	TIME [epoch: 8.33 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00027813441341408864		[learning rate: 0.00016699]
		[batch 20/20] avg loss: -0.001365080493965842		[learning rate: 0.00016679]
	Learning Rate: 0.000166785
	LOSS [training: -0.0005434730402758766 | validation: 0.0003497940066258885]
	TIME [epoch: 8.31 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016425709904894128		[learning rate: 0.00016658]
		[batch 20/20] avg loss: 0.0021204249472362396		[learning rate: 0.00016638]
	Learning Rate: 0.000166382
	LOSS [training: 0.00023892697837341322 | validation: 0.0015483417660883388]
	TIME [epoch: 8.3 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003695408918255143		[learning rate: 0.00016618]
		[batch 20/20] avg loss: -0.0005045171251420214		[learning rate: 0.00016598]
	Learning Rate: 0.000165979
	LOSS [training: -6.748811665825364e-05 | validation: 0.002704050086078651]
	TIME [epoch: 8.31 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019496354337532132		[learning rate: 0.00016578]
		[batch 20/20] avg loss: -0.004148900918629951		[learning rate: 0.00016558]
	Learning Rate: 0.000165577
	LOSS [training: -0.003049268176191583 | validation: 0.0014579477653395372]
	TIME [epoch: 8.33 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0044506970910906385		[learning rate: 0.00016538]
		[batch 20/20] avg loss: 0.00021270037416083065		[learning rate: 0.00016518]
	Learning Rate: 0.000165176
	LOSS [training: -0.0021189983584649036 | validation: 0.004959385218578116]
	TIME [epoch: 8.34 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000519067664582201		[learning rate: 0.00016498]
		[batch 20/20] avg loss: -0.0024453930276744197		[learning rate: 0.00016478]
	Learning Rate: 0.000164776
	LOSS [training: -0.0009631626815461093 | validation: -0.004994144752577904]
	TIME [epoch: 8.31 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023506315103462173		[learning rate: 0.00016458]
		[batch 20/20] avg loss: -0.004163611986990496		[learning rate: 0.00016438]
	Learning Rate: 0.000164377
	LOSS [training: -0.0032571217486683556 | validation: 0.006313907106534018]
	TIME [epoch: 8.31 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005848887523466949		[learning rate: 0.00016418]
		[batch 20/20] avg loss: -0.001710199683067745		[learning rate: 0.00016398]
	Learning Rate: 0.000163979
	LOSS [training: -0.003779543603267347 | validation: 0.0027301217601786274]
	TIME [epoch: 8.34 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004553016159073801		[learning rate: 0.00016378]
		[batch 20/20] avg loss: 0.00017392044120516796		[learning rate: 0.00016358]
	Learning Rate: 0.000163583
	LOSS [training: -0.002189547858934317 | validation: 0.004791931357031008]
	TIME [epoch: 8.34 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013117942442776409		[learning rate: 0.00016338]
		[batch 20/20] avg loss: -0.007181609909330096		[learning rate: 0.00016319]
	Learning Rate: 0.000163187
	LOSS [training: -0.004246702076803869 | validation: 0.0004037443095627699]
	TIME [epoch: 8.31 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003539929106788065		[learning rate: 0.00016299]
		[batch 20/20] avg loss: 0.010677298365067296		[learning rate: 0.00016279]
	Learning Rate: 0.000162791
	LOSS [training: 0.0035686846291396153 | validation: 0.006717623045755819]
	TIME [epoch: 8.31 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026151050532292374		[learning rate: 0.00016259]
		[batch 20/20] avg loss: -0.0017271265839817439		[learning rate: 0.0001624]
	Learning Rate: 0.000162397
	LOSS [training: 0.000443989234623747 | validation: 0.006813974594785017]
	TIME [epoch: 8.33 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007786913249520812		[learning rate: 0.0001622]
		[batch 20/20] avg loss: -0.005566178056862199		[learning rate: 0.000162]
	Learning Rate: 0.000162004
	LOSS [training: -0.00317243469090714 | validation: -0.0006760994287282988]
	TIME [epoch: 8.31 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: -5.155635459607975e-05		[learning rate: 0.00016181]
		[batch 20/20] avg loss: -0.001890952893360602		[learning rate: 0.00016161]
	Learning Rate: 0.000161612
	LOSS [training: -0.000971254623978341 | validation: 0.00646653265555536]
	TIME [epoch: 8.31 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034796138808429284		[learning rate: 0.00016142]
		[batch 20/20] avg loss: 0.0005320267050927891		[learning rate: 0.00016122]
	Learning Rate: 0.000161221
	LOSS [training: 0.0020058202929678593 | validation: 0.004534810808585981]
	TIME [epoch: 8.32 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027652578029799353		[learning rate: 0.00016103]
		[batch 20/20] avg loss: 0.0035974632619552		[learning rate: 0.00016083]
	Learning Rate: 0.000160831
	LOSS [training: 0.00041610272948763226 | validation: 0.0016867518318900872]
	TIME [epoch: 8.34 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00026075860533195805		[learning rate: 0.00016064]
		[batch 20/20] avg loss: -0.00017815494276550282		[learning rate: 0.00016044]
	Learning Rate: 0.000160441
	LOSS [training: -0.0002194567740487304 | validation: -0.0015501868712907855]
	TIME [epoch: 8.33 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006877259477591856		[learning rate: 0.00016025]
		[batch 20/20] avg loss: 0.0027729288691920265		[learning rate: 0.00016005]
	Learning Rate: 0.000160053
	LOSS [training: 0.0010426014607164205 | validation: -0.00168060415869849]
	TIME [epoch: 8.33 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002741572043264871		[learning rate: 0.00015986]
		[batch 20/20] avg loss: 0.00029045350477976354		[learning rate: 0.00015967]
	Learning Rate: 0.000159665
	LOSS [training: 8.148150226638343e-06 | validation: -0.0029185732172637706]
	TIME [epoch: 8.32 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00041722268802031875		[learning rate: 0.00015947]
		[batch 20/20] avg loss: -0.0017115638811238145		[learning rate: 0.00015928]
	Learning Rate: 0.000159279
	LOSS [training: -0.0006471705965517478 | validation: -0.001283464722019096]
	TIME [epoch: 8.33 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010935760135128505		[learning rate: 0.00015909]
		[batch 20/20] avg loss: -0.007214220101412416		[learning rate: 0.00015889]
	Learning Rate: 0.000158893
	LOSS [training: -0.0030603220439497825 | validation: 0.0020256206221831883]
	TIME [epoch: 8.34 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.155768076743678e-05		[learning rate: 0.0001587]
		[batch 20/20] avg loss: -0.0029663750582731496		[learning rate: 0.00015851]
	Learning Rate: 0.000158509
	LOSS [training: -0.0014374086887528569 | validation: 0.003054383680773788]
	TIME [epoch: 8.32 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020115505456560602		[learning rate: 0.00015832]
		[batch 20/20] avg loss: -0.004346993451540089		[learning rate: 0.00015812]
	Learning Rate: 0.000158125
	LOSS [training: -0.0011677214529420142 | validation: 0.003464600944993674]
	TIME [epoch: 8.31 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003416421990264837		[learning rate: 0.00015793]
		[batch 20/20] avg loss: 0.0012416987458622917		[learning rate: 0.00015774]
	Learning Rate: 0.000157742
	LOSS [training: -0.0010873616222012727 | validation: 0.002686663587904811]
	TIME [epoch: 8.33 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006417276658889128		[learning rate: 0.00015755]
		[batch 20/20] avg loss: -0.0011766982395243167		[learning rate: 0.00015736]
	Learning Rate: 0.00015736
	LOSS [training: -0.003796987449206722 | validation: -0.0036163543626922187]
	TIME [epoch: 8.33 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008963073216921011		[learning rate: 0.00015717]
		[batch 20/20] avg loss: -0.00023590408784918848		[learning rate: 0.00015698]
	Learning Rate: 0.000156979
	LOSS [training: -0.0005661057047706452 | validation: 0.003864398184081713]
	TIME [epoch: 8.32 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006968280509375934		[learning rate: 0.00015679]
		[batch 20/20] avg loss: -0.0005785402849306683		[learning rate: 0.0001566]
	Learning Rate: 0.000156599
	LOSS [training: -0.0006376841679341308 | validation: 0.004462445739266829]
	TIME [epoch: 8.32 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005676767742663071		[learning rate: 0.00015641]
		[batch 20/20] avg loss: -0.003718288332939527		[learning rate: 0.00015622]
	Learning Rate: 0.00015622
	LOSS [training: 0.000979239704861772 | validation: 0.005023423830027852]
	TIME [epoch: 8.35 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001701443710500666		[learning rate: 0.00015603]
		[batch 20/20] avg loss: -0.006314346189819256		[learning rate: 0.00015584]
	Learning Rate: 0.000155842
	LOSS [training: -0.004007894950159961 | validation: 0.0038016269004027673]
	TIME [epoch: 8.35 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033360203347917116		[learning rate: 0.00015565]
		[batch 20/20] avg loss: -0.004258223443375378		[learning rate: 0.00015546]
	Learning Rate: 0.000155465
	LOSS [training: -0.0037971218890835456 | validation: -0.0006628926617902389]
	TIME [epoch: 8.33 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024982397177414984		[learning rate: 0.00015528]
		[batch 20/20] avg loss: -0.001054993099948232		[learning rate: 0.00015509]
	Learning Rate: 0.000155088
	LOSS [training: -0.0017766164088448653 | validation: 0.0072488065590110735]
	TIME [epoch: 8.33 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033608997521147876		[learning rate: 0.0001549]
		[batch 20/20] avg loss: 0.002619000282623311		[learning rate: 0.00015471]
	Learning Rate: 0.000154713
	LOSS [training: -0.00037094973474573867 | validation: -0.0033714519915617014]
	TIME [epoch: 8.36 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003022384121141292		[learning rate: 0.00015453]
		[batch 20/20] avg loss: 0.003896866771906224		[learning rate: 0.00015434]
	Learning Rate: 0.000154338
	LOSS [training: 0.000437241325382466 | validation: 0.004978908491941049]
	TIME [epoch: 8.35 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007301800051248978		[learning rate: 0.00015415]
		[batch 20/20] avg loss: -0.003280032075350226		[learning rate: 0.00015396]
	Learning Rate: 0.000153965
	LOSS [training: -0.002005106040237562 | validation: 0.0048393780122345294]
	TIME [epoch: 8.31 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001989226016668367		[learning rate: 0.00015378]
		[batch 20/20] avg loss: -0.0013197824716953996		[learning rate: 0.00015359]
	Learning Rate: 0.000153592
	LOSS [training: -0.0016545042441818835 | validation: 0.005577508420341092]
	TIME [epoch: 8.32 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023537778868753913		[learning rate: 0.00015341]
		[batch 20/20] avg loss: 0.00011146690938167899		[learning rate: 0.00015322]
	Learning Rate: 0.00015322
	LOSS [training: -0.0011211554887468561 | validation: -0.0027798813783507645]
	TIME [epoch: 8.33 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037348060275946165		[learning rate: 0.00015303]
		[batch 20/20] avg loss: -0.0042755323314493925		[learning rate: 0.00015285]
	Learning Rate: 0.000152849
	LOSS [training: -0.004005169179522005 | validation: -0.0038688062956590305]
	TIME [epoch: 8.32 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029169852434997303		[learning rate: 0.00015266]
		[batch 20/20] avg loss: 0.00010696928765172413		[learning rate: 0.00015248]
	Learning Rate: 0.000152479
	LOSS [training: -0.0014050079779240028 | validation: -0.0025692936159526676]
	TIME [epoch: 8.32 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032942467152493465		[learning rate: 0.00015229]
		[batch 20/20] avg loss: -0.00047568778548026387		[learning rate: 0.00015211]
	Learning Rate: 0.00015211
	LOSS [training: 0.0014092794648845413 | validation: 0.008489361488243042]
	TIME [epoch: 8.31 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009782934289661034		[learning rate: 0.00015193]
		[batch 20/20] avg loss: -0.0019748375066759934		[learning rate: 0.00015174]
	Learning Rate: 0.000151742
	LOSS [training: -0.0004982720388549448 | validation: -0.00030376172790144143]
	TIME [epoch: 8.34 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.256841135574165e-05		[learning rate: 0.00015156]
		[batch 20/20] avg loss: -0.002102876593558591		[learning rate: 0.00015137]
	Learning Rate: 0.000151374
	LOSS [training: -0.0010201540911014247 | validation: 0.004449530747711474]
	TIME [epoch: 8.34 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003249810082797779		[learning rate: 0.00015119]
		[batch 20/20] avg loss: -0.00017516706669731995		[learning rate: 0.00015101]
	Learning Rate: 0.000151008
	LOSS [training: -0.001712488574747549 | validation: -0.002319187736318677]
	TIME [epoch: 8.33 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012168677524123292		[learning rate: 0.00015083]
		[batch 20/20] avg loss: -0.0029276363810799176		[learning rate: 0.00015064]
	Learning Rate: 0.000150642
	LOSS [training: -0.0020722520667461235 | validation: 0.003863533209213982]
	TIME [epoch: 8.32 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004040590786410271		[learning rate: 0.00015046]
		[batch 20/20] avg loss: -0.0029741168925009466		[learning rate: 0.00015028]
	Learning Rate: 0.000150278
	LOSS [training: -0.003507353839455609 | validation: -0.0016319236469155968]
	TIME [epoch: 8.34 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031695924393542866		[learning rate: 0.0001501]
		[batch 20/20] avg loss: -0.0014915288500821065		[learning rate: 0.00014991]
	Learning Rate: 0.000149914
	LOSS [training: -0.002330560644718196 | validation: 0.0014636023718256415]
	TIME [epoch: 8.35 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002503425154868345		[learning rate: 0.00014973]
		[batch 20/20] avg loss: -0.001067798796987926		[learning rate: 0.00014955]
	Learning Rate: 0.000149551
	LOSS [training: 0.0007178131789402094 | validation: 0.0032434079462528988]
	TIME [epoch: 8.32 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0058849468186657015		[learning rate: 0.00014937]
		[batch 20/20] avg loss: -0.002520834802339158		[learning rate: 0.00014919]
	Learning Rate: 0.000149189
	LOSS [training: 0.001682056008163272 | validation: -0.0001780818830914631]
	TIME [epoch: 8.31 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: -8.810905091443839e-05		[learning rate: 0.00014901]
		[batch 20/20] avg loss: 0.0023633651508755908		[learning rate: 0.00014883]
	Learning Rate: 0.000148828
	LOSS [training: 0.0011376280499805764 | validation: 0.00024127027199707766]
	TIME [epoch: 8.34 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008213983595025785		[learning rate: 0.00014865]
		[batch 20/20] avg loss: 0.0012756081849318074		[learning rate: 0.00014847]
	Learning Rate: 0.000148468
	LOSS [training: 0.00022710491271461436 | validation: 0.00014355939483476528]
	TIME [epoch: 8.31 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031057663597431457		[learning rate: 0.00014829]
		[batch 20/20] avg loss: -0.0003387491044892875		[learning rate: 0.00014811]
	Learning Rate: 0.000148108
	LOSS [training: 0.001383508627626929 | validation: 0.0006974568331574414]
	TIME [epoch: 8.31 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00537124054921589		[learning rate: 0.00014793]
		[batch 20/20] avg loss: 0.0005932265980974462		[learning rate: 0.00014775]
	Learning Rate: 0.00014775
	LOSS [training: -0.002389006975559222 | validation: 0.0018784439433834682]
	TIME [epoch: 8.32 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003802494144612979		[learning rate: 0.00014757]
		[batch 20/20] avg loss: -0.0007481107517818752		[learning rate: 0.00014739]
	Learning Rate: 0.000147392
	LOSS [training: -0.002275302448197428 | validation: 0.004125152288331976]
	TIME [epoch: 8.34 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022025581301493976		[learning rate: 0.00014721]
		[batch 20/20] avg loss: 0.0029854393068852896		[learning rate: 0.00014704]
	Learning Rate: 0.000147035
	LOSS [training: 0.0003914405883679459 | validation: 0.005271473376784035]
	TIME [epoch: 8.32 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001944236130998098		[learning rate: 0.00014686]
		[batch 20/20] avg loss: -0.00033028076787245		[learning rate: 0.00014668]
	Learning Rate: 0.000146679
	LOSS [training: -0.0011372584494352737 | validation: 0.0027371331226967183]
	TIME [epoch: 8.34 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004230279419918886		[learning rate: 0.0001465]
		[batch 20/20] avg loss: -0.002486535908476123		[learning rate: 0.00014632]
	Learning Rate: 0.000146324
	LOSS [training: -0.0033584076641975055 | validation: 0.007708491452921013]
	TIME [epoch: 8.32 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019926019000925353		[learning rate: 0.00014615]
		[batch 20/20] avg loss: 0.0017818531751175137		[learning rate: 0.00014597]
	Learning Rate: 0.00014597
	LOSS [training: 0.0018872275376050243 | validation: 0.0022964844498046233]
	TIME [epoch: 8.34 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012105410428418205		[learning rate: 0.00014579]
		[batch 20/20] avg loss: -0.0012694098125681744		[learning rate: 0.00014562]
	Learning Rate: 0.000145616
	LOSS [training: -2.943438486317702e-05 | validation: 0.0031337169535748306]
	TIME [epoch: 8.33 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019649598179177353		[learning rate: 0.00014544]
		[batch 20/20] avg loss: 0.00322984116244602		[learning rate: 0.00014526]
	Learning Rate: 0.000145264
	LOSS [training: 0.0025974004901818774 | validation: 0.0016421105205412507]
	TIME [epoch: 8.35 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007817087429508191		[learning rate: 0.00014509]
		[batch 20/20] avg loss: -0.005082252685102843		[learning rate: 0.00014491]
	Learning Rate: 0.000144912
	LOSS [training: -0.0021502719710760117 | validation: -0.002962864733267412]
	TIME [epoch: 8.31 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005223169440254178		[learning rate: 0.00014474]
		[batch 20/20] avg loss: -0.003233614057718924		[learning rate: 0.00014456]
	Learning Rate: 0.000144561
	LOSS [training: -0.004228391748986551 | validation: -0.0007102392703819369]
	TIME [epoch: 8.33 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003761107074762729		[learning rate: 0.00014439]
		[batch 20/20] avg loss: -0.0024967991705002537		[learning rate: 0.00014421]
	Learning Rate: 0.000144212
	LOSS [training: -0.003128953122631492 | validation: -0.00033697828045899884]
	TIME [epoch: 8.31 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008565792505915555		[learning rate: 0.00014404]
		[batch 20/20] avg loss: -0.001107988004317628		[learning rate: 0.00014386]
	Learning Rate: 0.000143862
	LOSS [training: -0.0001257043768630362 | validation: 0.005345471344542671]
	TIME [epoch: 8.31 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034260404506713605		[learning rate: 0.00014369]
		[batch 20/20] avg loss: -0.003884345465360878		[learning rate: 0.00014351]
	Learning Rate: 0.000143514
	LOSS [training: -0.0036551929580161194 | validation: 0.0003258473285976866]
	TIME [epoch: 8.31 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00036809613310451026		[learning rate: 0.00014334]
		[batch 20/20] avg loss: -0.0017598279322986947		[learning rate: 0.00014317]
	Learning Rate: 0.000143167
	LOSS [training: -0.0010639620327016024 | validation: 0.004084384499759879]
	TIME [epoch: 8.33 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038014086028668793		[learning rate: 0.00014299]
		[batch 20/20] avg loss: -0.002254764464451288		[learning rate: 0.00014282]
	Learning Rate: 0.00014282
	LOSS [training: 0.0007733220692077958 | validation: 0.0012172836256113054]
	TIME [epoch: 8.31 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003103866673809916		[learning rate: 0.00014265]
		[batch 20/20] avg loss: 0.0018245513287279771		[learning rate: 0.00014247]
	Learning Rate: 0.000142474
	LOSS [training: 0.0007570823306734928 | validation: -0.004074043213523938]
	TIME [epoch: 8.34 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003300521985350193		[learning rate: 0.0001423]
		[batch 20/20] avg loss: 0.00423870961678475		[learning rate: 0.00014213]
	Learning Rate: 0.000142129
	LOSS [training: 0.003769615801067472 | validation: 0.0073657868673946535]
	TIME [epoch: 8.33 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024938624495666434		[learning rate: 0.00014196]
		[batch 20/20] avg loss: -0.0005056020719298823		[learning rate: 0.00014179]
	Learning Rate: 0.000141785
	LOSS [training: 0.0009941301888183808 | validation: 0.004269746620807116]
	TIME [epoch: 8.34 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002431071144157005		[learning rate: 0.00014161]
		[batch 20/20] avg loss: -0.003093071369221603		[learning rate: 0.00014144]
	Learning Rate: 0.000141442
	LOSS [training: -0.0003310001125322993 | validation: -0.0035712662975258924]
	TIME [epoch: 8.32 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00033102897089436195		[learning rate: 0.00014127]
		[batch 20/20] avg loss: -0.0038440225938576197		[learning rate: 0.0001411]
	Learning Rate: 0.0001411
	LOSS [training: -0.0017564968114816288 | validation: 0.0010330941474435804]
	TIME [epoch: 8.35 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002373857262048773		[learning rate: 0.00014093]
		[batch 20/20] avg loss: -0.0043937368360787545		[learning rate: 0.00014076]
	Learning Rate: 0.000140758
	LOSS [training: -0.003383797049063763 | validation: 0.0008799576637912518]
	TIME [epoch: 8.32 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016955182648950237		[learning rate: 0.00014059]
		[batch 20/20] avg loss: -0.0013769894063036911		[learning rate: 0.00014042]
	Learning Rate: 0.000140417
	LOSS [training: -0.0015362538355993575 | validation: -0.0014764383480370766]
	TIME [epoch: 8.33 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011629150473457747		[learning rate: 0.00014025]
		[batch 20/20] avg loss: 0.0017759645003839887		[learning rate: 0.00014008]
	Learning Rate: 0.000140078
	LOSS [training: 0.0014694397738648816 | validation: 0.006601547452091527]
	TIME [epoch: 8.32 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010680141882551497		[learning rate: 0.00013991]
		[batch 20/20] avg loss: -0.003903105088539921		[learning rate: 0.00013974]
	Learning Rate: 0.000139738
	LOSS [training: -0.0024855596383975354 | validation: 0.00048132385081179686]
	TIME [epoch: 8.31 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008608678403765318		[learning rate: 0.00013957]
		[batch 20/20] avg loss: -0.0021925800794368212		[learning rate: 0.0001394]
	Learning Rate: 0.0001394
	LOSS [training: -0.0054006292416010696 | validation: -0.0005788385262389827]
	TIME [epoch: 8.31 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023178249728957414		[learning rate: 0.00013923]
		[batch 20/20] avg loss: -0.0012255948354603413		[learning rate: 0.00013906]
	Learning Rate: 0.000139063
	LOSS [training: -0.0017717099041780417 | validation: 0.002990211261870987]
	TIME [epoch: 8.33 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011142598491523659		[learning rate: 0.00013889]
		[batch 20/20] avg loss: -0.0011397429097524587		[learning rate: 0.00013873]
	Learning Rate: 0.000138726
	LOSS [training: -0.0011270013794524119 | validation: 0.003729922557582823]
	TIME [epoch: 8.31 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002117535747070535		[learning rate: 0.00013856]
		[batch 20/20] avg loss: 0.00011404566800744046		[learning rate: 0.00013839]
	Learning Rate: 0.00013839
	LOSS [training: -0.0010017450395315472 | validation: 0.0031607995362205304]
	TIME [epoch: 8.31 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003154397773610834		[learning rate: 0.00013822]
		[batch 20/20] avg loss: 5.459317285214692e-05		[learning rate: 0.00013806]
	Learning Rate: 0.000138055
	LOSS [training: -0.0015499023003793435 | validation: 0.0001007237396679773]
	TIME [epoch: 8.33 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011325179166735708		[learning rate: 0.00013789]
		[batch 20/20] avg loss: -0.006257028525338991		[learning rate: 0.00013772]
	Learning Rate: 0.000137721
	LOSS [training: -0.0025622553043327103 | validation: 0.00877694939912259]
	TIME [epoch: 8.37 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005785642573542823		[learning rate: 0.00013755]
		[batch 20/20] avg loss: -0.006130169410460394		[learning rate: 0.00013739]
	Learning Rate: 0.000137388
	LOSS [training: -0.0033543668339073387 | validation: 0.005911748293068881]
	TIME [epoch: 8.32 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004433147638185506		[learning rate: 0.00013722]
		[batch 20/20] avg loss: -0.001691173374884591		[learning rate: 0.00013705]
	Learning Rate: 0.000137055
	LOSS [training: -0.0010672440693515709 | validation: 0.0048162266161297426]
	TIME [epoch: 8.32 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004263563025730437		[learning rate: 0.00013689]
		[batch 20/20] avg loss: -0.00029519037929302426		[learning rate: 0.00013672]
	Learning Rate: 0.000136723
	LOSS [training: -0.002279376702511731 | validation: 0.004461088005802053]
	TIME [epoch: 8.34 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011694725049507342		[learning rate: 0.00013656]
		[batch 20/20] avg loss: -0.0018849564793380273		[learning rate: 0.00013639]
	Learning Rate: 0.000136392
	LOSS [training: -0.000357741987193646 | validation: 0.0022279702747314617]
	TIME [epoch: 8.35 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000653499754507288		[learning rate: 0.00013623]
		[batch 20/20] avg loss: -0.0003915653414138062		[learning rate: 0.00013606]
	Learning Rate: 0.000136062
	LOSS [training: 0.00013096720654674087 | validation: 0.008007371029540387]
	TIME [epoch: 8.31 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017031717594963723		[learning rate: 0.0001359]
		[batch 20/20] avg loss: 0.002533785597701487		[learning rate: 0.00013573]
	Learning Rate: 0.000135733
	LOSS [training: 0.0021184786785989297 | validation: 0.006355819425041351]
	TIME [epoch: 8.31 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00014641589418065812		[learning rate: 0.00013557]
		[batch 20/20] avg loss: 0.0044675220673083904		[learning rate: 0.0001354]
	Learning Rate: 0.000135404
	LOSS [training: 0.0021605530865638663 | validation: -0.00212558859231134]
	TIME [epoch: 8.32 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002952111059946742		[learning rate: 0.00013524]
		[batch 20/20] avg loss: -0.001197328890540448		[learning rate: 0.00013508]
	Learning Rate: 0.000135076
	LOSS [training: -0.0020747199752435947 | validation: 0.0027105727720278497]
	TIME [epoch: 8.33 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005345214243406406		[learning rate: 0.00013491]
		[batch 20/20] avg loss: 0.0022140714721067536		[learning rate: 0.00013475]
	Learning Rate: 0.000134749
	LOSS [training: -0.0015655713856498256 | validation: 0.0006103703323970145]
	TIME [epoch: 8.31 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012122984417035799		[learning rate: 0.00013459]
		[batch 20/20] avg loss: -0.0013333119130168021		[learning rate: 0.00013442]
	Learning Rate: 0.000134423
	LOSS [training: -6.050673565661136e-05 | validation: 0.0019856199854651115]
	TIME [epoch: 8.31 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021359969160810476		[learning rate: 0.00013426]
		[batch 20/20] avg loss: -0.0010188674755520397		[learning rate: 0.0001341]
	Learning Rate: 0.000134098
	LOSS [training: -0.0015774321958165437 | validation: -0.0010182254601974281]
	TIME [epoch: 8.31 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006045637600789656		[learning rate: 0.00013394]
		[batch 20/20] avg loss: 0.004445635844996536		[learning rate: 0.00013377]
	Learning Rate: 0.000133773
	LOSS [training: -0.00080000087789656 | validation: 0.0032957756165287065]
	TIME [epoch: 8.34 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004629074406068639		[learning rate: 0.00013361]
		[batch 20/20] avg loss: -0.0010932985432360078		[learning rate: 0.00013345]
	Learning Rate: 0.000133449
	LOSS [training: -0.0028611864746523235 | validation: -0.00014421196469611725]
	TIME [epoch: 8.34 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005070645041998349		[learning rate: 0.00013329]
		[batch 20/20] avg loss: -0.0016829487126505977		[learning rate: 0.00013313]
	Learning Rate: 0.000133126
	LOSS [training: -0.0033767968773244735 | validation: 0.007976368472471518]
	TIME [epoch: 8.32 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003697037225436511		[learning rate: 0.00013296]
		[batch 20/20] avg loss: -0.005731548264138728		[learning rate: 0.0001328]
	Learning Rate: 0.000132804
	LOSS [training: -0.0010172555193511085 | validation: 0.001159611311155113]
	TIME [epoch: 8.32 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001784587970309389		[learning rate: 0.00013264]
		[batch 20/20] avg loss: -0.003983200596611637		[learning rate: 0.00013248]
	Learning Rate: 0.000132482
	LOSS [training: -0.0028838942834605134 | validation: -0.0026015815882646314]
	TIME [epoch: 8.35 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0042489776620124944		[learning rate: 0.00013232]
		[batch 20/20] avg loss: -0.004950634548055078		[learning rate: 0.00013216]
	Learning Rate: 0.000132162
	LOSS [training: -0.004599806105033787 | validation: 0.0014078486593988712]
	TIME [epoch: 8.34 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031630793260703826		[learning rate: 0.000132]
		[batch 20/20] avg loss: -0.004705281038923118		[learning rate: 0.00013184]
	Learning Rate: 0.000131842
	LOSS [training: -0.003934180182496751 | validation: -0.0009626310612398086]
	TIME [epoch: 8.31 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002933656223521756		[learning rate: 0.00013168]
		[batch 20/20] avg loss: -0.00018583049890070932		[learning rate: 0.00013152]
	Learning Rate: 0.000131522
	LOSS [training: 0.0013739128623105235 | validation: -0.0025415926589154474]
	TIME [epoch: 8.31 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030725460106484845		[learning rate: 0.00013136]
		[batch 20/20] avg loss: -0.0030554860768426065		[learning rate: 0.0001312]
	Learning Rate: 0.000131204
	LOSS [training: -0.0030640160437455446 | validation: 0.0025389891513003427]
	TIME [epoch: 8.33 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004856064459610097		[learning rate: 0.00013105]
		[batch 20/20] avg loss: -0.0005605912386644439		[learning rate: 0.00013089]
	Learning Rate: 0.000130886
	LOSS [training: -0.002708327849137271 | validation: -0.00694706168854612]
	TIME [epoch: 8.32 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022050741743495092		[learning rate: 0.00013073]
		[batch 20/20] avg loss: -0.003622627184929066		[learning rate: 0.00013057]
	Learning Rate: 0.00013057
	LOSS [training: -0.0029138506796392884 | validation: 0.003959392029084636]
	TIME [epoch: 8.31 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006890430967606224		[learning rate: 0.00013041]
		[batch 20/20] avg loss: -0.0004873229806177285		[learning rate: 0.00013025]
	Learning Rate: 0.000130254
	LOSS [training: -0.0005881830386891755 | validation: 0.0008320094548800625]
	TIME [epoch: 8.31 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001550094345177685		[learning rate: 0.0001301]
		[batch 20/20] avg loss: 0.00044413397761436954		[learning rate: 0.00012994]
	Learning Rate: 0.000129938
	LOSS [training: -0.0005529801837816577 | validation: 0.0007771696388404901]
	TIME [epoch: 8.35 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00039212568597998094		[learning rate: 0.00012978]
		[batch 20/20] avg loss: -0.0023768438788821004		[learning rate: 0.00012962]
	Learning Rate: 0.000129624
	LOSS [training: -0.0013844847824310406 | validation: 0.001716263947989502]
	TIME [epoch: 8.33 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002864673108574768		[learning rate: 0.00012947]
		[batch 20/20] avg loss: -0.0002819163552493011		[learning rate: 0.00012931]
	Learning Rate: 0.00012931
	LOSS [training: 0.0012913783766627337 | validation: 0.007367070325685663]
	TIME [epoch: 8.32 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030962514837109072		[learning rate: 0.00012915]
		[batch 20/20] avg loss: -0.0013611369326537745		[learning rate: 0.000129]
	Learning Rate: 0.000128997
	LOSS [training: -0.0022286942081823414 | validation: 0.0026816374496130037]
	TIME [epoch: 8.32 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004547490672281441		[learning rate: 0.00012884]
		[batch 20/20] avg loss: -0.003713534520217951		[learning rate: 0.00012868]
	Learning Rate: 0.000128685
	LOSS [training: -0.004130512596249696 | validation: 0.0036268076307684662]
	TIME [epoch: 8.37 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038869737881560944		[learning rate: 0.00012853]
		[batch 20/20] avg loss: -0.0007113018818211014		[learning rate: 0.00012837]
	Learning Rate: 0.000128373
	LOSS [training: 0.001587835953167496 | validation: 0.002052111138695704]
	TIME [epoch: 8.32 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035888628304391068		[learning rate: 0.00012822]
		[batch 20/20] avg loss: -0.0049910122306225674		[learning rate: 0.00012806]
	Learning Rate: 0.000128062
	LOSS [training: -0.004289937530530838 | validation: 0.005209190079842772]
	TIME [epoch: 8.31 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005785246269513225		[learning rate: 0.00012791]
		[batch 20/20] avg loss: -0.0021604885194028453		[learning rate: 0.00012775]
	Learning Rate: 0.000127752
	LOSS [training: -0.0013695065731770839 | validation: -0.003593033155480827]
	TIME [epoch: 8.32 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000533983927971273		[learning rate: 0.0001276]
		[batch 20/20] avg loss: -0.0027061849740835117		[learning rate: 0.00012744]
	Learning Rate: 0.000127443
	LOSS [training: -0.0016200844510273927 | validation: -0.0023422929331472914]
	TIME [epoch: 8.33 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037704213340190363		[learning rate: 0.00012729]
		[batch 20/20] avg loss: -0.0014484169467280465		[learning rate: 0.00012713]
	Learning Rate: 0.000127134
	LOSS [training: -0.0026094191403735413 | validation: 0.003913739910773127]
	TIME [epoch: 8.32 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004050701973903484		[learning rate: 0.00012698]
		[batch 20/20] avg loss: -0.0013990145696526632		[learning rate: 0.00012683]
	Learning Rate: 0.000126827
	LOSS [training: -0.0027248582717780737 | validation: 0.0012329168627202396]
	TIME [epoch: 8.31 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00298172039574777		[learning rate: 0.00012667]
		[batch 20/20] avg loss: -0.0008103017843551124		[learning rate: 0.00012652]
	Learning Rate: 0.00012652
	LOSS [training: 0.001085709305696329 | validation: 0.00456847493887065]
	TIME [epoch: 8.32 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023648019974342634		[learning rate: 0.00012637]
		[batch 20/20] avg loss: -0.0037259031220790127		[learning rate: 0.00012621]
	Learning Rate: 0.000126213
	LOSS [training: -0.003045352559756638 | validation: 0.0032692371400132605]
	TIME [epoch: 8.35 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008913097830418999		[learning rate: 0.00012606]
		[batch 20/20] avg loss: -0.006407998403391149		[learning rate: 0.00012591]
	Learning Rate: 0.000125908
	LOSS [training: -0.002758344310174625 | validation: 0.00231017943866781]
	TIME [epoch: 8.32 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010125395404176948		[learning rate: 0.00012576]
		[batch 20/20] avg loss: -0.0038012823946142736		[learning rate: 0.0001256]
	Learning Rate: 0.000125603
	LOSS [training: -0.0013943714270982887 | validation: -0.0009275062552161386]
	TIME [epoch: 8.31 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004022901487395529		[learning rate: 0.00012545]
		[batch 20/20] avg loss: -0.005081295679260231		[learning rate: 0.0001253]
	Learning Rate: 0.000125299
	LOSS [training: -0.00455209858332788 | validation: 0.004582225694881502]
	TIME [epoch: 8.34 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00017567751015939957		[learning rate: 0.00012515]
		[batch 20/20] avg loss: -0.006674471777960117		[learning rate: 0.000125]
	Learning Rate: 0.000124996
	LOSS [training: -0.0034250746440597584 | validation: -0.002527697050052997]
	TIME [epoch: 8.32 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008946564453511371		[learning rate: 0.00012484]
		[batch 20/20] avg loss: 0.0015390040702528136		[learning rate: 0.00012469]
	Learning Rate: 0.000124693
	LOSS [training: 0.0003221738124508385 | validation: 0.0025941042624039934]
	TIME [epoch: 8.3 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003361330832767109		[learning rate: 0.00012454]
		[batch 20/20] avg loss: -0.003874713141377445		[learning rate: 0.00012439]
	Learning Rate: 0.000124391
	LOSS [training: -0.003618021987072277 | validation: -3.3253642260919485e-05]
	TIME [epoch: 8.31 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004506931848848136		[learning rate: 0.00012424]
		[batch 20/20] avg loss: -0.003763356766666276		[learning rate: 0.00012409]
	Learning Rate: 0.00012409
	LOSS [training: -0.004135144307757206 | validation: -0.002983626933285426]
	TIME [epoch: 8.31 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: -7.13655340308584e-05		[learning rate: 0.00012394]
		[batch 20/20] avg loss: -0.0012192018348495836		[learning rate: 0.00012379]
	Learning Rate: 0.00012379
	LOSS [training: -0.0006452836844402209 | validation: -0.001719793664158991]
	TIME [epoch: 8.33 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000134638127799396		[learning rate: 0.00012364]
		[batch 20/20] avg loss: -0.006426562161563856		[learning rate: 0.00012349]
	Learning Rate: 0.00012349
	LOSS [training: -0.003280600144681626 | validation: 0.0021937710938473636]
	TIME [epoch: 8.32 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007311331275540163		[learning rate: 0.00012334]
		[batch 20/20] avg loss: 0.00010772357854936126		[learning rate: 0.00012319]
	Learning Rate: 0.000123191
	LOSS [training: 0.00041942835305168875 | validation: 0.000696740395822102]
	TIME [epoch: 8.34 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008098005434738868		[learning rate: 0.00012304]
		[batch 20/20] avg loss: -0.0029707195246297057		[learning rate: 0.00012289]
	Learning Rate: 0.000122893
	LOSS [training: -0.0010804594905779094 | validation: -0.001206628004057196]
	TIME [epoch: 8.34 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003342910345540647		[learning rate: 0.00012274]
		[batch 20/20] avg loss: 0.0035892601349632584		[learning rate: 0.0001226]
	Learning Rate: 0.000122595
	LOSS [training: 0.00012317489471130576 | validation: 0.003531670800778648]
	TIME [epoch: 8.33 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003171769606262315		[learning rate: 0.00012245]
		[batch 20/20] avg loss: -0.004155273347663624		[learning rate: 0.0001223]
	Learning Rate: 0.000122298
	LOSS [training: -0.0036635214769629694 | validation: -0.0009479784261875396]
	TIME [epoch: 8.33 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004754725362454432		[learning rate: 0.00012215]
		[batch 20/20] avg loss: -0.007066370940039002		[learning rate: 0.000122]
	Learning Rate: 0.000122002
	LOSS [training: -0.005910548151246716 | validation: -0.0011510992971306862]
	TIME [epoch: 8.33 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00331996622050587		[learning rate: 0.00012185]
		[batch 20/20] avg loss: -0.008899557052857986		[learning rate: 0.00012171]
	Learning Rate: 0.000121707
	LOSS [training: -0.0027897954161760583 | validation: 0.002642811565478698]
	TIME [epoch: 8.32 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001766833795410912		[learning rate: 0.00012156]
		[batch 20/20] avg loss: -0.0031907967502492482		[learning rate: 0.00012141]
	Learning Rate: 0.000121412
	LOSS [training: -0.00247881527283008 | validation: 0.00014970763783629963]
	TIME [epoch: 8.32 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001595355922238364		[learning rate: 0.00012127]
		[batch 20/20] avg loss: -0.005944626894658559		[learning rate: 0.00012112]
	Learning Rate: 0.000121119
	LOSS [training: -0.0021746354862100976 | validation: -0.0019572962769054646]
	TIME [epoch: 8.31 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021960026215073615		[learning rate: 0.00012097]
		[batch 20/20] avg loss: -0.000612157539652058		[learning rate: 0.00012083]
	Learning Rate: 0.000120825
	LOSS [training: -0.0014040800805797096 | validation: 0.005163702161799812]
	TIME [epoch: 8.31 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028001499604031455		[learning rate: 0.00012068]
		[batch 20/20] avg loss: -0.00040288500099141134		[learning rate: 0.00012053]
	Learning Rate: 0.000120533
	LOSS [training: -0.001601517480697278 | validation: 0.0029933528344516904]
	TIME [epoch: 8.32 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005546305057028873		[learning rate: 0.00012039]
		[batch 20/20] avg loss: -0.0010081398108584372		[learning rate: 0.00012024]
	Learning Rate: 0.000120241
	LOSS [training: -0.0007813851582806624 | validation: 0.0016250862756373259]
	TIME [epoch: 8.32 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0039991384853196475		[learning rate: 0.0001201]
		[batch 20/20] avg loss: -0.0033694710674589214		[learning rate: 0.00011995]
	Learning Rate: 0.00011995
	LOSS [training: -0.0036843047763892847 | validation: -8.62279652156577e-05]
	TIME [epoch: 8.31 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002864292230718662		[learning rate: 0.0001198]
		[batch 20/20] avg loss: -0.0028751693978779553		[learning rate: 0.00011966]
	Learning Rate: 0.00011966
	LOSS [training: -0.002869730814298308 | validation: 7.582251158342983e-05]
	TIME [epoch: 8.33 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005765726345691076		[learning rate: 0.00011951]
		[batch 20/20] avg loss: -0.004257715161103953		[learning rate: 0.00011937]
	Learning Rate: 0.00011937
	LOSS [training: -0.005011720753397515 | validation: 0.0035385878604053925]
	TIME [epoch: 8.35 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017530146336233268		[learning rate: 0.00011923]
		[batch 20/20] avg loss: -0.0033953486097507723		[learning rate: 0.00011908]
	Learning Rate: 0.000119081
	LOSS [training: -0.000821166988063723 | validation: -0.003190283417522261]
	TIME [epoch: 8.32 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006869890774792634		[learning rate: 0.00011894]
		[batch 20/20] avg loss: -0.0015116618796818845		[learning rate: 0.00011879]
	Learning Rate: 0.000118793
	LOSS [training: -0.00419077632723726 | validation: -0.0033096821609504934]
	TIME [epoch: 8.32 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003387938385643264		[learning rate: 0.00011865]
		[batch 20/20] avg loss: -0.0037779978011458714		[learning rate: 0.00011851]
	Learning Rate: 0.000118505
	LOSS [training: -0.0035829680933945674 | validation: -0.0008963556086916135]
	TIME [epoch: 8.34 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020300374269805683		[learning rate: 0.00011836]
		[batch 20/20] avg loss: -0.0028304869478258323		[learning rate: 0.00011822]
	Learning Rate: 0.000118218
	LOSS [training: -0.0024302621874032005 | validation: -0.004057007891319952]
	TIME [epoch: 8.34 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007726781454481825		[learning rate: 0.00011808]
		[batch 20/20] avg loss: 0.002220231828963987		[learning rate: 0.00011793]
	Learning Rate: 0.000117932
	LOSS [training: -0.002753274812758919 | validation: 0.00399313020833072]
	TIME [epoch: 8.31 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005858050372080117		[learning rate: 0.00011779]
		[batch 20/20] avg loss: -0.003360459992195919		[learning rate: 0.00011765]
	Learning Rate: 0.000117646
	LOSS [training: -0.004609255182138018 | validation: -0.004054003649215814]
	TIME [epoch: 8.31 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007925640422937646		[learning rate: 0.0001175]
		[batch 20/20] avg loss: 0.002540571726456429		[learning rate: 0.00011736]
	Learning Rate: 0.000117362
	LOSS [training: 0.0008740038420813323 | validation: -0.0036286692452144774]
	TIME [epoch: 8.31 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002044167435444455		[learning rate: 0.00011722]
		[batch 20/20] avg loss: 0.0017508153909788088		[learning rate: 0.00011708]
	Learning Rate: 0.000117078
	LOSS [training: -0.00014667602223282316 | validation: -0.001549796323225858]
	TIME [epoch: 8.33 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018357529216400247		[learning rate: 0.00011694]
		[batch 20/20] avg loss: -0.0033227263951488605		[learning rate: 0.00011679]
	Learning Rate: 0.000116794
	LOSS [training: -0.002579239658394443 | validation: 0.0016632736431509512]
	TIME [epoch: 8.32 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005223163008544199		[learning rate: 0.00011665]
		[batch 20/20] avg loss: 0.0009258433759628303		[learning rate: 0.00011651]
	Learning Rate: 0.000116511
	LOSS [training: -0.002148659816290685 | validation: 0.001125836802946464]
	TIME [epoch: 8.32 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00021835804992304046		[learning rate: 0.00011637]
		[batch 20/20] avg loss: -0.0031489136634325816		[learning rate: 0.00011623]
	Learning Rate: 0.000116229
	LOSS [training: -0.0016836358566778107 | validation: -0.0004333488413950453]
	TIME [epoch: 8.33 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00261147091092569		[learning rate: 0.00011609]
		[batch 20/20] avg loss: -0.0036985185111662747		[learning rate: 0.00011595]
	Learning Rate: 0.000115948
	LOSS [training: -0.0031549947110459825 | validation: -0.005562061394909814]
	TIME [epoch: 8.35 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006264712845252336		[learning rate: 0.00011581]
		[batch 20/20] avg loss: -0.0019508849996034646		[learning rate: 0.00011567]
	Learning Rate: 0.000115667
	LOSS [training: -0.004107798922427901 | validation: 0.005140802898952416]
	TIME [epoch: 8.32 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002341101773921759		[learning rate: 0.00011553]
		[batch 20/20] avg loss: 0.0034024652985709786		[learning rate: 0.00011539]
	Learning Rate: 0.000115387
	LOSS [training: 0.0005306817623246101 | validation: 0.004338015693192593]
	TIME [epoch: 8.32 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014120594018386795		[learning rate: 0.00011525]
		[batch 20/20] avg loss: -0.003091113532991578		[learning rate: 0.00011511]
	Learning Rate: 0.000115108
	LOSS [training: -0.0022515864674151286 | validation: -0.0029145933057495043]
	TIME [epoch: 8.34 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00474338931005772		[learning rate: 0.00011497]
		[batch 20/20] avg loss: 0.002326995419303819		[learning rate: 0.00011483]
	Learning Rate: 0.000114829
	LOSS [training: -0.0012081969453769498 | validation: 0.0005308433068211408]
	TIME [epoch: 8.34 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035554346285959796		[learning rate: 0.00011469]
		[batch 20/20] avg loss: -0.0015473006396254907		[learning rate: 0.00011455]
	Learning Rate: 0.000114551
	LOSS [training: -0.002551367634110736 | validation: 0.0012407733606408515]
	TIME [epoch: 8.32 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00030969375872812063		[learning rate: 0.00011441]
		[batch 20/20] avg loss: -0.001647137817065121		[learning rate: 0.00011427]
	Learning Rate: 0.000114274
	LOSS [training: -0.000978415787896621 | validation: 0.0001382599401461139]
	TIME [epoch: 8.32 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004084383143476711		[learning rate: 0.00011414]
		[batch 20/20] avg loss: 0.0019204964194904199		[learning rate: 0.000114]
	Learning Rate: 0.000113997
	LOSS [training: -0.0010819433619931463 | validation: -0.003462914398122829]
	TIME [epoch: 8.32 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005977936825522635		[learning rate: 0.00011386]
		[batch 20/20] avg loss: 0.0010926701654605507		[learning rate: 0.00011372]
	Learning Rate: 0.000113721
	LOSS [training: -0.002442633330031041 | validation: 0.007197281955943783]
	TIME [epoch: 8.35 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0062699828095965845		[learning rate: 0.00011358]
		[batch 20/20] avg loss: -0.0004811507904052215		[learning rate: 0.00011345]
	Learning Rate: 0.000113446
	LOSS [training: -0.0033755668000009023 | validation: -0.0010159042371501589]
	TIME [epoch: 8.32 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007177216931038696		[learning rate: 0.00011331]
		[batch 20/20] avg loss: -0.0006086721087799954		[learning rate: 0.00011317]
	Learning Rate: 0.000113171
	LOSS [training: -0.003892944519909346 | validation: 0.001163185905077051]
	TIME [epoch: 8.34 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004724093146730027		[learning rate: 0.00011303]
		[batch 20/20] avg loss: -0.0010897513573956258		[learning rate: 0.0001129]
	Learning Rate: 0.000112897
	LOSS [training: -0.00030867102136131143 | validation: -0.003744450567454146]
	TIME [epoch: 8.33 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00470655630177641		[learning rate: 0.00011276]
		[batch 20/20] avg loss: -0.002286443888267425		[learning rate: 0.00011262]
	Learning Rate: 0.000112624
	LOSS [training: -0.003496500095021918 | validation: -0.004904613513834816]
	TIME [epoch: 8.34 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013865805599325945		[learning rate: 0.00011249]
		[batch 20/20] avg loss: -7.169706714156064e-05		[learning rate: 0.00011235]
	Learning Rate: 0.000112352
	LOSS [training: -0.0007291388135370775 | validation: 0.0025864817538914257]
	TIME [epoch: 8.33 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015266772186410976		[learning rate: 0.00011222]
		[batch 20/20] avg loss: -0.003334095233872813		[learning rate: 0.00011208]
	Learning Rate: 0.00011208
	LOSS [training: -0.0024303862262569555 | validation: -0.00020023713704555676]
	TIME [epoch: 8.34 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009630351377072418		[learning rate: 0.00011194]
		[batch 20/20] avg loss: -0.0005472973671646276		[learning rate: 0.00011181]
	Learning Rate: 0.000111808
	LOSS [training: 0.00020786888527130689 | validation: -0.001683195473114008]
	TIME [epoch: 8.32 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00549522684898356		[learning rate: 0.00011167]
		[batch 20/20] avg loss: -0.0016584422931292045		[learning rate: 0.00011154]
	Learning Rate: 0.000111538
	LOSS [training: -0.0035768345710563824 | validation: 0.003648284276314997]
	TIME [epoch: 8.33 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005786403649363734		[learning rate: 0.0001114]
		[batch 20/20] avg loss: -0.00379363078120665		[learning rate: 0.00011127]
	Learning Rate: 0.000111268
	LOSS [training: -0.004790017215285193 | validation: 0.006130433880197804]
	TIME [epoch: 8.32 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012136436805694091		[learning rate: 0.00011113]
		[batch 20/20] avg loss: -0.0035817536094109974		[learning rate: 0.000111]
	Learning Rate: 0.000110998
	LOSS [training: -0.002397698644990203 | validation: -0.0041535030566565295]
	TIME [epoch: 8.3 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018652204658133633		[learning rate: 0.00011086]
		[batch 20/20] avg loss: -0.0029443703129622624		[learning rate: 0.00011073]
	Learning Rate: 0.000110729
	LOSS [training: -0.0024047953893878136 | validation: 0.0023760606716831303]
	TIME [epoch: 8.3 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00396955188249302		[learning rate: 0.0001106]
		[batch 20/20] avg loss: -0.002355389067232152		[learning rate: 0.00011046]
	Learning Rate: 0.000110461
	LOSS [training: -0.003162470474862586 | validation: 0.0014983937571043373]
	TIME [epoch: 8.32 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018461696902308995		[learning rate: 0.00011033]
		[batch 20/20] avg loss: 4.444272967857547e-05		[learning rate: 0.00011019]
	Learning Rate: 0.000110194
	LOSS [training: -0.000900863480276162 | validation: 0.0014265410239003061]
	TIME [epoch: 8.31 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002750642419801409		[learning rate: 0.00011006]
		[batch 20/20] avg loss: -0.0037462239048283927		[learning rate: 0.00010993]
	Learning Rate: 0.000109927
	LOSS [training: -0.002010644073404267 | validation: 0.0032097792371764506]
	TIME [epoch: 8.34 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004957739050359048		[learning rate: 0.00010979]
		[batch 20/20] avg loss: -0.002881611173552722		[learning rate: 0.00010966]
	Learning Rate: 0.000109661
	LOSS [training: -0.003919675111955885 | validation: 0.0053511937515803455]
	TIME [epoch: 8.32 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013654157481639046		[learning rate: 0.00010953]
		[batch 20/20] avg loss: 0.005244127301170179		[learning rate: 0.0001094]
	Learning Rate: 0.000109396
	LOSS [training: 0.0019393557765031374 | validation: 0.0010171931090300229]
	TIME [epoch: 8.33 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0047569388740490125		[learning rate: 0.00010926]
		[batch 20/20] avg loss: 0.002676794624754032		[learning rate: 0.00010913]
	Learning Rate: 0.000109131
	LOSS [training: -0.0010400721246474902 | validation: 0.0034186739454529864]
	TIME [epoch: 8.32 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015422868255057023		[learning rate: 0.000109]
		[batch 20/20] avg loss: 0.004101856639080104		[learning rate: 0.00010887]
	Learning Rate: 0.000108867
	LOSS [training: 0.001279784906787201 | validation: -0.000763835749443941]
	TIME [epoch: 8.36 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008096906689204349		[learning rate: 0.00010873]
		[batch 20/20] avg loss: -0.0050830073119428635		[learning rate: 0.0001086]
	Learning Rate: 0.000108603
	LOSS [training: 0.0015069496886307423 | validation: 0.001942708618322212]
	TIME [epoch: 8.3 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015793414309748257		[learning rate: 0.00010847]
		[batch 20/20] avg loss: 3.725501326772955e-05		[learning rate: 0.00010834]
	Learning Rate: 0.00010834
	LOSS [training: -0.0007710432088535483 | validation: -0.0029993271392111456]
	TIME [epoch: 8.3 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031174666268965552		[learning rate: 0.00010821]
		[batch 20/20] avg loss: -0.004241682926178008		[learning rate: 0.00010808]
	Learning Rate: 0.000108078
	LOSS [training: -0.0036795747765372825 | validation: 0.004042304784138962]
	TIME [epoch: 8.28 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005793852263877361		[learning rate: 0.00010795]
		[batch 20/20] avg loss: -0.0001280458293145015		[learning rate: 0.00010782]
	Learning Rate: 0.000107816
	LOSS [training: -0.0003537155278511187 | validation: -0.0037577934404006493]
	TIME [epoch: 8.27 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003334053614799936		[learning rate: 0.00010769]
		[batch 20/20] avg loss: -0.002198934942527544		[learning rate: 0.00010756]
	Learning Rate: 0.000107555
	LOSS [training: -0.00276649427866374 | validation: -0.0015404190577758378]
	TIME [epoch: 8.32 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00019841618541313193		[learning rate: 0.00010742]
		[batch 20/20] avg loss: 0.003732527761339275		[learning rate: 0.00010729]
	Learning Rate: 0.000107295
	LOSS [training: 0.0017670557879630715 | validation: -0.0021131241758054866]
	TIME [epoch: 8.39 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002252970959374447		[learning rate: 0.00010716]
		[batch 20/20] avg loss: -0.0031190256955550737		[learning rate: 0.00010704]
	Learning Rate: 0.000107035
	LOSS [training: -0.0004330273680903136 | validation: 0.003148292701002689]
	TIME [epoch: 8.36 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016458236898084992		[learning rate: 0.00010691]
		[batch 20/20] avg loss: -0.0006790804118967309		[learning rate: 0.00010678]
	Learning Rate: 0.000106776
	LOSS [training: -0.0011624520508526153 | validation: 0.00661893186055235]
	TIME [epoch: 8.35 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00264352244258171		[learning rate: 0.00010665]
		[batch 20/20] avg loss: -0.004659963571497283		[learning rate: 0.00010652]
	Learning Rate: 0.000106518
	LOSS [training: -0.0036517430070394973 | validation: -0.005227555710852486]
	TIME [epoch: 8.35 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007844057094772769		[learning rate: 0.00010639]
		[batch 20/20] avg loss: 0.0011192486840093938		[learning rate: 0.00010626]
	Learning Rate: 0.00010626
	LOSS [training: 0.0009518271967433354 | validation: 0.001900610675340713]
	TIME [epoch: 8.39 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004639140737898671		[learning rate: 0.00010613]
		[batch 20/20] avg loss: -0.0015567331448377365		[learning rate: 0.000106]
	Learning Rate: 0.000106002
	LOSS [training: -0.0030979369413682034 | validation: 0.0036479933924724126]
	TIME [epoch: 8.37 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002697050230551653		[learning rate: 0.00010587]
		[batch 20/20] avg loss: -0.004407479019092936		[learning rate: 0.00010575]
	Learning Rate: 0.000105746
	LOSS [training: -0.003552264624822295 | validation: -0.005338158344648819]
	TIME [epoch: 8.37 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025934605995414422		[learning rate: 0.00010562]
		[batch 20/20] avg loss: -0.002907213295501159		[learning rate: 0.00010549]
	Learning Rate: 0.00010549
	LOSS [training: -0.002750336947521301 | validation: 0.0017309716806354018]
	TIME [epoch: 8.37 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035555737389738676		[learning rate: 0.00010536]
		[batch 20/20] avg loss: -0.00558926753370375		[learning rate: 0.00010523]
	Learning Rate: 0.000105234
	LOSS [training: -0.004572420636338809 | validation: 0.0019981637837794197]
	TIME [epoch: 8.39 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006701815211822439		[learning rate: 0.00010511]
		[batch 20/20] avg loss: 0.0024864555138692395		[learning rate: 0.00010498]
	Learning Rate: 0.00010498
	LOSS [training: -0.0021076798489765994 | validation: 2.4318029955309778e-05]
	TIME [epoch: 8.36 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013445227816636068		[learning rate: 0.00010485]
		[batch 20/20] avg loss: -0.0077878956297660566		[learning rate: 0.00010473]
	Learning Rate: 0.000104726
	LOSS [training: -0.003221686424051225 | validation: -0.0052363268707783794]
	TIME [epoch: 8.37 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030771964125942217		[learning rate: 0.0001046]
		[batch 20/20] avg loss: 0.0003301551231046465		[learning rate: 0.00010447]
	Learning Rate: 0.000104472
	LOSS [training: -0.0013735206447447873 | validation: 0.005742711190546855]
	TIME [epoch: 8.36 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004268819423342978		[learning rate: 0.00010435]
		[batch 20/20] avg loss: -0.00578752552088444		[learning rate: 0.00010422]
	Learning Rate: 0.000104219
	LOSS [training: -0.0007593530487707308 | validation: -0.001922479587155449]
	TIME [epoch: 8.38 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013677398556455936		[learning rate: 0.00010409]
		[batch 20/20] avg loss: 0.0005512579547444092		[learning rate: 0.00010397]
	Learning Rate: 0.000103967
	LOSS [training: 0.0009594989051950016 | validation: 0.00406906979584848]
	TIME [epoch: 8.36 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018463602088027678		[learning rate: 0.00010384]
		[batch 20/20] avg loss: -0.002127821506222695		[learning rate: 0.00010372]
	Learning Rate: 0.000103715
	LOSS [training: -0.00014073064870996356 | validation: 0.004748652127679368]
	TIME [epoch: 8.36 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002825751829347542		[learning rate: 0.00010359]
		[batch 20/20] avg loss: -0.002246005196131184		[learning rate: 0.00010346]
	Learning Rate: 0.000103464
	LOSS [training: -0.0012642901895329691 | validation: -0.0028777966195192156]
	TIME [epoch: 8.36 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004664660670182402		[learning rate: 0.00010334]
		[batch 20/20] avg loss: -0.0007175119136314162		[learning rate: 0.00010321]
	Learning Rate: 0.000103214
	LOSS [training: -0.002691086291906909 | validation: -0.00038596376402099753]
	TIME [epoch: 8.39 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004696880938410504		[learning rate: 0.00010309]
		[batch 20/20] avg loss: -0.0015729347231696604		[learning rate: 0.00010296]
	Learning Rate: 0.000102964
	LOSS [training: -0.003134907830790082 | validation: -0.0032191615337099954]
	TIME [epoch: 8.37 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001908523242203672		[learning rate: 0.00010284]
		[batch 20/20] avg loss: -0.007903630992438897		[learning rate: 0.00010271]
	Learning Rate: 0.000102714
	LOSS [training: -0.002997553875117612 | validation: -0.004943559618482709]
	TIME [epoch: 8.35 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005251244835666621		[learning rate: 0.00010259]
		[batch 20/20] avg loss: -0.0007910853094143846		[learning rate: 0.00010247]
	Learning Rate: 0.000102466
	LOSS [training: -0.0030211650725405033 | validation: -0.0029389895754669535]
	TIME [epoch: 8.35 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025523564259067382		[learning rate: 0.00010234]
		[batch 20/20] avg loss: -0.005031445366370494		[learning rate: 0.00010222]
	Learning Rate: 0.000102218
	LOSS [training: -0.003791900896138617 | validation: -0.0012973251433272734]
	TIME [epoch: 8.38 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: -7.970952259499335e-05		[learning rate: 0.00010209]
		[batch 20/20] avg loss: -0.005779402918450478		[learning rate: 0.00010197]
	Learning Rate: 0.00010197
	LOSS [training: -0.002929556220522736 | validation: -0.00027225955331527983]
	TIME [epoch: 8.36 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005503256986206119		[learning rate: 0.00010185]
		[batch 20/20] avg loss: -0.0029474650177580292		[learning rate: 0.00010172]
	Learning Rate: 0.000101723
	LOSS [training: -0.004225361001982073 | validation: -0.0015067269784190965]
	TIME [epoch: 8.36 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003845654741286033		[learning rate: 0.0001016]
		[batch 20/20] avg loss: -0.002216667799277238		[learning rate: 0.00010148]
	Learning Rate: 0.000101477
	LOSS [training: -0.0030311612702816357 | validation: -0.0002956726855666105]
	TIME [epoch: 8.36 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005799496027162969		[learning rate: 0.00010135]
		[batch 20/20] avg loss: 0.0034452945818956803		[learning rate: 0.00010123]
	Learning Rate: 0.000101232
	LOSS [training: -0.0011771007226336447 | validation: -0.003843348080284543]
	TIME [epoch: 8.37 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002637015728664081		[learning rate: 0.00010111]
		[batch 20/20] avg loss: -0.003439909303551582		[learning rate: 0.00010099]
	Learning Rate: 0.000100986
	LOSS [training: -0.0030384625161078314 | validation: 0.0036739989174265474]
	TIME [epoch: 8.36 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001009128704728393		[learning rate: 0.00010086]
		[batch 20/20] avg loss: -0.0011162486665759516		[learning rate: 0.00010074]
	Learning Rate: 0.000100742
	LOSS [training: -0.0010626886856521726 | validation: -0.002560898004344137]
	TIME [epoch: 8.36 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010368774870579453		[learning rate: 0.00010062]
		[batch 20/20] avg loss: 0.002816274623168783		[learning rate: 0.0001005]
	Learning Rate: 0.000100498
	LOSS [training: 0.0019265760551133645 | validation: 0.001817346788879794]
	TIME [epoch: 8.36 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028561298236052636		[learning rate: 0.00010038]
		[batch 20/20] avg loss: -0.0038167530470924497		[learning rate: 0.00010025]
	Learning Rate: 0.000100255
	LOSS [training: -0.00048031161174359337 | validation: 0.009664263387064883]
	TIME [epoch: 8.38 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00015167683880024964		[learning rate: 0.00010013]
		[batch 20/20] avg loss: 0.0013454504058152503		[learning rate: 0.00010001]
	Learning Rate: 0.000100012
	LOSS [training: 0.0005968867835074998 | validation: -0.0037693062844451654]
	TIME [epoch: 8.37 sec]
Finished training in 16788.743 seconds.
