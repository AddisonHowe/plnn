Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r1', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4230464175

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.770766225534008		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.973271533701481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.372018879617745 | validation: 7.3755240085309985]
	TIME [epoch: 48.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.189731771354036		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.6675641511314465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.92864796124274 | validation: 5.842911287283939]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.314446755800105		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.539141066929119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.926793911364612 | validation: 2.6609876058232356]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.317286333159367		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0793226713424335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.198304502250901 | validation: 1.811139481619171]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8621632843227136		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.81728556086078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8397244225917468 | validation: 1.6352264257722235]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6220192881753301		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.595954344545941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6089868163606358 | validation: 1.1467604129174833]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7096282009691943		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1662525009709797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4379403509700874 | validation: 1.0186792072048219]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0073737466124755		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.060394188195818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0338839674041467 | validation: 0.7276718965844369]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8952642624001529		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7880847734773991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8416745179387759 | validation: 1.6678384013769103]
	TIME [epoch: 8.86 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7897265098559167		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7031728350561339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7464496724560252 | validation: 0.5068638811321968]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0283858617897368		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5580354733707823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7932106675802595 | validation: 0.4937784874564954]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8227247361871408		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6691582684268471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7459415023069941 | validation: 0.7402573581575376]
	TIME [epoch: 8.86 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6284960288598512		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6284102323773504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6284531306186009 | validation: 0.6398367082689936]
	TIME [epoch: 8.85 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5989865981292735		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6076841456338726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6033353718815732 | validation: 1.8820365287290048]
	TIME [epoch: 8.85 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0612521672370456		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7096677796201155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8854599734285806 | validation: 0.4256813306332708]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.555900008935808		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5610316408381731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5584658248869906 | validation: 0.49321937573639946]
	TIME [epoch: 8.87 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5730365146910927		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4786261825600714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5258313486255821 | validation: 0.8021297675258232]
	TIME [epoch: 8.85 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6136998004939409		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6693543654986065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6415270829962738 | validation: 0.6096686753182843]
	TIME [epoch: 8.85 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.614849313719602		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.593629501395254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6042394075574281 | validation: 0.410940139588414]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5237567862258891		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5251512804137123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5244540333198008 | validation: 0.6920575390111579]
	TIME [epoch: 8.87 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5890579113781331		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.541024937831535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5650414246048341 | validation: 0.6563332294106619]
	TIME [epoch: 8.85 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8982677007790063		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4951451363626824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6967064185708443 | validation: 0.44916198173602073]
	TIME [epoch: 8.85 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5382839999161171		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4972942573270219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5177891286215697 | validation: 0.42835903099668415]
	TIME [epoch: 8.85 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5382397035082509		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5953424297507166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5667910666294838 | validation: 0.3729161162453974]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4768230096240441		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5367242054641926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5067736075441183 | validation: 0.502525159022567]
	TIME [epoch: 8.88 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5744830659653395		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.506071349054052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5402772075096958 | validation: 0.4471638146701954]
	TIME [epoch: 8.87 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4829357327652689		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5406234889183303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5117796108417995 | validation: 0.6955282170055875]
	TIME [epoch: 8.87 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5383668577703427		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5869493558130893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5626581067917161 | validation: 0.5092597019276823]
	TIME [epoch: 8.9 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5928419033157203		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.941048099035014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7669450011753671 | validation: 0.3903661975804559]
	TIME [epoch: 8.87 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5912585916943921		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48050909045993945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5358838410771658 | validation: 0.6190445358605903]
	TIME [epoch: 8.87 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5292564649596474		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49269747571115224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5109769703353998 | validation: 0.7651896904109184]
	TIME [epoch: 8.86 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5049354579900683		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5901964717380264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5475659648640473 | validation: 0.44222316336943535]
	TIME [epoch: 8.88 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4585848614870304		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4333929443297223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44598890290837634 | validation: 0.4440051840016904]
	TIME [epoch: 8.88 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48983326773510055		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4285729539525076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4592031108438041 | validation: 0.3778927068808427]
	TIME [epoch: 8.86 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4702480698848003		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4287278008279885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4494879353563944 | validation: 0.38397699794856627]
	TIME [epoch: 8.87 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4320556432442932		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47181010465423334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45193287394926324 | validation: 0.550492678983052]
	TIME [epoch: 8.87 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5007774326291264		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4816727574666955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.491225095047911 | validation: 0.7765448890674852]
	TIME [epoch: 8.89 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5147484919494119		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4586175871143022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4866830395318571 | validation: 0.39813687317531277]
	TIME [epoch: 9.59 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4254672713906441		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.42037651624283984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42292189381674195 | validation: 0.4712036026905484]
	TIME [epoch: 8.87 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4394539243111518		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47801173902419836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4587328316676751 | validation: 0.3744138224577651]
	TIME [epoch: 8.86 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5481049265025664		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4145335668900806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48131924669632353 | validation: 0.463014493415771]
	TIME [epoch: 8.89 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45155618094321853		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.37191600712708056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4117360940351495 | validation: 0.5184425145114044]
	TIME [epoch: 8.87 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4554605581688416		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3977921675556166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42662636286222905 | validation: 0.3272792684578852]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3573249874263632		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.39357703533108185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3754510113787226 | validation: 0.2950969794035545]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4074568910494836		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.34844674584759827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.377951818448541 | validation: 0.39533268745806005]
	TIME [epoch: 8.88 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4004065603134734		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.407696945154978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4040517527342257 | validation: 0.46686237891691656]
	TIME [epoch: 8.87 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6747421958683072		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.44196692849587366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5583545621820905 | validation: 0.32270255273899273]
	TIME [epoch: 8.85 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38544738997806016		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45540022994583795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42042380996194895 | validation: 0.4611932822072909]
	TIME [epoch: 8.85 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43871586363357473		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4731798372293389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45594785043145675 | validation: 0.3923674192338464]
	TIME [epoch: 8.86 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4323239490761841		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48804582479993297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4601848869380585 | validation: 0.486384361092708]
	TIME [epoch: 8.87 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4367191581232087		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3784746587221123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4075969084226605 | validation: 0.3031413025405429]
	TIME [epoch: 8.85 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3541039536808136		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4509201315374861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40251204260914986 | validation: 0.8792796656064594]
	TIME [epoch: 8.86 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46468394505349736		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3220565561269896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39337025059024344 | validation: 0.28816451757867634]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42546165184395823		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3281809246589663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3768212882514622 | validation: 0.32960246240463215]
	TIME [epoch: 8.88 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3491522241531414		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3504498719983009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34980104807572115 | validation: 0.48667125759946483]
	TIME [epoch: 8.86 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3443692359837521		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2852852065603385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3148272212720452 | validation: 0.39332455619954393]
	TIME [epoch: 8.85 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3738986666453806		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.32773423304036936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35081644984287497 | validation: 0.4737092579071135]
	TIME [epoch: 8.85 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3691127490741035		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.305654413625267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3373835813496853 | validation: 0.22478992978098347]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3985068422530168		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3398485394723912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3691776908627039 | validation: 0.217015039636388]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28659341825304463		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3109486802999785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29877104927651155 | validation: 0.5135844636499906]
	TIME [epoch: 8.86 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3873122716532113		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.473446554634774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43037941314399253 | validation: 0.3504101971080376]
	TIME [epoch: 8.86 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2653414392116117		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2943725089967929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2798569741042023 | validation: 0.2256945848125227]
	TIME [epoch: 8.88 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27322858016377916		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3936623913861116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3334454857749453 | validation: 0.1921533194506107]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5092723318381104		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27125613549504857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39026423366657953 | validation: 0.3765887936623441]
	TIME [epoch: 8.85 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3141229045786734		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.329465666568973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3217942855738232 | validation: 0.2603069840542389]
	TIME [epoch: 8.86 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3023986186927496		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5066455201660739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40452206942941177 | validation: 0.5184916579137122]
	TIME [epoch: 8.86 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40083796817169076		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3517292951745987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37628363167314477 | validation: 0.4236742623805573]
	TIME [epoch: 8.87 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35815934750198547		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3094652438299911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33381229566598836 | validation: 0.1848724720033708]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22491878227728143		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2706367231165049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24777775269689323 | validation: 0.20284270174332203]
	TIME [epoch: 8.85 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8239645171970755		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3254423726546037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5747034449258395 | validation: 0.18136461064727727]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4970926649706494		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2474251476595592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3722589063151044 | validation: 0.3580219933423194]
	TIME [epoch: 8.88 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27365288514115077		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21745587938259336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24555438226187204 | validation: 0.34421737816038306]
	TIME [epoch: 8.85 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27528583288826663		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3088530110339819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2920694219611243 | validation: 0.2233566682884312]
	TIME [epoch: 8.85 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23675046633173666		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28239245382550593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25957146007862125 | validation: 0.19752653916751933]
	TIME [epoch: 8.85 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18120660198582605		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3033910143876427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24229880818673433 | validation: 0.5304152413879171]
	TIME [epoch: 8.87 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44496544061734167		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3418325713520908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3933990059847162 | validation: 0.1492989432246755]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27237088179836355		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20463544743673218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23850316461754786 | validation: 0.16746892175098282]
	TIME [epoch: 8.85 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18700188318899372		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23159875112274228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.209300317155868 | validation: 0.169286716956559]
	TIME [epoch: 8.85 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3413388628166553		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26815538082925217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30474712182295377 | validation: 0.26211787520484164]
	TIME [epoch: 8.87 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24803409200175947		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4958462678097161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3719401799057378 | validation: 0.2966961715133343]
	TIME [epoch: 8.86 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2568098529405066		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2560820630252324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25644595798286945 | validation: 0.22314673690289966]
	TIME [epoch: 8.85 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2281641993907833		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20380329778842726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21598374858960523 | validation: 0.25435845846444194]
	TIME [epoch: 8.85 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25292621631633666		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28383501528764415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26838061580199035 | validation: 0.16690794294379807]
	TIME [epoch: 8.86 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22495441895723495		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19296599951624932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20896020923674213 | validation: 0.17021071644428976]
	TIME [epoch: 8.87 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21172467325068833		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3800487629041388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29588671807741357 | validation: 0.3164162059539799]
	TIME [epoch: 8.86 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23410458019865796		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6790301466837988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4565673634412285 | validation: 0.1295000160879677]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22351280535389284		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1684718138514527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19599230960267275 | validation: 0.13165954623061094]
	TIME [epoch: 8.86 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6880320795849986		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25762769188214574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4728298857335721 | validation: 0.21545417144728596]
	TIME [epoch: 8.88 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20017727073132036		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18073139220869802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19045433147000917 | validation: 0.14631970299519118]
	TIME [epoch: 8.85 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22916560504640757		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20395446671334402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21656003587987582 | validation: 0.41972281173773995]
	TIME [epoch: 8.85 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20759696326157648		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21305743174573796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21032719750365722 | validation: 0.23142585547134234]
	TIME [epoch: 8.85 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26176767643570975		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27674152710392713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26925460176981847 | validation: 0.17900867213773544]
	TIME [epoch: 8.88 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1678180473719909		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29971843236593587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23376823986896342 | validation: 0.15091082901488906]
	TIME [epoch: 8.86 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24450514567220316		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.33532275164454384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2899139486583735 | validation: 1.3250455291244942]
	TIME [epoch: 8.86 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5283422566907516		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2042654979365806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36630387731366615 | validation: 0.16941051343906782]
	TIME [epoch: 8.85 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1900063936662062		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22936671846734305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2096865560667746 | validation: 0.15346642908060543]
	TIME [epoch: 8.87 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17022895476476368		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24243253504176865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20633074490326617 | validation: 0.1502700494181573]
	TIME [epoch: 8.87 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3099204428832206		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36709973845507304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33851009066914683 | validation: 0.29062776641023663]
	TIME [epoch: 8.86 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34853960787095295		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2886319636630957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3185857857670244 | validation: 0.30031269035940883]
	TIME [epoch: 8.85 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1945732032807002		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21036995278044257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2024715780305714 | validation: 0.35975252662630985]
	TIME [epoch: 8.87 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17925304800885156		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15671901092496182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1679860294669067 | validation: 0.16815988498513396]
	TIME [epoch: 8.87 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10079658398625335		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12366671200713142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11223164799669241 | validation: 0.11326021295704368]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.176634875523015		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17908641369653125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17786064460977308 | validation: 0.131424669573166]
	TIME [epoch: 8.85 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18893739599913792		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2337946157318172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21136600586547755 | validation: 0.24973972267153993]
	TIME [epoch: 8.86 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17018349567291474		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1566240228006966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16340375923680567 | validation: 0.2101545786467443]
	TIME [epoch: 8.88 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.170765266359978		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2594298864512528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2150975764056154 | validation: 0.09714218674268707]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19164210065787035		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14504211883940193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16834210974863614 | validation: 0.16443290772249008]
	TIME [epoch: 8.85 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1146548659155721		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14823899419682174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13144693005619695 | validation: 0.15446583093880015]
	TIME [epoch: 8.85 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15582498321903204		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14638771458519426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15110634890211316 | validation: 0.284584970236622]
	TIME [epoch: 8.88 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26399583419626804		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13362764008080186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19881173713853495 | validation: 0.17042347903585625]
	TIME [epoch: 8.85 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18757607062948206		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1862371731222306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1869066218758563 | validation: 0.4311347224953657]
	TIME [epoch: 8.85 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21179883866608282		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14218828474077871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17699356170343075 | validation: 0.06840189206737712]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07212268956743129		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19535098126701667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13373683541722395 | validation: 0.12937752538726544]
	TIME [epoch: 8.86 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1487103110326235		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12415182249247966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13643106676255162 | validation: 0.18778644245721976]
	TIME [epoch: 8.86 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14698230233190987		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1428828925416961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14493259743680298 | validation: 0.10065132011445081]
	TIME [epoch: 8.85 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1497166539289231		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26290074418169285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20630869905530796 | validation: 0.18364053939599762]
	TIME [epoch: 9.38 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18942051808981777		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1556252749964527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17252289654313524 | validation: 0.25323029215797843]
	TIME [epoch: 8.87 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22007429483163263		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1773875401542742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1987309174929534 | validation: 0.13253767454720677]
	TIME [epoch: 8.87 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20765103617339276		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11471472294729226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1611828795603425 | validation: 0.09379977671292142]
	TIME [epoch: 8.86 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19641319286774472		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.916213679798231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5563134363329879 | validation: 0.40925748211279045]
	TIME [epoch: 8.86 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1759714507532459		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14230120119332348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1591363259732847 | validation: 0.1569812869966868]
	TIME [epoch: 8.86 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17743945127232924		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07708482445924332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12726213786578627 | validation: 0.10974891999022873]
	TIME [epoch: 8.88 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12448134584543658		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6732157657579643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3988485558017004 | validation: 0.14184088334690537]
	TIME [epoch: 8.86 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15968056395083535		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11230960251447979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13599508323265758 | validation: 0.1449814185558545]
	TIME [epoch: 8.86 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09590885799773727		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12608722964319313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11099804382046519 | validation: 0.09779978246292381]
	TIME [epoch: 8.86 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27699471429818107		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13335697723199083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.205175845765086 | validation: 0.08303781029743107]
	TIME [epoch: 8.88 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11010525719440582		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10838639493734648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10924582606587614 | validation: 0.16865569583528894]
	TIME [epoch: 8.86 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11132345772449384		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21165317017812746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16148831395131064 | validation: 0.1292391233798266]
	TIME [epoch: 8.86 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10020582591543388		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11441592825187026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10731087708365206 | validation: 0.12361031142640061]
	TIME [epoch: 8.86 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13246214852925084		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16863908911763875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15055061882344484 | validation: 0.10454683311541065]
	TIME [epoch: 8.88 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10902068719117805		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11021926785590556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10961997752354183 | validation: 0.20493869400613204]
	TIME [epoch: 8.87 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1415850832955166		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16017861804880085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15088185067215873 | validation: 0.12800082902689444]
	TIME [epoch: 8.85 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13053957455019136		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15974652962691932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14514305208855532 | validation: 0.0778554010402043]
	TIME [epoch: 9.06 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11929238221307752		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25946621292918004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18937929757112879 | validation: 0.06336133642141767]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14819630657017605		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2396430700175854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1939196882938807 | validation: 0.31061132664470864]
	TIME [epoch: 8.89 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33407412569314665		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14594076114567653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24000744341941158 | validation: 0.0687402076924118]
	TIME [epoch: 8.85 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17586194752577985		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17006532937908034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17296363845243007 | validation: 0.1538692877514549]
	TIME [epoch: 8.86 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1186411225776349		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10551043057761667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11207577657762577 | validation: 0.10358502704539123]
	TIME [epoch: 8.85 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11042016918919906		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2953584669697889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.202889318079494 | validation: 0.15748224135956562]
	TIME [epoch: 8.88 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11546896275248195		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10068089537250277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10807492906249236 | validation: 0.08080334086553131]
	TIME [epoch: 8.85 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1398424263068045		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11890807872054773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12937525251367613 | validation: 0.09653107976522515]
	TIME [epoch: 8.86 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17608248631870554		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16572046192290793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17090147412080675 | validation: 0.11642639510592967]
	TIME [epoch: 8.86 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07913093859903479		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10339953853889852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09126523856896665 | validation: 0.10140106549691691]
	TIME [epoch: 8.88 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13152348405517567		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13618621457905702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13385484931711633 | validation: 0.17383672470387657]
	TIME [epoch: 8.86 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13180044332696333		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.199429270300729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1656148568138462 | validation: 0.14187024703770534]
	TIME [epoch: 8.86 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09563469107721537		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0963891593962545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09601192523673494 | validation: 0.07055116543876136]
	TIME [epoch: 8.86 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10668876261949507		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1464786882979682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12658372545873164 | validation: 0.13793828889833623]
	TIME [epoch: 8.87 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15681394147182953		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1844929416093001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17065344154056483 | validation: 0.07404001537867133]
	TIME [epoch: 8.87 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12605441697060918		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07737305820288397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10171373758674655 | validation: 0.10133975304319005]
	TIME [epoch: 8.84 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36210341069353547		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2993001806445539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3307017956690447 | validation: 0.12258841608338127]
	TIME [epoch: 8.83 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1436513937500997		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14600325042208018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14482732208608992 | validation: 0.05131961370693569]
	TIME [epoch: 8.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6909414777810643		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08555681270944923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3882491452452568 | validation: 0.09055705971191673]
	TIME [epoch: 8.89 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0829803544515997		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13384737045533127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10841386245346549 | validation: 0.08815533931428914]
	TIME [epoch: 8.89 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09398834752572045		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12272509130240414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1083567194140623 | validation: 0.17351455560205886]
	TIME [epoch: 8.89 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3743290917544962		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3632426584878815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3687858751211888 | validation: 0.3078098181536385]
	TIME [epoch: 8.88 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22766762110146793		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18982213193206285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20874487651676535 | validation: 0.11429672284457347]
	TIME [epoch: 8.88 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08747818031739356		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07956248207513715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08352033119626537 | validation: 0.07526510717080986]
	TIME [epoch: 8.88 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1143522493008355		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1452280969647786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.129790173132807 | validation: 0.0924404951973152]
	TIME [epoch: 8.88 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0952258877416199		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0865483913713905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0908871395565052 | validation: 0.08840399446467473]
	TIME [epoch: 8.84 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08626398454405437		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10745564564563212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09685981509484323 | validation: 0.13692724383500304]
	TIME [epoch: 8.87 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1628618415046016		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3107096660736376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2367857537891196 | validation: 0.2593918359176659]
	TIME [epoch: 8.86 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8256097842640623		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.33306404048854443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5793369123763032 | validation: 0.27600271216335914]
	TIME [epoch: 8.86 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20056721454428525		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21275592593600123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20666157024014323 | validation: 0.17429569278854992]
	TIME [epoch: 8.84 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10008311406287337		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1715533961773814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1358182551201274 | validation: 0.2823911023868222]
	TIME [epoch: 8.86 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11596831026617213		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1047425740912615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1103554421787168 | validation: 0.07698768098387682]
	TIME [epoch: 8.88 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1491155398249633		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15686037264570715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1529879562353353 | validation: 0.12041082881648302]
	TIME [epoch: 8.86 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0884952298624108		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11890811429099894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10370167207670489 | validation: 0.08045172588942337]
	TIME [epoch: 8.85 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17518193228916087		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07069551925741936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1229387257732901 | validation: 0.07632434395795806]
	TIME [epoch: 8.86 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11846258719584686		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11407067096465617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11626662908025151 | validation: 0.09697760930480823]
	TIME [epoch: 8.87 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11492623962819522		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16361842808253838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13927233385536675 | validation: 0.09192591122029982]
	TIME [epoch: 8.85 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1093161132915759		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09207386114879053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10069498722018322 | validation: 0.12271928661584829]
	TIME [epoch: 8.86 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14438961610489143		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12099081790330665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13269021700409903 | validation: 0.18782151638871236]
	TIME [epoch: 8.85 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10654921956798916		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10310718250729771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10482820103764341 | validation: 0.1022569289986042]
	TIME [epoch: 8.87 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10435680555017741		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13699786151937027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12067733353477383 | validation: 0.11481163082087503]
	TIME [epoch: 8.86 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12436544875166829		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14771352487835027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13603948681500927 | validation: 0.24658893584687683]
	TIME [epoch: 8.85 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11580727394261466		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08984189126649933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10282458260455696 | validation: 0.11307332096918175]
	TIME [epoch: 8.86 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13435967211864702		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09266564988374985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11351266100119843 | validation: 0.0935262568635649]
	TIME [epoch: 9.01 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11040935510997654		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09666296678450419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10353616094724036 | validation: 0.1601254991118403]
	TIME [epoch: 8.87 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12380436387678928		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13525599978271663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12953018182975296 | validation: 0.08997076287620875]
	TIME [epoch: 8.85 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1737616787989642		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10252160394804828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13814164137350626 | validation: 0.10739100990138906]
	TIME [epoch: 8.85 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1478105367187204		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17834469131638794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1630776140175542 | validation: 0.11829638601836]
	TIME [epoch: 8.86 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09981290847113769		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1067818040139051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10329735624252138 | validation: 0.09851667772294807]
	TIME [epoch: 8.88 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11334141829204987		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15012459235334086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13173300532269533 | validation: 0.14873940572402786]
	TIME [epoch: 8.86 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1603621576765592		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18596555704889364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17316385736272646 | validation: 0.08801934049658566]
	TIME [epoch: 8.86 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12038963564267909		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11369333674414181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11704148619341044 | validation: 0.16727856691112408]
	TIME [epoch: 8.85 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10226585065505303		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14425920380561527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12326252723033412 | validation: 0.08996089145889158]
	TIME [epoch: 8.88 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16484962888891425		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12311863336372693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14398413112632058 | validation: 0.12785405258359706]
	TIME [epoch: 8.86 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10677720462096876		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12798205316178896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11737962889137883 | validation: 0.612461723935169]
	TIME [epoch: 8.86 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21108252524254972		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0867480312811194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14891527826183457 | validation: 0.0707708500193837]
	TIME [epoch: 8.86 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08444489883042101		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10026724643747094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09235607263394596 | validation: 0.16605107348568998]
	TIME [epoch: 8.87 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1600303406916863		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16795437853384393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16399235961276512 | validation: 0.07934312772037463]
	TIME [epoch: 8.85 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12086795325052879		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16053619756633217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14070207540843047 | validation: 0.08264499006104196]
	TIME [epoch: 8.85 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10508415054348186		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10927355108907326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10717885081627758 | validation: 0.04636590566544641]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1250798083853673		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12112052001908695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1231001642022271 | validation: 0.21997280593686794]
	TIME [epoch: 8.87 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14111047625583542		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14236371778877493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1417370970223052 | validation: 0.12785994856909266]
	TIME [epoch: 8.87 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23407124088236517		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5345621361786024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38431668853048373 | validation: 0.6247374825707198]
	TIME [epoch: 8.85 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5169974282932757		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5015645631778793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5092809957355774 | validation: 0.3487915989107673]
	TIME [epoch: 8.84 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5206039075078918		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2661092599210066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39335658371444915 | validation: 0.1379231668730957]
	TIME [epoch: 8.85 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11428632894355222		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09651375215459501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10540004054907362 | validation: 0.23596172808033516]
	TIME [epoch: 8.88 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1293285538059509		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13429136834808234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13180996107701665 | validation: 0.13473388266858013]
	TIME [epoch: 8.85 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1530718460720558		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.114674849358821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1338733477154384 | validation: 0.11693813468104056]
	TIME [epoch: 8.85 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09563171271611184		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10983103156188996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10273137213900088 | validation: 0.06327853256084408]
	TIME [epoch: 8.84 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08239581274446439		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16887068764016247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12563325019231344 | validation: 0.10473832289246127]
	TIME [epoch: 8.86 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09623984284435733		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13238411838827388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11431198061631562 | validation: 0.11754268226774628]
	TIME [epoch: 8.85 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11671241814338049		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12405150399551246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12038196106944646 | validation: 0.09647736135243584]
	TIME [epoch: 8.85 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09660373581550762		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11618392975520334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10639383278535548 | validation: 0.09101371549941903]
	TIME [epoch: 8.84 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1280305973498217		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12780719572878924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1279188965393055 | validation: 0.1224786202901135]
	TIME [epoch: 9.9 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11357417897925826		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18619102953056874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14988260425491348 | validation: 0.0893064695818479]
	TIME [epoch: 8.87 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10677277140223929		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20715788221587764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15696532680905845 | validation: 0.07107792604455256]
	TIME [epoch: 8.85 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1869380923173675		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1289311921230622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15793464222021486 | validation: 0.08010951267922917]
	TIME [epoch: 8.85 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11252291070388817		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10269364773744556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10760827922066687 | validation: 0.14318831323387055]
	TIME [epoch: 8.84 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08739315076504373		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11686187811927036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10212751444215706 | validation: 0.1757946019193795]
	TIME [epoch: 8.87 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13240791108267622		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2065910291236595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16949947010316785 | validation: 0.11974845130413359]
	TIME [epoch: 8.85 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10457817885116916		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10762088267827925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1060995307647242 | validation: 0.10215664719384715]
	TIME [epoch: 8.84 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15255329705726398		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0877413847989067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12014734092808535 | validation: 0.0722305235838168]
	TIME [epoch: 8.85 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10402832682349603		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1246935187193162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11436092277140611 | validation: 0.07942324902630246]
	TIME [epoch: 8.86 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12923610615799808		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16487930440370363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14705770528085088 | validation: 0.20824379966053658]
	TIME [epoch: 8.84 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12393776416980702		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11095600290150391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11744688353565547 | validation: 0.14026826491417865]
	TIME [epoch: 8.84 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10528473946357833		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12360386803160997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11444430374759415 | validation: 0.09076530652122204]
	TIME [epoch: 8.84 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1151377195339621		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11572663005937125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11543217479666668 | validation: 0.2055458184159636]
	TIME [epoch: 8.85 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2543703421348348		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1434264539140213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19889839802442805 | validation: 0.08020005539215437]
	TIME [epoch: 8.84 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23443308937282276		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18598972693160565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21021140815221423 | validation: 0.27562633869128605]
	TIME [epoch: 8.83 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2660015899483962		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24398414417190745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25499286706015173 | validation: 0.1252925683046932]
	TIME [epoch: 8.83 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2799159926046336		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.37793417332955775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32892508296709566 | validation: 0.27171369632893916]
	TIME [epoch: 8.84 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19305039979384359		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20740290581637816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20022665280511093 | validation: 0.2782160080234456]
	TIME [epoch: 8.87 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1074858574355552		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19586218327082705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15167402035319114 | validation: 0.19667500290062162]
	TIME [epoch: 8.84 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24067071166049486		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18265936110914394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21166503638481943 | validation: 0.07682427684725464]
	TIME [epoch: 8.84 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1600534911426345		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13214484378134927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1460991674619919 | validation: 0.11519355607456058]
	TIME [epoch: 8.84 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.172136334195071		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08831466225335863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1302254982242148 | validation: 0.07716259589000289]
	TIME [epoch: 8.86 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08478157602867673		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11950239491153732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10214198547010703 | validation: 0.09782639250809678]
	TIME [epoch: 8.85 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08838369068258138		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09594342920034257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.092163559941462 | validation: 0.18879018404849596]
	TIME [epoch: 8.85 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10541861616504496		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11263222985376833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10902542300940665 | validation: 0.0850049784408401]
	TIME [epoch: 8.84 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13147637503008675		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1387949702453068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13513567263769674 | validation: 0.07673313753508831]
	TIME [epoch: 8.86 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10287822404418052		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12329256109926616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11308539257172337 | validation: 0.3888208510648469]
	TIME [epoch: 8.85 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22748763168698966		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14333083723072923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18540923445885943 | validation: 0.0693587556820872]
	TIME [epoch: 8.84 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10400550381303583		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09430215282226606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09915382831765095 | validation: 0.19371393306129175]
	TIME [epoch: 8.85 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16160088593532562		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12550007620353448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14355048106943002 | validation: 0.12255413308246665]
	TIME [epoch: 8.84 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17969400677761221		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29569956272618114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23769678475189665 | validation: 0.32124517046340534]
	TIME [epoch: 8.87 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23992573534089665		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16196322585166262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20094448059627962 | validation: 0.13901443106266476]
	TIME [epoch: 8.84 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16441445263438492		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1627135033190372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16356397797671104 | validation: 0.09176955021102873]
	TIME [epoch: 8.84 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24509019487942335		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12500089141681778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18504554314812055 | validation: 0.11359671384410203]
	TIME [epoch: 8.83 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10087110522669132		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11769709631335527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10928410077002329 | validation: 0.14045849150706902]
	TIME [epoch: 8.86 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12214844944243741		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10124836703772815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11169840824008277 | validation: 0.1511802127920075]
	TIME [epoch: 8.85 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18801395501985083		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09431465028999901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1411643026549249 | validation: 0.09814826927489118]
	TIME [epoch: 8.84 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09806898370507144		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11849346545269372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10828122457888259 | validation: 0.14179880013293575]
	TIME [epoch: 8.85 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10098553948382952		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0881852002352727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0945853698595511 | validation: 0.10118405315252724]
	TIME [epoch: 8.86 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11828523260078641		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08837205527223488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10332864393651067 | validation: 0.11012119724551098]
	TIME [epoch: 8.85 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09620455900208279		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2169012244438941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15655289172298845 | validation: 0.15553509977466953]
	TIME [epoch: 8.84 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15786775718442922		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13207336842646075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.144970562805445 | validation: 0.06916893895388047]
	TIME [epoch: 8.85 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12645580783999139		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13501608476832733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13073594630415936 | validation: 0.23668029813134964]
	TIME [epoch: 8.85 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14744157396871554		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6080184736128862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37773002379080084 | validation: 0.5278601525665516]
	TIME [epoch: 8.84 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.317459196823631		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12737639697367983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22241779689865537 | validation: 0.11650543214187245]
	TIME [epoch: 8.84 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10653061283126693		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10462915449073043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10557988366099871 | validation: 0.09795112968298135]
	TIME [epoch: 8.84 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09450469541094389		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14500329249740848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11975399395417619 | validation: 0.08938075790338523]
	TIME [epoch: 8.84 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08756211404899608		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11204113915254554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09980162660077081 | validation: 0.09878621708351669]
	TIME [epoch: 8.86 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10335258700699398		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11606904699740553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10971081700219976 | validation: 0.11359501725153928]
	TIME [epoch: 8.85 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10413932530473768		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0796115263911477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0918754258479427 | validation: 0.1039233825256571]
	TIME [epoch: 8.85 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10884295802093254		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10556092283245258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10720194042669258 | validation: 0.058565246206314966]
	TIME [epoch: 8.85 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08966140076613639		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09634707667832629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09300423872223132 | validation: 0.08902039322860958]
	TIME [epoch: 8.87 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16994763311928818		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13286922912890792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15140843112409805 | validation: 0.09678235632055847]
	TIME [epoch: 8.85 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10961167408379782		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18488996504224026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14725081956301903 | validation: 0.21546064463953987]
	TIME [epoch: 8.84 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16270339485797078		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4502529715981886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30647818322807974 | validation: 0.1842329557195218]
	TIME [epoch: 8.86 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1591877256385416		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11917510526487629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13918141545170892 | validation: 0.11401398961371888]
	TIME [epoch: 8.87 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20873468636601342		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1594118044354129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18407324540071318 | validation: 0.1231619271638964]
	TIME [epoch: 8.85 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10480282534696332		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16420295686837832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13450289110767083 | validation: 0.1191489352367218]
	TIME [epoch: 8.85 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14710453663560633		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19155043243782494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16932748453671564 | validation: 0.12754114635797564]
	TIME [epoch: 8.84 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11511964240217601		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11763795196145901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1163787971818175 | validation: 0.08903960415039182]
	TIME [epoch: 8.84 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12150569916617485		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28651339485704586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20400954701161034 | validation: 0.25957543886338313]
	TIME [epoch: 8.87 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12366402544595186		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13966016166117828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.131662093553565 | validation: 0.30126952881801683]
	TIME [epoch: 8.86 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16238338695543358		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12557995657381366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14398167176462362 | validation: 0.10855353392015764]
	TIME [epoch: 8.85 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15084835571166347		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09948040077179694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12516437824173018 | validation: 0.06110193509559313]
	TIME [epoch: 8.84 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14217937411123432		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08671055387212133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1144449639916778 | validation: 0.07906441613356832]
	TIME [epoch: 8.87 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08238654704984769		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1736733735321127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12802996029098018 | validation: 0.08047436642967223]
	TIME [epoch: 8.85 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08743411092149064		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08819484549780589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08781447820964826 | validation: 0.07792911070350816]
	TIME [epoch: 8.84 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09759864729402509		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12173748684597877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10966806707000194 | validation: 0.04835542896508012]
	TIME [epoch: 8.83 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1303401402808263		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1524234846568815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1413818124688539 | validation: 0.1508302817881432]
	TIME [epoch: 8.86 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14721045660157206		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09873149664987682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12297097662572443 | validation: 0.10867908222248916]
	TIME [epoch: 8.85 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10980353915973873		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09587685970706018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10284019943339946 | validation: 0.09146400569391464]
	TIME [epoch: 8.84 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12517729463785174		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23010384407053638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1776405693541941 | validation: 0.24950566502963745]
	TIME [epoch: 8.85 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13786791535965995		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08806134444509604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11296462990237799 | validation: 0.07392921776084811]
	TIME [epoch: 8.85 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14890411839711443		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11749632810301922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1332002232500668 | validation: 0.14024223515152562]
	TIME [epoch: 8.86 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1888793907173206		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11333739718219224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1511083939497564 | validation: 0.13327034856625058]
	TIME [epoch: 8.91 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15992042203033657		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11475472965178471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13733757584106066 | validation: 0.09276610818547726]
	TIME [epoch: 8.84 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5252264083130445		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1696975017255481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3474619550192963 | validation: 0.21326879564618145]
	TIME [epoch: 8.85 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10616740053593217		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07820086220701152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09218413137147184 | validation: 0.07961791716752284]
	TIME [epoch: 8.87 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13334081721137556		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.085769798503027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10955530785720131 | validation: 0.09951794552101587]
	TIME [epoch: 8.84 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3088630964730154		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11419251061203224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21152780354252382 | validation: 0.11902007428780294]
	TIME [epoch: 8.83 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12643993023811392		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09201413410743868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10922703217277632 | validation: 0.030328789009143584]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08483977773956783		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07318508746217511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07901243260087147 | validation: 0.06681426532492757]
	TIME [epoch: 8.88 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0953274670775121		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10699400282577305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10116073495164257 | validation: 0.10710308797370871]
	TIME [epoch: 8.84 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11212594791101524		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08271561520999864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09742078156050696 | validation: 0.06156915758702934]
	TIME [epoch: 8.83 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06934352192582979		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08367502018318229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07650927105450603 | validation: 0.0466082799384233]
	TIME [epoch: 8.84 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08206476144208837		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08972339189844752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08589407667026794 | validation: 0.09289701839516681]
	TIME [epoch: 8.85 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08269602452131067		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1248625419212726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10377928322129164 | validation: 0.07757497015778565]
	TIME [epoch: 8.85 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.154970573430889		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1087686176937264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1318695955623077 | validation: 0.13456504231777305]
	TIME [epoch: 8.84 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07918098283875555		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.26286132182884525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1710211523338004 | validation: 0.4040633470058589]
	TIME [epoch: 8.84 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2908769220140781		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23412472321770283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2625008226158905 | validation: 0.10111228769372685]
	TIME [epoch: 8.84 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14413456569216004		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11017320620099377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1271538859465769 | validation: 0.12053415234741902]
	TIME [epoch: 8.86 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10805864927289503		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14941297290226305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1287358110875791 | validation: 0.09664287147075633]
	TIME [epoch: 8.88 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10486647414465278		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17271675271234993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13879161342850138 | validation: 0.18664540338512942]
	TIME [epoch: 8.84 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1291806451797575		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14436199557116752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13677132037546252 | validation: 0.07971324647700699]
	TIME [epoch: 8.84 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09436957016116512		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11804611957313896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10620784486715204 | validation: 0.10456023663161862]
	TIME [epoch: 8.86 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12014399537747693		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10381108969590677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11197754253669186 | validation: 0.0661193920369075]
	TIME [epoch: 8.84 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1228374830565658		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11899416835999707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12091582570828145 | validation: 0.06474259536248592]
	TIME [epoch: 8.84 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09641038740512083		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09671914328175454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09656476534343769 | validation: 0.06332356393531421]
	TIME [epoch: 8.84 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11847331038354766		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0886606007482655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10356695556590659 | validation: 0.07085528241420061]
	TIME [epoch: 8.86 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09585188414577067		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09929806438677312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0975749742662719 | validation: 0.1299854173412658]
	TIME [epoch: 8.85 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1121362973545423		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08474257266835375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09843943501144801 | validation: 0.19105466180021888]
	TIME [epoch: 8.83 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11476329118078092		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09577883715570756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10527106416824428 | validation: 0.09337778675147411]
	TIME [epoch: 8.83 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08078161183666646		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07534815069658621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07806488126662633 | validation: 0.05299163733168228]
	TIME [epoch: 8.85 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15659708926432292		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11623474909362815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13641591917897555 | validation: 0.23787325187855607]
	TIME [epoch: 8.86 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18994323320229486		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2138531362007406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2018981847015177 | validation: 0.11729263750816481]
	TIME [epoch: 8.84 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11224592514444094		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3534938678459728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2328698964952069 | validation: 0.07249638169605721]
	TIME [epoch: 8.84 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11290717540205389		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10551309146551194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1092101334337829 | validation: 0.07259570596881938]
	TIME [epoch: 8.84 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08699854683572636		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14933330677970763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11816592680771698 | validation: 0.11341052091420042]
	TIME [epoch: 8.86 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09735716975133604		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10659444276773686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10197580625953646 | validation: 0.09141211609122027]
	TIME [epoch: 8.86 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11993374109442231		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08766587325066953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10379980717254593 | validation: 0.148265621860658]
	TIME [epoch: 8.85 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10269115883701982		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11950298524427197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11109707204064592 | validation: 0.07426911541364936]
	TIME [epoch: 8.84 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14624582585331763		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1794789497769752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16286238781514642 | validation: 0.08225700774273165]
	TIME [epoch: 8.88 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12091696308258655		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1094621079222037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11518953550239512 | validation: 0.06910583077219029]
	TIME [epoch: 8.86 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18476580943926016		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11979426973242846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1522800395858443 | validation: 0.05991010776808926]
	TIME [epoch: 8.85 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09897824542112259		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1905499313612118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14476408839116717 | validation: 0.18610730413487508]
	TIME [epoch: 8.84 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12482984581821766		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11488010983673085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11985497782747426 | validation: 0.09383096763995454]
	TIME [epoch: 8.87 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08548285573095568		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10373932174693881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09461108873894725 | validation: 0.26193835486940753]
	TIME [epoch: 8.86 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1324266233012262		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13418907240861777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13330784785492195 | validation: 0.10204470598297738]
	TIME [epoch: 8.86 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09683794107811902		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14983787147652589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12333790627732248 | validation: 0.1379654337625543]
	TIME [epoch: 8.84 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10701263643104783		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10552888479510847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10627076061307816 | validation: 0.09147043127584205]
	TIME [epoch: 8.85 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13914316449776343		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1721728485485965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15565800652317996 | validation: 0.23007109228112443]
	TIME [epoch: 8.87 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1268937842196769		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09858376850157244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11273877636062465 | validation: 0.11496631806460558]
	TIME [epoch: 8.85 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13025955970889289		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6666567533998204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8984581565543566 | validation: 0.13586527894489886]
	TIME [epoch: 8.85 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13710165451224157		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1152645578953679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12618310620380474 | validation: 0.11209160024890716]
	TIME [epoch: 8.85 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12089720142583171		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12743785943077388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1241675304283028 | validation: 0.13345609576781212]
	TIME [epoch: 8.87 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14758870667487406		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10314356809528077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12536613738507738 | validation: 0.09409126029919995]
	TIME [epoch: 8.85 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1130497495824472		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1337797509360693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12341475025925826 | validation: 0.05808975570346961]
	TIME [epoch: 8.85 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12006427162467755		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1484750656076302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1342696686161539 | validation: 0.0781005421609769]
	TIME [epoch: 8.85 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3460844697557741		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.34054528105734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34331487540655703 | validation: 0.09761550109763473]
	TIME [epoch: 8.87 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10848095853995754		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13253650168334724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12050873011165238 | validation: 0.12226918916233118]
	TIME [epoch: 8.86 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11631443546335105		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10968140238641115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11299791892488112 | validation: 0.1211439313347147]
	TIME [epoch: 8.85 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23043819835837526		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09800836620899162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16422328228368344 | validation: 0.11906190751207275]
	TIME [epoch: 8.84 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12640753871122673		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10042865663869224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1134180976749595 | validation: 0.14280046354048265]
	TIME [epoch: 8.84 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13943008225222078		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12221202876526369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13082105550874223 | validation: 0.08477348180234628]
	TIME [epoch: 8.87 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12205021943631963		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10845102251473937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11525062097552949 | validation: 0.08527903591507907]
	TIME [epoch: 8.85 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09912385391032516		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11531488234032325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10721936812532422 | validation: 0.05957221647691835]
	TIME [epoch: 8.85 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1429830201818264		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10906231765949029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12602266892065836 | validation: 0.08008506100078902]
	TIME [epoch: 8.85 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11153820733650237		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07950836373625556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09552328553637897 | validation: 0.09346605795478154]
	TIME [epoch: 8.87 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14958366722299285		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10484084282318769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12721225502309028 | validation: 0.14676001260621108]
	TIME [epoch: 8.84 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12534991732720896		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08660581477689572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10597786605205232 | validation: 0.05394722512066398]
	TIME [epoch: 8.85 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07354414225323445		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0977555548823381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08564984856778626 | validation: 0.16497973765526522]
	TIME [epoch: 8.84 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1311101231520295		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1282190812572949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12966460220466222 | validation: 0.11557247236571355]
	TIME [epoch: 8.86 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09574236931023973		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10619536021571414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10096886476297692 | validation: 0.18476032359252642]
	TIME [epoch: 8.85 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13840505140097076		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1227389292492959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13057199032513334 | validation: 0.15364790302631978]
	TIME [epoch: 8.85 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12410724914722596		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12724763808178305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1256774436145045 | validation: 0.14020662231308784]
	TIME [epoch: 8.84 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14026341122932412		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08544080933505922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11285211028219166 | validation: 0.14046038103808528]
	TIME [epoch: 8.86 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10989621345929021		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11560920386485027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11275270866207025 | validation: 0.08968093791436624]
	TIME [epoch: 8.85 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09029361031521138		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17205344247977797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1311735263974947 | validation: 0.08726505587485132]
	TIME [epoch: 8.84 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1805026583942728		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0977570824843457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13912987043930922 | validation: 0.10448202065117178]
	TIME [epoch: 8.84 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1566837737670221		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11256468589056505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1346242298287936 | validation: 0.1706019058807477]
	TIME [epoch: 8.85 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10960292483476494		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22438582034401183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1669943725893884 | validation: 0.14655697324409822]
	TIME [epoch: 8.87 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2660937495992238		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24349849827257736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25479612393590056 | validation: 0.2539447599121889]
	TIME [epoch: 8.84 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3454183739035922		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5710777792465308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4582480765750616 | validation: 0.38356476943964724]
	TIME [epoch: 8.84 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21571372594796406		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11063410934671716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16317391764734057 | validation: 0.0985537540327606]
	TIME [epoch: 8.84 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09905474300613921		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11809715582673311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10857594941643615 | validation: 0.18473359318701824]
	TIME [epoch: 8.86 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13871527517025378		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14020970725946952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13946249121486162 | validation: 0.14802758811768968]
	TIME [epoch: 8.84 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1452828934844145		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1304108330841195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13784686328426698 | validation: 0.07695333923303474]
	TIME [epoch: 8.84 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10796003643975423		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12592517467928974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11694260555952199 | validation: 0.13781876679713945]
	TIME [epoch: 8.84 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20446668148624156		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15322699256377634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1788468370250089 | validation: 0.14560120591390505]
	TIME [epoch: 8.86 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09266625590762406		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08926213511792541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09096419551277472 | validation: 0.13557106227305277]
	TIME [epoch: 8.85 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1162195639893832		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1700431036138163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14313133380159976 | validation: 0.07674941741359285]
	TIME [epoch: 8.84 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07833723803332865		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08836678745980775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08335201274656821 | validation: 0.1344921222088159]
	TIME [epoch: 8.84 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17440762874288548		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13085439264167803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1526310106922818 | validation: 0.08981730823082146]
	TIME [epoch: 8.84 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14831462402249332		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17073117989745323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15952290195997326 | validation: 0.15959424758078095]
	TIME [epoch: 8.86 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08492862036139522		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11090063116951279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09791462576545398 | validation: 0.14476137650436757]
	TIME [epoch: 8.84 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11729880676104303		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09349224394118023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1053955253511116 | validation: 0.13750027734945242]
	TIME [epoch: 8.84 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10278771739803223		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07028589444189849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08653680591996535 | validation: 0.0564140088200438]
	TIME [epoch: 8.84 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1492764026079481		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11155683887360235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13041662074077523 | validation: 0.09729946581329385]
	TIME [epoch: 8.87 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21945159000405604		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23114081776385795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.225296203883957 | validation: 0.23825953640152017]
	TIME [epoch: 8.84 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20359678180307364		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23331412083451206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21845545131879285 | validation: 0.16375360150357413]
	TIME [epoch: 8.84 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18463490774617375		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1425988954723166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1636169016092452 | validation: 0.0864970211073005]
	TIME [epoch: 8.84 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33182593912162395		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3522311120921542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34202852560688907 | validation: 0.1649262307788963]
	TIME [epoch: 8.86 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10525495306880421		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13445143712762087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11985319509821253 | validation: 0.22120813125515623]
	TIME [epoch: 8.84 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11768250919309631		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11240921873999396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11504586396654515 | validation: 0.07571465534735121]
	TIME [epoch: 8.84 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14567880943450157		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1184358127456061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13205731109005384 | validation: 0.09838881404236482]
	TIME [epoch: 8.84 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08080793815900689		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08245459475464087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08163126645682389 | validation: 0.05729349935412784]
	TIME [epoch: 8.85 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10101201376207644		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18929968689964627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14515585033086137 | validation: 0.13740034282019942]
	TIME [epoch: 8.86 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08637233697065165		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08676471780971333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08656852739018249 | validation: 0.08831254146055795]
	TIME [epoch: 8.84 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13817536453975626		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0948072161281839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11649129033397006 | validation: 0.271845649106293]
	TIME [epoch: 8.85 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1364003570117606		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07433301848432813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10536668774804436 | validation: 0.0674974395198606]
	TIME [epoch: 8.85 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08360374029658707		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13562389641871198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10961381835764952 | validation: 0.14935420185460824]
	TIME [epoch: 8.87 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09690979164925108		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10074718293505029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09882848729215069 | validation: 0.08793603213495702]
	TIME [epoch: 8.85 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13403993792000013		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11589570195068562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12496781993534285 | validation: 0.07849110966308859]
	TIME [epoch: 8.84 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10372401727285427		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07947450817933205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09159926272609314 | validation: 0.05555214720404583]
	TIME [epoch: 8.84 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05841612044331035		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11776118632646038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08808865338488535 | validation: 0.05105771435351866]
	TIME [epoch: 8.86 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08560998538949127		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10884797404547834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09722897971748481 | validation: 0.09744916295792105]
	TIME [epoch: 8.85 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12998236850767314		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16358223938491823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14678230394629563 | validation: 0.15957175380061062]
	TIME [epoch: 8.84 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1420202851275864		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1094005248745789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12571040500108263 | validation: 0.10799001481043499]
	TIME [epoch: 8.85 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1255195199461008		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1523282171858648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13892386856598277 | validation: 0.1964811882861654]
	TIME [epoch: 8.86 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1632817963680258		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48861608737203915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3259489418700325 | validation: 0.1506738837783765]
	TIME [epoch: 8.85 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17105187757012932		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09974541395713032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1353986457636298 | validation: 0.08089725726037157]
	TIME [epoch: 8.85 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10520540346804379		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11213736035196178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10867138191000277 | validation: 0.23190753443144557]
	TIME [epoch: 8.84 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11482747075799442		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12179832345536859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11831289710668151 | validation: 0.13933063542293428]
	TIME [epoch: 8.85 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15143639018913718		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1399349093020281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14568564974558268 | validation: 0.11378820355832342]
	TIME [epoch: 8.87 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12826473729938323		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12486767355834305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12656620542886315 | validation: 0.10321597876321804]
	TIME [epoch: 8.84 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10929704102041679		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10973879500880215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10951791801460944 | validation: 0.10020716901542215]
	TIME [epoch: 8.84 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1674649634844779		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10106452125395501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13426474236921643 | validation: 0.06521711920074644]
	TIME [epoch: 8.84 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08721636938315683		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08812900934682913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08767268936499298 | validation: 0.13246031964854613]
	TIME [epoch: 8.87 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08627750789973454		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1415206595149499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11389908370734221 | validation: 0.11325634285010808]
	TIME [epoch: 8.84 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.133622153455289		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.071595731781077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10260894261818299 | validation: 0.1386235964944646]
	TIME [epoch: 8.84 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15734617036312742		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09345515147124715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12540066091718727 | validation: 0.11039599475391915]
	TIME [epoch: 8.85 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1119162331850011		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1248596755346912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11838795435984614 | validation: 0.2775707966889887]
	TIME [epoch: 8.86 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11266092115941066		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08526812392431742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09896452254186404 | validation: 0.12471703379743676]
	TIME [epoch: 8.85 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1463254838394173		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09759580466885942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12196064425413833 | validation: 0.10195888748373941]
	TIME [epoch: 8.84 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08251358837800615		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1253228245365489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10391820645727752 | validation: 0.12192265491125706]
	TIME [epoch: 8.83 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11416606597508157		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10285878659447238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.108512426284777 | validation: 0.1188898075458878]
	TIME [epoch: 8.85 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1527973292305303		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11103914289117793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13191823606085412 | validation: 0.09037574131793377]
	TIME [epoch: 8.87 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08974409583425047		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1090246533912232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09938437461273684 | validation: 0.10947629441386048]
	TIME [epoch: 8.84 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11153365422175088		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15259144227257992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1320625482471654 | validation: 0.12370644642453213]
	TIME [epoch: 8.83 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10564303205069805		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18117258223063487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14340780714066648 | validation: 0.09347596401586286]
	TIME [epoch: 8.84 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09981357792730955		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13043658386762955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11512508089746956 | validation: 0.1102381891872511]
	TIME [epoch: 8.87 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17235342430138284		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1939387694024744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1831460968519286 | validation: 0.10082214222685272]
	TIME [epoch: 8.84 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11231616640332387		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.082720271615344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09751821900933391 | validation: 0.08789259014445179]
	TIME [epoch: 8.84 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0990983745132454		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14609812091979424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12259824771651981 | validation: 0.11828102704058367]
	TIME [epoch: 8.84 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09256924985166705		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12453450773904617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10855187879535662 | validation: 0.09491550793241707]
	TIME [epoch: 8.86 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10961244841068207		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09469481751511809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10215363296290007 | validation: 0.0901061807936778]
	TIME [epoch: 8.85 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15509339659124827		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10785918023837063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13147628841480943 | validation: 0.06861869778549855]
	TIME [epoch: 8.84 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08359884416786463		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1340906812408242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10884476270434443 | validation: 0.11195062511354525]
	TIME [epoch: 8.84 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10861926231907648		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1329561496360562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12078770597756631 | validation: 0.2759265143087629]
	TIME [epoch: 8.85 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13924698880381264		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16215390454040116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15070044667210686 | validation: 0.6506102654613546]
	TIME [epoch: 8.86 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24033912765630155		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09927875533203498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16980894149416828 | validation: 0.07360064725994006]
	TIME [epoch: 8.83 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08135418119117756		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09281300546755183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08708359332936469 | validation: 0.06651119168068842]
	TIME [epoch: 8.84 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10529200754444204		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24089921953935578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1730956135418989 | validation: 0.37241803714954336]
	TIME [epoch: 8.83 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24908974420991306		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1610186584191805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2050542013145468 | validation: 0.07148063834826873]
	TIME [epoch: 8.85 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09463305881065433		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14199145538604663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11831225709835047 | validation: 0.12718672912753992]
	TIME [epoch: 8.84 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1410233796337263		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1747225061832546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15787294290849044 | validation: 0.13135405522679922]
	TIME [epoch: 8.84 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10821282413537629		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1379679232970188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12309037371619753 | validation: 0.15540995801945617]
	TIME [epoch: 8.84 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09178085381294163		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10021833146326428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09599959263810295 | validation: 0.1431751386497056]
	TIME [epoch: 8.86 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11122708065446527		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2684616935578868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18984438710617604 | validation: 0.16782020463353006]
	TIME [epoch: 8.84 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23904832925254887		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2480374987504554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2435429140015021 | validation: 0.6360094109085461]
	TIME [epoch: 8.84 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4517229275439947		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13194271396144733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.291832820752721 | validation: 0.1024283642310134]
	TIME [epoch: 8.85 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16312319241056722		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12713987021555198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14513153131305961 | validation: 0.14064158453023884]
	TIME [epoch: 8.86 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2587289855873205		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2208835251102566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23980625534878858 | validation: 0.1804835435255383]
	TIME [epoch: 8.85 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12067137968806944		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1275245033627113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12409794152539037 | validation: 0.21194797547357985]
	TIME [epoch: 8.84 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23029281502820503		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14886987490670228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18958134496745366 | validation: 0.18079027563759706]
	TIME [epoch: 8.84 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13411589456896977		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09167725880062227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.112896576684796 | validation: 0.1238081369209732]
	TIME [epoch: 8.84 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13383827891302708		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12454643168540931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1291923552992182 | validation: 0.059327609808281195]
	TIME [epoch: 8.87 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1454610042880974		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15629482048777535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15087791238793638 | validation: 0.08036429558119448]
	TIME [epoch: 8.84 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12257791998986722		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09988765035448835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11123278517217779 | validation: 0.09896078642980893]
	TIME [epoch: 8.84 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09674336278104063		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0793753489140949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08805935584756777 | validation: 0.1406777850000589]
	TIME [epoch: 8.84 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12117044157273697		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10602425157763229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11359734657518464 | validation: 0.08963232792419691]
	TIME [epoch: 8.86 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14028849688258682		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10389942928671017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12209396308464851 | validation: 0.09375755426005886]
	TIME [epoch: 8.84 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16169875489633673		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15696996187562634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1593343583859815 | validation: 0.1650076995955207]
	TIME [epoch: 8.84 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15153121186417176		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11368271898650913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13260696542534045 | validation: 0.11113800246959088]
	TIME [epoch: 8.83 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11181005887596304		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08194056480853551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09687531184224928 | validation: 0.09739751867657079]
	TIME [epoch: 8.86 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10011835656680405		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09934860442818635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0997334804974952 | validation: 0.10049361863306562]
	TIME [epoch: 8.84 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08572632966549583		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13479227456419687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11025930211484633 | validation: 0.13064732894411812]
	TIME [epoch: 8.84 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10696616287938843		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0952615505849407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10111385673216455 | validation: 0.058129419566701854]
	TIME [epoch: 9.73 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08282605803686419		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1393681609943437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11109710951560395 | validation: 0.23192542106248856]
	TIME [epoch: 8.84 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11665144847666764		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08296649613617105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09980897230641934 | validation: 0.09913877350372319]
	TIME [epoch: 8.85 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12419118138344717		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0957768405528049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10998401096812602 | validation: 0.0849220753983771]
	TIME [epoch: 8.83 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11439035302382324		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.097661123982814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1060257385033186 | validation: 0.0767140697597625]
	TIME [epoch: 8.84 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10163248805990877		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.39652866114157403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2490805746007414 | validation: 0.2457381733265454]
	TIME [epoch: 8.84 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11999304537139965		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12709149899599384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12354227218369673 | validation: 0.11680768616449036]
	TIME [epoch: 8.86 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10998213289185063		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10748902693103664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10873557991144363 | validation: 0.09927246440508938]
	TIME [epoch: 8.85 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12018779749154858		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09207801731395238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10613290740275047 | validation: 0.30041955487851424]
	TIME [epoch: 8.85 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2971070037503231		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14899312876020057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22305006625526186 | validation: 0.1736314585962974]
	TIME [epoch: 8.84 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0936390935055946		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14287745177202243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11825827263880853 | validation: 0.19883787281389748]
	TIME [epoch: 8.86 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15841832999539646		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08936014702062764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12388923850801206 | validation: 0.08578412270020244]
	TIME [epoch: 8.84 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21732391616450525		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11847971317502079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16790181466976306 | validation: 0.09601643937521523]
	TIME [epoch: 8.84 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07847995760587952		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14727354908361284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11287675334474619 | validation: 0.08005464836218554]
	TIME [epoch: 8.84 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1062184050750589		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13459561144047227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1204070082577656 | validation: 0.09665588888556695]
	TIME [epoch: 8.87 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08664497737045626		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12808741531145487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10736619634095557 | validation: 0.22541426706472834]
	TIME [epoch: 8.85 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10403698195803704		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10717690603815569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10560694399809638 | validation: 0.10497470298796625]
	TIME [epoch: 8.83 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09507129572442377		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09306757087636292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09406943330039334 | validation: 0.07634522541725257]
	TIME [epoch: 8.83 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14026956613696956		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10766353982586445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12396655298141698 | validation: 0.07955471604201642]
	TIME [epoch: 8.84 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09039320017899824		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11356807750505989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10198063884202908 | validation: 0.1466107748970094]
	TIME [epoch: 8.86 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09937777495580549		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1359453586037568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11766156677978112 | validation: 0.07593142789055612]
	TIME [epoch: 8.84 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09510727204348854		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1159061838187688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10550672793112867 | validation: 0.06546312285715336]
	TIME [epoch: 8.84 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1597613152920066		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09501352387760069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12738741958480362 | validation: 0.11965499123672632]
	TIME [epoch: 8.85 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19505135940525692		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15064559055736132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17284847498130912 | validation: 0.2432787696427098]
	TIME [epoch: 8.87 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1477143078351772		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25967022686920915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20369226735219317 | validation: 0.9705568238324437]
	TIME [epoch: 8.83 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8878172411256452		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2444398996637381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5661285703946917 | validation: 0.20823206249436338]
	TIME [epoch: 8.83 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2214107662801182		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3601277195965632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2907692429383407 | validation: 0.1900117617207046]
	TIME [epoch: 8.85 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19880770308180395		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08686280137406285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14283525222793342 | validation: 0.06033182288656609]
	TIME [epoch: 8.85 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09078343260651797		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07295415836198024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08186879548424911 | validation: 0.052331237615871135]
	TIME [epoch: 8.85 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09117291399953988		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22227020877114745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15672156138534363 | validation: 0.19200301003522202]
	TIME [epoch: 8.84 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14452578947685693		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10483268768089864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12467923857887779 | validation: 0.15555580842495453]
	TIME [epoch: 8.83 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09639729112006462		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13571772436079318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11605750774042892 | validation: 0.10305224038434929]
	TIME [epoch: 8.84 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11245597626159484		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12138211450397166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11691904538278326 | validation: 0.13724825930957307]
	TIME [epoch: 8.86 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11626277136612888		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12150533446404306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11888405291508594 | validation: 0.06980430917079858]
	TIME [epoch: 8.84 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09033481559143818		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09943437912854056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09488459735998936 | validation: 0.07913970345844051]
	TIME [epoch: 8.83 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13263015564915703		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11141470819227821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1220224319207176 | validation: 0.12287665760367211]
	TIME [epoch: 8.84 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1701028135107932		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12840217967991374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1492524965953535 | validation: 0.12874279854466353]
	TIME [epoch: 8.86 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17360529321803214		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1114559423107812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14253061776440665 | validation: 0.13112343999619383]
	TIME [epoch: 8.84 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12702678716737206		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10039992457677283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11371335587207246 | validation: 0.12683485865526872]
	TIME [epoch: 8.84 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12303920796858267		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0867504225864064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10489481527749453 | validation: 0.11455031641948774]
	TIME [epoch: 8.84 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11943789584606868		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18563113096075187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1525345134034103 | validation: 0.18150905966816488]
	TIME [epoch: 8.85 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12631318603511474		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11453017393109313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12042167998310394 | validation: 0.08013461590807985]
	TIME [epoch: 8.85 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10298365320502931		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11403029170776477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10850697245639704 | validation: 0.12694092231038956]
	TIME [epoch: 8.84 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11821747101038085		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13293667343548082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12557707222293085 | validation: 0.11429283360168094]
	TIME [epoch: 8.84 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13121399335592604		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17870645019598624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15496022177595617 | validation: 0.11822907902049595]
	TIME [epoch: 8.85 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13123985164557678		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2172137042989215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17422677797224914 | validation: 0.20439535919888402]
	TIME [epoch: 8.85 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.144711775572938		[learning rate: 0.0099862]
		[batch 20/20] avg loss: 0.07378305497050612		[learning rate: 0.0099709]
	Learning Rate: 0.00997088
	LOSS [training: 0.10924741527172203 | validation: 0.08752593286385335]
	TIME [epoch: 8.84 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08829269568249139		[learning rate: 0.0099556]
		[batch 20/20] avg loss: 0.2190977078589406		[learning rate: 0.0099403]
	Learning Rate: 0.00994031
	LOSS [training: 0.153695201770716 | validation: 0.22201635459232477]
	TIME [epoch: 8.83 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18192178416370672		[learning rate: 0.0099251]
		[batch 20/20] avg loss: 0.13910354694201627		[learning rate: 0.0099098]
	Learning Rate: 0.00990984
	LOSS [training: 0.16051266555286148 | validation: 0.08491412623565711]
	TIME [epoch: 8.84 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09041374629701449		[learning rate: 0.0098946]
		[batch 20/20] avg loss: 0.1647296776135682		[learning rate: 0.0098795]
	Learning Rate: 0.00987946
	LOSS [training: 0.12757171195529135 | validation: 0.09510292516408858]
	TIME [epoch: 8.86 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16439706390392916		[learning rate: 0.0098643]
		[batch 20/20] avg loss: 0.12599774686581472		[learning rate: 0.0098492]
	Learning Rate: 0.00984918
	LOSS [training: 0.14519740538487194 | validation: 0.08663549687372861]
	TIME [epoch: 8.85 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17197522490644326		[learning rate: 0.0098341]
		[batch 20/20] avg loss: 0.1555678132674495		[learning rate: 0.009819]
	Learning Rate: 0.00981899
	LOSS [training: 0.1637715190869464 | validation: 0.11758349548199133]
	TIME [epoch: 8.83 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13157219732493758		[learning rate: 0.0098039]
		[batch 20/20] avg loss: 0.09905385643829125		[learning rate: 0.0097889]
	Learning Rate: 0.00978889
	LOSS [training: 0.1153130268816144 | validation: 0.1012122140019498]
	TIME [epoch: 8.85 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.112129884466091		[learning rate: 0.0097739]
		[batch 20/20] avg loss: 0.09403107613132769		[learning rate: 0.0097589]
	Learning Rate: 0.00975888
	LOSS [training: 0.10308048029870935 | validation: 0.13703040586188683]
	TIME [epoch: 8.86 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11012497213002563		[learning rate: 0.0097439]
		[batch 20/20] avg loss: 0.11354547985142582		[learning rate: 0.009729]
	Learning Rate: 0.00972897
	LOSS [training: 0.11183522599072573 | validation: 0.15413024319750818]
	TIME [epoch: 8.85 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11291304294709832		[learning rate: 0.009714]
		[batch 20/20] avg loss: 0.16830632939780704		[learning rate: 0.0096991]
	Learning Rate: 0.00969914
	LOSS [training: 0.14060968617245267 | validation: 0.11749987444184497]
	TIME [epoch: 8.86 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11411762247195108		[learning rate: 0.0096843]
		[batch 20/20] avg loss: 0.09648531494226723		[learning rate: 0.0096694]
	Learning Rate: 0.00966941
	LOSS [training: 0.10530146870710914 | validation: 0.09527879515627335]
	TIME [epoch: 8.84 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1243495388593188		[learning rate: 0.0096546]
		[batch 20/20] avg loss: 0.2951134140808461		[learning rate: 0.0096398]
	Learning Rate: 0.00963977
	LOSS [training: 0.20973147647008245 | validation: 0.23669550042254822]
	TIME [epoch: 8.87 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10271775234123066		[learning rate: 0.009625]
		[batch 20/20] avg loss: 0.12463979981571878		[learning rate: 0.0096102]
	Learning Rate: 0.00961022
	LOSS [training: 0.1136787760784747 | validation: 0.1003291689704352]
	TIME [epoch: 8.86 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13365179562969948		[learning rate: 0.0095955]
		[batch 20/20] avg loss: 0.12742583211562392		[learning rate: 0.0095808]
	Learning Rate: 0.00958076
	LOSS [training: 0.1305388138726617 | validation: 0.09248695607930267]
	TIME [epoch: 8.85 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09254967928313969		[learning rate: 0.0095661]
		[batch 20/20] avg loss: 0.11248547227424584		[learning rate: 0.0095514]
	Learning Rate: 0.00955139
	LOSS [training: 0.10251757577869278 | validation: 0.12041903797568107]
	TIME [epoch: 8.84 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07851307235646932		[learning rate: 0.0095367]
		[batch 20/20] avg loss: 0.1270516628968084		[learning rate: 0.0095221]
	Learning Rate: 0.00952211
	LOSS [training: 0.10278236762663884 | validation: 0.20013486746401496]
	TIME [epoch: 8.84 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10199504673176829		[learning rate: 0.0095075]
		[batch 20/20] avg loss: 0.1497243540554353		[learning rate: 0.0094929]
	Learning Rate: 0.00949292
	LOSS [training: 0.1258597003936018 | validation: 0.083814544215293]
	TIME [epoch: 8.99 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08923513955662161		[learning rate: 0.0094784]
		[batch 20/20] avg loss: 0.19621631826988017		[learning rate: 0.0094638]
	Learning Rate: 0.00946382
	LOSS [training: 0.14272572891325092 | validation: 0.12777037452033121]
	TIME [epoch: 8.85 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12379456197926966		[learning rate: 0.0094493]
		[batch 20/20] avg loss: 0.14187541439446053		[learning rate: 0.0094348]
	Learning Rate: 0.00943481
	LOSS [training: 0.13283498818686507 | validation: 0.1726432072847344]
	TIME [epoch: 8.84 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09839455076811929		[learning rate: 0.0094203]
		[batch 20/20] avg loss: 0.09232562559444599		[learning rate: 0.0094059]
	Learning Rate: 0.00940589
	LOSS [training: 0.09536008818128262 | validation: 0.09493485319298543]
	TIME [epoch: 8.85 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1009337916028497		[learning rate: 0.0093915]
		[batch 20/20] avg loss: 0.11309043748083705		[learning rate: 0.0093771]
	Learning Rate: 0.00937706
	LOSS [training: 0.10701211454184337 | validation: 0.13745352191107085]
	TIME [epoch: 8.86 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1281651365028809		[learning rate: 0.0093627]
		[batch 20/20] avg loss: 0.09151031195106972		[learning rate: 0.0093483]
	Learning Rate: 0.00934831
	LOSS [training: 0.10983772422697528 | validation: 0.09245138845787873]
	TIME [epoch: 8.86 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09784013822557039		[learning rate: 0.009334]
		[batch 20/20] avg loss: 0.11108645407212281		[learning rate: 0.0093197]
	Learning Rate: 0.00931966
	LOSS [training: 0.1044632961488466 | validation: 0.06865185634675938]
	TIME [epoch: 8.85 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09281500466482873		[learning rate: 0.0093054]
		[batch 20/20] avg loss: 0.0971358617900198		[learning rate: 0.0092911]
	Learning Rate: 0.00929109
	LOSS [training: 0.09497543322742426 | validation: 0.14454824850903786]
	TIME [epoch: 8.85 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0840870864843917		[learning rate: 0.0092768]
		[batch 20/20] avg loss: 0.11780530686839874		[learning rate: 0.0092626]
	Learning Rate: 0.00926261
	LOSS [training: 0.10094619667639519 | validation: 0.09070645259815543]
	TIME [epoch: 8.88 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0967143283126799		[learning rate: 0.0092484]
		[batch 20/20] avg loss: 0.08829360662451925		[learning rate: 0.0092342]
	Learning Rate: 0.00923422
	LOSS [training: 0.09250396746859957 | validation: 0.20073162775021067]
	TIME [epoch: 8.86 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13357553733314692		[learning rate: 0.0092201]
		[batch 20/20] avg loss: 0.11673416116961663		[learning rate: 0.0092059]
	Learning Rate: 0.00920591
	LOSS [training: 0.1251548492513818 | validation: 0.06999842748699514]
	TIME [epoch: 8.85 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0827666120206065		[learning rate: 0.0091918]
		[batch 20/20] avg loss: 0.11107913603469484		[learning rate: 0.0091777]
	Learning Rate: 0.00917769
	LOSS [training: 0.09692287402765068 | validation: 0.1793925660930263]
	TIME [epoch: 8.85 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1432463090350884		[learning rate: 0.0091636]
		[batch 20/20] avg loss: 0.18171453974790935		[learning rate: 0.0091496]
	Learning Rate: 0.00914956
	LOSS [training: 0.16248042439149887 | validation: 0.10547489046008406]
	TIME [epoch: 8.87 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09504077044183946		[learning rate: 0.0091355]
		[batch 20/20] avg loss: 0.0875895943414489		[learning rate: 0.0091215]
	Learning Rate: 0.00912151
	LOSS [training: 0.09131518239164418 | validation: 0.0633875140502113]
	TIME [epoch: 8.87 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08684180016324528		[learning rate: 0.0091075]
		[batch 20/20] avg loss: 0.3658989583382197		[learning rate: 0.0090935]
	Learning Rate: 0.00909355
	LOSS [training: 0.2263703792507325 | validation: 0.2673279805356543]
	TIME [epoch: 8.85 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5491563402633727		[learning rate: 0.0090796]
		[batch 20/20] avg loss: 0.8430527724624548		[learning rate: 0.0090657]
	Learning Rate: 0.00906567
	LOSS [training: 0.6961045563629138 | validation: 0.27234297082897063]
	TIME [epoch: 8.86 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18349185336211252		[learning rate: 0.0090518]
		[batch 20/20] avg loss: 0.23578070310430316		[learning rate: 0.0090379]
	Learning Rate: 0.00903788
	LOSS [training: 0.20963627823320788 | validation: 0.10319451366812314]
	TIME [epoch: 8.85 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1012872355961012		[learning rate: 0.009024]
		[batch 20/20] avg loss: 0.0982020353489499		[learning rate: 0.0090102]
	Learning Rate: 0.00901018
	LOSS [training: 0.09974463547252553 | validation: 0.18810008338847464]
	TIME [epoch: 8.87 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1829586998094243		[learning rate: 0.0089964]
		[batch 20/20] avg loss: 0.09105353068962381		[learning rate: 0.0089826]
	Learning Rate: 0.00898256
	LOSS [training: 0.13700611524952405 | validation: 0.12969584642154888]
	TIME [epoch: 8.86 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17181382868536746		[learning rate: 0.0089688]
		[batch 20/20] avg loss: 0.16181957830398985		[learning rate: 0.008955]
	Learning Rate: 0.00895502
	LOSS [training: 0.16681670349467864 | validation: 0.17132089297238867]
	TIME [epoch: 8.86 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10363623689147158		[learning rate: 0.0089413]
		[batch 20/20] avg loss: 0.18869660355103948		[learning rate: 0.0089276]
	Learning Rate: 0.00892757
	LOSS [training: 0.14616642022125553 | validation: 0.07913131599572903]
	TIME [epoch: 8.85 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11323187822049721		[learning rate: 0.0089139]
		[batch 20/20] avg loss: 0.1137538845956219		[learning rate: 0.0089002]
	Learning Rate: 0.0089002
	LOSS [training: 0.11349288140805955 | validation: 0.09961577735183681]
	TIME [epoch: 8.87 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.167118155891789		[learning rate: 0.0088866]
		[batch 20/20] avg loss: 0.19545966472277151		[learning rate: 0.0088729]
	Learning Rate: 0.00887292
	LOSS [training: 0.18128891030728025 | validation: 0.26248530531751957]
	TIME [epoch: 8.85 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1835742790487817		[learning rate: 0.0088593]
		[batch 20/20] avg loss: 0.08131566223907896		[learning rate: 0.0088457]
	Learning Rate: 0.00884572
	LOSS [training: 0.1324449706439303 | validation: 0.07907489364848427]
	TIME [epoch: 8.85 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07815091102151986		[learning rate: 0.0088322]
		[batch 20/20] avg loss: 0.11557963670724644		[learning rate: 0.0088186]
	Learning Rate: 0.00881861
	LOSS [training: 0.09686527386438315 | validation: 0.1095287285798052]
	TIME [epoch: 8.84 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06637025405399574		[learning rate: 0.0088051]
		[batch 20/20] avg loss: 0.10781570252790473		[learning rate: 0.0087916]
	Learning Rate: 0.00879157
	LOSS [training: 0.08709297829095024 | validation: 0.04544644758505198]
	TIME [epoch: 8.88 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10521237073609764		[learning rate: 0.0087781]
		[batch 20/20] avg loss: 0.132644986949982		[learning rate: 0.0087646]
	Learning Rate: 0.00876462
	LOSS [training: 0.11892867884303979 | validation: 0.12071273236139259]
	TIME [epoch: 8.85 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07767246248640727		[learning rate: 0.0087512]
		[batch 20/20] avg loss: 0.10498485247675782		[learning rate: 0.0087378]
	Learning Rate: 0.00873776
	LOSS [training: 0.09132865748158256 | validation: 0.08007951765687044]
	TIME [epoch: 8.86 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08695545819244949		[learning rate: 0.0087244]
		[batch 20/20] avg loss: 0.11304374098842218		[learning rate: 0.008711]
	Learning Rate: 0.00871097
	LOSS [training: 0.09999959959043582 | validation: 0.07385039249794827]
	TIME [epoch: 8.84 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07424165463339231		[learning rate: 0.0086976]
		[batch 20/20] avg loss: 0.11826268357869804		[learning rate: 0.0086843]
	Learning Rate: 0.00868427
	LOSS [training: 0.09625216910604519 | validation: 0.06320432542312641]
	TIME [epoch: 8.85 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0875492856921056		[learning rate: 0.0086709]
		[batch 20/20] avg loss: 0.09591613515574307		[learning rate: 0.0086576]
	Learning Rate: 0.00865765
	LOSS [training: 0.09173271042392432 | validation: 0.1390706413547037]
	TIME [epoch: 8.87 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2778861870704782		[learning rate: 0.0086444]
		[batch 20/20] avg loss: 0.09777536597768795		[learning rate: 0.0086311]
	Learning Rate: 0.00863111
	LOSS [training: 0.18783077652408306 | validation: 0.14562101651803513]
	TIME [epoch: 8.86 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13139812425380087		[learning rate: 0.0086179]
		[batch 20/20] avg loss: 0.1437201334249245		[learning rate: 0.0086047]
	Learning Rate: 0.00860465
	LOSS [training: 0.13755912883936267 | validation: 0.08337346387092008]
	TIME [epoch: 8.86 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11599388689990309		[learning rate: 0.0085915]
		[batch 20/20] avg loss: 0.11616100067537496		[learning rate: 0.0085783]
	Learning Rate: 0.00857828
	LOSS [training: 0.11607744378763903 | validation: 0.041140656555608085]
	TIME [epoch: 8.86 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09162241377573564		[learning rate: 0.0085651]
		[batch 20/20] avg loss: 0.11803658361857527		[learning rate: 0.008552]
	Learning Rate: 0.00855198
	LOSS [training: 0.10482949869715548 | validation: 0.12144326039286324]
	TIME [epoch: 8.88 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1608760820077763		[learning rate: 0.0085389]
		[batch 20/20] avg loss: 0.09346812536968577		[learning rate: 0.0085258]
	Learning Rate: 0.00852576
	LOSS [training: 0.12717210368873105 | validation: 0.06992580930105102]
	TIME [epoch: 8.86 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1170005894449252		[learning rate: 0.0085127]
		[batch 20/20] avg loss: 0.06744373405367798		[learning rate: 0.0084996]
	Learning Rate: 0.00849963
	LOSS [training: 0.09222216174930159 | validation: 0.04292658992049267]
	TIME [epoch: 8.86 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1521334825029897		[learning rate: 0.0084866]
		[batch 20/20] avg loss: 0.1261075261859069		[learning rate: 0.0084736]
	Learning Rate: 0.00847358
	LOSS [training: 0.13912050434444828 | validation: 0.07065964817023428]
	TIME [epoch: 8.85 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12396627120900497		[learning rate: 0.0084606]
		[batch 20/20] avg loss: 0.14419496792134898		[learning rate: 0.0084476]
	Learning Rate: 0.0084476
	LOSS [training: 0.13408061956517697 | validation: 0.10216306894032866]
	TIME [epoch: 8.87 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10499510797042759		[learning rate: 0.0084346]
		[batch 20/20] avg loss: 0.07360517476248726		[learning rate: 0.0084217]
	Learning Rate: 0.0084217
	LOSS [training: 0.08930014136645742 | validation: 0.034273920011923496]
	TIME [epoch: 8.86 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13599418634231492		[learning rate: 0.0084088]
		[batch 20/20] avg loss: 0.11435807382276938		[learning rate: 0.0083959]
	Learning Rate: 0.00839589
	LOSS [training: 0.12517613008254214 | validation: 0.17702703178903073]
	TIME [epoch: 8.85 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12806938901610074		[learning rate: 0.008383]
		[batch 20/20] avg loss: 0.1316793270690402		[learning rate: 0.0083702]
	Learning Rate: 0.00837015
	LOSS [training: 0.12987435804257047 | validation: 0.17694345571028092]
	TIME [epoch: 8.85 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10852926216656042		[learning rate: 0.0083573]
		[batch 20/20] avg loss: 0.11235705361145572		[learning rate: 0.0083445]
	Learning Rate: 0.00834449
	LOSS [training: 0.11044315788900805 | validation: 0.10057138961218368]
	TIME [epoch: 8.85 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11949908501803601		[learning rate: 0.0083317]
		[batch 20/20] avg loss: 0.07388632271867912		[learning rate: 0.0083189]
	Learning Rate: 0.00831891
	LOSS [training: 0.09669270386835757 | validation: 0.049865067626127746]
	TIME [epoch: 8.87 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06011498317587396		[learning rate: 0.0083062]
		[batch 20/20] avg loss: 0.10194560688975329		[learning rate: 0.0082934]
	Learning Rate: 0.00829341
	LOSS [training: 0.08103029503281363 | validation: 0.09568908570314662]
	TIME [epoch: 8.85 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10806056551337954		[learning rate: 0.0082807]
		[batch 20/20] avg loss: 0.06730245589756398		[learning rate: 0.008268]
	Learning Rate: 0.00826799
	LOSS [training: 0.08768151070547178 | validation: 0.10154826604764518]
	TIME [epoch: 8.85 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1338868921705919		[learning rate: 0.0082553]
		[batch 20/20] avg loss: 0.1188423399949117		[learning rate: 0.0082426]
	Learning Rate: 0.00824265
	LOSS [training: 0.1263646160827518 | validation: 0.3353155694774664]
	TIME [epoch: 8.85 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23524689124993045		[learning rate: 0.00823]
		[batch 20/20] avg loss: 0.16875635275834494		[learning rate: 0.0082174]
	Learning Rate: 0.00821738
	LOSS [training: 0.20200162200413768 | validation: 0.3212468642548037]
	TIME [epoch: 8.87 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.137939655144423		[learning rate: 0.0082048]
		[batch 20/20] avg loss: 0.18732223969637266		[learning rate: 0.0081922]
	Learning Rate: 0.00819219
	LOSS [training: 0.1626309474203978 | validation: 0.18187498654212317]
	TIME [epoch: 8.84 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1165556623500533		[learning rate: 0.0081796]
		[batch 20/20] avg loss: 0.0768824634934518		[learning rate: 0.0081671]
	Learning Rate: 0.00816708
	LOSS [training: 0.09671906292175256 | validation: 0.08240182926115344]
	TIME [epoch: 8.85 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14450066169500908		[learning rate: 0.0081545]
		[batch 20/20] avg loss: 0.126463488256885		[learning rate: 0.008142]
	Learning Rate: 0.00814204
	LOSS [training: 0.13548207497594703 | validation: 0.145734165358248]
	TIME [epoch: 8.84 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1205144654069162		[learning rate: 0.0081296]
		[batch 20/20] avg loss: 0.16761022511958906		[learning rate: 0.0081171]
	Learning Rate: 0.00811708
	LOSS [training: 0.14406234526325262 | validation: 0.15532738940753565]
	TIME [epoch: 8.86 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13414070174266723		[learning rate: 0.0081046]
		[batch 20/20] avg loss: 0.0956222860712447		[learning rate: 0.0080922]
	Learning Rate: 0.0080922
	LOSS [training: 0.11488149390695597 | validation: 0.1091249067288887]
	TIME [epoch: 8.85 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11223677290809053		[learning rate: 0.0080798]
		[batch 20/20] avg loss: 0.12714010655706567		[learning rate: 0.0080674]
	Learning Rate: 0.00806739
	LOSS [training: 0.1196884397325781 | validation: 0.06514903999558519]
	TIME [epoch: 8.84 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09288900324235914		[learning rate: 0.008055]
		[batch 20/20] avg loss: 0.11325126635416605		[learning rate: 0.0080427]
	Learning Rate: 0.00804267
	LOSS [training: 0.10307013479826259 | validation: 0.12773088294044097]
	TIME [epoch: 8.86 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1450487351131647		[learning rate: 0.0080303]
		[batch 20/20] avg loss: 0.10339861331196092		[learning rate: 0.008018]
	Learning Rate: 0.00801801
	LOSS [training: 0.12422367421256282 | validation: 0.11433642360082166]
	TIME [epoch: 8.87 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08554848782138387		[learning rate: 0.0080057]
		[batch 20/20] avg loss: 0.08750877427156369		[learning rate: 0.0079934]
	Learning Rate: 0.00799343
	LOSS [training: 0.08652863104647378 | validation: 0.06822565080540716]
	TIME [epoch: 8.86 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07923293587640187		[learning rate: 0.0079812]
		[batch 20/20] avg loss: 0.12299377184536123		[learning rate: 0.0079689]
	Learning Rate: 0.00796893
	LOSS [training: 0.10111335386088154 | validation: 0.1523105867403599]
	TIME [epoch: 8.85 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09107445217194356		[learning rate: 0.0079567]
		[batch 20/20] avg loss: 0.08959441486949331		[learning rate: 0.0079445]
	Learning Rate: 0.0079445
	LOSS [training: 0.09033443352071845 | validation: 0.15291762639751766]
	TIME [epoch: 8.85 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0923074202581399		[learning rate: 0.0079323]
		[batch 20/20] avg loss: 0.06789099078541236		[learning rate: 0.0079201]
	Learning Rate: 0.00792015
	LOSS [training: 0.08009920552177612 | validation: 0.14109438460691442]
	TIME [epoch: 8.85 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10924109340554178		[learning rate: 0.007908]
		[batch 20/20] avg loss: 0.0809871384108897		[learning rate: 0.0078959]
	Learning Rate: 0.00789587
	LOSS [training: 0.09511411590821574 | validation: 0.2049277104497659]
	TIME [epoch: 8.87 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12466276388848847		[learning rate: 0.0078838]
		[batch 20/20] avg loss: 0.08133799228135365		[learning rate: 0.0078717]
	Learning Rate: 0.00787167
	LOSS [training: 0.10300037808492109 | validation: 0.04740928363786997]
	TIME [epoch: 8.85 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0667827359377668		[learning rate: 0.0078596]
		[batch 20/20] avg loss: 0.18629283289915202		[learning rate: 0.0078475]
	Learning Rate: 0.00784754
	LOSS [training: 0.12653778441845942 | validation: 0.1168249882921049]
	TIME [epoch: 8.84 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11989991300956153		[learning rate: 0.0078355]
		[batch 20/20] avg loss: 0.11305765102762351		[learning rate: 0.0078235]
	Learning Rate: 0.00782348
	LOSS [training: 0.11647878201859249 | validation: 0.12520751940333452]
	TIME [epoch: 8.85 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12827872576387678		[learning rate: 0.0078115]
		[batch 20/20] avg loss: 0.09574566620367406		[learning rate: 0.0077995]
	Learning Rate: 0.0077995
	LOSS [training: 0.1120121959837754 | validation: 0.12408979646381212]
	TIME [epoch: 8.87 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19398686386266267		[learning rate: 0.0077875]
		[batch 20/20] avg loss: 0.14887158518860782		[learning rate: 0.0077756]
	Learning Rate: 0.00777559
	LOSS [training: 0.17142922452563522 | validation: 0.18686740113606204]
	TIME [epoch: 8.86 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09892356382014207		[learning rate: 0.0077637]
		[batch 20/20] avg loss: 0.0991483642609445		[learning rate: 0.0077518]
	Learning Rate: 0.00775175
	LOSS [training: 0.09903596404054327 | validation: 0.0827338714980674]
	TIME [epoch: 8.85 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08167012319779991		[learning rate: 0.0077399]
		[batch 20/20] avg loss: 0.084451991519749		[learning rate: 0.007728]
	Learning Rate: 0.00772799
	LOSS [training: 0.08306105735877448 | validation: 0.03593113471393977]
	TIME [epoch: 8.85 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09422369260101675		[learning rate: 0.0077161]
		[batch 20/20] avg loss: 0.09662238120932436		[learning rate: 0.0077043]
	Learning Rate: 0.0077043
	LOSS [training: 0.09542303690517054 | validation: 0.05694891314410627]
	TIME [epoch: 8.88 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07846104816030065		[learning rate: 0.0076925]
		[batch 20/20] avg loss: 0.09185878000302757		[learning rate: 0.0076807]
	Learning Rate: 0.00768069
	LOSS [training: 0.08515991408166411 | validation: 0.1085584388998613]
	TIME [epoch: 8.85 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11409174357860094		[learning rate: 0.0076689]
		[batch 20/20] avg loss: 0.06291778316827983		[learning rate: 0.0076571]
	Learning Rate: 0.00765714
	LOSS [training: 0.0885047633734404 | validation: 0.06807995986609952]
	TIME [epoch: 8.85 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09696163953310975		[learning rate: 0.0076454]
		[batch 20/20] avg loss: 0.07279162530363226		[learning rate: 0.0076337]
	Learning Rate: 0.00763367
	LOSS [training: 0.08487663241837098 | validation: 0.08313889273810074]
	TIME [epoch: 8.85 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09473631365730675		[learning rate: 0.007622]
		[batch 20/20] avg loss: 0.094053077261541		[learning rate: 0.0076103]
	Learning Rate: 0.00761027
	LOSS [training: 0.09439469545942387 | validation: 0.10149314271222698]
	TIME [epoch: 8.86 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09081229437504111		[learning rate: 0.0075986]
		[batch 20/20] avg loss: 0.08353990598258722		[learning rate: 0.0075869]
	Learning Rate: 0.00758694
	LOSS [training: 0.08717610017881415 | validation: 0.12623627323878478]
	TIME [epoch: 8.88 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07937802045321639		[learning rate: 0.0075753]
		[batch 20/20] avg loss: 0.08825859709715622		[learning rate: 0.0075637]
	Learning Rate: 0.00756368
	LOSS [training: 0.08381830877518631 | validation: 0.16690387678320803]
	TIME [epoch: 8.85 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07172964974215748		[learning rate: 0.0075521]
		[batch 20/20] avg loss: 0.05952565259029019		[learning rate: 0.0075405]
	Learning Rate: 0.0075405
	LOSS [training: 0.06562765116622384 | validation: 0.07660661301927685]
	TIME [epoch: 8.85 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08756836312542635		[learning rate: 0.0075289]
		[batch 20/20] avg loss: 0.07277161985785831		[learning rate: 0.0075174]
	Learning Rate: 0.00751738
	LOSS [training: 0.08016999149164232 | validation: 0.03948053017921373]
	TIME [epoch: 8.85 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16534761410769955		[learning rate: 0.0075059]
		[batch 20/20] avg loss: 0.15896141393309762		[learning rate: 0.0074943]
	Learning Rate: 0.00749434
	LOSS [training: 0.16215451402039863 | validation: 0.14754618662864524]
	TIME [epoch: 8.87 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05965540944380789		[learning rate: 0.0074828]
		[batch 20/20] avg loss: 0.08735519973609437		[learning rate: 0.0074714]
	Learning Rate: 0.00747137
	LOSS [training: 0.07350530458995115 | validation: 0.1209703562685159]
	TIME [epoch: 8.85 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0713215140660047		[learning rate: 0.0074599]
		[batch 20/20] avg loss: 0.12579006240163107		[learning rate: 0.0074485]
	Learning Rate: 0.00744846
	LOSS [training: 0.09855578823381789 | validation: 0.07899078404162944]
	TIME [epoch: 8.85 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07573116504038513		[learning rate: 0.007437]
		[batch 20/20] avg loss: 0.06251102304564277		[learning rate: 0.0074256]
	Learning Rate: 0.00742563
	LOSS [training: 0.06912109404301395 | validation: 0.06403124359245838]
	TIME [epoch: 8.85 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14818853511078642		[learning rate: 0.0074142]
		[batch 20/20] avg loss: 0.07670321140608113		[learning rate: 0.0074029]
	Learning Rate: 0.00740287
	LOSS [training: 0.11244587325843378 | validation: 0.10255082083038908]
	TIME [epoch: 8.87 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07153121261639574		[learning rate: 0.0073915]
		[batch 20/20] avg loss: 0.08742970936344571		[learning rate: 0.0073802]
	Learning Rate: 0.00738017
	LOSS [training: 0.07948046098992073 | validation: 0.11555866082545632]
	TIME [epoch: 8.87 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07717206888116286		[learning rate: 0.0073689]
		[batch 20/20] avg loss: 0.0812930093916979		[learning rate: 0.0073576]
	Learning Rate: 0.00735755
	LOSS [training: 0.07923253913643037 | validation: 0.07253931953144838]
	TIME [epoch: 8.85 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09700907384263059		[learning rate: 0.0073463]
		[batch 20/20] avg loss: 0.07976308471735745		[learning rate: 0.007335]
	Learning Rate: 0.007335
	LOSS [training: 0.088386079279994 | validation: 0.1589952498847137]
	TIME [epoch: 8.86 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09836869007859789		[learning rate: 0.0073237]
		[batch 20/20] avg loss: 0.06816334322415543		[learning rate: 0.0073125]
	Learning Rate: 0.00731251
	LOSS [training: 0.08326601665137667 | validation: 0.06696370995333395]
	TIME [epoch: 8.87 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07253172356427773		[learning rate: 0.0073013]
		[batch 20/20] avg loss: 0.060872438038054165		[learning rate: 0.0072901]
	Learning Rate: 0.0072901
	LOSS [training: 0.06670208080116594 | validation: 0.05919000488183967]
	TIME [epoch: 8.86 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0780306987174489		[learning rate: 0.0072789]
		[batch 20/20] avg loss: 0.0709357162909521		[learning rate: 0.0072678]
	Learning Rate: 0.00726775
	LOSS [training: 0.07448320750420051 | validation: 0.04400831692293987]
	TIME [epoch: 8.86 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07028957538481811		[learning rate: 0.0072566]
		[batch 20/20] avg loss: 0.10186750194645322		[learning rate: 0.0072455]
	Learning Rate: 0.00724547
	LOSS [training: 0.08607853866563567 | validation: 0.06723671119903205]
	TIME [epoch: 8.85 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10639853679618246		[learning rate: 0.0072344]
		[batch 20/20] avg loss: 0.08304884902558991		[learning rate: 0.0072233]
	Learning Rate: 0.00722326
	LOSS [training: 0.0947236929108862 | validation: 0.06615311413109197]
	TIME [epoch: 8.85 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06179959814175001		[learning rate: 0.0072122]
		[batch 20/20] avg loss: 0.06517071118780494		[learning rate: 0.0072011]
	Learning Rate: 0.00720112
	LOSS [training: 0.06348515466477747 | validation: 0.050547378192755496]
	TIME [epoch: 8.87 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06424262118067911		[learning rate: 0.0071901]
		[batch 20/20] avg loss: 0.08666192915729146		[learning rate: 0.007179]
	Learning Rate: 0.00717904
	LOSS [training: 0.07545227516898528 | validation: 0.0654158627257403]
	TIME [epoch: 8.86 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06743259770305525		[learning rate: 0.007168]
		[batch 20/20] avg loss: 0.11013253581559661		[learning rate: 0.007157]
	Learning Rate: 0.00715704
	LOSS [training: 0.08878256675932593 | validation: 0.06229044799236646]
	TIME [epoch: 8.86 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06330044356761576		[learning rate: 0.0071461]
		[batch 20/20] avg loss: 0.051170578362237554		[learning rate: 0.0071351]
	Learning Rate: 0.0071351
	LOSS [training: 0.05723551096492665 | validation: 0.049732364382349854]
	TIME [epoch: 8.85 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06656077218393064		[learning rate: 0.0071242]
		[batch 20/20] avg loss: 0.11618593592983177		[learning rate: 0.0071132]
	Learning Rate: 0.00711323
	LOSS [training: 0.0913733540568812 | validation: 0.05760153859700184]
	TIME [epoch: 8.87 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09513642913707179		[learning rate: 0.0071023]
		[batch 20/20] avg loss: 0.07358623008759765		[learning rate: 0.0070914]
	Learning Rate: 0.00709142
	LOSS [training: 0.08436132961233471 | validation: 0.046474489045309784]
	TIME [epoch: 8.86 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08815119827455684		[learning rate: 0.0070805]
		[batch 20/20] avg loss: 0.16152345750103583		[learning rate: 0.0070697]
	Learning Rate: 0.00706968
	LOSS [training: 0.12483732788779629 | validation: 0.15232647084886375]
	TIME [epoch: 8.85 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12090966296509593		[learning rate: 0.0070588]
		[batch 20/20] avg loss: 0.3929422692741403		[learning rate: 0.007048]
	Learning Rate: 0.00704801
	LOSS [training: 0.2569259661196181 | validation: 0.14953603799116907]
	TIME [epoch: 8.87 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10348265382871329		[learning rate: 0.0070372]
		[batch 20/20] avg loss: 0.08696377095242228		[learning rate: 0.0070264]
	Learning Rate: 0.00702641
	LOSS [training: 0.0952232123905678 | validation: 0.1400735347610763]
	TIME [epoch: 8.88 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14769895129876326		[learning rate: 0.0070156]
		[batch 20/20] avg loss: 0.09090687617641988		[learning rate: 0.0070049]
	Learning Rate: 0.00700487
	LOSS [training: 0.11930291373759158 | validation: 0.13084007102183065]
	TIME [epoch: 8.86 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09146722055405516		[learning rate: 0.0069941]
		[batch 20/20] avg loss: 0.12660431457960297		[learning rate: 0.0069834]
	Learning Rate: 0.0069834
	LOSS [training: 0.10903576756682906 | validation: 0.07765426328863717]
	TIME [epoch: 8.85 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0971005813237019		[learning rate: 0.0069727]
		[batch 20/20] avg loss: 0.12231738397166428		[learning rate: 0.006962]
	Learning Rate: 0.00696199
	LOSS [training: 0.10970898264768311 | validation: 0.14122452634750177]
	TIME [epoch: 8.85 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13460922202862427		[learning rate: 0.0069513]
		[batch 20/20] avg loss: 0.09647134156499149		[learning rate: 0.0069406]
	Learning Rate: 0.00694065
	LOSS [training: 0.11554028179680786 | validation: 0.11065660274309377]
	TIME [epoch: 8.86 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22154534326845693		[learning rate: 0.00693]
		[batch 20/20] avg loss: 0.08294627262729563		[learning rate: 0.0069194]
	Learning Rate: 0.00691937
	LOSS [training: 0.15224580794787626 | validation: 0.10788107965812493]
	TIME [epoch: 8.87 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07571869388790098		[learning rate: 0.0069088]
		[batch 20/20] avg loss: 0.10179568739196065		[learning rate: 0.0068982]
	Learning Rate: 0.00689816
	LOSS [training: 0.08875719063993082 | validation: 0.05342412535537332]
	TIME [epoch: 8.85 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0622057321207868		[learning rate: 0.0068876]
		[batch 20/20] avg loss: 0.052642561262501		[learning rate: 0.006877]
	Learning Rate: 0.00687702
	LOSS [training: 0.0574241466916439 | validation: 0.03511028239898113]
	TIME [epoch: 8.85 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09340672887412135		[learning rate: 0.0068665]
		[batch 20/20] avg loss: 0.04801479681517138		[learning rate: 0.0068559]
	Learning Rate: 0.00685593
	LOSS [training: 0.07071076284464636 | validation: 0.038003453517027844]
	TIME [epoch: 8.84 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07307754431385874		[learning rate: 0.0068454]
		[batch 20/20] avg loss: 0.08197316620651272		[learning rate: 0.0068349]
	Learning Rate: 0.00683492
	LOSS [training: 0.07752535526018572 | validation: 0.07973355637234827]
	TIME [epoch: 8.87 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05620881541255972		[learning rate: 0.0068244]
		[batch 20/20] avg loss: 0.16274840771208404		[learning rate: 0.006814]
	Learning Rate: 0.00681397
	LOSS [training: 0.10947861156232189 | validation: 0.10741120823599806]
	TIME [epoch: 8.85 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08514994802744617		[learning rate: 0.0068035]
		[batch 20/20] avg loss: 0.05947745671475403		[learning rate: 0.0067931]
	Learning Rate: 0.00679308
	LOSS [training: 0.0723137023711001 | validation: 0.05308982995206221]
	TIME [epoch: 8.85 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07445907219224794		[learning rate: 0.0067827]
		[batch 20/20] avg loss: 0.058245434284277656		[learning rate: 0.0067723]
	Learning Rate: 0.00677225
	LOSS [training: 0.06635225323826278 | validation: 0.045225948681902606]
	TIME [epoch: 8.85 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08352660768761358		[learning rate: 0.0067619]
		[batch 20/20] avg loss: 0.062097595872160606		[learning rate: 0.0067515]
	Learning Rate: 0.0067515
	LOSS [training: 0.0728121017798871 | validation: 0.0347577555910733]
	TIME [epoch: 8.87 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05567996833409289		[learning rate: 0.0067411]
		[batch 20/20] avg loss: 0.06438727604714986		[learning rate: 0.0067308]
	Learning Rate: 0.0067308
	LOSS [training: 0.060033622190621375 | validation: 0.07666338406423556]
	TIME [epoch: 8.86 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10269697833623248		[learning rate: 0.0067205]
		[batch 20/20] avg loss: 0.07856341248673593		[learning rate: 0.0067102]
	Learning Rate: 0.00671017
	LOSS [training: 0.09063019541148418 | validation: 0.1090045050898553]
	TIME [epoch: 8.84 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06500027348657442		[learning rate: 0.0066999]
		[batch 20/20] avg loss: 0.061611475833337634		[learning rate: 0.0066896]
	Learning Rate: 0.0066896
	LOSS [training: 0.06330587465995603 | validation: 0.11045408783217296]
	TIME [epoch: 8.85 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058442632738525124		[learning rate: 0.0066793]
		[batch 20/20] avg loss: 0.09402249087153973		[learning rate: 0.0066691]
	Learning Rate: 0.00666909
	LOSS [training: 0.07623256180503243 | validation: 0.08567627032433323]
	TIME [epoch: 8.86 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055786279701075435		[learning rate: 0.0066589]
		[batch 20/20] avg loss: 0.07444430370102016		[learning rate: 0.0066486]
	Learning Rate: 0.00664865
	LOSS [training: 0.06511529170104782 | validation: 0.04996099625686523]
	TIME [epoch: 8.87 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07054830738612324		[learning rate: 0.0066384]
		[batch 20/20] avg loss: 0.07627832678496396		[learning rate: 0.0066283]
	Learning Rate: 0.00662827
	LOSS [training: 0.07341331708554358 | validation: 0.06664567382151722]
	TIME [epoch: 8.84 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09815525229852827		[learning rate: 0.0066181]
		[batch 20/20] avg loss: 0.056442601785842775		[learning rate: 0.0066079]
	Learning Rate: 0.00660795
	LOSS [training: 0.07729892704218554 | validation: 0.0523238549961514]
	TIME [epoch: 8.84 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04970247245116646		[learning rate: 0.0065978]
		[batch 20/20] avg loss: 0.06207271964057398		[learning rate: 0.0065877]
	Learning Rate: 0.00658769
	LOSS [training: 0.05588759604587022 | validation: 0.04520150888568866]
	TIME [epoch: 8.86 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05768703682424342		[learning rate: 0.0065776]
		[batch 20/20] avg loss: 0.06007898911940497		[learning rate: 0.0065675]
	Learning Rate: 0.0065675
	LOSS [training: 0.05888301297182419 | validation: 0.05120165828596301]
	TIME [epoch: 8.88 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09736902935096148		[learning rate: 0.0065574]
		[batch 20/20] avg loss: 0.08443139421589965		[learning rate: 0.0065474]
	Learning Rate: 0.00654737
	LOSS [training: 0.09090021178343054 | validation: 0.047580875929802]
	TIME [epoch: 8.86 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05525684898254526		[learning rate: 0.0065373]
		[batch 20/20] avg loss: 0.07189782825919216		[learning rate: 0.0065273]
	Learning Rate: 0.0065273
	LOSS [training: 0.0635773386208687 | validation: 0.06351952850720212]
	TIME [epoch: 8.85 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10862867126479825		[learning rate: 0.0065173]
		[batch 20/20] avg loss: 0.09139809248421005		[learning rate: 0.0065073]
	Learning Rate: 0.00650729
	LOSS [training: 0.10001338187450415 | validation: 0.034777733009302404]
	TIME [epoch: 8.84 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038216683036033594		[learning rate: 0.0064973]
		[batch 20/20] avg loss: 0.0856851835750799		[learning rate: 0.0064873]
	Learning Rate: 0.00648734
	LOSS [training: 0.061950933305556746 | validation: 0.10796075824577162]
	TIME [epoch: 8.86 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1375275233835071		[learning rate: 0.0064774]
		[batch 20/20] avg loss: 0.09605661111628572		[learning rate: 0.0064675]
	Learning Rate: 0.00646745
	LOSS [training: 0.11679206724989644 | validation: 0.10459156707394691]
	TIME [epoch: 8.86 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09328392957866885		[learning rate: 0.0064575]
		[batch 20/20] avg loss: 0.12071355454445871		[learning rate: 0.0064476]
	Learning Rate: 0.00644763
	LOSS [training: 0.10699874206156379 | validation: 0.08374760376081548]
	TIME [epoch: 8.85 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08201059597283217		[learning rate: 0.0064377]
		[batch 20/20] avg loss: 0.09459210656954307		[learning rate: 0.0064279]
	Learning Rate: 0.00642786
	LOSS [training: 0.08830135127118763 | validation: 0.08548201045270347]
	TIME [epoch: 8.85 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07209206620139004		[learning rate: 0.006418]
		[batch 20/20] avg loss: 0.04512708765313014		[learning rate: 0.0064082]
	Learning Rate: 0.00640816
	LOSS [training: 0.05860957692726008 | validation: 0.06418967451296746]
	TIME [epoch: 8.87 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06619436941579562		[learning rate: 0.0063983]
		[batch 20/20] avg loss: 0.06538693322220614		[learning rate: 0.0063885]
	Learning Rate: 0.00638852
	LOSS [training: 0.06579065131900087 | validation: 0.047823720925128335]
	TIME [epoch: 8.86 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0896522967857705		[learning rate: 0.0063787]
		[batch 20/20] avg loss: 0.04379328233814717		[learning rate: 0.0063689]
	Learning Rate: 0.00636893
	LOSS [training: 0.06672278956195883 | validation: 0.14683284100685273]
	TIME [epoch: 8.85 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11885389422165615		[learning rate: 0.0063592]
		[batch 20/20] avg loss: 0.05591517757137018		[learning rate: 0.0063494]
	Learning Rate: 0.00634941
	LOSS [training: 0.08738453589651318 | validation: 0.11247331621396522]
	TIME [epoch: 8.85 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06987877870734419		[learning rate: 0.0063397]
		[batch 20/20] avg loss: 0.09295929101401555		[learning rate: 0.0063299]
	Learning Rate: 0.00632995
	LOSS [training: 0.08141903486067986 | validation: 0.07300891150500004]
	TIME [epoch: 8.84 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08035725019997605		[learning rate: 0.0063202]
		[batch 20/20] avg loss: 0.09511648233036155		[learning rate: 0.0063105]
	Learning Rate: 0.00631054
	LOSS [training: 0.08773686626516881 | validation: 0.08173522361870073]
	TIME [epoch: 8.88 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14647766956517572		[learning rate: 0.0063009]
		[batch 20/20] avg loss: 0.07299641990194321		[learning rate: 0.0062912]
	Learning Rate: 0.0062912
	LOSS [training: 0.10973704473355943 | validation: 0.07106737541861288]
	TIME [epoch: 8.85 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08024649267168546		[learning rate: 0.0062815]
		[batch 20/20] avg loss: 0.07185017534055849		[learning rate: 0.0062719]
	Learning Rate: 0.00627191
	LOSS [training: 0.07604833400612199 | validation: 0.1303686516264727]
	TIME [epoch: 8.85 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07275524536762827		[learning rate: 0.0062623]
		[batch 20/20] avg loss: 0.07037873251292515		[learning rate: 0.0062527]
	Learning Rate: 0.00625269
	LOSS [training: 0.07156698894027672 | validation: 0.20678293324205788]
	TIME [epoch: 8.85 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0936150023804158		[learning rate: 0.0062431]
		[batch 20/20] avg loss: 0.052300240934337806		[learning rate: 0.0062335]
	Learning Rate: 0.00623352
	LOSS [training: 0.0729576216573768 | validation: 0.04870412269810231]
	TIME [epoch: 8.88 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08462106805878669		[learning rate: 0.006224]
		[batch 20/20] avg loss: 0.1842732594368802		[learning rate: 0.0062144]
	Learning Rate: 0.00621441
	LOSS [training: 0.13444716374783344 | validation: 0.10933195894047588]
	TIME [epoch: 8.86 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0885903706890736		[learning rate: 0.0062049]
		[batch 20/20] avg loss: 0.10237243019928381		[learning rate: 0.0061954]
	Learning Rate: 0.00619536
	LOSS [training: 0.09548140044417872 | validation: 0.0915331861887426]
	TIME [epoch: 8.85 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0721238554829716		[learning rate: 0.0061859]
		[batch 20/20] avg loss: 0.09570734224682619		[learning rate: 0.0061764]
	Learning Rate: 0.00617637
	LOSS [training: 0.0839155988648989 | validation: 0.06697315094694975]
	TIME [epoch: 8.85 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0680578884422676		[learning rate: 0.0061669]
		[batch 20/20] avg loss: 0.1534368322147154		[learning rate: 0.0061574]
	Learning Rate: 0.00615744
	LOSS [training: 0.11074736032849149 | validation: 0.1160631668956765]
	TIME [epoch: 8.86 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20483223687650906		[learning rate: 0.006148]
		[batch 20/20] avg loss: 0.1690148244604719		[learning rate: 0.0061386]
	Learning Rate: 0.00613856
	LOSS [training: 0.1869235306684905 | validation: 0.06469841019514962]
	TIME [epoch: 8.84 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06674370841291229		[learning rate: 0.0061291]
		[batch 20/20] avg loss: 0.041702957344178136		[learning rate: 0.0061197]
	Learning Rate: 0.00611974
	LOSS [training: 0.05422333287854521 | validation: 0.05263596267268843]
	TIME [epoch: 8.84 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05198371555294969		[learning rate: 0.0061104]
		[batch 20/20] avg loss: 0.08891074356450412		[learning rate: 0.006101]
	Learning Rate: 0.00610099
	LOSS [training: 0.07044722955872688 | validation: 0.11625603088339362]
	TIME [epoch: 8.84 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05202653947366219		[learning rate: 0.0060916]
		[batch 20/20] avg loss: 0.07561778935112465		[learning rate: 0.0060823]
	Learning Rate: 0.00608228
	LOSS [training: 0.06382216441239343 | validation: 0.0674913209528012]
	TIME [epoch: 8.9 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056884212320161665		[learning rate: 0.006073]
		[batch 20/20] avg loss: 0.06521211336268096		[learning rate: 0.0060636]
	Learning Rate: 0.00606364
	LOSS [training: 0.061048162841421304 | validation: 0.047071509162049746]
	TIME [epoch: 8.87 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06333315242368927		[learning rate: 0.0060543]
		[batch 20/20] avg loss: 0.050773448192497087		[learning rate: 0.0060451]
	Learning Rate: 0.00604505
	LOSS [training: 0.05705330030809317 | validation: 0.03987021593341188]
	TIME [epoch: 8.85 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04625070289548708		[learning rate: 0.0060358]
		[batch 20/20] avg loss: 0.04526351393996904		[learning rate: 0.0060265]
	Learning Rate: 0.00602652
	LOSS [training: 0.04575710841772805 | validation: 0.05147620159004863]
	TIME [epoch: 8.85 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05326648054394156		[learning rate: 0.0060173]
		[batch 20/20] avg loss: 0.061199709892257294		[learning rate: 0.006008]
	Learning Rate: 0.00600805
	LOSS [training: 0.057233095218099425 | validation: 0.14870218058809123]
	TIME [epoch: 8.85 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0815198723157619		[learning rate: 0.0059988]
		[batch 20/20] avg loss: 0.07644592034050704		[learning rate: 0.0059896]
	Learning Rate: 0.00598963
	LOSS [training: 0.07898289632813446 | validation: 0.028295496194871747]
	TIME [epoch: 8.88 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_667.pth
	Model improved!!!
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06003206346177792		[learning rate: 0.0059804]
		[batch 20/20] avg loss: 0.06609797593264495		[learning rate: 0.0059713]
	Learning Rate: 0.00597127
	LOSS [training: 0.06306501969721143 | validation: 0.07226131267406956]
	TIME [epoch: 8.85 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08440003799314433		[learning rate: 0.0059621]
		[batch 20/20] avg loss: 0.06943980054892349		[learning rate: 0.005953]
	Learning Rate: 0.00595297
	LOSS [training: 0.07691991927103392 | validation: 0.07126587841032304]
	TIME [epoch: 8.84 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08481767283744948		[learning rate: 0.0059438]
		[batch 20/20] avg loss: 0.05982720966346984		[learning rate: 0.0059347]
	Learning Rate: 0.00593472
	LOSS [training: 0.07232244125045965 | validation: 0.05551380788837619]
	TIME [epoch: 8.83 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03496929539460452		[learning rate: 0.0059256]
		[batch 20/20] avg loss: 0.05866882343201567		[learning rate: 0.0059165]
	Learning Rate: 0.00591652
	LOSS [training: 0.0468190594133101 | validation: 0.061762028589115864]
	TIME [epoch: 8.87 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07509410568672867		[learning rate: 0.0059074]
		[batch 20/20] avg loss: 0.12780793729723924		[learning rate: 0.0058984]
	Learning Rate: 0.00589839
	LOSS [training: 0.10145102149198396 | validation: 0.05557509712668826]
	TIME [epoch: 8.85 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050502591276607646		[learning rate: 0.0058893]
		[batch 20/20] avg loss: 0.05942386310181493		[learning rate: 0.0058803]
	Learning Rate: 0.00588031
	LOSS [training: 0.054963227189211296 | validation: 0.06581232294887805]
	TIME [epoch: 8.84 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05208452360356247		[learning rate: 0.0058713]
		[batch 20/20] avg loss: 0.06464973074914704		[learning rate: 0.0058623]
	Learning Rate: 0.00586228
	LOSS [training: 0.05836712717635476 | validation: 0.05900325830612915]
	TIME [epoch: 8.84 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09017126344925347		[learning rate: 0.0058533]
		[batch 20/20] avg loss: 0.08683252577707556		[learning rate: 0.0058443]
	Learning Rate: 0.00584431
	LOSS [training: 0.08850189461316452 | validation: 0.10537457284679282]
	TIME [epoch: 8.86 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08298164561305392		[learning rate: 0.0058353]
		[batch 20/20] avg loss: 0.0648262215383617		[learning rate: 0.0058264]
	Learning Rate: 0.0058264
	LOSS [training: 0.07390393357570782 | validation: 0.058810509568139935]
	TIME [epoch: 8.85 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08360610342289584		[learning rate: 0.0058175]
		[batch 20/20] avg loss: 0.05268464236476911		[learning rate: 0.0058085]
	Learning Rate: 0.00580854
	LOSS [training: 0.06814537289383249 | validation: 0.05884722986888482]
	TIME [epoch: 8.83 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05215657033592021		[learning rate: 0.0057996]
		[batch 20/20] avg loss: 0.07858238390997854		[learning rate: 0.0057907]
	Learning Rate: 0.00579073
	LOSS [training: 0.06536947712294938 | validation: 0.14200238067462573]
	TIME [epoch: 8.85 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.104104775271298		[learning rate: 0.0057818]
		[batch 20/20] avg loss: 0.08249105427089723		[learning rate: 0.005773]
	Learning Rate: 0.00577298
	LOSS [training: 0.09329791477109761 | validation: 0.052528808315422275]
	TIME [epoch: 8.84 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09068581164865532		[learning rate: 0.0057641]
		[batch 20/20] avg loss: 0.07405339853748097		[learning rate: 0.0057553]
	Learning Rate: 0.00575528
	LOSS [training: 0.08236960509306815 | validation: 0.08478674707301603]
	TIME [epoch: 8.86 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09965861704484266		[learning rate: 0.0057465]
		[batch 20/20] avg loss: 0.08317085128628963		[learning rate: 0.0057376]
	Learning Rate: 0.00573764
	LOSS [training: 0.09141473416556616 | validation: 0.10138240043185642]
	TIME [epoch: 8.84 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06841606688643709		[learning rate: 0.0057288]
		[batch 20/20] avg loss: 0.05554297699726314		[learning rate: 0.0057201]
	Learning Rate: 0.00572005
	LOSS [training: 0.06197952194185012 | validation: 0.10421617672768972]
	TIME [epoch: 8.84 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06400112350999405		[learning rate: 0.0057113]
		[batch 20/20] avg loss: 0.06665126677182258		[learning rate: 0.0057025]
	Learning Rate: 0.00570252
	LOSS [training: 0.0653261951409083 | validation: 0.03793031500521797]
	TIME [epoch: 8.83 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08846779371144943		[learning rate: 0.0056938]
		[batch 20/20] avg loss: 0.06195205717082114		[learning rate: 0.005685]
	Learning Rate: 0.00568504
	LOSS [training: 0.07520992544113528 | validation: 0.0915684532143391]
	TIME [epoch: 8.86 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05761552929264202		[learning rate: 0.0056763]
		[batch 20/20] avg loss: 0.07037519910275833		[learning rate: 0.0056676]
	Learning Rate: 0.00566761
	LOSS [training: 0.06399536419770016 | validation: 0.1459873336087774]
	TIME [epoch: 8.85 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11036048855562944		[learning rate: 0.0056589]
		[batch 20/20] avg loss: 0.04526814762821558		[learning rate: 0.0056502]
	Learning Rate: 0.00565024
	LOSS [training: 0.0778143180919225 | validation: 0.06672620827721477]
	TIME [epoch: 8.84 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07140402535090055		[learning rate: 0.0056416]
		[batch 20/20] avg loss: 0.07692590760400236		[learning rate: 0.0056329]
	Learning Rate: 0.00563292
	LOSS [training: 0.07416496647745147 | validation: 0.0763932020903764]
	TIME [epoch: 8.84 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043107044668001186		[learning rate: 0.0056243]
		[batch 20/20] avg loss: 0.048609970828769666		[learning rate: 0.0056156]
	Learning Rate: 0.00561565
	LOSS [training: 0.04585850774838542 | validation: 0.06608333425664865]
	TIME [epoch: 8.86 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04771435836943293		[learning rate: 0.005607]
		[batch 20/20] avg loss: 0.05416950930385629		[learning rate: 0.0055984]
	Learning Rate: 0.00559843
	LOSS [training: 0.050941933836644616 | validation: 0.034054371381425946]
	TIME [epoch: 8.85 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057324334440143496		[learning rate: 0.0055898]
		[batch 20/20] avg loss: 0.07241020804521356		[learning rate: 0.0055813]
	Learning Rate: 0.00558127
	LOSS [training: 0.06486727124267852 | validation: 0.05424918182395046]
	TIME [epoch: 8.84 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049808055525946915		[learning rate: 0.0055727]
		[batch 20/20] avg loss: 0.05276690783073464		[learning rate: 0.0055642]
	Learning Rate: 0.00556416
	LOSS [training: 0.051287481678340775 | validation: 0.04920091266056684]
	TIME [epoch: 8.84 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05741445561103829		[learning rate: 0.0055556]
		[batch 20/20] avg loss: 0.06622296603097827		[learning rate: 0.0055471]
	Learning Rate: 0.00554711
	LOSS [training: 0.061818710821008295 | validation: 0.05215234750217677]
	TIME [epoch: 8.85 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0692786517860979		[learning rate: 0.0055386]
		[batch 20/20] avg loss: 0.06910858730663763		[learning rate: 0.0055301]
	Learning Rate: 0.0055301
	LOSS [training: 0.06919361954636778 | validation: 0.09978497181429966]
	TIME [epoch: 8.85 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05330416949114293		[learning rate: 0.0055216]
		[batch 20/20] avg loss: 0.07025078368926778		[learning rate: 0.0055132]
	Learning Rate: 0.00551315
	LOSS [training: 0.06177747659020534 | validation: 0.06569067612223595]
	TIME [epoch: 8.84 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06511122723928801		[learning rate: 0.0055047]
		[batch 20/20] avg loss: 0.08780479349783535		[learning rate: 0.0054963]
	Learning Rate: 0.00549625
	LOSS [training: 0.07645801036856167 | validation: 0.10785703308122106]
	TIME [epoch: 8.84 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08221290064053502		[learning rate: 0.0054878]
		[batch 20/20] avg loss: 0.0771901285354478		[learning rate: 0.0054794]
	Learning Rate: 0.0054794
	LOSS [training: 0.07970151458799142 | validation: 0.03934727572667914]
	TIME [epoch: 8.84 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05306493099520063		[learning rate: 0.005471]
		[batch 20/20] avg loss: 0.07477520134072108		[learning rate: 0.0054626]
	Learning Rate: 0.00546261
	LOSS [training: 0.06392006616796085 | validation: 0.06454736847373489]
	TIME [epoch: 8.85 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05922293907856511		[learning rate: 0.0054542]
		[batch 20/20] avg loss: 0.07283636383128651		[learning rate: 0.0054459]
	Learning Rate: 0.00544586
	LOSS [training: 0.06602965145492581 | validation: 0.08175693750772144]
	TIME [epoch: 8.84 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04555283934710364		[learning rate: 0.0054375]
		[batch 20/20] avg loss: 0.07624944855554093		[learning rate: 0.0054292]
	Learning Rate: 0.00542917
	LOSS [training: 0.06090114395132229 | validation: 0.08676899152998742]
	TIME [epoch: 8.84 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059997564641342274		[learning rate: 0.0054208]
		[batch 20/20] avg loss: 0.08653777364424878		[learning rate: 0.0054125]
	Learning Rate: 0.00541253
	LOSS [training: 0.07326766914279552 | validation: 0.13313585898251767]
	TIME [epoch: 8.84 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06924419447287795		[learning rate: 0.0054042]
		[batch 20/20] avg loss: 0.05824812393441721		[learning rate: 0.0053959]
	Learning Rate: 0.00539593
	LOSS [training: 0.0637461592036476 | validation: 0.039364921449766]
	TIME [epoch: 8.86 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054236952142457905		[learning rate: 0.0053877]
		[batch 20/20] avg loss: 0.06005343153898422		[learning rate: 0.0053794]
	Learning Rate: 0.00537939
	LOSS [training: 0.05714519184072107 | validation: 0.0629614442447434]
	TIME [epoch: 8.84 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05483774708491003		[learning rate: 0.0053711]
		[batch 20/20] avg loss: 0.04789091795558375		[learning rate: 0.0053629]
	Learning Rate: 0.0053629
	LOSS [training: 0.05136433252024688 | validation: 0.08735141571539766]
	TIME [epoch: 8.83 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06984560335117244		[learning rate: 0.0053547]
		[batch 20/20] avg loss: 0.04309066532116002		[learning rate: 0.0053465]
	Learning Rate: 0.00534646
	LOSS [training: 0.05646813433616623 | validation: 0.061144464139056436]
	TIME [epoch: 8.83 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054655754324134574		[learning rate: 0.0053383]
		[batch 20/20] avg loss: 0.09716318669255833		[learning rate: 0.0053301]
	Learning Rate: 0.00533008
	LOSS [training: 0.07590947050834645 | validation: 0.05235211896399098]
	TIME [epoch: 8.86 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046967088203116535		[learning rate: 0.0053219]
		[batch 20/20] avg loss: 0.054383872601881475		[learning rate: 0.0053137]
	Learning Rate: 0.00531374
	LOSS [training: 0.05067548040249901 | validation: 0.06928531575828101]
	TIME [epoch: 8.85 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05597524588991942		[learning rate: 0.0053056]
		[batch 20/20] avg loss: 0.033281960032389805		[learning rate: 0.0052974]
	Learning Rate: 0.00529745
	LOSS [training: 0.04462860296115462 | validation: 0.024315868749232812]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_707.pth
	Model improved!!!
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047897299583940914		[learning rate: 0.0052893]
		[batch 20/20] avg loss: 0.03917704299047128		[learning rate: 0.0052812]
	Learning Rate: 0.00528121
	LOSS [training: 0.043537171287206085 | validation: 0.04602807493853718]
	TIME [epoch: 8.87 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06751843238317926		[learning rate: 0.0052731]
		[batch 20/20] avg loss: 0.06997883118454049		[learning rate: 0.005265]
	Learning Rate: 0.00526502
	LOSS [training: 0.06874863178385988 | validation: 0.0817395967240301]
	TIME [epoch: 8.87 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12284688789664341		[learning rate: 0.0052569]
		[batch 20/20] avg loss: 0.09337284172680894		[learning rate: 0.0052489]
	Learning Rate: 0.00524888
	LOSS [training: 0.1081098648117262 | validation: 0.10489980635805708]
	TIME [epoch: 8.95 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08582247383860868		[learning rate: 0.0052408]
		[batch 20/20] avg loss: 0.08691220497402638		[learning rate: 0.0052328]
	Learning Rate: 0.00523279
	LOSS [training: 0.08636733940631755 | validation: 0.13535362403263212]
	TIME [epoch: 8.86 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10512025189203564		[learning rate: 0.0052248]
		[batch 20/20] avg loss: 0.07789434744954345		[learning rate: 0.0052167]
	Learning Rate: 0.00521675
	LOSS [training: 0.09150729967078955 | validation: 0.06359875590470727]
	TIME [epoch: 8.87 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09584129370816516		[learning rate: 0.0052087]
		[batch 20/20] avg loss: 0.08895478842629648		[learning rate: 0.0052008]
	Learning Rate: 0.00520076
	LOSS [training: 0.09239804106723082 | validation: 0.13575364783756566]
	TIME [epoch: 8.87 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08820899016113816		[learning rate: 0.0051928]
		[batch 20/20] avg loss: 0.06560572030084395		[learning rate: 0.0051848]
	Learning Rate: 0.00518482
	LOSS [training: 0.07690735523099106 | validation: 0.050324218777978616]
	TIME [epoch: 8.9 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05089250982891559		[learning rate: 0.0051769]
		[batch 20/20] avg loss: 0.05700541442544432		[learning rate: 0.0051689]
	Learning Rate: 0.00516892
	LOSS [training: 0.05394896212717996 | validation: 0.08267148822571321]
	TIME [epoch: 8.87 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05479163335859279		[learning rate: 0.005161]
		[batch 20/20] avg loss: 0.06843744945495309		[learning rate: 0.0051531]
	Learning Rate: 0.00515308
	LOSS [training: 0.06161454140677295 | validation: 0.06824382723661249]
	TIME [epoch: 8.87 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05005932318477746		[learning rate: 0.0051452]
		[batch 20/20] avg loss: 0.06013051497392606		[learning rate: 0.0051373]
	Learning Rate: 0.00513728
	LOSS [training: 0.055094919079351753 | validation: 0.08765235650799771]
	TIME [epoch: 8.87 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11286956032326978		[learning rate: 0.0051294]
		[batch 20/20] avg loss: 0.09083724215830621		[learning rate: 0.0051215]
	Learning Rate: 0.00512153
	LOSS [training: 0.10185340124078797 | validation: 0.1038196779481215]
	TIME [epoch: 8.89 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07790274980807327		[learning rate: 0.0051137]
		[batch 20/20] avg loss: 0.07044637486571358		[learning rate: 0.0051058]
	Learning Rate: 0.00510583
	LOSS [training: 0.07417456233689343 | validation: 0.14927051194586066]
	TIME [epoch: 8.88 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08607898098251046		[learning rate: 0.005098]
		[batch 20/20] avg loss: 0.05551735391489719		[learning rate: 0.0050902]
	Learning Rate: 0.00509018
	LOSS [training: 0.07079816744870383 | validation: 0.17042198824215088]
	TIME [epoch: 8.87 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11017383367204399		[learning rate: 0.0050824]
		[batch 20/20] avg loss: 0.07100818500424347		[learning rate: 0.0050746]
	Learning Rate: 0.00507458
	LOSS [training: 0.09059100933814371 | validation: 0.07303765875223626]
	TIME [epoch: 8.87 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08148314897784201		[learning rate: 0.0050668]
		[batch 20/20] avg loss: 0.13664136486259393		[learning rate: 0.005059]
	Learning Rate: 0.00505902
	LOSS [training: 0.10906225692021795 | validation: 0.11316833118035427]
	TIME [epoch: 8.89 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1273256685592917		[learning rate: 0.0050513]
		[batch 20/20] avg loss: 0.1001852839210868		[learning rate: 0.0050435]
	Learning Rate: 0.00504352
	LOSS [training: 0.11375547624018922 | validation: 0.116964270419008]
	TIME [epoch: 8.88 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08822579770807532		[learning rate: 0.0050358]
		[batch 20/20] avg loss: 0.07165586356664395		[learning rate: 0.0050281]
	Learning Rate: 0.00502805
	LOSS [training: 0.07994083063735964 | validation: 0.17928722176311962]
	TIME [epoch: 8.87 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09841653923563279		[learning rate: 0.0050203]
		[batch 20/20] avg loss: 0.05471072631856111		[learning rate: 0.0050126]
	Learning Rate: 0.00501264
	LOSS [training: 0.07656363277709695 | validation: 0.03753160789820413]
	TIME [epoch: 8.87 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03870257711558119		[learning rate: 0.005005]
		[batch 20/20] avg loss: 0.030534347457548155		[learning rate: 0.0049973]
	Learning Rate: 0.00499728
	LOSS [training: 0.03461846228656468 | validation: 0.033725749828742047]
	TIME [epoch: 8.87 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04206922398731201		[learning rate: 0.0049896]
		[batch 20/20] avg loss: 0.03906407328813067		[learning rate: 0.004982]
	Learning Rate: 0.00498196
	LOSS [training: 0.040566648637721345 | validation: 0.12344735935817341]
	TIME [epoch: 8.89 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04261209961820629		[learning rate: 0.0049743]
		[batch 20/20] avg loss: 0.02980861088929951		[learning rate: 0.0049667]
	Learning Rate: 0.00496669
	LOSS [training: 0.03621035525375289 | validation: 0.0335861983863471]
	TIME [epoch: 8.87 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05612723272392791		[learning rate: 0.0049591]
		[batch 20/20] avg loss: 0.061420070851118426		[learning rate: 0.0049515]
	Learning Rate: 0.00495146
	LOSS [training: 0.058773651787523175 | validation: 0.046693968612704265]
	TIME [epoch: 8.87 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04830505403422331		[learning rate: 0.0049439]
		[batch 20/20] avg loss: 0.035872360781889115		[learning rate: 0.0049363]
	Learning Rate: 0.00493628
	LOSS [training: 0.04208870740805622 | validation: 0.053256835407984836]
	TIME [epoch: 8.86 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03854235895284111		[learning rate: 0.0049287]
		[batch 20/20] avg loss: 0.03620869743587446		[learning rate: 0.0049211]
	Learning Rate: 0.00492115
	LOSS [training: 0.037375528194357785 | validation: 0.06048451498738558]
	TIME [epoch: 8.89 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0392663826470543		[learning rate: 0.0049136]
		[batch 20/20] avg loss: 0.04282739292696101		[learning rate: 0.0049061]
	Learning Rate: 0.00490607
	LOSS [training: 0.041046887787007644 | validation: 0.021636590413092238]
	TIME [epoch: 8.87 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_732.pth
	Model improved!!!
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042099820882154616		[learning rate: 0.0048985]
		[batch 20/20] avg loss: 0.046453681734194174		[learning rate: 0.004891]
	Learning Rate: 0.00489103
	LOSS [training: 0.04427675130817439 | validation: 0.04086460851658341]
	TIME [epoch: 8.87 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06120159177848592		[learning rate: 0.0048835]
		[batch 20/20] avg loss: 0.08300073616711179		[learning rate: 0.004876]
	Learning Rate: 0.00487603
	LOSS [training: 0.07210116397279887 | validation: 0.0956640394832545]
	TIME [epoch: 8.86 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10335632294194867		[learning rate: 0.0048686]
		[batch 20/20] avg loss: 0.07328619354064767		[learning rate: 0.0048611]
	Learning Rate: 0.00486109
	LOSS [training: 0.08832125824129819 | validation: 0.06462822247979189]
	TIME [epoch: 8.88 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04763627431550033		[learning rate: 0.0048536]
		[batch 20/20] avg loss: 0.05255985731021382		[learning rate: 0.0048462]
	Learning Rate: 0.00484618
	LOSS [training: 0.05009806581285707 | validation: 0.036447546308102555]
	TIME [epoch: 8.87 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040037659967253884		[learning rate: 0.0048388]
		[batch 20/20] avg loss: 0.03794582788954615		[learning rate: 0.0048313]
	Learning Rate: 0.00483133
	LOSS [training: 0.038991743928400026 | validation: 0.04679734648556921]
	TIME [epoch: 8.87 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05338097069755075		[learning rate: 0.0048239]
		[batch 20/20] avg loss: 0.055248037651627294		[learning rate: 0.0048165]
	Learning Rate: 0.00481652
	LOSS [training: 0.054314504174589016 | validation: 0.07034301146264013]
	TIME [epoch: 8.86 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05333144821610134		[learning rate: 0.0048091]
		[batch 20/20] avg loss: 0.03756164811172399		[learning rate: 0.0048018]
	Learning Rate: 0.00480176
	LOSS [training: 0.04544654816391266 | validation: 0.05403805354212467]
	TIME [epoch: 8.88 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05912237109139813		[learning rate: 0.0047944]
		[batch 20/20] avg loss: 0.04710688297373647		[learning rate: 0.004787]
	Learning Rate: 0.00478704
	LOSS [training: 0.05311462703256731 | validation: 0.06855922482728671]
	TIME [epoch: 8.88 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0325633056853468		[learning rate: 0.0047797]
		[batch 20/20] avg loss: 0.060409565443821714		[learning rate: 0.0047724]
	Learning Rate: 0.00477236
	LOSS [training: 0.046486435564584254 | validation: 0.043199575901503334]
	TIME [epoch: 8.86 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05197059804810428		[learning rate: 0.004765]
		[batch 20/20] avg loss: 0.04627895598629907		[learning rate: 0.0047577]
	Learning Rate: 0.00475773
	LOSS [training: 0.04912477701720167 | validation: 0.03416370220360835]
	TIME [epoch: 8.87 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035647866491185315		[learning rate: 0.0047504]
		[batch 20/20] avg loss: 0.03255198199306801		[learning rate: 0.0047431]
	Learning Rate: 0.00474315
	LOSS [training: 0.03409992424212667 | validation: 0.03372826084961483]
	TIME [epoch: 8.89 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054350925014012244		[learning rate: 0.0047359]
		[batch 20/20] avg loss: 0.07163796939352876		[learning rate: 0.0047286]
	Learning Rate: 0.00472861
	LOSS [training: 0.06299444720377051 | validation: 0.09778234587217105]
	TIME [epoch: 8.89 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07518324677075196		[learning rate: 0.0047214]
		[batch 20/20] avg loss: 0.06566198783816987		[learning rate: 0.0047141]
	Learning Rate: 0.00471411
	LOSS [training: 0.07042261730446091 | validation: 0.06397456989893556]
	TIME [epoch: 8.87 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0774509524075917		[learning rate: 0.0047069]
		[batch 20/20] avg loss: 0.05647188238720258		[learning rate: 0.0046997]
	Learning Rate: 0.00469966
	LOSS [training: 0.06696141739739711 | validation: 0.10166392169793373]
	TIME [epoch: 8.87 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047482106764429835		[learning rate: 0.0046925]
		[batch 20/20] avg loss: 0.042355393041371744		[learning rate: 0.0046853]
	Learning Rate: 0.00468526
	LOSS [training: 0.0449187499029008 | validation: 0.03515930913194608]
	TIME [epoch: 8.87 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033770582962814294		[learning rate: 0.0046781]
		[batch 20/20] avg loss: 0.035989234686191814		[learning rate: 0.0046709]
	Learning Rate: 0.00467089
	LOSS [training: 0.03487990882450305 | validation: 0.03304152146597062]
	TIME [epoch: 8.89 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04560801142622264		[learning rate: 0.0046637]
		[batch 20/20] avg loss: 0.0445180376686216		[learning rate: 0.0046566]
	Learning Rate: 0.00465658
	LOSS [training: 0.04506302454742212 | validation: 0.21679649502585976]
	TIME [epoch: 8.87 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08228077173992386		[learning rate: 0.0046494]
		[batch 20/20] avg loss: 0.04088838143671002		[learning rate: 0.0046423]
	Learning Rate: 0.0046423
	LOSS [training: 0.06158457658831694 | validation: 0.035795279032963714]
	TIME [epoch: 8.86 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06986056905181817		[learning rate: 0.0046352]
		[batch 20/20] avg loss: 0.06310251282010293		[learning rate: 0.0046281]
	Learning Rate: 0.00462807
	LOSS [training: 0.06648154093596055 | validation: 0.042850324919260524]
	TIME [epoch: 8.86 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05837281896030577		[learning rate: 0.004621]
		[batch 20/20] avg loss: 0.06260814994715659		[learning rate: 0.0046139]
	Learning Rate: 0.00461388
	LOSS [training: 0.06049048445373116 | validation: 0.04536930478725617]
	TIME [epoch: 8.88 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048155388437488914		[learning rate: 0.0046068]
		[batch 20/20] avg loss: 0.048051112744421906		[learning rate: 0.0045997]
	Learning Rate: 0.00459974
	LOSS [training: 0.04810325059095541 | validation: 0.049881625179511155]
	TIME [epoch: 8.87 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041578066346999266		[learning rate: 0.0045927]
		[batch 20/20] avg loss: 0.05337104080379612		[learning rate: 0.0045856]
	Learning Rate: 0.00458564
	LOSS [training: 0.0474745535753977 | validation: 0.08094410856506946]
	TIME [epoch: 8.87 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06060303926424763		[learning rate: 0.0045786]
		[batch 20/20] avg loss: 0.10767265718154587		[learning rate: 0.0045716]
	Learning Rate: 0.00457158
	LOSS [training: 0.08413784822289674 | validation: 0.09629774733525605]
	TIME [epoch: 8.86 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09559893453599785		[learning rate: 0.0045646]
		[batch 20/20] avg loss: 0.08807902347358001		[learning rate: 0.0045576]
	Learning Rate: 0.00455757
	LOSS [training: 0.09183897900478895 | validation: 0.09095923013062568]
	TIME [epoch: 8.87 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0780709385955142		[learning rate: 0.0045506]
		[batch 20/20] avg loss: 0.05835960899766264		[learning rate: 0.0045436]
	Learning Rate: 0.0045436
	LOSS [training: 0.06821527379658841 | validation: 0.09098151654860473]
	TIME [epoch: 8.89 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06631431854022042		[learning rate: 0.0045366]
		[batch 20/20] avg loss: 0.04573984831406461		[learning rate: 0.0045297]
	Learning Rate: 0.00452967
	LOSS [training: 0.05602708342714251 | validation: 0.06853386328727999]
	TIME [epoch: 8.86 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09115628519899616		[learning rate: 0.0045227]
		[batch 20/20] avg loss: 0.06579941782380133		[learning rate: 0.0045158]
	Learning Rate: 0.00451579
	LOSS [training: 0.07847785151139874 | validation: 0.06571963691728547]
	TIME [epoch: 8.86 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07572294995105339		[learning rate: 0.0045089]
		[batch 20/20] avg loss: 0.05912390680733198		[learning rate: 0.0045019]
	Learning Rate: 0.00450194
	LOSS [training: 0.0674234283791927 | validation: 0.04699212685419652]
	TIME [epoch: 8.86 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13034466482383994		[learning rate: 0.004495]
		[batch 20/20] avg loss: 0.1314207409571157		[learning rate: 0.0044881]
	Learning Rate: 0.00448814
	LOSS [training: 0.13088270289047782 | validation: 0.07525336834784782]
	TIME [epoch: 8.88 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09281253825371341		[learning rate: 0.0044813]
		[batch 20/20] avg loss: 0.09520586378909186		[learning rate: 0.0044744]
	Learning Rate: 0.00447438
	LOSS [training: 0.09400920102140262 | validation: 0.11113282430080702]
	TIME [epoch: 8.87 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0883446479242134		[learning rate: 0.0044675]
		[batch 20/20] avg loss: 0.09964119578238495		[learning rate: 0.0044607]
	Learning Rate: 0.00446067
	LOSS [training: 0.09399292185329917 | validation: 0.06935124650882706]
	TIME [epoch: 8.86 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06239368399437721		[learning rate: 0.0044538]
		[batch 20/20] avg loss: 0.04538864986114587		[learning rate: 0.004447]
	Learning Rate: 0.00444699
	LOSS [training: 0.053891166927761545 | validation: 0.039203669179529166]
	TIME [epoch: 8.86 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046600174638908545		[learning rate: 0.0044402]
		[batch 20/20] avg loss: 0.03591292072045814		[learning rate: 0.0044334]
	Learning Rate: 0.00443336
	LOSS [training: 0.04125654767968334 | validation: 0.13909154891521144]
	TIME [epoch: 8.88 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07231344883564914		[learning rate: 0.0044266]
		[batch 20/20] avg loss: 0.06992454519434348		[learning rate: 0.0044198]
	Learning Rate: 0.00441977
	LOSS [training: 0.07111899701499633 | validation: 0.0625891284831134]
	TIME [epoch: 8.87 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053518994700101484		[learning rate: 0.004413]
		[batch 20/20] avg loss: 0.06848870353618915		[learning rate: 0.0044062]
	Learning Rate: 0.00440622
	LOSS [training: 0.06100384911814531 | validation: 0.060266351094807546]
	TIME [epoch: 8.86 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05029778211073088		[learning rate: 0.0043995]
		[batch 20/20] avg loss: 0.10179114151030669		[learning rate: 0.0043927]
	Learning Rate: 0.00439272
	LOSS [training: 0.07604446181051878 | validation: 0.3097483901004037]
	TIME [epoch: 8.86 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17571533834085884		[learning rate: 0.004386]
		[batch 20/20] avg loss: 0.08412675797087932		[learning rate: 0.0043793]
	Learning Rate: 0.00437925
	LOSS [training: 0.12992104815586908 | validation: 0.04414071957393796]
	TIME [epoch: 8.87 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038369321155222055		[learning rate: 0.0043725]
		[batch 20/20] avg loss: 0.024303911452559446		[learning rate: 0.0043658]
	Learning Rate: 0.00436583
	LOSS [training: 0.03133661630389075 | validation: 0.03673213183641229]
	TIME [epoch: 8.88 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029530930973409232		[learning rate: 0.0043591]
		[batch 20/20] avg loss: 0.0809283177854517		[learning rate: 0.0043524]
	Learning Rate: 0.00435245
	LOSS [training: 0.05522962437943045 | validation: 0.059587943552051506]
	TIME [epoch: 8.86 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06428559285021876		[learning rate: 0.0043458]
		[batch 20/20] avg loss: 0.03909890529481393		[learning rate: 0.0043391]
	Learning Rate: 0.0043391
	LOSS [training: 0.05169224907251635 | validation: 0.023091041332700114]
	TIME [epoch: 8.86 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054732515643378346		[learning rate: 0.0043324]
		[batch 20/20] avg loss: 0.04924383674620434		[learning rate: 0.0043258]
	Learning Rate: 0.0043258
	LOSS [training: 0.05198817619479135 | validation: 0.057108937450892475]
	TIME [epoch: 8.86 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05263158663740195		[learning rate: 0.0043192]
		[batch 20/20] avg loss: 0.055289891801191496		[learning rate: 0.0043125]
	Learning Rate: 0.00431254
	LOSS [training: 0.05396073921929671 | validation: 0.11094234691168986]
	TIME [epoch: 8.88 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05993215042655292		[learning rate: 0.0043059]
		[batch 20/20] avg loss: 0.0309366933571699		[learning rate: 0.0042993]
	Learning Rate: 0.00429932
	LOSS [training: 0.04543442189186141 | validation: 0.03035116733584541]
	TIME [epoch: 8.87 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03488415448131706		[learning rate: 0.0042927]
		[batch 20/20] avg loss: 0.02822690787859077		[learning rate: 0.0042861]
	Learning Rate: 0.00428614
	LOSS [training: 0.031555531179953925 | validation: 0.05332757805782778]
	TIME [epoch: 8.86 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03319705287696426		[learning rate: 0.0042796]
		[batch 20/20] avg loss: 0.052614792205155116		[learning rate: 0.004273]
	Learning Rate: 0.004273
	LOSS [training: 0.042905922541059696 | validation: 0.022107129201680325]
	TIME [epoch: 8.93 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028089988719673505		[learning rate: 0.0042664]
		[batch 20/20] avg loss: 0.043057676999785514		[learning rate: 0.0042599]
	Learning Rate: 0.00425991
	LOSS [training: 0.035573832859729516 | validation: 0.05129723444122794]
	TIME [epoch: 8.89 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04593515663474094		[learning rate: 0.0042534]
		[batch 20/20] avg loss: 0.031108201921647594		[learning rate: 0.0042468]
	Learning Rate: 0.00424685
	LOSS [training: 0.03852167927819426 | validation: 0.02562658883905025]
	TIME [epoch: 8.86 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05144751031045939		[learning rate: 0.0042403]
		[batch 20/20] avg loss: 0.03415297500494509		[learning rate: 0.0042338]
	Learning Rate: 0.00423383
	LOSS [training: 0.04280024265770223 | validation: 0.04562003482289949]
	TIME [epoch: 8.86 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03355217057483445		[learning rate: 0.0042273]
		[batch 20/20] avg loss: 0.05079266205915667		[learning rate: 0.0042209]
	Learning Rate: 0.00422085
	LOSS [training: 0.04217241631699555 | validation: 0.03561263173796486]
	TIME [epoch: 8.86 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037244000542828296		[learning rate: 0.0042144]
		[batch 20/20] avg loss: 0.04020770588733805		[learning rate: 0.0042079]
	Learning Rate: 0.00420791
	LOSS [training: 0.038725853215083164 | validation: 0.05484103842405808]
	TIME [epoch: 8.87 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03666308199798994		[learning rate: 0.0042015]
		[batch 20/20] avg loss: 0.0803081849674925		[learning rate: 0.004195]
	Learning Rate: 0.00419501
	LOSS [training: 0.0584856334827412 | validation: 0.023508706441105524]
	TIME [epoch: 8.86 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053796009399388486		[learning rate: 0.0041886]
		[batch 20/20] avg loss: 0.05048543076577233		[learning rate: 0.0041822]
	Learning Rate: 0.00418215
	LOSS [training: 0.0521407200825804 | validation: 0.041054594195035904]
	TIME [epoch: 8.86 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05369518357556933		[learning rate: 0.0041757]
		[batch 20/20] avg loss: 0.049686343205512726		[learning rate: 0.0041693]
	Learning Rate: 0.00416933
	LOSS [training: 0.05169076339054103 | validation: 0.04012414355870157]
	TIME [epoch: 8.86 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037074562005779146		[learning rate: 0.0041629]
		[batch 20/20] avg loss: 0.03484692921575135		[learning rate: 0.0041566]
	Learning Rate: 0.00415655
	LOSS [training: 0.035960745610765246 | validation: 0.03193294197458197]
	TIME [epoch: 8.86 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056751596295007437		[learning rate: 0.0041502]
		[batch 20/20] avg loss: 0.03316518412316726		[learning rate: 0.0041438]
	Learning Rate: 0.00414381
	LOSS [training: 0.04495839020908735 | validation: 0.026295760838753734]
	TIME [epoch: 8.89 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04138818300966076		[learning rate: 0.0041375]
		[batch 20/20] avg loss: 0.04712864998522795		[learning rate: 0.0041311]
	Learning Rate: 0.00413111
	LOSS [training: 0.04425841649744436 | validation: 0.06200018423387865]
	TIME [epoch: 9.8 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03190034460735515		[learning rate: 0.0041248]
		[batch 20/20] avg loss: 0.05254819581202057		[learning rate: 0.0041184]
	Learning Rate: 0.00411845
	LOSS [training: 0.04222427020968786 | validation: 0.057156973843513786]
	TIME [epoch: 8.86 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09017279624448013		[learning rate: 0.0041121]
		[batch 20/20] avg loss: 0.03184534490889415		[learning rate: 0.0041058]
	Learning Rate: 0.00410582
	LOSS [training: 0.06100907057668714 | validation: 0.08132683522901088]
	TIME [epoch: 8.86 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05646434745588382		[learning rate: 0.0040995]
		[batch 20/20] avg loss: 0.07517213905009958		[learning rate: 0.0040932]
	Learning Rate: 0.00409323
	LOSS [training: 0.0658182432529917 | validation: 0.0895341929653267]
	TIME [epoch: 8.88 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06308737875953468		[learning rate: 0.004087]
		[batch 20/20] avg loss: 0.050092601450492845		[learning rate: 0.0040807]
	Learning Rate: 0.00408069
	LOSS [training: 0.05658999010501377 | validation: 0.046994243633930434]
	TIME [epoch: 8.87 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044849448980384454		[learning rate: 0.0040744]
		[batch 20/20] avg loss: 0.05087363329022102		[learning rate: 0.0040682]
	Learning Rate: 0.00406818
	LOSS [training: 0.04786154113530274 | validation: 0.05952080951553468]
	TIME [epoch: 8.86 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05146708664702755		[learning rate: 0.0040619]
		[batch 20/20] avg loss: 0.057315179352350344		[learning rate: 0.0040557]
	Learning Rate: 0.00405571
	LOSS [training: 0.05439113299968893 | validation: 0.05772613692170127]
	TIME [epoch: 8.86 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048968100259211896		[learning rate: 0.0040495]
		[batch 20/20] avg loss: 0.06015332166406366		[learning rate: 0.0040433]
	Learning Rate: 0.00404328
	LOSS [training: 0.05456071096163778 | validation: 0.061213319744628555]
	TIME [epoch: 8.88 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0334329801790187		[learning rate: 0.0040371]
		[batch 20/20] avg loss: 0.06464495643730003		[learning rate: 0.0040309]
	Learning Rate: 0.00403088
	LOSS [training: 0.04903896830815936 | validation: 0.09307793187193482]
	TIME [epoch: 8.91 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045171890813439294		[learning rate: 0.0040247]
		[batch 20/20] avg loss: 0.03391047713787401		[learning rate: 0.0040185]
	Learning Rate: 0.00401852
	LOSS [training: 0.039541183975656646 | validation: 0.0650910407926221]
	TIME [epoch: 9.03 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061919039919704086		[learning rate: 0.0040124]
		[batch 20/20] avg loss: 0.04465391040221601		[learning rate: 0.0040062]
	Learning Rate: 0.00400621
	LOSS [training: 0.05328647516096006 | validation: 0.032904008801571794]
	TIME [epoch: 8.89 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11577206666338938		[learning rate: 0.0040001]
		[batch 20/20] avg loss: 0.03932236221805677		[learning rate: 0.0039939]
	Learning Rate: 0.00399393
	LOSS [training: 0.07754721444072307 | validation: 0.026596269754650723]
	TIME [epoch: 8.92 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048893840273122706		[learning rate: 0.0039878]
		[batch 20/20] avg loss: 0.04883437927081683		[learning rate: 0.0039817]
	Learning Rate: 0.00398168
	LOSS [training: 0.04886410977196977 | validation: 0.05023569632790383]
	TIME [epoch: 8.9 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05522534821534173		[learning rate: 0.0039756]
		[batch 20/20] avg loss: 0.03305148365727192		[learning rate: 0.0039695]
	Learning Rate: 0.00396948
	LOSS [training: 0.04413841593630683 | validation: 0.04212278718156011]
	TIME [epoch: 8.88 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05003358065791058		[learning rate: 0.0039634]
		[batch 20/20] avg loss: 0.028038130391410992		[learning rate: 0.0039573]
	Learning Rate: 0.00395731
	LOSS [training: 0.03903585552466079 | validation: 0.07425398785172115]
	TIME [epoch: 8.89 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05117604262195603		[learning rate: 0.0039512]
		[batch 20/20] avg loss: 0.06846792043279062		[learning rate: 0.0039452]
	Learning Rate: 0.00394518
	LOSS [training: 0.05982198152737332 | validation: 0.07397991120594222]
	TIME [epoch: 8.88 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05179627634656444		[learning rate: 0.0039391]
		[batch 20/20] avg loss: 0.06205471334796446		[learning rate: 0.0039331]
	Learning Rate: 0.00393308
	LOSS [training: 0.056925494847264446 | validation: 0.07482211892396244]
	TIME [epoch: 8.89 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06707149920105263		[learning rate: 0.0039271]
		[batch 20/20] avg loss: 0.08128153249443021		[learning rate: 0.003921]
	Learning Rate: 0.00392103
	LOSS [training: 0.07417651584774142 | validation: 0.06105658740149693]
	TIME [epoch: 8.86 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05115359814193018		[learning rate: 0.003915]
		[batch 20/20] avg loss: 0.07409455297014897		[learning rate: 0.003909]
	Learning Rate: 0.00390901
	LOSS [training: 0.06262407555603958 | validation: 0.0607715410201036]
	TIME [epoch: 8.86 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07046893642025301		[learning rate: 0.003903]
		[batch 20/20] avg loss: 0.09179471392170957		[learning rate: 0.003897]
	Learning Rate: 0.00389703
	LOSS [training: 0.08113182517098128 | validation: 0.05330053932529363]
	TIME [epoch: 8.86 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05209437021947726		[learning rate: 0.003891]
		[batch 20/20] avg loss: 0.04237202453763239		[learning rate: 0.0038851]
	Learning Rate: 0.00388508
	LOSS [training: 0.04723319737855482 | validation: 0.0313094001074023]
	TIME [epoch: 8.89 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08491096675090196		[learning rate: 0.0038791]
		[batch 20/20] avg loss: 0.07515548407742156		[learning rate: 0.0038732]
	Learning Rate: 0.00387317
	LOSS [training: 0.08003322541416177 | validation: 0.02564178664166545]
	TIME [epoch: 8.86 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03251853320635423		[learning rate: 0.0038672]
		[batch 20/20] avg loss: 0.026807413328121827		[learning rate: 0.0038613]
	Learning Rate: 0.0038613
	LOSS [training: 0.029662973267238036 | validation: 0.04861438519697829]
	TIME [epoch: 8.86 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05525997570109933		[learning rate: 0.0038554]
		[batch 20/20] avg loss: 0.05504705985550146		[learning rate: 0.0038495]
	Learning Rate: 0.00384946
	LOSS [training: 0.055153517778300395 | validation: 0.0413611160903998]
	TIME [epoch: 8.86 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028735901350036376		[learning rate: 0.0038436]
		[batch 20/20] avg loss: 0.02910363779321824		[learning rate: 0.0038377]
	Learning Rate: 0.00383766
	LOSS [training: 0.028919769571627306 | validation: 0.03942847510854892]
	TIME [epoch: 8.88 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03506082487764831		[learning rate: 0.0038318]
		[batch 20/20] avg loss: 0.036972617388063206		[learning rate: 0.0038259]
	Learning Rate: 0.0038259
	LOSS [training: 0.03601672113285577 | validation: 0.05709884305466294]
	TIME [epoch: 8.87 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04099285282760755		[learning rate: 0.00382]
		[batch 20/20] avg loss: 0.03178899702335123		[learning rate: 0.0038142]
	Learning Rate: 0.00381417
	LOSS [training: 0.03639092492547938 | validation: 0.026104441260924163]
	TIME [epoch: 8.86 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03132184988327375		[learning rate: 0.0038083]
		[batch 20/20] avg loss: 0.05260846940191688		[learning rate: 0.0038025]
	Learning Rate: 0.00380248
	LOSS [training: 0.04196515964259531 | validation: 0.05135667797384334]
	TIME [epoch: 8.86 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061105726248027147		[learning rate: 0.0037966]
		[batch 20/20] avg loss: 0.036885657475400443		[learning rate: 0.0037908]
	Learning Rate: 0.00379082
	LOSS [training: 0.048995691861713805 | validation: 0.041830768808698904]
	TIME [epoch: 8.88 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032678597104344675		[learning rate: 0.003785]
		[batch 20/20] avg loss: 0.03164569272675224		[learning rate: 0.0037792]
	Learning Rate: 0.0037792
	LOSS [training: 0.03216214491554846 | validation: 0.03484784401182417]
	TIME [epoch: 8.88 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041710688856864395		[learning rate: 0.0037734]
		[batch 20/20] avg loss: 0.05087961343176506		[learning rate: 0.0037676]
	Learning Rate: 0.00376762
	LOSS [training: 0.04629515114431472 | validation: 0.029751641774290995]
	TIME [epoch: 8.86 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0246134136426549		[learning rate: 0.0037618]
		[batch 20/20] avg loss: 0.04903623472316145		[learning rate: 0.0037561]
	Learning Rate: 0.00375607
	LOSS [training: 0.03682482418290818 | validation: 0.03304706202111198]
	TIME [epoch: 8.87 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03488238183404864		[learning rate: 0.0037503]
		[batch 20/20] avg loss: 0.06946120071082681		[learning rate: 0.0037446]
	Learning Rate: 0.00374455
	LOSS [training: 0.05217179127243773 | validation: 0.05179870437792429]
	TIME [epoch: 8.87 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04226176157254313		[learning rate: 0.0037388]
		[batch 20/20] avg loss: 0.03885307944939863		[learning rate: 0.0037331]
	Learning Rate: 0.00373307
	LOSS [training: 0.04055742051097088 | validation: 0.03796708731235411]
	TIME [epoch: 8.89 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03883763500735264		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 0.03667865050471468		[learning rate: 0.0037216]
	Learning Rate: 0.00372163
	LOSS [training: 0.03775814275603366 | validation: 0.0338473995794944]
	TIME [epoch: 8.87 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025619052866503556		[learning rate: 0.0037159]
		[batch 20/20] avg loss: 0.04982854233771047		[learning rate: 0.0037102]
	Learning Rate: 0.00371022
	LOSS [training: 0.03772379760210701 | validation: 0.03523175972986435]
	TIME [epoch: 8.86 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04171741819804065		[learning rate: 0.0037045]
		[batch 20/20] avg loss: 0.0413794707837942		[learning rate: 0.0036988]
	Learning Rate: 0.00369885
	LOSS [training: 0.04154844449091742 | validation: 0.046275265849114365]
	TIME [epoch: 8.86 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0460389981335531		[learning rate: 0.0036932]
		[batch 20/20] avg loss: 0.033920936624371054		[learning rate: 0.0036875]
	Learning Rate: 0.00368751
	LOSS [training: 0.039979967378962075 | validation: 0.03941382834780995]
	TIME [epoch: 8.88 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02702054444923228		[learning rate: 0.0036819]
		[batch 20/20] avg loss: 0.027861552091563886		[learning rate: 0.0036762]
	Learning Rate: 0.00367621
	LOSS [training: 0.02744104827039808 | validation: 0.02470885045772652]
	TIME [epoch: 8.87 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04342410589801897		[learning rate: 0.0036706]
		[batch 20/20] avg loss: 0.0345834506742157		[learning rate: 0.0036649]
	Learning Rate: 0.00366494
	LOSS [training: 0.039003778286117334 | validation: 0.050151622440861826]
	TIME [epoch: 8.86 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04679348977400624		[learning rate: 0.0036593]
		[batch 20/20] avg loss: 0.04015291300255762		[learning rate: 0.0036537]
	Learning Rate: 0.0036537
	LOSS [training: 0.04347320138828194 | validation: 0.03818215074568305]
	TIME [epoch: 8.86 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03540597634175957		[learning rate: 0.0036481]
		[batch 20/20] avg loss: 0.031156065119568106		[learning rate: 0.0036425]
	Learning Rate: 0.0036425
	LOSS [training: 0.033281020730663836 | validation: 0.03683503952213517]
	TIME [epoch: 8.88 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05425496122685698		[learning rate: 0.0036369]
		[batch 20/20] avg loss: 0.03467066748706786		[learning rate: 0.0036313]
	Learning Rate: 0.00363134
	LOSS [training: 0.04446281435696242 | validation: 0.06699353993562104]
	TIME [epoch: 8.88 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05033424968759086		[learning rate: 0.0036258]
		[batch 20/20] avg loss: 0.040225054720875036		[learning rate: 0.0036202]
	Learning Rate: 0.00362021
	LOSS [training: 0.04527965220423295 | validation: 0.03715144963304184]
	TIME [epoch: 8.86 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0360771346560642		[learning rate: 0.0036147]
		[batch 20/20] avg loss: 0.027922142987271616		[learning rate: 0.0036091]
	Learning Rate: 0.00360911
	LOSS [training: 0.03199963882166791 | validation: 0.042055192562873225]
	TIME [epoch: 8.86 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052387751308291444		[learning rate: 0.0036036]
		[batch 20/20] avg loss: 0.029058570215074138		[learning rate: 0.003598]
	Learning Rate: 0.00359805
	LOSS [training: 0.040723160761682795 | validation: 0.027253307849961144]
	TIME [epoch: 8.86 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019001845647747272		[learning rate: 0.0035925]
		[batch 20/20] avg loss: 0.0371780473500327		[learning rate: 0.003587]
	Learning Rate: 0.00358702
	LOSS [training: 0.028089946498889996 | validation: 0.05295677322264048]
	TIME [epoch: 8.89 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03309833090167586		[learning rate: 0.0035815]
		[batch 20/20] avg loss: 0.022605213960839254		[learning rate: 0.003576]
	Learning Rate: 0.00357602
	LOSS [training: 0.02785177243125756 | validation: 0.023987130919313905]
	TIME [epoch: 8.87 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03186539262949178		[learning rate: 0.0035705]
		[batch 20/20] avg loss: 0.04274637634049837		[learning rate: 0.0035651]
	Learning Rate: 0.00356506
	LOSS [training: 0.037305884484995076 | validation: 0.050123797697950306]
	TIME [epoch: 8.86 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06498633956555874		[learning rate: 0.0035596]
		[batch 20/20] avg loss: 0.0469870205587241		[learning rate: 0.0035541]
	Learning Rate: 0.00355413
	LOSS [training: 0.05598668006214143 | validation: 0.07247104137148211]
	TIME [epoch: 8.86 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06944752506963112		[learning rate: 0.0035487]
		[batch 20/20] avg loss: 0.1094366505989002		[learning rate: 0.0035432]
	Learning Rate: 0.00354324
	LOSS [training: 0.08944208783426565 | validation: 0.0832351396584371]
	TIME [epoch: 8.89 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09939200477354673		[learning rate: 0.0035378]
		[batch 20/20] avg loss: 0.08621329087865517		[learning rate: 0.0035324]
	Learning Rate: 0.00353237
	LOSS [training: 0.09280264782610095 | validation: 0.08844705784463597]
	TIME [epoch: 8.87 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08034647967620981		[learning rate: 0.003527]
		[batch 20/20] avg loss: 0.07762128656941504		[learning rate: 0.0035215]
	Learning Rate: 0.00352155
	LOSS [training: 0.07898388312281242 | validation: 0.077435426126025]
	TIME [epoch: 8.87 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.083116765763991		[learning rate: 0.0035161]
		[batch 20/20] avg loss: 0.04471660763166262		[learning rate: 0.0035108]
	Learning Rate: 0.00351075
	LOSS [training: 0.06391668669782682 | validation: 0.050245716032421284]
	TIME [epoch: 8.86 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07788456009506854		[learning rate: 0.0035054]
		[batch 20/20] avg loss: 0.0768102649791376		[learning rate: 0.0035]
	Learning Rate: 0.00349999
	LOSS [training: 0.07734741253710306 | validation: 0.08274905070967922]
	TIME [epoch: 8.88 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07571407221556085		[learning rate: 0.0034946]
		[batch 20/20] avg loss: 0.12628694397439325		[learning rate: 0.0034893]
	Learning Rate: 0.00348926
	LOSS [training: 0.10100050809497704 | validation: 0.11891218822429761]
	TIME [epoch: 8.87 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0837907466162611		[learning rate: 0.0034839]
		[batch 20/20] avg loss: 0.09610323563499168		[learning rate: 0.0034786]
	Learning Rate: 0.00347856
	LOSS [training: 0.0899469911256264 | validation: 0.07921229505473354]
	TIME [epoch: 9.45 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05008729575929262		[learning rate: 0.0034732]
		[batch 20/20] avg loss: 0.05221935646954497		[learning rate: 0.0034679]
	Learning Rate: 0.0034679
	LOSS [training: 0.05115332611441879 | validation: 0.06917529009776649]
	TIME [epoch: 8.87 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0364292117670869		[learning rate: 0.0034626]
		[batch 20/20] avg loss: 0.03507786286504298		[learning rate: 0.0034573]
	Learning Rate: 0.00345727
	LOSS [training: 0.03575353731606495 | validation: 0.06769767255401518]
	TIME [epoch: 8.87 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04446593899831885		[learning rate: 0.003452]
		[batch 20/20] avg loss: 0.024461491995160848		[learning rate: 0.0034467]
	Learning Rate: 0.00344667
	LOSS [training: 0.03446371549673985 | validation: 0.03067477109249926]
	TIME [epoch: 8.87 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034194877307759684		[learning rate: 0.0034414]
		[batch 20/20] avg loss: 0.01919102762335153		[learning rate: 0.0034361]
	Learning Rate: 0.00343611
	LOSS [training: 0.026692952465555607 | validation: 0.02767560685483962]
	TIME [epoch: 8.86 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022451921172924372		[learning rate: 0.0034308]
		[batch 20/20] avg loss: 0.025379304680094084		[learning rate: 0.0034256]
	Learning Rate: 0.00342557
	LOSS [training: 0.02391561292650923 | validation: 0.0490089569888304]
	TIME [epoch: 8.86 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037071184260986274		[learning rate: 0.0034203]
		[batch 20/20] avg loss: 0.044278495033863004		[learning rate: 0.0034151]
	Learning Rate: 0.00341507
	LOSS [training: 0.04067483964742464 | validation: 0.02741115559540469]
	TIME [epoch: 8.86 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028046297539264953		[learning rate: 0.0034098]
		[batch 20/20] avg loss: 0.022672755506559124		[learning rate: 0.0034046]
	Learning Rate: 0.0034046
	LOSS [training: 0.025359526522912033 | validation: 0.025303247448793826]
	TIME [epoch: 8.91 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03292301094180648		[learning rate: 0.0033994]
		[batch 20/20] avg loss: 0.034455934040954594		[learning rate: 0.0033942]
	Learning Rate: 0.00339417
	LOSS [training: 0.03368947249138054 | validation: 0.03812468384561457]
	TIME [epoch: 8.87 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0359331564915228		[learning rate: 0.003389]
		[batch 20/20] avg loss: 0.02928276160044832		[learning rate: 0.0033838]
	Learning Rate: 0.00338376
	LOSS [training: 0.03260795904598556 | validation: 0.015802386370660445]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_853.pth
	Model improved!!!
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019080961919326338		[learning rate: 0.0033786]
		[batch 20/20] avg loss: 0.03831184560386498		[learning rate: 0.0033734]
	Learning Rate: 0.00337339
	LOSS [training: 0.028696403761595664 | validation: 0.0380116524183812]
	TIME [epoch: 8.86 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0335613137880086		[learning rate: 0.0033682]
		[batch 20/20] avg loss: 0.03619205720510586		[learning rate: 0.0033631]
	Learning Rate: 0.00336305
	LOSS [training: 0.03487668549655723 | validation: 0.031799067653729106]
	TIME [epoch: 8.88 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031260516151224785		[learning rate: 0.0033579]
		[batch 20/20] avg loss: 0.028239503163973216		[learning rate: 0.0033527]
	Learning Rate: 0.00335274
	LOSS [training: 0.029750009657599004 | validation: 0.032415720774697135]
	TIME [epoch: 8.85 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02623609235592785		[learning rate: 0.0033476]
		[batch 20/20] avg loss: 0.029842406667780735		[learning rate: 0.0033425]
	Learning Rate: 0.00334246
	LOSS [training: 0.02803924951185429 | validation: 0.0316845089367859]
	TIME [epoch: 8.85 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052749607780749386		[learning rate: 0.0033373]
		[batch 20/20] avg loss: 0.01899530161878866		[learning rate: 0.0033322]
	Learning Rate: 0.00333222
	LOSS [training: 0.03587245469976901 | validation: 0.04934813731785756]
	TIME [epoch: 8.85 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023340800893022155		[learning rate: 0.0033271]
		[batch 20/20] avg loss: 0.03505343924867159		[learning rate: 0.003322]
	Learning Rate: 0.003322
	LOSS [training: 0.02919712007084687 | validation: 0.05770568179939337]
	TIME [epoch: 8.87 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03229788279698024		[learning rate: 0.0033169]
		[batch 20/20] avg loss: 0.04177942110056695		[learning rate: 0.0033118]
	Learning Rate: 0.00331182
	LOSS [training: 0.037038651948773586 | validation: 0.04165194063405021]
	TIME [epoch: 8.86 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027115931873370802		[learning rate: 0.0033067]
		[batch 20/20] avg loss: 0.02463277775164588		[learning rate: 0.0033017]
	Learning Rate: 0.00330167
	LOSS [training: 0.025874354812508337 | validation: 0.02601638511953288]
	TIME [epoch: 8.85 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026908621712945724		[learning rate: 0.0032966]
		[batch 20/20] avg loss: 0.0514786063664616		[learning rate: 0.0032915]
	Learning Rate: 0.00329155
	LOSS [training: 0.03919361403970366 | validation: 0.03457489366326244]
	TIME [epoch: 8.85 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024003651221083368		[learning rate: 0.0032865]
		[batch 20/20] avg loss: 0.025008295575605265		[learning rate: 0.0032815]
	Learning Rate: 0.00328146
	LOSS [training: 0.024505973398344316 | validation: 0.025917296842529285]
	TIME [epoch: 8.85 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039504511779940195		[learning rate: 0.0032764]
		[batch 20/20] avg loss: 0.056023211785512075		[learning rate: 0.0032714]
	Learning Rate: 0.0032714
	LOSS [training: 0.04776386178272614 | validation: 0.04972782170948589]
	TIME [epoch: 8.87 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04305382720977071		[learning rate: 0.0032664]
		[batch 20/20] avg loss: 0.048881808536432154		[learning rate: 0.0032614]
	Learning Rate: 0.00326137
	LOSS [training: 0.045967817873101426 | validation: 0.024997486036515977]
	TIME [epoch: 8.85 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020312627766556065		[learning rate: 0.0032564]
		[batch 20/20] avg loss: 0.027651947530444122		[learning rate: 0.0032514]
	Learning Rate: 0.00325137
	LOSS [training: 0.023982287648500095 | validation: 0.04621272891447377]
	TIME [epoch: 8.84 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025830226464074273		[learning rate: 0.0032464]
		[batch 20/20] avg loss: 0.03122368011880653		[learning rate: 0.0032414]
	Learning Rate: 0.00324141
	LOSS [training: 0.0285269532914404 | validation: 0.03884715727378968]
	TIME [epoch: 8.86 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027449460469444697		[learning rate: 0.0032364]
		[batch 20/20] avg loss: 0.02299456313212151		[learning rate: 0.0032315]
	Learning Rate: 0.00323147
	LOSS [training: 0.025222011800783112 | validation: 0.027107084158339297]
	TIME [epoch: 8.87 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026848720611827043		[learning rate: 0.0032265]
		[batch 20/20] avg loss: 0.02591249896762416		[learning rate: 0.0032216]
	Learning Rate: 0.00322156
	LOSS [training: 0.0263806097897256 | validation: 0.027164616599735084]
	TIME [epoch: 8.85 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031775840534271516		[learning rate: 0.0032166]
		[batch 20/20] avg loss: 0.02986765191666515		[learning rate: 0.0032117]
	Learning Rate: 0.00321169
	LOSS [training: 0.030821746225468332 | validation: 0.030332127555071163]
	TIME [epoch: 8.85 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023430539376379407		[learning rate: 0.0032068]
		[batch 20/20] avg loss: 0.0423637710621524		[learning rate: 0.0032018]
	Learning Rate: 0.00320184
	LOSS [training: 0.032897155219265915 | validation: 0.04619510896206974]
	TIME [epoch: 8.84 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05174685457986546		[learning rate: 0.0031969]
		[batch 20/20] avg loss: 0.02879432935213544		[learning rate: 0.003192]
	Learning Rate: 0.00319203
	LOSS [training: 0.04027059196600045 | validation: 0.054266386443570735]
	TIME [epoch: 8.86 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04255497965833959		[learning rate: 0.0031871]
		[batch 20/20] avg loss: 0.02935968871865042		[learning rate: 0.0031822]
	Learning Rate: 0.00318224
	LOSS [training: 0.035957334188495005 | validation: 0.05191774271502106]
	TIME [epoch: 8.85 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049730064676498445		[learning rate: 0.0031774]
		[batch 20/20] avg loss: 0.022455055465265442		[learning rate: 0.0031725]
	Learning Rate: 0.00317249
	LOSS [training: 0.03609256007088194 | validation: 0.02646411255032153]
	TIME [epoch: 8.84 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021901422919791632		[learning rate: 0.0031676]
		[batch 20/20] avg loss: 0.030974478553398972		[learning rate: 0.0031628]
	Learning Rate: 0.00316276
	LOSS [training: 0.0264379507365953 | validation: 0.03598648305155501]
	TIME [epoch: 8.84 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02578308341480067		[learning rate: 0.0031579]
		[batch 20/20] avg loss: 0.02773236844808527		[learning rate: 0.0031531]
	Learning Rate: 0.00315307
	LOSS [training: 0.026757725931442967 | validation: 0.032186467437247276]
	TIME [epoch: 8.85 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038330250704303195		[learning rate: 0.0031482]
		[batch 20/20] avg loss: 0.02326888379733212		[learning rate: 0.0031434]
	Learning Rate: 0.0031434
	LOSS [training: 0.030799567250817655 | validation: 0.03312740842562269]
	TIME [epoch: 8.86 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029431874444115623		[learning rate: 0.0031386]
		[batch 20/20] avg loss: 0.026185759644965684		[learning rate: 0.0031338]
	Learning Rate: 0.00313377
	LOSS [training: 0.027808817044540652 | validation: 0.019316450005048235]
	TIME [epoch: 8.85 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033676541995275436		[learning rate: 0.003129]
		[batch 20/20] avg loss: 0.025202072967801508		[learning rate: 0.0031242]
	Learning Rate: 0.00312416
	LOSS [training: 0.029439307481538467 | validation: 0.03263375710501598]
	TIME [epoch: 8.85 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06136398354327768		[learning rate: 0.0031194]
		[batch 20/20] avg loss: 0.034224180538514656		[learning rate: 0.0031146]
	Learning Rate: 0.00311458
	LOSS [training: 0.047794082040896156 | validation: 0.038401288496217045]
	TIME [epoch: 8.85 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028921281943601572		[learning rate: 0.0031098]
		[batch 20/20] avg loss: 0.03226873238825843		[learning rate: 0.003105]
	Learning Rate: 0.00310504
	LOSS [training: 0.03059500716593 | validation: 0.04324858659916678]
	TIME [epoch: 8.87 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04043006639128233		[learning rate: 0.0031003]
		[batch 20/20] avg loss: 0.021705520560033425		[learning rate: 0.0030955]
	Learning Rate: 0.00309552
	LOSS [training: 0.031067793475657877 | validation: 0.026801711173615746]
	TIME [epoch: 8.85 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03876978936719379		[learning rate: 0.0030908]
		[batch 20/20] avg loss: 0.024669849125344496		[learning rate: 0.003086]
	Learning Rate: 0.00308603
	LOSS [training: 0.03171981924626914 | validation: 0.01949207208388261]
	TIME [epoch: 8.85 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023091598248617114		[learning rate: 0.0030813]
		[batch 20/20] avg loss: 0.017464730363738988		[learning rate: 0.0030766]
	Learning Rate: 0.00307657
	LOSS [training: 0.020278164306178053 | validation: 0.02983792465439441]
	TIME [epoch: 8.84 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02288879887842348		[learning rate: 0.0030718]
		[batch 20/20] avg loss: 0.0258421938550223		[learning rate: 0.0030671]
	Learning Rate: 0.00306714
	LOSS [training: 0.02436549636672289 | validation: 0.022274319251820572]
	TIME [epoch: 8.87 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022715345431243664		[learning rate: 0.0030624]
		[batch 20/20] avg loss: 0.017865201973945814		[learning rate: 0.0030577]
	Learning Rate: 0.00305774
	LOSS [training: 0.020290273702594737 | validation: 0.04572659705693653]
	TIME [epoch: 8.85 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044532701073414026		[learning rate: 0.003053]
		[batch 20/20] avg loss: 0.019938236382436185		[learning rate: 0.0030484]
	Learning Rate: 0.00304836
	LOSS [training: 0.032235468727925105 | validation: 0.024826798454396195]
	TIME [epoch: 8.85 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033644611881461964		[learning rate: 0.0030437]
		[batch 20/20] avg loss: 0.018266589913144872		[learning rate: 0.003039]
	Learning Rate: 0.00303902
	LOSS [training: 0.02595560089730342 | validation: 0.020677904832349096]
	TIME [epoch: 8.84 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02903094368643937		[learning rate: 0.0030344]
		[batch 20/20] avg loss: 0.026431928409285342		[learning rate: 0.0030297]
	Learning Rate: 0.0030297
	LOSS [training: 0.027731436047862358 | validation: 0.020920567392194316]
	TIME [epoch: 8.87 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023831963525394923		[learning rate: 0.0030251]
		[batch 20/20] avg loss: 0.023714678437134006		[learning rate: 0.0030204]
	Learning Rate: 0.00302042
	LOSS [training: 0.023773320981264463 | validation: 0.025916417091740654]
	TIME [epoch: 8.85 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04031550420833777		[learning rate: 0.0030158]
		[batch 20/20] avg loss: 0.019468566701934458		[learning rate: 0.0030112]
	Learning Rate: 0.00301116
	LOSS [training: 0.029892035455136124 | validation: 0.029681866726644374]
	TIME [epoch: 8.84 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02676602793703193		[learning rate: 0.0030065]
		[batch 20/20] avg loss: 0.024355540667359253		[learning rate: 0.0030019]
	Learning Rate: 0.00300193
	LOSS [training: 0.025560784302195588 | validation: 0.049056377116333566]
	TIME [epoch: 8.85 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02071317024856129		[learning rate: 0.0029973]
		[batch 20/20] avg loss: 0.03712406582771821		[learning rate: 0.0029927]
	Learning Rate: 0.00299272
	LOSS [training: 0.028918618038139755 | validation: 0.02541559247097689]
	TIME [epoch: 8.85 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03434894010779581		[learning rate: 0.0029881]
		[batch 20/20] avg loss: 0.02618976411046021		[learning rate: 0.0029835]
	Learning Rate: 0.00298355
	LOSS [training: 0.03026935210912801 | validation: 0.027039140871585876]
	TIME [epoch: 8.87 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03589962037072925		[learning rate: 0.002979]
		[batch 20/20] avg loss: 0.02896975456872734		[learning rate: 0.0029744]
	Learning Rate: 0.0029744
	LOSS [training: 0.032434687469728295 | validation: 0.040980218626044125]
	TIME [epoch: 8.85 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05136777616487041		[learning rate: 0.0029698]
		[batch 20/20] avg loss: 0.027054262574807492		[learning rate: 0.0029653]
	Learning Rate: 0.00296529
	LOSS [training: 0.03921101936983896 | validation: 0.05950724013776379]
	TIME [epoch: 8.85 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03364518516334194		[learning rate: 0.0029607]
		[batch 20/20] avg loss: 0.02203197021504514		[learning rate: 0.0029562]
	Learning Rate: 0.0029562
	LOSS [training: 0.027838577689193532 | validation: 0.02586009325793375]
	TIME [epoch: 8.85 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03410263189415723		[learning rate: 0.0029517]
		[batch 20/20] avg loss: 0.025667882648141675		[learning rate: 0.0029471]
	Learning Rate: 0.00294713
	LOSS [training: 0.029885257271149455 | validation: 0.01703700479772772]
	TIME [epoch: 8.87 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02457745407225009		[learning rate: 0.0029426]
		[batch 20/20] avg loss: 0.04681332649007375		[learning rate: 0.0029381]
	Learning Rate: 0.0029381
	LOSS [training: 0.035695390281161914 | validation: 0.023617162603492614]
	TIME [epoch: 8.85 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03341812917385136		[learning rate: 0.0029336]
		[batch 20/20] avg loss: 0.03897475411262577		[learning rate: 0.0029291]
	Learning Rate: 0.00292909
	LOSS [training: 0.03619644164323857 | validation: 0.04569049501326052]
	TIME [epoch: 8.85 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028645350423428768		[learning rate: 0.0029246]
		[batch 20/20] avg loss: 0.032868080815776184		[learning rate: 0.0029201]
	Learning Rate: 0.00292011
	LOSS [training: 0.030756715619602477 | validation: 0.024484618684344314]
	TIME [epoch: 8.85 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02376890337085393		[learning rate: 0.0029156]
		[batch 20/20] avg loss: 0.040176411818245474		[learning rate: 0.0029112]
	Learning Rate: 0.00291116
	LOSS [training: 0.031972657594549696 | validation: 0.04306963707704063]
	TIME [epoch: 8.89 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042462463655499764		[learning rate: 0.0029067]
		[batch 20/20] avg loss: 0.03228047376680794		[learning rate: 0.0029022]
	Learning Rate: 0.00290224
	LOSS [training: 0.03737146871115386 | validation: 0.03343691670755215]
	TIME [epoch: 8.89 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02460921266589809		[learning rate: 0.0028978]
		[batch 20/20] avg loss: 0.017135996332846064		[learning rate: 0.0028933]
	Learning Rate: 0.00289334
	LOSS [training: 0.020872604499372074 | validation: 0.01788599149986811]
	TIME [epoch: 8.87 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029141022737317633		[learning rate: 0.0028889]
		[batch 20/20] avg loss: 0.029247384172075574		[learning rate: 0.0028845]
	Learning Rate: 0.00288447
	LOSS [training: 0.029194203454696605 | validation: 0.023112344989697168]
	TIME [epoch: 8.87 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02138935084539049		[learning rate: 0.00288]
		[batch 20/20] avg loss: 0.028062547804714906		[learning rate: 0.0028756]
	Learning Rate: 0.00287563
	LOSS [training: 0.024725949325052697 | validation: 0.022931429413243345]
	TIME [epoch: 8.87 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027805078110559635		[learning rate: 0.0028712]
		[batch 20/20] avg loss: 0.030658171396517487		[learning rate: 0.0028668]
	Learning Rate: 0.00286682
	LOSS [training: 0.02923162475353856 | validation: 0.023257961464375053]
	TIME [epoch: 8.88 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02138584706295748		[learning rate: 0.0028624]
		[batch 20/20] avg loss: 0.030540281026287242		[learning rate: 0.002858]
	Learning Rate: 0.00285803
	LOSS [training: 0.02596306404462236 | validation: 0.03925670122773654]
	TIME [epoch: 8.87 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04404499119938574		[learning rate: 0.0028536]
		[batch 20/20] avg loss: 0.056722803638904605		[learning rate: 0.0028493]
	Learning Rate: 0.00284927
	LOSS [training: 0.05038389741914519 | validation: 0.061954180900560235]
	TIME [epoch: 8.86 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04360888138249756		[learning rate: 0.0028449]
		[batch 20/20] avg loss: 0.03303372392757705		[learning rate: 0.0028405]
	Learning Rate: 0.00284053
	LOSS [training: 0.038321302655037304 | validation: 0.035192277679035426]
	TIME [epoch: 8.85 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0664122037408538		[learning rate: 0.0028362]
		[batch 20/20] avg loss: 0.0594210708914934		[learning rate: 0.0028318]
	Learning Rate: 0.00283183
	LOSS [training: 0.0629166373161736 | validation: 0.05382543266439492]
	TIME [epoch: 8.87 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047136937770698294		[learning rate: 0.0028275]
		[batch 20/20] avg loss: 0.05390887191569125		[learning rate: 0.0028231]
	Learning Rate: 0.00282315
	LOSS [training: 0.05052290484319477 | validation: 0.03515912331032864]
	TIME [epoch: 8.85 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03371869601556368		[learning rate: 0.0028188]
		[batch 20/20] avg loss: 0.04293365965180759		[learning rate: 0.0028145]
	Learning Rate: 0.00281449
	LOSS [training: 0.03832617783368564 | validation: 0.05260515094158323]
	TIME [epoch: 8.85 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03718889154533581		[learning rate: 0.0028102]
		[batch 20/20] avg loss: 0.028567349691220923		[learning rate: 0.0028059]
	Learning Rate: 0.00280586
	LOSS [training: 0.03287812061827836 | validation: 0.041550666338452495]
	TIME [epoch: 8.89 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034212521375867985		[learning rate: 0.0028016]
		[batch 20/20] avg loss: 0.02598448052531701		[learning rate: 0.0027973]
	Learning Rate: 0.00279726
	LOSS [training: 0.030098500950592494 | validation: 0.026727334173909267]
	TIME [epoch: 8.87 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02328029068592057		[learning rate: 0.002793]
		[batch 20/20] avg loss: 0.03141424968432892		[learning rate: 0.0027887]
	Learning Rate: 0.00278869
	LOSS [training: 0.02734727018512475 | validation: 0.02396837234429437]
	TIME [epoch: 8.85 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03644635987965939		[learning rate: 0.0027844]
		[batch 20/20] avg loss: 0.03444252644569708		[learning rate: 0.0027801]
	Learning Rate: 0.00278014
	LOSS [training: 0.03544444316267823 | validation: 0.038792167069803304]
	TIME [epoch: 8.85 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04398773459161816		[learning rate: 0.0027759]
		[batch 20/20] avg loss: 0.03621250683633335		[learning rate: 0.0027716]
	Learning Rate: 0.00277162
	LOSS [training: 0.04010012071397575 | validation: 0.02760538616606731]
	TIME [epoch: 8.85 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023896116242383587		[learning rate: 0.0027674]
		[batch 20/20] avg loss: 0.033610946266190336		[learning rate: 0.0027631]
	Learning Rate: 0.00276312
	LOSS [training: 0.02875353125428696 | validation: 0.05391637579415505]
	TIME [epoch: 8.87 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03939692139547416		[learning rate: 0.0027589]
		[batch 20/20] avg loss: 0.024762853234213008		[learning rate: 0.0027547]
	Learning Rate: 0.00275465
	LOSS [training: 0.03207988731484358 | validation: 0.027439950319646012]
	TIME [epoch: 8.85 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02243905313769534		[learning rate: 0.0027504]
		[batch 20/20] avg loss: 0.018637759930136173		[learning rate: 0.0027462]
	Learning Rate: 0.00274621
	LOSS [training: 0.020538406533915754 | validation: 0.021735035138702622]
	TIME [epoch: 8.85 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03648563872099873		[learning rate: 0.002742]
		[batch 20/20] avg loss: 0.0340584913270716		[learning rate: 0.0027378]
	Learning Rate: 0.00273779
	LOSS [training: 0.03527206502403517 | validation: 0.06220733583174243]
	TIME [epoch: 8.85 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04612803650374801		[learning rate: 0.0027336]
		[batch 20/20] avg loss: 0.03747406986706496		[learning rate: 0.0027294]
	Learning Rate: 0.0027294
	LOSS [training: 0.04180105318540648 | validation: 0.04609667744652618]
	TIME [epoch: 8.85 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031223135316687085		[learning rate: 0.0027252]
		[batch 20/20] avg loss: 0.026896799345721818		[learning rate: 0.002721]
	Learning Rate: 0.00272103
	LOSS [training: 0.029059967331204452 | validation: 0.03014432002607075]
	TIME [epoch: 8.87 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02873799814908225		[learning rate: 0.0027169]
		[batch 20/20] avg loss: 0.03596799852088749		[learning rate: 0.0027127]
	Learning Rate: 0.00271269
	LOSS [training: 0.03235299833498486 | validation: 0.0200501362623577]
	TIME [epoch: 8.85 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0494796239667538		[learning rate: 0.0027085]
		[batch 20/20] avg loss: 0.01951150403871471		[learning rate: 0.0027044]
	Learning Rate: 0.00270437
	LOSS [training: 0.03449556400273426 | validation: 0.019874315982210393]
	TIME [epoch: 8.85 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01578938289848912		[learning rate: 0.0027002]
		[batch 20/20] avg loss: 0.031834813876809184		[learning rate: 0.0026961]
	Learning Rate: 0.00269608
	LOSS [training: 0.02381209838764916 | validation: 0.03436266001795381]
	TIME [epoch: 8.85 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02701672263154444		[learning rate: 0.0026919]
		[batch 20/20] avg loss: 0.028101070292520354		[learning rate: 0.0026878]
	Learning Rate: 0.00268782
	LOSS [training: 0.027558896462032394 | validation: 0.018825405234835425]
	TIME [epoch: 8.87 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01509416873101598		[learning rate: 0.0026837]
		[batch 20/20] avg loss: 0.01485037099690121		[learning rate: 0.0026796]
	Learning Rate: 0.00267958
	LOSS [training: 0.014972269863958599 | validation: 0.02514676778316824]
	TIME [epoch: 8.85 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019997524196705536		[learning rate: 0.0026755]
		[batch 20/20] avg loss: 0.017541607524938448		[learning rate: 0.0026714]
	Learning Rate: 0.00267137
	LOSS [training: 0.018769565860821992 | validation: 0.022306819376371717]
	TIME [epoch: 8.85 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02608566522356645		[learning rate: 0.0026673]
		[batch 20/20] avg loss: 0.01930418516369896		[learning rate: 0.0026632]
	Learning Rate: 0.00266318
	LOSS [training: 0.022694925193632705 | validation: 0.02519209944516026]
	TIME [epoch: 8.85 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02068726766349675		[learning rate: 0.0026591]
		[batch 20/20] avg loss: 0.04485309362810225		[learning rate: 0.002655]
	Learning Rate: 0.00265501
	LOSS [training: 0.0327701806457995 | validation: 0.022696301007977353]
	TIME [epoch: 8.86 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027692053473517813		[learning rate: 0.0026509]
		[batch 20/20] avg loss: 0.018591429689451586		[learning rate: 0.0026469]
	Learning Rate: 0.00264687
	LOSS [training: 0.0231417415814847 | validation: 0.021534994867780217]
	TIME [epoch: 8.85 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02450883983093591		[learning rate: 0.0026428]
		[batch 20/20] avg loss: 0.011910359950844702		[learning rate: 0.0026388]
	Learning Rate: 0.00263876
	LOSS [training: 0.01820959989089031 | validation: 0.0366945038778541]
	TIME [epoch: 8.85 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019529295484394478		[learning rate: 0.0026347]
		[batch 20/20] avg loss: 0.043796385876456526		[learning rate: 0.0026307]
	Learning Rate: 0.00263067
	LOSS [training: 0.031662840680425505 | validation: 0.03952440284867592]
	TIME [epoch: 8.85 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02571053573555259		[learning rate: 0.0026266]
		[batch 20/20] avg loss: 0.027835230075811696		[learning rate: 0.0026226]
	Learning Rate: 0.00262261
	LOSS [training: 0.02677288290568215 | validation: 0.023828727373864947]
	TIME [epoch: 8.86 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02553011221416599		[learning rate: 0.0026186]
		[batch 20/20] avg loss: 0.02482700133477781		[learning rate: 0.0026146]
	Learning Rate: 0.00261457
	LOSS [training: 0.025178556774471905 | validation: 0.020572996059914972]
	TIME [epoch: 8.85 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015554212666255923		[learning rate: 0.0026106]
		[batch 20/20] avg loss: 0.0328075188714554		[learning rate: 0.0026066]
	Learning Rate: 0.00260655
	LOSS [training: 0.024180865768855662 | validation: 0.04038276929723798]
	TIME [epoch: 8.85 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03500282550100385		[learning rate: 0.0026026]
		[batch 20/20] avg loss: 0.03170647977533639		[learning rate: 0.0025986]
	Learning Rate: 0.00259856
	LOSS [training: 0.03335465263817011 | validation: 0.029608653577484892]
	TIME [epoch: 8.85 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05066880817871759		[learning rate: 0.0025946]
		[batch 20/20] avg loss: 0.047357793019287484		[learning rate: 0.0025906]
	Learning Rate: 0.0025906
	LOSS [training: 0.04901330059900254 | validation: 0.040891643045105425]
	TIME [epoch: 8.84 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02467008576379698		[learning rate: 0.0025866]
		[batch 20/20] avg loss: 0.028409716124236678		[learning rate: 0.0025827]
	Learning Rate: 0.00258266
	LOSS [training: 0.02653990094401683 | validation: 0.02661823960959872]
	TIME [epoch: 8.87 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0215366844966358		[learning rate: 0.0025787]
		[batch 20/20] avg loss: 0.017422007218120875		[learning rate: 0.0025747]
	Learning Rate: 0.00257474
	LOSS [training: 0.019479345857378337 | validation: 0.029150878107594206]
	TIME [epoch: 8.85 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018810341199915356		[learning rate: 0.0025708]
		[batch 20/20] avg loss: 0.0224667330231157		[learning rate: 0.0025668]
	Learning Rate: 0.00256685
	LOSS [training: 0.020638537111515528 | validation: 0.026915346365450253]
	TIME [epoch: 8.85 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023555567519073017		[learning rate: 0.0025629]
		[batch 20/20] avg loss: 0.019794253581345546		[learning rate: 0.002559]
	Learning Rate: 0.00255898
	LOSS [training: 0.021674910550209283 | validation: 0.023393215702772897]
	TIME [epoch: 8.85 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029047583988185223		[learning rate: 0.0025551]
		[batch 20/20] avg loss: 0.021988369551058203		[learning rate: 0.0025511]
	Learning Rate: 0.00255113
	LOSS [training: 0.02551797676962172 | validation: 0.019298796924619076]
	TIME [epoch: 8.87 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022681514014479126		[learning rate: 0.0025472]
		[batch 20/20] avg loss: 0.022384496682558923		[learning rate: 0.0025433]
	Learning Rate: 0.00254331
	LOSS [training: 0.02253300534851902 | validation: 0.0215397054228234]
	TIME [epoch: 8.85 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019453064822114476		[learning rate: 0.0025394]
		[batch 20/20] avg loss: 0.012405012144371425		[learning rate: 0.0025355]
	Learning Rate: 0.00253552
	LOSS [training: 0.01592903848324295 | validation: 0.02041762708550783]
	TIME [epoch: 8.84 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040677916343050464		[learning rate: 0.0025316]
		[batch 20/20] avg loss: 0.016758923764918426		[learning rate: 0.0025277]
	Learning Rate: 0.00252774
	LOSS [training: 0.028718420053984443 | validation: 0.029343732774874922]
	TIME [epoch: 8.85 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02776838199493219		[learning rate: 0.0025239]
		[batch 20/20] avg loss: 0.018228642349709202		[learning rate: 0.00252]
	Learning Rate: 0.00252
	LOSS [training: 0.02299851217232069 | validation: 0.017570449522087295]
	TIME [epoch: 8.87 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014769639512604151		[learning rate: 0.0025161]
		[batch 20/20] avg loss: 0.025344390890374897		[learning rate: 0.0025123]
	Learning Rate: 0.00251227
	LOSS [training: 0.020057015201489524 | validation: 0.03884309708195933]
	TIME [epoch: 8.85 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026558551243679108		[learning rate: 0.0025084]
		[batch 20/20] avg loss: 0.04083078287294352		[learning rate: 0.0025046]
	Learning Rate: 0.00250457
	LOSS [training: 0.03369466705831131 | validation: 0.02061688687820263]
	TIME [epoch: 8.85 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03148269513662637		[learning rate: 0.0025007]
		[batch 20/20] avg loss: 0.016287048192496344		[learning rate: 0.0024969]
	Learning Rate: 0.00249689
	LOSS [training: 0.02388487166456136 | validation: 0.009341900457214114]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_952.pth
	Model improved!!!
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015330827093879933		[learning rate: 0.0024931]
		[batch 20/20] avg loss: 0.019252255641266362		[learning rate: 0.0024892]
	Learning Rate: 0.00248924
	LOSS [training: 0.017291541367573153 | validation: 0.02713023538507739]
	TIME [epoch: 8.84 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030653665937744518		[learning rate: 0.0024854]
		[batch 20/20] avg loss: 0.02243428550302386		[learning rate: 0.0024816]
	Learning Rate: 0.00248161
	LOSS [training: 0.026543975720384187 | validation: 0.022026476584383697]
	TIME [epoch: 8.86 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02469072247757609		[learning rate: 0.0024778]
		[batch 20/20] avg loss: 0.028396529525966618		[learning rate: 0.002474]
	Learning Rate: 0.002474
	LOSS [training: 0.026543626001771348 | validation: 0.047086905484204765]
	TIME [epoch: 8.84 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015492056530104024		[learning rate: 0.0024702]
		[batch 20/20] avg loss: 0.020228724670306057		[learning rate: 0.0024664]
	Learning Rate: 0.00246642
	LOSS [training: 0.017860390600205042 | validation: 0.020342257817016023]
	TIME [epoch: 8.84 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02135152619227288		[learning rate: 0.0024626]
		[batch 20/20] avg loss: 0.029814944890020095		[learning rate: 0.0024589]
	Learning Rate: 0.00245886
	LOSS [training: 0.025583235541146488 | validation: 0.031854151490803284]
	TIME [epoch: 8.83 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024864548551233424		[learning rate: 0.0024551]
		[batch 20/20] avg loss: 0.027262470495244072		[learning rate: 0.0024513]
	Learning Rate: 0.00245132
	LOSS [training: 0.02606350952323875 | validation: 0.03000167678683819]
	TIME [epoch: 8.86 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01349559937243878		[learning rate: 0.0024476]
		[batch 20/20] avg loss: 0.035665404451149435		[learning rate: 0.0024438]
	Learning Rate: 0.00244381
	LOSS [training: 0.02458050191179411 | validation: 0.01162731265464811]
	TIME [epoch: 8.84 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00945122253681445		[learning rate: 0.0024401]
		[batch 20/20] avg loss: 0.03651532601663811		[learning rate: 0.0024363]
	Learning Rate: 0.00243631
	LOSS [training: 0.02298327427672628 | validation: 0.04242530113376955]
	TIME [epoch: 8.84 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020134031791184438		[learning rate: 0.0024326]
		[batch 20/20] avg loss: 0.011677116302748324		[learning rate: 0.0024288]
	Learning Rate: 0.00242885
	LOSS [training: 0.015905574046966374 | validation: 0.01600093014696683]
	TIME [epoch: 8.85 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014623639212257214		[learning rate: 0.0024251]
		[batch 20/20] avg loss: 0.013333587934637039		[learning rate: 0.0024214]
	Learning Rate: 0.0024214
	LOSS [training: 0.013978613573447127 | validation: 0.01838974409111285]
	TIME [epoch: 8.86 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020624007696949033		[learning rate: 0.0024177]
		[batch 20/20] avg loss: 0.015507454536350457		[learning rate: 0.002414]
	Learning Rate: 0.00241398
	LOSS [training: 0.018065731116649743 | validation: 0.019107292712179064]
	TIME [epoch: 8.84 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029288843632185556		[learning rate: 0.0024103]
		[batch 20/20] avg loss: 0.01851360482846689		[learning rate: 0.0024066]
	Learning Rate: 0.00240658
	LOSS [training: 0.023901224230326222 | validation: 0.023056086429277214]
	TIME [epoch: 8.84 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033934061856242474		[learning rate: 0.0024029]
		[batch 20/20] avg loss: 0.019815558664765042		[learning rate: 0.0023992]
	Learning Rate: 0.0023992
	LOSS [training: 0.026874810260503756 | validation: 0.04592627971173843]
	TIME [epoch: 8.84 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026705671954079115		[learning rate: 0.0023955]
		[batch 20/20] avg loss: 0.025968130301347835		[learning rate: 0.0023918]
	Learning Rate: 0.00239185
	LOSS [training: 0.026336901127713475 | validation: 0.020267261822622208]
	TIME [epoch: 8.85 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011276139920353746		[learning rate: 0.0023882]
		[batch 20/20] avg loss: 0.009122696079671134		[learning rate: 0.0023845]
	Learning Rate: 0.00238451
	LOSS [training: 0.010199418000012441 | validation: 0.007924180868391188]
	TIME [epoch: 8.85 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_967.pth
	Model improved!!!
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0212968634775218		[learning rate: 0.0023809]
		[batch 20/20] avg loss: 0.018042579878033982		[learning rate: 0.0023772]
	Learning Rate: 0.00237721
	LOSS [training: 0.019669721677777887 | validation: 0.027690261409850127]
	TIME [epoch: 8.84 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010125220876432944		[learning rate: 0.0023736]
		[batch 20/20] avg loss: 0.010726960409495459		[learning rate: 0.0023699]
	Learning Rate: 0.00236992
	LOSS [training: 0.010426090642964202 | validation: 0.02613417914863201]
	TIME [epoch: 8.85 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02517992417630648		[learning rate: 0.0023663]
		[batch 20/20] avg loss: 0.02066809436454816		[learning rate: 0.0023627]
	Learning Rate: 0.00236265
	LOSS [training: 0.022924009270427318 | validation: 0.016903688497055078]
	TIME [epoch: 8.84 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023248700900522876		[learning rate: 0.002359]
		[batch 20/20] avg loss: 0.013091085009203196		[learning rate: 0.0023554]
	Learning Rate: 0.00235541
	LOSS [training: 0.018169892954863036 | validation: 0.01824154233313276]
	TIME [epoch: 8.86 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020212703088562302		[learning rate: 0.0023518]
		[batch 20/20] avg loss: 0.0081056651242049		[learning rate: 0.0023482]
	Learning Rate: 0.00234819
	LOSS [training: 0.014159184106383598 | validation: 0.008602441109900286]
	TIME [epoch: 8.84 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013591161698636356		[learning rate: 0.0023446]
		[batch 20/20] avg loss: 0.016683584540915695		[learning rate: 0.002341]
	Learning Rate: 0.00234099
	LOSS [training: 0.015137373119776023 | validation: 0.04153545304576095]
	TIME [epoch: 8.83 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017618862455986727		[learning rate: 0.0023374]
		[batch 20/20] avg loss: 0.012803983982586507		[learning rate: 0.0023338]
	Learning Rate: 0.00233382
	LOSS [training: 0.015211423219286618 | validation: 0.014437075133363666]
	TIME [epoch: 8.84 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02412455008248069		[learning rate: 0.0023302]
		[batch 20/20] avg loss: 0.010840837683010642		[learning rate: 0.0023267]
	Learning Rate: 0.00232666
	LOSS [training: 0.017482693882745665 | validation: 0.03132703341377168]
	TIME [epoch: 8.86 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01585353803299467		[learning rate: 0.0023231]
		[batch 20/20] avg loss: 0.030597087411691058		[learning rate: 0.0023195]
	Learning Rate: 0.00231953
	LOSS [training: 0.023225312722342868 | validation: 0.015987036948014958]
	TIME [epoch: 8.84 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009587033804147252		[learning rate: 0.002316]
		[batch 20/20] avg loss: 0.022217004114432243		[learning rate: 0.0023124]
	Learning Rate: 0.00231242
	LOSS [training: 0.015902018959289752 | validation: 0.017124976732769054]
	TIME [epoch: 8.84 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030798114300936864		[learning rate: 0.0023089]
		[batch 20/20] avg loss: 0.013482881675436351		[learning rate: 0.0023053]
	Learning Rate: 0.00230533
	LOSS [training: 0.022140497988186607 | validation: 0.01581737923650027]
	TIME [epoch: 8.84 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022879008958445024		[learning rate: 0.0023018]
		[batch 20/20] avg loss: 0.01308360248089141		[learning rate: 0.0022983]
	Learning Rate: 0.00229826
	LOSS [training: 0.01798130571966822 | validation: 0.033346360047859985]
	TIME [epoch: 8.86 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021841555991736587		[learning rate: 0.0022947]
		[batch 20/20] avg loss: 0.04145787647174691		[learning rate: 0.0022912]
	Learning Rate: 0.00229122
	LOSS [training: 0.03164971623174175 | validation: 0.04373154517221362]
	TIME [epoch: 8.85 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048184577121591604		[learning rate: 0.0022877]
		[batch 20/20] avg loss: 0.03009339870305936		[learning rate: 0.0022842]
	Learning Rate: 0.0022842
	LOSS [training: 0.039138987912325486 | validation: 0.025990994604588973]
	TIME [epoch: 8.84 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03526340388914927		[learning rate: 0.0022807]
		[batch 20/20] avg loss: 0.030519895621900946		[learning rate: 0.0022772]
	Learning Rate: 0.00227719
	LOSS [training: 0.03289164975552512 | validation: 0.024177000073106105]
	TIME [epoch: 8.84 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029447436273577505		[learning rate: 0.0022737]
		[batch 20/20] avg loss: 0.03524459582205269		[learning rate: 0.0022702]
	Learning Rate: 0.00227021
	LOSS [training: 0.0323460160478151 | validation: 0.036148701363817135]
	TIME [epoch: 8.84 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017032444765621872		[learning rate: 0.0022667]
		[batch 20/20] avg loss: 0.022023561060210837		[learning rate: 0.0022633]
	Learning Rate: 0.00226325
	LOSS [training: 0.019528002912916353 | validation: 0.016683300433650017]
	TIME [epoch: 8.87 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02150447346059791		[learning rate: 0.0022598]
		[batch 20/20] avg loss: 0.04016635481388209		[learning rate: 0.0022563]
	Learning Rate: 0.00225632
	LOSS [training: 0.030835414137239996 | validation: 0.040489443482080056]
	TIME [epoch: 8.84 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03956668021760109		[learning rate: 0.0022529]
		[batch 20/20] avg loss: 0.030424193405292482		[learning rate: 0.0022494]
	Learning Rate: 0.0022494
	LOSS [training: 0.034995436811446796 | validation: 0.02597650103599647]
	TIME [epoch: 8.84 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019007721846610707		[learning rate: 0.0022459]
		[batch 20/20] avg loss: 0.022835237692531714		[learning rate: 0.0022425]
	Learning Rate: 0.0022425
	LOSS [training: 0.020921479769571212 | validation: 0.04136270568746357]
	TIME [epoch: 8.84 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04314979992668276		[learning rate: 0.0022391]
		[batch 20/20] avg loss: 0.03812482899224061		[learning rate: 0.0022356]
	Learning Rate: 0.00223563
	LOSS [training: 0.040637314459461685 | validation: 0.027285528986256573]
	TIME [epoch: 8.86 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05293982966572981		[learning rate: 0.0022322]
		[batch 20/20] avg loss: 0.024506865859091613		[learning rate: 0.0022288]
	Learning Rate: 0.00222878
	LOSS [training: 0.03872334776241071 | validation: 0.041902518728347965]
	TIME [epoch: 8.84 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03146262905432134		[learning rate: 0.0022254]
		[batch 20/20] avg loss: 0.024411907070021086		[learning rate: 0.0022219]
	Learning Rate: 0.00222194
	LOSS [training: 0.027937268062171207 | validation: 0.029272504784273096]
	TIME [epoch: 8.84 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02345022473285551		[learning rate: 0.0022185]
		[batch 20/20] avg loss: 0.023103430946895837		[learning rate: 0.0022151]
	Learning Rate: 0.00221513
	LOSS [training: 0.023276827839875676 | validation: 0.012634452580830083]
	TIME [epoch: 8.84 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016995969833003825		[learning rate: 0.0022117]
		[batch 20/20] avg loss: 0.02712519025072372		[learning rate: 0.0022083]
	Learning Rate: 0.00220834
	LOSS [training: 0.022060580041863773 | validation: 0.024913241757237276]
	TIME [epoch: 8.86 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011360035216041318		[learning rate: 0.002205]
		[batch 20/20] avg loss: 0.009576181673787638		[learning rate: 0.0022016]
	Learning Rate: 0.00220157
	LOSS [training: 0.010468108444914475 | validation: 0.024688650724123136]
	TIME [epoch: 8.84 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01391736894621023		[learning rate: 0.0021982]
		[batch 20/20] avg loss: 0.0057322728532744486		[learning rate: 0.0021948]
	Learning Rate: 0.00219483
	LOSS [training: 0.009824820899742338 | validation: 0.013541827737155628]
	TIME [epoch: 8.83 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01924158751637004		[learning rate: 0.0021915]
		[batch 20/20] avg loss: 0.010116470372269874		[learning rate: 0.0021881]
	Learning Rate: 0.0021881
	LOSS [training: 0.014679028944319956 | validation: 0.019941978744475727]
	TIME [epoch: 8.84 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029135780400558208		[learning rate: 0.0021847]
		[batch 20/20] avg loss: 0.03560730999554755		[learning rate: 0.0021814]
	Learning Rate: 0.00218139
	LOSS [training: 0.03237154519805288 | validation: 0.03735515148260976]
	TIME [epoch: 8.85 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035076295544691195		[learning rate: 0.002178]
		[batch 20/20] avg loss: 0.03610074323448348		[learning rate: 0.0021747]
	Learning Rate: 0.0021747
	LOSS [training: 0.035588519389587334 | validation: 0.036876025256364006]
	TIME [epoch: 8.85 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028811662242469227		[learning rate: 0.0021714]
		[batch 20/20] avg loss: 0.03162254060837802		[learning rate: 0.002168]
	Learning Rate: 0.00216804
	LOSS [training: 0.030217101425423627 | validation: 0.03657272422433761]
	TIME [epoch: 8.84 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039289851476990614		[learning rate: 0.0021647]
		[batch 20/20] avg loss: 0.02445783612321348		[learning rate: 0.0021614]
	Learning Rate: 0.00216139
	LOSS [training: 0.03187384380010205 | validation: 0.018961375858370006]
	TIME [epoch: 8.84 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016458831583612173		[learning rate: 0.0021581]
		[batch 20/20] avg loss: 0.03744713183809737		[learning rate: 0.0021548]
	Learning Rate: 0.00215477
	LOSS [training: 0.02695298171085478 | validation: 0.031701158728443146]
	TIME [epoch: 8.84 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029061495779363716		[learning rate: 0.0021515]
		[batch 20/20] avg loss: 0.04531555388982682		[learning rate: 0.0021482]
	Learning Rate: 0.00214816
	LOSS [training: 0.03718852483459527 | validation: 0.024777525114968026]
	TIME [epoch: 8.86 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02611808714805619		[learning rate: 0.0021449]
		[batch 20/20] avg loss: 0.03581119271657214		[learning rate: 0.0021416]
	Learning Rate: 0.00214158
	LOSS [training: 0.03096463993231417 | validation: 0.048906268671422656]
	TIME [epoch: 8.84 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06692753917227687		[learning rate: 0.0021383]
		[batch 20/20] avg loss: 0.031444310401145766		[learning rate: 0.002135]
	Learning Rate: 0.00213501
	LOSS [training: 0.04918592478671133 | validation: 0.025370278941235565]
	TIME [epoch: 8.84 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04608161005156302		[learning rate: 0.0021317]
		[batch 20/20] avg loss: 0.07377842490582147		[learning rate: 0.0021285]
	Learning Rate: 0.00212847
	LOSS [training: 0.059930017478692234 | validation: 0.06574970626171406]
	TIME [epoch: 8.84 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07057062105493936		[learning rate: 0.0021252]
		[batch 20/20] avg loss: 0.07628224660635397		[learning rate: 0.0021219]
	Learning Rate: 0.00212194
	LOSS [training: 0.07342643383064666 | validation: 0.1709383256454448]
	TIME [epoch: 8.86 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10598392456376579		[learning rate: 0.0021187]
		[batch 20/20] avg loss: 0.07415623468603026		[learning rate: 0.0021154]
	Learning Rate: 0.00211544
	LOSS [training: 0.09007007962489802 | validation: 0.07138479406769932]
	TIME [epoch: 8.84 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05676179978410025		[learning rate: 0.0021122]
		[batch 20/20] avg loss: 0.04840670794922976		[learning rate: 0.002109]
	Learning Rate: 0.00210895
	LOSS [training: 0.052584253866665 | validation: 0.04529305639832702]
	TIME [epoch: 8.84 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06016094978959541		[learning rate: 0.0021057]
		[batch 20/20] avg loss: 0.058054458464875114		[learning rate: 0.0021025]
	Learning Rate: 0.00210249
	LOSS [training: 0.059107704127235275 | validation: 0.07694700922582547]
	TIME [epoch: 8.84 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06261825554850899		[learning rate: 0.0020993]
		[batch 20/20] avg loss: 0.06873937166338834		[learning rate: 0.002096]
	Learning Rate: 0.00209604
	LOSS [training: 0.06567881360594865 | validation: 0.06663471032895081]
	TIME [epoch: 8.85 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05781485825123075		[learning rate: 0.0020928]
		[batch 20/20] avg loss: 0.07771025102939645		[learning rate: 0.0020896]
	Learning Rate: 0.00208962
	LOSS [training: 0.06776255464031361 | validation: 0.07840006533600297]
	TIME [epoch: 8.84 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057463617951238076		[learning rate: 0.0020864]
		[batch 20/20] avg loss: 0.047540778430294424		[learning rate: 0.0020832]
	Learning Rate: 0.00208321
	LOSS [training: 0.05250219819076625 | validation: 0.04474685126813885]
	TIME [epoch: 8.83 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0645378674860671		[learning rate: 0.00208]
		[batch 20/20] avg loss: 0.06379523295965892		[learning rate: 0.0020768]
	Learning Rate: 0.00207683
	LOSS [training: 0.06416655022286302 | validation: 0.06439595733529943]
	TIME [epoch: 8.84 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05823421161896589		[learning rate: 0.0020736]
		[batch 20/20] avg loss: 0.03491132125663359		[learning rate: 0.0020705]
	Learning Rate: 0.00207046
	LOSS [training: 0.04657276643779975 | validation: 0.035981080874190184]
	TIME [epoch: 8.85 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032245801352990586		[learning rate: 0.0020673]
		[batch 20/20] avg loss: 0.03880972448811405		[learning rate: 0.0020641]
	Learning Rate: 0.00206411
	LOSS [training: 0.03552776292055231 | validation: 0.04670149495938106]
	TIME [epoch: 8.87 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03049074462733563		[learning rate: 0.0020609]
		[batch 20/20] avg loss: 0.045128367278750296		[learning rate: 0.0020578]
	Learning Rate: 0.00205778
	LOSS [training: 0.03780955595304298 | validation: 0.03821952181181733]
	TIME [epoch: 8.84 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03369104031121893		[learning rate: 0.0020546]
		[batch 20/20] avg loss: 0.03244217937694767		[learning rate: 0.0020515]
	Learning Rate: 0.00205148
	LOSS [training: 0.03306660984408329 | validation: 0.057282697752444]
	TIME [epoch: 8.84 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03602344845903982		[learning rate: 0.0020483]
		[batch 20/20] avg loss: 0.0322855490488416		[learning rate: 0.0020452]
	Learning Rate: 0.00204519
	LOSS [training: 0.03415449875394071 | validation: 0.026524380509749708]
	TIME [epoch: 8.84 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024503202709120746		[learning rate: 0.0020421]
		[batch 20/20] avg loss: 0.023099386105034092		[learning rate: 0.0020389]
	Learning Rate: 0.00203892
	LOSS [training: 0.023801294407077418 | validation: 0.030100838293043376]
	TIME [epoch: 8.86 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027410407104334584		[learning rate: 0.0020358]
		[batch 20/20] avg loss: 0.019210212409352053		[learning rate: 0.0020327]
	Learning Rate: 0.00203267
	LOSS [training: 0.02331030975684332 | validation: 0.021101838751834617]
	TIME [epoch: 8.84 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019012662368075143		[learning rate: 0.0020296]
		[batch 20/20] avg loss: 0.024692106727421802		[learning rate: 0.0020264]
	Learning Rate: 0.00202644
	LOSS [training: 0.021852384547748474 | validation: 0.018242084923889606]
	TIME [epoch: 8.84 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016120618827987536		[learning rate: 0.0020233]
		[batch 20/20] avg loss: 0.021703558206834974		[learning rate: 0.0020202]
	Learning Rate: 0.00202023
	LOSS [training: 0.018912088517411257 | validation: 0.021209165357052404]
	TIME [epoch: 8.84 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021979799551785827		[learning rate: 0.0020171]
		[batch 20/20] avg loss: 0.015773381561807943		[learning rate: 0.002014]
	Learning Rate: 0.00201403
	LOSS [training: 0.018876590556796885 | validation: 0.02043216389288364]
	TIME [epoch: 8.85 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02038345477979365		[learning rate: 0.0020109]
		[batch 20/20] avg loss: 0.03611421750666806		[learning rate: 0.0020079]
	Learning Rate: 0.00200786
	LOSS [training: 0.028248836143230854 | validation: 0.03270105748051865]
	TIME [epoch: 8.84 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031077757339422567		[learning rate: 0.0020048]
		[batch 20/20] avg loss: 0.02161364607629384		[learning rate: 0.0020017]
	Learning Rate: 0.0020017
	LOSS [training: 0.02634570170785821 | validation: 0.04549310348594866]
	TIME [epoch: 8.84 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02589700578785991		[learning rate: 0.0019986]
		[batch 20/20] avg loss: 0.025962413764511926		[learning rate: 0.0019956]
	Learning Rate: 0.00199557
	LOSS [training: 0.025929709776185916 | validation: 0.029809183579960857]
	TIME [epoch: 8.84 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03259697155162567		[learning rate: 0.0019925]
		[batch 20/20] avg loss: 0.01987461765217235		[learning rate: 0.0019895]
	Learning Rate: 0.00198945
	LOSS [training: 0.02623579460189901 | validation: 0.021297259131962593]
	TIME [epoch: 8.85 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01965594769898825		[learning rate: 0.0019864]
		[batch 20/20] avg loss: 0.018588729503995156		[learning rate: 0.0019834]
	Learning Rate: 0.00198335
	LOSS [training: 0.019122338601491703 | validation: 0.026404974421900186]
	TIME [epoch: 8.86 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018465492043310796		[learning rate: 0.0019803]
		[batch 20/20] avg loss: 0.022522363447067158		[learning rate: 0.0019773]
	Learning Rate: 0.00197727
	LOSS [training: 0.02049392774518898 | validation: 0.0214123103996725]
	TIME [epoch: 8.84 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011466282732872464		[learning rate: 0.0019742]
		[batch 20/20] avg loss: 0.019556195703666453		[learning rate: 0.0019712]
	Learning Rate: 0.00197121
	LOSS [training: 0.015511239218269463 | validation: 0.03697984166687972]
	TIME [epoch: 8.84 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019263032094637148		[learning rate: 0.0019682]
		[batch 20/20] avg loss: 0.017335573565862142		[learning rate: 0.0019652]
	Learning Rate: 0.00196517
	LOSS [training: 0.01829930283024965 | validation: 0.023176739880896725]
	TIME [epoch: 8.84 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020791538250704424		[learning rate: 0.0019622]
		[batch 20/20] avg loss: 0.017485669534336647		[learning rate: 0.0019591]
	Learning Rate: 0.00195915
	LOSS [training: 0.01913860389252053 | validation: 0.03669909106279049]
	TIME [epoch: 8.86 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021953870042806085		[learning rate: 0.0019561]
		[batch 20/20] avg loss: 0.012316523722570312		[learning rate: 0.0019531]
	Learning Rate: 0.00195314
	LOSS [training: 0.017135196882688195 | validation: 0.0165232779582844]
	TIME [epoch: 8.85 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01149946149213867		[learning rate: 0.0019501]
		[batch 20/20] avg loss: 0.02032719846974589		[learning rate: 0.0019472]
	Learning Rate: 0.00194715
	LOSS [training: 0.015913329980942283 | validation: 0.033557624778942016]
	TIME [epoch: 8.84 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04243551025161455		[learning rate: 0.0019442]
		[batch 20/20] avg loss: 0.028654166125294984		[learning rate: 0.0019412]
	Learning Rate: 0.00194118
	LOSS [training: 0.03554483818845476 | validation: 0.028691306482885606]
	TIME [epoch: 8.84 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023645747580191164		[learning rate: 0.0019382]
		[batch 20/20] avg loss: 0.01607384819276058		[learning rate: 0.0019352]
	Learning Rate: 0.00193523
	LOSS [training: 0.019859797886475874 | validation: 0.024184540767988816]
	TIME [epoch: 8.86 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03058120209604049		[learning rate: 0.0019323]
		[batch 20/20] avg loss: 0.013954449120582618		[learning rate: 0.0019293]
	Learning Rate: 0.0019293
	LOSS [training: 0.022267825608311555 | validation: 0.01766840062111426]
	TIME [epoch: 8.84 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016046818071359378		[learning rate: 0.0019263]
		[batch 20/20] avg loss: 0.016470281715103		[learning rate: 0.0019234]
	Learning Rate: 0.00192339
	LOSS [training: 0.016258549893231188 | validation: 0.01869928559256743]
	TIME [epoch: 8.84 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006855739027153457		[learning rate: 0.0019204]
		[batch 20/20] avg loss: 0.015701776921617956		[learning rate: 0.0019175]
	Learning Rate: 0.00191749
	LOSS [training: 0.011278757974385708 | validation: 0.023374817374625018]
	TIME [epoch: 8.84 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008872894702496454		[learning rate: 0.0019145]
		[batch 20/20] avg loss: 0.0090092608547687		[learning rate: 0.0019116]
	Learning Rate: 0.00191161
	LOSS [training: 0.008941077778632577 | validation: 0.015339183001876523]
	TIME [epoch: 8.86 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010690982229674498		[learning rate: 0.0019087]
		[batch 20/20] avg loss: 0.008844757142195406		[learning rate: 0.0019058]
	Learning Rate: 0.00190575
	LOSS [training: 0.009767869685934951 | validation: 0.018593201137759224]
	TIME [epoch: 8.84 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010551514697019735		[learning rate: 0.0019028]
		[batch 20/20] avg loss: 0.011197798139134529		[learning rate: 0.0018999]
	Learning Rate: 0.00189991
	LOSS [training: 0.010874656418077132 | validation: 0.01424404481152123]
	TIME [epoch: 8.84 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03968418911319152		[learning rate: 0.001897]
		[batch 20/20] avg loss: 0.03613354419204718		[learning rate: 0.0018941]
	Learning Rate: 0.00189409
	LOSS [training: 0.03790886665261937 | validation: 0.017014117250425224]
	TIME [epoch: 8.84 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01102736237404836		[learning rate: 0.0018912]
		[batch 20/20] avg loss: 0.011969985689207688		[learning rate: 0.0018883]
	Learning Rate: 0.00188828
	LOSS [training: 0.011498674031628025 | validation: 0.024582144366411804]
	TIME [epoch: 8.84 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012684522262861564		[learning rate: 0.0018854]
		[batch 20/20] avg loss: 0.027918067122969337		[learning rate: 0.0018825]
	Learning Rate: 0.00188249
	LOSS [training: 0.02030129469291545 | validation: 0.015899522192485624]
	TIME [epoch: 8.86 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014832292930147636		[learning rate: 0.0018796]
		[batch 20/20] avg loss: 0.012469051223191845		[learning rate: 0.0018767]
	Learning Rate: 0.00187672
	LOSS [training: 0.013650672076669743 | validation: 0.018550912565181633]
	TIME [epoch: 8.84 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019975778560052955		[learning rate: 0.0018738]
		[batch 20/20] avg loss: 0.013104543534901775		[learning rate: 0.001871]
	Learning Rate: 0.00187097
	LOSS [training: 0.016540161047477363 | validation: 0.00549395625915867]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1046.pth
	Model improved!!!
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011506928087872258		[learning rate: 0.0018681]
		[batch 20/20] avg loss: 0.012595115974957109		[learning rate: 0.0018652]
	Learning Rate: 0.00186523
	LOSS [training: 0.012051022031414681 | validation: 0.015731108187822185]
	TIME [epoch: 8.85 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00930092845319375		[learning rate: 0.0018624]
		[batch 20/20] avg loss: 0.015047857162320594		[learning rate: 0.0018595]
	Learning Rate: 0.00185952
	LOSS [training: 0.01217439280775717 | validation: 0.02211964912087838]
	TIME [epoch: 8.87 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012540400764256434		[learning rate: 0.0018567]
		[batch 20/20] avg loss: 0.020984392951548064		[learning rate: 0.0018538]
	Learning Rate: 0.00185382
	LOSS [training: 0.016762396857902248 | validation: 0.025806006253472832]
	TIME [epoch: 8.84 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032804715333561135		[learning rate: 0.001851]
		[batch 20/20] avg loss: 0.023982862483788715		[learning rate: 0.0018481]
	Learning Rate: 0.00184813
	LOSS [training: 0.028393788908674927 | validation: 0.02783578550125245]
	TIME [epoch: 8.85 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015555879395954314		[learning rate: 0.0018453]
		[batch 20/20] avg loss: 0.030869029329349505		[learning rate: 0.0018425]
	Learning Rate: 0.00184247
	LOSS [training: 0.02321245436265191 | validation: 0.05996108327033969]
	TIME [epoch: 8.84 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07345234061499063		[learning rate: 0.0018396]
		[batch 20/20] avg loss: 0.04827406342390134		[learning rate: 0.0018368]
	Learning Rate: 0.00183682
	LOSS [training: 0.060863202019445985 | validation: 0.04365622481921908]
	TIME [epoch: 8.86 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049694891084619		[learning rate: 0.001834]
		[batch 20/20] avg loss: 0.04291019603549369		[learning rate: 0.0018312]
	Learning Rate: 0.00183119
	LOSS [training: 0.04630254356005634 | validation: 0.03760433530256766]
	TIME [epoch: 8.85 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046125282376589166		[learning rate: 0.0018284]
		[batch 20/20] avg loss: 0.03622793978192754		[learning rate: 0.0018256]
	Learning Rate: 0.00182558
	LOSS [training: 0.041176611079258345 | validation: 0.03945562597746835]
	TIME [epoch: 8.83 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04556741668060581		[learning rate: 0.0018228]
		[batch 20/20] avg loss: 0.031210988804978208		[learning rate: 0.00182]
	Learning Rate: 0.00181998
	LOSS [training: 0.03838920274279201 | validation: 0.025341782280189303]
	TIME [epoch: 8.83 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04424354285587312		[learning rate: 0.0018172]
		[batch 20/20] avg loss: 0.029261565884681866		[learning rate: 0.0018144]
	Learning Rate: 0.0018144
	LOSS [training: 0.03675255437027749 | validation: 0.030623912306524805]
	TIME [epoch: 8.85 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027047754592318042		[learning rate: 0.0018116]
		[batch 20/20] avg loss: 0.04129937498322134		[learning rate: 0.0018088]
	Learning Rate: 0.00180884
	LOSS [training: 0.03417356478776969 | validation: 0.03954800129970086]
	TIME [epoch: 8.85 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0326930828864191		[learning rate: 0.0018061]
		[batch 20/20] avg loss: 0.038070899606069436		[learning rate: 0.0018033]
	Learning Rate: 0.00180329
	LOSS [training: 0.03538199124624427 | validation: 0.03970703814056177]
	TIME [epoch: 8.84 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04117939670761674		[learning rate: 0.0018005]
		[batch 20/20] avg loss: 0.03287377626772627		[learning rate: 0.0017978]
	Learning Rate: 0.00179777
	LOSS [training: 0.0370265864876715 | validation: 0.03162734469099503]
	TIME [epoch: 8.83 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03920463110639159		[learning rate: 0.001795]
		[batch 20/20] avg loss: 0.06310595169077177		[learning rate: 0.0017923]
	Learning Rate: 0.00179226
	LOSS [training: 0.05115529139858167 | validation: 0.06455425503611875]
	TIME [epoch: 8.84 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06600548272350895		[learning rate: 0.0017895]
		[batch 20/20] avg loss: 0.03973936620387312		[learning rate: 0.0017868]
	Learning Rate: 0.00178676
	LOSS [training: 0.05287242446369104 | validation: 0.04075341105657843]
	TIME [epoch: 8.86 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04684873731838353		[learning rate: 0.001784]
		[batch 20/20] avg loss: 0.03381723607422734		[learning rate: 0.0017813]
	Learning Rate: 0.00178128
	LOSS [training: 0.04033298669630543 | validation: 0.039893696874403514]
	TIME [epoch: 8.84 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039075042460115054		[learning rate: 0.0017786]
		[batch 20/20] avg loss: 0.032667832896240395		[learning rate: 0.0017758]
	Learning Rate: 0.00177582
	LOSS [training: 0.03587143767817773 | validation: 0.04241340683032971]
	TIME [epoch: 8.84 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03329739411716146		[learning rate: 0.0017731]
		[batch 20/20] avg loss: 0.039769216935357746		[learning rate: 0.0017704]
	Learning Rate: 0.00177038
	LOSS [training: 0.036533305526259593 | validation: 0.0367777103971224]
	TIME [epoch: 8.84 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028982302412764293		[learning rate: 0.0017677]
		[batch 20/20] avg loss: 0.043489387027546865		[learning rate: 0.001765]
	Learning Rate: 0.00176495
	LOSS [training: 0.036235844720155584 | validation: 0.02681568452620409]
	TIME [epoch: 8.86 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02507744496947629		[learning rate: 0.0017622]
		[batch 20/20] avg loss: 0.030368962563711598		[learning rate: 0.0017595]
	Learning Rate: 0.00175954
	LOSS [training: 0.027723203766593946 | validation: 0.028061829236520328]
	TIME [epoch: 8.84 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026387217304357457		[learning rate: 0.0017568]
		[batch 20/20] avg loss: 0.031038797879518642		[learning rate: 0.0017541]
	Learning Rate: 0.00175415
	LOSS [training: 0.028713007591938056 | validation: 0.03993821697423306]
	TIME [epoch: 8.84 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042035156479905925		[learning rate: 0.0017515]
		[batch 20/20] avg loss: 0.036398783328148124		[learning rate: 0.0017488]
	Learning Rate: 0.00174877
	LOSS [training: 0.039216969904027024 | validation: 0.046652590963972604]
	TIME [epoch: 8.84 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03354291987240556		[learning rate: 0.0017461]
		[batch 20/20] avg loss: 0.03139359131780271		[learning rate: 0.0017434]
	Learning Rate: 0.00174341
	LOSS [training: 0.03246825559510414 | validation: 0.03744435791974985]
	TIME [epoch: 8.86 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025844367305786688		[learning rate: 0.0017407]
		[batch 20/20] avg loss: 0.028068672557259977		[learning rate: 0.0017381]
	Learning Rate: 0.00173807
	LOSS [training: 0.026956519931523336 | validation: 0.01499737968971548]
	TIME [epoch: 8.85 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018986531592783334		[learning rate: 0.0017354]
		[batch 20/20] avg loss: 0.016363112031625205		[learning rate: 0.0017327]
	Learning Rate: 0.00173274
	LOSS [training: 0.017674821812204266 | validation: 0.021778982630758642]
	TIME [epoch: 8.84 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017023990150836676		[learning rate: 0.0017301]
		[batch 20/20] avg loss: 0.018043624777789192		[learning rate: 0.0017274]
	Learning Rate: 0.00172743
	LOSS [training: 0.017533807464312934 | validation: 0.02609963764303972]
	TIME [epoch: 8.84 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021889390252346		[learning rate: 0.0017248]
		[batch 20/20] avg loss: 0.017267214672196045		[learning rate: 0.0017221]
	Learning Rate: 0.00172213
	LOSS [training: 0.019578302462271026 | validation: 0.03685872889501417]
	TIME [epoch: 8.84 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023821410336091094		[learning rate: 0.0017195]
		[batch 20/20] avg loss: 0.020102234736769285		[learning rate: 0.0017169]
	Learning Rate: 0.00171685
	LOSS [training: 0.02196182253643019 | validation: 0.024037076066824682]
	TIME [epoch: 8.86 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018298832727311216		[learning rate: 0.0017142]
		[batch 20/20] avg loss: 0.02773954211035687		[learning rate: 0.0017116]
	Learning Rate: 0.00171159
	LOSS [training: 0.02301918741883404 | validation: 0.034650264503329656]
	TIME [epoch: 8.84 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0312609680292686		[learning rate: 0.001709]
		[batch 20/20] avg loss: 0.05733583649779163		[learning rate: 0.0017063]
	Learning Rate: 0.00170634
	LOSS [training: 0.044298402263530115 | validation: 0.04337640108994641]
	TIME [epoch: 8.84 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0360912607670554		[learning rate: 0.0017037]
		[batch 20/20] avg loss: 0.03661463339999431		[learning rate: 0.0017011]
	Learning Rate: 0.00170111
	LOSS [training: 0.03635294708352486 | validation: 0.040363960602404575]
	TIME [epoch: 8.84 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023839772334156788		[learning rate: 0.0016985]
		[batch 20/20] avg loss: 0.03314684833790779		[learning rate: 0.0016959]
	Learning Rate: 0.0016959
	LOSS [training: 0.02849331033603229 | validation: 0.026899295115550656]
	TIME [epoch: 8.86 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035792385961617976		[learning rate: 0.0016933]
		[batch 20/20] avg loss: 0.029653306915189342		[learning rate: 0.0016907]
	Learning Rate: 0.0016907
	LOSS [training: 0.03272284643840366 | validation: 0.04037571750470231]
	TIME [epoch: 8.84 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030478425678717135		[learning rate: 0.0016881]
		[batch 20/20] avg loss: 0.06688104217991792		[learning rate: 0.0016855]
	Learning Rate: 0.00168552
	LOSS [training: 0.04867973392931753 | validation: 0.1105500782083591]
	TIME [epoch: 8.84 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08889554570794309		[learning rate: 0.0016829]
		[batch 20/20] avg loss: 0.07215912005525557		[learning rate: 0.0016804]
	Learning Rate: 0.00168035
	LOSS [training: 0.08052733288159933 | validation: 0.058398886852322585]
	TIME [epoch: 8.84 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05401162058449364		[learning rate: 0.0016778]
		[batch 20/20] avg loss: 0.14107521598102157		[learning rate: 0.0016752]
	Learning Rate: 0.0016752
	LOSS [training: 0.0975434182827576 | validation: 0.13939445172050852]
	TIME [epoch: 8.85 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15947976661152044		[learning rate: 0.0016726]
		[batch 20/20] avg loss: 0.11903192809759737		[learning rate: 0.0016701]
	Learning Rate: 0.00167006
	LOSS [training: 0.1392558473545589 | validation: 0.09117245666122517]
	TIME [epoch: 8.84 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10149914785740133		[learning rate: 0.0016675]
		[batch 20/20] avg loss: 0.11347042098210432		[learning rate: 0.0016649]
	Learning Rate: 0.00166495
	LOSS [training: 0.10748478441975282 | validation: 0.09038661848275582]
	TIME [epoch: 8.84 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09210296524097791		[learning rate: 0.0016624]
		[batch 20/20] avg loss: 0.10647493559496654		[learning rate: 0.0016598]
	Learning Rate: 0.00165984
	LOSS [training: 0.09928895041797223 | validation: 0.11325654020739587]
	TIME [epoch: 8.84 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10344479948887966		[learning rate: 0.0016573]
		[batch 20/20] avg loss: 0.07839515560397074		[learning rate: 0.0016548]
	Learning Rate: 0.00165475
	LOSS [training: 0.0909199775464252 | validation: 0.08278835407299971]
	TIME [epoch: 8.85 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07755528695736619		[learning rate: 0.0016522]
		[batch 20/20] avg loss: 0.07842667840110946		[learning rate: 0.0016497]
	Learning Rate: 0.00164968
	LOSS [training: 0.0779909826792378 | validation: 0.0757143575670252]
	TIME [epoch: 8.85 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06857767611947416		[learning rate: 0.0016472]
		[batch 20/20] avg loss: 0.04486566285028359		[learning rate: 0.0016446]
	Learning Rate: 0.00164462
	LOSS [training: 0.056721669484878876 | validation: 0.0415203338979913]
	TIME [epoch: 8.84 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04898564529096737		[learning rate: 0.0016421]
		[batch 20/20] avg loss: 0.04912024826361763		[learning rate: 0.0016396]
	Learning Rate: 0.00163958
	LOSS [training: 0.04905294677729251 | validation: 0.07158365070650108]
	TIME [epoch: 8.84 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055872910609473206		[learning rate: 0.0016371]
		[batch 20/20] avg loss: 0.038729894324057254		[learning rate: 0.0016346]
	Learning Rate: 0.00163456
	LOSS [training: 0.04730140246676523 | validation: 0.042255867224933935]
	TIME [epoch: 8.84 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04464286878880174		[learning rate: 0.001632]
		[batch 20/20] avg loss: 0.05411657949258357		[learning rate: 0.0016295]
	Learning Rate: 0.00162955
	LOSS [training: 0.049379724140692666 | validation: 0.03415238974808661]
	TIME [epoch: 8.86 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03997761779989144		[learning rate: 0.001627]
		[batch 20/20] avg loss: 0.04720235865154384		[learning rate: 0.0016246]
	Learning Rate: 0.00162455
	LOSS [training: 0.043589988225717635 | validation: 0.06173150393225893]
	TIME [epoch: 8.84 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05815711764944528		[learning rate: 0.0016221]
		[batch 20/20] avg loss: 0.054760798101299246		[learning rate: 0.0016196]
	Learning Rate: 0.00161957
	LOSS [training: 0.056458957875372265 | validation: 0.06731358803314957]
	TIME [epoch: 8.84 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04838624584221034		[learning rate: 0.0016171]
		[batch 20/20] avg loss: 0.04780951445267988		[learning rate: 0.0016146]
	Learning Rate: 0.00161461
	LOSS [training: 0.04809788014744511 | validation: 0.04279998566126832]
	TIME [epoch: 8.84 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04915290121755926		[learning rate: 0.0016121]
		[batch 20/20] avg loss: 0.06745287206631671		[learning rate: 0.0016097]
	Learning Rate: 0.00160966
	LOSS [training: 0.05830288664193799 | validation: 0.07555809902009719]
	TIME [epoch: 8.86 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05263009452412413		[learning rate: 0.0016072]
		[batch 20/20] avg loss: 0.04602686618331524		[learning rate: 0.0016047]
	Learning Rate: 0.00160472
	LOSS [training: 0.049328480353719686 | validation: 0.05190250644004083]
	TIME [epoch: 8.84 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039626315235515144		[learning rate: 0.0016023]
		[batch 20/20] avg loss: 0.0510040579057501		[learning rate: 0.0015998]
	Learning Rate: 0.0015998
	LOSS [training: 0.04531518657063262 | validation: 0.0668022778739099]
	TIME [epoch: 8.84 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06999823805047826		[learning rate: 0.0015973]
		[batch 20/20] avg loss: 0.054880888732615964		[learning rate: 0.0015949]
	Learning Rate: 0.0015949
	LOSS [training: 0.062439563391547104 | validation: 0.052557671266420775]
	TIME [epoch: 8.84 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051918275826909596		[learning rate: 0.0015925]
		[batch 20/20] avg loss: 0.04552211581528896		[learning rate: 0.00159]
	Learning Rate: 0.00159001
	LOSS [training: 0.04872019582109929 | validation: 0.04341741233291862]
	TIME [epoch: 8.86 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04968259633566486		[learning rate: 0.0015876]
		[batch 20/20] avg loss: 0.047863973141754515		[learning rate: 0.0015851]
	Learning Rate: 0.00158514
	LOSS [training: 0.04877328473870968 | validation: 0.04293487051592746]
	TIME [epoch: 8.84 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04092659801678882		[learning rate: 0.0015827]
		[batch 20/20] avg loss: 0.04313735545062859		[learning rate: 0.0015803]
	Learning Rate: 0.00158028
	LOSS [training: 0.04203197673370871 | validation: 0.060636645649263826]
	TIME [epoch: 8.85 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05426682163678846		[learning rate: 0.0015779]
		[batch 20/20] avg loss: 0.05240455367644201		[learning rate: 0.0015754]
	Learning Rate: 0.00157543
	LOSS [training: 0.05333568765661524 | validation: 0.05523813209156882]
	TIME [epoch: 8.84 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04335045722435628		[learning rate: 0.001573]
		[batch 20/20] avg loss: 0.04115425047562878		[learning rate: 0.0015706]
	Learning Rate: 0.0015706
	LOSS [training: 0.04225235384999253 | validation: 0.040562447732214665]
	TIME [epoch: 8.84 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030952153376003683		[learning rate: 0.0015682]
		[batch 20/20] avg loss: 0.03960339303563893		[learning rate: 0.0015658]
	Learning Rate: 0.00156579
	LOSS [training: 0.03527777320582131 | validation: 0.046191854136443236]
	TIME [epoch: 8.86 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034775371614742		[learning rate: 0.0015634]
		[batch 20/20] avg loss: 0.026735150438500176		[learning rate: 0.001561]
	Learning Rate: 0.00156099
	LOSS [training: 0.03075526102662109 | validation: 0.03800859878076488]
	TIME [epoch: 8.84 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04900223989586855		[learning rate: 0.0015586]
		[batch 20/20] avg loss: 0.030549652032958653		[learning rate: 0.0015562]
	Learning Rate: 0.0015562
	LOSS [training: 0.03977594596441361 | validation: 0.032109249995414396]
	TIME [epoch: 8.84 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04097782261976955		[learning rate: 0.0015538]
		[batch 20/20] avg loss: 0.036959329169671776		[learning rate: 0.0015514]
	Learning Rate: 0.00155143
	LOSS [training: 0.03896857589472067 | validation: 0.03755606745228217]
	TIME [epoch: 8.84 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02690291056416851		[learning rate: 0.0015491]
		[batch 20/20] avg loss: 0.04939194693138723		[learning rate: 0.0015467]
	Learning Rate: 0.00154668
	LOSS [training: 0.038147428747777865 | validation: 0.042020909317169224]
	TIME [epoch: 8.87 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03881685249651151		[learning rate: 0.0015443]
		[batch 20/20] avg loss: 0.04873815254272275		[learning rate: 0.0015419]
	Learning Rate: 0.00154194
	LOSS [training: 0.04377750251961713 | validation: 0.04185982873772768]
	TIME [epoch: 8.84 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04241012382198795		[learning rate: 0.0015396]
		[batch 20/20] avg loss: 0.041056985392773075		[learning rate: 0.0015372]
	Learning Rate: 0.00153721
	LOSS [training: 0.04173355460738051 | validation: 0.022546699497514186]
	TIME [epoch: 8.84 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021765840661286413		[learning rate: 0.0015349]
		[batch 20/20] avg loss: 0.023533674720017883		[learning rate: 0.0015325]
	Learning Rate: 0.0015325
	LOSS [training: 0.022649757690652146 | validation: 0.022293667103258752]
	TIME [epoch: 8.84 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024262656195787758		[learning rate: 0.0015301]
		[batch 20/20] avg loss: 0.02297201725518678		[learning rate: 0.0015278]
	Learning Rate: 0.0015278
	LOSS [training: 0.023617336725487263 | validation: 0.028599302294822895]
	TIME [epoch: 8.86 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020024365093984555		[learning rate: 0.0015255]
		[batch 20/20] avg loss: 0.021549180459508994		[learning rate: 0.0015231]
	Learning Rate: 0.00152312
	LOSS [training: 0.02078677277674678 | validation: 0.009878515534599824]
	TIME [epoch: 8.85 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02157939574727097		[learning rate: 0.0015208]
		[batch 20/20] avg loss: 0.019648501512091278		[learning rate: 0.0015184]
	Learning Rate: 0.00151845
	LOSS [training: 0.020613948629681123 | validation: 0.02354142625886648]
	TIME [epoch: 8.84 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02445226215252035		[learning rate: 0.0015161]
		[batch 20/20] avg loss: 0.012935324476249486		[learning rate: 0.0015138]
	Learning Rate: 0.00151379
	LOSS [training: 0.01869379331438491 | validation: 0.0033685489715781867]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1115.pth
	Model improved!!!
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006076212375376198		[learning rate: 0.0015115]
		[batch 20/20] avg loss: 0.030057397809867124		[learning rate: 0.0015092]
	Learning Rate: 0.00150915
	LOSS [training: 0.01806680509262166 | validation: 0.025180831946253146]
	TIME [epoch: 8.85 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023662926885606086		[learning rate: 0.0015068]
		[batch 20/20] avg loss: 0.021742752742609117		[learning rate: 0.0015045]
	Learning Rate: 0.00150453
	LOSS [training: 0.022702839814107603 | validation: 0.022207282212342742]
	TIME [epoch: 8.85 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01581738084435333		[learning rate: 0.0015022]
		[batch 20/20] avg loss: 0.020596746641796337		[learning rate: 0.0014999]
	Learning Rate: 0.00149991
	LOSS [training: 0.018207063743074826 | validation: 0.016198279563703774]
	TIME [epoch: 8.84 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022515748049968486		[learning rate: 0.0014976]
		[batch 20/20] avg loss: 0.03945368709125806		[learning rate: 0.0014953]
	Learning Rate: 0.00149532
	LOSS [training: 0.030984717570613267 | validation: 0.03371207471111199]
	TIME [epoch: 8.83 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047510315708798115		[learning rate: 0.001493]
		[batch 20/20] avg loss: 0.039775916901375684		[learning rate: 0.0014907]
	Learning Rate: 0.00149073
	LOSS [training: 0.043643116305086896 | validation: 0.03235365206787008]
	TIME [epoch: 8.83 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020714770926893597		[learning rate: 0.0014884]
		[batch 20/20] avg loss: 0.021704143085015327		[learning rate: 0.0014862]
	Learning Rate: 0.00148616
	LOSS [training: 0.021209457005954457 | validation: 0.024464849759327986]
	TIME [epoch: 8.86 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01806818365482859		[learning rate: 0.0014839]
		[batch 20/20] avg loss: 0.019954817936616773		[learning rate: 0.0014816]
	Learning Rate: 0.00148161
	LOSS [training: 0.019011500795722683 | validation: 0.0268921784964002]
	TIME [epoch: 8.84 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021700767350208462		[learning rate: 0.0014793]
		[batch 20/20] avg loss: 0.018856283847036893		[learning rate: 0.0014771]
	Learning Rate: 0.00147707
	LOSS [training: 0.020278525598622678 | validation: 0.01758027564263165]
	TIME [epoch: 8.83 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019633111592480774		[learning rate: 0.0014748]
		[batch 20/20] avg loss: 0.018163537037608188		[learning rate: 0.0014725]
	Learning Rate: 0.00147254
	LOSS [training: 0.01889832431504448 | validation: 0.038284649767532375]
	TIME [epoch: 8.83 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026467120482466418		[learning rate: 0.0014703]
		[batch 20/20] avg loss: 0.022814299650914274		[learning rate: 0.001468]
	Learning Rate: 0.00146802
	LOSS [training: 0.024640710066690347 | validation: 0.025906045264974767]
	TIME [epoch: 8.85 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013368229918105998		[learning rate: 0.0014658]
		[batch 20/20] avg loss: 0.009240384878195588		[learning rate: 0.0014635]
	Learning Rate: 0.00146352
	LOSS [training: 0.011304307398150793 | validation: 0.024224109144941868]
	TIME [epoch: 8.84 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012013089453076001		[learning rate: 0.0014613]
		[batch 20/20] avg loss: 0.009531125123761676		[learning rate: 0.001459]
	Learning Rate: 0.00145904
	LOSS [training: 0.010772107288418842 | validation: 0.013662684125298048]
	TIME [epoch: 8.83 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014335660170647893		[learning rate: 0.0014568]
		[batch 20/20] avg loss: 0.008232821246062911		[learning rate: 0.0014546]
	Learning Rate: 0.00145457
	LOSS [training: 0.011284240708355407 | validation: 0.020177626524958338]
	TIME [epoch: 8.84 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015335643537377402		[learning rate: 0.0014523]
		[batch 20/20] avg loss: 0.016227609067089983		[learning rate: 0.0014501]
	Learning Rate: 0.00145011
	LOSS [training: 0.01578162630223369 | validation: 0.024906095787772912]
	TIME [epoch: 8.84 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01547531415492912		[learning rate: 0.0014479]
		[batch 20/20] avg loss: 0.015095132034413506		[learning rate: 0.0014457]
	Learning Rate: 0.00144566
	LOSS [training: 0.015285223094671316 | validation: 0.009087598206056537]
	TIME [epoch: 8.84 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0061664642371044905		[learning rate: 0.0014434]
		[batch 20/20] avg loss: 0.009784329579431806		[learning rate: 0.0014412]
	Learning Rate: 0.00144123
	LOSS [training: 0.007975396908268149 | validation: 0.010750838042701197]
	TIME [epoch: 8.83 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014053959433621472		[learning rate: 0.001439]
		[batch 20/20] avg loss: 0.00845787379014335		[learning rate: 0.0014368]
	Learning Rate: 0.00143681
	LOSS [training: 0.011255916611882412 | validation: 0.008004479469103955]
	TIME [epoch: 8.86 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008946115788740699		[learning rate: 0.0014346]
		[batch 20/20] avg loss: 0.010956335651963717		[learning rate: 0.0014324]
	Learning Rate: 0.00143241
	LOSS [training: 0.009951225720352206 | validation: 0.011769463296082684]
	TIME [epoch: 8.86 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009782911103128542		[learning rate: 0.0014302]
		[batch 20/20] avg loss: 0.009602104909506363		[learning rate: 0.001428]
	Learning Rate: 0.00142802
	LOSS [training: 0.009692508006317454 | validation: 0.024864887837765637]
	TIME [epoch: 8.89 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023428568856981767		[learning rate: 0.0014258]
		[batch 20/20] avg loss: 0.02443026176188842		[learning rate: 0.0014236]
	Learning Rate: 0.00142364
	LOSS [training: 0.02392941530943509 | validation: 0.016998612314927598]
	TIME [epoch: 8.85 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01601282309407586		[learning rate: 0.0014215]
		[batch 20/20] avg loss: 0.017039387627664673		[learning rate: 0.0014193]
	Learning Rate: 0.00141928
	LOSS [training: 0.01652610536087027 | validation: 0.005730384461112966]
	TIME [epoch: 8.85 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01880256202621586		[learning rate: 0.0014171]
		[batch 20/20] avg loss: 0.010952750446846052		[learning rate: 0.0014149]
	Learning Rate: 0.00141492
	LOSS [training: 0.014877656236530956 | validation: 0.010167870482261275]
	TIME [epoch: 8.85 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012641993004906583		[learning rate: 0.0014128]
		[batch 20/20] avg loss: 0.0052619253066563815		[learning rate: 0.0014106]
	Learning Rate: 0.00141059
	LOSS [training: 0.00895195915578148 | validation: 0.016220813508600322]
	TIME [epoch: 8.88 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014229417015104679		[learning rate: 0.0014084]
		[batch 20/20] avg loss: 0.02070720837731297		[learning rate: 0.0014063]
	Learning Rate: 0.00140626
	LOSS [training: 0.01746831269620883 | validation: 0.012586433730122023]
	TIME [epoch: 8.84 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013726775071353938		[learning rate: 0.0014041]
		[batch 20/20] avg loss: 0.012639429019661821		[learning rate: 0.001402]
	Learning Rate: 0.00140195
	LOSS [training: 0.01318310204550788 | validation: 0.011834514238313356]
	TIME [epoch: 8.82 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00932959515793722		[learning rate: 0.0013998]
		[batch 20/20] avg loss: 0.010173754401648956		[learning rate: 0.0013977]
	Learning Rate: 0.00139765
	LOSS [training: 0.009751674779793088 | validation: 0.006244584382913209]
	TIME [epoch: 8.83 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008953099344169737		[learning rate: 0.0013955]
		[batch 20/20] avg loss: 0.006209790753745491		[learning rate: 0.0013934]
	Learning Rate: 0.00139337
	LOSS [training: 0.007581445048957617 | validation: 0.0203958271820395]
	TIME [epoch: 8.85 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014724077400061542		[learning rate: 0.0013912]
		[batch 20/20] avg loss: 0.03465817999223991		[learning rate: 0.0013891]
	Learning Rate: 0.0013891
	LOSS [training: 0.02469112869615073 | validation: 0.04171163724899713]
	TIME [epoch: 8.84 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01883586738954774		[learning rate: 0.001387]
		[batch 20/20] avg loss: 0.010655784132426769		[learning rate: 0.0013848]
	Learning Rate: 0.00138484
	LOSS [training: 0.014745825760987255 | validation: 0.009765879505164565]
	TIME [epoch: 8.82 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012585226720359836		[learning rate: 0.0013827]
		[batch 20/20] avg loss: 0.017045248342152708		[learning rate: 0.0013806]
	Learning Rate: 0.0013806
	LOSS [training: 0.01481523753125627 | validation: 0.01132850125302302]
	TIME [epoch: 8.83 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01354137882658778		[learning rate: 0.0013785]
		[batch 20/20] avg loss: 0.011597270273024974		[learning rate: 0.0013764]
	Learning Rate: 0.00137636
	LOSS [training: 0.012569324549806377 | validation: 0.01584304712138841]
	TIME [epoch: 8.81 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014490000195928882		[learning rate: 0.0013743]
		[batch 20/20] avg loss: 0.014922928943208671		[learning rate: 0.0013721]
	Learning Rate: 0.00137214
	LOSS [training: 0.014706464569568778 | validation: 0.02026490975205873]
	TIME [epoch: 8.86 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012018567056673549		[learning rate: 0.00137]
		[batch 20/20] avg loss: 0.011053192181716465		[learning rate: 0.0013679]
	Learning Rate: 0.00136794
	LOSS [training: 0.011535879619195005 | validation: 0.009750361862291118]
	TIME [epoch: 8.83 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011470727134527934		[learning rate: 0.0013658]
		[batch 20/20] avg loss: 0.013659339925583205		[learning rate: 0.0013637]
	Learning Rate: 0.00136375
	LOSS [training: 0.012565033530055569 | validation: 0.025201404675070704]
	TIME [epoch: 8.82 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019664108338814764		[learning rate: 0.0013617]
		[batch 20/20] avg loss: 0.02087518113882149		[learning rate: 0.0013596]
	Learning Rate: 0.00135956
	LOSS [training: 0.020269644738818128 | validation: 0.009186170934249473]
	TIME [epoch: 8.82 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0073111182209042915		[learning rate: 0.0013575]
		[batch 20/20] avg loss: 0.008514807533474809		[learning rate: 0.0013554]
	Learning Rate: 0.0013554
	LOSS [training: 0.00791296287718955 | validation: 0.016287044119784683]
	TIME [epoch: 8.83 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007455545820272179		[learning rate: 0.0013533]
		[batch 20/20] avg loss: 0.0009301588932238127		[learning rate: 0.0013512]
	Learning Rate: 0.00135124
	LOSS [training: 0.004192852356747995 | validation: 0.006508883150701697]
	TIME [epoch: 8.82 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006602408477591905		[learning rate: 0.0013492]
		[batch 20/20] avg loss: 0.023694140802341522		[learning rate: 0.0013471]
	Learning Rate: 0.0013471
	LOSS [training: 0.015148274639966713 | validation: 0.01827438465046689]
	TIME [epoch: 8.82 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014917125344571857		[learning rate: 0.001345]
		[batch 20/20] avg loss: 0.008572841727616463		[learning rate: 0.001343]
	Learning Rate: 0.00134297
	LOSS [training: 0.011744983536094161 | validation: 0.005415072957093776]
	TIME [epoch: 8.83 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003397925971934242		[learning rate: 0.0013409]
		[batch 20/20] avg loss: 0.018402404459772436		[learning rate: 0.0013389]
	Learning Rate: 0.00133885
	LOSS [training: 0.010900165215853339 | validation: 0.012429844554756313]
	TIME [epoch: 8.85 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007902963208355528		[learning rate: 0.0013368]
		[batch 20/20] avg loss: 0.020545806522077197		[learning rate: 0.0013348]
	Learning Rate: 0.00133475
	LOSS [training: 0.014224384865216363 | validation: 0.012599479441445429]
	TIME [epoch: 8.83 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023261502132396913		[learning rate: 0.0013327]
		[batch 20/20] avg loss: 0.010072106969503992		[learning rate: 0.0013307]
	Learning Rate: 0.00133066
	LOSS [training: 0.016666804550950452 | validation: 0.014408213190669763]
	TIME [epoch: 8.82 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009113540051552241		[learning rate: 0.0013286]
		[batch 20/20] avg loss: 0.015364124893898929		[learning rate: 0.0013266]
	Learning Rate: 0.00132658
	LOSS [training: 0.012238832472725584 | validation: 0.01840369441517863]
	TIME [epoch: 8.82 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024112352168967947		[learning rate: 0.0013245]
		[batch 20/20] avg loss: 0.015281718754331342		[learning rate: 0.0013225]
	Learning Rate: 0.00132251
	LOSS [training: 0.019697035461649644 | validation: 0.019925512148768212]
	TIME [epoch: 8.84 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017560793380559474		[learning rate: 0.0013205]
		[batch 20/20] avg loss: 0.014747817577394745		[learning rate: 0.0013185]
	Learning Rate: 0.00131846
	LOSS [training: 0.016154305478977112 | validation: 0.023323716236301972]
	TIME [epoch: 8.84 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016947010486066734		[learning rate: 0.0013164]
		[batch 20/20] avg loss: 0.0063396311811007465		[learning rate: 0.0013144]
	Learning Rate: 0.00131442
	LOSS [training: 0.011643320833583739 | validation: 0.008743428674491554]
	TIME [epoch: 8.83 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014823273998248052		[learning rate: 0.0013124]
		[batch 20/20] avg loss: 0.014818955027706197		[learning rate: 0.0013104]
	Learning Rate: 0.00131039
	LOSS [training: 0.014821114512977124 | validation: 0.025966930622269412]
	TIME [epoch: 8.83 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02037982115739003		[learning rate: 0.0013084]
		[batch 20/20] avg loss: 0.012680237722594911		[learning rate: 0.0013064]
	Learning Rate: 0.00130637
	LOSS [training: 0.016530029439992468 | validation: 0.009990664319440982]
	TIME [epoch: 8.83 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006398626881128548		[learning rate: 0.0013044]
		[batch 20/20] avg loss: 0.008759665875024632		[learning rate: 0.0013024]
	Learning Rate: 0.00130237
	LOSS [training: 0.007579146378076591 | validation: 0.018340343022953755]
	TIME [epoch: 8.84 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0054445387316488705		[learning rate: 0.0013004]
		[batch 20/20] avg loss: 0.009607474869849655		[learning rate: 0.0012984]
	Learning Rate: 0.00129837
	LOSS [training: 0.007526006800749265 | validation: 0.003414636029620174]
	TIME [epoch: 8.83 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006892460139682621		[learning rate: 0.0012964]
		[batch 20/20] avg loss: 0.016666653205582277		[learning rate: 0.0012944]
	Learning Rate: 0.00129439
	LOSS [training: 0.011779556672632446 | validation: 0.028836048206472918]
	TIME [epoch: 8.82 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012159822956186929		[learning rate: 0.0012924]
		[batch 20/20] avg loss: 0.018491703699094038		[learning rate: 0.0012904]
	Learning Rate: 0.00129043
	LOSS [training: 0.015325763327640482 | validation: 0.026635775474973384]
	TIME [epoch: 8.83 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0176010952868688		[learning rate: 0.0012884]
		[batch 20/20] avg loss: 0.0007870553306305319		[learning rate: 0.0012865]
	Learning Rate: 0.00128647
	LOSS [training: 0.009194075308749668 | validation: 0.004732841679549426]
	TIME [epoch: 8.85 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006476668686680111		[learning rate: 0.0012845]
		[batch 20/20] avg loss: 0.00674757975249861		[learning rate: 0.0012825]
	Learning Rate: 0.00128253
	LOSS [training: 0.00661212421958936 | validation: 0.01257889614038776]
	TIME [epoch: 8.84 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010361276875319549		[learning rate: 0.0012806]
		[batch 20/20] avg loss: 0.015303989652736754		[learning rate: 0.0012786]
	Learning Rate: 0.0012786
	LOSS [training: 0.012832633264028151 | validation: 0.012766979337517092]
	TIME [epoch: 8.82 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009302029350322665		[learning rate: 0.0012766]
		[batch 20/20] avg loss: 0.010875643724341523		[learning rate: 0.0012747]
	Learning Rate: 0.00127468
	LOSS [training: 0.010088836537332094 | validation: 0.03325060208605515]
	TIME [epoch: 8.82 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020079286604474778		[learning rate: 0.0012727]
		[batch 20/20] avg loss: 0.015584051452211992		[learning rate: 0.0012708]
	Learning Rate: 0.00127077
	LOSS [training: 0.017831669028343385 | validation: 0.014345537039264777]
	TIME [epoch: 8.84 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013505555193536209		[learning rate: 0.0012688]
		[batch 20/20] avg loss: 0.006620854991386126		[learning rate: 0.0012669]
	Learning Rate: 0.00126687
	LOSS [training: 0.010063205092461166 | validation: 0.014282022298994685]
	TIME [epoch: 8.83 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004370478487238203		[learning rate: 0.0012649]
		[batch 20/20] avg loss: 0.006405903707807642		[learning rate: 0.001263]
	Learning Rate: 0.00126299
	LOSS [training: 0.0053881910975229225 | validation: 0.018647920080078026]
	TIME [epoch: 8.82 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014988693524470081		[learning rate: 0.0012611]
		[batch 20/20] avg loss: 0.014976946720866326		[learning rate: 0.0012591]
	Learning Rate: 0.00125912
	LOSS [training: 0.014982820122668206 | validation: 0.021974833366342428]
	TIME [epoch: 8.82 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01722929744467192		[learning rate: 0.0012572]
		[batch 20/20] avg loss: 0.024508672195501745		[learning rate: 0.0012553]
	Learning Rate: 0.00125526
	LOSS [training: 0.020868984820086835 | validation: 0.023307243813156214]
	TIME [epoch: 8.83 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014327282288729112		[learning rate: 0.0012533]
		[batch 20/20] avg loss: 0.015530266540879283		[learning rate: 0.0012514]
	Learning Rate: 0.00125141
	LOSS [training: 0.014928774414804197 | validation: 0.022184993871969056]
	TIME [epoch: 8.84 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0166248013336858		[learning rate: 0.0012495]
		[batch 20/20] avg loss: 0.010431156813818506		[learning rate: 0.0012476]
	Learning Rate: 0.00124757
	LOSS [training: 0.013527979073752153 | validation: 0.026392831174991395]
	TIME [epoch: 8.83 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020684263282407488		[learning rate: 0.0012457]
		[batch 20/20] avg loss: 0.008861136564554237		[learning rate: 0.0012438]
	Learning Rate: 0.00124375
	LOSS [training: 0.014772699923480865 | validation: 0.006758052685073142]
	TIME [epoch: 8.83 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005159047423405394		[learning rate: 0.0012418]
		[batch 20/20] avg loss: 0.009191740428175584		[learning rate: 0.0012399]
	Learning Rate: 0.00123994
	LOSS [training: 0.00717539392579049 | validation: 0.0160212762831963]
	TIME [epoch: 8.83 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0072351600869168834		[learning rate: 0.001238]
		[batch 20/20] avg loss: 0.006416515622045346		[learning rate: 0.0012361]
	Learning Rate: 0.00123614
	LOSS [training: 0.006825837854481114 | validation: 0.008385502228818916]
	TIME [epoch: 8.85 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00952152462987737		[learning rate: 0.0012342]
		[batch 20/20] avg loss: 0.020542183044448492		[learning rate: 0.0012323]
	Learning Rate: 0.00123235
	LOSS [training: 0.015031853837162929 | validation: 0.017414857239272323]
	TIME [epoch: 8.84 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020944192685583615		[learning rate: 0.0012305]
		[batch 20/20] avg loss: 0.013145939047264393		[learning rate: 0.0012286]
	Learning Rate: 0.00122857
	LOSS [training: 0.017045065866424007 | validation: 0.019929973147401678]
	TIME [epoch: 8.84 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007999282664097525		[learning rate: 0.0012267]
		[batch 20/20] avg loss: 0.012417354392883986		[learning rate: 0.0012248]
	Learning Rate: 0.0012248
	LOSS [training: 0.010208318528490756 | validation: 0.016169618859096957]
	TIME [epoch: 8.83 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006791093325832799		[learning rate: 0.0012229]
		[batch 20/20] avg loss: 0.007636061790278197		[learning rate: 0.001221]
	Learning Rate: 0.00122105
	LOSS [training: 0.007213577558055498 | validation: 0.011612472131176322]
	TIME [epoch: 8.85 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007655195866131573		[learning rate: 0.0012192]
		[batch 20/20] avg loss: 0.0016376608412825475		[learning rate: 0.0012173]
	Learning Rate: 0.00121731
	LOSS [training: 0.00464642835370706 | validation: 0.019770306849392807]
	TIME [epoch: 8.84 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014146095545560098		[learning rate: 0.0012154]
		[batch 20/20] avg loss: 0.00016777596354631985		[learning rate: 0.0012136]
	Learning Rate: 0.00121357
	LOSS [training: 0.007156935754553208 | validation: 0.009504558474906263]
	TIME [epoch: 8.82 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00467180557523487		[learning rate: 0.0012117]
		[batch 20/20] avg loss: 0.0032667753945998072		[learning rate: 0.0012099]
	Learning Rate: 0.00120985
	LOSS [training: 0.003969290484917339 | validation: 0.006488058023588962]
	TIME [epoch: 8.83 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035657926662532773		[learning rate: 0.001208]
		[batch 20/20] avg loss: 0.0053542626010700724		[learning rate: 0.0012061]
	Learning Rate: 0.00120615
	LOSS [training: 0.004460027633661675 | validation: 0.011824618920246444]
	TIME [epoch: 8.84 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012047343687403885		[learning rate: 0.0012043]
		[batch 20/20] avg loss: 0.003938067866087399		[learning rate: 0.0012024]
	Learning Rate: 0.00120245
	LOSS [training: 0.0025714011174138938 | validation: 0.012615999269596775]
	TIME [epoch: 8.85 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014293125189665818		[learning rate: 0.0012006]
		[batch 20/20] avg loss: 0.0043247956634003635		[learning rate: 0.0011988]
	Learning Rate: 0.00119876
	LOSS [training: 0.0028770540911834726 | validation: 0.02712472716795259]
	TIME [epoch: 8.83 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023138981199490278		[learning rate: 0.0011969]
		[batch 20/20] avg loss: 0.012513537166169892		[learning rate: 0.0011951]
	Learning Rate: 0.00119509
	LOSS [training: 0.017826259182830085 | validation: 0.01975672196969479]
	TIME [epoch: 8.83 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011935781110825714		[learning rate: 0.0011933]
		[batch 20/20] avg loss: 0.0020715533789746002		[learning rate: 0.0011914]
	Learning Rate: 0.00119142
	LOSS [training: 0.007003667244900158 | validation: 0.014292381314089764]
	TIME [epoch: 8.83 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025019256267441057		[learning rate: 0.0011896]
		[batch 20/20] avg loss: 0.004131416368695976		[learning rate: 0.0011878]
	Learning Rate: 0.00118777
	LOSS [training: 0.0033166709977200414 | validation: 0.010722224993911181]
	TIME [epoch: 8.86 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007102413643701638		[learning rate: 0.001186]
		[batch 20/20] avg loss: 0.009133780573899634		[learning rate: 0.0011841]
	Learning Rate: 0.00118413
	LOSS [training: 0.008118097108800636 | validation: 0.021743124817325436]
	TIME [epoch: 8.83 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011087604887261127		[learning rate: 0.0011823]
		[batch 20/20] avg loss: 0.00746805215891454		[learning rate: 0.0011805]
	Learning Rate: 0.0011805
	LOSS [training: 0.009277828523087832 | validation: 0.006805814692359583]
	TIME [epoch: 8.83 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006523503721256677		[learning rate: 0.0011787]
		[batch 20/20] avg loss: 0.014321807709249106		[learning rate: 0.0011769]
	Learning Rate: 0.00117688
	LOSS [training: 0.010422655715252893 | validation: 0.0018193480432339704]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1197.pth
	Model improved!!!
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002792179192308375		[learning rate: 0.0011751]
		[batch 20/20] avg loss: 0.0064747251038785505		[learning rate: 0.0011733]
	Learning Rate: 0.00117328
	LOSS [training: 0.004633452148093463 | validation: 0.0008366751386008398]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1198.pth
	Model improved!!!
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005023961824551957		[learning rate: 0.0011715]
		[batch 20/20] avg loss: 0.005893170398000364		[learning rate: 0.0011697]
	Learning Rate: 0.00116968
	LOSS [training: 0.005458566111276161 | validation: 0.004500217684703405]
	TIME [epoch: 8.84 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0066773492370420465		[learning rate: 0.0011679]
		[batch 20/20] avg loss: 0.005222653548524371		[learning rate: 0.0011661]
	Learning Rate: 0.00116609
	LOSS [training: 0.005950001392783209 | validation: 0.002394657531222403]
	TIME [epoch: 8.82 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00012621118168014284		[learning rate: 0.0011643]
		[batch 20/20] avg loss: 0.015185665084026712		[learning rate: 0.0011625]
	Learning Rate: 0.00116252
	LOSS [training: 0.007655938132853426 | validation: 0.0034031130611061515]
	TIME [epoch: 8.83 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00299771328731089		[learning rate: 0.0011607]
		[batch 20/20] avg loss: 0.0021691781057081366		[learning rate: 0.001159]
	Learning Rate: 0.00115896
	LOSS [training: 0.0025834456965095126 | validation: 0.00343134569643892]
	TIME [epoch: 8.85 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004747364245888424		[learning rate: 0.0011572]
		[batch 20/20] avg loss: 0.0018867887974338578		[learning rate: 0.0011554]
	Learning Rate: 0.0011554
	LOSS [training: 0.003317076521661141 | validation: 0.008667590425419642]
	TIME [epoch: 8.83 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005423465435432902		[learning rate: 0.0011536]
		[batch 20/20] avg loss: 0.002248641699256217		[learning rate: 0.0011519]
	Learning Rate: 0.00115186
	LOSS [training: 0.0038360535673445605 | validation: -0.0030627276614899235]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1204.pth
	Model improved!!!
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022854386108926046		[learning rate: 0.0011501]
		[batch 20/20] avg loss: 0.008732286154378318		[learning rate: 0.0011483]
	Learning Rate: 0.00114833
	LOSS [training: 0.005508862382635461 | validation: 0.014484509323457087]
	TIME [epoch: 8.83 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011647943823319925		[learning rate: 0.0011466]
		[batch 20/20] avg loss: 0.025628557986349398		[learning rate: 0.0011448]
	Learning Rate: 0.00114481
	LOSS [training: 0.01863825090483466 | validation: 0.004745341575148593]
	TIME [epoch: 8.83 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002633815023254704		[learning rate: 0.0011431]
		[batch 20/20] avg loss: 0.012485394008344381		[learning rate: 0.0011413]
	Learning Rate: 0.0011413
	LOSS [training: 0.007559604515799542 | validation: 0.03136018794358549]
	TIME [epoch: 8.86 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013754553283925866		[learning rate: 0.0011395]
		[batch 20/20] avg loss: 0.00800170435304594		[learning rate: 0.0011378]
	Learning Rate: 0.0011378
	LOSS [training: 0.010878128818485903 | validation: 0.01804008023193719]
	TIME [epoch: 8.84 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018522442288926348		[learning rate: 0.0011361]
		[batch 20/20] avg loss: 0.017539148480745507		[learning rate: 0.0011343]
	Learning Rate: 0.00113431
	LOSS [training: 0.01803079538483593 | validation: 0.01797921809152727]
	TIME [epoch: 8.83 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01795970420536524		[learning rate: 0.0011326]
		[batch 20/20] avg loss: 0.019849040868517232		[learning rate: 0.0011308]
	Learning Rate: 0.00113084
	LOSS [training: 0.018904372536941237 | validation: 0.026872575362427483]
	TIME [epoch: 8.84 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012152489693549555		[learning rate: 0.0011291]
		[batch 20/20] avg loss: 0.019741254364431082		[learning rate: 0.0011274]
	Learning Rate: 0.00112737
	LOSS [training: 0.015946872028990318 | validation: 0.024110572983622867]
	TIME [epoch: 8.86 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020224787329438398		[learning rate: 0.0011256]
		[batch 20/20] avg loss: 0.023731196712239506		[learning rate: 0.0011239]
	Learning Rate: 0.00112391
	LOSS [training: 0.02197799202083895 | validation: 0.024503041661456573]
	TIME [epoch: 8.83 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01832672908654078		[learning rate: 0.0011222]
		[batch 20/20] avg loss: 0.012508104337939965		[learning rate: 0.0011205]
	Learning Rate: 0.00112047
	LOSS [training: 0.015417416712240372 | validation: 0.015242609293671435]
	TIME [epoch: 8.86 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011085462336610489		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.01508223394428294		[learning rate: 0.001117]
	Learning Rate: 0.00111703
	LOSS [training: 0.013083848140446714 | validation: 0.02142288400951788]
	TIME [epoch: 8.83 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012875841839995175		[learning rate: 0.0011153]
		[batch 20/20] avg loss: 0.02093237550358131		[learning rate: 0.0011136]
	Learning Rate: 0.00111361
	LOSS [training: 0.016904108671788248 | validation: 0.014088887064757135]
	TIME [epoch: 8.86 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012409204733619025		[learning rate: 0.0011119]
		[batch 20/20] avg loss: 0.02068058258602732		[learning rate: 0.0011102]
	Learning Rate: 0.0011102
	LOSS [training: 0.016544893659823173 | validation: 0.009275402573518079]
	TIME [epoch: 8.84 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010101517373629909		[learning rate: 0.0011085]
		[batch 20/20] avg loss: 0.010465416613435075		[learning rate: 0.0011068]
	Learning Rate: 0.00110679
	LOSS [training: 0.01028346699353249 | validation: 0.009431261871544276]
	TIME [epoch: 8.84 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011565754570911208		[learning rate: 0.0011051]
		[batch 20/20] avg loss: 0.007393873899825587		[learning rate: 0.0011034]
	Learning Rate: 0.0011034
	LOSS [training: 0.009479814235368396 | validation: 0.005438403168185307]
	TIME [epoch: 8.84 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010109689967365517		[learning rate: 0.0011017]
		[batch 20/20] avg loss: 0.008567712960914325		[learning rate: 0.0011]
	Learning Rate: 0.00110002
	LOSS [training: 0.009338701464139922 | validation: 0.013659732976137762]
	TIME [epoch: 8.85 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008884451674878523		[learning rate: 0.0010983]
		[batch 20/20] avg loss: 0.015387611688834268		[learning rate: 0.0010966]
	Learning Rate: 0.00109665
	LOSS [training: 0.012136031681856396 | validation: 0.014060628603103378]
	TIME [epoch: 8.85 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011067265487885856		[learning rate: 0.001095]
		[batch 20/20] avg loss: 0.006703221288687425		[learning rate: 0.0010933]
	Learning Rate: 0.00109328
	LOSS [training: 0.008885243388286642 | validation: 0.006095596667383412]
	TIME [epoch: 8.84 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005784351804080256		[learning rate: 0.0010916]
		[batch 20/20] avg loss: 0.007282469129591594		[learning rate: 0.0010899]
	Learning Rate: 0.00108993
	LOSS [training: 0.006533410466835925 | validation: 0.009878079460662811]
	TIME [epoch: 8.84 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005377346102866999		[learning rate: 0.0010883]
		[batch 20/20] avg loss: 0.008323584659539468		[learning rate: 0.0010866]
	Learning Rate: 0.00108659
	LOSS [training: 0.006850465381203234 | validation: 0.013202593879505934]
	TIME [epoch: 8.84 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011948807016701		[learning rate: 0.0010849]
		[batch 20/20] avg loss: 0.008713463155407249		[learning rate: 0.0010833]
	Learning Rate: 0.00108326
	LOSS [training: 0.010331135086054124 | validation: 0.009438744506099182]
	TIME [epoch: 8.85 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00835710545000702		[learning rate: 0.0010816]
		[batch 20/20] avg loss: 0.012905332539101023		[learning rate: 0.0010799]
	Learning Rate: 0.00107994
	LOSS [training: 0.010631218994554022 | validation: 0.02449355178176177]
	TIME [epoch: 8.83 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011313908422003475		[learning rate: 0.0010783]
		[batch 20/20] avg loss: 0.014340299424418782		[learning rate: 0.0010766]
	Learning Rate: 0.00107663
	LOSS [training: 0.01282710392321113 | validation: 0.008675918071864104]
	TIME [epoch: 8.83 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01732066859501775		[learning rate: 0.001075]
		[batch 20/20] avg loss: 0.006768387501832554		[learning rate: 0.0010733]
	Learning Rate: 0.00107333
	LOSS [training: 0.012044528048425155 | validation: 0.002543437157891699]
	TIME [epoch: 8.83 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008124806460471885		[learning rate: 0.0010717]
		[batch 20/20] avg loss: 0.004892261575433341		[learning rate: 0.00107]
	Learning Rate: 0.00107004
	LOSS [training: 0.006508534017952613 | validation: 0.0037240434725413305]
	TIME [epoch: 8.85 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006284767863262336		[learning rate: 0.0010684]
		[batch 20/20] avg loss: 0.007442423705873098		[learning rate: 0.0010668]
	Learning Rate: 0.00106676
	LOSS [training: 0.006863595784567718 | validation: 0.0035759561286682366]
	TIME [epoch: 8.83 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0041552707624823385		[learning rate: 0.0010651]
		[batch 20/20] avg loss: 0.0064553747935916		[learning rate: 0.0010635]
	Learning Rate: 0.00106349
	LOSS [training: 0.005305322778036969 | validation: 0.006916468952809703]
	TIME [epoch: 8.83 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021400289462636		[learning rate: 0.0010619]
		[batch 20/20] avg loss: 0.006300224384495308		[learning rate: 0.0010602]
	Learning Rate: 0.00106023
	LOSS [training: 0.004220126665379454 | validation: 0.0017676115834257627]
	TIME [epoch: 8.82 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002594876797041398		[learning rate: 0.0010586]
		[batch 20/20] avg loss: 0.006384853759074482		[learning rate: 0.001057]
	Learning Rate: 0.00105698
	LOSS [training: 0.00448986527805794 | validation: 0.011420004812491513]
	TIME [epoch: 8.85 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00409554696191301		[learning rate: 0.0010554]
		[batch 20/20] avg loss: 0.007371757356939031		[learning rate: 0.0010537]
	Learning Rate: 0.00105374
	LOSS [training: 0.005733652159426019 | validation: 0.010803817923211272]
	TIME [epoch: 8.85 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006582108818261602		[learning rate: 0.0010521]
		[batch 20/20] avg loss: 0.007419637415100956		[learning rate: 0.0010505]
	Learning Rate: 0.00105051
	LOSS [training: 0.007000873116681279 | validation: 0.014388207726978908]
	TIME [epoch: 8.83 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007617269374199409		[learning rate: 0.0010489]
		[batch 20/20] avg loss: 0.0020440312872261443		[learning rate: 0.0010473]
	Learning Rate: 0.00104729
	LOSS [training: 0.004830650330712776 | validation: -0.0037944537164041527]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1235.pth
	Model improved!!!
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038060558352385047		[learning rate: 0.0010457]
		[batch 20/20] avg loss: 0.004683171540075493		[learning rate: 0.0010441]
	Learning Rate: 0.00104408
	LOSS [training: 0.004244613687656999 | validation: 0.008551341086250818]
	TIME [epoch: 8.83 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007266171213417038		[learning rate: 0.0010425]
		[batch 20/20] avg loss: -0.0019984137214815864		[learning rate: 0.0010409]
	Learning Rate: 0.00104088
	LOSS [training: 0.002633878745967726 | validation: 0.008048850638048595]
	TIME [epoch: 8.89 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004183976610672016		[learning rate: 0.0010393]
		[batch 20/20] avg loss: 0.0005778086997809773		[learning rate: 0.0010377]
	Learning Rate: 0.00103769
	LOSS [training: 0.0023808926552264962 | validation: 0.0011698347538359505]
	TIME [epoch: 8.83 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0044148506877473		[learning rate: 0.0010361]
		[batch 20/20] avg loss: 0.00410389517202503		[learning rate: 0.0010345]
	Learning Rate: 0.00103451
	LOSS [training: 0.0042593729298861654 | validation: 0.008688838928377981]
	TIME [epoch: 8.82 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020360124350378814		[learning rate: 0.0010329]
		[batch 20/20] avg loss: 0.008889272969038037		[learning rate: 0.0010313]
	Learning Rate: 0.00103134
	LOSS [training: 0.005462642702037959 | validation: 0.0054042201517476105]
	TIME [epoch: 8.83 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00555007813928479		[learning rate: 0.0010298]
		[batch 20/20] avg loss: 0.006868480238260984		[learning rate: 0.0010282]
	Learning Rate: 0.00102817
	LOSS [training: 0.006209279188772888 | validation: 0.009342799282002516]
	TIME [epoch: 8.85 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010362729465116332		[learning rate: 0.0010266]
		[batch 20/20] avg loss: 0.018748527511816968		[learning rate: 0.001025]
	Learning Rate: 0.00102502
	LOSS [training: 0.014555628488466652 | validation: 0.009477523600423806]
	TIME [epoch: 8.84 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014775114405174691		[learning rate: 0.0010234]
		[batch 20/20] avg loss: 0.019073792510026458		[learning rate: 0.0010219]
	Learning Rate: 0.00102188
	LOSS [training: 0.016924453457600575 | validation: 0.03385309531682771]
	TIME [epoch: 8.83 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015891747208270995		[learning rate: 0.0010203]
		[batch 20/20] avg loss: 0.021848713622781724		[learning rate: 0.0010187]
	Learning Rate: 0.00101875
	LOSS [training: 0.018870230415526358 | validation: 0.01666860474294933]
	TIME [epoch: 8.83 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007004144339623042		[learning rate: 0.0010172]
		[batch 20/20] avg loss: 0.0032614634662061757		[learning rate: 0.0010156]
	Learning Rate: 0.00101562
	LOSS [training: 0.005132803902914608 | validation: 0.002845302453661148]
	TIME [epoch: 8.84 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013969253133691306		[learning rate: 0.0010141]
		[batch 20/20] avg loss: 0.0050753151288461095		[learning rate: 0.0010125]
	Learning Rate: 0.00101251
	LOSS [training: 0.0032361202211076195 | validation: 0.008565339099708042]
	TIME [epoch: 8.83 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01060784657075493		[learning rate: 0.001011]
		[batch 20/20] avg loss: 0.0030623167684867203		[learning rate: 0.0010094]
	Learning Rate: 0.00100941
	LOSS [training: 0.006835081669620824 | validation: 0.004405020784817214]
	TIME [epoch: 8.83 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0056103302906529914		[learning rate: 0.0010079]
		[batch 20/20] avg loss: 0.01308437584730556		[learning rate: 0.0010063]
	Learning Rate: 0.00100631
	LOSS [training: 0.009347353068979278 | validation: 0.011542137172824488]
	TIME [epoch: 8.82 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000915681553043656		[learning rate: 0.0010048]
		[batch 20/20] avg loss: 0.007690989200294977		[learning rate: 0.0010032]
	Learning Rate: 0.00100323
	LOSS [training: 0.004303335376669317 | validation: 0.008069455213697847]
	TIME [epoch: 8.84 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005514641964823255		[learning rate: 0.0010017]
		[batch 20/20] avg loss: 0.007223438658103319		[learning rate: 0.0010002]
	Learning Rate: 0.00100015
	LOSS [training: 0.006369040311463285 | validation: 0.0008200608669302248]
	TIME [epoch: 8.85 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007190157670535823		[learning rate: 0.00099862]
		[batch 20/20] avg loss: 0.00553828865223875		[learning rate: 0.00099709]
	Learning Rate: 0.000997087
	LOSS [training: 0.002409636442592584 | validation: 0.008505363278135693]
	TIME [epoch: 8.83 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008555168889702652		[learning rate: 0.00099556]
		[batch 20/20] avg loss: 0.013511373162972665		[learning rate: 0.00099403]
	Learning Rate: 0.000994031
	LOSS [training: 0.011033271026337658 | validation: 0.013645137330915078]
	TIME [epoch: 8.82 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012662611289329703		[learning rate: 0.00099251]
		[batch 20/20] avg loss: 0.00863283297157898		[learning rate: 0.00099098]
	Learning Rate: 0.000990984
	LOSS [training: 0.01064772213045434 | validation: 0.006809889803470388]
	TIME [epoch: 8.83 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005478089757433946		[learning rate: 0.00098946]
		[batch 20/20] avg loss: 0.006050389865562719		[learning rate: 0.00098795]
	Learning Rate: 0.000987946
	LOSS [training: 0.005764239811498332 | validation: 0.006876019794686468]
	TIME [epoch: 8.85 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0075143995119766535		[learning rate: 0.00098643]
		[batch 20/20] avg loss: 0.0077111774304319276		[learning rate: 0.00098492]
	Learning Rate: 0.000984918
	LOSS [training: 0.007612788471204289 | validation: 0.006911733421291833]
	TIME [epoch: 8.82 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032844746593965303		[learning rate: 0.00098341]
		[batch 20/20] avg loss: 0.006139050016205698		[learning rate: 0.0009819]
	Learning Rate: 0.000981899
	LOSS [training: 0.004711762337801114 | validation: 0.009763300751111854]
	TIME [epoch: 8.82 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005105849460022193		[learning rate: 0.00098039]
		[batch 20/20] avg loss: 0.00404288644774108		[learning rate: 0.00097889]
	Learning Rate: 0.000978889
	LOSS [training: 0.004574367953881637 | validation: 0.007510003089917949]
	TIME [epoch: 8.83 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0046410527630675194		[learning rate: 0.00097739]
		[batch 20/20] avg loss: 0.0027748327322835746		[learning rate: 0.00097589]
	Learning Rate: 0.000975888
	LOSS [training: 0.0037079427476755466 | validation: 0.00822321901371618]
	TIME [epoch: 8.85 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025969633611728532		[learning rate: 0.00097439]
		[batch 20/20] avg loss: 0.0032763191301774844		[learning rate: 0.0009729]
	Learning Rate: 0.000972897
	LOSS [training: 0.002936641245675169 | validation: 0.005228558549603938]
	TIME [epoch: 8.83 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019445991287270086		[learning rate: 0.0009714]
		[batch 20/20] avg loss: 0.0021627650781400635		[learning rate: 0.00096991]
	Learning Rate: 0.000969914
	LOSS [training: 0.0020536821034335363 | validation: 0.012134068716168176]
	TIME [epoch: 8.83 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00012456666645243656		[learning rate: 0.00096843]
		[batch 20/20] avg loss: 0.003043557926364596		[learning rate: 0.00096694]
	Learning Rate: 0.000966941
	LOSS [training: 0.001584062296408516 | validation: 0.005152803468290865]
	TIME [epoch: 8.83 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003975434960856448		[learning rate: 0.00096546]
		[batch 20/20] avg loss: 0.003425622908646976		[learning rate: 0.00096398]
	Learning Rate: 0.000963977
	LOSS [training: 0.0037005289347517125 | validation: 0.006720210388424622]
	TIME [epoch: 8.84 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001706194308624228		[learning rate: 0.0009625]
		[batch 20/20] avg loss: -0.0036563460319778683		[learning rate: 0.00096102]
	Learning Rate: 0.000961022
	LOSS [training: -0.0009750758616768199 | validation: -0.0011002913000079792]
	TIME [epoch: 8.83 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00861734699807879		[learning rate: 0.00095955]
		[batch 20/20] avg loss: 0.012785724708008525		[learning rate: 0.00095808]
	Learning Rate: 0.000958076
	LOSS [training: 0.010701535853043653 | validation: 0.005083622779423362]
	TIME [epoch: 8.83 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: -8.422938115732991e-05		[learning rate: 0.00095661]
		[batch 20/20] avg loss: -0.0011902206685012527		[learning rate: 0.00095514]
	Learning Rate: 0.000955139
	LOSS [training: -0.0006372250248292911 | validation: 0.005207278175975353]
	TIME [epoch: 8.83 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007610419627512967		[learning rate: 0.00095367]
		[batch 20/20] avg loss: 0.0054692631332808215		[learning rate: 0.00095221]
	Learning Rate: 0.000952211
	LOSS [training: 0.0031151525480160587 | validation: 0.0025198305619580147]
	TIME [epoch: 8.83 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016423702235783687		[learning rate: 0.00095075]
		[batch 20/20] avg loss: -0.003733814038644321		[learning rate: 0.00094929]
	Learning Rate: 0.000949292
	LOSS [training: -0.0010457219075329757 | validation: 0.004423192702605843]
	TIME [epoch: 8.85 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008676079576888993		[learning rate: 0.00094784]
		[batch 20/20] avg loss: -0.001379202014019909		[learning rate: 0.00094638]
	Learning Rate: 0.000946382
	LOSS [training: 0.0036484387814345416 | validation: 0.012621769280404312]
	TIME [epoch: 8.83 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001313399807596359		[learning rate: 0.00094493]
		[batch 20/20] avg loss: 0.009032894322496918		[learning rate: 0.00094348]
	Learning Rate: 0.000943481
	LOSS [training: 0.005173147065046638 | validation: 0.00441738947960596]
	TIME [epoch: 8.83 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008305827462256131		[learning rate: 0.00094203]
		[batch 20/20] avg loss: 0.005881671911773212		[learning rate: 0.00094059]
	Learning Rate: 0.000940589
	LOSS [training: 0.0070937496870146725 | validation: 0.015225200709869616]
	TIME [epoch: 8.83 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00827770255077435		[learning rate: 0.00093915]
		[batch 20/20] avg loss: 0.0013956709272725343		[learning rate: 0.00093771]
	Learning Rate: 0.000937706
	LOSS [training: 0.004836686739023442 | validation: 0.011806499124277597]
	TIME [epoch: 8.85 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007979553610237018		[learning rate: 0.00093627]
		[batch 20/20] avg loss: 0.003840514669139114		[learning rate: 0.00093483]
	Learning Rate: 0.000934831
	LOSS [training: 0.005910034139688066 | validation: 0.007882743070781564]
	TIME [epoch: 8.84 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008270489167925117		[learning rate: 0.0009334]
		[batch 20/20] avg loss: 0.019112359968943032		[learning rate: 0.00093197]
	Learning Rate: 0.000931966
	LOSS [training: 0.013691424568434075 | validation: 0.012825720528471012]
	TIME [epoch: 8.82 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004256266254952172		[learning rate: 0.00093054]
		[batch 20/20] avg loss: -0.002567234559428833		[learning rate: 0.00092911]
	Learning Rate: 0.000929109
	LOSS [training: 0.0008445158477616692 | validation: 0.002983659004771973]
	TIME [epoch: 8.83 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00019021796717189881		[learning rate: 0.00092768]
		[batch 20/20] avg loss: -0.004196618242839803		[learning rate: 0.00092626]
	Learning Rate: 0.000926261
	LOSS [training: -0.002193418105005851 | validation: -0.002313663505614348]
	TIME [epoch: 8.85 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008739513889151415		[learning rate: 0.00092484]
		[batch 20/20] avg loss: 0.006532026239693836		[learning rate: 0.00092342]
	Learning Rate: 0.000923421
	LOSS [training: 0.0028290374253893474 | validation: 0.019733319545912637]
	TIME [epoch: 8.84 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006397226944265415		[learning rate: 0.00092201]
		[batch 20/20] avg loss: -0.00257929889755563		[learning rate: 0.00092059]
	Learning Rate: 0.000920591
	LOSS [training: 0.0019089640233548924 | validation: -0.003266165893041859]
	TIME [epoch: 8.83 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008074055805962797		[learning rate: 0.00091918]
		[batch 20/20] avg loss: -0.0015309425082636143		[learning rate: 0.00091777]
	Learning Rate: 0.000917769
	LOSS [training: -0.0011691740444299467 | validation: 0.008687677992487894]
	TIME [epoch: 8.83 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012697814145769999		[learning rate: 0.00091636]
		[batch 20/20] avg loss: 0.0021905151497774123		[learning rate: 0.00091496]
	Learning Rate: 0.000914956
	LOSS [training: 0.0017301482821772054 | validation: -0.00021345046995992537]
	TIME [epoch: 8.83 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007831805471788178		[learning rate: 0.00091355]
		[batch 20/20] avg loss: 0.0019461514964889382		[learning rate: 0.00091215]
	Learning Rate: 0.000912151
	LOSS [training: 0.001364666021833878 | validation: 0.018178400680381533]
	TIME [epoch: 8.84 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006870221183590647		[learning rate: 0.00091075]
		[batch 20/20] avg loss: -0.0004866083520471057		[learning rate: 0.00090935]
	Learning Rate: 0.000909355
	LOSS [training: 0.0031918064157717705 | validation: 0.004361026687810563]
	TIME [epoch: 8.82 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005449210512867207		[learning rate: 0.00090796]
		[batch 20/20] avg loss: -0.0016474361589167534		[learning rate: 0.00090657]
	Learning Rate: 0.000906567
	LOSS [training: 0.001900887176975227 | validation: -0.0008550543523873871]
	TIME [epoch: 8.83 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001098822876376018		[learning rate: 0.00090518]
		[batch 20/20] avg loss: 0.004654218203882177		[learning rate: 0.00090379]
	Learning Rate: 0.000903788
	LOSS [training: 0.0017776976637530792 | validation: 0.007129751326027812]
	TIME [epoch: 8.82 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004712407562465234		[learning rate: 0.0009024]
		[batch 20/20] avg loss: 0.006066797317333784		[learning rate: 0.00090102]
	Learning Rate: 0.000901018
	LOSS [training: 0.005389602439899509 | validation: 0.008674041726646164]
	TIME [epoch: 8.85 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002589102167010449		[learning rate: 0.00089964]
		[batch 20/20] avg loss: 0.0074947120857371		[learning rate: 0.00089826]
	Learning Rate: 0.000898256
	LOSS [training: 0.005041907126373774 | validation: 0.014452302769479581]
	TIME [epoch: 8.83 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00820500355184364		[learning rate: 0.00089688]
		[batch 20/20] avg loss: 0.008632313360915762		[learning rate: 0.0008955]
	Learning Rate: 0.000895502
	LOSS [training: 0.008418658456379704 | validation: 0.0034924067542271503]
	TIME [epoch: 8.82 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007407310025016206		[learning rate: 0.00089413]
		[batch 20/20] avg loss: 0.00784644162262934		[learning rate: 0.00089276]
	Learning Rate: 0.000892757
	LOSS [training: 0.007626875823822772 | validation: 0.010743028255193663]
	TIME [epoch: 8.82 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018845842496875147		[learning rate: 0.00089139]
		[batch 20/20] avg loss: 0.0018712729109933292		[learning rate: 0.00089002]
	Learning Rate: 0.00089002
	LOSS [training: -6.6556693470927115e-06 | validation: 0.002910047822302345]
	TIME [epoch: 8.84 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004336957068853898		[learning rate: 0.00088866]
		[batch 20/20] avg loss: 0.0001972743329816555		[learning rate: 0.00088729]
	Learning Rate: 0.000887292
	LOSS [training: 0.00031548501993352294 | validation: 0.0036578271878172914]
	TIME [epoch: 8.83 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002342374755016492		[learning rate: 0.00088593]
		[batch 20/20] avg loss: 0.0076390485615877995		[learning rate: 0.00088457]
	Learning Rate: 0.000884572
	LOSS [training: 0.0026483369032856544 | validation: 0.01166644411702172]
	TIME [epoch: 8.82 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012256944634217098		[learning rate: 0.00088322]
		[batch 20/20] avg loss: 0.006432700420871562		[learning rate: 0.00088186]
	Learning Rate: 0.000881861
	LOSS [training: 0.009344822527544329 | validation: 0.015063703748739643]
	TIME [epoch: 8.82 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012598421356631192		[learning rate: 0.00088051]
		[batch 20/20] avg loss: 0.010832024790620433		[learning rate: 0.00087916]
	Learning Rate: 0.000879157
	LOSS [training: 0.011715223073625814 | validation: 0.0054109692765575736]
	TIME [epoch: 8.85 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038817204158546233		[learning rate: 0.00087781]
		[batch 20/20] avg loss: 0.008173449998374185		[learning rate: 0.00087646]
	Learning Rate: 0.000876462
	LOSS [training: 0.006027585207114402 | validation: 0.003032623965813799]
	TIME [epoch: 8.83 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005302529710707487		[learning rate: 0.00087512]
		[batch 20/20] avg loss: 0.010326425540345177		[learning rate: 0.00087378]
	Learning Rate: 0.000873776
	LOSS [training: 0.00781447762552633 | validation: 0.0006638583605569602]
	TIME [epoch: 8.82 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007000064950599091		[learning rate: 0.00087244]
		[batch 20/20] avg loss: 0.010446921119473058		[learning rate: 0.0008711]
	Learning Rate: 0.000871097
	LOSS [training: 0.008723493035036075 | validation: 0.011005333461166184]
	TIME [epoch: 8.83 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007452782284209702		[learning rate: 0.00086976]
		[batch 20/20] avg loss: 0.024169301502460334		[learning rate: 0.00086843]
	Learning Rate: 0.000868427
	LOSS [training: 0.012457289865440655 | validation: 0.03281245388665491]
	TIME [epoch: 8.82 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017651328573995418		[learning rate: 0.00086709]
		[batch 20/20] avg loss: 0.014160708771622926		[learning rate: 0.00086576]
	Learning Rate: 0.000865765
	LOSS [training: 0.01590601867280917 | validation: 0.013202492096053745]
	TIME [epoch: 8.84 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005764953251999742		[learning rate: 0.00086444]
		[batch 20/20] avg loss: 0.019286415746031228		[learning rate: 0.00086311]
	Learning Rate: 0.000863111
	LOSS [training: 0.012525684499015486 | validation: 0.014657986693100433]
	TIME [epoch: 8.82 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011009654419670395		[learning rate: 0.00086179]
		[batch 20/20] avg loss: 0.004885951217880895		[learning rate: 0.00086047]
	Learning Rate: 0.000860465
	LOSS [training: 0.007947802818775645 | validation: 0.0050088069358661565]
	TIME [epoch: 8.81 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010084810674028949		[learning rate: 0.00085915]
		[batch 20/20] avg loss: 0.012610271449689991		[learning rate: 0.00085783]
	Learning Rate: 0.000857828
	LOSS [training: 0.01134754106185947 | validation: 0.020038036605271065]
	TIME [epoch: 8.82 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006158079434033083		[learning rate: 0.00085651]
		[batch 20/20] avg loss: 0.010431967258029444		[learning rate: 0.0008552]
	Learning Rate: 0.000855198
	LOSS [training: 0.008295023346031265 | validation: 0.016288956194896342]
	TIME [epoch: 8.84 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012652527098322578		[learning rate: 0.00085389]
		[batch 20/20] avg loss: 0.003929867196408023		[learning rate: 0.00085258]
	Learning Rate: 0.000852576
	LOSS [training: 0.008291197147365299 | validation: 0.006941194111676342]
	TIME [epoch: 8.82 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005595402196331084		[learning rate: 0.00085127]
		[batch 20/20] avg loss: 0.009893880325460909		[learning rate: 0.00084996]
	Learning Rate: 0.000849963
	LOSS [training: 0.0077446412608959975 | validation: 0.0046736145989193555]
	TIME [epoch: 8.82 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008179073988163473		[learning rate: 0.00084866]
		[batch 20/20] avg loss: 0.008819985667475634		[learning rate: 0.00084736]
	Learning Rate: 0.000847357
	LOSS [training: 0.008499529827819553 | validation: 0.005106247626674408]
	TIME [epoch: 8.82 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005039117410028816		[learning rate: 0.00084606]
		[batch 20/20] avg loss: 0.003970732656625085		[learning rate: 0.00084476]
	Learning Rate: 0.00084476
	LOSS [training: 0.00450492503332695 | validation: 0.009582774741415346]
	TIME [epoch: 8.84 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035767070898971645		[learning rate: 0.00084346]
		[batch 20/20] avg loss: 0.0036667064368407663		[learning rate: 0.00084217]
	Learning Rate: 0.00084217
	LOSS [training: 4.499967347180111e-05 | validation: 0.004388324729991625]
	TIME [epoch: 8.83 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028340393718636974		[learning rate: 0.00084088]
		[batch 20/20] avg loss: 0.009522434870477072		[learning rate: 0.00083959]
	Learning Rate: 0.000839589
	LOSS [training: 0.0033441977493066863 | validation: 0.012954938014078071]
	TIME [epoch: 8.82 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004560515890451079		[learning rate: 0.0008383]
		[batch 20/20] avg loss: -0.0017687328083704642		[learning rate: 0.00083702]
	Learning Rate: 0.000837015
	LOSS [training: 0.0013958915410403072 | validation: 0.0009328753278353996]
	TIME [epoch: 8.82 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002299815081847528		[learning rate: 0.00083573]
		[batch 20/20] avg loss: -0.0036359317662539017		[learning rate: 0.00083445]
	Learning Rate: 0.000834449
	LOSS [training: -0.0006680583422031868 | validation: 0.010903403723651374]
	TIME [epoch: 8.82 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012957586414192894		[learning rate: 0.00083317]
		[batch 20/20] avg loss: 0.0013763080080065706		[learning rate: 0.00083189]
	Learning Rate: 0.000831891
	LOSS [training: 0.00133603332471293 | validation: -0.0031886670031438156]
	TIME [epoch: 8.85 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004500725934788558		[learning rate: 0.00083062]
		[batch 20/20] avg loss: -0.000495470242808582		[learning rate: 0.00082934]
	Learning Rate: 0.000829341
	LOSS [training: 0.002002627845989988 | validation: 0.00884367224836279]
	TIME [epoch: 8.82 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00796399674321652		[learning rate: 0.00082807]
		[batch 20/20] avg loss: 0.001083731881680627		[learning rate: 0.0008268]
	Learning Rate: 0.000826799
	LOSS [training: 0.004523864312448573 | validation: 0.011858881849150509]
	TIME [epoch: 8.83 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004243469675000849		[learning rate: 0.00082553]
		[batch 20/20] avg loss: 0.004782149037225328		[learning rate: 0.00082426]
	Learning Rate: 0.000824265
	LOSS [training: 0.004512809356113088 | validation: 0.0006525510116862213]
	TIME [epoch: 8.83 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005772577648871563		[learning rate: 0.000823]
		[batch 20/20] avg loss: 7.98837155612877e-05		[learning rate: 0.00082174]
	Learning Rate: 0.000821738
	LOSS [training: 0.0029262306822164254 | validation: 0.0103600929931787]
	TIME [epoch: 8.84 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009509841625259183		[learning rate: 0.00082048]
		[batch 20/20] avg loss: 0.0018769466482413265		[learning rate: 0.00081922]
	Learning Rate: 0.000819219
	LOSS [training: 0.0014139654053836224 | validation: 0.0016178660216580485]
	TIME [epoch: 8.82 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007672480304864438		[learning rate: 0.00081796]
		[batch 20/20] avg loss: 0.0005953951057451704		[learning rate: 0.00081671]
	Learning Rate: 0.000816708
	LOSS [training: -8.592646237063689e-05 | validation: 0.011862559700298366]
	TIME [epoch: 8.82 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010245258991828026		[learning rate: 0.00081545]
		[batch 20/20] avg loss: 0.005649078671072165		[learning rate: 0.0008142]
	Learning Rate: 0.000814204
	LOSS [training: 0.007947168831450097 | validation: 0.00811295913680192]
	TIME [epoch: 8.82 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026113182093516086		[learning rate: 0.00081296]
		[batch 20/20] avg loss: 0.0022362439922029393		[learning rate: 0.00081171]
	Learning Rate: 0.000811708
	LOSS [training: 0.002423781100777274 | validation: 0.0016007326127525311]
	TIME [epoch: 8.84 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004998317234193886		[learning rate: 0.00081046]
		[batch 20/20] avg loss: 0.002086787464763853		[learning rate: 0.00080922]
	Learning Rate: 0.00080922
	LOSS [training: 0.0035425523494788695 | validation: 0.010532515699891812]
	TIME [epoch: 8.83 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008521508166805634		[learning rate: 0.00080798]
		[batch 20/20] avg loss: 0.003965603593997419		[learning rate: 0.00080674]
	Learning Rate: 0.000806739
	LOSS [training: 0.006243555880401526 | validation: 0.005101714043998118]
	TIME [epoch: 8.82 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017265300540396916		[learning rate: 0.0008055]
		[batch 20/20] avg loss: 0.021884760035471122		[learning rate: 0.00080427]
	Learning Rate: 0.000804267
	LOSS [training: 0.019575030287934024 | validation: 0.01926091361918235]
	TIME [epoch: 8.82 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01107546203570816		[learning rate: 0.00080303]
		[batch 20/20] avg loss: 0.004167164026671613		[learning rate: 0.0008018]
	Learning Rate: 0.000801801
	LOSS [training: 0.007621313031189887 | validation: 0.009255991674053188]
	TIME [epoch: 8.84 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008918043733793794		[learning rate: 0.00080057]
		[batch 20/20] avg loss: 0.00552501031426819		[learning rate: 0.00079934]
	Learning Rate: 0.000799343
	LOSS [training: 0.007221527024030991 | validation: 0.005702935038932027]
	TIME [epoch: 8.85 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011501437793623157		[learning rate: 0.00079812]
		[batch 20/20] avg loss: 0.004933165269847136		[learning rate: 0.00079689]
	Learning Rate: 0.000796893
	LOSS [training: 0.008217301531735145 | validation: 0.009284837493111613]
	TIME [epoch: 8.82 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008181647366182977		[learning rate: 0.00079567]
		[batch 20/20] avg loss: 0.008996675451902709		[learning rate: 0.00079445]
	Learning Rate: 0.00079445
	LOSS [training: 0.008589161409042843 | validation: 0.014678337774390633]
	TIME [epoch: 8.83 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00859658102471053		[learning rate: 0.00079323]
		[batch 20/20] avg loss: 0.00659798167744504		[learning rate: 0.00079201]
	Learning Rate: 0.000792015
	LOSS [training: 0.007597281351077784 | validation: 0.0040825502693811145]
	TIME [epoch: 8.83 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005746806100049241		[learning rate: 0.0007908]
		[batch 20/20] avg loss: 0.01258474725051792		[learning rate: 0.00078959]
	Learning Rate: 0.000789587
	LOSS [training: 0.009165776675283581 | validation: 0.013281789174376727]
	TIME [epoch: 8.85 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00578960076821652		[learning rate: 0.00078838]
		[batch 20/20] avg loss: 0.006127748128844697		[learning rate: 0.00078717]
	Learning Rate: 0.000787166
	LOSS [training: 0.005958674448530608 | validation: 0.015191470558107661]
	TIME [epoch: 8.84 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005567009625902843		[learning rate: 0.00078596]
		[batch 20/20] avg loss: 0.006863596803419412		[learning rate: 0.00078475]
	Learning Rate: 0.000784754
	LOSS [training: 0.006215303214661128 | validation: 0.0023263616700272723]
	TIME [epoch: 8.82 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036224002597776543		[learning rate: 0.00078355]
		[batch 20/20] avg loss: 0.003952404013670209		[learning rate: 0.00078235]
	Learning Rate: 0.000782348
	LOSS [training: 0.003787402136723932 | validation: 0.009619088336050874]
	TIME [epoch: 8.83 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010884359850655372		[learning rate: 0.00078115]
		[batch 20/20] avg loss: 0.007524384194585829		[learning rate: 0.00077995]
	Learning Rate: 0.00077995
	LOSS [training: 0.0032179741047601456 | validation: 0.00719995496450504]
	TIME [epoch: 8.86 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009938392293227348		[learning rate: 0.00077875]
		[batch 20/20] avg loss: 0.004249719676468429		[learning rate: 0.00077756]
	Learning Rate: 0.000777559
	LOSS [training: 0.007094055984847887 | validation: 0.0065869118580542995]
	TIME [epoch: 8.84 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038163997700048032		[learning rate: 0.00077637]
		[batch 20/20] avg loss: 0.005332857504230478		[learning rate: 0.00077518]
	Learning Rate: 0.000775175
	LOSS [training: 0.00457462863711764 | validation: 0.004589452931445043]
	TIME [epoch: 8.83 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029794363302684734		[learning rate: 0.00077399]
		[batch 20/20] avg loss: 0.0031388440640807815		[learning rate: 0.0007728]
	Learning Rate: 0.000772799
	LOSS [training: 0.0030591401971746274 | validation: 0.006486820699652292]
	TIME [epoch: 8.83 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004587193751218582		[learning rate: 0.00077161]
		[batch 20/20] avg loss: 0.0012164067598383268		[learning rate: 0.00077043]
	Learning Rate: 0.00077043
	LOSS [training: 0.0029018002555284547 | validation: 0.008383785254530091]
	TIME [epoch: 8.85 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006424745403364837		[learning rate: 0.00076925]
		[batch 20/20] avg loss: 0.006921908074976525		[learning rate: 0.00076807]
	Learning Rate: 0.000768068
	LOSS [training: 0.006673326739170681 | validation: 0.00633362497490905]
	TIME [epoch: 8.84 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030123490180397955		[learning rate: 0.00076689]
		[batch 20/20] avg loss: -0.001311766592827428		[learning rate: 0.00076571]
	Learning Rate: 0.000765714
	LOSS [training: -0.002162057805433612 | validation: 0.0004402878198604038]
	TIME [epoch: 8.82 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003066431572286528		[learning rate: 0.00076454]
		[batch 20/20] avg loss: 0.000914785869769319		[learning rate: 0.00076337]
	Learning Rate: 0.000763367
	LOSS [training: 0.0006107145134989859 | validation: -0.0015959947527649978]
	TIME [epoch: 8.82 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012740844053824594		[learning rate: 0.0007622]
		[batch 20/20] avg loss: 0.0037245156415208216		[learning rate: 0.00076103]
	Learning Rate: 0.000761027
	LOSS [training: 0.0024993000234516402 | validation: 0.002498999014842512]
	TIME [epoch: 8.82 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015101291326978517		[learning rate: 0.00075986]
		[batch 20/20] avg loss: -0.00040684154035546086		[learning rate: 0.00075869]
	Learning Rate: 0.000758694
	LOSS [training: 0.0005516437961711956 | validation: 0.001828997848832751]
	TIME [epoch: 8.84 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007246188589036027		[learning rate: 0.00075753]
		[batch 20/20] avg loss: -0.004347839136753769		[learning rate: 0.00075637]
	Learning Rate: 0.000756368
	LOSS [training: -0.0018116101389250832 | validation: -5.120153138280176e-05]
	TIME [epoch: 8.83 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011161687631408843		[learning rate: 0.00075521]
		[batch 20/20] avg loss: 0.0038016992321791196		[learning rate: 0.00075405]
	Learning Rate: 0.00075405
	LOSS [training: 0.0024589339976600023 | validation: 0.00898090062413933]
	TIME [epoch: 8.83 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025731035612973014		[learning rate: 0.00075289]
		[batch 20/20] avg loss: -0.0044632532443777476		[learning rate: 0.00075174]
	Learning Rate: 0.000751738
	LOSS [training: -0.003518178402837524 | validation: 0.010806422397006491]
	TIME [epoch: 8.84 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027584555315146673		[learning rate: 0.00075059]
		[batch 20/20] avg loss: -0.002266432881379528		[learning rate: 0.00074943]
	Learning Rate: 0.000749434
	LOSS [training: 0.00024601132506756973 | validation: 0.004681967462143414]
	TIME [epoch: 8.85 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025745132935711995		[learning rate: 0.00074828]
		[batch 20/20] avg loss: 0.0005480528026269823		[learning rate: 0.00074714]
	Learning Rate: 0.000747137
	LOSS [training: -0.0010132302454721085 | validation: 0.005612625119511328]
	TIME [epoch: 8.83 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0051575275350250645		[learning rate: 0.00074599]
		[batch 20/20] avg loss: 0.004311910276133172		[learning rate: 0.00074485]
	Learning Rate: 0.000744846
	LOSS [training: 0.004734718905579119 | validation: 0.005133343605048713]
	TIME [epoch: 8.83 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004583983224066586		[learning rate: 0.0007437]
		[batch 20/20] avg loss: 0.0030195156921404226		[learning rate: 0.00074256]
	Learning Rate: 0.000742563
	LOSS [training: 0.003801749458103504 | validation: 0.004677176209847636]
	TIME [epoch: 8.83 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035573117829623595		[learning rate: 0.00074142]
		[batch 20/20] avg loss: 0.004580710324543129		[learning rate: 0.00074029]
	Learning Rate: 0.000740287
	LOSS [training: 0.004069011053752744 | validation: 0.0011606800610615433]
	TIME [epoch: 8.85 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0001372635605239474		[learning rate: 0.00073915]
		[batch 20/20] avg loss: -0.002107119083113901		[learning rate: 0.00073802]
	Learning Rate: 0.000738017
	LOSS [training: -0.0009849277612949767 | validation: 0.003322939570510695]
	TIME [epoch: 8.84 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007732552977389661		[learning rate: 0.00073689]
		[batch 20/20] avg loss: -0.0022424220331578684		[learning rate: 0.00073576]
	Learning Rate: 0.000735755
	LOSS [training: 0.0027450654721158964 | validation: 0.002299529591915132]
	TIME [epoch: 8.83 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005780299253719094		[learning rate: 0.00073463]
		[batch 20/20] avg loss: 0.002254827551032914		[learning rate: 0.0007335]
	Learning Rate: 0.0007335
	LOSS [training: -0.0017627358513430899 | validation: 0.0019081522154124559]
	TIME [epoch: 8.83 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014882001862447133		[learning rate: 0.00073237]
		[batch 20/20] avg loss: 0.0009442449023522863		[learning rate: 0.00073125]
	Learning Rate: 0.000731251
	LOSS [training: -0.0002719776419462136 | validation: -0.00037103803386421186]
	TIME [epoch: 8.84 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012288499928976477		[learning rate: 0.00073013]
		[batch 20/20] avg loss: 0.0018884294265804438		[learning rate: 0.00072901]
	Learning Rate: 0.00072901
	LOSS [training: 0.0015586397097390459 | validation: 0.006707555511764862]
	TIME [epoch: 8.86 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006238216347147397		[learning rate: 0.00072789]
		[batch 20/20] avg loss: -0.0014912192573242828		[learning rate: 0.00072677]
	Learning Rate: 0.000726775
	LOSS [training: 0.0023734985449115568 | validation: 0.011812500109386639]
	TIME [epoch: 8.83 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025280213829332104		[learning rate: 0.00072566]
		[batch 20/20] avg loss: 0.00408556709374604		[learning rate: 0.00072455]
	Learning Rate: 0.000724547
	LOSS [training: 0.003306794238339625 | validation: 0.013909970969642407]
	TIME [epoch: 8.84 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008986821224618395		[learning rate: 0.00072344]
		[batch 20/20] avg loss: 0.0023590722151608555		[learning rate: 0.00072233]
	Learning Rate: 0.000722326
	LOSS [training: 0.0016288771688113476 | validation: 0.010810614981509615]
	TIME [epoch: 8.84 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003620153513303164		[learning rate: 0.00072122]
		[batch 20/20] avg loss: 0.0026647878424402903		[learning rate: 0.00072011]
	Learning Rate: 0.000720112
	LOSS [training: 0.0015134015968853033 | validation: 0.010317733591744805]
	TIME [epoch: 8.85 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016674419296660532		[learning rate: 0.00071901]
		[batch 20/20] avg loss: 0.005235726356740221		[learning rate: 0.0007179]
	Learning Rate: 0.000717904
	LOSS [training: 0.010955072826700375 | validation: 0.012003425224775599]
	TIME [epoch: 8.83 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015258573010387563		[learning rate: 0.0007168]
		[batch 20/20] avg loss: 0.01388202244833123		[learning rate: 0.0007157]
	Learning Rate: 0.000715704
	LOSS [training: 0.014570297729359399 | validation: 0.00975340537546188]
	TIME [epoch: 8.83 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008131483628101798		[learning rate: 0.00071461]
		[batch 20/20] avg loss: 0.0032881545898218586		[learning rate: 0.00071351]
	Learning Rate: 0.00071351
	LOSS [training: 0.005709819108961829 | validation: 0.0031030092685537195]
	TIME [epoch: 8.83 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0045446662261045545		[learning rate: 0.00071242]
		[batch 20/20] avg loss: 0.0075099565660274575		[learning rate: 0.00071132]
	Learning Rate: 0.000711323
	LOSS [training: 0.006027311396066006 | validation: 0.012494149817945076]
	TIME [epoch: 8.85 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004188374099139347		[learning rate: 0.00071023]
		[batch 20/20] avg loss: 0.0031637070791063525		[learning rate: 0.00070914]
	Learning Rate: 0.000709142
	LOSS [training: -0.0005123335100164974 | validation: 0.005971409112604046]
	TIME [epoch: 8.84 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007429156001216179		[learning rate: 0.00070805]
		[batch 20/20] avg loss: -0.001446981914198865		[learning rate: 0.00070697]
	Learning Rate: 0.000706968
	LOSS [training: 0.002991087043508658 | validation: 0.003037150935813232]
	TIME [epoch: 8.83 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002809392732807257		[learning rate: 0.00070588]
		[batch 20/20] avg loss: -0.0027494011161451892		[learning rate: 0.0007048]
	Learning Rate: 0.000704801
	LOSS [training: 2.9995808331034073e-05 | validation: 0.005192655890487199]
	TIME [epoch: 8.83 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006496403958682577		[learning rate: 0.00070372]
		[batch 20/20] avg loss: -0.0017778885251363678		[learning rate: 0.00070264]
	Learning Rate: 0.000702641
	LOSS [training: 0.0023592577167731044 | validation: -0.0018924384539434694]
	TIME [epoch: 8.85 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000502541593394499		[learning rate: 0.00070156]
		[batch 20/20] avg loss: -0.003430477747119815		[learning rate: 0.00070049]
	Learning Rate: 0.000700487
	LOSS [training: -0.0014639680768626577 | validation: 0.00028328974285585316]
	TIME [epoch: 8.84 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031063537631086423		[learning rate: 0.00069941]
		[batch 20/20] avg loss: -0.002443169093743048		[learning rate: 0.00069834]
	Learning Rate: 0.000698339
	LOSS [training: -0.0027747614284258453 | validation: 0.008782174114618242]
	TIME [epoch: 8.83 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021873301239802067		[learning rate: 0.00069727]
		[batch 20/20] avg loss: 4.3503264975134045e-05		[learning rate: 0.0006962]
	Learning Rate: 0.000696199
	LOSS [training: 0.0011154166944776706 | validation: 0.005056922227430738]
	TIME [epoch: 8.83 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011319140874230709		[learning rate: 0.00069513]
		[batch 20/20] avg loss: -0.0015935901792321384		[learning rate: 0.00069406]
	Learning Rate: 0.000694065
	LOSS [training: -0.00023083804590453327 | validation: 0.002191764400445165]
	TIME [epoch: 8.83 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009287249480825868		[learning rate: 0.000693]
		[batch 20/20] avg loss: 0.00025060400595228736		[learning rate: 0.00069194]
	Learning Rate: 0.000691937
	LOSS [training: -0.00451832273743679 | validation: 0.003542957744335221]
	TIME [epoch: 8.85 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00228978188269955		[learning rate: 0.00069088]
		[batch 20/20] avg loss: 0.0004477940685625813		[learning rate: 0.00068982]
	Learning Rate: 0.000689816
	LOSS [training: -0.0009209939070684845 | validation: 0.007953222041436732]
	TIME [epoch: 8.83 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007101200772318532		[learning rate: 0.00068876]
		[batch 20/20] avg loss: -3.8593090210549495e-05		[learning rate: 0.0006877]
	Learning Rate: 0.000687701
	LOSS [training: -0.0035698969312645415 | validation: 0.0012398812963255847]
	TIME [epoch: 8.83 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004477905418057803		[learning rate: 0.00068665]
		[batch 20/20] avg loss: -0.0030114288857919433		[learning rate: 0.00068559]
	Learning Rate: 0.000685593
	LOSS [training: 0.00073323826613293 | validation: 0.0031554642466065036]
	TIME [epoch: 8.83 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002339871991381543		[learning rate: 0.00068454]
		[batch 20/20] avg loss: 0.0024568815062470245		[learning rate: 0.00068349]
	Learning Rate: 0.000683492
	LOSS [training: 5.850475743274077e-05 | validation: 0.008795548214535468]
	TIME [epoch: 8.86 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012524046407335371		[learning rate: 0.00068244]
		[batch 20/20] avg loss: 0.0010865502603183883		[learning rate: 0.0006814]
	Learning Rate: 0.000681397
	LOSS [training: 0.0011694774505259628 | validation: 0.005120842360166592]
	TIME [epoch: 8.83 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00010432941144300168		[learning rate: 0.00068035]
		[batch 20/20] avg loss: -0.0035793716760597555		[learning rate: 0.00067931]
	Learning Rate: 0.000679308
	LOSS [training: -0.0018418505437513785 | validation: -0.000390803900834786]
	TIME [epoch: 8.83 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010977915016493248		[learning rate: 0.00067827]
		[batch 20/20] avg loss: -0.002331952728153755		[learning rate: 0.00067723]
	Learning Rate: 0.000677225
	LOSS [training: -0.00171487211490154 | validation: 0.0029629700010027227]
	TIME [epoch: 8.83 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036060368973842736		[learning rate: 0.00067619]
		[batch 20/20] avg loss: 0.00032034306933006193		[learning rate: 0.00067515]
	Learning Rate: 0.000675149
	LOSS [training: 0.0019631899833571676 | validation: 0.002943631758687039]
	TIME [epoch: 8.85 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025542523248959746		[learning rate: 0.00067411]
		[batch 20/20] avg loss: -0.004130703352426948		[learning rate: 0.00067308]
	Learning Rate: 0.00067308
	LOSS [training: -0.003342477838661461 | validation: -0.004050473920137155]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1379.pth
	Model improved!!!
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030279227221296067		[learning rate: 0.00067205]
		[batch 20/20] avg loss: -0.0020425369944339646		[learning rate: 0.00067102]
	Learning Rate: 0.000671017
	LOSS [training: -0.002535229858281785 | validation: -0.00558581334698705]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1380.pth
	Model improved!!!
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034623050993895683		[learning rate: 0.00066999]
		[batch 20/20] avg loss: 0.0006348855762526792		[learning rate: 0.00066896]
	Learning Rate: 0.00066896
	LOSS [training: -0.0014137097615684452 | validation: 0.004690457426184591]
	TIME [epoch: 8.83 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017769140892382094		[learning rate: 0.00066793]
		[batch 20/20] avg loss: -0.000588900388227576		[learning rate: 0.00066691]
	Learning Rate: 0.000666909
	LOSS [training: -0.0011829072387328926 | validation: 0.003250767843683146]
	TIME [epoch: 8.84 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004258917375054644		[learning rate: 0.00066589]
		[batch 20/20] avg loss: -0.0009942041625530529		[learning rate: 0.00066486]
	Learning Rate: 0.000664865
	LOSS [training: -0.0026265607688038483 | validation: -0.0018069236126152118]
	TIME [epoch: 8.84 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019880791779779924		[learning rate: 0.00066384]
		[batch 20/20] avg loss: -0.005437470391214488		[learning rate: 0.00066283]
	Learning Rate: 0.000662827
	LOSS [training: -0.0017246956066182474 | validation: 0.0032634284616997727]
	TIME [epoch: 8.83 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004122603419779105		[learning rate: 0.00066181]
		[batch 20/20] avg loss: 0.0018958922395008568		[learning rate: 0.00066079]
	Learning Rate: 0.000660795
	LOSS [training: -0.0011133555901391248 | validation: 0.005061391711309009]
	TIME [epoch: 8.83 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005706929376481805		[learning rate: 0.00065978]
		[batch 20/20] avg loss: 0.0037307148121305325		[learning rate: 0.00065877]
	Learning Rate: 0.000658769
	LOSS [training: -0.0009881072821756364 | validation: 0.004523602558770163]
	TIME [epoch: 8.83 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0041577356655565076		[learning rate: 0.00065776]
		[batch 20/20] avg loss: -0.002455629457767252		[learning rate: 0.00065675]
	Learning Rate: 0.00065675
	LOSS [training: 0.0008510531038946279 | validation: 0.006134196669554406]
	TIME [epoch: 8.85 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005100781313531115		[learning rate: 0.00065574]
		[batch 20/20] avg loss: -0.007630068491511118		[learning rate: 0.00065474]
	Learning Rate: 0.000654737
	LOSS [training: -0.0012646435889900013 | validation: -0.001754424810163445]
	TIME [epoch: 8.82 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016725028056234852		[learning rate: 0.00065373]
		[batch 20/20] avg loss: 0.0017280531574891031		[learning rate: 0.00065273]
	Learning Rate: 0.00065273
	LOSS [training: 0.0017002779815562942 | validation: 0.004679962684424255]
	TIME [epoch: 8.82 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005359145252269389		[learning rate: 0.00065173]
		[batch 20/20] avg loss: -0.002560993753740872		[learning rate: 0.00065073]
	Learning Rate: 0.000650729
	LOSS [training: 0.0013990757492642577 | validation: 0.008145750145779972]
	TIME [epoch: 8.82 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023065238175191943		[learning rate: 0.00064973]
		[batch 20/20] avg loss: 0.004692708280956104		[learning rate: 0.00064873]
	Learning Rate: 0.000648734
	LOSS [training: 0.0011930922317184547 | validation: 0.00444782529550024]
	TIME [epoch: 8.83 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033704097574464922		[learning rate: 0.00064774]
		[batch 20/20] avg loss: 2.5737705402245104e-05		[learning rate: 0.00064675]
	Learning Rate: 0.000646745
	LOSS [training: -0.0016723360260221232 | validation: 0.007622977184798603]
	TIME [epoch: 8.83 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003188536320783378		[learning rate: 0.00064575]
		[batch 20/20] avg loss: -0.001112542761730557		[learning rate: 0.00064476]
	Learning Rate: 0.000644763
	LOSS [training: -0.0021505395412569676 | validation: 0.00213198143342378]
	TIME [epoch: 8.82 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012006526873969976		[learning rate: 0.00064377]
		[batch 20/20] avg loss: -1.9574660000553537e-05		[learning rate: 0.00064279]
	Learning Rate: 0.000642786
	LOSS [training: -0.0006101136736987756 | validation: -0.0022705986348653213]
	TIME [epoch: 8.82 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014085121447395981		[learning rate: 0.0006418]
		[batch 20/20] avg loss: 0.0026628943851570025		[learning rate: 0.00064082]
	Learning Rate: 0.000640816
	LOSS [training: 0.0006271911202087022 | validation: 0.0032259588656271517]
	TIME [epoch: 8.84 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033843802593368855		[learning rate: 0.00063983]
		[batch 20/20] avg loss: 0.007085996284289704		[learning rate: 0.00063885]
	Learning Rate: 0.000638852
	LOSS [training: 0.005235188271813294 | validation: 0.0016103125266394116]
	TIME [epoch: 8.83 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000489420287074982		[learning rate: 0.00063787]
		[batch 20/20] avg loss: 0.0006346592310181966		[learning rate: 0.00063689]
	Learning Rate: 0.000636893
	LOSS [training: 0.0005620397590465892 | validation: 0.005336971648545248]
	TIME [epoch: 8.83 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002791873138824721		[learning rate: 0.00063592]
		[batch 20/20] avg loss: 0.012163330259634002		[learning rate: 0.00063494]
	Learning Rate: 0.000634941
	LOSS [training: 0.007477601699229362 | validation: 0.013820953927701995]
	TIME [epoch: 8.83 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0054988367797642575		[learning rate: 0.00063397]
		[batch 20/20] avg loss: 0.0059281042587372886		[learning rate: 0.00063299]
	Learning Rate: 0.000632994
	LOSS [training: 0.005713470519250774 | validation: 0.009432961195437645]
	TIME [epoch: 8.83 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005434437460368651		[learning rate: 0.00063202]
		[batch 20/20] avg loss: 0.0023114041581458024		[learning rate: 0.00063105]
	Learning Rate: 0.000631054
	LOSS [training: 0.0038729208092572265 | validation: 0.00976371429851957]
	TIME [epoch: 8.85 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008075287900004847		[learning rate: 0.00063009]
		[batch 20/20] avg loss: -0.0006964217893392537		[learning rate: 0.00062912]
	Learning Rate: 0.00062912
	LOSS [training: 0.003689433055332797 | validation: 0.006770293077318412]
	TIME [epoch: 8.83 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013843178586903024		[learning rate: 0.00062815]
		[batch 20/20] avg loss: -0.002072784940328437		[learning rate: 0.00062719]
	Learning Rate: 0.000627191
	LOSS [training: -0.0017285513995093696 | validation: 0.00265830386527384]
	TIME [epoch: 8.83 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004269004520842049		[learning rate: 0.00062623]
		[batch 20/20] avg loss: -0.001113123340552437		[learning rate: 0.00062527]
	Learning Rate: 0.000625269
	LOSS [training: 0.0015779405901448062 | validation: 0.0014362384161588865]
	TIME [epoch: 8.83 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003930267824608032		[learning rate: 0.00062431]
		[batch 20/20] avg loss: 9.684026071942516e-05		[learning rate: 0.00062335]
	Learning Rate: 0.000623352
	LOSS [training: -0.00014809326087068922 | validation: 0.009662617847809219]
	TIME [epoch: 8.85 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011962009140023697		[learning rate: 0.0006224]
		[batch 20/20] avg loss: 0.004357000754339545		[learning rate: 0.00062144]
	Learning Rate: 0.000621441
	LOSS [training: 0.00815950494718162 | validation: 0.00702570366896073]
	TIME [epoch: 8.83 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005960688163649779		[learning rate: 0.00062049]
		[batch 20/20] avg loss: -0.0009456206778242504		[learning rate: 0.00061954]
	Learning Rate: 0.000619536
	LOSS [training: 0.0025075337429127645 | validation: 0.0013949555394353725]
	TIME [epoch: 8.84 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015054791551444026		[learning rate: 0.00061859]
		[batch 20/20] avg loss: 0.006023774172678767		[learning rate: 0.00061764]
	Learning Rate: 0.000617637
	LOSS [training: 0.002259147508767182 | validation: 0.0030879830280376337]
	TIME [epoch: 8.83 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001573733620307893		[learning rate: 0.00061669]
		[batch 20/20] avg loss: 0.0057117081625219284		[learning rate: 0.00061574]
	Learning Rate: 0.000615744
	LOSS [training: 0.0036427208914149105 | validation: 0.010987656538696303]
	TIME [epoch: 8.85 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00561388281386892		[learning rate: 0.0006148]
		[batch 20/20] avg loss: -0.0010470755834267494		[learning rate: 0.00061386]
	Learning Rate: 0.000613856
	LOSS [training: 0.0022834036152210853 | validation: 0.0018648770221386356]
	TIME [epoch: 8.84 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003984610070575311		[learning rate: 0.00061291]
		[batch 20/20] avg loss: -0.005047564048695666		[learning rate: 0.00061197]
	Learning Rate: 0.000611974
	LOSS [training: -0.0005314769890601773 | validation: 3.371816452322216e-05]
	TIME [epoch: 8.83 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: -7.540213422888592e-05		[learning rate: 0.00061104]
		[batch 20/20] avg loss: -0.002166185232473764		[learning rate: 0.0006101]
	Learning Rate: 0.000610099
	LOSS [training: -0.0011207936833513249 | validation: -0.0018096238192125027]
	TIME [epoch: 8.83 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019292361378846067		[learning rate: 0.00060916]
		[batch 20/20] avg loss: -0.0040747019134274795		[learning rate: 0.00060823]
	Learning Rate: 0.000608228
	LOSS [training: -0.0010727328877714363 | validation: 0.0014119285402938667]
	TIME [epoch: 8.83 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012029188121108578		[learning rate: 0.0006073]
		[batch 20/20] avg loss: 0.009320794564540833		[learning rate: 0.00060636]
	Learning Rate: 0.000606364
	LOSS [training: 0.005261856688325845 | validation: 0.0053001419275309925]
	TIME [epoch: 8.85 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0056912234490310906		[learning rate: 0.00060543]
		[batch 20/20] avg loss: 9.572197484611598e-05		[learning rate: 0.00060451]
	Learning Rate: 0.000604505
	LOSS [training: 0.002893472711938604 | validation: 0.0028901691396984834]
	TIME [epoch: 8.83 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014618631121144684		[learning rate: 0.00060358]
		[batch 20/20] avg loss: -0.003993908233543448		[learning rate: 0.00060265]
	Learning Rate: 0.000602652
	LOSS [training: -0.0012660225607144888 | validation: 0.002272243343597704]
	TIME [epoch: 8.83 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005277720762451148		[learning rate: 0.00060173]
		[batch 20/20] avg loss: -0.0008170798482513353		[learning rate: 0.0006008]
	Learning Rate: 0.000600805
	LOSS [training: 0.0022303204570999066 | validation: 0.004924616958813706]
	TIME [epoch: 8.83 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016924208908293264		[learning rate: 0.00059988]
		[batch 20/20] avg loss: 0.008156465301895793		[learning rate: 0.00059896]
	Learning Rate: 0.000598963
	LOSS [training: 0.003232022205533234 | validation: 0.011486099045474766]
	TIME [epoch: 8.86 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00606917012603479		[learning rate: 0.00059804]
		[batch 20/20] avg loss: 0.0020342344100534905		[learning rate: 0.00059713]
	Learning Rate: 0.000597127
	LOSS [training: 0.004051702268044138 | validation: -0.00019992367943504335]
	TIME [epoch: 8.84 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023596907644553517		[learning rate: 0.00059621]
		[batch 20/20] avg loss: 0.0004555925249683966		[learning rate: 0.0005953]
	Learning Rate: 0.000595296
	LOSS [training: -0.0009520491197434776 | validation: 0.011197023998375525]
	TIME [epoch: 8.83 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003704514596715161		[learning rate: 0.00059438]
		[batch 20/20] avg loss: 0.0031349390323512647		[learning rate: 0.00059347]
	Learning Rate: 0.000593472
	LOSS [training: 0.0034197268145332122 | validation: 0.00913521686708947]
	TIME [epoch: 8.83 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005692462173306681		[learning rate: 0.00059256]
		[batch 20/20] avg loss: -0.001472072034570199		[learning rate: 0.00059165]
	Learning Rate: 0.000591652
	LOSS [training: 0.0021101950693682412 | validation: 0.012760657191154027]
	TIME [epoch: 8.85 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000640661411783109		[learning rate: 0.00059074]
		[batch 20/20] avg loss: -0.0001847290680855631		[learning rate: 0.00058984]
	Learning Rate: 0.000589839
	LOSS [training: 0.00022796617184877286 | validation: 0.003930393866192603]
	TIME [epoch: 8.84 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016099290048836653		[learning rate: 0.00058893]
		[batch 20/20] avg loss: -0.004339650790118425		[learning rate: 0.00058803]
	Learning Rate: 0.000588031
	LOSS [training: -0.00136486089261738 | validation: -0.0003065377822117233]
	TIME [epoch: 8.83 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033180629049268688		[learning rate: 0.00058713]
		[batch 20/20] avg loss: -0.0016465164491955414		[learning rate: 0.00058623]
	Learning Rate: 0.000586228
	LOSS [training: -0.002482289677061205 | validation: 0.0023034259265881044]
	TIME [epoch: 8.83 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021025724924795923		[learning rate: 0.00058533]
		[batch 20/20] avg loss: 0.00031433222561594114		[learning rate: 0.00058443]
	Learning Rate: 0.000584431
	LOSS [training: -0.0008941201334318253 | validation: 0.002902569897030503]
	TIME [epoch: 8.84 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00032461898066177513		[learning rate: 0.00058353]
		[batch 20/20] avg loss: -0.0016585935738674979		[learning rate: 0.00058264]
	Learning Rate: 0.000582639
	LOSS [training: -0.0006669872966028612 | validation: 0.0029709820199733524]
	TIME [epoch: 8.85 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00048715671697180894		[learning rate: 0.00058175]
		[batch 20/20] avg loss: 0.003142724597674814		[learning rate: 0.00058085]
	Learning Rate: 0.000580854
	LOSS [training: 0.0018149406573233114 | validation: 0.00524800905279852]
	TIME [epoch: 8.83 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027684470368937197		[learning rate: 0.00057996]
		[batch 20/20] avg loss: 0.00046044305393886514		[learning rate: 0.00057907]
	Learning Rate: 0.000579073
	LOSS [training: -0.001154001991477427 | validation: 0.0021052589922886033]
	TIME [epoch: 8.83 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002655498053152315		[learning rate: 0.00057818]
		[batch 20/20] avg loss: -0.002450850817377412		[learning rate: 0.0005773]
	Learning Rate: 0.000577298
	LOSS [training: -0.0025531744352648633 | validation: 0.003217919360781979]
	TIME [epoch: 8.83 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008883765651075305		[learning rate: 0.00057641]
		[batch 20/20] avg loss: -0.006822900917555576		[learning rate: 0.00057553]
	Learning Rate: 0.000575528
	LOSS [training: -0.002967262176224023 | validation: -0.0019454379856969459]
	TIME [epoch: 8.85 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006770044473943702		[learning rate: 0.00057465]
		[batch 20/20] avg loss: 0.0013820873117606689		[learning rate: 0.00057376]
	Learning Rate: 0.000573764
	LOSS [training: -0.0026939785810915157 | validation: -0.0022581916207520037]
	TIME [epoch: 8.84 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0039712190993838195		[learning rate: 0.00057288]
		[batch 20/20] avg loss: -0.008217336214878591		[learning rate: 0.00057201]
	Learning Rate: 0.000572005
	LOSS [training: -0.002123058557747386 | validation: -0.0031479455034043712]
	TIME [epoch: 8.83 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004414651602224819		[learning rate: 0.00057113]
		[batch 20/20] avg loss: 0.001166741458943437		[learning rate: 0.00057025]
	Learning Rate: 0.000570252
	LOSS [training: -0.001623955071640691 | validation: 0.004532363044758616]
	TIME [epoch: 8.83 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009373895636564256		[learning rate: 0.00056938]
		[batch 20/20] avg loss: 0.0021053296973214215		[learning rate: 0.0005685]
	Learning Rate: 0.000568504
	LOSS [training: 0.0005839700668324977 | validation: 0.007360589378476246]
	TIME [epoch: 8.85 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00330410622159748		[learning rate: 0.00056763]
		[batch 20/20] avg loss: 0.005146232103452263		[learning rate: 0.00056676]
	Learning Rate: 0.000566761
	LOSS [training: 0.0009210629409273916 | validation: 0.009798860734469133]
	TIME [epoch: 8.84 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002134822070238092		[learning rate: 0.00056589]
		[batch 20/20] avg loss: 0.0003172961173101358		[learning rate: 0.00056502]
	Learning Rate: 0.000565024
	LOSS [training: 0.0012260590937741137 | validation: -0.0008037272755933964]
	TIME [epoch: 8.84 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004143806892223128		[learning rate: 0.00056416]
		[batch 20/20] avg loss: -0.005081150674573548		[learning rate: 0.00056329]
	Learning Rate: 0.000563292
	LOSS [training: -0.00046867189117520995 | validation: 4.445803393814048e-05]
	TIME [epoch: 8.83 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004819235753114293		[learning rate: 0.00056243]
		[batch 20/20] avg loss: -0.005824243005684161		[learning rate: 0.00056156]
	Learning Rate: 0.000561565
	LOSS [training: -0.005321739379399228 | validation: 0.0039773962386505345]
	TIME [epoch: 8.85 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022503493777916187		[learning rate: 0.0005607]
		[batch 20/20] avg loss: -0.0021916548195666254		[learning rate: 0.00055984]
	Learning Rate: 0.000559844
	LOSS [training: -0.0022210020986791214 | validation: 0.002655693284263156]
	TIME [epoch: 8.83 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034031303022118257		[learning rate: 0.00055898]
		[batch 20/20] avg loss: -0.0010601765010919842		[learning rate: 0.00055813]
	Learning Rate: 0.000558127
	LOSS [training: 0.0011714769005599208 | validation: -0.0042657268944444725]
	TIME [epoch: 8.84 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015491501335674353		[learning rate: 0.00055727]
		[batch 20/20] avg loss: -0.0027270138826702766		[learning rate: 0.00055642]
	Learning Rate: 0.000556416
	LOSS [training: -0.0005889318745514204 | validation: 0.003225335365352526]
	TIME [epoch: 8.83 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017776907192771079		[learning rate: 0.00055556]
		[batch 20/20] avg loss: -0.005522482502537059		[learning rate: 0.00055471]
	Learning Rate: 0.000554711
	LOSS [training: -0.0018723958916299754 | validation: 0.0027828699399530606]
	TIME [epoch: 8.83 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009368096485566103		[learning rate: 0.00055386]
		[batch 20/20] avg loss: -0.0015543103138413918		[learning rate: 0.00055301]
	Learning Rate: 0.00055301
	LOSS [training: -0.0003087503326423909 | validation: 0.0032933967855455055]
	TIME [epoch: 8.85 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004247235624466342		[learning rate: 0.00055216]
		[batch 20/20] avg loss: -0.003431137384139746		[learning rate: 0.00055132]
	Learning Rate: 0.000551315
	LOSS [training: -0.0015032069108465557 | validation: 0.0038436042833413396]
	TIME [epoch: 8.83 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021575909169251504		[learning rate: 0.00055047]
		[batch 20/20] avg loss: -0.0034224988985141905		[learning rate: 0.00054963]
	Learning Rate: 0.000549625
	LOSS [training: -0.0006324539907945204 | validation: 0.0004618068739820658]
	TIME [epoch: 8.82 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004009218512907878		[learning rate: 0.00054878]
		[batch 20/20] avg loss: -0.007808730032132781		[learning rate: 0.00054794]
	Learning Rate: 0.00054794
	LOSS [training: -0.005908974272520329 | validation: 0.0037793712754795646]
	TIME [epoch: 8.82 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019053384743223365		[learning rate: 0.0005471]
		[batch 20/20] avg loss: -0.004612827324673761		[learning rate: 0.00054626]
	Learning Rate: 0.000546261
	LOSS [training: -0.003259082899498048 | validation: 0.0035740635186633208]
	TIME [epoch: 8.85 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007655496616086435		[learning rate: 0.00054542]
		[batch 20/20] avg loss: 0.006715707471657871		[learning rate: 0.00054459]
	Learning Rate: 0.000544586
	LOSS [training: -0.0004698945722142818 | validation: 0.009724296965023627]
	TIME [epoch: 8.83 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001269860811719163		[learning rate: 0.00054375]
		[batch 20/20] avg loss: 0.0024790824568347346		[learning rate: 0.00054292]
	Learning Rate: 0.000542917
	LOSS [training: 0.0006046108225577856 | validation: 0.0025223496935413738]
	TIME [epoch: 8.84 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006482492334606958		[learning rate: 0.00054208]
		[batch 20/20] avg loss: -0.005900930396507496		[learning rate: 0.00054125]
	Learning Rate: 0.000541253
	LOSS [training: -0.006191711365557228 | validation: -0.00038038898801531643]
	TIME [epoch: 8.83 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004134165350014962		[learning rate: 0.00054042]
		[batch 20/20] avg loss: -0.004973598442237783		[learning rate: 0.00053959]
	Learning Rate: 0.000539593
	LOSS [training: -0.004553881896126373 | validation: 0.003130584657318446]
	TIME [epoch: 8.84 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00026452240477318545		[learning rate: 0.00053877]
		[batch 20/20] avg loss: -0.001446589700204421		[learning rate: 0.00053794]
	Learning Rate: 0.000537939
	LOSS [training: -0.0008555560524888033 | validation: 0.0025607737894020105]
	TIME [epoch: 8.82 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00027287106476714206		[learning rate: 0.00053711]
		[batch 20/20] avg loss: 0.0012039233913574664		[learning rate: 0.00053629]
	Learning Rate: 0.00053629
	LOSS [training: 0.0007383972280623045 | validation: 0.0002892825943470224]
	TIME [epoch: 8.83 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017195300709450044		[learning rate: 0.00053547]
		[batch 20/20] avg loss: -0.0018233666102354423		[learning rate: 0.00053465]
	Learning Rate: 0.000534646
	LOSS [training: -0.001771448340590224 | validation: 0.002479884264012458]
	TIME [epoch: 8.83 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005130220120314954		[learning rate: 0.00053383]
		[batch 20/20] avg loss: 0.007500846534456014		[learning rate: 0.00053301]
	Learning Rate: 0.000533007
	LOSS [training: 0.004006934273243755 | validation: 0.005046082188060391]
	TIME [epoch: 8.83 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003553832808671712		[learning rate: 0.00053219]
		[batch 20/20] avg loss: 0.007154401087338211		[learning rate: 0.00053137]
	Learning Rate: 0.000531374
	LOSS [training: 0.005354116948004962 | validation: 0.002373826512121365]
	TIME [epoch: 8.85 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006165835168042449		[learning rate: 0.00053056]
		[batch 20/20] avg loss: -0.003158457412872595		[learning rate: 0.00052974]
	Learning Rate: 0.000529745
	LOSS [training: 0.001503688877584926 | validation: 0.0008490341306305318]
	TIME [epoch: 8.83 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001476428189637922		[learning rate: 0.00052893]
		[batch 20/20] avg loss: 0.003306608911100778		[learning rate: 0.00052812]
	Learning Rate: 0.000528121
	LOSS [training: 0.0023915185503693495 | validation: 0.0033015880251403363]
	TIME [epoch: 8.83 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009608378171426419		[learning rate: 0.00052731]
		[batch 20/20] avg loss: -0.0016644237103500415		[learning rate: 0.0005265]
	Learning Rate: 0.000526502
	LOSS [training: -0.00035179294660369974 | validation: 0.003622384779229471]
	TIME [epoch: 8.82 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010477419953236402		[learning rate: 0.00052569]
		[batch 20/20] avg loss: 0.0001313866426054796		[learning rate: 0.00052489]
	Learning Rate: 0.000524888
	LOSS [training: -0.0004581776763590803 | validation: -0.000994527310186025]
	TIME [epoch: 8.85 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023680527183810854		[learning rate: 0.00052408]
		[batch 20/20] avg loss: 0.0034154480415349874		[learning rate: 0.00052328]
	Learning Rate: 0.000523279
	LOSS [training: 0.002891750379958036 | validation: -0.002389466797510349]
	TIME [epoch: 8.83 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006862170316849511		[learning rate: 0.00052248]
		[batch 20/20] avg loss: 0.0013385710190518963		[learning rate: 0.00052167]
	Learning Rate: 0.000521675
	LOSS [training: 0.0010123940253684237 | validation: 0.0044138341645662505]
	TIME [epoch: 8.83 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038049450143831888		[learning rate: 0.00052087]
		[batch 20/20] avg loss: -0.00556179356990007		[learning rate: 0.00052008]
	Learning Rate: 0.000520076
	LOSS [training: -0.0008784242777584404 | validation: -0.0031237850361646767]
	TIME [epoch: 8.83 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005927512216386448		[learning rate: 0.00051928]
		[batch 20/20] avg loss: -0.001155147612943033		[learning rate: 0.00051848]
	Learning Rate: 0.000518482
	LOSS [training: -0.00354132991466474 | validation: -0.005123852531422531]
	TIME [epoch: 8.84 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028825770357360767		[learning rate: 0.00051769]
		[batch 20/20] avg loss: -0.0030169142327504117		[learning rate: 0.00051689]
	Learning Rate: 0.000516892
	LOSS [training: -0.002949745634243245 | validation: -0.004303786554253742]
	TIME [epoch: 8.84 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0070055814848447836		[learning rate: 0.0005161]
		[batch 20/20] avg loss: -0.003646023780393648		[learning rate: 0.00051531]
	Learning Rate: 0.000515308
	LOSS [training: -0.005325802632619216 | validation: 0.010636058980331441]
	TIME [epoch: 8.83 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025117396263765286		[learning rate: 0.00051452]
		[batch 20/20] avg loss: -7.424487163945232e-05		[learning rate: 0.00051373]
	Learning Rate: 0.000513728
	LOSS [training: 0.001218747377368538 | validation: 0.002505192907102936]
	TIME [epoch: 8.82 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007976427084805723		[learning rate: 0.00051294]
		[batch 20/20] avg loss: -0.0013907924586050802		[learning rate: 0.00051215]
	Learning Rate: 0.000512153
	LOSS [training: -0.0010942175835428262 | validation: 0.01097270720838871]
	TIME [epoch: 8.85 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: -5.166085747184065e-05		[learning rate: 0.00051137]
		[batch 20/20] avg loss: 0.0018359751835232828		[learning rate: 0.00051058]
	Learning Rate: 0.000510583
	LOSS [training: 0.000892157163025721 | validation: 0.0004832484778580872]
	TIME [epoch: 8.83 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021458777965309445		[learning rate: 0.0005098]
		[batch 20/20] avg loss: -0.005249122912021856		[learning rate: 0.00050902]
	Learning Rate: 0.000509018
	LOSS [training: -0.0015516225577454552 | validation: -0.0056433648426789055]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1470.pth
	Model improved!!!
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038419299995114536		[learning rate: 0.00050824]
		[batch 20/20] avg loss: -0.002012328982894266		[learning rate: 0.00050746]
	Learning Rate: 0.000507458
	LOSS [training: -0.0029271294912028593 | validation: -0.0019273027361579515]
	TIME [epoch: 8.84 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00048021561379998494		[learning rate: 0.00050668]
		[batch 20/20] avg loss: -0.0013453633933180136		[learning rate: 0.0005059]
	Learning Rate: 0.000505902
	LOSS [training: -0.0009127895035589993 | validation: 0.0018089490058110671]
	TIME [epoch: 8.83 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004737629276967837		[learning rate: 0.00050513]
		[batch 20/20] avg loss: 0.0017445608526945509		[learning rate: 0.00050435]
	Learning Rate: 0.000504351
	LOSS [training: -0.0014965342121366426 | validation: 0.006871354080057723]
	TIME [epoch: 8.84 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002547520099521911		[learning rate: 0.00050358]
		[batch 20/20] avg loss: -0.0010599407466247797		[learning rate: 0.00050281]
	Learning Rate: 0.000502806
	LOSS [training: 0.0007437896764485657 | validation: 0.0025460012058553974]
	TIME [epoch: 8.82 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006328022083089869		[learning rate: 0.00050203]
		[batch 20/20] avg loss: -0.004355985613914032		[learning rate: 0.00050126]
	Learning Rate: 0.000501264
	LOSS [training: 0.000986018234587919 | validation: 0.0019026092295636307]
	TIME [epoch: 8.83 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006398438779841182		[learning rate: 0.0005005]
		[batch 20/20] avg loss: -0.001970387304504282		[learning rate: 0.00049973]
	Learning Rate: 0.000499728
	LOSS [training: -0.004184413042172733 | validation: -0.005632862629682811]
	TIME [epoch: 8.83 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005930214101256478		[learning rate: 0.00049896]
		[batch 20/20] avg loss: 0.0005014299128053304		[learning rate: 0.0004982]
	Learning Rate: 0.000498196
	LOSS [training: -0.002714392094225574 | validation: -0.0030693193473981845]
	TIME [epoch: 8.85 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014317236189823534		[learning rate: 0.00049743]
		[batch 20/20] avg loss: -0.0031188125443618007		[learning rate: 0.00049667]
	Learning Rate: 0.000496668
	LOSS [training: -0.0022752680816720766 | validation: 0.0032878498848730826]
	TIME [epoch: 8.83 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025639030516855614		[learning rate: 0.00049591]
		[batch 20/20] avg loss: -0.001889743823198078		[learning rate: 0.00049515]
	Learning Rate: 0.000495146
	LOSS [training: 0.000337079614243742 | validation: -0.004546244729819494]
	TIME [epoch: 8.83 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002303688931195311		[learning rate: 0.00049439]
		[batch 20/20] avg loss: 0.0039669102229099576		[learning rate: 0.00049363]
	Learning Rate: 0.000493628
	LOSS [training: 0.0008316106458573236 | validation: -0.0007761252788540475]
	TIME [epoch: 8.83 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0046439802413221175		[learning rate: 0.00049287]
		[batch 20/20] avg loss: 0.0018223574798301293		[learning rate: 0.00049211]
	Learning Rate: 0.000492115
	LOSS [training: 0.0032331688605761223 | validation: 0.0007521177149912449]
	TIME [epoch: 8.85 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007718866840288613		[learning rate: 0.00049136]
		[batch 20/20] avg loss: 0.002055900625859656		[learning rate: 0.00049061]
	Learning Rate: 0.000490606
	LOSS [training: -0.002831483107214478 | validation: -0.003209743608489642]
	TIME [epoch: 8.83 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004227420812832781		[learning rate: 0.00048985]
		[batch 20/20] avg loss: -0.003978090695448798		[learning rate: 0.0004891]
	Learning Rate: 0.000489103
	LOSS [training: -0.0041027557541407885 | validation: 0.0077464616471603544]
	TIME [epoch: 8.82 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.875259223716849e-05		[learning rate: 0.00048835]
		[batch 20/20] avg loss: 0.0017940015133255664		[learning rate: 0.0004876]
	Learning Rate: 0.000487603
	LOSS [training: 0.0009263770527813677 | validation: 0.005171645918619096]
	TIME [epoch: 8.83 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002874190592318268		[learning rate: 0.00048686]
		[batch 20/20] avg loss: -0.005498281815717687		[learning rate: 0.00048611]
	Learning Rate: 0.000486109
	LOSS [training: -0.004186236204017977 | validation: 0.003399234655623861]
	TIME [epoch: 8.83 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008484445299017089		[learning rate: 0.00048536]
		[batch 20/20] avg loss: 0.002383333042317835		[learning rate: 0.00048462]
	Learning Rate: 0.000484619
	LOSS [training: 0.005433889170667461 | validation: -0.002493525211655435]
	TIME [epoch: 8.84 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006132455565566946		[learning rate: 0.00048388]
		[batch 20/20] avg loss: 0.0001113985013461655		[learning rate: 0.00048313]
	Learning Rate: 0.000483133
	LOSS [training: 0.0003623220289514301 | validation: -0.0014940528047010602]
	TIME [epoch: 8.83 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0042319289917624545		[learning rate: 0.00048239]
		[batch 20/20] avg loss: -0.0020711524094123787		[learning rate: 0.00048165]
	Learning Rate: 0.000481652
	LOSS [training: -0.003151540700587417 | validation: 0.0018225007035862421]
	TIME [epoch: 8.83 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013031314134271843		[learning rate: 0.00048091]
		[batch 20/20] avg loss: 0.0002739240715766444		[learning rate: 0.00048018]
	Learning Rate: 0.000480175
	LOSS [training: -0.0005146036709252702 | validation: -0.00036322883171526366]
	TIME [epoch: 8.83 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005835858971775477		[learning rate: 0.00047944]
		[batch 20/20] avg loss: -0.007461656370016343		[learning rate: 0.0004787]
	Learning Rate: 0.000478704
	LOSS [training: -0.006648757670895909 | validation: 0.007496427973250893]
	TIME [epoch: 8.84 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032507554153196686		[learning rate: 0.00047797]
		[batch 20/20] avg loss: -0.008794414748626862		[learning rate: 0.00047724]
	Learning Rate: 0.000477236
	LOSS [training: -0.0060225850819732646 | validation: -0.003904057919928354]
	TIME [epoch: 8.82 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006894549281566384		[learning rate: 0.0004765]
		[batch 20/20] avg loss: 0.00036469711805309495		[learning rate: 0.00047577]
	Learning Rate: 0.000475773
	LOSS [training: -0.003264926081756643 | validation: 0.004626658441460462]
	TIME [epoch: 8.83 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0040636875230967325		[learning rate: 0.00047504]
		[batch 20/20] avg loss: -0.004027297559493695		[learning rate: 0.00047431]
	Learning Rate: 0.000474315
	LOSS [training: 1.8194981801518752e-05 | validation: -0.0036807908232207275]
	TIME [epoch: 8.82 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002637715755880634		[learning rate: 0.00047359]
		[batch 20/20] avg loss: -0.0014924413502751751		[learning rate: 0.00047286]
	Learning Rate: 0.000472861
	LOSS [training: 0.0005726372028027291 | validation: -0.0014754576025883966]
	TIME [epoch: 8.85 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031224770936718613		[learning rate: 0.00047214]
		[batch 20/20] avg loss: -0.0009845265226982266		[learning rate: 0.00047141]
	Learning Rate: 0.000471411
	LOSS [training: -0.002053501808185044 | validation: 0.003204299418948762]
	TIME [epoch: 8.84 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025575151381409785		[learning rate: 0.00047069]
		[batch 20/20] avg loss: -0.00033615884498582337		[learning rate: 0.00046997]
	Learning Rate: 0.000469966
	LOSS [training: 0.0011106781465775777 | validation: 0.006404613355676912]
	TIME [epoch: 8.83 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003085695388600866		[learning rate: 0.00046925]
		[batch 20/20] avg loss: -0.0031884680403021226		[learning rate: 0.00046853]
	Learning Rate: 0.000468526
	LOSS [training: -0.0031370817144514944 | validation: 0.00032649397351647974]
	TIME [epoch: 8.84 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007040587908302136		[learning rate: 0.00046781]
		[batch 20/20] avg loss: -0.004935930972075016		[learning rate: 0.00046709]
	Learning Rate: 0.000467089
	LOSS [training: -0.0021159360906224008 | validation: -0.001837006230829932]
	TIME [epoch: 8.84 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003147133432204678		[learning rate: 0.00046637]
		[batch 20/20] avg loss: -0.0012847255244692025		[learning rate: 0.00046566]
	Learning Rate: 0.000465658
	LOSS [training: -0.00221592947833694 | validation: -0.0015060691894029084]
	TIME [epoch: 8.84 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023984406510852334		[learning rate: 0.00046494]
		[batch 20/20] avg loss: -0.0008114381689607704		[learning rate: 0.00046423]
	Learning Rate: 0.00046423
	LOSS [training: -0.001604939410023002 | validation: 0.0019778621341230576]
	TIME [epoch: 8.83 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00013934323105594082		[learning rate: 0.00046352]
		[batch 20/20] avg loss: -0.005432592029902037		[learning rate: 0.00046281]
	Learning Rate: 0.000462807
	LOSS [training: -0.0026466243994230482 | validation: 0.004692034029723497]
	TIME [epoch: 8.82 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006592897798521314		[learning rate: 0.0004621]
		[batch 20/20] avg loss: -0.00028107580015432516		[learning rate: 0.00046139]
	Learning Rate: 0.000461388
	LOSS [training: -0.003436986799337819 | validation: -0.006290331180361422]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1502.pth
	Model improved!!!
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00824259878918485		[learning rate: 0.00046068]
		[batch 20/20] avg loss: -0.0030572251024236627		[learning rate: 0.00045997]
	Learning Rate: 0.000459974
	LOSS [training: -0.005649911945804257 | validation: 0.00041033937930382795]
	TIME [epoch: 8.85 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0057192808788924295		[learning rate: 0.00045927]
		[batch 20/20] avg loss: -0.004414968538636368		[learning rate: 0.00045856]
	Learning Rate: 0.000458564
	LOSS [training: -0.005067124708764399 | validation: 0.00328068313653697]
	TIME [epoch: 8.83 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009010314680427719		[learning rate: 0.00045786]
		[batch 20/20] avg loss: -0.006312994227523637		[learning rate: 0.00045716]
	Learning Rate: 0.000457158
	LOSS [training: -0.0036070128477832045 | validation: 0.007999097915681654]
	TIME [epoch: 8.82 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008477716469476499		[learning rate: 0.00045646]
		[batch 20/20] avg loss: -0.005810428353684702		[learning rate: 0.00045576]
	Learning Rate: 0.000455757
	LOSS [training: -0.0071440724115806 | validation: 0.0043233125421035]
	TIME [epoch: 8.83 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0041093094733640045		[learning rate: 0.00045506]
		[batch 20/20] avg loss: -0.004567215293659198		[learning rate: 0.00045436]
	Learning Rate: 0.00045436
	LOSS [training: -0.004338262383511602 | validation: 0.0017212999558918608]
	TIME [epoch: 8.85 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0051982428218408675		[learning rate: 0.00045366]
		[batch 20/20] avg loss: -0.003535710381609754		[learning rate: 0.00045297]
	Learning Rate: 0.000452967
	LOSS [training: 0.0008312662201155563 | validation: 0.0007978782661088898]
	TIME [epoch: 8.82 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004815737468256562		[learning rate: 0.00045227]
		[batch 20/20] avg loss: -0.004599548959247199		[learning rate: 0.00045158]
	Learning Rate: 0.000451579
	LOSS [training: -0.0020589876062107717 | validation: 0.001180900380944941]
	TIME [epoch: 8.82 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003589811717058327		[learning rate: 0.00045089]
		[batch 20/20] avg loss: -0.0020444862896951592		[learning rate: 0.00045019]
	Learning Rate: 0.000450194
	LOSS [training: -0.0028171490033767435 | validation: 0.004980674012809002]
	TIME [epoch: 8.83 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00271127040435762		[learning rate: 0.0004495]
		[batch 20/20] avg loss: -0.004528509374043846		[learning rate: 0.00044881]
	Learning Rate: 0.000448814
	LOSS [training: -0.0036198898892007337 | validation: -0.0016032341715301668]
	TIME [epoch: 8.84 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020877329099159175		[learning rate: 0.00044813]
		[batch 20/20] avg loss: -0.005068486845045786		[learning rate: 0.00044744]
	Learning Rate: 0.000447439
	LOSS [training: -0.0035781098774808513 | validation: 0.005160535971635386]
	TIME [epoch: 8.83 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007265114300385842		[learning rate: 0.00044675]
		[batch 20/20] avg loss: -0.0037494754803598317		[learning rate: 0.00044607]
	Learning Rate: 0.000446067
	LOSS [training: 0.0017578194100130048 | validation: 0.0012798668154631722]
	TIME [epoch: 8.82 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003204459008164013		[learning rate: 0.00044538]
		[batch 20/20] avg loss: -0.0027734982931117438		[learning rate: 0.0004447]
	Learning Rate: 0.0004447
	LOSS [training: -0.0029889786506378793 | validation: 0.0032963832679027893]
	TIME [epoch: 8.82 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003789516718011021		[learning rate: 0.00044402]
		[batch 20/20] avg loss: -2.9369132059710877e-05		[learning rate: 0.00044334]
	Learning Rate: 0.000443336
	LOSS [training: -0.0019094429250353664 | validation: 0.002595069977429384]
	TIME [epoch: 8.83 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017338381749292056		[learning rate: 0.00044266]
		[batch 20/20] avg loss: -0.002416298717527808		[learning rate: 0.00044198]
	Learning Rate: 0.000441977
	LOSS [training: -0.002075068446228506 | validation: -0.0003774831683692831]
	TIME [epoch: 8.84 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006195941663470079		[learning rate: 0.0004413]
		[batch 20/20] avg loss: -0.0037185548647546128		[learning rate: 0.00044062]
	Learning Rate: 0.000440622
	LOSS [training: -0.004957248264112348 | validation: 0.004072068006057187]
	TIME [epoch: 8.82 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004806305323906029		[learning rate: 0.00043995]
		[batch 20/20] avg loss: -0.0012650013600333564		[learning rate: 0.00043927]
	Learning Rate: 0.000439272
	LOSS [training: 0.0017706519819363359 | validation: -0.006850298301969944]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1518.pth
	Model improved!!!
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011577979350194197		[learning rate: 0.0004386]
		[batch 20/20] avg loss: 0.0019232897170376319		[learning rate: 0.00043793]
	Learning Rate: 0.000437925
	LOSS [training: 0.0015405438260285267 | validation: 0.008269803987896597]
	TIME [epoch: 8.83 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00042337487142547124		[learning rate: 0.00043725]
		[batch 20/20] avg loss: -0.006242458292104108		[learning rate: 0.00043658]
	Learning Rate: 0.000436583
	LOSS [training: -0.0033329165817647887 | validation: 0.0032031996821648]
	TIME [epoch: 8.84 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006034941855178647		[learning rate: 0.00043591]
		[batch 20/20] avg loss: 0.00025925671659371983		[learning rate: 0.00043524]
	Learning Rate: 0.000435244
	LOSS [training: -0.002887842569292464 | validation: 0.00012447299700319488]
	TIME [epoch: 8.83 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00329760022979627		[learning rate: 0.00043458]
		[batch 20/20] avg loss: -0.001217038628216227		[learning rate: 0.00043391]
	Learning Rate: 0.00043391
	LOSS [training: 0.0010402808007900217 | validation: -0.0026448831904633202]
	TIME [epoch: 8.82 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00036795291060390343		[learning rate: 0.00043324]
		[batch 20/20] avg loss: -0.006245586936084689		[learning rate: 0.00043258]
	Learning Rate: 0.00043258
	LOSS [training: -0.0033067699233442966 | validation: -0.004160813119583642]
	TIME [epoch: 8.82 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0041894341213470545		[learning rate: 0.00043192]
		[batch 20/20] avg loss: 0.0005264074948216292		[learning rate: 0.00043125]
	Learning Rate: 0.000431254
	LOSS [training: -0.0018315133132627119 | validation: 0.0030953518527875633]
	TIME [epoch: 8.84 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009239578909537185		[learning rate: 0.00043059]
		[batch 20/20] avg loss: 0.0008958924834324919		[learning rate: 0.00042993]
	Learning Rate: 0.000429932
	LOSS [training: -0.004171843213052348 | validation: 0.00016534861659326538]
	TIME [epoch: 8.83 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009473794309242303		[learning rate: 0.00042927]
		[batch 20/20] avg loss: 0.0016281820426891145		[learning rate: 0.00042861]
	Learning Rate: 0.000428614
	LOSS [training: -0.003922806133276594 | validation: -0.00187257971608205]
	TIME [epoch: 8.82 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017427461447595126		[learning rate: 0.00042796]
		[batch 20/20] avg loss: -0.00843075317241056		[learning rate: 0.0004273]
	Learning Rate: 0.0004273
	LOSS [training: -0.0033440035138255233 | validation: -0.0025992534682675976]
	TIME [epoch: 8.82 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005912369275849901		[learning rate: 0.00042664]
		[batch 20/20] avg loss: 0.002085796680365857		[learning rate: 0.00042599]
	Learning Rate: 0.000425991
	LOSS [training: -0.0019132862977420222 | validation: 0.003485335096728187]
	TIME [epoch: 8.84 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016476911361856836		[learning rate: 0.00042534]
		[batch 20/20] avg loss: -0.0017839048044838682		[learning rate: 0.00042468]
	Learning Rate: 0.000424685
	LOSS [training: -0.0017157979703347759 | validation: -0.0011993523751690973]
	TIME [epoch: 8.84 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006383832615502773		[learning rate: 0.00042403]
		[batch 20/20] avg loss: 0.0018743720710444381		[learning rate: 0.00042338]
	Learning Rate: 0.000423383
	LOSS [training: -0.002254730272229167 | validation: 0.0018184123374067494]
	TIME [epoch: 8.83 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006277611952340364		[learning rate: 0.00042273]
		[batch 20/20] avg loss: -0.001272992414102679		[learning rate: 0.00042208]
	Learning Rate: 0.000422085
	LOSS [training: -0.0037753021832215216 | validation: -0.0014653474241932258]
	TIME [epoch: 8.83 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0046440351545741325		[learning rate: 0.00042144]
		[batch 20/20] avg loss: -0.0031383455816729154		[learning rate: 0.00042079]
	Learning Rate: 0.000420791
	LOSS [training: -0.003891190368123524 | validation: 0.002252587473861029]
	TIME [epoch: 8.82 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0040583179632714015		[learning rate: 0.00042015]
		[batch 20/20] avg loss: -0.0068849161591601924		[learning rate: 0.0004195]
	Learning Rate: 0.000419501
	LOSS [training: -0.005471617061215797 | validation: 0.002106879989303875]
	TIME [epoch: 8.84 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006336967182280111		[learning rate: 0.00041886]
		[batch 20/20] avg loss: -0.006489747044996781		[learning rate: 0.00041822]
	Learning Rate: 0.000418215
	LOSS [training: -0.006413357113638447 | validation: -0.0020016303134469922]
	TIME [epoch: 8.83 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00421129990318398		[learning rate: 0.00041757]
		[batch 20/20] avg loss: -0.0035680915372430727		[learning rate: 0.00041693]
	Learning Rate: 0.000416933
	LOSS [training: -0.0038896957202135263 | validation: -0.0031447430126722857]
	TIME [epoch: 8.83 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006996446556608282		[learning rate: 0.00041629]
		[batch 20/20] avg loss: -0.00012307620905357077		[learning rate: 0.00041566]
	Learning Rate: 0.000415655
	LOSS [training: -0.003559761382830926 | validation: -0.001674051946959417]
	TIME [epoch: 8.82 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023564072045949838		[learning rate: 0.00041502]
		[batch 20/20] avg loss: 0.004705028181321046		[learning rate: 0.00041438]
	Learning Rate: 0.000414381
	LOSS [training: 0.0011743104883630308 | validation: 0.007135225821879868]
	TIME [epoch: 8.84 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019133549074299354		[learning rate: 0.00041375]
		[batch 20/20] avg loss: 0.005564267032821082		[learning rate: 0.00041311]
	Learning Rate: 0.000413111
	LOSS [training: 0.003738810970125509 | validation: 0.012356971805177266]
	TIME [epoch: 8.82 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022153539757698596		[learning rate: 0.00041248]
		[batch 20/20] avg loss: -0.0022371393425828627		[learning rate: 0.00041184]
	Learning Rate: 0.000411845
	LOSS [training: -1.089268340650146e-05 | validation: -0.001463748070568795]
	TIME [epoch: 8.82 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033687712871665774		[learning rate: 0.00041121]
		[batch 20/20] avg loss: -0.0041521393718946965		[learning rate: 0.00041058]
	Learning Rate: 0.000410582
	LOSS [training: -0.003760455329530637 | validation: -4.61504494929058e-05]
	TIME [epoch: 8.82 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004702114749701835		[learning rate: 0.00040995]
		[batch 20/20] avg loss: -0.0016600100470990625		[learning rate: 0.00040932]
	Learning Rate: 0.000409323
	LOSS [training: -0.0031810623984004477 | validation: 0.006462287300543382]
	TIME [epoch: 8.83 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003087596774768736		[learning rate: 0.0004087]
		[batch 20/20] avg loss: -0.0073905015059717305		[learning rate: 0.00040807]
	Learning Rate: 0.000408069
	LOSS [training: -0.0052390491403702345 | validation: -0.00025851462988746494]
	TIME [epoch: 8.83 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027426759791496663		[learning rate: 0.00040744]
		[batch 20/20] avg loss: -0.007056105914000516		[learning rate: 0.00040682]
	Learning Rate: 0.000406818
	LOSS [training: -0.004899390946575091 | validation: -0.001690147299491843]
	TIME [epoch: 8.82 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005392971472591374		[learning rate: 0.00040619]
		[batch 20/20] avg loss: 9.299709871703408e-05		[learning rate: 0.00040557]
	Learning Rate: 0.000405571
	LOSS [training: -0.0026499871869371697 | validation: -0.0016562372376228939]
	TIME [epoch: 8.82 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009367375087583052		[learning rate: 0.00040495]
		[batch 20/20] avg loss: -0.0025752673811074737		[learning rate: 0.00040433]
	Learning Rate: 0.000404328
	LOSS [training: -0.00175600244493289 | validation: -0.001737785121304838]
	TIME [epoch: 8.82 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018160048651837926		[learning rate: 0.00040371]
		[batch 20/20] avg loss: 0.004005198779737191		[learning rate: 0.00040309]
	Learning Rate: 0.000403088
	LOSS [training: 0.0010945969572766993 | validation: 0.006018973535576691]
	TIME [epoch: 8.9 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029967428794532664		[learning rate: 0.00040247]
		[batch 20/20] avg loss: 0.001010366181480231		[learning rate: 0.00040185]
	Learning Rate: 0.000401852
	LOSS [training: -0.000993188348986518 | validation: 0.004649798383474853]
	TIME [epoch: 8.82 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003300407355268832		[learning rate: 0.00040124]
		[batch 20/20] avg loss: -0.005446729961712698		[learning rate: 0.00040062]
	Learning Rate: 0.000400621
	LOSS [training: -0.004373568658490765 | validation: -0.00014878640423305885]
	TIME [epoch: 8.82 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001429465078441256		[learning rate: 0.00040001]
		[batch 20/20] avg loss: -0.008693066361275297		[learning rate: 0.00039939]
	Learning Rate: 0.000399393
	LOSS [training: -0.003631800641417021 | validation: -0.005080559453148914]
	TIME [epoch: 8.82 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00360066134480166		[learning rate: 0.00039878]
		[batch 20/20] avg loss: -0.004311996507750939		[learning rate: 0.00039817]
	Learning Rate: 0.000398168
	LOSS [training: -0.0039563289262763 | validation: -0.004474516255638818]
	TIME [epoch: 8.84 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032431384534296987		[learning rate: 0.00039756]
		[batch 20/20] avg loss: -0.004455178719102619		[learning rate: 0.00039695]
	Learning Rate: 0.000396948
	LOSS [training: -0.003849158586266159 | validation: 0.0022290231944075735]
	TIME [epoch: 8.83 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003464497304302161		[learning rate: 0.00039634]
		[batch 20/20] avg loss: -0.005029156737300186		[learning rate: 0.00039573]
	Learning Rate: 0.000395731
	LOSS [training: -0.004246827020801173 | validation: -0.001135504040095073]
	TIME [epoch: 8.82 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031175685647718034		[learning rate: 0.00039512]
		[batch 20/20] avg loss: -0.0035531823508554534		[learning rate: 0.00039452]
	Learning Rate: 0.000394518
	LOSS [training: -0.003335375457813629 | validation: -0.002224180134027803]
	TIME [epoch: 8.83 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023992995479394617		[learning rate: 0.00039391]
		[batch 20/20] avg loss: -0.00194821970264591		[learning rate: 0.00039331]
	Learning Rate: 0.000393308
	LOSS [training: -0.002173759625292686 | validation: 0.0027285917327515884]
	TIME [epoch: 8.84 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00510588697254301		[learning rate: 0.00039271]
		[batch 20/20] avg loss: -0.0034220900357494594		[learning rate: 0.0003921]
	Learning Rate: 0.000392103
	LOSS [training: -0.004263988504146236 | validation: -0.002154284139600322]
	TIME [epoch: 8.84 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006473090874157176		[learning rate: 0.0003915]
		[batch 20/20] avg loss: -0.003099098735253383		[learning rate: 0.0003909]
	Learning Rate: 0.000390901
	LOSS [training: -0.0018732039113345494 | validation: 0.0036922800560954547]
	TIME [epoch: 8.83 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006242261055334748		[learning rate: 0.0003903]
		[batch 20/20] avg loss: 0.0003496084156437398		[learning rate: 0.0003897]
	Learning Rate: 0.000389703
	LOSS [training: -0.002946326319845503 | validation: 0.002190618698268786]
	TIME [epoch: 8.82 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008641100453040996		[learning rate: 0.0003891]
		[batch 20/20] avg loss: 0.00013029532770293195		[learning rate: 0.00038851]
	Learning Rate: 0.000388508
	LOSS [training: -0.004255402562669032 | validation: 0.00444453422100102]
	TIME [epoch: 8.84 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0051932121582252255		[learning rate: 0.00038791]
		[batch 20/20] avg loss: 0.0018723622219160203		[learning rate: 0.00038732]
	Learning Rate: 0.000387317
	LOSS [training: -0.0016604249681546022 | validation: 0.0017759816305165962]
	TIME [epoch: 8.84 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012698357004228646		[learning rate: 0.00038672]
		[batch 20/20] avg loss: -0.001299853964305942		[learning rate: 0.00038613]
	Learning Rate: 0.00038613
	LOSS [training: -0.0012848448323644036 | validation: -0.002926016426215254]
	TIME [epoch: 8.82 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007226671386152303		[learning rate: 0.00038554]
		[batch 20/20] avg loss: 0.0010951342449320547		[learning rate: 0.00038495]
	Learning Rate: 0.000384946
	LOSS [training: -0.0030657685706101242 | validation: -0.0008932283932346518]
	TIME [epoch: 8.82 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004860074750074306		[learning rate: 0.00038436]
		[batch 20/20] avg loss: -0.002405595790496954		[learning rate: 0.00038377]
	Learning Rate: 0.000383766
	LOSS [training: -0.0036328352702856294 | validation: 0.0008782300296799107]
	TIME [epoch: 8.82 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007021214152835573		[learning rate: 0.00038318]
		[batch 20/20] avg loss: 0.003780324357228155		[learning rate: 0.00038259]
	Learning Rate: 0.00038259
	LOSS [training: -0.0016204448978037085 | validation: -0.004799682510466103]
	TIME [epoch: 8.84 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023554851200196635		[learning rate: 0.000382]
		[batch 20/20] avg loss: -0.001206548892229291		[learning rate: 0.00038142]
	Learning Rate: 0.000381417
	LOSS [training: -0.0017810170061244766 | validation: -0.003191334224609537]
	TIME [epoch: 8.82 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028865803735985355		[learning rate: 0.00038083]
		[batch 20/20] avg loss: -0.0058590131041178495		[learning rate: 0.00038025]
	Learning Rate: 0.000380248
	LOSS [training: -0.004372796738858192 | validation: -0.007107940953027057]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1565.pth
	Model improved!!!
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006310766821218811		[learning rate: 0.00037966]
		[batch 20/20] avg loss: -0.002353583879537392		[learning rate: 0.00037908]
	Learning Rate: 0.000379082
	LOSS [training: -0.0014923302808296365 | validation: 0.0088625012967909]
	TIME [epoch: 8.83 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: -8.265713646375007e-05		[learning rate: 0.0003785]
		[batch 20/20] avg loss: -0.0032212802301474427		[learning rate: 0.00037792]
	Learning Rate: 0.00037792
	LOSS [training: -0.0016519686833055963 | validation: 0.0020046076753235087]
	TIME [epoch: 8.85 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004239382262724065		[learning rate: 0.00037734]
		[batch 20/20] avg loss: -0.0013658579335332779		[learning rate: 0.00037676]
	Learning Rate: 0.000376762
	LOSS [training: -0.0028026200981286706 | validation: -0.004420863058835831]
	TIME [epoch: 8.83 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008699971319171156		[learning rate: 0.00037618]
		[batch 20/20] avg loss: -0.0032471493839436967		[learning rate: 0.00037561]
	Learning Rate: 0.000375607
	LOSS [training: -0.0059735603515574266 | validation: 0.004511832633087076]
	TIME [epoch: 8.82 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005533796051333638		[learning rate: 0.00037503]
		[batch 20/20] avg loss: -0.0020482709115854643		[learning rate: 0.00037446]
	Learning Rate: 0.000374455
	LOSS [training: -0.003791033481459551 | validation: -0.0029837171338082213]
	TIME [epoch: 8.83 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001276947076317598		[learning rate: 0.00037388]
		[batch 20/20] avg loss: -0.002308903620546061		[learning rate: 0.00037331]
	Learning Rate: 0.000373307
	LOSS [training: -0.0017929253484318293 | validation: 0.002274107957027824]
	TIME [epoch: 8.84 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002918676544213554		[learning rate: 0.00037273]
		[batch 20/20] avg loss: -0.0005421067408483809		[learning rate: 0.00037216]
	Learning Rate: 0.000372163
	LOSS [training: -0.0017303916425309678 | validation: -0.007089174297538255]
	TIME [epoch: 8.82 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001649504425835907		[learning rate: 0.00037159]
		[batch 20/20] avg loss: -0.00684798395211348		[learning rate: 0.00037102]
	Learning Rate: 0.000371022
	LOSS [training: -0.0025992397631387867 | validation: 0.0034616677340857556]
	TIME [epoch: 8.81 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011132661056163422		[learning rate: 0.00037045]
		[batch 20/20] avg loss: -0.0016668634775845689		[learning rate: 0.00036988]
	Learning Rate: 0.000369885
	LOSS [training: -0.0013900647916004552 | validation: 0.0007249986098705491]
	TIME [epoch: 8.83 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002560570190804947		[learning rate: 0.00036932]
		[batch 20/20] avg loss: -0.00045889242058499806		[learning rate: 0.00036875]
	Learning Rate: 0.000368751
	LOSS [training: 0.0010508388851099741 | validation: -0.0007399010608429876]
	TIME [epoch: 8.82 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025669855130371006		[learning rate: 0.00036819]
		[batch 20/20] avg loss: -0.000610098425757507		[learning rate: 0.00036762]
	Learning Rate: 0.000367621
	LOSS [training: -0.0015885419693973037 | validation: 0.0015872233568344908]
	TIME [epoch: 8.86 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007368287843916912		[learning rate: 0.00036706]
		[batch 20/20] avg loss: 0.0005490483865429195		[learning rate: 0.00036649]
	Learning Rate: 0.000366494
	LOSS [training: -0.003409619728686996 | validation: 0.0007457080688999776]
	TIME [epoch: 8.81 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004188791575948989		[learning rate: 0.00036593]
		[batch 20/20] avg loss: -0.0015754285453695206		[learning rate: 0.00036537]
	Learning Rate: 0.00036537
	LOSS [training: -0.0028821100606592548 | validation: -0.00023947329017047004]
	TIME [epoch: 8.83 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005898984766192531		[learning rate: 0.00036481]
		[batch 20/20] avg loss: -0.0013590221849291086		[learning rate: 0.00036425]
	Learning Rate: 0.00036425
	LOSS [training: -0.0036290034755608195 | validation: -5.179783363584538e-05]
	TIME [epoch: 8.81 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009183827927860338		[learning rate: 0.00036369]
		[batch 20/20] avg loss: -0.0046790718031322715		[learning rate: 0.00036313]
	Learning Rate: 0.000363134
	LOSS [training: -0.0027987272979591526 | validation: 0.0030450984356835543]
	TIME [epoch: 8.84 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033832599890225835		[learning rate: 0.00036258]
		[batch 20/20] avg loss: -0.001961622391941458		[learning rate: 0.00036202]
	Learning Rate: 0.000362021
	LOSS [training: -0.002672441190482021 | validation: 0.0009309451510664678]
	TIME [epoch: 8.81 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001006386320578275		[learning rate: 0.00036147]
		[batch 20/20] avg loss: -0.0017075905720487956		[learning rate: 0.00036091]
	Learning Rate: 0.000360911
	LOSS [training: -0.0013569884463135353 | validation: -0.003984195613030613]
	TIME [epoch: 8.82 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003416030623143172		[learning rate: 0.00036036]
		[batch 20/20] avg loss: -0.005107169115036917		[learning rate: 0.0003598]
	Learning Rate: 0.000359805
	LOSS [training: -0.004261599869090045 | validation: -0.004416635139257602]
	TIME [epoch: 8.81 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005703140927308375		[learning rate: 0.00035925]
		[batch 20/20] avg loss: -0.001550387606156851		[learning rate: 0.0003587]
	Learning Rate: 0.000358702
	LOSS [training: -0.0036267642667326128 | validation: -0.0023998880724594605]
	TIME [epoch: 8.83 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006733601486099741		[learning rate: 0.00035815]
		[batch 20/20] avg loss: 0.0008375642071387709		[learning rate: 0.0003576]
	Learning Rate: 0.000357602
	LOSS [training: -0.0029480186394804852 | validation: 0.0027880670368709195]
	TIME [epoch: 8.82 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003098914889132139		[learning rate: 0.00035705]
		[batch 20/20] avg loss: -0.0040026288744895045		[learning rate: 0.00035651]
	Learning Rate: 0.000356506
	LOSS [training: -0.0021562601817013594 | validation: 0.001145744763626944]
	TIME [epoch: 8.82 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005721555796962292		[learning rate: 0.00035596]
		[batch 20/20] avg loss: -0.00011980241173862082		[learning rate: 0.00035541]
	Learning Rate: 0.000355413
	LOSS [training: -0.002920679104350456 | validation: -0.0034079964673711305]
	TIME [epoch: 8.82 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: -7.231521535708911e-05		[learning rate: 0.00035487]
		[batch 20/20] avg loss: -0.005129654973390502		[learning rate: 0.00035432]
	Learning Rate: 0.000354323
	LOSS [training: -0.002600985094373795 | validation: 0.0006876148756614847]
	TIME [epoch: 8.83 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00477791964946727		[learning rate: 0.00035378]
		[batch 20/20] avg loss: 0.0005350881953444899		[learning rate: 0.00035324]
	Learning Rate: 0.000353237
	LOSS [training: -0.00212141572706139 | validation: -0.00044285985583765496]
	TIME [epoch: 8.84 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005945807145438015		[learning rate: 0.0003527]
		[batch 20/20] avg loss: -0.0017078578002277674		[learning rate: 0.00035215]
	Learning Rate: 0.000352155
	LOSS [training: -0.003826832472832891 | validation: -0.0008446072437739757]
	TIME [epoch: 8.82 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004747508489195139		[learning rate: 0.00035161]
		[batch 20/20] avg loss: -0.0023078005503026954		[learning rate: 0.00035108]
	Learning Rate: 0.000351075
	LOSS [training: -0.0035276545197489182 | validation: -0.001864067322602973]
	TIME [epoch: 8.83 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005193438291079287		[learning rate: 0.00035054]
		[batch 20/20] avg loss: -0.002406681432840009		[learning rate: 0.00035]
	Learning Rate: 0.000349999
	LOSS [training: -0.001463012630973969 | validation: -0.009382431737890051]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1592.pth
	Model improved!!!
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002158839976395763		[learning rate: 0.00034946]
		[batch 20/20] avg loss: -0.00962603936417102		[learning rate: 0.00034893]
	Learning Rate: 0.000348926
	LOSS [training: -0.00589243967028339 | validation: 0.001850704181085518]
	TIME [epoch: 8.84 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005820355590039084		[learning rate: 0.00034839]
		[batch 20/20] avg loss: -0.0003670144318970699		[learning rate: 0.00034786]
	Learning Rate: 0.000347856
	LOSS [training: -0.003093685010968077 | validation: -0.0039724668309548]
	TIME [epoch: 8.82 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006968866110688544		[learning rate: 0.00034732]
		[batch 20/20] avg loss: -0.006992683461417158		[learning rate: 0.00034679]
	Learning Rate: 0.00034679
	LOSS [training: -0.0038447850362430063 | validation: 0.005355549455709735]
	TIME [epoch: 8.82 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00023476385760190526		[learning rate: 0.00034626]
		[batch 20/20] avg loss: -0.010299437704897042		[learning rate: 0.00034573]
	Learning Rate: 0.000345727
	LOSS [training: -0.005032336923647568 | validation: 0.0031628109542296235]
	TIME [epoch: 8.82 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004293154365657201		[learning rate: 0.0003452]
		[batch 20/20] avg loss: 0.0001239306172114071		[learning rate: 0.00034467]
	Learning Rate: 0.000344667
	LOSS [training: -0.002084611874222897 | validation: 0.004791880485736096]
	TIME [epoch: 8.84 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001021689946152626		[learning rate: 0.00034414]
		[batch 20/20] avg loss: -0.004990149624388212		[learning rate: 0.00034361]
	Learning Rate: 0.000343611
	LOSS [training: -0.0030059197852704196 | validation: -0.0027070247482883295]
	TIME [epoch: 8.82 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033399499794457507		[learning rate: 0.00034308]
		[batch 20/20] avg loss: -0.001355784486264132		[learning rate: 0.00034256]
	Learning Rate: 0.000342557
	LOSS [training: -0.002347867232854941 | validation: -0.00435581052535474]
	TIME [epoch: 8.81 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006041931016763004		[learning rate: 0.00034203]
		[batch 20/20] avg loss: 0.0012036056550441578		[learning rate: 0.00034151]
	Learning Rate: 0.000341507
	LOSS [training: -0.0024191626808594225 | validation: 0.005991817949524705]
	TIME [epoch: 8.82 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011892715815264859		[learning rate: 0.00034098]
		[batch 20/20] avg loss: -0.0030460790452063917		[learning rate: 0.00034046]
	Learning Rate: 0.00034046
	LOSS [training: -0.007469397430235626 | validation: -0.002555229682666622]
	TIME [epoch: 8.84 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0048651294980586865		[learning rate: 0.00033994]
		[batch 20/20] avg loss: -0.004000496655263154		[learning rate: 0.00033942]
	Learning Rate: 0.000339417
	LOSS [training: -0.0044328130766609205 | validation: 0.003167761432012764]
	TIME [epoch: 8.83 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008876132225697971		[learning rate: 0.0003389]
		[batch 20/20] avg loss: -0.0025729321616776565		[learning rate: 0.00033838]
	Learning Rate: 0.000338376
	LOSS [training: -0.0008426594695539299 | validation: -0.002200498189267703]
	TIME [epoch: 8.81 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001317747296062493		[learning rate: 0.00033786]
		[batch 20/20] avg loss: -0.0011986376079487072		[learning rate: 0.00033734]
	Learning Rate: 0.000337339
	LOSS [training: -0.0012581924520056003 | validation: 3.843653980397537e-05]
	TIME [epoch: 8.82 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014920064334969399		[learning rate: 0.00033682]
		[batch 20/20] avg loss: -0.0022876873641853534		[learning rate: 0.0003363]
	Learning Rate: 0.000336305
	LOSS [training: -0.0018898468988411466 | validation: -0.0012009590722475202]
	TIME [epoch: 8.82 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00016195929558075734		[learning rate: 0.00033579]
		[batch 20/20] avg loss: -0.0010285947882173092		[learning rate: 0.00033527]
	Learning Rate: 0.000335274
	LOSS [training: -0.0005952770418990334 | validation: 0.006133792429159716]
	TIME [epoch: 8.84 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037482273422920847		[learning rate: 0.00033476]
		[batch 20/20] avg loss: 0.001155820109565952		[learning rate: 0.00033425]
	Learning Rate: 0.000334246
	LOSS [training: -0.0012962036163630662 | validation: 0.0012829540360289788]
	TIME [epoch: 8.81 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0057965171733441135		[learning rate: 0.00033373]
		[batch 20/20] avg loss: -0.0016667286277691762		[learning rate: 0.00033322]
	Learning Rate: 0.000333222
	LOSS [training: -0.003731622900556645 | validation: 0.004044078665591656]
	TIME [epoch: 8.81 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024095799082987086		[learning rate: 0.00033271]
		[batch 20/20] avg loss: -0.005153542928403831		[learning rate: 0.0003322]
	Learning Rate: 0.0003322
	LOSS [training: -0.001371981510052561 | validation: -0.004376656225017572]
	TIME [epoch: 8.81 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031490876224772495		[learning rate: 0.00033169]
		[batch 20/20] avg loss: -0.003150984703010623		[learning rate: 0.00033118]
	Learning Rate: 0.000331182
	LOSS [training: -0.0031500361627439357 | validation: -0.004934160808110185]
	TIME [epoch: 8.84 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002751987424029943		[learning rate: 0.00033067]
		[batch 20/20] avg loss: -0.0053396777138339695		[learning rate: 0.00033017]
	Learning Rate: 0.000330167
	LOSS [training: -0.004045832568931956 | validation: -0.004601092546307023]
	TIME [epoch: 8.82 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006104121620229962		[learning rate: 0.00032966]
		[batch 20/20] avg loss: -0.006250098770390872		[learning rate: 0.00032915]
	Learning Rate: 0.000329155
	LOSS [training: -0.0061771101953104176 | validation: 0.0002531846251240411]
	TIME [epoch: 8.82 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005865993890076286		[learning rate: 0.00032865]
		[batch 20/20] avg loss: -0.0017545838663703902		[learning rate: 0.00032815]
	Learning Rate: 0.000328146
	LOSS [training: -0.0038102888782233385 | validation: -0.0021936785604391544]
	TIME [epoch: 8.81 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006205043525898218		[learning rate: 0.00032764]
		[batch 20/20] avg loss: -0.0006117675237673639		[learning rate: 0.00032714]
	Learning Rate: 0.00032714
	LOSS [training: -0.0034084055248327903 | validation: 0.0024038362836882846]
	TIME [epoch: 8.83 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005036081913704421		[learning rate: 0.00032664]
		[batch 20/20] avg loss: -0.005089465378298394		[learning rate: 0.00032614]
	Learning Rate: 0.000326137
	LOSS [training: -0.005062773646001407 | validation: 0.00035240110655929943]
	TIME [epoch: 8.83 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005716702304171347		[learning rate: 0.00032564]
		[batch 20/20] avg loss: -0.000609808031385188		[learning rate: 0.00032514]
	Learning Rate: 0.000325137
	LOSS [training: -0.003163255167778267 | validation: -0.004130192419249022]
	TIME [epoch: 8.82 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002069282873033791		[learning rate: 0.00032464]
		[batch 20/20] avg loss: 0.0005602377916629632		[learning rate: 0.00032414]
	Learning Rate: 0.000324141
	LOSS [training: -0.0007545225406854143 | validation: -0.002861380274222428]
	TIME [epoch: 8.81 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00029621702067096536		[learning rate: 0.00032364]
		[batch 20/20] avg loss: -0.0030455510137874973		[learning rate: 0.00032315]
	Learning Rate: 0.000323147
	LOSS [training: -0.0013746669965582661 | validation: 0.0033848634577022195]
	TIME [epoch: 8.83 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007327062868839529		[learning rate: 0.00032265]
		[batch 20/20] avg loss: -0.0035149127641535974		[learning rate: 0.00032216]
	Learning Rate: 0.000322156
	LOSS [training: -0.005420987816496564 | validation: 0.007183372291972276]
	TIME [epoch: 8.83 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015632858666687131		[learning rate: 0.00032166]
		[batch 20/20] avg loss: -0.006331199977529879		[learning rate: 0.00032117]
	Learning Rate: 0.000321169
	LOSS [training: -0.0039472429220992955 | validation: 0.0003424265005274955]
	TIME [epoch: 8.81 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023834386168044295		[learning rate: 0.00032068]
		[batch 20/20] avg loss: 0.0002351996056103709		[learning rate: 0.00032018]
	Learning Rate: 0.000320184
	LOSS [training: -0.0010741195055970294 | validation: -0.0003436904498176356]
	TIME [epoch: 8.81 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002083781979238056		[learning rate: 0.00031969]
		[batch 20/20] avg loss: -0.007570783925096823		[learning rate: 0.0003192]
	Learning Rate: 0.000319203
	LOSS [training: -0.00482728295216744 | validation: -0.0008826493882942129]
	TIME [epoch: 8.82 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003206895430783731		[learning rate: 0.00031871]
		[batch 20/20] avg loss: -0.002983133974210768		[learning rate: 0.00031822]
	Learning Rate: 0.000318224
	LOSS [training: -0.0016519117586445704 | validation: 0.0008789669147104491]
	TIME [epoch: 8.85 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00036756200461048375		[learning rate: 0.00031774]
		[batch 20/20] avg loss: -0.010533618453227808		[learning rate: 0.00031725]
	Learning Rate: 0.000317249
	LOSS [training: -0.005450590228919145 | validation: 0.002276415273908423]
	TIME [epoch: 8.82 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022561064488486804		[learning rate: 0.00031676]
		[batch 20/20] avg loss: -0.006607561641360176		[learning rate: 0.00031628]
	Learning Rate: 0.000316276
	LOSS [training: -0.004431834045104429 | validation: -0.0007028121271633335]
	TIME [epoch: 8.82 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000618177077271392		[learning rate: 0.00031579]
		[batch 20/20] avg loss: -0.008844582271746771		[learning rate: 0.00031531]
	Learning Rate: 0.000315307
	LOSS [training: -0.004731379674509081 | validation: 0.0025989925789919105]
	TIME [epoch: 8.81 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004283630839297092		[learning rate: 0.00031482]
		[batch 20/20] avg loss: -0.0063617153684253445		[learning rate: 0.00031434]
	Learning Rate: 0.00031434
	LOSS [training: -0.005322673103861218 | validation: 0.0007330058604849668]
	TIME [epoch: 8.83 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035814915743003937		[learning rate: 0.00031386]
		[batch 20/20] avg loss: -0.0027263756055635946		[learning rate: 0.00031338]
	Learning Rate: 0.000313377
	LOSS [training: -0.0031539335899319946 | validation: -0.003749140284810058]
	TIME [epoch: 8.82 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00796461127152576		[learning rate: 0.0003129]
		[batch 20/20] avg loss: -0.0018471391972864951		[learning rate: 0.00031242]
	Learning Rate: 0.000312416
	LOSS [training: -0.004905875234406128 | validation: 0.004115820313133137]
	TIME [epoch: 8.82 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026027879998230786		[learning rate: 0.00031194]
		[batch 20/20] avg loss: -0.005062357866451389		[learning rate: 0.00031146]
	Learning Rate: 0.000311458
	LOSS [training: -0.0038325729331372335 | validation: -0.0007668820344869183]
	TIME [epoch: 8.81 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00347925099074537		[learning rate: 0.00031098]
		[batch 20/20] avg loss: -0.002793785589099999		[learning rate: 0.0003105]
	Learning Rate: 0.000310504
	LOSS [training: -0.0031365182899226844 | validation: -0.002434814867150257]
	TIME [epoch: 8.82 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003006833147564958		[learning rate: 0.00031003]
		[batch 20/20] avg loss: -0.007965762467402087		[learning rate: 0.00030955]
	Learning Rate: 0.000309552
	LOSS [training: -0.005486297807483523 | validation: 0.004655417469350996]
	TIME [epoch: 8.82 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006691047527352212		[learning rate: 0.00030908]
		[batch 20/20] avg loss: -0.0023133628485679566		[learning rate: 0.0003086]
	Learning Rate: 0.000308603
	LOSS [training: -0.004502205187960085 | validation: -0.0034346101870597243]
	TIME [epoch: 8.81 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006885376229338158		[learning rate: 0.00030813]
		[batch 20/20] avg loss: -0.007226252712891612		[learning rate: 0.00030766]
	Learning Rate: 0.000307657
	LOSS [training: -0.003957395167912714 | validation: -0.0021627437060478282]
	TIME [epoch: 8.81 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007840786575120664		[learning rate: 0.00030718]
		[batch 20/20] avg loss: -0.003392146281041713		[learning rate: 0.00030671]
	Learning Rate: 0.000306714
	LOSS [training: -0.005616466428081187 | validation: 0.0011194298557145046]
	TIME [epoch: 8.81 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008648211214283503		[learning rate: 0.00030624]
		[batch 20/20] avg loss: -0.006520316799792321		[learning rate: 0.00030577]
	Learning Rate: 0.000305774
	LOSS [training: -0.0036925689606103354 | validation: 0.0002630586294778422]
	TIME [epoch: 8.84 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034694670746080966		[learning rate: 0.0003053]
		[batch 20/20] avg loss: -0.0032467518924533837		[learning rate: 0.00030484]
	Learning Rate: 0.000304836
	LOSS [training: 0.00011135759107735594 | validation: 0.0005353580529204718]
	TIME [epoch: 8.81 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006382348554572599		[learning rate: 0.00030437]
		[batch 20/20] avg loss: 0.00040088624872160227		[learning rate: 0.0003039]
	Learning Rate: 0.000303902
	LOSS [training: -0.0029907311529254986 | validation: -0.006240661679288752]
	TIME [epoch: 8.81 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004834651947154168		[learning rate: 0.00030344]
		[batch 20/20] avg loss: -0.00522608480431719		[learning rate: 0.00030297]
	Learning Rate: 0.00030297
	LOSS [training: -0.005030368375735679 | validation: -0.00044232080164737647]
	TIME [epoch: 8.82 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005294944342746596		[learning rate: 0.00030251]
		[batch 20/20] avg loss: -0.008119638586674296		[learning rate: 0.00030204]
	Learning Rate: 0.000302042
	LOSS [training: -0.006707291464710446 | validation: -0.0007633300183856065]
	TIME [epoch: 8.83 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00341965848127173		[learning rate: 0.00030158]
		[batch 20/20] avg loss: 3.9128754248449717e-05		[learning rate: 0.00030112]
	Learning Rate: 0.000301116
	LOSS [training: -0.00169026486351164 | validation: 0.002739142216216749]
	TIME [epoch: 8.81 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011114140705954158		[learning rate: 0.00030065]
		[batch 20/20] avg loss: -0.0033326620403003964		[learning rate: 0.00030019]
	Learning Rate: 0.000300193
	LOSS [training: -0.002222038055447906 | validation: 0.0065305605554263815]
	TIME [epoch: 8.82 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015049966046106651		[learning rate: 0.00029973]
		[batch 20/20] avg loss: -0.006417768996282781		[learning rate: 0.00029927]
	Learning Rate: 0.000299272
	LOSS [training: -0.003961382800446723 | validation: 0.0026874583608995643]
	TIME [epoch: 8.81 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003399871066634322		[learning rate: 0.00029881]
		[batch 20/20] avg loss: -0.005398007046744026		[learning rate: 0.00029835]
	Learning Rate: 0.000298355
	LOSS [training: -0.0043989390566891735 | validation: 0.0028110008186870102]
	TIME [epoch: 8.84 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006696882309359357		[learning rate: 0.0002979]
		[batch 20/20] avg loss: -0.0077531038419293204		[learning rate: 0.00029744]
	Learning Rate: 0.00029744
	LOSS [training: -0.00722499307564434 | validation: -0.001317324770714239]
	TIME [epoch: 8.82 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008422538765901958		[learning rate: 0.00029698]
		[batch 20/20] avg loss: -0.00650611582977669		[learning rate: 0.00029653]
	Learning Rate: 0.000296529
	LOSS [training: -0.007464327297839324 | validation: -0.0016991449482210382]
	TIME [epoch: 8.82 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023531726259222764		[learning rate: 0.00029607]
		[batch 20/20] avg loss: -0.00262859430925607		[learning rate: 0.00029562]
	Learning Rate: 0.00029562
	LOSS [training: -0.0024908834675891733 | validation: -0.0012268713134098359]
	TIME [epoch: 8.82 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037018780060932175		[learning rate: 0.00029517]
		[batch 20/20] avg loss: -0.006595860340883707		[learning rate: 0.00029471]
	Learning Rate: 0.000294713
	LOSS [training: -0.005148869173488462 | validation: -0.005181481140673563]
	TIME [epoch: 8.82 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007561426960768848		[learning rate: 0.00029426]
		[batch 20/20] avg loss: 0.0004397512992154112		[learning rate: 0.00029381]
	Learning Rate: 0.00029381
	LOSS [training: -0.00356083783077672 | validation: 0.004350659831281906]
	TIME [epoch: 8.84 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004502108513514058		[learning rate: 0.00029336]
		[batch 20/20] avg loss: -0.0008623057930511934		[learning rate: 0.00029291]
	Learning Rate: 0.000292909
	LOSS [training: -0.0026822071532826256 | validation: -0.00374830267959711]
	TIME [epoch: 8.82 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005672269940062242		[learning rate: 0.00029246]
		[batch 20/20] avg loss: -0.007935442311932207		[learning rate: 0.00029201]
	Learning Rate: 0.000292011
	LOSS [training: -0.0068038561259972236 | validation: -0.0032760441355327995]
	TIME [epoch: 8.81 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004628531500838312		[learning rate: 0.00029156]
		[batch 20/20] avg loss: -0.001206877080133311		[learning rate: 0.00029112]
	Learning Rate: 0.000291116
	LOSS [training: -0.0029177042904858118 | validation: 0.005875341999116325]
	TIME [epoch: 8.81 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006142526843158773		[learning rate: 0.00029067]
		[batch 20/20] avg loss: -0.006446880211280428		[learning rate: 0.00029022]
	Learning Rate: 0.000290224
	LOSS [training: -0.006294703527219602 | validation: 0.004861414199894258]
	TIME [epoch: 8.84 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033610105019107024		[learning rate: 0.00028978]
		[batch 20/20] avg loss: -0.004323087582345397		[learning rate: 0.00028933]
	Learning Rate: 0.000289334
	LOSS [training: -0.003842049042128049 | validation: 0.0007236246951096246]
	TIME [epoch: 8.82 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002385343519218418		[learning rate: 0.00028889]
		[batch 20/20] avg loss: -0.002461163395524342		[learning rate: 0.00028845]
	Learning Rate: 0.000288447
	LOSS [training: -3.790993815296169e-05 | validation: -0.0023049697302891556]
	TIME [epoch: 8.82 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009321725235762227		[learning rate: 0.000288]
		[batch 20/20] avg loss: 0.00019309576463304694		[learning rate: 0.00028756]
	Learning Rate: 0.000287563
	LOSS [training: -0.004564314735564589 | validation: 0.006887268633572052]
	TIME [epoch: 8.83 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005586844724174282		[learning rate: 0.00028712]
		[batch 20/20] avg loss: -0.005039879543555033		[learning rate: 0.00028668]
	Learning Rate: 0.000286682
	LOSS [training: -0.005313362133864658 | validation: -0.004909607702781724]
	TIME [epoch: 8.83 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010351079740204305		[learning rate: 0.00028624]
		[batch 20/20] avg loss: -0.001177175209145924		[learning rate: 0.0002858]
	Learning Rate: 0.000285803
	LOSS [training: -0.005764127474675115 | validation: -0.0008858837567106924]
	TIME [epoch: 8.82 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016828657785057477		[learning rate: 0.00028536]
		[batch 20/20] avg loss: -0.01011110962764908		[learning rate: 0.00028493]
	Learning Rate: 0.000284927
	LOSS [training: -0.005896987703077413 | validation: 0.0023330380625486416]
	TIME [epoch: 8.82 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005775295053548733		[learning rate: 0.00028449]
		[batch 20/20] avg loss: -0.0011946145381444382		[learning rate: 0.00028405]
	Learning Rate: 0.000284053
	LOSS [training: -0.003484954795846586 | validation: 0.003272362119293034]
	TIME [epoch: 8.81 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002279337286110741		[learning rate: 0.00028362]
		[batch 20/20] avg loss: -0.0022546045445188756		[learning rate: 0.00028318]
	Learning Rate: 0.000283183
	LOSS [training: -0.0022669709153148084 | validation: -0.0037507265900033164]
	TIME [epoch: 8.82 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005526626730219975		[learning rate: 0.00028275]
		[batch 20/20] avg loss: -0.0026035853848541068		[learning rate: 0.00028231]
	Learning Rate: 0.000282315
	LOSS [training: -0.004065106057537042 | validation: -0.00208572351136785]
	TIME [epoch: 8.83 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019017626469566423		[learning rate: 0.00028188]
		[batch 20/20] avg loss: -0.003399660831591408		[learning rate: 0.00028145]
	Learning Rate: 0.000281449
	LOSS [training: -0.002650711739274025 | validation: -0.0035044712026473233]
	TIME [epoch: 8.82 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005298146821580526		[learning rate: 0.00028102]
		[batch 20/20] avg loss: -0.0018596096888089503		[learning rate: 0.00028059]
	Learning Rate: 0.000280586
	LOSS [training: -0.0035788782551947384 | validation: -0.0019228663879502221]
	TIME [epoch: 8.82 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005461465067860927		[learning rate: 0.00028016]
		[batch 20/20] avg loss: -0.0033409351029943047		[learning rate: 0.00027973]
	Learning Rate: 0.000279726
	LOSS [training: -0.004401200085427616 | validation: -0.00336059570881105]
	TIME [epoch: 8.83 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007105404872279659		[learning rate: 0.0002793]
		[batch 20/20] avg loss: -0.007593096849238784		[learning rate: 0.00027887]
	Learning Rate: 0.000278869
	LOSS [training: -0.007349250860759221 | validation: 0.0003514600102116974]
	TIME [epoch: 8.84 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006953579506212067		[learning rate: 0.00027844]
		[batch 20/20] avg loss: -0.0004373209491987347		[learning rate: 0.00027801]
	Learning Rate: 0.000278014
	LOSS [training: -0.0036954502277054004 | validation: 0.002328467266620958]
	TIME [epoch: 8.83 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0063515829502745385		[learning rate: 0.00027759]
		[batch 20/20] avg loss: -0.00033059287055847415		[learning rate: 0.00027716]
	Learning Rate: 0.000277162
	LOSS [training: -0.0033410879104165065 | validation: -0.004076414576143056]
	TIME [epoch: 8.81 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009954469406366607		[learning rate: 0.00027674]
		[batch 20/20] avg loss: -0.008546332661018147		[learning rate: 0.00027631]
	Learning Rate: 0.000276312
	LOSS [training: -0.004770889800827403 | validation: -0.0026800375148567116]
	TIME [epoch: 8.83 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007288906684051145		[learning rate: 0.00027589]
		[batch 20/20] avg loss: -0.007421003449628545		[learning rate: 0.00027547]
	Learning Rate: 0.000275465
	LOSS [training: -0.007354955066839845 | validation: -0.006774907358024013]
	TIME [epoch: 8.85 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007061060259488644		[learning rate: 0.00027504]
		[batch 20/20] avg loss: -0.0020379854458150138		[learning rate: 0.00027462]
	Learning Rate: 0.000274621
	LOSS [training: -0.00454952285265183 | validation: -0.0008514523661898833]
	TIME [epoch: 8.83 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007069781486409054		[learning rate: 0.0002742]
		[batch 20/20] avg loss: -0.00392140927313058		[learning rate: 0.00027378]
	Learning Rate: 0.000273779
	LOSS [training: -0.005495595379769817 | validation: 0.001336715764256766]
	TIME [epoch: 8.82 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006941599629537179		[learning rate: 0.00027336]
		[batch 20/20] avg loss: -0.00046502314714818893		[learning rate: 0.00027294]
	Learning Rate: 0.00027294
	LOSS [training: -0.003703311388342684 | validation: -0.001857026057878973]
	TIME [epoch: 8.83 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027733942194741864		[learning rate: 0.00027252]
		[batch 20/20] avg loss: -0.008977687333087344		[learning rate: 0.0002721]
	Learning Rate: 0.000272103
	LOSS [training: -0.005875540776280766 | validation: -0.003446742101551668]
	TIME [epoch: 8.84 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00677696174572232		[learning rate: 0.00027169]
		[batch 20/20] avg loss: -0.005654806822018689		[learning rate: 0.00027127]
	Learning Rate: 0.000271269
	LOSS [training: -0.006215884283870505 | validation: -0.0025060980099747506]
	TIME [epoch: 8.83 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024651371727321125		[learning rate: 0.00027085]
		[batch 20/20] avg loss: -0.0023025596710645555		[learning rate: 0.00027044]
	Learning Rate: 0.000270437
	LOSS [training: -0.0023838484218983345 | validation: 0.0025277745860987034]
	TIME [epoch: 8.81 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034078099631288904		[learning rate: 0.00027002]
		[batch 20/20] avg loss: 0.003764490710965998		[learning rate: 0.00026961]
	Learning Rate: 0.000269608
	LOSS [training: 0.00017834037391855357 | validation: 0.005589507373212639]
	TIME [epoch: 8.82 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032939003638512558		[learning rate: 0.00026919]
		[batch 20/20] avg loss: -0.0026836916753714213		[learning rate: 0.00026878]
	Learning Rate: 0.000268782
	LOSS [training: -0.0029887960196113388 | validation: -0.0012307224703440819]
	TIME [epoch: 8.82 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001299185915000018		[learning rate: 0.00026837]
		[batch 20/20] avg loss: -0.0008020600455901749		[learning rate: 0.00026796]
	Learning Rate: 0.000267958
	LOSS [training: -0.0010506229802950966 | validation: 0.0008605012895012455]
	TIME [epoch: 8.85 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007578776502316492		[learning rate: 0.00026755]
		[batch 20/20] avg loss: -0.0014203560857924825		[learning rate: 0.00026714]
	Learning Rate: 0.000267137
	LOSS [training: -0.004499566294054487 | validation: 0.005542615106527989]
	TIME [epoch: 8.82 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024579578295603533		[learning rate: 0.00026673]
		[batch 20/20] avg loss: -0.001992513733443898		[learning rate: 0.00026632]
	Learning Rate: 0.000266318
	LOSS [training: 0.00023272204805822785 | validation: -0.002578666461186134]
	TIME [epoch: 8.82 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006137163130230011		[learning rate: 0.00026591]
		[batch 20/20] avg loss: -0.0006074601277461219		[learning rate: 0.0002655]
	Learning Rate: 0.000265501
	LOSS [training: -0.003372311628988066 | validation: 0.0006414242963195405]
	TIME [epoch: 8.81 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024275531756719664		[learning rate: 0.00026509]
		[batch 20/20] avg loss: -0.007851079885244118		[learning rate: 0.00026469]
	Learning Rate: 0.000264687
	LOSS [training: -0.005139316530458042 | validation: 0.004778984940756547]
	TIME [epoch: 8.84 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002651352012717322		[learning rate: 0.00026428]
		[batch 20/20] avg loss: -0.0044315315977537005		[learning rate: 0.00026388]
	Learning Rate: 0.000263876
	LOSS [training: -0.003541441805235511 | validation: -0.005735468837952493]
	TIME [epoch: 8.82 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022727002127378966		[learning rate: 0.00026347]
		[batch 20/20] avg loss: -0.003469291970960382		[learning rate: 0.00026307]
	Learning Rate: 0.000263067
	LOSS [training: -0.00287099609184914 | validation: 0.008079758846149698]
	TIME [epoch: 8.82 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00039024785391196943		[learning rate: 0.00026266]
		[batch 20/20] avg loss: -0.0059803094934782934		[learning rate: 0.00026226]
	Learning Rate: 0.000262261
	LOSS [training: -0.003185278673695132 | validation: -0.004980210780764528]
	TIME [epoch: 8.82 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009164806804074237		[learning rate: 0.00026186]
		[batch 20/20] avg loss: -0.003999578637054238		[learning rate: 0.00026146]
	Learning Rate: 0.000261457
	LOSS [training: -0.0024580296587308303 | validation: 0.0010538283071395028]
	TIME [epoch: 8.84 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002805750256505304		[learning rate: 0.00026106]
		[batch 20/20] avg loss: -0.0065312248385861775		[learning rate: 0.00026066]
	Learning Rate: 0.000260655
	LOSS [training: -0.003125324906467824 | validation: -0.004245423937387086]
	TIME [epoch: 8.83 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003738825514738858		[learning rate: 0.00026026]
		[batch 20/20] avg loss: -0.00543391557108819		[learning rate: 0.00025986]
	Learning Rate: 0.000259856
	LOSS [training: -0.000847545028174666 | validation: 0.0005047302035142034]
	TIME [epoch: 8.82 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005179501401766707		[learning rate: 0.00025946]
		[batch 20/20] avg loss: -0.009302915604404213		[learning rate: 0.00025906]
	Learning Rate: 0.00025906
	LOSS [training: -0.0072412085030854586 | validation: -0.00019877556206719443]
	TIME [epoch: 8.82 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035178548598313935		[learning rate: 0.00025866]
		[batch 20/20] avg loss: -0.0033106236141020986		[learning rate: 0.00025827]
	Learning Rate: 0.000258266
	LOSS [training: -0.0034142392369667456 | validation: -0.00242855372112421]
	TIME [epoch: 8.83 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006841492268384208		[learning rate: 0.00025787]
		[batch 20/20] avg loss: -0.0023160154879784873		[learning rate: 0.00025747]
	Learning Rate: 0.000257474
	LOSS [training: -0.004578753878181348 | validation: 0.010149246285958298]
	TIME [epoch: 8.84 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033389055715879155		[learning rate: 0.00025708]
		[batch 20/20] avg loss: -0.0036429985091189464		[learning rate: 0.00025668]
	Learning Rate: 0.000256685
	LOSS [training: -0.00015204646876551526 | validation: 0.003773819728167826]
	TIME [epoch: 8.82 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005210308009292607		[learning rate: 0.00025629]
		[batch 20/20] avg loss: -0.005119720233719609		[learning rate: 0.0002559]
	Learning Rate: 0.000255898
	LOSS [training: -0.005165014121506108 | validation: -0.004783068804810655]
	TIME [epoch: 8.82 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006869117806023695		[learning rate: 0.00025551]
		[batch 20/20] avg loss: -0.0030884246753094782		[learning rate: 0.00025511]
	Learning Rate: 0.000255113
	LOSS [training: -0.0049787712406665855 | validation: -0.003458438334954844]
	TIME [epoch: 8.81 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00427822464180251		[learning rate: 0.00025472]
		[batch 20/20] avg loss: -0.0066160175726372936		[learning rate: 0.00025433]
	Learning Rate: 0.000254331
	LOSS [training: -0.0054471211072199014 | validation: -0.0011468513428615416]
	TIME [epoch: 8.84 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00547554641812745		[learning rate: 0.00025394]
		[batch 20/20] avg loss: -0.005102395177485855		[learning rate: 0.00025355]
	Learning Rate: 0.000253552
	LOSS [training: -0.005288970797806652 | validation: 0.0023226989903160707]
	TIME [epoch: 8.81 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00026657245675155045		[learning rate: 0.00025316]
		[batch 20/20] avg loss: -0.0029721879406868963		[learning rate: 0.00025277]
	Learning Rate: 0.000252775
	LOSS [training: -0.0013528077419676728 | validation: -0.00461574800613823]
	TIME [epoch: 8.82 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006388727769001855		[learning rate: 0.00025239]
		[batch 20/20] avg loss: -0.003217670054669771		[learning rate: 0.000252]
	Learning Rate: 0.000252
	LOSS [training: -0.004803198911835812 | validation: -0.002230667753094513]
	TIME [epoch: 8.82 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00376226223574795		[learning rate: 0.00025161]
		[batch 20/20] avg loss: -0.003133392366289264		[learning rate: 0.00025123]
	Learning Rate: 0.000251227
	LOSS [training: -0.0034478273010186073 | validation: -0.00562464731841816]
	TIME [epoch: 8.84 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038708665481142503		[learning rate: 0.00025084]
		[batch 20/20] avg loss: -0.004499043245021135		[learning rate: 0.00025046]
	Learning Rate: 0.000250457
	LOSS [training: -0.004184954896567692 | validation: -0.0007591932449046756]
	TIME [epoch: 8.82 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0051364944008649525		[learning rate: 0.00025007]
		[batch 20/20] avg loss: -0.010470293868733738		[learning rate: 0.00024969]
	Learning Rate: 0.000249689
	LOSS [training: -0.007803394134799346 | validation: -0.002896730977985736]
	TIME [epoch: 8.81 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003344403481622343		[learning rate: 0.00024931]
		[batch 20/20] avg loss: -0.00591573756694478		[learning rate: 0.00024892]
	Learning Rate: 0.000248924
	LOSS [training: -0.00463007052428356 | validation: -0.0060242187610286815]
	TIME [epoch: 8.82 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0074266689988318774		[learning rate: 0.00024854]
		[batch 20/20] avg loss: -0.00832861340310962		[learning rate: 0.00024816]
	Learning Rate: 0.000248161
	LOSS [training: -0.007877641200970748 | validation: -0.009054870932683813]
	TIME [epoch: 8.83 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021200432279535686		[learning rate: 0.00024778]
		[batch 20/20] avg loss: -0.009235524254179107		[learning rate: 0.0002474]
	Learning Rate: 0.0002474
	LOSS [training: -0.005677783741066337 | validation: -0.005639994347071227]
	TIME [epoch: 8.83 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009400034466501428		[learning rate: 0.00024702]
		[batch 20/20] avg loss: -0.002390058112357603		[learning rate: 0.00024664]
	Learning Rate: 0.000246642
	LOSS [training: -0.005895046289429516 | validation: -0.009605851152361788]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1706.pth
	Model improved!!!
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009036786321122753		[learning rate: 0.00024626]
		[batch 20/20] avg loss: -0.007248821016230564		[learning rate: 0.00024589]
	Learning Rate: 0.000245886
	LOSS [training: -0.008142803668676656 | validation: -0.001768984395074504]
	TIME [epoch: 8.84 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005426216929225855		[learning rate: 0.00024551]
		[batch 20/20] avg loss: -0.006252178592168356		[learning rate: 0.00024513]
	Learning Rate: 0.000245132
	LOSS [training: -0.005839197760697104 | validation: 0.0018731068494994957]
	TIME [epoch: 8.84 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006574436636502962		[learning rate: 0.00024476]
		[batch 20/20] avg loss: -0.009502814022449131		[learning rate: 0.00024438]
	Learning Rate: 0.000244381
	LOSS [training: -0.008038625329476048 | validation: -0.001542467058838119]
	TIME [epoch: 8.86 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002356655854290541		[learning rate: 0.00024401]
		[batch 20/20] avg loss: -0.0027231814525596337		[learning rate: 0.00024363]
	Learning Rate: 0.000243631
	LOSS [training: -0.002539918653425088 | validation: -0.00564547845823739]
	TIME [epoch: 8.84 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038063720314235397		[learning rate: 0.00024326]
		[batch 20/20] avg loss: -0.004795217719485825		[learning rate: 0.00024288]
	Learning Rate: 0.000242885
	LOSS [training: -0.004300794875454682 | validation: -0.009883161625783498]
	TIME [epoch: 8.84 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1711.pth
	Model improved!!!
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034399746261027887		[learning rate: 0.00024251]
		[batch 20/20] avg loss: -0.001666181538940853		[learning rate: 0.00024214]
	Learning Rate: 0.00024214
	LOSS [training: -0.0025530780825218204 | validation: -0.0037209903203965105]
	TIME [epoch: 8.86 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031493546460854027		[learning rate: 0.00024177]
		[batch 20/20] avg loss: -0.004831620985814864		[learning rate: 0.0002414]
	Learning Rate: 0.000241398
	LOSS [training: -0.003990487815950133 | validation: -0.005348176558496186]
	TIME [epoch: 8.87 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007491771763728247		[learning rate: 0.00024103]
		[batch 20/20] avg loss: -0.007975273670565127		[learning rate: 0.00024066]
	Learning Rate: 0.000240658
	LOSS [training: -0.007733522717146687 | validation: -0.0036551666202489634]
	TIME [epoch: 8.85 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006637059967301901		[learning rate: 0.00024029]
		[batch 20/20] avg loss: -0.00185782256644302		[learning rate: 0.00023992]
	Learning Rate: 0.00023992
	LOSS [training: -0.004247441266872461 | validation: -0.005319932878594211]
	TIME [epoch: 8.85 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007757556793825779		[learning rate: 0.00023955]
		[batch 20/20] avg loss: -0.002932726940944013		[learning rate: 0.00023918]
	Learning Rate: 0.000239185
	LOSS [training: -0.005345141867384896 | validation: 0.002904971361547406]
	TIME [epoch: 8.86 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00027267581125045776		[learning rate: 0.00023882]
		[batch 20/20] avg loss: -0.0039914795823249765		[learning rate: 0.00023845]
	Learning Rate: 0.000238451
	LOSS [training: -0.0021320776967877165 | validation: -0.0002292934069192409]
	TIME [epoch: 8.87 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004649029007146645		[learning rate: 0.00023809]
		[batch 20/20] avg loss: -0.002304741008485202		[learning rate: 0.00023772]
	Learning Rate: 0.000237721
	LOSS [training: -0.003476885007815923 | validation: -0.006005242478278324]
	TIME [epoch: 8.86 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002559594215685508		[learning rate: 0.00023736]
		[batch 20/20] avg loss: 0.0011721917590301922		[learning rate: 0.00023699]
	Learning Rate: 0.000236992
	LOSS [training: -0.0006937012283276579 | validation: -0.0013142126677584715]
	TIME [epoch: 8.85 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004145273533641448		[learning rate: 0.00023663]
		[batch 20/20] avg loss: -0.006637838549820992		[learning rate: 0.00023627]
	Learning Rate: 0.000236265
	LOSS [training: -0.00539155604173122 | validation: -0.0012787528114515884]
	TIME [epoch: 8.85 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00034507862207708783		[learning rate: 0.0002359]
		[batch 20/20] avg loss: 0.002049375480863137		[learning rate: 0.00023554]
	Learning Rate: 0.000235541
	LOSS [training: 0.0008521484293930244 | validation: -0.0039349078385139055]
	TIME [epoch: 8.86 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0052613240766758934		[learning rate: 0.00023518]
		[batch 20/20] avg loss: -0.0044836999434520455		[learning rate: 0.00023482]
	Learning Rate: 0.000234819
	LOSS [training: -0.00487251201006397 | validation: -0.0035989004672897875]
	TIME [epoch: 8.86 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035843707634564737		[learning rate: 0.00023446]
		[batch 20/20] avg loss: -0.0070729598461427515		[learning rate: 0.0002341]
	Learning Rate: 0.000234099
	LOSS [training: -0.005328665304799612 | validation: 0.003102601056040602]
	TIME [epoch: 8.85 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00019101381486028032		[learning rate: 0.00023374]
		[batch 20/20] avg loss: -0.007977865455072087		[learning rate: 0.00023338]
	Learning Rate: 0.000233382
	LOSS [training: -0.003893425820105903 | validation: -0.000320488779514589]
	TIME [epoch: 8.85 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005005723001365064		[learning rate: 0.00023302]
		[batch 20/20] avg loss: -0.007320949267956766		[learning rate: 0.00023267]
	Learning Rate: 0.000232666
	LOSS [training: -0.006163336134660915 | validation: -0.002168192641291367]
	TIME [epoch: 8.85 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005391977904043556		[learning rate: 0.00023231]
		[batch 20/20] avg loss: -0.0057793897500434096		[learning rate: 0.00023195]
	Learning Rate: 0.000231953
	LOSS [training: -0.005585683827043483 | validation: -0.006934711762265048]
	TIME [epoch: 8.87 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004250608912255182		[learning rate: 0.0002316]
		[batch 20/20] avg loss: -0.005715078691547261		[learning rate: 0.00023124]
	Learning Rate: 0.000231242
	LOSS [training: -0.0049828438019012215 | validation: -0.003364652161090359]
	TIME [epoch: 8.86 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007987475781449099		[learning rate: 0.00023089]
		[batch 20/20] avg loss: -0.0037082899343891043		[learning rate: 0.00023053]
	Learning Rate: 0.000230533
	LOSS [training: -0.005847882857919102 | validation: 0.0009389579053834269]
	TIME [epoch: 8.85 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004681277212608517		[learning rate: 0.00023018]
		[batch 20/20] avg loss: -0.0022934386204590206		[learning rate: 0.00022983]
	Learning Rate: 0.000229826
	LOSS [training: -0.00348735791653377 | validation: 0.004598663432787044]
	TIME [epoch: 8.85 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009953279947907605		[learning rate: 0.00022947]
		[batch 20/20] avg loss: -0.0038364188786566998		[learning rate: 0.00022912]
	Learning Rate: 0.000229122
	LOSS [training: -0.006894849413282155 | validation: -0.0014403934330023314]
	TIME [epoch: 8.87 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0063414952090923874		[learning rate: 0.00022877]
		[batch 20/20] avg loss: -0.007515051476042006		[learning rate: 0.00022842]
	Learning Rate: 0.00022842
	LOSS [training: -0.006928273342567197 | validation: -0.001983771411495679]
	TIME [epoch: 8.85 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010695803562197502		[learning rate: 0.00022807]
		[batch 20/20] avg loss: 0.0002161048855073209		[learning rate: 0.00022772]
	Learning Rate: 0.000227719
	LOSS [training: -0.005239849338345091 | validation: -0.0012283720556240678]
	TIME [epoch: 8.85 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006768626359105268		[learning rate: 0.00022737]
		[batch 20/20] avg loss: -0.012378760732467072		[learning rate: 0.00022702]
	Learning Rate: 0.000227021
	LOSS [training: -0.0065278116841888 | validation: -0.007618410864797628]
	TIME [epoch: 8.86 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009014417415981978		[learning rate: 0.00022667]
		[batch 20/20] avg loss: -0.005064623157133226		[learning rate: 0.00022633]
	Learning Rate: 0.000226325
	LOSS [training: -0.007039520286557602 | validation: 0.005414569683675329]
	TIME [epoch: 8.87 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0048873528315872775		[learning rate: 0.00022598]
		[batch 20/20] avg loss: -0.006689463047364047		[learning rate: 0.00022563]
	Learning Rate: 0.000225632
	LOSS [training: -0.005788407939475663 | validation: -0.004023035979639144]
	TIME [epoch: 8.85 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005283817013630778		[learning rate: 0.00022529]
		[batch 20/20] avg loss: -0.006076449152758449		[learning rate: 0.00022494]
	Learning Rate: 0.00022494
	LOSS [training: -0.005680133083194613 | validation: 0.0006553493061728819]
	TIME [epoch: 8.84 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004391901231080184		[learning rate: 0.00022459]
		[batch 20/20] avg loss: -0.005724083786620741		[learning rate: 0.00022425]
	Learning Rate: 0.00022425
	LOSS [training: -0.0050579925088504625 | validation: -0.003309598967405147]
	TIME [epoch: 8.85 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0075107506826205565		[learning rate: 0.00022391]
		[batch 20/20] avg loss: -0.0035344556179010417		[learning rate: 0.00022356]
	Learning Rate: 0.000223563
	LOSS [training: -0.0055226031502607995 | validation: -0.002597911099340397]
	TIME [epoch: 8.85 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01032939760504685		[learning rate: 0.00022322]
		[batch 20/20] avg loss: -0.0015221072366695858		[learning rate: 0.00022288]
	Learning Rate: 0.000222878
	LOSS [training: -0.0059257524208582175 | validation: -0.00415828868481195]
	TIME [epoch: 8.87 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009318999097859458		[learning rate: 0.00022254]
		[batch 20/20] avg loss: -0.007546676193631717		[learning rate: 0.00022219]
	Learning Rate: 0.000222195
	LOSS [training: -0.008432837645745586 | validation: 0.0019877188074855535]
	TIME [epoch: 8.85 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0075656670299392645		[learning rate: 0.00022185]
		[batch 20/20] avg loss: -0.0032455222561317393		[learning rate: 0.00022151]
	Learning Rate: 0.000221513
	LOSS [training: -0.005405594643035502 | validation: -0.0011578566960011097]
	TIME [epoch: 8.85 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005599800312695151		[learning rate: 0.00022117]
		[batch 20/20] avg loss: -0.007351820911983183		[learning rate: 0.00022083]
	Learning Rate: 0.000220834
	LOSS [training: -0.006475810612339167 | validation: -0.00757269566073396]
	TIME [epoch: 8.86 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00018930139640553612		[learning rate: 0.0002205]
		[batch 20/20] avg loss: -0.006920868971555481		[learning rate: 0.00022016]
	Learning Rate: 0.000220157
	LOSS [training: -0.0035550851839805087 | validation: -0.0015535126311581788]
	TIME [epoch: 8.87 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004859500394267749		[learning rate: 0.00021982]
		[batch 20/20] avg loss: -0.005196136366511085		[learning rate: 0.00021948]
	Learning Rate: 0.000219483
	LOSS [training: -0.005027818380389417 | validation: -0.004519857634253079]
	TIME [epoch: 8.86 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004892226980293164		[learning rate: 0.00021915]
		[batch 20/20] avg loss: 0.00012512525634195295		[learning rate: 0.00021881]
	Learning Rate: 0.00021881
	LOSS [training: -0.0023835508619756054 | validation: 0.005593087370440148]
	TIME [epoch: 8.85 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008734874721921793		[learning rate: 0.00021847]
		[batch 20/20] avg loss: -0.004128447134404618		[learning rate: 0.00021814]
	Learning Rate: 0.000218139
	LOSS [training: -0.0064316609281632065 | validation: -0.004162488667917595]
	TIME [epoch: 8.85 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005682281973362534		[learning rate: 0.0002178]
		[batch 20/20] avg loss: -0.008334767168931572		[learning rate: 0.00021747]
	Learning Rate: 0.00021747
	LOSS [training: -0.007008524571147053 | validation: 0.004882767191350617]
	TIME [epoch: 8.87 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006808123620813879		[learning rate: 0.00021714]
		[batch 20/20] avg loss: -0.002770311566522777		[learning rate: 0.0002168]
	Learning Rate: 0.000216804
	LOSS [training: -0.004789217593668328 | validation: -0.005869311713384957]
	TIME [epoch: 8.86 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038482685219897853		[learning rate: 0.00021647]
		[batch 20/20] avg loss: -0.003260854885876141		[learning rate: 0.00021614]
	Learning Rate: 0.000216139
	LOSS [training: -0.0035545617039329626 | validation: 0.00037271450864012]
	TIME [epoch: 8.85 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035041786290410425		[learning rate: 0.00021581]
		[batch 20/20] avg loss: -0.003889760068414274		[learning rate: 0.00021548]
	Learning Rate: 0.000215477
	LOSS [training: -0.003696969348727659 | validation: 0.0068380009641466595]
	TIME [epoch: 8.85 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004528250859775342		[learning rate: 0.00021515]
		[batch 20/20] avg loss: -0.009421229897485712		[learning rate: 0.00021482]
	Learning Rate: 0.000214816
	LOSS [training: -0.006974740378630528 | validation: -0.0015145784230666906]
	TIME [epoch: 8.88 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004837120666428196		[learning rate: 0.00021449]
		[batch 20/20] avg loss: -0.0030216023455049974		[learning rate: 0.00021416]
	Learning Rate: 0.000214157
	LOSS [training: -0.003929361505966598 | validation: -0.005012826490864887]
	TIME [epoch: 8.86 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004477193698917184		[learning rate: 0.00021383]
		[batch 20/20] avg loss: -0.0033789595109804125		[learning rate: 0.0002135]
	Learning Rate: 0.000213501
	LOSS [training: -0.003928076604948798 | validation: 0.0017985081824505903]
	TIME [epoch: 8.86 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00573941193772011		[learning rate: 0.00021317]
		[batch 20/20] avg loss: -0.0004464770420859951		[learning rate: 0.00021285]
	Learning Rate: 0.000212847
	LOSS [training: -0.0030929444899030533 | validation: -0.0033321593230499114]
	TIME [epoch: 8.86 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005547994565476204		[learning rate: 0.00021252]
		[batch 20/20] avg loss: -0.007457561933420233		[learning rate: 0.00021219]
	Learning Rate: 0.000212194
	LOSS [training: -0.006502778249448218 | validation: -0.0003502253119992602]
	TIME [epoch: 8.86 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002546444027727415		[learning rate: 0.00021187]
		[batch 20/20] avg loss: -0.009406163744561765		[learning rate: 0.00021154]
	Learning Rate: 0.000211544
	LOSS [training: -0.00597630388614459 | validation: 0.0039510052495372525]
	TIME [epoch: 8.88 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002170764037060932		[learning rate: 0.00021122]
		[batch 20/20] avg loss: -0.006439017175722271		[learning rate: 0.0002109]
	Learning Rate: 0.000210895
	LOSS [training: -0.004304890606391602 | validation: -0.005667192423449632]
	TIME [epoch: 8.86 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007919997896639951		[learning rate: 0.00021057]
		[batch 20/20] avg loss: -0.0029224817668261718		[learning rate: 0.00021025]
	Learning Rate: 0.000210249
	LOSS [training: -0.005421239831733061 | validation: -0.003813136095486868]
	TIME [epoch: 8.85 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017097617132954233		[learning rate: 0.00020993]
		[batch 20/20] avg loss: -0.004786554654750391		[learning rate: 0.0002096]
	Learning Rate: 0.000209604
	LOSS [training: -0.003248158184022907 | validation: -0.0012718944947139631]
	TIME [epoch: 8.86 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022248686147026873		[learning rate: 0.00020928]
		[batch 20/20] avg loss: -0.004984076486279626		[learning rate: 0.00020896]
	Learning Rate: 0.000208962
	LOSS [training: -0.003604472550491157 | validation: -0.003743875281003562]
	TIME [epoch: 8.88 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021891866453381164		[learning rate: 0.00020864]
		[batch 20/20] avg loss: -0.003913364544728004		[learning rate: 0.00020832]
	Learning Rate: 0.000208321
	LOSS [training: -0.0030512755950330606 | validation: -0.0023915448293014814]
	TIME [epoch: 8.86 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023205980296232952		[learning rate: 0.000208]
		[batch 20/20] avg loss: -0.005394130737039184		[learning rate: 0.00020768]
	Learning Rate: 0.000207683
	LOSS [training: -0.0038573643833312394 | validation: -0.004563024212650931]
	TIME [epoch: 8.86 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004319175231645085		[learning rate: 0.00020736]
		[batch 20/20] avg loss: -0.01067060414537149		[learning rate: 0.00020705]
	Learning Rate: 0.000207046
	LOSS [training: -0.007494889688508288 | validation: 0.003936425884160141]
	TIME [epoch: 8.85 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008937306256065128		[learning rate: 0.00020673]
		[batch 20/20] avg loss: -0.00394415953500115		[learning rate: 0.00020641]
	Learning Rate: 0.000206411
	LOSS [training: -0.006440732895533138 | validation: -0.004562677545832483]
	TIME [epoch: 8.88 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008237575195144868		[learning rate: 0.00020609]
		[batch 20/20] avg loss: -0.00876595670657586		[learning rate: 0.00020578]
	Learning Rate: 0.000205778
	LOSS [training: -0.008501765950860364 | validation: -0.0022140561449324058]
	TIME [epoch: 8.86 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005391804409804949		[learning rate: 0.00020546]
		[batch 20/20] avg loss: -0.0037953449789231977		[learning rate: 0.00020515]
	Learning Rate: 0.000205148
	LOSS [training: -0.004593574694364072 | validation: 0.00031996139562843334]
	TIME [epoch: 8.86 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038870395302255676		[learning rate: 0.00020483]
		[batch 20/20] avg loss: -0.0065309993591065		[learning rate: 0.00020452]
	Learning Rate: 0.000204519
	LOSS [training: -0.005209019444666033 | validation: -0.004185047095243878]
	TIME [epoch: 8.85 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009092341004478548		[learning rate: 0.00020421]
		[batch 20/20] avg loss: -0.0030953587542048755		[learning rate: 0.00020389]
	Learning Rate: 0.000203892
	LOSS [training: -0.0060938498793417116 | validation: 0.0004657801738670428]
	TIME [epoch: 8.87 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004774133190693674		[learning rate: 0.00020358]
		[batch 20/20] avg loss: -0.0027892109335912367		[learning rate: 0.00020327]
	Learning Rate: 0.000203267
	LOSS [training: -0.003781672062142454 | validation: -0.00718696517809681]
	TIME [epoch: 8.87 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008124534022860666		[learning rate: 0.00020296]
		[batch 20/20] avg loss: -0.00521112754850252		[learning rate: 0.00020264]
	Learning Rate: 0.000202644
	LOSS [training: -0.006667830785681593 | validation: -0.00553671344118997]
	TIME [epoch: 8.85 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004444889627249773		[learning rate: 0.00020233]
		[batch 20/20] avg loss: -0.0023302303028957644		[learning rate: 0.00020202]
	Learning Rate: 0.000202023
	LOSS [training: -0.00338755996507277 | validation: -0.004086447353044326]
	TIME [epoch: 8.85 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011193768672228127		[learning rate: 0.00020171]
		[batch 20/20] avg loss: -0.006337309769864917		[learning rate: 0.0002014]
	Learning Rate: 0.000201403
	LOSS [training: -0.0037283433185438647 | validation: -0.0001998624862221928]
	TIME [epoch: 8.85 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004139747343967852		[learning rate: 0.00020109]
		[batch 20/20] avg loss: -0.0076819861023953466		[learning rate: 0.00020079]
	Learning Rate: 0.000200786
	LOSS [training: -0.0059108667231815995 | validation: -0.004433318106991661]
	TIME [epoch: 8.88 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010470437604934257		[learning rate: 0.00020048]
		[batch 20/20] avg loss: -0.008151817656447843		[learning rate: 0.00020017]
	Learning Rate: 0.00020017
	LOSS [training: -0.004599430708470635 | validation: -0.01066945301936748]
	TIME [epoch: 8.86 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1774.pth
	Model improved!!!
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000941090089900282		[learning rate: 0.00019986]
		[batch 20/20] avg loss: -0.00770432360437154		[learning rate: 0.00019956]
	Learning Rate: 0.000199557
	LOSS [training: -0.004322706847135911 | validation: -0.001731977693824058]
	TIME [epoch: 8.84 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012222925166567344		[learning rate: 0.00019925]
		[batch 20/20] avg loss: -0.00787359587579251		[learning rate: 0.00019895]
	Learning Rate: 0.000198945
	LOSS [training: -0.004547944196224623 | validation: -0.0003824680258671544]
	TIME [epoch: 8.84 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004737549772630842		[learning rate: 0.00019864]
		[batch 20/20] avg loss: -0.006135177119163404		[learning rate: 0.00019834]
	Learning Rate: 0.000198335
	LOSS [training: -0.005436363445897123 | validation: -0.004578597471194105]
	TIME [epoch: 8.86 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006733189177069066		[learning rate: 0.00019803]
		[batch 20/20] avg loss: -0.0049138202721722025		[learning rate: 0.00019773]
	Learning Rate: 0.000197727
	LOSS [training: -0.005823504724620634 | validation: -0.00044106720264186364]
	TIME [epoch: 8.85 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002666202109543603		[learning rate: 0.00019742]
		[batch 20/20] avg loss: -0.0071525545749783525		[learning rate: 0.00019712]
	Learning Rate: 0.000197121
	LOSS [training: -0.004909378342260976 | validation: -0.001068925848432564]
	TIME [epoch: 8.84 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00924473466657982		[learning rate: 0.00019682]
		[batch 20/20] avg loss: -0.0044293906724574995		[learning rate: 0.00019652]
	Learning Rate: 0.000196517
	LOSS [training: -0.006837062669518659 | validation: 0.00042811881035499405]
	TIME [epoch: 8.84 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007529001106941167		[learning rate: 0.00019622]
		[batch 20/20] avg loss: -0.00978022407463798		[learning rate: 0.00019591]
	Learning Rate: 0.000195915
	LOSS [training: -0.008654612590789573 | validation: -0.0010424253105628298]
	TIME [epoch: 8.86 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0058303717808190466		[learning rate: 0.00019561]
		[batch 20/20] avg loss: -0.005293119625286483		[learning rate: 0.00019531]
	Learning Rate: 0.000195314
	LOSS [training: -0.005561745703052764 | validation: -0.00845770681149376]
	TIME [epoch: 8.86 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004128073431505212		[learning rate: 0.00019501]
		[batch 20/20] avg loss: -0.005238996177035253		[learning rate: 0.00019472]
	Learning Rate: 0.000194715
	LOSS [training: -0.004683534804270234 | validation: -0.0011599578270682573]
	TIME [epoch: 8.85 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00933148694437955		[learning rate: 0.00019442]
		[batch 20/20] avg loss: -0.0013370295729444137		[learning rate: 0.00019412]
	Learning Rate: 0.000194118
	LOSS [training: -0.005334258258661984 | validation: -0.005261613159173535]
	TIME [epoch: 8.85 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025024694773370464		[learning rate: 0.00019382]
		[batch 20/20] avg loss: -0.007672233764961159		[learning rate: 0.00019352]
	Learning Rate: 0.000193523
	LOSS [training: -0.005087351621149101 | validation: -0.007124624044659309]
	TIME [epoch: 8.85 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007179938601636382		[learning rate: 0.00019323]
		[batch 20/20] avg loss: -0.0062152256662934445		[learning rate: 0.00019293]
	Learning Rate: 0.00019293
	LOSS [training: -0.0066975821339649135 | validation: -0.007251538229296051]
	TIME [epoch: 8.87 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037097112158699765		[learning rate: 0.00019263]
		[batch 20/20] avg loss: -0.005661300161442907		[learning rate: 0.00019234]
	Learning Rate: 0.000192339
	LOSS [training: -0.004685505688656441 | validation: -0.0038176448131721956]
	TIME [epoch: 8.84 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005188481038624908		[learning rate: 0.00019204]
		[batch 20/20] avg loss: -0.003528825285687668		[learning rate: 0.00019175]
	Learning Rate: 0.000191749
	LOSS [training: -0.004358653162156288 | validation: -0.0002891369263038268]
	TIME [epoch: 8.85 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003120186429960806		[learning rate: 0.00019145]
		[batch 20/20] avg loss: -0.004670846150418616		[learning rate: 0.00019116]
	Learning Rate: 0.000191161
	LOSS [training: -0.0038955162901897116 | validation: -0.0036640081961087363]
	TIME [epoch: 8.84 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006076724047888414		[learning rate: 0.00019087]
		[batch 20/20] avg loss: -0.005841144980646478		[learning rate: 0.00019058]
	Learning Rate: 0.000190575
	LOSS [training: -0.005958934514267447 | validation: -0.006561789029112]
	TIME [epoch: 8.87 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006941666173250545		[learning rate: 0.00019028]
		[batch 20/20] avg loss: -0.0031917607407598208		[learning rate: 0.00018999]
	Learning Rate: 0.000189991
	LOSS [training: -0.005066713457005182 | validation: -0.0012786866289824538]
	TIME [epoch: 8.85 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008924572741680324		[learning rate: 0.0001897]
		[batch 20/20] avg loss: -0.006371376326899119		[learning rate: 0.00018941]
	Learning Rate: 0.000189409
	LOSS [training: -0.003631916800533576 | validation: -0.002605102985405615]
	TIME [epoch: 8.84 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008320748413814087		[learning rate: 0.00018912]
		[batch 20/20] avg loss: -0.0022510786143497327		[learning rate: 0.00018883]
	Learning Rate: 0.000188828
	LOSS [training: -0.0052859135140819104 | validation: -0.003735581064634505]
	TIME [epoch: 8.83 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004989972331836274		[learning rate: 0.00018854]
		[batch 20/20] avg loss: -0.005937880765468166		[learning rate: 0.00018825]
	Learning Rate: 0.000188249
	LOSS [training: -0.002719441766142269 | validation: -0.002685898032291083]
	TIME [epoch: 8.86 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004790066944420685		[learning rate: 0.00018796]
		[batch 20/20] avg loss: -0.004149571268764093		[learning rate: 0.00018767]
	Learning Rate: 0.000187672
	LOSS [training: -0.004469819106592388 | validation: -4.957587833103057e-05]
	TIME [epoch: 8.84 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008147666249759747		[learning rate: 0.00018738]
		[batch 20/20] avg loss: -0.0038522411600797083		[learning rate: 0.0001871]
	Learning Rate: 0.000187097
	LOSS [training: -0.005999953704919728 | validation: -0.004122218021272606]
	TIME [epoch: 8.84 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006909460442621561		[learning rate: 0.00018681]
		[batch 20/20] avg loss: -0.0027561434679623455		[learning rate: 0.00018652]
	Learning Rate: 0.000186523
	LOSS [training: -0.004832801955291953 | validation: -0.0036972313808750607]
	TIME [epoch: 8.84 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004681227530615		[learning rate: 0.00018624]
		[batch 20/20] avg loss: -0.0036756684548976716		[learning rate: 0.00018595]
	Learning Rate: 0.000185952
	LOSS [training: -0.0041784479927563355 | validation: -0.005527643620940299]
	TIME [epoch: 8.86 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003119271166825648		[learning rate: 0.00018567]
		[batch 20/20] avg loss: -0.007012092496787983		[learning rate: 0.00018538]
	Learning Rate: 0.000185382
	LOSS [training: -0.005065681831806815 | validation: -0.00526393297654087]
	TIME [epoch: 8.86 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009932155337936972		[learning rate: 0.0001851]
		[batch 20/20] avg loss: -0.007022886339421006		[learning rate: 0.00018481]
	Learning Rate: 0.000184813
	LOSS [training: -0.008477520838678991 | validation: -0.0009994923803128264]
	TIME [epoch: 8.84 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001312349424531825		[learning rate: 0.00018453]
		[batch 20/20] avg loss: -0.006891452162370022		[learning rate: 0.00018425]
	Learning Rate: 0.000184247
	LOSS [training: -0.004101900793450923 | validation: -0.005916282325492122]
	TIME [epoch: 8.84 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022659995021155413		[learning rate: 0.00018396]
		[batch 20/20] avg loss: -0.007053255297202435		[learning rate: 0.00018368]
	Learning Rate: 0.000183682
	LOSS [training: -0.004659627399658989 | validation: 0.0017830694186317044]
	TIME [epoch: 8.95 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035502987999550457		[learning rate: 0.0001834]
		[batch 20/20] avg loss: -0.010388213952455053		[learning rate: 0.00018312]
	Learning Rate: 0.000183119
	LOSS [training: -0.006969256376205049 | validation: -0.002168430060324341]
	TIME [epoch: 8.86 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008518231696391643		[learning rate: 0.00018284]
		[batch 20/20] avg loss: -0.010960687966958945		[learning rate: 0.00018256]
	Learning Rate: 0.000182558
	LOSS [training: -0.009739459831675295 | validation: -0.005877787636455698]
	TIME [epoch: 8.85 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006630529051059082		[learning rate: 0.00018228]
		[batch 20/20] avg loss: -0.006352981928120174		[learning rate: 0.000182]
	Learning Rate: 0.000181998
	LOSS [training: -0.006491755489589629 | validation: -0.004929782895947769]
	TIME [epoch: 8.84 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001487681591739136		[learning rate: 0.00018172]
		[batch 20/20] avg loss: -0.008275062812580034		[learning rate: 0.00018144]
	Learning Rate: 0.00018144
	LOSS [training: -0.004881372202159585 | validation: -0.004948641218027055]
	TIME [epoch: 8.84 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005398899062762375		[learning rate: 0.00018116]
		[batch 20/20] avg loss: -0.0025013494054830603		[learning rate: 0.00018088]
	Learning Rate: 0.000180884
	LOSS [training: -0.003950124234122718 | validation: 0.0013257709069270478]
	TIME [epoch: 8.86 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004508739528943065		[learning rate: 0.00018061]
		[batch 20/20] avg loss: -0.003705888855525253		[learning rate: 0.00018033]
	Learning Rate: 0.000180329
	LOSS [training: -0.00410731419223416 | validation: 0.0007170671011449163]
	TIME [epoch: 8.85 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013727034060753206		[learning rate: 0.00018005]
		[batch 20/20] avg loss: -0.01273340288939688		[learning rate: 0.00017978]
	Learning Rate: 0.000179777
	LOSS [training: -0.0070530531477360995 | validation: -0.0033133560047425613]
	TIME [epoch: 8.84 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001750030042171364		[learning rate: 0.0001795]
		[batch 20/20] avg loss: -0.0038373486333823275		[learning rate: 0.00017923]
	Learning Rate: 0.000179226
	LOSS [training: -0.002793689337776846 | validation: -0.004546096119005237]
	TIME [epoch: 8.84 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005998050582491638		[learning rate: 0.00017895]
		[batch 20/20] avg loss: -0.002326223576279928		[learning rate: 0.00017868]
	Learning Rate: 0.000178676
	LOSS [training: -0.004162137079385782 | validation: 0.002408399592482612]
	TIME [epoch: 8.86 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00872007704610047		[learning rate: 0.0001784]
		[batch 20/20] avg loss: -0.004088907757635062		[learning rate: 0.00017813]
	Learning Rate: 0.000178128
	LOSS [training: -0.006404492401867764 | validation: -0.00036288899510501235]
	TIME [epoch: 8.84 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004750487154130138		[learning rate: 0.00017786]
		[batch 20/20] avg loss: -0.003713742064676101		[learning rate: 0.00017758]
	Learning Rate: 0.000177582
	LOSS [training: -0.0042321146094031195 | validation: -0.0024304908137032654]
	TIME [epoch: 8.84 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00926991197066718		[learning rate: 0.00017731]
		[batch 20/20] avg loss: -0.0033208643212187423		[learning rate: 0.00017704]
	Learning Rate: 0.000177038
	LOSS [training: -0.006295388145942961 | validation: -0.0030652487214788806]
	TIME [epoch: 8.84 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004657597167932604		[learning rate: 0.00017677]
		[batch 20/20] avg loss: -0.006999951109190408		[learning rate: 0.0001765]
	Learning Rate: 0.000176495
	LOSS [training: -0.005828774138561506 | validation: -0.003627521602014828]
	TIME [epoch: 8.85 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0066681072706900905		[learning rate: 0.00017622]
		[batch 20/20] avg loss: -0.005349316472425749		[learning rate: 0.00017595]
	Learning Rate: 0.000175954
	LOSS [training: -0.0060087118715579195 | validation: -0.0014925571031422138]
	TIME [epoch: 8.87 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005888288565635332		[learning rate: 0.00017568]
		[batch 20/20] avg loss: -0.009701408405566832		[learning rate: 0.00017541]
	Learning Rate: 0.000175415
	LOSS [training: -0.007794848485601083 | validation: 0.0028264370501136344]
	TIME [epoch: 8.85 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033397935193618796		[learning rate: 0.00017515]
		[batch 20/20] avg loss: -0.007470026571034419		[learning rate: 0.00017488]
	Learning Rate: 0.000174877
	LOSS [training: -0.00540491004519815 | validation: 0.0005590032413960318]
	TIME [epoch: 8.85 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002591472779199684		[learning rate: 0.00017461]
		[batch 20/20] avg loss: -0.003218766207356445		[learning rate: 0.00017434]
	Learning Rate: 0.000174341
	LOSS [training: -0.002905119493278064 | validation: -0.005469831920217894]
	TIME [epoch: 8.85 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032216308195302163		[learning rate: 0.00017407]
		[batch 20/20] avg loss: -0.009542308820384267		[learning rate: 0.00017381]
	Learning Rate: 0.000173807
	LOSS [training: -0.0063819698199572405 | validation: -0.004197240589756331]
	TIME [epoch: 8.87 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005866884879595972		[learning rate: 0.00017354]
		[batch 20/20] avg loss: -0.002669548202068482		[learning rate: 0.00017327]
	Learning Rate: 0.000173274
	LOSS [training: -0.004268216540832228 | validation: -0.003742041466420716]
	TIME [epoch: 8.85 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038466799371449645		[learning rate: 0.00017301]
		[batch 20/20] avg loss: -0.00011489646677936083		[learning rate: 0.00017274]
	Learning Rate: 0.000172743
	LOSS [training: -0.0019807882019621625 | validation: -0.0029263852132531652]
	TIME [epoch: 8.84 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034164118363143106		[learning rate: 0.00017248]
		[batch 20/20] avg loss: -0.003718385851136578		[learning rate: 0.00017221]
	Learning Rate: 0.000172213
	LOSS [training: -0.003567398843725444 | validation: 0.002014355543939703]
	TIME [epoch: 8.85 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035703426474474865		[learning rate: 0.00017195]
		[batch 20/20] avg loss: 0.0013883112565726522		[learning rate: 0.00017169]
	Learning Rate: 0.000171685
	LOSS [training: -0.0010910156954374172 | validation: -0.0004115078229246405]
	TIME [epoch: 8.86 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0040580659153989515		[learning rate: 0.00017142]
		[batch 20/20] avg loss: -0.006856908162667238		[learning rate: 0.00017116]
	Learning Rate: 0.000171159
	LOSS [training: -0.005457487039033095 | validation: -0.0026193913032308207]
	TIME [epoch: 8.85 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009690648401003681		[learning rate: 0.0001709]
		[batch 20/20] avg loss: -0.0072149357615645645		[learning rate: 0.00017063]
	Learning Rate: 0.000170634
	LOSS [training: -0.008452792081284122 | validation: -0.0042241612302400785]
	TIME [epoch: 8.85 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002406392701919394		[learning rate: 0.00017037]
		[batch 20/20] avg loss: -0.005126353381213073		[learning rate: 0.00017011]
	Learning Rate: 0.000170111
	LOSS [training: -0.0037663730415662343 | validation: -0.0004934958684870193]
	TIME [epoch: 8.83 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020443207953502384		[learning rate: 0.00016985]
		[batch 20/20] avg loss: -0.007195041529463768		[learning rate: 0.00016959]
	Learning Rate: 0.00016959
	LOSS [training: -0.004619681162407003 | validation: -0.004727531483109212]
	TIME [epoch: 8.86 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009199354330813788		[learning rate: 0.00016933]
		[batch 20/20] avg loss: -0.0005145006211525703		[learning rate: 0.00016907]
	Learning Rate: 0.00016907
	LOSS [training: -0.0048569274759831796 | validation: 0.0017901649349642294]
	TIME [epoch: 8.85 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012926701531986524		[learning rate: 0.00016881]
		[batch 20/20] avg loss: -0.007390795577252587		[learning rate: 0.00016855]
	Learning Rate: 0.000168552
	LOSS [training: -0.00434173286522562 | validation: -0.004863289589303553]
	TIME [epoch: 8.84 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004383454074045839		[learning rate: 0.00016829]
		[batch 20/20] avg loss: -0.00969966583170258		[learning rate: 0.00016804]
	Learning Rate: 0.000168035
	LOSS [training: -0.004630660212148998 | validation: 0.0005123244038867242]
	TIME [epoch: 8.84 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010025305474293776		[learning rate: 0.00016778]
		[batch 20/20] avg loss: -0.003002962982186099		[learning rate: 0.00016752]
	Learning Rate: 0.00016752
	LOSS [training: -0.0020027467648077385 | validation: 0.0027376331002425276]
	TIME [epoch: 8.84 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023760715930987186		[learning rate: 0.00016726]
		[batch 20/20] avg loss: -0.005423066569899915		[learning rate: 0.00016701]
	Learning Rate: 0.000167006
	LOSS [training: -0.0038995690814993176 | validation: -0.003554283808881977]
	TIME [epoch: 8.86 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003061243610867501		[learning rate: 0.00016675]
		[batch 20/20] avg loss: -0.007650333177997538		[learning rate: 0.00016649]
	Learning Rate: 0.000166495
	LOSS [training: -0.005355788394432521 | validation: -0.002820227798797182]
	TIME [epoch: 8.84 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009572200903867983		[learning rate: 0.00016624]
		[batch 20/20] avg loss: -0.0067604757662006795		[learning rate: 0.00016598]
	Learning Rate: 0.000165984
	LOSS [training: -0.008166338335034332 | validation: -0.005265398557602291]
	TIME [epoch: 8.84 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005992686755333125		[learning rate: 0.00016573]
		[batch 20/20] avg loss: -0.004094338993832184		[learning rate: 0.00016548]
	Learning Rate: 0.000165475
	LOSS [training: -0.0050435128745826546 | validation: 0.0009472996458760202]
	TIME [epoch: 8.84 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003708250879606837		[learning rate: 0.00016522]
		[batch 20/20] avg loss: -0.006649348039354755		[learning rate: 0.00016497]
	Learning Rate: 0.000164968
	LOSS [training: -0.005178799459480797 | validation: -0.0002616018644354414]
	TIME [epoch: 8.86 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003084258632978465		[learning rate: 0.00016472]
		[batch 20/20] avg loss: -0.007997819170258255		[learning rate: 0.00016446]
	Learning Rate: 0.000164462
	LOSS [training: -0.0055410389016183614 | validation: -0.006236160623195413]
	TIME [epoch: 8.84 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003624131858530421		[learning rate: 0.00016421]
		[batch 20/20] avg loss: -0.008841125419486881		[learning rate: 0.00016396]
	Learning Rate: 0.000163958
	LOSS [training: -0.006232628639008651 | validation: -0.006338418260588698]
	TIME [epoch: 8.83 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008232109362899103		[learning rate: 0.00016371]
		[batch 20/20] avg loss: -0.0037814022754382045		[learning rate: 0.00016346]
	Learning Rate: 0.000163456
	LOSS [training: -0.006006755819168654 | validation: 0.004235962818578698]
	TIME [epoch: 8.84 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009238620994261957		[learning rate: 0.0001632]
		[batch 20/20] avg loss: -0.0014585868121485526		[learning rate: 0.00016295]
	Learning Rate: 0.000162955
	LOSS [training: -0.005348603903205254 | validation: -0.004722650467328749]
	TIME [epoch: 8.86 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0063639295173687745		[learning rate: 0.0001627]
		[batch 20/20] avg loss: -0.004260740954413955		[learning rate: 0.00016246]
	Learning Rate: 0.000162455
	LOSS [training: -0.005312335235891363 | validation: -0.0005691662997006979]
	TIME [epoch: 8.84 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022175795180249956		[learning rate: 0.00016221]
		[batch 20/20] avg loss: -0.006342597348130816		[learning rate: 0.00016196]
	Learning Rate: 0.000161957
	LOSS [training: -0.0042800884330779065 | validation: -0.0006591431195882053]
	TIME [epoch: 8.84 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00419036145011775		[learning rate: 0.00016171]
		[batch 20/20] avg loss: -0.004134132563651809		[learning rate: 0.00016146]
	Learning Rate: 0.000161461
	LOSS [training: -0.004162247006884779 | validation: -0.003910496140030337]
	TIME [epoch: 8.84 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004931738428502887		[learning rate: 0.00016121]
		[batch 20/20] avg loss: -0.00929947046325638		[learning rate: 0.00016097]
	Learning Rate: 0.000160966
	LOSS [training: -0.007115604445879634 | validation: 0.0008721183211919477]
	TIME [epoch: 8.84 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009033669201134433		[learning rate: 0.00016072]
		[batch 20/20] avg loss: -0.004275308697834301		[learning rate: 0.00016047]
	Learning Rate: 0.000160472
	LOSS [training: -0.006654488949484368 | validation: -0.0035372369040031796]
	TIME [epoch: 8.86 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0041981583039111346		[learning rate: 0.00016023]
		[batch 20/20] avg loss: -0.005022526746547654		[learning rate: 0.00015998]
	Learning Rate: 0.00015998
	LOSS [training: -0.004610342525229394 | validation: -0.002297652519341074]
	TIME [epoch: 8.83 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009709216549679356		[learning rate: 0.00015973]
		[batch 20/20] avg loss: -0.0057287793655763425		[learning rate: 0.00015949]
	Learning Rate: 0.00015949
	LOSS [training: -0.0033498505102721385 | validation: -0.004957371530765724]
	TIME [epoch: 8.83 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00874420105869652		[learning rate: 0.00015925]
		[batch 20/20] avg loss: -0.0053525463873371135		[learning rate: 0.000159]
	Learning Rate: 0.000159001
	LOSS [training: -0.007048373723016817 | validation: 0.002578357972306237]
	TIME [epoch: 8.83 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00682199787911304		[learning rate: 0.00015876]
		[batch 20/20] avg loss: -0.006124522785453739		[learning rate: 0.00015851]
	Learning Rate: 0.000158514
	LOSS [training: -0.00647326033228339 | validation: 0.0008367545477109542]
	TIME [epoch: 8.86 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002857512600890437		[learning rate: 0.00015827]
		[batch 20/20] avg loss: -0.008296337959324657		[learning rate: 0.00015803]
	Learning Rate: 0.000158028
	LOSS [training: -0.0055769252801075465 | validation: -0.002306417586974521]
	TIME [epoch: 8.84 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009157054771528134		[learning rate: 0.00015779]
		[batch 20/20] avg loss: -0.0006914095826361115		[learning rate: 0.00015754]
	Learning Rate: 0.000157543
	LOSS [training: -0.004924232177082125 | validation: -0.00024878160903280813]
	TIME [epoch: 8.83 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034389000446328243		[learning rate: 0.0001573]
		[batch 20/20] avg loss: -0.006170649861842471		[learning rate: 0.00015706]
	Learning Rate: 0.00015706
	LOSS [training: -0.004804774953237648 | validation: -0.0003908131706896953]
	TIME [epoch: 8.83 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003965688952999121		[learning rate: 0.00015682]
		[batch 20/20] avg loss: -0.0037947230207015397		[learning rate: 0.00015658]
	Learning Rate: 0.000156579
	LOSS [training: -0.003880205986850329 | validation: -0.0010300210852939352]
	TIME [epoch: 8.85 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035679977144164937		[learning rate: 0.00015634]
		[batch 20/20] avg loss: -0.007017648637243318		[learning rate: 0.0001561]
	Learning Rate: 0.000156099
	LOSS [training: -0.005292823175829906 | validation: -0.00610283089814208]
	TIME [epoch: 8.84 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0045268473455863795		[learning rate: 0.00015586]
		[batch 20/20] avg loss: -0.00655994489392579		[learning rate: 0.00015562]
	Learning Rate: 0.00015562
	LOSS [training: -0.005543396119756084 | validation: -0.004506429850902655]
	TIME [epoch: 8.83 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007478702770058743		[learning rate: 0.00015538]
		[batch 20/20] avg loss: -0.007648554371995029		[learning rate: 0.00015514]
	Learning Rate: 0.000155143
	LOSS [training: -0.007563628571026884 | validation: -0.002536227729979985]
	TIME [epoch: 8.83 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037342985185460876		[learning rate: 0.00015491]
		[batch 20/20] avg loss: -0.008154274855184218		[learning rate: 0.00015467]
	Learning Rate: 0.000154668
	LOSS [training: -0.005944286686865153 | validation: 0.000778438557590214]
	TIME [epoch: 8.86 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003126580371865225		[learning rate: 0.00015443]
		[batch 20/20] avg loss: -0.006799537328301486		[learning rate: 0.00015419]
	Learning Rate: 0.000154194
	LOSS [training: -0.0049630588500833555 | validation: -0.003770713715121563]
	TIME [epoch: 8.84 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0039802404855446664		[learning rate: 0.00015396]
		[batch 20/20] avg loss: -0.004831705656338588		[learning rate: 0.00015372]
	Learning Rate: 0.000153721
	LOSS [training: -0.004405973070941626 | validation: -0.0032378872393178954]
	TIME [epoch: 8.84 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010091020007135824		[learning rate: 0.00015349]
		[batch 20/20] avg loss: -0.00043255233880283643		[learning rate: 0.00015325]
	Learning Rate: 0.00015325
	LOSS [training: -0.005261786172969329 | validation: -0.003585339269918316]
	TIME [epoch: 8.83 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007738797381472035		[learning rate: 0.00015301]
		[batch 20/20] avg loss: -0.003826791672577625		[learning rate: 0.00015278]
	Learning Rate: 0.00015278
	LOSS [training: -0.005782794527024829 | validation: -0.006214438958182192]
	TIME [epoch: 8.83 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006502361538957049		[learning rate: 0.00015255]
		[batch 20/20] avg loss: -0.0025004830615186604		[learning rate: 0.00015231]
	Learning Rate: 0.000152312
	LOSS [training: -0.004501422300237855 | validation: 0.004359653839131394]
	TIME [epoch: 8.86 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010640376730663897		[learning rate: 0.00015208]
		[batch 20/20] avg loss: -0.00010930588988316298		[learning rate: 0.00015184]
	Learning Rate: 0.000151845
	LOSS [training: -0.005374841310273529 | validation: -0.0019147349987252385]
	TIME [epoch: 8.84 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008321014540811172		[learning rate: 0.00015161]
		[batch 20/20] avg loss: -0.00031415571433094264		[learning rate: 0.00015138]
	Learning Rate: 0.000151379
	LOSS [training: -0.004317585127571058 | validation: -0.001307405909140841]
	TIME [epoch: 8.84 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004166903627216593		[learning rate: 0.00015115]
		[batch 20/20] avg loss: -0.006402161710833294		[learning rate: 0.00015092]
	Learning Rate: 0.000150915
	LOSS [training: -0.005284532669024944 | validation: -0.0018282822143148797]
	TIME [epoch: 8.84 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00793678188850966		[learning rate: 0.00015068]
		[batch 20/20] avg loss: -0.0038071069639966024		[learning rate: 0.00015045]
	Learning Rate: 0.000150453
	LOSS [training: -0.005871944426253132 | validation: -0.0030863856751421398]
	TIME [epoch: 8.86 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0065318292949036435		[learning rate: 0.00015022]
		[batch 20/20] avg loss: -0.002501282450289401		[learning rate: 0.00014999]
	Learning Rate: 0.000149991
	LOSS [training: -0.004516555872596522 | validation: -0.0060718834517129]
	TIME [epoch: 8.84 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004422038579136099		[learning rate: 0.00014976]
		[batch 20/20] avg loss: -0.0053043060957286055		[learning rate: 0.00014953]
	Learning Rate: 0.000149532
	LOSS [training: -0.004863172337432354 | validation: 0.001939495771153579]
	TIME [epoch: 8.84 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038762314742826014		[learning rate: 0.0001493]
		[batch 20/20] avg loss: -0.009811170477461306		[learning rate: 0.00014907]
	Learning Rate: 0.000149073
	LOSS [training: -0.006843700975871952 | validation: -0.009971655939603113]
	TIME [epoch: 8.83 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007098472790451881		[learning rate: 0.00014884]
		[batch 20/20] avg loss: -0.00782087851325808		[learning rate: 0.00014862]
	Learning Rate: 0.000148616
	LOSS [training: -0.007459675651854979 | validation: -0.005343446046716533]
	TIME [epoch: 8.85 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006426388634718137		[learning rate: 0.00014839]
		[batch 20/20] avg loss: -0.004308077142494944		[learning rate: 0.00014816]
	Learning Rate: 0.000148161
	LOSS [training: -0.00536723288860654 | validation: 0.0015098950710374973]
	TIME [epoch: 8.85 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012963998192882928		[learning rate: 0.00014793]
		[batch 20/20] avg loss: -0.0011183586393251964		[learning rate: 0.00014771]
	Learning Rate: 0.000147707
	LOSS [training: -0.007041178416104063 | validation: 0.003001773791716533]
	TIME [epoch: 8.84 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00817002938878715		[learning rate: 0.00014748]
		[batch 20/20] avg loss: -0.005096770893694839		[learning rate: 0.00014725]
	Learning Rate: 0.000147254
	LOSS [training: -0.006633400141240994 | validation: -0.0029949498514245717]
	TIME [epoch: 8.84 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017872091587750193		[learning rate: 0.00014703]
		[batch 20/20] avg loss: -0.004499144946647636		[learning rate: 0.0001468]
	Learning Rate: 0.000146802
	LOSS [training: -0.0031431770527113275 | validation: -0.007178513234861387]
	TIME [epoch: 8.84 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008381062191562889		[learning rate: 0.00014658]
		[batch 20/20] avg loss: -0.0026272616978129607		[learning rate: 0.00014635]
	Learning Rate: 0.000146352
	LOSS [training: -0.005504161944687925 | validation: 7.023713220430589e-05]
	TIME [epoch: 8.86 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002565749902875387		[learning rate: 0.00014613]
		[batch 20/20] avg loss: -0.0068276788181839436		[learning rate: 0.0001459]
	Learning Rate: 0.000145904
	LOSS [training: -0.004696714360529665 | validation: -0.00401515696213924]
	TIME [epoch: 8.84 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003614172700426626		[learning rate: 0.00014568]
		[batch 20/20] avg loss: -0.00595613562411976		[learning rate: 0.00014546]
	Learning Rate: 0.000145457
	LOSS [training: -0.004785154162273194 | validation: -0.004696387695467144]
	TIME [epoch: 8.84 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004195506605979152		[learning rate: 0.00014523]
		[batch 20/20] avg loss: -0.005102303298619209		[learning rate: 0.00014501]
	Learning Rate: 0.000145011
	LOSS [training: -0.004648904952299181 | validation: -0.005497901422283697]
	TIME [epoch: 8.84 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003675935778565477		[learning rate: 0.00014479]
		[batch 20/20] avg loss: -0.006679897857440433		[learning rate: 0.00014457]
	Learning Rate: 0.000144566
	LOSS [training: -0.005177916818002955 | validation: 0.003441943227133651]
	TIME [epoch: 8.86 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003883163763547334		[learning rate: 0.00014434]
		[batch 20/20] avg loss: -0.0028748750331051382		[learning rate: 0.00014412]
	Learning Rate: 0.000144123
	LOSS [training: -0.003379019398326237 | validation: 0.0009824965630512033]
	TIME [epoch: 8.83 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005340410959648726		[learning rate: 0.0001439]
		[batch 20/20] avg loss: -0.0062205995725810245		[learning rate: 0.00014368]
	Learning Rate: 0.000143681
	LOSS [training: -0.005780505266114877 | validation: 0.00015440080672836462]
	TIME [epoch: 8.83 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00829998598096095		[learning rate: 0.00014346]
		[batch 20/20] avg loss: -0.006698571171140038		[learning rate: 0.00014324]
	Learning Rate: 0.000143241
	LOSS [training: -0.007499278576050493 | validation: 0.0012132782138957465]
	TIME [epoch: 8.83 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004229569512368509		[learning rate: 0.00014302]
		[batch 20/20] avg loss: -0.0040077741855980105		[learning rate: 0.0001428]
	Learning Rate: 0.000142802
	LOSS [training: -0.0041186718489832595 | validation: -0.002552702923560652]
	TIME [epoch: 8.85 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005671447016679682		[learning rate: 0.00014258]
		[batch 20/20] avg loss: -0.0010526371870724347		[learning rate: 0.00014236]
	Learning Rate: 0.000142364
	LOSS [training: -0.003362042101876059 | validation: -0.003144482281543161]
	TIME [epoch: 8.84 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011383409627170155		[learning rate: 0.00014215]
		[batch 20/20] avg loss: -0.006143891871793769		[learning rate: 0.00014193]
	Learning Rate: 0.000141928
	LOSS [training: -0.008763650749481964 | validation: -0.0006379438251845066]
	TIME [epoch: 8.83 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028922652969724746		[learning rate: 0.00014171]
		[batch 20/20] avg loss: -0.009153554220259362		[learning rate: 0.00014149]
	Learning Rate: 0.000141492
	LOSS [training: -0.006022909758615918 | validation: -0.00034279618155626907]
	TIME [epoch: 8.84 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007285459081578652		[learning rate: 0.00014128]
		[batch 20/20] avg loss: 0.000729479413794369		[learning rate: 0.00014106]
	Learning Rate: 0.000141059
	LOSS [training: -0.0032779898338921405 | validation: -0.0005846495503840487]
	TIME [epoch: 8.85 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003394805328107226		[learning rate: 0.00014084]
		[batch 20/20] avg loss: -0.005607009764338955		[learning rate: 0.00014063]
	Learning Rate: 0.000140626
	LOSS [training: -0.00450090754622309 | validation: 0.00038215737035957015]
	TIME [epoch: 8.84 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006251984605728953		[learning rate: 0.00014041]
		[batch 20/20] avg loss: -0.006981136583850013		[learning rate: 0.0001402]
	Learning Rate: 0.000140195
	LOSS [training: -0.006616560594789484 | validation: -0.003496283967826682]
	TIME [epoch: 8.83 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00252586405130451		[learning rate: 0.00013998]
		[batch 20/20] avg loss: -0.006036081978373306		[learning rate: 0.00013977]
	Learning Rate: 0.000139765
	LOSS [training: -0.004280973014838908 | validation: -0.00023033140598133954]
	TIME [epoch: 8.82 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036923438674456217		[learning rate: 0.00013955]
		[batch 20/20] avg loss: -0.006187890893247116		[learning rate: 0.00013934]
	Learning Rate: 0.000139337
	LOSS [training: -0.004940117380346368 | validation: -0.002813947197121996]
	TIME [epoch: 8.83 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003233769187815489		[learning rate: 0.00013912]
		[batch 20/20] avg loss: -0.002024551004104938		[learning rate: 0.00013891]
	Learning Rate: 0.00013891
	LOSS [training: -0.002629160095960214 | validation: 0.0012103339550469001]
	TIME [epoch: 8.85 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005249531490127774		[learning rate: 0.0001387]
		[batch 20/20] avg loss: -0.005319025176464315		[learning rate: 0.00013848]
	Learning Rate: 0.000138484
	LOSS [training: -0.005284278333296045 | validation: 0.0006095532213695369]
	TIME [epoch: 8.84 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005746675467746867		[learning rate: 0.00013827]
		[batch 20/20] avg loss: -0.00046039590262979713		[learning rate: 0.00013806]
	Learning Rate: 0.00013806
	LOSS [training: -0.003103535685188332 | validation: -0.003842516923605095]
	TIME [epoch: 8.83 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006708928186465168		[learning rate: 0.00013785]
		[batch 20/20] avg loss: -0.011939040837829172		[learning rate: 0.00013764]
	Learning Rate: 0.000137636
	LOSS [training: -0.00932398451214717 | validation: 0.0005147211211532325]
	TIME [epoch: 8.83 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007811503011818837		[learning rate: 0.00013743]
		[batch 20/20] avg loss: -0.005315304645460627		[learning rate: 0.00013721]
	Learning Rate: 0.000137215
	LOSS [training: -0.006563403828639731 | validation: 0.0006829469098899432]
	TIME [epoch: 8.85 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012043341827717776		[learning rate: 0.000137]
		[batch 20/20] avg loss: -0.006261072434569576		[learning rate: 0.00013679]
	Learning Rate: 0.000136794
	LOSS [training: -0.0037327033086706762 | validation: 0.0036857758987741147]
	TIME [epoch: 8.83 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004846120421396134		[learning rate: 0.00013658]
		[batch 20/20] avg loss: -0.005465671154747983		[learning rate: 0.00013637]
	Learning Rate: 0.000136375
	LOSS [training: -0.005155895788072059 | validation: -0.002332055957281157]
	TIME [epoch: 8.83 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005726736031088718		[learning rate: 0.00013617]
		[batch 20/20] avg loss: -0.0057263783054518225		[learning rate: 0.00013596]
	Learning Rate: 0.000135956
	LOSS [training: -0.00572655716827027 | validation: -0.007147309952642103]
	TIME [epoch: 8.83 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004797379824017495		[learning rate: 0.00013575]
		[batch 20/20] avg loss: -0.004797999942796192		[learning rate: 0.00013554]
	Learning Rate: 0.00013554
	LOSS [training: -0.004797689883406843 | validation: -0.00752522704031408]
	TIME [epoch: 8.84 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006566035605255832		[learning rate: 0.00013533]
		[batch 20/20] avg loss: -0.005144017652351829		[learning rate: 0.00013512]
	Learning Rate: 0.000135124
	LOSS [training: -0.005855026628803831 | validation: -0.004012721976347863]
	TIME [epoch: 8.84 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003711771736471035		[learning rate: 0.00013492]
		[batch 20/20] avg loss: -0.003409599808611573		[learning rate: 0.00013471]
	Learning Rate: 0.00013471
	LOSS [training: -0.003560685772541305 | validation: -0.004791787172748476]
	TIME [epoch: 8.84 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004839420723057456		[learning rate: 0.0001345]
		[batch 20/20] avg loss: -0.005791721288402255		[learning rate: 0.0001343]
	Learning Rate: 0.000134297
	LOSS [training: -0.005315571005729854 | validation: 0.003300516719072246]
	TIME [epoch: 8.83 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005135699640135751		[learning rate: 0.00013409]
		[batch 20/20] avg loss: -0.007024786439621717		[learning rate: 0.00013389]
	Learning Rate: 0.000133885
	LOSS [training: -0.006080243039878734 | validation: -0.0034214366939642115]
	TIME [epoch: 8.84 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0040643111463227796		[learning rate: 0.00013368]
		[batch 20/20] avg loss: -0.0068099697494295175		[learning rate: 0.00013347]
	Learning Rate: 0.000133475
	LOSS [training: -0.005437140447876149 | validation: 0.0027513309773174854]
	TIME [epoch: 8.86 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004922034685653473		[learning rate: 0.00013327]
		[batch 20/20] avg loss: -0.0027177932618803204		[learning rate: 0.00013307]
	Learning Rate: 0.000133066
	LOSS [training: -0.003819913973766897 | validation: -0.003026229540456204]
	TIME [epoch: 8.84 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0054942571722190045		[learning rate: 0.00013286]
		[batch 20/20] avg loss: -0.004616126455328605		[learning rate: 0.00013266]
	Learning Rate: 0.000132658
	LOSS [training: -0.005055191813773806 | validation: -0.0031683168255735392]
	TIME [epoch: 8.83 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034276095805475222		[learning rate: 0.00013245]
		[batch 20/20] avg loss: -0.002547433496650298		[learning rate: 0.00013225]
	Learning Rate: 0.000132251
	LOSS [training: -0.0029875215385989098 | validation: -0.006876580281293527]
	TIME [epoch: 8.83 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009401838221015559		[learning rate: 0.00013205]
		[batch 20/20] avg loss: -0.004989487369550737		[learning rate: 0.00013185]
	Learning Rate: 0.000131846
	LOSS [training: -0.0029648355958261465 | validation: 0.00018705040592842718]
	TIME [epoch: 8.85 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004993665604195599		[learning rate: 0.00013164]
		[batch 20/20] avg loss: -0.0109642601895927		[learning rate: 0.00013144]
	Learning Rate: 0.000131442
	LOSS [training: -0.007978962896894152 | validation: -0.003638180956607119]
	TIME [epoch: 8.83 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007282607329707173		[learning rate: 0.00013124]
		[batch 20/20] avg loss: -0.0047039531533381125		[learning rate: 0.00013104]
	Learning Rate: 0.000131039
	LOSS [training: -0.005993280241522644 | validation: -0.006818709654124304]
	TIME [epoch: 8.83 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024930741714440153		[learning rate: 0.00013084]
		[batch 20/20] avg loss: -0.009132180355335317		[learning rate: 0.00013064]
	Learning Rate: 0.000130637
	LOSS [training: -0.005812627263389666 | validation: -0.0017531715317827938]
	TIME [epoch: 8.83 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003163622789370603		[learning rate: 0.00013044]
		[batch 20/20] avg loss: -0.003913814767002305		[learning rate: 0.00013024]
	Learning Rate: 0.000130237
	LOSS [training: -0.0035387187781864547 | validation: -0.0019386317724641555]
	TIME [epoch: 8.85 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006945127787074359		[learning rate: 0.00013004]
		[batch 20/20] avg loss: -0.0029428975209365577		[learning rate: 0.00012984]
	Learning Rate: 0.000129837
	LOSS [training: -0.0049440126540054585 | validation: -0.00437402724487969]
	TIME [epoch: 8.84 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009538162942972009		[learning rate: 0.00012964]
		[batch 20/20] avg loss: -0.0016040005312708133		[learning rate: 0.00012944]
	Learning Rate: 0.000129439
	LOSS [training: -0.005571081737121413 | validation: 0.0018378917917223753]
	TIME [epoch: 8.83 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004538353142074803		[learning rate: 0.00012924]
		[batch 20/20] avg loss: -0.0014423009981431717		[learning rate: 0.00012904]
	Learning Rate: 0.000129043
	LOSS [training: -0.0029903270701089873 | validation: -0.004691310760333628]
	TIME [epoch: 8.87 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006880505187825525		[learning rate: 0.00012884]
		[batch 20/20] avg loss: -0.005174667671951234		[learning rate: 0.00012865]
	Learning Rate: 0.000128647
	LOSS [training: -0.006027586429888379 | validation: -0.0009290333117675516]
	TIME [epoch: 8.84 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005166128642076925		[learning rate: 0.00012845]
		[batch 20/20] avg loss: -0.002948995000631718		[learning rate: 0.00012825]
	Learning Rate: 0.000128253
	LOSS [training: -0.004057561821354321 | validation: -0.00047282237025420525]
	TIME [epoch: 8.84 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008578440639975779		[learning rate: 0.00012806]
		[batch 20/20] avg loss: 0.0015358407286683202		[learning rate: 0.00012786]
	Learning Rate: 0.00012786
	LOSS [training: -0.0035212999556537283 | validation: -0.009857784126808974]
	TIME [epoch: 8.83 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005817420294228116		[learning rate: 0.00012766]
		[batch 20/20] avg loss: -0.0064526161081353865		[learning rate: 0.00012747]
	Learning Rate: 0.000127468
	LOSS [training: -0.0035171790687790992 | validation: -0.006281311996670573]
	TIME [epoch: 8.83 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00514242356972981		[learning rate: 0.00012727]
		[batch 20/20] avg loss: -0.007825525512078442		[learning rate: 0.00012708]
	Learning Rate: 0.000127077
	LOSS [training: -0.006483974540904125 | validation: -0.004666603689295933]
	TIME [epoch: 8.83 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00352810596963045		[learning rate: 0.00012688]
		[batch 20/20] avg loss: -0.00576673994907588		[learning rate: 0.00012669]
	Learning Rate: 0.000126687
	LOSS [training: -0.0046474229593531655 | validation: -0.010177515005844778]
	TIME [epoch: 8.85 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001655172575515801		[learning rate: 0.00012649]
		[batch 20/20] avg loss: -0.007141326495525413		[learning rate: 0.0001263]
	Learning Rate: 0.000126299
	LOSS [training: -0.004398249535520607 | validation: -0.004273858398145546]
	TIME [epoch: 8.83 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0071795143811743236		[learning rate: 0.00012611]
		[batch 20/20] avg loss: -0.002990545419496332		[learning rate: 0.00012591]
	Learning Rate: 0.000125912
	LOSS [training: -0.005085029900335327 | validation: -0.0006587813995542564]
	TIME [epoch: 8.83 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00587519446189897		[learning rate: 0.00012572]
		[batch 20/20] avg loss: -0.002404219266044748		[learning rate: 0.00012553]
	Learning Rate: 0.000125526
	LOSS [training: -0.004139706863971859 | validation: -0.0014619457327924613]
	TIME [epoch: 8.83 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009255032817990838		[learning rate: 0.00012533]
		[batch 20/20] avg loss: -0.0016671379708317828		[learning rate: 0.00012514]
	Learning Rate: 0.000125141
	LOSS [training: -0.005461085394411312 | validation: -0.00474877199631489]
	TIME [epoch: 8.85 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004037570606589587		[learning rate: 0.00012495]
		[batch 20/20] avg loss: -0.0016266047916281689		[learning rate: 0.00012476]
	Learning Rate: 0.000124757
	LOSS [training: -0.002832087699108877 | validation: 0.003702331183967831]
	TIME [epoch: 8.84 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005823215538800612		[learning rate: 0.00012457]
		[batch 20/20] avg loss: -0.0028681033701947437		[learning rate: 0.00012438]
	Learning Rate: 0.000124375
	LOSS [training: -0.004345659454497678 | validation: -0.0017273223848126666]
	TIME [epoch: 8.83 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004688642782770122		[learning rate: 0.00012418]
		[batch 20/20] avg loss: -0.0031124871134748863		[learning rate: 0.00012399]
	Learning Rate: 0.000123994
	LOSS [training: -0.003900564948122505 | validation: -0.000515021428064226]
	TIME [epoch: 8.83 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001424518460003351		[learning rate: 0.0001238]
		[batch 20/20] avg loss: -0.00921023426666537		[learning rate: 0.00012361]
	Learning Rate: 0.000123614
	LOSS [training: -0.0053173763633343595 | validation: -0.001874741683705636]
	TIME [epoch: 8.85 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009708568124019515		[learning rate: 0.00012342]
		[batch 20/20] avg loss: -0.012811920694781534		[learning rate: 0.00012323]
	Learning Rate: 0.000123235
	LOSS [training: -0.006891388753591743 | validation: -0.0037826297474239327]
	TIME [epoch: 8.84 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00502700893887458		[learning rate: 0.00012305]
		[batch 20/20] avg loss: -0.005572836675916971		[learning rate: 0.00012286]
	Learning Rate: 0.000122857
	LOSS [training: -0.0052999228073957756 | validation: -0.001188776743076216]
	TIME [epoch: 8.83 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007070036580965636		[learning rate: 0.00012267]
		[batch 20/20] avg loss: -0.005108815904660103		[learning rate: 0.00012248]
	Learning Rate: 0.00012248
	LOSS [training: -0.006089426242812869 | validation: 0.0005598263488387412]
	TIME [epoch: 8.83 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006624090852358549		[learning rate: 0.00012229]
		[batch 20/20] avg loss: -0.008265301461606084		[learning rate: 0.0001221]
	Learning Rate: 0.000122105
	LOSS [training: -0.007444696156982317 | validation: -0.0034811225167231414]
	TIME [epoch: 8.83 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006075530737728762		[learning rate: 0.00012192]
		[batch 20/20] avg loss: -0.00815565813558276		[learning rate: 0.00012173]
	Learning Rate: 0.000121731
	LOSS [training: -0.00711559443665576 | validation: -0.0045595484259509395]
	TIME [epoch: 8.85 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004273698223421853		[learning rate: 0.00012154]
		[batch 20/20] avg loss: -0.0031941369291300934		[learning rate: 0.00012136]
	Learning Rate: 0.000121357
	LOSS [training: -0.0037339175762759733 | validation: -0.004422298920940706]
	TIME [epoch: 8.83 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00747254604908153		[learning rate: 0.00012117]
		[batch 20/20] avg loss: -0.0029149660448722073		[learning rate: 0.00012099]
	Learning Rate: 0.000120985
	LOSS [training: -0.005193756046976869 | validation: 0.002774326572915692]
	TIME [epoch: 8.84 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004017973373346759		[learning rate: 0.0001208]
		[batch 20/20] avg loss: -0.012498267892205752		[learning rate: 0.00012061]
	Learning Rate: 0.000120615
	LOSS [training: -0.008258120632776256 | validation: -3.85301169835569e-05]
	TIME [epoch: 8.83 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026557656559548274		[learning rate: 0.00012043]
		[batch 20/20] avg loss: -0.0034269029107757247		[learning rate: 0.00012024]
	Learning Rate: 0.000120245
	LOSS [training: -0.0030413342833652763 | validation: -0.006548026131598628]
	TIME [epoch: 8.86 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005851001184204203		[learning rate: 0.00012006]
		[batch 20/20] avg loss: -0.006106837020190377		[learning rate: 0.00011988]
	Learning Rate: 0.000119876
	LOSS [training: -0.00597891910219729 | validation: -0.005734911200578131]
	TIME [epoch: 8.83 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007014075818952864		[learning rate: 0.00011969]
		[batch 20/20] avg loss: -0.0003641764645258191		[learning rate: 0.00011951]
	Learning Rate: 0.000119509
	LOSS [training: -0.003689126141739342 | validation: -0.0006932733588777027]
	TIME [epoch: 8.83 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005078158454123841		[learning rate: 0.00011933]
		[batch 20/20] avg loss: -0.006815090379300716		[learning rate: 0.00011914]
	Learning Rate: 0.000119142
	LOSS [training: -0.005946624416712279 | validation: 0.0006700242480082432]
	TIME [epoch: 8.83 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004756074343530222		[learning rate: 0.00011896]
		[batch 20/20] avg loss: -0.006104660479653969		[learning rate: 0.00011878]
	Learning Rate: 0.000118777
	LOSS [training: -0.0054303674115920965 | validation: -0.0028002781386596153]
	TIME [epoch: 8.85 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005748147993217943		[learning rate: 0.0001186]
		[batch 20/20] avg loss: 0.0013161091756623492		[learning rate: 0.00011841]
	Learning Rate: 0.000118413
	LOSS [training: -0.002216019408777797 | validation: -0.0045235376467502204]
	TIME [epoch: 8.84 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021438608908015996		[learning rate: 0.00011823]
		[batch 20/20] avg loss: -0.0025534107367313676		[learning rate: 0.00011805]
	Learning Rate: 0.00011805
	LOSS [training: -0.002348635813766484 | validation: -0.0047472761241529795]
	TIME [epoch: 8.83 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008286612026073592		[learning rate: 0.00011787]
		[batch 20/20] avg loss: -0.0066827634670959144		[learning rate: 0.00011769]
	Learning Rate: 0.000117688
	LOSS [training: -0.007484687746584754 | validation: -2.060009756438535e-05]
	TIME [epoch: 8.83 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008302412948498811		[learning rate: 0.00011751]
		[batch 20/20] avg loss: -0.0036899967935025493		[learning rate: 0.00011733]
	Learning Rate: 0.000117328
	LOSS [training: -0.005996204871000679 | validation: -0.002320733966673598]
	TIME [epoch: 8.84 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003136167621917753		[learning rate: 0.00011715]
		[batch 20/20] avg loss: -0.0077446973674757825		[learning rate: 0.00011697]
	Learning Rate: 0.000116968
	LOSS [training: -0.005440432494696767 | validation: -0.00409412277290349]
	TIME [epoch: 8.85 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010851495287069942		[learning rate: 0.00011679]
		[batch 20/20] avg loss: -0.0037750858736603104		[learning rate: 0.00011661]
	Learning Rate: 0.000116609
	LOSS [training: -0.007313290580365127 | validation: -0.008371402594123428]
	TIME [epoch: 8.83 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031891422785648472		[learning rate: 0.00011643]
		[batch 20/20] avg loss: -0.006809283263621337		[learning rate: 0.00011625]
	Learning Rate: 0.000116252
	LOSS [training: -0.004999212771093091 | validation: 0.0012022904183824497]
	TIME [epoch: 8.83 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002283504424023574		[learning rate: 0.00011607]
		[batch 20/20] avg loss: -0.007971377954182244		[learning rate: 0.0001159]
	Learning Rate: 0.000115896
	LOSS [training: -0.005127441189102909 | validation: -0.0037688382178949317]
	TIME [epoch: 8.82 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007006039008694363		[learning rate: 0.00011572]
		[batch 20/20] avg loss: -0.0050520255594257504		[learning rate: 0.00011554]
	Learning Rate: 0.00011554
	LOSS [training: -0.006029032284060059 | validation: 0.0024895125103804247]
	TIME [epoch: 8.86 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028917141973111987		[learning rate: 0.00011536]
		[batch 20/20] avg loss: -0.006225929415562328		[learning rate: 0.00011519]
	Learning Rate: 0.000115186
	LOSS [training: -0.004558821806436764 | validation: -0.0063515882501191286]
	TIME [epoch: 8.83 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012290023236867101		[learning rate: 0.00011501]
		[batch 20/20] avg loss: -0.00486857366683643		[learning rate: 0.00011483]
	Learning Rate: 0.000114833
	LOSS [training: -0.008579298451851767 | validation: -0.0040282275299049685]
	TIME [epoch: 8.83 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007380195093871925		[learning rate: 0.00011466]
		[batch 20/20] avg loss: -0.01038829869097217		[learning rate: 0.00011448]
	Learning Rate: 0.000114481
	LOSS [training: -0.008884246892422048 | validation: -0.0013926819514683817]
	TIME [epoch: 8.83 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01037130927322075		[learning rate: 0.00011431]
		[batch 20/20] avg loss: -0.007124959966870481		[learning rate: 0.00011413]
	Learning Rate: 0.00011413
	LOSS [training: -0.008748134620045615 | validation: -0.002589843518663785]
	TIME [epoch: 8.86 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003477175678699205		[learning rate: 0.00011395]
		[batch 20/20] avg loss: -0.009331767641157727		[learning rate: 0.00011378]
	Learning Rate: 0.00011378
	LOSS [training: -0.004839742604513824 | validation: -0.006299751345642358]
	TIME [epoch: 8.83 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008019294751459214		[learning rate: 0.00011361]
		[batch 20/20] avg loss: -0.005318676950795474		[learning rate: 0.00011343]
	Learning Rate: 0.000113431
	LOSS [training: -0.006668985851127343 | validation: -0.010021925623076643]
	TIME [epoch: 8.83 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005410263307823518		[learning rate: 0.00011326]
		[batch 20/20] avg loss: -0.006218326529116442		[learning rate: 0.00011308]
	Learning Rate: 0.000113084
	LOSS [training: -0.005814294918469979 | validation: -0.005369598026752163]
	TIME [epoch: 8.83 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021375205963444998		[learning rate: 0.00011291]
		[batch 20/20] avg loss: -0.008026915881526596		[learning rate: 0.00011274]
	Learning Rate: 0.000112737
	LOSS [training: -0.005082218238935549 | validation: -6.225388991459358e-05]
	TIME [epoch: 8.86 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004927077847120337		[learning rate: 0.00011256]
		[batch 20/20] avg loss: -0.004047249005094832		[learning rate: 0.00011239]
	Learning Rate: 0.000112391
	LOSS [training: -0.004487163426107584 | validation: -0.004792799259272442]
	TIME [epoch: 8.83 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005524620485054083		[learning rate: 0.00011222]
		[batch 20/20] avg loss: -0.008957367959909968		[learning rate: 0.00011205]
	Learning Rate: 0.000112047
	LOSS [training: -0.007240994222482025 | validation: 0.0025580587237609394]
	TIME [epoch: 8.83 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004335162986266973		[learning rate: 0.00011188]
		[batch 20/20] avg loss: -0.008254593975892751		[learning rate: 0.0001117]
	Learning Rate: 0.000111703
	LOSS [training: -0.00629487848107986 | validation: -0.001332224013894065]
	TIME [epoch: 8.82 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009663971924318004		[learning rate: 0.00011153]
		[batch 20/20] avg loss: -0.00010017415637518764		[learning rate: 0.00011136]
	Learning Rate: 0.000111361
	LOSS [training: -0.0048820730403465965 | validation: -0.0007379751255469822]
	TIME [epoch: 8.83 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001750993728749808		[learning rate: 0.00011119]
		[batch 20/20] avg loss: 0.0002698548536204052		[learning rate: 0.00011102]
	Learning Rate: 0.00011102
	LOSS [training: -0.0007405694375647017 | validation: -9.588527400727689e-05]
	TIME [epoch: 8.85 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003162532744267204		[learning rate: 0.00011085]
		[batch 20/20] avg loss: -0.007184733668008647		[learning rate: 0.00011068]
	Learning Rate: 0.000110679
	LOSS [training: -0.005173633206137926 | validation: -0.005440753495483215]
	TIME [epoch: 8.83 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006138591728414392		[learning rate: 0.00011051]
		[batch 20/20] avg loss: -0.0026953407003475808		[learning rate: 0.00011034]
	Learning Rate: 0.00011034
	LOSS [training: -0.0044169662143809865 | validation: -0.008042773342374288]
	TIME [epoch: 8.83 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009599937471147868		[learning rate: 0.00011017]
		[batch 20/20] avg loss: -0.0015434532163235669		[learning rate: 0.00011]
	Learning Rate: 0.000110002
	LOSS [training: -0.005571695343735717 | validation: -0.008662265468341208]
	TIME [epoch: 8.83 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006372688138105408		[learning rate: 0.00010983]
		[batch 20/20] avg loss: -0.007790530892009387		[learning rate: 0.00010966]
	Learning Rate: 0.000109665
	LOSS [training: -0.0070816095150573995 | validation: 0.0007808102912311072]
	TIME [epoch: 8.85 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0074960078056735225		[learning rate: 0.0001095]
		[batch 20/20] avg loss: -0.004551405502427044		[learning rate: 0.00010933]
	Learning Rate: 0.000109328
	LOSS [training: -0.006023706654050283 | validation: -0.004310853600625287]
	TIME [epoch: 8.83 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004698664936296601		[learning rate: 0.00010916]
		[batch 20/20] avg loss: -0.0047929964056130335		[learning rate: 0.00010899]
	Learning Rate: 0.000108993
	LOSS [training: -0.004745830670954817 | validation: -0.006587056521262232]
	TIME [epoch: 8.82 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004605459653305799		[learning rate: 0.00010883]
		[batch 20/20] avg loss: -0.003044645426406573		[learning rate: 0.00010866]
	Learning Rate: 0.000108659
	LOSS [training: -0.0038250525398561865 | validation: -0.004444563547847071]
	TIME [epoch: 8.83 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006210818469141356		[learning rate: 0.00010849]
		[batch 20/20] avg loss: -0.005495445386923749		[learning rate: 0.00010833]
	Learning Rate: 0.000108326
	LOSS [training: -0.005853131928032553 | validation: -0.003584295289427957]
	TIME [epoch: 8.85 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034747272982630106		[learning rate: 0.00010816]
		[batch 20/20] avg loss: -0.008557058514510258		[learning rate: 0.00010799]
	Learning Rate: 0.000107994
	LOSS [training: -0.006015892906386635 | validation: -0.0024209767487257837]
	TIME [epoch: 8.84 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00552650444116949		[learning rate: 0.00010783]
		[batch 20/20] avg loss: -0.0006603151709207887		[learning rate: 0.00010766]
	Learning Rate: 0.000107663
	LOSS [training: -0.0030934098060451387 | validation: -0.006085972912460073]
	TIME [epoch: 8.83 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01025333266081347		[learning rate: 0.0001075]
		[batch 20/20] avg loss: -0.004206797994257228		[learning rate: 0.00010733]
	Learning Rate: 0.000107333
	LOSS [training: -0.007230065327535349 | validation: -0.004289283508478063]
	TIME [epoch: 8.83 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009977686094784288		[learning rate: 0.00010717]
		[batch 20/20] avg loss: -0.004097978998302512		[learning rate: 0.000107]
	Learning Rate: 0.000107004
	LOSS [training: -0.0070378325465434 | validation: -0.005903452079392576]
	TIME [epoch: 8.84 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006065427735362022		[learning rate: 0.00010684]
		[batch 20/20] avg loss: -0.004162868185771815		[learning rate: 0.00010668]
	Learning Rate: 0.000106676
	LOSS [training: -0.005114147960566919 | validation: -0.00019287631711170633]
	TIME [epoch: 8.84 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010023026998504315		[learning rate: 0.00010651]
		[batch 20/20] avg loss: -0.0052292439912335455		[learning rate: 0.00010635]
	Learning Rate: 0.000106349
	LOSS [training: -0.0076261354948689315 | validation: -0.0017406414693256285]
	TIME [epoch: 8.83 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00535284542750993		[learning rate: 0.00010619]
		[batch 20/20] avg loss: -0.012744809956134385		[learning rate: 0.00010602]
	Learning Rate: 0.000106023
	LOSS [training: -0.009048827691822156 | validation: -0.01095805653095667]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r1_20240216_193105/states/model_tr_study2_1981.pth
	Model improved!!!
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011145608855658063		[learning rate: 0.00010586]
		[batch 20/20] avg loss: -0.0058051202266837885		[learning rate: 0.0001057]
	Learning Rate: 0.000105698
	LOSS [training: -0.008475364541170923 | validation: -0.0075865248790513255]
	TIME [epoch: 8.83 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011545581630844421		[learning rate: 0.00010554]
		[batch 20/20] avg loss: -0.00792383601849193		[learning rate: 0.00010537]
	Learning Rate: 0.000105374
	LOSS [training: -0.009734708824668176 | validation: -0.006479329479673573]
	TIME [epoch: 8.85 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00731418610161777		[learning rate: 0.00010521]
		[batch 20/20] avg loss: -0.006381207568770293		[learning rate: 0.00010505]
	Learning Rate: 0.000105051
	LOSS [training: -0.006847696835194032 | validation: -0.0007235135512748571]
	TIME [epoch: 8.83 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006295698316239129		[learning rate: 0.00010489]
		[batch 20/20] avg loss: -0.005257246371888906		[learning rate: 0.00010473]
	Learning Rate: 0.000104729
	LOSS [training: -0.005776472344064018 | validation: 0.008583862845415055]
	TIME [epoch: 8.83 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004825490832767445		[learning rate: 0.00010457]
		[batch 20/20] avg loss: -0.004930291432400271		[learning rate: 0.00010441]
	Learning Rate: 0.000104408
	LOSS [training: -0.0048778911325838585 | validation: -0.0060928699925983]
	TIME [epoch: 8.83 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006294534730957665		[learning rate: 0.00010425]
		[batch 20/20] avg loss: -0.001563974066411818		[learning rate: 0.00010409]
	Learning Rate: 0.000104088
	LOSS [training: -0.0039292543986847416 | validation: -0.002151766945878225]
	TIME [epoch: 8.85 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034342668341233685		[learning rate: 0.00010393]
		[batch 20/20] avg loss: -0.004290316373199062		[learning rate: 0.00010377]
	Learning Rate: 0.000103769
	LOSS [training: -0.003862291603661216 | validation: -0.0013762891492968175]
	TIME [epoch: 8.83 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006899563623319514		[learning rate: 0.00010361]
		[batch 20/20] avg loss: -0.0052709579227216835		[learning rate: 0.00010345]
	Learning Rate: 0.000103451
	LOSS [training: -0.006085260773020598 | validation: -0.0037499954578924357]
	TIME [epoch: 8.83 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005079598637692822		[learning rate: 0.00010329]
		[batch 20/20] avg loss: -0.004938941752869049		[learning rate: 0.00010313]
	Learning Rate: 0.000103134
	LOSS [training: -0.005009270195280936 | validation: -0.007378115969656955]
	TIME [epoch: 8.83 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009435119639699126		[learning rate: 0.00010298]
		[batch 20/20] avg loss: -0.00018302250835559872		[learning rate: 0.00010282]
	Learning Rate: 0.000102817
	LOSS [training: -0.004809071074027362 | validation: -0.0044133701204999085]
	TIME [epoch: 8.84 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005888197513335368		[learning rate: 0.00010266]
		[batch 20/20] avg loss: -0.00547982857975815		[learning rate: 0.0001025]
	Learning Rate: 0.000102502
	LOSS [training: -0.0056840130465467585 | validation: -0.0006602415034752884]
	TIME [epoch: 8.84 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002235939022240234		[learning rate: 0.00010235]
		[batch 20/20] avg loss: -0.010234846484672449		[learning rate: 0.00010219]
	Learning Rate: 0.000102188
	LOSS [training: -0.006235392753456341 | validation: -0.003581746881548969]
	TIME [epoch: 8.82 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006669950503050502		[learning rate: 0.00010203]
		[batch 20/20] avg loss: -0.0067243725358490615		[learning rate: 0.00010187]
	Learning Rate: 0.000101875
	LOSS [training: -0.00669716151944978 | validation: -0.0004412552451932769]
	TIME [epoch: 8.83 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005203571095179509		[learning rate: 0.00010172]
		[batch 20/20] avg loss: -0.008229642906329665		[learning rate: 0.00010156]
	Learning Rate: 0.000101562
	LOSS [training: -0.006716607000754585 | validation: -0.0012407089813394816]
	TIME [epoch: 8.83 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004799460223834194		[learning rate: 0.00010141]
		[batch 20/20] avg loss: -0.00923618451712067		[learning rate: 0.00010125]
	Learning Rate: 0.000101251
	LOSS [training: -0.007017822370477435 | validation: 0.0014679562787655105]
	TIME [epoch: 8.85 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031183872100430257		[learning rate: 0.0001011]
		[batch 20/20] avg loss: -0.010143973842857361		[learning rate: 0.00010094]
	Learning Rate: 0.000100941
	LOSS [training: -0.006631180526450194 | validation: -0.0031516082934988075]
	TIME [epoch: 8.82 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001226765115421242		[learning rate: 0.00010079]
		[batch 20/20] avg loss: -0.0028272697175777996		[learning rate: 0.00010063]
	Learning Rate: 0.000100631
	LOSS [training: -0.0020270174164995205 | validation: -0.0052485734687926205]
	TIME [epoch: 8.83 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00842965344229565		[learning rate: 0.00010048]
		[batch 20/20] avg loss: -0.009428259623404437		[learning rate: 0.00010032]
	Learning Rate: 0.000100323
	LOSS [training: -0.008928956532850047 | validation: -0.004125910844374682]
	TIME [epoch: 8.82 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013274206619502326		[learning rate: 0.00010017]
		[batch 20/20] avg loss: -0.00020842563108099932		[learning rate: 0.00010002]
	Learning Rate: 0.000100015
	LOSS [training: -0.006741316125291663 | validation: 0.00010925851887126658]
	TIME [epoch: 8.86 sec]
Finished training in 17810.260 seconds.
